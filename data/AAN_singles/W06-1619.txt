Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,Sydney, July 2006. c?2006 Association for Computational LinguisticsExtremely Lexicalized Models for Accurate and Fast HPSG ParsingTakashi NinomiyaInformation Technology CenterUniversity of TokyoTakuya MatsuzakiDepartment of Computer ScienceUniversity of TokyoYoshimasa TsuruokaSchool of InformaticsUniversity of ManchesterYusuke MiyaoDepartment of Computer ScienceUniversity of TokyoJun?ichi TsujiiDepartment of Computer Science, University of TokyoSchool of Informatics, University of ManchesterSORST, Japan Science and Technology AgencyHongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper describes an extremely lexi-calized probabilistic model for fast andaccurate HPSG parsing.
In this model,the probabilities of parse trees are de-fined with only the probabilities of select-ing lexical entries.
The proposed modelis very simple, and experiments revealedthat the implemented parser runs aroundfour times faster than the previous modeland that the proposed model has a highaccuracy comparable to that of the previ-ous model for probabilistic HPSG, whichis defined over phrase structures.
Wealso developed a hybrid of our probabilis-tic model and the conventional phrase-structure-based model.
The hybrid modelis not only significantly faster but also sig-nificantly more accurate by two points ofprecision and recall compared to the pre-vious model.1 IntroductionFor the last decade, accurate and wide-coverageparsing for real-world text has been intensivelyand extensively pursued.
In most of state-of-the-art parsers, probabilistic events are defined overphrase structures because phrase structures aresupposed to dominate syntactic configurations ofsentences.
For example, probabilities were de-fined over grammar rules in probabilistic CFG(Collins, 1999; Klein and Manning, 2003; Char-niak and Johnson, 2005) or over complex phrasestructures of head-driven phrase structure gram-mar (HPSG) or combinatory categorial grammar(CCG) (Clark and Curran, 2004b; Malouf and vanNoord, 2004; Miyao and Tsujii, 2005).
Althoughthese studies vary in the design of the probabilisticmodels, the fundamental conception of probabilis-tic modeling is intended to capture characteristicsof phrase structures or grammar rules.
Althoughlexical information, such as head words, is knownto significantly improve the parsing accuracy, itwas also used to augment information on phrasestructures.Another interesting approach to this problemwas using supertagging (Clark and Curran, 2004b;Clark and Curran, 2004a; Wang and Harper, 2004;Nasr and Rambow, 2004), which was originallydeveloped for lexicalized tree adjoining grammars(LTAG) (Bangalore and Joshi, 1999).
Supertag-ging is a process where words in an input sen-tence are tagged with ?supertags,?
which are lex-ical entries in lexicalized grammars, e.g., elemen-tary trees in LTAG, lexical categories in CCG,and lexical entries in HPSG.
Supertagging was,in the first place, a technique to reduce the costof parsing with lexicalized grammars; ambiguityin assigning lexical entries to words is reducedby the light-weight process of supertagging be-fore the heavy process of parsing.
Bangalore andJoshi (1999) claimed that if words can be assignedcorrect supertags, syntactic parsing is almost triv-ial.
What this means is that if supertags are cor-rectly assigned, syntactic structures are almost de-155termined because supertags include rich syntac-tic information such as subcategorization frames.Nasr and Rambow (2004) showed that the accu-racy of LTAG parsing reached about 97%, assum-ing that the correct supertags were given.
Theconcept of supertagging is simple and interesting,and the effects of this were recently demonstratedin the case of a CCG parser (Clark and Curran,2004a) with the result of a drastic improvement inthe parsing speed.
Wang and Harper (2004) alsodemonstrated the effects of supertagging with astatistical constraint dependency grammar (CDG)parser.
They achieved accuracy as high as thestate-of-the-art parsers.
However, a supertagger it-self was used as an external tagger that enumeratescandidates of lexical entries or filters out unlikelylexical entries just to help parsing, and the bestparse trees were selected mainly according to theprobabilistic model for phrase structures or depen-dencies with/without the probabilistic model forsupertagging.We investigate an extreme case of HPSG pars-ing in which the probabilistic model is definedwith only the probabilities of lexical entry selec-tion; i.e., the model is never sensitive to charac-teristics of phrase structures.
The model is simplydefined as the product of the supertagging proba-bilities, which are provided by the discriminativemethod with machine learning features of wordtrigrams and part-of-speech (POS) 5-grams as de-fined in the CCG supertagging (Clark and Curran,2004a).
The model is implemented in an HPSGparser instead of the phrase-structure-based prob-abilistic model; i.e., the parser returns the parsetree assigned the highest probability of supertag-ging among the parse trees licensed by an HPSG.Though the model uses only the probabilities oflexical entry selection, the experiments revealedthat it was as accurate as the previous phrase-structure-based model.
Interestingly, this meansthat accurate parsing is possible using rather sim-ple mechanisms.We also tested a hybrid model of the su-pertagging and the previous phrase-structure-based probabilistic model.
In the hybrid model,the probabilities of the previous model are mul-tiplied by the supertagging probabilities insteadof a preliminary probabilistic model, which is in-troduced to help the process of estimation by fil-tering unlikely lexical entries (Miyao and Tsujii,2005).
In the previous model, the preliminaryprobabilistic model is defined as the probabilityof unigram supertagging.
So, the hybrid modelcan be regarded as an extension of supertaggingfrom unigram to n-gram.
The hybrid model canalso be regarded as a variant of the statistical CDGparser (Wang, 2003; Wang and Harper, 2004), inwhich the parse tree probabilities are defined asthe product of the supertagging probabilities andthe dependency probabilities.
In the experiments,we observed that the hybrid model significantlyimproved the parsing speed, by around three tofour times speed-ups, and accuracy, by around twopoints in both precision and recall, over the pre-vious model.
This implies that finer probabilisticmodel of lexical entry selection can improve thephrase-structure-based model.2 HPSG and probabilistic modelsHPSG (Pollard and Sag, 1994) is a syntactic the-ory based on lexicalized grammar formalism.
InHPSG, a small number of schemata describe gen-eral construction rules, and a large number oflexical entries express word-specific characteris-tics.
The structures of sentences are explained us-ing combinations of schemata and lexical entries.Both schemata and lexical entries are representedby typed feature structures, and constraints repre-sented by feature structures are checked with uni-fication.An example of HPSG parsing of the sentence?Spring has come?
is shown in Figure 1.
First,each of the lexical entries for ?has?
and ?come?is unified with a daughter feature structure of theHead-Complement Schema.
Unification providesthe phrasal sign of the mother.
The sign of thelarger constituent is obtained by repeatedly apply-ing schemata to lexical/phrasal signs.
Finally, theparse result is output as a phrasal sign that domi-nates the sentence.Given a set W of words and a set F of featurestructures, an HPSG is formulated as a tuple, G =?L,R?, whereL = {l = ?w,F ?|w ?
W, F ?
F} is a set oflexical entries, andR is a set of schemata; i.e., r ?
R is a partialfunction: F ?
F ?
F .Given a sentence, an HPSG computes a set ofphrasal signs, i.e., feature structures, as a result ofparsing.
Note that HPSG is one of the lexicalizedgrammar formalisms, in which lexical entries de-termine the dominant syntactic structures.156SpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2head-compHEAD  verbSUBJ  < >COMPS  < >HEAD  nounSUBJ  < >COMPS  < >1=?SpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2HEAD  verbSUBJ  <    >COMPS  < >1HEAD  verbSUBJ  < >COMPS  < >1subject-headhead-compFigure 1: HPSG parsing.Previous studies (Abney, 1997; Johnson et al,1999; Riezler et al, 2000; Malouf and van Noord,2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)defined a probabilistic model of unification-basedgrammars including HPSG as a log-linear modelor maximum entropy model (Berger et al, 1996).The probability that a parse result T is assigned toa given sentence w = ?w1, .
.
.
, wn?
isphpsg(T |w) = 1Zw exp(?u?ufu(T ))Zw =?T ?exp(?u?ufu(T ?
)),where ?u is a model parameter, fu is a featurefunction that represents a characteristic of parsetree T , and Zw is the sum over the set of all pos-sible parse trees for the sentence.
Intuitively, theprobability is defined as the normalized productof the weights exp(?u) when a characteristic cor-responding to fu appears in parse result T .
Themodel parameters, ?u, are estimated using numer-ical optimization methods (Malouf, 2002) to max-imize the log-likelihood of the training data.However, the above model cannot be easily es-timated because the estimation requires the com-putation of p(T |w) for all parse candidates as-signed to sentence w. Because the number ofparse candidates is exponentially related to thelength of the sentence, the estimation is intractablefor long sentences.
To make the model estimationtractable, Geman and Johnson (Geman and John-son, 2002) and Miyao and Tsujii (Miyao and Tsu-jii, 2002) proposed a dynamic programming algo-rithm for estimating p(T |w).
Miyao and TsujiiHEAD  verbSUBJ  <>COMPS <>HEAD  nounSUBJ  <>COMPS <>HEAD  verbSUBJ  <   >COMPS <>HEAD  verbSUBJ  <   >COMPS <   >HEAD  verbSUBJ  <   >COMPS <>subject-headhead-compSpring/NN has/VBZ come/VBN11 11 22froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>fbinary=head-comp, 1, 0,1, VP, has, VBZ,                    ,1, VP, come, VBN,HEAD  verbSUBJ  <NP>COMPS <VP>HEAD  verbSUBJ  <NP>COMPS <>flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>Figure 2: Example of features.
(2005) also introduced a preliminary probabilisticmodel p0(T |w) whose estimation does not requirethe parsing of a treebank.
This model is intro-duced as a reference distribution of the probabilis-tic HPSG model; i.e., the computation of parsetrees given low probabilities by the model is omit-ted in the estimation stage.
We have(Previous probabilistic HPSG)phpsg?
(T |w) = p0(T |w) 1Zw exp(?u?ufu(T ))Zw =?T ?p0(T ?|w) exp(?u?ufu(T ?
))p0(T |w) =n?i=1p(li|wi),where li is a lexical entry assigned to word wi in Tand p(li|wi) is the probability of selecting lexicalentry li for wi.In the experiments, we compared our modelwith the probabilistic HPSG model of Miyao andTsujii (2005).
The features used in their model arecombinations of the feature templates listed in Ta-ble 1.
The feature templates fbinary and funaryare defined for constituents at binary and unarybranches, froot is a feature template set for theroot nodes of parse trees, and flex is a feature tem-plate set for calculating the preliminary probabilis-tic model.
An example of features applied to theparse tree for the sentence ?Spring has come?
isshown in Figure 2.157fbinary =?
r, d, c,spl, syl, hwl, hpl, hll,spr, syr, hwr, hpr, hlr?funary = ?r, sy, hw, hp, hl?froot = ?sy, hw, hp, hl?flex = ?wi, pi, li?combinations of feature templates for fbinary?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?combinations of feature templates for funary?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?combinations of feature templates for froot?hw, hp, hl?, ?hw, hp?, ?hw, hl?,?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?combinations of feature templates for flex?wi, pi, li?, ?pi, li?r name of the applied schemad distance between the head words of the daughtersc whether a comma exists between daughtersand/or inside daughter phrasessp number of words dominated by the phrasesy symbol of the phrasal categoryhw surface form of the head wordhp part-of-speech of the head wordhl lexical entry assigned to the head wordwi i-th wordpi part-of-speech for wili lexical entry for wiTable 1: Features.3 Extremely lexicalized probabilisticmodelsIn the experiments, we tested parsing with the pre-vious model for the probabilistic HPSG explainedin Section 2 and other three types of probabilis-tic models defined with the probabilities of lexi-cal entry selection.
The first one is the simplestprobabilistic model, which is defined with onlythe probabilities of lexical entry selection.
It isdefined simply as the product of the probabilitiesof selecting all lexical entries in the sentence; i.e.,the model does not use the probabilities of phrasestructures like the previous models.Given a set of lexical entries, L, a sentence,w = ?w1, .
.
.
, wn?, and the probabilistic modelof lexical entry selection, p(li ?
L|w, i), the firstmodel is formally defined as follows:(Model 1)pmodel1(T |w) =n?i=1p(li|w, i),where li is a lexical entry assigned to word wiin T and p(li|w, i) is the probability of selectinglexical entry li for wi.The second model is defined as the product ofthe probabilities of selecting all lexical entries inthe sentence and the root node probability of theparse tree.
That is, the second model is also de-fined without the probabilities on phrase struc-tures:(Model 2)pmodel2(T |w) =1Zmodel2 pmodel1(T |w) exp???
?u(fu?froot)?ufu(T )??
?Zmodel2 =?T ?pmodel1(T ?|w) exp???
?u(fu?froot)?ufu(T ?)???
,where Zmodel2 is the sum over the set of all pos-sible parse trees for the sentence.The third model is a hybrid of model 1 and theprevious model.
The probabilities of the lexicalentries in the previous model are replaced with theprobabilities of lexical entry selection:(Model 3)pmodel3(T |w) =1Zmodel3 pmodel1(T |w) exp(?u?ufu(T ))Zmodel3 =?T ?pmodel1(T ?|w) exp(?u?ufu(T ?
)).In this study, the same model parameters usedin the previous model were used for phrase struc-tures.The probabilities of lexical entry selection,p(li|w, i), are defined as follows:(Probabilistic Model of Lexical Entry Selection)p(li|w, i) = 1Zw exp(?u?ufu(li,w, i))158fexlex =?wi?1, wi, wi+1,pi?2, pi?1, pi, pi+1, pi+2?combinations of feature templates?wi?1?, ?wi?, ?wi+1?,?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,?wi?1, wi?, ?wi, wi+1?,?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?Table 2: Features for the probabilities of lexicalentry selection.procedure Parsing(?w1, .
.
.
, wn?, ?L,R?, ?, ?, ?, ?, ?
)for i = 1 to nforeach F ?
?
{F |?wi, F ?
?
L}p =?u ?ufu(F?)pi[i?
1, i] ?
pi[i?
1, i] ?
{F ?
}if (p > ?[i?
1, i, F ?])
then?[i?
1, i, F ?]
?
pLocalThresholding(i?
1, i,?, ?
)for d = 1 to nfor i = 0 to n?
dj = i+ dfor k = i+ 1 to j ?
1foreach Fs ?
?
[i, k], Ft ?
?
[k, j], r ?
Rif F = r(Fs, Ft) has succeededp = ?
[i, k, Fs] + ?
[k, j, Ft] +?u ?ufu(F )pi[i, j] ?
pi[i, j] ?
{F}if (p > ?
[i, j, F ]) then?
[i, j, F ] ?
pLocalThresholding(i, j,?, ?
)GlobalThresholding(i, n, ?
)procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ?
?, ?
?, ??,?
?, ?
?, ?last, ?last, ?last, ?last, ?last)?
?
?0; ?
?
?0; ?
?
?0; ?
?
?0; ?
?
?0;loop while ?
?
?last and ?
?
?last and ?
?
?last and ?
?
?lastand ?
?
?lastcall Parsing(w, G, ?, ?, ?, ?, ?
)if pi[1, n] 6= ?
then exit?
?
?+??
; ?
?
?
+??;?
?
?+??
; ?
?
?
+??
; ?
?
?
+??
;Figure 3: Pseudo-code of iterative parsing forHPSG.Zw =?l?exp(?u?ufu(l?,w, i)),where Zw is the sum over all possible lexical en-tries for the word wi.
The feature templates usedin our model are listed in Table 2 and are wordtrigrams and POS 5-grams.4 Experiments4.1 ImplementationWe implemented the iterative parsing algorithm(Ninomiya et al, 2005) for the probabilistic HPSGmodels.
It first starts parsing with a narrow beam.If the parsing fails, then the beam is widened, andparsing continues until the parser outputs resultsor the beam width reaches some limit.
Thoughthe probabilities of lexical entry selection are in-troduced, the algorithm for the presented proba-bilistic models is almost the same as the originaliterative parsing algorithm.The pseudo-code of the algorithm is shown inFigure 3.
In the figure, the pi[i, j] representsthe set of partial parse results that cover wordswi+1, .
.
.
, wj , and ?
[i, j, F ] stores the maximumfigure-of-merit (FOM) of partial parse result Fat cell (i, j).
The probability of lexical entryF is computed as ?u ?ufu(F ) for the previousmodel, as shown in the figure.
The probabilityof a lexical entry for models 1, 2, and 3 is com-puted as the probability of lexical entry selection,p(F |w, i).
The FOM of a newly created partialparse, F , is computed by summing the values of?
of the daughters and an additional FOM of F ifthe model is the previous model or model 3.
TheFOM for models 1 and 2 is computed by only sum-ming the values of ?
of the daughters; i.e., weightsexp(?u) in the figure are assigned zero.
The terms?
and ?
are the thresholds of the number of phrasalsigns in the chart cell and the beam width for signsin the chart cell.
The terms ?
and ?
are the thresh-olds of the number and the beam width of lexicalentries, and ?
is the beam width for global thresh-olding (Goodman, 1997).4.2 EvaluationWe evaluated the speed and accuracy of parsingwith extremely lexicalized models by using Enju2.1, the HPSG grammar for English (Miyao et al,2005; Miyao and Tsujii, 2005).
The lexicon ofthe grammar was extracted from Sections 02-21 ofthe Penn Treebank (Marcus et al, 1994) (39,832sentences).
The grammar consisted of 3,797 lex-ical entries for 10,536 words1.
The probabilis-tic models were trained using the same portion ofthe treebank.
We used beam thresholding, globalthresholding (Goodman, 1997), preserved iterativeparsing (Ninomiya et al, 2005) and other tech-1An HPSG treebank is automatically generated from thePenn Treebank.
Those lexical entries were generated by ap-plying lexical rules to observed lexical entries in the HPSGtreebank (Nakanishi et al, 2004).
The lexicon, however, in-cluded many lexical entries that do not appear in the HPSGtreebank.
The HPSG treebank is used for training the prob-abilistic model for lexical entry selection, and hence, thoselexical entries that do not appear in the treebank are rarelyselected by the probabilistic model.
The ?effective?
tag setsize, therefore, is around 1,361, the number of lexical entrieswithout those never-seen lexical entries.159No.
of tested sentences Total No.
of Avg.
length of tested sentences?
40 words ?
100 words sentences ?
40 words ?
100 wordsSection 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0Table 3: Statistics of the Penn Treebank.Section 23 (?
40 + Gold POSs) Section 23 (?
100 + Gold POSs)LP LR UP UR Avg.
time LP LR UP UR Avg.
time(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152Section 23 (?
40 + POS tagger) Section 23 (?
100 + POS tagger)LP LR UP UR Avg.
time LP LR UP UR Avg.
time(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183Table 4: Experimental results for Section 23.niques for deep parsing2.
The parameters for beamsearching were determined manually by trial anderror using Section 22: ?0 = 4,??
= 4, ?last =20, ?0 = 1.0,??
= 2.5, ?last = 11.0, ?0 =12,??
= 4, ?last = 28, ?0 = 6.0,??
=2.25, ?last = 15.0, ?0 = 8.0,??
= 3.0, and?last = 20.0.
With these thresholding parame-ters, the parser iterated at most five times for eachsentence.We measured the accuracy of the predicate-argument relations output of the parser.
Apredicate-argument relation is defined as a tu-ple ?
?,wh, a, wa?, where ?
is the predicate type(e.g., adjective, intransitive verb), wh is the headword of the predicate, a is the argument label(MODARG, ARG1, ..., ARG4), and wa is thehead word of the argument.
Labeled precision(LP)/labeled recall (LR) is the ratio of tuples cor-rectly identified by the parser3.
Unlabeled pre-cision (UP)/unlabeled recall (UR) is the ratio oftuples without the predicate type and the argu-ment label.
This evaluation scheme was thesame as used in previous evaluations of lexicalizedgrammars (Hockenmaier, 2003; Clark and Cur-2Deep parsing techniques include quick check (Maloufet al, 2000) and large constituent inhibition (Kaplan et al,2004) as described by Ninomiya et al (2005), but hybridparsing with a CFG chunk parser was not used.
This is be-cause we did not observe a significant improvement for thedevelopment set by the hybrid parsing and observed only asmall improvement in the parsing speed by around 10 ms.3When parsing fails, precision and recall are evaluated,although nothing is output by the parser; i.e., recall decreasesgreatly.ran, 2004b; Miyao and Tsujii, 2005).
The ex-periments were conducted on an AMD Opteronserver with a 2.4-GHz CPU.
Section 22 of theTreebank was used as the development set, andthe performance was evaluated using sentences of?
40 and 100 words in Section 23.
The perfor-mance of each parsing technique was analyzed us-ing the sentences in Section 24 of ?
100 words.Table 3 details the numbers and average lengths ofthe tested sentences of ?
40 and 100 words in Sec-tions 23 and 24, and the total numbers of sentencesin Sections 23 and 24.The parsing performance for Section 23 isshown in Table 4.
The upper half of the tableshows the performance using the correct POSs inthe Penn Treebank, and the lower half shows theperformance using the POSs given by a POS tag-ger (Tsuruoka and Tsujii, 2005).
The left andright sides of the table show the performances forthe sentences of ?
40 and ?
100 words.
Ourmodels significantly increased not only the pars-ing speed but also the parsing accuracy.
Model3 was around three to four times faster and hadaround two points higher precision and recall thanthe previous model.
Surprisingly, model 1, whichused only lexical information, was very fast andas accurate as the previous model.
Model 2 alsoimproved the accuracy slightly without informa-tion of phrase structures.
When the automatic POStagger was introduced, both precision and recalldropped by around 2 points, but the tendency to-wards improved speed and accuracy was again ob-16076.00%78.00%80.00%82.00%84.00%86.00%88.00%0 100 200 300 400 500 600 700 800 900Parsing time (ms/sentence)F-scoreprevious modelmodel 1model 2model 3Figure 4: F-score versus average parsing time for sentences in Section 24 of ?
100 words.served.The unlabeled precisions and recalls of the pre-vious model and models 1, 2, and 3 were signifi-cantly different as measured using stratified shuf-fling tests (Cohen, 1995) with p-values < 0.05.The labeled precisions and recalls were signifi-cantly different among models 1, 2, and 3 andbetween the previous model and model 3, butwere not significantly different between the previ-ous model and model 1 and between the previousmodel and model 2.The average parsing time and labeled F-scorecurves of each probabilistic model for the sen-tences in Section 24 of?
100 words are graphed inFigure 4.
The superiority of our models is clearlyobserved in the figure.
Model 3 performed sig-nificantly better than the previous model.
Models1 and 2 were significantly faster with almost thesame accuracy as the previous model.5 Discussion5.1 SupertaggingOur probabilistic model of lexical entry selectioncan be used as an independent classifier for select-ing lexical entries, which is called the supertag-ger (Bangalore and Joshi, 1999; Clark and Curran,2004b).
The CCG supertagger uses a maximumentropy classifier and is similar to our model.We evaluated the performance of our probabilis-tic model as a supertagger.
The accuracy of the re-sulting supertagger on our development set (Sec-tion 22) is given in Table 5 and Table 6.
The testsentences were automatically POS-tagged.
Re-sults of other supertaggers for automatically ex-test data accuracy (%)HPSG supertagger 22 87.51(this paper)CCG supertagger 00/23 91.70 / 91.45(Curran and Clark, 2003)LTAG supertagger 22/23 86.01 / 86.27(Shen and Joshi, 2003)Table 5: Accuracy of single-tag supertaggers.
Thenumbers under ?test data?
are the PTB sectionnumbers of the test data.?
tags/word word acc.
(%) sentence acc.
(%)1e-1 1.30 92.64 34.981e-2 2.11 95.08 46.111e-3 4.66 96.22 51.951e-4 10.72 96.83 55.661e-5 19.93 96.95 56.20Table 6: Accuracy of multi-supertagging.tracted lexicalized grammars are listed in Table 5.Table 6 gives the average number of supertags as-signed to a word, the per-word accuracy, and thesentence accuracy for several values of ?, which isa parameter to determine how many lexical entriesare assigned.When compared with other supertag sets of au-tomatically extracted lexicalized grammars, the(effective) size of our supertag set, 1,361 lexicalentries, is between the CCG supertag set (398 cat-egories) used by Curran and Clark (2003) and theLTAG supertag set (2920 elementary trees) usedby Shen and Joshi (2003).
The relative order basedon the sizes of the tag sets exactly matches the or-der based on the accuracies of corresponding su-pertaggers.1615.2 Efficacy of extremely lexicalized modelsThe implemented parsers of models 1 and 2 werearound four times faster than the previous modelwithout a loss of accuracy.
However, what sur-prised us is not the speed of the models, butthe fact that they were as accurate as the previ-ous model, though they do not use any phrase-structure-based probabilities.
We think that thecorrect parse is more likely to be selected if thecorrect lexical entries are assigned high probabil-ities because lexical entries include specific infor-mation about subcategorization frames and syn-tactic alternation, such as wh-movement and pas-sivization, that likely determines the dominantstructures of parse trees.
Another possible rea-son for the accuracy is the constraints placed byunification-based grammars.
That is, incorrectparse trees were suppressed by the constraints.The best performer in terms of speed and ac-curacy was model 3.
The increased speed was,of course, possible for the same reasons as thespeeds of models 1 and 2.
An unexpected butvery impressive result was the significant improve-ment of accuracy by two points in precision andrecall, which is hard to attain by tweaking param-eters or hacking features.
This may be becausethe phrase structure information and lexical in-formation complementarily improved the model.The lexical information includes more specific in-formation about the syntactic alternation, and thephrase structure information includes informationabout the syntactic structures, such as the dis-tances of head words or the sizes of phrases.Nasr and Rambow (2004) showed that the accu-racy of LTAG parsing reached about 97%, assum-ing that the correct supertags were given.
We ex-emplified the dominance of lexical information inreal syntactic parsing, i.e., syntactic parsing with-out gold-supertags, by showing that the proba-bilities of lexical entry selection dominantly con-tributed to syntactic parsing.The CCG supertagging demonstrated fast andaccurate parsing for the probabilistic CCG (Clarkand Curran, 2004a).
They used the supertag-ger for eliminating candidates of lexical entries,and the probabilities of parse trees were calcu-lated using the phrase-structure-based model with-out the probabilities of lexical entry selection.
Ourstudy is essentially different from theirs in that theprobabilities of lexical entry selection have beendemonstrated to dominantly contribute to the dis-ambiguation of phrase structures.We have not yet investigated whether our resultscan be reproduced with other lexicalized gram-mars.
Our results might hold only for HPSG be-cause HPSG has strict feature constraints and haslexical entries with rich syntactic information suchas wh-movement.6 ConclusionWe developed an extremely lexicalized probabilis-tic model for fast and accurate HPSG parsing.The model is very simple.
The probabilities ofparse trees are defined with only the probabili-ties of selecting lexical entries, which are trainedby the discriminative methods in the log-linearmodel with features of word trigrams and POS 5-grams as defined in the CCG supertagging.
Ex-periments revealed that the model achieved im-pressive accuracy as high as that of the previousmodel for the probabilistic HPSG and that the im-plemented parser runs around four times faster.This indicates that accurate and fast parsing is pos-sible using rather simple mechanisms.
In addi-tion, we provided another probabilistic model, inwhich the probabilities for the leaf nodes in a parsetree are given by the probabilities of supertag-ging, and the probabilities for the intermediatenodes are given by the previous phrase-structure-based model.
The experiments demonstrated notonly speeds significantly increased by three to fourtimes but also impressive improvement in parsingaccuracy by around two points in precision and re-call.We hope that this research provides a novel ap-proach to deterministic parsing in which only lex-ical selection and little phrasal information with-out packed representations dominates the parsingstrategy.ReferencesSteven P. Abney.
1997.
Stochastic attribute-valuegrammars.
Computational Linguistics, 23(4):597?618.Srinivas Bangalore and Aravind Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?265.Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguis-tics, 22(1):39?71.162Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proc.
of ACL?05, pages 173?180.Stephen Clark and James R. Curran.
2004a.
The im-portance of supertagging for wide-coverage CCGparsing.
In Proc.
of COLING-04.Stephen Clark and James R. Curran.
2004b.
Parsingthe WSJ using CCG and log-linear models.
In Proc.of ACL?04, pages 104?111.Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
The MIT Press.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,Univ.
of Pennsylvania.James R. Curran and Stephen Clark.
2003.
Investigat-ing GIS and smoothing for maximum entropy tag-gers.
In Proc.
of EACL?03, pages 91?98.Stuart Geman and Mark Johnson.
2002.
Dynamic pro-gramming for parsing and estimation of stochasticunification-based grammars.
In Proc.
of ACL?02,pages 279?286.Joshua Goodman.
1997.
Global thresholding and mul-tiple pass parsing.
In Proc.
of EMNLP-1997, pages11?25.Julia Hockenmaier.
2003.
Parsing with generativemodels of predicate-argument structure.
In Proc.
ofACL?03, pages 359?366.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Proc.of ACL ?99, pages 535?541.R.
M. Kaplan, S. Riezler, T. H. King, J. T. MaxwellIII, and A. Vasserman.
2004.
Speed and accuracyin shallow and deep stochastic parsing.
In Proc.
ofHLT/NAACL?04.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
of ACL?03,pages 423?430.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute valuegrammars.
In Proc.
of IJCNLP-04 Workshop ?Be-yond Shallow Analyses?.Robert Malouf, John Carroll, and Ann Copestake.2000.
Efficient feature structure operations with-out compilation.
Journal of Natural Language En-gineering, 6(1):29?46.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proc.
ofCoNLL-2002, pages 49?55.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proc.
ofHLT 2002, pages 292?297.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilis-tic disambiguation models for wide-coverage HPSGparsing.
In Proc.
of ACL?05, pages 83?90.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,2005.
Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Leeand Oi Yee Kwong (Eds.
), Natural Language Pro-cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-oriented Grammar Development for Acquiring aHead-driven Phrase Structure Grammar from thePenn Treebank, pages 684?693.
Springer-Verlag.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2004.
An empirical investigation of the effect of lex-ical rules on parsing with a treebank grammar.
InProc.
of TLT?04, pages 103?114.Alexis Nasr and Owen Rambow.
2004.
Supertaggingand full parsing.
In Proc.
of the 7th InternationalWorkshop on Tree Adjoining Grammar and RelatedFormalisms (TAG+7).Takashi Ninomiya, Yoshimasa Tsuruoka, YusukeMiyao, and Jun?ichi Tsujii.
2005.
Efficacy of beamthresholding, unification filtering and hybrid pars-ing in probabilistic hpsg parsing.
In Proc.
of IWPT2005, pages 103?114.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Stefan Riezler, Detlef Prescher, Jonas Kuhn, and MarkJohnson.
2000.
Lexicalized stochastic modelingof constraint-based grammars using log-linear mea-sures and EM training.
In Proc.
of ACL?00, pages480?487.Libin Shen and Aravind K. Joshi.
2003.
A SNoWbased supertagger with application to NP chunking.In Proc.
of ACL?03, pages 505?512.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proc.
of HLT/EMNLP2005, pages 467?474.Wen Wang and Mary P. Harper.
2004.
A statisti-cal constraint dependency grammar (CDG) parser.In Proc.
of ACL?04 Incremental Parsing work-shop: Bringing Engineering and Cognition To-gether, pages 42?49.Wen Wang.
2003.
Statistical Parsing and LanguageModeling based on Constraint Dependency Gram-mar.
Ph.D. thesis, Purdue University.163
