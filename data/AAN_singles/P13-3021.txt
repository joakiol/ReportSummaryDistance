Proceedings of the ACL Student Research Workshop, pages 142?149,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSimple, readable sub-sentencesSigrid KlerkeCentre for Language TechnologyUniversity of Copenhagensigridklerke@gmail.comAnders S?gaardCentre for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractWe present experiments using a new unsu-pervised approach to automatic text sim-plification, which builds on sampling andranking via a loss function informed byreadability research.
The main idea isthat a loss function can distinguish goodsimplification candidates among randomlysampled sub-sentences of the input sen-tence.
Our approach is rated as equallygrammatical and beginner reader appro-priate as a supervised SMT-based baselinesystem by native speakers, but our setupperforms more radical changes that betterresembles the variation observed in humangenerated simplifications.1 IntroductionAs a field of research in NLP, text simplification(TS) has gained increasing attention recently, pri-marily for English text, but also for Brazilian Por-tuguese (Specia, 2010; Alu?sio et al 2008), Dutch(Daelemans et al 2004), Spanish (Drndarevicand Saggion, 2012), Danish (Klerke and S?gaard,2012), French (Seretan, 2012) and Swedish (Ry-bing and Smith, 2009; Decker, 2003).
Our experi-ments use Danish text which is similar to Englishin that it has a deep orthography making it hardto map between letters and sounds.
Danish has arelatively free word order and sparse morfology.TS can help readers with below average readingskills access information and may supply relevanttraining material, which is crucial for developingreading skills.
However, manual TS is as expen-sive as translation, which is a key limiting factoron the availability of easy-to-read material.
One ofthe persistent chalenges of TS is that different in-terventions are called for depending on the targetreader population.
Automatic TS is an effectiveway to counter these limitations.2 ApproachDefinitions of TS typically reflect varying targetreader populations and the methods studied.
Forour purposes we define TS to include any oper-ation on the linguistic structure and content of atext, intended to produce new text, which1.
has semantic content similar to (a part of) theoriginal text2.
requires less cognitive effort to decode andunderstand by a target reader, compared tothe original text.Operations on linguistic content may includedeletion, reordering and insertion of content,paraphrasing concepts, resolving references, etc.,while typography and layout are excluded as non-linguistic properties.We cast the problem of generating a more read-able sentence from an input as a problem of choos-ing a reasonable sub-sentence from the wordspresent in the original.
The corpus-example belowillustrates how a simplified sentence can be em-bedded as scattered parts of a non-simplified sen-tence.
The words in bold are the common partswhich make up almost the entire human generatedsimplification and constitutes a suitable simplifi-cation on its own.Original : Der er m?lt hvad der bliver betegnet som abnormt storem?ngder af radioaktivt materiale i havvand n?r det jordsk?lvsramteatomkraftv?rk i Japan .What has been termed an abnormally large amount of radioactivityhas been measured in sea water near the nuclear power plant thatwas hit by earthquakes in JapanSimplified : Der er m?lt en stor m?ngde radioaktivt materiale i havetn?r atom-kraftv?rket Fukushima i Japan .A large amount of radioactivity has been measured in the sea nearthe nuclear power plant Fukushima in JapanTo generate candidate sub-sentences we use arandom deletion procedure in combination with142general dependency-based heuristics for conserv-ing main sentence constituents, and then introducea loss-function for choosing between candidates.Since we avoid relying on a specialized parallelcorpus or a simplification grammar, which can beexpensive to create, the method is especially rel-evant for under-resourced languages and organi-zations.
Although we limit rewriting to deletions,the space of possible candidates grows exponen-tially with the length of the input sentence, pro-hibiting exhaustive candidate generation, which iswhy we chose to sample the deletions randomly.However, to increase the chance of sampling goodcandidates, we restrict the search space underthe assumption that some general patterns apply,namely, that the main verb and subject should al-ways be kept, negations should be kept and that ifsomething is kept that originally had objects, thoseobjects should also be kept.
Another way in whichwe restrict the candidate space is by splitting longsentences.
Some clauses are simple to identifyand extract, like relative clauses, and doing so candramatically reduce sentence length.
Both sim-ple deletions and extraction of clauses can be ob-served in professionally simplified text.
(Medero,2011; Klerke, 2012)The next section positions this research in thecontext of related work.
Section 4 presents the ex-perimental setup including generation and evalu-ation.
In Section 5, the results are presented anddiscussed and, finally, concluding remarks and fu-ture perspectives are presented in the last section.3 Related workApproaches for automatic TS traditionally focuson lexical substitution (De Belder and Moens,2012; Specia et al 2012; Yatskar et al 2010), onidentifying re-write rules at sentence level eithermanually (Chandrasekar et al 1996; Carroll et al1999; Canning et al 2000; Siddharthan, 2010;Siddharthan, 2011; Seretan, 2012) or automati-cally from parallel corpora (Woodsend and Lap-ata, 2011; Coster and Kauchak, 2011; Zhu et al2010) and possibly learning cues for when to ap-ply such changes (Petersen and Ostendorf, 2007;Medero, 2011; Bott et al 2012).Chandrasekar et al(1996) propose a structuralapproach, which uses syntactic cues to recover rel-ative clauses and appositives.
Sentence level syn-tactic re-writing has since seen a variety of man-ually constructed general sentence splitting rules,designed to operate both on dependencies andphrase structure trees, and typically including lex-ical cues (Siddharthan, 2011; Heilman and Smith,2010; Canning et al 2000).
Similar rules havebeen created from direct inspection of simplifica-tion corpora (Decker, 2003; Seretan, 2012) anddiscovered automatically from large scale alignedcorpora (Woodsend and Lapata, 2011; Zhu et al2010).In our experiment we apply few basic sentencesplitting rules as a pre-processing technique be-fore using an over-generating random deletion ap-proach.Carroll et al(1999) perform lexical substitutionfrom frequency counts and eliminate anaphora byresolving and replacing the referring expressionswith the entity referred to.
Their system furtherinclude compound sentence splitting and rewrit-ing of passive sentences to active ones (Canninget al 2000).
Research into lexical simplificationremains an active topic.
De Belder and Moens(2012; Specia et al(2012) are both recent pub-lications of new resources for evaluating lexicalsimplification in English consisting of lists of syn-onyms ranked by human judges.
Another typeof resource is graded word-lists as described inBrooke et al(2012).
Annotator agreement andcomparisons so far shows that it is easy to over-fit to reflect individual annotator and domain dif-ferences that are not of relevance to generalizedsystems.In a minimally supervised setup, our TS ap-proach can be modified to include lexical simpli-fications as part of the random generation process.This would require a broad coverage list of wordsand simpler synonyms, which could for instancebe extracted from a parallel corpus like the DSimcorpus.For the majority of research in automatic TSthe question of what constitutes cognitive load isnot discussed.
An exception is Siddharthan andKatsos (2012), who seek to isolate the psycho-linguistically motivated notions of sentence com-prehension from sentence acceptability by actuallymeasuring the effect of TS on cognition on a smallscale.Readability research is a line of research that ismore directly concerned with the nature of cogni-tive load in reading building on insights from psy-cholinguistics.
One goal is to develop techniquesand metrics for assessing the readability of unseen143text.
Such metrics are used as a tool for teachersand publishers, but existing standard metrics (likeFlesch-Kincaid (Flesch, 1948) and LIX (Bjorns-son, 1983)) were designed and optimized for easymanual application to human written text, requir-ing thehuman reader to assess that the text iscongruent and coherent.
More recent methodspromise to be applicable to unassessed text.
Lan-guage modeling in particular has shown to be arobust and informative component of systems forassessing text readability (Schwarm and Osten-dorf, 2005; Vajjala and Meurers, 2012) as it is bet-ter suited to evaluate grammaticality than standardmetrics.
We use language modeling alongside tra-ditional metrics for selecting good simplificationcandidates.4 Experiments4.1 Baseline SystemsWe used the original input text and the human sim-plified text from the sentence aligned DSim corpuswhich consist of 48k original and manually sim-plified sentences of Danish news wire text (Klerkeand S?gaard, 2012) as reference in the evaluations.In addition we trained a statistical machine trans-lation (SMT) simplification system, in effect trans-lating from normal news wire text to simplifiednews.
To train an SMT system, a large resourceof aligned parallel text and a language model ofthe target language are needed.
We combined the25 million words Danish Korpus 20001 with theentire 1.75 million words unaligned DSim cor-pus (Klerke and S?gaard, 2012) to build the lan-guage model2.
Including both corpora gives bet-ter coverage and assigns lower average ppl and asimlar difference in average ppl between the twosides of a held out part of the DSim corpus com-pared to using only the simplified part of DSimfor the language model.
Following Coster andKauchak (2011), we used the phrase-based SMTMoses (Koehn et al 2007), with GIZA++ word-alignment (Och and Ney, 2000) and phrase tableslearned from the sentence aligned portion of theDSim corpus.1http://korpus.dsl.dk/korpus2000/engelsk_hovedside2The LM was a 5-gram Knesser-Ney smoothed lowercasemodel, built using IRSTLM (Federico et al 2008)4.2 Experimental setupThree system variants were set up to generatesimplified output from the original news wire ofthe development and test partitions of the DSimcorpus.
The texts were dependency-parsed us-ing Bohnet?s parser (Bohnet, 2010) trained on theDanish Treebank3 (Kromann, 2003) with defaultsettings4.1.
Split only performed simple sentence split-ting.2.
Sample over-generated candidates by sam-pling the heuristically restricted space of ran-dom lexical deletions and ranking candidateswith a loss function.3.
Combined is a combination of the two, ap-plying the sampling procedure of Sample tothe split sentences from Split.Sentence Splitting We implemented sentencesplitting to extract relative clauses, as marked bythe dependency relation rel, coordinated clauses,coord, and conjuncts, conj, when at least a verband a noun is left in each part of the split.
Onlysplits resulting in sentences of more than threewords were considered.
Where applicable, re-ferred entities were included in the extracted sen-tence by using the dependency analysis to extractthe subtree of the former head of the new sen-tence5.
In case of more than one possibility, thesplit resulting in the most balanced division of thesentence was chosen and the rules were re-appliedif a new sentence was still longer than ten tokens.Structural Heuristics To preserve nodes fromlater deletion we applied heuristics using simplestructural cues from the dependency structures.We favored nodes headed by a subject relation,subj, and object relations, *obj, and negatingmodifiers (the Danish word ikke) under the as-sumption that these were most likely to be impor-tant for preserving semantics and generating well-formed candidates under the sampling proceduredescribed below.
The heuristics were applied bothto trees, acting by preserving entire subtrees andapplied to words, only preserving single tokens.3http://ilk.uvt.nl/conll/post_task_data.html4Performance of the parser on the treebank test set La-beled attatchment score (LAS) = 85.65 and Unlabeled at-tatchment score (UAS) = 90.295For a formal description see (Klerke, 2012)144This serves as a way of avoiding relying heavilyon possibly faulty dependency analyses and alsoavoid the risk of insisting on keeping long, com-plex or superfluous modifiers.Sampling Candidates for scoring were over-generated by randomly selecting parts of a (pos-sibly split) input sentence.
Either the selectednodes with their full sub-tree or the single tokensfrom the flat list of tokens were eliminated, unlessthey were previously selected for preservation bya heuristic.
Some additional interaction betweenheuristics and sampling happened when the dele-tions were performed on trees: deletion of subtreesallow non-continuous deletions when the parsesare non-projective, and nodes that were otherwiseselected for keeping may nevertheless be removedif they are part of a subtree of a node selected fordeletion.
After pruning, all nodes that used to haveoutgoing obj-relations had the first child node ofthese relations restored.4.3 ScoringWe rank candidates according to a loss functionincorporating both readability score (the lower,the more readable) and language model perplexity(the lower, the less perplexing) as described below.The loss function assigns values to the candidatessuch that the best simplification candidate receivesthe lowest score.The loss function is a weighted combination ofthree scores: perplexity (PPL), LIX and word-class distribution (WCD).
The PPL scores wereobtained from a 5-gram language model of Dan-ish6 We used the standard readability metric forDanish, LIX (Bjornsson, 1983)7.
Finally, theWCD measured the variation in universal pos-tag-distribution 8 compared to the observed tag-variation in the entire simplified corpus.
For PPLand LIX we calculated the difference between thescore of the input sentence and the candidate.Development data was used for tuning theweights of the loss function.
Because thecandidate-generation is free to produce extremelyshort candidates, we have to deal with candidates6The LM was Knesser-Ney smoothed, using the same cor-pora as the baseline system, without punctuation and built us-ing SRILM (Stolcke, 2002).7LIX is similar to the English Flesch-Kincaid grade levelin favoring short sentences with short words.
The formulais LIX = average sentence length + % long words , withlong words being of more than 6 characters.
(Anderson,1983) calculated a conversion from LIX to grade levels.8suggested by(Petrov et al 2011)receiving extremely low scores.
Those scoresnever arise in the professionally simplified text,so we eliminate extreme candidates by introduc-ing filters on all scores.
The lower limit was tunedexperimentally and fixed approximately two timesbelow the average difference observed betweenthe two parts of the aligned DSim corpus, thus lim-iting the reduction in PPL and LIX to 60% of theinput?s PPL and LIX.
The upper limit was fixedat the input-level plus 20% to allow more variedcandidates through the filters.
The WCD-filter ac-cepted all candidates with a tag-variance that fellbelow the 75-percentile observed variance in thesimplified training part of the DSim corpus.
Theresulting loss was calculated as the sum of threeweighted scores.Below is the loss function we minimized overthe filtered candidates t ?
Ts for each input sen-tence, s. The notation var() denotes the range al-lowed through a hard filter.
Using developmentdata we set the values of the term weights to?
= 1, ?
= 6 and ?
= 2.t?
= argmint?Tsloss(s, t)loss(s, t) = ?
?LIX(s, t)var(LIX(s)) + ?
?PPL(s, t)var(PPL(s))+ ?
?WCD(.75, t)WCD(.75)If no candidates passed through the filters, theinput sentence was kept.4.4 EvaluationEvaluation was performed by a group of proficientDanish speaking volunteers who received writteninstructions and responded anonymously via anonline form.
240 sentences were evaluated: sixversions of each of 40 test set sentences.
48sentences were evaluated by four judges, andthe remaining by one judge each.
The judgeswere asked to rate each sentence in terms ofgrammaticality and in terms of perceived beginnerreader appropriateness, both on a 5-point scale,with one signifying very good and five signifyingvery bad.
The evaluators had to rate six versionsof each sentence: original news wire, a humansimplified version, the baseline system, a splitsentence version (Split), a sampled only version(Sample), and a version combining the Split andSample techniques (Combined).
The presentationwas randomized.
Below are example outputs145for the baseline and the other three automaticsystems:BL: Der er hvad der bliver betegnet som abnormt store m?ngderradioaktivt materiale i havvand n?r frygter atomkraftv?rk .Split : Der er m?lt hvad.
Hvad bliver betegnet som abnormtstore m?ngder af radioaktivt materiale i havvand n?r detjordsk?lvsramte atomkraftv?rk i Japan .Sample: Der er m?lt hvad der bliver betegnet som store m?ngderaf radioaktivt materiale i havvand japan .Comb.
: Der er m?lt hvad.
Hvad bliver betegnet som store m?ngderaf radioaktivt materiale det atomkraftv?rk i japan .5 ResultsThe ranking of the systems in terms of begin-ner reader appropriateness and grammaticality, areshown in Figure 1.
From the test set of the DSimcorpus, 15 news wire texts were arbitrarily se-lected for evaluation.
For these texts we calcu-lated median LIX and PPL.
The results are shownin Table 1.
The sentences for human evaluationwere drawn arbitrarily from this collection.
Asexpected, the filtering of candidates and the lossfunction force the systems Sample and Combinedto choose simplifications with LIX and PPL scoresclose to the ones observed in the human simpli-fied version.
Split sentences only reduce LIX asa result of shorter sentences, however PPL is thehighest, indicating a loss of grammaticality.
Mostoften this was caused by tagger and parser errors.The baseline reduces PPL slightly, while LIX isunchanged.
This reflects the importance of thelanguage model in the SMT system.In the analyses below, the rating were collapsedto three levels.
For texts ranked by more thanone judge, we calculated agreement as Krippen-dorff?s ?.
The results are shown in Table 2.
Inaddition to sentence-wise agreement, the system-wise evaluation agreement was calculated as alljudges were evaluating the same 6 systems 8 timeseach.
We calculated ?
of the most frequent score(mode) assigned by each judge to each system.As shown in Table 2 this system score agreementwas only about half of the single sentence agree-ment, which reflect a notable instability in outputquality of all computer generated systems.
Thesame tendency is visible in both histograms in Fig-ure 1a and 1b.
While grammaticality is mostlyagreed upon when the scores are collapsed intothree bins (?
= 0.650), proficient speakers do notagree to the same extent on what constitutes be-ginner reader appropriate text (?
= 0.338).
Theaverage, mean and most frequent assigned ranksare recorded in Table 3.
Significant differences atp < 0.05 are reported in Table 4.1 very good2 3 4 5 very poor010203040506070Beginning reader appropriatenessvotes for each systemOriginalSimplifiedBaselineSplitSampleCombined(a) Sentence ?
Beginner1 very good2 3 4 5 very poor010203040506070Grammaticalityvotes for each systemOriginalSimplifiedBaselineSplitSampleCombined(b) Sentence ?
Grammar.Figure 1: Distribution of all rankings on systemsbefore collapsing rankings.Orig.
Simpl.
Base Split Sample Comb.PPL 222 174 214 234 164 177LIX 45 (10) 39 (8) 45 (10) 41(9) 36 (8) 32 (7)Table 1: LIX and PPL scores for reference textsand system generated output.
Medians are re-ported, because distributions are very skewed,which makes the mean a bad estimator of centraltendency.
LIX grade levels in parenthesis.Reflecting the fair agreement on grammatical-ity, all comparisons come out significant exceptthe human generated versions that are judged asequally grammatical and the Combined and Base-line systems that are indistinguishable in gram-maticality.
Beginner reader appropriateness is sig-nificantly better in the human simplified version146Systems SentencesBeginner reader 0.168 0.338Grammaticality 0.354 0.650Table 2: Krippendorff?s ?
agreement for full-textand sentence evaluation.
Agreement on systemranks was calculated from the most frequent scoreper judge per system.compared to all other versions, and the originalversion is significantly better than the Sample andSplit systems.
The remaining observed differencesare not significant due to the great variation inquality as expressed in Figure 1a.We found that our Combined system producedsentences that were as grammatical as the base-line and also frequently judged to be appropriatefor beginner readers.
The main source of erroraffecting both Combined and Split is faulty sen-tence splitting as a result of errors in tagging andparsing.
One way to avoid this in future develop-ment is to propagate several split variants to thefinal sampling and scoring.
In addition, the sys-tems Combined and Sample are prone to omittingimportant information that is perceived as missingwhen compared directly to the original, althoughthose two systems are the ones that score the clos-est to the human generated simplifications.
As canbe expected in a system operating exclusively atsentence level, coherence across sentence bound-aries remains a weak point.Another important point is that while the base-line system performs well in the evaluation, thisis likely due to its conservativeness: choosingsimplifications resembling the original input veryclosely.
This is evident both in our automatic mea-sures (see Table 1) and from manual inspection.Our systems Sample and Combine, on the otherhand, have been tuned to perform much more radi-cal changes and in this respect more closely modelthe changes we see in the human simplification.Combined is thus evaluated to be at level withthe baseline in grammaticality and beginner readerappropriateness, despite the fact that the baselinesystem is supervised.Conclusion and perspectivesWe have shown promising results for simplifica-tion of Danish sentences.
We have also shownthat using restricted over-generation and scoringcan be a feasible way for simplifying text with-out relying directly on large scale parallel corpora,Sent.
?
Beginner Sent.
?
Grammarx?
x?
mode x?
x?
modeHuman Simp.
1.44 1 1 1.29 1 1Orig.
2.14 1 1 1.32 1 1Base 2.58 3 1 1.88 2 1Split 3.31 3 5 2.44 3 3Sample 3.22 3 5 2.39 3 3Comb.
2.72 1 1 1.93 2 1Table 3: Human evaluation.
Mean (x?
), median (x?
)and most frequent (mode) of assigned ranks by be-ginner reader appropriateness and grammaticalityas assessed by proficient Danish speakers.Comb.
Sample Split Base Orig.Human Simp.
b, g b, g b, g b, g bOrig.
g b, g b, g gBase g gSplit gSample gTable 4: Significant differences between systemsin experiment b: Beginner reader appropriate-ness and g: Grammaticality.
Bonferroni-correctedMann-Whitney?s U for 15 comparisons, two-tailedtest.
A letter indicate significant difference at cor-rected p < 0.05 level.which for many languages do not exist.
To inte-grate language modeling and readability metrics inscoring is a first step towards applying results fromreadability research to the simplification frame-work.
Our error analysis showed that many errorscome from pre-processing and thus more robustNLP-tools for Danish are needed.
Future perspec-tives include combining supervised and unsuper-vised methods to exploit the radical unsuperviseddeletion approach and the knowledge obtainablefrom observable structural changes and potentiallexical simplifications.
We plan to focus on refin-ing the reliability of sentence splitting in the pres-ence of parser errors as well as on developing aloss function that incorporates more of the insightsfrom readability research, and to apply machinelearning techniques to the weighting of features.Specifically we would like to investigate the use-fulness of discourse features and transition proba-bilities (Pitler and Nenkova, 2008) for performingand evaluating full-text simplifications.AcknowledgementsThanks to Mirella Lapata and Kristian Woodsendfor their feedback and comments early in the pro-cess of this work and to the Emnlp@Cph groupand reviewers for their helpful comments.147ReferencesS.M.
Alu?sio, Lucia Specia, T.A.S.
Pardo, E.G.Maziero, H.M. Caseli, and R.P.M.
Fortes.
2008.
Acorpus analysis of simple account texts and the pro-posal of simplification strategies: first steps towardstext simplification systems.
In Proceedings of the26th annual ACM international conference on De-sign of communication, pages 15?22.
ACM.Jonathan Anderson.
1983.
LIX and RIX: Variations ona little-known readability index.
Journal of Reading,26(6):490?496.C.
H. Bjornsson.
1983.
Readability of Newspapersin 11 Languages.
Reading Research Quarterly,18(4):480?497.B Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 89?97.
Association forComputational Linguistics.S.
Bott, H. Saggion, and D. Figueroa.
2012.
A hy-brid system for spanish text simplification.
In ThirdWorkshop on Speech and Language Processing forAssistive Technologies (SLPAT), Montreal, Canada.Julian Brooke, Vivian Tsang, David Jacob, FraserShein, and Graeme Hirst.
2012.
Building Read-ability Lexicons with Unannotated Corpora.
In Pro-ceedings of the First Workshop on Predicting andImproving Text Readability for target reader popula-tions, pages 33?39, Montr{?
}al, Canada, June.
As-sociation for Computational Linguistics.Y.
Canning, J. Tait, J. Archibald, and R. Crawley.2000.
Cohesive generation of syntactically simpli-fied newspaper text.
Springer.John Carroll, G. Minnen, D. Pearce, Yvonne Canning,S.
Devlin, and J. Tait.
1999.
Simplifying textfor language-impaired readers.
In Proceedings ofEACL, volume 99, pages 269?270.
Citeseer.R.
Chandrasekar, Christine Doran, and B Srinivas.1996.
Motivations and methods for text simplifica-tion.
In Proceedings of the 16th conference on Com-putational linguistics-Volume 2, pages 1041?1044.Association for Computational Linguistics.William Coster and David Kauchak.
2011.
Simple En-glish Wikipedia: a new text simplification task.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, vol-ume 2, pages 665?669.
Association for Computa-tional Linguistics.W.
Daelemans, A. H?thker, and E.T.K.
Sang.
2004.Automatic sentence simplification for subtitling indutch and english.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation, pages 1045?1048.A.
Davison and R.N.
Kantor.
1982.
On the failure ofreadability formulas to define readable texts: A casestudy from adaptations.
Reading Research Quar-terly, pages 187?209.J.
De Belder and M.F.
Moens.
2012.
A dataset for theevaluation of lexical simplification.
ComputationalLinguistics and Intelligent Text Processing, pages426?437.Anna Decker.
2003.
Towards automatic grammati-cal simplification of Swedish text.
Master?s thesis,Stockholm University.Biljana Drndarevic and Horacio Saggion.
2012.
To-wards Automatic Lexical Simplification in Spanish:An Empirical Study.
In Proceedings of the FirstWorkshop on Predicting and Improving Text Read-ability for target reader populations, pages 8?16,Montr{?
}al, Canada, June.
Association for Compu-tational Linguistics.M Federico, N Bertoldi, and M Cettolo.
2008.IRSTLM: an open source toolkit for handling largescale language models.
In Ninth Annual Conferenceof the International Speech Communication Associ-ation.Rudolph Flesch.
1948.
A new readability yardstick.Journal of applied psychology, 32(3):221.Michael Heilman and Noah A Smith.
2010.
Extract-ing simplified statements for factual question gen-eration.
In Proceedings of the Third Workshop onQuestion Generation.Sigrid Klerke and Anders S?gaard.
2012.
DSim , aDanish Parallel Corpus for Text Simplification.
InProceedings of Language Resources and Evaluation(LREC 2012), pages 4015?4018.Sigrid Klerke.
2012.
Automatic text simplification indanish.
sampling a restricted space of rewrites to op-timize readability using lexical substitutions and de-pendency analyses.
Master?s thesis, University ofCopenhagen.P Koehn, H Hoang, A Birch, C Callison-Burch, M Fed-erico, N Bertoldi, B Cowan, W Shen, C Moran,R Zens, and Others.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.M T Kromann.
2003.
The Danish Dependency Tree-bank and the DTAG treebank tool.
In Proceedingsof the Second Workshop on Treebanks and LinguisticTheories (TLT), page 217.Julie Medero.
2011.
Identifying Targets for SyntacticSimplification.
In Proceedings of Speech and Lan-guage Technology in Education.148F.J.
Och and H. Ney.
2000.
A comparison of alignmentmodels for statistical machine translation.
In Pro-ceedings of the 18th conference on Computationallinguistics-Volume 2, pages 1086?1090.
Associationfor Computational Linguistics.S.E.
E Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analy-sis.
In the Proceedings of the Speech and LanguageTechnology for Education Workshop, pages 69?72.Citeseer.S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
Arxiv preprintArXiv:1104.2086.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Jonas Rybing and Christian Smith.
2009.
CogFLUXGrunden till ett automatiskt textf?renklingssystemf?r svenska.
Master?s thesis, Link?pings Univer-sitet.Sarah E Schwarm and Mari Ostendorf.
2005.
ReadingLevel Assessment Using Support Vector Machinesand Statistical Language Models.
In Proceedingsof the 43rd Annual Meeting of the ACL, pages 523?530.V.
Seretan.
2012.
Acquisition of syntactic simplifica-tion rules for french.
In Proceedings of LanguageResources and Evaluation (LREC 2012).Advaith Siddharthan and Napoleon Katsos.
2012.Offline Sentence Processing Measures for testingReadability with Users.
In Proceedings of the FirstWorkshop on Predicting and Improving Text Read-ability for target reader populations, pages 17?24,Montr{?
}al, Canada, June.
Association for Compu-tational Linguistics.Advaith Siddharthan.
2010.
Complex lexico-syntacticreformulation of sentences using typed dependencyrepresentations.
Proceedings of the 6th Interna-tional Natural Language Generation Conference.Advaith Siddharthan.
2011.
Text Simplification us-ing Typed Dependencies: A Comparison of the Ro-bustness of Different Generation Strategies.
In Pro-ceedings of the 13th European Workshop on NaturalLanguage Generation, pages 2?11.L.
Specia, S.K.
Jauhar, and R. Mihalcea.
2012.Semeval-2012 task 1: English lexical simplification.In Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012), pages 347?355.L.
Specia.
2010.
Translating from complex to simpli-fied sentences.
In Proceedings of the 9th interna-tional conference on Computational Processing ofthe Portuguese Language, pages 30?39.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing.S.
Vajjala and D. Meurers.
2012.
On improving theaccuracy of readability classification using insightsfrom second language acquisition.
In Proceedingsof the 7th Workshop on Innovative Use of NLP forBuilding Educational Applications (BEA7), pages163?173.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to Simplify Sentences with Quasi-SynchronousGrammar and Integer Programming.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (2011), pages 409?420.Mark Yatskar, Bo Pang, C. Danescu-Niculescu-Mizil,and Lillian Lee.
2010.
For the sake of simplic-ity: Unsupervised extraction of lexical simplifica-tions from Wikipedia.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 365?368.
Association forComputational Linguistics.Zhemin Zhu, Delphine Bernhard, and I. Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of The23rd International Conference on ComputationalLinguistics, pages 1353?1361.
Association for Com-putational Linguistics.149
