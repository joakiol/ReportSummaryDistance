Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 874?881,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Alignment for Languages with Scarce ResourcesUsing Bilingual Corpora of Other Language PairsHaifeng Wang      Hua Wu      Zhanyi LiuToshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng DistrictBeijing, 100738, China{wanghaifeng, wuhua, liuzhanyi}@rdc.toshiba.com.cnAbstractThis paper proposes an approach to im-prove word alignment for languages withscarce resources using bilingual corporaof other language pairs.
To perform wordalignment between languages L1 and L2,we introduce a third language L3.
Al-though only small amounts of bilingualdata are available for the desired lan-guage pair L1-L2, large-scale bilingualcorpora in L1-L3 and L2-L3 are available.Based on these two additional corporaand with L3 as the pivot language, webuild a word alignment model for L1 andL2.
This approach can build a wordalignment model for two languages evenif no bilingual corpus is available in thislanguage pair.
In addition, we build an-other word alignment model for L1 andL2 using the small L1-L2 bilingual cor-pus.
Then we interpolate the above twomodels to further improve word align-ment between L1 and L2.
Experimentalresults indicate a relative error rate reduc-tion of 21.30% as compared with themethod only using the small bilingualcorpus in L1 and L2.1 IntroductionWord alignment was first proposed as an inter-mediate result of statistical machine translation(Brown et al, 1993).
Many researchers buildalignment links with bilingual corpora (Wu,1997; Och and Ney, 2003; Cherry and Lin, 2003;Zhang and Gildea, 2005).
In order to achievesatisfactory results, all of these methods require alarge-scale bilingual corpus for training.
Whenthe large-scale bilingual corpus is unavailable,some researchers acquired class-based alignmentrules with existing dictionaries to improve wordalignment (Ker and Chang, 1997).
Wu et al(2005) used a large-scale bilingual corpus ingeneral domain to improve domain-specific wordalignment when only a small-scale domain-specific bilingual corpus is available.This paper proposes an approach to improveword alignment for languages with scarce re-sources using bilingual corpora of other languagepairs.
To perform word alignment between lan-guages L1 and L2, we introduce a third languageL3 as the pivot language.
Although only smallamounts of bilingual data are available for thedesired language pair L1-L2, large-scale bilin-gual corpora in L1-L3 and L2-L3 are available.Using these two additional bilingual corpora, wetrain two word alignment models for languagepairs L1-L3 and L2-L3, respectively.
And then,with L3 as a pivot language, we can build a wordalignment model for L1 and L2 based on theabove two models.
Here, we call this model aninduced model.
With this induced model, we per-form word alignment between languages L1 andL2 even if no parallel corpus is available for thislanguage pair.
In addition, using the small bilin-gual corpus in L1 and L2, we train another wordalignment model for this language pair.
Here, wecall this model an original model.
An interpo-lated model can be built by interpolating the in-duced model and the original model.As a case study, this paper uses English as thepivot language to improve word alignment be-tween Chinese and Japanese.
Experimental re-sults show that the induced model performs bet-ter than the original model trained on the smallChinese-Japanese corpus.
And the interpolatedmodel further improves the word alignment re-sults, achieving a relative error rate reduction of87421.30% as compared with results produced bythe original model.The remainder of this paper is organized asfollows.
Section 2 discusses the related work.Section 3 introduces the statistical word align-ment models.
Section 4 describes the parameterestimation method using bilingual corpora ofother language pairs.
Section 5 presents the in-terpolation model.
Section 6 reports the experi-mental results.
Finally, we conclude and presentthe future work in section 7.2 Related WorkA shared task on word alignment was organizedas part of the ACL 2005 Workshop on Buildingand Using Parallel Texts (Martin et al, 2005).The focus of the task was on languages withscarce resources.
Two different subtasks weredefined: Limited resources and Unlimited re-sources.
The former subtask only allows partici-pating systems to use the resources provided.The latter subtask allows participating systems touse any resources in addition to those provided.For the subtask of unlimited resources, As-wani and Gaizauskas (2005) used a multi-featureapproach for many-to-many word alignment onEnglish-Hindi parallel corpora.
This approachperformed local word grouping on Hindi sen-tences and used other methods such as dictionarylookup, transliteration similarity, expected Eng-lish words, and nearest aligned neighbors.
Martinet al (2005) reported that this method resulted inabsolute improvements of up to 20% as com-pared with the case of only using limited re-sources.
Tufis et al (2005) combined two wordaligners: one is based on the limited resourcesand the other is based on the unlimited resources.The unlimited resource consists of a translationdictionary extracted from the alignment of Ro-manian and English WordNet.
Lopez and Resnik(2005) extended the HMM model by integratinga tree distortion model based on a dependencyparser built on the English side of the parallelcorpus.
The latter two methods produced compa-rable results with those methods using limitedresources.
All the above three methods use somelanguage dependent resources such as dictionary,thesaurus, and dependency parser.
And somemethods, such as transliteration similarity, canonly be used for very similar language pairs.In this paper, besides the limited resources forthe given language pair, we make use of largeamounts of resources available for other lan-guage pairs to address the alignment problem forlanguages with scarce resources.
Our methoddoes not need language-dependent resources ordeep linguistic processing.
Thus, it is easy toadapt to any language pair where a pivot lan-guage and corresponding large-scale bilingualcorpora are available.3 Statistical Word AlignmentAccording to the IBM models (Brown et al,1993), the statistical word alignment model canbe generally represented as in equation (1).
?=a'c|f,a'c|fa,c|fa,)Pr()Pr()Pr((1)Where,  and  represent the source sentenceand the target sentence, respectivelyc f1.In this paper, we use a simplified IBM model4 (Al-Onaizan et al, 1999), which is shown inequation (2).
This version does not take into ac-count word classes in Brown et al (1993).
))))(()](([))()](([()|( )|()Pr(0,110,1111112000 00?????=>?=?==????+??=???????????
?=majjmajijmjajliiimjjjjpjdahjjdahjcftcnppm????
?
?c|fa,(2)ml,  are the lengths of the source sentence andthe target sentence respectively.j  is the position index of the target word.ja  is the position of the source word aligned tothe jth target word.i?
is the fertility of .
ic0p ,  are the fertility probabilities for ,and1p 0c110 =+ pp .
)|jajct(f  is the word translation probability.
)|( ii cn ?
is the fertility probability.
)( 11 ??
ijd ?
is the distortion probability for thehead word of the cept.
))((1 jpjd ?>  is the distortion probability forthe non-head words of the cept.1 This paper uses c and f to represent a Chinese sentenceand a Japanese sentence, respectively.
And e represents anEnglish sentence.875}:{min)( kkaikih == is the head of cept i.
}:{max)( kjjkaakjp ==<.i?
is the center of cept i.During the training process, IBM model 3 isfirst trained, and then the parameters in model 3are employed to train model 4.
For convenience,we describe model 3 in equation (3).
The maindifference between model 3 and model 4 lies inthe calculation of distortion probability.?????====?????????????
?=majjmjajliiliiimjjmlajdcftcnppm0,111112000),,|()|(!
)|()Pr( 00????
?
?c|fa,(3)4 Parameter Estimation Using BilingualCorpora of Other Language PairsAs shown in section 3, the word alignmentmodel mainly has three kinds of parameters thatmust be specified, including the translation prob-ability, the fertility probability, and the distortionprobability.
The parameters are usually estimatedby using bilingual sentence pairs in the desiredlanguages, namely Chinese and Japanese here.
Inthis section, we describe how to estimate the pa-rameters without using the Chinese-Japanesebilingual corpus.
We introduce English as thepivot language, and use the Chinese-English andEnglish-Japanese bilingual corpora to estimatethe parameters of the Chinese-Japanese wordalignment model.
With these two corpora, wefirst build Chinese-English and English-Japaneseword alignment models as described in section 3.Then, based on these two models, we estimatethe parameters of Chinese-Japanese word align-ment model.
The estimated model is named in-duced model.The following subsections describe themethod to estimate the parameters of Chinese-Japanese alignment model.
For reversed Japa-nese-Chinese word alignment, the parameterscan be estimated with the same method.4.1  Translation ProbabilityBasic Translation ProbabilityWe use the translation probabilities trainedwith Chinese-English and English-Japanese cor-pora to estimate the Chinese-Japanese probabil-ity as shown in equation (4).
In (4), we assumethat the translation probability  isindependent of the Chinese word .
),|(EJ ikj ceftic)|()|()|(),|()|(CEEJCEEJCJikekjikeikjijceteftcetceftcftkk??
?=?=(4)Where  is the translation probabilityfor Chinese-Japanese word alignment.is the translation probability trainedusing the English-Japanese corpus.
isthe translation probability trained using the Chi-nese-English corpus.
)|(CJ ij cft)|(EJ kj eft)|(CE ik cetCross-Language Word SimilarityIn any language, there are ambiguous wordswith more than one sense.
Thus, some noise maybe introduced by the ambiguous English wordwhen we estimate the Chinese-Japanese transla-tion probability using English as the pivot lan-guage.
For example, the English word "bank" hasat least two senses, namely:bank1 - a financial organizationbank2 - the border of a riverLet us consider the Chinese word:??
- bank2 (the border of a river)And the Japanese word:??
- bank1 (a financial organization)In the Chinese-English corpus, there is highprobability that the Chinese word "??
(bank2)"would be translated into the English word "bank".And in the English-Japanese corpus, there is alsohigh probability that the English word "bank"would be translated into the Japanese word "??
(bank1)".As a result, when we estimate the translationprobability using equation (4), the translationprobability of "??
(bank1)" given "??
(bank2)" is high.
Such a result is not what weexpect.In order to alleviate this problem, we intro-duce cross-language word similarity to improvetranslation probability estimation in equation (4).The cross-language word similarity describeshow likely a Chinese word is to be translated intoa Japanese word with an English word as thepivot.
We make use of both the Chinese-Englishcorpus and the English-Japanese corpus to calcu-late the cross language word similarity between aChinese word c and a Japanese word f given an876Input: An English word e , a Chinese word , and a Japanese word ; c fThe Chinese-English corpus; The English-Japanese corpus.
(1) Construct Set 1: identify those Chinese-English sentence pairs that include the given Chineseword  and English word , and put the English sentences in the pairs into Set 1. c e(2) Construct Set 2: identify those English-Japanese sentence pairs that include the given Englishword  and Japanese word , and put the English sentences in the pairs into Set 2. e f(3) Construct the feature vectors  and  of the given English word using all other words ascontext in Set 1 and Set 2, respectively.CEV EJV>=< ),(, ... ),,(),,( 1122111CE nn ctectecteV>=< ),(, ... ),,(),,( 2222211EJ nn ctectecteVWhere  is the frequency of the context word .
ijct je 0=ijct  if  does not occur in Set i .
je(4) Given the English word e , calculate the cross-language word similarity between the Chineseword  and the Japanese word  as in equation (5) c f????
?==jjjjjjjctctctctVVefcsim222121EJCE)()(),cos();,(                                     (5)Output: The cross language word similarity  of the Chinese word c and the Japaneseword given the English word);,( efcsimf eFigure 1.
Similarity CalculationEnglish word e. For the ambiguous English worde, both the Chinese word c and the Japaneseword f can be translated into e. The sense of aninstance of the ambiguous English word e can bedetermined by the context in which the instanceappears.
Thus, the cross-language word similar-ity between the Chinese word c and the Japaneseword f can be calculated according to the con-texts of their English translation e. We use thefeature vector constructed using the contextwords in the English sentence to represent thecontext.
So we can calculate the cross-languageword similarity using the feature vectors.
Thedetailed algorithm is shown in figure 1.
This ideais similar to translation lexicon extraction via abridge language (Schafer and Yarowsky, 2002).For example, the Chinese word "??"
and itsEnglish translation "bank" (the border of a river)appears in the following Chinese-English sen-tence pair:(a) ??????????
(b) They walked home along the river bank.The Japanese word "??"
and its Englishtranslation "bank" (a financial organization) ap-pears in the following English-Japanese sentencepair:(c) He has plenty of money in the bank.
(d) ???????????
?The context words of the English word "bank" insentences (b) and (c) are quite different.
The dif-ference indicates the cross language word simi-larity of the Chinese word "??"
and the Japa-nese word "??"
is low.
So they tend to havedifferent senses.Translation Probability Embedded with CrossLanguage Word SimilarityBased on the cross language word similaritycalculation in equation (5), we re-estimate thetranslation probability as shown in (6).
Then wenormalize it in equation (7).The word similarity of the Chinese word "??
(bank2)" and the Japanese word " ?
?
(bank1)" given the word English word "bank" islow.
Thus, using the updated estimation method,the translation probability of "??
(bank1)"given "??
(bank2)" becomes low.));,()|()|(()|('CEEJCJkjiikekjijefcsimceteftcftk?
?= ?
(6)?='CJCJCJ )|'(')|(')|(fiijij cftcftcft  (7)4.2  Fertility ProbabilityThe induced fertility probability is calculated asshown in (8).
Here, we assume that the probabil-877ity ),|(EJ iki cen ?
is independent of the Chineseword .
ic)|()|()|(),|()|(CEEJCEEJCJikekiikeikiiicetencetcencnkk?=?=?????
(8)Where, )|(CJ ii cn ?
is the fertility probability forthe Chinese-Japanese alignment.
)|(EJ ki en ?
isthe trained fertility probability for the English-Japanese alignment.4.3  Distortion Probability in Model 3With the English language as a pivot language,we calculate the distortion probability of model 3.For this probability, we introduce two additionalparameters: one is the position of English wordand the other is the length of English sentence.The distortion probability is estimated as shownin (9).)),,|Pr(),,,|Pr(),,,,|(Pr(),,|,Pr(),,,,|Pr(),,|,,Pr(),,|(,,,CJmlinmlinkmlinkjmlinkmlinkjmlinkjmlijdnknknk??=?==???
(9)Where, is the estimated distortionprobability.
is the introduced position of anEnglish word.
n  is the introduced length of anEnglish sentence.
).,|(CJ mlijdkIn the above equation, we assume that the po-sition probability  is independentof the position of the Chinese word and thelength of the Chinese sentence.
And we assumethat the position probability  is in-dependent of the length of Japanese sentence.Thus, we rewrite these two probabilities as fol-lows.
),,,,|Pr( mlinkj),,,|Pr( mlink),,|(),,|Pr(),,,,|Pr( EJ mnkjdmnkjmlinkj =?
),,|(),,|Pr(),,,|Pr( CE nlikdnliknmlik =?For the length probability, the English sen-tence length n  is independent of the word posi-tions i .
And we assume that it is uniformly dis-tributed.
Thus, we take it as a constant, and re-write it as follows.constant),|Pr(),,|Pr( == mlnmlinAccording to the above three assumptions, weignore the length probability .
Equa-tion (9) is rewritten in (10).
),|Pr( mln?
?=nknlikdmnkjdmlijd,CEEJCJ),,|(),,|().,|((10)4.4  Distortion Probability in Model 4In model 4, there are two parameters for the dis-tortion probability: one for head words and theother for non-head words.Distortion Probability for Head WordsThe distortion probability for headwords represents the relative position of the headword of the i)( 11 ??
ijd ?th cept and the center of the (i-1)thcept.
Let 1??=?
ijj ?
, then  is independent ofthe absolute position.
Thus, we estimate the dis-tortion probability by introducing another rela-tive positionj?'j?
of English words, which isshown in (11).???????=?=?
'EJCE,11CJ,1)'|(Pr)'()(jijjjdjjd ?
(11)Where, )( 1CJ1, ??=?
ijjd ?
is the estimated dis-tortion probability for head words in Chinese-Japanese alignment.
is the distortionprobability for head word in Chinese-Englishalignment.
)'(CE1, jd ?
)'|(PrEJ jj ??
is the translation prob-ability of relative Japanese position given rela-tive English position.In order to simplify , we introduceand  and let)'|(PrEJ jj ??
'j 1'?i?
1''' ??=?
ijj ?
, where  andare positions of English words.
We rewrite'j1'?i?
)'|(PrEJ jj ??
in (12).??=??=?????????=??=??'':,':,1'1EJ1'1EJEJ1'1'11),'|,(Pr)'|(Pr)'|(Prjjjjjjiiiiiiiijjjjjj????????
(12)The English word in position  is aligned tothe Japanese word in position , and the Englishword in position  is aligned to the Japaneseword in position .
'jj1'?i?1?i?We assume that  and  are independent,only depends on , and  only dependson .
Then  can be esti-mated as shown in (13).j 1?i?j 'j 1?i?1'?i?
),'|,(Pr 1'1EJ ??
ii jj ??878)|(Pr)'|(Pr),'|,(Pr1'1EJEJ1'1EJ????
?= iiiijjjj????
(13)Both of the two parameters in (13) representthe position translation probabilities.
Thus, wecan estimate them from the distortion probabilityin model 3.  is estimated as shown in(14).
And  can be estimated inthe same way.
In (14), we also assume that thesentence length distribution  is inde-pendent of the word position and that it is uni-formly distributed.
)'|(PrEJ jj)|(Pr 1'1EJ ??
ii ??
)'|,Pr( jml??
?=?==mlmlmlmljjdjmlmljjdjmljjj,EJ,EJ,EJEJ),,'|()'|,Pr(),,'|()'|,,(Pr)'|(Pr(14)Distortion Probability for Non-Head WordsThe distortion probability de-scribes the distribution of the relative position ofnon-head words.
In the same way, we introducerelative position of English words, and modelthe probability in (15).
))((1 jpjd ?>'j???>>????=?=?
'EJCE,1CJ,1)'|(Pr)'())((jjjjdjpjjd(15)))((CJ1, jpjjd ?=?> is the estimated distortionprobability for the non-head words in Chinese-Japanese alignment.
is the distortionprobability for non-head words in Chinese-English alignment.
)'(CE1, jd ?>)'|(PrEJ jj ??
is the translationprobability of the relative Japanese positiongiven the relative English position.In fact,  has the same interpreta-tion as in (12).
Thus, we introduce two parame-ters and  and let , whereand  are positions of English words.
Thefinal distortion probability for non-head wordscan be estimated as shown in (16).
)'|(PrEJ jj ??
'j )'( jp )'('' jpjj ?=?
'j )'( jp)))'(|)((Pr)'|(Pr)'(())((')'(':)'(,')(:)(,EJEJ'CE1,CJ1,???=?
?=??>>??
?=?=?jjpjjpjjjpjjpjjjpjpjjjdjpjjd(16)5 Interpolation ModelWith the Chinese-English and English-Japanesecorpora, we can build the induced model for Chi-nese-Japanese word alignment as described insection 4.
If we have small amounts of Chinese-Japanese corpora, we can build another wordalignment model using the method described insection 3, which is called the original model here.In order to further improve the performance ofChinese-Japanese word alignment, we build aninterpolated model by interpolating the inducedmodel and the original model.Generally, we can interpolate the inducedmodel and the original model as shown in equa-tion (17).
)(Pr)1( )(Pr)Pr(IO c|fa,c|fa,c|fa,?
?+?= ??
(17)Where is the original model trainedfrom the Chinese-Japanese corpus, andis the induced model trained from theChinese-English and English-Japanese corpora.
)(PrO c|fa,)(PrI c|fa,?
is an interpolation weight.
It can be a constantor a function of f  and .
cIn both model 3 and model 4, there are mainlythree kinds of parameters: translation probability,fertility probability and distortion probability.These three kinds of parameters have their owninterpretation in these two models.
In order toobtain fine-grained interpolation models, we in-terpolate the three kinds of parameters using dif-ferent weights, which are obtained in the sameway as described in Wu et al (2005).
t?
repre-sents the weights for translation probability.
n?represents the weights for fertility probability.d3?
and d4?
represent the weights for distortionprobability in model 3 and in model 4, respec-tively.
d4?
is set as the interpolation weight forboth the head words and the non-head words.The above four weights are obtained using amanually annotated held-out set.6 ExperimentsIn this section, we compare different wordalignment methods for Chinese-Japanese align-ment.
The "Original" method uses the originalmodel trained with the small Chinese-Japanesecorpus.
The "Basic Induced" method uses theinduced model that employs the basic translationprobability without introducing cross-languageword similarity.
The "Advanced Induced"method uses the induced model that introducesthe cross-language word similarity into the calcu-lation of the translation probability.
The "Inter-polated" method uses the interpolation of theword alignment models in the "Advanced In-duced" and "Original" methods.8796.1 DataThere are three training corpora used in this pa-per: Chinese-Japanese (CJ) corpus, Chinese-English (CE) Corpus, and English-Japanese (EJ)Corpus.
All of these tree corpora are from gen-eral domain.
The Chinese sentences and Japa-nese sentences in the data are automatically seg-mented into words.
The statistics of these threecorpora are shown in table 1.
"# Source Words"and "# Target Words" mean the word number ofthe source and target sentences, respectively.LanguagePairs#SentencePairs# SourceWords# TargetWordsCJ 21,977 197,072 237,834CE 329,350 4,682,103 4,480,034EJ 160,535 1,460,043 1,685,204Table 1.
Statistics for Training DataBesides the training data, we also have held-out data and testing data.
The held-out data in-cludes 500 Chinese-Japanese sentence pairs,which is used to set the interpolated weights de-scribed in section 5.
We use another 1,000 Chi-nese-Japanese sentence pairs as testing data,which is not included in the training data and theheld-out data.
The alignment links in the held-outdata and the testing data are manually annotated.Testing data includes 4,926 alignment links2.6.2  Evaluation MetricsWe use the same metrics as described in Wu et al(2005), which is similar to those in (Och and Ney,2000).
The difference lies in that Wu et al (2005)took all alignment links as sure links.If we use  to represent the set of alignmentlinks identified by the proposed methods andto denote the reference alignment set, the meth-ods to calculate the precision, recall, f-measure,and alignment error rate (AER) are shown inequations (18), (19), (20), and (21), respectively.It can be seen that the higher the f-measure is,the lower the alignment error rate is.
Thus, wewill only show precision, recall and AER scoresin the evaluation results.GSCS||||GCGSSSprecision?=      (18)||||CCGSSSrecall?=  (19)2 For a non one-to-one link, if m source words are aligned ton target words, we take it as one alignment link instead ofm?n alignment links.||||||2CGCGSSSSfmeasure +?=  (20)fmeasureSSSSAER ?=+?
?= 1||||||21CGCG  (21)6.3 Experimental ResultsWe use the held-out data described in section 6.1to set the interpolation weights in section 5. t?
isset to 0.3, n?
is set to 0.1, d3?
for model 3  is setto 0.5, and d4?
for model 4 is set to 0.1.
Withthese parameters, we get the lowest alignmenterror rate on the held-out data.For each method described above, we performbi-directional (source to target and target tosource) word alignment and obtain two align-ment results.
Based on the two results, we get aresult using "refined" combination as describedin (Och and Ney, 2000).
Thus, all of the resultsreported here describe the results of the "refined"combination.
For model training, we use theGIZA++ toolkit3.Method Precision Recall AERInterpolated 0.6955 0.5802 0.3673AdvancedInduced 0.7382 0.4803 0.4181BasicInduced 0.6787 0.4602 0.4515Original 0.6026 0.4783 0.4667Table 2.
Word Alignment ResultsThe evaluation results on the testing data areshown in table 2.
From the results, it can be seenthat both of the two induced models perform bet-ter than the "Original" method that only uses thelimited Chinese-Japanese sentence pairs.
The"Advanced Induced" method achieves a relativeerror rate reduction of 10.41% as compared withthe "Original" method.
Thus, with the Chinese-English corpus and the English-Japanese corpus,we can achieve a good word alignment resultseven if no Chinese-Japanese parallel corpus isavailable.
After introducing the cross-languageword similarity into the translation probability,the "Advanced Induced" method achieves a rela-tive error rate reduction of 7.40% as comparedwith the "Basic Induced" method.
It indicatesthat cross-language word similarity is effective inthe calculation of the translation probability.Moreover, the "interpolated" method further im-proves the result, which achieves relative error3 It is located at http://www.fjoch.com/ GIZA++.html.880rate reductions of 12.51% and 21.30% as com-pared with the "Advanced Induced" method andthe "Original" method.7 Conclusion and Future WorkThis paper presented a word alignment approachfor languages with scarce resources using bilin-gual corpora of other language pairs.
To performword alignment between languages L1 and L2,we introduce a pivot language L3 and bilingualcorpora in L1-L3 and L2-L3.
Based on these twocorpora and with the L3 as a pivot language, weproposed an approach to estimate the parametersof the statistical word alignment model.
This ap-proach can build a word alignment model for thedesired language pair even if no bilingual corpusis available in this language pair.
Experimentalresults indicate a relative error reduction of10.41% as compared with the method using thesmall bilingual corpus.In addition, we interpolated the above modelwith the model trained on the small L1-L2 bilin-gual corpus to further improve word alignmentbetween L1 and L2.
This interpolated model fur-ther improved the word alignment results byachieving a relative error rate reduction of12.51% as compared with the method using thetwo corpora in L1-L3 and L3-L2, and a relativeerror rate reduction of 21.30% as compared withthe method using the small bilingual corpus inL1 and L2.In future work, we will perform more evalua-tions.
First, we will further investigate the effectof the size of corpora on the alignment results.Second, we will investigate different parametercombination of the induced model and the origi-nal model.
Third, we will also investigate howsimpler IBM models 1 and 2 perform, in com-parison with IBM models 3 and 4.
Last, we willevaluate the word alignment results in a real ma-chine translation system, to examine whetherlower word alignment error rate will result inhigher translation accuracy.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical Machine TranslationFinal Report.
Johns Hopkins University Workshop.Niraj Aswani and Robert Gaizauskas.
2005.
AligningWords in English-Hindi Parallel Corpora.
In Proc.of the ACL 2005 Workshop on Building and UsingParallel Texts: Data-driven Machine Translationand Beyond, pages 115-118.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2): 263-311.Colin Cherry and Dekang Lin.
2003.
A ProbabilityModel to Improve Word Alignment.
In Proc.
of the41st  Annual Meeting of the Association for Compu-tational Linguistics (ACL-2003), pages 88-95.Sue J. Ker and Jason S. Chang.
1997.
A Class-basedApproach to Word Alignment.
Computational Lin-guistics, 23(2): 313-343.Adam Lopez and Philip Resnik.
2005.
ImprovedHMM Alignment Models for Languages withScarce Resources.
In Proc.
of the ACL-2005 Work-shop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 83-86.Joel Martin, Rada Mihalcea, and Ted Pedersen.
2005.Word Alignment for Languages with Scarce Re-sources.
In Proc.
of the ACL-2005 Workshop onBuilding and Using Parallel Texts: Data-drivenMachine Translation and Beyond, pages 65-74.Charles Schafer and David Yarowsky.
2002.
InducingTranslation Lexicons via Diverse Similarity Meas-ures and Bridge Languages.
In Proc.
of the 6thConference on Natural Language Learning 2002(CoNLL-2002), pages 1-7.Dan Tufis, Radu Ion, Alexandru Ceausu, and DanStefanescu.
2005.
Combined Word Alignments.
InProc.
of the ACL-2005 Workshop on Building andUsing Parallel Texts: Data-driven Machine Trans-lation and Beyond, pages 107-110.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proc.
of the 38thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2000), pages 440-447.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19-51.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377-403.Hua Wu, Haifeng Wang, and Zhanyi Liu.
2005.Alignment Model Adaptation for Domain-SpecificWord Alignment.
In Proc.
of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL-2005), pages 467-474.Hao Zhang and Daniel Gildea.
2005.
Stochastic Lexi-calized Inversion Transduction Grammar forAlignment.
In Proc.
of the 43rd Annual Meeting ofthe Association for Computational Linguistics(ACL-2005), pages 475-482.881
