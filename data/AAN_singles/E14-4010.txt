Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48?53,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsNon-Monotonic Parsing of Fluent umm I Mean Disfluent SentencesMohammad Sadegh RasooliDepartment of Computer ScienceColumbia University, New York, NY, USArasooli@cs.columbia.eduJoel TetreaultYahoo LabsNew York, NY, USAtetreaul@yahoo-inc.comAbstractParsing disfluent sentences is a challeng-ing task which involves detecting disflu-encies as well as identifying the syntacticstructure of the sentence.
While there havebeen several studies recently into solelydetecting disfluencies at a high perfor-mance level, there has been relatively lit-tle work into joint parsing and disfluencydetection that has reached that state-of-the-art performance in disfluency detec-tion.
We improve upon recent work in thisjoint task through the use of novel featuresand learning cascades to produce a modelwhich performs at 82.6 F-score.
It outper-forms the previous best in disfluency de-tection on two different evaluations.1 IntroductionDisfluencies in speech occur for several reasons:hesitations, unintentional mistakes or problems inrecalling a new object (Arnold et al., 2003; Merloand Mansur, 2004).
Disfluencies are often de-composed into three types: filled pauses (IJ) suchas ?uh?
or ?huh?, discourse markers (DM) suchas ?you know?
and ?I mean?
and edited words(reparandum) which are repeated or corrected bythe speaker (repair).
The following sentence illus-trates the three types:I want a flight to Boston?
??
?Reparandumuh???
?IJI mean?
??
?DMto Denver?
??
?RepairTo date, there have been many studies on disflu-ency detection (Hough and Purver, 2013; Rasooliand Tetreault, 2013; Qian and Liu, 2013; Wang etal., 2013) such as those based on TAGs and thenoisy channel model (e.g.
Johnson and Charniak(2004), Zhang et al.
(2006), Georgila (2009), andZwarts and Johnson (2011)).
High performancedisfluency detection methods can greatly enhancethe linguistic processing pipeline of a spoken dia-logue system by first ?cleaning?
the speaker?s ut-terance, making it easier for a parser to processcorrectly.
A joint parsing and disfluency detectionmodel can also speed up processing by mergingthe disfluency and parsing steps into one.
How-ever, joint parsing and disfluency detection mod-els, such as Lease and Johnson (2006), basedon these approaches have only achieved moder-ate performance in the disfluency detection task.Our aim in this paper is to show that a high perfor-mance joint approach is viable.We build on our previous work (Rasooli andTetreault, 2013) (henceforth RT13) to jointlydetect disfluencies while producing dependencyparses.
While this model produces parses at avery high accuracy, it does not perform as well asthe state-of-the-art in disfluency detection (Qianand Liu, 2013) (henceforth QL13).
In this pa-per, we extend RT13 in two important ways: 1)we show that by adding a set of novel features se-lected specifically for disfluency detection we canoutperform the current state of the art in disfluencydetection in two evaluations1and 2) we show thatby extending the architecture from two to six clas-sifiers, we can drastically increase the speed andreduce the memory usage of the model without aloss in performance.2 Non-monotonic Disfluency ParsingIn transition-based dependency parsing, a syntac-tic tree is constructed by a set of stack and bufferactions where the parser greedily selects an actionat each step until it reaches the end of the sentencewith an empty buffer and stack (Nivre, 2008).
Astate in a transition-based system has a stack ofwords, a buffer of unprocessed words and a set ofarcs that have been produced in the parser history.The parser consists of a state (or a configuration)1Honnibal and Johnson (2014) have a forthcoming paperbased on a similar idea but with a higher performance.48which is manipulated by a set of actions.
When anaction is made, the parser goes to a new state.The arc-eager algorithm (Nivre, 2004) is atransition-based algorithm for dependency pars-ing.
In the initial state of the algorithm, the buffercontains all words in the order in which they ap-pear in the sentence and the stack contains the arti-ficial root token.
The actions in arc-eager parsingare left-arc (LA), right-arc (RA), reduce (R) andshift (SH).
LA removes the top word in the stackby making it the dependent of the first word in thebuffer; RA shifts the first word in the buffer to thestack by making it the dependent of the top stackword; R pops the top stack word and SH pushesthe first buffer word into the stack.The arc-eager algorithm is a monotonic parsingalgorithm, i.e.
once an action is performed, subse-quent actions should be consistent with it (Honni-bal et al., 2013).
In monotonic parsing, if a wordbecomes a dependent of another word or acquiresa dependent, other actions shall not change thosedependencies that have been constructed for thatword in the action history.
Disfluency removal isan issue for monotonic parsing in that if an ac-tion creates a dependency relation, the other ac-tions cannot repair that dependency relation.
Themain idea proposed by RT13 is to change theoriginal arc-eager algorithm to a non-monotonicone so it is possible to repair a dependency treewhile detecting disfluencies by incorporating threenew actions (one for each disfluency type) into atwo-tiered classification process.
The structure isshown in Figure 1(a).
In short, at each state theparser first decides between the three new actionsand a parse action (C1).
If the latter is selected, an-other classifier (C2) is used to select the best parseaction as in normal arc eager parsing.The three additional actions to the arc-eager al-gorithm to facilitate disfluency detection are asfollows: 1) RP[i:j]: From the words outside thebuffer, remove words i to j from the sentence andtag them as reparandum, delete all of their depen-dencies and push all of their dependents onto thestack.
2) IJ[i]: Remove the first i words fromthe buffer (without adding any dependencies tothem) and tag them as interjection.
3) DM[i]:Remove the first i words from the buffer (with-out adding any dependencies) and tag them as dis-course marker.StateC1ParseRP[i:j]IJ[i]DM[i]C2LA RA RSH(a) A structure with two classifiers.IJ[i]C3DM[i]ParseC5C2IJDMC1 OtherC4 RPC6RLARA SHRP[i:j]State(b) A structure with six classifiers.Figure 1: Two kinds of cascades for disfluencylearning.
Circles are classifiers and light-coloredblocks show the final decision by the system.3 Model ImprovementsTo improve upon RT13, we first tried to learn allactions jointly.
Essentially, we added the threenew actions to the original arc-eager action set.However, this method (henceforth M1) performedpoorly on the disfluency detection task.
We be-lieve this stems from a feature mismatch, i.e.
someof the features, such as rough copies, are only use-ful for reparanda while some others are useful forother actions.
Speed is an additional issue.
Sincefor each state, there are many candidates for eachof the actions, the space of possible candidatesmakes the parsing time potentially squared.Learning Cascades One possible solution forreducing the complexity of the inference is to for-mulate and develop learning cascades where eachcascade is in charge of a subset of predictions withits specific features.
For this task, it is not es-sential to always search for all possible phrasesbecause only a minority of cases in speech textsare disfluent (Bortfeld et al., 2001).
For address-ing this problem, we propose M6, a new structurefor learning cascades, shown in Figure 1(b) witha more complex structure while more efficient interms of speed and memory.
In the new structure,we do not always search for all possible phraseswhich will lead to an expected linear time com-plexity.
The main processing overhead here is thenumber of decisions to make by classifiers but thisis not as time-intensive as finding all candidatephrases in all states.Feature Templates RT13 use different featuresets for the two classifiers: C2 uses the parse fea-49tures promoted in Zhang and Nivre (2011, Table1) and C1 uses features which are shown withregular font in Figure 2.
We show that one canimprove RT13 by adding new features to the C1classifier which are more appropriate for detectingreparanda (shown in bold in Figure 2).
We callthis new model M2E, ?E?
for extended.
In Figure3, the features for each classifier in RT13, M2E,M6 and M1 are described.We introduce the following new features: LIClooks at the number of common words between thereparandum candidate and words in the buffer; e.g.if the candidate is ?to Boston?
and the words in thebuffer are ?to Denver?, LIC[1] is one and LIC[2]is also one.
In other words, LIC is an indicatorof a rough copy.
The GPNG (post n-gram fea-ture) allows us to model the fluency of the result-ing sentence after an action is performed, withoutexplicitly going into it.
It is the count of possiblen-grams around the buffer after performing the ac-tion; e.g.
if the candidate is a reparandum action,this feature introduces the n-grams which will ap-pear after this action.
For example, if the sentenceis ?I want a flight to Boston | to Denver?
(where| is the buffer boundary) and the candidate is ?toBoston?
as reparandum, the sentence will look like?I want a flight | to Denver?
and then we can countall possible n-grams (both lexicalized and unlexi-calized) in the range i and j inside and outside thebuffer.
GBPF is a collection of baseline parse fea-tures from (Zhang and Nivre, 2011, Table 1).The need for classifier specific features be-comes more apparent in the M6 model.
Each ofthe classifiers uses a different set of features to op-timize performance.
For example, LIC featuresare only useful for the sixth classifier while postn-gram features are useful for C2, C3 and C6.
Forthe joint model we use the C1 features from M2Band the C1 features from M6.4 Experiments and EvaluationWe evaluate our new models, M2E and M6,against prior work on two different test conditions.In the first evaluation (Eval 1), we use the parsedsection of the Switchboard corpus (Godfrey et al.,1992) with the train/dev/test splits from Johnsonand Charniak (2004) (JC04).
All experimental set-tings are the same as RT13.
We compare our newmodels against this prior work in terms of disflu-ency detection performance and parsing accuracy.In the second evaluation (Eval 2), we compare ourAbbr.
DescriptionGS[i/j] First n Ws/POS outside ?
(n=1:i/j)GB[i/j] First n Ws/POS inside ?
(n=1:i/j)GL[i/j] Are n Ws/POS i/o ?
equal?
(n=1:i/j)GT[i] n last FGT; e.g.
parse:la (n=1:i)GTP[i] n last FGT e.g.
parse (n=1:i)GGT[i] n last FGT + POS of ?0(n=1:i)GGTP[i] n last CGT + POS of ?0(n=1:i)GN[i] (n+m)-gram of m/n POS i/o ?
(n,m=1:i)GIC[i] # common Ws i/o ?
(n=1:i)GNR[i] Rf.
(n+m)-gram of m/n POS i/o ?
(n,m=1:i)GPNG[i/j] PNGs from n/m Ws/POS i/o ?
(m,n:1:i/j)GBPF Parse features (Zhang and Nivre, 2011)LN[i,j] First n Ws/POS of the cand.
(n=1:i/j)LD Distance between the cand.
and s0LL[i,j] first n Ws/POS of rp and ?
equal?
(n=1:i/j)LIC[i] # common Ws for rp/repair (n=1:i)Figure 2: Feature templates used in this paper andtheir abbreviations.
?
: buffer, ?0: first word inthe buffer, s0: top stack word, Ws: words, rp:reparadnum, cand.
: candidate phrase, PNGs: postn-grams, FGT: fine-grained transitions and CGT:coarse-grained transitions.
Rf.
n-gram: n-gramfrom unremoved words in the state.Classifier FeaturesM2 FeaturesC1 (RT13) GS[4/4], GB[4/4], GL[4/6], GT[5], GTP[5]GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]LL[4/6], LDC1 (M2E) RT13 ?
(LIC[6], GBPF, GPNG[4/4]) - LDC2 GBPFM6 FeaturesC1 GBPF, GB[4/4], GL[4/6], GT[5], GTP[5]GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]C2 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]GN[4], GNR[4], GPNG[4/4], LD, LN[24/24]C3 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]GN[4], GNR[4], GPNG[4/4], LD, LN[12/12]C4 GBPF, GS[4/6], GT[5], GTP[5], GGT[5]GGTP[5], GN[4], GNR[4], GIC[13]C5 GBPFC6 GBPF, LL[4/6], GPNG[4/4]LN[6/6], LD, LIC[13]M1 Features: RT13 C1 features ?
C2 featuresFigure 3: Features for each model.
M2E is thesame as RT13 with extended features (bold fea-tures in Figure 2).
M6 is the structure with sixclassifiers.
Other abbreviations are described inFigure 2.work against the current best disfluency detectionmethod (QL13) on the JC04 split as well as on a10 fold cross-validation of the parsed section ofthe Switchboard.
We use gold POS tags for allevaluations.For all of the joint parsing models we use theweighted averaged Perceptron which is the sameas averaged Perceptron (Collins, 2002) but with a50loss weight of two for reparandum candidates asdone in prior work.
The standard arc-eager parseris first trained on a ?cleaned?
Switchboard corpus(i.e.
after removing disfluent words) with 3 train-ing iterations.
Next, it is updated by training it onthe real corpus with 3 additional iterations.
Forthe other classifiers, we use the same number ofiterations determined from the development set.Eval 1 The disfluency detection and parse re-sults on the test set are shown in Table 1 for thefour systems (M1, RT13, M2E and M6).
The jointmodel performs poorly on the disfluency detectiontask, with an F-score of 41.5, and the prior workperformance which serves as our baseline (RT13)has a performance of 81.4.
The extended versionof this model (M2E) raises performance substan-tially to 82.2.
This shows the utility of training theC1 classifier with additional features.
Finally, theM6 classifier is the top performing model at 82.6.Disfluency ParseModel Pr.
Rec.
F1 UAS F1M1 27.4 85.8 41.5 60.2 64.6RT13 85.1 77.9 81.4 88.1 87.6M2E 88.1 77.0 82.2 88.1 87.6M6 87.7 78.1 82.6 88.4 87.7Table 1: Comparison of joint parsing and disflu-ency detection methods.
UAS is the unlabeledparse accuracy score.The upperbound for the parser attachment ac-curacy (UAS) is 90.2 which basically means thatif we have gold standard disfluencies and removedisfluent words from the sentence and then parsethe sentence with a regular parser, the UAS willbe 90.2.
If we had used the regular parser to parsethe disfluent sentences, the UAS for correct wordswould be 70.7.
As seen in Table 1, the best parserUAS is 88.4 (M6) which is very close to the up-perbound, however RT13, M2E and M6 are nearlyindistinguishable in terms of parser performance.Eval 2 To compare against QL13, we use thesecond version of the publicly provided code andmodify it so it uses gold POS tags and retrain andoptimize it for the parsed section of the Switch-board corpus (these are known as mrg files, andare a subset of the section of the Switchboard cor-pus used in QL13, known as dps files).
Since theirsystem has parameters tuned for the dps Switch-board corpus we retrained it for a fair comparison.As in the reimplementation of RT13, we have eval-uated the QL13 system with optimal number oftraining iterations (10 iterations).
As seen in Table2, although the annotation in the mrg files is lessprecise than in the dps files, M6 outperforms allmodels on the JC04 split thus showing the powerof the new features and new classifier structure.Model JC04 split xvalRT13 81.4 81.6QL13 (optimized) 82.5 82.2M2E 82.2 82.8M6 82.6 82.7Table 2: Disfluency detection results (F1 score) onJC04 split and with cross-validation (xval)To test for robustness of our model, we per-form 10-fold cross validation after clustering filesbased on their name alphabetic order and creating10 data splits.
As seen in Table 2, the top modelis actually M2E, nudging out M6 by 0.1.
Morenoticeable is the difference in performance overQL13 which is now 0.6.Speed and memory usage Based on our Javaimplementation on a 64-bit 3GHz Intel CPU with68GB of memory, the speed for M6 (36 ms/sent)is 3.5 times faster than M2E (128 ms/sent) and 5.2times faster than M1 (184 ms/sent) and it requireshalf of the nonzero features overall compared toM2E and one-ninth compared to M1.5 Conclusion and Future DirectionsIn this paper, we build on our prior work by in-troducing rich and novel features to better handlethe detection of reparandum and by introducing animproved classifier structure to decrease the uncer-tainty in decision-making and to improve parserspeed and accuracy.
We could use early updating(Collins and Roark, 2004) for learning the greedyparser which is shown to be useful in greedy pars-ing (Huang and Sagae, 2010).
K-beam parsing is away to improve the model though at the expense ofspeed.
The main problem with k-beam parsers isthat it is complicated to combine classifier scoresfrom different classifiers.
One possible solutionis to modify the three actions to work on just oneword per action, thus the system will run in com-pletely linear time with one classifier and k-beamparsing can be done by choosing better featuresfor the joint parser.
A model similar to this idea isdesigned by Honnibal and Johnson (2014).51Acknowledgement We would like to thank thereviewers for their comments and useful insights.The bulk of this research was conducted whileboth authors were working at Nuance Commu-nication, Inc.?s Laboratory for Natural LanguageUnderstanding in Sunnyvale, CA.ReferencesJennifer E. Arnold, Maria Fagnano, and Michael K.Tanenhaus.
2003.
Disfluencies signal theee, um,new information.
Journal of Psycholinguistic Re-search, 32(1):25?36.Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,Michael F. Schober, and Susan E. Brennan.
2001.Disfluency rates in conversation: Effects of age, re-lationship, topic, role, and gender.
Language andSpeech, 44(2):123?147.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain.
Association forComputational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.
Associ-ation for Computational Linguistics.Kallirroi Georgila.
2009.
Using integer linear pro-gramming for detecting speech disfluencies.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, pages109?112, Boulder, Colorado.
Association for Com-putational Linguistics.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP-92), volume 1, pages 517?520.Matthew Honnibal and Mark Johnson.
2014.
Joint in-cremental disuency detection and dependency pars-ing.
Transactions of the Association for Computa-tional Linguistics (TACL), to appear.Matthew Honnibal, Yoav Goldberg, and Mark John-son.
2013.
A non-monotonic arc-eager transitionsystem for dependency parsing.
In Proceedings ofthe Seventeenth Conference on Computational Natu-ral Language Learning, pages 163?172, Sofia, Bul-garia.
Association for Computational Linguistics.Julian Hough and Matthew Purver.
2013.
Modellingexpectation in the self-repair processing of annotat-,um, listeners.
In The 17th Workshop on the Seman-tics and Pragmatics of Dialogue.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086, Uppsala, Sweden.
Association for Computa-tional Linguistics.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 33?39, Barcelona, Spain.Matthew Lease and Mark Johnson.
2006.
Early dele-tion of fillers in processing conversational speech.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Companion Volume:Short Papers, pages 73?76, New York City, USA.Association for Computational Linguistics.Sandra Merlo and Let?cia Lessa Mansur.
2004.Descriptive discourse: topic familiarity and dis-fluencies.
Journal of Communication Disorders,37(6):489?503.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineeringand Cognition Together, pages 50?57.
Associationfor Computational Linguistics.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 820?825, At-lanta, Georgia.
Association for Computational Lin-guistics.Mohammad Sadegh Rasooli and Joel Tetreault.
2013.Joint parsing and disfluency detection in linear time.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages124?129, Seattle, Washington, USA.
Associationfor Computational Linguistics.Wen Wang, Andreas Stolcke, Jiahong Yuan, and MarkLiberman.
2013.
A cross-language study on au-tomatic speech disfluency detection.
In Proceed-ings of the 2013 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages703?708, Atlanta, Georgia.
Association for Compu-tational Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
In52Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA.
Association for Computational Linguis-tics.Qi Zhang, Fuliang Weng, and Zhe Feng.
2006.
A pro-gressive feature selection algorithm for ultra largefeature spaces.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 561?568, Sydney, Aus-tralia.
Association for Computational Linguistics.Simon Zwarts and Mark Johnson.
2011.
The impactof language models and loss functions on repair dis-fluency detection.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages703?711, Portland, Oregon, USA.
Association forComputational Linguistics.53
