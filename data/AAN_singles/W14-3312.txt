Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122?129,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsAnaphora Models and Reordering for Phrase-Based SMTChristian Hardmeier Sara Stymne J?org Tiedemann Aaron Smith Joakim NivreUppsala UniversityDepartment of Linguistics and Philologyfirstname.lastname@lingfil.uu.seAbstractWe describe the Uppsala University sys-tems for WMT14.
We look at the integra-tion of a model for translating pronomi-nal anaphora and a syntactic dependencyprojection model for English?French.
Fur-thermore, we investigate post-ordering andtunable POS distortion models for English?German.1 IntroductionIn this paper we describe the Uppsala Universitysystems for WMT14.
We present three differentsystems.
Two of them are based on the document-level decoder Docent (Hardmeier et al., 2012; Hard-meier et al., 2013a).
In our English?French sys-tem we extend Docent to handle pronoun anaphora,and in our English?German system we add part-of-speech phrase-distortion models to Docent.
ForGerman?English we also have a system based onMoses (Koehn et al., 2007).
Again the focus ison word order, this time by using pre- and post-reordering.2 Document-Level DecodingTraditional SMT decoders translate texts as bagsof sentences, assuming independence between sen-tences.
This assumption allows efficient algorithmsfor exploring a large search space based on dy-namic programming (Och et al., 2001).
Because ofthe dynamic programming assumptions it is hard todirectly include discourse-level and long-distancefeatures into a traditional SMT decoder.In contrast to this very popular stack decodingapproach, our decoder Docent (Hardmeier et al.,2012; Hardmeier et al., 2013a) implements a searchprocedure based on local search.
At any stage ofthe search process, its search state consists of acomplete document translation, making it easy forfeature models to access the complete documentwith its current translation at any point in time.
Thesearch algorithm is a stochastic variant of standardhill climbing.
At each step, it generates a successorof the current search state by randomly applyingone of a set of state changing operations to a ran-dom location in the document, and accepts the newstate if it has a better score than the previous state.The operations are to change the translation of aphrase, to change the word order by swapping thepositions of two phrases or moving a sequence ofphrases, and to resegment phrases.
The initial statecan either be initialized randomly, or be based onan initial run from Moses.
This setup is not limitedby dynamic programming constraints, and enablesthe use of the full translated target document toextract features.3 English?FrenchOur English?French system is a phrase-based SMTsystem with a combination of two decoders, Moses(Koehn et al., 2007) and Docent (Hardmeier et al.,2013a).
The fundamental setup is loosely basedon the system submitted by Cho et al.
(2013) tothe WMT 2013 shared task.
Our phrase table istrained on data taken from the News commentary,Europarl, UN, Common crawl and 109corpora.The first three of these corpora were included in-tegrally into the training set after filtering out sen-tences of more than 80 words.
The Common crawland 109data sets were run through an additionalfiltering step with an SVM classifier, closely fol-lowing Mediani et al.
(2011).
The system includesthree language models, a regular 6-gram modelwith modified Kneser-Ney smoothing (Chen andGoodman, 1998) trained with KenLM (Heafield,2011), a 4-gram bilingual language model (Niehueset al., 2011) with Kneser-Ney smoothing trainedwith KenLM and a 9-gram model over Brown clus-ters (Brown et al., 1992) with Witten-Bell smooth-ing (Witten and Bell, 1991) trained with SRILM(Stolcke, 2002).122The latest version released in March is equipped with .
.
.
It is sold at .
.
.La derni`ere version lanc?ee en mars est dot?ee de .
.
.
?
est vendue .
.
.Figure 1: Pronominal Anaphora ModelOur baseline system achieved a cased BLEUscore of 33.2 points on the newstest2014 data set.Since the anaphora model used in our submissionsuffered from a serious bug, we do not discuss theresults of the primary submission in more detail.3.1 Pronominal Anaphora ModelOur pronominal anaphora model is an adaptationof the pronoun prediction model described by Hard-meier et al.
(2013b) to SMT.
The model consistsof a neural network that discriminatively predictsthe translation of a source language pronoun froma short list of possible target language pronouns us-ing features from the context of the source languagepronouns and from the translations of possibly re-mote antecedents.
The objective of this model is tohandle situations like the one depicted in Figure 1,where the correct choice of a target-language pro-noun is subject to morphosyntactic agreement withits antecedent.
This problem consists of severalsteps.
To score a pronoun, the system must decideif a pronoun is anaphoric and, if so, find potentialantecedents.
Then, it can predict what pronounsare likely to occur in the translation.
Our pronounprediction model is trained on both tasks jointly,including anaphora resolution as a set of latent vari-ables.
At test time, we split the network in twoparts.
The anaphora resolution part is run sepa-rately as a preprocessing step, whereas the pronounprediction part is integrated into the document-leveldecoder with two additional feature models.The features correspond to two copies of the neu-ral network, one to handle the singular pronoun itand one to handle the plural pronoun they.
Each net-work just predicts a binary distinction between twocases, il and elle for the singular network and ilsand elles for the plural network.
Unlike Hardmeieret al.
(2013b), we do not use an OTHER category tocapture cases that should not be translated with anyof these options.
Instead, we treat all other cases inthe phrase table and activate the anaphora modelsonly if one of their target pronouns actually occursin the output.To achieve this, we generate pronouns in twosteps.
In the phrase table training corpus, we re-place all pronouns that should be handled by theclassifier, i.e.
instances of il and elle aligned to itand instances of ils and elles aligned to they, withspecial placeholders.
At decoding time, if a place-holder is encountered in a target language phrase,the applicable pronouns are generated with equaltranslation model probability, and the anaphoramodel adds a score to discriminate between them.To reduce the influence of the language modelon pronoun choice and give full control to theanaphora model, our primary language model istrained on text containing placeholders instead ofpronouns.
Since all output pronouns can also begenerated without the interaction of the anaphoramodel if they are not aligned to a source languagepronoun, we must make sure that the languagemodel sees training data for both placeholders andactual pronouns.
However, for the monolingualtraining corpora we have no word alignments todecide whether or not to replace a pronoun by aplaceholder.
To get around this problem, we train a6-gram placeholder language model on the targetlanguage side of the Europarl and News commen-tary corpora.
Then, we use the Viterbi n-grammodel decoder of SRILM (Stolcke, 2002) to mappronouns in the entire language model training setto placeholders where appropriate.
No substitu-tions are made in the bilingual language model orthe Brown cluster language model.3.2 Dependency Projection ModelOur English?French system also includes a depen-dency projection model, which uses source-sidedependency structure to model target-side relationsbetween words.
This model assigns a score to eachdependency arc in the source language by consider-ing the target words aligned to the head and the de-pendent.
In Figure 2, for instance, there is an nsub-jpass arc connecting dominated to production.
Thehead is aligned to the target word domin?ee, whilethe dependent is aligned to the set {production,de}.The score is computed by a neural network takingas features the head and dependent words and theirpart-of-speech tags in the source language, the tar-get word sets aligned to the head and dependent,the label of the dependency arc, the distance be-tween the head and dependent word in the sourcelanguage as well as the shortest distance betweenany pair of words in the aligned sets.
The networkis a binary classifier trained to discriminate positiveexamples extracted from human-made reference123Domestic meat production is dominated by chicken .amodnnnsubjpassauxpass prep pobjpunctLa production int?erieure de viande est domin?ee par le poulet .Figure 2: Dependency projection modeltranslations from negative examples extracted fromn-best lists generated by a baseline SMT system.4 English?GermanFor English?German we have two systems, onebased on Moses, and one based on Docent.
In bothcases we have focused on word order, particularlyfor verbs and particles.Both our systems are trained on the same datamade available by WMT.
The Common crawl datawas filtered using the method of Stymne et al.(2013).
We use factored models with POS tagsas a second output factor for German.
The possi-bility to use language models for different factorshas been added to our Docent decoder.
Languagemodels include an in-domain news language model,an out-of-domain model trained on the target sideof the parallel training data and a POS languagemodel trained on tagged news data.
The LMs aretrained in the same way as for English?French.All systems are tuned using MERT (Och, 2003).Phrase-tables are filtered using entropy-based prun-ing (Johnson et al., 2007) as implemented in Moses.All BLEU scores are given for uncased data.4.1 Pre-Ordered Alignment andPost-Ordered TranslationThe use of syntactic reordering as a separate pre-processing step has already a long tradition in sta-tistical MT.
Handcrafted rules (Collins et al., 2005;Popovi?c and Ney, 2006) or data-driven models (Xiaand McCord, 2004; Genzel, 2010; Rottmann andVogel, 2007; Niehues and Kolss, 2009) for pre-ordering training data and system input have beenexplored in numerous publications.
For certainlanguage pairs, such as German and English, thismethod can be very effective and often improvesthe quality of standard SMT systems significantly.Typically, the source language is reordered to bettermatch the syntax of the target language when trans-lating between languages that exhibit consistentword order differences, which are difficult to handleby SMT systems with limited reordering capabil-ities such as phrase-based models.
Preordering isoften done on the entire training data as well to op-timize translation models for the pre-ordered input.Less common is the idea of post-ordering, whichrefers to a separate step after translating source lan-guage input to an intermediate target language withcorrupted (source-language like) word order (Na etal., 2009; Sudoh et al., 2011).In our experiments, we focus on the translationfrom English to German.
Post-ordering becomesattractive for several reasons: One reason is thecommon split of verb-particle constructions thatcan lead to long distance dependencies in Germanclauses.
Phrase-based systems and n-gram lan-guage models are not able to handle such relationsbeyond a certain distance and it is desirable to keepthem as connected units in the phrase translationtables.
Another reason is the possible distance offinite and infinitival verbs in German verb phrasesthat can lead to the same problems described abovewith verb-particle constructions.
The auxiliary ormodal verb is placed at the second position butthe main verb appears at the end of the associatedverb phrase.
The distances can be arbitrarily longand long-range dependencies are quite frequent.Similarly, negation particles and adverbials moveaway from the inflected verb forms in certain con-structions.
For more details on specific phenomenain German, we refer to (Collins et al., 2005; Go-jun and Fraser, 2012).
Pre-ordering, i.e.
movingEnglish words into German word order does notseem to be a good option as we loose the con-nection between related items when moving par-ticles and main verbs away from their associatedelements.
Hence, we are interested in reorderingthe target language German into English word or-der which can be beneficial in two ways: (i) Re-ordering the German part of the parallel trainingdata makes it possible to improve word alignment(which tends to prefer monotonic mappings) andsubsequent phrase extraction which leads to bettertranslation models.
(ii) We can explore a two-stepprocedure in which we train a phrase-based SMTmodel for translating English into German withEnglish word order first (which covers many long-distance relations locally) and then apply a secondsystem that moves words into place according tocorrect German syntax (which may involve long-range distortion).For simplicity, we base our experiments on hand-124crafted rules for some of the special cases discussedabove.
For efficiency reasons, we define our rulesover POS tag patterns rather than on full syntac-tic parse trees.
We rely on TreeTagger and applyrules to join verbs in discontinuous verb phrasesand to move verb-finals in subordinate clauses, tomove verb particles, adverbials and negation par-ticles.
Table 1 shows two examples of reorderedsentences together with the original sentences inEnglish and German.
Our rules implement roughheuristics to identify clause boundaries and wordpositions.
We do not properly evaluate these rulesbut focus on the down-stream evaluation of the MTsystem instead.It is therefore dangerous to extrapolate from short-term trends.Daher ist es gef?ahrlich, aus kurzfristigen Trends Prognosen abzuleiten.Daher ist gef?ahrlich es, abzuleiten aus kurzfristigen Trends Prognosen.The fall of Saddam ushers in the right circumstances.Der Sturz von Saddam leitet solche richtigen Umst?ande ein.Der Sturz von Saddam ein leitet solche richtigen Umst?ande.Table 1: Two examples of pre-ordering outputs.The first two lines are the original English andGerman sentences and the third line shows the re-ordered sentence.We use three systems based on Moses to com-pare the effect of reordering on alignment and trans-lation.
All systems are case-sensitive phrase-basedsystems with lexicalized reordering trained on dataprovided by WMT.
Word alignment is performedusing fast align (Dyer et al., 2013).
For tuning weuse newstest2011.
Additionally, we also test paral-lel data from OPUS (Tiedemann, 2012) filtered bya method adopted from Mediani et al.
(2011).To contrast our baseline system, we trained aphrase-based model on parallel data that has beenaligned on data pre-ordered using the reorderingrules for German, which has been restored to theoriginal word order after word alignment and be-fore phrase extraction (similar to (Carpuat et al.,2010; Stymne et al., 2010)).
We expect that theword alignment is improved by reducing crossingsand long-distance links.
However, the translationmodel as such has the same limitations as the base-line system in terms of long-range distortions.
Thefinal system is a two-step model in which we applytranslation and language models trained on pre-ordered target language data to perform the firststep, which also includes a reordered POS languagemodel.
The second step is also treated as a transla-tion problem as in Sudoh et al.
(2011), and in ourcase we use a phrase-based model here with lexical-ized reordering and a rather large distortion limitof 12 words.
Another possibility would be to applyanother rule set that reverts the misplaced wordsto the grammatically correct positions.
This, how-ever, would require deeper syntactic informationabout the target language to, for example, distin-guish main from subordinate clauses.
Instead, ourmodel is trained on parallel target language datawith the pre-ordered version as input and the orig-inal version as output language.
For this model,both sides are tagged and a POS language modelis used again as one of the target language factorsin decoding.
Table 2 shows the results in terms ofBLEU scores on the newstest sets from 2013 and2014.newstest2013 newstest2014baseline 19.3 19.1pre 19.4 19.3post 18.6 18.7baseline+OPUS 19.5 19.3pre+OPUS 19.5 19.3post+OPUS 19.7 18.8Table 2: BLEU4 scores for English-German sys-tems (w/o OPUS): Standard phrase-based (base-line); phrase-based with pre-ordered parallel cor-pus used for word alignment (pre); two-step phrase-based with post-reordering (post)The results show that pre-ordering has some ef-fect on word alignment quality in terms of support-ing better phrase extractions in subsequent steps.Our experiments show a consistent but small im-provement for models trained on data that havebeen prepared in this way.
In contrast, the two-stepprocedure is more difficult to judge in terms of au-tomatic metrics.
On the 2013 newstest data we cansee another small improvement in the setup thatincludes OPUS data but in most cases the BLEUscores go down, even below the baseline.
Theshort-comings of the two-step procedure are ob-vious.
Separating translation and reordering in apipeline adds the risk of error propagation.
Fur-thermore, reducing the second step to single-besttranslations is a strong limitation and using phrase-based models for the final reordering procedure isprobably not the wisest decision.
However, manualinspections reveals that many interesting phenom-ena can be handled even with this simplistic setup.Table 3 illustrates this with a few selected out-comes of our three systems.
They show how verb-particle constructions with long-range distortion125reference Schauspieler Orlando Bloom hat sich zur Trennung von seiner Frau , Topmodel Miranda Kerr , ge?au?ert .baseline Schauspieler Orlando Bloom hat die Trennung von seiner Frau , Supermodel Miranda Kerr .pre-ordering Schauspieler Orlando Bloom hat angek?undigt , die Trennung von seiner Frau , Supermodel Miranda Kerr .post-ordering Schauspieler Orlando Bloom hat seine Trennung von seiner Frau angek?undigt , Supermodel Miranda Kerr .reference Er gab bei einer fr?uheren Befragung den Kokainbesitz zu .baseline Er gab den Besitz von Kokain in einer fr?uheren Anh?orung .pre-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung .post-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung ein .reference Borussia Dortmund k?undigte daraufhin harte Konsequenzen an .baseline Borussia Dortmund k?undigte an , es werde schwere Folgen .pre-ordering Borussia Dortmund hat angek?undigt , dass es schwerwiegende Konsequenzen .post-ordering Borussia Dortmund k?undigte an , dass es schwere Folgen geben werde .Table 3: Selected translation examples from the newstest 2014 data; the human reference translation; thebaseline system, pre-ordering for word alignment and two-step translation with post-ordering.such as ?r?aumte ... ein?
can be created and howdiscontinuous verb phrases can be handled (?hat ...angek?undigt?)
with the two-step procedure.
Themodel is also often better in producing verb finalsin subordinate clauses (see the final example with?geben werde?).
Note that many of these improve-ments do not get any credit by metrics like BLEU.For example the acceptable expression ?r?aumte ein?which is synonymous to ?gab zu?
obtains less creditthen the incomplete baseline translation.
Interest-ing is also to see the effect of pre-ordering whenused for alignment only in the second system.
Thefirst example in Table 3, for example, includes acorrect main verb which is omitted in the baselinetranslation, probably because it is not extracted asa valid translation option.4.2 Part-of-Speech Phrase-Distortion ModelsTraditional SMT distortion models consist of twoparts.
A distance-based distortion cost is basedon the position of the last word in a phrase, com-pared to the first word in the next phrase, given thesource phrase order.
A hard distortion limit blockstranslations where the distortion is too large.
Thedistortion limit serves to decrease the complexityof the decoder, thus increasing its speed.In the Docent decoder, the distortion limit is notimplemented as a hard limit, but as a feature, whichcould be seen as a soft constraint.
We showed inprevious work (Stymne et al., 2013) that it wasuseful to relax the hard distortion limit by eitherusing a soft constraint, which could be tuned, orremoving the limit completely.
In that work westill used the standard parametrization of distortion,based on the positions of the first and last words inphrases.Our Docent decoder, however, always providesus with a full target translation that is step-wise im-proved, which means that we can apply distortionmeasures on the phrase-level without resorting toheuristics, which, for instance, are needed in thecase of the lexicalized reordering models in Moses(Koehn et al., 2005).
Because of this it is possibleto use phrase-based distortion, where we calculatedistortion based on the order of phrases, not on theorder of some words.
It is possible to parametrizephrase-distortion in different ways.
In this work weuse the phrase-distortion distance and a soft limiton the distortion distance, to mimic the word-baseddistortion.
In our experiments we always set thesoft limit to a distance of four phrases.
In additionwe use a measure based on how many crossingsa phrase order gives rise to.
We thus have threephrase-distortion features.As captured by lexicalized reordering models,different phrases have different tendencies to move.To capture this to some extent, we also decidedto add part-of-speech (POS) classes to our mod-els.
POS has previously successfully been usedin pre-reordering approaches (Popovi?c and Ney,2006; Niehues and Kolss, 2009).
The word typesthat are most likely to move long distances inEnglish?German translation are verbs and parti-cles.
Based on this observation we split phrasesinto two classes, phrases that only contains verbsand particles, and all other phrases.
For these twogroups we use separate phrase-distortion features,thus having a total of six part-of-speech phrase-distortion features.
All of these features are soft,and are optimized during tuning.In our system we initialize Docent by runningMoses with a standard distortion model and lexi-calized reordering, and then continuing the searchwith Docent including our part-of-speech phrase-distortion features.
Tuning was done separately forthe two components, first for the Moses component,and then for the Docent component initialized by126reference Laut Dmitrij Kislow von der Organisation ?Pravo na oryzhie?
kann man eine Pistole vom Typ Makarow f?ur 100 bis 300 Dollar kaufen.baseline Laut Dmitry Kislov aus der Rechten zu Waffen, eine Makarov Gun-spiele erworben werden k?onnen f?ur 100-300 Dollar.POS+phrase Laut Dmitry Kislov von die Rechte an Waffen, eine Pistole Makarov f?ur 100-300 Dollar erworben werden k?onnen.reference Die Waffen gelangen ?uber mehrere Kan?ale auf den Schwarzmarkt.baseline Der ?Schwarze?
Markt der Waffen ist wieder aufgef ?ullt ?uber mehrere Kan?ale.POS+phrase Der ?Schwarze?
Markt der Waffen durch mehrere Kan?ale wieder aufgef ?ullt ist.reference Mehr Kameras k?onnten m?oglicherweise das Problem l?osen...baseline M?oglicherweise k?onnte das Problem l?osen, eine gro?e Anzahl von Kameras...POS+phrase M?oglicherweise, eine gro?e Anzahl von Kameras k?onnte das Problem l?osen...Table 4: Selected translation examples from the newstest2013 data; the human reference translation; thebaseline system (Moses with lexicalized reordering) and the system with a POS+phrase distortion model.Moses with lexicalized reordering with its tunedweights.
We used newstest2009 for tuning.
Thetraining data was lowercased for training and de-coding, and recasing was performed using a sec-ond Moses run trained on News data.
As baselineswe present two Moses systems, without and withlexicalized reordering, in addition to standard dis-tortion features.Table 5 shows results with our different distor-tion models.
Overall the differences are quite small.The clearest difference is between the two Mosesbaselines, where the lexicalized reordering modelleads to an improvement.
With Docent, both theword distortion and phrase distortion without POSdo not help to improve on Moses, with a small de-crease in scores on one dataset.
This is not verysurprising, since lexical distortion is currently notsupported by Docent, and the distortion models arethus weaker than the ones implemented in Moses.For our POS phrase distortion, however, we see asmall improvement compared to Moses, despite thelack of lexicalized distortion.
This shows that thisdistortion model is actually useful, and can evensuccessfully replace lexicalized reordering.
In fu-ture work, we plan to combine this method with alexicalized reordering model, to see if the two mod-els have complementary strengths.
Our submittedsystem uses the POS phrase-distortion model.System Distortion newstest2013 newstest2014Moses word 19.4 19.3Moses word+LexReo 19.6 19.6Docent word 19.5 19.6Docent phrase 19.5 19.6Docent POS+phrase 19.7 19.7Table 5: BLEU4 scores for English?German sys-tems with different distortion models.If we inspect the translations, most of the differ-ences between the Moses baseline and the systemwith POS+phrase distortion are actually due to lex-ical choice.
Table 4 shows some examples wherethere are word order differences.
The result is quitemixed with respect to the placement of verbs.
Inthe first example, both systems put the verbs to-gether but in different positions, instead of splittingthem like the reference suggests.
In the secondexample, our system erroneously put the verbs atthe end, which would be fine if the sentence hadbeen a subordinate clause.
In the third example,the baseline system has the correct placement ofthe auxiliary ?k?onnte?, while our system is bet-ter at placing the main verb ?l?osen?.
In general,this indicates that our system is able to supportlong-distance distortion as it is needed in certaincases but sometimes overuses this flexibility.
Abetter model would certainly need to incorporatesyntactic information to distinguish main from sub-ordinate clauses.
However, this would add a lot ofcomplexity to the model.5 ConclusionWe have described the three Uppsala Universitysystems for WMT14.
In the English?French sys-tem we extend our document-level decoder Do-cent (Hardmeier et al., 2013a) to handle pronounanaphora and introduced a dependency projectionmodel.
In our two English?German system weexplore different methods for handling reordering,based on Docent and Moses.
In particular, we lookat post-ordering as a separate step and tunable POSphrase distortion.AcknowledgementsThis work forms part of the Swedish strategic re-search programme eSSENCE.
We also acknowl-edge the use of the Abel cluster, owned by theUniversity of Oslo and the Norwegian metacenterfor High Performance Computing (NOTUR) andoperated by the Department for Research Comput-ing at USIT, under project nn9106k.
Finally, wewould also like to thank Eva Pettersson, Ali Basirat,and Eva Martinez for help with human evaluation.127ReferencesPeter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational linguistics, 18(4):467?479.Marine Carpuat, Yuval Marton, and Nizar Habash.2010.
Improving Arabic-to-English statistical ma-chine translation by reordering post-verbal subjectsfor alignment.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 178?183, Uppsala, Swe-den.Stanley F. Chen and Joshua Goodman.
1998.
Anempirical study of smoothing techniques for lan-guage modeling.
Technical report, Computer Sci-ence Group, Harvard University, Cambridge, Mas-sachusetts, USA.Eunah Cho, Thanh-Le Ha, Mohammed Mediani, JanNiehues, Teresa Herrmann, Isabel Slawik, and AlexWaibel.
2013.
The Karlsruhe Institute of Technol-ogy translation systems for the WMT 2013.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 104?108, Sofia, Bulgaria.Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the ACL, pages 531?540, Ann Arbor, Michi-gan, USA.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM model 2.
In Proceedings of the 2013Conference of the NAACL: Human Language Tech-nologies, pages 644?648, Atlanta, Georgia, USA.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, pages 376?384, Beijing, China.Anita Gojun and Alexander Fraser.
2012.
Determin-ing the placement of German verbs in English?to?German SMT.
In Proceedings of the 13th Confer-ence of the EACL, pages 726?735, Avignon, France.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1179?1190, Jeju Island, Korea.Christian Hardmeier, Sara Stymne, J?org Tiedemann,and Joakim Nivre.
2013a.
Docent: A document-level decoder for phrase-based statistical machinetranslation.
In Proceedings of the 51st Annual Meet-ing of the ACL, Demonstration session, pages 193?198, Sofia, Bulgaria.Christian Hardmeier, J?org Tiedemann, and JoakimNivre.
2013b.
Latent anaphora resolution for cross-lingual pronoun prediction.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 380?391, Seattle,Washington, USA.
Association for ComputationalLinguistics.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland.
Associationfor Computational Linguistics.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 967?975, Prague, Czech Republic.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation, Pittsburgh, Penn-sylvania, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACL,Demo and Poster Sessions, pages 177?180, Prague,Czech Republic.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
TheKIT English?French translation systems for IWSLT2011.
In Proceedings of the International Workshopon Spoken Language Translation, pages 73?78, SanFrancisco, California, USA.Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-HyeokLee.
2009.
Improving fluency by reordering tar-get constituents using MST parser in English-to-Japanese phrase-based SMT.
In Proceedings ofMT Summit XII, pages 276?283, Ottawa, Ontario,Canada.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 206?214, Athens, Greece.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider context by using bilin-gual language models in machine translation.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 198?206, Edinburgh, Scot-land.
Association for Computational Linguistics.128Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for Statisti-cal Machine Translation.
In Proceedings of the ACL2001 Workshop on Data-Driven Machine Transla-tion, pages 55?62, Toulouse, France.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the42nd Annual Meeting of the ACL, pages 160?167,Sapporo, Japan.Maja Popovi?c and Hermann Ney.
2006.
POS-based re-orderings for statistical machine translation.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation (LREC?06), pages1278?1283, Genoa, Italy.Kay Rottmann and Stephan Vogel.
2007.
Word re-ordering in statistical machine translation with aPOS-based distortion model.
In Proceedings ofthe 11th International Conference on Theoreticaland Methodological Issues in Machine Translation,pages 171?180, Sk?ovde, Sweden.Andreas Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proceedings of theSeventh International Conference on Spoken Lan-guage Processing, pages 901?904, Denver, Col-orado, USA.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2010.
Vs and OOVs: Two problems for translationbetween German and English.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 183?188, Uppsala,Sweden.Sara Stymne, Christian Hardmeier, J?org Tiedemann,and Joakim Nivre.
2013.
Tunable distortion limitsand corpus cleaning for SMT.
In Proceedings of theEighth Workshop on Statistical Machine Translation,pages 225?231, Sofia, Bulgaria.Katsuhito Sudoh, Xianchao Wu, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Post-orderingin statistical machine translation.
In Proceedings ofMT Summit XIII, pages 316?323, Xiamen.
China.J?org Tiedemann.
2012.
Parallel data, tools and in-terfaces in OPUS.
In Proceedings of the 8th In-ternational Conference on Language Resources andEvaluation (LREC?12), pages 2214?2218, Istanbul,Turkey.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proceedings of the 20th Inter-national Conference on Computational Linguistics,pages 508?514, Geneva, Switzerland.129
