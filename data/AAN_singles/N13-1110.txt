Proceedings of NAACL-HLT 2013, pages 897?906,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSame Referent, Different Words:Unsupervised Mining of Opaque Coreferent MentionsMarta Recasens*, Matthew Can?, and Dan Jurafsky**Linguistics Department, Stanford University, Stanford, CA 94305?Computer Science Department, Stanford University, Stanford, CA 94305recasens@google.com, {mattcan,jurafsky}@stanford.eduAbstractCoreference resolution systems rely heav-ily on string overlap (e.g., Google Inc. andGoogle), performing badly on mentions withvery different words (opaque mentions) likeGoogle and the search giant.
Yet prior at-tempts to resolve opaque pairs using ontolo-gies or distributional semantics hurt precisionmore than improved recall.
We present a newunsupervised method for mining opaque pairs.Our intuition is to restrict distributional se-mantics to articles about the same event, thuspromoting referential match.
Using an En-glish comparable corpus of tech news, we builta dictionary of opaque coreferent mentions(only 3% are in WordNet).
Our dictionary canbe integrated into any coreference system (itincreases the performance of a state-of-the-artsystem by 1% F1 on all measures) and is eas-ily extendable by using news aggregators.1 IntroductionRepetition is one of the most common coreferentialdevices in written text, making string-match featuresimportant to all coreference resolution systems.
Infact, the scores achieved by just head match and arudimentary form of pronominal resolution1 are notfar from that of state-of-the-art systems (Recasensand Hovy, 2010).
This suggests that opaque men-tions (i.e., lexically different) such as iPad and theCupertino slate are a serious problem for modernsystems: they comprise 65% of the non-pronominal1Closest NP with the same gender and number.errors made by the Stanford system on the CoNLL-2011 data.
Solving this problem is critical for over-coming the recall gap of state-of-the-art systems(Haghighi and Klein, 2010; Stoyanov et al 2009).Previous systems have turned either to ontologies(Ponzetto and Strube, 2006; Uryupina et al 2011;Rahman and Ng, 2011) or distributional semantics(Yang and Su, 2007; Kobdani et al 2011; Bansaland Klein, 2012) to help solve these errors.
But nei-ther semantic similarity nor hypernymy are the sameas coreference: Microsoft and Google are distribu-tionally similar but not coreferent; people is a hy-pernym of both voters and scientists, but the peo-ple can corefer with the voters, but is less likelyto corefer with the scientists.
Thus ontologies leadto precision problems, and to recall problems likemissing NE descriptions (e.g., Apple and the iPhonemaker) and metonymies (e.g., agreement and word-ing), while distributional systems lead to precisionproblems like coreferring Microsoft and the Moun-tain View giant because of their similar vector rep-resentation (release, software, update).We increase precision by drawing on the intuitionthat referents that are both similar and participate inthe same event are likely to corefer.
We restrict dis-tributional similarity to collections of articles thatdiscuss the same event.
In the following two doc-uments on the Nexus One from different sources,we take the subjects of the identical verb release?Google and the Mountain View giant?as coreferent.Document 1: Google has released a software update.Document 2: The Mountain View giant released an update.Based on this idea, we introduce a new unsuper-vised method that uses verbs in comparable corpora897as pivots for extracting the hard cases of corefer-ence resolution, and build a dictionary of opaquecoreferent mentions (i.e., the dictionary entries arepairs of mentions).
This dictionary is then inte-grated into the Stanford coreference system (Lee etal., 2011), resulting in an average 1% improvementin the F1 score of all the evaluation measures.Our work points out the importance of context todecide whether a specific mention pair is coreferent.On the one hand, we need to know what semanticrelations are potentially coreferent (e.g., content andvideo).
On the other, we need to distinguish contextsthat are compatible for coreference?
(1) and (2-a)?from those that are not?
(1) and (2-b).
(1) Elemental helps those big media entities processcontent across a full slate of mobile devices.
(2) a. Elemental provides the picks and shovels tomake video work across multiple devices.b.
Elemental is powering the video for HBO Go.Our dictionary of opaque coreferent pairs is our so-lution to the first problem, and we report on somepreliminary work on context compatibility to ad-dress the second problem.2 Building a Dictionary for CoreferenceTo build a dictionary of semantic relations that areappropriate for coreference we will use a cluster ofdocuments about the same news event, which wecall a story.
Consider as an example the story Sprintblocks out vacation days for employees.
We deter-mine using tf-idf the representative verbs for thisstory, the main actions and events of the story (e.g.,block out).
Since these verbs are representative ofthe story, different instances across documents in thecluster are likely to refer to the same events (Sprintblocks out.
.
.
and the carrier blocks out.
.
.
).
By thesame logic, the subjects and objects of the verbs arealso likely to be coreferent (Sprint and the carrier).2.1 Comparable corpusTo build our dictionary, we require a monolingualcomparable corpus, containing clusters of docu-ments from different sources that discuss the samestory.
To ensure likely coreference, the story mustbe the very same; documents that are merely clus-tered by (general) topic do not suffice.
The corpusdoes not need to be parallel in the sense that docu-ments in the same cluster do not need to be sentencealigned.We used Techmeme,2 a news aggregator for tech-nology news, to construct a comparable corpus.
Itswebsite lists the major tech stories, each with linksto several articles from different sources.
We usedthe Readability API3 to download and extract the ar-ticle text for each document.
We scraped two yearsworth of data from Techmeme and only took storiescontaining at least 5 documents.
Our corpus con-tains approximately 160 million words, 25k stories,and 375k documents.
Using a corpus from Tech-meme means that our current coreference dictionaryis focused on the technological domain.
Our methodcan be easily extended to other domains, however,since getting comparable corpora is relatively sim-ple from the many similar news aggregator sites.2.2 ExtractionAfter building our corpus, we used Stanford?sCoreNLP tools4 to tokenize the text and annotate itwith POS tags and named entity types.
We parsedthe text using the MaltParser 1.7, a linear time de-pendency parser (Nivre et al 2004).5We then extracted the representative verbs of eachstory by ranking the verbs in each story accordingto their tf-idf scores.
We took the top ten to be therepresentative set.
For each of these verbs, we clus-tered together its subjects and objects (separately)across instances of the verb in the document clus-ter, excluding pronouns and NPs headed by the samenoun.
For example, suppose that crawl is a represen-tative verb and that in one document we have Googlecrawls web pages and The search giant crawls sitesin another document.
We will create the clusters{Google, the search giant} and {web pages, sites}.When detecting representative verbs, we keptphrasal verbs as a unit (e.g., give up) and excludedauxiliary and copular verbs,6 light verbs,7 and report2http://www.techmeme.com3http://www.readability.com/developers/api4http://nlp.stanford.edu/software/corenlp.shtml5http://www.maltparser.org6Auxiliary and copular verbs include appear, be, become,do, have, seem.7Light verbs include do, get, give, go, have, keep, make, put,set, take.898verbs,8 as they are rarely representative of a storyand tend to add noise to our dictionary.
To increaserecall, we also considered the synonyms from Word-Net and nominalizations from NomBank of the rep-resentative verbs, thus clustering together the sub-jects and objects of any synonym as well as the ar-guments of nominalizations.9 We used syntactic re-lations instead of semantic roles because the Malt-Parser is faster than any SRL system, but we checkedfor frequent syntactic structures in which the agentand patient are inverted, such as passive and ergativeconstructions.10From each cluster of subject or object mentions,we generated all pairs of mentions.
This forms theinitial version of our dictionary.
The next sectionsdescribe how we filter and generalize these pairs.2.3 FilteringWe manually analyzed 200 random pairs and clas-sified them into coreference and spurious relations.The spurious relations were caused by errors due tothe parser, the text extraction, and violations of ouralgorithm assumption (i.e., the representative verbdoes not refer to a unique event).
We employed a fil-tering strategy to improve the precision of the dictio-nary.
We used a total of thirteen simple rules, whichare shown in Table 1.
For instance, we sometimesget the same verb with non-coreferent arguments,especially in tech news that compare companies orproducts.
In these cases, NEs are often used, and sowe can get rid of a large number of errors by auto-matically removing pairs in which both mentions areNEs (e.g., Google and Samsung).Before filtering, 53% of all relations were goodcoreference relations versus 47% spurious ones.
Ofthe relations that remained after filtering, 74% were8Report verbs include argue, claim, say, suggest, tell, etc.9As a general rule, we extract possessive phrases as subjects(e.g.
Samsung?s plan) and of -phrases as objects (e.g.
develop-ment of the new logo).10We can easily detect passive subjects (i-b) as they have theirown dependency label, and ergative subjects (ii-b) using a listof ergative verbs extracted from Levin (1993).
(i) a.
Developers hacked the device.b.
The device was hacked.
(ii) a.
Police scattered the crowds.b.
The crowds scattered.Both mentions are NEsBoth mentions appear in the same documentObject of a negated verbEnumeration or list environmentSentence is ill-formedNumber NETemporal NEQuantifying nounCoordinatedVerb is preceded by a determiner or an adjectiveHead is not nominalSentence length ?
100Mention length ?
70% of sentence lengthTable 1: Filters to improve the dictionary precision.
Un-less otherwise noted, the filter was applied if either men-tion in the relation satisfied the condition.coreferent and only 26% were spurious.
In total,about half of the dictionary relations were removedin the filtering process, resulting in a total of 128,492coreferent pairs.2.4 GeneralizationThe final step of generating our dictionary is to pro-cess the opaque mention pairs so that they gener-alize better.
We strip mentions of any determiners,relative clauses, and -ing and -ed clauses.
However,we retain adjectives and prepositional modifiers be-cause they are sometimes necessary for corefer-ence to hold (e.g., online piracy and distributionof pirated material).
We also generalize NEs totheir types so that our dictionary entries can func-tion as templates (e.g., Cook?s departure becomes<person>?s departure), but we keep NE tokens thatare in the head position as these are pairs containingworld knowledge (e.g., iPad and slate).
Finally, wereplace all tokens with their lemmas.
Table 2 showsa snapshot of the dictionary.2.5 Semantics of coreferenceFrom manually classifying a sample of 200 dictio-nary pairs (e.g., Table 2), we find that our dictio-nary includes many synonymy (e.g., IPO and offer-ing) and hypernymy relations (e.g., phone and de-vice), which are the relations that are typically ex-tracted from ontologies for coreference resolution.However, not all synonyms and hypernyms are validfor coreference (recall the voters-people vs. scien-tists-people example in the introduction), so our dic-899Mention 1 Mention 2offering IPOuser consumerphone deviceApple companyhardware key digital lockiPad slatecontent photobug issuepassword login informationGoogle search giantsite companyfiling complaintcompany governmentTouchPad tabletmedical record medical fileversion handsetinformation credit cardgovernment chairmanapp softwareAndroid platformthe leadership change <person>?s departurechange updateTable 2: Coreference relations in our dictionary.tionary only includes the ones that are relevant forcoreference (e.g., update and change).
Furthermore,only 3% of our 128,492 opaque pairs are related inWordNet, confirming that our method is introducinga large number of new semantic relations.We also discover other semantic relations that arerelevant for coreference, such as various metonymyrelations like mentioning the part for the whole.Again though, we can use some part-whole rela-tions coreferentially (e.g., car and engine) but notothers (e.g., car and window).
Our dictionary in-cludes part-whole relations that have been observedas coreferent at least once (e.g., company and site).We also extract world-knowledge descriptions forNEs (e.g., Google and the Internet giant).3 Integration into a Coreference SystemWe next integrated our dictionary into an existingcoreference resolution system to see if it improvesresolution.3.1 Stanford coreference resolution systemOur baseline is the Stanford coreference resolutionsystem (Lee et al 2011) which was the highest-scoring system in the CoNLL-2011 Shared Task,Sieve number Sieve name1 Discourse processing2 Exact string match3 Relaxed string match4 Precise constructs5?7 Strict head match8 Proper head noun match9 Relaxed head match10 Pronoun matchTable 3: Rules of the baseline system.and was also part of the highest-scoring system inthe CoNLL-2012 Shared Task (Fernandes et al2012).
It is a rule-based system that includes a to-tal of ten rules (or ?sieves?)
for entity coreference,shown in Table 3.
The sieves are applied from high-est to lowest precision, each rule extending entities(i.e., mention clusters) built by the previous tiers, butnever modifying links previously made.
The major-ity of the sieves rely on string overlap.11The highly modular architecture made it easy forus to integrate additional sieves using our dictionaryto increase recall.3.2 Dictionary sievesWe propose four new sieves, each one using a differ-ent granularity level from our dictionary, with eachconsecutive sieve using higher precision relationsthan the previous one.
The Dict 1 sieve uses onlythe heads of mentions in each relation (e.g., devices).Dict 2 uses the heads and one premodifier, if it ex-ists (e.g., iOS devices).
Dict 3 uses the heads and upto two premodifiers (e.g., new iOS devices).
Dict 4uses the full mentions, including any postmodifiers(e.g., new iOS devices for businesses).We take advantage of frequency counts to get ridof low-precision coreference pairs and only keep(i) pairs that have been seen more than 75 times(Dict 1) or 15 times (Dict 2, Dict 3, Dict 4);and (ii) pairs with a frequency count larger than 8(Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normal-ized PMI score larger than 0.18.
We use the nor-malized PMI score (Bouma, 2009) as a measure ofassociation between the mentions mi and mj of a11Exceptions: sieve 1 links first-person pronouns inside aquotation with the speaker; sieve 4 links mention pairs that ap-pear in an appositive, copular, acronym, etc., construction; sieve10 implements generic pronominal coreference resolution.900dictionary pair, computed as(ln p(mi,mj)p(mi)p(mj)) /?
ln p(mi,mj)These thresholds were set on the development set.Since the different coreference rules in the Stan-ford system are arranged in decreasing order of pre-cision, we start by applying the sieve that uses thehighest-precision relations in the dictionary (Dict 4),followed by Dict 3, Dict 2, and Dict 1.
We addthese new sieves right before the last sieve, as thepronominal sieve can perform better if opaque men-tions have been successfully linked.
The currentsieves only use the dictionary for linking singularmentions, as the experiments on the dev showed thatplural mentions brought too much noise.For any mention pair under analysis, each sievechecks whether it is supported by the dictionary aswell as whether basic constraints are satisfied, suchas number, animacy and NE-type agreement, andNE?common noun order (not the opposite).4 Experiments4.1 DataAlthough our dictionary creation technology can ap-ply across domains, our current coreference dictio-nary is focused on the technical domain, so we cre-ated a coreference labeled corpus in this domain forevaluation.
We extracted new data from Techmeme(different from that used to extract the dictionary) tocreate a development and a test set.
It is importantto note that we do not need comparable data at thisstage.
A massive comparable corpus is only neededfor mining the coreference dictionary (Section 2);once it is built, it can be used for solving corefer-ence within and across documents.The annotation was performed by two experts, us-ing the Callisto annotation tool.
The developmentand test sets were annotated with coreference rela-tions following the OntoNotes guidelines (Pradhanet al 2007).
We annotated full NPs (with all mod-ifiers), excluding appositive phrases and predicatenominals.
Only premodifiers that were proper nounsor possessive phrases were annotated.
We extendedthe OntoNotes guidelines by also annotating single-tons.
Table 4 shows the dataset statistics.Dataset Stories Docs Tokens Entities MentionsDev 4 27 7837 1360 2279Test 24 24 8547 1341 2452Table 4: Dataset statistics: development (dev) and test.4.2 Evaluation measuresWe evaluated using six coreference measures, asthey sometimes provide different results and there isno agreement on a standard.
We used the scorer ofthe CoNLL-2011 Shared Task (Pradhan et al 2011).?
MUC (Vilain et al 1995).
Link-based metricthat measures how many links the true and sys-tem partitions have in common.?
B3 (Bagga and Baldwin, 1998).
Mention-basedmetric that measures the proportion of mentionoverlap between gold and predicted entities.?
CEAF-?3 (Luo, 2005).
Mention-based metricthat, unlike B3, enforces a one-to-one align-ment between gold and predicted entities.?
CEAF-?4 (Luo, 2005).
The entity-based ver-sion of the above metric.?
BLANC (Recasens and Hovy, 2011).
Link-based metric that considers both coreferenceand non-coreference links.?
CoNLL (Denis and Baldridge, 2009).
Averageof MUC, B3 and CEAF-?4.
It was the officialmetric of the CoNLL-2011 Shared Task.4.3 ResultsWe always start from the baseline, which corre-sponds to the Stanford system with the sieves listedin Table 3.
This is the set of sieves that won theCoNLL-2011 Shared Task (Pradhan et al 2011),and they exclude WordNet.Table 5 shows the incremental scores, on the de-velopment set, for the four sieves that use the dictio-nary, corresponding to the different granularity lev-els, from the highest precision one (Dict 4) to thelowest one (Dict 1).
The largest improvement isachieved by Dict 4 and Dict 3, as they improve re-call (R) without hurting precision (P).
R is equiva-lent to P for CEAF-?4, and vice versa.
The othertwo sieves increase R further, especially Dict 1,but also decrease P, although the trade-off for theF-score (F1) is still positive.
It is the best score, withthe exception of B3.901MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLLSystem R P F1 R P F1 R / P / F1 R P F1 R P B F1Baseline 55.9 72.8 63.3 74.1 89.8 81.2 74.6 85.2 73.6 79.0 66.6 87.1 72.6 74.5+Dict 4 57.0 72.8 63.9 75.1 89.4 81.6 75.3 85.2 74.3 79.4 68.2 87.3 74.2 75.0+Dict 3 57.6 72.8 64.3 75.4 89.3 81.7 75.5 85.1 74.6 79.5 68.4 87.2 74.4 75.2+Dict 2 57.6 72.5 64.2 75.4 89.1 81.7 75.4 85.0 74.6 79.5 68.4 87.0 74.3 75.1+Dict 1 58.4 71.9 64.5 75.7 88.5 81.6 75.5 84.6 75.1 79.6 68.6 86.6 74.4 75.2Table 5: Incremental results for the four sieves using our dictionary on the development set.
Baseline is the Stanfordsystem without the WordNet sieves.
Scores are on gold mentions.MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLLSystem R P F1 R P F1 R / P / F1 R P F1 R P B F1Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3*w/ Dict +Context64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5*Table 6: Performance on the test set.
Scores are on gold mentions.
Stars indicate a statistically significant differencewith respect to the baseline.Table 6 reports the scores on the test set and com-pares the scores obtained by adding the WordNetsieves to the baseline (w/ WN) with those obtainedby adding the dictionary sieves (w/ Dict).
Whereasadding WordNet only brings a small improvementin R that is much lower than the loss in P, the dic-tionary sieves succeed in increasing R by a largeramount and at a smaller cost to P, resulting in a sig-nificant improvement in F1: 1.2 points according toMUC, 0.8 points according to B3, 1.4 points accord-ing to CEAF-?3, 0.4 points according to CEAF-?4,2.6 points according to BLANC, and 0.7 points ac-cording to CoNLL.
Section 5.2 presents the last line(w/ Dict + Context).5 Discussion5.1 Error analysisThanks to the dictionary, the coreference system im-proves the baseline by establishing coreference linksbetween the bolded mentions in (3) and (4).
(3) With Groupon Inc.?s stock down by half from its IPOprice and the company heading into its first earningsreport since an accounting blowup [...] outlining op-portunity ahead and the promise of new products forthe daily-deals company.
(4) Thompson revealed the diagnosis as evidence arosethat seemed to contradict his story about why he wasnot responsible for a degree listed on his resume thathe does not have, the newspaper reports, citing anony-mous sources familiar with the situation [...] a Yahooboard committee appointed to investigate the matter.The first case requires world knowledge and the sec-ond case, semantic knowledge.We manually analyzed 40 false positive errorscaused by the dictionary sieves.
Only a small num-ber of them were due to noise in the dictionary.
Themajority of errors were due to the discourse context:the two mentions could be coreferent, but not in thegiven context.
For example, Apple and company arepotentially coreferent?which is successfully cap-tured by our dictionary?and while they are coref-erent in (5), they are not in (6).12(5) It will only get better as Apple will be updating itwith iOS6, an operating system that the company willlikely be showing off this summer.
(6) Since Apple reinvented the segment, Microsoft is thelatest entrant into the tablet market, banking on itsWindows 8 products to bridge the gap between PCsand tablets.
[...] The company showed off Windows 8last September.12Examples in this section show gold coreference relations inbold and incorrectly predicted coreferent mentions in italics.902In these cases it does not suffice to check whetherthe opaque mention pair is included in the corefer-ence dictionary, but we need a method for taking thesurrounding context into account.
In the next sectionwe present our preliminary work in this direction.5.2 Context fitTo help the coreference system choose the right an-tecedent in examples like (6), we exploit the factthat the company is closely followed by Windows 8,which is a clue for selecting Microsoft instead of Ap-ple as the antecedent.
We devise a contextual con-straint that rules out a mention pair if the contexts areincompatible.
To check for context compatibility,we borrow the idea of topic signatures from Lin andHovy (2000) and that Agirre et al(2001) used forWord Sense Disambiguation.
Instead of identifyingthe keywords of a topic, we find the NEs that tendto co-occur with another NE.
For example, the sig-nature for Apple should include terms like iPhone,MacBook, iOS, Steve Jobs, etc.
This is what we callthe NE signature for Apple.To construct NE signatures, we first compute thelog-likelihood ratio (LLR) statistic between NEs inour corpus (the same one used to build the dictio-nary).
Then, the signature for a NE, w, is the list ofk other NEs that have the highest LLR with w. TheLLR between two NEs, w1 and w2, is ?2 lnL(H1)L(H2),where H1 is the hypothesis thatP (w1 ?
sent|w2 ?
sent) = P (w1 ?
sent|w2 /?
sent),H2 is the hypothesis thatP (w1 ?
sent|w2 ?
sent) 6= P (w1 ?
sent|w2 /?
sent),and L(?)
is the likelihood.
We assume a binomialdistribution for the likelihood.Once we have NE signatures, we determine thecontext fit as follows.
When the system compares aNE antecedent with a (non-NE) anaphor, we checkwhether any NEs in the anaphor?s sentence are inthe antecedent?s signature.
We also check whetherthe antecedent is in the signature list of any NE?s inthe anaphor?s sentence.
If neither of these is true,we do not allow the system to link the antecedentand the anaphor.
In (6), Apple is not linked with thecompany because it is not in Windows?
signature,and Windows is not in Apple?s signature either (butMicrosoft is in Windows?
signature).The last two lines in Table 6 compare the scoreswithout using this contextual feature (w/ Dict) withthose using context (w/ Dict + Context).
Our featurefor context compatibility leads to a small but posi-tive improvement, taking the final improvement ofthe dictionary sieves to be about 1 percentage pointabove the baseline according to all six evaluationmeasures.
We leave as future work to test this ideaon a larger test set and refine it further so as to ad-dress more challenging cases where comparing NEsis not enough, like in (7).
(7) Snapchat will notify users [...] The program is avail-able for free in Apple?s App Store [...] While the com-pany ?attempts to delete image data as soon as possi-ble after the message is transmitted,?
it cannot guaran-tee messages will always be deleted.To resolve (7), it would be helpful to know thatSnapchat is a picture messaging platform, as thecontext mentions image data and messages.6 Related WorkExisting ontologies are not optimal for solvingopaque coreferent mentions because of both a preci-sion and a recall problem (Lee et al 2011; Uryupinaet al 2011).
On the other hand, using data-drivenmethods such as distributional semantics for coref-erence resolution suffers especially from a precisionproblem (Ng, 2007).
Our work combines ideas fromdistributional semantics and paraphrase acquisitionmethods in order to efficiently use contextual infor-mation to extract coreference relations.The main idea that we borrow from paraphraseacquisition is the use of monolingual (non-parallel)comparable corpora, which have been exploitedto extract both sentence-level (Barzilay and McK-eown, 2001) and sub-sentential-level paraphrases(Shinyama and Sekine, 2003; Wang and Callison-Burch, 2011).
To ensure that the NPs are coreferent,we limit the meaning of comparable corpora to col-lections of documents that report on the very samestory, as opposed to collections of documents thatare about the same (general) topic.
However, thedistinguishing factor is that while most paraphrasingstudies, including Lin and Pantel (2001), use NEs?or nouns in general?as pivots to learn paraphrasesof their surrounding context, we use verbs as pivotsto learn coreference relations at the NP level.There are many similarities between paraphraseand coreference, and our work is most similar to903that by Wang and Callison-Burch (2011).
However,some paraphrases that might not be considered tobe valid (e.g., under $200 and around $200) canbe acceptable coreference relations.
Unlike Wangand Callison-Burch (2011), we do not work on doc-ument pairs but on sets of at least five (comparable)documents, and we do not require sentence align-ment, but just verb alignment.Another source of inspiration is the work by Beanand Riloff (2004).
They use contextual roles (i.e.,the role that an NP plays in an event) for extract-ing patterns that can be used in coreference reso-lution, showing the relevance of verbs in decidingon coreference between their arguments.
However,they use a very small corpus (two domains) and donot aim to build a dictionary.
The idea of creatinga repository of extracted concept-instance relationsappears in Fleischman et al(2003), but restrictedto person-role pairs, e.g.
Yasser Arafat and leader.Although it was originally designed for answeringwho-is questions, Daume?
III and Marcu (2005) suc-cessfully used it for coreference resolution.The coreference relations that we extract mightoverlap but go beyond those detected by Bansal andKlein (2012)?s Web-based features.
First, they focuson NP headwords, while we extract full NPs, includ-ing multi-word mentions.
Second, the fact that theyuse the Google n-gram corpus means that the twoheadwords must appear at most four words apart,thus ruling out coreferent mentions that can only ap-pear far from each other.
Finally, while their extrac-tion patterns focus on synonymy and hypernymy re-lations, we discover other types of semantic relationsthat are relevant for coreference (Section 2.5).7 ConclusionsWe have pointed out an important problem with cur-rent coreference resolution systems: their heavy re-liance on string overlap.
Pronouns aside, opaquementions account for 65% of the errors made bystate-of-the-art systems.
To improve coreferencescores beyond 60-70%, we therefore need to makebetter use of semantic and world knowledge to dealwith non-identical-string coreference.
But, as wehave also shown, coreference is not the same as se-mantic similarity or hypernymy.
Only certain se-mantic relations in certain contexts are good cues forcoreference.
We therefore need semantic resourcesspecifically targeted at coreference.We proposed a new solution for detecting opaquemention pairs: restricting distributional similarity toa comparable corpus of articles about the very samestory, thus ensuring that similar mentions will alsolikely be coreferent.
We used this corpus to build adictionary focused on coreference, and successfullyextracted the specific semantic and world knowledgerelevant for coreference.
The resulting dictionarycan be added on top of any coreference system toincrease recall at a minimum cost to precision.
Inte-grated into the Stanford coreference resolution sys-tem, which won the CoNLL-2011 shared task, theF-score increases about 1 percentage point accord-ing to all of the six evaluation measures.
The dictio-nary and NE signatures are available on the Web.13We showed that apart from the need for extractingcoreference-specific semantic and world knowledge,we need to take into account the context surroundingthe mentions.
The results from our preliminary workfor identifying incompatible contexts is promising.Our unsupervised method for extracting opaquecoreference relations can be easily extended to otherdomains by using online news aggregators, andtrained on more data to build a more comprehensivedictionary that can increase recall even further.
Weintegrated the dictionary into a rule-based corefer-ence system, but it remains for future work to in-tegrate it into a learning-based architecture, wherethe system can combine the dictionary features withother features.
This can also make it easier to in-clude contextual features that take into account howwell a dictionary pair fits in a specific context.AcknowledgmentsWe would like to thank the members of the StanfordNLP Group, Valentin Spitkovsky, and Ed Hovy forvaluable comments at various stages of the project.The first author was supported by a Beatriu dePino?s postdoctoral scholarship (2010 BP-A 00149)from Generalitat de Catalunya.
We also gratefullyacknowledge the support of Defense Advanced Re-search Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0181.13http://nlp.stanford.edu/pubs/coref-dictionary.zip904ReferencesEneko Agirre, Olatz Ansa, David Martinez, and EduardHovy.
2001.
Enriching wordnet concepts with topicsignatures.
In Proceedings of the NAACLWorkshop onWordNet and Other Lexical Resources: Applications,Extensions and Customizations, pages 23?28.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings ofthe LREC 1998 Workshop on Linguistic Coreference,pages 563?566.Mohit Bansal and Dan Klein.
2012.
Coreference seman-tics from web features.
In Proceedings of ACL, pages389?398.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof ACL, pages 50?57.David Bean and Ellen Riloff.
2004.
Unsupervised learn-ing of contextual role knowledge for coreference reso-lution.
In Proceedings of NAACL-HTL.Geolof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
In Proceedingsof the Biennial GSCL Conference, pages 31?40.Hal Daume?
III and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In Proceedings ofHLT-EMNLP, pages 97?104.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassification.
Procesamiento del Lenguaje Natural,42:87?96.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidiu?.2012.
Latent structure perceptron with feature induc-tion for unrestricted coreference resolution.
In Pro-ceedings of CoNLL - Shared Task, pages 41?48.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online questionanswering: answering questions before they are asked.In Proceedings of ACL, pages 1?7.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In Proceed-ings of HLT-NAACL, pages 385?393.Hamidreza Kobdani, Hinrich Schu?tze, MichaelSchiehlen, and Hans Kamp.
2011.
Bootstrap-ping coreference resolution using word associations.In Proceedings of ACL, pages 783?792.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 Shared Task.
In Proceedingsof CoNLL - Shared Task, pages 28?34.Beth Levin.
1993.
English Verb Class and Alternations:A Preliminary Investigation.
University of ChicagoPress, Chicago.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summarization.In Proceedings of COLING, pages 495?501.Dekang Lin and Patrick Pantel.
2001.
DIRT - Discov-ery of inference rules from text.
In Proceedings of theACM SIGKDD, pages 323?328.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of HLT-EMNLP, pages25?32.Vincent Ng.
2007.
Shallow semantics for coreferenceresolution.
In Proceedings of IJCAI, pages 1689?1694.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof CoNLL, pages 49?56.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof HLT-NAACL, pages 192?199.Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Identifying entities and events inOntoNotes.
In Proceedings of ICSC, pages 446?453.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofCoNLL - Shared Task, pages 1?27.Altaf Rahman and Vincent Ng.
2011.
Coreference reso-lution with world knowledge.
In Proceedings of ACL,pages 814?824.Marta Recasens and Eduard Hovy.
2010.
Corefer-ence resolution across corpora: Languages, codingschemes, and preprocessing information.
In Proceed-ings of ACL, pages 1423?1432.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand index for coreference evaluation.Natural Language Engineering, 17(4):485?510.Yusuke Shinyama and Satoshi Sekine.
2003.
Paraphraseacquisition for information extraction.
In Proceedingsof ACL, pages 65?71.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.In Proceedings of ACL-IJCNLP, pages 656?664.Olga Uryupina, Massimo Poesio, Claudio Giuliano, andKateryna Tymoshenko.
2011.
Disambiguation andfiltering methods in using web knowledge for coref-erence resolution.
In Proceedings of FLAIRS, pages317?322.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6, pages 45?52.905Rui Wang and Chris Callison-Burch.
2011.
Para-phrase fragment extraction from monolingual compa-rable corpora.
In Proceedings of the 4th ACL Work-shop on Building and Using Comparable Corpora,pages 52?60.Xiaofeng Yang and Jian Su.
2007.
Coreference resolu-tion using semantic relatedness information from auto-matically discovered patterns.
In Proceedings of ACL,pages 528?535.906
