Analyzing Dependencies of Japanese Subordinate Clausesbased on Statistics of Scope Embedding PreferenceTakeh i to  Utsuro ,  Sh igeyuk i  N ish iokayama,  Masakazu  Fu j io ,  Yu j i  MatsumotoGraduate  Schoo l  of  In fo rmat ion  Sc ience,  Nara  Ins t i tu te  of  Sc ience and  Techno logy8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPANE-mail: utsuro @is.
aist-nara, ac.jp, URL: http://cl, aist-nara, ac.jp/-utsuro/Abst ractThis paper proposes a statistical method forlearning dependency preference of Japanesesubordinate clauses, in which scope embeddingpreference of subordinate clauses is exploitedas a useful information source for disambiguat-ing dependencies between subordinate clauses.Estimated dependencies of subordinate clausessuccessfully increase the precision of an existingstatistical dependency analyzer.1 In t roduct ionIn the Japanese language, since word order in asentence is relatively free compared with Euro-pean languages, dependency analysis has beenshown to be practical and effective in both rule-based and stochastic approaches to syntacticanalysis.
In dependency analysis of a Japanesesentence, among various source of ambiguitiesin a sentence, dependency ambiguities of sub-ordinate clauses are one of the most problem-atic ones, partly because word order in a sen-tence is relatively free.
In general, dependencyambiguities of subordinate clauses cause scopeambiguities of subordinate clauses, which resultin enormous number of syntactic ambiguities ofother types of phrases such as noun phrases.
11In our preliminary corpus analysis using the stochas-tic dependency analyzer of Fujio and Matsumoto (1998),about 30% of the 210,000 sentences in EDR bracketedcorpus (EDR, 1995) have dependency ambiguities of sub-ordinate clauses, for which the precision of chunk (bun-setsu) level dependencies is about 85.3% and that of sen-tence level is about 25.4% (for best one) ~ 35.8% (forbest five), while for the rest 70% of EDR bracketed cor-pus, the precision of chunk (bunsetsu) level dependenciesis about 86.7% and that of sentence l vel is about 47.5%(for best one) ~ 60.2% (for best five).
In addition tothat, when assuming that those ambiguities of subor-dinate clause dependencies are initially resolved in someway, the chunk level precision increases to 90.4%, and thesentence l vel precision to 40.6% (for best one) ~ 67.7%(for best five).
This result of our preliminary analysis110In the Japanese linguistics, a theory of Mi-nami (1974) regarding scope embedding pref-erence of subordinate clauses is well-known.Minami (1974) classifies Japanese subordinateclauses according to the breadths of their scopesand claim that subordinate clauses which inher-ently have narrower scopes are embedded withinthe scopes of subordinate clauses which inher-ently have broader scopes (details are in sec-tion 2).
By manually analyzing several raw cor-pora, Minami (1974) classifies various types ofJapanese subordinate clauses into three cate-gories, which are totally ordered by the embed-ding relation of their scopes.
In the Japanesecomputational linguistics community, Shirai etal.
(1995) employed Minami (1974)'s theory onscope embedding preference of Japanese sub-ordinate clauses and applied it to rule-basedJapanese dependency analysis.
However, intheir approach, since categories of subordinateclauses are obtained by manually analyzinga small number of sentences, their coverageagainst a large corpus such as EDR bracketedcorpus (EDR, 1995) is quite low.
2In order to realize a broad coverage and highperformance dependency analysis of Japanesesentences which exploits scope embedding pref-erence of subordinate clauses, we propose acorpus-based and statistical alternative to therule-based manual approach (section 3).
3clearly shows that dependency ambiguities of subordi-nate clauses are among the most problematic source ofsyntactic ambiguities in a Japanese sentence.2In our implementation, the coverage of the categoriesof Shirai et al (1995) is only 30% for all the subordinateclauses included in the whole EDR corpus.~Previous works on statistical dependency analysis in-clude Fujio and Matsumoto (1998) and Haruno et al(1998) in Japanese analysis as well as Lafferty et al(1992), Eisner (1996), and Collins (1996) in English anal-ysis.
In later sections, we discuss the advantages of ourapproach over several closely related previous works.Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese SentenceWord SegmentationPOS (+ conjugation form)TaggingBunsetsu SegmentationTenki ga yoi kara dekakeyounoun case- adjective predicate- verbparticle (base) conjunctive-particle (volitional)Tenki-ga yoi-kara dekakeyou(Chunking)English Translation weather subject fine because let's go out(Because the weather is fine, let's go out.
)First, we formalize the problem of decid-ing scope embedding preference as a classifi-cation problem, in which various types of lin-guistic information of each subordinate clauseare encoded as features and used for decidingwhich one of given two subordinate clauses hasa broader scope than the other.
As in the case ofShirai et al (1995), we formalize the problem ofdeciding dependency preference of subordinateclauses by utilizing the correlation of scope em-bedding preference and dependency preferenceof Japanese subordinate clauses.
Then, as a sta-tistical earning method, we employ the decisionlist learning method of Yarowsky (1994), whereoptimal combination of those features are se-lected and sorted in the form of decision rules,according to the strength of correlation betweenthose features and the dependency preferenceof the two subordinate clauses.
We evaluatethe proposed method through the experimenton learning dependency preference of Japanesesubordinate clauses from the EDR bracketedcorpus (section 4).
We show that the pro-posed method outperforms other related meth-ods/models.
We also evaluate the estimated e-pendencies of subordinate clauses in Fujio andMatsumoto (1998)'s framework of the statisti-cal dependency analysis of a whole sentence, inwhich we successfully increase the precisions ofboth chunk level and sentence level dependen-cies thanks to the estimated ependencies ofsubordinate clauses.2 Ana lyz ing  Dependenc ies  betweenJapanese  Subord inate  C lausesbased  on  Scope  Embedd ingPre ference2.1 Dependency  Analys is  of  AJapanese SentenceFirst, we overview dependency analysis of aJapanese sentence.
Since words in a Japanesesentence are not segmented by explicit delim-iters, input sentences are first word segmented,111Phrase StructureScope ofSubordin.~.ff..f.~...( !
(ffenki-ga) (yO~:ra))  \[ (dekakeyou))tDependency (modification) RelationFigure 1: An Example of Japanese SubordinateClause (taken from the Sentence of Table 1)part-of-speech tagged, and then chunked into asequence of segments called bunsetsus.
4 Eachchunk (bunsetsu) generally consists of a set ofcontent words and function words.
Then, de-pendency relations among those chunks are es-timated, where most practical dependency ana-lyzers for the Japanese language usually assumethe following two constraints:1.
Every chunk (bunsetsu) except he last onemodifies only one posterior chunk (bun-setsu).2.
No modification crosses to other modifica-tions in a sentence.Table 1 gives an example of word segmenta-tion, part-of-speech tagging, and bunsetsu seg-mentation (chunking) of a Japanese sentence,where the verb and the adjective are taggedwith their parts-of-speech as well as conjuga-tion forms.
Figure i shows the phrase structure,the bracketing, 5 and the dependency (modifica-tion) relation of the chunks (bunsetsus) withinthe sentence.4Word segmentation and part-of-speech tagging areperformed by the Japanese morphological analyzerChasen (Matsumoto et al, 1997), and chunking is doneby the preprocessor used in Fujio and Matsumoto (1998).5The phrase structure and the bracketing are shownjust for explanation, and we do not consider thembut consider only dependency relations in the analysisthroughout this paper.A Japanese subordinate clause is a clause whose head chunk satisfies the following properties.1.
The(a)(b)2.
The(a)(b)(c)(d)(e)(f)(g)(h)content words part of the chunk (bunsetsu) is one of the following types:A predicate (i.e., a verb or an adjective).nouns and a copula like "Noun1 dearu" (in English, "be Noun1").function words part of the chunk (bunsetsu) is one of the following types:Null.Adverb type such as "Verbl ippou-de" (in English, "(subject) Verb1 ..., on the other hand,").Adverbial noun type such as "Verb1 tame" (in English, "in order to Verb1").FormM noun type such as "Verb1 koto" (in English, gerund "Verbl-ing").Temporal noun type such as "Verb1 mae" (in English, "before (subject) Verb1 ...").A predicate conjunctive particle such as "Verbl ga" (in English, "although (subject) Verbl ...,").A quoting particle such as "Verbl to (iu)" (in English, "(say) that (subject) Verbl ...").
(a),,~(g) followed by topic marking particles and/or sentence-final particles.Figure 2: Definition of Japanese Subordinate Clause2.2 Japanese Subordinate ClauseThe following gives the definition of what we calla "Japanese subordinate clause" throughout thispaper.
A clause in a sentence is represented asa sequence of chunks.
Since the Japanese lan-guage is a head-final language, the clause headis the final chunk in the sequence.
A grammati-cal definition of a Japanese subordinate clause isgiven in Figure 2.
6 For example, the Japanesesentence in Table 1 has one subordinate clause,whose scope is indicated as the shaded rectanglein Figure 1.2.3 Scope Embedd ing  Pre ference  o fSubordinate ClausesWe introduce the concept of Minami (1974)'sclassification of Japanese subordinate clausesby describing the more specific classification byShirai et al (1995).
From 972 newspapersummary sentences, Shirai et al (1995) man-ually extracted 54 clause final function wordsof Japanese subordinate clauses and classifiedthem into the following three categories accord-ing to the embedding relation of their scopes.Category  A: Seven expressions representingsimultaneous occurrences such as "Verb1SThis definition includes adnominal or noun phrasemodifying clauses "Clause1 (NP1)" (in English, rela-tive clauses "(NP1) that Clause1").
Since an adnom-inal clause does not modify any posterior subordinateclauses, but modifies aposterior noun phrase, we regardadnominal clauses only as modifees when considering de-pendencies between subordinate clauses.to-tomoni (Clause2)" and "Verbl nagara(Clause2)".Category  B: 46 expressions representingcause and discontinuity such as "Verb1te (Clause2)" (in English "Verbl and(Clause2)") and "Verb1 node" (in English"because (subject) Verb1 ...,").Category  C: One expression representing in-dependence, "Verb1 ga" (in English, "al-though (subject) Verb1 ...,").The category A has the narrowest scope, whilethe category C has the broadest scope, i.e.,Category A -4 Category B -4 Category Cwhere the relation '-<~ denotes the embeddingrelation of scopes of subordinate clauses.
Then,scope embedding preference of Japanese subor-dinate clauses can be stated as below:Scope  Embedd ing  Pre ference  ofJapanese Subordinate Clauses1.
A subordinate clause can be embedded withinthe scope of another subordinate clause whichinherently has a scope of the same or a broaderbreadth.2.
A subordinate clause can not be embeddedwithin the scope of another subordinate clausewhich inherently has a narrower scope.For example, a subordinate clause of 'CategoryB' can be embedded within the scope of anothersubordinate clause of 'Category B' or 'CategoryC', but not within that of 'Category A'.
Figure 3112(a) Category A -.< Category CScopes  o f  Subord inate  C lausesCategory  Cboi l -  pol i te /past -  scorch- per fec t -po l i te /past -per iod  stir up-wi th  a l though-  comma( A l though ?
bo i led it with st i r r ing it up, it had  got  scorched.
)(b) Category C P- Category AScopes of Subordinate C ~r Categoc ) )boil- polite scorch fear- sbj exist- polite- hot_fwe-over stir_up-with (volitional)-period although- comma( Although there is some fear of  its getting scorched, let's boil it with stirring it up over a hot\]we.
)Figure 3: Examples of Scope Embedding of Japanese Subordinate Clauses(a) gives an example of an anterior Japanesesubordinate clause ( "kakimaze-nagara", Cate-gory A), which is embedded within the scopeof a posterior one with a broader scope ("ni-mashita-ga-,", Category C).
Since the poste-rior subordinate clause inherently has a broaderscope than the anterior, the anterior is embed-ded within the scope of the posterior.
On theother hand, Figure 3 (b) gives an example ofan anterior Japanese subordinate clause ("ari-masu-ga-,', Category C), which is not embed-ded within the scope of a posterior one with anarrower scope ( "kakimaze-nagara", CategoryA).
Since the posterior subordinate clause in-herently has a narrower scope than the anterior,the anterior is not embedded within the scopeof the posterior.2.4 P re ference  of Dependenc iesbetween Subord inate  Clauses basedon Scope Embedd ing  Pre ferenceFollowing the scope embedding preference ofJapanese subordinate clauses proposed by Mi-nami (1974), Shirai et al (1995) applied itto rule-based Japanese dependency analysis,and proposed the following preference of decid-ing dependencies between subordinate clauses.Suppose that a sentence has two subordinateclauses Clausez and Clause2, where the headvp chunk of Clauses precedes that of Clause2.Dependency Preference of JapaneseSubord inate  Clauses1.
The head vp chunk of Clause1 can modify thatof Clause2 if Clause2 inherently has a scope ofthe same or a broader breadth compared withthat of Clause1.2.
The head vp chunk of Clausez can not mod-ify that of Clause2 if Clause2 inherently has anarrower scope compared with that of Clause1.3 Learn ing  Dependency  Pre ferenceof  Japanese  Subord inate  C lausesAs we mentioned in section 1, the rule-basedapproach of Shirai et al (1995) to analyz-ing dependencies of subordinate clauses usingscope embedding preference has serious limi-tation in its coverage against corpora of largesize for practical use.
In order to overcomethe limitation of the rule-based approach, inthis section, we propose a method of learningdependency preference of Japanese subordinateclauses from a bracketed corpus.
We formalizethe problem of deciding scope embedding pref-erence as a classification problem, in which var-ious types of linguistic information of each sub-ordinate clause are encoded as features and usedfor deciding which one of given two subordinateclauses has a broader scope than the other.
Asa statistical learning method, we employ the de-cision list learning method of Yarowsky (1994).113Table 2: Features of Japanese Subordinate ClausesFeature Type # of Feat .
.
.
.
Each Binary FeaturePunctuation 2 with-comma, without-commaGrammatical adverb, adverbial-noun, formal-noun, temporal-noun,(some features have distinction 17 quoting-particle, copula, predicate-conjunctive-particle,of chunk-final/middle) topic-marking-particle, sentence-final-particle12 Conjugation form ofchunk-final conjugative wordLexical (lexicalized forms of'Grammatical' features,with more than9 occurrencesin EDR corpus)235stem, base, mizen, ren'you, rental, conditional,imperative, ta, tari, re, conjecture, volitionaladverb (e.g., ippou-de, irai), adverbial-noun (e.g., tame, baai)topic-marking-particle (e.g., ha, mo), quoting-particle (to),predicate-conjunctive-particle (e.g.,ga, kara),temporal-noun (e.g., ima, shunkan), formal-noun (e.g., koto),copula (dearu), sentence-final-particle (e.g., ka, yo)3.1 The  Task  Def in i t ionConsidering the dependency preference ofJapanese subordinate clauses described in sec-tion 2.4, the following gives the definition of ourtask of deciding the dependency of Japanesesubordinate clauses.
Suppose that a sen-tence has two subordinate clauses Clause1 andClause2, where the head vp chunk of Clauselprecedes that of Clause2.
Then, our task of de-ciding the dependency of Japanese subordinateclauses is to distinguish the following two cases:1.
The head vp chunk of Clausez modifies that ofClause2.2.
The head vp chunk of Clause1 does not modifythat of Clause2, but modifies that of anothersubordinate clause or the matrix clause whichfollows Clause2.Roughly speaking, the first corresponds to thecase where Clause2 inherently has a scope of thesame or a broader breadth compared with thatof Clause1, while the second corresponds to thecase where Clause2 inherently has a narrowerscope compared with that of Clause1.73.2 Dec is ion  L ist  Learn ingA decision list (Yarowsky, 1994) is a sorted listof the decision rules each of which decides thevalue of a decision D given some evidence E.Each decision rule in a decision list is sortedTOur modeling is slightly different from those of otherstandard approaches to statistical dependency analy-sis (Collins, 1996; Fujio and Matsumoto, 1998; Harunoet al, 1998) which simply distinguish the two cases: thecase where dependency relation holds between the giventwo vp chunks or clauses, and the case where dependencyrelation does not hold.
In contrast to those standard ap-proaches, we ignore the case where the head vp chunkof Clause1 modifies that of another subordinate clausewhich precedes Clause2.
This is because we assume thatthis case is more loosely related to the scope mbeddingpreference ofsubordinate clauses.in descending order with respect o some pref-erence value, and rules with higher preferencevalues are applied first when applying the deci-sion list to some new test data.First, let the random variable D represent-ing a decision varies over several possible values,and the random variable E representing someevidence varies over '1' and '0' (where '1' de-notes the presence of the corresponding pieceof evidence, '0' its absence).
Then, given sometraining data in which the correct value of thedecision D is annotated to each instance, theconditional probabilities P(D =x \[ E= 1) of ob-serving the decision D = x under the conditionof the presence of the evidence E (E = 1) arecalculated and the decision list is constructedby the following procedure.1.
For each piece of evidence, calculate the likeli-hood ratio of the conditional probability of a de-cision D = xl (given the presence of that pieceof evidence) to the conditional probability ofthe rest of the decisions D =-,xl:P(D=xl I E=I )l?g2 P(D='~xl \ [E=I )Then, a decision list is constructed with piecesof evidence sorted in descending order with re-spect to their likelihood ratios, s2.
The final line of a decision list is defined as 'adefault', where the likelihood ratio is calculatedas the ratio of the largest marginal probabilityof the decision D = xl to the marginal proba-Syarowsky (1994) discusses several techniques foravoiding the problems which arise when an observedcount is 0.
Among those techniques, we employ the sim-plest one, i.e., adding a small constant c~ (0.1 < c~ <0.25) to the numerator and denominator.
With thismodification, more frequent evidence is preferred whenthere exist several evidences for each of which the con-ditional probability P(D=x \[ E=I)  equals to 1.114(a) An Example Sentence with Chunking, Bracketing, and Dependency RelationsSubordinate ClausesClause2~"i:" .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ar i..,,:..:.,,,,,:i:,,:~:::,i::::i::ti:, :~ :~ f (~/o : ,a , ,~(  Cs..0 (~you~,ai,-taa-toi-)) (k~s~-ga)) (dete-tu u-.)
)Iraise-price10%-~ -although 3%- emphatic_.au?iliay - comma _verb (te-form) -comma involuntary dealer-charge-of case- sbj happen-will/may- ~dod( I f  the tax rate is 10%, the dealers will raise price, but, because it is 3%, there will happen to be the cases that the dealers pay the tax.
)(b) Feature Expression of Head VP Chunk of Subordinate ClausesHead VP Chunk of Subordinate Clause Feature SetSeg 1 : "neage-suru-ga-,"Se92 : "3%-ha-node-,"~-z = ~f with-comma, predicate-conjunctive-particle(chunk-final), kpredicate-conjunctive-particle(chunk-final)-"ga" }~'2 = I with-comma, chunk-final-conjugative-word-te-form }(c) Evidence-Decision Pairs for Decision List LearningEvidence E (E= 1) (feature names are abbreviated)Elwith-commaIwith-commare-formwith-comma, te-formwith-comma.
.
.with-comma.
?
.with-comma?
.
.with-comma.
.
.with-commawith-commapred-conj-particle(final).
o ?with-comma, pred-conj-particle(final).
.
.pred-conj-particle(final)- "ga".
?
.with-comma, pred-conj-particle(final)-"ga"?
?
.Decision D"beyond""beyond""beyond""beyond".
.
."beyond""beyond".
.
."beyond"?
.
,Figure 4: An Example of Evidence-Decision Pair of Japanese Subordinate Clausesbility of the rest of the decisions D-=--xz: final conjugative word: used when the chunk-P(D=xl )l?g2 P(D="x l )The 'default' decision of this final line is D-= xzwith the largest marginal probability.3.3 Feature  of  Subord inate  C lausesJapanese subordinate clauses defined in sec-tion 2.2 are encoded using the following fourtypes of features: i) Punctuation: representswhether the head vp chunk of the subordinateclause is marked with a comma or not, ii) Gram-matical: represents parts-of-speech of functionwords of the head vp chunk of the subordi-nate clause, 9 iii) Conjugation form of chunk-9Terms of parts-of-speech tags and conjugation formsare borrowed from those of the Japanese morphologicalana/ysis ystem Chasen (Matsumoto et al, 1997).final word is conjugative, iv) Lexical: lexicalizedforms of 'Grammatical '  features which appearmore than 9 times in EDR corpus.
Each fea-ture of these four types is binary and its valueis '1' or '0' ('1' denotes the presence of the cor-responding feature, '0' its absence).
The wholefeature set shown in Table 2 is designed so as tocover the 210,000 sentences of EDR corpus.3.4 Dec is ion  L ist  Learn ing  ofDependency  Pre ference  ofSubord inate  C lausesFirst, in the modeling of the evidence, we con-sider every possible correlation (i.e., depen-dency) of the features of the subordinate clauseslisted in section 3.3.
Furthermore, since it isnecessary to consider the features for both of thegiven two subordinate clauses, we consider all115the possible combination of features of the an-terior and posterior head vp chunks of the giventwo subordinate clauses.
More specifically, letSeg\] and Seg2 be the head vp chunks of thegiven two subordinate clauses (Segl is the ante-rior and Seg2 is the posterior).
Also let 9Vl and9r2 be the sets of features which Segl and Seg2have, respectively (i.e., the values of these fea-tures are '1').
We consider every possible subsetF1 and F2 of ~-1 and ~2, respectively, and thenmodel the evidence of the decision list learningmethod as any possible pair (F1, F2)3 ?Second, in the modeling of the decision, wedistinguish the two cases of dependency rela-tions described in section 3.1.
We name the firstcase as the decision "modify", while the secondas the decision "beyond".3.5 ExampleFigure 4 illustrates an example of transformingsubordinate clauses into feature xpression, andthen obtaining training pairs of an evidence anda decision from a bracketed sentence.
Figure 4(a) shows an example sentence which containstwo subordinate clauses Clause1 and Clause2,with chunking, bracketing, and dependency re-lations of chunks.
Both of the head vp chunksSegl and Seg2 of Clause1 and Clause2 modifythe sentence-final vp chunk.
As shown in Fig-ure 4 (b), the head vp chunks Segl and Seg2have feature sets ~'1 and ~'2, respectively.
Then,every possible subsets F1 and F2 of ~1 and~2 are considered, n respectively, and trainingpairs of an evidence and a decision are collectedas in Figure 4 (c).
In this case, the value of thedecision D is "beyond", because Segl modifiesthe sentence-final vp chunk, which follows Seg 2.1?Our formalization of the evidence of decision listlearning has an advantage over the decision tree learn-ing (Quinlan, 1993) approach to feature selection of de-pendency analysis (Haruno et al, 1998).
In the featureselection procedure of the decision tree learning method,the utility of each feature is evaluated independently,and thus the utility of the combination of more than onefeatures is not evaluated irectly.
On the other hand, inour formalization of the evidence of decision list learn-ing, we consider every possible pair of the subsets F1 andFz, and thus the util ity of the combination of more thanone features is evaluated irectly.lXSince the feature 'predicate-conjunctive-particle(chunk-final)' subsumes 'predicate-conjunctive-particle(chunk-final)-"ga", they are not consideredtogether as one evidence.i .
\Coverage (Model (b)) .
.
.
.
.
.
.  "
...... \~  \ P~OurModet) --.-- "~.. \ \0.5 0.55 0.6 0.65 0.7 0.75 0.8 0,85 0.9 0.95Lower Bound of P(DIE )Figure 5: Precisions and Coverages of DecidingDependency between Two Subordinate Clauses100 , , ,~  , , ' Our Model , '~_~ Model (a) ......... ......  Model (b) ...........9590ii .
.
.
.
.
.
.
.
~ , ."m.
.8075 .
.
.
.
"i "": .......... '~.
,0 20 40 60 80 100Coverage (%)Figure 6: Correlation of Coverages and Precisions4 Exper iments  and Evaluat ionWe divided the 210,000 sentences of the wholeEDR bracketed Japanese corpus into 95% train-ing sentences and 5~0 test sentences.
Then,we extracted 162,443 pairs of subordinateclauses from the 199,500 training sentences, andlearned a decision list for dependency prefer-ence of subordinate clauses from those pairs.The default decision in the decision list isD ="beyond", where the marginal probabilityP(D = "beyond") = 0.5378, i.e., the baselineprecision of deciding dependency between twosubordinate clauses is 53.78 %.
We limit the fre-quency of each evidence-decision pair to be morethan 9.
The total number of obtained evidence-decision pairs is 7,812.
We evaluate the learneddecision list through several experiments.
12First, we apply the learned decision list todeciding dependency between two subordinateclauses of the 5% test sentences.
We changethe threshold of the probability P(D I E) 13 in12Details of the experimental evaluation will be pre-sented in Utsuro (2000).I~P( D I E) can be used equivalently to the likelihood116the decision list and plot the trade-off betweencoverage and precision.
14 As shown in the plotof "Our Model" in Figure 5, the precision variesfrom 78% to 100% according to the changes ofthe threshold of the probability P(D I E).Next, we compare our model with the othertwo models: (a) the model learned by apply-ing the decision tree learning method of Harunoet al (1998) to our task of deciding depen-dency between two subordinate clauses, and (b)a decision list whose decisions are the followingtwo cases, i.e., the case where dependency rela-tion holds between the given two vp chunks orclauses, and the case where dependency relationdoes not hold.
The model (b) corresponds to amodel in which standard approaches to statis-tical dependency analysis (Collins, 1996; Fujioand Matsumoto, 1998; Haruno et al, 1998) areapplied to our task of deciding dependency be-tween two subordinate clauses.
Their resultsare also in Figures 5 and 6.
Figure 5 shows that"Our Model" outperforms the other two mod-els in coverage.
Figure 6 shows that our modeloutperforms both of the models (a) and (b) incoverage and precision.Finally, we examine whether the estimateddependencies of subordinate clauses improvethe precision of Fujio and Matsumoto (1998)'sstatistical dependency analyzer.
15 Dependingon the threshold of P(D \[ E), we achieve0.8,,~1.8% improvement in chunk level precision,and 1.6~-4.7% improvement in sentence level, is5 Conc lus ionThis paper proposed a statistical method forlearning dependency preference of Japaneseratio.14Coverage: the rate of the pairs of subordinate clauseswhose dependencies are decided by the decision list,against he total pairs of subordinate clauses, Precision:the rate of the pairs of subordinate clauses whose depen-dencies are correctly decided by the decision list, againstthose covered pairs of subordinate clauses.15Fujio and Matsumoto (1998)'s lexicalized depen-dency analyzer is similar to that of Collins (1996), wherevarious features were evaluated through performancetest and an optimal feature set was manually selected.16The upper bounds of the improvement in chunk leveland sentence level precisions, which are estimated byproviding Fujio and Matsumoto (1998)'s tatistical de-pendency analyzer with correct dependencies of subor-dinate clauses extracted from the bracketing of the EDRcorpus, are 5.1% and 15%, respectively.subordinate clauses, in which scope embed-ding preference of subordinate clauses is ex-ploited.
We evaluated the estimated ependen-cies of subordinate clauses through several ex-periments and showed that our model outper-formed other related models.Re ferencesM.
Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings of the34th Annual Meeting of ACL, pages 184-191.EDR (Japan Electronic Dictionary Research Insti-tute, Ltd.).
1995.
EDR Electronic DictionaryTechnical Guide.J.
Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proceed-ings of the 16th COLING, pages 340-345.M.
Fujio and Y. Matsumoto.
1998.
Japanese de-pendency structure analysis based on lexicalizedstatistics.
In Proceedings of the 3rd Conference onEmpirical Methods in Natural Language Process-ing, pages 88--96.M.
Haruno, S. Shirai, and Y. Oyama.
1998.
Us-ing decision trees to construct a practical parser.In Proceedings of the 17th COLING and the 36thAnnual Meeting of ACL, pages 505-511.J.
Lafferty, D. Sleator, and D. Temperley.
1992.Grammatical trigrams: A probabilistic model oflink grammar.
In Proceedings of the AAAI FallSymposium: Probabilistic Approaches to NaturalLanguage, pages 89-97.Y.
Matsumoto, A. Kitauchi, T. Yamashita,O.
Imaichi, and T. Imamura.
1997.
Japanesemorphological nalyzer ChaSen 1.0 users manual.Information Science Technical Report NAIST-IS-TR9?007, Nara Institute of Science and Technol-ogy.
(in Japanese).F.
Minami.
1974.
Gendai Nihongo no Kouzou.Taishuukan Shoten.
(in Japanese).J.
R. Quinlan.
1993.
CJ.5: Programs for MachineLearning.
Morgan Kaufmann.S.
Shirai, S. Ikehara, A. Yokoo, and J. Kimura.
1995.A new dependency analysis method based onsemantically embedded sentence structures andits performance on Japanese subordinate clauses.Transactions of Information Processing Society ofJapan, 36(10):2353-2361.
(in Japanese).T.
Utsuro.
2000.
Learning preference of depen-dency between Japanese subordinate clauses andits evaluation i  parsing.
In Proceedings of the PadInternational Conference on Language Resourcesand Evaluation.
(to appear).D.
Yarowsky.
1994.
Decision lists for lexical ambi-guity resolution: Application to accent restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of A CL, pages 88-95.117
