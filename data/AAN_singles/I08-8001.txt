A Transformation-based Sentence Splitting Method for Statistical Ma-chine TranslationJonghoon Lee, Donghyeon Lee and Gary Geunbae LeeDepartment of Computer Science and EngineeringPohang University of Science & Technology (POSTECH){jh21983, semko, gblee}@postech.ac.krAbstractWe propose a transformation based sen-tence splitting method for statistical ma-chine translation.
Transformations are ex-panded to improve machine translationquality after automatically obtained frommanually split corpus.
Through a series ofexperiments we show that the transforma-tion based sentence splitting is effectivepre-processing to long sentence translation.1 IntroductionStatistical approaches to machine translation havebeen studied actively, after the formalism of statis-tical machine translation (SMT) is proposed byBrown et al (1993).
Although many approaches ofthem were effective, there are still lots of problemsto solve.
Among others, we have an interest in theproblems occurring with long sentence decoding.Various problems occur when we try to translatelong input sentences because a longer sentencecontains more possibilities of selecting translationoptions and reordering phrases.
However, reorder-ing models in traditional phrase-based systems arenot sufficient to treat such complex cases when wetranslate long sentences (Koehn et al 2003).Some methods which can offer powerful reor-dering policies have been proposed like syntaxbased machine translation (Yamada and Knight,2001) and Inversion Transduction Grammar (Wu,1997).
Although these approaches are effective,decoding long sentences is still difficult due totheir computational complexity.
As the length ofan input sentence becomes longer, the analysis anddecoding become more complex.
The complexitycauses approximations and errors inevitable duringthe decoding search.In order to reduce this kind of difficulty causedby the complexity, a long sentence can be paraph-rased by several shorter sentences with the samemeaning.
Generally, however, decomposing acomplex sentence into sub-sentences requires in-formation of the sentence structures which can beobtained by syntactic or semantic analysis.
Unfor-tunately, the high level syntactic and semanticanalysis can be erroneous and costs as expensive asSMT itself.
So, we don?t want to fully analyze thesentences to get a series of sub-sentences, and ourapproach to this problem considers splitting onlycompound sentences.In the past years, many research works wereconcerned with sentence splitting methods to im-prove machine translation quality.
This idea hadbeen used in speech translation (Furuse et al 1998)and example based machine translation (Doi andSumita, 2004).
These research works achievedmeaningful results in terms of machine translationquality.
Unfortunately, however, the method ofDoi and Sumita using n-gram is not available if thesource language is Korean.
In Korean language,most of sentences have special form of endingmorphemes at the end.
For that reason, we shoulddetermine not only the splitting position but alsothe ending morphemes that we should replace in-stead of connecting morphemes.
And the Furuse etal?s method involves parsing which requires heavycost.In this paper we propose a transformation basedsplitting method to improve machine translationquality which can be applied to the translationtasks with Korean as a source language.2 MethodsOur task is splitting a long compound sentence intoshort sub-sentences to improve the performance ofphrase-based statistical machine translation system.We use a transformation based approach toaccomplish our goal.2.1 A Concept of TransformationThe transformation based learning (TBL) is a kindof rule learning methods.
The formalism of TBL isintroduced by Brill (1995).
In past years, the TBLapproach was used to solve various problems innatural language processing such as part of speech(POS) tagging and parsing (Brill, 1993).A transformation consists of two parts: a trigger-ing environment and a rewriting rule.
And the re-writing rule consists of a source pattern and a tar-get pattern.
Our consideration is how to get theright transformations and apply them to split thelong sentences.A transformation works in the following man-ner; some portion of the input is changed by therewriting rule if the input meets a condition speci-fied in the triggering environment.
The rewritingrule finds the source pattern in the input and rep-laces it with the target pattern.
For example, sup-pose that a transformation which have a triggeringenvironment A, source pattern B and target patternC.
We can describe this transformation as a sen-tence: if a condition A is satisfied by an input sen-tence, then replace pattern B in the input sentencewith pattern C.2.2 A Transformation Based Sentence Split-ting MethodNormally, we have two choices when there aretwo or more transformations available for an inputpattern at the same time.
The first choice is apply-ing the transformation one by one, and the secondchoice is applying them simultaneously.
Thechoice is up to the characteristics of the problemthat we want to solve.
In our problem, we choosethe former strategy which is applying the transfor-mations one by one, because it gives direct intui-tion about the process of splitting sentences.
Bychoosing this strategy, we can design splittingprocess as a recursive algorithm.At first, we try to split an input sentence intotwo sub-sentences.
If the sentence has been split bysome transformation, the result involves exactlytwo sub-sentences.
And then we try to split eachsub-sentence again.
We repeat this process in re-cursive manner until no sub-sentences are split.In the above process, a sentence is split into atmost two sub-sentences through a single trial.
In asingle trial, a transformation works in the follow-ing manner:  If an input sentence satisfies the envi-ronment, we substitute the source pattern into thetarget pattern.
That is, replace the connecting mor-phemes with the proper ending morphemes.
Andthen we split the sentence with pre-defined posi-tion in the transformation.
And finally, we insertthe junction word that is also pre-defined in thetransformation between the split sentences after thesub sentences are translated independently.From the above process, we can notice easilythat a transformation for sentence splitting consistsof the four components: a triggering environment,a rewriting rule, a splitting position and a junctiontype.
The contents of each component are as fol-lows.
(1) A triggering environment contains a se-quence of morphemes with their POS tags.
(2) Arewriting consists of a pair of sequences of POStagged morphemes.
(3) A junction type can haveone of four types: ?and?, ?or?, ?but?
and ?NULL?.
(4) A splitting position is a non-negative integerthat means the position of starting word of secondsub-sentence.2.3 Learning the Transformation for Sen-tence SplittingAt the training phase, TBL process determinesthe order of application (or rank) of the transforma-tions to minimize the error-rate defined by a spe-cific measure.
The order is determined by choosingthe best rule for a given situation and applying thebest rule for each situation iteratively.
In the sen-tence splitting task, we maximize the machinetranslation quality with BLEU score (Papineni etal., 2001) instead of minimizing the error of sen-tence splitting.During the training phase, we determine the or-der of applying transformation after we build a setof transformations.
To build the set of transforma-tions, we need manually split examples to learn thetransformations.Building a transformation starts from extractinga rewriting rule by calculating edit-distance matrixbetween an original sentence and its split formfrom the corpus.
We can easily extract the differentparts from the matrix.BaseBLEU :=  BLEU score of the baseline systemS := Split example sentenceT := Extracted initial transformationfor each t?
Tfor each s?Swhile truetry to split s with tif mis-splitting is occurredExpand environmentelse exit while loopif environment cannot be expandedexit while loopS?
:= apply t to SDecode S?BLEU := measure BLEUDiscard t if BLEU < BaseBLEUsort  T w.r.t.
BLEUFrom the difference pattern, we can make thesource pattern of a rewriting rule by taking the dif-ferent parts of the original sentence side.
Similarly,the target pattern can be obtained from the differ-ent parts of split form.
And the junction type andsplitting position are directly obtained from thedifference pattern.
Finally, the transformation iscompleted by setting the triggering environment assame to the source pattern.
The set of initial trans-formations is obtained by repeating this process onall the examples.The Transformations for sentence splitting arebuilt from the initial transformations through ex-panding process.
In the expanding process, eachrule is applied to the split examples.
We expandthe triggering environment with some heuristics (insection 2.4), if a sentence is a mis-split.And finally, in order to determine the rank ofeach transformation, we sorted the extracted trans-formations by decreasing order of resulted BLEUscores after applying the transformation to eachtraining sentence.
And some transformations arediscarded if they decrease the BLEU score.
Thisprocess is different from original TBL.
The mod-ified TBL learning process is described in figure 1.2.4 Expanding Triggering EnvironmentsExpanding environment should be treated verycarefully.
If the environment is too specific, thetransformation cannot be used in real situation.
Onthe other hand, if it is too general, then the trans-formation becomes erroneous.Our main strategy for expanding the environ-ment is to increase context window size of thetriggering environment one by one until it causesno error on the training sentences.
In this manner,we can get minimal error-free transformations onthe sentence splitting corpus.We use two different windows to define a trig-gering environment: one for morpheme and anoth-er for its part of speech (POS) tag.
Figure 2 showsthis concept of two windows.
The circles corres-pond to sequences of morphemes and POS tags ina splitting example.
Window 1 represents a mor-pheme context and window 2 represents a POS tagcontext.
The windows are independently expandedfrom the initial environment which consists of amorpheme ?A?
and its POS tag.
In the figure, win-dow 1 is expanded to one forward morpheme andone backward morpheme while window 2 is ex-panded to two backward POS tags.In order to control these windows, we definedsome heuristics by specifying the following threepolicies of expanding windows: no expansion,forward only and forward and backward.
Fromthose three polices, we have 9 combinations ofheuristics because we have two windows.
By ob-serving the behavior of these heuristics, we canestimate what kind of information is most impor-tant to determine the triggering environment.Figure 1.
Modified TBL for sentence splittingFigure 2.
Window-based heuristics for triggeringenvironmentsTest No.
Window1 policy Window2 policyTest 1No expansionNo expansionTest 2 Forward onlyTest 3 Free expansionTest 4Forward onlyNo expansionTest 5 Forward onlyTest 6 Free expansionTest 7Free expansionNo expansionTest 8 Forward onlyTest 9 Free expansionTable 2.Experimental setupWe have at most 4 choices for a single step ofthe expanding procedure: forward morpheme,backward morpheme, forward POS tag, and back-ward POS tag.
We choose one of them in a fixedorder: forward POS tag, forward morpheme,backward POS tag and backward morpheme.These choices can be limited by 9 heuristics.
Forexample, suppose that we use a heuristic with for-ward policy on morpheme context window and noexpansion policy for POS tag context window.
Inthis case we have only one choice: forward mor-pheme.3  ExperimentsWe performed a series of experiments on Koreanto English translation task to see how the sentencesplitting affects machine translation quality andwhich heuristics are the best.
Our baseline systembuilt with Pharaoh (Koehn, 2004) which is mostpopular phrase-based decoder.
And trigram lan-guage model with KN-discounting (Kneser andNey, 1995) built by SRILM toolkit (Stolcke, 2002)is used.TestNo.# of  af-fected sen-tencesBLEU scoreBeforesplittingAftersplittingTest 1 209 0.1778 0.1838Test 2 142 0.1564 0.1846Test 3 110 0.1634 0.1863Test 4 9 0.1871 0.2150Test 5 96 0.1398 0.1682Test 6 100 0.1452 0.1699Test 7 8 0.2122 0.2433Test 8 157 0.1515 0.1727Test 9 98 0.1409 0.1664Table 1 shows the corpus statistics used in theexperiments.
The training corpus for MT systemhas been built by manually translating Korean sen-tences which are collected from various sources.We built 123,425 sentence pairs for training SMT,1,577 pairs for splitting and another 1,577 pairs fortesting.
The domain of the text is daily conversa-tions and travel expressions.
The sentence splittingcorpus has been built by extracting long sentencesfrom the source-side mono-lingual corpus.
Thesentences in the splitting corpus have been manual-ly split.The experimental settings for comparing 9 heu-ristics described in the section 2.4 are listed in ta-ble 2.
Each experiment corresponds to a heuristic.To see the effect of sentence splitting on transla-tion quality, we evaluated BLEU score for affectedsentenced by the splitting.
The results are shownin table 3.
Each test number shows the effect oftransformation-based sentence splitting with dif-ferent window selection heuristics listed in table 2.The scores are consistently increased with signifi-cant differences.
After analyzing the results of ta-ble 3, we notice that we can expect some perfor-SMT SplittingKorean English Before Split After SplitTrain # of Sentences 123,425 1,577 1,906# of Words 1,083,912 916,950 19,918 20,243Vocabulary 15,002 14,242 1,956 1,952Test #of Sentences 1,577 - -Table 1.
Corpus statisticsTable 3.
BLEU scores of affected sentencesmance gain when the average sentence length islong.The human evaluation shows more promisingresults in table 4.
In the table, the superior changemeans that the splitting results in better translationand inferior means the opposite case.
Two ratiosare calculated to see the effects of sentence split-ting.
The ratio ?sup/inf?
shows the ratio of superiorover inferior splitting.
And ratio trans/changeshows how many sentences are affected by a trans-formation in an average.
In most of the experi-ments, the number of superior splitting is overthree times larger than that of inferior ones.
Thisresult means that the sentence splitting is a helpfulpre-processing for machine translation.We listed some example translations affected bysentence splitting in the table 5.
In the three cases,junction words don?t appear in the results of trans-lation after split because their junction types areNULL that involves no junction word.
Althoughseveral kinds of improvements are observed in su-perior cases, the most interesting case occurs inout-of-vocabulary (OOV) cases.
A translation re-sult has a tendency to be a word salad whenOOV?s are included in the input sentence.
In thiscase, the whole sentence may lose its originalmeaning in the result of translation.
But after split-ting the input sentence, the OOV?s have a highchance to be located in one of the split sub-sentences.
Then the translation result can save atleast a part of its original meaning.
This case oc-curs easily if an input sentence includes only oneOOV.
The Superior change of table 5 is the case.Although both baseline and split are far from thereference, split catches some portion of the mean-ing.TestNo.# of trans-formations(rules)# ofchanges(sentences)# of supe-riorchanges# of infe-riorchanges# of insig-nificantchangesRatioSup/InfRatiotrans/change1 34 209 60 30 119 2.00 6.152 177 142 43 9 90 4.78 0.8023 213 110 29 9 72 3.22 0.5164 287 9 4 1 4 4.00 0.0315 206 96 25 4 67 6.25 0.4666 209 100 23 8 69 2.88 0.4787 256 8 3 1 4 3.00 0.0318 177 157 42 10 102 4.20 0.8879 210 98 21 4 73 5.25 0.467Table 4.
Human evaluation resultsSuperior changeReference I saw that some items are on sale on window .
what are they ?BaselineWhat kind of items do you have this item in OOV some discount, I get adiscount ?SplitYou have this item in OOV some discount .
what kind of items do I geta discount ?InsignificantchangeReference What is necessary to be issued a new credit card?Baseline I ?d like to make a credit card .
What do I need?Split I ?d like to make a credit card .
What is necessary?Inferior changeReferenceI ?d like to make a reservation by phone and tell me the phone numberplease .BaselineI ?d like to make a reservation but can you tell me the phone number ,please .Split I  ?d like to make a reservation .
can you tell me the , please .Table 5.
Example translations (The sentences are manually re-cased for readability)Most of the Inferior cases are caused by mis-splitting.
Mis-splitting includes a case of splitting asentence that should not be split or splitting a sen-tence on the wrong position.
This case can be re-duced by controlling the heuristics described insection 2.4.
But the problem is that the effort toreducing inferior cases also reduces the superiorcases.
To compare the heuristics each other in thiscondition, we calculated the ratio of superior andinferior cases.
The best heuristic is test no.
5 interms of the ratio of sup/inf.The test no.
4 and 7 show that a trans-formationbecomes very specific when lexical information isused alone.
Hence the ratio trans/change becomesbelow 0.01 in this case.
And test no.
1 shows thatthe transformations with no environment expan-sion are erroneous since it has the lowest ratio ofsup/inf.4 ConclusionWe introduced a transformation based sentencesplitting method for machine translation as a effec-tive and efficient pre-processing.
A transformationconsists of a triggering environment and a rewrit-ing rule with position and junction type informa-tion.
The triggering environment of a transforma-tion is extended to be error-free with respect totraining corpus after a rewriting rule is extractedfrom manually split examples.
The expandingprocess for the transformation can be generalizedby adding POS tag information into the triggeringenvironment.The experimental results show that the effect ofsplitting is clear in terms of both automatic evalua-tion metric and human evaluation.
The results con-sistently state that the statistical machine transla-tion quality can be improved by transformationbased sentence splitting method.AcknowledgmentsThis research was supported by the MIC (Ministryof Information and Communication), Korea, underthe ITRC (Information Technology Research Cen-ter) support program supervised by the IITA (Insti-tute of Information Technology Assessment) (II-TA-2006-C1090-0603-0045).
The parallel corpuswas courteously provided by Infinity Telecom, Inc.ReferencesEric Brill.
1993.
Transformation-based error-drivenparsing.
In Proc.
of third International Workshop onParsing.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A CaseStudy in Part-of-Speech Tagging.
ComputationalLinguistics 21(4):543-565.Peter F. Brown, Stephen A. Della Pietra, VincentJ.Della Pietra and Robert L. Mercer.
1993.
The Ma-thematics of Statistical Machine Translation: Parame-ter estimation.
Computational Linguistics, 19(2):263-312.Takao Doi and Eiichiro Sumita.
2004.
Splitting inputsentence for machine translation using languagemodel with sentence similarity.
In Proc.
of the 20thinternational conference on Computational Linguis-tics.Osamu Furuse, Setsuo Yamada and Kazuhide Yamamo-to.
1998.
Splitting Long or Ill-formed Input for Ro-bust Spoken-language Translation.
In Proc of the 36thannual meeting on Association for ComputationalLinguistics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram lnguage modeling.
In Proc.of the International Conference on Acoustics, Speech,and Signal Processing (ICASSP).Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proc.
of the 6th Conference of the Associationfor Machine translation in the Americas.Philipp Koehn, Franz Josef Och and Kevin Knight.2003.
Statistical Phrase-Based Translation.
In Proc ofthe of the 2003 Conference of the North AmericanChapter of the Association for Computational Lin-guistics on Human Language Technology.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of Machine Translation.
Technical ReportRC22176, IBM.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proc.
of the 7th InternationalConference on Spoken Language Processing (ICSLP).Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics 23(3):377-404.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation Model.
In Proc.
of the confe-rence of the Association for Computational Linguis-tics (ACL).
