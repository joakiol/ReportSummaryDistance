Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 927?936,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA new Approach to Improving Multilingual Summarization using aGenetic AlgorithmMarina LitvakBen-Gurion Universityof the NegevBeer Sheva, Israellitvakm@bgu.ac.ilMark LastBen-Gurion Universityof the NegevBeer Sheva, Israelmlast@bgu.ac.ilMenahem FriedmanBen-Gurion Universityof the NegevBeer Sheva, Israelfmenahem@bgu.ac.ilAbstractAutomated summarization methods canbe defined as ?language-independent,?
ifthey are not based on any language-specific knowledge.
Such methods canbe used for multilingual summarizationdefined by Mani (2001) as ?processingseveral languages, with summary in thesame language as input.?
In this pa-per, we introduce MUSE, a language-independent approach for extractive sum-marization based on the linear optimiza-tion of several sentence ranking measuresusing a genetic algorithm.
We tested ourmethodology on two languages?Englishand Hebrew?and evaluated its perfor-mance with ROUGE-1 Recall vs. state-of-the-art extractive summarization ap-proaches.
Our results show that MUSEperforms better than the best known multi-lingual approach (TextRank1) in both lan-guages.
Moreover, our experimental re-sults on a bilingual (English and Hebrew)document collection suggest that MUSEdoes not need to be retrained on each lan-guage and the same model can be usedacross at least two different languages.1 IntroductionDocument summaries should use a minimumnumber of words to express a document?s mainideas.
As such, high quality summaries can sig-nificantly reduce the information overload manyprofessionals in a variety of fields must contend1We evaluated several summarizers?SUMMA, MEAD,Microsoft Word Autosummarize and TextRank?on the DUC2002 corpus.
Our results show that TextRank performedbest.
In addition, TextRank can be considered language-independent as long as it does not perform any morphologicalanalysis.with on a daily basis (Filippova et al, 2009), as-sist in the automated classification and filtering ofdocuments, and increase search engines precision.Automated summarization methods canuse different levels of linguistic analysis:morphological, syntactic, semantic and dis-course/pragmatic (Mani, 2001).
Although thesummary quality is expected to improve whena summarization technique includes languagespecific knowledge, the inclusion of that knowl-edge impedes the use of the summarizer onmultiple languages.
Only systems that performequally well on different languages withoutlanguage-specific knowledge (including linguisticanalysis) can be considered language-independentsummarizers.The publication of information on the Internetin an ever-increasing variety of languages 2 dic-tates the importance of developing multilingualsummarization approaches.
There is a particu-lar need for language-independent statistical tech-niques that can be readily applied to text in anylanguage without depending on language-specificlinguistic tools.
In the absence of such techniques,the only alternative to language-independent sum-marization would be the labor-intensive transla-tion of the entire document into a common lan-guage.Here we introduce MUSE (MUltilingual Sen-tence Extractor), a new approach to multilingualsingle-document extractive summarization wheresummarization is considered as an optimization ora search problem.
We use a Genetic Algorithm(GA) to find an optimal weighted linear combina-tion of 31 statistical sentence scoring methods thatare all language-independent and are based on ei-ther a vector or a graph representation of a docu-ment, where both representations are based on a2 Gulli and Signorini (2005) used Web searches in 75 dif-ferent languages to estimate the size of the Web as of the endof January 2005.927word segmentation.We have evaluated our approach on two mono-lingual corpora of English and Hebrew documentsand, additionally, on one bilingual corpora com-prising English and Hebrew documents.
Our eval-uation experiments sought to- Compare the GA-based approach for single-document extractive summarization (MUSE) tothe best known sentence scoring methods.- Determine whether the same weighting model isapplicable across two different languages.This paper is organized as follows.
The nextsection describes the related work in statisticalextractive summarization.
Section 3 introducesMUSE, the GA-based approach to multilingualsingle-document extractive summarization.
Sec-tion 4 presents our experimental results on mono-lingual and bilingual corpora.
Our conclusionsand suggestions for future work comprise the fi-nal section.2 Related WorkExtractive summarization is aimed at the selec-tion of a subset of the most relevant fragmentsfrom a source text into the summary.
The frag-ments can be paragraphs (Salton et al, 1997), sen-tences (Luhn, 1958), keyphrases (Turney, 2000)or keywords (Litvak and Last, 2008).
Statisti-cal methods for calculating the relevance scoreof each fragment can be categorized into sev-eral classes: cue-based (Edmundson, 1969), key-word- or frequency-based (Luhn, 1958; Edmund-son, 1969; Neto et al, 2000; Steinberger andJezek, 2004; Kallel et al, 2004; Vanderwende etal., 2007), title-based (Edmundson, 1969; Teufeland Moens, 1997), position-based (Baxendale,1958; Edmundson, 1969; Lin and Hovy, 1997;Satoshi et al, 2001) and length-based (Satoshi etal., 2001).Considered the first work on sentence scoringfor automated text summarization, Luhn (1958)based the significance factor of a sentence on thefrequency and the relative positions of signifi-cant words within a sentence.
Edmundson (1969)tested different linear combinations of four sen-tence ranking scoring methods?cue, key, title andposition?to identify that which performed beston a training corpus.
Linear combinations of sev-eral statistical sentence ranking methods were alsoapplied in the MEAD (Radev et al, 2001) andSUMMA (Saggion et al, 2003) approaches, bothof which use the vector space model for text repre-sentation and a set of predefined or user-specifiedweights for a combination of position, frequency,title, and centroid-based (MEAD) features.
Gold-stein et al (1999) integrated linguistic and statisti-cal features.
In none of these works, however, didthe researchers attempt to find the optimal weightsfor the best linear combination.Information retrieval and machine learningtechniques were integrated to determine sentenceimportance (Kupiec et al, 1995; Wong et al,2008).
Gong and Liu (2001) and Steinberger andJezek (2004) used singular value decomposition(SVD) to generate extracts.
Ishikawa et al (2002)combined conventional sentence extraction and atrainable classifier based on support vector ma-chines.Some authors reduced the summarization pro-cess to an optimization or a search problem.
Has-sel and Sjobergh (2006) used a standard hill-climbing algorithm to build summaries that max-imize the score for the total impact of the sum-mary.
A summary consists of first sentences fromthe document was used as a starting point for thesearch, and all neighbours (summaries that canbe created by simply removing one sentence andadding another) were examined, looking for a bet-ter summary.Kallel et al (2004) and Liu et al (2006b)used genetic algorithms (GAs), which are knownas prominent search and optimization meth-ods (Goldberg, 1989), to find sets of sentences thatmaximize summary quality metrics, starting froma random selection of sentences as the initial pop-ulation.
In this capacity, however, the high com-putational complexity of GAs is a disadvantage.To choose the best summary, multiple candidatesshould be generated and evaluated for each docu-ment (or document cluster).Following a different approach, Turney (2000)used a GA to learn an optimized set of parame-ters for a keyword extractor embedded in the Ex-tractor tool.3 Ora?san et al (2000) enhanced thepreference-based anaphora resolution algorithmsby using a GA to find an optimal set of values forthe outcomes of 14 indicators and apply the opti-mal combination of values from data on one textto a different text.
With such approach, trainingmay be the only time-consuming phase in the op-eration.3http://www.extractor.com/928Today, graph-based text representations are be-coming increasingly popular, due to their abil-ity to enrich the document model with syntacticand semantic relations.
Salton et al (1997) wereamong the first to make an attempt at using graph-based ranking methods in single document ex-tractive summarization, generating similarity linksbetween document paragraphs and using degreescores in order to extract the important paragraphsfrom the text.
Erkan and Radev (2004) and Mi-halcea (2005) introduced algorithms for unsuper-vised extractive summarization that rely on theapplication of iterative graph-based ranking algo-rithms, such as PageRank (Brin and Page, 1998)and HITS (Kleinberg, 1999).
Their methods rep-resent a document as a graph of sentences inter-connected by similarity relations.
Various sim-ilarity functions can be applied: cosine similar-ity as in (Erkan and Radev, 2004), simple over-lap as in (Mihalcea, 2005), or other functions.Edges representing the similarity relations can beweighted (Mihalcea, 2005) or unweighted (Erkanand Radev, 2004): two sentences are connected iftheir similarity is above some predefined thresholdvalue.3 MUSE ?
MUltilingual SentenceExtractorIn this paper we propose a learning approachto language-independent extractive summariza-tion where the best set of weights for a linear com-bination of sentence scoring methods is found bya genetic algorithm trained on a collection of doc-ument summaries.
The weighting vector thus ob-tained is used for sentence scoring in future sum-marizations.
Since most sentence scoring methodshave a linear computational complexity, only thetraining phase of our approach is time-consuming.3.1 Sentence scoring methodsOur work is aimed at identifying the best linearcombination of the 31 sentence scoring methodslisted in Table 1.
Each method description in-cludes a reference to the original work where themethod was proposed for extractive summariza-tion.
Methods proposed in this paper are denotedby new.
Formulas incorporate the following nota-tion: a sentence is denoted by S, a text documentby D, the total number of words in S by N , the to-tal number of sentences in D by n, the sequentialnumber of S in D by i, and the in-document termfrequency of the term t by tf(t).
In the LUHNmethod, Wi and Ni are the number of keywordsand the total number of words in the ith cluster, re-spectively, such that clusters are portions of a sen-tence bracketed by keywords, i.e., frequent, non-common words.4Figure 1 demonstrates the taxonomy of themethods listed in Table 1.
Methods that requirepre-defined threshold values are marked with across and listed in Table 2 together with the aver-age threshold values obtained after method eval-uation on English and Hebrew corpora.
Eachmethod was evaluated on both corpora, with dif-ferent threshold t ?
[0, 1] (only numbers with onedecimal digit were considered).
Threshold val-ues resulted in the best ROUGE-1 scores, wereselected.
A threshold of 1 means that all termsare considered, while a value of 0 means thatonly terms with the highest rank (tf, degree, orpagerank) are considered.
The methods are di-vided into three main categories?structure-, vec-tor-, and graph-based?according to the text rep-resentation model, and each category is dividedinto sub-categories.Section 3.3 describes our application of a GA tothe summarization task.Table 2: Selected thresholds for threshold-basedscoring methodsMethod ThresholdLUHN 0.9LUHN DEG 0.9LUHN PR 0.0KEY [0.8, 1.0]KEY DEG [0.8, 1.0]KEY PR [0.1, 1.0]COV 0.9COV DEG [0.7, 0.9]COV PR 0.13.2 Text representation modelsThe vector-based scoring methods listed in Ta-ble 1 use tf or tf-idf term weights to evaluatesentence importance.
In contrast, representationused by the graph-based methods (except for Tex-tRank) is based on the word-based graph represen-tation models described in (Schenker et al, 2004).Schenker et al (2005) showed that such graphrepresentations can outperform the vector spacemodel on several document categorization tasks.In the graph representation used by us in this work4Luhn?s experiments suggest an optimal limit of 4 or 5non-significant words between keywords.929Table 1: Sentence scoring metricsName Description SourcePOS F Closeness to the beginning of the document: 1i (Edmundson, 1969)POS L Closeness to the end of the document: i (Baxendale, 1958)POS B Closeness to the borders of the document: max( 1i , 1n?i+1 ) (Lin and Hovy, 1997)LEN W Number of words in the sentence (Satoshi et al, 2001)LEN CH Number of characters in the sentence5LUHN maxi?
{clusters(S)}{CSi}, CSi = W2iNi (Luhn, 1958)KEY Sum of the keywords frequencies:?t?
{Keywords(S)} tf(t) (Edmundson, 1969)COV Ratio of keywords number (Coverage): |Keywords(S)||Keywords(D)| (Liu et al, 2006a)TF Average term frequency for all sentence words:?t?Stf(t)N (Vanderwende et al, 2007)TFISF?t?S tf(t) ?
isf(t), isf(t) = 1 ?log(n(t))log(n) , (Neto et al, 2000)n(t) is the number of sentences containing tSVD Length of a sentence vector in ?2 ?
V T after computing Singular Value (Steinberger and Jezek, 2004)Decomposition of a term by sentences matrix A = U?V TTITLE O Overlap similarity6 to the title: sim(S, T ) = |S?T |min{|S|,|T |} (Edmundson, 1969)TITLE J Jaccard similarity to the title: sim(S, T ) = |S?T ||S?T |TITLE C Cosine similarity to the title: sim(~S, ~T ) = cos(~S, ~T ) = ~S?~T|~S|?|~T |D COV O Overlap similarity to the document complement newsim(S,D ?
S) = |S?T |min{|S|,|D?S|}D COV J Jaccard similarity to the document complement sim(S,D ?
S) = |S?T ||S?D?S|D COV C Cosine similarity to the document complement cos(~S, ~D ?
S) = ~S?
~D?S|~S|?| ~D?S|LUHN DEG Graph-based extensions of LUHN, KEY and COV measures respectively.KEY DEG Node degree is used instead of a word frequency: words are consideredCOV DEG significant if they are represented by nodes having a degree higherthan a predefined thresholdDEG Average degree for all sentence nodes:?i?
{words(S)}DegiNGRASE Frequent sentences from bushy paths are selected.
Each sentence in the bushypath gets a domination score that is the number of edges with its label in thepath normalized by the sentence length.
The relevance score for a sentenceis calculated as a sum of its domination scores over all paths.LUHN PR Graph-based extensions of LUHN, KEY and COV measures respectively.KEY PR Node PageRank score is used instead of a word frequency: words are consideredCOV PR significant if they are represented by nodes having a PageRank score higherthan a predefined thresholdPR Average PageRank for all sentence nodes:?t?SPR(t)NTITLE E O Overlap-based edge matching between title and sentence graphsTITLE E J Jaccard-based edge matching between title and sentence graphsD COV E O Overlap-based edge matching between sentence and a document complementgraphsD COV E J Jaccard-based edge matching between sentence and a document complementgraphsML TR Multilingual version of TextRank without morphological analysis: (Mihalcea, 2005)Sentence score equals to PageRank (Brin and Page, 1998) rank of its node:WS(Vi) = (1 ?
d) + d ?
?Vj?In(Vi)wji?Vk?Out(Vj)wjkWS(Vj)nodes represent unique terms (distinct words) andedges represent order-relationships between twoterms.
There is a directed edge from A to B if an Aterm immediately precedes the B term in any sen-tence of the document.
We label each edge withthe IDs of sentences that contain both words in thespecified order.3.3 Optimization?learning the best linearcombinationWe found the best linear combination of the meth-ods listed in Table 1 using a Genetic Algorithm(GA).
GAs are categorized as global search heuris-tics.
Figure 2 shows a simplified GA flowchart.A typical genetic algorithm requires (1) a geneticrepresentation of the solution domain, and (2) afitness function to evaluate the solution domain.We represent the solution as a vector of weights930Language-independent sentencescoringmethodsStructure-basedVector-basedGraph-basedPosition Length Frequency Similarity Degree SimilarityPagerankTitle DocumentPOS_FPOS_LPOS_BLEN_WLEN_CHLUHNKEYCOVTFTFIISFSVDTITLE_OTITLE_JTITLE_CD_COV_O*D_COV_J*D_COV_C*LUHN_DEG*KEY_DEG*COV_DEG*DEG*GRASE*LUHN_PR*KEY_PR*COV_PR*PR*ML_TRTitle DocumentTITLE_E_O*TITLE_E_J*D_COV_E_O*D_COV_E_J*Figure 1: Taxonomy of language-independent sentence scoring methodsSelectionMatingCrossoverMutationTerminate?BestgeneyesnoInitializationReproductionFigure 2: Simplified flowchart of a Genetic Algo-rithmfor a linear combination of sentence scoringmethods?real-valued numbers in the unlimitedrange normalized in such a way that they sum upto 1.
The vector size is fixed and it equals to thenumber of methods used in the combination.Defined over the genetic representation, the fit-ness function measures the quality of the repre-sented solution.
We use ROUGE-1 Recall (Linand Hovy, 2003) as a fitness function for mea-suring summarization quality, which is maximizedduring the optimization procedure.Below we describe each phase of the optimiza-tion procedure in detail.Initialization GA will explore only a small partof the search space, if the population is too small,whereas it slows down if there are too many solu-tions.
We start from N = 500 randomly gener-ated genes/solutions as an initial population, thatempirically was proven as a good choice.
Eachgene is represented by a weighting vector vi =w1, .
.
.
, wD having a fixed number of D ?
31 ele-ments.
All elements are generated from a standardnormal distribution, with ?
= 0 and ?2 = 1, andnormalized to sum up to 1.
For this solution rep-resentation, a negative weight, if it occurs, can beconsidered as a ?penalty?
for the associated met-ric.Selection During each successive generation, aproportion of the existing population is selected tobreed a new generation.
We use a truncation se-lection method that rates the fitness of each so-lution and selects the best fifth (100 out of 500)of the individual solutions, i.e., getting the maxi-mal ROUGE value.
In such manner, we discard?bad?
solutions and prevent them from reproduc-tion.
Also, we use elitism?method that preventslosing the best found solution in the population bycopying it to the next generation.Reproduction In this stage, newgenes/solutions are introduced into the popu-lation, i.e., new points in the search space areexplored.
These new solutions are generatedfrom those selected through the following geneticoperators: mating, crossover, and mutation.In mating, a pair of ?parent?
solutions is ran-domly selected, and a new solution is created us-ing crossover and mutation, that are the most im-portant part of a genetic algorithm.
The GA per-formance is influenced mainly by these two opera-tors.
New parents are selected for each new child,and the process continues until a new populationof solutions of appropriate size N is generated.Crossover is performed under the assumption931that new solutions can be improved by re-usingthe good parts of old solutions.
However it isgood to keep some part of population from onegeneration to the next.
Our crossover operator in-cludes a probability (80%) that a new and differentoffspring solution will be generated by calculat-ing the weighted average of two ?parent?
vectorsaccording to (Vignaux and Michalewicz, 1991).Formally, a new vector v will be created fromtwo vectors v1 and v2 according to the formulav = ?
?
v1 + (1?
?)
?
v2 (we set ?
= 0.5).
Thereis a probability of 20% that the offspring will be aduplicate of one of its parents.Mutation in GAs functions both to preserve theexisting diversity and to introduce new variation.It is aimed at preventing GA from falling into lo-cal extreme, but it should not be applied too often,because then GA will in fact change to randomsearch.
Our mutation operator includes a proba-bility (3%) that an arbitrary weight in a vector willbe changed by a uniformly randomized factor inthe range of [?0.3, 0.3] from its original value.Termination The generational process is re-peated until a termination condition?a plateau ofsolution/combination fitness such that successiveiterations no longer produce better results?hasbeen reached.
The minimal improvement in ourexperiments was set to ?
= 1.0E ?
21.4 Experiments4.1 OverviewThe MUSE summarization approach was eval-uated using a comparative experiment on twomonolingual corpora of English and Hebrew textsand on a bilingual corpus of texts in both lan-guages.
We intentionally chose English and He-brew, which belong to distinct language families(Indo-European and Semitic languages, respect-fully), to ensure that the results of our evaluationwould be widely generalizable.
The specific goalsof the experiment are to:- Evaluate the optimal sentence scoring models in-duced from the corpora of summarized documentsin two different languages.- Compare the performance of the GA-based mul-tilingual summarization method proposed in thiswork to the state-of-the-art approaches.- Compare method performance on both lan-guages.- Determine whether the same sentence scoringmodel can be efficiently used for extractive sum-marization across two different languages.4.2 Text preprocessingCrucial to extractive summarization, proper sen-tence segmentation contributes to the quality ofsummarization results.
For English sentences,we used the sentence splitter provided with theMEAD summarizer (Radev et al, 2001).
A sim-ple splitter that can split the text at periods, excla-mation points, or question marks was used for theHebrew text.74.3 Experiment designThe English text material we used in our experi-ments comprised the corpus of summarized doc-uments available to the single document summa-rization task at the Document Understanding Con-ference, 2002 (DUC, 2002).
This benchmarkdataset contains 533 news articles, each accompa-nied by two to three human-generated abstracts ofapproximately 100 words each.For the Hebrew language, however, to the bestof our knowledge, no summarization benchmarksexist.
To generate a corpus of summarized Hebrewtexts, therefore, we set up an experiment wherehuman assessors were given 50 news articles of250 to 830 words each from the Website of theHaaretz newspaper.8 All assessors were providedwith the Tool Assisting Human Assessors (TAHA)software tool9 that enables sentences to be easilyselected and stored for later inclusion in the doc-ument extract.
In total, 70 undergraduate studentsfrom the Department of Information Systems En-gineering, Ben Gurion University of the Negevparticipated in the experiment.
Each student par-ticipant was randomly assigned ten different doc-uments and instructed to (1) spend at least fiveminutes on each document, (2) ignore dialogs andquotations, (3) read the whole document beforebeginning sentence extraction, (4) ignore redun-dant, repetitive, and overly detailed information,and (5) remain within the minimal and maximalsummary length constraints (95 and 100 words, re-spectively).
Summaries were assessed for qualityby comparing each student?s summary to those ofall the other students using the ROUGE evalua-7Although the same set of splitting rules may be used formany different languages, separate splitters were used for En-glish and Hebrew because the MEAD splitter tool is restrictedto European languages.8http://www.haaretz.co.il9TAHA can be provided upon request932tion toolkit adapted to Hebrew10 and the ROUGE-1 metric (Lin and Hovy, 2003).
We filtered all thesummaries produced by assessors that received av-erage ROUGE score below 0.5, i. e. agreed withthe rest of assessors in less than 50% of cases.Finally, our corpus of summarized Hebrew textswas compiled from the summaries of about 60%of the most consistent assessors, with an aver-age of seven extracts per single document11.
TheROUGE scores of the selected assessors are dis-tributed between 50 and 57 percents.The third, bilingual, experimental corpus wasassembled from documents in both languages.4.4 Experimental ResultsWe evaluated English and Hebrew summaries us-ing ROUGE-1, 2, 3, 4, L, SU and W metrics, de-scribed in (2004).
In agreement with Lin?s (2004)conclusion, our results for the different metricswere not statistically distinguishable.
However,ROUGE-1 showed the largest variation across themethods.
In the following comparisons, all resultsare presented in terms of the ROUGE-1 Recallmetric.We estimated the ROUGE metric using 10-foldcross validation.
The results of training and testingcomprise the average ROUGE values obtained forEnglish, Hebrew, and bilingual corpora (Table 3).Since we experimented with a different number ofEnglish and Hebrew documents (533 and 50, re-spectively), we have created 10 balanced bilingualcorpora, each with the same number of Englishand Hebrew documents, by combining approxi-mately 50 randomly selected English documentswith all 50 Hebrew documents.
Each corpus wasthen subjected to 10-fold cross validation, and theaverage results for training and testing were calcu-lated.We compared our approach (1) with amultilingual version of TextRank (denoted byML TR) (Mihalcea, 2005) as the best knownmultilingual summarizer, (2) with MicrosoftWord?s Autosummarize function12 (denoted byMS SUM) as a widely used commercial summa-10The regular expressions specifying ?word?
were adaptedto Hebrew alphabet.
The same toolkit was used for sum-maries evaluation on Hebrew corpus.11Dataset is available at http://www.cs.bgu.ac.il/?litvakm/research/12We reported the following bug to Microsoft: MicrosoftWord?s Document.Autosummarize Method returns differentresults from the output of the AutoSummarize Dialog Box.In our experiments, the Method results were used.rizer, and (3) with the best single scoring methodin each corpus.
As a baseline, we compiled sum-maries created from the initial sentences (denotedby POS F).
Table 4 shows the comparative re-sults (ROUGE mean values) for English, Hebrew,and bilingual corpora, with the best summarizerson top.
Pairwise comparisons between summa-rizers indicated that all methods (except POS Fand ML TR in the English and bilingual corporaand D COV J and POS F in the Hebrew corpus)were significantly different at the 95% confidencelevel.
MUSE performed significantly better thanTextRank in all three corpora and better than thebest single methods COV DEG in English andD COV J in Hebrew corpora respectively.Two sets of features?the full set of 31 sen-tence scoring metrics and the 10 best bilingualmetrics determined in our previous work13 usinga clustering analysis of the methods results onboth corpora?were tested on the bilingual corpus.The experimental results show that the optimizedcombination of the 10 best metrics is not signif-icantly distinguishable from the best single met-ric in the multilingual corpus ?
COV DEG.
Thedifference between the combination of all 31 met-rics and COV DEG is significant only with a one-tailed p-value of 0.0798 (considered not very sig-nificant).
Both combinations significantly outper-formed all the other summarizers that were com-pared.
Table 4 contains the results of MUSE-trained weights for all 31 metrics.Our experiments showed that the removal ofhighly-correlated metrics (the metric with thelower ROUGE value out of each pair of highly-correlated metrics) from the linear combinationslightly improved summarization quality, but theimprovement was not statistically significant.
Dis-carding bottom ranked features (up to 50%), also,did not affect the results significantly.Table 5 shows the best vectors generated fromtraining MUSE on all the documents in the En-glish, Hebrew, and multilingual (one of 10 bal-anced) corpora and their ROUGE training scoresand number of GA iterations.While the optimal values of the weights are ex-pected to be nonnegative, among the actual re-sults are some negative values.
Although thereis no simple explanation for this outcome, it maybe related to a well-known phenomenon from Nu-merical Analysis called over-relaxation (Friedman13submitted to publication933and Kandel, 1994).
For example, Laplace equa-tion ?xx + ?yy = 0 is iteratively solved over agrid of points as follows: At each grid point let?
(n), ?
(n) denote the nth iteration as calculatedfrom the differential equation and its modified fi-nal value, respectively.
The final value is chosenas ??
(n) + (1 ?
?)?(n?1).
While the sum of thetwo weights is obviously 1, the optimal value of ?,which minimizes the number of iterations neededfor convergence, usually satisfies 1 < ?
< 2(i.e., the second weight 1?
?
is negative) and ap-proaches 2 the finer the grid gets.
Though some-what unexpected, this surprising result can be rig-orously proved (Varga, 1962).Table 3: Results of 10-fold cross validationENG HEB MULTTrain 0.4483 0.5993 0.5205Test 0.4461 0.5936 0.5027Table 4: Summarization performance.
MeanROUGE-1Metric ENG HEB MULTMUSE 0.4461 0.5921 0.4633COV DEG 0.4363 0.5679 0.4588D COV J 0.4251 0.5748 0.4512POS F 0.4190 0.5678 0.4440ML TR 0.4138 0.5190 0.4288MS SUM 0.3097 0.4114 0.3184Assuming efficient implementation, most met-rics have a linear computational complexity rela-tive to the total number of words in a document- O(n).
As a result, MUSE total computationtime, given a trained model, is also linear (at fac-tor of the number of metrics in a combination).The training time is proportional to the number ofGA iterations multiplied by the number of indi-viduals in a population times the fitness evaluation(ROUGE) time.
On average, in our experimentsthe GA performed 5?
6 iterations?selection andreproduction?before reaching convergence.5 Conclusions and future workIn this paper we introduced MUSE, a new, GA-based approach to multilingual extractive sum-marization.
We evaluated the proposed method-ology on two languages from different languagefamilies: English and Hebrew.
The experimen-tal results showed that MUSE significantly out-performed TextRank, the best known language-Table 5: Induced weights for the best linear com-bination of scoring metricsMetric ENG HEB MULTCOV DEG 8.490 0.171 0.697KEY DEG 15.774 0.218 -2.108KEY 4.734 0.471 0.346COV PR -4.349 0.241 -0.462COV 10.016 -0.112 0.865D COV C -9.499 -0.163 1.112D COV J 11.337 0.710 2.814KEY PR 0.757 0.029 -0.326LUHN DEG 6.970 0.211 0.113POS F 6.875 0.490 0.255LEN CH 1.333 -0.002 0.214LUHN -2.253 -0.060 0.411LUHN PR 1.878 -0.273 -2.335LEN W -13.204 -0.006 1.596ML TR 8.493 0.340 1.549TITLE E J -5.551 -0.060 -1.210TITLE E O -21.833 0.074 -1.537D COV E J 1.629 0.302 0.196D COV O 5.531 -0.475 0.431TFISF -0.333 -0.503 0.232DEG 3.584 -0.218 0.059D COV E O 8.557 -0.130 -1.071PR 5.891 -0.639 1.793TITLE J -7.551 0.071 1.445TF 0.810 0.202 -0.650TITLE O -11.996 0.179 -0.634SVD -0.557 0.137 0.384TITLE C 5.536 -0.029 0.933POS B -5.350 0.347 1.074GRASE -2.197 -0.116 -1.655POS L -22.521 -0.408 -3.531Score 0.4549 0.6019 0.526Iterations 10 6 7independent approach, in both Hebrew and En-glish using either monolingual or bilingual cor-pora.
Moreover, our results suggest that the sameweighting model is applicable across multiple lan-guages.
In future work, one may:- Evaluate MUSE on additional languages and lan-guage families.- Incorporate threshold values for threshold-basedmethods (Table 2) into the GA-based optimizationprocedure.- Improve performance of similarity-based metricsin the multilingual domain.- Apply additional optimization techniques likeEvolution Strategy (Beyer and Schwefel, 2002),which is known to perform well in a real-valuedsearch space.- Extend the search for the best summary to theproblem of multi-object optimization, combiningseveral summary quality metrics.934AcknowledgmentsWe are grateful to Michael Elhadad and GalinaVolk from Ben-Gurion University for providingthe ROUGE toolkit adapted to the Hebrew alpha-bet, and to Slava Kisilevich from the Universityof Konstanz for the technical support in evaluationexperiments.ReferencesP.
B. Baxendale.
1958.
Machine-made index for tech-nical literaturean experiment.
IBM Journal of Re-search and Development, 2(4):354?361.H.-G. Beyer and H.-P. Schwefel.
2002.
Evolutionstrategies: A comprehensive introduction.
JournalNatural Computing, 1(1):3?52.S.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
Computernetworks and ISDN systems, 30(1-7):107?117.DUC.
2002.
Document understanding conference.http://duc.nist.gov.H.
P. Edmundson.
1969.
New methods in automaticextracting.
ACM, 16(2).G.
Erkan and D. R. Radev.
2004.
Lexrank: Graph-based lexical centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research,22:457?479.K.
Filippova, M. Surdeanu, M. Ciaramita, andH.
Zaragoza.
2009.
Company-oriented extractivesummarization of financial news.
In Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 246?254.M.
Friedman and A. Kandel.
1994.
Fundamentals ofComputer Numerical Analysis.
CRC Press.D.
E. Goldberg.
1989.
Genetic algorithms in search,optimization and machine learning.
Addison-Wesley.J.
Goldstein, M. Kantrowitz, V. Mittal, and J. Car-bonell.
1999.
Summarizing text documents: Sen-tence selection and evaluation metrics.
In Proceed-ings of the 22nd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 121?128.Y.
Gong and X. Liu.
2001.
Generic text summarizationusing relevance measure and latent semantic analy-sis.
In Proceedings of the 24th ACM SIGIR confer-ence on Research and development in informationretrieval, pages 19?25.A.
Gulli and A. Signorini.
2005.
The indexable web ismore than 11.5 billion pages.
http://www.cs.uiowa.edu/?asignori/web-size/.M.
Hassel and J. Sjobergh.
2006.
Towards holisticsummarization: Selecting summaries, not sentences.In Proceedings of Language Resources and Evalua-tion.K.
Ishikawa, S-I.
ANDO, S-I.
Doi, and A. Okumura.2002.
Trainable automatic text summarization usingsegmentation of sentence.
In Proceedings of 2002NTCIR 3 TSC workshop.F.
J. Kallel, M. Jaoua, L. B. Hadrich, and A. BenHamadou.
2004.
Summarization at laris labora-tory.
In Proceedings of the Document Understand-ing Conference.J.M.
Kleinberg.
1999.
Authoritative sources in ahyperlinked environment.
Journal of the ACM(JACM), 46(5):604?632.J.
Kupiec, J. Pedersen, and F Chen.
1995.
A trainabledocument summarizer.
In Proceedings of the 18thannual international ACM SIGIR conference, pages68?73.C.Y.
Lin and E. Hovy.
1997.
Identifying topics by po-sition.
In Proceedings of the fifth conference on Ap-plied natural language processing, pages 283?290.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In NAACL ?03: Proceedings ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 71?78.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out(WAS 2004), pages 25?26.M.
Litvak and M. Last.
2008.
Graph-based keywordextraction for single-document summarization.
InProceedings of the Workshop on Multi-source Multi-lingual Information Extraction and Summarization,pages 17?24.D.
Liu, Y.
He, D. Ji, and H. Yang.
2006a.
Genetic al-gorithm based multi-document summarization.
Lec-ture Notes in Computer Science, 4099:1140.D.
Liu, Y. Wang, C. Liu, and Z. Wang.
2006b.
Mul-tiple documents summarization based on geneticalgorithm.
Lecture Notes in Computer Science,4223:355.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research and Develop-ment, 2:159?165.Inderjeet Mani.
2001.
Automatic Summarization.
Nat-ural Language Processing, John Benjamins Publish-ing Company.Rada Mihalcea.
2005.
Language independent extrac-tive summarization.
In AAAI?05: Proceedings of the20th national conference on Artificial intelligence,pages 1688?1689.935J.L.
Neto, A.D. Santos, C.A.A.
Kaestner, and A.A. Fre-itas.
2000.
Generating text summaries through therelative importance of topics.
Lecture Notes in Com-puter Science, pages 300?309.Constantin Ora?san, Richard Evans, and Ruslan Mitkov.2000.
Enhancing preference-based anaphora res-olution with genetic algorithms.
In DimitrisChristodoulakis, editor, Proceedings of the SecondInternational Conference on Natural Language Pro-cessing, volume 1835, pages 185 ?
195, Patras,Greece, June 2?
4.
Springer.Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang.
2001.
Experiments in single and multidoc-ument summarization using mead.
First DocumentUnderstanding Conference.Horacio Saggion, Kalina Bontcheva, and Hamish Cun-ningham.
2003.
Robust generic and query-basedsummarisation.
In EACL ?03: Proceedings of thetenth conference on European chapter of the Associ-ation for Computational Linguistics.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
In-formation Processing and Management, 33(2):193?207.C.
N. Satoshi, S. Satoshi, M. Murata, K. Uchimoto,M.
Utiyama, and H. Isahara.
2001.
Sentence ex-traction system assembling multiple evidence.
InProceedings of 2nd NTCIR Workshop, pages 319?324.A.
Schenker, H. Bunke, M. Last, and A. Kandel.
2004.Classification of web documents using graph match-ing.
International Journal of Pattern Recognitionand Artificial Intelligence, 18(3):475?496.A.
Schenker, H. Bunke, M. Last, and A. Kandel.
2005.Graph-theoretic techniques for web content mining.J.
Steinberger and K. Jezek.
2004.
Text summarizationand singular value decomposition.
Lecture Notes inComputer Science, pages 245?254.S.
Teufel and M. Moens.
1997.
Sentence extraction asa classification task.
In Proceedings of the Workshopon Intelligent Scalable Summarization, ACL/EACLConference, pages 58?65.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336.L.
Vanderwende, H. Suzuki, C. Brockett, andA.
Nenkova.
2007.
Beyond sumbasic: Task-focused summarization with sentence simplificationand lexical expansion.
Information processing andmanagement, 43(6):1606?1618.R.S.
Varga.
1962.
Matrix Iterative Methods.
Prentice-Hall.G.
A. Vignaux and Z. Michalewicz.
1991.
A ge-netic algorithm for the linear transportation problem.IEEE Transactions on Systems, Man and Cybernet-ics, 21:445?452.K.F.
Wong, M. Wu, and W. Li.
2008.
Extractive sum-marization using supervised and semi-supervisedlearning.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume 1,pages 985?992.936
