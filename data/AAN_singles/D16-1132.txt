Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244?1254,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsIntra-Sentential Subject Zero Anaphora Resolution usingMulti-Column Convolutional Neural NetworkRyu Iida Kentaro Torisawa Jong-Hoon OhCanasai Kruengkrai Julien KloetzerNational Institute of Information and Communications TechnologyKyoto 619-0289, Japan{ryu.iida,torisawa,rovellia,canasai,julien}@nict.go.jpAbstractThis paper proposes a method for intra-sentential subject zero anaphora resolution inJapanese.
Our proposed method utilizes aMulti-column Convolutional Neural Network(MCNN) for predicting zero anaphoric rela-tions.
Motivated by Centering Theory andother previous works, we exploit as clues boththe surface word sequence and the dependencytree of a target sentence in our MCNN.
Eventhough the F-score of our method was lowerthan that of the state-of-the-art method, whichachieved relatively high recall and low preci-sion, our method achieved much higher pre-cision (>0.8) in a wide range of recall lev-els.
We believe such high precision is cru-cial for real-world NLP applications and thusour method is preferable to the state-of-the-artmethod.1 IntroductionIn such pro-drop languages as Japanese, Chineseand Italian, pronouns are frequently omitted in text.For example, the subject of uketa (suffered) is unre-alized in the following Japanese example (1):(1) sono-houkokusho-wa seifui-gathe report-TOP governmenti-SUBJjouyaku-o teiketsushi (?i-ga) keizaitekinitreaty-OBJ make iti-SUBJ economicallyhigai-o uke-ta koto-o shitekishi-tadamage-OBJ suffer-PAST COMP point out-PASTThe report pointed out that the governmentiagreed to a treaty and (iti) suffered economically.The omitted argument is called a zero anaphor,which is represented using ?.
In example (1), zeroanaphor ?i refers to its antecedent, seifui (govern-ment).
Such a reference phenomenon is called zeroanaphora.
Identifying zero anaphoric relations isan essential task in developing such accurate NLPapplications as information extraction and machinetranslation for pro-drop languages.
For example, inJapanese, 60% of subjects in newspaper articles areunrealized as zero anaphors (Iida et al, 2007).This paper proposes a method for intra-sententialsubject zero anaphora resolution, in which a zeroanaphor and its antecedent appear in the same sen-tence and the zero anaphor must be a subject ofa predicate, for Japanese.
We target subject zeroanaphors because they represent 85% of the intra-sentential zero anaphora in our data set (example(1) is such a case).
Furthermore, this work focuseson intra-sentential zero anaphora because inter-sentential cases, in which a zero anaphor and its an-tecedent do not appear in the same sentence, are ex-tremely difficult.
The accuracy of the state-of-the-art method for resolving inter-sentential anaphora islow (Sasano and Kurohashi, 2011), and we believethe current technologies are not mature enough todeal with inter-sentential cases.Our method locally predicts the likelihood of azero anaphoric relation between every possible com-bination of potential zero anaphor and potential an-tecedent without considering the other (potential)zero anaphoric relations in the same sentence.
Thefinal determination of zero anaphoric relations foreach zero anaphor in a given sentence is done ina greedy way; only the most likely candidate an-tecedent for each zero anaphor is selected as its an-tecedent as far as the likelihood score exceeds a1244given threshold.
This approach contrasts with globaloptimization methods (Yoshikawa et al, 2011; Iidaand Poesio, 2011; Ouchi et al, 2015), which haverecently become popular.
These methods use theconstraints among possible zero anaphoric relations,such as ?if a candidate antecedent is identified asthe antecedent of a subject zero anaphor of a predi-cate, the candidate cannot be referred to by the ob-ject zero anaphor of the same predicate?, and deter-mine an optimal set of zero anaphoric relations inan entire sentence while satisfying such constraints,using such optimization techniques as sentence-wiseglobal learning (Ouchi et al, 2015) and integer lin-ear programming (Iida and Poesio, 2011).Although the global optimization methods haveoutperformed the previous greedy-style methods,our contention is that greedy-style methods can still,in a certain sense, outperform the state-of-the-artglobal optimization methods.
Ouchi et al (2015)?sglobal optimization method achieved the state-of-the-art F-score for Japanese intra-sentential subjectzero anaphora resolution, but its performance hasnot yet reached a level of practical use.
In our set-ting, for example, it actually obtained a precision ofonly 0.61, and even after attempting to obtain morereliable zero anaphoric relations by several modi-fications, we could only achieve 0.80 precision atextremely low recall levels (<0.01).
On the otherhand, while our proposed greedy-style method ob-tained a lower F-score than Ouchi et al?s method,it achieved much higher precision in a wide rangeof recall levels (e.g., around 0.8 precision at 0.25in recall and around 0.7 precision at 0.4 in recall).We believe such high precision is crucial to real-world applications, even though the recall remainslow, and thus our method is preferable to Ouchi etal.
?s method in that sense.In our proposed method, we use a Multi-columnConvolutional Neural Network (MCNN) (Cires?an etal., 2012), which is a variant of a Convolutional Neu-ral Network (CNN) (LeCun et al, 1998).
AnMCNNhas several independent columns, each of which hasits own convolutional and pooling layers.
The out-puts of all the columns are combined in the finallayer to provide a final prediction.
In this work, mo-tivated by Centering Theory (Grosz et al, 1995) andother previous works, we exploit as distinct columnsthe word sequences obtained from the surface wordsequence and the dependency tree of a target sen-tence in our MCNN.
Although the existing worksalso exploited such word sequences, they used onlyparticular types of information from them as featuresbased on the researchers?
linguistic insights.
In con-trast, we minimized such feature engineering due tousing an MCNN.The rest of this paper is organized as follows.In Section 2, we briefly overview previous workon zero anaphora resolution.
In Section 3, wepresent the procedure of our zero anaphora resolu-tion method and explain the column sets used in ourMCNN architecture.
We evaluate how effectivelyour method recognizes intra-sentential subject zeroanaphora in Section 4 and summarize this work anddiscuss future directions in Section 5.2 Related workThe typical zero anaphora resolution algorithms pro-posed so far have exploited the information of apredicate that potentially has a zero anaphor and itscandidate antecedent in a supervised manner (Sekiet al, 2002; Iida et al, 2003; Isozaki and Hirao,2003; Iida et al, 2006; Taira et al, 2008; Sasanoet al, 2008; Imamura et al, 2009; Hayashibe et al,2011; Iida and Poesio, 2011; Sasano and Kurohashi,2011; Yoshikawa et al, 2011).
In addition, existingworks have exploited the dependency path betweena predicate and a candidate antecedent either by en-coding such paths to the set of binary features of thewords that appear in the path (Iida and Poesio, 2011)or by mining from the paths the sub-trees that effec-tively discriminate zero anaphoric relations (Iida etal., 2006).
However, both methods just focus on thedependency paths between a predicate and a candi-date antecedent without exploiting other structuralfragments in the dependency tree representing a tar-get sentence, whereas our method uses the text frag-ments that cover the entire dependency tree.Another important clue was derived from dis-course theories, such as Centering Theory (Groszet al, 1995).
In this theory, (zero) anaphoric phe-nomenon is explained based on the rules and prin-ciples regarding the recency and saliency of candi-date antecedents.
Okumura and Tamura (1996) de-veloped a rule-based method based on the idea ofCentering Theory.
Iida et al (2003) and Imamura et1245al.
(2009) used as features for machine learning theresults of rule-based antecedent identification basedon a variant of Centering Theory (Nariyama, 2002).However, we observed that actual anaphoric phe-nomena often do not obey Centering Theory.
Torobustly resolve zero anaphora, we need to exploreadditional clues that are represented in a target sen-tence (or text).Recent work by Iida et al (2015) newly intro-duced a sub-problem of zero anaphora resolution,subject sharing recognition, which is the task thatjudges whether two predicates have the same sub-ject.
In their method, a network of subject sharingpredicates is created by their subject sharing rec-ognizer, and then zero anaphora resolution is per-formed by propagating a subject to the unrealizedsubject positions through the path in the network.Even though the accuracy of subject sharing recog-nition exceeds that of zero anaphora resolution, thezero anaphoric relations identified using the resultsof subject sharing recognition are limited to thosethat can be reached by subject sharing relations.
Therecall of this method is not high.Although most zero anaphora resolution methodsindependently identify a zero anaphoric relation foreach predicate, some previous works optimized theglobal assignment of zero anaphoric relations in anentire sentence (or an entire text) while satisfyingseveral constraints among zero anaphoric relations.For example, Iida and Poesio (2011) found the bestassignment of subject zero anaphoric relations usinginteger linear programming.
As mentioned in the In-troduction, Ouchi et al (2015) estimated the globalscore of all of the predicate-argument assignments ina sentence, which include the assignments of intra-sentential zero anaphoric relations, to find the bestassignment using a hill-climbing technique.
Theirmethod has an advantage: it can exploit complicatedrelations (e.g., the combination of two potential zeroanaphoric relations) as features to directly decidemore than one predicate-argument relation simulta-neously.
We adopted Ouchi et al (2015)?s methodas a baseline in Section 4 because it achieved thestate-of-the-art performance for intra-sentential zeroanaphora resolution.Collobert et al (2011) proposed CNN architec-ture that can be applied to various NLP tasks, suchas PoS tagging, chunking, named entity recognitionand semantic role labeling.
Following this work,CNNs have been utilized in such NLP tasks as docu-ment classification (Kalchbrenner et al, 2014; Kim,2014; Johnson and Zhang, 2015), paraphrase (Huet al, 2014; Yin and Schu?tze, 2015) and relationextraction (Liu et al, 2013; Zeng et al, 2014; dosSantos et al, 2015; Nguyen and Grishman, 2015).MCNNs were first introduced for image classifica-tion (Cires?an et al, 2012).
In NLP tasks, they havebeen utilized for question-answering (Dong et al,2015) and relation extraction (Zeng et al, 2015).Our MCNN architecture was inspired by a Siamesearchitecture (Chopra et al, 2005), which we extendto a multi-column network and replace its similaritymeasure with a softmax function at its top.3 Proposed methodOur proposed method consists of the following foursteps:Step 1 Extract every pair of a predicate and a can-didate antecedent, ?predi, candi?, that appearsin a target sentence.Step 2 Predict the probability of each pair using ourMCNN.Step 3 Rank in descending order all the pairs bytheir probabilities obtained in Step 2.Step 4 Choose the top pair ?predi, candi?
in theranked list and fill the zero anaphor position ofpredicate predi by candi if the position has notalready been filled by another candidate.
Re-move ?predi, candi?
from the list and repeatthis step as long as the score of the chosen pairexceeds a given threshold.In Step 1, we extract set of pairs ?predi, candi?in which candidate antecedent candi is paired withpredicate predi.
Note that we extracted predicatepredi, instead of a zero anaphor that is an unreal-ized subject of predi, because the (potential) zeroanaphor of predi is omitted in the text and cannotbe extracted directly.In Step 2, our MCNN gives a probability that in-dicates the likelihood of a zero anaphoric relationto judge for each pair whether candi fills the blanksubject position of predi through zero anaphora andranks all of the pairs by the probabilities in Step 3.1246!
"#$%&'((( (((SURFSEQ!
"#$#%&'( )*%+*%,*(((!
"#$%&% !&'($)*" +,$-"#$), .&'#&.
!$!/' 01/'*"'),1$#&."
#,.,),1!/'.&#'!/'.")""*+,-.",/"*0$+",)""*+,-.",/"*0$+",.
"*0%,12 ."*0%,13((()""*+,-.",/"*0$+",)""*+,-.",/"*0$+",.
"*0%,14 ."*0%,15((()""*+,-.",/"*0$+",.
"*0%,16PREDCONTEXT.&,7+7&$81&,$8.878,$ 9.&,7+: );87+.&$819);87+:DEPTREE.&,7+);87+(((((((((.&,7+ );87+ );87+.&,7+ );87+)&;!88'$;&.$1<";7!8=08,.8 8'$;&.$1<";7!8=08,.8)""*+,-.",/"*0$+",.
"*0%,1>BASEFigure 1: Our multi-column CNN architectureFinally, in Step 4 we actually fill candi in theblank subject positions of predi in a greedy stylein the order of the ranked list in Step 3, i.e., thezero anaphora resolution with a higher probability isdone before that with a lower probability.
If the sub-ject position is already occupied by another candi-date antecedent, candidate antecedents are no longerfilled at that position.3.1 Design of columns used in MCNNIn Step 2 of our method, we use a Multi-columnConvolutional Neural Network (MCNN).
Note thatzero anaphoric phenomena can be divided into twodifferent referential phenomena: anaphoric (i.e.,an antecedent precedes its zero anaphor) and cat-aphoric (i.e., a zero anaphor precedes its antecedent)cases.
To capture this difference, we divided the setof training instances into two subsets by the relativeoccurrence positions of a predicate and a candidateantecedent and respectively trained two independentMCNNs using each set.Our MCNN simultaneously uses four columnsets, as illustrated in Figure 1.
In the following ex-planation for each column set, we assume that can-didate antecedent candi precedes predicate predi inthe surface order (for the opposite case, i.e., the cat-aphoric case, the positions of candi and predi areswitched).BASE The first column set consists of one col-umn, which stores the word vectors of the bunsetsu!
"#$%&&&&&&&&&'()$%*(+*,+*'+*$+Figure 2: Columns (a, b, c, d) in DEPTREE column setphrases1 including either candi or predi.
We callthis column set the BASE column set.SURFSEQ The second column set consists of threecolumns, which store the word vectors of (a) thesurface word sequence spanning from the beginningof the sentence to candi, (b) the sequence betweencandi and predi, and (c) the remainder, i.e., frompredi to the end of the sentence.
Note that candiand predi are not included in any column of thiscolumn set.
We call this column set the SURFSEQcolumn set.DEPTREE The third set consists of four columns.We extracted four partial dependency trees from theentire dependency tree of a target sentence: (a) thedependency path between predi and candi, (b) thesub-trees that depend on predi, (c) the sub-trees onwhich candi depends and (d) the remaining sub-trees, which are illustrated in Figure 2.
Note thatcandi and predi are not included in the partial trees.Each column stores the word vectors of the word se-quence in which the words in (the set of) the partialtrees are ordered by their surface order.
We call thisset the DEPTREE column set.PREDCONTEXT The fourth set consists of threecolumns, which store the word vectors of (a) thebunsetsu phrase including predi, (b) the surfaceword sequence that appears before (a) (from the be-ginning of the sentence) and (c) the sequence thatappears after (a) (until the end of the sentence).
Wecall this column set the PREDCONTEXT column set.Among the four column sets, the SURFSEQ col-umn set was designed to introduce the clues based1A bunsetsu phrase is a Japanese base phrase that consistsof at least one content word optionally followed by functionwords.1247!"#$%&'(")%%*&(+&!"+%,)--&+.
/0&+"(1$1!
*"+2 '1314&315&4"6&+$3&$% %+&1%7 &8"$"3#819971'6:";<,);<,);< ";<";<,);<(/%"!#819#0&'28"$<&3;&''&'(891),&8"3!:!
"#$%&'&()$#*+%Figure 3: Dependency tree of example (1)on Centering Theory, in which the antecedent fora given zero anaphor can basically be identified bythe recency and saliency properties of a candidateantecedent.
More precisely, in the set of the mostsalient candidate antecedents, the most recent oneis preferred.
For example, suppose example (2)in which the predicate increase has a subject zeroanaphor and its antecedent is France:(2) nihon-wa shoshikataisaku-niJapan-TOP countermeasures to falling birth rate-IOBJshippaishi-taga, furansu-wa sore-ni seikoushifail-PAST/BUT France-TOP it-IOBJ succeed(?i-ga) shusseiritsu-o fuyashiteiru(iti-SUBJ) birth rate-OBJ increaseJapan failed to develop countermeasures to itsfalling birth rate, but Francei succeeded and (?i)increased its birth rate.In this situation, there are two most salient candi-date antecedents, Japan and France, because theyare marked with topic marker wa, which basicallyindicates the highest degree of candidate saliency.In this case, France is selected as the antecedent be-cause it appears more recently than Japan, and suchrecency can be estimated by consulting the surfaceword sequence between France and increase: noother salient candidates are included in the word se-quence.
Also, the other two types of word sequences(i.e., the sequence that spans from the beginning ofthe sentence to candi and that spans from predi toits end) are important for confirming whether a moresalient candidate than candi appears in each wordsequence.
If such a more salient candidate is found,it should be a stronger candidate of the antecedent.The DEPTREE column set is introduced for cap-turing a different aspect of intra-sentential zeroanaphora.
In the explanation based on CenteringTheory, the most salient candidate (e.g., the candi-date marked with wa (topic marker)) is selected as!
"#$%&'((( ((()"*+,-./$"*!
'0 '1 ((( '2$2#34&5,5&6.
*#.&$7*.8%&9!, #"*,!8:*&%"#"$"%/"4/&$.4&$3"4((( (((;&'89""534:/"4-"57$3"4!8:*&%"7$97$!,#*"%%&'89""534:, "#"$<.*,/"57%4!
"7$97$!,#*"%%&'89""534:, "#"$<.
*,/"57%4!3497$,)"*+!.=7.4/.Figure 4: Column of our MCNNan antecedent, but example (1) in Section 1 cannotbe interpreted based on saliency and recency.
In ex-ample (1), the report is the most salient candidate inthe sentence because it is marked with topic markerwa, but the less salient candidate government be-comes the antecedent of zero anaphor ?.
Such aproblem is often solved by introducing the depen-dency tree of a sentence.
Figure 3 represents thedependency tree of example (1) in which the an-tecedent of ?i appears in the embedded clause.
Insuch a case, an antecedent probably exists amongthe most salient candidates in the embedded clause.To introduce such structural clues, we used the par-tial dependency trees as columns in the DEPTREEcolumn set.Anaphoricity determination, which is the task ofjudging whether a candidate anaphor has an an-tecedent, was established as a subtask of coreferenceresolution.
This problem was basically solved byexploring the possible candidate antecedents for agiven anaphor candidate in its search space, and theresults were used for improving the overall perfor-mance of coreference resolution, especially in En-glish (Ng, 2004; Wiseman et al, 2015).
Inspiredby such previous works, we designed the PRED-CONTEXT set to determine the anaphoricity of zeroanaphors, i.e., to judge whether a zero anaphor can-didate has its antecedent in a sentence, by consultingthe surface word sequences before and after predi.3.2 MCNN architectureIn our MCNN (Figure 4), we represent each wordin text fragment t by d-dimensional embedding vec-1248tor xi and t by matrix T = [x1, .
.
.
, x|t|].2 T isthen wired to a set of M feature maps where eachfeature map is a vector.
Each element O in thefeature map is computed by a filter denoted by fj(1 ?
j ?
M ) from the N -gram word sequencesin t for a fixed integer N , as O = ReLU(Wfj ?xi:i+N?1+bfj ), where ?
denotes element-wise mul-tiplication followed by the summation of the result-ing elements (i.e., a Frobenious inner product ofWfj and xi:i+N?1) and ReLU(x) = max(0, x).
Inother words, we construct a feature map by convolv-ing a text fragment with a filter, which is parameter-ized by weightWfj ?
Rd?N and bias bfj ?
R. Notethat there can be several sets of feature maps whereeach set covers N -grams for different N .
Note thatthe weight of the feature maps for each N -gram ineach column set is shared.As a whole, these feature maps are referred to asa convolution layer.
The next layer is called a pool-ing layer.
Here we use max-pooling (Scherer et al,2010; Collobert et al, 2011), which simply selectsthe maximum value among the elements in the samefeature map.
Our assumption is that the maximumvalue indicates the existence of a strong clue, i.e.,N -gram, for our final judgment.
The selected maxi-mum values from all the M feature maps are simplyconcatenated, and the resulting M -dimensional vec-tor is given to our final layer.The final layer has vectors coming from multiplefeature maps in multiple columns.
They are againsimply concatenated and constitute a high dimen-sional feature vector.
The final layer applies a linearsoftmax function to produce the class probabilitiesof the zero anaphoric labels: true and false.
We usea mini-batch stochastic gradient descent (SGD) withthe Adadelta update rule (Zeiler, 2012), apply ran-dom initialization within (-0.01, 0.01) for Wfj , andinitialize the remaining parameters at zero.4 Experiments4.1 Revising annotation resultsIn our preliminary investigation of the intra-sentential zero anaphoric relations in the NAISTText Corpus (Iida et al, 2007), since we found moreannotation errors than we expected, we decided to2We use zero padding for dealing with text fragments ofvariable length (Kim, 2014).revise the annotation results.
In this revision, weadditionally annotated the subject sharing relations,where two predicates have the same subject regard-less whether the subject is realized or omitted, be-tween pairs of predicates in our data set.
Note thattwo predicates can have a subject sharing relationeven if neither has a realized subject as far as a sub-ject exists that can naturally fill the subject positionof the two predicates.
We used the annotated resultsof subject sharing relations to efficiently detect theannotation errors of intra-sentential zero anaphoricrelations, as shown below.Twenty-six human annotators directly annotatedthe subject sharing relations for pairs of predicatesin a sentence.
For this annotation, we automaticallyextracted from the NAIST Text Corpus all the pairsof predicates that appear in the same sentence andobtained 227,517 predicate pairs.
For making theannotation results more reliable, each subject shar-ing relation was individually judged by three anno-tators, and the final label was decided by a majorityvote.
After that, further revisions of the subject shar-ing relations and the zero anaphoric relations wereperformed by focusing on the inconsistent annota-tions between the newly annotated subject sharingrelations and the original predicate-argument rela-tions in the NAIST Text Corpus.
More precisely,we scrutinized the suspicious annotations such thata subject, which was determined through the anno-tated subject sharing relations, is not the same asa subject that was directly annotated in the NAISTText Corpus.
In this revision phase, both the subjectsharing and zero anaphora relations for such suspi-cious instances were independently re-annotated bythree annotators, and their final labels of both rela-tions were determined by a majority of the their de-cisions.3 As a result, 2,120 zero anaphoric instanceswere newly added to the corpus and 1,184 instanceswere removed from it for a total of 19,049 instancesof intra-sentential subject zero anaphoric relations.43We are planning to release the annotated results and in-formation on the data separation used in our evaluation fromhttps://alaginrc.nict.go.jp/.4After this revision, a small number of inconsistent anno-tated results have both a syntactically dependent subject and asubject zero anaphor because the revision was performed lo-cally.
There were 30 inconsistent instances in the testing setand 100 in the training and development sets.
We only removedsuch instances from the testing set without changing the other1249Type #docs #sentences #zero anaphors(intra-sentential)train 1,757 23,152 11,453dev 586 7,526 3,691test 586 7,705 3,875Table 1: Statistics of our data set4.2 Experimental settingsThe documents in the corpus were divided into fivesubsets, three of which were used as a training dataset, one as a development data set, and one as a test-ing data set.
The statistics of our data set are sum-marized in Table 1.
We evaluated the performanceof our intra-sentential subject zero anaphora resolu-tion method and three baseline methods describedbelow using the revised annotated results in our dataset.We implemented our MCNN using Theano(Bastien et al, 2012).
We pre-trained 300-dimensional word embedding vectors for 1,658,487words5 using Skip-gram with a negative-samplingalgorithm (Mikolov et al, 2013)6 on a set of allthe sentences extracted from Wikipedia articles7(35,975,219 sentences).
We removed from the train-ing data all the words that only appeared once be-fore training.
In training, we treated them as un-known words and assigned them a random vector.To avoid overfitting, we applied early-stopping anddropout (Hinton et al, 2012) of 0.5 to the final layer.We used an SGD with mini-batches of 100 and alearning rate decay of 0.95.
We ran ten epochsthrough all of the training data, where each epochconsisted of many mini-batch updates.
We utilized3-, 4- and 5-grams with 100 filters each and used theF-score of positive instances as our evaluation met-ric.
The total number of the nodes in the final layersof our MCNN was 3,300: 11 columns ?
3 N -gram?
100 filters.Word segmentation, PoS tagging and dependencyparsing of the sentences in the NAIST Text Corpuswere performed by a Japanese morphological ana-lyzer, MeCab8 (Kudo et al, 2004), and a depen-two sets.5Words occurring less than five times in all the sentenceswere ignored to train the word embedding vectors.6We set the skip distance to 5 and the number of negativesamples to 10.7https://archive.org/details/jawiki-201501188http://taku910.github.io/mecab/dency parser, J.DepP9 (Yoshinaga and Kitsuregawa,2009).4.3 BaselinesWe compared our method with three baseline meth-ods.
The first baseline is a single-column convolu-tional neural network in which the column includesthe entire surface word sequence of a sentence.
Togive the positions of predi and candi to the network,we concatenated to each word vector an additional2-dimensional vector, where the first element is setto one if the corresponding word is predi, the sec-ond element is set to 1 if the corresponding word iscandi, and otherwise they are set to 0.
This baselinewas adopted for estimating the impact of a multi-column network compared to a single-column one.The remaining two baselines are Ouchi et al(2015)?s global optimization method and Iida et al(2015)?s method based on subject sharing recogni-tion.
Note that Ouchi?s method outputs predicate-argument relations for three grammatical roles (subj,obj, iobj), but for this evaluation we used onlythe outputs related to intra-sentential subject zeroanaphora resolution.
As done in Ouchi et al (2015),we averaged their performances across ten indepen-dent runs because the initial random assignment ofthe predicate-argument relations that was employedin their method changes the performance.
Ouchi?smethod does not require any development data set,so we used both the development and training datasets for training their joint model.
For training thesubject sharing recognizer used in Iida?s method, weused the annotated subject sharing relations in thetraining and development data sets.
In these twobaselines, we used the same morphological analyzerand dependency analyzer as for our method.4.4 ResultsTable 2 shows the results for each method.
Theirperformances were evaluated by measuring recall,precision, F-score and average precision (Avg.P).To assess the effectiveness of each column set intro-duced in Section 3.1, we evaluated the performanceof our method using every possible combinationof column sets that includes at least the BASEcolumn set.
We also gave the precision-recall (PR)9http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/1250Method #cols.
Recall Precision F-score Avg.POuchi et al (ACL2015) ?
0.539 0.612 0.573 0.670Iida et al (EMNLP2015) ?
0.484 0.357 0.411 ?single column CNN (w/ position vec.)
1 0.365 0.524 0.430 0.540MCNN BASE 1 0.446 0.394 0.419 0.448BASE+SURFSEQ 4 0.458 0.597 0.518 0.679BASE+DEPTREE 5 0.339 0.688 0.454 0.690BASE+SURFSEQ+DEPTREE 8 0.417 0.695 0.521 0.730BASE+SURFSEQ+PREDCONTEXT 7 0.459 0.631 0.531 0.702BASE+DEPTREE+PREDCONTEXT 8 0.298 0.728 0.422 0.702BASE+SURFSEQ+DEPTREE+PREDCONTEXT (Proposed) 11 0.418 0.704 0.525 0.732#cols.
stands for the number of columns used in each MCNN.Table 2: Results of intra-sentential subject zero anaphora resolutioncurves of our method using the four column sets(BASE+SURFSEQ+DEPTREE+PREDCONTEXT),the single column baseline, and Ouchi?s methodin Figure 5 to investigate the behavior of eachmethod at a high precision level.10 The PR-curvesof our method and the single-column baseline wereplotted just by altering the threshold parameters inStep 4 of our method (See Section 3).
In contrast,the PR-curve of Ouchi?s method cannot be easilyplotted because it gives a score to each sentence,not to each zero anaphoric relation.
For plottingthe PR-curve, we used the normalized global scoreof a sentence as the score of any zero anaphoricrelations in the sentence.11 Note that the recall oftheir PR-curve reached just 0.539, shown in Table 2,because we could not estimate the scores of thezero anaphoric relations that were not outputted bytheir method.
The PR-curves of the other methodsalso fail to reach 1.0 in recall.
This is becausethe zero anaphoric relations are exclusive; a zeroanaphor does not refer to more than one antecedent.If a method provides an incorrect zero anaphoricrelation, a correct relation for the same zero anaphorwill never be provided in its output.
Also, note thatthe average precision of each method was calculatedby averaging the precisions at the available recall10The PR-curve of Iida et al (2015)?s method was not plottedbecause it does not provide the score of each zero anaphoricrelation.11The global score provided by Ouchi?s method becomesgreater based on the number of predicate-argument pairs in asentence.
To control this, we normalized the original globalscore by the sum of the frequencies of the single or doublepredicate-argument pairs because the feature functions were ap-plied to such pairs in their method.
This achieved the best per-formance among the normalization schemes we have tried sofar.0.10.20.30.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7precisionrecallMCNN: ProposedOuchi (ACL2015)Ouchi (ACL2015) PRsingle-column CNNIida (EMNLP2015)Figure 5: PR-curves of each methodlevels for each method.The results in Table 2 show that our method usingall the column sets achieved the best average pre-cision among the combination of column sets thatinclude at least the BASE column set.
This sug-gests that all of the clues introduced by our fourcolumn sets are effective for performance improve-ment.
Table 2 also demonstrates that our method us-ing all the column sets obtained better average pre-cision than the strongest baseline, Ouchi?s method,in spite of an unfavorable condition for it.12 Theresults also show that our method with all of thecolumn sets achieved a better F-score than Iida?smethod and the single-column baseline.
However,it achieved a lower F-score than Ouchi?s method.This was caused by the choice of different recall lev-els for computing the F-score.
In contrast, the PR-12When calculating the average precision of each method,the relatively low values in precision at high recall levels (i.e.,from 0.54 to 0.67) were used in our method but not in Ouchi?smethod, as seen in Figure 5.1251Set Method Recall Precision F-score Avg.PAnaphoric single-column CNN (w/ position vec.)
0.445 0.525 0.481 0.341MCNN (BASE) 0.591 0.330 0.424 0.367MCNN (BASE+SURFSEQ) 0.555 0.566 0.560 0.565MCNN (BASE+DEPTREE) 0.389 0.615 0.476 0.518MCNN (BASE+SURFSEQ+DEPTREE) 0.503 0.660 0.571 0.599MCNN (BASE+SURFSEQ+PREDCONTEXT) 0.535 0.611 0.570 0.581MCNN (BASE+DEPTREE+PREDCONTEXT) 0.330 0.699 0.449 0.528MCNN (Proposed) 0.492 0.673 0.569 0.602Cataphoric single-column CNN (w/ position vec.)
0.163 0.293 0.209 0.163MCNN (BASE) 0.171 0.130 0.148 0.099MCNN (BASE+SURFSEQ) 0.202 0.417 0.272 0.257MCNN (BASE+DEPTREE) 0.268 0.438 0.332 0.329MCNN (BASE+SURFSEQ+DEPTREE) 0.195 0.525 0.285 0.330MCNN (BASE+SURFSEQ+PREDCONTEXT) 0.258 0.406 0.316 0.276MCNN (BASE+DEPTREE+PREDCONTEXT) 0.240 0.488 0.322 0.341MCNN (Proposed) 0.251 0.522 0.339 0.337Table 3: Results of instance-wise evaluation for anaphoric and cataphoric setscurves for these two methods in Figure 5 show thatour method obtained higher precision than Ouchi?smethod at all recall levels.
Particularly, it got highprecision in a wide range of recall levels (e.g.,around 0.8 in precision at 0.25 in recall and around0.7 in precision at 0.4 in recall), while the precisionobtained by Ouchi?s method at 0.25 in recall was justaround 0.65.
We believe this difference becomescrucial when using the outputs of each method fordeveloping accurate real-world NLP applications.In addition to an evaluation that used all of thetest instances, we also investigated how our methodperformed differently for anaphoric and cataphoriccases.
In this evaluation, we first divided our data setinto anaphoric and cataphoric sets by the relative po-sition of the candidate antecedent and evaluated theperformance by measuring the recall, precision, F-score and average precision for each set.
This eval-uation was done instance-wise, where we took intoaccount each pair of a predicate and its candidate an-tecedent as a classification target, while in the pre-vious evaluation the performance was measured forthe set of zero anaphors in the test set.
Thus, thefigures in Table 2 and Table 3 are not comparable.Note that we only compared our method with thebaseline using a single-column convolutional neuralnetwork because the other baselines are not able tooutput the score of each instance for measuring theiraverage precision.The results in Table 3 show that our MCNN-basedmethod achieved better average precision than thesingle-column CNN baseline except the method thatuses only the BASE column set for the cataphoriccase.
The results also demonstrate that each columnset consistently contributes to improving the aver-age precision for both the anaphoric and cataphoriccases.
However, Table 3 shows that the average pre-cision for the cataphoric set remains low.
As onefuture direction for further improvement, we needto explore clues for identifying cataphoric relationsmore accurately.5 ConclusionThis paper proposed an accurate method for intra-sentential subject zero anaphora resolution us-ing a Multi-column Convolutional Neural Network(MCNN).
As clues, our MCNN exploits both thesurface word sequence and the dependency tree of atarget sentence.
Our experimental results show thatthe proposed method achieved better precision thanthe strong baselines in a wide range of recall levels.As future work, we plan to use our MCNN archi-tecture for inter-sentential zero anaphora resolutionand develop highly accurate NLP applications usingour intra-sentential subject zero anaphora resolutionmethod.AcknowledgementWe thank Hiroki Ouchi for providing his predicate-argument analyzer that was proposed in Ouchi et al(2015).1252ReferencesFre?de?ric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.
InProceedings of the NIPS 2012 Workshop: Deep Learn-ing and Unsupervised Feature Learning.Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, withapplication to face verification.
In Proceedings ofComputer Vision and Pattern Recognition Conference,pages 539?546.Dan Claudiu Cires?an, Ueli Meier, and Ju?rgen Schmidhu-ber.
2012.
Multi-column deep neural networks forimage classification.
In Computer Vision and PatternRecognition, pages 3642?3649.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015.
Ques-tion answering over freebase with multi-column con-volutional neural networks.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing, pages 260?269.Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015.Classifying relations by ranking with convolutionalneural networks.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing, pages 626?634.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto.2011.
Japanese predicate argument structure analysisexploiting argument position and type.
In Proceedingsof 5th International Joint Conference on Natural Lan-guage Processing, pages 201?209.Geoffrey E .
Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2012.
Im-proving neural networks by preventing co-adaptationof feature detectors.
CoRR, abs /1207.0580.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Processingsof Advances in Neural Information Processing Systems27 (NIPS 2014), pages 2042?2050.Ryu Iida and Massimo Poesio.
2011.
A cross-lingualILP solution to zero anaphora resolution.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 804?813.Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-sumoto.
2003.
Incorporating contextual cues in train-able models for coreference resolution.
In Proceed-ings of the 2003 EACL Workshop on The Computa-tional Treatment of Anaphora, pages 23?30.Ryu Iida, Kentaro Inui, and Yuji Matsumoto.
2006.
Ex-ploiting syntactic patterns as clues in zero-anaphoraresolution.
In Processings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for ComputationalLinguistics, pages 625?632.Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Annotating a Japanese text corpus withpredicate-argument and coreference relations.
In Pro-ceedings of the ACL Workshop: ?Linguistic AnnotationWorkshop?, pages 132?139.Ryu Iida, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, and Julien Kloetzer.
2015.
Intra-sententialzero anaphora resolution using subject sharing recog-nition.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing,pages 2179?2189.Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009.Discriminative approach to predicate-argument struc-ture analysis with zero-anaphora resolution.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing, pages85?88.Hideki Isozaki and Tsutomu Hirao.
2003.
Japanese zeropronoun resolution based on ranking rules and ma-chine learning.
In Proceedings of the 2003 Conferenceon Empirical Methods in Natural Language Process-ing, pages 184?191.Rie Johnson and Tong Zhang.
2015.
Effective use ofword order for text categorization with convolutionalneural networks.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 103?112.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics, pages 655?665.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1746?1751.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields to Japanese1253morphological analysis.
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing, pages 230?237.Yann LeCun, Le?on Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
In Proceedings of the IEEE,pages 2278?2324.Chunyang Liu, Wenbo Sun, Wenhan Chao, and Wanxi-ang Che.
2013.
Convolution neural network for re-lation extraction.
In Proceedings of the 9th Interna-tional Conference of Advanced Data Mining and Ap-plications, pages 231?242.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InProceedings of Advances in Neural Information Pro-cessing Systems 26 (NIPS 2013), pages 3111?3119.Shigeko Nariyama.
2002.
Grammar for ellipsis resolu-tion in Japanese.
In Proceedings of the 9th Interna-tional Conference on Theoretical and MethodologicalIssues in Machine Translation, pages 135?145.Vincent Ng.
2004.
Learning noun phrase anaphoricityto improve conference resolution: Issues in represen-tation and optimization.
In Proceedings of the 42ndMeeting of the Association for Computational Linguis-tics, pages 151?158.Thien Huu Nguyen and Ralph Grishman.
2015.
Rela-tion extraction: Perspective from convolutional neuralnetworks.
In Proceedings of the 1st Workshop on Vec-tor Space Modeling for Natural Language Processing,pages 39?48.Manabu Okumura and Kouji Tamura.
1996.
Zero pro-noun resolution in Japanese discourse based on center-ing theory.
In Proceedings of The 16th InternationalConference on Computational Linguistics, pages 871?876.Hiroki Ouchi, Hiroyuki Shindo, Kevin Duh, and YujiMatsumoto.
2015.
Joint case argument identificationfor Japanese predicate argument structure analysis.
InProceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing, pages 961?970.Ryohei Sasano and Sadao Kurohashi.
2011.
A discrim-inative approach to Japanese zero anaphora resolutionwith large-scale lexicalized case frames.
In Proceed-ings of 5th International Joint Conference on NaturalLanguage Processing, pages 758?766.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2008.
A fully-lexicalized probabilistic modelfor Japanese zero anaphora resolution.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics, pages 769?776.Dominik Scherer, Andreas Mu?ller, and Sven Behnke.2010.
Evaluation of pooling operations in convolu-tional architectures for object recognition.
In Proceed-ings of the 20th International Conference on ArtificialNeural Networks, pages 92?101.Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.2002.
A probabilistic method for analyzing Japaneseanaphora integrating zero pronoun detection and reso-lution.
In Processings of the 19th International Con-ference on Computational Linguistics, pages 1?7.Hirotoshi Taira, Sanae Fujita, andMasaaki Nagata.
2008.A Japanese predicate argument structure analysis us-ing decision lists.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 523?532.Sam Wiseman, Alexander M. Rush, Stuart Shieber, andJason Weston.
2015.
Learning anaphoricity and an-tecedent ranking features for coreference resolution.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing, pages 1416?1426.Wenpeng Yin and Hinrich Schu?tze.
2015.
Convolutionalneural network for paraphrase identification.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 901?911.Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Mat-sumoto.
2011.
Jointly extracting Japanese predicate-argument relation with Markov Logic.
In Proceedingsof 5th International Joint Conference on Natural Lan-guage Processing, pages 1125?1133.Naoki Yoshinaga and Masaru Kitsuregawa.
2009.
Poly-nomial to linear: Efficient classification with conjunc-tive features.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1542?1551.Matthew D. Zeiler.
2012.
ADADELTA: An adaptivelearning rate method.
In arXiv:1212.5701 (Dec 27,2012).Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, andJun Zhao.
2014.
Relation classification via convolu-tional deep neural network.
In Proceedings of the 25thInternational Conference on Computational Linguis-tics, pages 2335?2344.Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.2015.
Distant supervision for relation extraction viapiecewise convolutional neural networks.
In Proceed-ings of the 2015 Conference on Empirical Methods inNatural Language Processing, pages 1753?1762.1254
