Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1203?1208,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsWhy Read if You Can Scan?Trigger Scoping Strategy for Biographical Fact ExtractionDian Yu and Heng JiComputer Science DepartmentRensselaer Polytechnic InstituteTroy, NY, USA{yud2,jih}@rpi.eduSujian LiPeking UniversityKey Laboratory ofComputational LinguisticsBeijing, Chinalisujian@pku.edu.cnChin-Yew LinMicrosoft Research AsiaBeijing, Chinacyl@microsoft.comAbstractThe rapid growth of information sourcesbrings a unique challenge to biographicalinformation extraction: how to find specificfacts without having to read all the words.An effective solution is to follow the humanscanning strategy which keeps a specifickeyword in mind and searches within aspecific scope.
In this paper, we mimic ascanning process to extract biographicalfacts.
We use event and relation triggers askeywords, identify their scopes and applytype constraints to extract answers within thescope of a trigger.
Experiments demonstratethat our approach outperforms state-of-the-artmethods up to 26% absolute gain in F-scorewithout using any syntactic analysis orexternal knowledge bases.1 IntroductionExtracting biographical information is an impor-tant task because it can help readers understandan ongoing event more easily by providing thebackground biographical information of participantsin this event.
In fact, this task has been part of theText Analysis Conference (TAC) - Knowledge BasePopulation (KBP) Slot Filling (SF) Track (Ji et al,2010; Ji et al, 2011; Surdeanu, 2013; Surdeanu andJi, 2014) for years.Overall, state-of-the-art research still needs im-provement.
A typical approach is based on patternswhich include triggers (e.g., (Sun et al, 2011; Li etal., 2012)).
Here trigger is defined as the small-est extent of a text which most clearly expressesan event occurrence or indicates a relation type.High-quality patterns yield quite high precision butrelatively low recall.
In addition, it?s relativelyexpensive to maintain and update a set of extractionpatterns.Furthermore, we carefully investigated the TAC-KBP SF 2012 ground truth corpus and find that94.36% of the biographical facts are mentioned in asentence containing indicative fact-specific triggers.For example, born is a trigger for extracting birth-related facts.
Triggers are crucial in predictingthe type of facts (Aguilar et al, 2014).
However,most previous studies only focused on using triggersto create more patterns (e.g., (Li et al, 2013)).Therefore the critical problem is how to make themost of triggers in biographical fact extraction?We observe that people tend to scan a documentwhen they want to quickly find a biographical factwithin limited time.
According to Douglas andFrazier (2001), scanning is a strategy for quicklyfinding specific information (keywords or ideas) ina text while ignoring its broader meaning.
Scanninginvolves skipping words, but the emphasis is thatthe reader knows what to look for and rapidlyscans until words are found and closer reading canoccur (Phipps, 1983).There are five steps in implementing scanningstrategy according to Arnold (1999):1.
Keep in mind what you are searching for.2.
Anticipate in what form the information islikely to appear ?
number, proper nouns, etc.3.
Analyze the organization of the content beforestarting to scan.12034.
Let your eyes run rapidly over several lines ofprint at a time.5.
When you find the sentence that has the infor-mation you seek, read the entire sentence.Educators have verified that scanning is an ef-fective strategy in enhancing reading comprehen-sion (Motallebzadeh and Mamdoohi, 2011).
Thereare two important aspects in the scanning strategy:keywords and their corresponding scopes.
For bio-graphical fact extraction, triggers can easily act asthe keywords used by human during scanning andthus we focus on identifying the scopes of triggers.Given a sentence that contains one or more trig-gers, we define trigger scope as the shortest frag-ment that is related to a trigger.
Based on ourobservation, each fact-specific trigger has its ownscope and its corresponding facts seldom appearoutside of its scope.
In the following sentence, ifwe can identify the scope of graduated, a triggerfor education-related facts, we can skip the rest ofthe sentence after 1965 even though ChesterbrookAcademy is an educational organization.She [<graduated> from Barnard in 1965] andsoon began teaching English at Chesterbrook A-cademy in Pennsylvania.1In this paper, we study the effect of triggersby learning their linguistic scopes at the sentencelevel and apply this strategy to extract 11 types ofbiographical facts, namely, birth date, death date,birth place, death place, residence place, education,parents, spouse, children, siblings and other familyas described in the KBP SF task.We design our extraction process following thescanning steps corresponding to Arnold?s scanningtheory.1.
Let the computer know the query and the facttype to be extracted.2.
Let the computer know what form or entitytype the candidate answer is likely to appear ?person, organization, phrase, time, etc.3.
Locate all the triggers of the given fact type andrecognize their respective scopes.1The scope is marked with [] and the trigger is marked with<>.4.
Within each scope, extract candidate answerswhich satisfy the entity type constraint in 2.The contributions of our paper are as follows.?
We are the first to study the application oftrigger scoping in biographical fact extraction.?
Our approach does not rely on any externalknowledge bases for training or manually cre-ated fact-specific rules, and yet dramaticallyadvances state-of-the-art.2 ApproachIn this section, we present the detailed approach ofapplying trigger scoping to biographical fact extrac-tion.
In Section 2.1, we first introduce the annotationmethods of constructing the gold-standard datasetfor evaluating scope identification.
We use thesentence in Figure 1 as our illustrative example.triggers, we define trigger scope as th sh rtestfragment that is related to a trigger.
Based onour observation, each fact-specific trigger has itsown scope and its corresponding facts seldomappear outside of its scope.
For example, in thefollowing sentence, if we can identify the scopeof graduated, a trigger for education-related facts,we can skip the rest of the sentence after 1965 eventhough Chesterbrook Academy is an educationalorganization.She [<graduated> from Barnard in 1965] andoon began teachi g English t Chest brook A-ad my in Pennsylvania.1In this paper, we study the effect of trigg rsby learning their linguistic scopes at the sentencelevel and apply this strategy to biographical factextraction on 11 biographical facts, namely, birthdate, death date, birth place, death place, resi-dence place, education, parents, spouse, children,siblings and other family as described in SF.We design our extraction process following thescanning steps corresponding to Arnold?s scan-ning theory.1.
Let the computer know the query and the factto be extracted.2.
Let the computer know what form or entitytype the candidate answer is likely to appear?
person, organization, phrase, time, etc.3.
Locate all the triggers of the given fact andrecognize their respective scopes.4.
Within each scope, extract candidate answerssatisfying the entity type constraint in 2.The contributions of our paper are as follows.?
We are the first to study the application oftrigger scoping in biographical fact extrac-tion.?
The system does not rely on any externalknowledge bases for training or manuallycreated fact-specific rules, and yet dramati-cally advances state-of-the-art.2 ApproachIn this section, we present an approach of applyingtrigger scoping to biographical fact extraction,with the sentence ?
as a walk-through example.In Section 2.1, we first introduce the annotationmethods of constructing the gold-standard datasetfor the scope identification assessment.1The scope is marked with [] and the trigger is markedwith <>.Paul Francis Conrad and his [twin<brother>, James], were [<born> inCedar Rapids, Iowa, on June 27, 1924],[<sons> of Robert H. Conrad and FlorenceLawler Conrad].2.1 Trigger and Scope Annotation2.1.1 Basic issuesIn a text, sentences with triggers of birth, death,family, residence and education information areconsidered for annotation.
We will not annotatea sentence if it inherently hints a biographical factwithout support of lexical evidence.During the annotation, triggers are marked byangle brackets: <resident>, <native>, etc.
andthe scope of the trigger is denoted by squarebrackets as shown in sentence ?.2.1.2 Trigger TaggingWe mined fact-specific trigger lists from existingpatterns, rules and ground truth sentences fromKBP 2012 SF corpus.
Triggers for each fact arealso mined by mapping various knowledge bases,including Wikipedia Infoboxes, Freebase (Bol-lacker et al, 2008), DBPedia (Auer et al, 2007)and YAGO (Suchanek et al, 2007), into the Gi-gaword corpus2and Wikipedia articles via distantsupervision (Mintz et al, 2009).
In our experi-ment, we use 343 triggers in total and for each factthere are about 38 triggers in average.We examine all the sentences containing anypossible triggers.
The presence of a word inone trigger list does not necessarily mean that thesentence contains an event or a relation.For instance, the second child in the followingsentence is part of an organization?s name.He and his wife, Ann McGarry Buchwald movedto Washington in 1963 with their [<child>], whowas adopted from orphanages and [<child> wel-fare agencies] in Ireland, Spain and France.We also keep such sentences and annotate theirtrigger scopes without distinction.We only mark the syntactic head of a triggerphrase.
Following this strategy, we mark childrenfor the noun phrase foster children.2.1.3 Scope TaggingDuring the scope annotation, we first include thetrigger within its own scope and then mark its2http://catalog.ldc.upenn.edu/LDC2011T07Figure 1: Trigger and scope annotation example.2.1 Trigger a d Scope Annotation2.1.1 Basic issuesIn a text, the sentences containing biographicalfacts (e.g., birth, death, family, residence or educa-tion) are considered for annotation.
We disc r asent nce if it expr sses a biographical fact withoutsurfa e cues.During annotation, triggers are marked by anglebrackets (e.g., <resident>), and the scope bound-aries of a trigger are denoted by square brackets asshown in Figure 1.2.1.2 Trigger TaggingWe mined fact-specific trigger lists from existingpatterns (Chen et al, 2010; Min et al, 2012; Li et al,2012) and ground truth sentences from KBP 20121204SF corpus.
In our experiment, we use 343 triggersand 38 triggers on average for each fact type2.We examine all the sentences containing any pos-sible triggers.
The presence of a word in one triggerlist does not necessarily mean that the sentencecontains an event or a relation.
For instance, thesecond child in the following sentence is part of anorganization?s name.He and his wife, Ann McGarry Buchwald movedto Washington in 1963 with their [<child>], whowas adopted from orphanages and [<child> wel-fare agencies] in Ireland, Spain and France.We also keep such sentences and annotate theirtrigger scopes without distinction.Note that we only mark the syntactic head of atrigger phrase.
For example, we mark child for thenoun phrase the second child.2.1.3 Scope TaggingDuring the scope annotation, we first include thetrigger within its own scope and then mark its leftand right boundaries.
Usually the left boundary isthe trigger itself.When there are multiple triggers in the samesentence, we annotate each trigger?s scope separate-ly since it is possible that the scopes of differenttriggers are overlapped or nested as shown in thefollowing instance (the scope of daughters coversthe scope of wife):Pavarotti had three [<daughters> with his firstwife, Lorenza, Cristina and Giuliana; and one,Alice, with his second wife].Pavarotti had three daughters with his first[<wife>], Lorenza, Cristina and Giuliana; andone, Alice, with his second [<wife>].The scope of a word is not transitive.
In thephrase ?his [<son>?s home] in Washington?, homeis within son?s scope and in Washington is withinhome?s scope, however, the last prepositional phraseis outside of son?s scope.2.2 Scope IdentificationWe will introduce two methods for identifying trig-ger scopes.2The trigger lists are publicly available for research purposesat: http://nlp.cs.rpi.edu/data/triggers.zip2.2.1 Rule-based MethodThis method is used to investigate the perfor-mance of trigger scoping strategy when we do nothave any labeled data.
We use trigger as the leftscope boundary.
A verb or trigger with other facttypes is regarded as the right boundary.The rule-based scoping result of the walk-throughexample is as follows:Paul Francis Conrad and his twin [<brother>,James, were] [<born> in Cedar Rapids, Iowa,on June 27, 1924,] [<sons> of Robert H.Conrad and Florence Lawler Conrad.
]2.2.2 Supervised ClassificationAlternatively we regard scope identification as aclassification task.
For each detected trigger, scopeidentification is performed as a binary classificationof each token in the sentence as to whether it iswithin or outside of a trigger?s scope.We apply the Stanford CoreNLP toolkit (Manninget al, 2014) to annotate part-of-speech tags andnames in each document.
We design the followingfeatures to train a classifier.?
Position: The feature takes value 1 if the wordappears before the trigger, and 0 otherwise.?
Distance: The distance (in words) between theword and the trigger.?
POS: POS tags of the word and the trigger.?
Name Entity: The name entity type of theword.?
Interrupt: The feature takes value 1 if there is averb or a trigger with other fact type betweenthe trigger and the word, and 0 otherwise.Verbs and triggers with other fact types caneffectively change the current topic or continuein another way.Note that the trained classifier can make predic-tions that result in nonconsecutive blocks of scopetokens.
In this case, we aggregate the labels of allthe words of an entity to assign a global label, whichmeans that we assign the entity the majority label ofthe words it contains.1205Fact TypeRecall (%) Precision (%) F-score (%)1 2 3 1 2 3 1 2 3per:place of birth 59.4 88.2 88.2 76.0 87.0 88.2 66.7 87.6 88.2per:date of birth 59.1 94.4 100.0 100.0 94.4 100.0 74.3 94.4 100.0per:place of death 55.4 92.4 86.1 86.1 58.9 63.6 67.4 71.9 73.1per:date of death 46.4 98.2 96.5 81.3 48.3 53.4 59.1 64.7 68.8per:place of residence 60.0 68.9 68.9 40.4 64.2 61.3 48.3 66.5 64.9per:school attended 54.3 65.8 68.4 86.4 67.6 76.5 66.7 66.7 72.2per:parents 41.9 75.7 73.0 68.4 31.8 50.0 52.0 44.8 59.3per:sibling 50.0 76.2 76.2 61.5 59.3 55.2 55.2 66.7 64.0per:spouse 36.0 63.3 81.7 78.3 54.3 49.5 49.3 58.5 61.6per:children 39.5 61.8 76.4 73.2 58.5 71.6 51.3 60.1 73.9per:other family 23.1 66.7 71.4 75.0 53.9 53.6 35.3 59.6 61.2overall 47.7 77.4 80.6 75.1 61.7 65.7 56.9 67.4 71.6Table 1: performance on KBP 2013 (1:state-of-the-art; 2:rule-based; 3: SVMs).2.3 Biographical Fact ExtractionFor each relevant document of a given query, we useStanford CoreNLP to find the coreferential mentionsof the query and then return all the sentences whichcontain at least one query entity mention.
Foreach trigger in a sentence, we extract the entitieswhich satisfy fact-specific constraints within its s-cope.
As shown in Figure 1, brother is the triggerfor per:siblings and the candidate fact should be aperson name.
Thus we return all the person names(e.g., James) within brother?s scope as the queryPaul?s siblings.3 Experiments and Discussion3.1 DataWe use the KBP 2012 and 2013 SF corpora asthe development and testing data sets respectively.There are 50 person queries each year.From the KBP 2012 SF corpus, we annotat-ed 2,806 sentences in formal writing from newsreports as the gold-standard trigger scoping dataset.
We randomly partitioned the labeled data andperformed ten-fold cross-validation using LIBSVMtoolkit (Chang and Lin, 2011).
We employ theclassification model trained from all the labeled sen-tences to classify tokens in the unlabeled sentences.3.2 Results3.2.1 Scope IdentificationThe scope identification evaluation results of therule-based method and the SVMs with the RBFkernel are presented in Table 2.
We can see thatthe supervised classification method performs bettersince it incorporates the weights of different featuresrather than simply applying hard constraints.
Inaddition, it allows the answers to appear before atrigger as shown in the following sentence.
Our rule-based method fails to extract Fred since it appearsbefore the trigger married:She was a part of a group of black intellectualswho included philosopher and poet [Fred Clifton,whom she <married> in 1958].Fact GroupAccuracy (%) F-score (%)Rule SVMs Rule SVMsBirth 85.97 96.66 80.01 94.21Death 92.31 94.56 82.16 89.01Residence 90.67 95.67 76.11 83.25Family 92.49 94.11 75.30 77.31Education 91.51 93.87 88.46 90.65Table 2: Scope identification results.3.2.2 Biographical Fact ExtractionThe fact extraction results in Table 1 demonstrateour trigger scoping strategy can outperform state-of-the-art methods.
For a certain fact type, we choosethe SF system which has the best performancefor comparison.
Specifically, we compare withtwo successful approaches: (1) the combination ofdistant supervision and rules (e.g., (Grishman, 2013;Roth et al, 2013)); (2) patterns based on dependencypaths (e.g., (Li et al, 2013; Yu et al, 2013)).The advantage of our method lies in trigger-driven1206exploration.
The positions of facts in the sentencecan be very flexible and therefore difficult to becaptured using a limited number of patterns.
Forexample, the patterns in table 23fail to extractJames in Figure 1.
However, the ways in which weexpress the trigger and words it dominated tend tobe relatively fixed.
For example, all the followingpatterns contain a fact-specific trigger and also factsusually appear within its scope.PER:SIBLING[Q] poss?1brother appos [A][Q] appos?1brother appos [A][Q] appos brother appos-1 [A][Q] nsubjpass?1survived agent brother appos [A][Q] poss?1sister appos [A][Q] appos?1sister appos [A][Q] appos sister appos?1[A][Q] nsubjpass?1survived agent sister appos [A]Table 3: Patterns used for extracting sibling facts (Li etal., 2013).
Q: Query, A: Answer.The limitation of our method is that we assumea sentence centers around only one person thusevery biographical fact mentioned should be relatedto the centroid person.
For example, our methodmistakenly extracted February as the death-date factfor both Reina and Orlando in the following case.Also at the mass was Reina Tamayo, the motherof Orlando Zapata, who [<died> in February]after an 85-day hunger strike to protest the fate ofpolitical prisoners here.In order to solve this problem, we need to furtheranalyze the relation between the query entity men-tion and the trigger so that we can identify OrlandoZapata is irrelevant to the death-related fact.4 Related WorkPrevious successful approaches to construct the bio-graphical knowledge base are relatively expensive:Distant Supervision (Surdeanu et al, 2010) re-lies upon external knowledge bases and it is time-consuming to manually write or edit patterns (Sunet al, 2011; Li et al, 2012).
The main impact ofour trigger scoping strategy is to narrow down thetext span of searching for facts, from sentence-level3A poss?1B means there is a possession modifier relation(poss) between B and A.to fragment-level.
We only focus on analyzing thecontent which is likely to contain an answer.Our trigger scoping method is also partially in-spired from the negation scope detection work (e.g.,(Szarvas et al, 2008; Elkin et al, 2005; Chapman etal., 2001; Morante and Daelemans, 2009; Agarwaland Yu, 2010)) and reference scope identification inciting sentences (Abu-Jbara and Radev, 2011; Abu-Jbara and Radev, 2012).5 Conclusions and Future WorkIn this paper we explore the role of triggers and theirscopes in biographical fact extraction.
We imple-ment the trigger scoping strategy using two simplebut effective methods.
Experiments demonstratethat our approach outperforms state-of-the-art with-out any syntactic analysis and external knowledgebases.In the future, we will aim to explore how togenerate a trigger list for a ?surprise?
new fact typewithin limited time.AcknowledgementThis work was supported by the U.S. DARPA AwardNo.
FA8750-13-2-0045 in the Deep Explorationand Filtering of Text (DEFT) Program, the U.S.Army Research Laboratory under Cooperative A-greement No.
W911NF-09-2-0053 (NS-CTA), U.S.NSF CAREER Award under Grant IIS-0953149,U.S.
AFRL DREAM project, IBM Faculty Award,Google Research Award, Disney Research Award,Bosch Research Award, and RPI faculty start-upgrant.
The views and conclusions contained in thisdocument are those of the authors and should notbe interpreted as representing the official policies,either expressed or implied, of the U.S. Government.The U.S. Government is authorized to reproduceand distribute reprints for Government purposesnotwithstanding any copyright notation here on.ReferencesA.
Abu-Jbara and D. Radev.
2011.
Coherent citation-based summarization of scientific papers.
In Proc.
As-sociation for Computational Linguistics (ACL2011).Association for Computational Linguistics.A.
Abu-Jbara and D. Radev.
2012.
Reference scopeidentification in citing sentences.
In Proc.
Human1207Language Technologies conference - North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL 2012).S.
Agarwal and H. Yu.
2010.
Biomedical negation scopedetection with conditional random fields.
Journalof the American medical informatics association,17(6):696?701.J.
Aguilar, C. Beller, P. McNamee, and B.
Van Durme.2014.
A comparison of the events and relations acrossace, ere, tac-kbp, and framenet annotation standards.ACL 2014 Workshop on Events.Arnold.
1999.
Skimming and scanning.
In Reading andStudy Skills Lab.C.
Chang and C. Lin.
2011.
Libsvm: a library for supportvector machines.
ACM Transactions on IntelligentSystems and Technology (TIST), 2(3):27.W.
Chapman, W. Bridewell, P. Hanbury, G. Cooper,and B. Buchanan.
2001.
A simple algorithm foridentifying negated findings and diseases in dischargesummaries.
Journal of biomedical informatics,34(5):301?310.Z.
Chen, S. Tamang, A. Lee, X. Li, W. Lin, M. Snover,J.
Artiles, M. Passantino, and H. Ji.
2010.
Cuny-blender tac-kbp2010 entity linking and slot fillingsystem description.
In Proc.
Text Analysis Conference(TAC 2012).D.
Douglas and S. Frazier.
2001.
Teaching by principles:An interactive approach to language pedagogy (2nded.).
TESOL Quarterly, 35(2):341?342.P.
Elkin, S. Brown, B. Bauer, C. Husser, W. Carruth,L.
Bergstrom, and D. Wahner-Roedler.
2005.
Acontrolled trial of automated classification of negationfrom clinical notes.
BMC medical informatics anddecision making, 5(1):13.R.
Grishman.
2013.
Off to a cold start: New yorkuniversitys 2013 knowledge base population systems.In Proc.
Text Analysis Conference (TAC 2013).H.
Ji, R. Grishman, H. Dang, K. Griffitt, and J. Ellis.2010.
Overview of the tac 2010 knowledge basepopulation track.
In Proc.
Text Analysis Conference(TAC 2010).H.
Ji, R. Grishman, and H. Dang.
2011.
An overview ofthe tac2011 knowledge base population track.
In Proc.Text Analysis Conference (TAC 2011).Y.
Li, S. Chen, Z. Zhou, J. Yin, H. Luo, L. Hong, W. Xu,G.
Chen, and J. Guo.
2012.
Pris at tac2012 kbp track.In Proc.
of Text Analysis Conference (TAC 2012).Y.
Li, Y. Zhang, D. Li, X. Tong, J. Wang, N. Zuo,Y.
Wang, W. Xu, G. Chen, and J. Guo.
2013.
Pris attac2013 kbp track.
In Proc.
Text Analysis Conference(TAC 2013).C.
Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,and D. McClosky.
2014.
The Stanford CoreNLP nat-ural language processing toolkit.
In Proc.
Associationfor Computational Linguistics (ACL2014).B.
Min, X. Li, R. Grishman, and A.
Sun.
2012.
Newyork university 2012 system for kbp slot filling.
Proc.Text Analysis Conference (TAC 2012).R.
Morante and W. Daelemans.
2009.
Learning thescope of hedge cues in biomedical texts.
In Proc.ACL 2009 Workshop on Current Trends in BiomedicalNatural Language Processing.K.
Motallebzadeh and N. Mamdoohi.
2011.
Languagelearning strategies: A key factor to improvementof toefl candidates reading comprehension ability.International Journal of Linguistics, 3(1):E26.R.
Phipps.
1983.
The Successful Student?s handbook: AStep-By-Step Guide to Study, Reading, and ThinkingSkills.
Seattle and London: University of WashingtonPress.B.
Roth, T. Barth, M. Wiegand, M. Singh, and D. Klakow.2013.
Effective slot filling based on shallow distantsupervision methods.
Proc.
Text Analysis Conference(TAC 2013).A.
Sun, R. Grishman, W. Xu, and B. Min.
2011.
Newyork university 2011 system for kbp slot filling.
InProc.
Text Analysis Conference (TAC 2011).M.
Surdeanu and H. Ji.
2014.
Overview of theenglish slot filling track at the tac2014 knowledgebase population evaluation.
In Proc.
Text AnalysisConference (TAC2014).M.
Surdeanu, D. McClosky, J. Tibshirani, J. Bauer,A.
Chang, V. Spitkovsky, and C. Manning.
2010.
Asimple distant supervision approach for the tac-kbpslot filling task.
In Proc.
Text Analysis Conference(TAC 2010).M.
Surdeanu.
2013.
Overview of the tac2013knowledge base population evaluation: English slotfilling and temporal slot filling.
In Proc.
Text AnalysisConference (TAC 2013).G.
Szarvas, V. Vincze, R. Farkas, and J. Csirik.2008.
The bioscope corpus: annotation for negation,uncertainty and their scope in biomedical texts.
InProc.
ACL Workshop on Current Trends in BiomedicalNatural Language Processing.D.
Yu, H. Li, T. Cassidy, Q. Li, H. Huang, Z. Chen,H.
Ji, Y. Zhang, and D. Roth.
2013.
Rpi-blender tac-kbp2013 knowledge base population system.
In Proc.Text Analysis Conference (TAC 2013).1208
