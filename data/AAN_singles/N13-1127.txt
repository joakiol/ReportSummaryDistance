Proceedings of NAACL-HLT 2013, pages 1072?1081,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsEmergence of Gricean Maxims from Multi-Agent Decision TheoryAdam Vogel, Max Bodoia, Christopher Potts, and Dan JurafskyStanford UniversityStanford, CA, USA{acvogel,mbodoia,cgpotts,jurafsky}@stanford.eduAbstractGrice characterized communication in termsof the cooperative principle, which enjoinsspeakers to make only contributions that servethe evolving conversational goals.
We showthat the cooperative principle and the associ-ated maxims of relevance, quality, and quan-tity emerge from multi-agent decision theory.We utilize the Decentralized Partially Observ-able Markov Decision Process (Dec-POMDP)model of multi-agent decision making whichrelies only on basic definitions of rationalityand the ability of agents to reason about eachother?s beliefs in maximizing joint utility.
Ourmodel uses cognitively-inspired heuristics tosimplify the otherwise intractable task of rea-soning jointly about actions, the environment,and the nested beliefs of other actors.
Ourexperiments on a cooperative language taskshow that reasoning about others?
belief states,and the resulting emergent Gricean commu-nicative behavior, leads to significantly im-proved task performance.1 IntroductionGrice (1975) famously characterized communica-tion among rational agents in terms of an overarch-ing cooperative principle and a set of more specificmaxims, which enjoin speakers to make contribu-tions that are truthful, informative, relevant, clear,and concise.
Since then, there have been many at-tempts to derive the maxims (or perhaps just their ef-fects) from more basic cognitive principles concern-ing how people make decisions, formulate plans,and collaborate to achieve goals.
This researchtraces to early work by Lewis (1969) on signalingsystems.
It has recently been the subject of ex-tensive theoretical discussion (Clark, 1996; Merin,1997; Blutner, 1998; Parikh, 2001; Beaver, 2002;van Rooy, 2003; Benz et al 2005; Franke, 2009)and has been tested experimentally using one-stepgames in which the speaker produces a message andthe hearer ventures a guess as to its intended refer-ent (Rosenberg and Cohen, 1964; Dale and Reiter,1995; Golland et al 2010; Stiller et al 2011; Frankand Goodman, 2012; Krahmer and van Deemter,2012; Degen and Franke, 2012; Rohde et al 2012).To date, however, these theoretical models and ex-periments have not been extended to multi-step in-teractions extending over time and involving bothlanguage and action together, which leaves this workrelatively disconnected from research on planningand goal-orientation in artificial agents (Perraultand Allen, 1980; Allen, 1991; Grosz and Sidner,1986; Bratman, 1987; Hobbs et al 1993; Allenet al 2007; DeVault et al 2005; Stone et al2007; DeVault, 2008).
We attribute this in largepart to the complexity of Gricean reasoning itself,which requires agents to model each other?s beliefstates.
Tracking these as they evolve over time in re-sponse to experiences is extremely demanding.
Ourapproach complements slot-filling dialog systems,where the focus is on managing speech recogni-tion uncertainty (Young et al 2010; Thomson andYoung, 2010).However, recent years have seen significant ad-vances in multi-agent decision-theoretic models andtheir efficient implementation.
With the current pa-per, we seek to show that the Decentralized Par-1072tially Observable Markov Decision Process (Dec-POMDP) provides a robust, flexible foundation forimplementing agents that communicate in a Griceanmanner.
Dec-POMDPs are multi-agent, partially-observable models in which agents maintain be-lief distributions over the underlying, hidden worldstate, including the beliefs of the other players, andspeech actions change those beliefs.
In this setting,informative, relevant communication emerges as thebest way to maximize joint utility.The complexity of pragmatic reasoning is stillforbidding, though.
Correspondingly, optimal de-cision making in Dec-POMDPs is NEXP complete(Bernstein et al 2002).
To manage this issue, weintroduce several cognitively-plausible approxima-tions which allow us to simplify the Dec-POMDP toa single-agent POMDP, for which relatively efficientsolvers exist (Spaan and Vlassis, 2005).
We demon-strate our algorithms on a variation of the Cards task,a partially-observable collaborative search problem(Potts, 2012).
Spatial language comprises the bulkof communication in the Cards task, and we dis-cuss a model of spatial semantics in Section 3.
Us-ing this task and a model of the meaning of spatiallanguage, we next discuss two agents that play thegame: ListenerBot (Section 4) makes decisions us-ing a single-agent POMDP that does not take intoaccount the beliefs or actions of its partner, whereasDialogBot (Section 5) maintains a model of its part-ner?s beliefs.
As a result of the cooperative structureof the underlying model and the effects of commu-nication within it, DialogBot?s contributions are rel-evant, truthful, and informative, which leads to sig-nificantly improved task performance.2 The Cards Task and CorpusThe Cards corpus consists of 1,266 transcripts1 froman online, two-person collaborative game in whichtwo players explore a maze-like environment, com-municating with each other via a text chat window(Figure 1).
A deck of playing cards has been dis-tributed randomly around the environment, and theplayers?
task is to find six consecutive cards of thesame suit.
Our implemented agents solve a sim-plified version of this task in which the two agents1Released by Potts (2012) at http://cardscorpus.christopherpotts.netFigure 1: The Cards corpus gameboard.
Player 1?slocation is marked ?P1?.
The nearby yellow boxesmark card locations.
The dialogue history and chatwindow are at the top.
This board, the one we usethroughout, consists of 231 open grid squares.must both end up co-located with a single card, theAce of Spades (AS).
This is much simpler than thesix-card version from the human?human corpus, butit involves the same kind of collaborative goal andforces our agents to deal with the same kind of par-tial knowledge about the world as the humans did.Each agent knows its own location, but not his part-ner?s, and a player can see the AS only when co-located with it.
The agents use (simplified) Englishto communicate with each other.3 Spatial SemanticsMuch of the communication in the Cards task in-volves referring to spatial locations on the board.Accordingly, we focus on spatial language for ourartificial agents.
In this section, we present a modelof spatial semantics, which we create by leveragingthe human?human Cards transcripts.
We discuss thespatial semantic representation, how we classify thesemantics of new locative expressions, and our useof spatial semantics to form a high-level state spacefor decision making.3.1 Semantic RepresentationPotts (2012) released annotations, derived from theCards corpus, which reduce 599 of the players?statements about their locations to formulae of theform ?
(?1 ?
??
?
?
?k), where ?
is a domain and?1, .
.
.
,?k are semantic literals.
For example, the ut-terance ?
(I?m) at the top right of the board?
is anno-tated as BOARD(top?
right), and ?
(I?m) in bottom1073of the C room?
is annotated as C room(bottom).
Ta-ble 1 lists the full set of semantic primitives that ap-pear as domain expressions and literals.Because the Cards transcripts are so highly struc-tured, we can interpret these expressions in termsof the Cards world itself.
For a given formula?
= ?
(?1 ?
??
?
?
?k), we compute the number oftimes that a player identified its location with (anutterance translated as) ?
while standing on gridsquare (x,y).
These counts are smoothed usinga simple 2D-smoothing scheme, detailed in (Potts,2012), and normalized in the usual manner to form adistribution over board squares Pr((x,y)|?).
Thesegrounded interpretations are the basis for commu-nication between the artificial agents we define inSection 4.BOARD, SQUARE, right, middle, top, left, bot-tom, corner, approx, precise, entrance, C room,hall, room, sideways C, loop, reverse C,U room, T room, deadend, wall, sideways FTable 1: The spatial semantic primitives.3.2 Semantics ClassifierUsing the corpus examples of utterances paired withtheir spatial semantic representations, we learn a setof classifiers to predict a spatial utterance?s semanticrepresentation.
We train a binary classifier for eachsemantic primitive ?i using a log-linear model withsimple bag of words features.
The words are notnormalized or stemmed and we use whitespace tok-enization.
We additionally train a multi-class clas-sifier for all possible domains ?
.
At test time, weuse the domain classifier and each primitive binaryclassifier to produce a semantic representation.3.3 Semantic State SpaceThe decision making algorithms that we discuss inSection 4 are highly sensitive to the size of the statespace.
The full representation of the game boardconsists of 231 squares.
Representing the locationof both players and the location of the card requires3233 = 12,326,391 states, well beyond the capabil-ities of current decision-making algorithms.To ameliorate this difficulty, we cluster squarestogether using the spatial referring expression cor-Figure 2: Semantic state space clusters with k = 16.pus.
This approach follows from research thatshows that humans?
mental spatial representationsare influenced by their language (Hayward and Tarr,1995).
Our intuition is that human players do notconsider all possible locations of the card and play-ers, but instead lump them into semantically coher-ent states, such as ?the card is in the top right cor-ner.?
Following this intuition, we cluster states to-gether which have similar referring expressions, al-lowing our agents to use language as a cognitivetechnology and not just a tool for communication.For each board square (x,y) we form a vector?
(x,y) with ?i(x,y) = Pr((x,y)|?i), where ?i is theith distinct semantic representation in the corpus.This forms a 136-dimensional vector for each boardsquare.
We then use k-means clustering with a Eu-clidean distance metric in this semantic space tocluster states which are referred to similarly.Figure 2 shows a clustering for k = 16 which weutilize for the remainder of the paper.
Denoting theboard regions by {1, .
.
.
,Nregions}, we compute theprobability of an expression ?
referring to a regionr by averaging over the squares in the region:Pr(r|?i) ?
?(x,y)?
region rPr((x,y)|?i)|{(x,y)|(x,y) ?
region r}|4 ListenerBotWe first introduce ListenerBot, an agent that doesnot take into account the actions or beliefs of itspartner.
ListenerBot decides what actions to takeusing a Partially Observable Markov Decision Pro-cess (POMDP).
This allows ListenerBot to track itsbeliefs about the location of the card and to incor-porate linguistic advice.
However, ListenerBot doesnot produce utterances.1074A POMDP is defined by a tuple(S,A,T,O,?,R,b0,?).
We explicate each com-ponent with examples from our task.
Figure 3(a)provides the POMDP influence diagram.States S is the finite state space of the world.
Thestate space S of ListenerBot consists of the locationof the player p and the location of the card c. Asdiscussed above in Section 3.3, we cluster squares ofthe board into Nregions semantically coherent regions,denoted by {1, .
.
.
,Nregions}.
The state space overthese regions is defined asS := {(p,c)|p,c ?
{1, .
.
.
,Nregions}}Two regions r1 and r2 are called adjacent, writtenadj(r1,r2), if any of their constituent squares touch.Actions A is the set of actions available to theagent.
ListenerBot can only take physical actionsand has no communicative ability.
Physical actionsin our region-based state space are composed of twotypes: traveling to a region and searching a region.?
travel(r): travel to region r?
search: player exhaustively searches the cur-rent regionTransition Distributions The transition distribu-tion T (s?|a,s) models the dynamics of the world.This represents the ramifications of physical actionssuch as moving around the map.
For a state s =(p,c) and action a = travel(r), the player moves toregion r if it is adjacent to p, and otherwise stays inthe same place:T ((p?,c?)|travel(r),(p,c))=??????????????
?1 adj(r, p)?
p?
= r?c = c?1 ?adj(r, p)?
p = p?
?c = c?0 otherwiseSearch actions are only concerned with observationsand do not change the state of the world:2T ((p?,c?
)|search,(p,c)) = 1[p?
= p?
c?
= c]The travel and search high-level actions are trans-lated into low-level (up, down, left, right) actionsusing a simple A?
path planner.Observations Agents receive observations froma set O according to an observation distribution21[Q] is the indicator function, which is 1 if proposition Qis true and 0 otherwise.?(o|s?,a).
Observations include properties of thephysical world, such as the location of the card, andalso natural language utterances, which serve to in-directly change agents?
beliefs about the world andthe beliefs of their interlocutors.Search actions generate two possible observa-tions: ohere and o?here, which denote the presenceor absence of the card from the current region.?(ohere|(p?,c?
),search) = 1[p?
= c?]?(o?here|(p?,c?
),search) = 1[p?
6= c?
]Travel actions do not generate meaningful observa-tions:?(o?here|(p?,c?
), travel) = 1Linguistic Advice We model linguistic advice asanother form of observation.
Agents receive mes-sages from a finite set ?, and each message ?
?
?has a semantics, or distribution over the state spacePr(s|?).
In the Cards task, we use the semantic dis-tributions defined in Section 3.
To combine the se-mantics of language with the standard POMDP ob-servation model, we apply Bayes?
rule:Pr(?
|s) = Pr(s|?)Pr(?)??
?
Pr(s|?
?)Pr(?
?
)(1)The prior, Pr(?
), can be derived from corpus data.By treating language as just another form of ob-servation, we are able to leverage existing POMDPsolution algorithms.
This approach contrasts withprevious work on communication in Dec-POMDPs,where agents directly share their perceptual obser-vations (Pynadath and Tambe, 2002; Spaan et al2008), an assumption which does not fit natural lan-guage.Reward The reward function R(s,a) : S?
R rep-resents the goals of the agent, who chooses actionsto maximize reward.
The goal of the Cards task isfor both players to be on top of the card, so any ac-tion that leads to this state receives a high reward R+.All other actions receive a small negative reward R?,which gives agents an incentive to finish the task asquickly as possible.R((p,c),a) ={R+ p = cR?
p 6= cLastly, ?
?
[0,1) is the discount factor, specifyingthe trade-off between immediate and future rewards.1075s s?o o?aR(a) ListenerBot POMDPs s?o1 o?1o2 o?2a1a2R(b) Full Dec-POMDPs s?o o?aRs?
s??
(c) DialogBot POMDPFigure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DialogBot ap-proximation POMDP.
The ListenerBot (a) only considers his own location p and the card location c. In thefull Dec-POMDP (b), both agents receive individual observations and choose actions independently.
Opti-mal decision making requires tracking all possible histories of beliefs of the other agent.
In diagram (c), Di-alogBot approximates the full Dec-POMDP as single-agent POMDP.
At each time step, DialogBot marginal-izes out the possible observations o?
that ListenerBot received, yielding an expected belief state b?.Initial Belief State The initial belief state, b0 ??
(S), is a distribution over the state space S. Lis-tenerBot begins each game with a known initial lo-cation p0 but a uniform distribution over the locationof the card c:b0(p,c) ?
{1Nregionsp = p00 otherwiseBelief Update and Decision Making The key de-cision making problem in POMDPs is the construc-tion of a policy pi : ?(S)?
A, a function from beliefsto actions which dictates how the agent acts.
Deci-sion making in POMDPs proceeds as follows.
Theworld starts in a hidden state s0 ?
b0.
The agentexecutes action a0 = pi(b0).
The underlying hid-den world state transitions to s1 ?
T (s?|a0,s0), theworld generates observation o0 ?
?
(o|s1,a0), andthe agent receives reward R(s0,a0).
Using the obser-vation o0, the agent constructs a new belief b1 ??
(S)using Bayes?
rule:bat ,ott+1 (s?)
= Pr(s?|at ,ot ,bt)=Pr(ot |at ,s?,bt)Pr(s?|at ,bt)Pr(ot |bt ,at)=?
(ot |s?,at)?s?S T (s?|at ,s)bt(s)?s???
(ot |s?
?,at)?s?S T (s?
?|at ,s)bt(s)This process is referred to as belief update and isanalogous to the forward algorithm in HMMs.
To in-corporate communication into the standard POMDPmodel, we consider observations (o,?)
?
O ?
?which are a combination of a perceptual observationo and a received message ?
.
The semantics of themessage ?
is included in the belief update equationusing Pr(s|?
), derived in Equation 1:bat ,ot ,?tt+1 (s?)
=?
(o|s?,a) Pr(s?|?)Pr(?)??
???
Pr(s?|?
?)Pr(?
?)
?s?S T (s?|a,s)bt(s)?s???S?(o|s??,a)Pr(s??|?)Pr(?)??
???
Pr(s??|?
?)Pr(?
?)
?s?S T (s?
?|a,s)bt(s)Using this new belief state b1, the agent selects anaction a1 = pi(b1), and the process continues.An initial belief state b0 and a policy pi to-gether define a Markov chain over pairs of statesand actions.
For a given policy pi , we define avalue function V pi : ?(S)?
R which represents theexpected discounted reward with respect to thatMarkov chain:V pi(b0) =??t=0?
t E[R(bt ,at)|b0,pi]The goal of the agent is find a policy pi?
which max-imizes the value of the initial belief state:pi?
= argmaxpiV pi(b0)Exact computation of pi?
is PSPACE-complete (Pa-padimitriou and Tsitsiklis, 1987), making approx-imation algorithms necessary for all but the sim-plest problems.
We use Perseus (Spaan and Vlassis,2005), an anytime approximate point-based value it-1076eration algorithm.5 DialogBotWe now introduce DialogBot, a Cards agent whichis capable of producing linguistic advice.
To decidewhen and how to speak, DialogBot maintains a dis-tribution over its partner?s beliefs and reasons aboutthe effects his utterances will have on those beliefs.To handle these complexities, DialogBot modelsthe world as a Decentralized Partially ObservableMarkov Decision Process (Dec-POMDP) (Bernsteinet al 2002).
See Figure 3(b) for the influence dia-gram.
The definition of Dec-POMDPs mirrors thatof the POMDP, with the following changes.There is a finite set I of agents, which we re-strict to two.
Each agent takes an action ai ateach time step, forming a joint action ~a = (a1,a2).Each agent receives its own observation oi accord-ing to ?(o1,o2|a1,a2,s?).
The transition distribu-tions T (s?|a1,a2,s) and the reward R(s,a1,a2) bothdepend on both agents?
actions.Optimal decision making in Dec-POMDPs re-quires maintaining a probability distribution overall possible sequences of actions and observations(a?1, o?1, .
.
.
, a?t , o?t) that the other player might havereceived.
As t increases, we have an exponential in-crease in the belief states an agent must consider.Confirming this informal intuition, decision mak-ing in Dec-POMDPs is NEXP-complete, a complex-ity class above P-SPACE (Bernstein et al 2002).This computational complexity limits the applica-tion of Dec-POMDPs to very small problems.
Toaddress this difficulty we make several simplifyingassumptions, allowing us to construct a single-agentPOMDP which approximates the full Dec-POMDP.Firstly, we assume that other agents do not takeinto account our own beliefs, i.e., the other agentacts like a ListenerBot.
This bypasses the infinitelynested belief problem by assuming that other agentstrack one less level of nested beliefs, a commonapproach (Goodman and Stuhlmu?ller, 2012; Gmy-trasiewicz and Doshi, 2005).Secondly, instead of tracking the full tree of pos-sible observation histories, we maintain a point es-timate b?
of the other agent?s beliefs, which weterm the expected belief state.
Rather than track-ing each possible observation/action history of theother agent, at each time step we marginalize outthe observations they could have received.
Figure 4compares this approach with exact belief update.Thirdly, we assume that the other agent acts ac-cording to a variant of the QMDP approximation(Littman et al 1995).
Under this approximation, theother agent solves a fully-observable MDP versionof the ListenerBot POMDP, yielding an MDP pol-icy p?i : S?
A.
This critically allows us to approxi-mate the other agent?s belief update using a speciallyformed POMDP, which we detail next.State Space To construct the approximate single-agent POMDP from the full Dec-POMDP problem,we formulate the state space as S?
S. (See Figure3(c) for the influence diagram.)
We write a state(s, s?)
?
S?
S, where s is DialogBot?s beliefs aboutthe true state of the world, and s?
is DialogBot?s esti-mate of the other agent?s beliefs.Transition Distribution The main difficultyin constructing the approximate single-agentPOMDP is specifying the transition distribu-tion T ((s?, s??
)|a,(s, s?)).
To address this, webreak this distribution into two components:T ((s?, s??
)|a,(s, s?))
= T?
(s?
?|s?,a,(s, s?
))T (s?|a,s, s?
).The first term dictates how DialogBot updates itsbeliefs about the other agent?s beliefs:T?
(s?
?|s?,a,(s, s?))
= Pr(s?
?|s?,a,(s, s?))=?o??OPr(s?
?|a, o?, s?,s)Pr(o?|s?,a, p?i(s?))=?o??O(?(o?|s?
?,a, p?i(s?
))T (s?
?|a, p?i(s?
), s?)?s????(o?|s??
?,a, p?i(s?
))T (s??
?|a, p?i(s?
), s?)??
(o?|s?,a, p?i(s?
)))We sum over all observations o?
the other agent couldhave received, updating our probability of s??
as Lis-tenerBot would have, multiplied by the probabilitythat ListenerBot would have received that observa-tion, ?
(o?|s?, p?i(s?)).
The QMDP approximation al-lows us to simulate ListenerBot?s belief update inT?
(s?
?|s?,a,(s, s?)).
Exact belief update would requireaccess to b?
: by using p?i(s?)
we can estimate the actionthat ListenerBot would have taken.In cases where s?
contradicts s such that for all o?
ei-ther ?
(o?|s?, p?i(s?))
= 0 or ?(o?|s?
?, p?i(s?))
= 0, we redis-tribute the belief mass uniformly: T?
(s?
?|s?,a,(s, s?
))?1077b?tb?o1t+1o1b?o2t+1o2b?o1,o1t+2o1b?o1,o2t+2o2b?o2,o1t+2o1b?o2,o2t+2o2(a) Exact multi-agent belief trackingb?to1o2o1o2b?t+1o1o2o1o2b?t+2(b) Approximate multi-agent belief trackingFigure 4: Exact multi-agent belief tracking compared with our approximate approach.
Each node representsa belief state.
In exact tracking (a), the agent tracks every possible history of observations that its partnercould have received, which grows exponentially in time.
In approximate update (b), the agent considers eachpossible observation and then averages the resulting belief states, weighted by the probability the other agentreceived that observation, resulting in a single summary belief state b?t+1.
Under the QMDP approximation,the agent considers what action the other agent would have taken if it completely believed the world was ina certain state.
Thus, there are four belief states resulting from b?t , as opposed to two in the exact case.1 ?s??
6= s?.
This approach to managing contradictionis analogous to logical belief revision (Alchourrono?net al 1985; Ga?rdenfors, 1988; Ferme?
and Hansson,2011).Speech Actions Speech actions are modeled byhow they change the beliefs of the other agent.The effects of a speech actions are modeled inT?
(s?
?|s?,a,(s, s?
)), our model of how ListenerBot?s be-liefs change.
For a speech action a = say(?)
with?
?
?,T?
(s?
?|s?,a,(s, s?))
=?o??O(?(o?|s?
?,a, p?i(s?))Pr(?
|s??
)T (s?
?|a, p?i(s?
), s?)?s????(o?|s??
?,a, p?i(s?))Pr(?
|s???
)T (s??
?|a, p?i(s?
), s?)??
(o?|s?,a, p?i(s?
)))DialogBot is equipped with the five mostfrequent speech actions: BOARD(middle),BOARD(top), BOARD(bottom), BOARD(left),and BOARD(right).
It produces concrete utterancesby selecting a sentence from the training corpuswith the desired semantics.Reward DialogBot receives a large reward whenboth it and its partner are located on the card, and anegative cost when moving or speaking:R((p,c, p?, c?
),a) ={R+ p = c?
p?
= cR?
p 6= c?
p?
6= cDialogBot?s reward is not dependent on the beliefsof the other player, only the true underlying state ofthe world.6 Experimental ResultsWe now experimentally evaluate our semantic clas-sifiers and the agents?
task performance.6.1 Spatial Semantics ClassifiersWe report the performance of our spatial seman-tics classifiers, although their accuracy is not the fo-cus of this paper.
We use 10-fold cross validationon a corpus of 577 annotated utterances.
We usedsimple bag-of-words features, so overfitting the datawith cross validation is not a pressing concern.
Ofthe 577 utterances, our classifiers perfectly labeled325 (56.3% accuracy).
The classifiers correctly pre-dicted the domain ?
of 515 (89.3%) utterances.
The1078precision of our binary semantic primitive classifierswas 9691126 = .861 and recall9691242 = .780, yielding F1measure .818.6.2 Cards Task EvaluationWe evaluated our ListenerBot and DialogBot agentsin the Cards task.
Using 500 randomly generatedinitial player and card locations, we tested eachcombination of ListenerBot and DialogBot partners.Agents succeeded at a given initial position if theyboth reached the card within 50 moves.
Table 2shows how many trials each dyad won and howmany high-level actions they took to do so.Agents % Success MovesLB & LB 84.4% 19.8LB & DB 87.2% 17.5DB & DB 90.6% 16.6Table 2: The evaluation for each combination ofagents.
LB = ListenerBot; DB = DialogBot.Collaborating DialogBots performed the best,completing more trials and using fewer moves thanthe ListenerBots.
The DialogBots initially explorethe space in a similar manner to the ListenerBots,but then share card location information.
This leadsto shorter interactions, as once the DialogBot findsthe card, the other player can find it more quickly.In the combination of ListenerBot and DialogBot,we see about half of the improvement over two Lis-tenerBots.
Roughly 50% of the time, the Listener-Bot finds the card first, which doesn?t help the Di-alogBot find the card any faster.7 Emergent PragmaticsGrice?s original model of pragmatics (Grice, 1975)involves the cooperative principle and four maxims:quality (?say only what you know to be true?
), rela-tion (?be relevant?
), quantity (?be as informative asis required; do not say more than is required?
), andmanner (roughly, be clear and concise).In most interactions, DialogBot searches for thecard and then reports its location to the other agent.These reports obey quality in that they are made onlywhen based on actual observations.
The behavioris not hard-coded, but rather emerges, because onlyaccurate information serves the agents?
goals.
Incontrast, sub-optimal policies generated early in thePOMDP solving process sometimes lie about cardlocations.
Since this behavior confuses the otheragent and thus has a lower utility, it gets replacedby truthful communication as the policies improve.We also capture the effects of relation and the firstclause of quantity, because the nature of the rewardfunction and the nested belief structures ensure thatDialogBot offers only relevant, informative informa-tion.
For instance, when DialogBot finds the card inthe lower left corner, it alternates saying ?left?
and?bottom?, effectively overcoming its limited gener-ation capabilities.
Again, early sub-optimal policiessometimes do not report the location of the card atall, thereby failing to fulfill these maxims.We expect these models to produce behavior con-sistent with manner and the second clause of quan-tity, but evaluating this claim will require a richer ex-perimental paradigm.
For example, if DialogBot hada larger and more structured vocabulary, it wouldhave to choose between levels of specificity as wellas more or less economical forms.8 ConclusionWe have shown that cooperative pragmatic behaviorcan arise from multi-agent decision-theoretic mod-els in which the agents share a joint utility func-tion and reason about each other?s belief states.Decision-making in these models is intractable,which has been a major obstacle to achieving exper-imental results in this area.
We introduced a seriesof approximations to manage this intractability: (i)combining low-level states into semantically coher-ent high-level ones; (ii) tracking only an averagedsummary of the other agent?s potential beliefs; (iii)limiting belief state nesting to one level, and (iv)simplifying each agent?s model of the other?s be-liefs so as to reduce uncertainty.
These approxima-tions bring the problems under sufficient control thatthey can be solved with current POMDP approxi-mation algorithms.
Our experimental results high-light the rich pragmatic behavior this gives rise toand quantify the communicative value of such be-havior.
While there remain insights from earlier the-oretical proposals and logic-based methods that wehave not fully captured, our current results support1079the notion that probabilistic decision-making meth-ods can yield robust, widely applicable models thataddress the real-world difficulties of partial observ-ability and uncertainty.AcknowledgmentsThis research was supported in part by ONRgrants N00014-10-1-0109 and N00014-13-1-0287and ARO grant W911NF-07-1-0216.ReferencesCarlos E. Alchourrono?n, Peter Ga?rdenfors, and DavidMakinson.
1985.
On the logic of theory change: Par-tial meets contradiction and revision functions.
Jour-nal of Symbolic Logic, 50(2):510?530.James F. Allen, Nathanael Chambers, George Ferguson,Lucian Galescu, Hyuckchul Jung, Mary Swift, andWilliam Taysom.
2007.
PLOW: A collaborativetask learning agent.
In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence,pages 1514?1519.
AAAI Press, Vancouver, BritishColumbia, Canada.James F. Allen.
1991.
Reasoning About Plans.
MorganKaufmann, San Francisco.David Beaver.
2002.
Pragmatics, and that?s an order.
InDavid Barker-Plummer, David Beaver, Johan van Ben-them, and Patrick Scotto di Luzio, editors, Logic, Lan-guage, and Visual Information, pages 192?215.
CSLI,Stanford, CA.Anton Benz, Gerhard Ja?ger, and Robert van Rooij, edi-tors.
2005.
Game Theory and Pragmatics.
PalgraveMcMillan, Basingstoke, Hampshire.Daniel S. Bernstein, Robert Givan, Neil Immerman, andShlomo Zilberstein.
2002.
The complexity of decen-tralized control of Markov decision processes.
Mathe-matics of Operations Research, 27(4):819?840.Reinhard Blutner.
1998.
Lexical pragmatics.
Journal ofSemantics, 15(2):115?162.Michael Bratman.
1987.
Intentions, Plans, and PracticalReason.
Harvard University Press.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press, Cambridge.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Judith Degen and Michael Franke.
2012.
Optimal rea-soning about referential expressions.
In Proceedingsof SemDIAL 2012, Paris, September.David DeVault, Natalia Kariaeva, Anubha Kothari, IrisOved, and Matthew Stone.
2005.
An information-state approach to collaborative reference.
In Proceed-ings of the ACL Interactive Poster and DemonstrationSessions, pages 1?4, Ann Arbor, MI, June.
Associationfor Computational Linguistics.David DeVault.
2008.
Contribution Tracking: Partici-pating in Task-Oriented Dialogue under Uncertainty.Ph.D.
thesis, Rutgers University, New Brunswick, NJ.Eduardo Ferme?
and Sven Ove Hansson.
2011.
AGM 25years: Twenty-five years of research in belief change.Journal of Philosophical Logic, 40(2):295?331.Michael C. Frank and Noah D. Goodman.
2012.
Predict-ing pragmatic reasoning in language games.
Science,336(6084):998.Michael Franke.
2009.
Signal to Act: Game Theoryin Pragmatics.
ILLC Dissertation Series.
Institute forLogic, Language and Computation, University of Am-sterdam.Peter Ga?rdenfors.
1988.
Knowledge in Flux: Modelingthe Dynamics of Epistemic States.
MIT Press.Piotr J. Gmytrasiewicz and Prashant Doshi.
2005.
Aframework for sequential planning in multi-agent set-tings.
Journal of Artificial Intelligence Research,24:24?49.Dave Golland, Percy Liang, and Dan Klein.
2010.
Agame-theoretic approach to generating spatial descrip-tions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 410?419, Cambridge, MA, October.
ACL.Noah D. Goodman and Andreas Stuhlmu?ller.
2012.Knowledge and implicature: Modeling language un-derstanding as social cognition.
In Proceedings of theThirty-Fourth Annual Conference of the Cognitive Sci-ence Society.H.
Paul Grice.
1975.
Logic and conversation.
In PeterCole and Jerry Morgan, editors, Syntax and Semantics,volume 3: Speech Acts, pages 43?58.
Academic Press,New York.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-put.
Linguist., 12(3):175?204, July.William G. Hayward and Michael J. Tarr.
1995.
Spa-tial language and spatial representation.
Cognition,55:39?84.Jerry Hobbs, Mark Stickel, Douglas Appelt, and PaulMartin.
1993.
Interpretation as abduction.
ArtificialIntelligence, 63(1?2):69?142.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.Computational Linguistics, 38(1):173?218.David Lewis.
1969.
Convention.
Harvard UniversityPress, Cambridge, MA.
Reprinted 2002 by Blackwell.1080Michael L. Littman, Anthony R. Cassandra, andLeslie Pack Kaelbling.
1995.
Learning policies forpartially observable environments: Scaling up.
In Ar-mand Prieditis and Stuart J. Russell, editors, ICML,pages 362?370.
Morgan Kaufmann.Arthur Merin.
1997.
If all our arguments had to be con-clusive, there would be few of them.
ArbeitspapiereSFB 340 101, University of Stuttgart, Stuttgart.Christos Papadimitriou and John N. Tsitsiklis.
1987.
Thecomplexity of markov decision processes.
Math.
Oper.Res., 12(3):441?450, August.Prashant Parikh.
2001.
The Use of Language.
CSLI,Stanford, CA.C.
Raymond Perrault and James F. Allen.
1980.
A plan-based analysis of indirect speech acts.
American Jour-nal of Computational Linguistics, 6(3?4):167?182.Christopher Potts.
2012.
Goal-driven answers in theCards dialogue corpus.
In Nathan Arnett and RyanBennett, editors, Proceedings of the 30th West CoastConference on Formal Linguistics, Somerville, MA.Cascadilla Press.David V. Pynadath and Milind Tambe.
2002.
The com-municative multiagent team decision problem: Ana-lyzing teamwork theories and models.
Journal of Ar-tificial Intelligence Research, 16:2002.Hannah Rohde, Scott Seyfarth, Brady Clark, GerhardJa?ger, and Stefan Kaufmann.
2012.
Communicat-ing with cost-based implicature: A game-theoretic ap-proach to ambiguity.
In The 16th Workshop on the Se-mantics and Pragmatics of Dialogue, Paris, Septem-ber.Robert van Rooy.
2003.
Questioning to resolve decisionproblems.
Linguistics and Philosophy, 26(6):727?763.Seymour Rosenberg and Bertram D. Cohen.
1964.Speakers?
and listeners?
processes in a word commu-nication task.
Science, 145:1201?1203.Matthijs T. J. Spaan and Nikos Vlassis.
2005.
Perseus:Randomized point-based value iteration for POMDPs.Journal of Artificial Intelligence Research, 24(1):195?220, August.Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlas-sis.
2008.
Multiagent planning under uncertainty withstochastic communication delays.
In In Proc.
of the18th Int.
Conf.
on Automated Planning and Schedul-ing, pages 338?345.Alex Stiller, Noah D. Goodman, and Michael C. Frank.2011.
Ad-hoc scalar implicature in adults and chil-dren.
In Proceedings of the 33rd Annual Meeting ofthe Cognitive Science Society, Boston, July.Matthew Stone, Richmond Thomason, and David De-Vault.
2007.
Enlightened update: A computationalarchitecture for presupposition and other pragmaticphenomena.
To appear in Donna K. Byron; CraigeRoberts; and Scott Schwenter, Presupposition Accom-modation.Blaise Thomson and Steve Young.
2010.
Bayesian up-date of dialogue state: A pomdp framework for spokendialogue systems.
Comput.
Speech Lang., 24(4):562?588, October.Steve Young, Milica Gas?ic?, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, and KaiYu.
2010.
The hidden information state model: Apractical framework for pomdp-based spoken dialoguemanagement.
Comput.
Speech Lang., 24(2):150?174,April.1081
