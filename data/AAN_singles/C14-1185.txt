Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1962?1973, Dublin, Ireland, August 23-29 2014.Exploring Syntactic Features for Native Language Identification:A Variationist Perspective on Feature Encoding andEnsemble OptimizationSerhiy BykhSeminar f?ur SprachwissenschaftUniversit?at T?ubingensbykh@sfs.uni-tuebingen.deDetmar MeurersSeminar f?ur SprachwissenschaftUniversit?at T?ubingendm@sfs.uni-tuebingen.deAbstractIn this paper, we systematically explore lexicalized and non-lexicalized local syntactic featuresfor the task of Native Language Identification (NLI).
We investigate different types of featurerepresentations in single- and cross-corpus settings, including two representations inspired by avariationist perspective on the choices made in the linguistic system.
To combine the differentmodels, we use a probabilities-based ensemble classifier and propose a technique to optimize andtune it.
Combining the best performing syntactic features with four types of n-grams outperformsthe best approach of the NLI Shared Task 2013.1 Introduction and related workNative Language Identification (NLI) is the task of identifying the native language of a writer by analyz-ing texts written by this writer in a non-native language.
NLI started to attract attention in computationallinguistics with the work of Koppel et al.
(2005).
Since then, the interest has increased steadily, leadingto the First NLI Shared Task in 2013, with 29 participating teams (Tetreault et al., 2013).The task of NLI is usually treated as a text classification problem with the L1s as classes.
A wide rangeof features, reaching from character or word-based n-grams to different types of syntactic models havebeen employed in NLI.
For example, Wong and Dras (2011) utilized character and part-of-speech (POS)n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., localtrees.
Their approach with a binary representation of non-lexicalized rules (except for those rules lexi-calized with function words and punctuation) outperformed a setup using only lexical features, such asn-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002).
Swansonand Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG)rules replacing terminals (except for function words) by a special symbol.
TSG outperformed CFG fea-tures in their settings.
Among several options, Brooke and Hirst (2012) explored using non-lexicalizedCFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al.,2011), and Lang-8 (Brooke and Hirst, 2013a).
The authors conclude that including CFG features gen-erally boosts the performance of the system.
In the context of the First NLI Shared Task, in Bykh et al.
(2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information.Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson andCharniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013).Before extending syntactic modeling further, in this paper we want to systematically explore the rangeof options involving CFG rule features for NLI.
We consider non-lexicalized and lexicalized CFG fea-tures, and different feature representations, from binary encodings to a normalized frequency encodinginspired by a variationist sociolinguistic perspective.Previous research in this domain often limited the use of lexicalized rules given that the lexicalizationmay lead to an unintended topic or domain dependence.
Yet, NLI research has since established thatlexical features, such as word-based n-grams, are among the best performing features both in single-This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1962and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley,2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013),making them an essential component of any approach with state-of-the-art performance.
At the sametime, the question whether an NLI approach and its results capture general characteristics of languageand language learning instead of only encoding the characteristics of a specific data set remains anessential concern.
In the experiments in this paper, we thus include experiments on both a topic-balancedsingle-corpus and on a highly heterogeneous cross-corpus data set.The range of feature types used in NLI research raises a further question, namely how the differentsources of information are best combined.
The most simple solution is to put all features into a singlevector.
However, Tetreault et al.
(2012) pointed out that the performance can be increased by using aprobability-estimate based ensemble (meta-classifier), which was confirmed in Bykh et al.
(2013) andCimino et al.
(2013).
But which models are worth integrating into such a meta-classifier?
Some ofthe models may be redundant despite performing well individually; on the other hand, some modelsmay improve the ensemble despite performing relatively poorly by itself.
We explore this issue byimplementing a basic ensemble optimization algorithm performing model selection.In terms of the structure of the paper, in section 2 we first introduce the corpora used in the single-corpus and cross-corpus settings.
Section 3 then presents the first set of experiments, systematicallyexploring lexicalized and unlexicalized Context-Free Grammar Rules (CFGR) as features.
Given thesignificant complexity of the overall feature space, we then explore model selection for optimizing theensemble classifier in section 4.
In section 5, we combine the CFGR features with n-grams, resulting inthe best accuracy reported for the standard TOEFL11 test set.
Section 6 sums up the paper and sketchessome directions for future research.2 DataThe research in this paper makes use of two sets of data:First, there is the TOEFL11 (T11) data set (Blanchard et al., 2013), which was introduced for the NLIShared Task 2013 and has become a standard frame of reference for NLI research.
We use this standardsetup for single-corpus evaluation, where each L1 is represented by 1100 essays, of which 100 essaysare singled out in the standard test set.
The remaining 1000 essays per L1 (= T11 train ?
dev) constituteour training data in the single-corpus settings.Second, we make use of a range of other learner corpora to study how well the results generalize.Concretely, for our cross-corpus settings we employ the NT11 corpus of Bykh et al.
(2013), whichconsists of the ICLE (Granger et al., 2009), FCE (Yannakoudakis et al., 2011), BALC (Randall andGroom, 2009), ICNALE (Ishikawa, 2011), and T?UTEL-NLI (Bykh et al., 2013) corpora.
In total NT11includes 5843 texts, with the following division into languages: Arabic (846), Chinese (1048), French(456), German (500), Hindu (400), Italian (467), Japanese (447), Korean (684), Spanish (446), Telugu(200), Turkish (349).
In the cross-corpus settings, we train on NT11 and test on the standard T11 test set.3 Systematically exploring Context-Free Grammar Rules (CFGR)3.1 FeaturesIn this paper, we focus on the CFG production rules (CFGR) as syntactic features for the task of NLI.CFG rules are the most basic and widely used local syntactic units modularizing the overall syntacticanalysis of a sentence.
We parsed the T11 and NT11 corpora using the Stanford Parser (Klein andManning, 2002) and extracted all CFG rules from the T11 and NT11 training sets.
On this basis wedefined the following tree feature types:1.
CFGRph: Only phrasal CFG production rules excluding all terminals?
S?
NP VP, NP?
D NN, .
.
.2.
CFGRlex: Only lexicalized CFG production rules of the type preterminal?
terminal?
JJ?
nice, JJ?
quick, NN?
vacation, .
.
.3.
CFGRph?lex= CFGRph?
CFGRlex(i.e., the union of the above two)1963A variationist perspective on feature representation We explore four different feature representa-tions: The two standard ones are a frequency-based (freq) representation, where the values are the rawcounts of the occurrences of the rule in the given parsed document, and a binary (bin) representation,which only indicates whether a rule is present or absent in that document.Complementing these standard feature representations, we explored two options that take as startingpoint the observation that CFG rules with the same left-hand side category represent different ways torewrite that category.
So in a sense, under a top-down perspective, there is a choice between differentways of realizing a given category.This is reminiscent of variationist sociolinguistic analysis, where one studies the linguistic choicesmade by a given speaker and connects the choices with extra-linguistic variables such as the age orgender of a speaker.
For example, in William Labov?s field-defining study ?The Social Stratificationof (r) in New York City Department Stores?
from his book ?Sociolinguistic Patterns?
(Labov, 1972),he found that the presence or absence of the consonant [r] in postvocalic position (e.g., car, fourth)correlates with the ranking of people in status or prestige, i.e., social stratification.
Speakers thus makechoices in how to realize a given variable by producing one of the variants (see also Tagliamonte, 2011).Inspired by this perspective, in Meurers et al.
(2013) we discussed how a variationist perspective onsyntactic alternations can provide interpretable features for NLI classification.Under a variationist perspective, producing one of the variants of a given variable also means notchoosing the other variants of that variable.
So it is this grouping of observations that we want to takeinto account in terms of encoding local trees as features when we interpret the mother category as thevariable to be realized and the different CFG rules with that left-hand side as variants of that variable.This results in two feature representations, a simple one (vars) and a weighted one (varw).The varsand varwfrequency normalizations for each variant v from the set of variants V realizing aparticular variable out of the set of variables V is defined as follows:vars(v ?
V ) =f(v)F (V )varw(v ?
V ) = vars(v) ?
w(V )Here, f(v) yields the frequency x of a particular variant v, F (V ) is the sum over the frequencies ofall variants v realizing the variable V , and w(V ) is the weight for the variable V :f(v) = xF (V ) =?v?Vf(v)w(V ?
V ) =F (V )n?i=1F (Vi)The weighting applied in varwtakes into account the frequency proportion of each variable V in theoverall variables set V , assigning higher weights for more frequent variables.
Mathematically it reducesto normalizing each variant by the sum of the frequencies over all variants across all variables, i.e., tothe relative frequency of each variant v with respect to the set of all variables V .
At the same time,we will see in the next section that the individual variables keep an independent status in terms of theclassification setup, where we train a separate classifier for each variable.19643.2 ResultsClassifier We use the L2-regularized Logistic Regression from the LIBLINEAR package (Fan et al.,2008), which we accessed through WEKA (Hall et al., 2009).
To obtain results for all feature repre-sentations which are comparable across the different settings we uniformly scale all values employingthe -Z option of WEKA.
This means that the freq feature representation based on the raw frequenciesin essence also becomes normalized.
This is particularly relevant in the context of the cross-corpusevaluation, where raw frequencies are particularly questionable given highly variable text sizes.Single- vs. cross-corpus results The results for the three feature types using the four different featurerepresentations are presented in Table 1.
The chance baseline for the given data setup is 9.1%.
Thereare big accuracy differences between the single- and cross-corpus settings despite very similar featurecounts.
The drop for the cross-corpus settings is roughly around12compared to the single-corpus settings.This is in line with previous results on the same data sets using a wide range of features (Bykh et al.,2013), confirming the fact that obtaining high cross-corpus results remains challenging in NLI.features single-corpus (sc): T11 trainingfreq bin varsvarwfeat.
#CFGRph50.00% 44.27% 48.45% 49.82% 14,713CFGRlex75.73% 72.45% 71.00% 76.91% 83,402CFGRph?lex78.18% 73.55% 75.36% 78.82% 98,115features cross-corpus (cc): NT11 trainingfreq bin varsvarwfeat.
#CFGRph21.27% 22.91% 26.27% 27.73% 15,253CFGRlex26.73% 32.00% 28.82% 36.82% 78,923CFGRph?lex28.27% 34.27% 32.55% 38.82% 94,176Table 1: Results for the CFGR feature variants obtained on the standard T11 test setBest feature type The CFGRlexfeature type clearly outperforms the more abstract CFGRphfeaturetype, yielding up to 28% difference in accuracy for the single-corpus and up to 9% for the cross-corpussettings.
In contrast to previous research assuming that lexicalized trees are too topic-specific, the resultsshow that CFGRlexis a valuable feature type in both the single-corpus and the cross-corpus settings.The CFGRlexfeatures combine syntactic and lexical information, such as the fact that a given token witha particular POS is used, e.g., the token can being used as a noun in There is a can of beer in the fridgeinstead of as the more frequent modal verb use in He can dance.
Note that this is different from usingword and POS unigrams as features, where the relevant connection is lost.
In both the T11 data, whichis topic balanced, for single-corpus evaluation and the very heterogeneous NT11 data containing a widerange of topics for cross-corpus evaluation, we obtained consistently better results for CFGRlexthan forCFGRph.
Some syntactic rules including lexical information thus seem to generalize well across topics.Combining CFGRphand CFGRlexinto CFGRph?lexgives an additional boost in performance.Best feature representation There are clear differences in Table 1 between the results for the fourfeature representations.
varwyields the best accuracies in five out of six settings, across different featuretypes and corpora.The results show that WEKA-normalized raw frequencies such as freq yield the worst results in across-corpus setting but perform very well single-corpus, which is in line with the assumption that rawfrequency features do not generalize well.
In our experiments, the performance of freq in a cross-corpussetting is up to 10.55% worse than what is yielded by varw, despite comparable single-corpus perfor-mance.
freq also consistently performs worse than varsin the cross-corpus setting, despite outperformingvarssingle-corpus.1965Using binary features (bin) yields better results cross-corpus than freq, whereas in the single-corpussetting it is the other way round.
The abstraction introduced by the binary feature representation thusshows a positive effect in terms of the capability of the features to generalize to other data sets.For the abstract CFGRphfeatures, varsperforms better than freq or bin in the cross-corpus setting.The fact that the varwis performing consistently better than varsshows that weighting is important.Hence, incorporating the insight from variationist sociolinguistics is not only conceptually interesting asa theoretical perspective, but also provides a quantitative advantage in terms of performance.CFGR categories as variables As mentioned above, the best performance is achieved by combiningCFGRphand CFGRlexinto the CFGRph?lexfeature type using the weighted variationist feature rep-resentation varw.
Thus, we focused on that feature type and explored it more in depth.
We did so bysplitting the overall varwnormalized CFGRph?lexfeature set by the variable, i.e., the different mothernodes.
We trained separate models, where each of those models consists of features encoding the differ-ent variants, i.e., the different realizations in which a given mother node can be rewritten.
Our aim wasto investigate the accuracy of the individual variable-based models and their contribution to the overallperformance.
Figures 1 and 2 depict the single-corpus (sc) and cross-corpus (cc) accuracies yielded byeach individual variable-based model, for presentation reasons shown separately for the CFGRphandthe CFGRlexsubsets.ADJPADVPCONJPFRAG INTJ LST NAC NP NX PP PRN PRT QP RRC S SBARSBARQ SINV SQ UCP VPWHADJPWHADVPWHNPWHPP X0,00%5,00%10,00%15,00%20,00%25,00%30,00%35,00%40,00%45,00%50,00%scccmodelsaccuracyFigure 1: Accuracy for the individual CFGRphvariable based models, varwnormalizedThe CFGRphresults in Figure 1 show that a small subset of variables performs relatively well.
Mostof the models perform poorly, yielding accuracies close to the chance baseline.
The best performingvariables are essentially the main phrasal categories, such as S, NP, VP, PP, ADJP, ADVP or SBAR.The results for the CFGRlexin Figure 2 show a similar pattern.
There is a subset of variables whichperform relatively well, usually models based on the main POS categories, such as the nominal (NN) andverbal (VB) categories as well as adjectives (JJ), prepositions (IN) and adverbs (RB).
Some punctuationmarks also seem to play a role.
The rest of the models yields accuracies around the chance baseline.This might be due to data sparsity given that the main POS categories also are the most frequent.
Butthose main categories also have the highest number of variants through which they can be realized.
Thegood performance of the models for the variables with the highest number of variants thus confirms theassumption that the choice of one of the realization options of a given category is influenced by the L1.Should we focus only on those high-performing models ?
or do the other models also contain relevant,independent information which is worth preserving?
We address that question in the next section.1966AA AD DJ PV CO NF RR RRG RRI TI LD FF FFS FFSI FFI SDJ SXI SGS SGSQ GB GBG GBI GS IUL JX WH 0B 0BD 0B, 0BF 0BS 0B% ODJ OS OSQOGB5TGB55GGB5 1 Q 22 3 4 s ccm3mmod3mmoem3mmoed3mmolm3mmold3mmoam3mmoad3mmoum3mmoud3mmodm3mmoryyy?????r?yy??
?y?Figure 2: Accuracy for the individual CFGRlexvariable based models, varwnormalized4 Ensemble optimization and tuningEnsemble generation To combine the individual models, we employ a probability-estimate-based en-semble approach, following Tetreault et al.
(2012) and Bykh et al.
(2013).
This meta-classifier combinesthe probability distributions provided by the individual classifier for each of the incorporated models asfeatures.
To obtain the ensemble training files, we performed 10-fold cross-validation for each model onthe corresponding training set and took the probability estimate distributions.
For testing, we took theprobability estimate distribution yielded by each individual model trained on the corresponding trainingset and tested on the T11 test set.
To obtain the probability estimates for the individual models we usedLIBLINEAR as described in section 3.2.
The ensembles were trained and tested using LIBSVM with anRBF kernel (Chang and Lin, 2011), which outperformed LIBLINEAR for this purpose.Ensemble optimization (+opt) The growing range of features used for NLI raises the question of howto perform model selection.
Even when analyzing a single feature type in depth, as we do in section 3.2,we already must determine which of the low-performing models to keep in an ensemble.
We approachthe question with a simple incremental ensemble optimization algorithm performing model selection.Algorithm 1 Ensemble Optimization / Ensemble Model SelectionMa?
{m1, ...,mn} .
overall ensemble, i.e., all ensemble modelsMb?
?
.
current best performing ensemblewhile Ma6= ?
do .
iterate until Mais emptymb?
MAX(Ma) .
get the model with the highest accuracy mbout of MaMt?Mb?
{mb} .
join the previous best performing ensemble Mband {mb}if ACC(Mt) > ACC(Mb) then .
check if the new ensemble is performing better than MbMb?Mt.
if the accuracy improves, store the new ensemble in Mbend ifREMOVE(mb,Ma) .
remove mbfrom Maend while1967In each iteration step the optimization algorithm shown in Algorithm 1 retrieves the current best singlemodel mbout of the model set Ma(which is initialized with the overall model set for a particular setting),joins it with the previous best performing ensemble Mb(which is initialized to ?
), compares the accuracyof that new ensemble with the accuracy of the previous best ensemble.
It retains the new ensemble as thebest ensemble if the accuracy improves, or keeps the previous best ensemble as best ensemble otherwise.In Algorithm 1, we describe only the gist of the optimization, omitting some details to keep it transparent.Some ambiguities have to be resolved.
If there are several models in Mayielding the same accuracy, onehas to decide, which of them to pick as the next mb.
We resolve that issue by always picking the modelwith the least number of features.
When several models yield the same accuracy and have the samenumber of features, we resort to alphabetical order.
The optimization is always carried out using 10-foldcross-validation results on the training data (to obtain the accuracy ranking on Maand to perform eachoptimization step).
The test set is not part of the optimization at any point.
Only after optimization is theresulting ensemble applied to the test set and we report the corresponding accuracies.Ensemble tuning (+all) In order to further tune the ensemble, we explore the following idea: Wegenerate a single ensemble model mn+1based on all of the features used in a particular setting, i.e., all thefeatures incorporated by the models m1.
.
.mn.
Then we include that mn+1model in the Maensembleas just another model, and use that new M+1aensemble either directly or as basis for the optimization.Since mn+1incorporates all of the features of interest for a particular setting, it is expected to yield morereliable probability estimates than the other individual ensemble models in M+1a, each covering onlya subset of that feature set.
Incorporating such an mn+1into the ensemble may stabilize the resultingsystem, i.e., the machine learning algorithms may learn to rely on mn+1in settings, where the rest ofthe included models m1.
.
.mnshow a rather poor individual performance and are of limited use.
In thetables and explanations below, we refer to the model mn+1as [all] and to the M+1aensemble as +all.For building the mn+1model included in the M+1aensemble there are two options.
We can buildit on the basis of the probabilities of the models or on the union of the original feature values of thosemodels.
In the former case, the final ensemble model essentially is a meta-meta-classifier.
For the settingsintegrating the same type of feature representations (cf.
results in Tables 2 and 4), we use the originalfeature values merged into a single vector to build mn+1.
For the settings integrating different featuretypes (cf.
results in Table 6), we use the probability estimates from the models m1.
.
.mnto build mn+1.Ensemble results for the CFGR variables The ensemble results for the separate variable-based mod-els for the CFGRph?lexfeature type are presented in Table 2.
We provide single-corpus (sc) and cross-corpus (cc) results for different ensemble settings, where +/- opt states whether ensemble optimizationwas performed, and +/- all whether tuning was employed.
Concretely, (-opt, -all) means that the ensem-ble Mawas used without any optimization or tuning, and correspondingly (+opt, +all) means that theoptimized and tuned version of Ma(i.e., the optimized version of the ensemble M+1a) was employed.
Inthe remaining two cases (+opt, -all) and (-opt, +all) either optimization or tuning was used, respectively.The column baseline lists the corresponding results from Table 1, which were obtained by putting all thefeatures in a single vector.
The number in parentheses specifies the number of models combined in theensemble: in the features column, it shows the overall number of separate variable-based models, and inthe +opt columns, it is the number of models selected by the optimization algorithm.features data baseline ensemble-opt +opt-all +all -all +allCFGRph?lex(71) sc 78.82% 66.00% 79.18% 71.27% (14) 79.64% (8)cc 38.82% 18.09% 34.18% 32.55% (10) 39.00% (1)Table 2: Results for the CFGRph?lexensembles with different optimization settingsThe results show that generating an ensemble using all of the individual variable-based models withoutoptimization and tuning (-opt, -all) leads to a big accuracy drop compared to the baseline.
The fact that1968the drop in the cross-corpus setting is more than 20% is particularly striking.
We assume that this is dueto the poor performance of most of the individual models, yielding probabilities of little use overall.
Thefew relatively well-performing models we discussed in section 3.2 apparently are flooded by the noiseintroduced by the others.
Thus, for a set of rather low-performing models without any optimization, itseems preferable to provide the classifier with access to the individual features instead of to the noisyprobability estimates.
The optimization (+opt, -all) leads to a clear improvement over the non-optimizedsettings.
In the single-corpus setting only 14 of the 71 models were kept and in cross-corpus only 10.Table 3 shows the selected models in the order in which they are selected by the ensemble optimizationalgorithm.
For (+opt, -all), the table basically consists of the best performing variables (i.e., the modelscontaining as features the different ways to rewrite the given mother category) as discussed in section 3.2,suggesting that the algorithm makes meaningful choices.data CFGRph?lex: selected models+opt, -all +opt, +allsc [NN]+[JJ]+[RB]+[NNS]+[VB]+[NP]+[S]+[VP] [all]+[NN]+[JJ]+[RB]+[PRP]+[VBN]+[NNP]+[WDT] (8)+[IN]+[VBP]+[VBG]+[VBN]+[NNP]+[,] (14)cc [NN]+[JJ]+[NNS]+[NP]+[RB]+[VB]+[VP]+[NNP] [all] (1)+[S]+[IN] (10)Table 3: The CFGRph?lexmodel sets selected by optimizationThe flipside of the coin is that low-performing models generally were not found to have a positiveeffect and thus were not included.
Yet, optimization by itself is not successful overall given that the(+opt, -all) accuracy remains below the single feature set baseline.Applying tuning without optimization (-opt, +all) outperforms the optimization result.
Thus, includ-ing the overall model [all] in the ensemble improves the meta-classifier.
In the single-corpus setting, theaccuracy is slightly higher than the baseline, in cross-corpus it remains below the baseline.Turning on both optimization and tuning (+opt, +all) yields the overall best results of Table 2, 79.64%for single-corpus and 39% for the cross-corpus setting.
The corresponding entry in Table 3 shows thattuning significantly reduces the number of selected models.
This is not unexpected given that the overallmodel [all] essentially includes all the information.
In the cross-corpus setting, [all] indeed is the onlymodel selected.
Interestingly, in the single-corpus setting, the optimization algorithm identifies someadditional models to improve the accuracy, mainly ones that also perform well individually.
While thisamounts to adding information that in principle is already available to the [all] model, the improvementmay stem from the abstract nature of the probability estimates used as features of the meta-classifier.When both optimization and tuning are applied, the tuning apparently stabilizes the ensemble leading tohigher performance, and the optimization algorithm further improves the result by reducing the noise.5 Combining CFGR with four types of n-gramsBased on the systematic exploration of the CFGR domain, we turn to combining our new feature typeCFGRph?lexwith n-gram features as the best performing features for NLI (Tetreault et al., 2013; Jarviset al., 2013).
Adapting the n-gram approach we presented in Bykh and Meurers (2012), we use allrecurring n-grams with 1 ?
n ?
10 at different levels of representation, including the word-based (W),open-class POS-based (OP) and POS-based (P) n-grams from our previous work as well as lemma-based(L) n-grams (Jarvis et al., 2013).
We employ binary feature encoding for all n-gram types.For POS-tagging we use the OpenNLP1toolkit, for lemmatizing we employ the MATE2tools(Bj?orkelund et al., 2010).
To obtain a fine grained, flexible n-gram setting, we generate an ensemblemodel for each n-gram type and each n, which results in 40 n-gram models.1http://opennlp.apache.org2https://code.google.com/p/mate-tools1969Table 4 provides the results for the n-gram ensembles built on the basis of the recurring word-, lemma-,POS-, OCPOS-based n-grams with 1 ?
n ?
10 in the same format as Table 2 for CFGRph?lex.3Different from the CFGRph?lexcase, the results for the n-gram ensemble model without optimizationor tuning (-opt, -all) already are 4?5% higher than the single vector baseline.features data baseline ensemble-opt +opt-all +all -all +allN-GRAMS (40) sc 77.09% 82.27% 82.55% 83.00% (13) 82.27% (8)cc 31.00% 34.91% 34.55% 36.45% (6) 35.45% (6)Table 4: Results for the n-gram ensembles with different optimization settingsThe best results, 83% for single-corpus and 36.45% for the cross-corpus setting, are obtained by ap-plying the optimization.
The n-gram ensembles seem to benefit more from optimization than from tuningin general.
The feature counts for the n-grams (single-corpus: 4,822,874; cross-corpus: 3,687,375) arefar higher than for CFGRph?lex(single-corpus: 98,115; cross-corpus: 94,176), so there may be morenoise in the [all] model, making it less useful for the tuning step.Table 5 lists the models selected by the optimization algorithm in order in which they are selected.The n-gram types and the n of the model is indicated, e.g., ?[OP-3]?
means ?OCPOS-based trigrams?.data N-GRAMS: selected models+opt, -all +opt, +allsc [W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-3] [all]+[W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[OP-4]+[L-4] (8)+[OP-1]+[OP-5]+[P-3]+[P-5]+[P-2]+[OP-8] (13)cc [W-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-2] (6) [W-2]+[W-1]+[all]+[L-1]+[L-3]+[P-4] (6)Table 5: The n-gram model sets selected by optimizationFor the more surface-based n-gram (word- and lemma-based), the optimizer selected only up to n = 3,whereas for the more abstract ones (POS- and OCPOS-based), models up to n = 8 were included.
Thus,when abstracting from the surface, one can get some useful information out of longer n-grams thatapparently is not contained in the short surface-based ones.
Different from the CFGRph?lexvariables-based ensemble, we here find that relatively low-performing models such as those considering longer nn-grams are kept when optimizing the ensemble.Having established the performance of the n-gram ensembles, we can turn to combining theCFGRph?lexand n-gram models.
The results are presented in Table 6.features data ensemble-opt +opt-all +all -all +all(a) CFGRph?lex(71) + N-GRAMS (40) sc 82.09% 82.91% 82.91% (20) 83.55% (6)cc 34.09% 36.00% 36.73% (8) 38.45% (3)(b) CFGRph?lex(71) + N-GRAMS [+opt, -all] (ME) sc 83.09% 83.73% 82.64% (4) 84.18% (5)cc 37.36% 39.55% 38.00% (3) 40.27% (3)(c) CFGRph?lex[+opt, +all] (ME) + N-GRAMS (40) sc 83.73% 84.82% 84.73% (13) 83.82% (13)cc 36.82% 38.91% 42.00% (5) 43.00% (4)(d) CFGRph?lex[+opt, +all] (ME) + N-GRAMS [+opt, -all] (ME) sc 83.45% 83.45% 83.45% (2) 83.36% (2)cc 41.27% 42.00% 41.27% (2) 40.55% (2)Table 6: Optimization results combining n-grams and CFGRph?lex3For space reasons, we cannot present the individual results for the separate n-gram models here, but interested readerscan consult Bykh and Meurers (2012), where word-, POS- and OCPOS-based n-gram results are discussed in detail.
Thelemma-based n-grams we are adding here perform very much like the word-based n-grams.1970We explore four different ways to combine the two model sets, and the table shows the best results foreach of the setups in bold, once for the single-corpus and once for the cross-corpus setting.For the results of setup (a), we use the ensemble consisting of all individual models separately.In (b), the CFGRph?lexmodels are included as in (a), but we replace the n-gram models by a singlemeta-ensemble model (ME) generated using the best n-grams setting (+opt, -all), which consists of 13models for single-corpus and six models for the cross-corpus setting (see Table 4).
ME thus is a meta-meta-classifier, generated by applying the ensemble model generation routine to an ensemble.In (c), we invert the (b) setting: The CFGRph?lexfeatures are replaced by a meta-ensemble generatedusing the best performing CFGRph?lexsetting (+opt, +all), which consists of eight models for thesingle-corpus, and one model for the cross-corpus setting (see Table 2).Finally, in (d) we combine the meta-ensemble for CFGRph?lexwith the meta-ensemble for the n-grams obtaining an ensemble consisting of two modelsThe best results of 84.82% in the single-corpus setting and 43% cross-corpus, underlined in the table,are obtained in setup (c).
These are the overall best results across all experiments described in this paper.The best result in the single-corpus setting involves tuning only, whereas in the cross-corpus setting itinvolves tuning and optimization selecting the models [all]+[CFGR +all +opt]+[W-2]+[W-1].The single-corpus accuracy of 84.82% is the best result reported so far for the NLI Shared Task 2013data with the T11 train ?
dev set for training and the T11 test set for testing.
The best previous resultwas 83.6% (Jarvis et al., 2013).In the cross-corpus setting, the 43% accuracy also outperforms the previous best result on theNT11 data (Bykh et al., 2013) by 4.5%.In sum, the overall best results in the single-corpus and cross-corpus settings are obtained starting withthe whole n-gram model set plus an optimized CFGRph?lexmeta-ensemble.
This confirms the useful-ness of the optimized ensemble setup and underlines that combining a range of linguistic properties, fromn-grams at different levels of abstraction to local syntactic trees characteristics, is a particularly fruitfulapproach for native language identification as a good example of an experimental task putting linguisticmodeling to the test with real-life data.6 ConclusionsIn the research presented, we systematically explored non-lexicalized and lexicalized CFG productionrules (CFGR) as features for the task of NLI using both single-corpus and cross-corpus settings.
Includ-ing lexicalized CFG rule features clearly improved the results in both setting so that it seems worthwhilenot to discard them a priori, which was the standard in previous research.Pursuing a variationist perspective to CFGR feature representation resulted in improved performanceand it supported an in-depth exploration of the contribution of the different variables and variants aswell as of the value of local syntactic features for NLI in general.
Training a separate classifier for eachvariable provides quantitative advantages by facilitating high-performing ensemble setups and supportsa qualitative discussion of the categories reflecting the choices made by the learners with a given L1.Investigating different meta-classifier setups, we explored ensemble optimization and tuning tech-niques that improved the accuracy over putting all features in a single vector or a basic ensemble setup.Combining the syntactic CFGR with four types of n-grams yielded a single-corpus accuracy of 84.82%on the TOEFL11 test set.
To the best of our knowledge this is the highest accuracy reported so far onthis standard data set of the NLI Shared Task 2013.
The combined model also outperformed our bestprevious cross-corpus result on the NT11 corpus.In terms of future work, we intend to explore a broader range of linguistic features from a variationistperspective, for example on the morphological level.
To investigate the generalizability of the types offeatures used, we also plan to apply our approach to NLI targeting second langauges other than English.1971ReferencesAnders Bj?orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues.
2010.
A high-performance syntactic andsemantic dependency parser.
In Demonstration Volume of the 23rd International Conference on ComputationalLinguistics (COLING 2010), Beijing, pages 23?27.
https://code.google.com/p/mate-tools/.Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow.
2013.
TOEFL11: Acorpus of non-native english.
Technical report, Educational Testing Service.Julian Brooke and Graeme Hirst.
2012.
Robust, lexicalized native language identification.
In Proceedings of the24th International Conference on Computational Linguistics (COLING), pages 391?408, Mumbai, India.Julian Brooke and Graeme Hirst.
2013a.
Native language detection with ?cheap?
learner corpora.
In SylvianeGranger, Ga?etanelle Gilquin, and Fanny Meunier, editors, Twenty Years of Learner Corpus Research.
LookingBack, Moving Ahead.
Proceedings of the First Learner Corpus Research Conference (LCR 2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Julian Brooke and Graeme Hirst.
2013b.
Using other learner corpora in the 2013 nli shared task.
In Proceedingsof the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT2013, Atlanta, GA.Serhiy Bykh and Detmar Meurers.
2012.
Native language identification using recurring n-grams ?
investigatingabstraction and domain dependence.
In Proceedings of the 24th International Conference on ComputationalLinguistics (COLING), pages 425?440, Mumbay, India.Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and Detmar Meurers.
2013.
Combining shallow and linguisticallymotivated features in native language identification.
In Proceedings of the 8th Workshop on Innovative Use ofNLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A library for support vector machines.
ACM Transactionson Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Andrea Cimino, Felice Dell?Orletta, Giulia Venturi, and Simonetta Montemagni.
2013.
Linguistic profiling basedon general?purpose features and native language identification.
In Proceedings of the 8th Workshop on Innova-tive Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.R.E.
Fan, K.W.
Chang, C.J.
Hsieh, X.R.
Wang, and C.J.
Lin.
2008.
Liblinear: A library for large linearclassification.
The Journal of Machine Learning Research, 9:1871?1874.
Software available at http://www.csie.ntu.edu.tw/?cjlin/liblinear.Binyam Gebrekidan Gebre, Marcos Zampieri, Peter Wittenburg, and Tom Heskes.
2013.
Improving native lan-guage identification with tf-idf weighting.
In Proceedings of the 8th Workshop on Innovative Use of NLP forBuilding Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.S.
Granger, E. Dagneaux, and F. Meunier.
2002. International Corpus of Learner English.
Presses Universitairesde Louvain, Louvain-la-Neuve.Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot, 2009. International Corpus of LearnerEnglish, Version 2.
Presses Universitaires de Louvain, Louvain-la-Neuve.Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten.
2009.
Theweka data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10?18.Shin?ichiro Ishikawa.
2011.
A new horizon in learner corpus studies: The aim of the ICNALE projects.
InG.
Weir, S. Ishikawa, and K. Poonpon, editors, Corpora and language technologies in teaching, learning andresearch, pages 3?11.
University of Strathclyde Publishing, Glasgow, UK.
http://language.sakura.ne.jp/icnale/index.html.Scott Jarvis and Scott A. Crossley, editors.
2012.
Approaching Language Transfer through Text Classification:Explorations in the Detection-based Approach.
Second Language Acquisition.
Multilingual Matters.Scott Jarvis, Yves Bestgen, and Steve Pepper.
2013.
Maximizing classification accuracy in native language iden-tification.
In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications(BEA-8) at NAACL-HLT 2013, Atlanta, GA.Dan Klein and Christopher D. Manning.
2002.
Fast exact inference with a factored model for natural languageparsing.
In Advances in Neural Information Processing Systems 15 (NIPS 2002).1972Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.
Determining an author?s native language by mining a textfor errors.
In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery indata mining (KDD ?05), pages 624?628, New York.William Labov.
1972.
Sociolinguistic Patterns.
University of Pennsylvania Press.Andr?e Lynum.
2013.
Native language identification using large scale lexical features.
In Proceedings of the8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,Atlanta, GA.Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.
2013.
Nli shared task 2013: Mq submission.
InProceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) atNAACL-HLT 2013, Atlanta, GA.Detmar Meurers, Julia Krivanek, and Serhiy Bykh.
2013.
On the automatic analysis of learner corpora: Nativelanguage identification as experimental testbed of language modeling between surface features and linguisticabstraction.
In Diachrony and Synchrony in English Corpus Studies, Frankfurt am Main.
Peter Lang.Mick Randall and Nicholas Groom.
2009.
The BUiD Arab learner corpus: a resource for studying the acquisitionof L2 english spelling.
In Proceedings of the Corpus Linguistics Conference (CL), Liverpool, UK.Benjamin Swanson and Eugene Charniak.
2012.
Native language detection with tree substitution grammars.
InProceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: ShortPapers), pages 193?197, Jeju Island, Korea, July.
Association for Computational Linguistics.Ben Swanson and Eugene Charniak.
2013.
Extracting the native language signal for second language acquisition.In Proceedings of NAACL-HLT.
Association for Computational Linguistics.Ben Swanson.
2013.
Exploring syntactic representations for native language identification.
In Proceedings of the8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,Atlanta, GA.Sali A. Tagliamonte.
2011.
Variationist Sociolinguistics: Change, Observation, Interpretation.
John Wiley &Sons.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Martin Chodorow.
2012.
Native tongues, lost and found:Resources and empirical evaluations in native language identification.
In Proceedings of the 24th InternationalConference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.
A report on the first native language identification sharedtask.
In Proceedings of the Eighth Workshop on Building Educational Applications Using NLP, Atlanta, GA,USA, June.
Association for Computational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploiting parse structures for native language identification.
InProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,Edinburgh, Scotland, UK., July.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume 1, HLT ?11, pages 180?189, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.
Corpus available: http://ilexir.co.uk/applications/clc-fce-dataset.1973
