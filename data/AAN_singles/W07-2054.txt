Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 253?256,Prague, June 2007. c?2007 Association for Computational LinguisticsNUS-PT: Exploiting Parallel Texts forWord Sense Disambiguation in the English All-Words TasksYee Seng Chan and Hwee Tou Ng and Zhi ZhongDepartment of Computer Science, National University of Singapore3 Science Drive 2, Singapore 117543{chanys, nght, zhongzhi}@comp.nus.edu.sgAbstractWe participated in the SemEval-2007coarse-grained English all-words taskand fine-grained English all-words task.We used a supervised learning approachwith SVM as the learning algorithm.
Theknowledge sources used include local col-locations, parts-of-speech, and surroundingwords.
We gathered training examplesfrom English-Chinese parallel corpora,SEMCOR, and DSO corpus.
While thefine-grained sense inventory of WordNetwas used to train our system employed forthe fine-grained English all-words task, oursystem employed for the coarse-grainedEnglish all-words task was trained with thecoarse-grained sense inventory released bythe task organizers.
Our scores (for bothrecall and precision) are 0.825 and 0.587for the coarse-grained English all-wordstask and fine-grained English all-words taskrespectively.
These scores put our systemsin the first place for the coarse-grainedEnglish all-words task1 and the secondplace for the fine-grained English all-wordstask.1 IntroductionIn this paper, we describe the systems we devel-oped for the coarse-grained English all-words task1A system developed by one of the task organizers of thecoarse-grained English all-words task gave the highest over-all score for the coarse-grained English all-words task, but thisscore is not considered part of the official scores.and fine-grained English all-words task of SemEval-2007.
In the coarse-grained English all-words task,systems have to perform word sense disambiguation(WSD) of all content words (noun, adjective, verb,and adverb) occurring in five documents, using acoarse-grained version of the WordNet sense inven-tory.
In the fine-grained English all-words task, sys-tems have to predict the correct sense of verbs andhead nouns of the verb arguments occurring in threedocuments, according to the fine-grained sense in-ventory of WordNet.Results from previous SENSEVAL English all-words task have shown that supervised learninggives the best performance.
Further, the best per-forming system in SENSEVAL-3 English all-wordstask (Decadt et al, 2004) used training data gatheredfrom multiple sources, highlighting the importanceof having a large amount of training data.
Hence,besides gathering examples from the widely usedSEMCOR corpus, we also gathered training exam-ples from 6 English-Chinese parallel corpora and theDSO corpus (Ng and Lee, 1996).We developed 2 separate systems; one for eachtask.
For both systems, we performed supervisedword sense disambiguation based on the approachof (Lee and Ng, 2002) and using Support VectorMachines (SVM) as our learning algorithm.
Theknowledge sources used include local collocations,parts-of-speech (POS), and surrounding words.
Oursystem employed for the coarse-grained English all-words task was trained with the coarse-grained senseinventory released by the task organizers, while oursystem employed for the fine-grained English all-words task was trained with the fine-grained sense253inventory of WordNet.In the next section, we describe the differentsources of training data used.
In Section 3, we de-scribe the knowledge sources used by the learningalgorithm.
In Section 4, we present our official eval-uation results, before concluding in Section 5.2 Training CorporaWe gathered training examples from parallel cor-pora, SEMCOR (Miller et al, 1994), and the DSOcorpus.
In this section, we describe these corporaand how examples gathered from them are combinedto form the training data used by our systems.
Asthese data sources use an earlier version of the Word-Net sense inventory as compared to the test data ofthe two tasks we participated in, we also discuss theneed to map between different versions of WordNet.2.1 Parallel TextResearch in (Ng et al, 2003; Chan and Ng, 2005)has shown that examples gathered from parallel textsare useful for WSD.
In this evaluation, we gath-ered training data from 6 English-Chinese parallelcorpora (Hong Kong Hansards, Hong Kong News,Hong Kong Laws, Sinorama, Xinhua News, andEnglish translation of Chinese Treebank), availablefrom the Linguistic Data Consortium (LDC).
Togather examples from these parallel corpora, we fol-lowed the approach in (Ng et al, 2003).
Briefly, af-ter ensuring the corpora were sentence-aligned, wetokenized the English texts and performed word seg-mentation on the Chinese texts (Low et al, 2005).We then made use of the GIZA++ software (Och andNey, 2000) to perform word alignment on the paral-lel corpora.
Then, we assigned some possible Chi-nese translations to each sense of an English wordw.
From the word alignment output of GIZA++, weselected those occurrences of w which were alignedto one of the Chinese translations chosen.
The En-glish side of these occurrences served as trainingdata for w, as they were considered to have been dis-ambiguated and ?sense-tagged?
by the appropriateChinese translations.We note that frequently occurring words are usu-ally highly polysemous and hard to disambiguate.To maximize the benefits of using parallel texts, wegathered training data from parallel texts for the setof most frequently occurring noun, adjective, andverb types in the Brown Corpus (BC).
These wordtypes (730 nouns, 326 adjectives, and 190 verbs)represent 60% of the noun, adjective, and verb to-kens in BC.2.2 SEMCORThe SEMCOR corpus (Miller et al, 1994) is oneof the few currently available, manually sense-annotated corpora for WSD.
It is widely used byvarious systems which participated in the Englishall-words task of SENSEVAL-2 and SENSEVAL-3,including one of the top performing teams (Hosteet al, 2001; Decadt et al, 2004) which had per-formed consistently well in both SENSEVAL all-words tasks.
Hence, we also gathered examplesfrom SEMCOR as part of our training data.2.3 DSO CorpusBesides SEMCOR, the DSO corpus (Ng and Lee,1996) also contains manually annotated examplesfor WSD.
As part of our training data, we gath-ered training examples for each of the 70 verb typespresent in the DSO corpus.2.4 Combination of Training DataSimilar to the top performing supervised systemsof previous SENSEVAL all-words tasks, we usedthe annotated examples available from the SEMCORcorpus as part of our training data.
In gathering ex-amples from parallel texts, a maximum of 1,000 ex-amples were gathered for each of the frequently oc-curring noun and adjective types, while a maximumof 500 examples were gathered for each of the fre-quently occurring verb types.
In addition, a max-imum of 500 examples were gathered for each ofthe verb types present in the DSO corpus.
For eachword, the examples from the parallel corpora andDSO corpus were randomly chosen but adhering tothe sense distribution (proportion of each sense) ofthat word in the SEMCOR corpus.2.5 Sense InventoryThe test data of the two SemEval-2007 tasks we par-ticipated in are based on the WordNet-2.1 sense in-ventory.
However, the examples we gathered fromthe parallel texts and the SEMCOR corpus are basedon the WordNet-1.7.1 sense inventory.
Hence, there254is a need to map these examples from WordNet-1.7.1to WordNet-2.1 sense inventory.
For this, we relyprimarily on the WordNet sense mappings automat-ically generated by the work of (Daude et al, 2000).To ensure the accuracy of the mappings, we per-formed some manual corrections of our own, focus-ing on the set of most frequently occurring nouns,adjectives, and verbs.
For the verb examples fromthe DSO corpus which are based on the WordNet-1.5 sense inventory, we manually mapped them toWordNet-2.1 senses.3 WSD SystemFollowing the approach of (Lee and Ng, 2002), wetrain an SVM classifier for each word using theknowledge sources of local collocations, parts-of-speech (POS), and surrounding words.
We omit thesyntactic relation features for efficiency reasons.
Forlocal collocations, we use 11 features: C?1,?1, C1,1,C?2,?2, C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1,C?1,2, and C1,3, where Ci,j refers to the orderedsequence of tokens in the local context of an am-biguous word w. Offsets i and j denote the startingand ending position (relative to w) of the sequence,where a negative (positive) offset refers to a tokento its left (right).
For parts-of-speech, we use 7 fea-tures: P?3, P?2, P?1, P0, P1, P2, P3, where P0 isthe POS of w, and P?i (Pi) is the POS of the ith to-ken to the left (right) of w. For surrounding words,we consider all unigrams (single words) in the sur-rounding context of w. These words can be in a dif-ferent sentence from w.4 EvaluationWe participated in two tasks of SemEval-2007: thecoarse-grained English all-words task and the fine-grained English all-words task.
In both tasks, whenthere is no training data at all for a particular word,we tag all test examples of the word with its firstsense in WordNet.
Since our systems give exactlyone answer for each test example, recall is the sameas precision.
Hence we will just report the micro-average recall in this section.4.1 Coarse-Grained English All-Words TaskOur system employed for the coarse-grained En-glish all-words task was trained with the coarse-English all-words Training datatask SC+DSO SC+DSO+PTCoarse-grained 0.817 0.825Fine-grained 0.578 0.587Table 1: Scores for the coarse-grained English all-words task and fine-grained English all-words task,using different sets of training data.
SC+DSOrefers to using examples gathered from SEMCORand DSO corpus.
Similarly, SC+DSO+PT refers tousing examples gathered from SEMCOR, DSO cor-pus, and parallel texts.Doc-ID Recall No.
of test instancesd001 0.883 368d002 0.881 379d003 0.834 500d004 0.761 677d005 0.814 345Table 2: Score of each individual test document, forthe coarse-grained English all-words task.grained WordNet-2.1 sense inventory released bythe task organizers.
We obtained a score of 0.825in this task, as shown in Table 1 under the columnSC + DSO + PT .
It turns out that among the16 participants of this task, the system which re-turned the best score was developed by one of thetask organizers.
Since the score of this system isnot considered part of the official scores, our scoreputs our system in the first position among the par-ticipants of this task.
For comparison, the WordNetfirst sense baseline score as calculated by the taskorganizers is 0.789.
To gauge the contribution ofparallel text examples, we retrained our system us-ing only examples gathered from the SEMCOR andDSO corpus.
As shown in Table 1 under the col-umn SC + DSO, this gives a score of 0.817 whenscored against the answer keys released by the taskorganizers.
Although adding examples from paralleltexts gives only a modest improvement in the scores,we note that this improvement is achieved from arelatively small set of word types which are foundto be frequently occurring in BC.
Future work canexplore expanding the set of word types by automat-ing the process of assigning Chinese translations toeach sense of an English word, with the use of suit-255able bilingual lexicons.As part of the evaluation results, the task organiz-ers also released the scores of our system on each ofthe 5 test documents.
We show in Table 2 the scorewe obtained for each document, along with the to-tal number of test instances in each document.
Wenote that our system obtained a relatively low scoreon the fourth document, which is a Wikipedia entryon computer programming.
To determine the rea-son for the low score, we looked through the list oftest words in that document.
We noticed that thenoun program has 20 test instances occurring in thatfourth document.
From the answer keys released bythe task organizers, all 20 test instances belong to thesense of ?a sequence of instructions that a computercan interpret and execute?, which we do not haveany training examples for.
Similarly, we noticed thatanother noun programming has 27 test instances oc-curring in the fourth document which belong to thesense of ?creating a sequence of instructions to en-able the computer to do something?, which we donot have any training examples for.
Thus, these twowords alone account for 47 of the errors made by oursystem in this task, representing 2.1% of the 2,269test instances of this task.4.2 Fine-Grained English All-Words TaskOur system employed for the fine-grained Englishall-words task was trained on examples taggedwith fine-grained WordNet-2.1 senses (mapped fromWordNet-1.7.1 senses and 1.5 senses as describedearlier).
Unlike the coarse-grained English all-words task, the correct POS tag and lemma of eachtest instance are not given in the fine-grained task.Hence, we used the POS tag from the mrg parsefiles released as part of the test data and performedlemmatization using WordNet.
We obtained a scoreof 0.587 in this task, as shown in Table 1.
This ranksour system in second position among the 14 partic-ipants of this task.
If we exclude parallel text ex-amples and train only on examples gathered fromthe SEMCOR and DSO corpus, we obtain a score of0.578.5 ConclusionIn this paper, we describe the approach taken byour systems which participated in the coarse-grainedEnglish all-words task and fine-grained English all-words task of SemEval-2007.
Using training exam-ples gathered from parallel texts, SEMCOR, and theDSO corpus, we trained supervised WSD systemswith SVM as the learning algorithm.
Evaluation re-sults show that this approach achieves good perfor-mance in both tasks.6 AcknowledgementsYee Seng Chan is supported by a Singapore Millen-nium Foundation Scholarship (ref no.
SMF-2004-1076).ReferencesYee Seng Chan and Hwee Tou Ng.
2005.
Scaling up wordsense disambiguation via parallel texts.
In Proc.
of AAAI05,pages 1037?1042.Jordi Daude, Lluis Padro, and German Rigau.
2000.
MappingWordNets using structural information.
In Proc.
of ACL00,pages 504?511.Bart Decadt, Veronique Hoste, Walter Daelemans, and Antalvan den Bosch.
2004.
GAMBL, genetic algorithm opti-mization of memory-based WSD.
In Proc.
of SENSEVAL-3,pages 108?112.Veronique Hoste, Anne Kool, and Walter Daelemans.
2001.Classifier optimization and combination in the English allwords task.
In Proc.
of SENSEVAL-2, pages 83?86.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empirical evalu-ation of knowledge sources and learning algorithms for wordsense disambiguation.
In Proc.
of EMNLP02, pages 41?48.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.
Amaximum entropy approach to Chinese word segmentation.In Proc.
of the Fourth SIGHAN Workshop on Chinese Lan-guage Processing, pages 161?164.George A. Miller, Martin Chodorow, Shari Landes, ClaudiaLeacock, and Robert G. Thomas.
1994.
Using a seman-tic concordance for sense identification.
In Proc.
of HLT94Workshop on Human Language Technology, pages 240?243.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrating mul-tiple knowledge sources to disambiguate word sense: Anexemplar-based approach.
In Proc.
of ACL96, pages 40?47.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Exploit-ing parallel texts for word sense disambiguation: An empir-ical study.
In Proc.
of ACL03, pages 455?462.Franz Josef Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In Proc.
of ACL00, pages 440?447.256
