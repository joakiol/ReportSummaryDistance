Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?19,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsUnderstanding Differences in Perceived Peer-Review Helpfulness usingNatural Language ProcessingWenting XiongUniversity of PittsburghDepartment of Computer SciencePittsburgh, PA, 15260wex12@cs.pitt.eduDiane LitmanUniversity of PittsburghDepartment of Computer Science &Learning Research and Development CenterPittsburgh, PA, 15260litman@cs.pitt.eduAbstractIdentifying peer-review helpfulness is an im-portant task for improving the quality of feed-back received by students, as well as for help-ing students write better reviews.
As we tailorstandard product review analysis techniques toour peer-review domain, we notice that peer-review helpfulness differs not only betweenstudents and experts but also between typesof experts.
In this paper, we investigate howdifferent types of perceived helpfulness mightinfluence the utility of features for automaticprediction.
Our feature selection results showthat certain low-level linguistic features aremore useful for predicting student perceivedhelpfulness, while high-level cognitive con-structs are more effective in modeling experts?perceived helpfulness.1 IntroductionPeer review of writing is a commonly recommendedtechnique to include in good writing instruction.
Itnot only provides more feedback compared to whatstudents might get from their instructors, but alsoprovides opportunities for students to practice writ-ing helpful reviews.
While existing web-based peer-review systems facilitate peer review from the logis-tic aspect (e.g.
collecting papers from authors, as-signing reviewers, and sending reviews back), therestill remains the problem that the quality of peerreviews varies, and potentially good feedback isnot written in a helpful way.
To address this is-sue, we propose to add a peer-review helpfulnessmodel to current peer-review systems, to automat-ically predict peer-review helpfulness based on fea-tures mined from textual reviews using Natural Lan-guage Processing (NLP) techniques.
Such an intel-ligent component could enable peer-review systemsto 1) control the quality of peer reviews that are sentback to authors, so authors can focus on the help-ful ones; and 2) provide feedback to reviewers withrespect to their reviewing performance, so studentscan learn to write better reviews.In our prior work (Xiong and Litman, 2011), weexamined whether techniques used for predicting thehelpfulness of product reviews (Kim et al, 2006)could be tailored to our peer-review domain, wherethe definition of helpfulness is largely influenced bythe educational context of peer review.
While previ-ously we used the average of two expert-providedratings as our gold standard of peer-review help-fulness1, there are other types of helpfulness rating(e.g.
author perceived helpfulness) that could be thegold standard, and that could potentially impact thefeatures used to build the helpfulness model.
In fact,we observe that peer-review helpfulness seems todiffer not only between students and experts (exam-ple 1), but also between types of experts (example2).In the following examples, students judge helpful-ness with discrete ratings from one to seven; expertsjudge it using a one to five scale.
Higher ratings onboth scales correspond to the most helpful reviews.Example 1:Student rating = 7, Average expert rating = 2 The1Averaged ratings are considered more reliable since theyare less noisy.10author also has great logic in this paper.
How canwe consider the United States a great democracywhen everyone is not treated equal.
All of the mainpoints were indeed supported in this piece.Student rating = 3, Average expert rating = 5 Ithought there were some good opportunities toprovide further data to strengthen your argument.For example the statement ?These methods ofintimidation, and the lack of military force offeredby the government to stop the KKK, led to therescinding of African American democracy.
?Maybe here include data about how .
.
.
(126 words)Example 2:Writing-expert rating = 2, Content-expert rating = 5Your over all arguements were organized in someorder but was unclear due to the lack of thesis inthe paper.
Inside each arguement, there was noorder to the ideas presented, they went back andforth between ideas.
There was good support tothe arguements but yet some of it didnt not fit yourarguement.Writing-expert rating = 5, Content-expert rating = 2First off, it seems that you have difficulty writingtransitions between paragraphs.
It seems that youend your paragraphs with the main idea of eachparagraph.
That being said, .
.
.
(173 words) As afinal comment, try to continually move your paper,that is, have in your mind a logical flow with everyparagraph having a purpose.To better understand such differences and inves-tigate their impact on automatically assessing peer-review helpfulness, in this paper, we compare help-fulness predictions using our many different pos-sibilities for gold standard ratings.
In particular,we compare the predictive ability of features acrossgold standard ratings by examining the most use-ful features and feature ranks using standard featureselection techniques.
We show that paper ratingsand lexicon categories that suggest clear transitionsand opinions are most useful in predicting helpful-ness as perceived by students, while review lengthis generally effective in predicting expert helpful-ness.
While the presence of praise and summarycomments are more effective in modeling writing-expert helpfulness, providing solutions is more use-ful in predicting content-expert helpfulness.2 Related WorkTo our knowledge, no prior work on peer reviewfrom the NLP community has attempted to auto-matically predict peer-review helpfulness.
Instead,the NLP community has focused on issues such ashighlighting key sentences in papers (Sandor andVorndran, 2009), detecting important feedback fea-tures in reviews (Cho, 2008; Xiong and Litman,2010), and adapting peer-review assignment (Gar-cia, 2010).
However, many NLP studies have beendone on the helpfulness of other types of reviews,such as product reviews (Kim et al, 2006; Ghoseand Ipeirotis, 2010), movie reviews (Liu et al,2008), book reviews (Tsur and Rappoport, 2009),etc.
Kim et al (2006) used regression to predict thehelpfulness ranking of product reviews based on var-ious classes of linguistic features.
Ghose and Ipeiro-tis (2010) further examined the socio-economic im-pact of product reviews using a similar approachand suggested the usefulness of subjectivity analy-sis.
Another study (Liu et al, 2008) of movie re-views showed that helpfulness depends on review-ers?
expertise, their writing style, and the timelinessof the review.
Tsur and Rappoport (2009) proposedRevRank to select the most helpful book reviews inan unsupervised fashion based on review lexicons.To tailor the utility of this prior work on help-fulness prediction to educational peer reviews, wewill draw upon research on peer review in cognitivescience.
One empirical study of the nature of peer-review feedback (Nelson and Schunn, 2009) foundthat feedback implementation likelihood is signif-icantly correlated with five feedback features.
Ofthese features, problem localization ?pinpointingthe source of the problem and/or solution in the orig-inal paper?
and solution ?providing a solution tothe observed problem?
were found to be most im-portant.
Researchers (Cho, 2008; Xiong and Lit-man, 2010) have already shown that some of theseconstructs can be automatically learned from tex-tual input using Machine Learning and NLP tech-niques.
In addition to investigating what proper-ties of textual comments make peer-review helpful,researchers also examined how the comments pro-duced by students versus by different types of ex-perts differ (Patchan et al, 2009).
Though focusingon differences between what students and experts11produce, such work sheds light on our study of stu-dents?
and experts?
helpfulness ratings of the samestudent comments (i.e.
what students and expertsvalue).Our work in peer-review helpfulness predictionintegrates the NLP techniques and cognitive-scienceapproaches mentioned above.
We will particularlyfocus on examining the utility of features motivatedby related work from both areas, with respect to dif-ferent types of gold standard ratings of peer-reviewhelpfulness for automatic prediction.3 DataIn this study, we use a previously annotated peer-review corpus (Nelson and Schunn, 2009; Patchanet al, 2009) that was collected in an introduc-tory college history class using the freely availableweb-based peer-review SWoRD (Scaffolded Writ-ing and Rewriting in the Discipline) system (Choand Schunn, 2007).
The corpus consists of 16 pa-pers (about six pages each) and 189 reviews (vary-ing from twenty words to about two hundred words)accompanied by numeric ratings of the papers.
Eachreview was manually segmented into idea units (de-fined as contiguous feedback referring to a singletopic) (Nelson and Schunn, 2009), and these ideaunits were then annotated by two independent an-notators for various coding categories, such as feed-back type (praise, problem, and summary), problemlocalization, solution, etc.
For example, the sec-ond case in Example 1, which only has one ideaunit, was annotated as feedbackType = problem,problemlocalization = True, and solution =True.
The agreement (Kappa) between the two an-notators is 0.92 for FeedbackType, 0.69 for localiza-tion, and 0.89 for solution.2Our corpus also contains author provided backevaluations.
At the end of the peer-review assign-ment, students were asked to provide back evalu-ation on each review that they received by ratingreview helpfulness using a discrete scale from oneto seven.
After the corpus was collected, one writ-2For Kappa value interpretation, Landis and Koch (1977)propose the following agreement standard: 0.21-0.40 = ?Fair?
;0.41-0.60 = ?Moderate?
; 0.61-0.80 = ?Substantial?
; 0.81-1.00= ?Almost Perfect?.
Thus, while localization signals are moredifficult to annotate, the inter-annotator agreement is still sub-stantial.ing expert and one content expert were also asked torate review helpfulness with a slightly different scalefrom one to five.
For our study, we will also com-pute the average ratings given by the two experts,yielding four types of possible gold-standard ratingsof peer-review helpfulness for each review.
Figure 1shows the rating distribution of each type.
Interest-ingly, we observed that expert ratings roughly followa normal distribution, while students are more likelyto give higher ratings (as illustrated in Figure 1).4 FeaturesOur features are motivated by the prior work in-troduced in Section 2, in particular, NLP work onpredicting product-review helpfulness (Kim et al,2006), as well as work on automatically learningcognitive-science constructs (Nelson and Schunn,2009) using NLP (Cho, 2008; Xiong and Litman,2010).
The complete list of features is shown in Ta-ble 3 and described below.
The computational lin-guistic features are automatically extracted basedon the output of syntactic analysis of reviews andpapers3.
These features represent structural, lexi-cal, syntactic and semantic information of the tex-tual content, and also include information for identi-fying certain important cognitive constructs:?
Structural features consider the general struc-ture of reviews, which includes review length interms of tokens (reviewLength), number of sen-tences (sentNum), the average sentence length(sentLengthAve), percentage of sentences thatend with question marks (question%), andnumber of exclamatory sentences (exclams).?
Lexical features are counts of ten lexical cat-egories (Table 1), where the categories werelearned in a semi-supervised way from reviewlexicons in a pilot study.
We first manually cre-ated a list of words that were specified as signalwords for annotating feedbackType and prob-lem localization in the coding manual; thenwe supplemented the list with words selectedby a decision tree model learned using a Bag-of-Words representation of the peer reviews.3We used MSTParser (McDonald et al, 2005) for syntacticanalysis.12Figure 1: Distribution of peer-review helpfulness when rated by students and expertsTag Meaning Word listSUG suggestion should, must, might, could, need, needs, maybe, try, revision, wantLOC location page, paragraph, sentenceERR problem error, mistakes, typo, problem, difficulties, conclusionIDE idea verb consider, mentionLNK transition however, butNEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, morePOS positive words great, good, well, clearly, easily, effective, effectively, helpful, verySUM summarization main, overall, also, how, jobNOT negation not, doesn?t, don?tSOL solution revision specify correctionTable 1: Ten lexical categoriesCompared with commonly used lexical uni-grams and bigrams (Kim et al, 2006), theselexical categories are equally useful in model-ing peer-review helpfulness, and significantlyreduce the feature space.4?
Syntactic features mainly focus on nouns andverbs, and include percentage of tokens that arenouns, verbs, verbs conjugated in the first per-son (1stPVerb%), adjectives/adverbs, and openclasses, respectively.?
Semantic features capture two important peer-4Lexical categories help avoid the risk of over-fitting, givenonly 189 peer reviews in our case compared to more than tenthousand Amazon.com reviews used for predicting product re-view helpfulness (Kim et al, 2006).review properties: their relevance to the maintopics in students?
papers, and their opinionsentiment polarities.
Kim et al (2006) ex-tracted product property keywords from exter-nal resources based on their hypothesis thathelpful product reviews refer frequently to cer-tain product properties.
Similarly, we hypothe-size that helpful peer reviews are closely relatedto domain topics that are shared by all studentspapers in an assignment.
Our domain topic setcontains 288 words extracted from the collec-tion of student papers using topic-lexicon ex-traction software5; our feature (domainWord)5The software extracts topic words based on topic signa-tures (Lin and Hovy, 2000), and was kindly provided by AnnieLouis.13Feature DescriptionregTag% The percentage of problems in reviews that could be matched with a localization pattern.soDomain% The percentage of sentences where any domain word appears between the subject and the object.dDeterminer The number of demonstrative determiners.windowSize For each review sentence, we search for the most likely referred window of words in the relatedpaper, and windowSize is the average number of words of all windows.Table 2: Localization featurescounts how many words of a given review be-long to the extracted set.
For sentiment po-larities, we extract positive and negative sen-timent words from the General Inquirer Dictio-naries 6, and count their appearance in reviewsin terms of their sentiment polarity (posWord,negWord).?
Localization features are motivated by lin-guistic features that are used for automaticallypredicting problem localization (an importantcognitive construct for feedback understand-ing and implementation) (Nelson and Schunn,2009), and are presented in Table 2.
To illus-trate how these features are computed, considerthe following critique:The section of the essay on AfricanAmericans needs more careful at-tention to the timing and reasonsfor the federal governments decisionto stop protecting African Americancivil and political rights.The review has only one sentence, in which oneregular expression is matched with ?the sectionof?
thus regTag% = 1; no demonstrative de-terminer, thus dDeterminer = 0; ?African?and ?Americans?
are domain words appearingbetween the subject ?section?
and the object?attention?, so soDomain is true for this sen-tence and thus soDomain% = 1 for the givenreview.In addition to the low-level linguistic features pre-sented above, we also examined non-linguistic fea-tures that are derived from the ratings and priormanual annotations of the corpus, described in Sec-tion 3.6http://www.wjh.harvard.edu/ inquirer/homecat.htm?
Cognitive-science features are motivated byan empirical study (Nelson and Schunn, 2009)which suggests significant correlation betweencertain cognitive constructs (e.g.
feedbackType,problem localization, solution) and review im-plementation likelihood.
Intuitively, helpfulreviews are more likely to get implemented,thus we introduced these features to capturedesirable high-level characteristics of peer re-views.
Note that in our corpus these cogni-tive constructs are manually coded at the idea-unit level (Nelson and Schunn, 2009), how-ever, peer-review helpfulness is rated at the re-view level.7 Our cognitive-science features ag-gregate the annotations up to the review-levelby reporting the percentage of idea-units ina review that exhibit each characteristic: thedistribution of review types (praise%, prob-lem%, summary%), the percentage of problem-localized critiques (localization%), as well asthe percentage of solution-provided ones (solu-tion%).?
Social-science features introduce elements re-flecting interactions between students in a peer-review assignment.
As suggested in relatedwork on product review helpfulness (Kimet al, 2006; Danescu-Niculescu-Mizil et al,2009), some social dimensions (e.g.
customeropinion on related product quality) are of greatinfluence in the perceived helpfulness of prod-uct reviews.
Similarly, in our case, we intro-duced related paper ratings (pRating) ?
to con-sider whether and how helpfulness ratings areaffected by the rating that the paper receives8?
and the absolute difference between the rat-7Details of different granularity levels of annotation can befound in (Nelson and Schunn, 2009).8That is, to examine whether students give higher ratings topeers who gave them higher paper ratings in the first place.14ing and the average score given by all review-ers (pRatingDiff ) ?
to measure the variation inperceived helpfulness of a given review.5 ExperimentsWe take a machine learning approach to model dif-ferent types of perceived helpfulness (student help-fulness, writing-expert helpfulness, content-experthelpfulness, average-expert helpfulness) based oncombinations of linguistic and non-linguistic fea-tures extracted from our peer-review corpus.
Thenwe compare the different helpfulness types in termsof the predictive power of features used in their cor-responding models.
For comparison purpose, weconsider the linguistic and non-linguistic featuresboth separately and in combination, which generatesthree set of features: 1) linguistic features, 2) non-linguistic features, and 3) all features.
For each setof features, we train four models, each correspond-ing to a different kind of helpfulness rating.
For eachlearning task (three by four), we use two standardfeature selection algorithms to find the most usefulfeatures based on 10-fold cross validation.
First, weperform Linear Regression with Greedy Stepwisesearch (stepwise LR) to select the most useful fea-tures when testing in each of the ten folds, and counthow many times each features is selected in the tentrials.
Second, we use Relief Feature Evaluation9with Ranker (Relief) (Kira and Rendell, 1992; Wit-ten and Frank, 2005) to rank all used features basedon their average merits (the ability of the given fea-ture to differentiate between two example pairs) often trials.10Although both methods are supervised, the wrap-per is ?more aggressive?
because its feature evalu-ation is based on the performance of the regressionmodel and thus the resulting feature set is tailoredto the learning algorithm.
In contrast, Relief doesnot optimize feature sets directly for classifier per-formance, thus it takes into account class informa-tion in a ?less aggressive?
manner than the Wrappermethod.
We use both methods in our experiment to9Relief evaluates the worth of an attribute by repeatedlysampling an instance and changing the value of the given at-tribute based on the nearest instance of the same and differentclass.10Both algorithms are provided by Weka(http://www.cs.waikato.ac.nz/ml/weka/).provide complementary perspectives.
While the for-mer can directly tell us what features are most use-ful, the latter gives feature ranks which provide moredetailed information about differences between fea-tures.
To compare the feature selection results, weexamine the four kind of helpfulness models foreach of the three feature sets separately, as presentedbelow.
Note that the focus of this paper is compar-ing feature utilities in different helpfulness modelsrather than predicting those types of helpfulness rat-ings.
(Details of how the average-expert model per-forms can be found in our prior work (Xiong andLitman, 2011).
)5.1 Feature Selection of Linguistic FeaturesTable 4 presents the feature selection results of com-putational linguistic features used in modeling thefour different types of peer-review helpfulness.
Thefirst row lists the four sources of helpfulness ratings,and each column represents a corresponding model.The second row presents the most useful featuresin each model selected by stepwise LR, where ?#of folds?
refers to the number of trials in which thegiven feature appears in the resulting feature set dur-ing the 10-fold cross validation.
Here we only reportfeatures that are selected by no less than five folds(half the time).
The third row presents feature rankscomputed using Relief, where we only report the topsix features due to the space limit.
Features are or-dered in descending ranks, and the average merit andits standard deviation is reported for each one of thefeatures.The selection result of stepwise LR shows thatreviewLength is most useful for predicting experthelpfulness in general, while specific lexicon cate-gories (i.e.
LNK, and NOT) and positive words (pos-Word) are more useful in predicting student helpful-ness.
When looking at the ranking result, we observethat transition cues (LNK) and posWord are alsoranked high in the student-helpfulness model, al-though question% and suggestion words (SUG) areranked highest.
For expert-helpfulness models, win-dowSize and posWord, which are not listed in the se-lected features for expert helpfulness (although theyare selected for students), are actually ranked highfor modeling average-expert helpfulness.
While ex-clamatory sentence number (exclams) and summa-rization cues are ranked top for the writing expert,15Type FeaturesStructural reviewLength, sentNum, sentLengthAve, question%, exclamsLexical SUG, LOC, ERR, IDE, LNK, NEG, POS, SUM, NOT, SOL (Table 1)Syntactic noun%, verb%, 1stPVerb%, adj+adv%, opClass%Semantic domainWord, posWord, negWordLocalization regTag%, soDomain%, dDeterminer, windowSize (Table 2)Cognitive-science praise%, problem%, summary%, localization%, solution%Social-science pRating, pRatingDiffTable 3: Summary of featuresSource Students Writing expert Content expert Expert averageFeature # of folds Feature # of folds Feature # of folds Feature # of foldsLNK 9 reviewLength 8 reviewLength 10 reviewLength 10Stepwise posWord 8 question% 6 sentNum 8LR NOT 6 sentNum 5 question% 8windowSize 6 1stPVerb% 5POS 5ReliefFeature Merit Feature Merit Feature Merit Feature Meritquestion% .019?
.002 exclams .010?
.003 question% .010?
.004 exclams .010?
.003SUG .015?
.003 SUM .008?
.004 ERR .009?
.003 question% .011?
.004LNK .014?
.003 NEG .006?
.004 SUG .009?
.004 windowSize .008?
.002sentLengthAve .012?
.003 negWord .005?
.002 posWord .007?
.002 posWord .006?
.002POS .011?
.002 windowSize .004?
.002 exclams .006?
.001 reviewLength .004?
.001posWord .010?
.001 sentNum .003?
.001 1stPVerb% .007?
.004 sentLengthAve .004?
.001Table 4: Feature selection based on linguistic featuresthe percentage of questions (question%) and errorcues (ERR) are ranked top for the content-expert.
Inaddition, the percentage of words that are verbs con-jugated in the first person (1stPVerb%) is both se-lected and ranked high in the content-expert helpful-ness model.
Out of the four models, SUG are rankedhigh for predicting both students and content-experthelpfulness.
These observations indicate that bothstudents and experts value questions (question%)and suggestions (SUG) in reviews, and students par-ticularly favor clear signs of logic flow in review ar-guments (LNK), positive words (posWord), as wellas reference of their paper content which providesexplicit context information (windowSize).
In addi-tion, experts in general prefer longer reviews (re-viewLength), and the writing expert thinks clearsummary signs (SUM) are important indicators ofhelpful peer reviews.5.2 Feature Selection of non-LinguisticFeaturesWhen switching to the high-level non-linguistic fea-tures (Table 5), we find that solution% is always se-lected (in all ten trials) as a most useful feature forpredicting all four kind of helpfulness, and is alsoranked high for content-expert and student helpful-ness.
Especially for the content-expert, solution%has a much higher merit (0.013) compared to all theother features (?
0.03).
This agrees with our ob-servation in section 5.1 that SUG are ranked high inboth cases.
localization% is selected as one of themost useful features in the content-expert helpful-ness model, which is also ranked top in the studentmodel (though not selected frequently by stepwiseLR).
For modeling the writing-expert helpfulness,praise (praise%) is more important than problemand summary, and the paper rating (pRating) losesits predictive power compared to how it works in theother models.
In contrast, pRating is both selectedand ranked high for predicting students?
perceivedhelpfulness.5.3 Feature Selection of All FeaturesWhen considering all features together as reportedin Table 6, pRating is only selected in the student-helpfulness model, and still remains to be the mostimportant feature for predicting students?
perceivedhelpfulness.
As for experts, the structural feature16Source Students Writing expert Content expert Expert averageFeature # of folds Feature # of folds Feature # of folds Feature # of foldsStepwise pRating 10 solution% 10 localization% 10 solution% 10LR solution% 10 solution% 10 pRating 10problem% 9 pRating 10 localization% 9ReliefFeature Merit Feature Merit Feature Merit Feature Meritlocalization% .012?
.003 praise% .008?
.002 solution% .013?
.005 problem% .004?
.002pRatingDiff .010?
.002 problem% .007?
.002 pRating .003?
.002 localization% .004?
.006pRating .007?
.002 summary% .001?
.004 praise% .001?
.002 praise% .003?
.003solution% .006?
.005 localization% .001?
.005 localization% .001?
.004 solution% .002?
.004problem% .004?
.002 pRating .004?
.004 problem% .001?
.002 pRating .005?
.003summary% .004?
.003 pRatingDiff .007?
.002 pRating .002?
.003 pRatingDiff .006?
.005Table 5: Feature selection based on non-linguistic featuresSource Students Writing expert Content expert Expert averageFeature # of folds Feature # of folds Feature # of folds Feature # of foldsStepwise pRating 10 reviewLength 10 reviewLength 10 reviewLength 10LR dDeterminer 7 problem% 8 problem% 6pRatingDiff 5sentNum 5ReliefFeature Merit Feature Merit Feature Merit Feature MeritpRating .030?
.006 exclams .016?
.003 solution% .025?
.003 exclams .015?
.004NOT .019?
.004 praise% .015?
.003 domainWord .012?
.002 question% .012?
.004pRatingDiff .019?
.005 SUM .013?
.004 regTag% .012?
.007 LOC .007?
.002sentNum .014?
.002 summary% .008?
.003 reviewLength .009?
.002 sentNum .007?
.002question% .014?
.003 problem% .009?
.003 question% .010?
.003 reviewLength .007?
.001NEG .013?
.002 reviewLength .004?
.001 sentNum .008?
.002 praise% .008?
.004Table 6: Feature selection based on all featuresreviewLength stands out from all other features inboth the writing-expert and the content-expert mod-els.
Interestingly, it is the number of sentences (sent-Num) rather than review length of structure featuresthat is useful in the student-helpfulness model.
Anddemonstrative determiners (dDeterminer) is also se-lected, which indicates that having a clear sign ofcomment targets is considered important from thestudents?
perspective.
When examining the model?sranking result, we find that more lexicon categoriesare ranked high for students compared to other kindof helpfulness.
Specifically, NOT appears highagain, suggesting clear expression of opinion is im-portant in predicting student-helpfulness.
Acrossfour types of helpfulness, again, we observed thatthe writing expert tends to value praise and summary(indicated by both SUM and summary%) in reviewswhile the content-expert favors critiques, especiallysolution provided critiques.5.4 DiscussionBased on our observations from the above threecomparisons, we summarize our findings with re-spect to different feature types and provide inter-pretation: 1) review length (in tokens) is generallyeffective in predicting expert perceived helpfulness,while number of sentences is more useful in mod-eling student perceived helpfulness.
Interestingly,there is a strong correlation between these two fea-tures (r = 0.91, p ?
0.001), and why one is selectedover the other in different helpfulness models needsfurther investigation.
2) Lexical categories such astransition cues, negation, and suggestion words areof more importance in modeling student perceivedhelpfulness.
This might indicate that students pre-fer clear expression of problem, reference and evenopinion in terms of specific lexicon clues, the lack ofwhich is likely to result in difficulty in their under-standing of the reviews.
3) As for cognitive-sciencefeatures, solution is generally an effective indica-tor of helpful peer reviews.
Within the three feed-back types of peer reviews, praise is valued highby the writing expert.
(It is interesting to noticethat although praise is shown to be more impor-tant than problem and summary for modeling thewriting-expert helpfulness, positive sentiment wordsdo not appear to be more predictive than negativesentiments.)
In contrast, problem is more desirable17from the content expert?s point of view.
Althoughstudents assign less importance to the problem them-selves, solution provided peer reviews could be help-ful for them with respect to the learning goal of peer-review assignments.
4) Paper rating is a very ef-fective feature for predicting review helpfulness per-ceived by students, which is not the case for eitherexpert.
This supports the argument of social aspectsin people?s perception of review helpfulness, and italso reflects the fact that students tend to be nice toeach other in such peer-review interactions.
How-ever, this dimension might not correspond with thereal helpfulness of the reviews, at least from the per-spective of both the writing expert and content ex-pert.6 Conclusion and Future WorkWe have shown that the type of helpfulness to bepredicted does indeed influence the utility of dif-ferent feature types for automatic prediction.
Low-level general linguistic features are more predic-tive when modeling students?
perceived helpfulness;high-level theory supported constructs are more use-ful in experts?
models.
However, in the related areaof automated essay scoring (Attali and Burstein,2006), others have suggested the need for the useof validated features related to meaningful dimen-sions of writing, rather than low-level (but easy toautomate) features.
In this perspective, our worksimilarly poses challenge to the NLP community interms of how to take into account the education-oriented dimensions of helpfulness when applyingtraditional NLP techniques of automatically pred-icating review helpfulness.
In addition, it is im-portant to note that predictive features of perceivedhelpfulness are not guaranteed to capture the natureof ?truly?
helpful peer reviews (in contrast to theperceived ones).In the future, we would like to investigate howto integrate useful dimensions of helpfulness per-ceived by different audiences in order to come upwith a ?true?
helpfulness gold standard.
We wouldalso like to explore more sophisticated features andother NLP techniques to improve our model of peer-review helpfulness.
As we have already built modelsto automatically predict certain cognitive constructs(problem localization and solution), we will replacethe annotated cognitive-science features used herewith their automatic predictions, so that we can buildour helpfulness model fully automatically.
Finally,we would like to integrate our helpfulness modelinto a real peer-review system and evaluate its per-formance extrinsically in terms of improving stu-dents?
learning and reviewing performance in futurepeer-review assignments.AcknowledgmentsThis work was supported by the Learning Researchand Development Center at the University of Pitts-burgh.
We thank Melissa Patchan and Chris Schunnfor generously providing the manually annotatedpeer-review corpus.
We are also grateful to MichaelLipschultz and Chris Schunn for their feedbackwhile writing this paper.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
The Journal of Technology,Learning and Assessment (JTLA), 4(3), February.Kwangsu Cho and Christian D. Schunn.
2007.
Scaf-folded writing and rewriting in the discipline: A web-based reciprocal peer review system.
Computers andEducation, 48:409?426.Kwangsu Cho.
2008.
Machine classification of peercomments in physics.
In Proceedings of the First In-ternational Conference on Educational Data Mining(EDM2008), pages 192?196.Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinberg, and Lillian Lee.
2009.
How opin-ions are received by online communities: A case studyon Amazon.com helpfulness votes.
In Proceedings ofWWW, pages 141?150.Raquel M. Crespo Garcia.
2010.
Exploring documentclustering techniques for personalized peer assessmentin exploratory courses.
In Proceedings of Computer-Supported Peer Review in Education (CSPRED) Work-shop in the Tenth International Conference on Intelli-gent Tutoring Systems (ITS 2010).Anindya Ghose and Panagiotis G. Ipeirotis.
2010.
Esti-mating the helpfunless and economic impact of prod-uct reviews: Mining text and reviewer characteristics.IEEE Transactions on Knowledge and Data Engineer-ing, 99.Soo-Min Kim, Patrick Pantel, Tim Chklovski, and MarcoPennacchiotti.
2006.
Automatically assessing reviewhelpfulness.
In Proceedings of the 2006 Conference18on Empirical Methods in Natural Language Process-ing (EMNLP2006), pages 423?430, Sydney, Australia,July.Kenji Kira and Larry A. Rendell.
1992.
A practicalapproach to feature selection.
In Derek H. Sleemanand Peter Edwards, editors, ML92: Proceedings ofthe Ninth International Conference on Machine Learn-ing, pages 249?256, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33:159?174.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text summa-rization.
In Proceedings of the 18th conference onComputational linguistics, volume 1 of COLING ?00,pages 495?501, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.2008.
Modeling and predicting the helpfulness of on-line reviews.
In Proceedings of the Eighth IEEE Inter-national Conference on Data Mining, pages 443?452,Los Alamitos, CA, USA.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 91?98, Stroudsburg, PA, USA.
Association forComputational Linguistics.Melissa M. Nelson and Christian D. Schunn.
2009.
Thenature of feedback: how different types of peer feed-back affect writing performance.
Instructional Sci-ence, 37(4):375?401.Melissa M. Patchan, Davida Charney, and Christian D.Schunn.
2009.
A validation study of students?
endcomments: Comparing comments by students, a writ-ing instructor, and a content instructor.
Journal ofWriting Research, 1(2):124?152.Agnes Sandor and Angela Vorndran.
2009.
Detect-ing key sentences for automatic assistance in peer-reviewing research articles in educational sciences.
InProceedings of the 47th Annual Meeting of the Associ-ation for Computational Linguistics and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the Asian Federation of Natural LanguageProcessing (ACL-IJCNLP), pages 36?44.Oren Tsur and Ari Rappoport.
2009.
Revrank: A fullyunsupervised algorithm for selecting the most helpfulbook reviews.
In Proceedings of the Third Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM2009), pages 36?44.IH Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques, SecondEdition.
Morgan Kaufmann, San Francisco, CA.Wenting Xiong and Diane Litman.
2010.
Identifyingproblem localization in peer-review feedback.
In Pro-ceedings of Tenth International Conference on Intelli-gent Tutoring Systems (ITS2010), volume 6095, pages429?431.Wenting Xiong and Diane Litman.
2011.
Automaticallypredicting peer-review helpfulness.
In Proceedings49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies(ACL/HLT), Portland, Oregon, June.19
