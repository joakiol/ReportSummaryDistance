A Unified Framework for Automatic Evaluation usingN-gram Co-Occurrence StatisticsRadu SORICUTInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292, USAradu@isi.eduEric BRILLMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAbrill@microsoft.comAbstractIn this paper we propose a unified frameworkfor automatic evaluation of NLP applicationsusing N-gram co-occurrence statistics.
Theautomatic evaluation metrics proposed to datefor Machine Translation and AutomaticSummarization are particular instances fromthe family of metrics we propose.
We showthat different members of the same family ofmetrics explain best the variations obtainedwith human evaluations, according to theapplication being evaluated (MachineTranslation, Automatic Summarization, andAutomatic Question Answering) and theevaluation guidelines used by humans forevaluating such applications.1 IntroductionWith the introduction of the BLEU metric formachine translation evaluation (Papineni et al2002), the advantages of doing automaticevaluation for various NLP applications havebecome increasingly appreciated: they allow forfaster implement-evaluate cycles (by by-passingthe human evaluation bottleneck), less variation inevaluation performance due to errors in humanassessor judgment, and, not least, the possibility ofhill-climbing on such metrics in order to improvesystem performance (Och 2003).
Recently, asecond proposal for automatic evaluation has comefrom the Automatic Summarization community(Lin and Hovy, 2003), with an automaticevaluation metric called ROUGE, inspired byBLEU but twisted towards the specifics of thesummarization task.An automatic evaluation metric is said to besuccessful if it is shown to have high agreementwith human-performed evaluations.
Humanevaluations, however, are subject to specificguidelines given to the human assessors whenperforming the evaluation task; the variation inhuman judgment is therefore highly influenced bythese guidelines.
It follows that, in order for anautomatic evaluation to agree with a human-performed evaluation, the evaluation metric usedby the automatic method must be able to account,at least to some degree, for the bias induced by thehuman evaluation guidelines.
None of theautomatic evaluation methods proposed to date,however, explicitly accounts for the differentcriteria followed by the human assessors, as theyare defined independently of the guidelines used inthe human evaluations.In this paper, we propose a framework forautomatic evaluation of NLP applications which isable to account for the variation in the humanevaluation guidelines.
We define a family ofmetrics based on N-gram co-occurrence statistics,for which the automatic evaluation metricsproposed to date for Machine Translation andAutomatic Summarization can be seen as particularinstances.
We show that different members of thesame family of metrics explain best the variationsobtained with human evaluations, according to theapplication being evaluated (Machine Translation,Automatic Summarization, and QuestionAnswering) and the guidelines used by humanswhen evaluating such applications.2 An Evaluation Plane for NLPIn this section we describe an evaluation planeon which we place various NLP applicationsevaluated using various guideline packages.
Thisevaluation plane is defined by two orthogonal axes(see Figure 1): an Application Axis, on which weorder NLP applications according to thefaithfulness/compactness ratio that characterizesthe application?s input and output; and a GuidelineAxis, on which we order various human guidelinepackages, according to the precision/recall ratiothat characterizes the evaluation guidelines.2.1 An Application Axis for EvaluationWhen trying to define what translating andsummarizing means, one can arguably suggest thata translation is some ?as-faithful-as-possible?rendering of some given input, whereas a summaryis some ?as-compact-as-possible?
rendering ofsome given input.
As such, Machine Translation(MT) and Automatic Summarization (AS) are onthe extremes of a faithfulness/compactness (f/c)ratio between inputs and outputs.
In between thesetwo extremes lie various other NLP applications: ahigh f/c ratio, although lower than MT?s,characterizes Automatic Paraphrasing (paraphrase:To express, interpret, or translate with latitude);close to the other extreme, a low f/c ratio, althoughhigher than AS?s, characterizes AutomaticSummarization with view-points (summarizationwhich needs to focus on a given point of view,extern to the document(s) to be summarized).Another NLP application, Automatic QuestionAnswering (QA), has arguably a close-to-1 f/cratio: the task is to render an answer about thething(s) inquired for in a question (the faithfulnessside), in a manner that is concise enough to beregarded as a useful answer (the compactnessside).2.2 An Guideline Axis for EvaluationFormal human evaluations make use of variousguidelines that specify what particular aspects ofthe output being evaluated are consideredimportant, for the particular application beingevaluated.
For example, human evaluations of MT(e.g., TIDES 2002 evaluation, performed by NIST)have traditionally looked at two different aspectsof a translation: adequacy (how much of thecontent of the original sentence is captured by theproposed translation) and fluency (how correct isthe proposed translation sentence in the targetlanguage).
In many instances, evaluationguidelines can be linearly ordered according to theprecision/recall (p/r) ratio they specify.
Forexample, evaluation guidelines for adequacyevaluation of MT  have a low p/r ratio, because ofthe high emphasis on recall (i.e., content isrewarded) and low emphasis on precision (i.e.,verbosity is not penalized); on the other hand,evaluation guidelines for fluency of MT have ahigh p/r ratio, because of the low emphasis onrecall (i.e., content is not rewarded) and highemphasis on wording (i.e., extraneous words arepenalized).
Another evaluation we consider in thispaper, the DUC 2001 evaluation for AutomaticSummarization (also performed by NIST), hadspecific guidelines for coverage evaluation, whichmeans a low p/r ratio, because of the highemphasis on recall (i.e., content is rewarded).
Lastbut not least, the QA evaluation for correctness wediscuss in Section 4 has a close-to-1 p/r ratio forevaluation guidelines (i.e., both correct content andprecise answer wording are rewarded).When combined, the application axis and theguideline axis define a plane in which particularevaluations are placed according to theirapplication/guideline coordinates.
In Figure 1 weillustrate this evaluation plane, and the evaluationexamples mentioned above are placed in this planeaccording to their coordinates.3 A Unified Framework for AutomaticEvaluationIn this section we propose a family of evaluationmetrics based on N-gram co-occurrence statistics.Such a family of evaluation metrics providesflexibility in terms of accommodating both variousNLP applications and various values ofprecision/recall ratio in the human guidelinepackages used to evaluate such applications.3.1 A Precision-focused Family of MetricsInspired by the work of Papineni et al (2002) onBLEU, we define a precision-focused family ofmetrics, using as parameter a non-negative integerN.
Part of the definition includes a list of stop-words (SW) and a function for extracting the stemof a given word (ST).Suppose we have a given NLP application forwhich we want to evaluate the candidate answerset Candidates for some input sequences, given aFigure 1: Evaluation plane for NLP applicationsadequacy evaluationTIDES?MT(2002)precisionrecallprecisionrecallfaithfulnesscompactnesslow faithfulnesscompactness ASMTfluency evaluationTIDES?MT(2002)QA(2004)correctness evaluationcoverage evaluationDUC?AS (2001)Guideline AxisQAlow highhighApplicationAxisreference answer set References.
For eachindividual candidate answer C, we define S(C,n)as the multi-set of n-grams obtained from thecandidate answer C after stemming the unigramsusing ST and eliminating the unigrams found inSW.
We therefore define a precision score:?
??
??
??
?=}{ ),(}{ ),()()()(CandidatesC nCSngramCandidatesC nCSngramclipngramCountngramCountnPwhere Count(ngram) is the number of n-gramcounts, and Countclip(ngram) is the maximumnumber of co-occurrences of ngram in thecandidate answer and its reference answer.Because the denominator in the P(n) formulaconsists of a sum over the proposed candidateanswers, this formula is a precision-orientedformula, penalizing verbose candidates.
Thisprecision score, however, can be made artificiallyhigher when proposing shorter and shortercandidate answers.
This is offset by adding abrevity penalty, BP:???<??
?= ?
||||,||||,1|)|/||1( rcBifercBifBP cBrwhere |c| equals the sum of the lengths of theproposed answers, |r| equals the sum of the lengthsof the reference answers, and B is a brevityconstant.We define now a precision-focused family ofmetrics, parameterized by a non-negative integerN, as:)))(log(exp()(1nPwBPNPSNnn?=?=This family of metrics can be interpreted as aweighted linear average of precision scores forincreasingly longer n-grams.
As the values of theprecision scores decrease roughly exponentiallywith the increase of N, the logarithm is needed toobtain a linear average.
Note that the metrics ofthis family are well-defined only for N?s smallenough to yield non-zero P(n) scores.
For testcorpora of reasonable size, the metrics are usuallywell-defined for N?4.The BLEU proposed by Papineni et al (2002)for automatic evaluation of machine translation ispart of the family of metrics PS(N), as theparticular metric obtained when N=4, wn?s are 1/N,the brevity constant B=1, the list of stop-words SWis empty, and the stemming function ST is theidentity function.3.2 A Recall-focused Family of MetricsAs proposed by Lin and Hovy (2003), aprecision-focused metric such as BLEU can betwisted such that it yields a recall-focused metric.In a similar manner, we define a recall-focusedfamily of metrics, using as parameter a non-negative integer N, with a list of stop-words (SW)and a function for extracting the stem of a givenword (ST) as part of the definition.As before, suppose we have a given NLPapplication for which we want to evaluate thecandidate answer set Candidates for some inputsequences, given a reference answer setReferences.
For each individual reference answerR, we define S(R,n)  as the multi-set of n-gramsobtained from the reference answer R afterstemming the unigrams using ST and eliminatingthe unigrams found in SW. We therefore define arecall score as:?
??
??
??
?=}{Re ),(}{Re ),()()()(ferencesR nRSngramferencesR nRSngramclipngramCountngramCountnRwhere, as before, Count(ngram) is the number ofn-gram counts, and Countclip(ngram) is themaximum number of co-occurrences of ngram inthe reference answer and its correspondingcandidate answer.
Because the denominator in theR(n) formula consists of a sum over the referenceanswers, this formula is essentially a recall-oriented formula, which penalizes incompletecandidates.
This recall score, however, can bemade artificially higher when proposing longer andlonger candidate answers.
This is offset by addinga wordiness penalty, WP:???>??
?= ?
||||,||||,1|)|/||1( rcWifercWifWP rcWwhere |c| and |r| are defined as before, and W is awordiness constant.We define now a recall-focused family ofmetrics, parameterized by a non-negative integerN, as:)))(log(exp()(1nRwWPNRSNnn?=?=This family of metrics can be interpreted as aweighted linear average of recall scores forincreasingly longer n-grams.
For test corpora ofreasonable size, the metrics are usually well-defined for N?4.The ROUGE metric proposed by Lin and Hovy(2003) for automatic evaluation of machine-produced summaries is part of the family ofmetrics RS(N), as the particular metric obtainedwhen N=1, wn?s are 1/N, the wordiness constantW=?, the list of stop-words SW is their own , andthe stemming function ST is the one defined by thePorter stemmer (Porter 1980).3.3 A Unified Framework for AutomaticEvaluationThe precision-focused metric family PS(N) andthe recall-focused metric family RS(N) defined inthe previous sections are unified under the metricfamily AEv(?,N), defined as:)()1()()()(),(NPSNRSNPSNRSNAEv ?
?+?= ??
?This formula extends the well-known F-measurethat combines recall and precision numbers into asingle number (van Rijsbergen, 1979), bycombining recall and precision metric families intoa single metric family.
For ?=0, AEv(?,N) is thesame as the recall-focused family of metricsRS(N); for ?=1, AEv(?, ?N) is the same as theprecision-focused family of metrics PS(N).
For ?in between 0 and 1, AEv(?,N) are metrics thatbalance recall and precision according to ?.
For therest of the paper, we restrict the parameters of theAEv(?,N) family as follows: ?
varies continuouslyin [0,1], N varies discretely in {1,2,3,4}, the linearweights wn are 1/N, the brevity constant is 1, thewordiness constant is 2, the list of stop-words SWis our own 626 stop-word list, and the stemmingfunction ST is the one defined by the Porterstemmer (Porter 1980).We establish a correspondence between theparameters of the family of metrics AEv(?,N) andthe evaluation plane in Figure 1 as follows: ?parameterizes the guideline axis (x-axis) of theplane, such that ?=0 corresponds to a lowprecision/recall (p/r) ratio, and ?=1 corresponds toa high p/r ratio; N parameterizes the applicationaxis (y-axis) of the plane, such that N=1corresponds to a low faithfulness/compactness (f/c)ratio (unigram statistics allow for a lowrepresentation of faithfulness, but a highrepresentation of compactness), and N=4corresponds to a high f/c ratio (n-gram statistics upto 4-grams allow for a high representation offaithfulness, but a low representation ofcompactness).This framework enables us to predict that ahuman-performed evaluation is best approximatedby metrics that have similar f/c ratio as theapplication being evaluated and similar p/r ratio asthe evaluation package used by the humanassessors.
For example, an application with a highf/c ratio, evaluated using a low p/r ratio evaluationguideline package (an example of this is theadequacy evaluation for MT in TIDES 2002), isbest approximated by the automatic evaluationmetric defined by a low ?
and a high N; anapplication with a close-to-1 f/c ratio, evaluatedusing an evaluation guideline packagecharacterized by a close-to-1 p/r ratio (such as thecorrectness evaluation for Question Answering inSection 4.3) is best approximated by an automaticmetric defined by a median ?
and a median N.4 Evaluating the Evaluation FrameworkIn this section, we present empirical resultsregarding the ability of our family of metrics toapproximate human evaluations of variousapplications under various evaluation guidelines.We measure the amount of approximation of ahuman evaluation by an automatic evaluation asthe value of the coefficient of determination R2between the human evaluation scores and theautomatic evaluation scores for various systemsimplementing Machine Translation,Summarization, and Question Answeringapplications.
In this framework, the coefficient ofdetermination R2 is to be interpreted as thepercentage from the total variation of the humanevaluation (that is, why some system?s output isbetter than some other system?s output, from thehuman evaluator?s perspective) that is captured bythe automatic evaluation (that is, why somesystem?s output is better than some other system?soutput, from the automatic evaluation perspective).The values of R2 vary between 0 and 1, with avalue of 1 indicating that the automatic evaluationexplains perfectly the human evaluation variation,and a value of 0 indicating that the automaticevaluation explains nothing from the humanevaluation variation.
All the results for the valuesof R2 for the family of metrics AEv(?,N) arereported with ?
varying from 0 to 1 in 0.1increments, and N varying from 1 to 4.4.1 Machine Translation EvaluationThe Machine Translation evaluation carried outby NIST in 2002 for DARPA?s TIDES programmeinvolved 7 systems that participated in theChinese-English track.
Each system was evaluatedby a human judge, using one reference extractedfrom a list of 4 available reference translations.Each of the 878 test sentences was evaluated bothfor adequacy (how much of the content of theoriginal sentence is captured by the proposedtranslation) and fluency (how correct is theproposed translation sentence in the targetlanguage).
From the publicly available data for thisevaluation (TIDES 2002), we compute the valuesof R2 for 7 data points (corresponding to the 7systems participating in the Chinese-Englishtrack), using as a reference set one of the 4 sets ofreference translations available.In Table 1, we present the values of thecoefficient of determination R2 for the family ofmetrics AEv(?,N), when considering only thefluency scores from the human evaluation.
Asmentioned in Section 2, the evaluation guidelinesfor fluency have a high precision/recall ratio,whereas MT is an application with a highfaithfulness/compactness ratio.
In this case, ourevaluation framework predicts that the automaticevaluation metrics that explain most of thevariation in the human evaluation must have a high?
and a high N. As seen in Table 1, our evaluationframework correctly predicts the automaticevaluation metrics that explain most of thevariation in the human evaluation: metricsAEv(1,3), AEv(0.9,3), and AEv(1,4) capture mostof the variation: 79.04%, 78.94%, and 78.87%,respectively.
Since metric AEv(1,4) is almost thesame as the BLEU metric (modulo stemming andstop word elimination for unigrams), our resultsconfirm the current practice in the MachineTranslation community, which commonly usesBLEU for automatic evaluation.
For comparisonpurposes, we also computed the value of R2 forfluency using the BLEU score formula given in(Papineni et al, 2002), for the 7 systems using thesame one reference, and we obtained a similarvalue, 78.52%; computing the value of R2 forfluency using the BLEU scores computed with all 4references available yielded a lower value for R2,64.96%, although BLEU scores obtained withmultiple references are usually considered morereliable.In Table 2, we present the values of thecoefficient of determination R2 for the family ofmetrics AEv(?,N), when considering only theadequacy scores from the human evaluation.
Asmentioned in Section 2, the evaluation guidelinesfor adequacy have a low precision/recall ratio,whereas MT is an application with highfaithfulness/compactness ratio.
In this case, ourevaluation framework predicts that the automaticevaluation metrics that explain most of thevariation in the human evaluation must have a low?
and a high N. As seen in Table 2, our evaluationframework correctly predicts the automaticevaluation metric that explains most of thevariation in the human evaluation: metric AEv(0,4)captures most of the variation, 83.04%.
Forcomparison purposes, we also computed the valueof R2 for adequacy using the BLEU score formulagiven in (Papineni et al, 2002), for the 7 systemsusing the same one reference, and we obtain asimilar value, 83.91%; computing the value of R2for adequacy using the BLEU scores computedwith all 4 references available also yielded a lowervalue for R2, 62.21%.4.2 Automatic Summarization EvaluationThe Automatic Summarization evaluationcarried out by NIST for the DUC 2001 conferenceinvolved 15 participating systems.
We focus hereon the multi-document summarization task, inwhich 4 generic summaries (of 50, 100, 200, and400 words) were required for a given set ofdocuments on a single subject.
For this evaluation30 test sets were used, and each system wasevaluated by a human judge using one referenceextracted from a list of 2 reference summaries.One of the evaluations required the assessors tojudge the coverage of the summaries.
Thecoverage of a summary was measured bycomparing a system?s units versus the units of areference summary, and assessing whether eachsystem unit expresses all, most, some, hardly any,or none of the current reference unit.
A finalevaluation score for coverage was obtained using acoverage score computed as a weighted recallscore (see (Lin and Hovy 2003) for moreinformation on the human summary evaluation).From the publicly available data for this evaluation(DUC 2001), we compute the values of R2 for 15data points available (corresponding to the 15participating systems).In Tables 3-4 we present the values of thecoefficient of determination R2 for the family ofmetrics AEv(?,N), when considering the coverage4 76.10 76.45 76.78 77.10 77.40 77.69 77.96 78.21 78.45 78.67 78.873 76.11 76.6 77.04 77.44 77.80 78.11 78.38 78.61 78.80 78.94 79.042 73.19 74.21 75.07 75.78 76.32 76.72 76.96 77.06 77.03 76.87 76.581 31.71 38.22 44.82 51.09 56.59 60.99 64.10 65.90 66.50 66.12 64.99N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 1: R2 values for the family of metrics AEv(?,N), for fluency scores in MT evaluation4 83.04 82.58 82.11 81.61 81.10 80.56 80.01 79.44 78.86 78.26 77.643 81.80 81.00 80.16 79.27 78.35 77.39 76.40 75.37 74.31 73.23 72.112 80.84 79.46 77.94 76.28 74.51 72.63 70.67 68.64 66.55 64.42 62.261 62.16 66.26 69.18 70.59 70.35 68.48 65.24 60.98 56.11 50.98 45.88N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 2: R2 values for the family of metrics AEv(?,N), for adequacy scores in MT evaluationscores from the human evaluation, for summariesof 200 and 400 words, respectively (the values ofR2 for summaries of 50 and 100 words showsimilar patterns).
As mentioned in Section 2, theevaluation guidelines for coverage have a lowprecision/recall ratio, whereas AS is an applicationwith low faithfulness/compactness ratio.
In thiscase, our evaluation framework predicts that theautomatic evaluation metrics that explain most ofthe variation in the human evaluation must have alow ?
and a low N. As seen in Tables 3-4, ourevaluation framework correctly predicts theautomatic evaluation metric that explain most ofthe variation in the human evaluation: metricAEv(0,1) explains 90.77% and 92.28% of thevariation in the human evaluation of summaries oflength 200 and 400, respectively.
Since metricAEv(0, 1) is almost the same as the ROUGE metricproposed by Lin and Hovy (2003) (they only differin the stop-word list they use), our results alsoconfirm the proposal for such metrics to be usedfor automatic evaluation by the AutomaticSummarization community.4.3 Question Answering EvaluationOne of the most common approaches toautomatic question answering (QA) restricts thedomain of questions to be handled to so-calledfactoid questions.
Automatic evaluation of factoidQA is often straightforward, as the number ofcorrect answers is most of the time limited, andexhaustive lists of correct answers are available.When removing the factoid constraint, however,the set of possible answer to a (complex, beyond-factoid) question becomes unfeasibly large, andconsequently automatic evaluation becomes achallenge.In this section, we focus on an evaluation carriedout in order to assess the performance of a QAsystem for answering questions from theFrequently-Asked-Question (FAQ) domain(Soricut and Brill, 2004).
These are generallyquestions requiring a more elaborated answer thana simple factoid (e.g., questions such as: ?Howdoes a film qualify for an Academy Award??
).In order to evaluate such a system a human-performed evaluation was performed, in which 11versions of the QA system (various modules wereimplemented using various algorithms) wereseparately evaluated.
Each version was evaluatedby a human evaluator, with no reference answeravailable.
For this evaluation 115 test questionswere used, and the human evaluator was asked toassess whether the proposed answer was correct,somehow related, or wrong.
A unique rankingnumber was achieved using a weighted average ofthe scored answers.
(See (Soricut and Brill, 2004)for more details concerning the QA task and theevaluation procedure.
)One important aspect in the evaluation procedurewas devising criteria for assigning a rating to ananswer which was not neither correct nor wrong.One of such cases involved so-called floodedanswers: answers which contain the correctinformation, along with several other unrelatedpieces of information.
A first evaluation has beencarried with a guideline package asking the humanassessor to assign the rating correct to floodedanswers.
In Table 5, we present the values of thecoefficient of determination R2 for the family ofmetrics AEv(?,N) for this first QA evaluation.
Onthe guideline side, the guideline package used inthis first QA evaluation has a low precision/recallratio, because the human judge is asked to evaluatebased on the content provided by a given answer(high recall), but is asked to disregard theconciseness (or lack thereof) of the answer (lowprecision); consequently, systems that focus on4 67.10 66.51 65.91 65.29 64.65 64.00 63.34 62.67 61.99 61.30 60.613 69.55 68.81 68.04 67.24 66.42 65.57 64.69 63.79 62.88 61.95 61.002 74.43 73.29 72.06 70.74 69.35 67.87 66.33 64.71 63.03 61.30 59.511 90.77 90.77 90.66 90.42 90.03 89.48 88.74 87.77 86.55 85.05 83.21N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 3: R2 for the family of metrics AEv(?,N), for coverage scores in AS evaluation (200 words)4 81.24 81.04 80.78 80.47 80.12 79.73 79.30 78.84 78.35 77.84 77.313 84.72 84.33 83.86 83.33 82.73 82.08 81.39 80.65 79.88 79.07 78.242 89.54 88.56 87.47 86.26 84.96 83.59 82.14 80.65 79.10 77.53 75.921 92.28 91.11 89.70 88.07 86.24 84.22 82.05 79.74 77.30 74.77 72.15N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 4: R2 for the family of metrics AEv(?,N), for coverage scores in AS evaluation (400 words)giving correct and concise answers are notdistinguished from systems that give correctanswers, but have no regard for concision.
On theapplication side, as mentioned in Section 2, QA isarguably an application characterized by a close-to-1 faithfulness/compactness ratio.
In this case,our evaluation framework predicts that theautomatic evaluation metrics that explain most ofthe variation in the human evaluation must have alow ?
and a median N. As seen in Table 5, ourevaluation framework correctly predicts theautomatic evaluation metric that explain most ofthe variation in the human evaluation: metricAEv(0,2) explains most of the human variation,91.72%.
Note that other members of the AEv( ?
?,N)family do not explain nearly as well the variationin the human evaluation.
For example, theROUGE-like metric AEv(0,1) explains only61.61% of the human variation, while the BLEU-like metric AEv(1,4) explains a mere 17.7% of thehuman variation (to use such a metric in order toautomatically emulate  the human QA evaluation isclose to performing an evaluation assigningrandom ratings to the output answers).In order to further test the prediction power ofour evaluation framework, we carried out a secondQA evaluation, using a different evaluationguideline package: a flooded answer was ratedonly somehow-related.
In Table 6, we present thevalues of the coefficient of determination R2 forthe family of metrics AEv(?,N) for this second QAevaluation.
Instead of performing this secondevaluation from scratch, we actually simulated itusing the following methodology: 2/3 of the outputanswers rated correct of the systems ranked 1st, 2nd,3rd, and 6th by the previous human evaluation havebeen intentionally over-flooded using two long andout-of-context sentences, while their ratings werechanged from correct to somehow-related.
Such achange simulated precisely the change in theguideline package, by downgrading floodedanswers.
This means that, on the guideline side, theguideline package used in this second QAevaluation has a close-to-1 precision/recall ratio,because the human judge evaluates now based bothon the content and the conciseness of a givenanswer.
At the same time, the application remainsunchanged, which means that on the applicationside we still have a close-to-1faithfulness/compactness ratio.
In this case, ourevaluation framework predicts that the automaticevaluation metrics that explain most of thevariation in the human evaluation must have amedian ?
and a median N. As seen in Table 6, ourevaluation framework correctly predicts theautomatic evaluation metric that explain most ofthe variation in the human evaluation: metricAEv(0.3,2) explains most of the variation in thehuman evaluation, 86.26%.
Also note that, whilethe R2 values around AEv(0.3,2) are stillreasonable, evaluation metrics that are further andfurther away from it have increasingly lower R2values, meaning that they are more and moreunreliable for this task.
The high correlation ofmetric AEv(0.3,2) with human judgment, however,suggests that such a metric is a good candidate forperforming automatic evaluation of  QA systemsthat go beyond answering factoid questions.5 ConclusionsIn this paper, we propose a unified frameworkfor automatic evaluation based on N-gram co-occurrence statistics, for NLP applications forwhich a correct answer is usually an unfeasiblylarge set (e.g., Machine Translation, Paraphrasing,Question Answering, Summarization, etc.).
Thesuccess of BLEU in doing automatic evaluation ofmachine translation output has often ledresearchers to blindly try to use this metric forevaluation tasks for which it was more or less4 63.40 57.62 51.86 46.26 40.96 36.02 31.51 27.43 23.78 20.54 17.703 81.39 76.38 70.76 64.76 58.61 52.51 46.63 41.09 35.97 31.33 27.152 91.72 89.21 85.54 80.78 75.14 68.87 62.25 55.56 49.04 42.88 37.201 61.61 58.83 55.25 51.04 46.39 41.55 36.74 32.12 27.85 23.97 20.54N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 5: R2 for the family of metrics AEv(?,N), for correctness scores, first QA evaluation4 79.94 79.18 75.80 70.63 64.58 58.35 52.39 46.95 42.11 37.87 34.193 76.15 80.44 81.19 78.45 73.07 66.27 59.11 52.26 46.08 40.68 36.042 67.76 77.48 84.34 86.26 82.75 75.24 65.94 56.65 48.32 41.25 35.421 56.55 60.81 59.60 53.56 45.38 37.40 30.68 25.36 21.26 18.12 15.69N/?
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Table 6: R2 for the family of metrics AEv(?,N), for correctness scores, second QA evaluationappropriate (see, e.g., the paper of Lin and Hovy(2003), in which the authors start with theassumption that BLEU might work forsummarization evaluation, and discover afterseveral trials a better candidate).Our unifying framework facilitates theunderstanding of when various automaticevaluation metrics are able to closely approximatehuman evaluations for various applications.
Givenan application app and an evaluation guidelinepackage eval, the faithfulness/compactness ratio ofthe application and the precision/recall ratio of theevaluation guidelines determine a restricted area inthe evaluation plane in Figure 1 which bestcharacterizes the (app, eval) pair.
We haveempirically demonstrated that the metrics from theAEv( ?
?,N) family that best approximate humanjudgment are those that have the ?
and Nparameters in the determined restricted area.
Toour knowledge, this is the first proposal regardingautomatic evaluation in which the automaticevaluation metrics are able to account for thevariation in human judgment due to specificevaluation guidelines.ReferencesDUC.
2001.
The Document UnderstandingConference.
http://duc.nist.gov.C.Y.
Lin and E. H. Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-Occurrence Statistics.
In Proceedings of theHLT/NAACL 2003: Main Conference, 150-156.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
InProceedings of the ACL 2002, 311-318.M.
F. Porter.
1980.
An algorithm for SuffixStripping.
Program, 14: 130-137.F.
J. Och.
2003.
Minimum Error Rate Training forStatistical Machine Translation.
In Proceedingsof the ACL 2003, 160-167.R.
Soricut and E. Brill.
2004.
Automatic QuestionAnswering: Beyond the Factoid.
In Proceedingsof the HLT/NAACL 2004: Main Conference, 57-64.TIDES.
2002.
The Translingual InformationDetection, Extraction, and Summarizationprogramme.
http://tides.nist.gov.C.
J. van Rijsbergen.
1979.
Information Retrieval.London: Butterworths.
Second Edition.
