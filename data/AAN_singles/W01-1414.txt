Adding Domain Specificity to an MT systemJessie PinkhamMicrosoft ResearchOne Microsoft WayRedmond, WA 98152jessiep@microsoft.comMonica Corston-OliverButler Hill Groupmoco@butlerhill.comAbstractIn the development of a machinetranslation system, one important issueis being able to adapt to a specificdomain without requiring time-consuming lexical work.
We haveexperimented with using a statisticalword-alignment algorithm to deriveword association pairs (French-English)that complement an existing multi-purpose bilingual dictionary.
This wordassociation information is added to thesystem at the time of the automaticcreation of our translation patterndatabase, thereby making this databasemore domain specific.
This techniquesignificantly improves the overallquality of translation, as measured in anindependent blind evaluation.1 IntroductionThe machine translation system describedhere is a French-English translation systemwhich uses a French broad coverage analyzer, alarge multi-purpose French dictionary, a largeFrench-English bilingual lexicon, an applicationindependent English natural language generationcomponent and a transfer component.
Thetransfer component consists of high-qualitytransfer patterns automatically acquired fromsentence-aligned bilingual corpora using analignment grammar and algorithm described indetail in Menezes (2001) (see Figure 1 for anoverview of the French-English MT system).The transfer component consists only ofcorrespondences learned during the alignmentprocess.
Training takes place on alignedsentences which have been analyzed by theFrench and English analysis systems to yielddependency structures specific to our systementitled Logical Forms (LF).
The LF structures,when aligned, allow the extraction of lexical andstructural translation correspondences which arestored for use at runtime in the transfer database.The transfer database can also be thought of asan example-base of conceptual structurerepresentations.
See Figure 2 for an illustrationof the training process.The transfer database for French-English wastrained on approximately 200,000 pairs ofaligned sentences from computer manuals andhelp files.
In these aligned pairs, the French textwas produced by human translators from theoriginal English version.Sample sentences from the training set are:French training sentence:Dans le menu D?marrer, pointez surProgrammes, sur Outils d'administration(commun), puis cliquez sur Gestionnaire desutilisateurs pour les domaines.English training sentence:On the Start menu, point to Programs, pointto Administrative Tools (Common), and thenclick User Manager for Domains.The French-English lexicon is used duringthe training period of the transfer component toestablish initial, tentative, word correspondencesduring the alignment process.
The sources forthe bilingual dictionary were: CambridgeUniversity Press English-French, Soft-ArtEnglish-French, and Langenscheidt French-English and English-French dictionaries.
TheEnglish-French translation data was reversed tocreate French-English pairs in order to augmentthe size of the dictionary, with a final translationcount of 75,000 pairs.However, quick examination of the samplesentence above shows that many terms arehighly specific to the domain, e.g menuD?marrer <-> Start menu.
To further add tothe specificity of the vocabulary available to thealignment process, we added translation pairsextracted from the actual domain, usingstatistical word/phrase assignment, as describedbelow.
This resulted in one file of automaticallycreated French English translationcorrespondences, or word associations (WA),and a second file of specialized multi-wordtranslation correspondences which we term TitleAssociations (TA).
These files, of size 30,000and 2600 respectively, added to the quality ofthe alignments and to overall translation quality.2 Domain Specificity2.1 Word-Association listMoore (2001) describes a method for learningtranslation relationship between words frombilingual corpora.
The five step process isrestated here:1.
Extract word lemmas from the LogicalForm created by parsing the rawtraining data.2.
Compute association scores forindividual lemmas.3.
Hypothesize occurrences of compoundsin the training data, replacing lemmasconstituting hypothesized occurrencesof a compound with a single tokenrepresenting the compound.4.
Recompute association scores forcompounds and remaining individualindividual lemmas.5.
Recompute association scores, takinginto account only co-occurrences suchthat there is no equally strong orstronger association for either item inthe aligned logical-form pair.The word-association list (WA) was created byapplying this method to our training data set of200,000 aligned French-English sentences ofcomputer manual and help file data.
A Frenchlinguist determined the best cutoff for the rawdata, i.e.
determined the association score whichwould determine the cutoff, and otherwise leftthe file unedited for inclusion in the transfertraining stage.
For internal reasons, we usedonly associations which are conceptually singleword to single word, where a single word can bedefined as an item returned as one unit by theanalyzer, even though it might be a multi-worditem in the source text, e.g base_de_donn?e <->database.
The files included 30,000 pairs, whichin their totality, were judged to be 60%accurate1.Figure 2The word association file was used only intraining (see Figure 2) to enhance theopportunity for alignment during the detectionof transfer patterns.Examples of WA pairings :?
cliquer click?
processeur CPU?
?clairage lighting?
http://www.mcafee.comhttp://www.mcafee.com?
nettoyer scavenge?
conversion translation?
Requ?te/?dition query/edit2.2 Title Association listThe second file used was a specialized filecreated using the same algorithm, but allowingmulti-word titles that are all in capitals inEnglish to associate with multiple words inFrench that have mixed capitalization on majorcontent words.
Because these phrases areidentified by using capitalization, they are alsoreferred to as Captoids (Moore, 2001).
Itemssuch as Organizational Units, which occur withcomplete capitalization in English, are1 The size of of the WA file of 42,486 reported in Moore2001 includes multiple word associations which were notused in this experiment.associated with the French translation, Unit?s d'organisation, a unit which is less easilyidentified on its own, due to the mixed case.The information yields approximately 2600pairs of this type:Unit?s d' organisation <->Organizational UnitsVoir aussi <-> Related TopicsThis title association file (TA) is used intraining of the transfer patterns but are alsoadded to the processing of the French trainingtext; they are treated as multi-word lexicalentries similar to any French dictionary entry.They become part of the translation dictionaryas well.
The inclusion of Voir aussi as a lexicalnoun phrase at the analysis stage (French)allows it to parse correctly, and permits thecorrect translation.
Many of the occurrences ofTitle association pairs are menu names whichare syntactically verb phrases (Voir aussi) andwould have parsed less well without the TA file.
(1)Source: Pour plus d'informations sur l'utilisationdu Gestionnaire de p?riph?riques, consultezVoir aussi.Reference: For more information about usingDevice Manager, see Related Topics.ALL translation: For more information aboutusing of the manager of devices, see RelatedTopics.NONE translation: See for more informationon using of the Device Manager; also See.However, the evaluation shows that theoverall effect of title associations is much lessthan that of word associations, presumablybecause the frequency of these items is low inthe overall test set.3 Experiment and MethodologyIn order to evaluate the relative quality of thetranslations with and without the wordassociation and title association strategies, weperformed several evaluations of machinetranslation quality.
These evaluations wereperformed by an independent organization thatprovides support for NL applicationdevelopment; the evaluators are completelyindependent of development activities.We performed two separate sets ofevaluations.
In the first, we evaluated the fullversion of our system with the WordAssociation and Title Association componentsagainst versions of the system from which wehad removed those components.
We thusexpected that versions of the system with theWA and TA components would outperformthose without.In the second evaluation, we tested theversions of our system with and without the WAand TA components against a benchmarksystem (the latest release of the French-EnglishSystran system, run with settings appropriate forthe computer domain) to see whether theaddition of the combination of these componentswould significantly improve our scores withrespect to that benchmark.3.1 Evaluation designFor each condition to be tested, sevenevaluators were asked to evaluate the same setof 250 blind test sentences.
For each sentence,raters were presented with a reference sentence,the original English translation from which thehuman French translation was derived.
In orderto maintain consistency among raters who mayhave different levels of fluency in the sourcelanguage, raters were not shown the originalFrench sentence (for similar methodologies, seeRingger et al, 2001; White et al, 1993).
Raterswere also shown two machine translations, onefrom the system with the component beingtested (System 1), and one from the comparisonsystem (System 2).
Because the order of the twomachine translation sentences was randomizedon each sentence, evaluators could notdetermine which sentence was from System 1.The order of presentation of sentences was alsorandomized for each rater in order to eliminateany ordering effect.The raters were asked to make a three-waychoice.
For each sentence, the raters were todetermine which of the two automaticallytranslated sentences was the better translation ofthe (unseen) source sentence, assuming that thereference sentence was a perfect translation,with the option of choosing ?neither?
if thedifferences were negligible.
Raters wereinstructed to use their best judgment about therelative importance of fluency/style andaccuracy/content preservation.
We chose to usethis simple three-way scale in order to avoidmaking any a priori judgments about the relativeimportance of these parameters for subjectivejudgments of quality.
The three-way scale alsoallows sentences to be rated on the same scale,regardless of whether the differences betweenoutput from system 1 and system 2 weresubstantial or relatively small; and regardless ofwhether either version of the system producedan adequate translation.The scoring system is similarly simple; eachjudgment by a rater was represented as 1(sentence from System 1 judged better), 0(neither sentence judged better), or -1 (System 2judged better).
The score for each condition isthe mean of the scores of all sentences for allraters.4 Results4.1 Results with multiple versions of oursystemIn order to isolate the effects of the WA and TAcomponents on the system as a whole, we built 3new versions of the system:?
NONE: Includes neither TA nor WA.?
No TA: Includes WA but not TA.?
No WA: Includes TA but not WA.We evaluated each of these versions of thesystem against our baseline system (ALL),which contains both the WA and TAcomponents.
Our hypothesis was that theremoval of each of the two components wouldcause the experimental systems to significantlyunderperform the ALL system.We evaluated 250 sentences2 in eachcondition in which the output strings for System1 (ALL) and System 2 (NONE, NoWA, andNoTA, respectively) were not identical.
In otherwords, this analysis shows the amount ofimprovement between the systems in only thosesentences which show any change at all in eachcondition.
For each condition, we calculated thestatistical significance of the hypothesis that ALLsystem is better than the comparison system (e.g.that the score is greater than 0), taking intoaccount both variations in the sentence sample,and variations across the judgments of individualraters.2 The data used for testing is blind, i.e.
withheld fromdevelopment and not included in the training set.ConditionScore SampleSizeSignificanceALL/NONE 0.233 +/- .095 250 > .99999ALL/NoWA 0.267 +/- .09 250 > .99999ALL/NoTA 0.063 +/- .093 250 .91Table 1: Results with differences onlyThe results show that, for sentences affected bythe combination of the WA and TA components,the ALL condition is significantly better than theNONE condition, at a significance level of 0.95.In addition, for sentences affected by thepresence of the WA component only, the ALLcondition is significantly better than the No WAcondition.
However, the ALL condition is notsignificantly better the NoTA condition.Another question of interest is the effect ofthe experimental components on the corpus as awhole, rather than just on the sentences thatchanged; it is possible that the effects we foundmight have become diluted below thesignificance threshold because of sparsity of thedifferences across the whole corpus.
Rather thando additional evaluations, we determined theproportion of differences in each condition, andextrapolated a larger sample, assuming thatsentences which were absolutely identical wouldreceive a score of 0, using the same 250judgments as in the previous analysis.Condition iffsheckedotal diffsn test setf 2965ercentf diffsn test setProjectedsamplesize to get250 diffsNONE /ALL 250 1307 19.13 567NoWA/ALL 250 1170 21.37 634NoTA/ALL 250 280 89.29 2647Table 2: Projected sample sizesAs expected, the results using the projectedsample were still positive, though the scoreswere lower due to the larger sample size.
Again,the improvements in the NONE/ALL andNoWA/ALL conditions are significant acrossthe whole data set.Condition Score SamplesizeSignificanceNONE /ALL 0.103 +/-.04567 > .99999NoWA/ALL 0.105 +/-.035634 > .99999NoTA/ALL 0.006 +/-.0082647 .90Table 3: Results across whole sample4.2 Results against benchmark systemIn a second analysis, we tested to see if theexperimental changes to the system improvedthe performance of our system against ourregular benchmark.
We selected a randomsample of 250 sentences, and translated themusing first the ALL, and then the NONE,versions of our system.
We also translated themusing the benchmark system.
We predicted thatsentences translated using the ALL systemwould be significantly better than the sentencestranslated using the NONE system in itsperformance against the benchmark.Condition Score SamplesizeNONE /benchmark -0.18 +/- .1 250ALL/benchmark -0.14 +/- .11 250Table 4: Results against Benchmark system.The difference between these two scores is onthe border of significance using a one-tailedpaired t-test (p = .051825; t = -1.6334).5 DiscussionThe premise of the experiment described herewas that pairs of translations which wereautomatically derived from the training datawould increase the number of transfer pairingsfound and improve the quality of translation.The results show that the combination of theword association list and title association listdoes in fact give us an improvement in qualityof translation.We have measured the change in size in thetransfer database, and found that the databaseshows increased numbers of transfer patternsretained (transfer patterns seen only once werediscarded) when the word association file isused, for instance:Condition Unique transfers keptNONE 316518ALL 368853Table 5: Increase in patterns keptWe have found from informal observationthat increased number of transfers in the transferdatabase correlates with better performance,particularly if the translation correspondenceincludes more than one word.Whereas the WA and TA files have beenjudged elsewhere on the quality of thetranslation pairs themselves (Moore 2001), weare primarily interested in whether the datainteracts in a positive way with a full-scaleautomatic alignment process.
The result mightappear disappointing at first glance, since it isbarely significant.
However, our experience isthat a gain of .04 against the benchmarkrepresents a noticeable difference in qualitytranslation from the user?s perspective.It is important to note as well that this resultwas achieved even in the presence of a sizeabletranslation dictionary.
We found that thecombination of the bilingual dictionary and thestructural mapping in the alignment process hadalready enabled a number of ?domain specific?translation correspondences, e.g.
journal <-> logas in example (2) below.
In a sense, thealignment algorithm had been able to overcomesome domain specific lexical gaps on its own.The evaluation results give us a number ofillustrations of improved transfer patterns.
Theonly difference between the output categorizedas NONE and the output categorized as ALL isthe use of a transfer database trained with boththe WA and TA files included.
(2)Source: Le tableau ci-dessous explique lafonction des diff?rentes options disponiblesdans l'onglet Journal des transactions de labo?te de dialogue Propri?t?s de la base dedonn?es.Reference: This table shows the options andtheir functions available on the TransactionLog tab of the Database Properties dialogbox.ALL translation: The table explains thefunction of different options available in theTransaction Log tab of the dialog Propertiesbox of the database below.NONE translation: The table explains thefunction of different options available in thetab transactions Log of the dialog Propertiesbox of the database below.The pattern which caused the improvement isthe correspondence (Journal des transactions<-> Transaction Log) was learned on differentpairs of sentences during the alignment phasedue to the presence of the word log introducedby the word association file.Without the addition of log at alignment time,the alignment process mapped Journal to Log,but not the more complex mapping for Journaldes transactions.
Compare the translations fromthe FE dictionary to the pairs from the wordassociation file (where ordering representsfrequency of each translation).
Note that theWA list has learned the most relevant technicaltranslation (log), which was lacking in the FEdictionary, but also the most frequent generaltranslation (journal):FE dictionary(journal)=(journal magazine diary newspaper)Word association list(journal)=(log journal newspaper)A similar case below (3) shows that theinclusion of the word push as a translation of?mission in the word association file allows for acorrect pattern in the transfer database:r?plication par ?missionpush replication(3)Source: Pour configurer un serveur WINS afind'utiliser une r?plication par ?mission, vouspouvez faire votre choix parmi plusieursoptions configurables de la console WINS.Reference: To configure a WINS server to usepush replication, you can choose fromseveral WINS console configurable options.ALL translation: For configuring a WIN serverto use a push replication, you can do yourchoice among options configurable of theWIN console.NONE translation: For configuring a WINserver to use a replication by program, youcan do your choice among optionsconfigurable of the console WIN.FE dictionary: (?mission)=(programtransmission broadcasting emissionbroadcast issue uttering)Word association list: (?mission)=(issue pushTransmit transmit issuance)This example is quite interesting, because thelink of push to ?mission is helpful, even thoughis would be judged incorrect in a standardevaluation of the pairings themselvesWe have described the improvements sofar as increases in domain specificity, but theeffect is more wide-spread.
We find that theadded information allows for creation andretention of such generally better patterns asthose in example (4):(4)Source: Assurez-vous qu'il y a du papier dansl'imprimante.Reference: Make sure there is paper in yourprinting device.ALL translation: Make sure that there is apaper in the printer.NONE translation: Provide that a paper in theprinter becomes.We note the improved transfer patterns for Makesure and there is.The incidence of faulty translation patternslearned because of incorrect word-associationshas been difficult to measure, but appears to below.
One instance was the learnedcorrespondence of ?teindre <-> off (instead ofturn_off).
We believe this could be avoided bymore accurate preservation of information fromour Logical Form representation in step one ofthe Moore algorithm.5.1 Future improvementsThe experiment presented here is the firststep in our search for techniques that contributeto the quality of the translations by providingdomain specific additions.We are working to find the most productivemethod for pruning low accuracy pairs (but stillwithout hand-editing).
We have already seenthat if the data is truncated to maximize theaccuracy of the word associations, the impact onthe translation quality drops off, presumablybecause the high frequency pairs in the wordassociation file contribute fewer unknowntranslations than the larger noisier file.
Thissuggests that in the process of seeding anautomatic alignment process such as ours, recallis more important than precision..ReferencesFrederking, Robert, and Ralf Brown.
1996.
ThePangloss-Lite Machine Translation System.In Proceedings of the Conference of theAssociation for Machine Translation in theAmericas.
268-272.Frederking, Robert, et al 1994.
IntegratingTranslations From Multiple Sources Withinthe Pangloss Mark III Machine TranslationSystem.
In Technology Partnerships forCrossing the Language Barrier: Proceedingsof the First Conference of the Association ofMachine Translation in the Americas.
73-80.Melamed, I. Dan.
1996.
Automatic Constructionof Clean Broad-Coverage TranslationLexicons.
In Proceedings of the SecondConference of the Association for MachineTranslation in the Americas.
125-134.Menezes, Arul and Steve Richardson.
2001.
ABest-First Alignment Algorithm forAutomatic Extraction of Transfer Mappingsfrom Bilingual Corpora.
In Proceedings of theData-Driven MT workshop, ACL 2001.Moore, Robert C. 2001.
Towards a Simple andAccurate Statistical Approach to LearningTranslation Relationships Between Words.
InProceedings of the Data-Driven MTworkshop, ACL 2001.Ringger, Eric K., Monica Corston-Oliver, andRobert C. Moore.
2001.
Using Word-Perplexity for Automatic Evaluation ofMachine Translation.
Unpublished ms.Hideo Watanabe, Sadao Kurohashi and EijiAramaki.
2000.
Finding StructuralCorrespondences from Bilingual ParsedCorpus for Corpus-based Translation.
InProceedings of COLING: The 18thInternational Conference on ComputationalLinguistics.
906-912.White, John S., Theresa A. O'Connell, and LynnM.
Carlson.
1993.
Evaluation of machinetranslation.
In Human Language Technology:Proceedings of a Workshop (ARPA).
206-210.
