2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221?231,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsStructured Ramp Loss Minimization for Machine TranslationKevin Gimpel and Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractThis paper seeks to close the gap betweentraining algorithms used in statistical machinetranslation and machine learning, specificallythe framework of empirical risk minimization.We review well-known algorithms, arguingthat they do not optimize the loss functionsthey are assumed to optimize when applied tomachine translation.
Instead, most have im-plicit connections to particular forms of ramploss.
We propose to minimize ramp loss di-rectly and present a training algorithm that iseasy to implement and that performs compa-rably to others.
Most notably, our structuredramp loss minimization algorithm, RAMPION,is less sensitive to initialization and randomseeds than standard approaches.1 IntroductionEvery statistical MT system relies on a training al-gorithm to fit the parameters of a scoring function toexamples from parallel text.
Well-known examplesinclude MERT (Och, 2003), MIRA (Chiang et al,2008), and PRO (Hopkins and May, 2011).
Whilesuch procedures can be analyzed as machine learn-ing algorithms?e.g., in the general framework ofempirical risk minimization (Vapnik, 1998)?theirprocedural specifications have made this difficult.From a practical perspective, such algorithms are of-ten complex, difficult to replicate, and sensitive toinitialization, random seeds, and other hyperparam-eters.In this paper, we consider training algorithms thatare first specified declaratively, as loss functions tobe minimized.
We relate well-known training algo-rithms for MT to particular loss functions.
We showthat a family of structured ramp loss functions (Doet al, 2008) is useful for this analysis.
For example,McAllester and Keshet (2011) recently suggestedthat, while Chiang et al (2008, 2009) described theiralgorithm as ?MIRA?
(Crammer et al, 2006), in factit targets a kind of ramp loss.
We note here other ex-amples: Liang et al (2006) described their algorithmas a variant of the perceptron (Collins, 2002), whichhas a unique loss function, but the loss actually opti-mized is closer to a particular ramp loss (that differsfrom the one targeted by Chiang et al).
Och andNey (2002) sought to optimize log loss (likelihoodin a probabilistic model; Lafferty et al, 2001) butactually optimized a version of the soft ramp loss.Why isn?t the application of ML to MT morestraightforward?
We note two key reasons: (i) MLgenerally assumes that the correct output can alwaysbe scored by a model, but in MT the reference trans-lation is often unreachable, due to a model?s limitedexpressive power or search error, requiring the useof ?surrogate?
references; (ii) MT models nearly al-ways include latent derivation variables, leading tonon-convex losses that have generally received littleattention in ML.
In this paper, we discuss how thesetwo have caused a disconnect between the loss func-tion minimized by an algorithm in ML and the lossminimized when it is adapted for MT.From a practical perspective, our framework leadsto a simple training algorithm for structured ramploss based on general optimization techniques.
Ouralgorithm is simple to implement and, being a batchalgorithm like MERT and PRO, can easily be inte-221grated with any decoder.
Our experiments show thatour algorithm, which we call RAMPION, performscomparably to MERT and PRO, is less sensitive torandomization and initialization conditions, and isrobust in large-feature scenarios.2 Notation and BackgroundLet X denote the set of all strings in a source lan-guage and, for a particular x ?
X, let Y(x) denotethe set of its possible translations (correct and incor-rect) in the target language.
In typical models formachine translation, a hidden variable is assumedto be constructed during the translation process.1Regardless of its specific form, we will refer to it asa derivation and denote it h ?
H(x), where H(x)is the set of possible values of h for the input x.Derivations will always be coupled with translationsand therefore we define the set T(x) ?
Y(x)?H(x)of valid output pairs ?y,h?
for x.To model translation, we use a linear model pa-rameterized by a parameter vector ?
?
?.
Given avector f(x,y,h) of feature functions on x, y, andh, and assuming ?
contains a component for eachfeature function, output pairs ?y,h?
for a given in-put x are selected using a simple argmax decisionrule: ?y?,h??
= argmax?y,h??T(x)?>f(x,y,h)?
??
?score(x,y,h;?
).The training problem for machine translation cor-responds to choosing ?.
There are many ways to dothis, and we will describe each in terms of a partic-ular loss function loss : XN ?
YN ??
?
R thatmaps an input corpus, its reference translations, andthe model parameters to a real value indicating thequality of the parameters.
Risk minimization cor-responds to choosingargmin???
Ep(X,Y ) [loss (X,Y ,?)]
(1)where p(X,Y ) is the (unknown) true joint distri-bution over corpora.
We note that the loss functiondepends on the entire corpus, while the decoder op-erates independently on one sentence at a time.
Thisis done to fit the standard assumptions in MT sys-tems: the evaluation metric (e.g., BLEU) depends on1For phrase-based MT, a segmentation of the sourceand target sentences into phrases and an alignment betweenthem (Koehn et al, 2003).
For hierarchical phrase-based MT, aderivation under a synchronous CFG (Chiang, 2005).the entire corpus and does not decompose linearly,while the model score does.
Since in practice we donot know p(X,Y ), but we do have access to an ac-tual corpus pair ?X?, Y?
?, where X?
= {x(i)}Ni=1 andY?
= {y(i)}Ni=1, we instead consider regularizedempirical risk minimization:argmin???
loss(X?, Y?
,?)
+R(?)
(2)where R(?)
is the regularization function used tomitigate overfitting.
The regularization function isfrequently a squared norm of the parameter vector,such as the `1 or `2 norm, but many other choicesare possible.
In this paper, we use `2.Models are evaluated using a task-specific notionof error, here encoded as a cost function, cost :YN ?
YN ?
R?0, such that the worse a translationis, the higher its cost.
The cost function will typi-cally make use of an automatic evaluation metric formachine translation; e.g., cost might be 1 minus theBLEU score (Papineni et al, 2001).2We note that our analysis in this paper is appli-cable for understanding the loss function being op-timized given a fixed set of k-best lists.3 However,most training procedures periodically invoke the de-coder to generate new k-best lists, which are thentypically merged with those from previous trainingiterations.
It is an open question how this practiceaffects the loss function being optimized by the pro-cedure as a whole.Example 1: MERT.
The most commonly-usedtraining algorithm for machine translation is mini-mum error rate training, which seeks to directlyminimize the cost of the predictions on the trainingdata.
This idea has been used in the pattern recogni-tion and speech recognition communities (Duda andHart, 1973; Juang et al, 1997); its first applicationto MT was by Och (2003).
The loss function takesthe following form: losscost(X?, Y?
,?)=cost??Y?
,{argmax?y,h??T(x(i))score(x(i),y,h;?)}Ni=1??
(3)2We will abuse notation and allow cost to operate on bothsets of sentences as well as individual sentences.
For nota-tional convenience we also let cost accept hidden variables butassume that the hidden variables do not affect the value; i.e.,cost(?y,h?, ?y?,h??)
= cost(y, ?y?,h??)
= cost(y,y?
).3Cherry and Foster (2012) have concurrently performed asimilar analysis.222MERT directly minimizes the corpus-level costfunction of the best outputs from the decoder with-out any regularization (i.e., R(?)
= 0).4 The loss isnon-convex and not differentiable for cost functionslike BLEU, so Och (2003) developed a coordinateascent procedure with a specialized line search.MERT avoids the need to compute feature vec-tors for the references (?1(i)) and allows corpus-level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the diffi-culty of the search lead to instabilities during learn-ing.
Remedies have been suggested, typically in-volving additional search directions and experimentreplicates (Cer et al, 2008; Moore and Quirk, 2008;Foster and Kuhn, 2009; Clark et al, 2011).
But de-spite these improvements, MERT is ineffectual fortraining weights for large numbers of features; inaddition to anecdotal evidence from the MT com-munity, Hopkins and May (2011) illustrated withsynthetic data experiments that MERT struggles in-creasingly to find the optimal solution as the numberof parameters grows.Example 2: Probabilistic Models.
By exponenti-ating and normalizing score(x,y,h;?
), we obtaina conditional log-linear model, which is useful fortraining criteria with probabilistic interpretations:p?
(y,h|x) = 1Z(x,?)
exp{score(x,y,h;?)}
(4)The log loss then defines losslog(X?, Y?
,?)
=?
?Ni=1 log p?
(y(i) | x(i)).Example 3: Bayes Risk.
The term ?risk?
as usedabove should not be confused with the Bayes riskframework, which uses a probability distribution(Eq.
4) and a cost function to define a loss:lossB risk =?Ni=1 Ep?
(y,h|x(i))[cost(y(i),y)] (5)The use of this loss is often simply called ?riskminimization?
in the speech and MT communities.Bayes risk is non-convex, whether or not latent vari-ables are present.
Like MERT, it naturally avoidsthe need to compute features for y(i) and uses acost function, making it appealing for MT.
Bayesrisk minimization first appeared in the speech recog-nition community (Kaiser et al, 2000; Povey and4However, Cer et al (2008) and Macherey et al (2008)achieved a sort of regularization by altering MERT?s line search.Woodland, 2002) and more recently has been ap-plied to MT (Smith and Eisner, 2006; Zens et al,2007; Li and Eisner, 2009).3 Training Methods for MTIn this section we consider other ML-inspired ap-proaches to MT training, situating each in the frame-work from ?2: ramp, perceptron, hinge, and ?soft?losses.
Each of the first three kinds of losses can beunderstood as a way of selecting, for each x(i), twocandidate translation/derivation pairs: ?y?,h??
and?y?,h??.
During training, the loss function can beimproved by increasing the score of the former anddecreasing the score of the latter, through manipu-lation of the parameters ?.
Figure 1 gives a generalvisualization of some of the key output pairs that areconsidered for these roles.
Learning alters the scorefunction, or, in the figure, moves points horizontallyso that scores approximate negated costs.3.1 Structured Ramp Loss MinimizationThe structured ramp loss (Do et al, 2008) is anon-convex loss function with certain attractive the-oretical properties.
It is an upper bound on losscost(Eq.
3) and is a tighter bound than other loss func-tions (Collobert et al, 2006).
Ramp loss has beenshown to be statistically consistent in the sensethat, in the limit of infinite training data, mini-mizing structured ramp loss reaches the minimumvalue of losscost that is achievable with a linearmodel (McAllester and Keshet, 2011).
This is truewhether or not latent variables are present.Consistency in this sense is not a common prop-erty of loss functions; commonly-used convex lossfunctions such as the perceptron, hinge, and loglosses (discussed below) are not consistent, becausethey are all sensitive to outliers or otherwise noisytraining examples.
Ramp loss is better at dealingwith outliers in the training data (Collobert et al,2006).There are three forms of latent structured ramploss: Eq.
6?8 (Fig.
2).
Ramp losses are appealing forMT because they do not require computing the fea-ture vector of y(i) (?1(i)).
The first form, Eq.
6, sets?y?,h??
to be the current model prediction (?y?, h?
?in Fig.
1) and ?y?,h??
to be an output that is bothfavored by the model and has high cost.
Such an223yy^y*^?
*^y*score- costscore- cost- costscore- cost y*yscore- cost y*y^score- cost y*y^cost diminished?+y-?y?,h??
= argmin?y,h?
?T (x(i))cost(y(i),y)?y,h?
= argmax?y,h?
?T (x(i))score(x(i),y,h;?)?
cost(y(i),y)?y?,h??
= argmax?y,h?
?T (x(i))score(x(i),y,h;?)
+ cost(y(i),y) score- cost?y?, h??
= argmax?y,h?
?T (x(i))score(x(i),y,h;?
)score- cost ?y?,h??
= argmin?y,h?
?T (x(i))cost(y(i),y)?y,h?
= argmaxTy,h???(x(i))score(x(i),y,h;?)?
cost(y(i),y)?y?,h??
= argmax?y,h?
?T (x(i))score(x(i),y,h;?)
+ cost(y(i),y)?=y, =h?
; argmat?y,h?
?T (x(i))score(x(i),y,h+?
)score- costargmin?y,h??K(x(i))cost(y(i),y)argmin?y,h?
?K(x(i))cost(y(i),y)Figure 1: Hypothetical output space of a translation model for an input sentence x(i).
Each point corresponds to asingle translation/derivation output pair.
Horizontal ?bands?
are caused by output pairs with the same translation (andhence the same cost) but different derivations.
The left plot shows the entire output space and the right plot highlightsoutputs in the k-best list.
Choosing the output with the lowest cost in the k-best list is similar to finding ?y+,h+?.output is shown as ?y?,h??
in Fig.
1; finding y?is often called cost-augmented decoding, which isalso used to define hinge loss (?3.3).The second form, Eq.
7, penalizes the modelprediction (?y?,h??
= ?y?, h??)
and favors an out-put pair that has both high model score and lowcost; this is the converse of cost-augmented decod-ing and therefore we call it cost-diminished decod-ing; ?y?,h??
= ?y+,h+?
in Fig.
1.
The third form,Eq.
8, sets ?y?,h??
= ?y+,h+?
and ?y?,h??
=?y?,h??.
This loss underlies RAMPION.
It is sim-ilar to the loss optimized by the MIRA-inspired al-gorithm used by Chiang et al (2008, 2009).Optimization The ramp losses are continuous butnon-convex and non-differentiable, so gradient-based optimization methods are not available.5 For-tunately, Eq.
8 can be optimized by using a concave-convex procedure (CCCP; Yuille and Rangarajan,2002).
CCCP is a batch optimization algorithm forany function that is the the sum of a concave and aconvex function.
The idea is to approximate the sumas the convex term plus a tangent line to the con-cave function at the current parameter values; theresulting sum is convex and can be optimized with(sub)gradient methods.5For non-differentiable, continuous, convex functions, sub-gradient-based methods are available, such as stochastic sub-gradient descent (SSD), and it is tempting to apply them here.However, non-convex functions are not everywhere subdiffer-entiable and so a straightforward application of SSD may en-counter problems in practice.With our loss functions, CCCP first imputes theoutputs in the concave terms in each loss (i.e., solvesthe negated max expressions) for the entire trainingset and then uses an optimization procedure to op-timize the loss with the imputed values fixed.
Anyconvex optimization procedure can be used once thenegated max terms are solved; we use stochasticsubgradient descent (SSD) but MIRA could be eas-ily used instead.The CCCP algorithm we use for optimizinglossramp 3, which we call RAMPION, is shown asAlg.
1.
Similar algorithms can easily be derived forthe other ramp losses.
The first step done on eachiteration is to generate k-best lists for the full tun-ing set (line 3).
We then run CCCP on the k-bestlists for T ?
iterations (lines 4?15).
This involves firstfinding the translation to update towards for all sen-tences in the tuning set (lines 5?7), then making pa-rameter updates in an online fashion with T ??
epochsof stochastic subgradient descent (lines 8?14).
Thesubgradient update for the `2 regularization term isdone in line 11 and then for the loss in line 12.6Unlike prior work that targeted similar loss func-tions (Watanabe et al, 2007; Chiang et al, 2008;Chiang et al, 2009), we do not use a fully online al-gorithm such as MIRA in an outer loop because weare not aware of an online learning algorithm withtheoretical guarantees for non-differentiable, non-convex loss functions like the ramp losses.
CCCP6`2 regularization done here regularizes toward ?0, not 0.224lossramp 1 =N?i=1?
max?y,h??Ti(scorei(y,h;?))
+ max?y,h??Ti(scorei(y,h;?)
+ costi(y)) (6)lossramp 2 =N?i=1?
max?y,h??Ti(scorei(y,h;?)?
costi(y)) + max?y,h??Ti(scorei(y,h;?))
(7)lossramp 3 =N?i=1?
max?y,h??Ti(scorei(y,h;?)?
costi(y)) + max?y,h??Ti(scorei(y,h;?)
+ costi(y)) (8)lossperc =N?i=1?
maxh:?y(i),h??Tiscorei(y(i),h;?)
+ max?y,h??Tiscorei(y,h;?)
(9)lossperc kbest =n?i=1?score(x(i), argmin?y,h?
?Ki(costi(y)) ;?
)+ max?y,h??Tiscorei(y,h;?)
(10)?N?i=1?
max?y,h??Ti(scorei(y,h;?)?
?icosti(y)) + max?y,h??Tiscorei(y,h;?)
(11)Figure 2: Formulae mentioned in text for latent-variable loss functions.
Each loss is actually a function loss(X?, Y?
,?
);we suppress the arguments for clarity.
?Ti?
is shorthand for ?T(x(i)).?
?Ki?
is shorthand for the k-best list for x(i).?costi(?)?
is shorthand for ?cost(y(i), ?).?
?scorei(?)?
is shorthand for ?score(x(i), ?).?
As noted in ?3.4, any operatorof the form maxs?S can be replaced by log?s?S exp, known as softmax, giving many additional loss functions.is fundamentally a batch optimization algorithm andhas been used for solving many non-convex learn-ing problems, such as latent structured SVMs (Yuand Joachims, 2009).3.2 Structured PerceptronThe stuctured perceptron algorithm (Collins, 2002)was considered by Liang et al (2006) as an alterna-tive to MERT.
It requires only a decoder and comeswith some attractive guarantees, at least for mod-els without latent variables.
Liang et al modifiedthe perceptron in several ways for use in MT.
Thefirst was to generalize it to handle latent variables.The second change relates to the need to computethe feature vector for the reference translation y(i),which may be unreachable (?1(i)).
To address this,researchers have proposed the use of surrogates thatare both favored by the current model parametersand similar to the reference.
Och and Ney (2002)were the first to do so, using the translation on ak-best list with the highest evaluation metric scoreas y?.
This practice was followed by Liang et al(2006) and others with success (Arun and Koehn,2007; Watanabe et al, 2007).7Perceptron Loss Though typically described and7Liang et al (2006) also tried a variant that updated directlyto the reference when it is reachable (?bold updating?
), but theyand others found that Och and Ney?s strategy worked better.analyzed procedurally, it is straightforward to showthat Collins?
perceptron (without latent variables)equates to SSD with fixed step size 1 on loss:N?i=1?score(x(i),y(i);?
)+ maxy?Y(x(i))score(x(i),y;?
)(12)This loss is convex but ignores cost functions.In our notation, y?
= y(i) and y?
=argmaxy?Y(x(i)) score(x(i),y;?
).Adaptation for MT We chart the transformationsfrom Eq.
12 toward the loss Liang et al?s algorithmactually optimized.
First, generalize to latent vari-ables; see Eq.
9 (Fig.
2), sacrificing convexity.
Sec-ond, to cope with unreachable references, use a k-best surrogate as shown in Eq.
10 (Fig.
2), whereKi ?
T(x(i))k is a set containing the k best out-put pairs for x(i).
Now the loss only depends ony(i) through the cost function.
(Even without hid-den variables, this loss can only be convex when thek-best list is fixed, keeping y?
unchanged across it-erations.
Updating the k-best lists makes y?
dependon ?, resulting in a non-convex loss.
)It appears that Eq.
10 (Fig.
2) is the loss thatLiang et al (2006) sought to optimize, using SSD.
Inlight of footnote 5 and the non-convexity of Eq.
10(Fig.
2), we have no theoretical guarantee that suchan algorithm will find a (local) optimum.225Input: inputs {x(i)}Ni=1, references {y(i)}Ni=1, init.weights ?0, k-best list size k, step size ?, `2reg.
coeff.
C, # iters T , # CCCP iters T ?, #SSD iters T ?
?Output: learned weights: ??
?
?0;1for iter ?
1 to T do2{Ki}Ni=1 ?
Decode({x(i)}Ni=1,?, k);3for iter ?
?
1 to T ?
do4for i?
1 to N do5?y+i ,h+i ?
?6argmax?y,h?
?Ki scorei(y,h;?)?
costi(y);end7for iter ??
?
1 to T ??
do8for i?
1 to N do9?y?,h??
?10argmax?y,h?
?Ki scorei(y,h;?)
+ costi(y);?
?= ?C(???0N);11?
+= ?
(f(x(i),y+i ,h+i )?
f(x(i),y?,h?
));12end13end14end15end16return ?
;17Algorithm 1: RAMPION.We note that Eq.
10 is similar to Eq.
11 (Fig.
2),where each ?
is used to trade off between model andcost.
Fig.
1 illustrates the similarity by showing thatthe min-cost output on a k-best list resides in a simi-lar region of the output space as ?y+,h+?
computedfrom the full output space.
While it is not the casethat we can always choose ?i so as to make the twolosses equivalent, they are similar in that they up-date towards some y?
with high model score andlow cost.
Eq.
11 corresponds to Eq.
7 (Fig.
2), thesecond form of the latent structured ramp loss.Thus, one way to understand Liang et al?s algo-rithm is as a form of structured ramp loss.
However,another interpretation is given by McAllester et al(2010), who showed that procedures like that usedby Liang et al approach direct cost minimization inthe limiting case.3.3 Large-Margin MethodsA related family of approaches for training MT mod-els involves the margin-infused relaxed algorithm(MIRA; Crammer et al, 2006), an online large-margin training algorithm.
It has recently shownsuccess for MT, particularly when training modelswith large feature sets (Watanabe et al, 2007; Chi-ang et al, 2008; Chiang et al, 2009).
In order toapply it to MT, Watanabe et al and Chiang et almade modifications similar to those made by Lianget al for perceptron training, namely the extensionto latent variables and the use of a surrogate refer-ence with high model score and low cost.Hinge Loss It can be shown that 1-best MIRA corre-sponds to dual coordinate ascent for the structuredhinge loss when using `2 regularization (Martins etal., 2010).
The structured hinge is the loss underly-ing maximum-margin Markov networks (Taskar etal., 2003): setting y?
= y(i) and:y?
= argmaxy?Y(x(i))(score(x(i),y;?)
+ cost(y(i),y))(13)Unlike the perceptron losses, which penalize thehighest-scoring outputs, hinge loss penalizes an out-put that is both favored by the model and has highcost.
Such an output is shown as ?y?,h??
in Fig.
1;the structured hinge loss focuses on pushing suchoutputs to the left.
As mentioned in ?3.1, finding y?is often called cost-augmented decoding.Structured hinge loss is convex, can incorporatea cost function, and can be optimized with severalalgorithms, including SSD (Ratliff et al, 2006).Adaptation for MT While prior work has usedMIRA-like algorithms for training machine transla-tion systems, the proposed algorithms did not actu-ally optimize the structured hinge loss, for similarreasons to those mentioned above for the perceptron:latent variables and surrogate references.
Incorpo-rating latent variables in the hinge loss results inthe latent structured hinge loss (Yu and Joachims,2009).
Like the latent perceptron, this loss is non-convex and inappropriate for MT because it requirescomputing the feature vector for y(i).
By using asurrogate instead of y(i), the actual loss optimizedbecomes closer to Eq.
8 (Fig.
2), the third form ofthe latent structured ramp loss.Watanabe et al (2007) and Arun and Koehn(2007) used k-best oracles like Liang et al, but Chi-ang et al (2008, 2009) used a different approach, ex-plicitly defining the surrogate as ?y+,h+?
in Fig.
1.While the method of Chiang et al showed impres-226sive performance improvements, its implementationis non-trivial, involving a complex cost function anda parallel architecture, and it has not yet been em-braced by the MT community.
Indeed, the com-plexity of Chiang et als algorithm was one of thereasons cited for the development of PRO (Hopkinsand May, 2011).
In this paper, we have sought toisolate the loss functions used in prior work like thatby Chiang et al and identify simple, generic opti-mization procedures for optimizing them.
We offerRAMPION as an alternative to Chiang et als MIRAthat is simpler to implement and achieves empiricalsuccess in experiments (?4).3.4 Likelihood and Softened LossesWe can derive new loss functions from the aboveby converting any ?max?
operator to a ?softmax?
(log?exp, where the set of elements under thesummation is the same as under the max).
For exam-ple, the softmax version of the perceptron loss is thewell-known log loss (?2, Ex.
2), the loss underlyingthe conditional likelihood training criterion whichis frequently used when a probabilistic interpreta-tion of the learned model is desired, as in conditionalrandom fields (Lafferty et al, 2001).Och and Ney (2002) popularized the use of log-linear models for MT and initially sought to opti-mize log loss, but by using the min-cost transla-tion on a k-best list as their surrogate, we argue thattheir loss was closer to the soft ramp loss obtainedby softening the second max in lossramp 2 in Eq.
7(Fig.
2).
The same is true for others who aimed tooptimize log loss for MT (Smith and Eisner, 2006;Zens et al, 2007; Cer, 2011).The softmax version of the latent variable percep-tron loss, Eq.
9 (Fig.
2), is the latent log loss inher-ent in latent-variable CRFs (Quattoni et al, 2004).Blunsom et al (2008) and Blunsom and Osborne(2008) actually did optimize latent log loss for MT,discarding training examples for which y(i) was un-reachable by the model.Finally, we note that ?softening?
the ramp lossin Eq.
6 (Fig.
2) results in the Jensen riskbound from Gimpel and Smith (2010), which isa computationally-attractive upper bound on theBayes risk.4 ExperimentsThe goal of our experiments is to compare RAM-PION (Alg.
1) to state-of-the-art methods for train-ing MT systems.
RAMPION minimizes lossramp 3,which we found in preliminary experiments to workbetter than other loss functions tested.8System and Datasets We use the Moses phrase-based MT system (Koehn et al, 2007) and considerUrdu?English (UR?EN), Chinese?English(ZH?EN) translation, and Arabic?English(AR?EN) translation.9 We trained a Moses systemusing default settings and features, except forsetting the distortion limit to 10.
Word alignmentwas performed using GIZA++ (Och and Ney, 2003)in both directions, the grow-diag-final-andheuristic was used to symmetrize the alignments,and a max phrase length of 7 was used for phraseextraction.
We estimated 5-gram language modelsusing the SRI toolkit (Stolcke, 2002) with modifiedKneser-Ney smoothing (Chen and Goodman, 1998).For each language pair, we used the English sideof the parallel text and 600M words of randomly-selected sentences from the Gigaword v4 corpus(excluding NYT and LAT).For UR?EN, we used parallel data from theNIST MT08 evaluation consisting of 1.2M Urduwords and 1.1M English words.
We used half ofthe documents (882 sentences) from the MT08 testset for tuning.
We used the remaining half forone test set (?MT08??)
and MT09 as our other testset.
For ZH?EN, we used 303k sentence pairsfrom the FBIS corpus (LDC2003E14).
We seg-mented the Chinese data using the Stanford Chi-nese segmenter (Chang et al, 2008) in ?CTB?
mode,giving us 7.9M Chinese words and 9.4M Englishwords.
We used MT03 for tuning and used MT02and MT05 for testing.For AR?EN, we used data provided by the LDC8We only present full results using lossramp 3.
We foundthat minimizing lossramp 1 did poorly, resulting in single-digitBLEU scores, and that lossramp 2 reached high BLEU scores onthe tuning data but failed to generalize well.
Softened versionsof the ramp losses performed comparably to lossramp 3 but wereslightly worse on both tuning and held-out data.9We found similar trends for other language pairs and sys-tems, including Hiero (Chiang, 2005).
A forthcoming reportwill present these results, as well as experiments with additionalloss functions, in detail.227for the NIST evaluations, including 3.29M sentencepairs of UN data and 982k sentence pairs of non-UN data.
The Arabic data was preprocessed usingan HMM segmenter that splits off attached prepo-sitional phrases, personal pronouns, and the futuremarker (Lee et al, 2003).
The common stylisticsentence-initial wa# (and ...) was removed from thetraining and test data.
The resulting corpus con-tained 130M Arabic tokens and 130M English to-kens.
We used MT06 for tuning and three test sets:MT05, the MT08 newswire test set (?MT08 NW?
),and the MT08 weblog test set (?MT08 WB?
).For all languages we evaluated translation outputusing case-insensitive IBM BLEU (Papineni et al,2001).Training Algorithms Our baselines are MERT andPRO as implemented in the Moses toolkit.10 PROuses the hyperparameter settings from Hopkins andMay (2011), including k-best lists of size 1500 and25 training iterations.11 MERT uses k-best lists ofsize 100 and was run to convergence.
For bothMERT and PRO, previous iterations?
k-best listswere merged in.For RAMPION, we used T = 20, T ?
= 10,T ??
= 5, k = 500, ?
= 0.0001, and C = 1.Our cost function is ?
(1 ?
BLEU+1(y,y?))
whereBLEU+1(y,y?)
returns the BLEU+1 score (Lin andOch, 2004) for reference y and hypothesis y?.
Weused ?
= 10.
We used these same hyperparametervalues for all experiments reported here and foundthem to perform well across other language pairsand systems.124.1 ResultsTable 1 shows our results.
MERT and PRO were run3 times with differing random seeds and averages10The PRO algorithm samples pairs of translations from k-best lists on each iteration and trains a binary classifier to rankpairs according to the cost function.
The loss function under-lying PRO depends on the choice of binary classifier and alsoon the sampling strategy.
We leave an analysis of PRO?s lossfunction to future work.11Hopkins and May used 30 iterations, but showed that train-ing had converged by 25.12We found performance to be better when using a smallervalue of T ?
; we suspect that using small T ?
guards against over-fitting to any particular set of k-best lists.
We also found thevalue of ?
to affect performance, although ?
?
{1, 5, 10} allworked well.
Performance was generally insensitive to C. Wefixed ?
= 0.0001 early on and did little tuning to it.35 363536MT02BLEUTune BLEU35 363435MT05BLEUTune BLEUMERTPRORampionFigure 3: ZH?EN training runs.
The cluster of PROpoints to the left corresponds to one of the random initialmodels; MERT and RAMPION were able to recover whilePRO was not.and standard deviations are shown.
The three al-gorithms perform very similarly on the whole, withcertain algorithms performing better on certain lan-guages.
MERT shows larger variation across ran-dom seeds, as reported by many others in the com-munity.
On average across all language pairs andtest sets, RAMPION leads to slightly higher BLEUscores.4.2 Sensitivity AnalysisWe now measure the sensitivity of these trainingmethods to different initializers and to randomnessin the algorithms.
RAMPION is deterministic, butMERT uses random starting points and search di-rections and PRO uses random sampling to choosepairs for training its binary classifier.For initial models, we used the default parame-ters in Moses as well as two randomly-generatedmodels.13 We ran RAMPION once with each of thethree initial models, and MERT and PRO three timeswith each.
This allows us to compare variance dueto initializers as well as due to the nondeterminismin each algorithm.
Fig.
3 plots the results.
WhilePRO exhibits a small variance for a given initializer,as also reported by Hopkins and May (2011), it had13The default weights are 0.3 for reordering features, 0.2 forphrase table features, 0.5 for the language model, and -1 for theword penalty.
We generated each random model by samplingeach feature weight from aN(?, ?2) with ?
equal to the defaultweight for that feature and ?
= |?/2|.228MethodUR?EN ZH?EN AR?ENavgMT08?
MT09 MT02 MT05 MT05 MT08 NW MT08 WBMERT 24.5 (0.1) 24.6 (0.0) 35.7 (0.3) 34.2 (0.2) 55.0 (0.7) 49.8 (0.3) 32.6 (0.2) 36.6PRO 24.2 (0.1) 24.2 (0.1) 36.3 (0.1) 34.5 (0.0) 55.6 (0.1) 49.6 (0.0) 31.7 (0.0) 36.6RAMPION 24.5 24.6 36.4 34.7 55.5 49.8 32.1 36.8Table 1: %BLEU on several test sets for UR?EN, ZH?EN, and AR?EN translation.
Algorithms with randomization(MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed bystandard deviations in parentheses.
All results in this table used a single initial model (the default Moses weights).The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERTand PRO and 7 for RAMPION.MethodUR?EN ZH?ENTune MT08?
MT09 Tune MT02 MT05PRO 29.4 22.3 23.0 40.9 35.7 33.6RAMPION 27.8 24.2 24.6 38.8 36.2 34.3Table 2: %BLEU with large feature sets.trouble recovering from one of the random initializ-ers.
Therefore, while the within-initializer variancefor PRO tended to be smaller than that of MERT,PRO?s overall range was larger.
RAMPION foundvery similar weights regardless of ?0.4.3 Adding FeaturesFinally, we compare RAMPION and PRO with an ex-tended feature set; MERT is excluded as it fails insuch settings (Hopkins and May, 2011).We added count features for common monolin-gual and bilingual lexical patterns from the parallelcorpus: the 1k most common bilingual word pairsfrom phrase extraction, 200 top unigrams, 1k top bi-grams, 1k top trigrams, and 4k top trigger pairs ex-tracted with the method of Rosenfeld (1996), rankedby mutual information.
We integrated the featureswith our training procedure by using Moses to gen-erate lattices instead of k-best lists.
We used cubepruning (Chiang, 2007) to incorporate the additional(potentially non-local) features while extracting k-best lists from the lattices to pass to the training al-gorithms.14Results are shown in Table 2.
We find that PROfinds much higher BLEU scores on the tuning databut fails to generalize, leading to poor performanceon the held-out test sets.
We suspect that incorporat-ing regularization into training the binary classifierwithin PRO may mitigate this overfitting.
RAMPIONis more stable by contrast.
This is a challenginglearning task, as lexical features are prone to over-14In cube pruning, each node?s local n-best list had n = 100.fitting with a small tuning set.
Hopkins and May(2011) similarly found little gain on test data whenusing extended feature sets in phrase-based transla-tion for these two language pairs.Results for AR?EN translation were similar andare omitted for space; these and additional experi-ments will be included in a forthcoming report.5 ConclusionWe have framed MT training as empirical risk min-imization and clarified loss functions that were op-timized by well-known procedures.
We have pro-posed directly optimizing the structured ramp lossimplicit in prior work with a novel algorithm?RAMPION?which performs comparably to state-of-the-art training algorithms and is empiricallymore stable.
Our source code, which integrateseasily with Moses, is available at www.ark.cs.cmu.edu/MT.AcknowledgmentsWe thank Colin Cherry, Chris Dyer, Joseph Keshet,David McAllester, and members of the ARK researchgroup for helpful comments that improved this paper.This research was supported in part by the NSF throughCAREER grant IIS-1054319, the U. S. Army ResearchLaboratory and the U. S. Army Research Office undercontract/grant number W911NF-10-1-0533, and SandiaNational Laboratories (fellowship to K. Gimpel).ReferencesA.
Arun and P. Koehn.
2007.
Online learning methodsfor discriminative training of phrase based statisticalmachine translation.
In Proc.
of MT Summit XI.P.
Blunsom and M. Osborne.
2008.
Probabilistic infer-ence for machine translation.
In Proc.
of EMNLP.P.
Blunsom, T. Cohn, and M. Osborne.
2008.
A discrim-inative latent variable model for statistical machinetranslation.
In Proc.
of ACL.229D.
Cer, D. Jurafsky, and C. Manning.
2008.
Regular-ization and search for minimum error rate training.
InProc.
of ACL-2008 Workshop on Statistical MachineTranslation.D.
Cer.
2011.
Parameterizing Phrase Based Statisti-cal Machine Translation Models: An Analytic Study.Ph.D.
thesis, Stanford University.P.
Chang, M. Galley, and C. Manning.
2008.
Optimiz-ing Chinese word segmentation for machine transla-tion performance.
In Proc.
of ACL-2008 Workshop onStatistical Machine Translation.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report 10-98, Harvard University.C.
Cherry and G. Foster.
2012.
Batch tuning strategiesfor statistical machine translation.
In Proc.
of NAACL.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of EMNLP.D.
Chiang, W. Wang, and K. Knight.
2009.
11,001 newfeatures for statistical machine translation.
In Proc.
ofNAACL-HLT.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.J.
H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011.Better hypothesis testing for statistical machine trans-lation: Controlling for optimizer instability.
In Proc.of ACL.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
of EMNLP.R.
Collobert, F. Sinz, J. Weston, and L. Bottou.
2006.Trading convexity for scalability.
In ICML.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.C.
B.
Do, Q.
Le, C. H. Teo, O. Chapelle, and A. Smola.2008.
Tighter bounds for structured estimation.
InProc.
of NIPS.R.
O. Duda and P. E. Hart.
1973.
Pattern classificationand scene analysis.
John Wiley, New York.G.
Foster and R. Kuhn.
2009.
Stabilizing minimum errorrate training.
In Proc.
of Fourth Workshop on Statisti-cal Machine Translation.K.
Gimpel and N. A. Smith.
2010.
Softmax-marginCRFs: Training log-linear models with cost functions.In Proc.
of NAACL.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InProc.
of EMNLP.B.
H. Juang, W. Chou, and C. H. Lee.
1997.
Minimumclassification error rate methods for speech recogni-tion.
Speech and Audio Processing, IEEE Transac-tions on, 5(3):257?265, may.J.
Kaiser, B. Horvat, and Z. Kacic.
2000.
A novel lossfunction for the overall risk criterion based discrimina-tive training of hmm models.
In Proc.
of ICSLP.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL (demosession).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.Y.
Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.2003.
Language model based Arabic word segmenta-tion.
In Proc.
of ACL.Z.
Li and J. Eisner.
2009.
First- and second-order ex-pectation semirings with applications to minimum-risktraining on translation forests.
In Proc.
of EMNLP.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of COLING-ACL.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metrics formachine translation.
In Proc.
of Coling.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008.Lattice-based minimum error rate training for statisti-cal machine translation.
In EMNLP.A.
F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,P.
M. Q. Aguiar, and M A. T. Figueiredo.
2010.
Learn-ing structured classifiers with dual coordinate descent.Technical report, Carnegie Mellon University.D.
McAllester and J. Keshet.
2011.
Generalizationbounds and consistency for latent structural probit andramp loss.
In Proc.
of NIPS.D.
McAllester, T. Hazan, and J. Keshet.
2010.
Directloss minimization for structured prediction.
In Proc.of NIPS.R.
C. Moore and C. Quirk.
2008.
Random restartsin minimum error rate training for statistical machinetranslation.
In Proc.
of Coling.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of ACL.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In Proc.
of ACL.230K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.D.
Povey and P. C. Woodland.
2002.
Minimum phoneerror and I-smoothing for improved discrimative train-ing.
In Proc.
of ICASSP.A.
Quattoni, M. Collins, and T. Darrell.
2004.
Condi-tional random fields for object recognition.
In NIPS17.N.
Ratliff, J.
A. Bagnell, and M. Zinkevich.
2006.Subgradient methods for maximum margin structuredlearning.
In ICML Workshop on Learning in Struc-tured Output Spaces.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Computer,Speech and Language, 10(3).D.
A. Smith and J. Eisner.
2006.
Minimum risk an-nealing for training log-linear models.
In Proc.
ofCOLING-ACL.A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Advances in NIPS 16.V.
Vapnik.
1998.
Statistical learning theory.
Wiley.T.
Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In Proc.
of EMNLP-CoNLL.C.
J. Yu and T. Joachims.
2009.
Learning structuralSVMs with latent variables.
In Proc.
of ICML.A.
L. Yuille and Anand Rangarajan.
2002.
The concave-convex procedure (CCCP).
In Proc.
of NIPS.
MITPress.R.
Zens, S. Hasan, and H. Ney.
2007.
A systematic com-parison of training criteria for statistical machine trans-lation.
In Proc.
of EMNLP.231
