Clustering and Diversifying Web SearchResults with Graph-Based WordSense InductionAntonio Di Marco?Sapienza University of RomeRoberto Navigli?Sapienza University of RomeWeb search result clustering aims to facilitate information search on the Web.
Rather than theresults of a query being presented as a flat list, they are grouped on the basis of their similarityand subsequently shown to the user as a list of clusters.
Each cluster is intended to representa different meaning of the input query, thus taking into account the lexical ambiguity (i.e.,polysemy) issue.
Existing Web clustering methods typically rely on some shallow notion oftextual similarity between search result snippets, however.
As a result, text snippets with noword in common tend to be clustered separately even if they share the same meaning, whereassnippets with words in common may be grouped together even if they refer to different meaningsof the input query.In this article we present a novel approach to Web search result clustering based on theautomatic discovery of word senses from raw text, a task referred to as Word Sense Induction.Key to our approach is to first acquire the various senses (i.e., meanings) of an ambiguousquery and then cluster the search results based on their semantic similarity to the word sensesinduced.
Our experiments, conducted on data sets of ambiguous queries, show that our approachoutperforms both Web clustering and search engines.1.
IntroductionThe Web is by far the largest information archive available worldwide.
This vast pool oftext contains information of the most wildly disparate kinds, and is potentially capableof satisfying virtually any conceivable user need.
Unfortunately, however, in this settingretrieving the precise item of information that is relevant to a given user search can belike looking for a needle in a haystack.
State-of-the-art search engines such as Googleand Yahoo!
generally do a good job at retrieving a small number of relevant results fromsuch an enormous collection of data (i.e., retrieving with high precision, low recall).Such systems today, however, still find themselves up against the lexical ambiguity issue?
Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.E-mail: {dimarco,navigli}@di.uniroma1.it.Submission received: 25 April 2012; revised submission received: 26 July 2012; accepted for publication:12 September 2012.doi:10.1162/COLI a 00148?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 3(Furnas et al1987; Navigli 2009), that is, the linguistic property due to which a singleword may convey different meanings.Recently, the degree of ambiguity of Web queries has been studied using WordNet(Miller et al1990; Fellbaum 1998) and Wikipedia1 as sources of ambiguous words.2It has been estimated that around 4% of Web queries and 16% of the most frequentqueries are ambiguous (Sanderson 2008), as also confirmed in later studies (Clough etal.
2009; Song et al2009).
An example of an ambiguous query is Butterfly effect, whichcould refer to either chaos theory, a film, a band, an album, a novel, or a collection ofpoetry.
Similarly, black spider could refer to either an arachnid, a car, or a frying pan, andso forth.Lexical ambiguity is often the consequence of the low number of query words thatWeb users, on average, tend to type (Kamvar and Baluja 2006).
This issue could besolved by expanding the initial query with unequivocal cue words.
Interestingly, theaverage query length is continually growing.
The average number of words per query isnow estimated around three words per query,3 a number that is still too low to eradicatepolysemy.The fact that there may be different informative needs for the same user query hasbeen tackled by diversifying search results, an approach whereby a list of heterogene-nous results is presented, and Web pages that are similar to ones already near the topare prevented from ranking too highly in the list (Agrawal et al2009; Swaminathan,Mathew, and Kirovski 2009).
Today even commercial search engines are starting torerank and diversify their results.
Unfortunately, recent work suggests that diversitydoes not yet play a primary role in ranking algorithms (Santamar?
?a, Gonzalo, andArtiles 2010), but it undoubtedly has the potential to do so (Chapelle, Chang, and Liu2011).Another mainstream approach to the lexical ambiguity issue is that of Web cluster-ing engines (Carpineto et al2009), such as Carrot4 and Yippy.5 These systems groupsearch results by providing a cluster for each specific meaning of the input query.
Userscan then select the cluster(s) and the pages therein that best answer their informationneeds.
These approaches, however, do not perform any semantic analysis of searchresults, clustering them solely on the basis of their lexical similarity.For instance, given the query snow leopard, Google search returns, among others,the snippets reported in Table 1.6 In the third column of the table we provide the correctmeanings associated with each snippet (i.e., either the operating system or the animalsense).
Although snippets 2, 4, and 5 all refer to the same meaning, they have no contentword in common apart from our query words.
As a result, a traditional Web clusteringengine would most likely assign these snippets to different clusters.
Moreover, snippet6 shares words with snippets referring to both query meanings (i.e., snippets 1, 2, and3 in Table 1), thus making it even harder for Web clustering engines to group search1 http://www.wikipedia.org.2 Note that we focus here on the ambiguity of queries in terms of their polysemy, rather than on theidentification of aspects or subsenses of a given meaning of a query, as was done in recent work on topicidentification (Wang, Chakrabarti, and Punera 2009; Xue and Yin 2011; Wu, Madhavan, and Halevy2011).
We discuss this point further in Section 2.6.3 See Hitwise on 2008?2009 Google data: http://www.hitwise.com/us/press-center/press-releases/google-searches-apr-09.4 http://search.carrot2.org/stable/search.5 http://search.yippy.com.6 Results retrieved in May 2011.710Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 1Some of the top-ranking snippets returned for snow leopard by Google search.# Snippet Meaning1 To advance Mac OS X Leopard, Apple engineers... SOFTWARE2 The snow leopard (Uncia uncia or Panthera uncia) is a moderatelylarge cat native to the mountain rangesANIMAL3 Mac OS X Snow Leopard (version 10.6) is the seventh and current major... SOFTWARE4 Get the facts on snow leopards.
Endangered Species Act (ESA): thesnow leopard is listed as endangered...ANIMAL5 Snow leopards are exceptional athletes capable of making huge leaps overravines.ANIMAL6 Snow Leopard.
Even the name seems to underpromise ?
it?s thefirst ?big cat?
OS X codename to referenceSOFTWAREresults effectively.
Finally, none of the top-ranking snippets refers to The Snow Leopard,a popular 1978 book by Peter Matthiessen.In this article, we present a novel approach to Web search result clustering thatexplicitly addresses the language ambiguity issue.
Key to our approach is the use ofWord Sense Induction (WSI), that is, techniques aimed at automatically discovering thedifferent meanings of a given term (i.e., query).
Each sense of the query is representedas a cluster of words co-occurring in raw text with the query.
Each search result snippetreturned by a Web search engine is then mapped to the most appropriate meaning(i.e., cluster) and the resulting clustering of snippets is returned.This article provides four main contributions: We present a general evaluation framework for Web search resultclustering, which we also exploit to perform a large-scale end-to-endexperimental comparison of several graph-based WSI algorithms.
In fact,the output of WSI (i.e., the automatically discovered senses) is evaluatedin terms of both the quality of the corresponding search result clustersand the resulting ability to diversify search results.
This is in contrast withmost literature in the field of Word Sense Induction, where experimentsare mainly performed in vitro (i.e., not in the context of an everydayapplication; Matsuo et al2006; Manandhar et al2010). In order to test whether our results were strongly dependent onthe evaluation measures we implemented in the framework, wecomplemented our extrinsic experimental evaluation with a qualitativeanalysis of the automatically induced senses.
This study was performedvia a manual evaluation carried out by several human annotators. We present novel versions of previously proposed WSI graph-basedalgorithms, namely, SquaT++ and Balanced Maximum Spanning Tree(B-MST) (the former is an enhancement of the original SquaT algorithm[Navigli and Crisafulli 2010], and the latter is a variant of MST [Di Marcoand Navigli 2011] that produces more balanced clusters). We show how, thanks to our framework, WSI can be successfullyintegrated into real-world applications, such as Web search result711Computational Linguistics Volume 39, Number 3Table 2The top five categories returned by the Open Directory Project for the query snow leopard.ODP Category # pagesScience: Biology: Flora and Fauna: .
.
.
Felidae: Uncia 6Kids and Teens: School Time: Science: .
.
.
Leopards: Snow Leopards 4Science: Environment: Biodiversity: Conservation: Mammals: Felines 3Kids and Teens: School Time: Science: .
.
.
Animals: Endangered Species 1Computers: Emulators: Apple: Macintosh: SheepShaver 1clustering, so as to outperform non-semantic state-of-the-art Webclustering systems.
To the best of our knowledge, with the exceptionof some very preliminary results (Ve?ronis 2004; Basile, Caputo,and Semeraro 2009), this is the first time that unsupervised textunderstanding techniques have been shown to considerably boostan Information Retrieval task in a solid evaluation framework.This article extends previous conference work (Navigli and Crisafulli 2010;Di Marco and Navigli 2011) by performing a novel, in-depth study of the interactionsbetween different corpora and several different WSI algorithms, including novel ones,within the same framework, and, additionally, by providing a comparison with a state-of-the-art search result clustering engine.The article is structured as follows: in Section 2 we present related work, inSection 3 we illustrate our approach, end-to-end experiments are reported in Section 4,and in vitro experiments are discussed in Section 5.
We present a time performanceanalysis in Section 6, and conclude the paper in Section 7.2.
Related WorkOur work is aimed at addressing the difficulties arising within the different approachesto the issue of lexical ambiguity in Web Information Retrieval.
Given the large bodyof work in this field, in this section we summarize the main research directions onthe topic.2.1 Web DirectoriesIn Web 1.0?mainly based on static Web pages?the solution to clustering search resultswas that of manually organizing and categorizing Web sites.
The resulting repositoriesare called Web directories and list Web sites by category and possible subcategories.These categories are sometimes organized as taxonomies (like in the Open DirectoryProject, ODP7).Although Web directories are not search engines, information can be searchedtherein.
So, given a query, the returned search results are organized by category.
Forinstance, given the query snow leopard the ODP returns the categories shown in Table 27 http://www.dmoz.org.712Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI(the number of matching Web pages is reported in the second column).
As can be seenfrom this example, the Web directory approach has evident limits:1.
It is static, thus it needs manual updates to cover new pages and newmeanings (e.g., the book sense of snow leopard is not considered in ODP).2.
It covers only a small portion of the Web (e.g., we only have one Web pagecategorized with the computing sense of snow leopard, cf.
the last row ofTable 2).3.
It classifies Web pages using coarse categories.
This latter feature ofWeb directories makes it difficult to distinguish between instances of thesame kind (e.g., pages about artists with the same surname classified asArts:Music:Bands and Artists).Although methods for the automatic classification of Web documents have beenproposed (Liu et al2005; Xue et al2008, inter alia) and some problems have beentackled effectively (Bennett and Nguyen 2009), these approaches are usually supervisedand still suffer from reliance on a predefined taxonomy of categories.
Finally, it has beenreported that directory-based systems are among the most ineffective solutions to Webinformation retrieval (Bruza, McArthur, and Dennis 2000).2.2 Semantic Information RetrievalA second approach to query ambiguity consists of associating explicit semantics (i.e.,word senses or concepts) with queries and documents, that is, performing SemanticInformation Retrieval (SIR).
SIR is performed by indexing and searching conceptsrather than terms, that is, by means of Word Sense Disambiguation (WSD; Navigli 2009),thus potentially coping with two linguistic phenomena: expressing a single meaningwith different words (synonymy) and using the same word to express various differentmeanings (polysemy).
The main idea is that assigning concepts to words can potentiallyovercome these two issues, enabling a shift from the lexical to the semantic level to beachieved.Over the years, various methods for SIR have been proposed (Krovetz and Croft1992; Voorhees 1993; Mandala, Tokunaga, and Tanaka 1998; Gonzalo, Penas, and Verdejo1999; Kim, Seo, and Rim 2004; Liu, Yu, and Meng 2005, inter alia).
Contrasting resultshave been reported on the benefits of these techniques, however: It has been shownthat WSD has to be very accurate to benefit Information Retrieval (Sanderson 1994)?aresult that was later debated (Gonzalo, Penas, and Verdejo 1999; Stokoe, Oakes, and Tait2003).
Also, it has been reported that WSD has to be very precise on minority senses anduncommon terms, rather than on frequent words (Krovetz and Croft 1992; Sanderson2000).The main drawback of SIR is that it relies on the existence of a reference dictionaryto perform WSD (typically, WordNet) and thus suffers this dictionary?s static nature andits inherent paucity of most proper nouns.
This latter problem is particularly importantfor Web searches, as users tend to retrieve more information about named entities(e.g., singers, artists, cities) than concepts (such as abstract information about singers orartists).
Although lexical knowledge resources that integrate lexicographic senses withnamed entites on a large scale have recently been created (Navigli and Ponzetto 2012),713Computational Linguistics Volume 39, Number 3it is still to be shown that their use for SIR is beneficial.
Moreover, these resources donot yet tackle the dynamic evolution of language.In contrast, our WSI approach to search result clustering automatically discoversboth lexicographic and encyclopedic senses of a query (including new ones), thus takinginto account all of the mentioned issues.2.3 Search Result ClusteringA more popular approach to query ambiguity is that of search result clustering.
Typ-ically, given a query, the system starts from a flat list of text snippets returned fromone or more commonly available search engines and clusters them on the basis of somenotion of textual similarity.
At the root of the clustering approach lies van Rijsbergen?scluster hypothesis (van Rijsbergen 1979, page 45): ?closely associated documents tendto be relevant to the same requests,?
whereas results concerning different meanings ofthe input query are expected to belong to different clusters.Approaches to search result clustering can be classified as data-centric ordescription-centric (Carpineto et al2009).
The former focus more on the problem ofdata clustering than on presenting the results to the user.
A pioneering example isScatter/Gather (Cutting et al1992), which divides the data set into a small numberof clusters and, after the selection of a group, performs clustering again and proceedsiteratively.
Developments of this approach have been proposed that improve on clusterquality and retrieval performance (Ke, Sugimoto, and Mostafa 2009).
Other data-centricapproaches use agglomerative hierarchical clustering (e.g., LASSI [Maarek et al2000]),rough sets (Ngo and Nguyen 2005), or exploit link information (Zhang, Hu, and Zhou2008).Description-centric approaches are, instead, more focused on the description toproduce for each cluster of search results.
Among the most popular and successfulapproaches are those based on suffix trees.
Suffix trees are rooted directed trees thatcontain all the suffixes of a string s. The label of each edge is a non-empty substring ofs and each vertex v is labeled with the concatenation of the edge labels on the pathfrom the root to v. If we view the search result snippets to be clustered as a set ofstrings (i.e., their bag of words), each vertex of the corresponding suffix tree can beconsidered as a set of documents that share a phrase (i.e., the label of the vertex itself)and therefore the vertices represent a set of base clusters B = (b1, b2, .
.
.
, bn).
The originalSuffix Tree Clustering (STC; Zamir et al1997; Zamir and Etzioni 1998) algorithm obtainsthe final clustering by merging the clusters in B with a high overlap in the documentsthey contain.
A scoring function is defined, based on both the number of documents inthe base cluster and the length of the common phrase, with the aim of returning onlythe top k clusters.Later developments improved the performance of the STC algorithm usingdocument?document similarity scores in order to overcome the low scalability of theoriginal approach (Branson and Greenberg 2002).
Crabtree, Gao, and Andreae (2005)identified an issue in the original scoring function whereby unreasonably high scorestend to be assigned to clusters obtained as a result of the merging of very similarbase clusters.
To solve this problem, they proposed the Extended Suffix Tree Clusteringalgorithm (ESTC) with a novel scoring function and a new procedure for selecting thetop k clusters to be returned.More recent approaches based on suffix trees extract relevant keyphrases fromgeneralized suffix trees (i.e., trees which contain suffixes of a set of strings714Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIS = {s1, s2, .
.
.
, s|S|}) in order to choose meaningful labels for the output clusters(Bernardini, Carpineto, and D?Amico 2009; Carpineto, D?Amico, and Bernardini 2011).Other approaches to description-centric search result clustering in the literatureare based on formal concept analysis (Carpineto and Romano 2004), singular valuedecomposition (Osinski and Weiss 2005), spectral clustering (Cheng et al2005), spectralgeometry (Liu et al2008), link analysis (Gelgi, Davulcu, and Vadrevu 2007), and graphconnectivity measures (Di Giacomo et al2007).
Search result clustering has also beenviewed as a supervised salient phrase ranking task (Zeng et al2004).Whereas search result clustering has heretofore been performed without the explicituse of lexical semantics, in our work we show how to exploit search result clusteringas the common evaluation framework of both semantic and non-semantic clusteringengines.2.4 DiversificationRather than clustering the top search results by their similarity, one can aim at rerankingthem on the basis of criteria that maximize their diversity, so as to present top resultswhich are as different from each other as possible.
This technique, called diversificationof search results, is a recent research topic that, yet again, deals with the query ambiguityissue.
To some extent, today?s search engines, such as Google and Yahoo!, apply somediversification technique to their top-ranking results.One of the first examples of diversification algorithms was based on the use ofsimilarity functions to measure the diversity between documents and between docu-ment and query (Carbonell and Goldstein 1998).
Other diversification techniques useconditional probabilities to determine which document is most different from higher-ranking ones (Chen and Karger 2006), or use affinity ranking (Zhang et al2005), basedon topic variance and coverage.An algorithm called Essential Pages (Swaminathan, Mathew, and Kirovski 2009)has been proposed that aims to reduce information redundancy and returns Web pagesthat maximize coverage with respect to the input query.
In this approach the Websearch results for a query q are transformed into bags of words containing the termsoccurring in the corresponding Web page.
Frequency information from raw corpora isthen used to find relevant words for q, that is, words which are generally infrequent,but occur often in the results retrieved for q.
The coverage score of a search result r isthen calculated as a function of the number of terms relevant for q and contained inr.
Another interesting approach reformulates the problem explicitly in terms of howto minimize the risk of dissatisfaction for the average user (Agrawal et al2009).
Agreedy algorithm is proposed that balances between relevance and diversity of thesearch results.
The algorithm is evaluated using generalizations of classical InformationRetrieval metrics that are based on statistical considerations and take into account theintentions of the user.More recently, vector space model representations have been explored to improvediversity in search results (Santamar?
?a, Gonzalo, and Artiles 2010).
Web page resultsare represented as vectors and compared against vector representations of encyclopedicentries available from Wikipedia using cosine similarity.
Search results are diversifiedaccordingly.Finally, in the last few years the specific structure of the Web has been exploitedto perform diversification, as proposed by Ma, Lyu, and King (2010), who make use ofMarkov random walks on query-URL bipartite graphs, and Chandar and Carterette715Computational Linguistics Volume 39, Number 3(2010), who cluster search results by exploiting the links in Web pages in order toidentify the subtopics of the returned documents.2.5 Word Sense InductionA fifth solution to the query ambiguity issue is Word Sense Induction (WSI), namely, theautomatic discovery of word (i.e., query) senses from raw text (see Navigli [2009, 2012]for a survey).
WSI allows us to go beyond the surface similarity of Web snippets (whichhampers the performance of Web search result clustering) by dynamically acquiring aninventory of senses of the input query.
The core idea is to then use these query senses tocluster the Web snippets returned by a traditional search engine.Very little work on this topic exists: Vector-based WSI was successfully shown toimprove bag-of-words ad hoc Information Retrieval (Schu?tze and Pedersen 1995) andpreliminary studies (Udani et al2005; Chen, Za?
?ane, and Goebel 2008) have providedinteresting insights into the use of WSI for Web search result clustering.
A more recentattempt at automatically identifying query meanings is based on the use of hiddentopics (Nguyen et al2009).
In this approach, however, topics (estimated from a uni-versal data set) are query-independent and thus their number needs to be establishedbeforehand.
In contrast, we aim to cluster snippets on the basis of a dynamic and finer-grained notion of sense.An exploratory study on ten query words has shown that the majority of relevantuses of query words can be identified using graph-based WSI (Ve?ronis 2004).
In thepresent work we take this preliminary finding to the next level, by studying the impactof several graph-based WSI algorithms on a large scale and by integrating them into aWeb search result clustering framework.
As a result, we are able not only to performan end-to-end evaluation of WSI approaches, but also to compare them with traditionalsearch result clustering techniques, which instead lack explicit semantics for the querymeanings.2.6 Aspect IdentificationOver recent years a line of research has been developed in the field of InformationRetrieval that makes use of query logs and clickthrough information to identify andmodel the aspects of a given query in terms of the user intents for that query.
Aspectscan be identified by exploiting those queries in the past that enabled the user to retrievedocuments that are close to the current input query (Wang and Zhai 2007).
A differentapproach aims, instead, at extracting related queries from query logs as candidateaspects and discarding duplicate and redundant aspects using search results.
WikipediaInfoBoxes are used to cluster candidate aspects into classes (Wu, Madhavan, and Halevy2011).
Latent aspects of queries can also be extracted from query reformulations withinhistorical search session logs (Wang, Chakrabarti, and Punera 2009).
More recently, atopic modeling approach based on query logs and click data has been proposed thataims at discovering generic aspects pervading manually fixed categories of namedentities (Xue and Yin 2011).
The implicit user-specific aspect of a query can be obtainedfrom short query log sessions of other users using a Markov logic learning model.
Thisresults in the documents that best model the user?s intentions when entering a query(Mihalkova and Mooney 2009).
Finally, a semi-supervised approach has recently beenapplied to create class labels that are later assigned to latent clusters of queries using aHierarchical Dirichlet Process (Reisinger and Pasca 2011).716Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIThis line of research has some points of contact with WSI, but also importantdifferences: Most important, aspect identification aims at discriminating betweenvery fine-grained facets of a given query, such as those of rental, pricing,and accidents of a car, in contrast to WSI whose goal is that of inducingdifferent meanings of the given query, such as car as a motor vehicle,railroad car, song, novel, or even primitive in the LISP programminglanguage.
In this respect, the two tasks are complementary, becauseonce WSI has discovered the different senses of a query, then one canapply aspect identification to detect subsenses of each meaning. Much work based on query logs and click data requires reliable statistics,which are not always available in all languages.
WSI relies instead onraw text corpora, which can easily be obtained for any language.
Thisdifference also holds for custom search engines not working on the Web,which might not have enough statistics from their users, but could insteadresort to raw (domain) corpora. Privacy and availability issues are often mentioned in connection withquery logs and clickthrough data, therefore making research on this topichard to replicate and evaluate objectively, especially in comparison withother systems.The framework presented in this article focuses on the ambiguity of queries at themeaning level, leaving the further application of aspect identification techniques tofuture work, in the hope that the previously mentioned issues of privacy and availabilitywill somehow be mitigated.3.
Semantically Enhanced Search Result ClusteringWeb search result clustering is usually performed in three main steps:1.
Given a query q, a search engine is used to retrieve a list of resultsR = (r1, .
.
.
, rn).2.
A clustering C = (C1, .
.
.
, Cm) of the results in R is obtained by meansof a clustering algorithm.3.
The clusters in C are optionally labeled with an appropriate algorithm(e.g., Zamir and Etzioni 1998; Carmel, Roitman, and Zwerdling 2009)for visualization purposes.First, we preprocess the set R of search results returned by the search engine(Section 3.1).
Next, to inject semantics into search result clustering, we proposeimproving Step 2 by means of a WSI algorithm: Given a query q, we first dynamicallyinduce, from a text corpus, the set of word senses of q (Section 3.2); next, we cluster theWeb results on the basis of the word senses previously induced (Section 3.3).
We showour framework in Figure 1.3.1 Preprocessing of Web Search ResultsAs a result of submitting our query q to a search engine, we obtain a list of relevantsearch results R = (r1, .
.
.
, rn).
In order to make this list usable by a clustering algorithm,717Computational Linguistics Volume 39, Number 3Figure 1The workflow of semantically enhanced Web search result clustering.each result ri is processed by means of four steps aimed at transforming it into a bag ofwords bi:1.
We obtain the snippet si corresponding to the result ri.2.
We apply tokenization to si, thus splitting the string into tokens andsetting them to lowercase.3.
We augment the current token set with multi-word expressions obtainedby compounding subsequent word tokens up to ?
words (a parameterwhose tuning is described later in Section 4.1.4).
The terms in the resultingtoken set are lemmatized using WordNet as reference lexicon.
We removetokens that are not in the WordNet lexicon (e.g., the, esa).4.
We remove the stopwords (e.g., get, on, be, as) and the target query words(e.g., snow, leopard, snow leopard) from the token set.An example of the application of the four steps to a snippet returned for the query snowleopard is shown in Table 3.
As a result of this process, we obtain a list of bags of wordsB = (b1, .
.
.
, bn), where bi is the bag of words of the search result ri.3.2 Graph-Based Word Sense InductionThe next step is to dynamically discover the senses of the input query q and provide arepresentation for them that will later be used for semantically clustering the snippetspreprocessed in the previous step.
WSI algorithms are unsupervised techniques aimedat automatically identifying the set of senses denoted by a word.
These methods induceword senses from text by clustering word occurrences on the basis of the idea that aTable 3Processing steps for one of the search results of the query snow leopard.step outputinitial snippet ?Get the facts on snow leopards.
Endangered Species Act (ESA): thesnow leopard is listed as endangered?tokenization { get, the, facts, on, snow, leopards, endangered, species, act, esa,leopard, is, listed, as }compounding andlemmatization{ get, fact, on, snow, leopard, snow leopard, endangered, species,endangered species, act, be, listed, as }stopword and querywords removal{ fact, endangered, species, endangered species, act, listed }718Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIgiven word?used in a specific sense?tends to co-occur with the same neighboringwords (Harris 1954).
Several approaches to WSI have been proposed in the literature(see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors(e.g., Schu?tze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks(Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011),and co-occurrence graphs (e.g., Widdows and Dorow 2002).In our work, we chose to focus on approaches based on co-occurrence graphs fortwo reasons:i) They have been shown to achieve state-of-the-art performance in standardevaluation tasks (Agirre et al2006b; Agirre and Soroa 2007; Korkontzelosand Manandhar 2010).ii) Other approaches are either based on syntactic dependency statistics (Lin1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on alarge scale for many domains and languages, or based on large matrixcomputation methods such as context-group discrimination (Schu?tze1998), non-negative matrix factorization (Van de Cruys and Apidianaki2011) and Clustering by Committee (Lin and Pantel 2002).
Instead, in ourapproach we aim to exploit the relational structure of word co-occurrenceswith lower requirements (i.e., using just a stopword list, a lemmatizer,and a compounder, cf.
Section 3.1), assuming that the semantics of a wordare represented by means of a co-occurrence graph whose vertices areco-occurrences and whose edges are co-occurrence relations.We therefore integrated the following algorithms into our framework: Curvature clustering (Dorow et al2005), an algorithm based on theparticipation ratio of words in graph triangles, that is, complete graphswith three vertices. Squares, Triangles, and Diamonds (SquaT++), an algorithm thatintegrates two graph patterns previously exploited in the literature(Navigli and Crisafulli 2010), namely, squares and triangles, with a novelpattern called diamond. Balanced Maximum Spanning Tree Clustering (B-MST), an extension ofa WSI algorithm based on the calculation of a Maximum Spanning Tree(Di Marco and Navigli 2011) that aims at balancing the number ofco-occurrences in each sense cluster. HyperLex (Ve?ronis 2004), an algorithm based on the identification of hubs(representing basic meanings) in co-occurrence graphs. Chinese Whispers (Biemann 2006), a randomized algorithm thatpartitions the graph vertices by iteratively transferring the mainstreammessage (i.e., word sense) to neighboring vertices.All of these graph algorithms for WSI consist of a common step, namely, co-occurrence graph construction (described in Section 3.2.1) and a second step, namely,the discovery of word senses, whose implementation depends on the specific algorithmadopted.
We discuss the second phase of each algorithm separately (Section 3.2.2).719Computational Linguistics Volume 39, Number 3Table 4Example co-occurrences of word w = lion.word w?
c(w?)
c(w, w?)
Dice(w, w?
)animal 213,414 5,109 0.2534videogame 201,342 4,945 0.2042mac 194,056 4,940 0.1568africa 189,011 4,521 0.1961feline 167,487 4,548 0.1472cat 161,980 4,493 0.1214savannah 159,693 3,535 0.1091predator 145,239 3,643 0.1065apple 140,670 3,261 0.1043tiger 134,702 2,147 0.1024technology 129,483 2,017 0.0097software 113,045 1,846 0.0084iPod 112,100 1,803 0.0070simulation 93,899 1,367 0.00313.2.1 Step 1: Graph Construction.
Given a target query q, we build a co-occurrence graphGq = (V, E) such that V is the set of words8 co-occurring with q, and E is the set ofundirected edges, each denoting a co-occurrence between pairs of words in V. Weharvest the statistics for co-occurring words V from a text corpus (we used two differentcorpora, see Section 4.1.2), which was previously tokenized and lemmatized.First, for each word w we calculate the total number c(w) of its occurrences andthe number of times c(w, w?)
that w occurs together with some word w?
in the samecontext (to this end, we use the lemmas corresponding to inflected forms in the text).For instance, in Table 4, assuming w = lion, we show the absolute count c(w?)
of somewords (second column) together with the joint co-occurrence count c(w, w?)
of words w?occurring with w = lion in the same context (third column).
Note that the co-occurrencesw?
may refer to different senses of word w?for example, africa and savannah refer tothe animal sense of lion, whereas technology and software to the operating system sense.Moreover, w?
may be ambiguous itself in the context of w (e.g., tiger as either an animalor an operating system).Second, we calculate the Dice coefficient to determine the strength of co-occurrencebetween any two words w and w?
:9Dice(w, w?)
=2c(w, w?
)c(w) + c(w?).
(1)Table 4 reports the Dice coefficients in the fourth column for the example words.The rationale behind the use of the Dice coefficient, as opposed to, for example, asimple co-occurrence count such as c(w, w?
), is that dividing by the average of the total8 Because our application (i.e., Web search result clustering) typically deals with nominal senses,and to avoid overly large graphs, we restrict our vocabulary to nouns only.9 We note that the Dice coefficient can have a probabilistic interpretation in terms of the conditionalprobabilities P(w|w? )
and P(w?|w) or, alternatively, the joint probability P(w, w? )
and the marginalprobabilities P(w) and P(w? )
(Smadja, McKeown, and Hatzivassiloglou 1996).720Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIcounts of the two words drastically decreases the ranking of words that tend to co-occurfrequently with many other words (home, page, etc.
).Finally, we use the occurrence and co-occurrence counts just collected to constructthe co-occurrence graph Gq = (V, E) for the input query q.
The pseudocode of ourgraph construction procedure is shown in Algorithm 1 and consists of the followingsteps:a. Initialization with snippet words (lines 1?2): Initially we set V tocontain all the content words from the bags of words obtained fromthe snippet results of query q, that is, V :=?nj=1 bj, where bj is the bagof words corresponding to the search result rj ?
R as obtained after thepreprocessing step (see Section 3.1).
We also set E := ?, that is, theedge set is initially empty.b.
Adding first-order co-occurrences (lines 3?5): We augment V with thehighest-ranking words co-occurring with query q in the selected textcorpus, that is, those words w for which the following equations aresatisfied:??
?c(q, w)c(q)?
?Dice(q, w) ?
??
(2)where ?
and ??
are experimentally tuned thresholds (cf.
Section 4.1.4).c.
Adding second-order co-occurrences (lines 6?11): Optionally, we create anauxiliary copy V(0) of V. For each word w ?
V(0) we augment V with thosewords w?
which are strongly related to w in the text corpus.
In other wordswe add w?
to V if both Equations (2) are satisfied for the pair of words wand w?.Algorithm 1 The graph construction algorithm.Input: query q, the bag of words (b1, ?
?
?
, bn) for qOutput: a graph Gq = (V, E)1: V :=?nj=1 bj2: E := ?3: for each word w in the corpus4: if c(q, w)/c(q) ?
?
and Dice(q, w) ?
??
then5: V := V ?
{w}6: if second order = true then7: V(0) := V8: for each word w ?
V(0)9: for each word w?
in the corpus10: if c(w, w?
)/c(w) ?
?
and Dice(w, w?)
?
??
then11: V := V ?
{w?
}12: for each (w, w?)
?
V ?
V s. t. w = w?13: if Dice(w, w?)
?
?
then14: E := E ?
{{w, w?
}}15: remove all disconnected vertices from V16: return Gq = (V, E)721Computational Linguistics Volume 39, Number 3d.
Creating the co-occurrence graph (lines 12?15): For each pair of words(w, w?)
?
V ?
V, we add the corresponding edge {w, w?}
to E with weightDice(w, w?)
if the following condition is satisfied:Dice(w, w?)
?
?
(3)where ?
is a confidence threshold for the co-occurrence relation.
Notethat we use a threshold ??
to select which vertices to add to the graph Gq(Step [b]) and we use a potentially different threshold ?
for the selection ofwhich edges to add to Gq.
Finally, we remove from V all the disconnectedvertices (i.e., those with degree 0).As a result of this algorithm a co-occurrence graph Gq for the query q is pro-duced.
Consider again the target word lion and let us assume that the words inTable 4 are the only co-occurrences of lion.
In Figure 2 we show the execution of thefour steps of our graph construction algorithm for the input query lion, assuming(a) videogame macfelineanimal(b) videogamesoftwaremactigerappleanimalfelinesavannahafricapredator(c) videogamesimulationsoftwaretechnologyjavamactigeriPodappleanimalfelinecatsavannahafricapredator malawi(d) videogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator0.0150.0090.040.0050.0030.00150.0020.0010.0270.050.0070.00120.0450.060.0380.0170.0590.00740.00340.00230.00130.042Figure 2Graph construction for the lion example: (a) initializing V with the snippets words (lines 1?2);(b) adding first-order co-occurrences to V (lines 3?5); (c) adding second-order co-occurrencesto V (lines 6?11); (d) adding the edges corresponding to strong relations and removingdisconnected vertices (lines 12?15).722Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI?
= 0.38, ??
= ?
= 0.003, and c(lion) = 350, 727.
First, we initialize the graph with thewords in the snippets returned for lion (Figure 2a), next we add the words co-occurring with the query (Figure 2b), then second-order co-occurrences, that is, wordsco-occurring with those just added to the graph (Figure 2c), and finally we add thoseedges between word pairs whose Dice value is above a threshold (Figure 2d).3.2.2 Step 2: Sense Discovery.
All the graph-based WSI algorithms that we implementedin our framework are designed to discover the senses of an input term, which inour specific application is the query q.
This process of meaning discovery is carriedout through the use of the relational and structural information contained in the co-occurrence graph we have just created.
In fact, a co-occurrence graph Gq = (V, E) fora query q contains: (i) vertices w ?
V corresponding to words highly related to q, and(ii) edges e ?
E representing co-occurrence relations between vertices (i.e., words) in V.The key idea behind graph-based WSI is to obtain a partition S = (S1, .
.
.
, Sm) of Gqsuch that each component Si = (Vi, Ei) contains structurally (i.e., semantically) relatedvertices (i.e., words).
In other words, each vertex set Vi is intended to contain only wordsrelated to a specific sense of q.
As a result S is the sense inventory for the query q andeach Si is a sense cluster.We now introduce each graph-based WSI algorithm in detail.Curvature.
The curvature algorithm aims at quantifying how strongly the neighbors ofa vertex are related to each other.
To measure this degree of correlation, the curvaturecoefficient for a vertex w is calculated as follows:curv(w) =# triangles w participates in# triangles w could participate in(4)where a triangle is a cycle of length 3.
The numerator of Equation (4) is trivially calcu-lated as the number of links between neighbors of w, and the denominator is calculatedby counting all the possible pairs of neighbors.
According to Equation (4), the curvaturecoefficient can assume values between 0 and 1.
A vertex whose neighbors are highlyconnected (i.e., with a high value of curvature) is assumed to be part of a componentthat represents a specific meaning of the target query.
Conversely, a vertex with lowcurvature acts as a connection between different meanings.The curvature algorithm is designed to identify the meaning components by meansof the removal of all vertices whose curvature is below a certain threshold ?.
For ex-ample, we can attribute two different meanings to the word Napoleon, namely, a Frenchemperor and an American city.
By looking at the graph in Figure 3 we can easily findNapoleonFrancerevolutionOhioAmericaFigure 3Example of curvature for Napoleon.723Computational Linguistics Volume 39, Number 3that Napoleon participates in two triangles (represented by continuous lines) and it po-tentially could also participate in four additional triangles (i.e., those including dashedlines).
It follows that curv(Napoleon) = 26 = 0.33.
The deletion of the vertex Napoleonresults in two components (respectively, containing the vertices { France, revolution }and { Ohio, America }) representing the two mentioned meanings.SquaT++.
The curvature clustering algorithm is based on the hunch that local connec-tivity is correlated with meaning consistency.
We take this idea to the next level byproposing a more elaborate local connectivity approach that exploits three differentgraph patterns, namely: triangles (i.e., cycles of length 3, like in curvature clustering),squares (i.e., cycles of length 4) and diamonds (i.e., graphs with 4 vertices and 5 edges,forming a square with a diagonal), hence the name SquaT++ (Squares, Triangles, and?more?).
We determine the strength of the three patterns for a vertex w in the co-occurrence graph as follows:Tri(w) =# triangles w participates in# triangles w could participate in(5)Sqr(w) =# squares w participates in# squares w could participate in(6)Dia(w) =# diamonds w participates in# diamonds w could participate in(7)where w is a vertex.
Then we linearly combine the three measures as follows:SquaT++(w) = ?
?
Tri(w) + ?
?
Sqr(w) + ?
?
Dia(w) (8)where ?+ ?+ ?
= 1.
Similarly to the curvature algorithm, the sense clusters are ob-tained by removing all those vertices whose SquaT++ value is below a threshold ?.
InFigure 4(a) we show in bold the vertices selected for removal, and in Figure 4(b) thesense clusters obtained after removal, namely: { videogame, simulation, software }, { iPod,apple, mac }, and { cat, animal, predator, africa, savannah }.SquaT++ is a generalization of the curvature algorithm in that: (i) it uses the trianglepattern to calculate curvature, and (ii) it disconnects the graph using the same algorithmas curvature.
SquaT++ is a novel algorithm, however, that extends the previouslyproposed SquaT (Navigli and Crisafulli 2010), based on triangles and squares, by in-troducing a new pattern, namely, the diamond, whose clustering coefficient is linearlycombined with the other two.
Moreover, in our experiments we tested two versions ofSquaT++: the traditional one in which the coefficient is calculated on vertices (like inEquation (8)), and a variant calculated on edges.
Our hunch here is that removing low-ranking edges rather than vertices might produce more informative clusters, becauseno word is removed from the original graph.
In what follows, we refer to the vertexversion as SquaT++V and to the variant on edges as SquaT++E, and we refer to thegeneral algorithm as SquaT++.B-MST.
A more global approach to the identification of sense components is theBalanced Maximum Spanning Tree (B-MST), which is based on the computation of724Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI(a) videogamesimulationsoftwaretechnologymactigeriPodapplefelinecatanimal savannahafricapredator(b) videogamesimulationsoftwaremaciPodappleanimalcatsavannahafricapredator(c) videogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator(d) videogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator(e)lionvideogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator(f) videogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator(g) videogamesimulationsoftwaretechnologymactigeriPodappleanimalfelinecatsavannahafricapredator0.0150.0090.040.0050.0030.00150.0020.0010.0270.050.0070.00120.0450.060.0380.0170.0590.00740.00340.00230.00130.042??
?0.0150.0090.040.0050.0030.00150.0020.0010.0270.050.0070.00120.0450.060.0380.0170.0590.00740.00340.00230.00130.042Figure 4The lion example: (a) SquaT++ selection and (b) removal of edges below the threshold; (c) B-MSTspanning tree calculation and (d) edge removal; (e) HyperLex hub selection and (f) identificationof word senses; (g) Chinese Whispers cluster creation.725Computational Linguistics Volume 39, Number 3the Maximum Spanning Tree (MST) of the co-occurrence graph.
Cluster meanings areidentified by iteratively removing the edges which represent structurally weak rela-tions, i.e., those with lower weight in the MST.
The procedure is as follows: Eliminate from Gq all vertices whose degree is 1. Calculate the maximum spanning tree TGq of the graph Gq (e.g., the boldedges in Figure 4(c) represent the maximum spanning tree for our initialgraph). The original MST algorithm for WSI, proposed by Di Marco and Navigli(2011), iteratively eliminates the minimum-weight edge e ?
TGq whosedegree ?
2, until N connected components (i.e., word clusters) areobtained or there are no more edges to eliminate.
The problem with thisapproach is that it can generate unbalanced clusters (i.e., a few very largeclusters and several small clusters); for this reason we developed theB-MST variant which calculates an appropriate cluster mean cardinality10and removes an edge e ?
TGq if its elimination does not lead to connectedcomponents with cardinality less than half of the calculated mean value.This additional constraint prevents the creation of very small clusters,while at the same time avoiding artificial equal-size clusters.Following our lion example, and assuming that the value of the only parameter ofB-MST (i.e., the maximum number N of meanings to be identified) is set to 3, we obtainthe clusters in Figure 4(d).HyperLex.
Another option for sense discovery is that of HyperLex, which identifies themost interconnected vertices in the graph Gq, called hubs.
Each hub acts as the ?root?of a specific component of Gq and, correspondingly, a meaning of the target query q.First, a list L of the vertices w?
of the graph Gq is created and sorted by their absolutecount c(w?)
in decreasing order.
Each vertex w?
?
L is then selected as hub if it satisfiesthe following conditions:?????????degree(w?)maxw??
?V degree(w??)?
??{w?,w??
}?E Dice(w?, w??)degree(w?)?
??
(9)that is, the normalized degree of vertex w?
and the average weight of the edges in-cident on w?
must be, respectively, above the thresholds ?
and ??.
Once it has beenselected, the hub and all its neighbors are removed from L so as to avoid neighboringvertices from also being selected as hubs.
The hub selection process stops when thenext vertex in the sorted list does not satisfy either of the Equations (9) or if the list Lis empty.As an example, consider the co-occurrence graph in Figure 2(d).
A list of the verticesin the graph is created, sorted by c(w?
), as shown in Table 4.
For the purpose of our10 We calculate the mean cardinality of a cluster by dividing the total number of vertices in the graph by themaximum number N of clusters that we want to obtain.726Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIexample, let us assume ?
= 0.5 and ??
= 0.015.
The first hub to be selected is animal.All its neighbors (tiger, feline, cat, predator, africa, and savannah in our example) are alsoremoved from the list.
The next hub is videogame (its neighbors simulation, software, andtechnology are also removed from the list).
The last hub is mac; after the removal of itsneighbor from the list (apple) the last vertex to be examined is iPod, which cannot beselected as hub because it does not satisfy the second condition of Equation (9).
Theselected hubs are shown as rectangles in Figure 4(e).Once the hub selection process is complete, the target query q is added to the setof vertices V of graph Gq and each hub is connected to q with an infinite-weight edge(see vertex lion and its edges added to the graph in Figure 4(e)).
Then, a maximumspanning tree Tq of the graph is calculated starting from vertex q (see the bold edgesin Figure 4(e)).
As a result, Tq will include all the infinite-weight edges from q to itsdirect descendants, namely, the hubs.
Vertex q is then removed from the graph so thateach subtree rooted at a hub in Tq represents a word sense for the target query q (seeFigure 4(f)).
In our example, three clusters are produced: { videogame, simulation, software,technology }, { mac, apple, iPod }, and { animal, feline, tiger, cat, predator, africa, savannah }.Note that, in our example, HyperLex and SquaT++ found the same meanings for thequery word lion (namely, the animal, the operating system, and the videogame), butproduced different clusters (e.g., HyperLex assigns the word tiger to the animal clusterwhereas SquaT++ removes it from the graph).
Finally, notice that in HyperLex thenumber of senses is dynamically chosen on the basis of the co-occurrences of q andthe algorithm?s thresholds.An alternative approach to hub selection as performed in HyperLex consists ofusing the PageRank algorithm to sort the vertices of the co-occurrence graph and choosethe best ranking ones as hubs of the target word (Agirre et al2006b).
Given that theperformance of this variant is comparable to that of HyperLex, in this work we focus onthe original version of the induction algorithm.Chinese Whispers.
All the previously presented algorithms work in a top?down fashion,that is, they iteratively remove edges or vertices from an initial co-occurrence graphuntil a number of partitions are obtained.
The last algorithm we consider, called ChineseWhispers, works, instead, bottom?up.
The pseudocode, shown in Algorithm 2, consistsof the following two steps:1.
First, the algorithm assigns a distinct class i to each vertex vi and creates aclustering C containing the singleton clusters Ci (lines 1?4 of thealgorithm).2.
Second, a series of iterations is performed aimed at merging the clusters(lines 5?11).
Specifically, at each iteration the algorithm analyzes eachvertex v in random order and assigns it to the majority class among thoseassociated with its neighbors.
In other words, it assigns each vertex v to theclass c that maximizes the sum of the weights of the edges {u, v} incidenton v such that c is the class of u, according to the following formula:class(v) := argmaxc?
{u,v}?E(Gq )s.t.
class(u)=cDice(u, v) (10)727Computational Linguistics Volume 39, Number 3Algorithm 2 The Chinese Whispers algorithm.Input: a graph Gq = (V, E) to be clusteredOutput: a clustering C of the vertices in V1: for each vi ?
V2: class(vi) := i3: Ci := {vi}4: C := {Ci : i = 1, .
.
.
, |V|}5: repeat6: C?
:= C7: for each v ?
V, randomized order8: class(v) := argmaxc?
{u,v}?E(Gq )s.t.
class(u)=cDice(u, v)9: for each i do Ci := {v ?
V : class(v) = i}10: C := {Ci : Ci = ?
}11: until C = C?12: return CAs soon as an iteration produces no change in the clustering (line 11), the algorithmstops and outputs the final clustering (line 12).
In contrast to the previous algorithm,Chinese Whispers is parameter-free.
Figure 4(g) shows an output example for thisalgorithm on the lion co-occurrence graph.3.3 Clustering of Web Search ResultsWe are now ready to semantically cluster our Web search results R, which we previouslytransformed into bags of words B (cf.
Section 3.1).
To this end we use the automaticallydiscovered senses for our input query q (cf.
Section 3.2).
We adopt different measures,each of which calculates the similarity between a bag of words bi ?
B and the senseclusters {S1, .
.
.
, Sm} acquired as a result of Word Sense Induction.Given a result bi, the sense cluster closer to bi will be selected as the most likelymeaning of ri.
Formally:Sense(ri) =????
?argmaxj=1,...,msim(bi, Sj) if maxj=1,...,msim(bi, Sj) > 00 else(11)where sim(bi, Sj) is a generic similarity value between bi and Sj (0 denotes that no senseis assigned to result ri).
As a result of sense assignment for each ri ?
R, we obtain aclustering C = (C1, .
.
.
, Cm) such that:Cj = {ri ?
R : Sense(ri) = j} (12)that is, Cj contains the search results classified with the j-th sense of query q.We now present three different similarity measures between snippet bags of wordsand sense clusters (cf.
Equation (11)), which we implemented in our framework.728Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIWord Overlap.
It calculates the size of the intersection between the two word sets:simWO(bi, Sj) =|bi ?
Vj||bi|(13)where Sj = (Vj, Ej) as defined in Section 3.2.2.Degree Overlap.
It calculates the sum of the degrees in the co-occurrence graph compo-nent of Sj of the snippet?s words in bi:simDO(bi, Sj) =?w?bi?Vj degree(w, Sj)|bi| ?
|Ej|(14)where degree(w, Sj) is the number of edges incident on w in the Sj component of theco-occurrence graph.Token Overlap.
The third measure is similar in spirit to Word Overlap, but takes intoaccount each token occurrence in the snippet bag of words bi:simTO(bi, Sj) =?w?bi?Vj c(w, ri)?w?bi c(w, ri)(15)where c(w, ri) is the number of occurrences of the word w in the result ri.3.4 Cluster SortingAs a natural consequence of the different similarity values between snippet results anda given cluster, first, not all the snippets will have the same degree of relevance for thecluster, and second, the produced clusters will show a different ?quality?
depending onthe relevance of the search results therein.
We thus sort the clusters in our clusteringC using a similarity-based notion of cluster ?quality.?
For each cluster Cj ?
C, we de-termine its similarity with the corresponding meaning Sj by calculating the followingformula:avgsim(Cj, Sj) =?ri?Cj sim(bi, Sj)|Cj|(16)The formula determines the average similarity between the bags of words bi ofthe search results ri in cluster Cj and the corresponding sense cluster Sj.
The similarityfunction sim is the same as that stated in Equation (11) and defined in Section 3.3.Finally, we rank the elements ri within each cluster Cj by their similarity sim(bi, Sj).We note that the ranking and optimality of clusters can be improved with more sophis-ticated techniques (e.g., Crabtree, Gao, and Andreae 2005; Kurland 2008; Kurland andDomshlak 2008; Lee, Croft, and Allan 2008).
This is beyond the scope of this article,however.729Computational Linguistics Volume 39, Number 34.
In Vivo Experiments: Web Search Result ClusteringWe now present two extrinsic experiments aimed at determining the impact of WSIwhen integrated into Web search result clustering.
We first describe our experimentalset-up (Section 4.1).
Next, we present a first experiment focused on the quality of theoutput search result clusters (Section 4.2) and a second experiment on the degree ofdiversification of semantically enhanced versus non-semantic search result clusteringalgorithms (Section 4.3).4.1 Experimental Set-up4.1.1 Lexicon.
In all our experiments our lexicon was given by the entire WordNetvocabulary (Miller et al1990; Fellbaum 1998) augmented with the set of queries in ourtest data sets.4.1.2 Corpora.
To calculate the co-occurrence strength between words we need a largecorpus to extract co-occurrence counts and calculate the Dice values (cf.
Equation (1)).
Tothis end we performed separate experiments on two different corpora and constructedthe corresponding co-occurrence databases: Google Web1T (Brants and Franz 2006): This corpus is a largecollection of n-grams (n = 1, .
.
.
, 5)?namely, windows of n consecutivetokens?occurring in one terabyte of Web documents as collected byGoogle.
We consider all the co-occurrences for lemmas which appear inthe same n-gram (we applied the WordNet lemmatizer to obtain thecanonical form of any word sequence). ukWaC (Ferraresi et al2008): This corpus was constructed by crawlingthe .uk domain and obtaining a large sample of Web pages that wereautomatically part-of-speech tagged using the TreeTagger tool.
For thiscorpus we considered all the co-occurrences of WordNet lemmas thatappear in the same sentence.We selected these two corpora for their very different natures, namely: GoogleWeb1T is a very large corpus, but with very narrow contexts (5-grams) with a mini-mum occurrence frequency; ukWaC represents a smaller portion of the Web, but withlarger contexts.
This enabled us to observe the behavior of WSI algorithms when co-occurrences were extracted from different kinds of textual source.
In Table 5 we showexamples of the contexts available in the two corpora for the same word (i.e., lion) andthe content words that are found to co-occur with it (shown in italics in Table 5).Table 5Example of contexts for the word lion in the Web1T and ukWaC corpora (target word in bold,co-occurring words in italics).corpus context exampleWeb1T roar of the lion inukWaC Wilson?s Zoo and its sad lion had given way to the brave attempt to create an early?Safari Park?.730Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 6The pseudoword data set.pseudoword1 pizza*blog2 banana*plush3 kalashnikov*mollusk*sky4 hurricane*glue*modem5 pistol*stair*yacht*semantics6 potassium*razor*walrus*calendula7 monarchy*archery*google*locomotive*beach8 hyena*helium*soccer*ukulele*wife9 human*orchid*candela*colosseum*movie*guitar10 journey*harmonica*vine*mustache*rhino*police11 glossary*river*dad*kitchen*aikido*geranium*italy12 microbe*hug*ship*skull*beer*giraffe*mathematics4.1.3 Tuning Set.
Given that our graph construction step and our WSI algorithms haveparameters, we created a data set to perform tuning.
In order to fix the parameter valuesindependently of our application we created this data set by means of pseudowords(Schu?tze 1992; Yarowsky 1993).
A pseudoword is an ambiguous artificial word createdby concatenating two or more monosemous words.
Each monosemous word representsa meaning of the pseudoword.
For example, given the words pizza and blog we cancreate the pseudoword pizza*blog.
The list of pseudowords we used is reported inTable 6.The powerful property of pseudowords is that they enable the automatic construc-tion of sense-tagged corpora with virtually no effort.
In fact, we automatically createdour tuning data set as follows:1.
First, we collected the top 100 results retrieved by Yahoo!
for each meaning(i.e., monosemous word) of the pseudoword (e.g., pizza and blog forpizza*blog).2.
We created a set of 100 snippets for the ?pseudoword?
query (e.g.,pizza*blog) by selecting snippets from each meaning of the pseudowordin a number that was proportional to their total occurrence count.
Forinstance, if pizza and blog occur, respectively, 73,000 and 27,000 times in thereference corpus (e.g., ukWaC), we selected 73 snippets from pizza and 27from blog.
As a result we simulated the distribution of the two senses ofthe pseudoword within the retrieved snippets.3.
Finally, within each of the 100 snippets, we replaced each monosemousword occurrence (e.g., pizza and blog) with the pseudoword itself (i.e.,pizza*blog).
As a result we obtained a set of 100 snippets for eachambiguous pseudoword.4.1.4 Parameters.
We used our tuning set to select, first, the optimal values of the pa-rameters needed to perform graph construction, and, second, to choose the parametervalues specific to each graph-based WSI algorithm.
To find the best configurations weperformed tuning by combining the three evaluation measures of Adjusted Rand Index,Jaccard Index, and F1 (introduced in Section 4.2.1).731Computational Linguistics Volume 39, Number 3Table 7The optimal parameter values for graph creation obtained as a result of tuning.Web1T ukWaCparameter CurvatureSquaT++VSquaT++EB-MSTHyperLexChineseWhispersCurvatureSquaT++VSquaT++EB-MSTHyperLexChineseWhispersmax comp.
length (?)
2 2 2 2 2 2 2 2 2 2 2 2min co-occurr.
(?)
5E-2 5E-2 5E-2 5E-2 5E-2 2E-1 2E-1 5E-1 2E-1 2E-1 2E-1 2E-1min Dice (??)
1E-2 1E-2 1E-2 1E-2 5E-2 5E-2 1E-4 1E-4 1E-2 1E-2 1E-4 5E-2min edge weight (?)
9E-4 9E-4 9E-4 9E-4 9E-4 9E-4 6E-3 3E-3 3E-3 3E-3 7E-3 3E-3co-occurrence order 1 1 1 1 1 1 1 1 1 1 1 1Graph construction.
Because all our WSI algorithms draw on the co-occurrence graph,we first tuned the parameters for graph construction for each of the two corpora(cf.
Section 3.2.1), namely: the maximum length of the compounds extracted from thecorpus (?
), the minimum number of co-occurrences (?)
and minimum Dice value (??
)for vertex addition, and the minimum weight for a graph edge (?)
and vertex additionusing first versus second-order co-occurrences.
In Table 7 we show the values for theseparameters that optimize the performance of each WSI algorithm on the two corpora.11In all our runs we used the Word Overlap as a similarity measure for Web search resultclustering.We observed that the optimal values for many of the parameters used for graphconstruction were stable across algorithms, whereas they changed across corpora dueto the different scales of the two corpora.
Instead, the maximum compound length andthe co-occurrence order were fixed for all configurations.
For the former we observed noperformance increase with longer compound lengths.
For the latter we found negligibleimprovements with second-order co-occurrences, at the cost, however, of increasing thesize of the resulting graph exponentially.
Given the large number of experiments thatwould be involved, we decided to avoid this additional workload and use first-orderco-occurrences in all our experiments.WSI algorithms.
Next, for each graph-based WSI algorithm, we kept the given optimalvalues fixed for building the co-occurrence graphs for the tuning set queries, whilevarying the parameter values of the WSI algorithm, using Word Overlap as similar-ity measure for Web search result clustering.
In Table 8 we show the optimal valuesfor each algorithm when using Web1T (third column) and ukWaC (fourth column)to build the co-occurrence graph.
Chinese Whispers is not shown as it is parameter-free (cf.
Section 3.2.2).
For SquaT++, together with the ?
threshold, we also tuned thethree coefficient values ?, ?, and ?, that is, we needed to find the best values for thecoefficients in Equation (8).
The optimal coefficient combinations are shown in Table 9for SquaT++ on vertices and edges, when using the two corpora for graph construction.The values indicate that all the three graph patterns provide a positive contribution tothe algorithm?s performance, with the same coefficients for SquaT++ on vertices and11 To this end we used empirically chosen parameters for each WSI algorithm, while delaying the optimalchoice of these parameters to the next paragraph.732Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 8The WSI algorithms?
parameters.Web1T ukWaCCurvature removal threshold (?)
0.25 0.35SquaT++ vertex removal threshold (?)
0.07 0.2edge removal threshold (?)
0.2 0.25B-MST number of clusters (N) 4 4HyperLex min hub degree (?)
0.05 0.06min edge weight (??)
0.004 0.01Table 9Optimal values for the three graph patterns used in SquaT++.Web1T ukWaC?
?
?
?
?
?SquaT++V 0.34 0.16 0.50 0.34 0.50 0.16SquaT++E 0.34 0.16 0.50 0.34 0.50 0.16edges.
Interestingly, we observe that, whereas the contribution of triangles (weightedby ?)
is the same across corpora, the respective weights of squares (?)
and diamonds(?)
are flipped.
After inspection we found that the graphs obtained with Web1T are lessinterconnected than those produced with ukWac.
Consequently, diamonds are sparserbut more reliable in the Web1T setting, whereas they are much more frequent, and thusnoisier, in the ukWaC setting.4.1.5 Test Sets.
We conducted our in vivo experiments on two test sets of ambiguousqueries: AMBIENT (AMBIguous ENTries), a data set that contains 44 ambiguousqueries.12 The sense inventory for the meanings (i.e., subtopics)13 ofqueries is given by Wikipedia disambiguation pages.
For instance, giventhe beagle query, its disambiguation page in Wikipedia provides themeanings of dog, Mars lander, computer search service, beer brand, and soforth.
The top 100 Web results of each query returned by the Yahoo!
searchengine were tagged with the most appropriate query senses according toWikipedia (amounting to 4,400 sense-annotated search results).
To ourknowledge, this is currently the largest data set of ambiguous queriesavailable on-line.
In fact, other existing data sets, such as those from theTREC Interactive Tracks, are not focused on distinguishing the subtopicsof a query.12 http://credo.fub.it/ambient.13 In the following we use the terms subtopic and word sense interchangeably.
As stated in the Introduction,this work focuses on query disambiguation along the lines of Word Sense Induction and Disambiguation(Navigli 2009), rather than aspect identification, which concerns subtle distinctions within the samemeaning of a query.733Computational Linguistics Volume 39, Number 3Table 10Statistics on the AMBIENT and MORESQUE data sets.data set queries queries by length average1 2 3 4 subtopicsAMBIENT 44 35 6 3 0 17.9MORESQUE 114 0 47 36 31 6.6 MORESQUE (MORE Sense-tagged QUEry results), a data set that wedeveloped as an integration of AMBIENT following guidelines providedby its authors.14 In fact, our aim was to study the behavior of Web searchalgorithms on queries of different lengths, ranging from one to four words.The AMBIENT data set, however, is composed in the main of one-wordqueries.
MORESQUE provides dozens of queries of length 2, 3, and 4,together with the top 100 results from Yahoo!
for each query annotatedprecisely as was done in the AMBIENT data set.
We decided not todiscontinue the use of Yahoo!
mainly for homogeneity reasons.Wikipedia has already been used as a sense inventory by, among others, Bunescuand Pasca (2006), Mihalcea (2007), and Gabrilovich and Markovitch (2009).
Santamar?
?a,Gonzalo, and Artiles (2010) have investigated in depth the benefit of using Wikipediaas the sense inventory for diversifying search results, showing that Wikipedia offersmuch more sense coverage for search results than other resources such as WordNet.We report the statistics on the composition of the two data sets in Table 10.
Giventhat the snippets could possibly be annotated with more than one Wikipedia subtopic,we also determined the average number of subtopics per snippet.
This amounted to1.01 for AMBIENT and 1.04 for MORESQUE for snippets with at least one subtopicannotation.
We can thus conclude that multiple subtopic annotations are infrequent.Finally, we analyzed how the different subtopics are distributed over the snippet resultsfor each query.
To do this we calculated the standard deviation of the subtopic popula-tion for each individual query, which we show in Figure 5.
We observed a considerabledifference in the standard deviations of shorter and longer queries (e.g., between thosefrom the AMBIENT data set [from 1 to 44 in the figure] and the MORESQUE data set[from 45 to 158]).
We further calculated the average standard deviation over the twodata sets?
queries, obtaining 6.5 for AMBIENT and 13.1 for MORESQUE.
Therefore weanticipate that the longer the query length, the more unbalanced will be the distributionof its subtopics over the top-ranking results.In line with previous experiments on search result clustering, our data set doesnot contain monosemous queries for two reasons: (i) we are interested in queries withmultiple meanings, and (ii) monosemous queries would increase the performance ofour experiments because no diversification would be needed for them.4.1.6 Systems.
We performed a comparison of our semantically enhanced search resultclustering systems with nonsemantic ones.14 http://lcl.uniroma1.it/moresque.734Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI010203040500  20  40  60  80  100  120  140  160stddevquery IDFigure 5Standard deviations for the subtopic population of the AMBIENT queries (1?44) and theMORESQUE queries (45?158).Semantically enhanced systems.
We integrated our graph-based WSI algorithms (Curva-ture, SquaT++, B-MST, HyperLex, and Chinese Whispers; cf.
Section 3.2) into our searchresult clustering framework.
We tested each algorithm when combined with any of thesnippet-to-sense similarity measures introduced in Section 3.3.Nonsemantic systems.
We compared our semantically enhanced systems with four Webclustering engines, namely: Lingo (Osinski and Weiss 2005): A Web clustering engine implementedin the Carrot open-source framework15 that clusters the most frequentphrases extracted using suffix arrays. Suffix Tree Clustering (STC) (Zamir and Etzioni 1998): The original Websearch clustering approach based on suffix trees. KeySRC (Bernardini, Carpineto, and D?Amico 2009): A state-of-the-artWeb clustering engine built on top of STC with part-of-speech pruningand dynamic selection of the cut-off level of the clustering dendrogram. Yippy16 (formerly Clusty): A state-of-the-art metasearch engine developedby Viv?
?simo aimed at clustering search results into meaningful topics.For Lingo and STC we used the Carrot implementation which we integrated intoour framework.
Conversely, for Yippy we used the on-line output provided by the Websearch engine.15 http://project.carrot2.org.16 http://search.yippy.com.735Computational Linguistics Volume 39, Number 34.1.7 Baselines.
We compared the four systems against three baselines: Singletons: Each snippet is clustered as a separate singleton (i.e., thecardinality of the resulting clustering C is |C| = |R|). All-in-one: All the snippets are clustered into a single cluster (i.e., |C| = 1). Wikipedia clustering: Given an input query q, we apply Equation (13) tomatch the bag of content words of each snippet against that of eachWikipedia page representing a meaning of q (we use the disambiguationpage of q as its sense inventory).
The snippet is then added to the clustercorresponding to the best-matching Wikipedia page.
Given q, we obtain aclustering whose size is determined by the number of meanings in theWikipedia disambiguation page of q.The first two baselines help us determine whether the evaluation measures havea bias towards very small (singletons) or big clusters (all-in-one).
The third baseline,based on Wikipedia, is a tough one in that?in contrast to our systems?it relies on apredefined sense inventory (which is the same as that used in the manual classificationof the test set) to cluster the snippets.
Consequently the baseline does not ?induce?the senses, but just classifies (or labels) each snippet with the best-matching Wikipediasense of the input query.4.2 Experiment 1: Evaluation of the Clustering Quality4.2.1 Evaluation Measures.
In this first experiment our goal is to evaluate the qualityof the output produced by our search result clustering systems.
Unfortunately, theclustering evaluation problem is a notably hard issue, and one for which there exists nounequivocal solution.
Many evaluation measures have been proposed in the literature(Rand 1971; Zhao and Karypis 2004; Rosenberg and Hirschberg 2007; Geiss 2009, interalia) so, in order to get exhaustive results, we tested three different clustering qualitymeasures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which weintroduce hereafter.
Each of these measures M(C,G) calculates the quality of a clusteringC, output for a given query q, against the gold standard clustering G for that query.
Wethen determine the overall results on the entire set of queries Q in the test set accordingto the measure M by averaging the values of M(C,G) obtained for each single testquery q ?
Q.Adjusted Rand Index.
Given a gold standard clustering G, the Rand Index (RI; Rand 1971)of a clustering C is a measure of clustering agreement commonly used in the literature,calculated as follows:RI(C,G) = TP + TNTP + FP + FN + TN(17)where TP is the number of true positives (i.e., snippet pairs) that are in the same clusterboth in C and G, TN is the number of true negatives (i.e., pairs which are in differentclusters in both clusterings), and FP and FN are, respectively, the number of falsepositives and false negatives.
For the gold standard G we use the clustering inducedby the sense annotations provided in our data sets for each snippet (i.e., each clustercontains the snippets manually associated with a particular Wikipedia page, that is,subtopic, of the query).736Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIRand Index determines the percentage of snippet pairs that are in the same con-figuration in both C and G, but its main weakness is that it does not take chance intoaccount.
In fact, the expected value of the RI of two random clusterings is not a constantvalue (e.g., 0).
This issue is addressed by the Adjusted Rand Index (ARI; Hubert andArabie 1985), which corrects the RI for chance agreement and makes it vary accordingto expectation:ARI(C,G) = RI(C,G) ?
E(RI(C,G))max RI(C,G) ?
E(RI(C,G)) (18)where E(RI(C,G)) is the expected value of the RI.
Given two clusterings C = (C1, .
.
.
, Cm)and G = (G1, G2, .
.
.
, Gg), we first quantify the degree of overlap between C and G usingthe contingency table reported in Table 11, where nij denotes the number of objects incommon between Gi and Cj (i.e., nij = |Gi ?
Cj|) and ai and bj represent, respectively,the number of objects in Gi and Cj.
Now, Equation (18) can be reformulated as follows(Steinley 2004):ARI(C,G) =?ij(nij2)?
[?i(ai2)?j(bj2)]/(N2)12 [?i(ai2)+?j(bj2)] ?
[?i(ai2)?j(bj2)]/(N2)(19)Differently from the original RI (which ranges between 0 and 1), the ARI rangesbetween ?1 and +1 and is 0 when the index equals its expected value.
Given the issueswith RI, in our experiments we focused on ARI.Jaccard Index.
The ARI compares a clustering C with a gold standard G both in termsof the snippets occurring in the same cluster (TP) and those which are assigned todifferent clusters (TN).
There are typically many TN in a clustering, however; thereforethis measure tends to overweight the usefulness of snippets placed in different clusters.The Jaccard Index (JI) is a measure that addresses this issue.
JI is calculated as follows:JI(C,G) = TPTP + FP + FN(20)In fact, in contrast to RI (cf.
Equation (17)), neither the numerator nor the denomi-nator of JI include the TN term.Table 11Contingency table for the clusterings G and C.GCC1 C2 ?
?
?
Cm SumsG1 n11 n12 ?
?
?
n1m a1G2 n21 n22 ?
?
?
n2m a2.......... .
.......Gg ng1 ng2 ?
?
?
ngm agSums b1 b2 ?
?
?
bm N737Computational Linguistics Volume 39, Number 3F1-Measure.
The ARI and the JI calculate the clustering quality using snippet pairs asthe basic unit.
Instead, a clustering C can be evaluated by focusing on the precision ofthe single clusters and the topics recalled by them, that is, we evaluate C according toits precision (P) and recall (R) against a gold standard G. Precision determines howaccurately the clusters of C represent the topics in the gold standard G, and recallmeasures how accurately the topics in G are covered by the clusters in C.The precision of a cluster Cj ?
C can be calculated as follows (Crabtree, Gao, andAndreae 2005):P(Cj) =|Ctj ||Cj|(21)where t is the majority topic in Cj for a given query,17 and Ctj is the set of snippets inCj which are tagged with subtopic t in the gold standard G. The recall of a topic t is,instead, calculated as:R(t) =|?Cj?Ct Ctj |nt (22)where Ct is the subset of clusters of C whose majority topic is t, and nt is the number ofsnippets tagged with subtopic t in the gold standard.
The total precision and recall ofthe clustering C are then calculated as:P =?Cj?C P(Cj)|Cj|?Cj?C |Cj|; R =?t?T R(t)nt?t?T nt(23)where T is the set of subtopics in the gold standard G for the given query.
The twovalues of P and R are then combined into their harmonic mean, namely, the F1 measure(van Rijsbergen 1979):F1(C,G) = 2PRP + R (24)Note that, in contrast with ARI, in calculating precision and recall we do notconsider untagged gold standard snippets.4.2.2 Results and Discussion.
We show the results of the WSI algorithms in Table 12.With few exceptions, the results obtained on the two corpora are comparable.
SquaT++,which extends Curvature with the Square and Diamond patterns, obtains higher perfor-mance.
Although integrating three different graph patterns is beneficial, the differencebetween using edges and vertices to do so is mostly marginal.The first important insight is that the best results, shown in bold in Table 12, areconsistent across corpora and similarity measures (i.e., WO, DO, and TO), thus showingthe robustness of the WSI algorithms when co-occurrences are extracted from differenttextual sources.
The pairwise evaluation measures (i.e., ARI and JI), however, rank the17 The majority topic for a cluster Cj ?
C is the topic t for which there exists the maximum number ofsnippets in Cj tagged with t.738Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 12Results of graph-based WSI algorithms with Web1T and ukWaC and various similarity measures(Word Overlap, Degree Overlap, and Token Overlap).
The reported measures are Adjusted RandIndex (ARI), Jaccard Index (JI), and F1 (percentages).
We also show the average number ofclusters per query produced by each algorithm.Algorithm Sim.
Web1T ukWaCARI JI F1 # cl.
ARI JI F1 # cl.CurvatureWO 67.03 74.10 58.34 2.3 64.86 72.74 58.84 3.5DO 66.88 73.76 58.67 2.3 64.02 71.04 59.85 3.5TO 67.14 74.04 58.36 2.3 65.03 72.46 58.73 3.5SquaT++VWO 69.65 75.69 59.19 2.1 69.27 75.55 59.18 2.3DO 69.21 75.45 59.19 2.1 68.73 75.14 59.75 2.3TO 69.67 75.69 59.19 2.1 69.33 75.55 59.23 2.3SquaT++EWO 69.88 75.82 59.39 2.7 69.84 75.74 59.70 3.9DO 69.63 75.74 60.99 2.7 69.86 75.35 63.00 3.9TO 69.88 75.83 59.40 2.7 69.86 75.70 59.78 3.9B-MSTWO 60.76 71.51 64.56 5.0 61.15 72.24 65.57 5.0DO 66.48 69.37 64.84 5.0 67.60 70.02 67.41 5.0TO 63.17 71.21 64.04 5.0 64.18 71.93 65.46 5.0HyperLexWO 60.86 72.05 65.41 13.0 56.59 72.00 70.69 17.0DO 66.27 68.00 71.91 13.0 65.92 67.31 76.88 17.0TO 62.82 70.87 65.08 13.0 61.64 70.61 70.42 17.0Chinese WhispersWO 67.75 75.37 60.25 12.5 68.77 75.45 59.66 6.5DO 65.95 69.49 70.33 12.5 67.86 72.34 66.16 6.5TO 67.57 74.69 60.50 12.5 68.97 75.28 59.79 6.5WSI algorithms differently from the F1 measure.
In fact, when we focus on pairwiseevaluation measures, SquaT++ outperforms all other systems on both corpora, withChinese Whispers ranking second.
B-MST and HyperLex obtain lower results.
When welook into the precision of the output clusters and the recall of the gold-standard topics(i.e., we calculate F1), however, we observe an inverse trend: HyperLex, B-MST and, toa lesser extent, Chinese Whispers achieve the best performance, whereas Curvature andSquaT++ obtain lower F1.
This is because, assuming comparable precision, producingmore clusters (as is done by HyperLex, B-MST, and Chinese Whispers) implies morechances to obtain higher recall, thus better diversifying among the topics of the retrievedsearch results.
More specifically, B-MST and, especially, HyperLex benefit from the useof ukWaC in terms of F1 performance, with HyperLex gaining around 5% when movingfrom Web1T to ukWaC.Finally, in most cases we observe negligible differences between the three differentsimilarity measures (i.e., WO, DO, TO, cf.
Section 3.3), with some exceptions concerningB-MST, HyperLex, and Chinese Whispers.We now report the best results for our WSI algorithms in Table 13, compared againstthose of nonsemantic systems (i.e., Lingo, STC, and KeySRC, cf.
Section 4.1.6) and ourthree baselines (i.e., all-in-one, singleton, and Wikipedia, cf.
Section 4.1.7).
For the WSIalgorithms we show the results when using the WO measure, because, first, DO usesgraph information and thus cannot be applied to nonsemantic systems, and, second, in739Computational Linguistics Volume 39, Number 3Table 13A comparison between different search result clustering approaches (percentages).
The bestresults for each of the three classes of algorithms is shown in bold.Algorithm Web1T ukWaCARI JI F1 #cl.
ARI JI F1 # cl.WSI-basedCurvature 67.03 74.10 58.34 2.3 64.86 72.74 58.84 3.5SquaT++V 69.65 75.69 59.19 2.1 69.27 75.55 59.18 2.3SquaT++E 69.88 75.82 59.39 2.7 69.84 75.74 59.70 3.9B-MST 60.76 71.51 64.56 5.0 61.15 72.24 65.57 5.0HyperLex 60.86 72.05 65.41 13.0 56.59 72.00 70.69 17.0Chinese Whispers 67.75 75.37 64.25 12.5 68.77 75.45 59.66 6.5SRC systemsLingo ?0.53 36.36 16.73 2.0 ?0.53 36.36 16.73 2.0STC ?7.90 38.23 14.96 2.0 ?7.90 38.23 14.96 2.0KeySRC 14.34 27.77 63.11 18.5 14.34 27.77 63.11 18.5BaselinesAll-in-one 0.00 47.12 42.40 1.0 0.00 47.12 42.40 1.0Singleton 0.00 0.00 68.17 100.0 0.00 0.00 68.17 100.0Wikipedia 13.83 56.02 14.33 5.7 13.83 56.02 14.33 5.7most cases (as remarked earlier) there are negligible differences between the two othersimilarity measures (i.e., WO and TO, see Table 12).Our first finding here is that WSI-based search result clustering outperforms allother approaches across all evaluation measures on the two corpora, except for KeySRCand the singleton baseline when using the F1 measure.
We note, however, that althoughKeySRC outperforms the WSI algorithms based on graph patterns in terms of F1, itattains very low ARI and JI results.
Even worse, the singleton baseline produces trivial,meaningless clusterings, as measured by ARI and JI.
The all-in-one baseline, instead,obtains non-zero JI (thanks to the true positives taken into account), but again zero ARI.Further, its F1 is lower than singleton, because of its lower recall.
The Wikipedia baselinefares well compared with the other baselines in terms of ARI and JI, but achieves lowerF1, again because of low recall.
Finally, KeySRC consistently outperforms the other SRCsystems in terms of ARI and F1.In order to perform a fair comparison of our systems with Yippy we used a modifiedversion of our test set that retains only the Yahoo!
results that were also returned byYippy.
The average number of results over all queries in the resulting data set is 24.4,with a minimum and maximum number of 3 and 56 results per query, respectively.We report the results on the reduced data set in Table 14.
Among the classical searchresult clustering systems, Yippy performs worse in terms of ARI and JI.
Instead, whenwe focus on the precision and recall of the output clusters, Yippy outperforms all othernonsemantic systems, while lagging behind all WSI algorithms (which use Web1T).
Onefinding here is that, even in the presence of a smaller number of snippets per query,semantic systems perform best, whereas other approaches, which rely (like KeySRC) onthe availability of a sufficient number of snippets, fall short.4.3 Experiment 2: Evaluation of the Clustering Diversity4.3.1 Evaluation Measure.
Most of today?s search engines return a flat list of search results.We thus performed a second experiment aimed at quantifying the impact of our Websearch result clustering systems on flat-list search engines.
In other words, our goal was740Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 14A comparison of different SRC approaches, including Yippy (on a reduced version of theAMBIENT+MORESQUE data set; WSI systems use Web1T).Algorithm ARI JI F1 # cl.WSI-basedCurvature 63.21 75.22 64.86 2.1SquaT++V 64.25 75.29 64.99 2.0SquaT++E 64.13 75.96 65.30 2.3B-MST 53.72 76.51 70.82 5.0HyperLex 55.93 76.63 72.04 7.9Chinese Whispers 55.41 74.83 70.52 8.4SRC systemsLingo ?1.58 35.65 17.00 2.0STC 7.91 38.34 15.04 2.0KeySRC ?0.01 31.80 32.90 3.3Yippy 3.80 14.81 64.52 14.9BaselinesAll-in-one 0.00 45.66 48.30 1.0Singleton 0.00 0.00 72.19 24.4?Wikipedia 10.00 52.05 15.60 5.8?
Corresponding to the average number of snippet results in the reduced data set.to determine how many different meanings of a query are covered in the top-rankingresults shown to the user.
One natural way of measuring such performance is givenby S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision atrecall r) (Zhai, Cohen, and Lafferty 2003).
S-recall@K counts the number of different sub-topics retrieved for q in the top K results returned:S-recall@K =| ?Ki=1 subtopics(ri)|m (25)where subtopics(ri) is the set of subtopics manually assigned to the search result ri andm is the number of subtopics for query q in the gold standard.
In order to cut out somenoise, we calculated the S-recall@K considering only the subtopics assigned to at leasttwo snippets.S-precision@r instead determines the ratio of different subtopics retrieved for q inthe first Kr documents, where Kr is the minimum number of top results for which thesystem achieves recall r. Formally:S-precision@r =| ?Kri=1 subtopics(ri)|Kr(26)So whereas S-recall@K aims at determining the performance of a system atretrieving the largest number of topics for the query q in the K top-ranking results,S-precision@r quantifies the ratio of distinct subtopics covered by the minimal set ofresults returned for which the system obtains a specific recall r. Note that unambiguousqueries would perform with S-precision@r = S-recall@K = 1 for all values of r and K.1818 Here again we focus on the polysemy of queries in the traditional (computational) linguistic sense.
SeeSection 2.6 for a discussion.741Computational Linguistics Volume 39, Number 3Table 15S-recall@K on all queries (percentages).
The results for WSI algorithms are reported for bothWeb1T and ukWaC.KSystem 3 4 5 6 7 8 9 10 15 20Web1TCurvature 48.2 53.5 57.1 60.5 64.6 67.4 69.4 72.6 81.5 86.2SquaT++V 47.1 52.0 55.5 59.3 61.9 65.6 68.4 70.4 79.4 86.2SquaT++E 47.6 51.9 56.2 59.6 62.6 64.5 67.0 69.0 78.5 84.4B-MST 49.1 55.8 59.4 62.5 65.6 67.8 70.0 72.3 80.0 85.5HyperLex 50.4 55.5 60.5 63.4 66.2 69.3 71.2 72.9 78.9 84.9Chinese Whispers 48.8 53.3 58.3 62.2 65.4 68.5 70.8 72.8 78.9 84.5ukWaCCurvature 47.2 51.8 56.8 59.5 62.5 65.4 67.0 68.4 76.3 83.4SquaT++V 47.1 51.9 56.7 59.4 62.9 65.6 68.1 70.3 78.8 84.4SquaT++E 47.9 51.2 55.1 58.6 61.5 64.8 67.8 69.6 78.9 84.9B-MST 49.9 55.3 61.0 63.9 66.9 70.7 73.7 75.6 83.3 87.5HyperLex 51.1 59.3 64.6 67.3 71.3 73.9 74.9 76.6 83.4 87.6Chinese Whispers 49.7 54.5 57.7 61.2 64.0 66.7 69.5 71.4 79.4 84.0KeySRC 39.5 46.1 48.7 51.4 54.3 57.1 59.6 61.7 68.2 72.5EP 36.1 41.4 44.6 50.8 53.5 55.0 57.1 59.2 67.9 73.3Yahoo!
42.3 47.6 51.4 54.6 57.3 59.4 61.0 63.4 69.1 73.3These two measures are only suitable, however, for systems returning ranked lists(such as Yahoo!
and Essential Pages).
In order to apply them to search result clusteringsystems, we flatten each clustering to a list of search results.
To do so, given a clusteringC = (C1, C2, .
.
.
, Cm), we add to the initially empty list the first element19 of each clusterCj (j = 1, .
.
.
, m); then we iterate the process by selecting the second element of eachcluster Cj such that |Cj| ?
2, and so on.
The remaining elements returned by the searchengine, but not included in any cluster of C, are appended to the bottom of the list intheir original order.4.3.2 Results and Discussion.
The results in terms of S-recall@K are shown in Table 15.
Thefirst key finding is that, independently of the adopted corpus for graph construction,each of the WSI algorithms outperforms all nonsemantic systems, including the state-of-the-art search resulting clustering engine (KeySRC), Essential Pages (see Section 2.4),and the Yahoo!
baseline.
This result provides strong evidence that inducing senses fora given ambiguous query is beneficial for the diversification of snippet results.
Notall WSI algorithms perform the same, however: In fact, we observe that exploitinglocal graph patterns (as done by Curvature and SquaT++) typically leads to worseresults compared with other graph-based approaches.
We do not observe substantialdifferences between Curvature and SquaT++ on edges and vertices.
We hypothesizethat the lack of significant difference in the diversification performance of the threepattern-based WSI algorithms is due to the lower number of clusters they produce(in the order of around two to three clusters, cf.
Table 12).19 Recall that the snippets within a cluster are sorted by relevance, cf.
Section 3.4.742Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIThe best performance on both corpora is, instead, obtained by HyperLex, tailedby B-MST and Chinese Whispers.
HyperLex is more complex and requires the tuningof many parameters (Agirre et al2006a), however.
Interestingly, we observe that theranking of WSI algorithms according to S-recall@K closely matches that obtained withthe F1 measure for clustering quality.
Finally, among the nonsemantic alternatives,Yahoo!
fares well and surpasses KeySRC and EP.To get futher insights into the performance of the best semantic systems, in Fig-ure 6 we graphed the values of S-recall@K for representative systems, namely, B-MST,0.10.20.30.40.50.60.70.80.910  5  10  15  20  25S-recall@KKB-MSTSquaT++VKeySRCEssential PagesYahoo!0.10.20.30.40.50.60.70.80.910  5  10  15  20  25S-recall@KKB-MSTSquaT++VKeySRCEssential PagesYahoo!Figure 6S-recall@K trend for B-MST, SquaT++V , KeySRC, Essential Pages, and Yahoo!
on Web1T (top)and ukWaC (bottom).743Computational Linguistics Volume 39, Number 3SquaT++V, and the three nonsemantic systems.
The results shown in the figure arethose obtained with Web1T (top) and ukWaC (bottom).
We can see that SquaT++V lagsbehind B-MST especially for low values of K. As also remarked previously, Yahoo!
tendsto perform better than KeySRC.As regards S-precision@r, shown in Table 16, again all WSI algorithms outperformnonsemantic systems.
The general trend observed for S-recall@K is confirmed here:HyperLex generally achieves the best values of S-precision@r, with good performancefor all other semantic systems.
All in all, HyperLex has the best balance between recalland precision, with better diversification performance on ukWaC, and therefore lookslike the most suitable choice.
B-MST, however, is much simpler and requires just oneparameter (i.e., the number of clusters), which can also be exploited by the user to getfiner- or coarser-grained search result groups.
As was previously done for S-recall@K,we also graphed the values of S-precision@r for the same representative systems inFigure 7.5.
In Vitro Experiment: Evaluating the Induced SensesAlthough the primary aim of this work was to demonstrate a relevant, end-to-end appli-cation of sense discovery techniques, we performed an additional in vitro experimentaimed at verifying the quality of the discovered senses independently of the task inwhich they are used.When performing in vitro evaluations, no single intrinsic measure provides aclear hint as to which algorithm performs best (Manandhar et al2010).
In fact, somemeasures favor large clusters, whereas others are based on the expectaction that theWSI algorithm will discover more fine-grained sense distinctions.
To provide furtherinsights into the clusters produced by our graph-based WSI algorithms, we performedTable 16S-precision@r on all queries (percentages).
The results for WSI algorithms are reported for bothWeb1T and ukWaC.rSystem 50 60 70 80 90Web1TCurvature 46.0 33.8 30.7 25.2 21.9SquaT++V 45.9 34.5 28.3 24.0 21.0SquaT++E 42.8 35.3 29.1 23.6 20.4B-MST 43.6 35.3 29.3 25.7 21.6HyperLex 46.5 38.0 31.3 26.5 22.5Chinese Whispers 49.4 35.2 28.9 24.2 21.9ukWaCCurvature 37.9 33.2 27.4 24.3 20.4SquaT++V 45.0 34.4 28.8 25.5 22.2SquaT++E 42.0 34.0 29.3 25.2 20.5B-MST 49.3 36.7 33.5 26.3 22.5HyperLex 51.4 40.0 32.4 27.6 22.6Chinese Whispers 45.1 36.6 30.1 24.5 20.5KeySRC 29.3 22.3 17.7 15.4 12.0EP 33.6 24.9 18.9 16.1 13.2Yahoo!
36.1 25.7 18.7 15.5 12.6744Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI0.10.150.20.250.30.350.40.450.550  55  60  65  70  75  80  85  90S-precision@rrB-MSTSquaT++VKeySRCEssential PagesYahoo!0.10.150.20.250.30.350.40.450.550  55  60  65  70  75  80  85  90S-precision@rrB-MSTSquaT++VKeySRCEssential PagesYahoo!Figure 7S-precision@r trend for B-MST, SquaT++V , KeySRC, Essential Pages, and Yahoo!
on Web1T (top)and ukWaC (bottom).a qualitative evaluation of the output clusters.
To this end we randomly selected17 queries from our query data set.
For each query, we submitted in random orderthe output of three representative WSI algorithms on the ukWaC corpus, namely,Curvature, HyperLex, and B-MST, to five annotators.We show an excerpt of the evaluation procedure for the query excalibur inTable 17.
On the left side of the table we propose an example of an anonymized set ofthree clusterings (i.e., one for each algorithm, shown in columns 2?4) presented to ourannotators.
Each algorithm produced a group of clusters, each of which consisted of aset of words strictly related to the meaning conveyed by the cluster itself, as discussed in745Computational Linguistics Volume 39, Number 3Table 17An example of the manual evaluation procedure for the query excalibur: We show a clusteringtriple proposed to the evaluator (left side) and an example of produced ranking (right side).Clustering A Clustering B Clustering CCluster 1movie movie hotelbook DVD moviecasino video reviewbook offerCluster 2sword swordstone lineageartillery stoneCluster 3comic hotelcomic strip chicagocasinorank algorithm1st B2nd A3rd CSection 3.2.2.
The annotators were asked to rank the three clusterings according to theirown preference (ties were allowed).
On the right side of Table 17 we show an exampleof ranking for the three clusterings.
In the example, clustering B was deemed to be morerepresentative, because it better models three meanings of excalibur, namely: the film-novel meaning, the sword meaning, and the hotel casino meaning, whereas clusteringA mixes the movie and the casino meaning within cluster 1, and, even worse, clusteringC just provides a singleton cluster.Finally, for each query, and for the entire set of 17 queries, we calculated the averageranking obtained by each WSI algorithm.
The overall results are shown in Table 18(last row): 1.7 for HyperLex, 1.8 for B-MST, and 2.4 for Curvature.
This experimentcorroborates the findings obtained from our extrinsic experiments: Curvature is theworst-ranking system (probably because of the low number of induced senses), whereasHyperLex and B-MST are more apt to discriminate between the meanings of an inputquery.
It is worth noting that the annotators often assigned the same rank to the clus-ters produced by B-MST and HyperLex, confirming our extrinsic finding that the twoalgorithms tend to have a similar behavior, compared with local graph pattern WSI.6.
Time Performance AnalysisFinally, because we are interested in the real-world application of the WSI techniqueswe discussed, we decided to collect statistics about the execution times of each systemon the AMBIENT and MORESQUE data sets.
We carried out this performance analysison a workstation using Sun Java 1.6 VM running on OpenSuse 11.4 (64 bit) with 16 GBPC3-15000 RAM, Intel Xeon E3-1240@3.30 GHz, and 1.5 TB hard disk space.In the graph construction step, in common with all WSI algorithms, most (i.e., about80%) of the computational load is due to the interaction with the database managementsystem (DBMS, we used MySQL 5.1), and the remaining CPU time is used for popu-lating the graph.
On average constructing a co-occurrence graph takes 10?12 secondsper query.
We note, however, that our algorithms were not engineered to work in anenterprise, possibly distributed, environment, with a commercial DBMS.
Moreover, afully engineered architecture might appropriately precalculate and cache the graphsconcerning the most frequent queries.746Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSITable 18Results of the manual evaluations.
The average scores assigned by the assessors to each one ofthe 17 queries are shown in columns 2, 3, and 4.
In the last row we report the average resultsover all queries.query id B-MST HyperLex Curvature1 2.0 1.0 3.02 1.4 2.0 2.43 2.2 1.6 2.24 1.4 2.4 2.25 2.6 2.0 1.46 1.2 1.8 3.07 2.2 1.4 2.18 2.2 2.4 1.49 1.8 2.4 1.610 1.8 1.6 2.211 1.6 1.4 3.012 1.6 1.8 2.613 1.8 1.2 3.014 1.8 1.2 3.015 1.8 1.8 2.016 1.6 1.4 3.017 1.8 1.2 3.0all 1.8 1.7 2.4The average time performance of WSI algorithms including sense discovery andsnippet clustering (but excluding graph construction) are shown in Table 19, expressedin average number of seconds per query for both corpora.
These numbers are comparedwith the time performance of nonsemantic systems (bottom part of the table).We observe that, among pattern-based algorithms, SquaT++ has a high runtimecost, due to the heavy calculation of three different graph patterns.
SquaT++E isparticularly onerous in the presence of large amounts of edges, which is the casefor ukWaC.
Curvature, instead, has a lower cost, because the triangle pattern is lessTable 19Execution times expressed in seconds.
For WSI algorithms, the reported times include the sensediscovery and snippet clustering steps and excludes the graph construction step.Algorithm Web1T ukWaCCurvature 0.34 0.34SquaT++V 28.98 14.45WSI-based SquaT++E 21.49 169.13systems B-MST 0.24 0.27HyperLex 0.16 0.13Chinese Whispers 0.28 0.35SRC systemsLingo 0.27STC 0.20KeySRC 1.00??
Estimated by the authors of KeySRC.747Computational Linguistics Volume 39, Number 3onerous to compute.
Interestingly, the algorithms which we experimentally foundto perform best (i.e., B-MST, HyperLex, and Chinese Whispers) have a much lowercomputational load compared with graph-pattern based algorithms.
We found thatHyperLex is particularly fast, with an average time of 0.1 seconds per query.
Finally, weobserve that the cost of the best WSI algorithms is not very far off that of nonsemanticSRC systems.7.
ConclusionsIn this article we have presented a novel approach to Web search result clustering basedon the automatic discovery of word senses from raw text.
Key to our approach is theidea of, first, automatically inducing senses for the target query and, second, clusteringthe search results based on their semantic similarity to the word senses induced.A sizeable body of work looking at the benefit of word senses for Web searchalready exists at the intersection between lexical semantics and information retrieval.That research, however, has focused almost exclusively on classical Word Sense Dis-ambiguation, with contrasting and often inconclusive results.
In this article, instead,we provide clear indication on the usefulness of a looser notion of sense to cope withambiguous queries.In fact, our experiments on data sets of queries of different lengths show that ourapproach outperforms all nonsemantic approaches to Web search result clustering.
Themain advantage of using Word Sense Induction lies in its dynamic production of wordsenses that cover both concepts (e.g., beagle as a specific breed of dog) and instances (e.g.,beagle as a specific instance of a space lander).
This is in contrast with static dictionariessuch as WordNet that are typically used in Word Sense Disambiguation and which, bytheir very nature, mainly encode concepts.Not only have we shown that graph-based WSI, when applied to search resultclustering, surpasses its nonsemantic alternatives, but we have also provided an end-to-end evaluation framework that enables fair comparison of WSI algorithms.
As a result,we are able to overcome many of the issues with the evaluation of clustering algorithms(von Luxburg, Williamson, and Guyon 2012), including the lack of a single unbiasedintrinsic measure (Manandhar et al2010).
Moreover, new WSI algorithms can be addedat any time and compared with those already integrated into the framework.
Buildingupon this, we are currently organizing a Semeval-2013 task for the extrinsic evaluationof WSI algorithms.20 As of today, we are releasing a new data set of 114 ambiguousqueries and 11,400 sense-annotated snippets.21 Given the present paucity of ambiguousquery data sets available (Sanderson 2008), we hope our data set will be useful in futurecomparative experiments.Thanks to its modular structure, our framework can easily be extended in manyother ways, including the addition of new snippet similarity measures, text corpora,query data sets, evaluation measures, and so on.
Although our graphs are centered onwords (as vertices), we are also interested in testing new graph construction proceduresbased on the use of collocations as vertices, as done by Korkontzelos and Manandhar(2010).
Furthermore, the framework is independent of the target language, in that it justrequires a large-enough corpus for co-occurrence extraction in that language and somebasic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder).20 http://www.cs.york.ac.uk/semeval-2013/task11/.21 The data set is available at http://lcl.uniroma1.it/moresque/.748Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIAs future work, the framework might be integrated with distributional semanticsmodels and techniques (Baroni and Lenci 2010; Erk, Pado?, and Pado?
2010; Mitchell andLapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012,inter alia).Finally we note that, although in this article our framework was applied to poly-semous queries only, nothing prevents it from being used to perform experiments atdifferent levels of sense granularity.
A qualitative evaluation of preliminary experimentsin aspect identification (cf.
Section 2.6), which requires the detection of very fine-grainedsubsenses of possibly monosemous queries, showed that WSI also seems to performwell in this task.
Given the high number of monosemous queries submitted to Websearch engines, we believe that further investigation in this direction may well revealadditional benefits of WSI for Web Information Retrieval.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI no.
259234 and theCASPUR High-Performance ComputingGrants 515/2011 and 118/2012.Thanks go to Google for providing theWeb1T corpus for research purposes,Claudio Carpineto and MassimilianoD?Amico for producing the output ofKeySRC and Essential Pages, and StanislawOsinski and Dawid Weiss for their helpwith Lingo and STC.
Additional thanks goto Jim McManus and the three anonymousreviewers for their helpful comments.ReferencesAgirre, Eneko, David Mart?
?nez, Oier Lo?pezde Lacalle, and Aitor Soroa.
2006a.Evaluating and optimizing the parametersof an unsupervised graph-based WSDalgorithm.
In Proceedings of the 1stWorkshop on Graph-Based Algorithms forNatural Language Processing, pages 89?96,New York.Agirre, Eneko, David Mart?
?nez, OierLo?pez de Lacalle, and Aitor Soroa.2006b.
Two graph-based algorithmsfor state-of-the-art WSD.
In Proceedingsof the 2006 Conference on EmpiricalMethods in Natural Language Processing,pages 585?593, Sydney.Agirre, Eneko and Aitor Soroa.
2007.UBC-AS: A graph based unsupervisedsystem for induction and classification.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations,pages 346?349, Prague.Agrawal, Rakesh, Sreenivas Gollapudi,Alan Halverson, and Samuel Ieong.
2009.Diversifying search results.
In Proceedingsof the 2nd International Conference on WebSearch and Web Data Mining, pages 5?14,Barcelona.Baroni, Marco and Alessandro Lenci.2010.
Distributional memory: Ageneral framework for corpus-basedsemantics.
Computational Linguistics,36(4):673?721.Basile, Pierpaolo, Annalina Caputo, andGiovanni Semeraro.
2009.
Exploitingdisambiguation and discriminationin Information Retrieval systems.In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference onWeb Intelligence and Intelligent AgentTechnology - Volume 03, pages 539?542,Washington, DC.Bennett, Paul N. and Nam Nguyen.
2009.Refined experts: Improving classificationin large taxonomies.
In Proceedings of the32nd Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 11?18,Boston, MA.Bernardini, Andrea, Claudio Carpineto,and Massimiliano D?Amico.
2009.Full-subtopic retrieval withkeyphrase-based search results clustering.In Proceedings of 2009 IEEE/WIC/ACMInternational Conference on Web Intelligence,pages 206?213, Milan.Biemann, Chris.
2006.
Chinese whispers?anefficient graph clustering algorithmand its application to Natural LanguageProcessing problems.
In Proceedings of the1st Workshop on Graph-Based Algorithms forNatural Language Processing, pages 73?80,New York.Boleda, Gemma, Sabine Schulte im Walde,and Toni Badia.
2012.
Modeling regularpolysemy: A study on the semanticclassification of Catalan adjectives.Computational Linguistics, 38(3):575?616.Branson, S. and A. Greenberg.
2002.Clustering Web search results using suffix749Computational Linguistics Volume 39, Number 3tree methods.
In Technical Report CS276AFinal Project, Stanford University.Brants, Thorsten and Alex Franz.
2006.
Web1T 5-gram, ver.
1, ldc2006t13.
In LinguisticData Consortium, Philadelphia, PA.Brody, Samuel and Mirella Lapata.
2009.Bayesian Word Sense Induction.
InProceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 103?111,Athens.Bruza, Peter, Robert McArthur, and SimonDennis.
2000.
Interactive Internetsearch: Keyword, directory and queryreformulation mechanisms compared.
InProceedings of the 23rd Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 280?287, Athens.Bunescu, Razvan C. and Marius Pasca.
2006.Using encyclopedic knowledge for namedentity disambiguation.
In Proceedings of11th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 9?16, Trento.Carbonell, Jaime and Jade Goldstein.1998.
The use of MMR, diversity-basedreranking for reordering documents andproducing summaries.
In Proceedings ofthe 21st Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 335?336,Melbourne.Carmel, David, Haggai Roitman, andNaama Zwerdling.
2009.
Enhancingcluster labeling using Wikipedia.
InProceedings of the 32nd Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 139?146, Boston, MA.Carpineto, Claudio, Massimiliano D?Amico,and Andrea Bernardini.
2011.
Fulldiscrimination of subtopics in searchresults with keyphrase-based clustering.Web Intelligence and Agent Systems,9(4):337?349.Carpineto, Claudio, Stanislaw Osin?ski,Giovanni Romano, and Dawid Weiss.2009.
A survey of Web clustering engines.ACM Computing Surveys, 41(3):1?38.Carpineto, Claudio and Giovanni Romano.2004.
Exploiting the potential of conceptlattices for Information Retrieval withCREDO.
Journal of Universal ComputerScience, 10(8):985?1013.Chandar, Praveen and Ben Carterette.
2010.Diversification of search results usingwebgraphs.
In Proceedings of the 33rdInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 869?870, Geneva.Chapelle, Olivier, Yi Chang, and Tie-Yan Liu.2011.
Future directions in learningto rank.
Journal of Machine LearningResearch?Proceedings Track, 14:91?100.Chen, Harr and David R. Karger.
2006.
Lessis more: Probabilistic models for retrievingfewer relevant documents.
In Proceedings ofthe 29th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 429?436,Seattle, WA.Chen, Jiyang, Osmar R.
Za?
?ane, and RandyGoebel.
2008.
An unsupervised approachto cluster web search results based onword sense communities.
In Proceedingsof 2008 IEEE/WIC/ACM InternationalConference on Web Intelligence,pages 725?729, Sydney.Cheng, David, Santosh Vempala, RaviKannan, and Grant Wang.
2005.
Adivide-and-merge methodology forclustering.
In Proceedings of the 24thACM SIGACT-SIGMOD-SIGARTSymposium on Principles of DatabaseSystems, pages 196?205, Baltimore, MD.Clarke, Daoud.
2012.
A context-theoreticframework for compositionality indistributional semantics.
ComputationalLinguistics, 38(1):41?71.Clough, Paul, Mark Sanderson, MuradAbouammoh, Sergio Navarro, andMonica Lestari Paramita.
2009.
Multipleapproaches to analysing query diversity.
InProceedings of the 32nd Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 734?735, Boston, MA.Crabtree, Daniel, Xiaoying Gao, and PeterAndreae.
2005.
Improving Web clusteringby cluster selection.
In Proceedings of 2005IEEE/WIC/ACM International Conference onWeb Intelligence, pages 172?178,Compiegne.Cutting, Douglass R., David R. Karger,Jan O. Pedersen, and John W. Tukey.
1992.Scatter/Gather: A cluster-based approachto browsing large document collections.
InProceedings of the 15th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 318?329, Copenhagen.Di Giacomo, Emilio, Walter Didimo,Luca Grilli, and Giuseppe Liotta.
2007.Graph visualization techniques for Webclustering engines.
IEEE Transactions onVisualization and Computer Graphics,13(2):294?304.750Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIDi Marco, Antonio and Roberto Navigli.2011.
Clustering Web search results withmaximum spanning trees.
In Proceedings ofthe XIIth International Conference of theItalian Association for Artificial Intelligence,pages 201?212, Palermo.Dorow, Beate, Dominic Widdows, KatarinaLing, Jean-Pierre Eckmann, Danilo Sergi,and Elisha Moses.
2005.
Using curvatureand Markov clustering in graphs forlexical acquisition and word sensediscrimination.
In Proceedings of theMeaning-2005 Workshop, Trento.Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.2010.
A flexible, corpus-driven model ofregular and inverse selectional preferences.Computational Linguistics, 36(4):723?763.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Database.
MIT Press,Cambridge, MA.Ferraresi, Adriano, Eros Zanchetta, MarcoBaroni, and Silvia Bernardini.
2008.Introducing and evaluating ukWaC,a very large Web-derived corpus ofEnglish.
In Proceedings of the 4th Web asCorpus Workshop (WAC-4), pages 47?54,Marrakech.Furnas, G. W., T. K. Landauer, L. M.Gomez, and S. T. Dumais.
1987.
Thevocabulary problem in human-systemcommunication.
Commununications ofACM, 30(11):964?971.Gabrilovich, Evgeniy and Shaul Markovitch.2009.
Wikipedia-based semanticinterpretation for natural languageprocessing.
Journal of Artificial IntelligenceResearch (JAIR), 34:443?498.Geiss, Johanna.
2009.
Creating a goldstandard for sentence clustering inmulti-document summarization.
InProceedings of the 47th Annual Meetingof the Association for ComputationalLinguistics and the 4th International JointConference on Natural Language Processingof the AFNLP, pages 96?104, Singapore.Gelgi, Fatih, Hasan Davulcu, and SrinivasVadrevu.
2007.
Term ranking for clusteringWeb search results.
In Proceedings of10th International Workshop on the Weband Databases, Beijing, China.Gonzalo, Julio, Anselmo Penas, and FelisaVerdejo.
1999.
Lexical ambiguity andInformation Retrieval revisited.
InProceedings of the Joint SIGDAT Conferenceon Empirical Methods in Natural LanguageProcessing and Very Large Corpora,pages 195?202, College Park, MD.Harris, Zellig.
1954.
Distributional structure.Word, 10:146?162.Hubert, L. and P. Arabie.
1985.
Comparingpartitions.
Journal of Classification,2(1):193?218.Kamvar, Maryam and Shumeet Baluja.2006.
A large scale study of wirelesssearch behavior: Google mobile search.In Proceedings of the 2006 Conference onHuman Factors in Computing Systems,pages 701?709, Montre?al.Ke, Weimao, Cassidy R. Sugimoto, andJaved Mostafa.
2009.
Dynamicity vs.effectiveness: Studying online clusteringfor Scatter/Gather.
In Proceedings of the32nd Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 19?26,Boston, MA.Kim, Sang-Bum, Hee-Cheol Seo, andHae-Chang Rim.
2004.
InformationRetrieval using word senses: Root sensetagging approach.
In Proceedings of the27th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 258?265,Sheffield.Korkontzelos, Ioannis and SureshManandhar.
2010.
UoY: Graphs ofunambiguous vertices for word senseinduction and disambiguation.In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation,pages 355?358, Uppsala.Krovetz, Robert and William B. Croft.1992.
Lexical ambiguity and InformationRetrieval.
ACM Transactions on InformationSystems, 10(2):115?141.Kurland, Oren.
2008.
The opposite ofsmoothing: A language model approachto ranking query-specific documentclusters.
In Proceedings of the 31st AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 171?178, Singapore.Kurland, Oren and Carmel Domshlak.
2008.A rank-aggregation approach to searchingfor optimal query-specific clusters.
InProceedings of the 31st Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 547?554, Singapore.Lee, Kyung Soon, W. Bruce Croft, and JamesAllan.
2008.
A cluster-based resamplingmethod for pseudo-relevance feedback.
InProceedings of the 31st Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 235?242, Singapore.Lin, Dekang.
1998.
Automatic retrieval andclustering of similar words.
In Proceedings751Computational Linguistics Volume 39, Number 3of the 17th International Conference onComputational Linguistics, pages 768?774,Montreal.Lin, Dekang and Patrick Pantel.
2002.Discovering word senses from text.In Proceedings of the 8th ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining, pages 613?619,Edmonton.Liu, Shuang, Clement Yu, and Weiyi Meng.2005.
Word Sense Disambiguationin queries.
In Proceedings of the 2005ACM CIKM International Conference onInformation and Knowledge Management,pages 525?532, Bremen.Liu, Tie-Yan, Yiming Yang, Hao Wan,Hua-Jun Zeng, Zheng Chen, andWei-Ying Ma.
2005.
Support vectormachines classification with a verylarge-scale taxonomy.
SIGKDDExplorations, 7(1):36?43.Liu, Ying, Wenyuan Li, Yongjing Lin, andLiping Jing.
2008.
Spectral geometry forsimultaneously clustering and rankingquery search results.
In Proceedings of the31st Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 539?546,Singapore.Ma, Hao, Michael R. Lyu, and Irwin King.2010.
Diversifying query suggestionresults.
In Proceedings of the 24th AAAIConference on Artificial Intelligence, AAAI2010, pages 1,399?1,404, Atlanta, GA.Maarek, Yoelle, Ron Fagin, Israel Ben-Shaul,and Dan Pelleg.
2000.
Ephemeraldocument clustering for Web applications.IBM Research Report RJ 10186.
Haifa.Manandhar, Suresh, Ioannis P. Klapaftis,Dmitriy Dligach, and Sameer S. Pradhan.2010.
SemEval-2010 task 14: Word senseinduction & disambiguation.
In Proceedingsof the 5th International Workshop on SemanticEvaluation, pages 63?68, Uppsala.Mandala, Rila, Takenobu Tokunaga, andHozumi Tanaka.
1998.
The use of WordNetin Information Retrieval.
In Proceedings ofthe COLING-ACL workshop on Usage ofWordnet in Natural Language Processing,pages 31?37, Montre?al.Matsuo, Yutaka, Takeshi Sakaki, Ko?kiUchiyama, and Mitsuru Ishizuka.
2006.Graph-based word clustering using a Websearch engine.
In Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing, pages 542?550,Sydney.Mihalcea, Rada.
2007.
Using Wikipedia forautomatic Word Sense Disambiguation.In Human Language Technology Conference ofthe North American Chapter of the Associationof Computational Linguistics, pages 196?203,Rochester, NY.Mihalkova, L. and R. Mooney.
2009.
Learningto disambiguate search queries from shortsessions.
In Proceedings of Machine Learningand Knowledge Discovery in Databases (2),pages 111?127, Bled.Miller, George A., R. T. Beckwith,Christiane D. Fellbaum, D. Gross, andK.
Miller.
1990.
WordNet: an onlinelexical database.
International Journal ofLexicography, 3(4):235?244.Mitchell, Jeff and Mirella Lapata.
2010.Composition in distributional modelsof semantics.
Cognitive Science,34(8):1388?1429.Navigli, Roberto.
2009.
Word SenseDisambiguation: A survey.
ACMComputing Surveys, 41(2):1?69.Navigli, Roberto.
2012.
A quick tour of WordSense Disambiguation, induction andrelated approaches.
In Proceedings of the38th Conference on Current Trends inTheory and Practice of Computer Science,pages 115?129, Spindleruv Mly?n.Navigli, Roberto and Giuseppe Crisafulli.2010.
Inducing word senses to improveWeb search result clustering.
In Proceedingsof the 2010 Conference on EmpiricalMethods in Natural Language Processing,pages 116?126, Boston, MA.Navigli, Roberto and Simone Paolo Ponzetto.2012.
The automatic construction,evaluation and application of awide-coverage multilingual semanticnetwork.
Artificial Intelligence,193:217?250.Ngo, Chi Lang and Hung Son Nguyen.2005.
A method of Web search resultclustering based on rough sets.
InProceedings of 2005 IEEE/WIC/ACMInternational Conference on Web Intelligence,pages 673?679, Compiegne.Nguyen, Cam-Tu, Xuan-Hieu Phan,Susumu Horiguchi, Thu-Trang Nguyen,and Quang-Thuy Ha.
2009.
Web searchclustering and labeling with hidden topics.ACM Transactions on Asian LanguageInformation Processing, 8(3):1?40.Osinski, Stanislaw and Dawid Weiss.
2005.A concept-driven algorithm for clusteringsearch results.
IEEE Intelligent Systems,20(3):48?54.Rand, W. M. 1971.
Objective criteria for theevaluation of clustering methods.
Journalof the American Statistical Association,66(336):846?850.752Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSIReisinger, Joseph and Marius Pasca.
2011.Fine-grained class label markup of searchqueries.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 1,200?1,209, Portland, OR.Rosenberg, Andrew and Julia Hirschberg.2007.
V-measure: A conditionalentropy-based external cluster evaluationmeasure.
In Proceedings of the 2007 JointConference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning, pages 410?420,Prague.Sanderson, Mark.
1994.
Word SenseDisambiguation and InformationRetrieval.
In Proceedings of the 17th AnnualInternational ACM-SIGIR Conference onResearch and Development in InformationRetrieval, pages 142?151, Dublin.Sanderson, Mark.
2000.
Retrieving with goodsense.
Information Retrieval, 2(1):49?69.Sanderson, Mark.
2008.
Ambiguous queries:Test collections need more sense.
InProceedings of the 31st Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 499?506, Singapore.Santamar?
?a, Celina, Julio Gonzalo, andJavier Artiles.
2010.
Wikipedia as senseinventory to improve diversity in Websearch results.
In Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics, ACL 2010,pages 1,357?1,366, Uppsala.Schu?tze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings of the 1992ACM/IEEE Conference on Supercomputing,pages 787?796, Los Alamitos, CA.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?124.Schu?tze, Hinrich and Jan Pedersen.
1995.Information Retrieval based on wordsenses.
In Proceedings of SDAIR?95,pages 161?175, Las Vegas, NV.Silberer, Carina and Mirella Lapata.2012.
Grounded models of semanticrepresentation.
In Proceedings ofthe 2012 Joint Conference on EmpiricalMethods in Natural LanguageProcessing and Computational NaturalLanguage Learning, pages 1,423?1,433,Jeju Island.Smadja, Frank, Kathleen R. McKeown,and Vasileios Hatzivassiloglou.
1996.Translating collocations for bilinguallexicons: A statistical approach.Computational Linguistics, 22(1):1?38.Song, Ruihua, Zhenxiao Luo, Jian-Yun Nie,Yong Yu, and Hsiao-Wuen Hon.
2009.Identification of ambiguous queries inWeb search.
Information Processing andManagement, 45:216?229.Steinley, Doug.
2004.
Properties of theHubert-Arabie adjusted Rand index.Psychological Methods, 9(3):386?396.Stokoe, Christopher, Michael J. Oakes,and John I. Tait.
2003.
Word SenseDisambiguation in Information Retrievalrevisited.
In Proceedings of the 26th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 159?166, Toronto.Swaminathan, Ashwin, Cherian V. Mathew,and Darko Kirovski.
2009.
Essential pages.In Proceedings of 2009 IEEE/WIC/ACMInternational Conference on Web Intelligence,pages 173?182, Milan.Udani, Goldee, Shachi Dave, AnthonyDavis, and Tim Sibley.
2005.
Noun senseinduction using Web search results.
InProceedings of the 28th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 657?658, Salvador.Van de Cruys, Tim and MariannaApidianaki.
2011.
Latent semantic wordsense induction and disambiguation.In Proceedings of the 49th Annual Meetingof the Association for ComputationalLinguistics: Human Language Technologies,pages 1,476?1,485, Portland, OR.van Rijsbergen, C. J.
1979.
InformationRetrieval, second edition.
Butterworths,London.Ve?ronis, Jean.
2004.
HyperLex: Lexicalcartography for information retrieval.Computer, Speech and Language,18(3):223?252.von Luxburg, Ulrike, Robert C. Williamson,and Isabelle Guyon.
2012.
Clustering:Science or art?
Journal of MachineLearning Research?Proceedings Track,27:65?80.Voorhees, Ellen M. 1993.
Using WordNetto disambiguate word senses for textretrieval.
In Proceedings of the 16th AnnualInternational ACM-SIGIR Conference onResearch and Development in InformationRetrieval, pages 171?180, Pittsburgh, PA.Wang, Xuanhui, Deepayan Chakrabarti,and Kunal Punera.
2009.
Mining broadlatent query aspects from search sessions.In Proceedings of the 15th ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining, pages 867?876,Paris.753Computational Linguistics Volume 39, Number 3Wang, Xuanhui and ChengXiang Zhai.2007.
Learn from Web search logs toorganize search results.
In Proceedings ofthe 30th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 87?94,Amsterdam.Widdows, Dominic and Beate Dorow.
2002.A graph model for unsupervised lexicalacquisition.
In Proceedings of the 19thInternational Conference on ComputationalLinguistics, pages 1?7, Taipei.Wu, Fei, Jayant Madhavan, and Alon Y.Halevy.
2011.
Identifying aspects forWeb-search queries.
Journal of ArtificialIntelligence Research (JAIR), 40:677?700.Xue, Gui-Rong, Dikan Xing, Qiang Yang,and Yong Yu.
2008.
Deep classification inlarge-scale text hierarchies.
In Proceedingsof the 31st Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 619?626,Singapore.Xue, Xiaobing and Xiaoxin Yin.
2011.
Topicmodeling for named entity queries.
InProceedings of the 20th ACM InternationalConference on Information and KnowledgeManagement, pages 2009?2012, New York.Yarowsky, David.
1993.
One sense percollocation.
In Proceedings of the ARPAWorkshop on Human Language Technology,pages 266?271, Princeton, NJ.Zamir, Oren and Oren Etzioni.
1998.
Webdocument clustering: A feasibilitydemonstration.
In Proceedings of the 21stAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval, pages 46?54, Melbourne.Zamir, Oren, Oren Etzioni, Omid Madani,and Richard M. Karp.
1997.
Fast andintuitive clustering of Web documents.In Proceedings of the Third InternationalConference on Knowledge Discovery andData Mining, pages 287?290, NewportBeach, CA.Zeng, Hua-Jun, Qi-Cai He, Zheng Chen,Wei-Ying Ma, and Jinwen Ma.
2004.Learning to cluster Web search results.
InProceedings of the 27th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 210?217, Sheffield.Zhai, ChengXiang, William W. Cohen,and John Lafferty.
2003.
Beyondindependent relevance: Methods andevaluation metrics for subtopic retrieval.In Proceedings of the 26th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 10?17, Toronto.Zhang, Benyu, Hua Li, Yi Liu, Lei Ji,Wensi Xi, Weiguo Fan, Zheng Chen,and Wei-Ying Ma.
2005.
Improving Websearch results using affinity graph.
InProceedings of the 28th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,SIGIR 2005, pages 504?511, Salvador.Zhang, Xiaodan, Xiaohua Hu, and XiaohuaZhou.
2008.
A comparative evaluationof different link types on enhancingdocument clustering.
In Proceedings of the31st Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 555?562,Singapore.Zhao, Ying and George Karypis.
2004.Empirical and theoretical comparisons ofselected criterion functions for documentclustering.
Machine Learning, 55(3):311?331.754
