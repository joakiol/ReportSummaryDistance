Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516?520,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Annotated Corpus of Quoted Opinions in News ArticlesTim O?Keefe James R. Curran Peter Ashwell Irena Koprinskae-lab, School of Information TechnologiesUniversity of SydneyNSW 2006, Australia{tokeefe,james,pash4408,irena}@it.usyd.edu.auAbstractQuotes are used in news articles as evi-dence of a person?s opinion, and thus area useful target for opinion mining.
How-ever, labelling each quote with a polarityscore directed at a textually-anchored tar-get can ignore the broader issue that thespeaker is commenting on.
We addressthis by instead labelling quotes as support-ing or opposing a clear expression of apoint of view on a topic, called a positionstatement.
Using this we construct a cor-pus covering 7 topics with 2,228 quotes.1 IntroductionNews articles are a useful target for opinion min-ing as they discuss salient opinions by newswor-thy people.
Rather than asserting what a person?sopinion is, journalists typically provide evidenceby using reported speech, and in particular, directquotes.
We focus on direct quotes as expressionsof opinion, as they can be accurately extracted andattributed to a speaker (O?Keefe et al, 2012).Characterising the opinions in quotes remainschallenging.
In sentiment analysis over productreviews, polarity labels are commonly used be-cause the target, the product, is clearly identified.However, for quotes on topics of debate, the targetand meaning of polarity labels is less clear.
For ex-ample, labelling a quote about abortion as simplypositive or negative is uninformative, as a speakercan use either positive or negative language to sup-port or oppose either side of the debate.Previous work (Wiebe et al, 2005; Balahuret al, 2010) has addressed this by giving eachexpression of opinion a textually-anchored target.While this makes sense for named entities, it doesnot apply as obviously for topics, such as abortion,that may not be directly mentioned.
Our solutionis to instead define position statements, which areAbortion: Women should have the right to choose an abortion.Carbon tax: Australia should introduce a tax on carbon or anemissions trading scheme to combat global warming.Immigration: Immigration into Australia should be maintainedor increased because its benefits outweigh any negatives.Reconciliation: The Australian government should formallyapologise to the Aboriginal people for past injustices.Republic: Australia should cease to be a monarchy with theQueen as head of state and become a republic with an Australianhead of state.Same-sex marriage: Same-sex couples should have the right toattain the legal state of marriage as it is for heterosexual couples.Work choices: Australia should introduce WorkChoices to giveemployers more control over wages and conditions.Table 1: Topics and their position statements.clear statements of a viewpoint or position on aparticular topic.
Quotes related to this topic canthen be labelled as supporting, neutral, or oppos-ing the position statement.
This disambiguates themeaning of the polarity labels, and allows us todetermine the side of the debate that the speakeris on.
Table 1 shows the topics and position state-ments used in this work, and some example quotesfrom the republic topic are given below.
Note thatthe first example includes no explicit mention ofthe monarchy or the republic.Positive: ?I now believe that the time has come.
.
.
for us tohave a truly Australian constitutional head of state.
?Neutral: ?The establishment of an Australian republic is es-sentially a symbolic change, with the main arguments, forand against, turning on national identity.
.
.
?Negative: ?I personally think that the monarchy is a traditionwhich we want to keep.
?With this formulation we define an annotationscheme and build a corpus covering 7 topics, with100 documents per topic.
This corpus includes3,428 quotes, of which 1,183 were marked in-valid, leaving 2,228 that were marked as support-ing, neutral, or opposing the relevant topic state-ment.
All quotes in our corpus were annotated bythree annotators, with Fleiss?
?
values of between0.43 and 0.45, which is moderate.5162 BackgroundEarly work in sentiment analysis (Turney, 2002;Pang et al, 2002; Dave et al, 2003; Blitzer et al,2007) focused on product and movie reviews,where the text under analysis discusses a singleproduct or movie.
In these cases, labels like posi-tive and negative are appropriate as they align wellwith the overall communicative goal of the text.Later work established aspect-oriented opinionmining (Hu and Liu, 2004), where the aim is tofind features or aspects of products that are dis-cussed in a review.
The reviewer?s position oneach aspect can then be classified as positive ornegative, which results in a more fine-grained clas-sification that can be combined to form an opin-ion summary.
These approaches assume that eachdocument has a single source (the document?s au-thor), whose communicative goal is to evaluate awell-defined target, such as a product or a movie.However this does not hold in news articles, wherethe goal of the journalist is to present the view-points of potentially many people.Several studies (Wiebe et al, 2005; Wilsonet al, 2005; Kim and Hovy, 2006; Godbole et al,2007) have looked at sentiment in news text, withsome (Balahur and Steinberger, 2009; Balahuret al, 2009, 2010) focusing on quotes.
In all ofthese studies the authors have textually-anchoredthe target of the sentiment.
While this makes sensefor targets that can be resolved back to named enti-ties, it does not apply as obviously when the quoteis arguing for a particular viewpoint in a debate,as the topic may not be mentioned explicitly andpolarity labels may not align to sides of the debate.Work on debate summarisation and subgroupdetection (Somasundaran and Wiebe, 2010; Abu-Jbara et al, 2012; Hassan et al, 2012) has of-ten used data from online debate forums, partic-ularly those forums where users are asked to se-lect whether they support or oppose a given propo-sition before they can participate.
This is simi-lar to our aim with news text, where instead of atextually-anchored target, we have a proposition,against which we can evaluate quotes.3 Position StatementsOur goal in this study is to determine which side ofa debate a given quote supports.
Assigning polar-ity labels to a textually-anchored target does notwork here for several reasons.
Quotes may notmention the debate topic, there may be many rel-No cont.
ContextTopic Quotes AA ?
AA ?Abortion 343 .77 .57 .73 .53Carbon tax 278 .71 .42 .57 .34Immigration 249 .58 .18 .58 .25Reconcil.
513 .66 .37 .68 .44Republic 347 .68 .51 .71 .58Same-sex m. 246 .72 .51 .71 .55Work choices 269 .72 .45 .65 .44Total 2,245 .69 .43 .66 .45Table 2: Average Agreement (AA) and Fleiss?
?over the valid quotesevant textually-anchored targets for a single topic,and polarity labels do not necessarily align withsides of a debate.We instead define position statements, whichclearly state the position that one side of the debateis arguing for.
We can then characterise opinionsas supporting, neutral towards, or opposing thisparticular position.
Position statements should notargue for a particular position, rather they shouldsimply state what the position is.
Table 1 showsthe position statements that we use in this work.4 AnnotationFor our task we expect a set of news articles ona given topic as input, where the direct quotes inthe articles have been extracted and attributed tospeakers.
A position statement will have been de-fined, that states a point of view on the topic, anda small subset of quotes will have been labelled assupporting, neutral, or opposing the given state-ment.
A system performing this task would thenlabel the remaining quotes as supporting, neutral,or opposing, and return them to the user.A major contribution of this work is that weconstruct a fully labelled corpus, which can beused to evaluate systems that perform the task de-scribed above.
To build this corpus we employedthree annotators, one of whom is an author, whilethe other two were hired using the outsourcingwebsite Freelancer1.
Our data is drawn from theSydney Morning Herald2 archive, which rangesfrom 1986 until 2009, and it covers seven topicsthat were subject to debate within Australian newsmedia during that time.
For each topic we used1http://www.freelancer.com2http://www.smh.com.au517No cont.
ContextTopic Quotes AA ?
AA ?Abortion 343 .78 .52 .74 .46Carbon tax 278 .72 .39 .59 .19Immigration 249 .58 .08 .58 .14Reconcil.
513 .66 .31 .69 .36Republic 347 .69 .39 .72 .41Same-sex m. 246 .73 .43 .73 .40Work choices 269 .73 .40 .67 .32Total 2,245 .70 .36 .68 .32Table 3: Average Agreement (AA) and Fleiss?
?when the labels are neutral versus non-neutralApache Solr3 to find the top 100 documents thatmatched a manually-constructed search query.
Alldocuments were tokenised and POS-tagged and thenamed entities were found using the system fromHachey et al (2013).
Finally, the quotes were ex-tracted and attributed to speakers using the systemfrom O?Keefe et al (2012).For the first part of the task, annotators wereasked to label each quote without considering anycontext.
In other words they were asked to onlyuse the text of the quote itself as evidence for anopinion, not the speaker?s prior opinions or thetext of the document.
They were then asked to la-bel the quote a second time, while considering thetext surrounding the quote, although they were stillasked to ignore the prior opinions of the speaker.For each of these choices annotators were given afive-point scale ranging from strong or clear op-position to strong or clear support, where supportor opposition is relative to the position statement.Annotators were also asked to mark instanceswhere either the speaker or quote span was incor-rectly identified, although they were asked to con-tinue annotating the quote as though it were cor-rect.
They were also asked to mark quotes thatwere invalid due to either the quote being off-topic, or the item not being a quote (e.g.
book ti-tles, scare quotes, etc.
).5 Corpus resultsIn order to achieve the least amount of noise inour corpus, we opted to discard quotes that any an-notator had marked as invalid.
From the originalset of 3,428 quotes, 1,183 (35%) were removed,which leaves 2,245 (65%).
From the original cor-pus, 23% were marked off-topic, which shows that3http://lucene.apache.org/solr/in order to label opinions in news, a system wouldfirst have to identify the topic-relevant parts of thetext.
The annotators further indicated that 16%were not quotes, and there were a small number ofcases (<1%) where the quote span was incorrect.Annotators were able to select multiple reasons fora quote being invalid.Table 2 shows both Fleiss?
?
and the raw agree-ment averaged between annotators for each topic.We collapsed the two supporting labels together,as well as the two opposing labels, such that weend up with a classification of opposes vs. neu-tral vs. supports.
The no context and context casesscored 0.69 and 0.66 in raw agreement, while the?
values were 0.43 and 0.45, which is moderate.Intuitively we expect that the confusion islargely between neutral and the two polar labels.To examine this we merged all the non-neutral la-bels into one group and calculated the agreementbetween the non-neutral group and the neutral la-bel, as shown in Table 3.
For the non-neutral vs.neutral agreement we find that despite stability inraw agreement, Fleiss?
?
drops substantially, to0.36 (no context) and 0.32 (context).For comparison we remove all neutral annota-tions and focus on disagreement between the po-lar labels.
For this we cannot use Fleiss?
?, as itrequires a fixed number of annotations per quote,however we can average the pairwise ?
values be-tween annotators, which results in values of 0.93(no context) and 0.92 (context).
Though they arenot directly comparable, the magnitude of the dif-ference between the numbers (0.36 and 0.32 vs.0.93 and 0.92) indicates that deciding when anopinion provides sufficient evidence of support oropposition is the main challenge facing annotators.To adjudicate the decisions annotators made, weopted to take a majority vote for cases of twoor three-way agreement, while discarding caseswhere annotators did not agree (1% of quotes).The final distribution of labels in the corpus isshown in Table 4.
For both the no context andcontext cases the largest class was neutral with61% and 46% of the corpus respectively.
The dropin neutrality between the no context and contextcases shows that the interpretation of a quote canchange based on the context it is placed in.6 DiscussionIn refining our annotation scheme we noted severalfactors that make annotation difficult.518No context ContextTopic Quotes Opp.
Neut.
Supp.
Quotes Opp.
Neut.
Supp.Abortion 343 .13 .63 .25 340 .16 .52 .32Carbon tax 273 .09 .70 .21 273 .14 .44 .42Immigration 247 .09 .72 .19 245 .12 .64 .23Reconciliation 509 .05 .57 .38 503 .07 .42 .50Republic 345 .24 .48 .28 342 .32 .37 .32Same-sex marriage 246 .16 .55 .28 243 .25 .38 .37Work choices 265 .14 .72 .14 266 .26 .50 .24Total 2,228 .12 .61 .26 2,212 .18 .46 .36Table 4: Label distribution for the final corpus.Opinion relevance When discussing a topic,journalists will often delve into the related aspectsand opinions that people hold.
This introduces achallenge as annotators need to decide whether aparticular quote is on-topic enough to be labelled.For instance, these quotes by the same speakerwere in an article on the carbon tax:1) ?Whether it?s a stealth tax, the emissions trading scheme,whether it?s an upfront.
.
.
tax like a carbon tax, there will notbe any new taxes as part of the Coalition?s policy?2) ?I don?t think it?s something that we should rush into.
Butcertainly I?m happy to see a debate about the nuclear option.
?In the first quote the speaker is voicing opposi-tion to a tax on carbon, which is easy to annotatewith our scheme.
However in the second quote,the speaker is discussing nuclear power in relationto a carbon tax, which is much more difficult, as itis unclear whether is is off-topic or neutral.Obfuscation and self-contradiction Whilejournalists usually quote someone to provideevidence of the person?s opinion, there are somecases where they include quotes to show thatthe person is inconsistent.
The following quotesby the same speaker were included in an arti-cle to illustrate that the speaker?s position wasinconsistent:1) ?My point is that.
.
.
the most potent argument in favour ofthe republic, is that why should we have a Briton as the Queen?
who, of course, in reality is also the Queen of Australia ?but a Briton as the head of State of Australia?2) ?The Coalition supports the Constitution not because wesupport the.
.
.
notion of the monarchy, but because we supportthe way our present Constitution works?The above example also indicates a level of ob-fuscation that is reasonably common for politi-cians.
Neither of the quotes actually expresses aclear statement of how the speaker feels about apotential republic.
The first quote is an opinionabout the strongest argument in favour of a re-public, without necessarily making that argument,while the second quote states a party line, with acaveat that might indicate personal disagreement.Annotator bias This task is prone to be influ-enced by an annotator?s biases, including their po-litical or cultural background, their opinion aboutthe topic or speaker, or their level of knowledgeabout the topic.7 ConclusionIn this work we examined the problem of anno-tating opinions in news articles.
We proposed toexploit quotes, as they are used by journalists toprovide evidence of an opinion, and are easy toextract and attribute to speakers.
Our key con-tribution is that rather than requiring a textually-anchored target for each quote, we instead labelquotes as supporting, neutral, or opposing a posi-tion statement, which states a particular viewpointon a topic.
This allowed us to resolve ambigu-ities that arise when considering a polarity labeltowards a topic.
We next defined an annotationscheme and built a corpus, which covers 7 top-ics, with 100 documents per topic, and a total of2,228 annotated quotes.
Future work will includebuilding a system able to perform the task we havedefined, as well as extending this work to includeindirect quotes.AcknowledgementsO?Keefe was supported by a University of SydneyMerit scholarship and a Capital Markets CRC top-up scholarship.
This work was supported by ARCDiscovery grant DP1097291 and the Capital Mar-kets CRC Computable News project.519ReferencesAmjad Abu-Jbara, Mona Diab, Pradeep Dasigi,and Dragomir Radev.
2012.
Subgroup detec-tion in ideological discussions.
In Proceedingsof the 50th Annual Meeting of the Associationfor Computational Linguistics, pages 399?409.Alexandra Balahur and Ralf Steinberger.
2009.Rethinking sentiment analysis in the news:From theory to practice and back.
Proceedingsof the First Workshop on Opinion Mining andSentiment Analysis.Alexandra Balahur, Ralf Steinberger, MijailKabadjov, Vanni Zavarella, Erik Van Der Goot,Matina Halkia, Bruno Pouliquen, and JenyaBelyaeva.
2010.
Sentiment analysis in the news.In Proceedings of the 7th International Confer-ence on Language Resources and Evaluation,pages 2216?2220.Alexandra Balahur, Ralf Steinberger, Erik VanDer Goot, Bruno Pouliquen, and Mijail Kabad-jov.
2009.
Opinion mining on newspa-per quotations.
In Proceedings of the 2009IEEE/WIC/ACM International Joint Confer-ence on Web Intelligence and Intelligent AgentTechnology, pages 523?526.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxesand blenders: Domain adaptation for sentimentclassification.
In Proceedings of the 45th An-nual Meeting of the Association of Computa-tional Linguistics, pages 440?447.Kushal Dave, Steve Lawrence, and David Pen-nock.
2003.
Mining the peanut gallery: Opinionextraction and semantic classification of prod-uct reviews.
In Proceedings of the 12th inter-national conference on World Wide Web, pages519?528.Namrata Godbole, Manjunath Srinivasaiah, andSteven Skiena.
2007.
Large-scale sentimentanalysis for news and blogs.
In Proceedingsof the International Conference on Weblogs andSocial Media.Ben Hachey, Will Radford, Joel Nothman,Matthew Honnibal, and James R. Curran.
2013.Evaluating entity linking with Wikipedia.
Arti-ficial Intelligence.Ahmed Hassan, Amjad Abu-Jbara, and DragomirRadev.
2012.
Detecting subgroups in online dis-cussions by modeling positive and negative re-lations among participants.
In Proceedings ofthe 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning, pages59?70.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings ofthe tenth ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Min-ing, pages 168?177.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressedin online news media text.
In Proceedings of theWorkshop on Sentiment and Subjectivity in Text,pages 1?8.Tim O?Keefe, Silvia Pareti, James R. Curran,Irena Koprinska, and Matthew Honnibal.
2012.A sequence labelling approach to quote attri-bution.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning, pages 790?799.Bo Pang, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
: Senti-ment classification using machine learningtechniques.
In Proceedings of the 2002Conference on Empirical Methods in NaturalLanguage Processing, pages 79?86.Swapna Somasundaran and Janyce Wiebe.
2010.Recognizing stances in ideological on-line de-bates.
In Proceedings of the NAACL HLT2010 Workshop on Computational Approachesto Analysis and Generation of Emotion in Text,pages 116?124.Peter Turney.
2002.
Thumbs up or thumbs down?
:Semantic orientation applied to unsupervisedclassification of reviews.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, pages 417?424.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions andemotions in language.
Language Resources andEvaluation, 39(2/3):165?210.Theresa Wilson, Paul Hoffmann, Swapna Soma-sundaran, Jason Kessler, Janyce Wiebe, YejinChoi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
OpinionFinder: a systemfor subjectivity analysis.
In Proceedings ofHLT/EMNLP Interactive Demonstrations.520
