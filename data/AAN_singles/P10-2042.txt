Proceedings of the ACL 2010 Conference Short Papers, pages 225?230,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBlocked Inference in Bayesian Tree Substitution GrammarsTrevor CohnDepartment of Computer ScienceUniversity of SheffieldT.Cohn@dcs.shef.ac.ukPhil BlunsomComputing LaboratoryUniversity of OxfordPhil.Blunsom@comlab.ox.ac.ukAbstractLearning a tree substitution grammar isvery challenging due to derivational am-biguity.
Our recent approach used aBayesian non-parametric model to inducegood derivations from treebanked input(Cohn et al, 2009), biasing towards smallgrammars composed of small generalis-able productions.
In this paper we presenta novel training method for the model us-ing a blocked Metropolis-Hastings sam-pler in place of the previous method?s lo-cal Gibbs sampler.
The blocked sam-pler makes considerably larger moves thanthe local sampler and consequently con-verges in less time.
A core componentof the algorithm is a grammar transforma-tion which represents an infinite tree sub-stitution grammar in a finite context freegrammar.
This enables efficient blockedinference for training and also improvesthe parsing algorithm.
Both algorithms areshown to improve parsing accuracy.1 IntroductionTree Substitution Grammar (TSG) is a compellinggrammar formalism which allows nonterminalrewrites in the form of trees, thereby enablingthe modelling of complex linguistic phenomenasuch as argument frames, lexical agreement andidiomatic phrases.
A fundamental problem withTSGs is that they are difficult to estimate, even inthe supervised scenario where treebanked data isavailable.
This is because treebanks are typicallynot annotated with their TSG derivations (how todecompose a tree into elementary tree fragments);instead the derivation needs to be inferred.In recent work we proposed a TSG model whichinfers an optimal decomposition under a non-parametric Bayesian prior (Cohn et al, 2009).This used a Gibbs sampler for training, which re-peatedly samples for every node in every trainingtree a binary value indicating whether the node isor is not a substitution point in the tree?s deriva-tion.
Aggregated over the whole corpus, these val-ues and the underlying trees specify the weightedgrammar.
Local Gibbs samplers, although con-ceptually simple, suffer from slow convergence(a.k.a.
poor mixing).
The sampler can get easilystuck because many locally improbable decisionsare required to escape from a locally optimal solu-tion.
This problem manifests itself both locally toa sentence and globally over the training sample.The net result is a sampler that is non-convergent,overly dependent on its initialisation and cannot besaid to be sampling from the posterior.In this paper we present a blocked Metropolis-Hasting sampler for learning a TSG, similar toJohnson et al (2007).
The sampler jointly updatesall the substitution variables in a tree, makingmuch larger moves than the local single-variablesampler.
A critical issue when developing aMetroplis-Hastings sampler is choosing a suitableproposal distribution, which must have the samesupport as the true distribution.
For our model thenatural proposal distribution is a MAP point esti-mate, however this cannot be represented directlyas it is infinitely large.
To solve this problem wedevelop a grammar transformation which can suc-cinctly represent an infinite TSG in an equivalentfinite Context Free Grammar (CFG).
The trans-formed grammar can be used as a proposal dis-tribution, from which samples can be drawn inpolynomial time.
Empirically, the blocked sam-pler converges in fewer iterations and in less timethan the local Gibbs sampler.
In addition, we alsoshow how the transformed grammar can be usedfor parsing, which yields theoretical and empiri-cal improvements over our previous method whichtruncated the grammar.2252 BackgroundA Tree Substitution Grammar (TSG; Bod etal.
(2003)) is a 4-tuple, G = (T,N, S,R), whereT is a set of terminal symbols, N is a set of non-terminal symbols, S ?
N is the distinguished rootnonterminal and R is a set of productions (rules).The productions take the form of tree fragments,called elementary trees (ETs), in which each in-ternal node is labelled with a nonterminal and eachleaf is labelled with either a terminal or a nonter-minal.
The frontier nonterminal nodes in each ETform the sites into which other ETs can be substi-tuted.
A derivation creates a tree by recursive sub-stitution starting with the root symbol and finish-ing when there are no remaining frontier nonter-minals.
Figure 1 (left) shows an example deriva-tion where the arrows denote substitution.
A Prob-abilistic Tree Substitution Grammar (PTSG) as-signs a probability to each rule in the grammar,where each production is assumed to be condi-tionally independent given its root nonterminal.
Aderivation?s probability is the product of the prob-abilities of the rules therein.In this work we employ the same non-parametric TSG model as Cohn et al (2009),which we now summarise.
The inference prob-lem within this model is to identify the posteriordistribution of the elementary trees e given wholetrees t. The model is characterised by the use ofa Dirichlet Process (DP) prior over the grammar.We define the distribution over elementary trees ewith root nonterminal symbol c asGc|?c, P0 ?
DP(?c, P0(?|c))e|c ?
Gcwhere P0(?|c) (the base distribution) is a distribu-tion over the infinite space of trees rooted with c,and ?c (the concentration parameter) controls themodel?s tendency towards either reusing elemen-tary trees or creating novel ones as each traininginstance is encountered.Rather than representing the distribution Gc ex-plicitly, we integrate over all possible values ofGc.
The key result required for inference is thatthe conditional distribution of ei, given e?i,=e1 .
.
.
en\ei and the root category c is:p(ei|e?i, c, ?c, P0)=n?iei,cn?i?,c + ?c+?cP0(ei|c)n?i?,c + ?c(1)where n?iei,c is the number number of times ei hasbeen used to rewrite c in e?i, and n?i?,c =?e n?ie,cSNPNPGeorgeVPVhatesNPNPbroccoliSNP,1GeorgeVP,0V,0hatesNP,1broccoliFigure 1: TSG derivation and its corresponding Gibbs statefor the local sampler, where each node is marked with a bi-nary variable denoting whether it is a substitution site.is the total count of rewriting c. Henceforth weomit the ?i sub-/super-script for brevity.A primary consideration is the definition of P0.Each ei can be generated in one of two ways:by drawing from the base distribution, where theprobability of any particular tree is proportional to?cP0(ei|c), or by drawing from a cache of previ-ous expansions of c, where the probability of anyparticular expansion is proportional to the numberof times that expansion has been used before.
InCohn et al (2009) we presented base distributionsthat favour small elementary trees which we ex-pect will generalise well to unseen data.
In thiswork we show that if P0 is chosen such that itdecomposes with the CFG rules contained withineach elementary tree,1 then we can use a novel dy-namic programming algorithm to sample deriva-tions without ever enumerating all the elementarytrees in the grammar.The model was trained using a local Gibbs sam-pler (Geman and Geman, 1984), a Markov chainMonte Carlo (MCMC) method in which randomvariables are repeatedly sampled conditioned onthe values of all other random variables in themodel.
To formulate the local sampler, we asso-ciate a binary variable with each non-root inter-nal node of each tree in the training set, indicat-ing whether that node is a substitution point ornot (illustrated in Figure 1).
The sampler then vis-its each node in a random schedule and resamplesthat node?s substitution variable, where the proba-bility of the two different configurations are givenby (1).
Parsing was performed using a Metropolis-Hastings sampler to draw derivation samples fora string, from which the best tree was recovered.However the sampler used for parsing was biased1Both choices of base distribution in Cohn et al (2009)decompose into CFG rules.
In this paper we focus on thebetter performing one, PC0 , which combines a PCFG appliedrecursively with a stopping probability, s, at each node.226because it used as its proposal distribution a trun-cated grammar which excluded all but a handfulof the unseen elementary trees.
Consequently theproposal had smaller support than the true model,voiding the MCMC convergence proofs.3 Grammar TransformationWe now present a blocked sampler using theMetropolis-Hastings (MH) algorithm to performsentence-level inference, based on the work ofJohnson et al (2007) who presented a MH samplerfor a Bayesian PCFG.
This approach repeats thefollowing steps for each sentence in the trainingset: 1) run the inside algorithm (Lari and Young,1990) to calculate marginal expansion probabil-ities under a MAP approximation, 2) sample ananalysis top-down and 3) accept or reject using aMetropolis-Hastings (MH) test to correct for dif-ferences between the MAP proposal and the truemodel.
Though our model is similar to John-son et al (2007)?s, we have an added complica-tion: the MAP grammar cannot be estimated di-rectly.
This is a consequence of the base distri-bution having infinite support (assigning non-zeroprobability to infinitely many unseen tree frag-ments), which means the MAP has an infinite ruleset.
For example, if our base distribution licencesthe CFG production NP?
NP PP then our TSGgrammar will contain the infinite set of elemen-tary trees NP?
NP PP, NP?
(NP NP PP) PP,NP?
(NP (NP NP PP) PP) PP, .
.
.
with decreas-ing but non-zero probability.However, we can represent the infinite MAP us-ing a grammar transformation inspired by Good-man (2003), which represents the MAP TSG in anequivalent finite PCFG.2 Under the transformedPCFG inference is efficient, allowing its use asthe proposal distribution in a blocked MH sam-pler.
We represent the MAP using the grammartransformation in Table 1 which separates the ne,cand P0 terms in (1) into two separate CFGs, A andB.
Grammar A has productions for every ET withne,c ?
1 which are assigned unsmoothed proba-bilities: omitting the P0 term from (1).3 GrammarB has productions for every CFG production li-censed under P0; its productions are denoted using2Backoff DOP uses a similar packed representation to en-code the set of smaller subtrees for a given elementary tree(Sima?an and Buratto, 2003), which are used to smooth itsprobability estimate.3The transform assumes inside inference.
For Viterbi re-place the probability for c?
sign(e) withn?e,c+?cP0(e|c)n?
?,c+?c.For every ET, e, rewriting c with non-zero count:c?
sign(e)n?e,cn?
?,c+?cFor every internal node ei in e with children ei,1, .
.
.
, ei,nsign(ei)?
sign(ei,1) .
.
.
sign(ei,n) 1For every nonterminal, c:c?
c?
?cn?
?,c+?cFor every pre-terminal CFG production, c?
t:c?
?
t PCFG(c?
t)For every unary CFG production, c?
a:c?
?
a PCFG(c?
a)sac?
?
a?
PCFG(c?
a)(1?
sa)For every binary CFG production, c?
ab:c?
?
ab PCFG(c?
ab)sasbc?
?
ab?
PCFG(c?
ab)sa(1?
sb)c?
?
a?b PCFG(c?
ab)(1?
sa)sbc?
?
a?b?
PCFG(c?
ab)(1?
sa)(1?
sb)Table 1: Grammar transformation rules to map a MAP TSGinto a CFG.
Production probabilities are shown to the right ofeach rule.
The sign(e) function creates a unique string sig-nature for an ET e (where the signature of a frontier node isitself) and sc is the Bernoulli probability of c being a substi-tution variable (and stopping the P0 recursion).primed (?)
nonterminals.
The rule c ?
c?
bridgesfrom A to B, weighted by the smoothing termexcluding P0, which is computed recursively viachild productions.
The remaining rules in gram-mar B correspond to every CFG production in theunderlying PCFG base distribution, coupled withthe binary decision whether or not nonterminalchildren should be substitution sites (frontier non-terminals).
This choice affects the rule probabilityby including a s or 1 ?
s factor, and child sub-stitution sites also function as a bridge back fromgrammar B to A.
In this way there are often twoequivalent paths to reach the same chart cell usingthe same elementary tree ?
via grammar A usingobserved TSG productions and via grammar B us-ing P0 backoff; summing these yields the desirednet probability.Figure 2 shows an example of the transforma-tion of an elementary tree with non-zero count,ne,c ?
1, into the two types of CFG rules.
Bothparts are capable of parsing the string NP, saw, NPinto a S, as illustrated in Figure 3; summing theprobability of both analyses gives the model prob-ability from (1).
Note that although the probabili-ties exactly match the true model for a single ele-mentary tree, the probability of derivations com-posed of many elementary trees may not matchbecause the model?s caching behaviour has beensuppressed, i.e., the counts, n, are not incrementedduring the course of a derivation.For training we define the MH sampler as fol-lows.
First we estimate the MAP grammar over227S?
NP VP{V{saw},NP}n?e,Sn?
?,S+?SVP{V{saw},NP} ?
V{saw} NP 1V{saw} ?
saw 1S?
S?
?Sn??,S+?SS??
NP VP?
PCFG(S?
NP VP)sNP (1?
sV P )VP??
V?
NP PCFG(VP?
V NP)(1?
sV )sNPV??
saw PCFG(V?
saw)Figure 2: Example of the transformed grammar for the ET(S NP (VP (V saw) NP)).
Taking the product of the rulescores above the line yields the left term in (1), and the prod-uct of the scores below the line yields the right term.SS{NP,{VP{V{hates}},NP}}NPGeorgeVP{V{hates}},NPV{hates}hatesNPbroccoliSS?NPGeorgeVP?V?hatesNPbroccoliFigure 3: Example trees under the grammar transform, whichboth encode the same TSG derivation from Figure 1.
The lefttree encodes that the S?
NP (VP (V hates) NP elementarytree was drawn from the cache, while for the right tree thissame elementary tree was drawn from the base distribution(the left and right terms in (1), respectively).the derivations of training corpus excluding thecurrent tree, which we represent using the PCFGtransformation.
The next step is to sample deriva-tions for a given tree, for which we use a con-strained variant of the inside algorithm (Lari andYoung, 1990).
We must ensure that the TSGderivation produces the given tree, and thereforeduring inside inference we only consider spansthat are constituents in the tree and are labelledwith the correct nonterminal.
Nonterminals aresaid to match their primed and signed counter-parts, e.g., NP?
and NP{DT,NN{car}} both matchNP.
Under the tree constraints the time complex-ity of inside inference is linear in the length of thesentence.
A derivation is then sampled from theinside chart using a top-down traversal (Johnsonet al, 2007), and converted back into its equiva-lent TSG derivation.
The derivation is scored withthe true model and accepted or rejected using theMH test; accepted samples then replace the cur-rent derivation for the tree, and rejected samplesleave the previous derivation unchanged.
Thesesteps are then repeated for another tree in the train-ing set, and the process is then repeated over thefull training set many times.Parsing The grammar transform is not only use-ful for training, but also for parsing.
To parse asentence we sample a number of TSG derivationsfrom the MAP which are then accepted or rejectedinto the full model using a MH step.
The samplesare obtained from the same transformed grammarbut adapting the algorithm for an unsupervised set-ting where parse trees are not available.
For thiswe use the standard inside algorithm applied tothe sentence, omitting the tree constraints, whichhas time complexity cubic in the length of the sen-tence.
We then sample a derivation from the in-side chart and perform the MH acceptance test.This setup is theoretically more appealing than ourprevious approach in which we truncated the ap-proximation grammar to exclude most of the zerocount rules (Cohn et al, 2009).
We found thatboth the maximum probability derivation and treewere considerably worse than a tree constructedto maximise the expected number of correct CFGrules (MER), based on Goodman?s (2003) algo-rithm for maximising labelled recall.
For this rea-son we the MER parsing algorithm using sampledMonte Carlo estimates for the marginals over CFGrules at each sentence span.4 ExperimentsWe tested our model on the Penn treebank usingthe same data setup as Cohn et al (2009).
Specifi-cally, we used only section 2 for training and sec-tion 22 (devel) for reporting results.
Our modelswere all sampled for 5k iterations with hyperpa-rameter inference for ?c and sc ?
c ?
N , but incontrast to our previous approach we did not useannealing which we did not find to help general-isation accuracy.
The MH acceptance rates werein excess of 99% across both training and parsing.All results are averages over three runs.For training the blocked MH sampler exhibitsfaster convergence than the local Gibbs sam-pler, as shown in Figure 4.
Irrespective of theinitialisation the blocked sampler finds higherlikelihood states in many fewer iterations (thesame trend continues until iteration 5k).
To befair, the blocked sampler is slower per iteration(roughly 50% worse) due to the higher overheadsof the grammar transform and performing dy-namic programming (despite nominal optimisa-tion).4 Even after accounting for the time differ-4The speed difference diminishes with corpus size: onsections 2?22 the blocked sampler is only 19% slower per2280 100 200 300 400 500?330000?325000?320000?315000?310000?305000iterationlog likelihoodBlock maximal initBlock minimal initLocal minimal initLocal maximal initFigure 4: Training likelihood vs. iteration.
Each samplingmethod was initialised with both minimal and maximal ele-mentary trees.Training truncated transformLocal minimal init 77.63 77.98Local maximal init 77.19 77.71Blocked minimal init 77.98 78.40Blocked maximal init 77.67 78.24Table 2: Development F1 scores using the truncated pars-ing algorithm and the novel grammar transform algorithm forfour different training configurations.ence the blocked sampler is more effective than thelocal Gibbs sampler.
Training likelihood is highlycorrelated with generalisation F1 (Pearson?s cor-relation efficient of 0.95), and therefore improvingthe sampler convergence will have immediate ef-fects on performance.Parsing results are shown in Table 2.5 Theblocked sampler results in better generalisation F1scores than the local Gibbs sampler, irrespective ofthe initialisation condition or parsing method used.The use of the grammar transform in parsing alsoyields better scores irrespective of the underlyingmodel.
Together these results strongly advocatethe use of the grammar transform for inference ininfinite TSGs.We also trained the model on the standard Penntreebank training set (sections 2?21).
We ini-tialised the model with the final sample from arun on the small training set, and used the blockedsampler for 6500 iterations.
Averaged over threeruns, the test F1 (section 23) was 85.3 an improve-iteration than the local sampler.5Our baseline ?Local maximal init?
slightly exceeds pre-viously reported score of 76.89% (Cohn et al, 2009).ment over our earlier 84.0 (Cohn et al, 2009)although still well below state-of-the-art parsers.We conjecture that the performance gap is due tothe model using an overly simplistic treatment ofunknown words, and also a further mixing prob-lems with the sampler.
For the full data set thecounts are much larger in magnitude which leadsto stronger modes.
The sampler has difficulty es-caping such modes and therefore is slower to mix.One way to solve the mixing problem is for thesampler to make more global moves, e.g., withtable label resampling (Johnson and Goldwater,2009) or split-merge (Jain and Neal, 2000).
An-other way is to use a variational approximation in-stead of MCMC sampling (Wainwright and Jor-dan, 2008).5 DiscussionWe have demonstrated how our grammar trans-formation can implicitly represent an exponentialspace of tree fragments efficiently, allowing usto build a sampler with considerably better mix-ing properties than a local Gibbs sampler.
Thesame technique was also shown to improve theparsing algorithm.
These improvements are inno way limited to our particular choice of a TSGparsing model, many hierarchical Bayesian mod-els have been proposed which would also permitsimilar optimised samplers.
In particular mod-els which induce segmentations of complex struc-tures stand to benefit from this work; Examplesinclude the word segmentation model of Goldwa-ter et al (2006) for which it would be trivial toadapt our technique to develop a blocked sampler.Hierarchical Bayesian segmentation models havealso become popular in statistical machine transla-tion where there is a need to learn phrasal transla-tion structures that can be decomposed at the wordlevel (DeNero et al, 2008; Blunsom et al, 2009;Cohn and Blunsom, 2009).
We envisage similarrepresentations being applied to these models toimprove their mixing properties.A particularly interesting avenue for further re-search is to employ our blocked sampler for un-supervised grammar induction.
While it is diffi-cult to extend the local Gibbs sampler to the casewhere the tree is not observed, the dynamic pro-gram for our blocked sampler can be easily usedfor unsupervised inference by omitting the treematching constraints.229ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP (ACL-IJCNLP), pages 782?790, Suntec, Singapore, Au-gust.Rens Bod, Remko Scha, and Khalil Sima?an, editors.2003.
Data-oriented parsing.
Center for the Studyof Language and Information - Studies in Computa-tional Linguistics.
University of Chicago Press.Trevor Cohn and Phil Blunsom.
2009.
A Bayesianmodel of syntax-directed tree to string grammar in-duction.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 352?361, Singapore, August.Trevor Cohn, Sharon Goldwater, and Phil Blun-som.
2009.
Inducing compact but accurate tree-substitution grammars.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL),pages 548?556, Boulder, Colorado, June.John DeNero, Alexandre Bouchard-Co?te?, and DanKlein.
2008.
Sampling alignment structure undera Bayesian translation model.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 314?323, Honolulu,Hawaii, October.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, Gibbs distributions and the Bayesianrestoration of images.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 6:721?741.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 673?680,Sydney, Australia, July.Joshua Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod et al (Bod et al, 2003),chapter 8.Sonia Jain and Radford M. Neal.
2000.
A split-mergeMarkov chain Monte Carlo procedure for the Dirich-let process mixture model.
Journal of Computa-tional and Graphical Statistics, 13:158?182.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 317?325,Boulder, Colorado, June.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Proceedings ofHuman Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 139?146,Rochester, NY, April.Karim Lari and Steve J.
Young.
1990.
The esti-mation of stochastic context-free grammars usingthe inside-outside algorithm.
Computer Speech andLanguage, 4:35?56.Khalil Sima?an and Luciano Buratto.
2003.
Backoffparameter estimation for the dop model.
In NadaLavrac, Dragan Gamberger, Ljupco Todorovski, andHendrik Blockeel, editors, ECML, volume 2837 ofLecture Notes in Computer Science, pages 373?384.Springer.Martin J Wainwright and Michael I Jordan.
2008.Graphical Models, Exponential Families, and Vari-ational Inference.
Now Publishers Inc., Hanover,MA, USA.230
