Multi-Criteria-based Active Learning for Named Entity RecognitionDan Shen?
?1 Jie Zhang??
Jian Su?
Guodong Zhou?
Chew-Lim Tan??
Institute for Infocomm Technology21 Heng Mui Keng TerraceSingapore 119613?
Department of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg{shendan,zhangjie,tancl}@comp.nus.edu.sg1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germanydshen@coli.uni-sb.deAbstractIn this paper, we propose a multi-criteria -based active learning approach and effec-tively apply it to named entity recognition.Active learning targets to minimize thehuman annotation efforts by selecting ex-amples for labeling.
To maximize the con-tribution of the selected examples, weconsider the multiple criteria: informative-ness, representativeness and diversity  andpropose measures to quantify them.
Morecomprehensively, we incorporate all thecriteria using two selection strategies, bothof which result in less labeling cost thansingle-criterion-based method.
The resultsof the named entity recognition in bothMUC-6 and GENIA show that the labelingcost can be reduced by at least 80% with-out degrading the performance.1 IntroductionIn the machine learning approaches of natural lan-guage processing (NLP), models are generallytrained on large annotated corpus.
However, anno-tating such corpus is expensive and time-consuming, which makes it difficult to adapt anexisting model to a new domain.
In order to over-come this difficulty, active learning (sample selec-tion) has been studied in more and more NLPapplications such as POS tagging (Engelson andDagan 1999), information extraction (Thompson etal.
1999), text classif ication (Lewis and Catlett1994; McCallum and Nigam 1998; Schohn andCohn 2000; Tong and Koller 2000; Brinker 2003),statistical parsing (Thompson et al 1999; Tang etal.
2002; Steedman et al 2003), noun phrasechunking (Ngai and Yarowsky 2000), etc.Active learning is based on the assumption thata small number of annotated examples and a largenumber of unannotated examples are available.This assumption is valid in most NLP tasks.
Dif-ferent from supervised learning in which the entirecorpus are labeled manually, active learning is toselect the most useful example for labeling and addthe labeled example  to training set to retrain model.This procedure is repeated until the model achievesa certain level of performance.
Practically, a batchof examples are selected at a time, called batched-based sample selection (Lewis and Catlett 1994)since it is time consuming to retrain the model ifonly one new example is added to the training set.Many existing work in the area focus on two ap-proaches: certainty-based methods (Thompson etal.
1999; Tang et al 2002; Schohn and Cohn 2000;Tong and Koller 2000; Brinker 2003) and commit-tee-based methods (McCallum and Nigam 1998;Engelson and Dagan 1999; Ngai and Yarowsky2000) to select the most informative examples forwhich the current model are most uncertain.Being the first piece of work on active learningfor name entity recognition (NER) task, we targetto minimize the human annotation efforts yet stillreaching the same level of performance as a super-vised learning approach.
For this purpose, wemake a more comprehensive consideration on thecontribution of individual examples, and more im-portantly maximizing the contribution of a batchbased on three criteria : informativeness, represen-tativeness and diversity.First, we propose three scoring functions toquantify the informativeness of an example , whichcan be used to select the most uncertain examples.Second, the representativeness measure is furtherproposed to choose the examples representing themajority.
Third, we propose two diversity consid-erations (global and local) to avoid repetitionamong the examples of a batch.
Finally, two com-bination strategies with the above three criteria areproposed to reach the maximum effectiveness onactive learning for NER.We build our NER model using Support Vec-tor Machines (SVM).
The experiment shows thatour active learning methods achieve a promisingresult in this NER task.
The results in both MUC-6 and GENIA show that the amount of the labeledtraining data can be reduced by at least 80% with-out degrading the quality of the named entity rec-ognizer.
The contributions not only come from theabove measures, but also the two sample selectionstrategies which effectively incorporate informa-tiveness, representativeness and diversity criteria.To our knowledge, it is the first work on consider-ing the three criteria all together for active learning.Furthermore, such measures and strategies can beeasily adapted to other active learning tasks as well.2 Multi-criteria for NER Active LearningSupport Vector Machines (SVM) is a powerfulmachine learning method, which has been appliedsuccessfully in NER tasks, such as (Kazama et al2002; Lee et al 2003).
In this paper, we apply ac-tive learning methods to a simple  and effectiveSVM model to recognize one class of names at atime, such as protein names, person names, etc.
InNER, SVM is to classify a word into positive class?1?
indicating that the word is a part of an entity,or negative class ?-1?
indicating that the word isnot a part of an entity.
Each word in SVM is rep-resented as a high-dimensional feature vector in-cluding surface word information, orthographicfeatures, POS feature and semantic trigger features(Shen et al 2003).
The semantic trigger featuresconsist of some special head nouns for an entityclass which is supplied by users.
Furthermore, awindow (size = 7), which represents the local con-text of the target word w, is also used to classify w.However, for active learning in NER, it is notreasonable to select a single word without contextfor human to label.
Even if we require human tolabel a single word, he has to make an additioneffort to refer to the context of the word.
In ouractive learning process, we select a word sequencewhich consists of a machine-annotated named en-tity and its context rather than a single word.Therefore, all of the measures we propose for ac-tive learning should be applied to the machine-annotated named entities and we have to furtherstudy how to extend the measures for words tonamed entities.
Thus, the active learning in SVM-based NER will be more complex than that in sim-ple classification tasks, such as text classif icationon which most SVM active learning works areconducted (Schohn and Cohn 2000; Tong andKoller 2000; Brinker 2003).
In the next part, wewill introduce informativeness, representativenessand diversity measures for the SVM-based NER.2.1 InformativenessThe basic idea of informativeness criterion is simi-lar to certainty-based sample selection methods,which have been used in many previous works.
Inour task, we use a distance-based measure toevaluate the informativeness of a word and extendit to the measure of an entity using three scoringfunctions.
We prefer the examples with high in-formative degree for which the current model aremost uncertain.2.1.1 Informativeness Measure for WordIn the simplest linear form, training SVM is to finda hyperplane that can separate the posit ive andnegative examples in training set with maximummargin.
The margin is defined by the distance ofthe hyperplane to the nearest of the positive andnegative examples.
The training examples whichare closest to the hyperplane are called supportvectors.
In SVM, only the support vectors are use-ful for the classification, which is different fromstatistical models.
SVM training is to get thesesupport vectors and their weights from training setby solving quadratic programming problem.
Thesupport vectors can later be used to classify the testdata.Intuitively, we consider the informativeness ofan example  as how it can make effect on the sup-port vectors by adding it to training set.
An exam-ple may be informative for the learner if thedistance of its feature vector to the hyperplane isless than that of the support vectors to the hyper-plane (equal to 1).
This intuition is also justifiedby (Schohn and Cohn 2000; Tong and Koller 2000)based on a version space analysis.
They state thatlabeling an example that lies on or close to the hy-perplane is guaranteed to have an effect on the so-lution.
In our task, we use the distance to measurethe informativeness of an example.The distance of a word?s feature vector to thehyperplane is computed as follows:1( ) ( , )Ni i iiDist y k ba== +?w s wwhere w is the feature vector of the word, ai, yi, sicorresponds to the weight, the class and the featurevector of the ith support vector respectively.
N isthe number of the support vectors in current model.We select the example with minimal Dist,which indicates that it comes closest to the hyper-plane in feature space.
This example is consideredmost informative for current model.2.1.2 Informativeness Measure for NamedEntityBased on the above informativeness measure for aword, we compute the overall informativeness de-gree of a named entity NE.
In this paper, we pro-pose three scoring functions as follows.
Let NE =w1?wN in which wi is the feature vector of the ithword of NE.?
Info_Avg: The informativeness of NE isscored by the average distance of the words inNE to the hyperplane.
( ) 1 ( )iiN EInfo NE Dist?= - ?wwwhere, wi is the feature vector of the ith word inNE.?
Info_Min: The informativeness of NE isscored by the minimal distance of the words inNE.
( ) 1 { ( )}iiNEInfo NE Min Dist?= -ww?
Info_S/N: If the distance of a word to the hy-perplane is less than a threshold a (= 1 in ourtask), the word is considered with short dis-tance.
Then, we compute the proportion of thenumber of words with short distance to the to-tal number of words in the named entity anduse this proportion to quantify the informa-tiveness of the named entity.
( ( ) )( ) iiN ENUM DistInfo NENa?<= wwIn Section 4.3, we will evaluate the effective-ness of these scoring functions.2.2 RepresentativenessIn addition to the most informative example, wealso prefer the most representative example.
Therepresentativeness of an example can be evaluatedbased on how many examples there are similar ornear to it.
So, the examples with high representa-tive degree are less likely to be an outlier.
Addingthem to the training set will have effect on a largenumber of unlabeled examples.
There are only afew works considering this selection criterion(McCallum and Nigam 1998; Tang et al 2002) andboth of them are specific to their tasks, viz.
textclassification and statistical parsing.
In this section,we compute the simila rity between words using ageneral vector-based measure, extend this measureto named entity level using dynamic time warpingalgorithm and quantify the representativeness of anamed entity by its density.2.2.1 Similarity Measure  between WordsIn general vector space model, the similarity be-tween two vectors may be measured by computingthe cosine value of the angle between them.
Thesmaller the angle is, the more similar between thevectors are.
This measure, called cosine-similaritymeasure, has been widely used in information re-trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).In our task, we also use it to quantify the similaritybetween two words.
Particularly, the calculation inSVM need be projected to a higher dimensionalspace by using a certain kernel function ( , )i jK w w .Therefore, we adapt the cosine-similarity measureto SVM as follows:( , )( , )( , ) ( , )i ji ji i j jkSimk k=w ww ww w w wwhere, wi and wj are the feature vectors of thewords i and j.
This calculation is also supported by(Brinker 2003)?s work.
Furthermore, if we use thelinear kernel ( , )i j i jk = ?w w w w , the measure isthe same as the traditional cosine similarity meas-ure cos i ji jq?=?w ww wand may be regarded as ageneral vector-based similarity measure.2.2.2 Similarity Meas ure between Named En-titiesIn this part, we compute the similarity between twomachine-annotated named entities given the simi-larities between words.
Regarding an entity as aword sequence, this work is analogous to thealignment of two sequences.
We employ the dy-namic time warping (DTW) algorithm (Rabiner etal.
1978) to find an optimal alignment between thewords in the sequences which maximize the accu-mulated similarity degree between the sequences.Here, we adapt it to our task.
A sketch of themodified algorithm is as follows.Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) andNE2 = w21w22?w2m?w2M, (m = 1,?, M) denote twoword sequences to be matched.
NE1 and NE2 con-sist of M and N words respectively.
NE1(n) = w1nand NE2(m) = w2m.
A similarity value Sim(w1n ,w2m)has been known for every pair of words (w1n,w2m)within NE1 and NE2.
The goal of DTW is to find apath, m = map(n), which map n onto the corre-sponding m such that the accumulated similaritySim* along the path is maximized.1 2{ ( )} 1* { ( ( ), ( ( ))}Nm a p n nSim M a x Sim N E n N E m a p n== ?A dynamic programming method is used to deter-mine the optimum path map(n).
The accumulatedsimilarity SimA to any grid point (n, m) can be re-cursively calculated as1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -Finally, * ( , )ASim Sim N M=Certainly, the overall similarity measure Sim*has to be normalized as longer sequences normallygive higher similarity value.
So, the similarity be-tween two sequences NE1 and NE2 is calculated as1 2*( , )( , )SimSim NE NEMax N M=2.2.3 Representativeness Measure for NamedEntityGiven a set of machine-annotated named entitiesNESet = {NE1, ?
, NEN}, the representativeness ofa named entity NEi in NESet is quantified by itsdensity.
The density of NEi is defined as the aver-age similarity between NEi and all the other enti-ties NEj in NESet as follows.
( , )( )1i jj iiSim NE NEDensity N EN?=-?If NEi has the largest density among all the entitiesin NESet, it can be regarded as the centroid of NE-Set and also the most representative examples inNESet.2.3 DiversityDiversity criterion is to maximize the training util-ity of a batch.
We prefer the batch in which theexamples have high variance to each other.
Forexample, given the batch size 5, we try not to se-lect five repetitious examples at a time.
To ourknowledge, there is only one work (Brinker 2003)exploring this criterion.
In our task, we proposetwo methods: local and global, to make the exam-ples diverse enough in a batch.2.3.1 Global ConsiderationFor a global consideration, we cluster all namedentities in NESet based on the similarity measureproposed in Section 2.2.2.
The named entities inthe same cluster may be considered similar to eachother, so we will select the named entities fromdifferent clusters at one time.
We employ a K-means clustering algorithm (Jelinek 1997), whichis shown in Figure 1.Given:NESet = {NE1, ?
, NEN}Suppose:The number of clusters is KInitialization:Randomly equally partition {NE1, ?
, NEN} into Kinitial clusters Cj (j = 1, ?
, K).Loop until the number of changes for the centroids ofall clusters is less than a threshold?
Find the centroid of each cluster Cj (j = 1, ?
, K).arg ( ( , ))j i jj iNE C NE CNECent max Sim NE NE?
?= ??
Repartition {NE1, ?
, NEN} into K clusters.
NEiwill be assigned to Cluster Cj if( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j?
?Figure 1: Global Consideration for Diversity: K-Means Clustering algorithmIn each round, we need to compute the pair-wise similarities within each cluster to get the cen-troid of the cluster.
And then, we need to computethe similarities between each example and all cen-troids to repartition the examples.
So, the algo-rithm is time-consuming.
Based on the assumptionthat N examples are uniformly distributed betweenthe K clusters, the time complexity of the algo-rithm is about O(N2/K+NK) (Tang et al 2002).
Inone of our experiments, the size of the NESet (N) isaround 17000 and K is equal to 50, so the timecomplexity is about O(106).
For efficiency, wemay filter the entities in NESet before clusteringthem, which will be further discussed in Section 3.2.3.2 Local ConsiderationWhen selecting a machine-annotated named entity,we compare it with all previously selected namedentities in the current batch.
If the similarity be-tween them is above a threshold ?, this examplecannot be allowed to add into the batch.
The orderof selecting examples is based on some measure,such as informativeness measure, representative-ness measure or their combination.
This local se-lection method is shown in Figure 2.
In this way,we avoid selecting too similar examples (similarityvalue ?
?)
in a batch.
The threshold ?
may be theaverage similarity between the examples in NESet.Given:NESet = {NE1, ?
, NEN}BatchSet with the maximal size K.Initialization:BatchSet  = emptyLoop until BatchSet is full?
Select NEi based on some measure from NESet.?
RepeatFlag = false;?
Loop from j = 1 to CurrentSize(BatchSet)If ( , )i jSim NE NE b?
ThenRepeatFlag = true;Stop the Loop;?
If RepeatFlag == false Thenadd NEi into BatchSet?
remove NEi from NESetFigure 2: Local Consideration for DiversityThis consideration only requires O(NK+K2)computational time.
In one of our experiments (N?17000 and K = 50), the time complexity is aboutO(105).
It is more efficient than clustering algo-rithm described in Section 2.3.1.3 Sample Selection strategiesIn this section, we will study how to combine andstrike a proper balance between these criteria, viz.informativeness, representativeness and diversity,to reach the maximum effectiveness on NER activelearning.
We build two strategies to combine themeasures proposed above.
These strategies arebased on the varying priorities of the criteria andthe varying degrees to satisfy the criteria.?
Strategy 1: We first consider the informative-ness criterion.
We choose m examples with themost informativeness score from NESet to an in-termediate set called INTERSet.
By this pre-selecting, we make the selection process faster inthe later steps since the size of INTERSet is muchsmaller than that of NESet.
Then we cluster theexamples in INTERSet and choose the centroid ofeach cluster into a batch called BatchSet.
The cen-troid of a cluster is the most representative exam-ple in that cluster since it has the largest density.Furthermore, the examples in different clustersmay be considered diverse to each other.
By thismeans, we consider representativeness and diver-sity criteria at the same time.
This strategy isshown in Figure 3.
One limitation of this strategyis that clustering result may not reflect the distribu-tion of whole sample space since we only clusteron INTERSet for efficiency.
The other is that sincethe representativeness of an example is only evalu-ated on a cluster.
If the cluster size is too small,the most representative example in this cluster maynot be representative in the whole sample space.Given:NESet = {NE1, ?
, NEN}BatchSet with the maximal size K.INTERSet with the maximal size MSteps :?
BatchSet  = ??
INTERSet = ??
Select M entities with most Info score from NESetto INTERSet.?
Cluster the entities in INTERSet into K clusters?
Add the centroid entity of each cluster to BatchSetFigure 3: Sample Selection Strategy 1?
Strategy 2: (Figure 4) We combine the infor-mativeness and representativeness criteria  usingthe functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , inwhich the Info and Density  value of NEi are nor-malized first.
The individual importance of eachcriterion in this function is adjusted by the trade-off parameter l ( 0 1l?
? )
(set to 0.6 in ourexperiment).
First, we select a candidate exampleNEi with the maximum value of this function fromNESet.
Second, we consider diversity criterionusing the local method in Section 3.3.2.
We addthe candidate example NEi to a batch only if NEi isdifferent enough from any previously selected ex-ample in the batch.
The threshold ?
is set to theaverage pair-wise similarity of the entities in NE-Set.Given:NESet = {NE1, ?
, NEN}BatchSet with the maximal size K.Initialization:BatchSet  = ?Loop until BatchSet is full?
Select NEi which have the maximum value for thecombination function between Info score and Den-sity socre from NESet.arg ( ( ) (1 ) ( ))ii i iN E NESetN E Max Info NE Density NEl l?= + -?
RepeatFlag = false;?
Loop from j = 1 to CurrentSize(BatchSet)If ( , )i jSim NE NE b?
ThenRepeatFlag = true;Stop the Loop;?
If RepeatFlag == false Thenadd NEi into BatchSet?
remove NEi from NESetFigure 4: Sample Selection Strategy 24 Experimental Results and Analysis4.1 Experiment SettingsIn order to evaluate the effectiveness of our selec-tion strategies, we apply them to recognize protein(PRT) names in biomedical domain using GENIAcorpus V1.1 (Ohta et al 2002) and person (PER),location (LOC), organization (ORG) names innewswire domain using MUC-6 corpus.
First, werandomly split the whole corpus into three parts: aninitial training set to build an in itial model, a testset to evaluate the performance of the model andan unlabeled set to select examples.
The size ofeach data set is shown in Table 1.
Then, iteratively,we select a batch of examples following the selec-tion strategies proposed, require human experts tolabel them and add them into the training set.
Thebatch size K = 50 in GENIA and 10 in MUC-6.Each example is defined as a machine-recognizednamed entity and its context words (previous 3words and next 3 words).Domain Class Corpus Initial Training Set Test Set Unlabeled SetBiomedical PRT GENIA1.1 10 sent.
(277 words) 900 sent.
(26K words) 8004 sent.
(223K words)PER 5 sent.
(131 words) 7809 sent.
(157K words)LOC 5 sent.
(130 words) 7809 sent.
(157K words)NewswireORGMUC-65 sent.
(113 words)602 sent.
(14K words)7809 sent.
(157K words)Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG)The goal of our work is to minimize the humanannotation effort to learn a named entity recognizerwith the same performance level as supervisedlearning.
The performance of our model is evalu-ated using ?precision/recall/F-measure?.4.2 Overall Result in GENIA and MUC-6In this section, we evaluate our selection strategiesby comparing them with a random selectionmethod, in which a batch of examples is randomlyselected iteratively, on GENIA and MUC-6 corpus.Table 2 shows the amount of training data neededto achieve the performance of supervised learningusing various selection methods, viz.
Random,Strategy1 and Strategy2.
In GENIA, we find:?
The model achieves 63.3 F-measure using 223Kwords in the supervised learning.?
The best performer is Strategy2 (31K words),requiring less than 40% of the training data thatRandom (83K words) does and 14% of the train-ing data that the supervised learning does.?
Strategy1 (40K words) performs slightly worsethan Strategy2, requiring 9K more words.
It isprobably because Strategy1 cannot avoid select-ing outliers if a cluster is too small.?
Random (83K words) requires about 37% of thetraining data that the supervised learning does.
Itindicates that only the words in and around anamed entity are useful for classification and thewords far from the named entity may not behelpful.Class Supervised Random Strategy1 Strategy2PRT 223K (F=63.3) 83K 40K 31KPER 157K (F=90.4) 11.5K 4.2K 3.5KLOC 157K (F=73.5) 13.6K 3.5K 2.1KORG 157K (F=86.0) 20.2K 9.5K 7.8KTable 2: Overall Result in GENIA and MUC-6Furthermore, when we apply our model to news-wire domain (MUC-6) to recognize person, loca-tion and organization names, Strategy1 andStrategy2 show a more promising result by com-paring with the supervised learning and Random,as shown in Table 2.
On average, about 95% ofthe data can be reduced to achieve the same per-formance with the supervised learning in MUC-6.It is probably because NER in the newswire do-main is much simpler than that in the biomedicaldomain (Shen et al 2003) and named entities areless and distributed much sparser in the newswiretexts than in the biomedical texts.4.3 Effectiveness of Informativeness-basedSelection MethodIn this section, we investigate the effectiveness ofinformativeness criterion in NER task.
Figure 5shows a plot of training data size versus F-measureachieved by the informativeness-based measures inSection 3.1.2: Info_Avg, Info_Min  and Info_S/N aswell as Random.
We make the comparisons inGENIA corpus.
In Figure 5, the horizontal line isthe performance level (63.3 F-measure) achievedby supervised learning (223K words).
We findthat the three informativeness-based measures per-form similarly and each of them outperforms Ran-dom.
Table 3 highlights the various data sizes toachieve the peak performance using these selectionmethods.
We find that Random (83K words) onaverage requires over 1.5 times as much as data toachieve the same performance as the informative-ness-based selection methods (52K words).0.50.550.60.650 20 40 60 80K wordsFSupervisedRandomInfo_MinInfo_S/NInfo_AvgFigure 5: Active learning curves: effectiveness of the three in-formativeness-criterion-based selections comparing with theRandom selection.Supervised Random Info_Avg Info_Min Info_ S/N223K 83K 52.0K 51.9K 52.3KTable 3: Training data sizes for various selection methods toachieve the same performance level as the supervised learning4.4 Effectiveness of Two Sample SelectionStrategiesIn addition to the informativeness criterion, wefurther incorporate representativeness and diversitycriteria into active learning using two strategiesdescribed in Section 3.
Comparing the two strate-gies with the best result of the single-criterion-based selection methods Info_Min , we are to jus-tify that representativeness and diversity are alsoimportant factors for active learning.
Figure 6shows the learning curves for the various methods:Strategy1, Strategy2 and Info_Min.
In the begin-ning iterations (F-measure < 60), the three methodsperformed similarly.
But with the larger trainingset, the efficiencies of Stratety1 and Strategy2 be-gin to be evident.
Table 4 highlights the final re-sult of the three methods.
In order to reach theperformance of supervised learning, Strategy1(40K words) and Strategyy2 (31K words) requireabout 80% and 60% of the data that Info_Min(51.9K) does.
So we believe the effective combi-nations of informativeness, representativeness anddiversity will help to learn the NER model morequickly and cost less in annotation.0.50.550.60.650 20 40 60 K wordsFSupervisedInfo_MinStrategy1Strategy2Figure 6: Active learning curves: effectiveness of the twomulti-criteria-based selection strategies comparing with theinformativeness-criterion-based selection (Info_Min).Info_Min Strategy1 Strategy251.9K 40K 31KTable 4: Comparisons of training data sizes for the multi-criteria-based selection strategies and the informativeness-criterion-based selection (Info_Min) to achieve the same per-formance level as the supervised learning.5 Related WorkSince there is no study on active learning for NERtask previously, we only introduce general activelearning methods here.
Many existing active learn-ing methods are to select the most uncertain exam-ples using various measures (Thompson et al 1999;Schohn and Cohn 2000; Tong and Koller 2000;Engelson and Dagan 1999; Ngai and Yarowsky2000).
Our informativeness-based measure issimilar to these works.
However these works justfollow a single criterion.
(McCallum and Nigam1998; Tang et al 2002) are the only two worksconsidering the representativeness criterion in ac-tive learning.
(Tang et al 2002) use the densityinformation to weight the selected examples whilewe use it to select examples.
Moreover, the repre-sentativeness measure we use is relatively generaland easy to adapt to other tasks, in which the ex-ample selected is a sequence of words, such as textchunking, POS tagging, etc.
On the other hand,(Brinker 2003) first incorporate diversity in activelearning for text classification.
Their work is simi-lar to our local consideration in Section 2.3.2.However, he didn?t further explore how to avoidselecting outliers to a batch.
So far, we haven?tfound any previous work integrating the informa-tiveness, representativeness and diversity all to-gether.6 Conclusion and Future WorkIn this paper, we study the active learning in amore complex NLP task, named entity recognition.We propose a multi-criteria -based approach to se-lect examples based on their informativeness, rep-resentativeness and diversity, which areincorporated all together by two strategies (localand global).
Experiments show that, in both MUC-6 and GENIA, both of the two strategies combin-ing the three criteria outperform the single criterion(informativeness).
The labeling cost can be sig-nificantly reduced by at least 80% comparing withthe supervised learning.
To our best knowledge,this is not only the first work to report the empiri-cal results of active learning for NER, but also thefirst work to incorporate the three criteria all to-gether for selecting examples.Although the current experiment results arevery promising, some parameters in our experi-ment, such as the batch size K and the l in thefunction of strategy 2, are decided by our experi-ence in the domain.
In practical application, theoptimal value of these parameters should be de-cided automatically based on the training process.Furthermore, we will study how to overcome thelimitation of the strategy 1 discussed in Section 3by using more effective clustering algorithm.
An-other interesting work is to study when to stop ac-tive learning.ReferencesR.
Baeza-Yates and B. Ribeiro-Neto.
1999.
Mod-ern Information Retrieval.
ISBN 0-201-39829-X.K.
Brinker.
2003.
Incorporating Diversity in Ac-tive Learning with Support Vector Machines.
InProceedings of ICML, 2003.S.
A. Engelson and I. Dagan.
1999.
Committee-Based Sample Selection for Probabilistic Classi-fiers.
Journal of Artifical Intelligence Research.F.
Jelinek.
1997.
Statistical Methods for SpeechRecognition.
MIT Press.J.
Kazama, T. Makino, Y. Ohta and J. Tsujii.
2002.Tuning Support Vector Machines for Biomedi-cal Named Entity Recognition.
In Proceedingsof the ACL2002 Workshop on NLP in Biomedi-cine.K.
J. Lee, Y. S. Hwang and H. C. Rim.
2003.
Two-Phase Biomedical NE Recognition based onSVMs.
In Proceedings of the ACL2003 Work-shop on NLP in Biomedicine.D.
D. Lewis and J. Catlett.
1994.
HeterogeneousUncertainty Sampling for Supervised Learning.In Proceedings of ICML, 1994.A.
McCallum and K. Nigam.
1998.
Employing EMin Pool-Based Active Learning for Text Classi-fication.
In Proceedings of ICML, 1998.G.
Ngai and D. Yarowsky.
2000.
Rule Writing orAnnotation: Cost-efficient Resource Usage forBase Noun Phrase Chunking.
In Proceedings ofACL, 2000.T.
Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii.2002.
The GENIA corpus: An annotated re-search abstract corpus in molecular biology do-main.
In Proceedings of HLT 2002.L.
R. Rabiner, A. E. Rosenberg and S. E. Levinson.1978.
Considerations in Dynamic Time WarpingAlgorithms for Discrete Word Recognition.
InProceedings of IEEE Transactions on acoustics,speech and signal processing.
Vol.
ASSP-26,NO.6.D.
Schohn and D. Cohn.
2000.
Less is More: Ac-tive Learning with Support Vector Machines.
InProceedings of the 17th International Confer-ence on Machine Learning.D.
Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan.2003.
Effective Adaptation of a Hidden MarkovModel-based Named Entity Recognizer for Bio-medical Domain.
In Proceedings of theACL2003 Workshop on NLP in Biomedicine.M.
Steedman, R. Hwa, S. Clark, M. Osborne, A.Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker andJ.
Crim.
2003.
Example Selection for Bootstrap-ping Statistical Parsers.
In Proceedings of HLT-NAACL, 2003.M.
Tang, X. Luo and S. Roukos.
2002.
ActiveLearning for Statistical Natural Language Pars-ing.
In Proceedings of the ACL 2002.C.
A. Thompson, M. E. Califf and R. J. Mooney.1999.
Active Learning for Natural LanguageParsing and Information Extraction.
In Proceed-ings of ICML 1999.S.
Tong and D. Koller.
2000.
Support Vector Ma-chine Active Learning with Applications to TextClassification.
Journal of Machine Learning Re-search.V.
Vapnik.
1998.
Statistical learning theory.N.Y.
:John Wiley.
