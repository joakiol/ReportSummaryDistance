Priors in Bayesian Learning of Phonological RulesSharon Goldwater and Mark JohnsonDepartment of Cognitive and Linguistic SciencesBox 1978Brown UniversityProvidence, RI 02912USA{sharon goldwater, mark johnson}@brown.eduAbstractThis paper describes a Bayesian procedure for un-supervised learning of phonological rules from anunlabeled corpus of training data.
Like Goldsmith?sLinguistica program (Goldsmith, 2004b), whoseoutput is taken as the starting point of this proce-dure, our learner returns a grammar that consists ofa set of signatures, each of which consists of a setof stems and a set of suffixes.
Our grammars dif-fer from Linguistica?s in that they also contain a setof phonological rules, specifically insertion, dele-tion and substitution rules, which permit our gram-mars to collapse far more words into a signaturethan Linguistica can.
Interestingly, the choice ofBayesian prior turns out to be crucial for obtaining alearner that makes linguistically appropriate gener-alizations through a range of different sized trainingcorpora.1 IntroductionUnsupervised learning presents unusual challengesto the field of computational linguistics.
In super-vised systems, the task of learning can often be re-stricted to finding the optimal values for the param-eters of a pre-specified model.
In contrast, an un-supervised learning system must often propose thestructure of the model itself, as well as the valuesfor any parameters in that model.
In general, thereis a trade-off between the structural complexity of amodel and its ability to explain a set of data.
Oneway to deal with this trade-off is by using Bayesianlearning techniques, where the objective functionused to evaluate the overall goodness of a systemtakes the formPr(H)Pr(D|H)where Pr(H) is the prior probability of the hypoth-esized model H , and Pr(D|H) is the likelihood ofthe data D given that model.
In a Bayesian sys-tem, we want to find the hypothesis H for whichPr(H)Pr(D|H) is highest (or, equivalently, where?
log Pr(H)?
log Pr(D|H) is lowest).
While cal-culating the likelihood of the data given a particu-lar hypothesis is generally straightforward, the moredifficult question in Bayesian learning is how to de-termine the prior probabilities of various hypothe-ses.In this paper, we compare the results of usingtwo different prior distributions for an unsupervisedlearning task in the domain of morpho-phonology.Our goal is to learn transformation rules of the formx ?
y / C , where x and y are individual charac-ters (or the empty character ) and C is some rep-resentation of the context licensing the transforma-tion.
Our input is an existing segmentation of wordsfrom the Penn Treebank (Marcus et al, 93) intostems and suffixes.
This segmentation is providedby the Linguistica morphological analyzer (Gold-smith, 2001; Goldsmith, 2004b), itself an unsuper-vised algorithm.
Using the transformation rules welearn, we are able to output a new segmentation thatmore closely matches our linguistic intuitions.1We are not the first to apply Bayesian learningtechniques for unsupervised learning of morphol-ogy and phonology.
Several other researchers havealso pursued these methods, usually within a Mini-mum Description Length (MDL) framework (Ris-sanen, 1989).
In MDL approaches, ?
log Pr(H)is taken to be proportional to the length of H insome standard encoding, and ?
log Pr(D|H) is thelength of D using the encoding specified by H .MDL-based systems have been relatively successfulfor tasks including word segmentation (de Marcken,1996; Brent and Cartwright, 1996), morphological1Since we use ordinary text, rather than phonological tran-scriptions, as input, the rules we learn are really spelling rules,not phonological rules.
We believe that the work discussedhere would be equally applicable, and possibly more success-ful, with phonological transcriptions.
However, since we wishto have an entirely unsupervised system and we require a mor-phological segmentation as input, we are currently limited bythe capabilities of Linguistica, which requires standard textualinput.
For the remainder of this paper, we use ?phonology?
and?phonological rules?
in a broad sense to include orthography aswell.Barcelona, July 2004Association for Computations LinguisticsACL Special Interest Group on Computational Phonology (SIGPHON)Proceedings of the Workshop of the?????????liftjumprollwalk.
.
.????????????????????s?ed?ing.
.
.????????
?Figure 1: An example signaturesegmentation (Goldsmith, 2001; Creutz and Lagus,2002), discovery of syllabicity and sonority (Elli-son, 1993), and learning constraints on vowel har-mony and consonant clusters (Ellison, 1994).
How-ever, our work shows that a straightforward MDLapproach, where the prior ?
log Pr(H) depends onthe length of the phonological rules and the rest ofthe grammar in the obvious way, does not result in asuccessful system for learning phonological rules.We discuss why this is so, and then present sev-eral changes that can be made to the prior in orderto learn phonological rules successfully.
Our con-clusion is that, although Bayesian techniques canbe successful for unsupervised learning of linguis-tic information, careful choice of the prior, with at-tention to both linguistic and statistical factors, isimportant.In the remainder of this paper, we first reviewthe basics behind Goldsmith?s Linguistica program,which serves as the starting point for our own work.We then explain the additional framework neces-sary for learning phonological rules, and describeour search algorithm.
In Section 5, we describe theresults of two experiments using our search algo-rithm, first with an MDL prior, then with a modifiedprior.
We discuss why the modified prior works bet-ter for our task, and implications for other Bayesianlearners.2 LinguisticaSince our algorithm is designed to take as inputa morphological analysis produced by Linguistica,we first briefly review what that analysis consists ofand how it is arrived at.
Linguistica is based on theMDL principle, which states that the optimal hy-pothesis to explain a set of data is the one that min-imizes the total number of bits required to describeboth the hypothesis and the data under that hypoth-esis.
Information theory tells us that the descriptionlength of the data under a given hypothesis is simplythe negative log likelihood of the data, so the MDLcriterion is equivalent to a Bayesian prior favoringhypotheses that can be described succintly.Linguistic hypotheses (grammars) all containsome primitive types.
Linguistica uses three primi-tive types in its grammar: stems, suffixes, and sig-natures.
2 Each signature is associated with a setof stems, and each stem is associated with exactlyone signature representing those suffixes with whichit combines freely.
For example, walk and jumpmight be associated with the signature ?.ed.ing.s?
(see Figure 1), while bad might be associated with?.ly?.
Unanalyzed words can be thought of as be-longing to the ??
signature.
A possible grammarunder this scenario consists of a set of signatures,where each signature contains a set of stems and aset of suffixes.
Rather than modeling the probabil-ity of each word in the corpus directly, the gram-mar assumes that each word consists of a stem anda (possibly empty) suffix, and assigns a probabilityto each word w according toPr(w = t + f) = Pr(?)Pr(t|?
)Pr(f |?
),where ?
is the signature containing stem t. (Wehave adopted Goldsmith?s notation here, using f todenote suffixes, t for stems, and ?
for signatures.
)Clearly, grouping words into signatures will causetheir probabilities to be modeled less well than mod-eling each word individually.
The negative log like-lihood of the corpus will therefore increase, and thisportion of the description length will grow.
How-ever, listing each word individually in the grammarrequires as many stems as there are words.
As-signing words to signatures significantly reduces thenumber of stems, and thus the length of the gram-mar.
If the stems are chosen well, then the length ofthe grammar will decrease more than the length ofthe encoded corpus will increase, leading to an over-all win.
Goldsmith (2001) provides a detailed de-scription of the exact grammar encoding and searchheuristics used to find the optimal set of stems, suf-fixes, and signatures under this type of model.Goldsmith?s algorithm is not without its prob-lems, however.
We concern ourselves here with itstendency to postulate spurious signatures in caseswhere phonological constraints operate.
For exam-ple, many English verb stems ending in e are placedin the signature ?e.ed.es.ing?, while stems not end-ing in e have the signature ?.ed.ing.s?.
This is dueto the fact that the stem-final e deletes before suf-fixes beginning in e or i.
Similarly, words like matchand index are likely to be given the signature ?.es?,whereas most nouns would be ?.s?.
The toy gram-mar G1 in Figure 2 illustrates the sort of analysisproduced by Linguistica.Goldsmith himself has noted the problem of spu-rious signatures (Goldsmith, 2004a), and recent ver-2Linguistica actually can perform prefix analysis as well assuffix analysis, but in our work we used only the suffixing func-tions.
?1 = ({work, roll}?
{, ed, ing, er})?2 = ({din, bik}?
{e, ed, ing, er})?3 = ({wait}?
{, ed, er})?4 = ({carr}?
{y, ied, ier})?5 = ({carry}?
{, ing})?6 = ({bike, booth, worker}?
{, s})?7 = ({beach, match}?
{, es})Figure 2: G1: A Sample Linguistica Grammarsions of Linguistica include some functionality de-voted to detecting allomorphs.
Superficially, ourwork may seem similar to Goldsmith?s, but in fact itis quite different.
First of all, the allomorphic vari-ation detected by Linguistica is suffix-based.
Thatis, suffixes are proposed that operate to delete cer-tain stem-final material.
For example, a suffix (e)ingcould be proposed in order to include both hopeand walk in the signature ?.(e)ing.s?.
This suf-fix is actually separate in the grammar from theordinary ing suffix, so there is no recognition ofthe fact that any occurrence of ing in any signa-ture should delete a preceding stem-final e. More-over, this approach is not really phonological, inthe sense that other suffixes beginning with i mightor might not be analyzed as deleting stem-finale.
While many languages do contain some affix-specific morpho-phonological processes, our goalhere is to find phonological rules that apply at allstem-suffix boundaries, given certain context crite-ria.A second major difference between the allomor-phy detection in Linguistica and the work presentedhere is that a Linguistica suffix such as (e)ing is as-sumed to delete any stem-final e, without exception.While this assumption may be valid in this case,there are other suffixes and phonological processesthat are not categorical.
For example, the Englishplural s requires insertion of an e after certain stems,including those ending in x or s. However, thereis no simple way to describe the context for thisrule based solely on orthography, because of stemssuch as beach (+es) and stomach (+s).
For this rea-son, and to add robustness against errors in the inputmorphological segmentation, we allow stems to belisted in the grammar as exceptions to phonologicalrules.In addition to these theoretical differences, thework presented here covers a wider range of phono-logical processes than does Linguistica.
Linguis-tica is capable of detecting only stem-final deletion,whereas our algorithm can also detect insertion (asin match + s ?
matches) and stem-final substitu-tion (as in carry + ed?
carried).
In the followingsection we discuss the structure of the grammar weuse to describe the words in our corpus.3 A Morpho-Phonological GrammarSince the morphology we use as input to our pro-gram is obtained directly from Linguistica, ourgrammar is necessarily similar to the one in thatprogram.
As discussed above, Linguistica containsthree primitives types in its grammar: signatures,stems, and suffixes.
We add one more primitive typeto our grammar, the notion of a rule.Each rule consists of a transformation, for ex-ample  ?
e or y ?
i, and a conditioning con-text.
A context consists of a string of four charac-ters XtytyfXf , where Xi ?
{C, V,#} (consonant,vowel, end-of-word) and yi is in the set of charactersin our text.3 The first half of the context is from theend of the stem, and the second half is from the be-ginning of the suffix.
For example, the stem-suffixpair jump + ed has the context CpeC .
All trans-formations are assumed to occur stem-finally, i.e.at the second context position (or after the secondposition, for insertions).
Of course, these contextsare more detailed than necessary for certain phono-logical rules, and don?t capture all the informationrequired for others.
In future work, we plan to al-low for different types of contexts and generaliza-tion over contexts, but for the present, all contextshave the same form.Using these four primitives, we can construct agrammar in the following way: As in Goldsmith?swork, we list a set of signatures, each of whichcontains a set of stems and suffixes.
In addition,we list a set of phonological rules.
In many cases,only one rule will apply in a particular context, inwhich case it applies to all stem-suffix pairs thatmeet its context.
If more than one rule applies, welist the rule with the most common transformationfirst and assume that it applies unless a particularstem specifies otherwise.
Stems can thus be listedas exceptions to rules by using a non-default *no-change* rule with the appropriate context.
Notethat the more exceptions a rule has, the more expen-sive it is to add to the grammar: each new type oftransformation in a particular context must be listed,and each stem requiring a non-default transforma-tion must specify the transformation required.
Anyprior preferring short grammars will therefore tend3The knowledge of which characters are consonants andwhich are vowels is the only information we provide to our pro-gram, other than the text corpus and the Linguistica-producedmorphology.
Aside from the C/V distinction, our program isentirely knowledge-free.
?1 = ({work, roll, dine, carry}?
{, ed, er, ing})?2 = ({bike}?
{, ed, er, ing, s})?3 = ({wait}?
{, ed, er})?4 = ({booth (r5), worker, beach, match}?
{, s})r1 = e? / CeeCr2 = e? / CeiCr3 = y?
i / CyeCr4 = ?e / Chs#r5 = *no-change* / Chs#Figure 3: G2: A Sample Grammar with Transfor-mation Rulesto reject rules requiring many exceptions (i.e.
thosewithout a consistent application context).Grammar G2, in Figure 3, shows a sample of thekind of grammar we use.
This grammar generatesexactly the same wordforms as G1, but using fewersignatures due to the effects of the phonologicalrules.
All the stem-suffix parings in this grammarundergo the default rules for their contexts exceptfor the stem booth, which is listed as an exceptionto the e-insertion rule.
For booth + s, the grammartherefore generates booths, not boothes.Our model generates data in much the same wayas Goldsmith?s: a word is generated by selecting asignature and then independently generating a stemand suffix from that signature.
This means thatthe likelihood of the data takes the same form inour model as in Goldsmith?s, namely Pr(w) =Pr(?)Pr(t|?
)Pr(f |?
), where the word w consistsof a stem t and a suffix f both drawn from thesame signature ?.
Our model differs from Gold-smith?s in the way that stems and suffixes are pro-duced; because we use phonological rules a greatmany more stems and suffixes can belong to a sin-gle signature.
We defer discussion of how we definethe prior probability over grammars to Section 5,and assume for the moment that we are given priorand likelihood functions that can evaluate the utilityof a grammar and training data.4 Search AlgorithmSince it is clearly infeasible to evaluate the utility ofevery possible grammar, we need a search algorithmto guide us toward a good solution.
Our algorithmuses certain heuristics to make small changes to theinitial grammar (the one provided by Linguistica),evaluating each change using our objective func-tion, and accepting or rejecting it based on the resultof evaluation.
Our algorithm contains three majorcomponents: a procedure to find signatures that aresimilar in ways suggesting phonological change, aprocedure to identify possible contexts for phono-logical change, and a procedure to collapse relatedsignatures and add phonological rules to the gram-mar.
We discuss each of these components in turn.4.1 Identifying Similar SignaturesAn important first step in simplifying the mor-phological analysis of our data using phonologi-cal rules is to identify signatures that might be re-lated via such rules.
Since our algorithm considersthree different types of possible phonological pro-cesses (deletion, substitution, and insertion), thereare three different ways in which signatures may berelated.
We need to look for pairs of signatures thatare similar in any of these three ways.Insertion We look for potential insertion rules byfinding pairs of signatures in which all suffixes butone are common to both signatures.
The distinctpair of suffixes must be such that one can be formedfrom the other by inserting a single character at thebeginning.
Example pairs found by our algorithminclude ?.s?/?.es?
and ?.y?/?.ly?.
In searchingfor these pairs (as well as deletion and substitutionpairs), we consider only pairs where each signaturecontains at least two stems.
This is partly in theinterests of efficiency and partly due to the fact thatsignatures with only one stem are often less reliable.Deletion Signature pairs exhibiting possible dele-tion behavior are similar to those exhibiting inser-tion behavior, except that one of the suffixes notcommon to both signatures must be the empty suf-fix.
Examples of possible deletion pairs include?.ed.ing?/?e.ed.ing?
and ?.ed.ing?/?ed.ing.s?.Substitution In a possible substitution pair, onesignature (the one potentially exhibiting stem-finalsubstitution) contains suffixes that all begin withone of two characters: the basic stem-final char-acter, and the substituted character.
The signature?ied.ier.y?
from G1 is such a signature.
The othersignature in a possible substitution pair must con-tain the empty suffix, and the two signatures mustbe identical when the first character of each suffix inthe first signature is removed.
Possible substitutionpairs include ?ied.ier.y?/?.ed.er?
and ?ous.y?/?.us?.Using the set of similar signatures we have de-tected, we can propose a set of possible phonolog-ical processes in our data.
Some transformations,such as e ?
, will be suggested by more than onepair of signatures, while others, such as y ?
o,will occur with only one pair.
We create a list ofall the possible transformations, ranked accordingto the number of signature pairs attesting to them.4.2 Identifying possible contextsOnce we have found a set of possible transforma-tions, we need to identify the contexts in whichthose transformations might apply.
To see how thisworks, suppose we are looking at the proposed e-deletion rule and our input grammar is G1.
Usingone of the signature pairs attesting to this rule, suchas ?.ed.er.ing?/?e.ed.er.ing?, we can find possibleconditioning contexts by examining the set of stemsand suffixes in the second signature.
If we want toreanalyze the stems din and bik as dine and bike,we hypothesize that each wordform generated us-ing the suffixes present in both signatures (ed, er,and ing) must have deleted an e. We can find thecontext for this deletion by looking at these suffixestogether with the reanalyzed stems.
The contextsfor deletion that we would get from {bike, dine} ?
{ed, ing} are {CeeC , CeiC}.4Our methods for finding possible contexts forsubstitution and insertion rules are similar: reana-lyze the stems and suffixes in the signature hypoth-esized to require a phonological rule, combine them,and note the context generated.
In this way, we canget contexts such as CyeC for the y ?
i rule (fromcarry + ed) and V xs# for the ?
?
e rule (fromindex + s).Just as we ranked the set of possible phonologi-cal rules according to the number of signature pairsattesting to them, we can rank the set of contextsproposed for each rule.
We do this by calculatingr = Pr(Xtyt|yfXf )/Pr(Xtyt), the ratio betweenthe probability of seeing a particular stem contextgiven a particular suffix context to the prior proba-bility of the stem context.
If a stem context (suchas Ce) is quite common overall but hardly ever ap-pears before a particular suffix context (iC), this isgood evidence that some phonological process hasmodified the stem in the context of that suffix.
Lowvalues of r are therefore better evidence of condi-tioning for a rule than are high values of r.4.3 Collapsing signaturesGiven a set of similar signature pairs, the rulesrelating them, and the possible contexts for thoserules, we need to determine which rules are actu-ally phonologically legitimate and which are sim-ply accidents of the data.
We do this by simplyconsidering each rule and context in turn, proceed-ing from the most attested to least attested rules andfrom most likely to least likely contexts.
For eachrule-context pair, we add the rule to the grammar4The reasoning we use to finding conditioning contexts fordeletion rules was also described by Goldsmith (2004a), and issimilar to the much earlier work of Johnson (1984).FINDPHONORULES()1 G?
grammar produced by Linguistica2 R?
ordered set of possible rules3 for each r ?
R4 do5 Cr ?
ordered set of possible contexts for r6 C ?
?7 while Cr 6= ?8 do c?
next c ?
Cr9 Cr ?
Cr \ {c}10 C ?
C ?
{c}11 G?
?
collapseInContext(G, r, C)12 G?
?
pruneRules(G?
)13 if score(G?)
< score(G)14 then G?
G?15 return GCOLLAPSEINCONTEXT(G, r, C)1 for each ?i ?
G2 do for each ?j ?
G3 do if (?i?r ?j) ?
(?
(t, f) ?
?i, ctx(t, f) ?
C)4 then collapseSigs(?i, ?j)Figure 4: Pseudocode for our search algorithmwith that context and collapse any pairs of signa-tures related by the rule, as long as all stem-suffixpairs contain a context at least as likely as the oneunder consideration.
Collapsing a pair of signa-tures means reanalyzing all the stems and suffixesin one of the signatures, and possibly adding excep-tions for any stems that don?t fit the rule.
We havefound that exceptions are often required to handlestems that were originally misanalyzed by Linguis-tica.
For that reason, we prune the rules added to thegrammar, and for each rule, if fewer than 2% of thestems require exceptions, we assume that these areerrors and de-analyze the stems, returning the word-forms they generated to the ??
signature.
We thenevaluate the new analysis using our objective func-tion, and accept it if it scores better than our previ-ous analysis.
Otherwise, we revert to the previousanalysis and continue trying new rule-context pairs.Pseudocode for our algorithm is presented in Fig-ure 4.
We use the notation ?i?r ?j to indicate that?i and ?j are similar with respect to rule r, with ?jbeing the more ?basic?
signature (i.e.
adding r tothe grammar would allow us to move the stems in?i into ?j).Note that collapsing a pair of signatures does notalways result in an overall reduction in the numberof signatures in the grammar.
To see why this isMorph Only Morph+PhonSmall Large Small LargeTokens 100k 888k 100k 888kTypes 11313 35631 11313 35631?s 435 1634 404 1594Singleton ?s 280 1231 259 1215Stems 8255 24529 8186 24379Non- Stems 2363 7673 2286 7494Table 1: Grammatical Analysis of our Corporaso, consider the effect of collapsing ?1 and ?2 andadding r1 and r2 (the e-deletion rules) to G1.
Whenthe stem bik gets reanalyzed as bike, the algorithmrecognizes that bike is already a stem in the gram-mar, so rather than placing the reanalyzed stem in?1, it combines the reanalyzed suffixes {, ed, er,ing} with the suffixes {, s} from ?6 and creates anew signature for the stem bike ?
?.ed.er.ing.s?.The two stems carr and carry are also combinedin this way, but in that case, the combined suffixesform a signature already present in the grammar, sono new signature is required.5 ExperimentsFor our experiments with learning phonologicalrules, we used two different corpora obtained fromthe Penn Treebank.
The larger corpus contains thewords from sections 2-21 of the treebank, filtered toremove most numbers, acronyms, and words con-taining puctuation.
This corpus consists of approx-imately 900,000 tokens.
The smaller corpus is sim-ply the first 100,000 words from the larger corpus.We ran each corpus through the Linguistica pro-gram to obtain an initial morphological segmenta-tion.
Statistics on the results of this segmentationare shown in the left half of Table 1.
?Singletonsignatures?
are those containing a single stem, and?Non- stems?
refers to stems in a signature otherthan the ??
signature, i.e.
those stems that combinewith at least one non- suffix.The original function we used to evaluate the util-ity of our grammars was an MDL prior very simi-lar to the one described by Goldsmith (2001).
Thisprior is simply the number of bits required to de-scribe the grammar using a fairly straightforwardencoding.
The encoding essentially lists all the suf-fixes in the grammar along with pointers to eachone; then lists the phonological rules with theirpointers; then lists all the signatures.
Each signa-ture is a list of stems and their pointers, and a list ofpointers to suffixes.
Each exceptional stem also hasInit.
Grammar Change# ?s 1617 -10# Stems 24374 -17Grammar Size: 1335425 +520?s, Suffixes 53933 -253Stems 1280617 +493Phonology 875 +279Likelihood: 6478490 +166Total: 7813915 +686Table 2: Effects of adding y ?
i rules under MDLprior (large corpus).a pointer to a phonological rule.5Our algorithm considered a total of 11 possibletransformations in the small corpus and 40 in thelarge corpus, but using this prior, only a single typeof transformation appeared in any rule in the finalgrammar: e ?
, with seven contexts in the smallcorpus and eight contexts in the large corpus.
Inanalyzing why our algorithm failed to accept anyother types of rules, we realized that there were sev-eral problems with the MDL prior.
Consider whathappens to the overall evaluation when two signa-tures are collapsed.
In general, the likelihood of thecorpus will go down, because the stem and suffixprobabilities in the combined signature will not fitthe true probabilities of the words as well as twoseparate signatures could.
For large corpora like theones we are using, this likelihood drop can be quitelarge.
In order to counterbalance it, there must be alarge gain in the prior.But now look at Table 2, which shows the effectsof adding all the y ?
i rules to the grammar forthe large corpus under the MDL prior.
The firsttwo lines give the number of signatures and stemsin each grammar.
The next line shows the totallength (in bits) of each grammar, and this value isthen broken down into three different components:the overhead caused by listing the signatures andtheir suffixes, the length of the stem list (not in-cluding the length required to specify exceptions torules), and the length of the phonological compo-nent (including both rules and exception specifica-tions).
Finally, we have the negative log likelihoodunder each grammar and the total MDL cost (gram-mar plus likelihood).As expected, the likelihood term for the grammar5There are some additional complexities in the grammar en-coding that we have not mentioned, due to the fact that stemscan be recursively analyzed using shorter stems.
These com-plexities are irrelevant to the points we wish to make here, butare described in detail in Goldsmith (2001).Init.
Grammar Change# ?s 1601 -7# Stems 24386 -7Grammar Size: 1249629 -318?s, Suffixes 241465 -493Stems 1005887 -111Phonology 2277 +286Likelihood: 6478764 +39Total: 7728393 -279Table 3: Effects of adding y ?
i rules under modi-fied prior (large corpus).with y ?
i rules has increased, indicating a dropin the probability of the corpus under this gram-mar.
But notice that the total grammar size hasalso increased, leading to an overall evaluation thatis worse than for the original grammar.
There aretwo main reasons for this increase in grammar size.Initially, the more puzzling of the two is the factthat the number of bits required to list all the stemshas increased, despite the fact that the number ofstems has decreased due to reanalyzing some pairsof stems into single stems.
It turns out that this ef-fect is due to the encoding used for stems, which issimply a bitwise encoding of each character in thestem.
This encoding means that longer stems re-quire longer descriptions.
When reanalysis requiresshifting a character from a suffix onto the entire setof stems in a signature (as in {certif, empt, hurr} ?
{ied, y} ?
{certify, empty, hurry} ?
{, ed}), therecan be a large gain in description length simply dueto the extra characters in the stems.
If the number ofstems eliminated through reanalysis is high enough(as it is for the e ?
 rules), this stem length effectwill be outweighed.
But when only a few stems areeliminated relative to the number that get longer, theoverall length of the stem list increases.However, even without the stem list, the grammarwith y ?
i rules would still be slightly longer thanthe grammar without them.
In this case, the rea-son in that under our MDL prior, it is quite efficientto encode a signature and its suffixes.
Therefore thegrammar reduction caused by removing a few signa-tures is not enough to outweigh the increase causedby adding a few phonological rules.Using these observations as a guideline, we re-designed our prior by assigning a fixed cost to eachstem and increasing the overhead cost for signa-tures.
The new overhead function is equal to thesum of the lengths of all the suffixes in the signature,times a constant factor.
This function means there ismore incentive to collapse two signatures that shareseveral suffixes, such as ?e.ed.er.ing?/?.ed.er.ing?,than to collapse signatures sharing only a single suf-fix, such as ?ing.s?/?.ing?.
This behavior is exactlywhat we want, since these shorter pairs are morelikely to be accidental.
Table 3 shows the effectsof adding the y ?
i rules under this new prior.The starting grammar is somewhat different fromthe one in Table 2, because more rules have alreadybeen added by the time the y ?
i rules are consid-ered.
The important point, however, is that the costof each component of the grammar changes in thedirection we expect it to, and the total grammar costis reduced enough to more than make up for the lossin likelihood.With this new prior, our algorithm was more suc-cessful, learning from the large corpus the three ma-jor transformations for English (e ?
,  ?
e, andy ?
i) with a total of 22 contexts.
Eight of theserules, such as ?
e / V xs# and y ?
i / CyeC ,had no exceptions.
Of the remaining rules, the ex-ceptions to six of the rules were correctly analyzedstems (for example, unhappy + ly?
unhappily andnecessary + ly?
necessarily but sly + ly?
slyly),while the remaining eight rules contained misan-alyzed exceptions (such as overse + er ?
over-seer, which was listed as an exception to the rulee? / CeeC , rather than being reanalyzed as over-see + er).
In the small corpus, no y ?
i rules werelearned due to the fact that no similar signatures at-testing to these rules were found.Using these phonological rules, a total of 31 sig-natures in the small corpus and 57 signatures in thelarge corpus were collapsed, with subsequent re-analysis of 225 and 528 stems, respectively.
Thisrepresents 7-10% of all the non- stems.
The finalgrammars are summarized in the right half of Table1.6 ConclusionThe work described here is clearly preliminary withrespect to learning phonological rules and usingthose rules to simplify an existing morphology.
Ournotion of context, for example, is somewhat impov-erished; our system might benefit from using con-texts with variable lengths and levels of generality,such as those in Albright and Hayes (2003).
Wealso cannot handle transformations that require ruleordering or more than one-character changes.
Onereason we have not yet implemented these additionsis the difficulty of designing a heuristic search thatcan handle the additional complexity required.
Weare therefore working toward implementing a moregeneral search procedure that will allow us to ex-plore a larger grammar space, allowing greater flex-ibility with rules and contexts.
Once some of theseimprovements have been implemented, we hope toexplore the possibilities for learning in languageswith richer morphology and phonology than En-glish.Our point in this paper, however, is not to presenta fully general learner, but to emphasize that in aBayesian system, the choice of prior can be crucialto the success of the learning task.
Learning is atrade-off between finding an explanation that fits thecurrent data (maximizing the likelihood) and main-taining the ability to generalize to new data (max-imizing the prior).
The MDL framework is a wayto formalize this trade-off that is intuitively appeal-ing and seems straightforward to implement, but wehave shown that a simple MDL approach is not thebest way to achieve our particular task.
There are atleast two reasons for this.
First, the obvious encod-ing of stems actually penalizes the addition of cer-tain types of phonological rules, even when addingthese rules reduces the number of stems in the gram-mar.
More importantly, the type of grammar wewant to learn allows two different kinds of general-izations: the grouping of stems into signatures, andthe addition of phonological rules.
Simply speci-fying a method of encoding each type of general-ization may not result in a linguistically appropriatetrade-off during learning.
In particular, we discov-ered that our MDL encoding for signatures was tooefficient relative to the encoding for rules, leadingthe system to prefer not to add rules.
Our large cor-pus size already puts a great deal of pressure on thesystem to keep signatures separate, since this leadsto a better fit of the data.
In order to learn most ofthe rules, we therefore had to significantly increasethe cost of signatures.We are not the first to note that with an MDL-style prior the choice of encoding makes a differ-ence to the linguistic appropriateness of the result-ing grammar.
Chomsky himself (Chomsky, 1965)points out that the reason for using certain typesof notation in grammar rules is to make clear thetypes of generalizations that lead to shorter gram-mars.
However, our experience emphasizes the factthat very little is still known about how to chooseappropriate encodings (or, more generally, priors).As researchers continue to attempt more sophisti-cated Bayesian learning tasks, they will encountermore interactions between different kinds of gener-alizations.
As a result, the question of how to de-sign a good prior will become increasingly impor-tant.
Our primary goal for the future is therefore toinvestigate exactly what assumptions go into decid-ing whether a grammar is linguistically sound, andto determine how to specify those assumptions ex-plicitly in a Bayesian prior.AcknowledgementsThe authors would like to thank Eugene Charniakand the anonymous reviewers for helpful comments.This work was supported by NSF grants 9870676and 0085940, NIMH grant 1R0-IMH60922-01A2,and an NDSEG fellowship.ReferencesA.
Albright and B. Hayes.
2003.
Rules vs.analogy in english pass tenses: a computa-tional/experimental study.
Cognition, 90:119?161.M.
Brent and T. Cartwright.
1996.
Distributionalregularity and phonotactic constraints are usefulfor segmentation.
Cognition, 61:93?125.N.
Chomsky.
1965.
Aspects of the Theory of Syn-tax.
MIT Press, Cambridge.M.
Creutz and K. Lagus.
2002.
Unsupervised dis-covery of morphemes.
In Proceedings of theWorkshop on Morphological and PhonologicalLearning at ACL ?02, pages 21?30.C.
de Marcken.
1996.
Unsupervised Language Ac-quisition.
Ph.D. thesis, Massachusetts Institute ofTechnology.T.
M. Ellison.
1993.
The Machine Learning ofPhonological Structure.
Ph.D. thesis, Universityof Western Australia.T.
M. Ellison.
1994.
The iterative learning ofphonological constraints.
Computational Lin-guistics, 20(3).J.
Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Computa-tional Linguistics, 27:153?198.J.
Goldsmith.
2004a.
An algorithm for the unsuper-vised learning of morphology.
Preliminary draftas of January 1.J.
Goldsmith.
2004b.
Linguis-tica.
Executable available athttp://humanities.uchicago.edu/faculty/goldsmith/.M.
Johnson.
1984.
A discovery procedure for cer-tain phonological rules.
In Proceedings of COL-ING.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.93.
Building a large annotated corpus of english:the penn treebank.
Computational Linguistics,19(2).Rissanen.
1989.
Stochastic Complexity and Statis-tical Inquiry.
World Scientific Co., Singapore.
