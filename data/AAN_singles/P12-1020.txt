Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184?193,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsBootstrapping a Unified Model of Lexical and Phonetic AcquisitionMicha Elsnermelsner0@gmail.comILCC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKSharon Goldwatersgwater@inf.ed.ac.ukILCC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKJacob Eisensteinjacobe@gmail.comSchool of Interactive ComputingGeorgia Institute of TechnologyAtlanta, GA, 30308, USAAbstractDuring early language acquisition, infants mustlearn both a lexicon and a model of phonet-ics that explains how lexical items can varyin pronunciation?for instance ?the?
might berealized as [Di] or [D@].
Previous models of ac-quisition have generally tackled these problemsin isolation, yet behavioral evidence suggestsinfants acquire lexical and phonetic knowledgesimultaneously.
We present a Bayesian modelthat clusters together phonetic variants of thesame lexical item while learning both a lan-guage model over lexical items and a log-linearmodel of pronunciation variability based on ar-ticulatory features.
The model is trained ontranscribed surface pronunciations, and learnsby bootstrapping, without access to the truelexicon.
We test the model using a corpus ofchild-directed speech with realistic phoneticvariation and either gold standard or automati-cally induced word boundaries.
In both casesmodeling variability improves the accuracy ofthe learned lexicon over a system that assumeseach lexical item has a unique pronunciation.1 IntroductionInfants acquiring their first language confront twodifficult cognitive problems: building a lexicon ofword forms, and learning basic phonetics and phonol-ogy.
The two tasks are closely related: knowing whatsounds can substitute for one another helps in clus-tering together variant pronunciations of the sameword, while knowing the environments in which par-ticular words can occur helps determine which soundchanges are meaningful and which are not (Feldman(a) intended: /ju want w2n/ /want e kUki/(b) surface: [j@ waP w2n] [wan @ kUki](c) unsegmented: [j@waPw2n] [wan@kUki](d) idealized: /juwantw2n/ /wantekUki/Figure 1: The utterances you want one?
want a cookie?represented (a) using a canonical phonemic encoding foreach word and (b) as they might be pronounced phoneti-cally.
Lines (c) and (d) remove the word boundaries (butnot utterance boundaries) from (b) and (a), respectively.et al, 2009).
For instance, if an infant who alreadyknows the word [ju] ?you?
encounters a new word[j@], they must decide whether it is a new lexical itemor a variant of the word they already know.
Evidencefor the correct conclusion comes from the pronun-ciation (many English vowels are reduced to [@] inunstressed positions) and the context?if the nextword is ?want?, ?you?
is a plausible choice.To date, most models of infant language learn-ing have focused on either lexicon-building or pho-netic learning in isolation.
For example, many mod-els of word segmentation implicitly or explicitlybuild a lexicon while segmenting the input streamof phonemes into word tokens; in nearly all casesthe phonemic input is created from an orthographictranscription using a phonemic dictionary, thus ab-stracting away from any phonetic variability (Brent,1999; Venkataraman, 2001; Swingley, 2005; Gold-water et al, 2009, among others).
As illustratedin Figure 1, these models attempt to infer line (a)from line (d).
However, (d) is an idealization: realspeech has variability, and behavioral evidence sug-gests that infants are still learning about the phoneticsand phonology of their language even after beginningto segment words, rather than learning to neutralize184the variations first and acquiring the lexicon after-wards (Feldman et al, 2009, and references therein).Based on this evidence, a more realistic model ofearly language acquisition should propose a methodof inferring the intended forms (Figure 1a) from theunsegmented surface forms (1c) while also learning amodel of phonetic variation relating the intended andsurface forms (a) and (b).
Previous models with sim-ilar goals have learned from an artificial corpus witha small vocabulary (Driesen et al, 2009; Ra?sa?nen,2011) or have modeled variability only in vowels(Feldman et al, 2009); to our knowledge, this paperis the first to use a naturalistic infant-directed corpuswhile modeling variability in all segments, and toincorporate word-level context (a bigram languagemodel).
Our main contribution is a joint lexical-phonetic model that infers intended forms from seg-mented surface forms; we test the system using in-put with either gold standard word boundaries orboundaries induced by an existing unsupervised seg-mentation model (Goldwater et al, 2009).
We showthat in both cases modeling variability improves theaccuracy of the learned lexicon over a system thatassumes each intended form has a unique surfaceform.Our model is conceptually similar to those usedin speech recognition and other applications: weassume the intended tokens are generated from a bi-gram language model and then distorted by a noisychannel, in particular a log-linear model of phoneticvariability.
But unlike speech recognition, we haveno ?intended-form, surface-form?
training pairs totrain the phonetic model, nor even a dictionary ofintended-form strings to train the language model.Instead, we initialize the noise model using featureweights based on universal linguistic principles (e.g.,a surface phone is likely to share articulatory featureswith the intended phone) and use a bootstrappingprocess to iteratively infer the intended forms andretrain the language model and noise model.
Whilewe do not claim that the particular inference mech-anism we use is cognitively plausible, our positiveresults further support the claim that infants can anddo acquire phonetics and the lexicon in concert.2 Related workOur work is inspired by the lexical-phonetic modelof Feldman et al (2009).
They extend a model forclustering acoustic tokens into phonetic categories(Vallabha et al, 2007) by adding a lexical level thatsimultaneously clusters word tokens (which containthe acoustic tokens) into lexical entries.
Includingthe lexical level improves the model?s phonetic cat-egorization, and a follow-up study on artificial lan-guage learning (Feldman, 2011) supports the claimthat human learners use lexical knowledge to distin-guish meaningful from unimportant phonetic con-trasts.
Feldman et al (2009) use a real-valued rep-resentation for vowels (formant values), but assumeno variability in consonants, and treat each word to-ken independently.
In contrast, our model uses asymbolic representation for sounds, but models vari-ability in all segment types and incorporates a bigramword-level language model.To our knowledge, the only other lexicon-buildingsystems that also learn about phonetic variability arethose of Driesen et al (2009) and Ra?sa?nen (2011).These systems learn to represent lexical items andtheir variability from a discretized representation ofthe speech stream, but they are tested on an artifi-cial corpus with only 80 vocabulary items that wasconstructed so as to ?avoid strong word-to-word de-pendencies?
(Ra?sa?nen, 2011).
Here, we use a natu-ralistic corpus, demonstrating that lexical-phoneticlearning is possible in this more general setting andthat word-level context information is important fordoing so.Several other related systems work directly fromthe acoustic signal and many of these do use natu-ralistic corpora.
However, they do not learn at boththe lexical and phonetic/acoustic level.
For example,Park and Glass (2008), Aimetti (2009), Jansen et al(2010), and McInnes and Goldwater (2011) presentlexicon-building systems that use hard-coded acous-tic similarity measures rather than learning aboutvariability, and they only extract and cluster a fewfrequent words.
On the phonetic side, Varadarajan etal.
(2008) and Dupoux et al (2011) describe systemsthat learn phone-like units but without the benefit oftop-down information.A final line of related work is on word segmenta-tion.
In addition to the models mentioned in Section1, which use phonemic input, a few models of wordsegmentation have been tested using phonetic input(Fleck, 2008; Rytting, 2007; Daland and Pierrehum-bert, 2010).
However, they do not cluster segmented185Figure 2: Our generative model of the surface tokens sfrom intended tokens x, which occur with left and rightcontexts l and r.word tokens into lexical items (none of these mod-els even maintains an explicit lexicon), nor do theymodel or learn from phonetic variation in the input.3 Lexical-phonetic modelOur lexical-phonetic model is defined using the stan-dard noisy channel framework: first a sequence ofintended word tokens is generated using a languagemodel, and then each token is transformed by a proba-bilistic finite-state transducer to produce the observedsurface sequence.
In this section, we present themodel in a hierarchical Bayesian framework to em-phasize its similarity to existing models, in particu-lar those of Feldman et al (2009) and Goldwater etal.
(2009).
In our actual implementation, however,we use approximation and MAP point estimates tomake our inference process more tractable; we dis-cuss these simplifications in Section 4.Our observed data consists of a (segmented) se-quence of surface words s1 .
.
.
sn.
We wish to re-cover the corresponding sequence of intended wordsx1 .
.
.
xn.
As shown in Figure 2, si is produced fromxi by a transducer T : si ?
T (xi), which modelsphonetic changes.
Each xi is sampled from a dis-tribution ?
which represents word frequencies, andits left and right context words, li and ri, are drawnfrom distributions conditioned on xi, in order to cap-ture information about the environments in whichxi appears: li ?
PL(xi), ri ?
PR(xi).
Because thenumber of word types is not known in advance, ?
isdrawn from a Dirichlet process DP (?
), and PL(x)and PR(x) have Pitman-Yor priors with concentra-tion parameter 0 and discount d (Teh, 2006).Our generative model of xi is unusual for two rea-sons.
First, we treat each xi independently ratherthan linking them via a Markov chain.
This makesthe model deficient, since li overlaps with xi?1 andso forth, generating each token twice.
During in-ference, however, we will never compute the jointprobability of all the data at once, only the prob-abilities of subsets of the variables with particularintended word forms u and v. As long as no two ofthese words are adjacent, the deficiency will have noeffect.
We make this independence assumption forcomputational reasons?when deciding whether tomerge u and v into a single lexical entry, we computethe change in estimated probability for their contexts,but not the effect on other words for which u and vthemselves appear as context words.Also unusual is that we factor the joint probabil-ity (l, x, r) as p(x)p(l|x)p(r|x) rather than as a left-to-right chain p(l)p(x|l)p(r|x).
Given our indepen-dence assumption above, these two quantities aremathematically equivalent, so the difference mattersonly because we are using smoothed estimates.
Ourfactorization leads to a symmetric treatment of leftand right contexts, which simplifies implementation:we can store all the context parameters locally asPL(?|x) rather than distributed over various P (x|?
).Next, we explain our transducer T .
A weightedfinite-state transducer (WFST) is a variant of a finite-state automaton (Pereira et al, 1994) that reads aninput string symbol-by-symbol and probabilisticallyproduces an output string; thus it can be used tospecify a conditional probability on output stringsgiven an input.
Our WFST (Figure 3) computes aweighted edit distance, and is implemented usingOpenFST (Allauzen et al, 2007).
It contains a statefor each triplet of (previous, current, next) phones;conditioned on this state, it emits a character out-put which can be thought of as a possible surfacerealization of current in its particular environment.The output can be the empty string , in which casecurrent is deleted.
The machine can also insert char-acters at any point in the string, by transitioning to aninsert state (previous, , current) and then returningwhile emitting some new character.The transducer is parameterized by the probabil-ities of the arcs.
For instance, all arcs leaving thestate (?, D, i) consume the character D and emit somecharacter c with probability p(c|?, D, i).
Following186Figure 3: The fragment of the transducer responsible forinput string [Di] ?the?.
?...?
represents an output arc foreach possible character, including the empty string ; ?
isthe word boundary marker.Dreyer et al (2008), we parameterize these distribu-tions with a log-linear model.
The model features arebased on articulatory phonetics and distinguish threedimensions of sound production: voicing, place ofarticulation and manner of articulation.Features are generated from four positional tem-plates (Figure 4): (curr)?out, (prev, curr)?out,(curr, next)?out and (prev, curr, next)?out.
Eachtemplate is instantiated once per articulatory dimen-sion, with prev, curr, next and out replaced by theirvalues for that dimension: for instance, there aretwo voicing values, voiced and unvoiced1 and the(curr)?out template for [D] producing [d] wouldbe instantiated as (voiced)?voiced.
To capturetrends specific to particular sounds, each templateis instantiated again using the actual symbol forcurr and articulatory values for everything else (e.g.,[D]?unvoiced).
An additional template,?out, cap-tures the marginal frequency of the output symbol.There are also faithfulness features, same-sound,same-voice, same-place and same-manner whichcheck if curr is exactly identical to out or sharesthe exact value of a particular feature.Our choice of templates and features is based onstandard linguistic principles: we expect that chang-ing only a single articulatory dimension will be moreacceptable than changing several, and that the artic-ulatory dimensions of context phones are importantbecause of assimilatory and dissimilatory processes(Hayes, 2011).
In modern phonetics and phonology,these generalizations are usually expressed as Opti-mality Theory constraints; log-linear models such asours have previously been used to implement stochas-1We use seven place values and five manner values (stop,nasal stop, fricative, vowel, other).
Empty segments like  and ?are assigned a special value ?no-value?
for all features.Figure 4: Some features generated for (?, D, i)?
d. Eachblack factor node corresponds to a positional template.The features instantiated for the (curr)?out and ?outtemplate are shown in full, and we show some of thefeatures for the (curr,next)?out template.tic Optimality Theory models (Goldwater and John-son, 2003; Hayes and Wilson, 2008).4 InferenceGlobal optimization of the model posterior is diffi-cult; instead we use Viterbi EM (Spitkovsky et al,2010; Allahverdyan and Galstyan, 2011).
We beginwith a simple initial transducer and alternate betweentwo phases: clustering together surface forms, andreestimating the transducer parameters.
We iteratethis procedure until convergence (when successiveclustering phases find nearly the same set of merges);this tends to take about 5 or 6 iterations.In our clustering phase, we improve the modelposterior as much as possible by greedily makingtype merges, where, for a pair of intended word formsu and v, we replace all instances of xi = u withxi = v. We maintain the invariant that each intendedword form?s most common surface form must beitself; this biases the model toward solutions withlow distortion in the transducer.4.1 Scoring mergesWe write the change in the log posterior probabilityof the model resulting from a type merge of u to v as?
(u, v), which factors into two terms, one dependingon the surface string and the transducer, and the otherdepending on the string of intended words.
In order toensure that each intended word form?s most commonsurface form is itself, we define ?
(u, v) = ??
if uis more common than v.We write the log probability of x being transducedto s as T (s|x).
If we merge u into v, we no longer187need to produce any surface forms from u, but insteadwe must derive them from v. If #(?)
counts theoccurrences of some event in the current state of themodel, the transducer component of ?
is:?T =?s#(xi=u, si=s)(T (s|v)?
T (s|u)) (1)This term is typically negative, voting against amerge, since u is more similar to itself than to v.The language modeling term relating to the in-tended string again factors into multiple components.The probability of a particular li, xi, ri can be brokeninto p(xi)p(li|xi)p(ri|xi) according to the model.We deal first with the p(xi) unigram term, consid-ering all tokens where xi ?
{u, v} and computingthe probability pu = p(xi = u|xi ?
{u, v}).
Bydefinition of a Dirichlet process, the marginal over asubset of the variables will be Dirichlet, so for ?
> 1we have the MAP estimate:pu =#(xi=u) + ??
1#(xi ?
{u, v}) + 2(??
1)(2)pv = p(xi = v|xi ?
{u, v}) is computed similarly.If we decide to merge u into v, however, the proba-bility p(xi = v|xi ?
{u, v}) becomes 1.
The changein log-probability resulting from the merge is closelyrelated to the entropy of the distribution:?U = ?#(xi=u) log(pu)?#(xi=v) log(pv) (3)This change must be positive and favors merging.Next, we consider the change in probability fromthe left contexts (the derivations for right contexts areequivalent).
If u and v are separate words, we gen-erate their left contexts from different distributionsp(l|u) and p(l|v), while if they are merged, we mustgenerate all the contexts from the same distributionp(l|{u, v}).
This change is:?L =?l#(l, u){log(p(l|{u, v}))?
log(p(l|u)}+?l#(l, v){log(p(l|{u, v}))?
log(p(l|v)}In a full Bayesian model, we would integrate overthe parameters of these distributions; instead, weuse Kneser-Ney smoothing (Kneser and Ney, 1995)which has been shown to approximate the MAP solu-tion of a hierarchical Pitman-Yor model (Teh, 2006;Goldwater et al, 2006).
The Kneser-Ney discount2d is a tunable parameter of our system, and con-trols whether the term favors merging or not.
If d issmall, p(l|u) and p(l|v) are close to their maximum-likelihood estimates, and ?L is similar to a Jensen-Shannon divergence; it is always negative and dis-courages mergers.
As d increases, however, p(l|u)for rare words approaches the prior distribution; inthis case, merging two words may result in betterposterior parameters than estimating both separately,since the combined estimate loses less mass to dis-counting.Because neither the transducer nor the languagemodel are perfect models of the true distribution,they can have incompatible dynamic ranges.
Often,the transducer distribution is too peaked; to remedythis, we downweight the transducer probability by?, a parameter of our model, which we set to .5.Downweighting of the acoustic model versus the LMis typical in speech recognition (Bahl et al, 1980).To summarize, the full change in posterior is:?
(u, v) = ?U + ?L + ?R + ?
?T (4)There are four parameters.
The transducer regular-ization r = 1 and unigram prior ?
= 2, which weset ad-hoc, have little impact on performance.
TheKneser-Ney discount d = 2 and transducer down-weight ?
= .5 have more influence and were tunedon development data.4.2 Clustering algorithmIn the clustering phase, we start with an initial solu-tion in which each surface form is its own intendedpronunciation and iteratively improve this solutionby merging together word types, picking (approxi-mately) the best merger at each point.We begin by computing a set of candidate mergersfor each surface word type u.
This step saves timeby quickly rejecting mergers which are certain to getvery low transducer scores.
We reject a pair u, v ifthe difference in their length is greater than 4, or ifboth words are longer than 4 segments, but, whenwe consider them as unordered bags of segments, theDice coefficient between them is less than .5.For each word u and all its candidates v, we com-pute ?
(u, v) as in Equation 4.
We keep track of the2We use one discount, rather than several as in modified KN.188Input: vocabulary of surface forms uInput: C(u): candidate intended forms of uOutput: intend(u): intended form of uforeach u ?
vocab do// initializationv?(u)?
argmaxv ?C(u) ?
(u, v);??(u)?
?
(u, v?(u))intend(u)?
uadd u to queue Q with priority ??
(u))while top(Q) > ??
dou?
pop(Q)recompute v?(u),??
(u)if ??
(u) > 0 then// merge u with best mergerintend(u)?
v?
(u)update ?
(x, u) ?x : v?
(x) = uremove u from C(x) ?xupdate ?
(x, v) ?x : v?
(x) = vupdate ?
(v, x) ?x ?
C(v)if updated ?
> ??
for any words thenreset ?
?, v?
for those words// (these updates canincrease a word?s priorityfrom ??
)else if ??
(u) 6= ??
then// reject but leave in queue??(u)?
?
?Algorithm 1: Our clustering phase.current best target v?
(u) and best score ??
(u), usinga priority queue.
At each step of the algorithm, wepop the u with the current best ??
(u), recomputeits scores, and then merge it with v?
(u) if doing sowould improve the model posterior.
In an exact al-gorithm, we would then need to recompute most ofthe other scores, since merging u and v?
(u) affectsother words for which u and v?
(u) are candidates,and also words for which they appear in the contextset.
However, recomputing all these scores would beextremely time-consuming.3 Therefore, we recom-pute scores for only those words where the previousbest merger was either u or v?(u).
(If the best mergewould not improve the probability, we reject it, butsince its score might increase if we merge v?
(u), weleave u in the queue, setting its ?
score to ??
; thisscore will be updated if we merge v?(u).
)Since we recompute the exact scores ?
(u, v) im-mediately before merging u, the algorithm is guaran-3The transducer scores can be cached since they depend onlyon surface forms, but the language model scores cannot.teed never to reduce the posterior probability.
It canpotentially make changes in the wrong order, sincenot all the ?s are recomputed in each step, but mostchanges do not affect one another, so performingthem out of order has no impact.
Empirically, wefind that mutually exclusive changes (usually of theform (u, v) and (v, w)) tend to differ enough in initialscore that they are evaluated in the correct order.4.3 Training the transducerTo train the transducer on a set of mappings betweensurface and intended forms, we find the maximum-probability state sequence for each mapping (anotherapplication of Viterbi EM) and extract features foreach state and its output.
Learning weights is thena maximum-entropy problem, which we solve usingOrthant-wise Limited-memory Quasi-Newton.4To construct our initial transducer, we first learnweights for the marginal distribution on surfacesounds by training the max-ent system with only thebias features active.
Next, we manually set weights(Table 1) for insertions and deletions, which do notappear on the surface, and for faithfulness features.Other features get an initial weight of 0.5 Experiments5.1 DatasetOur corpus is a processed version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) fromCHILDES (MacWhinney, 2000), which contains or-thographic transcriptions of parent-child dyads withinfants aged 13-23 months.
Brent and Cartwright(1996) created a phonemic version of this corpusby extracting all infant-directed utterances and con-verted them to a phonemic transcription using a dic-tionary.
This version, which contains 9790 utterances(33399 tokens, 1321 types), is now standard for wordsegmentation, but contains no phonetic variability.Since producing a close phonetic transcription ofthis data would be impractical, we instead constructan approximate phonetic version using informationfrom the Buckeye corpus (Pitt et al, 2007).
Buckeyeis a corpus of adult-directed conversational Ameri-can English, and has been phonetically transcribed4We use the implementation of Andrew and Gao (2007) withan l2 regularizer and regularization parameter r = 1; althoughthis could be tuned, in practice it has little effect on results.189Feature Weightoutput-is-x marginal p(x)output-is- 0same-sound 5same-{place,voice, manner} 2insertion -3Table 1: Initial transducer weights.?about?
ahbawt:15, bawt:9, ihbawt:4, ahbawd:4, ih-bawd:4, ahbaat:2, baw:1, ahbaht:1, erbawd:1,bawd:1, ahbaad:1, ahpaat:1, bah:1, baht:1,ah:1, ahbahd:1, ehbaat:1, ahbaed:1, ihbaht:1,baot:1?wanna?
waanah:94, waanih:37, wahnah:16, waan:13,wahneh:8, wahnih:5, wahney:3, waanlih:3,wehnih:2, waaneh:2, waonih:2, waaah:1,wuhnih:1, wahn:1, waantah:1, waanaa:1,wowiy:1, waaih:1, wah:1, waaniy:1Table 2: Empirical distribution of pronunciations of?about?
and ?wanna?
in our dataset.by hand to indicate realistic pronunciation variability.To create our phonetic corpus, we replace each phone-mic word in the Bernstein-Ratner-Brent corpus witha phonetic pronunciation of that word sampled fromthe empirical distribution of pronunciations in Buck-eye (Table 2).
If the word never occurs in Buckeye,we use the original phonemic version.Our corpus is not completely realistic as a sam-ple of child-directed speech.
Since each pronuncia-tion is sampled independently, it lacks coarticulationand prosodic effects, and the distribution of pronun-ciations is derived from adult-directed rather thanchild-directed speech.
Nonetheless, it represents pho-netic variability more realistically than the Bernstein-Ratner-Brent corpus, while still maintaining the lexi-cal characteristics of infant-directed speech (as com-pared to the Buckeye corpus, with its much largervocabulary and more complex language model).We conduct our development experiments on thefirst 8000 input utterances, holding out the remain-ing 1790 for evaluation.
For evaluation experiments,we run the system on all 9790 utterances, reportingscores on only the last 1790.5.2 MetricsWe evaluate our results by generalizing the threesegmentation metrics from Goldwater et al (2009):word boundary F-score, word token F-score, andlexicon (word type) F-score.0 1 2 3 4 5Iteration7576777879808182Token FLexicon FFigure 5: System scores over 5 iterations.In our first set of experiments we evaluate howwell our system clusters together surface forms de-rived from the same intended form, assuming goldstandard word boundaries.
We do not evaluate theinduced intended forms directly against the gold stan-dard intended forms?we want to evaluate clustermemberships and not labels.
Instead we computea one-to-one mapping between our induced lexicalitems and the gold standard, maximizing the agree-ment between the two (Haghighi and Klein, 2006).Using this mapping, we compute mapped token F-score5 and lexicon F-score.In our second set of experiments, we use unknownword boundaries and evaluate the segmentations.
Wereport the standard word boundary F and unlabeledword token F as well as mapped F. The unlabeled to-ken score counts correctly segmented tokens, whetherassigned a correct intended form or not.5.3 Known word boundariesWe first run our system with known word boundaries(Table 3).
As a baseline, we treat every surface tokenas its own intended form (none).
This baseline hasfairly high accuracy; 65% of word tokens receivethe most common pronunciation for their intendedform.6 As an upper bound, we find the best intendedform for each surface type (type ubound).
This cor-rectly resolves 91% of tokens; the remaining error isdue to homophones (surface types corresponding tomore than one intended form).
We also test our sys-5When using the gold word boundaries, the precision andrecall are equal and this is is the same as the accuracy; in seg-mentation experiments the two differ, because with fewer seg-mentation boundaries, the system proposes fewer tokens.
Onlycorrectly segmented tokens which are also mapped to the correctform count as matches.6The lexicon recall is not quite 100% because one rare wordappears only as a homophone of another word.190System Tok F Lex P Lex R Lex Fnone 65.4 50.2 99.7 66.7initializer 75.2 83.2 73.3 78.0system 79.2 87.1 75.9 81.1oracle trans.
82.7 88.7 83.8 86.2type ubound 91.0 97.5 98.0 97.7Table 3: Results on 1790 utterances (known boundaries).Boundaries Unlabeled TokensP R F P R Fno var.
90.1 80.3 84.9 74.5 68.7 71.5w/var.
70.4 93.5 80.3 56.5 69.7 62.4Table 4: Degradation in dpseg segmentation perfor-mance caused by pronunciation variation.Mapped Tokens Lexicon (types)P R F P R Fnone 39.8 49.0 43.9 37.7 49.1 42.6init 42.2 52.0 56.5 50.1 40.8 45.0sys 44.2 54.5 48.8 48.6 43.1 45.7Table 5: Results on 1790 utterances (induced boundaries).tem using an oracle transducer (oracle trans.
)?thetransducer estimated from the upper-bound mapping.This scores 83%, showing that our articulatory fea-ture set captures most, but not all, of the availableinformation.
At the beginning of bootstrapping, oursystem (init) scores 75%, but this improves to 79%after five iterations of reestimation (system).
Mostlearning occurs in the first two or three iterations(Figure 5).To determine the importance of different parts ofour system, we run a few ablation tests on develop-ment data.
Context information is critical to obtaina good solution; setting ?L and ?R to 0 lowers ourdev token F-score from 83% to 75%.
Initializingall feature weights to 0 yields a poor initial solution(18% dev token F instead of 75%), but after learn-ing the result is only slightly lower than using theweights in Table 1 (78% rather than 80%), showingthat the system is quite robust to initialization.5.4 Unknown word boundariesAs a simple extension of our model to the case ofunknown word boundaries, we interleave it with anexisting model of word segmentation, dpseg (Gold-water et al, 2009).7 In each iteration, we run thesegmenter, then bootstrap our model for five itera-tions on the segmented output.
We then concatenatethe intended word sequence proposed by our modelto produce the next iteration?s segmenter input.Phonetic variation is known to reduce the perfor-mance of dpseg (Fleck, 2008; Boruta et al, 2011)and our experiments confirm this (Table 4).
Usinginduced word boundaries also makes it harder torecover the lexicon (Table 5), lowering the baselineF-score from 67% to 43%.
Nevertheless, our systemimproves the lexicon F-score to 46%, with token Frising from 44% to 49%, demonstrating the system?sability to work without gold word boundaries.
Un-fortunately, performing multiple iterations betweenthe segmenter and lexical-phonetic learner has littlefurther effect; we hope to address this issue in future.6 ConclusionWe have presented a noisy-channel model that si-multaneously learns a lexicon, a bigram languagemodel, and a model of phonetic variation, while us-ing only the noisy surface forms as training data.It is the first model of lexical-phonetic acquisitionto include word-level context and to be tested on aninfant-directed corpus with realistic phonetic variabil-ity.
Whether trained using gold standard or automati-cally induced word boundaries, the model recoverslexical items more effectively than a system that as-sumes no phonetic variability; moreover, the use ofword-level context is key to the model?s success.
Ul-timately, we hope to extend the model to jointly inferword boundaries along with lexical-phonetic knowl-edge, and to work directly from acoustic input.
How-ever, we have already shown that lexical-phoneticlearning from a broad-coverage corpus is possible,supporting the claim that infants acquire lexical andphonetic knowledge simultaneously.AcknowledgementsThis work was supported by EPSRC grantEP/H050442/1 to the second author.7dpseg1.2 from http://homepages.inf.ed.ac.uk/sgwater/resources.html191ReferencesGuillaume Aimetti.
2009.
Modelling early languageacquisition skills: Towards a general statistical learningmechanism.
In Proceedings of the Student ResearchWorkshop at EACL.Armen Allahverdyan and Aram Galstyan.
2011.
Compar-ative analysis of Viterbi training and ML estimation forHMMs.
In Advances in Neural Information ProcessingSystems (NIPS).Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A general and efficient weighted finite-state trans-ducer library.
In Proceedings of the Ninth Interna-tional Conference on Implementation and Applicationof Automata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Galen Andrew and Jianfeng Gao.
2007.
Scalable trainingof L1-regularized log-linear models.
In ICML ?07.Lalit Bahl, Raimo Bakis, Frederick Jelinek, and RobertMercer.
1980.
Language-model/acoustic-channel-model balance mechanism.
Technical disclosure bul-letin Vol.
23, No.
7b, IBM, December.Nan Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.L.
Boruta, S. Peperkamp, B.
Crabbe?, E. Dupoux, et al2011.
Testing the robustness of online word segmenta-tion: effects of linguistic diversity and phonetic varia-tion.
ACL HLT 2011, page 1.Michael Brent and Timothy Cartwright.
1996.
Distribu-tional regularity and phonotactic constraints are usefulfor segmentation.
Cognition, 61:93?125.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discovery.Machine Learning, 34:71?105, February.R.
Daland and J.B. Pierrehumbert.
2010.
Learningdiphone-based segmentation.
Cognitive Science.Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing,EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.Association for Computational Linguistics.Joris Driesen, Louis ten Bosch, and Hugo Van hamme.2009.
Adaptive non-negative matrix factorization ina computational model of language acquisition.
InProceedings of Interspeech.E.
Dupoux, G. Beraud-Sudreau, and S. Sagayama.
2011.Templatic features for modeling phoneme acquisition.In Proceedings of the 33rd Annual Cognitive ScienceSociety.Naomi Feldman, Thomas Griffiths, and James Morgan.2009.
Learning phonetic categories by learning a lexi-con.
In Proceedings of the 31st Annual Conference ofthe Cognitive Science Society (CogSci).Naomi Feldman.
2011.
Interactions between word andspeech sound categorization in language acquisition.Ph.D.
thesis, Brown University.Margaret M. Fleck.
2008.
Lexicalized phonotactic wordsegmentation.
In Proceedings of ACL-08: HLT, pages130?138, Columbus, Ohio, June.
Association for Com-putational Linguistics.Sharon Goldwater and Mark Johnson.
2003.
Learning OTconstraint rankings using a maximum entropy model.In J. Spenader, A. Eriksson, and Osten Dahl, editors,Proceedings of the Stockholm Workshop on Variationwithin Optimality Theory, pages 111?120, Stockholm.Stockholm University.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006.
Interpolating between types and tokens by esti-mating power-law generators.
In Advances in NeuralInformation Processing Systems (NIPS) 18.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
In In 46thAnnual Meeting of the ACL, pages 398?406.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA, June.
Association for Computational Lin-guistics.Bruce Hayes and Colin Wilson.
2008.
A maximum en-tropy model of phonotactics and phonotactic learning.Linguistic Inquiry, 39(3):379?440.Bruce Hayes.
2011.
Introductory Phonology.
John Wileyand Sons.A.
Jansen, K. Church, and H. Hermansky.
2010.
Towardsspoken term discovery at scale with zero resources.
InProceedings of Interspeech, pages 1676?1679.R.
Kneser and H. Ney.
1995.
Improved backing-off for M-gram language modeling.
In Proc.
ICASSP ?95, pages181?184, Detroit, MI, May.B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Vol 2: The Database.
LawrenceErlbaum Associates, Mahwah, NJ, 3rd edition.Fergus R. McInnes and Sharon Goldwater.
2011.
Un-supervised extraction of recurring words from infant-directed speech.
In Proceedings of the 33rd AnnualConference of the Cognitive Science Society.A.
S. Park and J. R. Glass.
2008.
Unsupervised pat-tern discovery in speech.
IEEE Transactions on Audio,Speech and Language Processing, 16:186?197.192Fernando Pereira, Michael Riley, and Richard Sproat.1994.
Weighted rational transductions and their ap-plication to human language processing.
In HLT.Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-ling, William Raymond, Elizabeth Hume, and EricFosler-Lussier.
2007.
Buckeye corpus of conversa-tional speech (2nd release).Okko Ra?sa?nen.
2011.
A computational model of wordsegmentation from continuous speech using transitionalprobabilities of atomic acoustic events.
Cognition,120(2):28.Anton Rytting.
2007.
Preserving Subsegmental Varia-tion in Modeling Word Segmentation (Or, the Raisingof Baby Mondegreen).
Ph.D. thesis, The Ohio StateUniversity.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher D. Manning.
2010.
Viterbi trainingimproves unsupervised dependency parsing.
In Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning, pages 9?17, Up-psala, Sweden, July.
Association for ComputationalLinguistics.D.
Swingley.
2005.
Statistical clustering and the contentsof the infant vocabulary.
Cognitive Psychology, 50:86?132.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 985?992, Sydney,Australia, July.
Association for Computational Linguis-tics.G.K.
Vallabha, J.L.
McClelland, F. Pons, J.F.
Werker, andS.
Amano.
2007.
Unsupervised learning of vowelcategories from infant-directed speech.
Proceedingsof the National Academy of Sciences, 104(33):13273?13278.B.
Varadarajan, S. Khudanpur, and E. Dupoux.
2008.
Un-supervised learning of acoustic sub-word units.
In Pro-ceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics on Human LanguageTechnologies: Short Papers, pages 165?168.
Associa-tion for Computational Linguistics.A.
Venkataraman.
2001.
A statistical model for worddiscovery in transcribed speech.
Computational Lin-guistics, 27(3):351?372.193
