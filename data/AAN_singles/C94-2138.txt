A Reest imat ion  A lgor i thm fi~r I ' robabi l ist ic Recto's ire ~lYansitionNetwork*)" Young S. t Ian,  mtd Key-Sun ( , tu l(;enter for Artificial Intelligence(;omputer Science l)epartme'at(,~vnter for Artificial Intelligence Rese, archKorea Advanced lrtstitute of Science and TechnologyTacjou, 305-70I, Koreayshau@cskiug.kaist, ac.kr, kschoi(~cskiug.kaist,ac.krAbstractProb~bilistic l{,ecursive Tr~msition Network(Pl~TN) isan elevated version of t{51'N to model and process lan-.guages in stoch~st, ic parameters.
The representationis a direct derivation front the H,TN and keeps muchthe spirit of ltidden Markov Model at the same tint(,.We present a reestimation algorithm \['or Ptl,TN that is~ variation of Inside-Ontside algorithm that comput, esthe vMues of the probabilistic parameters from samplesentences (parsed or unparsed).1. lntrodu(:t ionIn this pal)er , we introduce a network represen-tation, Probabilistic Recursive Transitio.
Networkthat is directly derived fl'Oln R'CN and ItMM, andpresent an estimation algorithm lot tile proba-bilistic paraHteters.
PR;12N is a \]\[\]TN mJgmentedwith probabilities in the transitions ~md statesand with the lexical distributions in the transi--tions, or is the Hidden Markov Model augmentedwith a stack that makes some traltsitions deterministic.The paramete.r esthnation of PI{;I'N is developed as a wu'iation of Inside()utside algorithm.The hlsidc ()utside algorithm has becn appliede(,10t, I;o ~, ,.~* recently by Jelinek (1{t9{/) and \],ari(1991).
The algorithm was first introduced byBaker in 1.979 and is the context free lmtguageversion o\[ Forward-.Backw~rd algorithm in IIid-.
*This research is partly supported by KOSEF (Km:eaScience altd Teclntology l"oundation) under tit{= title "AStudy mt the Bnilding '\[~echni(lues for \[txdmst Km~wledgebased Systems" from 19911 through 1994.den Markov Models.
Its theoretical lbund~Ltion islaid by Baam aud Weh:h in the late 6l)'s, whichin tarn is a type of the F,M Mgorithm in statistics(Rabiner, 1989).Kupiec (1991) introduced a trellis based es-.timation Mgorithm of Hidden SCFG that aecommodates both ilnside-Outside ~dgorithm andl!brward-.Backward ",flgorithm.
The meaning ofour work can be sought from the use of moreplain topology of I{TN, whereas Kupiec's work isa unilied version of tbrward-.backword and InsideOutside ~lgorithms.
Nonetheless, the implemen.ration of reestimation Mgorittun carries no moretheoretical significance than the applicative fliciency and variation for differing representationssince B~ker first apt)lied it to CI"Gs.2.
Probabi l is t ic  Recurs ive  Tran-s it ion NetworkA probabilistic ff.l.
'N (PRTN, hereafter) denotedby A is ~ 4 tuple.A is ~ transition m~trix containing tr~n.sitionprobabilities, ~tnd 13 is aiL observation matrix con-taining probabil ity distribution of the words observable at each terminM transition where rowand column correspond to terminM transitionsand a list of words respective, ly.
F specilies thetypes of transitions, and D2 denotes a stack.
Thefirst two model parameters are the same as that ofI\[MM, thus typed transitions and the existence ofa stack art', what distinguishes I ' ttTN fl'om t\[MM.859The stack operations are associated with tran-sitions.
According to the stack operation, tran-sitions are classified into three types.
The firsttype is push transition in which state identifica-tion is pushed into the stack.
The second type ispop transition which is selected by the content ofstack.
Transitions of the third type are not com-mitted to stack operation.
The three types arealso accompanied by different grammatical  impli-cation, hence grammatical  categories are assignedto trartsitions except pop transitions.
Push transi-tions are associated with nonterminal categories,and will be called nonterminal transition when itis more transparent in later discussions.
In gen-eral, the grammar expressed in PRTN consistsof layers.
A layer is a fragment of network thatcorresponds to a nonterminal.
The third type oftransit ion is linked to the category of terminals(words), titus is named terminal transition.
Alsoa table of probabil ity distribution of words is de-fined on each terminal transition.
In the contextof HMMs, tile words in the terminal transitionare observations to be generated.
Pop transitionsrepresent returning of a layer to one of its possiblymultiple higher layers.The network topology of PI~TN is not differ-ent fi-om that of RTN.
In a conceptual drawingof a grammar, each layer looks like an indepen-dent network.
Compared with conceptual draw-ing of the network, an operational view providesmore vivid representation i  which actual pathsor parses are composed.
The only difference be-tween the two is that in operational view a nonter-minal transition is connected irectly to the firststate of the corresponding layer.
In this paper,the parses or paths are assumed to be sequencesof dark-headed transitions (see Fig.
I for exam-ple).Before we start explaining the algorithms let usdefine some notations.
There is one start statedenoted by 8, and one final state denoted byf .
Also let us ca\]\] states immediately followinga terminal transition terminal state, and states atwhich pop transitions are defined pop state.
Somemore notations are as follows.?
f i r s t ( l )  returns the first state of layer I.?
last(l) returns the last state of layer 1.?
layer(,s) returns the layer state s belongs to.?
bout(l) returns the states from which layer lbranches out.?
bin(l) returns the states to which layer I returns.?
terminal(1) returns a set of terminal edges inlayer I.?
nonterminal ( l )  returns a set of nonterminaledges in layer 1.?
i j  denotes the edge between states i and j .?
\[i,j\] denotes the network segment betweenstates i and j .?
Wa~ b is  an  observation sequence coveringfrom ath to bth observations.3.
Reestilnation AlgorithmPRTN is a RTN with probabilistic transitionsand words 1 that can be estimated from samplesentences by means of statistical techniques, wepresent a reestimation algorithm for obtaining theprobabilities of transitions and the observationsymbols (words) defined at each terminal transi-tion.
Inside-Outside algorithm provides a formalbasis for estimating parameters of context freelanguages uch that the probabilities of the ob-servation sequences (sample sentences ) are max-imized.
The reestimation algorithm iterativelyestimates the probabilistic parameters until theprobabil ity of sample sentence(s) reaches a cer-tain stability.
The reestimation algorithm forPItTN is a variation of Inside-Outside algorithmcustomized for the representation.
The algorithmto be discussed is defined only for well formed ob-servation sequences.Def in i t ion  1 An observation sequence is wellformed if there exists at least a path that gen-erates the sequence in the network and starts atS and ends at 2:'.Let an obserw~tion sequence of length N denotedbyW- W~W~.
.
.Wu.We start explaining the reestimation Mgorithm bydefining Inside-probability.The Inside probabil ity denoted by PI(i)s~t ofstate i is the probabil ity that a portion of layer(i)1we do not consider probabilistic states in this p~per.860E- -+ T ~.
EE--+T'IF--* F * T3'.--~ FF - -+(  E )F - - *  acalling returustates !
;ta.tcs(F~ 1.o o,40- 1,O 0304 - _ _ ~  .
.
.
.
.
.
.
.
~-~ ~" -?Figure 1: Illustration of PI?TN.
A parse is composed of dard-heatded transitions.
(front state i to the last state of the layer) generattes W;~t.
Thatt is, it is the probatbility thatta certain fragment of a layer generates at certainsegment of an input sentence, and this can becomputed by summing the probabilities of all thepossible paths in the layer segment hat generatethe given input segment.where  c = las t ( layer ( i ) ) .More constructive re.presentation of Inside probatbility is thenktwh, c re  ik C tcrminal(h~ycr(i)),ia < i) ),, ,  = ),v ~ bin( layer( j ) ) ,'\].
'he paths starting at state i arc classilied into twocases according to the type of hnmedi~te transi--tion fl'om i: it can be of terminal or nonterminaltype, In ease of terminal, ~J'ter the probatbility ofthe terminal transition is taken into account, therest of the layer segment is responsible for the in-put segment short of one word just generated bythe terminM tratnsition, in caase of nontmminM,first the transition probabilities (push and respec-tive pop tratnsitions) atre multiplied, then depend-ing on the coverage of the nonterminal transition(sublatyer) the rest of the current latyer is responsi-ble for the rmnaining input sequence after done bythe sublaycr.
After the last observation is made,the last state (pop state) of layer(i) should bereached.
:1 i r  i :=l:)I(i)vH~t = 0 otherwise.Fig.
2 is the pi('toriM view of the Inside prob-ability.
A well formed sequence can begin oidyat state ,S, thus to be strict, t~(5)  has additionalproduct term F(,5) that can be computed alsousing InsideOutside algorithm.
Now define theOutside probability.The Outside probatbility denoted by Po(i, j).,~~.is the probatbility thatt patrtial sequences, Wl~.,qand Wt+1~N, are generated provided that the par-tiatt sequence, Ws~t, is generated by \[i,j\] givenruodel, A.
This is a complementary point ofInside-probability.
This time, we look at the out-side of given layer segme,tt and input segment.Assunfing a given latyer segment generates a giveninput segment, we want to colnpute the probat-bility that the surrounding portion of the wholeI'R:i'N generates the rest of the input sequence.861layer(i) ~._:,,.. ik ._(~)__~ ... (~___~...layer(j) (~  ?
?
?
- - '~I SFigure 2: Illustration of Inside probability.The Outside probability is computed first byconsidering the current layer consisting of twoparts after' excluding \[i,j\] that are captured inInside-probability.
Beyond the current layer issimply an Outside probability with respect o thecurrent layer.And by definition,Po(i,j),~t = p( \ [ s ,  i\] ~ w>~_~, \[j, y \ ]  -~W,+I~N I A )axfaev Xx a=l  b=t+l*~( f , i)o~,~ Pd j )~~beo(  ~, y)o~~ .P;(f, i)~~tFig.
3 shows the network configuration in com-puting the Outside probability, t'~(f,i)=~~_t isthe probability that sequence, W=~~I, is gener-ated by layer(i) left to state i. PI(j)t+l~b is theprobability that sequence Wt+l~b is generated bylayer(i) right to state j.
The portions of W notcovered by W=~b is then left to the parent layersof layer(i).P~(f, i).,~t is a slight wriation of Inside proba-bility in which PI(f)=~b'S in the Inside probabil-ity formula are replaced by P~(f, i)a~b.
\[ts actualcomputation is done as follows:PI(f),~t i f s_<t ,1 i f s>tandf=i ,0 i fs  >t  and f )Ai.wheTe x E bout ( layer ( i ) ) ,y e b~n(layer(i)),f = f i r s t ( layer ( i ) ) ,e = las t ( layer ( i ) ) ,l ayer ( i )  = layer ( j ) ,layer(~) = layer(y).x represents a state from which layer(i)branches out, and y represents a state to whichlayer(j) returns to.
Every time a different com-bination of left and right sequences with respectto W~~t is tried in the layer states i and j belongto, the rest of remaining sequences i the Outsideprobability at the layer above layer(i).When there is no subsequence to the right ofW~~b (i.e., b = N),Po(i,j)a~N = 1.It is basically the same as Inside probability ex-cept that it carries a state identification i to checkthe vMidity of stop state.
If there are observationsleft for generation (s _< t), things are done just asin computing Inside probability, ignoring i. Whenboundary point is reached (s > t), if the last stateis i, it returns 1, and 0, otherwise.The probability of an observation sequence canbe computed using Inside probability ~sp(wJA)  -- P ( \ [ s ,a=\ ] -~ w>NI~)= P , ( s ) I _N .Now we can derive the reestimation algorithm forJi and/~ using the Inside and Outside probabilLties.
As the result of constrained maximization ofBantu's auxiliary function, we have the followingform of reestimation for each transition (Rabiner1989).862layer(x) ~ )layer(i) @__ , _1 ,,,-, , , I l+l  NIFigure 3: I l lustration of Outside probability.expected no.
of transitions from i to jd~j =expected no.
of transitions front iThe expected frequency is defined for each ofthe thre(, types of transition.
For a terminal tran-sition,~N ?
E,.=~ aijb(ij, W,.)l'o(i,j),.~,.
Et( i j )  == r (w  I a)For a nontcrminal transition,alj Et( i j )~2~ e~(ik) + )2k e,.
(i~)For nonterminal transitions,aijE,a( i j )?2k e~(ik) + ?
;k e,.
(ik)And for pop transitions, notice that only poptransitions are possible at a pop state,__, ~N ~.~ aijPi(j).~~ta~,,I'o(i, v)~~t E~,ov(ij)P),~t(ij) =- '~=~ aij -~'( w I ~ ) E~ z,:,,o;,( i~ )- .+"lDh?7'~ '\[' = la .~t ( l f ,~y?? '
( j ) ) ,  ',J ~ bin(layer(j)) ,  For a terminal transition i j  aud ~I, observation1, ,y ,~( i )  = 1.y,~( .0) ,  l .
: , j~ .
( j ) :~  l~'r( , , l  y'"b?l "uv is apop  transit ion., -+Y'-,t .,.t.
wt=~, aijb(ij, Wt) l 'o( i ,  J)t~tFor a pop transition, b(ij, w) ~:~\]V  -~ )2t=~ aijb(ij, Wt)l'o(i,j)t~tEpov( ij ) :: v(w I a)where u E: bout(layer(i)),j (~ bin(layer(i)),v := f i rst ( layer( i ) ) ,l .
., j~,.(,.)
- l .y~,, .
( j ) ,l<,jc,,,(~)- l~y~,,(0,u'~ is a nonterminM transitiolt .Considering that tr~msitions of terminal andnonterminM types can occur together at a state,the reestim~tion \['or terminal tr~msitions i doneas follows:'fi le reestimation process co~ltinues until theprobabil ity of the observation sequences reaches acertain stability.
It is not nnusuM to assume thatthe tra.iHing set can be very large, and even growindefinitely in non trivial applications in whichcase additive traini~tg c~n be tried using a smooth-ing tectmiquc as in (Jarre and I'ieraccini \] 987).The complexity of \[Itside-Outside ~dgorithm isO(N a) both in the mnnber of states and inputlength (l~ari 1990).
The ei\[iciency comes from thefact that the algorithm successfully exploits thecontext-freeness, l!br instance, the ge~mration ofsubstrings by a nonterminal is independent of tit(;surroundings of the .aonterminal, and this is \]towthe product of the Inside and Outside probabil ities works and the COlnplexity is derived.8634.
Conc lus ionRecently several probabilistic parsing approacheshave been suggested such as SCFG, probabilis-tic GLR, and probabilistic link grammar (Laf-ferty, 1992).
Kupiec extended the reestimationalgorithm for SCFG to cover non-Chomsky nor-mal forms (Carroll, 1993).
This paper further ad-vances the trend by implanting the Inside-Outsidealgorithm on the plain topology of RTN whichdistinguishes itself from Kupiec's work.\[8\] Lari, K.; and Young, S. J.
(1991).
"Applica-tions of stochastic ontext-free grammars using the Inside-Outside algorithm."
ComputerSpeech and Language 5: 237-257.\[9\] Rabiner, Lawrence R. (1989).
A Tutorial onHidden Markov Models and Selected Applica-tions in Speech Recognition.
Proceedings of theIEEE ~27 (2).References\[1\] Baker, J. K. (1979).
Trainable Grammarsfor Speech I{~ecognition.
Speech CommunicationPapers for the 97th Meeting of the acousticalSociety of America (D.H. Klatt & J.J. Wolf,eds): 547-550.\[2\] Baum, L. E. (1972).
An Inequality and As-sociated Maximization Technique in Statisti-cal Estimation for Probabilistic Functions of aMarkov Process."
Inequalities 3: 1-8.\[3\] Carroll J., and Briscoe E. (1993).
General-ized probabilistic LR parsing of natural lan-guage (Corpora) with unification-based gram-mars.
ACL 19 (1).
25-59.\[4\] Jarre, A., and Pieraccini, R. (1987).
"SomeExperiments on HMM Speaker Adaptation,"Proceedings of ICASSP, paper 29.5.\[5\] John Lafferty., Daniel Sleator.
and Davy Tem-perley.
(1992).
Grammatical trigrams: a prob-abilistic model of link grammar.
In Proceedingsof AAAI Fall symposium on Probabilistie Ap-proaches to Natural Language Processing, Cam-bridge, MA.
89-97.\[6\] Jelinek, F. Lafferty, J. D. and Mercer R. L.(1990).
Basic Methods of Probabilistic ContextFree Grammars.
IBM RC 16374.
IBM Contin-uous Speech Recognition Group.\[7\] Kupiec, Julian (1991).
A Trellis-Based Algo-rithm For Estimating the Parameters of a ttid-den Stochastic Context-Free Grammar.
Pro-ceedings, Speech and Natural Language Work-shop.
sponsored by DARPA.
Pacific Grove:241-246.864
