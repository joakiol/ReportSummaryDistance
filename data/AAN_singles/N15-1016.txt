Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 153?163,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsCombining Language and Visionwith a Multimodal Skip-gram ModelAngeliki Lazaridou Nghia The Pham Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{angeliki.lazaridou|thenghia.pham|marco.baroni}@unitn.itAbstractWe extend the SKIP-GRAM model of Mikolovet al (2013a) by taking visual information intoaccount.
Like SKIP-GRAM, our multimodalmodels (MMSKIP-GRAM) build vector-basedword representations by learning to predictlinguistic contexts in text corpora.
However,for a restricted set of words, the models arealso exposed to visual representations of theobjects they denote (extracted from naturalimages), and must predict linguistic and visualfeatures jointly.
The MMSKIP-GRAM mod-els achieve good performance on a variety ofsemantic benchmarks.
Moreover, since theypropagate visual information to all words, weuse them to improve image labeling and re-trieval in the zero-shot setup, where the testconcepts are never seen during model training.Finally, the MMSKIP-GRAM models discoverintriguing visual properties of abstract words,paving the way to realistic implementations ofembodied theories of meaning.1 IntroductionDistributional semantic models (DSMs) derivevector-based representations of meaning from pat-terns of word co-occurrence in corpora.
DSMs havebeen very effectively applied to a variety of seman-tic tasks (Clark, 2015; Mikolov et al, 2013b; Turneyand Pantel, 2010).
However, compared to humansemantic knowledge, these purely textual models,just like traditional symbolic AI systems (Harnad,1990; Searle, 1984), are severely impoverished, suf-fering of lack of grounding in extra-linguistic modal-ities (Glenberg and Robertson, 2000).
This observa-tion has led to the development of multimodal dis-tributional semantic models (MDSMs) (Bruni et al,2014; Feng and Lapata, 2010; Silberer and Lapata,2014), that enrich linguistic vectors with perceptualinformation, most often in the form of visual fea-tures automatically induced from image collections.MDSMs outperform state-of-the-art text-basedapproaches, not only in tasks that directly requireaccess to visual knowledge (Bruni et al, 2012), butalso on general semantic benchmarks (Bruni et al,2014; Silberer and Lapata, 2014).
However, currentMDSMs still have a number of drawbacks.
First,they are generally constructed by first separatelybuilding linguistic and visual representations of thesame concepts, and then merging them.
This is ob-viously very different from how humans learn aboutconcepts, by hearing words in a situated perceptualcontext.
Second, MDSMs assume that both linguis-tic and visual information is available for all words,with no generalization of knowledge across modal-ities.
Third, because of this latter assumption offull linguistic and visual coverage, current MDSMs,paradoxically, cannot be applied to computer visiontasks such as image labeling or retrieval, since theydo not generalize to images or words beyond theirtraining set.We introduce the multimodal skip-gram models,two new MDSMs that address all the issues above.The models build upon the very effective skip-gramapproach of Mikolov et al (2013a), that constructsvector representations by learning, incrementally, topredict the linguistic contexts in which target wordsoccur in a corpus.
In our extension, for a subsetof the target words, relevant visual evidence from153natural images is presented together with the cor-pus contexts (just like humans hear words accompa-nied by concurrent perceptual stimuli).
The modelmust learn to predict these visual representationsjointly with the linguistic features.
The joint objec-tive encourages the propagation of visual informa-tion to representations of words for which no directvisual evidence was available in training.
The result-ing multimodally-enhanced vectors achieve remark-ably good performance both on traditional seman-tic benchmarks, and in their new application to the?zero-shot?
image labeling and retrieval scenario.Very interestingly, indirect visual evidence also af-fects the representation of abstract words, paving theway to ground-breaking cognitive studies and novelapplications in computer vision.2 Related WorkThere is by now a large literature on multimodaldistributional semantic models.
We focus here ona few representative systems.
Bruni et al (2014)propose a straightforward approach to MDSM in-duction, where text- and image-based vectors for thesame words are constructed independently, and then?mixed?
by applying the Singular Value Decompo-sition to their concatenation.
An empirically supe-rior model has been proposed by Silberer and La-pata (2014), who use more advanced visual repre-sentations relying on images annotated with high-level ?visual attributes?, and a multimodal fusionstrategy based on stacked autoencoders.
Kiela andBottou (2014) adopt instead a simple concatena-tion strategy, but obtain empirical improvements byusing state-of-the-art convolutional neural networksto extract visual features, and the skip-gram modelfor text.
These and related systems take a two-stage approach to derive multimodal spaces (uni-modal induction followed by fusion), and they areonly tested on concepts for which both textual andvisual labeled training data are available (the pio-neering model of Feng and Lapata (2010) did learnfrom text and images jointly using Topic Models,but was shown to be empirically weak by Bruni etal.
(2014)).Howell et al (2005) propose an incremental mul-timodal model based on simple recurrent networks(Elman, 1990), focusing on grounding propagationfrom early-acquired concrete words to a larger vo-cabulary.
However, they use subject-generated fea-tures as surrogate for realistic perceptual informa-tion, and only test the model in small-scale simula-tions of word learning.
Hill and Korhonen (2014),whose evaluation focuses on how perceptual infor-mation affects different word classes more or lesseffectively, similarly to Howell et al, integrate per-ceptual information in the form of subject-generatedfeatures and text from image annotations into a skip-gram model.
They inject perceptual informationby merging words expressing perceptual featureswith corpus contexts, which amounts to linguistic-context re-weighting, thus making it impossible toseparate linguistic and perceptual aspects of the in-duced representation, and to extend the model withnon-linguistic features.
We use instead authentic im-age analysis as proxy to perceptual information, andwe design a robust way to incorporate it, easily ex-tendible to other signals, such as feature norm orbrain signal vectors (Fyshe et al, 2014).The recent work on so-called zero-shot learningto address the annotation bottleneck in image la-beling (Frome et al, 2013; Lazaridou et al, 2014;Socher et al, 2013) looks at image- and text-basedvectors from a different perspective.
Instead of com-bining visual and linguistic information in a com-mon space, it aims at learning a mapping fromimage- to text-based vectors.
The mapping, inducedfrom annotated data, is then used to project imagesof objects that were not seen during training ontolinguistic space, in order to retrieve the nearest wordvectors as labels.
Multimodal word vectors shouldbe better-suited than purely text-based vectors forthe task, as their similarity structure should be closerto that of images.
However, traditional MDSMs can-not be used in this setting, because they do not coverwords for which no manually annotated training im-ages are available, thus defeating the generalizingpurpose of zero-shot learning.
We will show be-low that our multimodal vectors, that are not ham-pered by this restriction, do indeed bring a signifi-cant improvement over purely text-based linguisticrepresentations in the zero-shot setup.Multimodal language-vision spaces have alsobeen developed with the goal of improving cap-tion generation/retrieval and caption-based imageretrieval (Karpathy et al, 2014; Kiros et al, 2014;154Mao et al, 2014; Socher et al, 2014).
These meth-ods rely on necessarily limited collections of cap-tioned images as sources of multimodal evidence,whereas we automatically enrich a very large corpuswith images to induce general-purpose multimodalword representations, that could be used as inputembeddings in systems specifically tuned to captionprocessing.
Thus, our work is complementary to thisline of research.3 Multimodal Skip-gram Architecture3.1 Skip-gram ModelWe start by reviewing the standard SKIP-GRAMmodel of Mikolov et al (2013a), in the versionwe use.
Given a text corpus, SKIP-GRAM aimsat inducing word representations that are good atpredicting the context words surrounding a targetword.
Mathematically, it maximizes the objectivefunction:1TT?t=1???
?c?j?c,j 6=0log p(wt+j|wt)??
(1)where w1, w2, ..., wTare words in the trainingcorpus and c is the size of the window aroundtarget wt, determining the set of context words tobe predicted by the induced representation of wt.Following Mikolov et al, we implement a subsam-pling option randomly discarding context words asan inverse function of their frequency, controlled byhyperparameter t. The probability p(wt+j|wt), thecore part of the objective in Equation 1, is given bysoftmax:p(wt+j|wt) =eu?wt+jTuwt?Ww?=1eu?w?Tuwt(2)where uwand u?ware the context and target vectorrepresentations of word w respectively, and W isthe size of the vocabulary.
Due to the normaliza-tion term, Equation 2 requires O(|W |) time com-plexity.
A considerable speedup to O(log |W |), isachieved by using the hierarchical version of Equa-tion 2 (Morin and Bengio, 2005), adopted here.3.2 Injecting visual knowledgeWe now assume that word learning takes place in asituated context, in which, for a subset of the targetwords, the corpus contexts are accompanied by athe cutecatsat on the matlittle CAT +=maximize context prediction maximize similaritymap to visual spaceFigure 1: ?Cartoon?
of MMSKIP-GRAM-B.
Lin-guistic context vectors are actually associated toclasses of words in a tree, not single words.
SKIP-GRAM is obtained by ignoring the visual objective,MMSKIP-GRAM-A by fixing Mu?vto the identitymatrix.visual representation of the concepts they denote(just like in a conversation, where a linguisticutterance will often be produced in a visual sceneincluding some of the word referents).
The visualrepresentation is also encoded in a vector (wedescribe in Section 4 below how we constructit).
We thus make the skip-gram ?multimodal?
byadding a second, visual term to the original linguis-tic objective, that is, we extend Equation 1 as follow:1TT?t=1(Lling(wt) + Lvision(wt)) (3)where Lling(wt) is the text-based skip-gram ob-jective?
?c?j?c,j 6=0log p(wt+j|wt), whereas theLvision(wt) term forces word representations to takevisual information into account.
Note that if a wordwtis not associated to visual information, as issystematically the case, e.g., for determiners andnon-imageable nouns, but also more generally forany word for which no visual data are available,Lvision(wt) is set to 0.We now propose two variants of the visual objec-tive, resulting in two distinguished multi-modal ver-sions of the skip-gram model.3.3 Multi-modal Skip-gram Model AOne way to force word embeddings to take visualrepresentations into account is to try to directlyincrease the similarity (expressed, for example,by the cosine) between linguistic and visual rep-155resentations, thus aligning the dimensions of thelinguistic vector with those of the visual one (recallthat we are inducing the first, while the second isfixed), and making the linguistic representation of aconcept ?move?
closer to its visual representation.We maximize similarity through a max-marginframework commonly used in models connectinglanguage and vision (Weston et al, 2010; Frome etal., 2013).
More precisely, we formulate the visualobjective Lvision(wt) as:??w?
?Pn(w)max(0, ??
cos(uwt, vwt)+ cos(uwt, vw?))
(4)where the minus sign turns a loss into a cost, ?
isthe margin, uwtis the target multimodally-enhancedword representation we aim to learn, vwtis the cor-responding visual vector (fixed in advance) and vw?ranges over visual representations of words (fea-tured in our image dictionary) randomly sampledfrom distribution Pn(wt).
These random visual rep-resentations act as ?negative?
samples, encouraginguwtto be more similar to its own visual representa-tion than to that of other words.
The sampling distri-bution is currently set to uniform, and the number ofnegative samples controlled by hyperparameter k.3.4 Multi-modal Skip-gram Model BThe visual objective in MMSKIP-GRAM-A has thedrawback of assuming a direct comparison of lin-guistic and visual representations, constraining themto be of equal size.
MMSKIP-GRAM-B lifts thisconstraint by including an extra layer mediating be-tween linguistic and visual representations (see Fig-ure 1 for a sketch of MMSKIP-GRAM-B).
Learningthis layer is equivalent to estimating a cross-modalmapping matrix from linguistic onto visual repre-sentations, jointly induced with linguistic word em-beddings.
The extension is straightforwardly imple-mented by substituting, into Equation 4, the wordrepresentation uwtwith zwt= Mu?vuwt, whereMu?vis the cross-modal mapping matrix to be in-duced.
To avoid overfitting, we also add an L2 reg-ularization term for Mu?vto the overall objective(Equation 3), with its relative importance controlledby hyperparamer ?.4 Experimental SetupThe parameters of all models are estimated by back-propagation of error via stochastic gradient descent.Our text corpus is a Wikipedia 2009 dump compris-ing approximately 800M tokens.1To train the multi-modal models, we add visual information for 5,100words that have an entry in ImageNet (Deng et al,2009), occur at least 500 times in the corpus andhave concreteness score ?
0.5 according to Turneyet al (2011).
On average, about 5% tokens in thetext corpus are associated to a visual representation.To construct the visual representation of a word, wesample 100 pictures from its ImageNet entry, andextract a 4096-dimensional vector from each pictureusing the Caffe toolkit (Jia et al, 2014), togetherwith the pre-trained convolutional neural network ofKrizhevsky et al (2012).
The vector correspondsto activation in the top (FC7) layer of the network.Finally, we average the vectors of the 100 picturesassociated to each word, deriving 5,100 aggregatedvisual representations.Hyperparameters For both SKIP-GRAM and theMMSKIP-GRAM models, we fix hidden layer sizeto 300.
To facilitate comparison between MMSKIP-GRAM-A and MMSKIP-GRAM-B, and since the for-mer requires equal linguistic and visual dimension-ality, we keep the first 300 dimensions of the visualvectors.
For the linguistic objective, we use hierar-chical softmax with a Huffman frequency-based en-coding tree, setting frequency subsampling optiont= 0.001 and window size c= 5, without tuning.The following hyperparameters were tuned on thetext9 corpus:2MMSKIP-GRAM-A: k=20, ?=0.5;MMSKIP-GRAM-B: k=5, ?=0.5, ?=0.0001.5 Experiments5.1 Approximating human judgmentsBenchmarks A widely adopted way to test DSMsand their multimodal extensions is to measure howwell model-generated scores approximate humansimilarity judgments about pairs of words.
We puttogether various benchmarks covering diverse as-pects of meaning, to gain insights on the effect ofperceptual information on different similarity facets.Specifically, we test on general relatedness (MEN,Bruni et al (2014), 3K pairs), e.g., pickles are re-lated to hamburgers, semantic (?
taxonomic) simi-1http://wacky.sslmit.unibo.it2http://mattmahoney.net/dc/textdata.html156larity (Simlex-999, Hill et al (2014), 1K pairs; Sem-Sim, Silberer and Lapata (2014), 7.5K pairs), e.g.,pickles are similar to onions, as well as visual sim-ilarity (VisSim, Silberer and Lapata (2014), samepairs as SemSim with different human ratings), e.g.,pickles look like zucchinis.Alternative Multimodal Models We compareour models against several recent alternatives.
Wetest the vectors made available by Kiela and Bottou(2014).
Similarly to us, they derive textual featureswith the skip-gram model (from a portion of theWikipedia and the British National Corpus) and usevisual representations extracted from the ESP data-set (von Ahn and Dabbish, 2004) through a convo-lutional neural network (Oquab et al, 2014).
Theyconcatenate textual and visual features after normal-izing to unit length and centering to zero mean.
Wealso test the vectors that performed best in the evalu-ation of Bruni et al (2014), based on textual featuresextracted from a 3B-token corpus and SIFT-basedBag-of-Visual-Words visual features (Sivic and Zis-serman, 2003) extracted from the ESP collection.Bruni and colleagues fuse a weighted concatenationof the two components through SVD.
We further re-implement both methods with our own textual andvisual embeddings as CONCATENATION and SVD(with target dimensionality 300, picked without tun-ing).
Finally, we present for comparison the resultson SemSim and VisSim reported by Silberer and La-pata (2014), obtained with a stacked-autoencodersarchitecture run on textual features extracted fromWikipedia with the Strudel algorithm (Baroni et al,2010) and attribute-based visual features (Farhadi etal., 2009) extracted from ImageNet.All benchmarks contain a fair amount of wordsfor which we did not use direct visual evidence.
Weare interested in assessing the models both in termsof how they fuse linguistic and visual evidence whenthey are both available, and for their robustness inlack of full visual coverage.
We thus evaluate themin two settings.
The visual-coverage columns of Ta-ble 1 (those on the right) report results on the subsetsfor which all compared models have access to directvisual information for both words.
We further reportresults on the full sets (?100%?
columns of Table1) for models that can propagate visual informationand that, consequently, can meaningfully be testedon words without direct visual representations.Results The state-of-the-art visual CNN FEA-TURES alone perform remarkably well, outperform-ing the purely textual model (SKIP-GRAM) in twotasks, and achieving the best absolute performanceon the visual-coverage subset of Simlex-999.
Re-garding multimodal fusion (that is, focusing onthe visual-coverage subsets), both MMSKIP-GRAMmodels perform very well, at the top or just belowit on all tasks, with comparable results for the twovariants.
Their performance is also good on thefull data sets, where they consistently outperformSKIP-GRAM and SVD (that is much more stronglyaffected by lack of complete visual information).They?re just a few points below the state-of-the-artMEN correlation (0.8), achieved by Baroni et al(2014) with a corpus 3 larger than ours and exten-sive tuning.
MMSKIP-GRAM-B is close to the stateof the art for Simlex-999, reported by the resourcecreators to be at 0.41 (Hill et al, 2014).
Most im-pressively, MMSKIP-GRAM-A reaches the perfor-mance level of the Silberer and Lapata (2014) modelon their SemSim and VisSim data sets, despite thefact that the latter has full visual-data coverage anduses attribute-based image representations, requir-ing supervised learning of attribute classifiers, thatachieve performance in the semantic tasks compa-rable or higher than that of our CNN features (seeTable 3 in Silberer and Lapata (2014)).
Finally, ifthe multimodal models (unsurprisingly) bring abouta large performance gain over the purely linguisticmodel on visual similarity, the improvement is con-sistently large also for the other benchmarks, con-firming that multimodality leads to better semanticmodels in general, that can help in capturing differ-ent types of similarity (general relatedness, strictlytaxonomic, perceptual).While we defer to further work a better un-derstanding of the relation between multimodalgrounding and different similarity relations, Table2 provides qualitative insights on how injectingvisual information changes the structure of se-mantic space.
The top SKIP-GRAM neighbours ofdonuts are places where you might encounter them,whereas the multimodal models relate them to othertake-away food, ranking visually-similar pizzas atthe top.
The owl example shows how multimodal157ModelMEN Simlex-999 SemSim VisSim100% 42% 100% 29% 100% 85% 100% 85%KIELA AND BOTTOU - 0.74 - 0.33 - 0.60 - 0.50BRUNI ET AL.
- 0.77 - 0.44 - 0.69 - 0.56SILBERER AND LAPATA - - - - 0.70 - 0.64 -CNN FEATURES - 0.62 - 0.54 - 0.55 - 0.56SKIP-GRAM 0.70 0.68 0.33 0.29 0.62 0.62 0.48 0.48CONCATENATION - 0.74 - 0.46 - 0.68 - 0.60SVD 0.61 0.74 0.28 0.46 0.65 0.68 0.58 0.60MMSKIP-GRAM-A 0.75 0.74 0.37 0.50 0.72 0.72 0.63 0.63MMSKIP-GRAM-B 0.74 0.76 0.40 0.53 0.66 0.68 0.60 0.60Table 1: Spearman correlation between model-generated similarities and human judgments.
Right columnsreport correlation on visual-coverage subsets (percentage of original benchmark covered by subsets on firstrow of respective columns).
First block reports results for out-of-the-box models; second block for visualand textual representations alone; third block for our implementation of multimodal models.Target SKIP-GRAM MMSKIP-GRAM-A MMSKIP-GRAM-Bdonut fridge, diner, candy pizza, sushi, sandwich pizza, sushi, sandwichowl pheasant, woodpecker, squirrel eagle, woodpecker, falcon eagle, falcon, hawkmural sculpture, painting, portrait painting, portrait, sculpture painting, portrait, sculpturetobacco coffee, cigarette, corn cigarette, cigar, corn cigarette, cigar, smokingdepth size, bottom, meter sea, underwater, level sea, size, underwaterchaos anarchy, despair, demon demon, anarchy, destruction demon, anarchy, shadowTable 2: Ordered top 3 neighbours of example words in purely textual and multimodal spaces.
Only donutand owl were trained with direct visual information.models pick taxonomically closer neighbours ofconcrete objects, since often closely related thingsalso look similar (Bruni et al, 2014).
In particular,both multimodal models get rid of squirrels andoffer other birds of prey as nearest neighbours.No direct visual evidence was used to induce theembeddings of the remaining words in the table, thatare thus influenced by vision only by propagation.The subtler but systematic changes we observe insuch cases suggest that this indirect propagationis not only non-damaging with respect to purelylinguistic representations, but actually beneficial.For the concrete mural concept, both multimodalmodels rank paintings and portraits above lessclosely related sculptures (they are not a form ofpainting).
For tobacco, both models rank cigarettesand cigar over coffee, and MMSKIP-GRAM-Bavoids the arguably less common ?crop?
sensecued by corn.
The last two examples show how themultimodal models turn up the embodiment levelin their representation of abstract words.
For depth,their neighbours suggest a concrete marine setupover the more abstract measurement sense pickedby the MMSKIP-GRAM neighbours.
For chaos,they rank a demon, that is, a concrete agent of chaosat the top, and replace the more abstract notion ofdespair with equally gloomy but more imageableshadows and destruction (more on abstract wordsbelow).5.2 Zero-shot image labeling and retrievalThe multimodal representations induced by ourmodels should be better suited than purely text-based vectors to label or retrieve images.
In particu-lar, given that the quantitative and qualitative resultscollected so far suggest that the models propagatevisual information across words, we apply them toimage labeling and retrieval in the challenging zero-shot setup (see Section 2 above).33We will refer here, for conciseness?
sake, to image label-ing/retrieval, but, as our visual vectors are aggregated represen-tations of images, the tasks we?re modeling consist, more pre-cisely, in labeling a set of pictures denoting the same object andretrieving the corresponding set given the name of the object.158Setup We take out as test set 25% of the 5.1Kwords we have visual vectors for.
The multimodalmodels are re-trained without visual vectors forthese words, using the same hyperparameters asabove.
For both tasks, the search for the correctword label/image is conducted on the whole set of5.1K word/visual vectors.In the image labeling task, given a visual vectorrepresenting an image, we map it onto word space,and label the image with the word correspondingto the nearest vector.
To perform the vision-to-language mapping, we train a Ridge regression by 5-fold cross-validation on the test set (for SKIP-GRAMonly, we also add the remaining 75% of word-imagevector pairs used in estimating the multimodal mod-els to the Ridge training data).4In the image retrieval task, given a linguis-tic/multimodal vector, we map it onto visual space,and retrieve the nearest image.
For SKIP-GRAM, weuse Ridge regression with the same training regimeas for the labeling task.
For the multimodal mod-els, since maximizing similarity to visual represen-tations is already part of their training objective, wedo not fit an extra mapping function.
For MMSKIP-GRAM-A, we directly look for nearest neighboursof the learned embeddings in visual space.
ForMMSKIP-GRAM-B, we use the Mu?vmappingfunction induced while learning word embeddings.Results In image labeling (Table 3) SKIP-GRAMis outperformed by both multimodal models, con-firming that these models produce vectors that aredirectly applicable to vision tasks thanks to visualpropagation.
The most interesting results howeverare achieved in image retrieval (Table 4), whichis essentially the task the multimodal models havebeen implicitly optimized for, so that they could beapplied to it without any specific training.
The strat-egy of directly querying for the nearest visual vec-tors of the MMSKIP-GRAM-A word embeddingsworks remarkably well, outperforming on the higherranks SKIP-GRAM, which requires an ad-hoc map-ping function.
This suggests that the multimodal4We use one fold to tune Ridge ?, three to estimate the map-ping matrix and test in the last fold.
To enforce strict zero-shotconditions, we exclude from the test fold labels occurring inthe LSVRC2012 set that was employed to train the CNN ofKrizhevsky et al (2012), that we use to extract visual features.P@1 P@2 P@10 P@20 P@50SKIP-GRAM 1.5 2.6 14.2 23.5 36.1MMSKIP-GRAM-A 2.1 3.7 16.7 24.6 37.6MMSKIP-GRAM-B 2.2 5.1 20.2 28.5 43.5Table 3: Percentage precision@k results in the zero-shot image labeling task.P@1 P@2 P@10 P@20 P@50SKIP-GRAM 1.9 3.3 11.5 18.5 30.4MMSKIP-GRAM-A 1.9 3.2 13.9 20.2 33.6MMSKIP-GRAM-B 1.9 3.8 13.2 22.5 38.3Table 4: Percentage precision@k results in the zero-shot image retrieval task.embeddings we are inducing, while general enoughto achieve good performance in the semantic tasksdiscussed above, encode sufficient visual informa-tion for direct application to image analysis tasks.This is especially remarkable because the word vec-tors we are testing were not matched with visualrepresentations at model training time, and are thusmultimodal only by propagation.
The best perfor-mance is achieved by MMSKIP-GRAM-B, confirm-ing our claim that its Mu?vmatrix acts as a multi-modal mapping function.5.3 Abstract wordsWe have already seen, through the depth and chaosexamples of Table 2, that the indirect influence ofvisual information has interesting effects on the rep-resentation of abstract terms.
The latter have re-ceived little attention in multimodal semantics, withHill and Korhonen (2014) concluding that abstractnouns, in particular, do not benefit from propagatedperceptual information, and their representation iseven harmed when such information is forced onthem (see Figure 4 of their paper).
Still, embod-ied theories of cognition have provided considerableevidence that abstract concepts are also groundedin the senses (Barsalou, 2008; Lakoff and John-son, 1999).
Since the word representations producedby MMSKIP-GRAM-A, including those pertainingto abstract concepts, can be directly used to searchfor near images in visual space, we decided to ver-ify, experimentally, if these near images (of concretethings) are relevant not only for concrete words, as159expected, but also for abstract ones, as predicted byembodied views of meaning.More precisely, we focused on the set of 200words that were sampled across the USF norms con-creteness spectrum by Kiela et al (2014) (2 wordshad to be excluded for technical reasons).
Thisset includes not only concrete (meat) and abstract(thought) nouns, but also adjectives (boring), verbs(teach), and even grammatical terms (how).
Somewords in the set have relatively high concretenessratings, but are not particularly imageable, e.g.
:hot, smell, pain, sweet.
For each word in the set,we extracted the nearest neighbour picture of itsMMSKIP-GRAM-A representation, and matched itwith a random picture.
The pictures were selectedfrom a set of 5,100, all labeled with distinct words(the picture set includes, for each of the words as-sociated to visual information as described in Sec-tion 4, the nearest picture to its aggregated visualrepresentation).
Since it is much more common forconcrete than abstract words to be directly repre-sented by an image in the picture set, when search-ing for the nearest neighbour we excluded the pic-ture labeled with the word of interest, if present (e.g.,we excluded the picture labeled tree when pickingthe nearest neighbour of the word tree).
We ran aCrowdFlower5survey in which we presented eachtest word with the two associated images (random-izing presentation order of nearest and random pic-ture), and asked subjects which of the two picturesthey found more closely related to the word.
Wecollected minimally 20 judgments per word.
Sub-jects showed large agreement (median proportion ofmajority choice at 90%), confirming that they under-stood the task and behaved consistently.We quantify performance in terms of proportionof words for which the number of votes for the near-est neighbour picture is significantly above chanceaccording to a two-tailed binomial test.
We set sig-nificance at p<0.05 after adjusting all p-values withthe Holm correction for running 198 statistical tests.The results in Table 5 indicate that, in about halfthe cases, the nearest picture to a word MMSKIP-GRAM-A representation is meaningfully related tothe word.
As expected, this is more often the case forconcrete than abstract words.
Still, we also observe a5http://www.crowdflower.comglobal |words| unseen |words|all 48% 198 30% 127concrete 73% 99 53% 30abstract 23% 99 23% 97Table 5: Subjects?
preference for nearest visualneighbour of words in Kiela et al (2014) vs. randompictures.
Figure of merit is percentage proportionof significant results in favor of nearest neighbouracross words.
Results are reported for the whole set,as well as for words above (concrete) and below (ab-stract) the concreteness rating median.
The unseencolumn reports results when words exposed to directvisual evidence during training are discarded.
Thewords columns report set cardinality.freedom theorygod together placewrongFigure 2: Examples of nearest visual neighbours ofsome abstract words: on the left, cases where sub-jects preferred the neighbour to the random foil; onthe right, cases where they did not.significant preference for the model-predicted near-est picture for about one fourth of the abstract terms.Whether a word was exposed to direct visual evi-dence during training is of course making a big dif-ference, and this factor interacts with concreteness,as only two abstract words were matched with im-ages during training.6When we limit evaluation toword representations that were not exposed to pic-tures during training, the difference between con-crete and abstract terms, while still large, becomesless dramatic than if all words are considered.Figure 2 shows four cases in which subjects ex-pressed a strong preference for the nearest visualneighbour of a word.
Freedom, god and theory arestrikingly in agreement with the view, from embod-ied theories, that abstract words are grounded in rel-6In both cases, the images actually depict concrete senses ofthe words: a memory board for memory and a stop sign for stop.160evant concrete scenes and situations.
The togetherexample illustrates how visual data might ground ab-stract notions in surprising ways.
For all these cases,we can borrow what Howell et al (2005) say aboutvisual propagation to abstract words (p. 260):Intuitively, this is something like trying to explainan abstract concept like love to a child by usingconcrete examples of scenes or situations that areassociated with love.
The abstract concept is neverfully grounded in external reality, but it does inheritsome meaning from the more concrete concepts towhich it is related.Of course, not all examples are good: the last col-umn of Figure 2 shows cases with no obvious rela-tion between words and visual neighbours (subjectspreferred the random images by a large margin).The multimodal vectors we induce also display aninteresting intrinsic property related to the hypothe-sis that grounded representations of abstract wordsare more complex than for concrete ones, since ab-stract concepts relate to varied and composite situa-tions (Barsalou and Wiemer-Hastings, 2005).
A nat-ural corollary of this idea is that visually-groundedrepresentations of abstract concepts should be morediverse: If you think of dogs, very similar images ofspecific dogs will come to mind.
You can also imag-ine the abstract notion of freedom, but the nature ofthe related imagery will be much more varied.
Re-cently, Kiela et al (2014) have proposed to measureabstractness by exploiting this very same intuition.However, they rely on manual annotation of picturesvia Google Images and define an ad-hoc measureof image dispersion.
We conjecture that the repre-sentations naturally induced by our models displaya similar property.
In particular, the entropy of ourmultimodal vectors, being an expression of how var-ied the information they encode is, should correlatewith the degree of abstractness of the correspondingwords.
As Figure 3(a) shows, there is indeed a dif-ference in entropy between the most concrete (meat)and most abstract (hope) words in the Kiela et al set.To test the hypothesis quantitatively, we mea-sure the correlation of entropy and concretenesson the 200 words in the Kiela et al (2014) set.7Figure 3(b) shows that the entropies of both the7Since the vector dimensions range over the real numberline, we calculate entropy on vectors that are unit-normed af-ter adding a small constant insuring all values are positive.
(a)Model ?WORD FREQUENCY 0.22KIELA ET AL.
-0.65SKIP-GRAM 0.05MMSKIP-GRAM-B 0.04MMSKIP-GRAM-A -0.75MMSKIP-GRAM-B* -0.71(b)Figure 3: (a) Distribution of MMSKIP-GRAM-Avector activation for meat (blue) and hope (red).
(b)Spearman ?
between concreteness and various mea-sures on the Kiela et al (2014) set.MMSKIP-GRAM-A representations and those gen-erated by mapping MMSKIP-GRAM-B vectors ontovisual space (MMSKIP-GRAM-B*) achieve veryhigh correlation (but, interestingly, not MMSKIP-GRAM-B).
This is further evidence that multimodallearning is grounding the representations of bothconcrete and abstract words in meaningful ways.6 ConclusionWe introduced two multimodal extensions of SKIP-GRAM.
MMSKIP-GRAM-A is trained by directlyoptimizing the similarity of words with their visualrepresentations, thus forcing maximum interactionbetween the two modalities.
MMSKIP-GRAM-B in-cludes an extra mediating layer, acting as a cross-modal mapping component.
The ability of the mod-els to integrate and propagate visual information re-sulted in word representations that performed well inboth semantic and vision tasks, and could be used asinput in systems benefiting from prior visual knowl-edge (e.g., caption generation).
Our results with ab-stract words suggest the models might also help intasks such as metaphor detection, or even retriev-ing/generating pictures of abstract concepts.
Theirincremental nature makes them well-suited for cog-nitive simulations of grounded language acquisition,an avenue of research we plan to explore further.AcknowledgmentsWe thank Adam Liska, Tomas Mikolov, the re-viewers and the NIPS 2014 Learning Semantics au-dience.
We were supported by ERC 2011 Start-ing Independent Research Grant n. 283554 (COM-POSES).161ReferencesMarco Baroni, Eduard Barbu, Brian Murphy, and Mas-simo Poesio.
2010.
Strudel: A distributional semanticmodel based on properties and types.
Cognitive Sci-ence, 34(2):222?254.Marco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of ACL, pages 238?247,Baltimore, MD.Lawrence Barsalou and Katja Wiemer-Hastings.
2005.Situating abstract concepts.
In D. Pecher andR.
Zwaan, editors, Grounding Cognition: The Roleof Perception and Action in Memory, Language, andThought, pages 129?163.
Cambridge University Press,Cambridge, UK.Lawrence Barsalou.
2008.
Grounded cognition.
AnnualReview of Psychology, 59:617?645.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012.
Distributional semantics inTechnicolor.
In Proceedings of ACL, pages 136?145,Jeju Island, Korea.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Arti-ficial Intelligence Research, 49:1?47.Stephen Clark.
2015.
Vector space models oflexical meaning.
In Shalom Lappin and ChrisFox, editors, Handbook of Contemporary Seman-tics, 2nd ed.
Blackwell, Malden, MA.
Inpress; http://www.cl.cam.ac.uk/?sc609/pubs/sem_handbook.pdf.Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, andLi Fei-Fei.
2009.
Imagenet: A large-scale hierarchi-cal image database.
In Proceedings of CVPR, pages248?255, Miami Beach, FL.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their attributes.In Proceedings of CVPR, pages 1778?1785, MiamiBeach, FL.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedings ofHLT-NAACL, pages 91?99, Los Angeles, CA.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semantic em-bedding model.
In Proceedings of NIPS, pages 2121?2129, Lake Tahoe, NV.Alona Fyshe, Partha P Talukdar, Brian Murphy, andTom M Mitchell.
2014.
Interpretable semantic vec-tors from a joint model of brain-and text-based mean-ing.
In In Proceedings of ACL, pages 489?499.Arthur Glenberg and David Robertson.
2000.
Sym-bol grounding and meaning: A comparison of high-dimensional and embodied theories of meaning.
Jour-nal of Memory and Language, 3(43):379?401.Stevan Harnad.
1990.
The symbol grounding problem.Physica D: Nonlinear Phenomena, 42(1-3):335?346.Felix Hill and Anna Korhonen.
2014.
Learning abstractconcept embeddings from multi-modal data: Sinceyou probably can?t see what I mean.
In Proceedingsof EMNLP, pages 255?265, Doha, Qatar.Felix Hill, Roi Reichart, and Anna Korhonen.2014.
SimLex-999: Evaluating semanticmodels with (genuine) similarity estimation.http://arxiv.org/abs/arXiv:1408.3456.Steve Howell, Damian Jankowicz, and Suzanna Becker.2005.
A model of grounded language acquisition:Sensorimotor features improve lexical and grammat-ical learning.
Journal of Memory and Language,53:258?276.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Convo-lutional architecture for fast feature embedding.
arXivpreprint arXiv:1408.5093.Andrej Karpathy, Armand Joulin, and Li Fei-Fei.
2014.Deep fragment embeddings for bidirectional imagesentence mapping.
In Proceedings of NIPS, pages1097?1105, Montreal, Canada.Douwe Kiela and L?eon Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In Proceedings ofEMNLP, pages 36?45, Doha, Qatar.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representationsusing image dispersion: Why less is sometimes more.In Proceedings of ACL, pages 835?841, Baltimore,MD.Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel.2014.
Unifying visual-semantic embeddings withmultimodal neural language models.
In Proceed-ings of the NIPS Deep Learning and Representa-tion Learning Workshop, Montreal, Canada.
Pub-lished online: http://www.dlworkshop.org/accepted-papers.Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.2012.
ImageNet classification with deep convolutionalneural networks.
In Proceedings of NIPS, pages 1097?1105, Lake Tahoe, Nevada.George Lakoff and Mark Johnson.
1999.
Philosophy inthe Flesh: The Embodied Mind and Its Challenge toWestern Thought.
Basic Books, New York.Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014.Is this a wampimuk?
cross-modal mapping between162distributional semantics and the visual world.
In Pro-ceedings of ACL, pages 1403?1414, Baltimore, MD.Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and AlanYuille.
2014.
Explain images with multimodal re-current neural networks.
In Proceedings of the NIPSDeep Learning and Representation Learning Work-shop, Montreal, Canada.
Published online: http://www.dlworkshop.org/accepted-papers.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeff Dean.
2013a.
Distributed representations ofwords and phrases and their compositionality.
In Pro-ceedings of NIPS, pages 3111?3119, Lake Tahoe, NV.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.Frederic Morin and Yoshua Bengio.
2005.
Hierarchicalprobabilistic neural network language model.
In Pro-ceedings of AISTATS, pages 246?252, Barbados.Maxime Oquab, Leon Bottou, Ivan Laptev, and JosefSivic.
2014.
Learning and transferring mid-levelimage representations using convolutional neural net-works.
In Proceedings of CVPR.John Searle.
1984.
Minds, Brains and Science.
HarvardUniversity Press, Cambridge, MA.Carina Silberer and Mirella Lapata.
2014.
Learninggrounded meaning representations with autoencoders.In Proceedings of ACL, pages 721?732, Baltimore,Maryland.Josef Sivic and Andrew Zisserman.
2003.
Video Google:A text retrieval approach to object matching in videos.In Proceedings of ICCV, pages 1470?1477, Nice,France.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, NV.Richard Socher, Quoc Le, Christopher Manning, andAndrew Ng.
2014.
Grounded compositional se-mantics for finding and describing images with sen-tences.
Transactions of the Association for Computa-tional Linguistics, 2:207?218.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identifi-cation through concrete and abstract context.
In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of CHI,pages 319?326, Vienna, Austria.Jason Weston, Samy Bengio, and Nicolas Usunier.
2010.Large scale image annotation: learning to rank withjoint word-image embeddings.
Machine learning,81(1):21?35.163
