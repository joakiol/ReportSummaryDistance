Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 99?104,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval?07 Task 19: Frame Semantic Structure ExtractionCollin Baker, Michael EllsworthInternational Computer Science InstituteBerkeley, California{collinb,infinity}@icsi.berkeley.eduKatrin ErkComputer Science Dept.University of TexasAustinkatrin.erk@mail.utexas.eduAbstractThis task consists of recognizing wordsand phrases that evoke semantic frames asdefined in the FrameNet project (http://framenet.icsi.berkeley.edu),and their semantic dependents, which areusually, but not always, their syntacticdependents (including subjects).
The train-ing data was FN annotated sentences.
Intesting, participants automatically annotatedthree previously unseen texts to match goldstandard (human) annotation, including pre-dicting previously unseen frames and roles.Precision and recall were measured both formatching of labels of frames and FEs andfor matching of semantic dependency treesbased on the annotation.1 IntroductionThe task of labeling frame-evoking words with ap-propriate frames is similar to WSD, while the task ofassigning frame elements is called Semantic RoleLabeling (SRL), and has been the subject of severalshared tasks at ACL and CoNLL.
For example, inthe sentence ?Matilde said, ?I rarely eat rutabaga,?
?said evokes the Statement frame, and eat evokesthe Ingestion frame.
The role of SPEAKER in theStatement frame is filled by Matilda, and the roleof MESSAGE, by the whole quotation.
In the Inges-tion frame, I is the INGESTOR and rutabaga fills theINGESTIBLES role.
Since the ingestion event is con-tained within the MESSAGE of the Statement event,we can represent the fact that the message conveyedwas about ingestion, just by annotating the sentencewith respect to these two frames.After training on FN annotations, the participants?systems labeled three new texts automatically.
Theevaluation measured precision and recall for framesand frame elements, with partial credit for incorrectbut closely related frames.
Two types of evaluationwere carried out: Label matching evaluation, inwhich the participant?s labeled data was compareddirectly with the gold standard labeled data, and Se-mantic dependency evaluation, in which both thegold standard and the submitted data were first con-verted to semantic dependency graphs in XML for-mat, and then these graphs were compared.There are three points that make this task harderand more interesting than earlier SRL tasks: (1)while previous tasks focused on role assignment, thecurrent task also comprises the identification of theappropriate FrameNet frame, similar to WSD, (2)the task comprises not only the labeling of individ-ual predicates and their arguments, but also the inte-gration of all labels into an overall semantic depen-dency graph, a partial semantic representation ofthe overall sentence meaning based on frames androles, and (3) the test data includes occurrences offrames that are not seen in the training data.
Forthese cases, participant systems have to identify theclosest known frame.
This is a very realistic sce-nario, encouraging the development of robust sys-tems showing graceful degradation in the face of un-known events.992 Frame semantics and FrameNetThe basic concept of Frame Semantics is that manywords are best understood as part of a group ofterms that are related to a particular type of eventand the participants and ?props?
involved in it (Fill-more, 1976; Fillmore, 1982).
The classes of eventsare the semantic frames of the lexical units (LUs)that evoke them, and the roles associated with theevent are referred to as frame elements (FEs).
Thesame type of analysis applies not only to events butalso to relations and states; the frame-evoking ex-pressions may be single words or multi-word ex-pressions, which may be of any syntactic category.Note that these FE names are quite frame-specific;generalizations over them are expressed via explicitFE-FE relations.The Berkeley FrameNet project (hereafter FN)(Fillmore et al, 2003) is creating a computer- andhuman-readable lexical resource for English, basedon the theory of frame semantics and supported bycorpus evidence.
The current release (1.3) of theFrameNet data, which has been freely available forinstructional and research purposes since the fallof 2006, includes roughly 780 frames with roughly10,000 word senses (lexical units).
It also containsroughly 150,000 annotation sets, of which 139,000are lexicographic examples, with each sentence an-notated for a single predicator.
The remainder arefrom full-text annotation in which each sentence isannotated for all predicators; 1,700 sentences are an-notated in the full-text portion of the database, ac-counting for roughly 11,700 annotation sets, or 6.8predicators (=annotation sets) per sentence.
Nearlyall of the frames are connected into a single graphby frame-to-frame relations, almost all of whichhave associated FE-to-FE relations (Fillmore et al,2004a)2.1 Frame Semantics of textsThe ultimate goal is to represent the lexical se-mantics of all the sentences in a text, based onthe relations between predicators and their depen-dents, including both phrases and clauses, whichmay, in turn, include other predicators; although thishas been a long-standing goal of FN (Fillmore andBaker, 2001), automatic means of doing this are onlynow becoming available.Consider a sentence from one of the testing texts:(1) This geography is important in understandingDublin.In the frame semantic analysis of this sentence,there are two predicators which FN has analyzed:important and understanding, as well as one whichwe have not yet analyzed, geography.
In addition,Dublin is recognized by the NER system as a loca-tion.
In the gold standard annotation, we have theannotation shown in (2) for the Importance frame,evoked by the target important, and the annotationshown in (3) for the Grasp frame, evoked by under-standing.
(2) [FACTOR This geography] [COP is] IMPOR-TANT [UNDERTAKING in understanding Dublin].
[INTERESTED PARTY INI](3) This geography is important in UNDER-STANDING [PHENOMENON Dublin].
[COGNIZERCNI]The definitions of the two frames begin like this:Importance: A FACTOR affects the outcome of anUNDERTAKING, which can be a goal-oriented activ-ity or the maintenance of a desirable state, the workin a FIELD, or something portrayed as affecting anINTERESTED PARTY.
.
.Grasp: A COGNIZER possesses knowledge aboutthe workings, significance, or meaning of an idea orobject, which we call PHENOMENON, and is able tomake predictions about the behavior or occurrenceof the PHENOMENON.
.
.Using these definitions and the labels, and the factthat the target and FEs of one frame are subsumedby an FE of the other, we can compose the mean-ings of the two frames to produce a detailed para-phrase of the meaning of the sentence: Somethingdenoted by this geography is a factor which affectsthe outcome of the undertaking of understanding thelocation called ?Dublin?
by any interested party.
Wehave not dealt with geography as a frame-evokingexpression, although we would eventually like to.
(The preposition in serves only as a marker of theframe element UNDERTAKING.
)In (2), the INTERESTED PARTY is not a label onany part of the text; rather, it is marked INI, for ?in-definite null instantiation?, meaning that it is con-ceptually required as part of the frame definition,absent from the sentence, and not recoverable fromthe context as being a particular individual?meaning100that this geography is important for anyone in gen-eral?s understanding of Dublin.
In (3), the COG-NIZER is ?constructionally null instantiated?, as thegerund understanding licenses omission of its sub-ject.
The marking of null instantiations is importantin handling text coherence and was part of the goldstandard, but as far as we know, none of the partici-pants attempted it, and it was ignored in the evalua-tion.Note that we have collapsed the two null instan-tiated FEs, the INTERESTED PARTY of the impor-tance frame and the COGNIZER in the Grasp frame,since they are not constrained to be distinct.2.2 Semantic dependency graphsSince the role fillers are dependents (broadly speak-ing) of the predicators, the full FrameNet annotationof a sentence is roughly equivalent to a dependencyparse, in which some of the arcs are labeled with rolenames; and a dependency graph can be derived algo-rithmically from FrameNet annotation; an early ver-sion of this was proposed by (Fillmore et al, 2004b)Fig.
1 shows the semantic dependency graph de-rived from sentence (1); this graphical representa-tion was derived from a semantic dependency XMLfile (see Sec.
5).
It shows that the top frame in thissentence is evoked by the word important, althoughthe syntactic head is the copula is (here given themore general label ?Support?).
The labels on thearcs are either the names of frame elements or indi-cations of which of the daughter nodes are seman-tic heads, which is important in some versions ofthe evaluation.
The labels on nodes are either framenames (also colored gray), syntactic phrases types(e.g.
NP), or the names of certain other syntactic?connectors?, in this case, Marker and Support.3 Definition of the task3.1 Training dataThe major part of the training data for the task con-sisted of the current data release from FrameNet(Release 1.3), described in Sec.2 This was supple-mented by additional training data made availablethrough SemEval to participants in this task.
In ad-dition to updated versions of some of the full-text an-notation from Release 1.3, three files from the ANCwere included: from Slate.com, ?StephanopoulosImportance:importantMarker: inUndertakingNPFactorGrasp:understandingSemHeadThis geographyHeadNE:location:DublinDenotedFE: locationPhenomenon<s>Supp: isHead.SemHeadFigure 1: Sample Semantic Dependency GraphCrimes?
and ?Entrepreneur as Madonna?, and fromthe Berlitz travel guides, ?History of Jerusalem?.3.2 Testing dataThe testing data was made up of three texts, noneof which had been seen before; the gold standardconsisted of manual annotations (by the FrameNetteam) of these texts for all frame evoking expres-sions and the fillers of the associated frame ele-ments.
All annotation of the testing data was care-fully reviewed by the FN staff to insure its cor-rectness.
Since most of the texts annotated in theFN database are from the NTI website (www.nti.org), we decided to take two of the three test-ing texts from there also.
One, ?China Overview?,was very similar to other annotated texts suchas ?Taiwan Introduction?, ?Russia Overview?, etc.available in Release 1.3.
The other NTI text,?Work Advances?, while in the same domain, wasshorter and closer to newspaper style than the restof the NTI texts.
Finally, the ?Introduction to101Sents NEs FramesTokens TypesWork 14 31 174 77China 39 90 405 125Dublin 67 86 480 165Totals 120 207 1059 272Table 1: Summary of Testing DataDublin?, taken from the American National Cor-pus (ANC, www.americannationalcorpus.org) Berlitz travel guides, is of quite a differentgenre, although the ?History of Jerusalem?
text inthe training data was somewhat similar.
Table 1gives some statistics on the three testing files.
Togive a flavor of the texts, here are two sentences;frame evoking words are in boldface:From ?Work Advances?
: ?The Iranians are nowwilling to accept the installation of cameras onlyoutside the cascade halls, which will not enable theIAEA to monitor the entire uranium enrichmentprocess,?
the diplomat said.From ?Introduction to Dublin?
: And in thiscity, where literature and theater have historicallydominated the scene, visual arts are finally com-ing into their own with the new Museum of ModernArt and the many galleries that display the work ofmodern Irish artists.4 ParticipantsA number of groups downloaded the training or test-ing data, but in the end, only three groups submittedresults: the UTD-SRL group and the LTH group,who submitted full results, and the CLR group whosubmitted results for frames only.
It should also benoted that the LTH group had the testing data forlonger than the 10 days allowed by the rules of theexercise, which means that the results of the twoteams are not exactly comparable.
Also, the resultsfrom the CLR group were initially formatted slightlydifferently from the gold standard with regard tocharacter spacing; a later reformatting allowed theirresults to be scored with the other groups?.The LTH system used only SVM classifiers, whilethe UTD-SRL system used a combination of SVMand ME classifiers, determined experimentally.
TheCLR system did not use classifiers, but hand-writtensymbolic rules.
Please consult the separate systempapers for details about the features used.5 EvaluationThe labels-only matching was similar to previousshared tasks, but the dependency structure evalua-tion deserves further explanation: The XML seman-tic dependency structure was produced by a programcalled fttosem, implemented in Perl, which goessentence by sentence through a FrameNet full-textXML file, taking LU, FE, and other labels and usingthem to structure a syntactically unparsed piece of asentence into a syntactic-semantic tree.
Two basicprinciples allow us to produce this tree: (1) LUs arethe sole syntactic head of a phrase whose semanticsis expressed by their frame and (2) each label spanis interpreted as the boundaries of a syntactic phrase,so that when a larger label span subsumes a smallerone, the larger span can be interpreted as a the highernode in a hierarchical tree.
There are a fair num-ber of complications, largely involving identifyingmismatches between syntactic and semantic headed-ness.
Some of these (support verbs, copulas, mod-ifiers, transparent nouns, relative clauses) are anno-tated in the data with their own labels, while oth-ers (syntactic markers, e.g.
prepositions, and auxil-iary verbs) must be identified using simple syntacticheuristics and part-of-speech tags.For this evaluation, a non-frame node counts asmatching provided that it includes the head of thegold standard, whether or not non-head children ofthat node are included.
For frame nodes, the partici-pants got full credit if the frame of the node matchedthe gold standard.5.1 Partial credit for related framesOne of the problems inherent in testing against un-seen data is that it will inevitably contain lexicalunits that have not previously been annotated inFrameNet, so that systems which do not generalizewell cannot get them right.
In principle, the deci-sion as to what frame to add a new LU to should behelped by the same criteria that are used to assignpolysemous lemmas to existing frames.
However,in practice this assignment is difficult, precisely be-cause, unlike WSD, there is no assumption that allthe senses of each lemma are defined in advance; if102the system can?t be sure that a new use of a lemmais in one of the frames listed for that lemma, thenit must consider all the 800+ frames as possibili-ties.
This amounts to the automatic induction offine-grained semantic similarity from corpus data, anotoriously difficult problem (Stevenson and Joanis,2003; Schulte im Walde, 2003).For LUs which clearly do not fit into any exist-ing frames, the problem is still more difficult.
In thecourse of creating the gold standard annotation ofthe three testing texts, the FN team created almost 40new frames.
We cannot ask that participants hit uponthe new frame name, but the new frames are not cre-ated in a vacuum; as mentioned above, they are al-most always added to the existing structure of frame-to-frame relations; this allows us to give credit forassignment to frames which are not the precise onein the gold standard, but are close in terms of frame-to-frame relations.
Whenever participants?
proposedframes were wrong but connected to the right frameby frame relations, partial credit was given, decreas-ing by 20% for each link in the frame-frame relationgraph between the proposed frame and the gold stan-dard.
For FEs, each frame element had to match thegold standard frame element and contain at least thesame head word in order to gain full credit; again,partial credit was given for frame elements relatedvia FE-to-FE relations.6 ResultsText Group Recall Prec.
F1Dublin UTD-SRL 0.4188 0.7716 0.5430China UTD-SRL 0.5498 0.8009 0.6520Work UTD-SRL 0.5251 0.8382 0.6457Dublin LTH 0.5184 0.7156 0.6012China LTH 0.6261 0.7731 0.6918Work LTH 0.6606 0.8642 0.7488Dublin CLR 0.3984 0.6469 0.4931China CLR 0.4621 0.6302 0.5332Work CLR 0.5054 0.7452 0.6023Table 2: Frame Recognition onlyThe strictness of the requirement of exact bound-ary matching (which depends on an accurate syntac-tic parse) is compounded by the cascading effect ofsemantic classification errors, as seen by comparingText Group Recall Prec.
F1Label matching onlyDublin UTD-SRL 0.27699 0.55663 0.36991China UTD-SRL 0.31639 0.51715 0.39260Work UTD-SRL 0.31098 0.62408 0.41511Dublin LTH 0.36536 0.55065 0.43926China LTH 0.39370 0.54958 0.45876Work LTH 0.41521 0.61069 0.49433Semantic dependency matchingDublin UTD-SRL 0.26238 0.53432 0.35194China UTD-SRL 0.31489 0.53145 0.39546Work UTD-SRL 0.30641 0.61842 0.40978Dublin LTH 0.36345 0.54857 0.43722China LTH 0.40995 0.57410 0.47833Work LTH 0.45970 0.67352 0.54644Table 3: Results for combined Frame and FE recog-nitionthe F-scores in Table 3 with those in Table 2.
Thedifficulty of the task is reflected in the F-scores ofaround 35% for the most difficult text in the mostdifficult condition, but participants still managed toreach F-scores as high as 75% for the more limitedtask of Frame Identification (Table 2), which moreclosely matches traditional Senseval tasks, despitethe lack of a full sense inventory.
The difficultyposed by having such an unconstrained task led tounderstandably low recall scores in all participants(between 25 and 50%).
The systems submitted bythe teams differed in their sensitivity to differencesin the texts: UTD-SRL?s system varied by around10% across texts, while LTH?s varied by 15%.There are some rather encouraging results also.The participants rather consistently performed bet-ter with our more complex, but also more useful andrealistic scoring, including partial credit and grad-ing on semantic dependency rather than exact spanmatch (compare the top and bottom halves of Table3).
The participants all performed relatively well onthe frame-recognition task, with precision scores av-eraging 63% and topping 85%.7 DiscussionThe testing data for this task turned out to be espe-cially challenging with regard to new frames, since,in an effort to annotate especially thoroughly, almost10340 new frames were created in the process of an-notating these three specific passages.
One resultof this was that the test passages had more unseenframes than a random unseen passage, which prob-ably lowered the recall on frames.
It appears thatthis was not entirely compensated by giving partialcredit for related frames.This task is a more advanced and realistic versionof the Automatic Semantic Role Labeling task ofSenseval-3 (Litkowski, 2004).
Unlike that task, thetesting data was previously unseen, participants hadto determine the correct frames as a first step, andparticipants also had to determine FE boundaries,which were given in the Senseval-3.A crucial difference from similar approaches,such as SRL with PropBank roles (Pradhan et al,2004) is that by identifying relations as part of aframe, you have identified a gestalt of relations thatenables far more inference, and sentences from thesame passage that use other words from the sameframe will be easier to link together.
Thus, theFN SRL results are translatable fairly directly intoformal representations which can be used for rea-soning, question answering, etc.
(Scheffczyk etal., 2006; Frank and Semecky, 2004; Sinha andNarayanan, 2005).Despite the problems with recall, the participantshave expressed a determination to work to improvethese results, and the FN staff are eager to collabo-rate in this effort.
A project is now underway at ICSIto speed up frame and LU definition, and another tospeed up the training of SRL systems is just begin-ning, so the prospects for improvement seem good.This material is based in part upon work sup-ported by the National Science Foundation underGrant No.
IIS-0535297.ReferencesCharles J. Fillmore and Collin F. Baker.
2001.
Framesemantics for text understanding.
In Proceedingsof WordNet and Other Lexical Resources Workshop,Pittsburgh, June.
NAACL.Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L Petruck.
2003.
Background to FrameNet.International Journal of Lexicography, 16.3:235?250.Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.2004a.
FrameNet as a ?Net?.
In Proceedings ofLREC, volume 4, pages 1091?1094, Lisbon.
ELRA.Charles J. Fillmore, Josef Ruppenhofer, and Collin F.Baker.
2004b.
FrameNet and representing the linkbetween semantic and syntactic relations.
In Chu-ren Huang and Winfried Lenders, editors, Frontiersin Linguistics, volume I of Language and LinguisitcsMonograph Series B, pages 19?59.
Inst.
of Linguistics,Acadmia Sinica, Taipei.Charles J. Fillmore.
1976.
Frame semantics and the na-ture of language.
Annals of the New York Academy ofSciences, 280:20?32.Charles J. Fillmore.
1982.
Frame semantics.
In Lin-guistics in the Morning Calm, pages 111?137.
Han-shin Publishing Co., Seoul, South Korea.Anette Frank and Jiri Semecky.
2004.
Corpus-basedinduction of an LFG syntax-semantics interface forframe semantic processing.
In Proceedings of the 5thInternational Workshop on Linguistically InterpretedCorpora (LINC 2004), Geneva, Switzerland.Ken Litkowski.
2004.
Senseval-3 task: Automatic label-ing of semantic roles.
In Rada Mihalcea and Phil Ed-monds, editors, Senseval-3: Third International Work-shop on the Evaluation of Systems for the SemanticAnalysis of Text, pages 9?12, Barcelona, Spain, July.Association for Computational Linguistics.Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
2004.
Shallowsemantic parsing using support vector machines.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages233?240, Boston, Massachusetts, USA, May 2 - May7.
Association for Computational Linguistics.Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.2006.
Ontology-based reasoning about lexical re-sources.
In Alessandro Oltramari, editor, Proceedingsof ONTOLEX 2006, pages 1?8, Genoa.
LREC.Sabine Schulte im Walde.
2003.
Experiments on thechoice of features for learning verb classes.
In Pro-ceedings of the 10th Conference of the EACL (EACL-03).Steve Sinha and Srini Narayanan.
2005.
Model basedanswer selection.
In Proceedings of the Workshop onTextual Inference, 18th National Conference on Artifi-cial Intelligence, PA, Pittsburgh.
AAAI.Suzanne Stevenson and Eric Joanis.
2003.
Semi-supervised verb class discovery using noisy features.In Proceedings of the 7th Conference on Natural Lan-guage Learning (CoNLL-03), pages 71?78.104
