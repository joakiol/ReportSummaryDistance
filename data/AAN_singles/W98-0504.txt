rg!How to define a context- f ree backbone for DGs:Imp lement ing  a DG in the LFG formal ismNorber t  BrSkerUniversitiit StuttgartAzenbergstr.
12D-70174 StuttgartNOBI~IMS.UNI-STUTTGART.DEAbst ractThis paper presents a multidimensional Depen-dency Grammar (DG), which decouples the de-pendency tree from word order, such that sur-face ordering is not determined by traversingthe dependency tree.
We develop the notionof a word order domain structure, which islinked but structurally dissimilar to the syn-tactic dependency tree.
We then discuss theimplementation f such a DG using constructsfrom a unification-based phrase-structure ap-proach, namely Lexical-Functional Grammar(LFG).
Particular attention isgiven to the anal-ysis of discontinuities in DG in terms of LFG'sfunctional uncertainty.1 In t roduct ionRecently, the concept of valency has gained con-siderable attention.
Not only do all linguis-tic theories refer to some reformulation of thetraditional notion of valency (in the form of 0-grid, subcategorization list, argument list, or ex-tended domain of locality); there is a growingnumber of parsers based on binary relations be-tween words (Eisner, 1997; Maruyama, 1990).Even theories based on phrase structure mayhave processing models based on relations be-tween lexical items (Rambow & Joshi, 1994).Against he background of this interest in thevalency concept, and the fact that word orderis one of the main difference between phrase-structure based approaches (henceforth PSG)and dependency grammar (DG), this paper willpropose a word order description for DG anddescribe its implementation.
First, we will mo-tivate the separation of surface order and depen-dency relations within DG, and make a specificarchitectural proposal for their linking.
Second,we will briefly sketch Lexical-Functional Gram-mar (LFG), and then show in detail how onemight use the formal constructs provided byLFG to encode the proposed DG architecture.Our position will be that dependency re-lations are motivated semantically (Tesni~re,1959), and need not be projective.
We arguefor so-called word order domains, consisting ofpartially ordered sets of words and associatedwith nodes in the dependency tree.
These orderdomains constitute a tree defined by set inclu-sion, and surface word order is determined bytraversing this tree.
A syntactic analysis there-fore consists of two linked, but dissimilar trees.The paper thus sheds light on two questions.A very early result on the weak generative equiv-alence of context-free grammars and DGs sug-gested that DGs are incapable of describing sur-face word order (Gaifman, 1965).
J This resulthas been criticised to apply only to impover-ished DGs which do not properly represent for-mally the expressivity ofcontemporary DG vari-ants (Neuhaus & BrSker, 1997), and our use ofa context-free backbone with further constraintsimposed by dependency relations further sup-ports the view that DG is not a notational ~ri-ant of context-free grammar.
The second ques-tion addressed is that of efficient processing ofdiscontinuous DGs.
By converting a native DGgrammar into LFG rules, we are able to profitfrom the state of the art in context-free parsingtechnology.
A context-free base (or skeleton)has often been cited as a prerequisite for practi-cal applicability of a natural anguage grammar(Erbach & Uszkoreit, 1990), and we here showthat a DG can meet his criterion with ease.Sec.
2 will briefly review approaches to wordorder in DG, and Sec.
3 introduces word orderdomains as our proposal.
LFG is briefly intro-duced in Sec.
4, and the encoding of DG withinthe LFG framework is the topic of Sec.
5.29IIIIIIIIIiIlIIiI!i2 Word  Order  in DGA very brief characterization f DG is thatit recognizes only lexical, not phrasal nodes,which are linked by directed, typed, binary rela-tions to form a dependency tree (Tesni~re, 1959;Hudson, 1993).
If these relations are moti-vated semantically, such dependency trees canbe non-projective.
Consider the extracted NPin "Beans, I know John likes".
A projective treewould require "Beans" to be connected to either"1" or "know" - none of which is conceptually di-rectly related to "Beans".
It is "likes" that deter-mines syntactic features of "Beans" and whichprovides a semantic role for it.
The only con-nection between "know" and "Beans" is that thefinite verb allows the extraction of"Beans", thusdefining order restrictions for the NP.
The fol-lowing overview of DG flavors shows that var-ious mechanisms (global rules, general graphs,procedural means) are generally employed to liftthe limitation of projectivity and discusses someshortcomings of these proposals.Functional Generat ive Description (Sgallet al, 1986) assumes a language-independentunderlying order, which is represented asa pro-jective dependency tree.
This abstract represen-tation of the sentence is mapped via orderingrules to the concrete surface realization.
Re-cently, Kruijff (1997) has given a categorial-style formulation of these ordering rules.
He as-sumes associative categorial operators, permut-ing the arguments o yield the surface ordering.One difference to our proposal is that we ar-gue for a representational account of word order(based on valid structures representing word or-der), eschewing the non-determinism introducedby unary categorial operators; the second iffer-ence is the avoidance of an underlying structure,which stratifies the theory and makes incremen-tal processing difficult.Meaning-Text Theory (Melc'flk, 1988) as-sumes even strata of representation.
The rulesmapping from the unordered ependency treesof surface-syntactic representations onto the an-notated lexeme sequences ofdeep-morphologicalrepresentations include global ordering ruleswhich allow discontinuities.
These rules havenot yet been formally specified (Melc'~k &Pertsov, 1987p.187f) (but see the proposal byRambow & Joshi (in print)).30Word Grammar  (WG, Hudson (1990)) isbased on general graphs instead of trees.
Theordering of two linked words is specified togetherwith their dependency relation, as in the propo-sition "object of verb follows it".
Extrac-tion of, e.g., objects is analyzed by establish-ing an additional dependency called v i s i to rbetween the verb and the extractee, which re-quires the reverse order, as in "v i s i to r  ofverb precedes it".
Resulting inconsistencies,e.g.
in case of an extracted object, are not re-solved.
This approach compromises the seman-tic motivation of dependencies byadding purelyorder-induced ependencies.Dependency Unif ication Grammar(DUG, Hellwig (1986)) defines a tree-likedata structure for the representation f syntac-tic analyses.
Using morphosyntactic featureswith special interpretations, a word definesabstract positions into which modifiers aremapped.
Partial orderings and even discon-tinuities can thus be described by allowinga modifier to occupy a position defined bysome transitive head.
The approach requiresthat the parser interprets everal features in aspecial way, and it cannot restrict he scope ofdiscontinuities.Slot Grammar  (McCord, 1990) employs anumber of rule types, some of which are ex-clusively concerned with precedence.
So-calledhead/slot and slot/slot ordering rules describethe precedence in projective trees, referring toarbitrary predicates over head and modifiers.Extractions (i.e., discontinuities) are merelyhandled by a mechanism built into the parser.3 Word  Order  DomainsExtending the previous discussion, we requirethe following of a word order description for DG:?
not to compromise the semantic motivationof dependencies,?
to be able to restrict discontinuities to cer-tain constructions and delimit their scope,?
to be lexicalized without requiring lexicalambiguities for the representation f order-ing alternatives,?
to be declarative (i.e., independent of ananalysis procedure), andIiI ,i ,I ,,!'
/ , " .
,.
der J unge;  i gess.ehen.
', , :, , .
:?
, "den  Mann; ;  " - .
.
- "  , ,?
- .
.
.
.
.
- - .
_ .o .
?Figure 1: Dependency Tree and Order Domainsfor (1)do,.Mann Junge gesehenFigure 2: Order Domain Structure for (1)?
to be formally precise and consistent.The subsequent definition of an order domainstructure and its linking to the dependency treesatisify these requirements.3.1 The  Order  Domain  S t ructureA word order domain is a set of words, general-izing the notion of positions in DUG.
The car-dinality of an order domain may be restrictedto at most one element, at least one element,or - by conjunction - to exactly one element.Each word is associated with a sequence of orderdomains, one of which must contain the worditself, and each of these domains may requirethat its elements have certain features.
Orderdomains can be partially ordered based on setinclusion: If an order domain d contains wordw (which is not associated with d), every wordw' contained in a domain d t associated with wis also contained in d; therefore, d' C d for eachd' associated with w. This partial ordering in-duces a tree on order domains, which we callthe order domain structure.
The order domainstructure constitutes a projective tree over theinput, where order domains loosely correspondto partial phrases.
(1) Den Mann hat der Junge gesehen.the manAcc has the bOyNOM seen'The boy has seen the man.
'Take the German example (1).
Its dependencytree is shown in Fig.
1, with word order domainsindicated by dashed circles.
The finite verb,"hat", defines a sequence of domains, (dl, d2, d3),which roughly correspond to the topologicalfields in the German main clause.
The nounsand the participle ach define a single order do-main.
Set inclusion gives rise to the domainstructure in Fig.
2, where the individual wordsare attached by dashed lines to their includingdomains.3.2 Surface Order ingHow is the surface order derived from an or-der domain structure?
First of all, the orderingof domains is inherited by their respective le-ments, i.e., "Mann" precedes (any element of)d2, "hat" follows (any element of) dl, etc.Ordering within a domain, e.g., of "hat" andd6, or ds and d6, is based on precedence pred-icates (adapting the precedence predicates ofWG).
There are two different ypes, one order-ing a word with respect o any other element ofthe domain it is associated with (e.g., "hat" withrespect o d6), and another ordering two modi-tiers, referring to the dependency relations theyoccupy (d5 and d6, referring to subj and vpart).A verb like "hat" introduces three precedencepredicates, requiring other words (within thesame domain) to follow itself and the participleto follow subject and object, resp.
: 1"hat" => <.A subj < vpartA obj < vpartInformally, the first conjunct is satisfied byally domain in which no word precedes "hat",and the second and third conjuncts are satisfiedby any domain ill which no subject or objectfollows a participle (vpart).
The obj must bementioned for "hat", although "hat" does not di-rectly govern objects, because objects may beplaced by "hat", and not their immediate gov-ernors.
The domain structure in Fig.2 satisfiesthese restrictions since nothing follows the par-ticiple, and because "den Mann" is not an ele-ment of (\]2, which contains "hat".
This is an im-portant interaction of order domains and prece-dence predicates: Order domains define scopes1For more details on the exact syntax and the seman-tics of these propositions, ee (BrSker, 1998b).31I1iII!I1I1IIIiiIIIIfor precedence predicates.
In this way, we takeinto account that dependency trees are flatterthan PS-based ones 2 and avoid the formal in-consistencies noted above for WG.3.3 L inking Domain  S t ructure  andDependency  TreeOrder domains easily extend to discontinuousdependencies.
Consider the non-projective treein Fig.1.
Assuming that the finite verb gov-erns the participle, no projective dependencybetween the object "den Mann" and the partici-ple "gesehen" can be established.
We allow non-projectivity by loosening the linking between de-pendency tree and domain structure: A modi-fier (e.g., "Mann") may not only be inserted intoa domain associated with its direct head ("gese-hen"), but also into a domain of a transitive head("hat"), which we will call the positional head.The possibility of inserting a word into a do-main of some transitive head raises the ques-tions of how to require continuity (as neededin nmst cases), and how to limit the distancebetween the governor and the modifier.
Bothquestions will be soh,ed with reference to thedependency relation.
From a descriptive view-point, the syntactic onstruction isoften cited todetermine the possibility and scope of disconti-nuities (Bhatt, 1990; Matthews, 1981).
In PS-based accounts, the construction is representedby phrasal categories, and extraction is lim-ited 1)3-" bounding nodes (e.g., Haegeman (1994),Becker et al (1991)).
In dependency-based ac-counts, the construction is represented by thedependency relation, which is typed or labelledto indicate constructional distinctions which areconfigurationally defined in PSG.
Given this cor-respondence, it is natural to employ dependen-cies in the description of discontinuities as fol-lows: For each modifier, a set of dependencytypes is defined which may link the direct headand the positional head of the modifier ("gese-hen" and "hat", respectively).
If this set isempty, both heads are identical and a contin-uous attachment results.
The impossibility ofextraction from, e.g., a finite verb phrase followsfrom the fact that the dependency embedding fi-nite verbs, propo, may not appear on any path2Note that each phrasal level in PS-based trees definesa scope for linear precedence rules, which only apply tosister nodes.32between a direct and a positional head.4 A Br ie f  Rev iew o f  LFGThis section introduces key concepts of LFGwhich are of interest in Sec.
5 and is necessarilyvery short.
Further information can be found inBresnan & Kaplan (1982) and Dalrymple t al.
(1995).LFG posits several different representationlevels, called projections.
Within a projection,a certain type of linguistic knowledge is repre-sented, which explains differences in the formalsetup (data types and operations) of the projec-tions.
The two standard projections, and thoseused here, are the constituent (c-) structure andthe functional (f-) structure (Kaplan (1995) andHalvorsen & Kaplan (1995) discuss the projec-tion idea in more detail).
C-structure is definedin terms of context-free phrase structure rules,and thus forms a projective tree of categoriesover the input.
It is assumed to encode lan-guage particularities with respect o the set ofcategories and the possible orderings.
The f-structure is constructed fi'om additional annota-tions attached to the phrase structure rules, andhas the form of an attribute-value matrix or fea-ture structure.
It is assumed to  represent moreor less langnage-independent information aboutgrammatical functions and predicate-argumentstructure.
In addition to the usual unificationoperation, LFG employs existential and nega-tive constraints on features, which allow the for-nmlation of constraints about the existence offeatures without specifying the associated value.Consider the following rules, which are usedfor illustration only and do not constitute acanonical LFG analysis.S =~ NP VP(TosJ)=~ 1"=$(,tcAsE)=accNP =~ Det N(I"sPEC)~"~J " T--J,VP =~ V NPT=J .
(l"svsJ)=~(,I.TENSE) (J.CASE)=nomV(TvcoMP)=J.,-,(~.
'reNse)Assuming reasonable lexical insertion rules,the context-free part of these rules assigns thec-structure to the left of Fig.
3 to example(1).
The annotations are associated with right-hand side elements of the rules and define the!
,!i\]!!
rF~k:'iII= ==================================================================== a?
: 1"" \[CASE nom\]?
SUBJ':"'.1 ,,,...- v .......
,:,.,../'-,,,.""
....  'I I ..... t ............
I ........... " " ........ - .
.
.
.
.
?o - "Den Mann hat der Junge gesehenFigure 3: C-structure (left) and f-structure (right) for (I)f-structure of the sentence, which is displayed tothe right of Fig.
3.
Each c-structure node is asso-ciated with an f-structure node as shown by thearrows.
The f-structure node associated withthe left-hand side of a rule may be accessed withthe $ metavariable, while the f-structure nodeof a right-hand side element may be accessedwith the $ metavariable.
The mapping from c-structure nodes to f-structure nodes is not one-to-one, however, since the feature structures oftwo distinct c-structure nodes may be identi-fied (via the $=$ annotation), and additionalembedded features may be introduced (such asCASE).
Assuming that only finite verbs carrythe TENSE feature, the existential constraint($TENSE) requires a finite verb at the begin-ning of the VP, while the negative constraint.~($TENSE) forbids finite verbs at the end ofthe VP.
Note that unspecified feature structuresare displayed as \[ \] in the figure, and that muchmore information (esp.
predicate-argument in-formation) will come from the lexical entries.Another important construct of LFG is func-tional uncertainty (Kaplan & Zaenen, 1995;Kaplan & Maxwell, 1995).
Very often (mostnotably, in extraction or control constructions)the path of f-structure attributes to write downis indeterminate.
In this case, one may writedown a description of this path (using a regu-lar language over attribute names) and let theparser check every path described (possibly re-sulting in ambiguities warranted by f-structuredifferences only).
Our little grammar may beextended to take advantage of functional uncer-tainty in two ways.
First, if you want to permutesubject and object (as is possible in German),you might change the S rule to the following:S =~ NP VP(t{os~ I susJ})=~ t=~The f-structure node of the initial NP maynow be inserted in either the OBJ or the SUBJattribute of the sentence's f-structure, which isexpressed by the disjunction {OBJiSUBJ} inthe annotation.
(Of course, you have to restrictthe CASE feature suitably, which can be done inthe verb's subcategorization.)
The other regularnotation which we will use is the Kleene star.Assume a different f-structure analysis, wherethe object of infinite verbs is embedded underVCOMP.
The S rule from above would have tobe changed to the following:S => NP VP('~{(VCOMP) OBJ I SUBJ})=~ ~'=~,But this rule will only analyse verb groupswith zero or one auxiliary, because the VCOMPattribute is optional in the path description.Examples like Den Mann will der Junge gese-hen haben with several auxiliaries are not cov-ered, because the main verb is embedded under(VCOMP VCOMP).
The natural solution is touse the Kleene star as follows, which allows zeroor more occurrences of the attribute VCOMP.S =~ NP VP(l"{vcoMP* oBJ I suBJ})=~ t--~.A property which is important for our use offunctional uncertainty is already evident fromthese examples: Functional uncertainty is non-constructive, i.e., the attribute paths derivedfrom such an annotation are not constructedanew (which in case of the Kleene star wouldlead to infinitely many solutions), but must al-ready exist in the f-structure.33IIIIIlIiiIIIIIIlIiII5 Encod ing  DG in LFG 5.2 Topological  fields5.1 The  Implementat ion  P la t t fo rm As we have seen in Sec.
3, the order domainThe plattform used is the Xerox Lin- structure is a projective tree over the input.
Soguistic Environment (XLE, see also it is natural to encode the domain structure inhttp://www.parc.xerox, c m/istl/groups/nltt/xlef~ ntext'free rules, resulting in a tree as shownwhich implements a large part of LFG theory in Fig.
4.
Categories which have a status as or-plus a number of abbreviatory devices.
It der domains are named dora*, to be distinguish-includes a parser, a generator, support for two-level morphology and different ypes of lexicaas well as a user-friendly graphical interfacewith the ability to browse through the set ofanalyses, to work in batch mode for testingpurposes, etc.We will be using two abbreviatory devices be-low, which are shortly introduced here.
Both donot show up in the final output, rather they al-low the grammar writer to state various general-izations more succintly.
The first is the so-calledmetacategory, which allows several c-structurecategories to be merged into one.
So if weare writing (2), we introduce a metacategorydomVfin (representing the domain sequence offinite verbs) to be used in other rules, but wewill never see such a category in the c-structure.Rather, the expansion of the metacategory is di-rectly attached to the mother node of the meta-category (cf.
Fig.
4).
(2) domVfin = domINITIAL domMIDDLE domFINALable from preterminal categories (such as Vfin,I, .
.
.
; these cannot be converted to metacate-gories).
As notational convention, domC will bethe name of the (meta)category defining the or-der domain sequence for a word of class C. Elim-inating the preterminal categories yields exactlythe domain structure given in Fig.
2.A complete algorithmic description of how toderive phrase-structure rules from order domaindefinitions would require a lenghty introductionto more of XLE's c-structure constructs, andtherefore we illustrate the conversion with hand-coded rules.
For example, a noun introducesone order domain without cardinality restric-tions.
Assuming a metacategory DOMAIN stand-ing for an arbitrary domain, we define the fol-lowing rules for the domain sequences of nouns,full stops, and determiners:(5) domN =~ DOMAIN* N DOMAIN*.domI =~ DOMAIN I.domD =~ D.The second abbreviatory construct is the tem-plate, which groups several functional annota-tions under one heading, possibly with someparameters.
A very important emplate is theVALENCY template defined in (3), which definesa dependency relation on f-structure (see be-low for discussion).
We require three parame-ters (each introduced by underscore), the first ofwhich indicates optionality (opt vs. req values),the second gives the name of the dependency re-lation, and the third the word class required ofthe modifier.
(4) shows a usage of a template,which begins with an @ (at) sign and lists thetemplate name with any parameters enclosed inparentheses.VALENCY (_o _d _c) = { _o = opt~(T_d)(3) ~ (?_d CLASS) = _c(?_d LEXEME) ).
(4) @(VALENCY req OBJ N).A complex example is the finite verb, whichintroduces three domains, each with differentcardinality restrictions.
This is encoded in thefollowing rules:domVfin fi domINITIAL domMIDDLE domFINAL.
(6) domINIT IAL~ DOMAIN.domMIDDLE ~ DOMAIN* Vfin DOMAIN*.domFINAL ~ ( DOMAIN ).Note tile use of a metacategory here, whichdoes not appear in tlle c-structure output (asseen in Fig.
4), but still allows you to refer toall elements placed by a finite verb in one word.The definition of DOMAIN is trivial: It is just ametacategory expandable to every domain: 3aA number of efficiency optimizations can be di-rectly compiled into these c-structure rules.
MentioningDOMAIN is much too permissive in most cases (e.g., withinthe NP), and can be optimized to allow only domains in-troduced by words which may actually be modifiers atthis point.34!m !mlC$ 1: R00T: 220i.
.
..I ..........do~I  : 218......
..::.:::?-?"
........... .
............... i ............... ~ : '~ . "
: '~ '1=: ' : .~  -.-.> ..........domlNITI/~L :159 do~IDDLE : 189 do~FIbrAL :19S- : I : 117I .
.
.
.
:""~":~'~'<::'= ................................. Id0~:149 V?
ia :42  do~N:lTSi :do~Vpp:188: .. :118. .
.
.
.
.
.
.
?
:  .
.
.
.
.
.
.
.
"::~ .
.
.
.
.
.
.
.
.
.
.
.dosaD:14S N:28  : .hat :43  do \ ]aD:173  :N :74  Vpp:lO8I I ....
I ?1 ?
I ?V: l  Mann:29 D:$2 J tmge:75  gesehen:109I Iden : 2 der  : g lFigure 4: C-structure for (1)(7) DOMAIN = { domVfin I domI I domN IdomD }.5.3 va lenc ies  and DependencyRelat ionsThe dependency tree is, at least in our ap-proach, an unordered tree with labelled rela-tions between nodes representing words.
Thisis very similar to the formal properties of the f-structure, which we will therefore use to encodeit.
We have already presented the VALENCY tem-plate in (3) and will now explain it.
{.-- I ""}represents a disjunction of possibilities, and theparameter _o (for optionality)controls their se-lection.
In case we provide the opt value, there?
is an option to forbid the existence of the de-pendency, expressed by the negative constraint--~($_d).
Regardless of the value of _o, there isanother option to introduce an attribute named_d (for dependency) which contains a CLASSattribute with a value specified by the thirdparameter, _c.
The existential constraint forthe LEXEME attribute requires that some otherword (which specifies a LEXFA~IE) is unified intothe feature _d, thereby filling this valency slot.The use of a defining constraint for the CLASSattribute constructs the feature, allowing non-constructive functional uncertainty to fill in themodifier (as explained below).A typical lexical entry is shown in (8), wherethe surface form is followed by the c-structurecategory and some template invocations.
Theseexpand to annotations defining the CLASS andLEXEME features, and use the VALENCY templateto define the valency frame.
(8 )hat Vfin ?
(Vfin aux-per fect_ )@(VALENCY req SUBJ N)@(VALENCY req VPART Vpp).5.4 Cont inuous  and  D iscont inuousAt tachmentSo far we get only a c-structure where wordsare associated with f-structures containing va-lency frames.
To get the f-structure shown inFig.
5~ (numbers refer to c-structure node num-bers of Fig.
4) we need to establish dependencyrelations, i.e., need to put the f-structures asso-ciated with preterminal nodes together into onelarge f-structure.
Establishing dependency re-lations between the words relies heavily on themechanism of functional uncertainty.
First, wemust identify on f-structure the head of eachorder domain sequence.
For this, we annotatein every c-structure rule the category of thehead word with the template ~(HEAV), whichidentifies the head word's f-structure with theorder domain's f-structure (cf.
(9)).
Second,all other c-structure categories (which representmodifiers) are annotated with the ~(MODIFIER)template defined in (10).
This template statesthat the f-structure of the modifier (referencedby .~) may be placed under some dependency at-tribute path of the f-structure of the head (ref-erenced by ~).
These paths are of the form p d,where p is a (possibly empty) regular expressionover dependency attributes, and d is a depen-dency attribute, d names the dependency rela-tion the modifier finally fills, while p describesthe path of dependencies which may separatethe positional from the direct head of the mod-ifier.
The MODIFIER template thus completelydescribes the legal discontinuities: If p is emptyfor a dependency d, modifiers in dependency dare always continuously attached (i.e., in an or-der domain defined by their direct head).
Thisis thecase for the subject (in dependency SUB J)and the determiner (in dependency SPEC), inthis example.
On the other hand, a non-emptypath p allows the modifier to 'float up' the de-pendency tree to any transitive head reachablevia p. In our example, objects depending on par-ticiples may thus float into domains of the finiteverb (across VPART dependencies), and relativeclauses (in dependency RELh) may float from thenoun's domain into the finite verb's domains.
(9) HEAD = I=$.35IIIiIiI!iIIIIIIIIIi117?
L -, FIELD ~iddl eJ;OBJ ;PEC 5112:XE~ definit_, CLASS D. CASE no74 .r~l~: Junge_.
CLASS N. CASE noraROPO )RDER ~L  -, FIELD middle\]~n~ \[FIELD initial \]F .
.
.
.
.
~L  \[42.0~'R-PZLr~0 IJ/P/L~T )B3 ~PEC I~EXEME definit_~ CLASS D, CASE ace28~.EI~G: Mann~ CLASS N, C,~E acc108 .E:~IG: sehen_.
C~SS Vpp42 .E~G: aux-'>erfect_, CLASS Vfin.E~ME aussage_, CLASS IFigure 5: F-structure for (1)(lO)MODIFIER = $=(T{PROPOISUBJIVPART* OBJIVPARTISPEC\[ {SUBJ\[OBJ\[VPART}* RELA})The grammar defined so far overgenerates inthat, e.g., relative clauses may be placed into themiddle field.
To require placement in specificdomains, additional features are used, which dis-tinguish topological fields (e.g., via ($FIELD) =middle annotations on c-structure).
A relativeclause can then be constrained to occur only inthe final field by adding constraints on these fea-tures.
This mechanism is very similar to de-scribing agreement or government (e.g., of caseor number), which also uses standard featuresnot discussed here.
With these additions, thefinal rules for finite verbs look as follows:domINITIALdomMIDDLE(11)doBFINALDOMAIN:@(MODIFIER)(~FIELD) = initial.Vf in:@(HEAD)(~FIELD) = middle;DOMAIN*:~(MODIFIER)($FIELD) = middle;(DOMAIN:?
(MODIFIER)(~FIELD) = final ).5.5  Miss ing LinksAs is to be expected if you use something forpurposes it was not designed to be used for,there are some missing links.
The most promi-nent one is the lack of binary precedence predi-cates over dependency relations.
There is, how-ever, a close relative, which might be used forimplementing precedence predicates.
Zaenen &Kaplan (1995) introduced f-precedence <!
intoLFG, which allows to express on f-structure con-straints on the order of the c-structure nodesmapping to the current f-structure.
So we mightwrite the following annotations to order the fi-nite verb with respect o its modifiers, or to or-der subject and object.
(12) (T) </ (T{SUBJIOBJ\[VPART}).
(J'SUBJ) </ (J'oBa).Tile problem with f-precedence, however, isthat is does not respect he scope restrictionswhich we defined for precedence predicates?
I.e.,a topicalized object is not exempt from theabove constraints, and thus would result in pars-ing failure.
To restrict he scope of f-precedenceto order domains (aka, certain c-structure cat-egories) would require an explicit encoding ofthese domains on f-structure?6 Conc lus ionWe have presented a new approach to wordorder which preserves traditional notions (se-mantically motivated ependencies, topologicalfields) while being fully lexicalized and formallyprecise (BrSker, 1997).
Word order domains aresets of partially ordered words associated withwords.
A word is contained in an order domainof its head, or may float into an order domainof a transitive head, resulting in a discontinu-ous dependency tree while retaining a projec-tive order domain structure.
Restrictions on thefloating are expressed in a lexicalized fashion in36,I,iII i I:!I i Ititerms of dependency relations.
V~re have alsoshown how the order domains can be used todefine a context-free backbone for DG, and useda grammar development environment for anno-tated phrase-structure grammars to encode theDG.A number of questions immediately arise,some of which will hopefully be answered un-til the time of the workshop.
On the theoreticalside, this work has argued for a strict separa-tion of precedence and categorial information inLFG (or PSG in general, see (BrSker, 1998a)).Can these analyses and insights be transferred?On the practical side, can the conversion wesketched be used to create efficient large-scaleDGs?
Or will the amount of f-structural inde-terminacy introduced by  our use of functionaluncertainty lead to overly long processing?
And,last and most challenging, when will the firstlarge treebank with dependency annotation beavailable, and will it be derived from XLE's f-structure output?ReferencesBecker, T., A. Joshi & O. Rambow (1991).
Long-Distance scrambling and tree-adjoining gram-mar.
In Proc.
5th Conf, of the European Chap-ter of the ACL, pp.
21-26.Bhatt, C. (1990).
Die syntaktische Struktur derNominalphrase im Deutschen.
Studien zurdeutschen Grammatik 38.
Tiibingen: Narr.Bresnan, J.
& R. Kaplan (Eds.)
(1982).
The Men-tal Representation of Grammatical Relations.Cambridge, MA: MIT Press.BrSker, N. (1997).
Eine Dependenzgrammatikzur Kopplung heterogener Wissenssysteme aufmodallogischer Basis.
Dissertation, DeutschesSeminar, Universit~it Freiburg.BrSker, N. (!998a).
A Projection Architecture forDependency Grammar and How it Comparesto LFG.
In Proc.
1998 lnt'l Lexical-FunctionalGrammar Conference.
(accepted as alternatepaper) Brisbane/AUS: Jun 30-Jul 2, 1998.BrSker, N. (1998b).
Separating Surface Order andSyntactic Relations in a Dependency Grammar.In COLING-ACL 98 - Proc.
of the 17th Intl.Conf.
on Computational Linguistics and 36thAnnual Meeting of the ACL.
Montreal/CAN,Aug 10-14, 1998.Dalrymple, M., R. Kaplan, J. Maxwell & A. Zae-nen (Eds.)
(1995).
Formal Issues in Le~cal-Functional Grammar.
CSLI Lecture Notes 47,Stanford/CA: CSLI.Eisner, J.
(1997).
Bilexical Grammars and a Cubic-Time Probabilistic Parser.
In Proc.
of lnt'lWorkshop on Parsing Technologies, pp.
54--65.Boston/MA: MIT.Erbach, G. & H. Uszkoreit (1990).
Grammar Engi-neering: Problens and Prospects.
CLAUS Re-port 1.
Saarbrficken/DE: University of Saar-briicken.Gaifman, H. (1965).
Dependency Systems andPhrase Structure Systems.
Information andControl, 8:304-337.Haegeman, L. (1994).
Introduction to Governmentand Binding.
Oxford/UK: Basil Blackwell.Halvorsen, P.-K. & R. Kaplan (1995).
Projectionsand Semantic Description i  Lexical-FunctionalGrammar.
In M. Dalrymple, R. Kaplan, J. I.Maxwell & A. Zaenen (Eds.
), Formal Issuesin Lezical-launctional Grammar, pp.
279-292.Stanford University.Hellwig, P. (1986).
Dependency Unification Gram-mar.
In Proc.
l l th Int'l Conf.
on Computa-tional Linguistics, pp.
195-198.Hudson, R. (1990).
English Word Grammar.
Ox-ford/UK: Basil Blackwell.Hudson, R. (1993).
Recent developments in depen-dency theory.
In J. Jacobs, A. v. Stechow,W.
Sternefeld & T. Vennemann (Eds.
), Syn-tax.
Ein internationales Handbuch zeitgenSssis-cher Forschung, pp.
329-338.
Berlin: Walter deGruyter.Kaplan, R. (1995).
The formal architecture ofLexical-FUnctional Grammar.
In M. Dalrym-pie, R. Kaplan, J. I. Maxwell & A. Zae-nen (Eds.
), Formal Issues in Lexical-FunctionalGrammar, pp.
7-27.
Stanford University.Kaplan, R. & J. Maxwell (1995).
An Algorithmfor Functional Uncertainty.
In M. Dalrymp!e,R.
Kaplan, J. I. Maxwell & A. Zaenen (Eds.
),Formal Issues in Lexical-Functional Grammar,pp.
177-198.
Stanford University.Kaplan, R. & A. Zaenen (1995).
Long-distance De-pendencies, Constituent Structure, and Func-tional Uncertainty.
In M. Dalrymple, R. Ka-plan, J. I. Maxwell & A. Zaenen (Eds.
), For-mal Issues in Lexical-Functional Grammar, pp.137-166.
Stanford University.Kruijff, G.-J.
v. (!997).
A Basic Dependency-BasedLogical Grammar.
Draft Manuscript.
Prague:Charles University.Maruyama, H. (1990).
Structural Disambiguationwith Constraint Propagation.
In Proc.
28thAnnual Meeting of the ACL, pp.
31-38.
Pitts-burgh/PA.Matthews, P. (1981).
Syntax.
Cambridge Text-books in Linguistics, Cambridge/UK: Cam-bridge Univ.
Press.McCord, M. (1990).
Slot Grammar: A System forSimpler Construction ofPractical Natural Lan-guage Grammars.
In R. Studer (Ed.
), Natural37Language and Logic, pp.
118-145.
Berlin, Hei-delberg: Springer.Melc'fik, I.
(1988).
Dependency Syntax: Theory andPractice.
Albany/NY: State Univ.
Press of NewYork..Melc'fak, I.
& N. Pertsov (1987).
Surface Syntaxof English: A Formal Model within the MTTFramework.
Philadelphia/PA: John Benjamins.Neuhaus, P. & N. BrSker (1997).
The Complexity ofRecognition of Linguistically Adequate Depen-dency Grammars.
In Prvc.
35th Annual Meet-ing of the ACL and 8th Conf.
of the EACL, pp.33?-343.
Madrid, July 7-12, 1997.Rainbow, O.
& A. Joshi (1994).
A Processing Modelfor Free Word Order Languages.
In C. J.Clifton, L. brazier & K. Rayner (Eds.
), Per-spectives on Sentence Processing.
Hillsdale/NJ:Lawrence Erlbaum.Rambow, O.
& A. Joshi ((in print)).
A FormalLook at Dependency Grammars and Phrase-Structure Grammars, with special considera-tion of word-order phenomena.
In L.
Wanner(Ed.
), Current Issues in Meaning- Text- Theory.London: Pinter.Sgall, P., E. Hajicova & J. Panevova (1986).
TheMeaning of the Sentence in its Semantic andPragmatic Aspects.
Dordrecht/NL: D.Reidel.Tesni~re, L. (1959).
Elemdnts de syntaxe structurale.Paris: Klincksiek.Zaenen, A.
& R. Kaplan (1995).
Formal Devicesfor Linguistic Generalizations: West GermanicWord Order in LFG.
In M. Dalrymple, R. Ka-plan, J. Maxwell & A. Zaenen (Eds.
), For-mal Issues in Lexicab Functional Grammar, pp.215-240.
CSLI Lecture Notes 47, Stanford/CA:CSLI.38
