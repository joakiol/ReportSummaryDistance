The Text REtrieval Conferences (TRECs)Donna HarmanNational Institute of Standards and TechnologyGaithersburg, MD 20899There have been four Text REtrieval Conferences(TRECs); TREC-1 in November 1992, TREC-2 in Au-gust 1993, TREC-3 in November 1994 and TREC-4 inNovember 1995.
The number of participating systemshas grown from 25 in TREC-1 to 36 in TREC-4, includ-ing most of the major text retrieval software companiesand most of the universities doing research in text re-trieval (see table for some of the participants).
The di-versity of the participating roups has ensured thatTREC represents many different approaches to text re-trieval, while the emphasis on individual experimentsevaluated in a common setting has proven to be a majorstrength of TREC.The test design and test collection used for documentdetection i  TIPSTER was also used in TREC.
The par-ticipants ran the various tasks, sent results into NIST forevaluation, presented the results at the TREC confer-ences, and submitted papers for a proceedings.
The testcollection consists of over 1 million documents from di-verse full-text sources, 250 topics, and the set of rele-vant documents or "right answers" to those topics.
ASpanish collection has been built and used duringTREC-3 and TREC-4, with a total of 50 topics.TREC-1 required significant system rebuilding bymost groups due to the huge increase in the size of thedocument collection (from a traditional test collection ofseveral megabytes in size to the 2 gigabyte TIPSTERcollection).
The results from TREC-2 showed signifi-cant improvements over the TREC-1 results, and shouldbe viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a2 gigabyte collection.TREC-3 therefore provided the first opportunity formore complex experimentation.
The major experimentsin TREC-3 included the development of automaticquery expansion techniques, the use of passages or sub-documents o increase the precision of retrieval results,and the use of the training information to select only thebest terms for routing queries.
Some groups exploredhybrid approaches (such as the use of the Rocchiomethodology in systems not using a vector space mod-el), and others tried approaches that were radically dif-ferent from their original approaches.TREC-4 allowed a continuation of many of thesecomplex experiments.
The topics were made muchshorter and this change triggered extensive investiga-tions in automatic query expansion.
There were alsofive new tasks, called tracks.
These were added to helpfocus research on certain known problem areas, and in-eluded such issues as investigating searching as an inter-active task by examining the process as well as the out-come, investigating techniques for merging results fromthe various TREC subcollections, examining the effectsof corrupted ata, and evaluating routing systems usinga specific effectiveness measure.
Additionally moregroups participated ina track for Spanish retrieval.The TREC conferences have proven to be very suc-cessful, allowing broad participation in the overallDARPA TIPSTER effort, and causing widespread use ofa very large test collection.
All conferences have hadvery open, honest discussions of technical issues, andthere have been large amounts of "cross-fertilization" ofideas.
This will be a continuing effort, with a TREC-5conference scheduled inNovember of 1996.A Sample of the TREC-4 ParticipantsCLARITECH/Carnegie Melon UniversityCITRI, AustraliaCity University, LondonCorneH UniversityDepartment ofDefenseExcalibur Technologies, Inc.GE Corporate R & D/New York UniversityGeorge Mason UniversityHNC, Inc.Lexis-NexisLogicon Operating SystemsNEC CorporationNew Mexico State UniversityQueens College, CUNYRutgers University (two groups)Siemens Corporate Research Inc.Swiss Federal Institute of Technology (ETH)University of Cafifomia - BerkeleyUniversity of Massachusetts at AmherstUniversity of WaterlooXerox Palo Alto Research Center373The Text REtrieval Conferences (TRECs)Donna HarmanNational Institute of Standards and TechnologyGaithersburg, MD 208991.
INTRODUCTIONPhase two of the TIPSTER project included twoworkshops for evaluating document detection (informa-tion retrieval) projects: the third and fourth Text RE-trieval Conferences (TRECs).
These workshops wereheld at the National Institute of Standards and Technolo-gy (NIST) in November of 1994 and 1995 respectively.The conferences included evaluation ot only of theTIPSTER contractors, but also of many information re-trieval groups outside of the TIPSTER project.
Theconferences were run as workshops that provided a fo-rum for participating groups to discuss their system re-sults on the retrieval tasks done using the TIP-STER/TREC collection.
As with the first two TRECs,the goals of these workshops were:?
To encourage r search in text retrieval based on large-scale test collections?
To increase communication among industry,academia, nd government by creating an open forumfor exchange of research ideas?
To speed the transfer of technology from researchlabs into commercial products by demonstrating sub-stantial improvements in retrieval methodologies onreal-world problems?
To increase the availability of appropriate evaluationtechniques for use by industry and academia, includ-ing development of new evaluation techniques moreapplicable to current systems?
To serve as a showcase for state-of-the-art retrievalsystems for DARPA and its clients.The number of participating systems has grown from25 in TREC-1 to 32 in TREC-3 (see Table 1) and to 36in TREC-4 (see Table 2).
These systems include mostof the major text retrieval software companies and mostof the universities doing research in text retrieval.
Notethat whereas the universities tend to participate veryyear, the companies often skip years because of theamount of effort required to run the TREC tests.By opening the evaluation to all interested groups,TIPSTER has ensured that TREC represents many dif-ferent approaches totext retrieval.
The emphasis on di-verse experiments evaluated within a common settinghas proven to be a major strength of TREC.The research done by the participating groups in thefour TREC conferences has varied, but has followed ageneral pattern.
TREC-1 (1992) required significantsystem rebuilding by most groups, due to the huge in-crease in the size of the document collection from a tra-ditional test collection of several megabytes in size tothe 2 gigabyte TIPSTER collection.
The second TRECconference (TREC-2) occurred in August of 1993, lessthan 10 months after the first conference.
The results(using new test opics) showed significant improvementsover the TREC- 1 results, but should be viewed as an ap-propriate baseline representing the 1993 state-of-the-artretrieval techniques as scaled up to handling a 2 giga-byte collection.TREC-3 provided an opportunity for complex experi-mentation.
The experiments included the developmentof automatic query expansion techniques, the use of pas-sages or subdocuments to increase the precision of re-trieval results, and the use of training information tohelp systems elect only the best terms for queries.Some groups explored hybrid approaches ( uch as theuse of the Rocchio methodology in systems not using avector space model), and others tried approaches thatwere radically different from their original approaches.For example, experiments in manual query expansionwere done by the University of California at Berkeley,and experiments in combining information from threevery different retrieval techniques were done by theSwiss Federal Institute of Technology (ETH).
For moredetails on the specific system approaches, ee the com-plete overview of the TREC-3 conference, including pa-pers from the participating groups \[1\].TREC-4 presented a continuation of many of thesecomplex experiments, and also included a set of five fo-cussed tasks, called tracks.
Both the main tasks weremore difficult -- the test topics were much shorter, andthe test documents were harder to retrieve.
Severalgroups made major changes in their retrieval algorithms,and all groups had difficulty working with the very shorttopics.
Many interesting experiments were done in thetracks, including 10 groups that worked with Spanish374Australian National UniversityCLARITECH/Camegie Mellon UniversityCity University, LondonDublin City UniversityFulcrumLogicon Operating SystemsMead Data CentralNew York UniversityQueens CollegeSiemens Corporate Research Inc.TRW/ParacelUniversity of California - BerkeleyUniversity of Massachusetts at AmherstUniversity of MinnesotaUniversite de Neuchatel, SwitzerlandWest Publishing Co.BellcoreCITRI, AustraliaComeU UniversityEnvironment Research Institute of MichiganGeorge Mason UniversityMayo Clinic/FoundationNational Security AgencyNEC CorporationRutgers University (two groups)Swiss Federal Institute of Technology (ETH)Universitaet Dortmund, GermanyUniversity of Central FloridaVPI&SU (Virginia Tech)University of TorontoVerity Inc.Xerox Palo Alto Research CenterTable 1:TREC-3 ParticipantsAustralian National UniversityCITRI, AustraliaComell UniversityDublin City UniversityFS ConsultingGeorge Mason UniversityHNC, Inc.InText Systems (Australia)Logicon Operating SystemsNEC CorporationOracle CorporationRutgers University (two groups)Swiss Federal Institute of Technology (ETH)University of California- BerkeleyUniversity of Central FloridaUniversity of KansasUniversity of TorontoUniversity of WaterlooCLARITECH/Camegie Mellon UniversityCity University, LondonDepartment of DefenseExcalibur Technologies, Inc.GE Corporate R & D/New York UniversityGeorgia Institute of TechnologyInformation Technology InstituteLexis-NexisNational University of SingaporeNew Mexico State UniversityQueens College, CUNYSiemens Corporate Research Inc.Universite de NeuchatelUniversity of California - Los AngelesUniversity of GlasgowUniversity of Massachusetts at AmherstUniversity of VirginiaXerox Palo Alto Research CenterTable 2:TREC-4 Participantsdata, and 11 groups that ran extensive experiments in in-teractive retrieval.
Details of specific system approach-es are in the proceedings ofthe TREC-4 conference \[2\].2.
THE TASKS2.1 The Main TasksAll four TREC conferences have centered around twomain tasks based on traditional information retrievalmodes: a "routing" task and an "adhoc" task.
In therouting task it is assumed that the same questions are al-ways being asked, but that new data is being searched.This task is similar to that done by news clipping ser-vices or by library profiling systems.
In the adhoc task,it is assumed that new questions are being asked againsta static set of data.
This task is similar to how a re-searcher might use a library, where the collection isknown, but where the questions likely to be asked areunknown.In TREC the routing task is represented by usingknown topics and known relevant documents for thosetopics, but new data for testing.
The training for thistask is shown in the left-hand column of Figure 1.
The375participants are given a set of known (or training) topics,along with a set of documents, including known relevantdocuments for those topics.
The topics consist of natu-ral language text describing a user's information eed(see section 3.3 for details).
The topics are used to cre-ate a set of queries (the actual input to the retrieval sys-tem) which are then used against he training docu-ments.
This is represented byQ1 in the diagram.
Manysets of Q1 queries might be built to help adjust systemsto this task, to create better weighting algorithms, and ingeneral to prepare the system for testing.
The results ofthis training are used to create Q2, the routing queries tobe used against he test documents (testing task shownon the middle column of Figure 1).The 50 routing topics for testing are a specific subsetof the training topics (selected by NIST).
In TREC-3the routing topics corresponded to the TREC-2 adhoctopics, i.e., topics 100-150.
The test documents forTREC-3 were the documents on disk 3 (see section 3.2).Although this disk had been part of the general trainingdata, there were no relevance judgments for topics100-150 made on this disk of documents.
This less-than-optimal testing was required by the last-minute un-availability of new data.In TREC-4 a slightly different methodology was usedto select he routing topics and test data.
Because of thedifficulty in getting new data, it was decided to selectthe new data first, and then select topics that matchedthe data.
The ready availability of more Federal Regis-ter documents suggested the use of topics that tended tofind relevant documents in the Federal Register.
Twen-ty-five of the routing topics were picked using this crite-ria.
This also created a subcoUection of the longer,more structured Federal Register documents for lateruse in the research community.
The second set of 25routing topics was selected to build a subeollection ithe domain of computers.
The testing documents for thecomputer issues were documents from the Intemet, pluspart of the Ziff coUection.The adhoc task is represented by new topics forknown documents.
This task is shown on the right-handside of Figure 1, where the 50 new test topics are usedto create Q3 as the adhoc queries for searching againstthe training documents.
Fifty new topics (numbers150-200) were generated for TREC-3, with fifty addi-tional new topics created for TREC-4 (numbers201-250).
The known documents used in TREC-3 wereon disks 1 and 2, and those used in TREC-4 were ondisks 2 and 3.
Sections 3.2 and 3.3 give more detailsabout he documents used and the topics that were creat-ed.
The results from searches using Q2 and Q3 are theofficial test results ent to NIST for the routing and ad-hoc tasks.Topics+01TrainingQueries"3 GigabytesTrainingDocuments50RoutingTopics+Q250 RoutingQueriesRoutingDocuments50AdhocTopics+Q350 AdhocQueriesit '2 Gigabytes DocumentsFigure 1.
TREC Main TasksIn addition to clearly defining the tasks, other guide-lines are provided in TREC.
These guidelines deal withthe methods of indexing and knowledgebase construc-tion and with the methods of generating the queriesfrom the supplied topics.
In general, they are con-structed to reflect an actual operational environment,and to allow as fair as possible separation among thediverse query construction approaches.
Three genericcategories of query construction were defined, based onthe mount and kind of manual intervention used.1.
Automatic (completely automatic query construc-tion)2.
Manual (manual query construction)3.
Interactive (use of interactive techniques to con-struct he queries)The participants were able to choose between twolevels of participation: Category A, full participation, orCategory B, full participation using a reduced ataset(1/4 of the full document set).
Each participating groupwas provided the data and asked to turn in either one ortwo sets of results for each topic.
When two sets ofresults were sent, they could be made using differentmethods of creating queries, or different methods ofsearching these queries.
Groups could choose to do therouting task, the adhoc task, or both, and were asked tosubmit he top 1000 documents retrieved for each topicfor evaluation.2.2 The  TracksOne of the goals of TREC is to provide a commontask evaluation that allows cross-system comparisons.This has proven to be a key strength in TREC.
The sec-ond major strength is the loose definition of the twomain tasks allowing a wide range of experiments.
Theaddition of secondary tasks (tracks) in TREC-4 com-bines these strengths by creating a common evaluation376for tasks that are either elated to the main tasks, or are amore focussed implementation f those tasks.Five formal tracks were run in TREC-4: a multilin-gual track, an interactive track, a database mergingtrack, a "confusion" track, and a filtering track.
InTREC-3, four out of the five tracks were run as prelimi-nary investigations into the feasibility of running formaltracks in TREC-4.The multilingual track represents an extension of theadhoc task to a second language (Spanish).
An informalSpanish test was run in TREC-3, but the data arrivedlate and few groups were able to take part.
In TREC~the track was made official and 10 groups took part.There were about 200 megabytes of Spanish data (the ElNorte newspaper f om Monterey, Mexico), and 25 top-ics.
Groups used the adhoc task guidelines, and submit-ted the top 1000 documents retrieved for each of the 25Spanish topics.The interactive track focusses the adhoc task on theprocess of doing searches interactively.
It was felt bymany groups that TREC uses evaluation for a batchretrieval environment rather than the more commoninteractive environments seen today.
However there arefew tools for evaluating interactive systems, and nonethat seem appropriate o TREC.
The use of the interac-tive query construction method in TREC-3 demon-strated interest in using interactive search techniques, oa formal track was formed for TREC-4.
The interactivetrack has a double goal of developing better methodolo-gies for interactive evaluation and investigating in depthhow users search the TREC topics.
Eleven groups tookpart in this track in TREC-4.
A subset of the adhoc top-ics was used, and many different ypes of experimentswere run.
The common thread was that all groups usedthe same topics, performed the same task(s), andrecorded the same information about how the searcheswere done.
Task 1 was to retrieve as many relevant doc-nments as possible within a certain timeframe.
Task 2was to construct the best query possible.The database merging task also represents a focussingof.the adhoc task.
In this case the goal was to investi-gate techniques for merging results from the variousTREC subcoUections (as opposed to treating the collec-tions as a single entity).
Several groups tried these tech-niques in TREC-3 and it was decided to form a track inthis area for TREC-4.
There were 10 subcollectionsdefined corresponding to the various dates of the data,i.e.
the three different years of the Wall Street Journal,the two different years of the AP newswire, the two setsof Ziff documents (one on each disk), and the three sin-gle subcollections (the Federal Register, the San JoseMercury News, and the U.S.
Patents).
The 3participating groups ran the adhoc topics separately oneach of the 10 subcollections, merged the results, andsubmitted these results, along with a baseline run treat-ing the subcollections a a single collection.The "confusion" track represents an extension of thecurrent asks to deal with corrupted ata such as wouldcome from OCR or speech input.
This was a new trackproposed uring the TREC-3 conference.
The track fol-lowed the adhoc task, but using only the category Bdata.
This data was randomly corrupted at NIST usingcharacter deletions, ubstitutions, and additions to createdata with a 10% and 20% error rate (i.e., 10% or 20% ofthe characters were affected).
Note that this process isneutral in that it does not model OCR or speech input.Four groups used the baseline and 10% corruption level:only two groups tried the 20% level.The filtering track represents a variation of the currentrouting track.
For several years some participants havebeen concerned about the definition of the routing task.and a few groups experimented in TREC-3 with analternative definition of routing.
In TREC-4 the trackwas formalized.
It used the same topics, training docu-ments, and test documents as the routing task.
The dif-ference was that the results submitted for the filteringruns were unranked sets of documents satisfying three"utility function" criteria.
These criteria were designedto approximate a high precision run, a high recall ran,and a "balanced" run.
For more details on this track seethe paper "The TREC-4 Filtering Track" by DavidLewis (in the TREC-4 proceedings).3.
THE TEST  COLLECT ION (ENGL ISH)3.1 IntroductionLike most traditional retrieval collections, there arethree distinct parts to this collection -- the documents.the questions or topics, and the relevance judgments or"right answers.
"3.2 The DocumentsThe documents were distributed on CD-ROMs withabout 1 gigabyte of data on each, compressed to fit.
ForTREC-3 and TREC-4, disks 1, 2 and 3 were all avail-able as training material (see Table 3).
In TREC-3.disks 1 and 2 were also used for the adhoc task, and disk3 for the routing task.
In TREC-4, disks 2 and 3 wereused for the adhoc task, and new data (also shown inTable 3) was used for the routing task.
The followingshows the actual contents of each of the three CD-ROMs (disks 1, 2, and 3).377Subset of collection WSJ (disks 1 and 2) AP ZIFF FR (disks 1 and 2) DOESJMN (disk 3) PAT (disk 3)Size of collection(megabytes)(disk 1)(disk 2)(disk 3)Number of records(disk 1)(disk 2)(disk 3)Median number ofterms per record(disk 1)(disk 2)(disk 3)Average number ofterms per record(disk 1)(disk 2)(disk 3)27024729098,73274,52090,25718221827932937733725924124284,67879,91978,32135334635837537037924517834975,18056~20161,02118116711926221124525,96019,8606,711186226,08741239426331331528961017107335438289Training and Adhoc TaskCollectionSourceZiff (disk 3)Federal Register (1994)IR DigestNews GroupsVirtual WorldsSize inMbytes249283723728Terms per RecordMean \] Median263 119456 3902,383 2,225340 235416 225Total Records161,02155,554455102,59810,152Routing Task, TREC-4Table 3: Document StatisticsDisk 1?
WSJ -  Wall Street Journal (1987, 1988, 1989)?
AP  - AP Newswire (1989)?
ZIFF - Articles from Computer Select disks (Ziff-Davis Publishing)?
F R  - Federal Register (1989)?
DOE - Short abstracts from DOE publicationsDisk 2?
WSJ -  Wall Street Journal (1990, 1991, 1992)?
AP -- AP Newswire (1988)?
ZIFF -- Articles from Computer Select disks?
FR  - Federal Register (1988)Disk 3?
S JMN - -  San Jose Mercury News (1991)?
AP -- AP Newswire (1990)?
ZIFF -- Articles from Computer Select disks?
PAT -- U.S.
Patents (1993)Table 3 shows some basic document collection statis-tics.
Although the collection sizes are roughly equiv-alent in megabytes, there is a range of document lengthsacross collections, from very short documents (DOE) tovery long fiR), Also, the range of document lengthswithin a collection varies.
For example, the documents378from the AP are similar in length, but the WSJ, the ZIFFand especially the FR documents have much widerrange of lengths within their collections.The documents are uniformly formatted into SGML,with a DTD included for each collection to allow easyparsing.<DOC><DOCNO> WSJ880406-O090 </DOCNO><HL> AT&T Unveils Services to Upgrade Phone Net-works Under Global Plan </HL ><AUTHOR> Janet Guyon (WSJ Staff) </AUTHOR><DATELINE> NEW YORK </DATELINE><TEXT>American Telephone & Telegraph Co. introduced thefirst of a new generation of phone services with broad</TEXT></DOC>3.3 The TopicsIn designing the TREC task, there was a consciousdecision made to provide "user need" statements ratherthan more traditional queries.
Two major issues wereinvolved in this decision.
First, there was a desire toallow a wide range of query construction methods bykeeping the topic (the need statement) distinct from thequery (the actual text submitted to the system).
The sec-ond issue was the ability to increase the amount ofinformation available about each topic, in particular toinclude with each topic a clear statement ofwhat criteriamake a document relevant.Sample TREC-1/TREC-2 topic<top><head> 7~pster Topic Description<num> Number: 066<dora> Domain: Science and Technology<title> Topic: Natural Language Processing<desc> Description:Document will identi y a type of natural languageprocessing technology which is being developed ormarketed in the U.S.<narr> Narrative:A relevant document will identi~ a company or insti-tution developing or marketing a natural languageprocessing technology, identify the technology, andidenti~ one or more features of the company's prod-uct.<con> Concept(s):1. natural anguage processing2.
translation, language, dictionary, font3.
software applications<fac> Factor(s):<nat> Nationality: U.S.</fac><de f> Definition(s):</top>Each topic is formatted in the same standard methodto allow easier automatic construction of queries.Besides a beginning and an end marker, each topic has anumber, a short title, and a one-sentence description.There is a narrative section which is aimed at providinga complete description of document relevance for theassessors.
Each topic also has a concepts ection with alist of concepts related to the topic.
This section isdesigned to provide a mini-knowledgebase bout a topicsuch as a real searcher might possess.
Additionally eachtopic can have a definitions ection and/or a factors ec-tion.
The definition section has one or two of the defini-tions critical to a human understanding of the topic.The factors ection is included to allow easier automaticquery building by listing specffic items from the narra-tive that constrain the documents hat are relevant.
Twoparticular factors were used in the TREC-1/TREC-2topics: a time factor (current, before a given date, etc.
)and a nationality factor (either involving only certaincountries or excluding certain countries).The new (adhoc) topics used in TREC-3 reflect aslight change in direction.
Whereas theTREC-1/TREC-2 topics were designed to mimic a realuser's need, and were written by people who are actualusers of a retrieval system, they were intended to repre-sent long-standing information eeds for which a usermight be willing to create elaborate topics.
This madethem more suited to the routing task than to the adhoctask, where users are likely to ask much shorter ques-tions.
The adhoc topics used in TREC-3 (topics151-200) are not only much shorter, but also are missingthe complex structure of the earlier topics.
In particularthe concepts field has been removed because it was feltthat real adhoc questions would not contain this field,and because inclusion of the field discouraged researchinto techniques for expansion of "too short" user needexpressions.
The shorter topics do not create a problemfor the muting task, as experience in TREC-1 and 2 hasshown that the use of the training documents allows ashorter topic (or no topic at all).379Sample TREC-3 topic<num> Number: 168<title> Topic: Financing AMTRAK<desc> Description:A document will address the ngle of the Federal Gov-ernment in financing the operation of the NationalRailroad Transportation Corporation (AMTRAK).<narr> Narrative: A relevant document must pro-vide information on the government's responsibilityto make AMTRAK an economically viable enti~.
Itcould also discuss the privatization of AMTRAK as analternative to continuing overnment subsidies.
Doc-uments comparing overnment subsidies given to airand bus transportation with those provided toAMTRAK would also be relevant.In addition to being shorter, the new topics were writ-ten by the same group of people that did the relevancejudgments (see next section).
Specifically, each of thenew topics (numbers 151-200) was developed from agenuine need for information brought in by the asses-sors.
Each assessor constructed his/her own topics fromsome initial statements of interest, and performed all therelevance assessments on these topics (with a fewexceptions).However, participants in TREC-3 felt that the topicswere still too long compared with what users normallysubmit o operational retrieval systems.
Therefore theTREC-4 topics were made even shorter.
Only one fieldwas used (i.e.
there is no title field and no narrativefield).Sample TREC..4 Topic<nura> Number: 207<desc> What are the prospects of the Quebec sepa-ratists achieving independence from the rest ofCanada?Table 4 gives the average number of terms in the title,description, arrative, and concept fields (all three fieldsfor TREC-1 and TREC-2, no concept field in TREC-3,and only a description field in TREC-4).
As can beseen, the topics are indeed much shorter, particularly ingoing from TREC-3 to TREC-4.Mean MedianTREC-1 131 127TREC-2 157 161TREC-3 107 105TREC-4 16 17Table 4: Topic Lengths3.4 The Relevance JudgmentsThe relevance judgments are of critical importance toa test collection.
For each topic it is necessary to com-pile a list of relevant documents; hopefully as compre-hensive a list as possible.
All four TRECs have usedthe pooling method \[3\] to assemble the relevance assess-ments.
In this method a pool of possible relevant docu-ments is created by taking a sample of documentsselected by the various participating systems.
This sam-pie is then shown to the human assessors.
The particu-lar sampling method used in TREC is to take the top100 documents retrieved in each submitted run for agiven topic and merge them into the pool for assess-ment.
This is a valid sampling technique since all thesystems used ranked retrieval methods, with those docu-ments most likely to be relevant returned first.A measure of the effect of pooling can be seen byexamining the overlap of retrieved ocuments.
Table 5shows the statistics from the merging operations in thefour TREC conferences.
For example, in TREC-1 andTREC-2 the top 100 documents from each run (33 runsin TREC-1 and 40 runs in TREC-2) could have pro-duced a total of 3300 and 4000 documents o be judged(for the adhoc task).
The average number of documentsactually judged per topic (those that were unique) was1279 (39%) for TREC-1 and 1106 (28%) for TREC-2.Note that even though the number of runs has increasedby more than 20% (adhoc), the number of unique docu-ments found has actually dropped.
The percentage ofrelevant documents found, however, has not changedmuch.
The more accurate results going from TREC-1 toTREC-2 mean that fewer nonrelevant documents arebeing found by the systems.
This trend continued inTREC-3, with a major drop (particularly for the routingtask) that reflects increased accuracy in rejecting nonrel-evant documents.
In TREC-4, the trend was reversed.In the case of the adhoc task (including most of thetrack runs also), there is a slight increase in the percent-age of unique documents found, probably caused by thewider variety of expansion terms used by the systems tocompensate for the lack of a narrative section in thetopic.
A larger percentage increase is seen in the rout-ing task, due to fewer runs being pooled, i.e., a higherpercentage of documents i likely to be unique.
Alsothe TREC-4 routing task was more difficult, both380because of the long Federal Register documents andbecause there was a mismatch of the testing data to thetraining data (for the computer topics).
Both these fac-tors led to less accurate filtering of nonrelevant docu-ments.The total number of relevant documents found hasdropped with each TREC, and that drop has been causedby a deliberate tightening of the topics each year to bet-ter guarantee completeness of the relevance judgments(see below for more details on this).AdhocPossible Actual RelevantTREC-1 3300 1279 (39%) 277 (22%)TREC-2 4000 1106 (28%) 210 (19%)TREC-3 4800 1005 (21%) 146 (15%)TREC--4 7300 1710 (24%) 130 (7.5%)RoutingPossible Actual RelevantTREC-1 2200 1067 (49%) 371 (35%)TREC-2 4000 1466 07%) 210 (14%)TREC-3 4900 703 (14%) 146 (21%)TREC-4 3800 957 (25%) 132 (14%)Table 5: Overlap of Submitted ResultsEvaluation of retrieval results using the assessmentsfrom this sampfing method is based on the assumptionthat the vast majority of relevant documents have beenfound and that documents hat have not been judged canbe assumed to be not relevant.
A test of this assumptionwas made using TREC-2 results, and again during theTREC-3 evaluation.
In both cases, a second set of 100documents was examined from each system, using onlya sample of topics and systems in TREC-2, and using alltopics and systems in TREC-3.For the TREC-2 completeness tests, a median of 21new relevant documents per topic was found (11%increase in total relevant documents).
This averages to3 new relevant documents found in the second 100 doc-uments for each system, and this is a high estimate forall systems since the 7 runs sampled for additional judg-ments were from the better systems.
Similar resultswere found for the more complete TREC-3 testing, witha median of 30 new relevant documents per topic for theadhoc task, and 13 new ones for the routing task.
Thisaverages to well less than one new relevant documentper run, since 48 runs from all systems were used in theadhoc test (49 runs in the routing test).
These testsshow that the levels of completeness found during theTREC-2 and TREC-3 testing are quite acceptable forthis type of evaluation.The number of new relevant documents found wasshown to be correlated with the original number of rele-vant documents.
Table 6 shows the breakdown for the50 adhoc topics in TREC-3.
The median of 30 new rel-evant documents occurs for a topic with 122 originalrelevant documents.
Topics with many more relevantdocuments initially tend to have more new ones found,and this has led to a greater emphasis on using topicswith fewer elevant documents.TREC-3 -- Relevant DocumentsFound above 100Percent No.
ofNew Rel.
Topics0% 11-9% 1210-19% 720-29% 2230-36% 8AverageMedianAverage AverageNew Rel.
No.
Rel.0 853 6513 9659 237137 38150 19630 122Table 6: Relationship between completeness and theinitial number of relevant documentsIn addition to the completeness i ue, relevance judg-ments need to be checked for consistency.
In each ofthe TREC evaluations, each topic was judged by a sin-gle assessor to ensure the best consistency of judgment.Some testing of this consistency was done afterTREC-2, when a sample of the topics and documentswas rejudged by a second assessor.
The results howedan average agreement between the two judges of about80%.
In TREC-4 all the adhoc topics had samplesre judged by two additional assessors, with the resultsbeing about 72% agreement among all three judges, and88% agreement between the initial judge and either oneof the two additional judges.
This is a remarkably highlevel of agreement in relevance assessment, and proba-bly is due to the general lack of ambiguity in the topics.4.
EVALUATIONAn important component of TREC was to provide acommon evaluation fonun.
Standard recall/precisionfigures have been calculated for each TREC system,along with some single-value evaluation measures.
Newfor TREC-3 was a histogram for each system showingperformance on each topic.
In general, more emphasishas been placed on a "per topic analysis' in an effort toget beyond the problems of averaging across topics.3811 .ooSample  Reca l l /P rec i s ion  Curves"30.800.60O.400 .20o.ooiimO.00 0 .20  0 .40  0 .60  0 .80  1 .OOReca l l+ System A+ System BFigure 2.
A Sample Recall/Precision Curve.Work has been done, however, to find statistical differ-ences among the systems (see paper "A Statistical Anal-ysis of the TREC-3 Data" by Jean Tague-Sutcliffe andJames Blustein in the TREC-3 proceedings.)
Addition-ally charts have been published in the proceedings thatconsolidate information provided by the systemsdescribing features and system timing, and allowingsome primitive comparison of the amount of effortneeded to produce the results.4.1 Definition of Recall/PrecisionFigure 2 shows typical recall/precision curves.
The xaxis plots the recall values at fixed levels of recall,whereRecall =number of  relevant items retrievedtotal number of  relevant items in collectionThe y axis plots the average precision values at thosegiven recall values, where precision is calculated byPrecision =number of relevant items retrievedtotal number of items retrievedThese curves represent averages over the 50 topics.The averaging method was developed many years ago\[4\] and is well accepted by the infomaation retrievalcommunity.
The curves show system performanceacross the full range of retrieval, i.e., at the early stageof retrieval where the highly-ranked ocuments givehigh accuracy or precision, and at the final stage ofretrieval where there is usually a low accuracy, but morecomplete retrieval.
The use of these curves assumes aranked output from a system.
Systems that provide anunranked set of documents are known to be less effec-tive and therefore were not tested in the TREC program.The curves in figure 2 show that system A has a muchhigher precision at the low recall end of the graph andtherefore is more accurate.
System B however hashigher precision at the high recall end of the curve andtherefore will give a more complete set of relevant docu-ments, assuming that the user is willing to look furtherin the ranked list.4.2 Single-Value Evaluation MeasuresIn addition to recall/precision curves, there are 2 sin-gle-value measures used in TREC.The first measure, the non-interpolated average preci-sion, corresponds to the area under an ideal (non-interpolated) recall/precision curve.
To compute thisaverage, a precision average for each topic is first calcu-lated.
This is done by computing the precision afterevery retrieved relevant document and then averaging382these precisions over the total number of retrieved rele-vant documents for that opic.
These topic averages arethen combined (averaged) across all topics in the appro-priate set to create the non-interpolated average preci-sion for that set.The second measure used is an average of the preci-sion for each topic after I00 documents have beenretrieved for that topic.
This measure isuseful becauseit reflects a clearly comprehended retrieval point.
Ittook on added importance in the TREC environmentbecause only the top 100 documents retrieved for eachtopic were actually assessed.
For this reason it producesa guaranteed valuation point for each system.5.
RESULTS5.1 IntroductionOne of the important goals of the TREC conferencesis that the participating roups freely devise their ownexperiments within the TREC task.
For some groupsthis means doing the routing and/or adhoc task with thegoal of achieving high retrieval effectiveness perfor-mance.
For other groups, however, the goals are morediverse and may mean experiments in efficiency,unusual ways of using the data, or experiments in how"users" would view the TREC paradigm.The overview of the results discusses the effec-tiveness of the systems and analyzes ome of the simi-larities and differences in the approaches that weretaken.
It points to some of the other experiments run inTREC-3 where results cannot be measured completelyusing recall/precision measures, and discusses the tracksin TREC-4.In all cases, readers are referred to the system papersin the TREC-3 and TREC-4 proceedings for moredetails.5.2 TREC-3 Adhoc ResultsThe TREC-3 adhoc evaluation used new topics (top-ics 151-200) against wo disks of training documents(disks 1 and 2).
A dominant feature of the adhoc task inTREC-3 was the removal of the concepts field in thetopics (see more on this in the discussion of the topics,section 3.3) Many of the participating groups designedtheir experiments around techniques to expand theshorter and less "rich" topics.There were 48 sets of results for adhoc evaluation iTREC-3, with 42 of them based on runs for the full dataset.
Of these, 28 used automatic construction fqueries,12 used manual construction, and 2 used interactiveconstruction.Figure 3 shows the recall/precision curves for the 6TREC-3 groups with the highest non-interpolated aver-age precision using automatic onstruction of queries.The runs are ranked by the average precision and onlyone run is shown per group (both official Cornell runswould have qualified for this set).A short summary of the techniques used in these runsshows the breadth of the approaches.
For more detailson the various runs and procedures, please see the citedpaper in the TREC-3 proceedings.cityal -- City University, London ("Okapi at TREC-3"by S.E.
Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu and M. Gatford) used a probabilistic termweighting scheme similar to that used in TREC-2, butexpanded the topics by up to 40 terms (average around20) automatically selected from the top 30 documentsretrieved.
They also used dynamic passage retrieval inaddition to the whole document retrieval in their finalranking.1NQ101 -- University of Massachusetts at Amherst("Document Retrieval and Routing Using theINQUERY System" by John Broglio, James P. Callan,W.
Brace Croft and Daniel W. Nachbar) used a versionofprobabilistic weighting that allows easy combining ofevidence (an inference net).
Their basic term weightingformula (and query processing) was simplified from thatused in TREC-2, and they also used passage retrievaland whole document information in their ranking.
Thetopics were expanded by 30 phrases that were automati-caUy selected from a phrase "thesaurus" that had beenpreviously built automatically from the entire corpus ofdocuments.CrnlEA -- ComeU University ("Automatic QueryExpansion Using SMART: TREC-3 by Chris Buckley,Gerard Salton, James Allan and Amit Singhal) used thevector-space SMART system, with term weighting simi-lar to that done in TREC-2.
The top 30 documents wereused in a Rocchio relevance f edback technique to mas-sively expand (500 terms + 10 phrases) the topics.
Nopassage retrieval was done in this run; the second Cor-nell run (CrnlLA) used their local/global weightingschemes (with no topic expansion).westpl -- West Publishing Company ("TREC-3 Ad HocRetrieval and Routing Experiments using the WIN Sys-tem" by Paul Thompson, Howard Turtle, Bokyung Yangand James Flood) used their commercial product (WIN)which is based on the same inference method used in1NQIO1.
Both passages and whole documents were3831.0Best Automatic Adhocoom0.80.60.40.20.0 -a--- r-0.0 0.2 0.4 0.6 0.8Reca l lFigure 3.
Best TREC-3 Automatic Adhoc Results.1.0= cityal= INQ101& CralEA.
westplx pircslt ETH002used in document ranking, but only minimal topicexpansion was used, with that expansion based on pre-constructed general-purpose synonym classes for abbre-viations and other exact synonyms.pircsl - Queens College, CUNY ("TREC-3 Ad-Hoc,Routing Retrieval and Thresholding Experiments usingPIRCS" by K.L.
Kwok, L. Grunfeld and D.D.
Lewis)used a spreading activation model on subdocuments(550-word chunks).
Topic expansion was done byallowing activation from the top 6 documents in addi-tion to the terms in the original topic.
The highest 30terms were chosen, with an average of 11 of those not inthe original topic.ETHO02 -- Swiss Federal Institute of Technology (ETH)("Improving a Basic Retrieval Method by Links andPassage Level Evidence" by Daniel Knaus, Elke Mitten-dorf and Peter Schauble) used a completely new methodin TREC-3 based on combining information from threevery different retrieval techniques.
The three techniquesare a vector-space system, a passage retrieval methodusing a Hidden Markov model, and a "topic expansion"method based on document links generated automati-caUy from analysis of common phrases.The dominant new themes in the automatic adhocruns are the use of some type of term expansion beyondthe terms contained in the "less rich" (TREC-3) topics,and some form of passage or subdocument retrieval ele-ment.
Note that term expansion is mostly a recalldevice; adding new terms to a topic increases thechances of matching the wide variation of terms usuallyfound in relevant documents.
But adding terms alsoincreases the "noise" factor, so accuracy may need to beimproved via a precision device, and hence the use ofpassages, ubdocuments, or more local weighting.Two main types of term expansion were used by thesetop groups: term expansion based on a pre-constructedthesaurus (for example the INQUERY PhraseFinder)and term expansion based on selected terms from thetop X documents (as done by City, ComeU, and PIRCS).Both techniques worked well.
The top 3 runs (cityal,1NQI01, and CrnlEA) have excellent performance (seeFigure 3) in the "middle" recall range (30 to 80%), withthis performance likely coming from the query expan-sion.The use of the top 30 documents as a source of terms,as opposed to using the entire corpus, should be sensi-tive to the quality of the documents in this initial set.Notably, for 6 of the 8 topics in which the INQI01 runwas superior (a 20% or more improvement in averageprecision) to the cityal run, the 1NQ101 run was alsosuperior to the CrnlEA run.
These topics tended to have384CRyINQUERY(11 pt.
average)CornellETHPIRCSbase run passages expansion both0.337 - !
0.388 (15%)0.3180.28420.25780.368 (16%)0.3302 (16%)0.2853 (11%)0.27640.348 (9%)0.3419 (20%)0.2737 (6%)0.401 (19%)0.381 (20%)0.2916 (13%)0.3001 (9%)Table 7: Comparison of Performance (Average Precision)for Passage Retrieval and Topic Expansionfewer relevant documents, but also tended to be topicsfor which the systems bringing terms in manually (suchas by manually selecting from a thesaurus or outsidesources) also did well.Another factor in topic expansion is the number ofterms being added to the topics.
The average number ofterms in the queries is widely varied, with the Citygroup averaging around 50 terms (20 terms from expan-sion), the INQUERY system using around 100 terms onaverage, and the Comell system using 550 terms onaverage.
This huge variation seemed to have little effecton results, largely because ach group found the level oftopic expansion appropriate for their retrieval tech-niques.
The cityal run tended to "miss" more relevantdocuments han the CrnlEA run (7 topics were seriouslyhurt by this problem), but was better able to rank rele-vant documents within the 1000 document cutoff so thatmore relevant documents appeared in the top 100 docu-ments.
This better ranking could have happenedbecause of the many fewer terms that were used, orcould be caused by the use of passage retrieval in theCity run.The use of passages or subdocuments to reduce thenoise effect of large documents has been used for sev-eral years in the PIRCS system.
City, INQUERY andCornell all did many experiments for TREC-3 to firstdetermine the correct length of a passage, and then tofind the appropriate use of passages in their rankingschemes.
INQUERY and Cornell use overlapped pas-sages of fixed length (200 words) as compared to City'snon-overlapped passages of 4 to 30 paragraphs inlength.
All three systems use information from pas-sages and whole documents retrieved rather than pas-sage retrieval alone.
(Cornell's version of this is calledlocal/global weighting.)
Both INQUERY and City com-bined the passage retrieval with query expansion; Cor-nell did two separate runs.The westpl run did not use topic expansion, althougha rrfixture of passages and whole documents was used inthe final ranking of documents.
The performance hassuffered for this in the middle recall range.
West Pub-lishing used their production system to see how far itdiffered from the research systems and therefore did notwant to use more radical topic expansion methods.Additionally they used a shortened topic (title + descrip-tion + first sentence of narrative) because it was moresimilar in length to the topics submitted by their users.The INQI01 run had 18 topics with superior perfor-mance to the westpl run, mostly because of new rele-vant documents being retrieved to the top 1000 docu-ment set.
The westpl nan was superior to the INQIO1run for l I topics, mostly caused by better anking forthose topics.The pircsl system used both passage retrieval (sub-documents) and topic expansion.
This system used farfewer top documents for expansion (the top 6 asopposed to the top 30), and this may have hurt perfor-mance.
There were 22 topics in which the INQIO1 runwas superior to the pircs2 nan, and these were mostlybecause of missed relevant documents.
Even thoughboth systems added about he same number of expan-sion terms, using only the top 6 documents as a sourceof terms for spreading activation might have providedtoo much focussing of the concepts.The ETHO01 run used both topic expansion and pas-sages, in addition to a baseline vector-space system.Both the topic expansion and the passage determinationwere completely new (untried) techniques; additionallythere are known difficulties in combining multiple meth-ods.
In comparison to the ComeU expansion results(CrnlEA), the main problems appear to be missed rele-vant documents for all 17 of the topics where the Cor-nell results were superior.
The ETH results were supe-rior for 8 topics, mostly because of better ranking.Clearly this is a very promising approach and moreexperimentation s eeded.Table 7 shows a breakdown of improvements fromexpansion and passage retrieval that combines informa-tion from the non-official runs given in the individualpapers.
In general groups eem to be getting about 20%improvement over their own baselines (less for ETH andPIRCS), with that improvement coming in different per-centages from passage retrieval or expansion, dependingon the specific retrieval techniques being used.3851.0Best Manual Adhoco ,m.4t -0.80.60.4-0.20.0 !0.0I I I I I0.2 0.4 0.6 0.8RecallFigure 4.
Best TREC-3 Manual Adhoc Results.1.0= INQI02: Brkly7.t ASSCTV1: VTc2s2x pircs2: rutfualFigure 4 shows the recall/precision curves for the 6TREC-3 groups with the highest non-interpolated aver-age precision using manual construction of queries.
Ashort summary of the techniques used in these runs fol-lows.
Again, for more details on the various runs andprocedures, ee the cited papers in the TREC-3 proceed-ings.INQI02  -- University of Massachusetts at Amherst.This run is a manual modification of the INQIO1 run,with strict rules for the modifications to only allowremoval of words and phrases, modification of weights,and addition of proximity restrictions.Brkly7 - University of California, Berkeley ("Experi-ments in the Probabilistic Retrieval of Full Text Docu-ments" by William S. Cooper, Aitao Chert and FredricC.
Gey) is a modification of the Brkly6 run, with thatmodification being the manual expansion of the queriesby adding synonyms found from other sources.
TheBrkly6 run uses a logistic regression model to combineinformation from 6 measures of document relevancybased on term matches and term distribution.
The coef-ficients were learned from the training data in a mannersimilar to that done in TREC-2, but the specific set ofmeasures used has been expanded and modified forTREC-3.
No passage retrieval was done.ASSCTV1 - Mead Data Central, Inc ("Query Expan-sion/Reduction and its Impact on Retrieval Effec-tiveness" by X. Allan Lu and Robert B Keefer) is also amanual expansion of queries using an associative the-saurus built from the TREC data.
The retrieval systemused in ASSCTV1 is the SMART system.VTc2s2 -- Virginia Tech ("Combination of MultipleSearches" by Joseph A. Shaw and Edward A.
Fox) useda combination of multiple types of queries, with 2 typesof natural language vector-space queries and 3 types ofmanually constructed P-Norm (soft Boolean) queries.pircs2 - Queens College, CUNY.
This run is a modifi-cation of the base PIRCS system to use manually con-structed soR Boolean queries.rutfuaI - Rutgers University ("Decision Level DataFusion for Routing of Documents in the TREC3 Con-text: A Best Cases Analysis of Worst Case Results" byPaul B. Kantor) used data fusion methods to combinethe retrieval ranks from three different retrieval schemesall using the INQUERY system.
Two of the schemesused Boolean queries (one with ranking and one with-out) and the third used the same queries without opera-tots.The three dominant themes in the runs using386manually constructed queries are manual modificationof automatically generated queries (INQI02), manualexpansion of queries (Brkly7 and ASSCTV1) and com-bining of multiple retrieval techniques or queries.
Threeruns can be compared to a "baseline" run to check theeffects of manual versus automatic query construction.INQi02, the manually modified version of 1NQ101,had a 15% improvement in average precision overINQi01, and 17 topics that were superior in perfor-mance for the manual system (as opposed to only 3 forthe automatic system).
An analysis of those topicsshows that many more relevant documents were in thetop 1000 documents and the top 100 documents, proba-bly caused by manually eliminating much of the noisethat was producing higher ranks for nonrelevant docu-ments.
This noise elimination could have happenedbecause many spurious terms had been manuallyremoved from the queries (INQI02 had an average ofabout 30 terms as opposed to nearly 100 terms in1NQI01), or could have come from the use of the prox-imity operators.The Brkly7 run, a manually expanded version ofBrkly6, used about the same number of terms as theINQI02 run (around 36 terms on average), but the termshad been manually pulled from multiple sources (asopposed to editing an automatic expansion as done byINQUERY).
The improvement from Brkly6 to Brkly7 isa 34% gain in average precision, with 25 topics havingsuperior performance in the manually expanded run.Note however that there was no topic expansion done inthe automatic Brkly6 run, so this improvement repre-sents the results of a good manual topic expansion overno expansion at all.The INQUERY system outperforms the Berkeley sys-tem by 14% in average precision, with much of that dif-ference coming in the high recall end of the graph (seeFigure 4).
This is consistent with the difference in theirtopic expansion techniques in that the automatic expan-sion (even manually edited) is likely to bring in termsthat users might not select from "non-focussed" sources.The ASSCTV1 nan also represents a manual expansioneffort, but using a pre-built thesaurus as opposed tousing textual sources for the expansion.
The topicswere expanded to create a query averaging around 135terms and then were run using the default CornellSMART system.
A comparison of the automaticallyexpanded CrnlEA run and the manually expanded ASS-CTVI run shows minimal difference in average preci-sion, but superior performance in 18 of the topics for themanual expansion (as opposed to only 10 of the topicshaving superior performance for the automatic Cornellrun).
In both cases, the improvements come fromfinding more relevant documents because of the expan-sions, but different expansion methods help differenttopics.The pircs2 run is a manual query version of the base-line PIRCS system.
A soft Boolean query is createdfrom the topic, but no topic expansion is done.
There isminimal difference in average precision between the twoPIRCS runs, but more topics show superior performancefor the soft Boolean query pircs2 run (8 superior topicsversus 4 superior topics for the topic expansion pircslrun).
It is not clear whether this difference comes fromthe increased precision of the soft Boolean approach orfrom the relatively poor performance ofthe PIRCS termexpansion results.In TREC-3, as opposed to TRECs 1 and 2, the man-ual query construction methods perform better than theirautomatic ounterparts.
The removal of some of thetopic structure (the concepts) has allowed ifferences toappear that could not be seen in earlier TRECs.
Sincetopic expansion was necessary to produce top scores,the superiority of the manual expansion over no expan-sion in the Berkeley runs should not be surprising.
Lessclear is why the manual modifications in the INQI02run showed superior performance to the automatic nanwith no modifications.
The likely explanation is that theautomatic term expansion methods are relatively uncon-trolled in TREC-3 and manual intervention plays animportant role.The last two groups in the top six systems using man-ual query construction used some form of combinationof retrieval techniques.
The Virginia Tech group(VTc2s2) combined the results of up to 5 different typesof query constraction (3 P-Norms with different P val-ues and 2 vector-space, one short and one manuallyexpanded) to create their results.
They used a simplecombination method (adding all the similarity values)and tested various combinations of query types.
Theirbest result combined only two of the query types, one aP-Norm and one a vector-space.
A series of additionalruns (see paper for details) confirmed that the bestmethod was to combine the results of the best two querytechniques (the "long" vector-space and the P=2 P-Norm).
They concluded that improvements from com-bining results only occurred when the input techniqueswere sufficiently different.Although the Rutgers group (rutfual) used moreelaborate combining techniques, they came to the sameconclusion.
Combining different retrieval techniquesoffers improvements over a single technique (over 30%for the Virginia Tech group), but the input techniquesneed to be more varied to get further improvements.But the more varied the individual techniques, the more3871.0TREC3/TREC2 ComparisonAutomatic & Manual Adhoco0 .8 -0.6-0.2-0.00.0........
~.
':~ ....... INQ002 (TREC-2)...... .~ ....... siems2 (TREC-2)----:.-~.~ CLARTM (TREC-2)-- INQ102 (TREC-3)?
cityal (TREC-3)A Brkly7 (TREC-3)0.2 0.4 0.6 0.8 1 .oRecallFigure 5.
Comparison of Adhoc Results for TREC-2 and TREC-3need for elaborate combining methods uch as used inthe rutfual run.
The automatic ETHO01 run best exem-pli.fies the direction needed here; first getting "good"performance for three very different but complementarytechniques and then discovering the best ways of com-bining results.Several comments should be made with respect to theoverall adhoc recall/precision averages.
First, the betterresults are very similar and it is unlikely that there is anystatistical difference between them.
The Scheffe" testsrun by Jean Tague-Sutcliffe (see paper "A StatisticalAnalysis of the TREC-3 Data" by Jean Tague-Sutcliffeand James Blustein in the TREC-3 proceedings) howthat the top 20 category A runs (manual and automaticmixed) are all statistically equivalent at the a=0.05level.
This lack of system differentiation comes fromthe very wide performance variation across topics (thecross-topic variance is much greater than the cross-system variance) and points to the need for moreresearch into how to statistically characterize the TRECresults.As a second point, it should be noted that these adhocresults represent significant improvements overTREC-2.
Figure 5 shows the top three systems inTREC-3 and the top three systems in TREC-2.
Thisimprovement was unexpected as the removal of theconcepts ection seemed likely to cause a considerableperformance drop (up to 30% was predicted).
Insteadthe advance of topic expansion techniques caused majorimprovements in performance with less "user" input (theconcepts).
Because of the different sets of topicsinvolved, the exact amount of improvement cannot becomputed.
However the CorneU group has run oldersystems (those used in TREC-1 and TREC-2) againstthe TREC-3 topics.
This shows an improvement of20%for their expansion run (CrnlEA) over the TREC-2 sys-tem, and this is likely to be typical for many of the sys-terns this year.5.3 TREC-4 Adhoe ResultsThe TREC-4 adhoc evaluation used new topics (top-ics 201-250) against wo disks of training documents(disks 2 and 3).
A dominant feature of the adhoc task inTREC-4 was the much shorter topics (see more on thisin the discussion of the topics, section 3.3).
Manygroups tried their automatic query expansion methodson the shorter topics (with good success); other groupsalso did manual query construction experiments o con-trast hese methods for the very short opics.There were 39 sets of results for adhoc evaluation inTREC-4, with 33 of them based on runs for the full dataset.
Of these, 14 used automatic construction of queries,3880.60.40.20.0Best Automatic Adhoc1.00.8 t0.0 0.2 0.4 0.6 0.8 1.0RecallFigure 6.
Best TREC-4 Automatic Adhoc Results._- CmlAE-- pircsl.t cityal1NQ201x sieraslI,, citfi2and 19 used manual construction.
All of the category Bgroups used automatic construction fthe queries.Figure 6 shows the recall/precision curves for the 6TREC-4 groups with the highest non-interpolated aver-age precision using automatic onstruction of queries.The runs are ranked by the average precision and onlyone run is shown per group (both official Cornell runswould have qualified for this set).A short summary of the techniques used in these runsshows the breadth of the approaches and the changes inapproach from TREC-3.
For more details on the variousruns and procedures, please see the cited papers in theTREC-4 proceedings.Crn lEA  --  Comell University ("New RetrievalApproaches Using SMART: TREC-4" by Chris Buck-ley, Amit Singhal, Mandar Mitra, (Gerald Salton)) usedthe SMART system, but with a non-cosine length nor-realization method.
The top 20 documents were used tolocate 50 terms and 10 phrases for expansion, as con-trasted with using the top 30 documents o massivelyexpand (500 terms + 10 phrases) the topics as inTREC-3.
This change in expansion techniques wasmostly due to the major change in the basic algorithm.However, additional care was taken not to overexpandthe very short topics.
Work has continued at Comell inimproving their radical new matching algorithm, andfurther information can be found in \[5\].p i rcs l  -- Queens College, CUNY ("TREC-4 Ad-Hoc,Routing Retrieval and Filtering Experiments usingPIRCS" by K.L.
Kwok and L. Gmnfeld) used a spread-ing activation model on subdocuments (550-wordchunks).
It was expected that this type of model wouldbe particularly affected by the shorter topics, and experi-ments were run trying several methods of topic expan-sion.
For this automatic run, expansion was done byselecting 50 terms from the top 40 subdocuments inaddition to the terms in the original topic.
Several otherexperiments were made using manual modifica-tions/expansions of the topics and these are reportedwith the manual adhoc results.
The experiments withshort topics has continued and further esults can beseen in \[6\].c i tya l  --  City University, London ("Okapi at TREC-4"by S.E.
Robertson, S. Walker, M.M.
Beaulieu, M. Gat-ford and A, Payne") used a probabilistic term weightingscheme similar to that used in TREC-3.
An average of20 terms were automatically selected from the top 50documents retrieved (only initial and final passages ofthese documents were used for term selection).
The useof passages seemed to have httle effect.
This run was a389base run for theft experiments in manual query editing.INQ201 -- University of Massachusetts at Amherst("Recent Experiments with INQUERY" by James Allan,Lisa Bellesteros, James P. Callan, W. Bruce Croft andZhihong Lu) used a version of probabilistic weightingthat allows easy combining of evidence (an inferencenet).
Their basic term weighting formula underwent amajor change between TREC-3 and TREC-4 that com-bined the TREC-3 INQUERY weighting with theOKAPI (City University) weighting.
They also usedpassage retrieval as in TREC-3, but found it detrimentalin TREC-4.
The topics were expanded by 30 phrasesthat were automatically selected from a phrase "the-saurus" (InFinder) that had previously been built auto-matically from the entire corpus of documents.
Expan-sion did not work as well as in TREC-3, and additionalwork comparing the use of InFinder and the use of thetop documents for expansion is reported in \[7\].s iems l  -- Siemens Corporate Research ("SiemensTREC-4 Report: Further Experiments with DatabaseMerging" by Ellen M. Voorhees) used the SMARTretrieval strategies from TREC-3 in this run (their baserun for the database merging track).
The standard vec-tor normalization was used, and query expansion wasdone using the Rocchio method to select up to 100terms and 10 phrases from the top 15 documentsretrieved.citr i2 -- RMIT, Australia ("Similarity Measures forShort Queries" by Ross Wilkinson, Justin Zobel, andRon Sacks-Davis) was the result of a series of investiga-tions into similarity measures.
The best of these mea-sures combined the standard cosine measure with theOKAPI measure.
No topic expansion was done for thisrun .It is interesting to note that many of the systems didcritical work on their term weighting/similarity mea-sures between TREC-3 and TREC-4.
Three of the top 6runs were results of major revisions in the basic rankingalgorithms, revisions that were the outcome of extensiveanalysis work on previous TREC results.
At Cornellthey investigated the problems with using a cosine nor-realization on the long documents inTREC.
This inves-tigation resulted in a completely new term weight-ing/similarity strategy that performs well for all lengthsof documents.
The University of Massachusetts exam-ined the issue of dealing with terms having a high fie-quency in documents (which is also related to documentlength).
The result of their investigation was a termweighting algorithm that combined the OKAPI algo-rithm (City University) for high frequency terms withthe old INQUERY algorithm for lower frequency terms.The work at RMIT (the citri2 run) was part on theirongoing effort o test various term weighting schemes.These experiments in more sophisticated termweighting and matching algorithms are yet another stepin the adaptation of retrieval systems to a full-text envi-ronment.
The issues of long documents, with theirhigher frequency terms, mean that the algorithms origi-nally built for abstract-length documents need rethink-ing.
This did not happen in earlier TRECs because theproblem seemed less important than, for example, dis-coveting automatic query expansion methods inTREC-3.The dominant new feature in TREC-4 was the veryshort topics.
These topics were much shorter than anyprevious TREC topics (an average reduction from 107terms in TREC-3 to 16 terms in TREC-4).
In generalthe participating roups took two approaches: 1) theyused roughly the same techniques that they would haveon the longer topics, and 2) most of them tried someinvestigative manual experiments.
Of the 6 runs shownin Figure 6, two runs ( INQ201 and c i tya l )  used a simi-lar number and source of expansion terms as for thelonger queries.
The SMART group (Crn/AE) usedmany fewer terms because of their new algorithms.
Thep i rcs l  run was a result of more expansion, but this wasdue to corrections of problems in TREC-3 as opposed tochanges needed for the shorter topics.
The run fromSiemens i ems l  was made as a baseline for databasemerging, and therefore had less expansion.
There wasno expansion i  the cirri21 run.Figure 7 shows the comparison of results betweenTREC-3 and TREC-4 for 4 of the groups that did wellin each evaluation.
As expected, all groups had worseperformance.
The performance for City University,where similar algorithms were used in TREC-3 andTREC-4, dropped by 36%.
A similar drop (34%) wastrue for the INQUERY results, even though the newalgorithm resulted an almost 5% improvement in results(for the TREC-4 topics).
Whereas the CorneU resultsrepresented a major improvement in performance overthe TREC-3 algorithms, their overall performancedropped by 14%.This points to several issues that need further investi-gation in TREC-5.
First, experiments must still con-tinue on the shorter topics, since this represents he typi-cal initial input query.
The results from the shorter top-ics may be so poor that the top documents provide mis-leading expansion terms.
This was a major concern inTREC-3 and analysis of this issue is clearly needed.The fact that passage retrieval, which provided substan-tial improvement of results in TREC-3, did not help3901.0TREC-3 vs TREC-4 Automaticg~O ,mr~0.80.60.40.20.0....
?
*,~ -.-- eityal.
.
.
.
~ .
.
.
.
INQ101.
.
.
.
.
e~ .
.
.
.
.
.
.
CrnlEA~-~ pireslA CmlAE-- pircs 1= cityal* INQ2010.0 0.2 0.4 0.6 0.8 1.0RecallFigure 7.
Comparison of Automatic Adhoc Results for TREC-3 and TREC-4(TREC-3)fr~c-3)(TREC-3)ffREC-3)(TREC-4)ffREC-4)ffREC-4)(TREC-4)1.0TREC-3 vs TREC-4 Manualm.520.80.60.40.20.0.
.
.
.
~- - - - -  INQ102.
.
.
.
.
?
.
.
.
.
.
B rk ly?.
.
.
.
.
~ .
.
.
.
.
ASSCTV1..... ~..... pircs2CnQst2.
- - I - -  INQ202Brklyl0pircs20.0 0.2 0.4 0.6 0.8 1.0RecallFigure 8.
Comparison of Manual Adhoc Results for TREC-3 and TREC-4(T~C-3)(TREC-3)(TREC-3)(TREC-3)(TREC-4)(TREC-4)(TREC-4)(TREC-4)391Best Manual Adhoc1.0 p0.8 40.6 -or~0.4 -0 .2 -0 .0 -  I0.0 0.2 0.4 0.6 0.8 1.0RecallFigure 9.
Best TREC-4 Manual Adhoc Results.7.
CnQst2pires2.t uwgcllINQ202CLARTFI Brklyl0with the shorter TREC-4 topics indicates that othertypes of "noise" control may be needed for short topics.It may be that the statistical "clues" presented by theseshorter topics are simply not enough to provide goodretrieval performance and that better human-aided sys-terns need to be tested.However, the manual systems also suffered majordrops in performance (see Figure 8).
This leads to asecond issue, i.e.
a need for further investigation i to thecauses of the generally poorer performance in theTREC-4 adhoe task.
It may be that he narrative sectionof the topic is necessary to make the intent of the userclear to both the manual query builder and the automaticsystems.
The fact that machine performance mirroredhuman performance in TREC-4 makes the decrease inautomatic system performance more acceptable, but stillrequires further analysis into why both types of queryconstruction were so affected by the very short opics.Figure 9 shows the recall/precision curves for the 6TREC-4 groups with the highest non-interpolated aver-age precision using manual construction of queries.
Ashort summary of the techniques used in these runs fol-lows.
Again, for more details on the various runs andprocedures, see the cited papers in the TREC-4 proceed-ings.CnQst2 -- Excalibur Corporation ("The ExcaliburTREC-4 System, Preparations and Results" by Paul E.Nelson) used manually built queries.
This system uses atwo-level searching scheme in which the documents arefirst ranked via coarse-grain methods, and then theresulting subset is further efined.
There are thesaurustools available for expansion, and this run was the resultof many experiments into such issues as term groupingsand assignment ofterm strengths.pircs2 -- Queens College, CUNY ("TREC-4 Ad-Hoc,Routing Retrieval and Filtering Experiments usingPIRCS" by K.L.
Kwok and L. Grunfeld) is a manualmodification of the automatic queries in pircsl.
Themodification was to replicate words (this increases theweight) and to add a few associated words (an averageof 1.73 words per query or at most 3 content words).The simple replication of words led to a 12% increase inperformance; adding the associated words (the pircs2run) upped this increase to 30% improvement over theinitial automatic query.uwgcll -- University of Waterloo ("Shortest SubstringRanking (MultiText Experiments for TREC-4)" byCharles L.A. Clarke, Gordon V. Cormack, and Forbes J.Burkowski) used queries that were manually built in aspecial query language called CGCL.
This query392language uses Boolean operators and proximity con-straints to create intervals of text that satisfy specificconditions.
The ranking algorithms rely on combiningthe results of increasing less restrictive queries until the1000 document list is created.1NQ202 -- University of Massachusetts at Amherst("Recent Experiments with INQUERY" by James Allan,Lisa Bellesteros, James P. Callan, W. Bruce Croft andZhihong Lu) This run is a manual modification of the1NQ201 run, with strict rules for the modifications thatonly allow removal of words and phrases, modificationof weights, and addition of proximity restrictions.
Thistype of manual modification i creased overall averageprecision by 21%.
The same types of modificationgained only 15.5% in TREC-3.CLARTF-  CLARITECH Corporation ("CLAR/TTREC-4 Experiments" by David A. Evans, NatasaMflic-Frayling, and Robert G. Lefferts).
used theCLARIT system in a machine-aided manual query con-struction process.
The initial query terms were manu-ally modified and weighted, and then terms were manu-ally selected for addition to the query based on an auto-.matic thesaurus extraction process.
This particular runused a manually-built "required terms filter" to locatethe best document windows for use in the thesaurusextraction process.Brklyl 0 -- University of California, Berkeley ("LogisticRegression at TREC4: Probabilistic Retrieval from FullText Document Collections" by Fredric C. Gey, AitaoChen, Jianzhang He and Jason Meggs) uses manually-reformulated queries including expansion using theNews database of the MELVYL electronic atalog toeither add specific instances or synonyms and relatedterms.
The basic retrieval system is a logistic regressionmodel that combines information from 6 measures ofdocument relevancy based on term matches and termdistribution.
The coefficients were learned from thetraining data.These 6 runs (and most of the other manual runs) canbe divided into three different styles of manual queryconstruction.
The first group uses an automatic queryconstruction method as a starting point, and then manu-ally modifies the results.
The INQ202 run is a goodexample of this, where words and phrases wereremoved, term weights were modified, and proximityrestrictions were added to the initial automatic query.The pircs2 results were based on reweighting of theautomatically-generated terms and then adding a fewnew terms.
The cityml (not shown) results were basedon pre-editing the automatically-generated query, andthen post-editing the automatic expansion of that query.The results of these manual modifications werehighly varied.
The manual edits performed by City Uni-versity were only marginally effective.
Manual modifi-cation of term weights eemed to have more impact, asis illustrated by the 12% improvement in the pircs2 run,and also by some unknown percentage ofthe INQUERYmanual results.
However the addition of a few expan-sion terms in the pircs2 run, or the use of proximityrestrictions (INQ202) look to be the most promisingmanual modifications.
Note that several of the runs inthis top 6 make heavy use of some type of proximityrestrictions.
The ConQuest group found majorimprovements from term grouping, and the Multitextsystem from the University of Waterloo relies on prox-imity restrictions for their results.
Since proximityrestrictions are related to the use of phrases (either sta-tistical or syntactic) or the use of additional local infor-mation, this area is clearly a focus for further esearch.The second group, exemplified by uwgcll andBrklylO, used queries completely manually generatedusing some type of auxiliary information resource suchas online dictionaries (uwgcll) or news databases(BrklylO).
The query generated for uwgcH usesBoolean-type restrictors, whereas the query generatedfor BrklylO uses natural language.The third type of manual query construction i volvesa more complex type of human-machine interaction.Both the CnQst2 run and the CLARTF run are results ofexperiments examining a multi-stage process of queryconstruction.
The ConQuest group starts with a manualquery, and then expands this query semi-automaticallyby manually choosing the correct senses of terms toexpand.
Then they manually modify the term weightsand term grouping.
The CLARITECH group manuallymodifies queries that are automatically generated, andthen provides various levels of user control of an auto-matic expansion process (see the CLARITECH paperfor several experiments involving this user control).Note that hese three styles of manual query construc-tion require various levels of user effort and training.Simple edits of automatic queries, user term weighting,and (less likely) proximity restrictions can be done by arelatively untrained user.
The performance of theseusers is not apt to be as good as the 1NQ202 or pircs2results, however, since both of these runs were theresults of the primary system developers functioning asusers .The complete manual generation of queries (such asthe uwgcll or BrklylO efforts) require the types of skillscurrently seen in search intermediaries.
Using specificquery languages takes lots of training, and learning tofind reasonable t rms to expand topics is an art acquired3931.0Best Routing?
)o~ d:0.80.60.40.20.0 i0.0I I I i I0.2 0.4 0.6 0.8Recal lFigure 10.
Best TREe-3 Routing Results.1.0= cityrlpircs3A INQ103dortR1x lsir2I crnlRR..... m ..... Brkly8..... ~ .... westp2..... & ...... losPA1UCF101.... .,~-- nyuir2: FDF2only after lots of practice.
This should be contrastedwith the third type of query construction.
The complexinteraction with the user exemplified by the CnQst2 andCLARTF runs requires a different ype (and possiblylevel) of skills and training.
These systems are a com-pletely new model of search engine, and it will be nec-essary to develop different skills and new "mental mod-els" in order that users can become proficient in search-ing.The amount of effort and training required to achievethese improvements in automatic results hould not pre-clude using these techniques.
Indeed the majorimprovements shown by these methods illustrate theimportance of continuing investigation into the bestplaces for human intervention.
Furthermore, studieshave shown that users feel a need for more control oftheir searching and this control is absent from currentautomatic systems.5.4 TREC-3 Routing ResultsThe routing evaluation used a subset of the trainingtopics (topics 101-150 were used) against he disk oftest d~urnents (disk 3).
Although this disk had beenused in TREC-2, its use in TREC-3 was unexpected asnew data had been promised.
The last minute unavail-ability of this new data made the reuse of disk 3necessary, but since groups had not been training withthis disk (and no relevance judgments were available forthis disk against topics 101-150), the routing resultsshould not be biased by the reuse of old material.The routing task in TREC has remained constant;however there has been a major evolution in the ttirustof the research for this task.
There was minimal trainingdata for TREC-1, and most groups felt that their resultswere even more preliminary than for the adhoc resultsbecause the training data that was available was incom-plete and inconsistent.
This means that routing becamea particularly interesting challenge in TREC-2 whenadequate training data (the results from TREC-1 adhoctopics) became available.The TREC-2 results therefore represent an excellentbaseline of what could be achieved using traditionalalgorithms with large amounts of relevance information.Most notable was the effective use of the Rocchio feed-back algorithm in SMART, where up to 500 new termswere added to the routing topics from the training data.Equally good results were achieved by a probabilisticsystem from the University of Dortmund, where only 30terms were added, but very precise term weighting waslearned from the training data.
Manual construction ofqueries consistently gave poorer performance as theavailability of training data allowed an automatic tuning394of the queries that would be difficult to duplicate manu-ally without extensive analysis.For TREC-3, many groups made only minor modifi-cations to their TREC-2 techniques (and concentratedon the adhoc task).
There were a total of 49 sets ofresults for routing evaluation, with 46 of them based onruns for the full data set.
Of the 46 systems using thefull data set, 24 used automatic onstruction of queries,18" used manual construction, and 4 used interactivequery construction.Figure 10 shows the recall/precision curves for the 12TREC-3 groups with the highest non-interpolated aver-age precision for the routing queries.
(The grey linesonly serve to allow more systems to be shown.)
Thenms are ranked by the average precision and only onerun per group is shown (both official runs sometimeswould have qualified for this set).
A short summary ofthe techniques used in these runs follows.
For moredetails on the various runs and procedures, please seethe cited papers in the TREC-3 proceedings.c i ty r l  - -  City University, London ("Okapi at TREC-3"by S.E.
Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu and M. Gatford) used the same probabilistictechniques as for the adhoc task, but constructed thequery using a very selective set of terms (17 on average)from the relevant documents.p i rcs3  -- Queens College, CUNY ("TREC-3 Ad-Hoc,Routing Retrieval and Thresholding Experiments usingPIRCS" by K.L.
Kwok, L. Grunfeld and D.D.
Lewis)used a spreading activation model based on the topicand on terms selected from about 35% of the relevantmaterial.INQI03  - University of Massachusetts at Amherst("Document Retrieval and Routing Using theINQUERY System" by John Broglio, James P. Callan,W.
Bruce Croft and Daniel W. Nachbar) used the infer-ence net engine (same as for the adhoc task), with topicexpansion of about 60 terms selected from the relevantdocuments.dor tR1  --  University of Dortmund ("Routing and Ad-hoe Retrieval with the TREC-3 Collection in a Dis-tributed Loosely Federated Environment" by NikolausWalczuch, Norbert Fuhr, Michael Pollmann and BirgitSievers) used the SMART retrieval system with a Roc-chio relevance feedback expansion adding 12% newterms and 4% new phrases from the training documents.l s i r2  -- Bellcore ("Latent Semantic Indexing (LSI):TREC-3 Report" by Susan Dumais) used the latentsemantic indexing system to construct areduced imen-sion vector centroid of the relevant documents (no usewas made of the topics).Crn lRR -- Comell University ("Automatic QueryExpansion Using SMART: TREC-3 by Chris Buckley,Gerard Salton, James Allan and Amit Singhal) used thevector-space SMART system and a basic Rocchio rele-vance feedback algorithm adding about 300 terms and30 phrases to the topic.Brk ly8  - -  University of California, Berkeley ("Experi-ments in the Probabilistic Retrieval of Full Text Docu-ments" by William S. Cooper, Aitao Chen and FredricC.
Gey) used only the relevant documents to select alarge number of terms (average 1,357 terlns/topic)which were combined and weighted using a logoddsformula.
A chi-square test was used to select he terms.westp2  - West Publishing Company ("TREC-3 Ad HocRetrieval and Routing Experiments using the WIN Sys-tem" by Paul Thompson, Howard Turtle, Bokyung Yangand James Flood) used their commercial product (WIN),but expanded the topics using up to 50 terms from spe-cially selected parts of relevant documents.l osPA l  - Logicon, Inc. ("Research in Automatic ProfileCreation and Relevance Ranking with LMDS" by JulianA.
Yochum) constructed profiles based on the top 10selected terms from the relevant documents, with termselection based on binomial probability distributions.The profile was used to select all documents containingany of those terms and the documents were then rankedusing a weighting formula.UCFIO1 - University of Central Florida ("UsingDatabase Schemas to Detect Relevant Information" byJames Dfiscoll, Gary Theis and Gene Billings) manuallyconstructed entity-relationship (ER) schemas for eachtopic and also manually created synonym lists for eachlabelled component in the ER schema.
These schemasand lists were then used to select and rank documents.nyu i r2  - -  New York University ("Natural LanguageInformation Retrieval: TREC-3 Report" by TomekStrzalkowski, Jose CarbaUo and Mihnea Marinescu)used NLP techniques to discover syntactic phrases inthe documents.
Both single terms and phrases wereindexed and specially weighted.
The nyu i r2  run  usedtopic expansion based on the relevant documents.11 of these runs were abbreviated runs from one groupFDF2 --  Paracel, Inc. ("The FDF Query Generation395Workbench" by K.I.
Yu, P. Scheibe and F. Nordby) useda series of tools to generate profiles.
These tools usedstatistical methods to create several alternative queries,and automatically evaluated the queries against thetraining data to select he best query for each topic.The recall/precision curves shown in Figure 10 arevery close in performance for the routing, with theScheffe" tests done by Jean Tague-Sutcliffe showing thatthere is no significant differences between the top 22runs.
It is, however, useful to look at the results on a pertopic basis to find trends in performance across tech-niques.The main issue for the TREC-3 routing runs is how tobest select erms for topic expansion.
Note that for theadhoc task the main issue was how to expand a topicbeyond its original terms, hopefully with as little loss inprecision as possible.
For the routing task, however, thepool of terms for expansion is easily determined (i.e.,the terms in the relevant documents), and the problem ishow to select erms from this very large pool.
Corre-spondingly, the major differences in results between therouting runs are not how many relevant documents were"missed" (as for the adhoc task), but how well the rele-vant documents were ranked.An example of this is a comparison between the twoCity runs.
The cityrl system used all relevant docu-ments to select he top T terms, where T varied between3 and 100 (average 47).
Then they used the trainingmaterial to optimize the queries, selecting only thoseterms that improved results.
On average only about 17terms were used in an optimized query.
The unopti-mized version of these queries was used at the cityr2 run(not shown in Figure 10), which did not work as well.The difference in average precision between the twonms is only about 12%, but the optimized cityrl run had14 superior topics (topics with a 20% or moreimprovement in average precision), all caused by betterranking (more relevant documents moved into the top100 documents from the top 1000 documents).
A simi-lar comparison can be made between the cityrl run andthe pircs3 run.
Even though there were more relevantdocuments found by the pircs3 technique, the cityrl runhad 15 superior topics (versus 7 superior forpircs3), allcaused by better anking.The ability to assign better ranks to relevant docu-ments is not strictly tied to being highly selective ofterms.
A comparison of the cityrl, pircs3, INQI03 andCrnIRR runs shows that he INQUERY and PIRCS tech-niques both used an average of around 100 terms intheir queries and retrieved the largest number of relevantdocuments in the top 1000 documents.
The cityrl nan,with only about 17 terms, missed a few relevantdocuments, but did a much better job of ranking theones they found.
However, even though the CrnlRR runused a massive xpansion of greater than 300 terms, theCrnlRR runs were stronger in ranking than in findingrelevant documents.
A comparison of the 1NQ103 runto that of Cornell shows that Cornell had 12 "inferior"topics, mostly due to missed relevant documents, and 9superior topics, mostly due to better ranking.
Clearlythe appropriate number of terms to use in a routingquery varies across retrieval techniques.
This sameresult was seen in the adhoc task, where the appropriatenumber of expansion terms also varied across ystems.The top routing results tend to fall into three cate-gories -- those groups that used minimal effort in select-ing terms (CrnlRR, lsir2), those groups that selectedterms based on using only a portion of the relevantmaterial (pircs3 and westp2), and those groups that usedall the material, but carefully selected terms (cityrl,1NQ103, brkly8 and losPA1).Both the Cornell runs and the LSI runs were repeatsof their TREC-2 techniques.
The LSI runs tested usingonly the topic to create a query (no expansion) versususing all the relevant documents (no topic) to create acentroid for use as the query (the Isir2 run).
There is a30% improvement using the relevant documents only.The Comell runs used both the topic and a massive Roc-chio relevance feedback expansion (300+ terms).
Bothgroups used techniques based on a vector-space model(loosely based for the LSI technique), and this modelappears to be able to effectively rank documents despitevery massive queries.
The strength of the Cornell rank-ing was mentioned before, but the LSI ranking is com-parable or even better (18 superior topics for LSI, 9 forCornell, all caused by better anking).Two groups (the PIRCS system and the WIN systemfrom West) experimented with using only portions ofthe training data.
This is mostly an efficiency issue, butalso serves as a term selection method.
The pircs4 run(not shown in Figure 10) used only short documents,where short is defined as not more than 160 unique non-stop stems.
This run did somewhat worse than thepircs3 run, where a combination of these short docu-ments and the top 2400 subdocuments were used.
Inboth runs many fewer documents were used (12% and35% of the relevant material respectively), yet theresults were excellent.
The West group tried multipleexperiments u ing various egments of the relevant doc-uments (best documents only, best 200 paragraphs, andbest op paragraph).
Up to 50 terms were added using acombination ofthe various approaches, with selection ofapproaches done on a per topic basis.
This selective useof material caused some relevant documents to bemissed.
A comparison of the westp2 run and the3961NQI03 run shows that the 12 topics in which the1NQI03 run was superior were mostly caused by newrelevant documents being found, whereas the 7 topics inwhich the westp2 run was superior were all caused bybetter anking.Four groups (cityrl, INQI03, brkly8, and LosPA1)used all the relevant documents, but made careful selec-tion of the terms to use.
The City results have alreadybeen discussed.
The INQI03 run used an adaptation ofthe Roechio algorithm with their inference ngine tech-nique.
A statistical formula was used to select he top32 terms to use for expansion for each topic, and then30 additional terms were selected based on their prox-imity to those terms already selected.
This techniqueretrieved a large number of the relevant documents intothe top 1000 slots, but had more difficulties doing theranking within that set.
The brkly8 run selected an aver-age of over 1000 terms by using a chi-square test toindicate which stems were statistically associated withdocument relevance to a topic.
These terms wereweighted and used as the query.
The losPA1 run used asimilar technique, calculating a binomial probability toselect he top 1000 terms, selecting a pool of documentsusing an OR of the top 10 terms, and then scoring thedocuments using a weighting algorithm based on occur-rances of the 1000 terms in those documents.
If resultsfrom these two systems are compared to the more tradi-tional 1NQI03 method, it seems that the strengths ofthese methods are in the ranking, with some problems inmissing relevant documents.As was the case in earlier TRECs, the manual con-struction of routing queries was not very competitivewith automatic query construction.
The manual INQ104run, consisting of a merge of the INQI03 queries and amanually edited version of these queries was little dif-ferent in results from the 1NQ103 run.
An exception tothis was the reasonable r sults of the UCFI01 run.
Thisrun combined manually constructed etailed entity-relationship schema with manually constructed syn-onym lists.
These were run against the documents, pro-ducing results that are comparable with the automaticresults.There is some improvement in overall routing resultscompared with those from TREC-2.
This is mostlyshown by the comparative position of the CrnIRR run,which was the "top-ranked" run in TREC-2, and now ismore the "middle of the pack.
"5.5 TREC-4 Routing ResultsThe routing evaluation used a specifically selectedsubset of the training topics, with that selection guidedby the availability of new testing data.
The ease ofobtaining more Federal Register documents suggestedthe use of topics that tended to find relevant documentsin the Federal Register and 25 of the routing topics werepicked using this criteria.
The second set of 25 routingtopics were selected to build a subcollection in thedomain of computers.
The testing documents for thecomputer issues were documents from the Internet, pluspart of the Ziff collection (see table 3).There were a total of 28 sets of results for routingevaluation, with 26 of them based on runs for the fulldata set.
Of the 26 systems using the full data set, 23used automatic construction ofqueries, and 3 used man-ual construction.
There were 2 sets of category B rout-ing results, both using automatic construction ofqueries.Figure 11 shows the recall/precision curves for the 6TREC-4 groups with the highest non-interpolated aver-age precision for the routing queries.
The runs areranked by the average precision.
A short summary ofthe techniques used in these runs follows.
For moredetails on the various runs and procedures, please seethe cited papers in the TREC-4 proceedings.1NQ203 -- University of Massachusetts at Amherst("Recent Experiments with INQUERY" by James Allan,Lisa Bellesteros, James P. Callan, W. Bruce Croft andZhihong Lu) used the inference net engine (same as forthe adhoe task).
They made major refinements of thealgorithms used in TREC-3.
The queries were con-structed using a Rocchio weighting approach for termsin relevant and non-relevant training documents, andthen these queries were expanded by 250 new concepts(adjacent erm pairs) found in the 200-word best-matching windows in the relevant documents.
Furtherexperiments were made in weighting terms, includinguse of the Dynamic Feedback Optimization from Cor-nell (and City University).cityr2 -- City University, London ("Okapi at TREC-4"by S.E.
Robertson, S. Walker, M.M.
Beaulieu, M. Gat-ford and A. Payne") used the same probabilistic tech-niques as for the adhoc task, but constructed the queryusing a very selective set of terms (36 on average) fromthe relevant documents.
The method used for termselection involved optimizing the query based on tryingdifferent combinations of terms from the relevant docu-ments.
Since this is a very compute-intensive m thod,the work for TREC--4 looked for more efficient methods.pircsC-- Queens College, CUNY ("TREC-4 Ad-Hoc,Routing Retrieval and Filtering Experiments usingPIRCS" by K.L.
Kwok and L. Grunfeld) used the samespreading activation model used in the adhoc task, but3971.0Best Routingoom0.80.60.40.20.0 -F"0.0 0.2 0.4 0.6 0.8Recal lFigure 11.
Best TREC-4 Routing Results.1.0-- INQ203: cityr2A pircsC?
xeroxlCrnlRE!
nyuge2combined the results of four different query experts.Two of these query experts used different levels of topicexpansion (80 terms and 350 terms), and other two weretrained on specific subsets of the data (FR and Ziff vsWSJ, AP and SJMN).xerox1  - Xerox Research Center ("Xerox Site Report:Four TREC-4 Tracks" by Marti Hearst, Jan Pedersen,Peter Pirolli, Hinrich Schutze, Gregory Grefenstette andDavid Hull) used a complex routing algorithm thatinvolved using LSI techniques to discover the best fea-tures, and then used three different classification tech-niques (combined) to rank the documents selected bythese features.Crn lRE  - CorneU University ("New RetrievalApproaches Using SMART: TREC-4" by Chris Buck-ley, Amit Singhal, Mandar Mitra, (Gerald Salton))worked with the same new SMART algorithms used inthe adhoc task.
Because of inexperience with these newalgorithms, minimal query expansion was used (only 50single terms, as opposed to the TREC-4 300 terms).Dynamic query optimization was tried, but did not help.nyuge2 - GE Corporate Research and New York Uni-versity ("Natural Language Information Retrieval:TREC-4 Report" by Tomek Strzalkowski and Jose PerezCarballo) used NLP techniques to discover syntacticphrases in the documents.
Both single terms andphrases were indexed and specially weighted.
Thenyuge2 run  used topic expansion of up to 200 terms andphrases based on the relevant documents.The issue of what features of documents should beused for retrieval was the paramount issue for all thesegroups (plus most of the other groups doing the routingtask).
It is interesting that he six groups hown in Fig-ure 11 have used very different methods.
The Cornellgroup used traditional Rocchio relevance feedbackmethods to locate and weight 50 terms and 10 statisticalphrases.
The statistical phrases are based on term co-occurance information for the whole collection, not justthe relevant and nonrelevant documents.
The GE/NYUgroup did a massive xpansion using 200 terms and syn-tactic phrases, with those phrases created from a fullparse of the entire collection of documents.
Thesemethods can be contrasted with the INQUERY group,who started with a traditional Rocchio approach toselect and weight 50 terms, but then expanded the queryby 250 word pairs selected from only portions of the rel-evant documents.The other three groups used less traditional methods.The group from City University repeated their very suc-cessful technique from TREC-3, in which they first used398an ordering function to produce a list of terms as candi-date terms for the query.
This list was then optimizedby repeatedly trying different sets of terms.
The finalterm set in the cityr2 run used an average of 36 termsper query, with the number varying across queries.
TheXerox group started by expanding the query using Roc-chio techniques, and used this expanded query to select2000 documents.
These 2000 documents were then fedinto a LSI process to reduce the dimensionality of thefinal feature set.
The final group, the pircsC run fromQueens College, CUNY, was the result of four differentexpansions, two using different levels of expansion andtwo using different subeollections of documents for theexpansion.In addition to using different methods to select hefeatures for the queries, two of the groups experimentedwith different ways of combining these features.
Thegroup from Xerox used three different classificationtechniques, combining the results from these three"experts".
The pircsC group combined the results oftheir four query expansion experts.
Both groups foundthat the combination of experts outperformed using asingle method, even when one method (large expansionin the pircs2 case and neural networks in the xerox1case) was generally superior.
Also both groups foundthat there was a huge variation in performance acrosstopics, with some topics performing best for each of thevarious experts.The use of two different subeoUections of topics (25in each set) for the routing task was, in general, not uti-lized by the various groups.
However, it is very interest-ing to examine the results of the 6 groups hown in Fig-ure 11 when broken into the two subsets.
This is shownin Figure 12.
The most prominant feature of thesegraphs is the difference in the shape of the curves.
TheFederal Register subeoUecfion results (shown in grey)have a sharper drop in precision early in the curve, butbetter performance in general in the high recall end ofthe curve.
Two differences in the subeollections accountfor this.
First, the 25 topics in the FR subeollectionretrieved significantly fewer relevant documents, anaverage of 99 relevant documents, as opposed to anaverage of 164 relevant documents for the computertopics.
Additionally most of these relevant documentsare Federal Register documents, which are very longand traditionally have been difficult to retrieve.
Thesedifferences account for the sharp drop in precision in thelow recall end of the curve.
The higher performance ofmost of these 6 systems at the high recall end of thecurve is somewhat more puzzling.
It may that he typesof terminology in these subcollections are such thattraining is more effective in the FR subcollection.Note that certain of the 6 systems seem more affectedby the two subcollections.
For example, the pircsC runis actually better for the FR subcollection than for thecomputer collection.
This is likely because this systemchunks all documents into 550 word segments, andtherefore is less affected by the long FR documents.
Incontrast, he INQUERY system has excellent results forthe computer topics, but a sharp drop in high precisionresults for the FR collectionThere looks to be minimal improvement in overallrouting results compared with those from TREC-3 (Fig-ure 13).
However, the TREC-4 topics were more diffi-cult, particularly the FR topics.
Despite the harder top-ics, many of the systems achieved performanceimprovements, particularly at the high recall end of thecurves.
This indicates that he ability to find useful fea-tures that can retrieve the "hard-to-find" documents igrowing.
Such techniques as the use of word pairs fromhighly ranked sections of relevant documents by theINQUERY system, and the use of multiple experts inthe pircsC and xerox1 runs are showing promise.6, TREC-4  TRACKSStarting with TREC-1, there have always been groupsthat have pursued ifferent goals than achieving highrecall/precision performances on the adhoc and routingtasks.
For example, the group from CITRI, Royal Mel-bourne Institute of Technology, has investigated effi-ciency issues in several of the TREC evaluations.
ByTREC-3 some of these areas had attracted severalgroups, all working towards the same goal.
Thesebecame informal working groups, and in TREC-4 theseworking groups were formalized into "tracks", with spe-citie guidelines.6.1 The Multilinguai TrackOne of these tracks investigated the issues of retrievalin languages other than English.
An informal Spanishtest was run in TREC-3, but the data arrived late andfew groups were able to take part.
A formal multilin-gual track was formed in TREC-4 and 10 groups tookpart.
Both TREC-3 and TREC-4 used the same docu-ments, about 200 megabytes of the E1 Norte newspaperfrom Monterey, Mexico, but there were 25 different top-ics for each evaluation.
Groups used the adhoc taskguidelines, and submitted the top 1000 documentsretrieved for each of the 25 Spanish topics.In TREC-3, four groups tried this task.
Since therewas no training data for testing (similar to the startupproblems for TREC-1), the groups used simple tech-niques.
No graphs are shown for the results ince therewere not enough groups to create a sufficient relevancepool.
For more details on the individual experiments,3991.0Routing SubcollectionsFederal Register & Computer Topicsogh0.80.60.40.20.00.0 0.2 0.4 0.6 0.8 1.0Reca l lFigure 12.
Comparison of  Results for Federal Register Topics_- INQ203 (Comp)= cityr2 (Comp)pircsC (Comp)x CmlRE (Comp)t nyuge2 (Comp)and Computer Topics--~.------~ INQ203 (FR)....... ,.~ ..... cityr2 (FR)...... ~ ..... pircsC (FR)...... ** ....... CmlRE (FR)...... .~- ...... nyuge2 (FR)1.010.8 -~0.6 g~o .uom0.40.20.0TREC-3 vs TREC-4 Routing Comparison0.0 0.2 0.4 0.6 0.8 1.0Reca l lFigure 13.
Comparison of  Routing Results for TREC-3 and TREC-4..... ~- ..... INQ103 (TREC-3)~-~.- ..... cityr 1 (TREC-3)..... .~---.
p i rcs3  (TR.
.EC-3)- - -4~-  CmlRR.
(TREC-3)-~- - - -~  nyu i r2  (TREC-3)_- INQ203 (TREC-4)= cityr2 (TREC-4)A pircsC (TREC-4)CmIRE (TREC-4)I nyuge2 (TREC-4)400see the cited papers in the TREC-3 proceedings.Crn lVS ,  Crn lES  -- Cornell University ("AutomaticQuery Expansion Using SMART: TREC-3 by ChrisBuckley, Gerard Salton, James Allan and Arnit Singhal)used a baseline SMART run (Crn lVS)  and a SMARTrun with massive topic expansion (Crn lES)  similar totheir TREC-3 English adhoc run.
A simple stemmerand a stoplist of 342 terms were used.SINO02,  S INO01 -- University of Massachusetts atAmherst ("Document Retrieval and Routing Using theINQUERY System" by John Broglio, James P. Callan,W.
Bruce Croft and Daniel W. Nachbar) used theINQUERY system, with SINO01 being a manually mod-ified version of a basic TREC-3 automatic INQUERYrun ($1N002) .
There was no topic expansion.
A Span-ish stemmer produced a 12% improvement in laterexperiments.DCUSP1 - Dublin City University ("Indexing Struc-tures Derived from Syntax in TREC-3: System Descrip-tion" by Alan Smeaton, Ruairi O'DonneU and FergusKelledy) used a trigram retrieval model, with weightingof the trigrams from traditional frequency weighting.
ASpanish stemmer based on the Porter algorithms wasalso used.er ims l  -- Environmental Research Institute of Michigan("Using an N-Gram-Based Document Representationwith a Vector Processing Retrieval Model" by WilliamCavnar) used a quad-gram retrieval model, also withweighting using some of the traditional weightingmechanisms.The major result from this very preliminary experimentin a second language was the ease of porting theretrieval techniques across languages.
Comell reportedthat only 5 to 6 hours of system changes were necessary(beyond creation of any stemmers or stopword lists).Three of these four groups also did the Spanish taskin TREC-4, along with 7 new groups.
This time therewas training data (the results of TREC-3), and groupswere able to do more elaborate testing.
Figure 14 showsthe recall/precision curves for these 10 TREC-4 groups,ordered by non-interpolated average precision.
Thecited papers are in the TREC-4 proceedings.UCFSP1 - University of Central Florida ("Multi-lingual Text Filtering Using Semantic Modeling" byJames R. Driscoll, Sara Abbott, Kai-Lin Hu, MichaelMiller and Gary Theis) used semantic modeling of thetopics.
A profile (entity-relationship schema) was man-ually built for each topic and lists of synonyms wereconstructed, including the use of an automatic Spanishverb form generator.
The synonym list and domain list(instances of entities) were carefillly built by SaraAbbott as part of a student summer project.S INQ010 - University of Massachusetts at Amherst("Recent Experiments with INQUERY" by James Allan,Lisa Bellesteros, James P. Callan, W. Bruce Croft andZhihong Lu) was a Spanish version of the automaticTREC-4 INQ201 run for the adhoc tests.
The Spanishstemmer from TREC-3 was used, and terms wereexpanded using the basic InFinder technique (with anew noun phrase recognizer for Spanish).xerox-sp2  -- Xerox Research Center ("Xerox SiteReport: Four TREC-4 Tracks" by Marti Hearst, Jan Ped-ersen, Peter Pirolli, Hinrich Schutze, Gregory Grefen-stette and David Hull) tested several Spanish languageanalysis tools, including a finite-state morphology and ahidden-Markov part-of-speech tagger to produce correctstemmed forms and to identify verbs and noun phrases.The SMART system was used as the basic searchengine.
Expansion was done using the top 20 retrieveddocuments.Crn lSE  -- Comell University ("New RetrievalApproaches Using SMART: TREC-4" by Chris Buck-ley, Amit Singhal, Mandar M/tra, (Gerald Salton)) is arepeat of the TREC-3 work, using a simple stemmer andstopword list, and expanding by 50 terms from the top20 documents.
The TREC-3 version of SMART wasused.gmuauto  - George Mason University ("ImprovingAccuracy and Run-Time Performance for TREC-4" byDavid A. Grossrnan, David O. Holmes, Ophir Frieder,Matthew D. Nguyen and Christopher E. Kingsbury)used 5-grams with a vector-space type system for rank-ing.
A Spanish stopword list was constructed using aSpanish linguist to prune a list of the most frequent 500terms in the text.Brk lySP3  - University of California, Berkeley ("Logis-tic Regression at TREC4: Probabilistic Retrieval fromFull Text Document Collections" by Fredric C. Gey,Aitao Chen, Jianzhang He and Jason Meggs) trainedtheir logistic regression method on the Spanish resultsfrom TREC-3.
They also built a rule-based Spanishstemmer, including a borrowed file of all verb forms forirregular verbs.
The queries were formed manually bytranslating them into English, searching the MELVYLNEWS database, reformulating the English queriesbased on these searches, and then translating the queriesback into Spanish.4011.0Spanish TREC-40.
m0.8 -~ ..t0.6-0.4 -0.2 -0 .0 -~0.0--:.
,.,\...~.
........ ~,,?
.
.
.
k?
.
.
.
.
.
.
.
.
.
:0.2 0.4 0.6 0.8Reca l lFigure 14.
Results of TREC-4 Spanish Track1.0- -~.~ .... UCFSPI~-.~ .... SINO10?
~ xerox-sp2........ ~ ...... CruISE...... ~ ...... gmuauto...... ,,~ ...... BrklySP3...... ~ ..... eitri-sp2...... ~'~.~ --~ DCUSPO....... -': ....... ACQSPA~---~-~ c r rdmlOc i t r i - sp2  - RMIT, Australia ("Similarity Measures forShort Queries" by Ross Wilkinson, Justin Zobel, andRon Sacks-Davis) tried the combination methods usedfor their English results.
A stop-list of 316 words wascreated, along with a Spanish stemmer that principallyremoved regular verb sutfixes.
Experiments were doneusing combinations ofstopped and stemmed results.DCUSPO - -  Dublin City University ("TREC-4 Experi-ments at Dublin City University: Thresholding PostingLists, Query Expansion with WordNet and POS Taggingin Spanish" by Alan F. Smeaton, Fergus Kelledy andRuairi O'Donnell) used the NMSU part-of-speech tag-ger (at NMSU) as input to the SMART system.
Thismethod also produced the base forms of the terms.
Thetraditional ~f* IDF  weighting was used, but adjectiveswere double-weighted.ACQSPA - -  Department of Defense ("Acquaintance:Language-Independent Document Categorization by N-Grams" by Stephen Huffman) used a 5-gram methodwhich normalizes the resulting document vectors bysubtracting a "collection" centroid vector.
Minimaltopic expansion was done.crn lmlO - -  New Mexico State University ("A TRECEvaluation of Query Translation Methods for Multi-Lingual Text Retrieval" by Mark Davis and Ted Dun-ning) investigated five different methods of query trans-lation.
The Spanish topics were first manually trans-lated into English for use in these tests.
Then five dif-ferent methods were used to automatically translate thetopics into Spanish.
The five methods were 1) a term-by-term translation using a bilingual dictionary, 2) useof the parallel corpus (UN corpus) for high-frequencyterms, 3) use of a parallel corpus to locate statisticallysignificant terms, 4) optimization of 2) and 5) an LSItechnique on the parallel corpus.In general the groups participating in the Spanish taskwere using the same techniques as for English.
This isconsistent with the philosophy that the basic searchengine techniques are language-independent.
Only theauxiliary techniques, uch as stopword lists and stem-mers, need to be language dependent.
Several of thegroups did major linguistic work on these auxiliary files,such as the noun-phrase identifier necessary for expan-sion using InFinder (the INQUERY system) and the twonew Spanish stemmers (Brk lySP3  and  c i t r i - sp2) .
Twogroups used n-gram methods, as did two of the groupsin TREC-3.Several other issues unique to this track should bementioned.
First, the outstanding results from the Uni-versity of Central Florida indicate the benefits of very402careful building of the manual queries, in this case bybuilding extensive synonym sets and other such lists.The utility of this technique outside the rather limiteddomain of the TREC-4 topic set is a question however.The group from Xerox did extensive work with Spanishlanguage tools, but the effort had the same type of mini-real effects generally seen in English.
As a final point,the query translation experiments by New Mexico StateUniversity demonstrated a very interesting approach tothe problem of multilingual retrieval, and hopefully willbe followed by better esults in TREC-5.This track will be run again in TREC-5, with newSpanish data and 25 new Spanish topics.
Also new forTREC-5 will be a Chinese retrieval task, with Chinesedata and 25 Chinese topics.6.2 The Confusion TrackThe "confusion" track represents an extension of thecurrent asks to deal with corrupted ata such as wouldcome from OCR or speech input.
This was a new trackproposed uring the TREC-3 conference.
The track fol-lowed the adhoc task, but using only the category Bdata.
This data was randomly corrupted at NIST usingcharacter deletions, ubstitutions, and additions to createdata with a 10% and 20% error ate (i.e., 10% or 20% ofthe characters were affected).
Note that this process isneutral in that it does not model OCR or speech input.Four groups used the baseline and 10% corruption level;only two groups tried the 20% level.
Figure 15 showsthe recall/precision curves for the confusion track,ordered by non-interpolated average precision.
Two orthree runs are shown for each group, the base run (nocorruption), the 10% corruption level, and (sometimes)the 20% corruption level.
The cited papers are in theTREC--4 proceedings.CrnlB, CrnlBclO -- Cornell University ("New RetrievalApproaches Using SMART: TREC-4" by Chris Buck-ley, Amit Singhal, Mandar Mitra, (Gerald Salton)) useda two-pass correction technique (only one-pass i imple-mented for this nm).
In the first pass, the query isexpanded by all variants that are one transformationfrom the query word.
The second pass improves thefinal ranking of the documents.
This method avoids theuse of a dictionary for correction of corrupted text.ACQUNC, ACQCIO, ACQC20 -- Department ofDefense ("Acquaintance: Language-Independent Docu-ment Categorization by N-Grams" by Stephen Huffman)used an n-gram method which normalizes the resultingdocument vectors by subtracting a "collection" centroidvector.
A 5-gram was used for the 10% corruption leveland a 4-gram for the 20% level.gmucO, gmuclO -- George Mason University Clmprov-ing Accuracy and Run-Time Performance for TREC-4"by David A. Grossman, David O. Holmes, OphirFrieder, Matthew D. Nguyen and Christopher E. Kings-bury) used a 4-gram method with a vector-space typesystem for ranking.
A thresholding technique was triedthat only worked with the best 75 percent of the 4-gramquery in order to improve fficiency.rutfum, rutfuv, rutscn20 - Rutgers University ("TwoExperiments on Retrieval with Corrupted Data andClean Queries in the TREC-4 Adlaoc Task Environment:Data Fusion and Pattern Scanning" by Kwong Bor Ngand Paul B. Kantor) tried the use of 5-grams and datafusion.
The first experiment merged the results of tworuns, one using 5-grams and one using words.
The sec-ond experiment was a pattern scanning scheme calleddotted 5-grams.Since this was the first time this task had been tried,and since also there were very few participating groups,not much can be said about the results.
Three of thefour groups used N-grams, a method that is not knownfor the best results on uncorrupted ata.
The fourthgroup was unable to implement their full algorithms intime for the results.
The track will be run again inTREC-5.
Actual OCR output will be used at that time,as opposed to the randomly corrupted ata used inTREC-4.6.3 The Database Merging TrackA third area, that of properly handling heterogeneouscollections uch as the five main "subcollections" inTREC, was addressed inTREC-3 by the Siemens group(see paper "The Collection Fusion Problem" by EllenVoorhees; Narendra Gupta and Ben Johnson-Laird inthe TREC-3 proceedings).
This group examined twodifferent collection fusion techniques and was able toobtain results within 10% of the average precision of arun using a merged collection index.
This type of inves-tigation is important for real-world collections, and alsoto allow researchers totake advantage of possible varia-tions in retrieval techniques for heterogeneous collec-tions.The general interest in this area led to the formationof a formal track in TREC-4.
There were 10 subcollec-tions defined corresponding tothe various dates of thedata, i.e.
the three different years of the Wall Street Jour-nal, the two different years of the AP newswire, the twosets of Ziff documents (one on each disk), and the threesingle subcollections (the Federal Register, the San JoseMercury News, and the U.S.
Patents).
The 3 participat-ing groups ran the adhoc topics separately on each ofthe 10 subcollections, merged the results, and submitted4031.0Confusion TREC-4g~o .m0.80.60.40.20.00.0 0.2 0.4 0.6 0.8 1.0Recal lF igure  15.
Results o f  TREC-4  Confus ion Track-- CmlB (base run)CmlBcl0X grout0 (base run)...... ~ ..... gmucl0A ACQUNC (base run)...... ~ ..... ACQC10.
.
.
.
.
.
...:.~ .
.
.
.
.
.
ACQC20= rutfum (base run)rutfuv (base run).... ~ ..... rutscn20oom1.00.8 t0.60.40.20.0Database Merging TREC-4\\ \0.0 0.2 0.4 0.6 0.8 1.0Recal lF igure  16.
Results o f  TREC-4 Database Merging Track_- padreZ(base run)-----,~  padreWx INQ2Ol(base run)..... ~ ...... INQ207A siemsl(base run)..... ,,.-~ ...... siems2404these results, along with a baseline run treating the sub-collections as a single collection,Figure 16 shows the recall/precision curves for thistrack, ordered by non-interpolated average precision.Two runs are shown for each group, the base run(indexed as a single database), and the best of theirmerged runs.
The cited papers are in the TREC-4 pro-ceedings.padreZ, padreW- -  Australian National University("Proximity Operators -- So Near and Yet So Far" byDavid Hawking and Paul Thistlewaite) used manualqueries with proximity operators.
Since there are nocollection-dependent variables in this system, the runusing the 10 separate collections i equivalent to the runusing the entire collection.1NQ201, INQ207 -- University of Massachusetts atAmherst ("Recent Experiments with INQUERY" byJames Allan, Lisa Bellesteros, James P. Callan, W.Bruce Croft and Zhihong Lu) tried five variations of abasic method of collection merging \[8\].
The basicmethod scored each collection against he topic, andthen weighted the document results by their collectionscore.siemsl, siems2 - Siemens Corporate Research("Siemens TREC-4 Report: Further Experiments withDatabase Merging" by Ellen M. Voorhees) tried two dif-ferent methods, both based on information about theprevious queries (training topics) as opposed to usinginformation about he document collection itself.If results are produced without use of collection irtfor-matiort, hen the merging process is trivial, as illustratedby the padre runs.
Certainly this is one method of han-dling the problems of merging results from differentdatabases.
However this precludes using informationabout he collection to modify the various algorithms inthe search engine, and, even more importantly, it doesnot deal with the issue about which collection to select.An implied question in this track is the hypothesis thatone might want to bias searching towards certain collec-tions, either by developing collection scores (such as theINQUERY work) or by developing a sense of historyfrom previous queries (the Siemens work).More work needs to be done in this area, and hope-fully more groups will try this track in TREC-5.6.4 The Filtering TrackThe filtering track represents a variation of the currentrouting task.
For several years some participants havebeen concerned about he definition of the routing task,and a few groups experimented in TREC-3 with analternative method of evaluating routing.
For details onone of these experiments, ee the paper "TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experimentsusing PIRCS" by K.L.
Kwok, L. Grunfeld and D.D.Lewis in the TREC-3 proceedings.In TREC-4 the track was formalized and used thesame topics, training documents, and test documents asthe routing task.
The difference was that he results ub-mitted for the filtering runs were unranked sets of docu-ments atisfying three "utility function" criteria.
Thesecriteria were designed to approximate a high precisionrun, a high recall run, and a "balanced" run.
For moredetails, see the paper "The TREC-4 Filtering Track" byDavid Lewis (in the TREC-4 proceedings).Figure 17 shows the results of the four groups thattried this track.
There are 3 pairs of bars for each sys-tem, one pair corresponding toeach of the three utilityfunction criteria.
The first of the pairs (the left-most andthe right-most bars) correspond to the high preci-sion/low recall run.
The second pair (the second andfifth bars) correspond ~o the balanced (medium preci-sion/medium recall) run, and the third pair (highrecall/low precision run) are shown in the middle twobars.One desired type of system behavior is the "stairstep"effect seen, for example, in the run from HNC SoftwareInc.
(see paper "Using CONVECTIS, A Context Vector-Based Indexing System for TREC-4" by Joel L. Car-leton, William R. Caid and Robert V. Sasseen in theTREC-4 proceedings).
When this system is comparedwith the next two systems (pircs and xerox), it can beseen that while the HNC system got a better separationof the runs, the other two groups got better esults ingeneral, particularly for the balanced run.This was the first time this track had been tried, andthe development of evaluation techniques was the mostcritical area.
Now that these techniques are in place, itis expected that more groups will take part in the trackin TREC-5.6.5 The Interactive TrackThe largest area of focussed experimentation iTREC-3 was in interactive query construction, with fourgroups participating.
One of the questions addressed bythese groups was how well humans could perform therouting task, given a "rules-free" environment andaccess to the training material.
The larger issueaddressed by these experiments, however, was the entireinteraction process in retrieval systems, ince the "batch4050.5 -2Fi l ter ing TREC-4?
.
~ ?
.
.
.
.
.
.
i ?
-^e0"Ill,?
- .
?
?
i .
.
.
.
ip i r~ x ~SystemHigh  Prec i s ion........................... Med ium Prec i s ionLow Prec i s ionH igh  Reca l  !.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Medium Reca l lLow Reca l lF igure  17 .
Resu l ts  o f  TREC-4  F i l te r ing  Track1.0Best Interactive RoutingQ0.8 -0.60.4Interactive_- TOP IC2rutir2J.
c ityi  1rutir 1Not  Interactive..... ~- ..... TOP IC  1...... ~ ..... c i tya l0.20.0010 0.2 0.4 0.6 0.8 1.0RecallFigure 18.
Use of the Interactive Query Construction i TREC-3406mode" evaluation of TREC does not reflect he way thatmost systems are used.Figure 18 shows the three sets of results for the cate-gory A interactive runs in TREC-3, plus several baselineruns for comparison.
A short summary of the systemsfollows, and readers are referred to the individual papersin the TREC-3 proceedings for more details.TOPIC2 -- Verity, Inc. ("Interactive Document RetrievalUsing TOPIC (A report on the TREC-3 experiment)" byRichard Tong) used 12 Verity staff members ranging insearch experience using TOPIC from novice to expert obuild their queries.
The initial queries were the manual-constructed queries used by Verity in TREC-2, and theresults from these queries are shown in Figure 18 asTOPIC1.
The searchers then improved the initialqueries by periodically evaluating their "improved"queries against the training data.
When sufficientlyimproved scores were achieved, the queries weredeclared final and used for TREC-3.rutirl,  rutir2 - Rutgers University ("New Tools and OldHabits: The Interactive Searching Behavior of ExpertOnline Searches using INQUERY" by Jurgen Koene-mann, Richard Quatrain, Colleen Cool and NicholasBelkin) used the INQUERY system and had 10 experi-enced online searchers with no prior experience usingthat system build their queries.
The entire query build-ing process was restricted to 20 minutes per topic, andused the training data both for automatic relevance f ed-back (if desired) and for the searchers to check if agiven retrieved ocument was relevant (as opposed toperiodically evaluating their results).
At some pointduring the 20 minute limit the queries were declared fin-ished by the searchers and the results from these queriesare shown in Figure 18 as rutirl.
As a comparison, theexperimenters al o did the task themselves (rutir2).cityil -- City University, London ("Okapi at TREC-3"by S.E.
Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu and M. Gatford) used the OKAPI team assearchers.
The initial query was manually generatedusing traditional operations.
The retrieved ocuments(or a brief summary of them) were then displayed, andsearchers checked the relevance judgments (generallyviewing 10 or 12 relevant documents).
Automatic rele-vance feedback was then applied and the searcherscould choose to modify the resulting query or not (35 ofthe 50 topics were modified).
Multiple iterations couldbe done before a decision was made on the final query.Not shown in Figure 18 is a category B interactive r sultfrom the University of Toronto ("Interactive Explorationas a Formal Text Retrieval Method: How Well canInteractivity Compensate for Unsophisticated RetrievalAlgorithms" by Nipon Charoenkitkarn, Mark Chignelland Gene Golovchinsky).
This group developed theirTREC experiments from what was initially a browsingsystem.
Boolean operators and promixity operatorswere used to construct the initial query.
The querieswere then "loosened" until around 1000 documents wereretrieved.
Then the results of these queries were runagainst he training data and reviewed, with changespossibly made to the query based on retrieval results.As a group, the interactive results were considerablyworse than the automatic routing results.
This wassomewhat unexpected since in all four cases the queriescould be classified as the best manual queries possible.Although no definite reasons have been cited for this,the likely cause is the very strong performance of theautomatic systems given the large amounts of trainingdata.A comparison of the City interactive run (cityil) andthe City automatic run (cityal) illustrates the problems.For BOTH runs, the query lengths were short, an aver-age of around 17 terms.
Only about 20% of these termswere in common, i.e., the searchers (cityil) and the"computer" (cityal) picked different sets of terms.
Thedifference in the results from these queries, however, isvery large, as shown in Figure 18.
The automatic runhas a 63% improvement in average precision, and 33topics with superior results (a 20% or moreimprovement in average precision) versus one topic withinferior esults.Regardless of the poorer performance, all four groupswere able to draw interesting conclusions about theirown interactive experiments.
The Verity group found a24% improvement in results (TOPIC1 to TOPIC2) thatcan be obtained by humans using the training materialover the (manually created) initial query.
Other groupswere able to gain insight into better tools needed bytheir system or insight into how online searchers handlethe new techniques available.
Of particular interest arethe reports in these papers about the detailedhuman/computer interactions, asthis provides insight onhow systems might work in an operational setting.A formal interactive track was formed for TREC-4,with the double goal of developing better methodologiesfor interactive evaluation and investigating in depth howusers search the TREC topics.
Eleven groups took partin this track in TREC-4, using a subset of the adhoc top-ics.
Many different types of experiments were run, butthe common thread was that all groups used the sametopics, performed the same task(s), and recorded thesame information about how the searches were done.Task 1 was to retrieve as many relevant documents as407possible within a certain timeframe.
Task 2 was to con-stmct he best query possible.Three of the four groups that did interactive queryconstruction i  TREC-3 also participated in TREC-4.Seven new groups also tried this track.
The cited papersare in the TREC-4 proceedings.ru t in t l ,  rut int2  - Rutgers University ("Using RelevanceFeedback and Ranking in Interactive Searching" byNicholas J. Belkin, Colleen Cool, Jurgen Koenemann,Kwong Bor Ng and Soyeon Park) recruited 50 searchersfor this task.
The INQUERY search engine was used,and the particular emphasis was on studying the use ofranking and relevance f edback by these searchers.c i ty i l  - -  City University, London ("Okapi at TREC-3"by S.E.
Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu and M. Gatford) used members of their teamto evaluate theft new GUI interface to OKAPI.
Theyconcentrated onexamining the various tages of search-ing, and kept notes on items of interest, such as howmany rifles were examined, how many iterations wererun, and how the queries were edited at various times inthe search process.UoJTo l  - -  University of Toronto ("Is Recall Relevant?An Analysis of How User Interface Conditions affectStrategies and Performance in Large Scale TextRetrieval" by Nipon Charoenkitkarn, Mark H. Chignelland Gene Golovchinsky) used 36 searchers on a newversion of their system called BrowslR.
The goal oftheir experiments was to compare three different strate-gies for constructing queries: a text markup (similar tothat done by this group in TREC-3), a query typingmethod, and a hybrid method.
Both experts and noviceswere used.ETHI01  --  Swiss Federal Institute of Technology (ETH)("Highlighting Relevant Passages for Users of Interac-tive SPIDER Retrieval System" by Daniel Knaus, ElkeMittendorf and Peter Sc~uble and Paraic Sheridan)experimented with several algorithms to highlight hemost relevant passages, and tested this on I 1 users as anaid to relevance f edback.XERINT1,  XEROXINT2 --  Xerox Research Center("Xerox Site Report: Four TREC-4 Tracks" by MartiHearst, Jan Pedersen, Peter Pirolli, Hinrich Schutze,Gregory Grefenstette and David Hull) tried three differ-ent modes of searching interfaces.
The first was theScatter/Gather method of visualizing the documentspace, the second was the TileBars to visualize the doc-urnents, and the third was the more traditional rankedlist of titles from a vector space search engine.CLART1- -  CLARITECH Corporation ("CLARITTREC-4 Interactive Experiments" by Natasa Milic-Frayling, Cheng-Xiang Zhai, Xiang Tong, Michael P.Mastroianni, David A. Evans and Robert G. Lefferts)used the CLARIT system interactively to study theeffects of the quality of a user's relevance judgments,the effects of rime constraints on searching, and theeffects of relevance feedback on the final results ofqueries.LNBOOL - -  Lexis-Nexis ("Interactive Boolean Search inTREC4" by David James Miller, John D. Hold and X.Allan Lu) used expert Boolean searchers and the com-mercial Lexis-Nexis software to compare retrieval per-formance between Boolean and non-Boolean systems.gat in l ,  gat in2  - -  Georgia Institute od Technology("Interactive TREC-4 at Georgia Tech" by AravindanVeerasamy) investigated the effectiveness ofa new visu-alization tool that shows the distribution of query termsacross the document space.ACQ1NT-  Department of Defense ("Acquaintance:Language-Independent Document Categorization by N-Grams" by Stephen Huffman) used the Parentage infor-mation visualization system which shows clusters ofdocuments, along with the terms which characterizethose clusters.Crn l l I ,  C rn l l2  - -  ComeU University ("New RetrievalApproaches Using SMART: TREC-4" by Chris Buck-ley, Arnit Singhal, Mandar Mitra, (Gerald Salton)) didan experiment totest how much of the document eededto be read in order to determine document relevancy forinput to relevance f edback.
They tested quick scans vsfull reading.The various results presented from this track werevery interesting and useful.
However, all participantswere concerned about the difficulties of comparingresults.
One of the major outcomes of this track inTREC-4 was the awareness of the large number of vari-ables that need to be controlled in order to compareresults.
Some of these, such as the variation in perfor-mance across topics, affect all the TREC tasks, but thehuman element in the interactive track compounds theproblem immensely.
The emphasis in TREC-5 workwill be on learning to control or monitor some of thesevariables as a first step to providing better evaluationmethodology.7.
SummaryThe TREC-3 and TREC-4 evaluations have producedmany important experiments for all the participating408groups.
Some general conclusions can be drawn fromeach evaluation effort.The main conclusions that can be drawn fromTREC-3 are as follows:?
Automatic construction of routers or filters fromtraining data was very effective, much more effectivethan manual construction of these types of queries.This held even if the manual construction was basedon unrestricted use of the training data.?
Expansion of the "less-rich" TREC-3 topics washighly successful, using either automatic topic expan-sion, manual topic expansion, or manually modifiedversions of automatically expanded topics.
Many dif-ferent techniques were effective, with research justbeginning in this new area.?
The use of passage retrieval, subdocnments, and localweighting brought consistent performanceimprovements, especially in the adhoc task.
Experi-ments in TREC-3 showed continued improvementcoming from various methods of using these tech-niques to improve ranking.?
Preliminary results suggested that the extension ofbasic English retrieval techniques into another lan-guage (in particular Spanish) did not appear difficult.TREC-3 represented the first large-scale test of thisportability issue.Do these conclusions hold in the real world of textretrieval?
Certainly the use of automatic construction ofrouters will work in any environment having reasonableamounts of training material.
Of greater question is thetransferability of the adhoc results.
Two particularissues need to be addressed here.
First, even though thetopics in TREC-3 were 'less-rich", they were still con-siderably longer than most queries used in operationalsettings.
A couple of sentences i  likely to be the maxi-mum a user is willing to type into a computer, and it isunclear if the TREC topic expansion methods wouldwork on these shorter input strings.
Shorter topics mayalso need different echniques of passage retrieval andlocal weighting.
TREC-4 addressed this issue by usingappropriately shorter topics.The second mismatch of the TREC-3 (and TREC-4)results to the real-world is the emphasis on high recall inTREC.
Requesting 1000 ranked documents and calcu-lating the results on these goes well beyond average userneeds.
Karen Sparck Jones addressed this issue bylooking at retrieval performance based only on the top30 documents retrieved \[9\], and has updated her conclu-sions for TREC-3 and TREC-4 in appendices to theappropriate proceedings.
An improvement of 20% inprecision at this cutoff means that six additional relevantdocuments will be returned to the user, and this is likelyto be noticeable by many users.
Many of the techniquesused in TREC produced this difference; additionallysome of the tools being investigated in TREC, such asthe topic expansion tools, will make query modificationmuch easier for the average user.The main conclusions that can be drawn fromTREC-4 are as follows:?
The much shorter topics in the adhoc task caused allsystems trouble.
The expansion methods used inTREC-3 continued to work, but obviously neededmodifications.
The types of passage retrieval used inTREC-3 did not work.
The fact that the performanceof the manually-build queries was also hurt by theshort opics implies that there are some issues involv-ing the use of very short topics in TREC that needfimher investigation.
It may be that the statistical"clues" presented by these shorter topics are simplynot enough to provide good retrieval performance inthe batch testing environment of TREC.
The topicsto be used in TREC-5 will contain both a short and along version to aid in these timber investigations.?
Despite the problems with the short topics, many ofthe systems made major modifications to their termweighting algorithms.
In particular, the SMARTgroup from ComeU University and the INQUERYgroup from the University of Massachusetts atAmherst produced new algorithms that yielded muchbetter esults (on the longer TREC-3 queries), andtheir TREC-4 results were not lowered as much asthey would have been.?
There were five tracks run in TREC-4.?
Interactive - 11 groups investigated searching asan interactive task by examining the process aswell as the outcome.
The major result of this track,in addition to interesting experiments, was anawareness ofthe difficulties of comparing results inan interactive t sting environment.?
Multilingual - 10 groups working with 250megabytes of Spanish and 25 topics verified theease of porting to a new language (at least in a lan-guage with no problems in locating word bound-aries).
Additionally some improved Spanish stem-mers were built.?
Multiple database merging - 4 groups investigatedtechniques for merging results from the variousTREC subeollections.?
Data corruption -- 4 groups examined the effects ofcorrupted ata (such as would come from an OCRenvironment) by using corrupted versions of the409?
Filtering -- 4 groups evaluated routing systems onthe basis of retrieving an unranked set of docu-ments optimizing aspecific effectiveness measure.The results from these last 3 tracks were inconclu-sive, and should be viewed as a first-pass at thesefocussed tasks.There will be a fifth TREC conference in 1996, andmost of the systems that participated in TREC-4 willbe back, along with additional groups.
The routingand adhoc tasks will be done again, with differentdata, and new topics similar in length to the TREC-3topics.
In addition, all five tracks will be run again,with new data.
The Multihngual track will be runwith Spanish and, as a first time, with Chinese dataand topics.AcknowledgmentsThe author would like to gratefully acknowledge thecontinued support of the Software and IntelhgentSystems Technology Office of the Defense AdvancedResearch Projects Agency for the TREC conferences.Special thanks also go to the TREC program commit-tee and the staff at NIST.7.
REFERENCES\[1\] Harman D.
(Ed.).
(1994).
Overview of the ThirdText REtrieval Conference (TREC-3).
National Insti-tute of Standards and Technology Special Publication500-225, Gaithersburg, Md.
20899.\[2\] Harman D.
(Ed.).
(1996).
The Fourth TextREtrieval Conference (TREC-4).
National Institute ofStandards and Technology Special Publication, inpress.\[3\]Sparck Jones K. and Van Rijsbergen C. (1975).Report on the Need for and Provision of an '?deal"Information Retrieval Test Collection, British LibraryResearch and Development Report 5266, ComputerLaboratory, University of Cambridge.\[4\] Salton G. and McGiU M. (1983).
Introduction toModern Information Retrieval.
New York, NY.
:McGraw-Hill.\[5\] Singhal A., Buckley C., and Mitra M. (1996).Pivoted Document Length Normalization.
In: Pro-ceedings of the 19th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, in press.\[6\] Kwok K.L.
(1996).
A New Method of WeightingQuery Terms.
In: Proceedings of the 19th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, in press.\[7\] Xu J. and Croft W.B.
(1996).
Query Expansionusing Local and Global Document Analysis.
In: Pro-ceedings of the 19th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, in press.\[8\] CaUan J.P., Lu Z., and Croft W.B.
(1996).
Search-ing Distributed Collections with Inference Networks.In: Proceedings of the 18th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, 21-29.\[9\] Sparck Jones K. (1995).
Reflections on TREC.Information Processing and Management, 31(3),291-314.410
