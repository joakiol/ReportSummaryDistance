Squibs and DiscussionsAssessing Agreement on Classification Tasks:The Kappa StatisticJean Car let ta  ?University of EdinburghCurrently, computational linguists and cognitive scientists working in the area of discourse anddialogue argue that their subjective judgments are reliable using several different statistics, noneof which are easily interpretable or comparable to each other.
Meanwhile, researchers in contentanalysis have already experienced the same difficulties and come up with a solution in the kappastatistic.
We discuss what is wrong with reliability measures as they are currently used fordiscourse and dialogue work in computational linguistics and cognitive science, and argue thatwe would be better off as afield adopting techniques from content analysis.1.
IntroductionComputational linguistic and cognitive science work on discourse and dialogue relieson subjective judgments.
For instance, much current research on discourse phenomenadistinguishes between behaviors which tend to occur at or around discourse segmentboundaries and those which do not (Passonneau and Litman 1993; Kowtko, Isard, andDoherty 1992; Litman and Hirschberg 1990; Cahn 1992).
Although in some cases dis-course segments are defined automatically (e.g., Rodrigues and Lopes' \[1992\] definitionbased on temporal relationships), more usually discourse segments are defined subjec-tively, based on the intentional structure of the discourse, and then other phenomenaare related to them.
At one time, it was considered sufficient when working with suchjudgments to show examples based on the authors' interpretation (paradigmatically,(Grosz and Sidner \[1986\], but also countless others).
Research was judged accordingto whether or not the reader found the explanation plausible.
Now, researchers arebeginning to require evidence that people besides the authors themselves can under-stand, and reliably make, the judgments underlying the research.
This is a reasonablerequirement, because if researchers cannot even show that people can agree about hejudgments on which their research is based, then there is no chance of replicating theresearch results.
Unfortunately, as a field we have not yet come to agreement abouthow to show reliability of judgments.
For instance, consider the following argumentsfor reliability.
We have chosen these examples both for the clarity of their argumentsand because, taken as a set, they introduce the full range of issues we wish to discuss.. Kowtko, Isard, and Doherty (1992; henceforth KID), in arguing that it ispossible to mark conversational move boundaries, cite separately foreach of three naive coders the ratio of the number of times they agreedwith an "expert" coder about the existence of a boundary over thenumber of times either the naive coder or the expert marked a boundary.They do not describe any restrictions on possible boundary sites.Human Communication Research Centre, 2Buccleuch Place, Edinburgh EH8 9LW, ScotlandComputational Linguistics Volume 22, Number 2...Once conversational move boundaries have been marked on a transcript,KID argue that naive coders can reliably place moves into one of thirteenexclusive categories.
They cite pairwise agreement percentages figuredover all thirteen categories, again looking at each of the three naivecoders separately.
Litman and Hirschberg (1990) use this same pairwisetechnique for assessing the reliability of cue phrase categorization, usingtwo equal-status coders and three categories.Silverman et al (1992), in arguing that sets of coders can agree on arange of category distinctions involved in the TOBI system for labelingEnglish prosody, cite the ratio of observed agreements over possibleagreements, measuring over all possible pairings of the coders.
Forexample, they use this measure for determining the reliability of theexistence and category of pitch accents, phrase accents, and boundarytones.
They measure agreement over both a pool of highly experiencedcoders and a larger pool of mixed-experience oders, and argueinformally that since the level of agreement is not much differentbetween the two, their coding system is easy to learn.Passonneau and Litman (1993), in arguing that naive subjects canreliably agree on whether or not given prosodic phrase boundaries arealso discourse segment boundaries, measure reliability using "percentagreement," defined as the ratio of observed agreements with themajority opinion among seven naive coders to possible agreements withthe majority opinion.Although (1) and KID's use of (2) differ slightly from Litman and Hirschberg's useof (2), (3) and (4) in clearly designating one coder as an "expert," all of these studieshave n coders place some kind of units into m exclusive categories.
Note that the casesof testing for the existence of a boundary can be treated as coding "yes" and "no"categories for each of the possible boundary sites; this treatment is used by measures(3) and (4) but not by measure (1).
All four approaches seem reasonable when takenat face value.
However, the four measures of reliability bear no relationship to eachother.
Worse yet, since none of them take into account he level of agreement one wouldexpect coders to reach by chance, none of them are interpretable even on their own.We first explain what effect chance xpected agreement has on each of these measures,and then argue that we should adopt the kappa statistic (Siegel and Castellan 1988)as a uniform measure of reliability.2.
Chance Expected AgreementMeasure (2) seems a natural choice when there are two coders, and there are severalpossible extensions when there are more coders, including citing separate agreementfigures for each important pairing (as KID do by designating an expert), countinga unit as agreed only if all coders agree on it, or measuring one agreement over allpossible pairs of coders thrown in together.
Taking just the two-coder case, the amountof agreement we would expect coders to reach by chance depends on the number andrelative proportions of the categories used by the coders.
For instance, consider whathappens when the coders randomly place units into categories instead of using anestablished coding scheme.
If there are two categories occurring in equal proportions,on average the coders would agree with each other half of the time: each time thesecond coder makes a choice, there is a fifty/fifty chance of coming up with the same250Carletta Assessing Agreementcategory as the first coder.
If, instead, the two coders were to use four categories inequal proportions, we would expect hem to agree 25% of the time (since no matterwhat the first coder chooses, there is a 25% chance that the second coder will agree.
)And if both coders were to use one of two categories, but use one of the categories95% of the time, we would expect hem to agree 90.5% of the time (.952 4- .052 , or,in words, 95% of the time the first coder chooses the first category, with a .95 chanceof the second coder also choosing that category, and 5% of the time the first coderchooses the second category, with a .05 chance of the second coder also doing so).This makes it impossible to interpret raw agreement figures using measure (2).
Thissame problem affects all of the possible ways of extending measure (2) to more thantwo coders.Now consider measure (3), which has an advantage over measure (2) when thereis a pool of coders, none of whom should be distinguished, in that it produces onefigure that sums reliability over all coder pairs.
Measure (3) still falls foul of the sameproblem with expected chance agreement as measure (2) because it does not take intoaccount he number of categories occurring in the coding scheme.Measure (4) is a different approach to measuring over multiple undifferentiatedcoders.
Note that although Passonneau and Litman are looking at the presence orabsence of discourse segment boundaries, measure (4) takes into account agreementthat a prosodic phrase boundary is not a discourse segment boundary, and thereforetreats the problem as a two-category distinction.
Measure (4) falls foul of the same basicproblem with chance agreement as measures (2) and (3), but in addition, the statisticitself guarantees atleast 50% agreement by only pairing off coders against the majorityopinion.
It also introduces an "expert" coder by the back door in assuming that themajority is always right, although this stance is somewhat at odds with Passonneauand Litman's subsequent assessment of a boundary's trength, from one to seven,based on the number of coders who noticed it.Measure (1) looks at almost exactly the same type of problem as measure (4), thepresence or absence of some kind of boundary.
However, since one coder is explicitlydesignated as an "expert," it does not treat he problem as a two-category distinction,but looks only at cases where either coder marked a boundary as present.
Withoutknowing the density of conversational move boundaries in the corpus, this makes itdifficult to assess how well the coders agreed on the absence of boundaries, or tocompare measures (1) and (4).
In addition, note that since false positives and missednegatives are rolled together in the denominator f the figure, measure (1) does notreally distinguish expert and naive coder roles as much as it might.
Nonetheless, thisstyle of measure does have some advantages over measures (2), (3), and (4), sincethese measures produce artificially high agreement figures when one category of a setpredominates, as is the case with boundary judgments.
One would expect measure(1)'s results to be high under any circumstances, and it is not affected by the densityof boundaries.So far, we have shown that all four of these measures produce figures that areat best, uninterpretable and at worst, misleading.
KID make no comment about themeaning of their figures other than to say that the amount of agreement they showis reasonable; Silverman et al simply point out that where figures are calculated overdifferent numbers of categories, they are not comparable.
On the other hand, Passon-neau and Litman note that their figures are not properly interpretable and attempt toovercome this failing to some extent by showing that the agreement which they haveobtained at least significantly differs from random agreement.
Their method for show-ing this is complex and of no concern to us here, since all it tells us is that it is safeto assume that the coders were not coding randomly--reassuring, butno guarantee251Computational Linguistics Volume 22, Number 2of reliability.
It is more important o ask how different he results are from random andwhether or not the data produced by coding is too noisy to use for the purpose forwhich it was collected.3.
The Kappa StatisticThe concerns of these researchers are largely the same as those in the field of contentanalysis (see especially Krippendorff \[1980\] and Weber \[1985\]), which has been throughthe same problems as we are currently facing and in which strong arguments havebeen made for using the kappa coefficient of agreement (Siegel and Castellan 1988) asa measure of reliability.
1The kappa coefficient (K) measures pairwise agreement among a set of codersmaking category judgments, correcting for expected chance agreement:K - P(A) - P(E)1 - P(E)where P(A) is the proportion of times that the coders agree and P(E) is the proportionof times that we would expect hem to agree by chance, calculated along the lines ofthe intuitive argument presented above.
(For complete instructions on how to calculateK, see Siegel and Castellan \[1988\].)
When there is no agreement other than that whichwould be expected by chance, K is zero.
When there is total agreement, K is one.
It ispossible, and sometimes useful, to test whether or not K is significantly different fromchance, but more importantly, interpretation of the scale of agreement is possible.Krippendorff (1980) discusses what constitutes an acceptable l vel of agreement,while giving the caveat hat it depends entirely on what one intends to do with thecoding.
For instance, he claims that finding associations between two variables thatboth rely on coding schemes with K K .7 is often impossible, and says that contentanalysis researchers generally think of K > .8 as good reliability, with .67 < K < .8allowing tentative conclusions to be drawn.
We would add two further caveats.
First,although kappa addresses many of the problems we have been struggling with asa field, in order to compare K across studies, the underlying assumptions governingthe calculation of chance expected agreement s ill require the units over which codingis performed to be chosen sensibly and comparably.
(To see this, compare, for in-stance, what would happen to the statistic if the same discourse boundary agreementdata were calculated variously over a base of clause boundaries, transcribed wordboundaries, and transcribed phoneme boundaries.)
Where no sensible choice of unitis available pretheoretically, measure (1) may still be preferred.
Secondly, coding dis-course and dialogue phenomena, nd especially coding segment boundaries, may beinherently more difficult than many previous types of content analysis (for instance,1 There are several variants of the kappa coefficient in the literature, including one, Scott's pi, whichactually has been used at least once in our field, to assess agreement on move boundaries inmonologues using action assembly theory (Grosz and Sidner 1986).
Krippendorff's c~ is more generalthan Siegel and Castellan's K in that Krippendorff extends the argument from category data to intervaland ratio scales; this extension might be useful for, for instance, judging the reliability of TOBI breakindex coding, since some researchers treat these codes as inherently scalar (Silverman et al 1992).Krippendorff's c~ and Siegel and Castellan's K differ slightly when used on category judgments in theassumptions under which expected agreement is calculated.
Here we use Siegel and Castellan's Kbecause they explain their statistic more clearly, but the value of c~ is so closely related, especiallyunder the usual expectations for reliability studies, that Krippendorff's statements about c~ hold, andwe conflate the two under the more general name "kappa."
The advantages and disadvantages ofdifferent forms and extensions of kappa have been discussed in many fields but especially in medicine;see, for example, Berry (1992); Goldman (1992); Kraemer (1980); Soeken and Prescott (1986).252Carletta Assessing Agreementdividing newspaper articles based on subject matter).
Whether we have reached (orwill be able to reach) a reasonable vel of agreement in our work as a field remains tobe seen; our point here is merely that if, as a community, we adopt clearer statistics,we will be able to compare results in a standard way across different coding schemesand experiments and to evaluate current developments--and that will illuminate bothour individual results and the way forward.4.
Expert Versus Naive CodersIn assessing the amount of agreement among coders of category distinctions, the kappastatistic normalizes for the amount of expected chance agreement and allows a singlemeasure to be calculated over multiple coders.
This makes it applicable to the stud-ies we have described, and more besides.
However, we have yet to discuss the roleof expert coders in such studies.
KID designate one particular coder as the expert.Passonneau and Litman have only naive coders, but in essence have an expert opin-ion available on each unit classified in terms of the majority opinion.
Silverman etal.
treat all coders indistinguishably, although they do build an interesting argumentabout how agreement levels shift when a number of less-experienced transcribers areadded to a pool of highly experienced ones.
We would argue that in subjective cod-ings such as these, there are no real experts.
We concur with Krippendorff that whatcounts is how totally naive coders manage based on written instructions.
Comparingnaive and expert coding as KID do can be a useful exercise, but rather than assess-ing the naive coders' accuracy, it in fact measures how well the instructions conveywhat these researchers think they do.
(Krippendorff gives well-established techniquesthat generalize on this sort of "odd-man-out" result, which involve isolating particularcoders, categories, and kinds of units to establish the source of any disagreement.)
InPassonneau and Litman, the reason for comparing to the majority opinion is less cleanDespite our argument, here are occasions when one opinion should be treatedas the expert one.
For instance, one can imagine determining whether coders usinga simplified coding scheme match what can be obtained by some better but moreexpensive method, which might itself be either objective or subjective.
In these cases,we would argue that it is still appropriate to use the kappa statistic, in a variationwhich looks only at pairings of agreement with the expert opinion rather than atall possible pairs of coders.
This variation could be achieved by interpreting P(A) asthe proportion of times that the naive coders agree with the expert and P(E) as theproportion of times we would expect he naive coders to agree with the expert bychance.5.
ConclusionsWe have shown that existing measures of reliability in discourse and dialogue workare difficult to interpret, and we have suggested a replacement measure, the kappastatistic, which has a number of advantages over these measures.
Kappa is widelyaccepted in the field of content analysis.
It is interpretable, allows different results tobe compared, and suggests a set of diagnostics in cases where the reliability results arenot good enough for the required purpose.
We suggest that this measure be adoptedmore widely within our own research community.AcknowledgmentsThis work was supported by grant numberG9111013 of the U.K. Joint CouncilsInitiative in Cognitive Science andHuman-Computer Interaction and anInterdisciplinary Research Centre Grant253Computational Linguistics Volume 22, Number 2from the Economic and Social ResearchCouncil (U.K.) to the Universities ofEdinburgh and Glasgow.ReferencesBerry, C. C. 1992.
The kappa statistic.
Journalof the American Medical Association,268(18):2513.Cahn, Janet.
1992.
An investigation i to thecorrelation of cue phrase, unfilled pauses,and the structuring of spoken discourse.In Proceedings ofthe IRCS Workshop onProsody in Natural Speech (IRCS Report92-37), August.Greene, John O. and Joseph N. Cappella.1986.
Cognition and talk: The relationshipof semantic units to temporal patterns offluency in spontaneous speech.
Languageand Speech, 29(2):141-157.Goldman, L. R. 1992.
The kappa statistic--inreply.
Journal of the American MedicalAssociation, 268(18):2513-4.Grosz, Barbara nd Candace Sidner.
1986.Attentions, intentions, and the structureof discourse.
Computational Linguistics,12(3):175-204.Kowtko, Jacqueline C., Stephen D. Isard,and Gwyneth M. Doherty.
1992.Conversational games within dialogue.Technical Report HCRC/RP-31, HumanCommunication Research Centre,University of Edinburgh, June.Kraemer, Helena Chmura.
1980.
Extensionof the kappa coefficent.
Biometrics,36:207-216.Krippendorff, Klaus.
1980.
Content Analysis:An introduction to its Methodology.
SagePublications.Litman, Diane and Julia Hirschberg.
1990.Disambiguating cue phrases in text andspeech.
In Proceedings ofthe ThirteenthInternational Conference on ComputationalLinguistics (COLING-90), volume 2,pages 251-256.Passonneau, Rebecca J. and Diane J. Litman.1993.
Intention-based segmentation:human reliability and correlation withlinguistic ues.
In Proceedings ofthe 31stAnnual Meeting of the ACL, pages 148-155,June.Rodrigues, Irene Pimenta nd Jos@Gabriel P. Lopes.
1992.
Temporal structureof discourse.
In Proceedings oftheFourteenth International Conference onComputational Linguistics (COLING-92),volume 1, pages 331-337.Silverman, Kim, Mary Beckman, JohnPitrelli, Marl Ostendorf, Colin Wightman,Patti Price, Janet Pierrehumbert, and JuliaHirschberg.
1992.
TOBI: A standard forlabeling English prosody.
In InternationalConference on Speech and LanguageProcessing (ICSLP), volume 2, pages867-870.Siegel, Sidney and N. J. Castellan, Jr. 1988.Nonparametric Statistics for the BehavioralSciences.
Second edition.
McGraw-Hill.Soeken, K. and P. Prescott.
1986.
Issues inthe use of kappa to assess reliability.Medical Care, 24:733-743.Weber, Robert Philip.
Basic Content Analysis.Sage Publications.254
