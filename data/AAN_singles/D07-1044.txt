Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
421?429, Prague, June 2007. c?2007 Association for Computational LinguisticsBayesian Document Generative Model with Explicit Multiple TopicsIssei SatoGraduate School of Information Scienceand Technology,The University of Tokyosato@r.dl.itc.u-tokyo.ac.jpHiroshi NakagawaInformation Technology Center,The University of Tokyonakagawa@dl.itc.u-tokyo.ac.jpAbstractIn this paper, we proposed a novel prob-abilistic generative model to deal with ex-plicit multiple-topic documents: ParametricDirichlet Mixture Model(PDMM).
PDMMis an expansion of an existing probabilis-tic generative model: Parametric MixtureModel(PMM) by hierarchical Bayes model.PMM models multiple-topic documents bymixing model parameters of each singletopic with an equal mixture ratio.
PDMMmodels multiple-topic documents by mix-ing model parameters of each single topicwith mixture ratio following Dirichlet dis-tribution.
We evaluate PDMM and PMMby comparing F-measures using MEDLINEcorpus.
The evaluation showed that PDMMis more effective than PMM.1 IntroductionDocuments, such as those seen on Wikipedia andFolksonomy, have tended to be assigned with ex-plicit multiple topics.
In this situation, it is impor-tant to analyze a linguistic relationship between doc-uments and the assigned multiple topics .
We at-tempt to model this relationship with a probabilisticgenerative model.
A probabilistic generative modelfor documents with multiple topics is a probabilitymodel of the process of generating documents withmultiple topics.
By focusing on modeling the gener-ation process of documents and the assigned multi-ple topics, we can extract specific properties of doc-uments and the assigned multiple topics.
The modelcan also be applied to a wide range of applicationssuch as automatic categorization for multiple topics,keyword extraction and measuring document simi-larity, for example.A probabilistic generative model for documentswith multiple topics is categorized into the followingtwo models.
One model assumes a topic as a latenttopic.
We call this model the latent-topic model.
Theother model assumes a topic as an explicit topic.
Wecall this model the explicit-topic model.In a latent-topic model, a latent topic indicatesnot a concrete topic but an underlying implicittopic of documents.
Obviously this model usesan unsupervised learning algorithm.
Representa-tive examples of this kind of model are LatentDirichlet Allocation(LDA)(D.M.Blei et al, 2001;D.M.Blei et al, 2003) and Hierarchical DirichletProcess(HDP)(Y.W.Teh et al, 2003).In an explicit-topic model, an explicit topic indi-cates a concrete topic such as economy or sports, forexample.
A learning algorithm for this model is asupervised learning algorithm.
That is, an explicittopic model learns model parameter using a trainingdata set of tuples such as (documents, topics).
Rep-resentative examples of this model are ParametricMixture Models(PMM1 and PMM2)(Ueda, N. andSaito, K., 2002a; Ueda, N. and Saito, K., 2002b).
Inthe remainder of this paper, PMM indicates PMM1because PMM1 is more effective than PMM2.In this paper, we focus on the explicit topic model.In particular, we propose a novel model that is basedon PMM but fundamentally improved.The remaining part of this paper is organized asfollows.
Sections 2 explains terminology used in the421following sections.
Section 3 explains PMM that ismost directly related to our work.
Section 4 pointsout the problem of PMM and introduces our newmodel.
Section 5 evaluates our new model.
Section6 summarizes our work.2 TerminologyThis section explains terminology used in this paper.K is the number of explicit topics.
V is the numberof words in the vocabulary.
V = {1, 2, ?
?
?
, V } isa set of vocabulary index.
Y = {1, 2, ?
?
?
,K} is aset of topic index.
N is the number of words in adocument.
w = (w1, w2, ?
?
?
, wN ) is a sequence ofN words where wn denotes the nth word in the se-quence.
w is a document itself and is called wordsvector.
x = (x1, x2, ?
?
?
, xV ) is a word-frequencyvector, that is, BOW(Bag Of Words) representationwhere xv denotes the frequency of word v. wvntakes a value of 1(0) when wn is v ?
V (is notv ?
V ).
y = (y1, y2, ?
?
?
, yK) is a topic vectorinto which a document w is categorized, where yitakes a value of 1(0) when the ith topic is (not) as-signed with a document w. Iy ?
Y is a set of topicindex i, where yi takes a value of 1 in y.?i?Iyand ?i?Iy denote the sum and product for all i inIy, respectively.
?
(x) is the Gamma function and?
is the Psi function(Minka, 2002).
A probabilisticgenerative model for documents with multiple top-ics models a probability of generating a documentwin multiple topics y using model parameter ?, i.e.,models P (w|y,?).
A multiple categorization prob-lem is to estimate multiple topics y?
of a documentw?
whose topics are unknown.
The model parame-ters are learned by documents D = {(wd, yd)}Md=1,where M is the number of documents.3 Parametric Mixture ModelIn this section, we briefly explain Parametric Mix-ture Model(PMM)(Ueda, N. and Saito, K., 2002a;Ueda, N. and Saito, K., 2002b).3.1 OverviewPMM models multiple-topic documents by mixingmodel parameters of each single topic with an equalmixture ratio, where the model parameter ?iv is theprobability that word v is generated from topic i.This is because it is impractical to use model param-eter corresponding to multiple topics whose num-ber is 2K ?
1(all combination of K topics).
PMMachieved more useful results than machine learn-ing methods such as Naive Bayes, SVM, K-NN andNeural Networks (Ueda, N. and Saito, K., 2002a;Ueda, N. and Saito, K., 2002b).3.2 FormulationPMM employs a BOW representation and is formu-lated as follows.P (w|y, ?)
= ?Vv=1(?
(v,y, ?
))xv (1)?
is a K ?
V matrix whose element is ?iv =P (v|yi = 1).
?
(v,y, ?)
is the probability that wordv is generated from multiple topics y and is de-fined as the linear sum of hi(y) and ?iv as follows:?
(v,y, ?)
=?Ki=1 hi(y)?ivhi(y) is a mixture ratio corresponding to topic iand is formulated as follows:hi(y) =yi?Kj=1 yj,?Ki=1 hi(y) = 1.
(if yi = 0, then hi(y) = 0)3.3 Learning Algorithm of Model ParameterThe learning algorithm of model parameter ?
inPMM is an iteration method similar to the EM al-gorithm.
Model parameter ?
is estimated by max-imizing ?Md=1P (wd|yd, ?)
in training documentsD = {(wd, yd)}Md=1.
Function g corresponding toa document d is introduced as follows:gdiv(?)
=h(yd)?iv?Kj=1 hj(yd)?jv(2)The parameters are updated along with the followingformula.?
(t+1)iv =1C(M?dxdvgdiv(?
(t)) + ?
?
1) (3)xdv is the frequency of word v in document d. Cis the normalization term for?Vv=1 ?iv = 1. ?
isa smoothing parameter that is Laplace smoothingwhen ?
is set to two.
In this paper, ?
is set to twoas the original paper.4 Proposed ModelIn this section, firstly, we mention the problem re-lated to PMM.
Then, we explain our solution of theproblem by proposing a new model.4224.1 OverviewPMM estimates model parameter ?
assuming thatall of mixture ratios of single topic are equal.
It isour intuition that each document can sometimes bemore weighted to some topics than to the rest of theassigned topics.
If the topic weightings are averagedover all biases in the whole document set, they couldbe canceled.
Therefore, model parameter ?
learnedby PMM can be reasonable over the whole of docu-ments.However, if we compute the probability of gener-ating an individual document, a document-specifictopic weight bias on mixture ratio is to be consid-ered.
The proposed model takes into account thisdocument-specific bias by assuming that mixture ra-tio vector pi follows Dirichlet distribution.
This isbecause we assume the sum of the element in vec-tor pi is one and each element pii is nonnegative.Namely, the proposed model assumes model param-eter of multiple topics as a mixture of model pa-rameter on each single topic with mixture ratio fol-lowing Dirichlet distribution.
Concretely, given adocument w and multiple topics y , it estimatesa posterior probability distribution P (pi|x, y) byBayesian inference.
For convenience, the proposedmodel is called PDMM(Parametric Dirichlet Mix-ture Model).In Figure 1, the mixture ratio(bias) pi =(pi1, pi2, pi3),?3i=1 pii = 1, pii > 0 of three topics isexpressed in 3-dimensional real spaceR3.
The mix-ture ratio(bias) pi constructs 2D-simplex inR3.
Onepoint on the simplex indicates one mixture ratio pi ofthe three topics.
That is, the point indicates multipletopics with the mixture ratio.
PMM generates doc-uments assuming that each mixture ratio is equal.That is, PMM generates only documents with mul-tiple topics that indicates the center point of the 2D-simplex in Figure 1.
On the contrary, PDMM gen-erates documents assuming that mixture ratio pi fol-lows Dirichlet distribution.
That is, PDMM can gen-erate documents with multiple topics whose weightscan be generated by Dirichlet distribution.4.2 FormulationPDMM is formulated as follows:P (w|y, ?, ?
)=?P (pi|?, y)?Vv=1(?
(v,y, ?, pi))xvdpi (4)Figure 1: Topic Simplex for Three Topicspi is a vector whose element is pii(i ?
Iy).
pii is amixture ratio(bias) of model parameter correspond-ing to single topic i where pii > 0,?i?Iy pii = 1.pii can be considered as a probability of topic i , i.e.,pii = P (yi = 1|pi).
P (pi|?, y) is a prior distri-bution of pi whose index i is an element of Iy, i.e.,i ?
Iy.
We use Dirichlet distribution as the prior.
?is a parameter vector of Dirichlet distribution corre-sponding to pii(i ?
Iy).
Namely, the formulation isas follows.P (pi|?, y) =?
(?i?Iy ?i)?i?Iy?
(?i)?i?Iypi?i?1i (5)?
(v,y, ?, pi) is the probability that word v is gener-ated frommultiple topics y and is denoted as a linearsum of pii(i ?
Iy) and ?iv(i ?
Iy) as follows.?
(v,y, ?, pi) =?i?Iypii?iv (6)=?i?IyP (yi = 1|pi)P (v|yi = 1, ?)
(7)4.3 Variational Bayes Method for EstimatingMixture RatioThis section explains a method to estimate theposterior probability distribution P (pi|w, y, ?, ?
)of a document-specific mixture ratio.
Basically,P (pi|w, y, ?, ?)
is obtained by Bayes theorem us-ing Eq.(4).
However, that is computationally im-practical because a complicated integral computa-tion is needed.
Therefore we estimate an approx-imate distribution of P (pi|w, y, ?, ?)
using Varia-tional Bayes Method(H.Attias, 1999).
The concreteexplanation is as follows423Use Eqs.
(4)(7).P (w, pi|y, ?, ?)
=P (pi|?, y)?Vv=1(?i?IyP (yi = 1|pi)P (v|yi = 1, ?
))xvTransform document expression of above equa-tion into words vector w = (w1, w2, ?
?
?
, wN ).P (w, pi|y, ?, ?)
=P (pi|?, y)?Nn=1?in?IyP (yin = 1|pi)P (wn|yin = 1, ?
)By changing the order of?and ?, we haveP (w, pi|y, ?, ?)
=P (pi|?, y)?i?INy?Nn=1P (yin = 1|pi)P (wn|yin = 1, ?)(?i?INy??i1?Iy?i2?Iy?
?
?
?iN?Iy)Express yin = 1 as zn = i.P (w|y, ?, ?)
=?
?z?INyP (pi|?, y)?Nn=1P (zn|pi)P (wn|zn, ?)dpi(?z?INy??z1?Iy?z2?Iy?
?
?
?zN?Iy) (8)Eq.
(8) is regarded as Eq.
(4) rewritten by introducinga new latent variable z = (z1, z2, ?
?
?
, zN ).P (w|y, ?, ?)
=?
?z?INyP (pi, z, w|y, ?, ?
)dpi (9)Use Eqs.
(8)(9)P (pi, z, w|y, ?, ?
)= P (pi|?, y)?Nn=1P (zn|pi)P (wn|zn, ?)
(10)Hereafter, we explain Variational Bayes Methodfor estimating an approximate distribution ofP (pi, z|w, y, ?, ?)
using Eq.(10).
This approach isthe same as LDA(D.M.Blei et al, 2001; D.M.Blei etal., 2003).
The approximate distribution is assumedto be Q(pi, z|?, ?).
The following assumptions areintroduced.Q(pi, z|?, ?)
= Q(pi|?)Q(z|?)
(11)Q(pi|?)
=?
(?i?Iy ?i)?i?Iy?
(?i)?i?Iypi?i?1i (12)Q(z|?)
= ?Nn=1Q(zn|?)
(13)Q(zn|?)
= ?Ki=1(?ni)zin (14)Q(pi|?)
is Dirichlet distribution where ?
is its pa-rameter.
Q(zn|?)
is Multinomial distribution where?ni is its parameter and indicates the probabilitythat the nth word of a document is topic i, i.e.P (yin = 1).
zin is a value of 1(0) when zn is (not)i.
According to Eq.
(11), Q(pi|?)
is regarded as anapproximate distribution of P (pi|w, y, ?, ?
)The log likelihood of P (w|y, ?, ?)
is derived asfollows.logP (w|y, ?, ?)=?
?z?INyQ(pi, z|?, ?
)dpi logP (w|y, ?, ?)=?
?z?INyQ(pi, z|?, ?)
logP (pi, z, w|y, ?, ?
)Q(pi, z|?, ?)dpi+?
?z?INyQ(pi, z|?, ?)
logQ(pi, z|?, ?
)P (pi, z|w, y, ?, ?
)dpilogP (w|y, ?, ?)
= F [Q] + KL(Q,P ) (15)F [Q] =?
?z?INyQ(pi,z|?,?)
log P (pi,z,w|y,?,?)Q(pi,z|?,?)
dpiKL(Q,P ) =?
?z?INyQ(pi,z|?,?)
log Q(pi,z|?,?
)P (pi,z|w,y,?,?
)dpiKL(Q,P ) is the Kullback-Leibler Divergencethat is often employed as a distance betweenprobability distributions.
Namely, KL(Q,P )indicates a distance between Q(pi, z|?, ?)
andP (pi, z|w, y, ?, ?).
logP (w|y, ?, ?)
is notrelevant to Q(pi, z|?, ?).
Therefore, Q(pi, z|?, ?
)that maximizes F [Q] minimizes KL(Q,P ),and gives a good approximate distribution ofP (pi, z|w, y, ?, ?
).We estimate Q(pi, z|?, ?
), concretely its param-eter ?
and ?, by maximizingF [Q] as follows.Using Eqs.
(10)(11).F [Q] =?Q(pi|?)
logP (pi|?, y)d?
(16)+?
?z?INyQ(pi|?)Q(z|?)
log?Nn=1P (zn|pi)d?
(17)+?z?INyQ(z|?)
log?Nn=1P (wn|zn, ?)
(18)??Q(pi|?)
logQ(pi|?)d?
(19)??z?INyQ(z|?)
logQ(z|?)
(20)424= log ?
(?i?Iy ?j)?
?i?Iy log ?
(?i)+?i?Iy(?i ?
1)(?(?i)??
(?j?Iy ?j))(21)+N?n=1?i?Iy?ni(?(?i)??
(?j?Iy?j)) (22)+N?n=1?i?IyV?j=1?niwjn log ?ij (23)?
log ?
(?j?Iy?j) +?i?Iylog ?(?j?Iy?j)?
?i?Iy(?i ?
1)(?(?i)??
(?j?Iy?j)) (24)?N?n=1?i?Iy?ni log ?ni (25)F [Q] is known to be a function of ?i and ?ni fromEqs.
(21) through (25).
Then we only need to re-solve the maximization problem of nonlinear func-tion F [Q] with respect to ?i and ?ni.
In this case,the maximization problem can be resolved by La-grange multiplier method.First, regard F [Q] as a function of ?i, whichis denoted as F [?i].
Then ,?i does not have con-straints.
Therefore we only need to find the follow-ing ?i, where?F [?i]?
?i= 0.
The resultant ?i is ex-pressed as follows.
?i = ?i +N?n=1?ni (i ?
Iy) (26)Second, regard F [Q] as a function of ?ni, which isdenoted asF [?ni].
Then, considering the constraintthat?i?Iy ?ni = 1, Lagrange function L[?ni] is ex-pressed as follows:L[?ni] = F [?ni] + ?
(?i?Iy?ni ?
1) (27)?
is a so-called Lagrange multiplier.We find the following ?ni where?L[?ni]?
?ni = 0.?ni =?iwnCexp(?(?i)??
(?j?Iy?j)) (i ?
Iy)) (28)C is a normalization term.
By Eqs.
(26)(28), we ob-tain the following updating formulas of ?i and ?ni.?
(t+1)i = ?i +N?n=1?
(t)ni (i ?
Iy) (29)?
(t+1)ni =?iwnCexp(?(?
(t+1)i )??(?j?Iy?
(t+1)j )) (30)Using the above updating formulas , we can es-timate parameters ?
and ?, which are specific to adocument w and topics y.
Last of all , we show apseudo code :vb(w, y) which estimates ?
and ?.
Inaddition , we regard ?
, which is a parameter of aprior distribution of pi, as a vector whose elementsare all one.
That is because Dirichlet distributionwhere each parameter is one becomes Uniform dis-tribution.?
Variational Bayes Method for PDMM???
?function vb(w, y):1.
Initialize ?i?
1 ?i ?
Iy2.
Compute ?
(t+1), ?
(t+1) using Eq.(29)(30)3.
if ?
?
(t+1) ?
?
(t) ?< & ?
?
(t+1) ?
?
(t) ?< 4.
then return (?
(t+1), ?
(t+1)) and halt5.
else t?
t + 1 and goto step (2)???????????????????
?4.4 Computing Probability of GeneratingDocumentPMM computes a probability of generating a docu-ment w on topics y and a set of model parameter ?as follows:P (w|y,?)
= ?Vv=1(?
(v,y, ?
))xv (31)?
(v,y, ?)
is the probability of generating a wordv on topics y that is a mixture of model parame-ter ?iv(i ?
Iy) with an equal mixture ratio.
On theother hand, PDMM computes the probability of gen-erating a word v on topics y using ?iv(i ?
Iy) andan approximate posterior distribution Q(pi|?)
as fol-lows:425?
(v,y, ?, ?)=?(?i?Iypii?iv)Q(pi|?
)dpi (32)=?i?Iy?piiQ(pi|?
)dpi?iv (33)=?i?Iyp?ii?iv (34)p?ii =?piiQ(pi|?
)dpi =?i?j?Iy?j(C.M.Bishop,2006)The above equation regards the mixture ratio oftopics y of a document w as the expectation p?ii(i ?Iy) of Q(pi|?).
Therefore, a probability of gener-ating w P (w|y,?)
is computed with ?
(v,y, ?, ?
)estimated in the following manner:P (w|y,?)
= ?Vv=1(?
(v,y, ?, ?
)))xv (35)4.5 Algorithm for Estimating Multiple Topicsof DocumentPDMM estimates multiple topics y?
maximizinga probability of generating a document w?, i.e.,Eq.(35).
This is the 0-1 integer problem(i.e., NP-hard problem), so PDMM uses the same approxi-mate estimation algorithm as PMM does.
But it isdifferent from PMM?s estimation algorithm in thatit estimates the mixture ratios of topics y by Varia-tional Bayes Method as shown by vb(w,y) at step 6in the following pseudo code of the estimation algo-rithm:?
Topics Estimation Algorithm???????
?function prediction(w):1.
Initialize S ?
{1, 2, ?
?
?
}, yi ?
0 fori(1, 2 ?
?
?
,K)2. vmax ?
??3.
while S is not empty do4.
foreach i ?
S do5.
yi ?
1, yj?S\i ?
06.
Compute ?
by vb(w, y)7. v(i)?
P (w|y)8. end foreach9.
i?
?
argmax v(i)10. if v(i?)
> vmax11.
yi?
?
1, S ?
S\i?, vmax ?
v(i?)12.
else13.
return y and halt???????????????????
?5 EvaluationWe evaluate the proposed model by using F-measureof multiple topics categorization problem.5.1 DatasetWe use MEDLINE1 as a dataset.
In this experiment,we use five thousand abstracts written in English.MEDLINE has a metadata set called MeSH Term.For example, each abstract has MeSH Terms such asRNAMessenger and DNA-Binding Proteins.
MeSHTerms are regarded as multiple topics of an abstract.In this regard, however, we use MeSH Terms whosefrequency are medium(100-999).
We did that be-cause the result of experiment can be overly affectedby such high frequency terms that appear in almostevery abstract and such low frequency terms that ap-pear in very few abstracts.
In consequence, the num-ber of topics is 88.
The size of vocabulary is 46,075.The proportion of documents with multiple topics onthe whole dataset is 69.8%, i.e., that of documentswith single topic is 30.2%.
The average of the num-ber of topics of a document is 3.4.
Using TreeTag-ger2, we lemmatize every word.
We eliminate stopwords such as articles and be-verbs.5.2 Result of ExperimentWe compare F-measure of PDMM with that ofPMM and other models.F-measure(F) is as follows:F = 2PRP+R , P =|Nr?Ne||Ne|, R = |Nr?Ne||Nr| .Nr is a set of relevant topics .
Ne is a set of esti-mated topics.
A higher F-measure indicates a betterability to discriminate topics.
In our experiment, wecompute F-measure in each document and averagethe F-measures throughout the whole document set.We consider some models that are distinct inlearning model parameter ?.
PDMM learns modelparameter ?
by the same learning algorithm asPMM.
NBM learns model parameter ?
by NaiveBayes learning algorithm.
The parameters are up-dated according to the following formula: ?iv =Miv+1C .
Miv is the number of training documentswhere a word v appears in topic i.
C is normaliza-tion term for?Vv=1 ?iv = 1.1http://www.nlm.nih.gov/pubs/factsheets/medline.html2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/426The comparison of these models with respect toF-measure is shown in Figure 2.
The horizontal axisis the proportion of test data of dataset(5,000 ab-stracts).
For example, 2% indicates that the numberof documents for learning model is 4,900 and thenumber of documents for the test is 100.
The verticalaxis is F-measure.
In each proportion, F-measure isan average value computed from five pairs of train-ing documents and test documents randomly gener-ated from dataset.F-measure of PDMM is higher than that of othermethods on any proportion, as shown in Figure2.
Therefore, PDMM is more effective than othermethods on multiple topics categorization.Figure 3 shows the comparison of models withrespect to F-measure, changing proportion of mul-tiple topic document for the whole dataset.
The pro-portion of document for learning and test are 40%and 60%, respectively.
The horizontal axis is theproportion of multiple topic document on the wholedataset.
For example, 30% indicates that the pro-portion of multiple topic document is 30% on thewhole dataset and the remaining documents are sin-gle topic , that is, this dataset is almost single topicdocument.
In 30%.
there is little difference of F-measure among models.
As the proportion of mul-tiple topic and single topic document approaches90%, that is, multiple topic document, the differ-ences of F-measure among models become appar-ent.
This result shows that PDMM is effective inmodeling multiple topic document.Figure 2: F-measure Results5.3 DiscussionIn the results of experiment described in section5.2, PDMM is more effective than other models inFigure 3: F-measure Results changing Proportion ofMultiple Topic Document for Datasetmultiple-topic categorization.
If the topic weight-ings are averaged over all biases in the whole oftraining documents, they could be canceled.
Thiscancellation can lead to the result that model pa-rameter ?
learned by PMM is reasonable over thewhole of documents.
Moreover, PDMM computesthe probability of generating a document using amixture of model parameter, estimating the mixtureratio of topics.
This estimation of the mixture ra-tios, we think, is the key factor to achieve the re-sults better than other models.
In addition, the es-timation of a mixture ratio of topics can be effec-tive from the perspective of extracting features ofa document with multiple topics.
A mixture ratioof topics assigned to a document is specific to thedocument.
Therefore, the estimation of the mixtureratio of topics is regarded as a projection from aword-frequency space of QV where Q is a set ofinteger number to a mixture ratio space of topics[0, 1]K in a document.
Since the size of vocabu-lary is much more than that of topics, the estima-tion of the mixture ratio of topics is regarded as adimension reduction and an extraction of features ina document.
This can lead to analysis of similarityamong documents with multiple topics.
For exam-ple, the estimated mixture ratio of topics [Compara-tive Study]C[Apoptosis] and [Models,Biological] inone MEDLINE abstract is 0.656C0.176 and 0.168,respectively.
This ratio can be a feature of this doc-ument.Moreover, we can obtain another interesting re-sults as follows.
The estimation of mixture ratios oftopics uses parameter ?
in section 4.3.
We obtaininteresting results from another parameter ?
thatneeds to estimate ?.
?ni is specific to a document.
A427Table 1: Word List of Document X whose Topics are[Female], [Male] and [Biological Markers]Ranking Top10 Ranking Bottom101(37) biomarkers 67(69) indicate2(19) Fusarium 68(57) problem3(20) non-Gaussian 69(45) use4(21) Stachybotrys 70(75) %5(7) chrysogenum 71(59) correlate6(22) Cladosporium 72(17) population7(3) mould 73(15) healthy8(35) Aspergillus 7433) response9(23) dampness 75(56) man10(24) 1SD 76(64) woman?ni indicates the probability that a word wn belongsto topic i in a document.
Therefore we can computethe entropy on wn as follows:entropy(wn) =?Ki=1 ?ni log(?ni)We rank words in a document by this entropy.
Forexample, a list of words in ascending order of theentropy in document X is shown in Table 1.
A valuein parentheses is a ranking of words in decending or-der of TF-IDF(= tf ?
log(M/df),where tf is termfrequency in a test document, df is document fre-quency andM is the number of documents in the setof doucuments for learning model parameters) (Y.Yang and J. Pederson, 1997) .
The actually assignedtopics are [Female] , [Male] and [Biological Mark-ers], where each estimated mixture ratio is 0.499 ,0.460 and 0.041, respectively.The top 10 words seem to be more technical thanthe bottom 10 words in Table 1.
When the entropy ofa word is lower, the word is more topic-specific ori-ented, i.e., more technical .
In addition, this rankingof words depends on topics assigned to a document.When we assign randomly chosen topics to the samedocument, generic terms might be ranked higher.For example, when we rondomly assign the topics[Rats], [Child] and [Incidence], generic terms suchas ?use?
and ?relate?
are ranked higher as shownin Table 2.
The estimated mixture ratio of [Rats],[Child] and [Incidence] is 0.411, 0.352 and 0.237,respectively.For another example, a list of words in ascendingorder of the entropy in document Y is shown in Ta-ble 3.
The actually assigned topics are Female, An-imals, Pregnancy and Glucose..
The estimated mix-ture ratio of [Female], [Animals] ,[Pregnancy] andTable 2: Word List of Document X whose Topics are[Rats], [Child] and [Incidence]Ranking Top10 Ranking Bottom101(69) indicate 67(56) man2(63) relate 68(47) blot3(53) antigen 69(6) exposure4(45) use 70(54) distribution5(3) mould 71(68) evaluate6(4) versicolor 72(67) examine7(35) Aspergillus 73(59) correlate8(7) chrysogenum 74(58) positive9(8) chartarum 75(1) IgG10(9) herbarum 76(60) adult[Glucose] is 0.442, 0.437, 0.066 and 0.055, respec-tively In this case, we consider assigning sub topicsof actual topics to the same document Y.Table 4 shows a list of words in document Y as-signed with the sub topics [Female] and [Animals].The estimated mixture ratio of [Female] and [An-imals] is 0.495 and 0.505, respectively.
Estimatedmixture ratio of topics is chaged.
It is interestingthat [Female] has higher mixture ratio than [Ani-mals] in actual topics but [Female] has lower mix-ture ratio than [Animals] in sub topics [Female] and[Animals].
According to these different mixture ra-tios, the ranking of words in docment Y is changed.Table 5 shows a list of words in document Y as-signed with the sub topics [Pregnancy] and [Glu-cose].
The estimated mixture ratio of [Pregnancy]and [Glucose] is 0.502 and 0.498, respectively.
Itis interesting that in actual topics, the ranking ofgglucose-insulinh and ?IVGTT?
is high in documentY but in the two subset of actual topics, gglucose-insulinh and ?IVGTT?
cannot be find in Top 10words.The important observation known from these ex-amples is that this ranking method of words in a doc-ument can be assosiated with topics assigned to thedocument.
?
depends on ?
seeing Eq.(28).
This isbecause the ranking of words depends on assignedtopics, concretely, mixture ratios of assigned topics.TF-IDF computed from the whole documents can-not have this property.
Combined with existing theextraction method of keywords, our model has thepotential to extract document-specific keywords us-ing information of assigned topics.428Table 3: Word List of Document Y whose Ac-tual Topics are [Femaile],[Animals],[Pregnancy]and [Glucose]Ranking Top 10 Ranking Bottom 101(2) glucose-insulin 94(93) assess2(17) IVGTT 95(94) indicate3(11) undernutrition 96(74) CT4(12) NR 97(28) %5(13) NRL 98(27) muscle6(14) GLUT4 99(85) receive7(56) pregnant 100(80) status8(20) offspring 101(100) protein9(31) pasture 102(41) age10(32) singleton 103(103) conclusionTable 4: Word List of Document Y whose Topics are[Femaile]and [Animals]Ranking Top 10 Ranking Bottom 101(31) pasture 94(65) insulin2(32) singleton 95(76) reduced3(33) insulin-signaling 96(27) muscle4(34) CS 97(74) CT5(35) euthanasia 98(68) feed6(36) humane 99(100) protein7(37) NRE 100(80) status8(38) 110-term 101(85) receive9(50) insert 102(41) age10(11) undernutrition 103(103) conclusion6 Concluding RemarksWe proposed and evaluated a novel probabilisticgenerative models, PDMM, to deal with multiple-topic documents.
We evaluated PDMM and othermodels by comparing F-measure using MEDLINEcorpus.
The results showed that PDMM is more ef-fective than PMM.
Moreover, we indicate the poten-tial of the proposed model that extracts document-specific keywords using information of assignedtopics.Acknowledgement This research was funded inpart by MEXT Grant-in-Aid for Scientific Researchon Priority Areas ?i-explosion?
in Japan.ReferencesH.Attias 1999.
Learning parameters and structure of la-tent variable models by variational Bayes.
in Proc ofUncertainty in Artificial Intelligence.C.M.Bishop 2006.
Pattern Recognition And MachineTable 5: Word List of Document Y whose Topics are[Pregnancy]and [Glucose]Ranking Top 10 Ranking Bottom 101(84) mass 94(18) IVGTT2(74) CT 95(72) metabolism3(26) requirement 96(73) metabolic4(45) intermediary 97(57) pregnant5(50) insert 98(58) prenatal6(53) feeding 99(59) fetal7(55) nutrition 100(3) gestation8(61) nutrient 101(20) offspring9(31) pasture 102(65) insulin10(32) singleton 103(16) glucoseLearning (Information Science and Statistics), p.687.Springer-Verlag.D.M.
Blei, Andrew Y. Ng, and M.I.
Jordan.
2001.
LatentDirichlet Allocation.
Neural Information ProcessingSystems 14.D.M.
Blei, Andrew Y. Ng, and M.I.
Jordan.
2003.
La-tent Dirichlet Allocation.
Journal of Machine Learn-ing Research, vol.3, pp.993-1022.Minka 2002.
Estimating a Dirichlet distribution.
Techni-cal Report.Y.W.Teh, M.I.Jordan, M.J.Beal, and D.M.Blei.
2003.Hierarchical dirichlet processes.
Technical Report653, Department Of Statistics, UC Berkeley.Ueda, N. and Saito, K. 2002.
Parametric mixture modelsfor multi-topic text.
Neural Information ProcessingSystems 15.Ueda, N. and Saito, K. 2002.
Singleshot detection ofmulti-category text using parametric mixture models.ACM SIG Knowledge Discovery and Data Mining.Y.
Yang and J. Pederson 1997.
A comparative study onfeature selection in text categorization.
Proc.
Interna-tional Conference on Machine Learning.429
