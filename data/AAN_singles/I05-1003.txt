R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
22 ?
33, 2005.?
Springer-Verlag Berlin Heidelberg 2005The Use of Monolingual Context Vectors for MissingTranslations in Cross-Language Information RetrievalYan Qu1, Gregory Grefenstette2, and David A. Evans11Clairvoyance Corporation, 5001 Baum Boulevard, Suite 700,Pittsburgh, PA, 15213, USA{yqu, dae}@clairvoyancecorp.com2LIC2M/SCRI/LIST/DTSI/CEA, B.P.6,92265 Fontenay-aux-Roses Cedex, France{Gregory.Grefenstette}@cea.frAbstract.
For cross-language text retrieval systems that rely on bilingual dic-tionaries for bridging the language gap between the source query language andthe target document language, good bilingual dictionary coverage is imperative.For terms with missing translations, most systems employ some approaches forexpanding the existing translation dictionaries.
In this paper, instead of lexiconexpansion, we explore whether using the context of the unknown terms can helpmitigate the loss of meaning due to missing translation.
Our approaches consistof two steps: (1) to identify terms that are closely associated with the unknownsource language terms as context vectors and (2) to use the translations of theassociated terms in the context vectors as the surrogate translations of the un-known terms.
We describe a query-independent version and a query-dependentversion using such monolingual context vectors.
These methods are evaluatedin Japanese-to-English retrieval using the NTCIR-3 topics and data sets.
Em-pirical results show that both methods improved CLIR performance for shortand medium-length queries and that the query-dependent context vectors per-formed better than the query-independent versions.1   IntroductionFor cross-language text retrieval systems that rely on bilingual dictionaries for bridg-ing the language gap between the source query language and the target documentlanguage, good bilingual dictionary coverage is imperative [8,9].
Yet, translations forproper names and special terminology are often missing in available dictionaries.Various methods have been proposed for finding translations of names and terminol-ogy through transliteration [5,11,13,14,16,18,20] and corpus mining [6,7,12,15,22].In this paper, instead of attempting to find the candidate translations of terms withouttranslations to expand existing translation dictionaries, we explore to what extentsimply using text context can help mitigate the missing translation problem and forwhat kinds of queries.
The context-oriented approaches include (1) identifying wordsthat are closely associated with the unknown source language terms as context vectorsand (2) using the translations of the associated words in the context vectors as thesurrogate translations of the unknown words.
We describe a query-independentThe Use of Monolingual Context Vectors for Missing Translations 23version and a query-dependent version using such context vectors.
We evaluate thesemethods in Japanese-to-English retrieval using the NTCIR-3 topics and data sets.
Inparticular, we explore the following questions:?
Can translations obtained from context vectors help CLIR performance??
Are query-dependent context vectors more effective than query-independentcontext vectors for CLIR?In the balance of this paper, we first describe related work in Section 2.
The methodsof obtaining translations through context vectors are presented in Section 3.
The CLIRevaluation system and evaluation results are presented in Section 4 and Section 5, re-spectively.
We summarize the paper in Section 6.2   Related WorkIn dictionary-based CLIR applications, approaches for dealing with terms with missingtranslations can be classified into three major categories.
The first is a do-nothing ap-proach by simply ignoring the terms with missing translations.
The second categoryincludes attempts to generate candidate translations for a subset of unknown terms, suchas names and technical terminology, through phonetic translation between differentlanguages (i.e., transliteration) [5,11,13,14,16,18,20].
Such methods generally yieldtranslation pairs with reasonably good accuracy reaching about 70% [18].
Empiricalresults have shown that the expanded lexicons can significantly improve CLIR systemperformance [5,16,20].
The third category includes approaches for expanding existingbilingual dictionaries by exploring multilingual or bilingual corpora.
For example, the?mix-lingual?
feature of the Web has been exploited for locating translation pairs bysearching for the presence of both Chinese and English text in a text window [22].
Inwork focused on constructing bilingual dictionaries for machine translation, automatictranslation lexicons are compiled using either clean aligned parallel corpora [12,15] ornon-parallel comparable corpora [6,7].
In work with non-parallel corpora, contexts ofsource language terms and target language terms and a seed translation lexicon arecombined to measure the association between the source language terms and potentialtranslation candidates in the target language.
The techniques with non-parallel corporasave the expense of constructing large-scale parallel corpora with the tradeoff of loweraccuracy, e.g., about 30% accuracy for the top-one candidate [6,7].
To our knowledge,the usefulness of such lexicons in CLIR systems has not been evaluated.While missing translations have been addressed in dictionary-based CLIR systems,most of the approaches mentioned above attempt to resolve the problem through dic-tionary expansion.
In this paper, we explore non-lexical approaches and their effective-ness on mitigating the problem of missing translations.
Without additional lexiconexpansion, and keeping the unknown terms in the source language query, we extractcontext vectors for these unknown terms and obtain their translations as the surrogatetranslations for the original query terms.
This is motivated by the pre-translation feed-back techniques proposed by several previous studies [1,2].
Pre-translation feedbackhas been shown to be effective for resolving translation ambiguity, but its effect onrecovering the lost meaning due to missing translations has not been empirically evalu-ated.
Our work provides the first empirical results for such an evaluation.24 Y. Qu, G. Grefenstette, and D.A.
Evans3   Translation via Context Vectors3.1   Query-Independent Context VectorsFor a source language term t, we define the context vector of term t as:tC = ??
ittttt ,...,,,, 4321where terms 1t  to it  are source language terms that are associated with term t withina certain text window in some source language corpus.
In this report, the associatedterms are terms that co-occur with term t above a pre-determined cutoff threshold.Target language translations of term t are derived from the translation of the knownsource language terms in the above context vectors:trans(t) = <trans(t1), trans(t2), ?, trans(tn)>Selection of the source language context terms for the unknown term above is onlybased on the association statistics in an independent source language corpus.
It doesnot consider other terms in the query as context; thus, it is query independent.
Usingthe Japanese-to-English pair as an example, the steps are as follows:1.
For a Japanese term t that is unknown to the bilingual dictionary, extractconcordances of term t within a window of P bytes (we used P=200 bytesor 100 Japanese characters) in a Japanese reference corpus.2.
Segment the extracted Japanese concordances into terms, removing stop-words.3.
Select the top N (e.g., N=5) most frequent terms from the concordances toform the context vector for the unknown term t.4.
Translate these selected concordance terms in the context vector into Eng-lish to form the pseudo-translations of the unknown term t.Note that, in the translation step (Step 4) of the above procedure, the source lan-guage association statistics for selecting the top context terms and frequencies of theirtranslations are not used for ranking or filtering any translations.
Rather, we rely onthe Cross Language Information Retrieval system?s disambiguation function to selectthe best translations in context of the target language documents [19].3.2   Query-Dependent Context VectorsWhen query context is considered for constructing context vectors and pseudo-translations, the concordances containing the unknown terms are re-ranked based onthe similarity scores between the window concordances and the vector of the knownterms in the query.
Each window around the unknown term is treated as a document,and the known query terms are used.
This is based on the assumption that the topranked concordances are likely to be more similar to the query; subsequently, thecontext terms in the context vectors provide better context for the unknown term.Again, using the Japanese-English pair as an example, the steps are as follows:The Use of Monolingual Context Vectors for Missing Translations 251.
For a Japanese term t unknown to the bilingual dictionary, extract a window oftext of P bytes (we used P=200 bytes or 100 Japanese characters) aroundevery occurrence of term t in a Japanese reference corpus.2.
Segment the Japanese text in each window into terms and remove stopwords.3.
Re-rank the window based on similarity scores between the terms found in thewindow and the vector of the known query terms.4.
Obtain the top N (e.g., N=5) most frequently occurring terms from the top M(e.g., M=100) ranking windows to form the Japanese context vector for theunknown term t.5.
Translate each term in the Japanese context vector into English to form thepseudo-translations of the unknown term t.The similarity scores are based on Dot Product.The main difference between the two versions of context vectors is whether theother known terms in the query are used for ranking the window concordances.Presumably, the other query terms provide a context-sensitive interpretation of theunknown terms.
When M is extremely large, however, the query-dependent versionshould approach the performance of the query-independent version.We illustrate both versions of the context vectors with topic 23(?????????????
?President Kim Dae-Jung's policy toward Asia?
)from NTCIR-3:First, the topic is segmented into terms, with the stop words removed:???
; ???
; ???
; ?
?Then, the terms are categorized as ?known?
vs. ?unknown?
based on the bilingualdictionary:Unknown:Query23: ???Known:Query23:???Query23:???Query23:?
?Next, concordance windows containing the unknown term ???
are extracted:??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?Next, the text in each window is segmented by a morphological processor intoterms with stopwords removed [21].In the query-independent version, we simply select the top 5 most frequently oc-curring terms in the concordance windows.
The top 5 source language context termsfor ???
are:26 Y. Qu, G. Grefenstette, and D.A.
Evans3527:?3399:??3035:???2658:??901:?????
?1Then, the translations of the above context terms are obtained from the bilingualdictionary to provide pseudo-translations for the unknown term ??
?, with therelevant translations in italics:???
?
?
?
gold???
?
?
?
metal???
?
?
?
money???
???
?
????
?
???
?
chief executive???
?
???
?
president???
?
???
?
presidential???
?
??
?
korea???
???????
?
?With the query-dependent version, the segmented concordances are ranked bycomparing the similarity between the concordance vector and the known term vector.Then we take the 100 top ranking concordances and, from this smaller set, select thetop 5 most frequently occurring terms.
This time, the top 5 context terms are:1391:???1382:?1335:??1045:??379:?????
?In this example, the context vectors from both versions are the same, even thoughthe terms are ranked in different orders.
The pseudo-translations from the contextvectors are:???
?
???
?
chief executive???
?
???
?
president???
?
???
?
presidential???
?
?
?
gold???
?
?
?
metal???
?
?
?
money???
???
?
????
?
??
?
korea???
???????
?
?1Romanization of the katakana name ??????
could produce a correct transliteration ofthe name in English, which is not addressed in this paper.
Our methods for name translitera-tion can be found in [18,20].The Use of Monolingual Context Vectors for Missing Translations 274   CLIR SystemWe evaluate the usefulness of the above two methods for obtaining missing transla-tions in our Japanese-to-English retrieval system.
Each query term missing from ourbilingual dictionary is provided with pseudo-translations using one of the methods.The CLIR system involves the following steps:First, a Japanese query is parsed into terms2 with a statistical part of speech taggerand NLP module [21].
Stopwords are removed from query terms.
Then query termsare split into a list of known terms, i.e., those that have translations from bilingualdictionaries, and a list of unknown terms, i.e., those that do not have translations frombilingual dictionaries.
Without using context vectors for unknown terms, translationsof the known terms are looked up in the bilingual dictionaries and our disambiguationmodule selects the best translation for each term based on coherence measures be-tween translations [19].The dictionaries we used for Japanese to English translation are based on edict3,which we expanded by adding translations of missing English terms from a core Eng-lish lexicon by looking them up using BabelFish4.
Our final dictionary has a total of210,433 entries.
The English corpus used for disambiguating translations is about703 MB of English text from NTCIR-4 CLIR track5.
For our source language corpus,we used the Japanese text from NTCIR-3.When context vectors are used to provide translations for terms missing from our dic-tionary, first, the context vectors for the unknown terms are constructed as describedabove.
Then the same bilingual lexicon is used for translating the context vectors tocreate a set of pseudo-translations for the unknown term t.  We keep all the pseudo-translations as surrogate translations of the unknown terms, just as if they really werethe translations we found for the unknown terms in our bilingual dictionary.We use a corpus-based translation disambiguation method for selecting the bestEnglish translations for a Japanese query word.
We compute coherence scores oftranslated sequences created by obtaining all possible combinations of the translationsin a source sequence of n query words (e.g., overlapping 3-term windows in our ex-periments).
The coherence score is based on the mutual information score for eachpair of translations in the sequence.
Then we take the sum of the mutual informationscores of all translation pairs as the score of the sequence.
Translations with the high-est coherence scores are selected as best translations.
More details on translationdisambiguation can be found in [19].Once the best translations are selected, indexing and retrieval of documents in thetarget language is based on CLARIT [4].
For this work, we use the dot product func-tion for computing similarities between a query and a document:2In these experiments, we do not include multiple-word expression such as ????
(warcrime) as terms, because translation of most compositional multiple-word expressions can begenerally constructed from translations of component words (??
and ??)
and our empiri-cal evaluation has not shown significant advantages of a separate model of phrase translation.3http://www.csse.monash.edu.au/~jwb/j_edict.html4http://world.altavista.com/5http://research.nii.ac.jp/ntcir/ntcir-ws4/clir/index.html28 Y. Qu, G. Grefenstette, and D.A.
Evans)()(),( tWtWDPsimDPtDP???
?=  .
(1)where WP(t) is the weight associated with the query term t and WD(t) is the weightassociated with the term t in the document D.  The two weights are computed asfollows:)()()( tIDFtTFtW DD ?=  .
(2))()()()( tIDFtTFtCtW PP ?
?=  .
(3)where IDF and TF are standard inverse document frequency and term frequency sta-tistics, respectively.
IDF(t) is computed with the target corpus for retrieval.
Thecoefficient C(t) is an ?importance coefficient?, which can be modified either manuallyby the user or automatically by the system (e.g., updated during feedback).For query expansion through (pseudo-) relevance feedback, we use pseudo-relevance feedback based on high-scoring sub-documents to augment the queries.That is, after retrieving some sub-documents for a given topic from the target corpus,we take a set of top ranked sub-documents, regarding them as relevant sub-documentsto the query, and extract terms from these sub-documents.
We use a modified Roc-chio formula for extracting and ranking terms for expansion:NumDocDocSetDtDTFtIDFtRocchio??
?=)()()((4)where IDF(t) is the Inverse Document Frequency of term t in reference database,NumDoc the number of sub-documents in the given set of sub-documents, and TFD(t)the term frequency score for term t in sub-document D.Once terms for expansion are extracted and ranked, they are combined with theoriginal terms in the query to form an expanded query.expQQknewQ +?=  (5)in which Qnew, Qorig, Qexp stand for the new expanded query, the original query, andterms extracted for expansion, respectively.
In the experiments reported in Section 5,we assign a constant weight to all expansion terms (e.g., 0.5)5   Experiments5.1   Experiment SetupFor evaluation, we used NTCIR-3 Japanese topics6.
Of the 32 topics that have rele-vance judgments, our system identifies unknown terms as terms not present in ourexpanded Japanese-to-English dictionary described above.
The evaluation of the6http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings3/index.htmlThe Use of Monolingual Context Vectors for Missing Translations 29effect of using context vectors is based only on the limited number of topics that con-tain these unknown terms.
The target corpus is the NTCIR-3 English corpus, whichcontains 22,927 documents.
The statistics about the unknown terms for short (i.e., thetitle field only), medium (i.e., the description field only), and long (i.e., the descrip-tion and the narrative fields) queries are summarized below.
The total number ofunknown terms that we treated with context vectors was 83 (i.e., 6+15+62).Short Medium LongNo.
of topics containing unknown terms 57 148 249Avg No.
of terms in topics (total) 3.2 (16) 5.4 (75) 36.2 (86.9)Avg.
No.
of unknown terms (total) 1 (6) 1.1 (15) 2.610 (62)For evaluation, we used the mean average precision and recall for the top 1000documents and also precision@30, as defined in TREC retrieval evaluations.We compare three types of runs, both with and without post-translation pseudo-relevance feedback.?
Runs without context vectors (baselines)?
Runs with query-dependent context vectors?
Runs with query-independent context vectors5.2   Empirical ObservationsTables 1-4 present the performance statistics for the above runs.
For the runs withtranslation disambiguation (Tables 1-2), using context vectors improved overall re-call, average precision, and precision at 30 documents for short queries.
Contextvectors moderately improved recall, average precision (except for the query inde-pendent version), and precision at 30 documents for medium length queries.For the long queries, we do not observe any advantages of using either query-dependent or query-independent versions of the context vectors.
This is probablybecause the other known terms in long queries provide adequate context for recover-ing the loss of missing translation of the unknown terms.
Adding candidate transla-tions from context vectors only makes the query more ambiguous and inexact.When all translations were kept (Tables 3-4), i.e., when no translation disambigua-tion was performed, we only see overall improvement in recall for short and medium-length queries.
We do not see any advantage of using context vectors for improvingaverage precision or precision at 30 documents.
For longer queries, the performancestatistics were overall worse than the baseline.
As pointed out in [10], when all trans-lations are kept without proper weighting of the translations, some terms get morefavorable treatment than other terms simply because they contain more translations.So, in models where all translations are kept, proper weighting schemes should bedeveloped, e.g., as suggested in related research [17].7Topics 4, 23, 26, 27, 33.8Topics 4, 5, 7, 13, 14, 20, 23, 26, 27, 28, 29, 31, 33, 38.9Topics 2, 4, 5, 7, 9, 13, 14, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 31, 33, 37, 38, 42, 43, 50.10The average number of unique unknown terms is 1.4.30 Y. Qu, G. Grefenstette, and D.A.
EvansTable 1.
Performance statistics for short, medium, and long queries.
Translations were disam-biguated; no feedback was used.
Percentages show change over the baseline runs.No Feedback Recall Avg.
Precision Prec@30ShortBaseline 28/112 0.1181 0.05With context vectors(query independent)43/112(+53.6%)0.1295(+9.7%)0.0667(+33.4%)With context vectors(query dependent)43/112(+53.6%)0.1573(+33.2%)0.0667(+33.4)MediumBaseline 113/248 0.1753 0.1231With context vectors(query independent)114/248(+0.9%)0.1588(-9.5%)0.1256(+2.0%)With context vectors(query dependent)115/248(+1.8%)0.1838(+4.8%)0.1282(+4.1%)LongBaseline 305/598 0.1901 0.1264With context vectors(query independent)308/598(+1.0%)0.1964(+3.3%)0.1125(-11.0%)With context vectors(query dependent)298/598(-2.3%)0.1883(-0.9%)0.1139(-9.9%)Table 2.
Performance statistics for short, medium, and long queries.
Translations weredisambiguated; for pseudo-relevance feedback, the top 30 terms from top 20 subdocumentswere selected based on the Rocchio formula.
Percentages show change over the baseline runs.With Feedback Recall Avg.
Precision Prec@30ShortBaseline 15/112 0.1863 0.0417With context vectors(query independent)40/112(+166.7%)0.1812(-2.7%)0.0417(+0.0%)With context vectors(query dependent)40/112(+166.7%)0.1942(+4.2%)0.0417(+0.0%)MediumBaseline 139/248 0.286 0.1513With context vectors(query independent)137(-1.4%)0.2942(+2.9%)0.1538(+1.7%)With context vectors(query dependent)141(+1.4%)0.3173(+10.9%)0.159(+5.1%)LongBaseline 341/598 0.2575 0.1681With context vectors(query independent)347/598(+1.8%)0.2598(+0.9%)0.1681(+0.0%)With context vectors(query dependent)340/598(-0.3%)0.2567(-0.3%)0.1639(-2.5%)The Use of Monolingual Context Vectors for Missing Translations 31Table 3.
Performance statistics for short, medium, and long queries.
All translations were keptfor retrieval; pseudo-relevance feedback was not used.
Percentages show change over thebaseline runs.No Feedback Recall Avg.
Precision Prec@30ShortBaseline 33/112 0.1032 0.0417With context vectors(query independent)57/112(+72.7%)0.0465(-54.9%)0.05(+19.9%)With context vectors(query dependent)41/112(+24.2%)0.1045(-0.2%)0.0417(+0%)MediumBaseline 113/248 0.1838 0.0846With context vectors(query independent)136/248(+20.4%)0.1616(-12.1%)0.0769(-9.1%)With context vectors(query dependent)122/248(+8.0%)0.2013(+9.5%)0.0769(-9.1%)LongBaseline 283 0.1779 0.0944With context vectors(query independent)295/598(+4.2%)0.163(-8.4%)0.0917(-2.9%)With context vectors(query dependent)278/598(-1.8%)0.1566(-12.0%)0.0931(-1.4%)Table 4.
Performance statistics for short, medium, and long queries.
All translations were keptfor retrieval; for pseudo-relevance feedback, the top 30 terms from top 20 subdocuments wereselected base on the Rocchio formula.
Percentages show change over the baseline runs.With Feedback Recall Avg.
Precision Prec@30ShortBaseline 40/112 0.1733 0.0417With context vectors(query independent)69/112(+72.5%)0.1662(-4.1%)0.1583(+279.6%)With context vectors(query dependent)44/112(+10.0%)0.1726(-0.4%)0.0417(+0.0%)MediumBaseline 135/248 0.2344 0.1256With context vectors(query independent)161/248(+19.3%)0.2332(-0.5%)0.1333(+6.1%)With context vectors(query dependent)139/248(+3.0%)0.2637(+12.5%)0.1154(-8.1%)LongBaseline 344/598 0.2469 0.1444With context vectors(query independent)348/598(+1.2%)0.2336(-5.4%)0.1333(-7.7%)With context vectors(query dependent)319/598(-7.3%)0.2033(-17.7%)0.1167(-19.2%)32 Y. Qu, G. Grefenstette, and D.A.
Evans6   Summary and Future WorkWe have used context vectors to obtain surrogate translations for terms that appear inqueries but that are absent from bilingual dictionaries.
We have described two typesof context vectors: a query-independent version and a query-dependent version.
Inthe empirical evaluation, we have examined the interaction between the use of contextvectors with other factors such as translation disambiguation, pseudo-relevance feed-back, and query lengths.
The empirical findings suggest that using query-dependentcontext vectors together with post-translation pseudo-relevance feedback and transla-tion disambiguation can help to overcome the meaning loss due to missing transla-tions for short queries.
For longer queries, the longer context in the query seems tomake the use of context vectors unnecessary.The paper presents only our first set on experiments of using context to recovermeaning loss due to missing translations.
In our future work, we will verify the ob-servations with other topic sets and database sources; verify the observations withother language pairs, e.g., Chinese-to-English retrieval; and experiment with differentparameter settings such as context window size, methods for context term selection,different ways of ranking context terms, and the use of the context term ranking incombination with disambiguation for translation selection.References1.
Ballesteros, L., and Croft, B.:  Dictionary Methods for Cross-Language Information Re-trieval.
In Proceedings of Database and Expert Systems Applications (1996) 791?801.2.
Ballesteros, L., Croft, W. B.:  Resolving Ambiguity for Cross-Language Retrieval.
InProceedings of SIGIR (1998) 64?71.3.
Billhardt, H., Borrajo, D., Maojo, V.:  A Context Vector Model for Information Retrieval.Journal of the American Society for Information Science and Technology, 53(3) (2002)236?249.4.
Evans, D. A., Lefferts, R. G.:  CLARIT?TREC Experiments.
Information Processing andManagement, 31(3) (1995) 385?395.5.
Fujii, A., Ishikawa, T.:  Japanese/English Cross-Language Information Retrieval: Explora-tion of Query Translation and Transliteration.
Computer and the Humanities, 35(4) (2001)389?420.6.
Fung, P.:  A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora toNon-parallel Corpora.
In Proceedings of AMTA (1998) 1?17.7.
Fung, P., Yee, L. Y.:  An IR Approach for Translating New Words from Nonparallel,Comparable Texts.
In Proceedings of COLING-ACL (1998) 414?420.8.
Hull, D. A., Grefenstette, G.: Experiments in Multilingual Information Retrieval.
In Pro-ceedings of the 19th Annual International ACM SIGIR Conference on Research and De-velopment in Information Retrieval (1996) 49?57.9.
Grefenstette, G.: Evaluating the Adequacy of a Multilingual Transfer Dictionary for CrossLanguage Information Retrieval.
In Proceedings of LREC (1998) 755?758.10.
Grefenstette, G.:  The Problem of Cross Language Information Retrieval.
In G. Grefen-stette, ed., Cross Language Information Retrieval, Kluwer Academic Publishers (1998)1?9.The Use of Monolingual Context Vectors for Missing Translations 3311.
Grefenstette, G., Qu, Y., Evans, D. A.:  Mining the Web to Create a Language Model forMapping between English Names and Phrases and Japanese.
In Proceedings of the 2004IEEE/WIC/ACM International Conference on Web Intelligence (2004) 110?116.12.
Ido, D., Church, K., Gale, W. A.:  Robust Bilingual Word Alignment for Machine AidedTranslation.
In Proceedings of the Workshop on Very Large Corpora: Academic and In-dustrial Perspectives (1993) 1?8.13.
Jeong, K. S., Myaeng, S, Lee, J. S., Choi, K. S.:  Automatic Identification and Back-transliteration of Foreign Words for Information Retrieval.
Information Processing andManagement, 35(4) (1999) 523?540.14.
Knight, K, Graehl, J.:  Machine Transliteration.
Computational Linguistics: 24(4) (1998)599?612.15.
Kumano, A., Hirakawa, H.:  Building an MT dictionary from Parallel Texts Based on Lin-guistic and Statistical Information.
In Proceedings of the 15th International Conference onComputational Linguistics (COLING) (1994) 76?81.16.
Meng, H., Lo, W., Chen, B., Tang, K.: Generating Phonetic Cognates to Handel NamedEntities in English-Chinese Cross-Language Spoken Document Retrieval.
In Proc of theAutomatic Speech Recognition and Understanding Workshop (ASRU 2001) (2001).17.
Pirkola, A., Puolamaki, D., Jarvelin, K.: Applying Query Structuring in Cross-LanguageRetrieval.
Information Management and Processing: An International Journal.
Vol 39 (3)(2003) 391?402.18.
Qu, Y., Grefenstette, G.:  Finding Ideographic Representations of Japanese Names in LatinScripts via Language Identification and Corpus Validation.
In Proceedings of the 42ndAnnual Meeting of the Association for Computational Linguistics (2004) 183?190.19.
Qu, Y., Grefenstette, G., Evans, D. A.: Resolving Translation Ambiguity Using Monolin-gual Corpora.
In Peters, C., Braschler, M., Gonzalo, J.
(eds): Advances in Cross-LanguageInformation Retrieval: Third Workshop of the Cross-Language Evaluation Forum, CLEF2002, Rome, Italy, September 19?20, 2002.
Lecture Notes in Computer Science, Vol2785.
Springer (2003) 223?241.20.
Qu, Y., Grefenstette, G., Evans, D. A:  Automatic Transliteration for Japanese-to-EnglishText Retrieval.
In Proceedings of the 26th Annual International ACM SIGIR Conferenceon Research and Development in Information Retrieval (2003) 353?360.21.
Qu, Y., Hull, D. A., Grefenstette, G., Evans, D. A., Ishikawa, M., Nara, S., Ueda, T.,Noda, D., Arita, K., Funakoshi, Y., Matsuda, H.:  Towards Effective Strategies for Mono-lingual and Bilingual Information Retrieval: Lessons Learned from NTCIR-4.
ACMTransactions on Asian Language Information Processing.
(to appear)22.
Zhang, Y., Vines, P:  Using the web for automated translation extraction in cross-languageinformation retrieval.
In Proceedings of the 27th Annual International ACM SIGIR Con-ference on Research and Development in Information Retrieval (2004) 162?169.
