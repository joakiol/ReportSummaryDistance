Generation Systems ShouM Choose Their Words*Mitchell MarcusAT&T Bell LaboratoriesMurray Hill, NJ 07974Almost all current natural language generation (NLG) systems, as contrasted with currentNL understanding (NLU) systems, have one somewhat surprising property in common: Mostcurrent NLG systems don't use words at all.
Such systems operate by incrementally specifyingfragments of linguistic structure in a top-down fashion, typically inserting specific lexical items onlywhen the frontier of the structure is encountered.
In some important sense, these systems have noreal knowledge of lexical semantics and only rarely make lexical choices.
Instead they choosebetween one tree fragment or another, only rarely able to see the leaves for the trees.
Somehowthe fact that particular words have particular meanings is incidental to the operation of thesesystems; they use fragments of linguistic structure which eventually have words as their frontiers,but they have little or no explicit knowledge of these words and what they mean.
At best, thesesystems assume that each conceptual primitive corresponds to a particular unique lexical item orphrase, trivializing the problem of lexical choice to one of table lookup, and trivializing theproblem of lexical semantics to the claim that the meaning of the word can be represented by thesame word in upper case, more or less.
While this practice may suffice for generation systems fornarrow application domains, it is most certainly wrong from a cognitive point of view.What makes all this surprising is that current research in generation focusses on such subtleand difficult matters as responding appropriately to the users intentions, correctly structuring theillocutionary force of a generated utterance, correctly utilizing rhetorical structures and the like.While we have undertaken the understanding of such subtle phenomena, and have attempted tobuild systems which sound highly fluent, we have avoided research on what would make suchsystems mean the literal content of the words they use.The alternative, which we must face up to sooner or later, is to attack the closely relatedproblems of lexical semantics and lexical choice directly.
In an ideal world, I believe, a NLGsystem should have available to it a lexicon of words and their meanings, represented in a non-domain specific way, along with a general mechanism which uses this representation to map amessage in "Mentalese" into whatever words are appropriate to realiT~ng the meaning embodied init.
One might well expect such a system to somehow compile or invert this lexicon into arepresentation which allows this process to be executed efficiently, but the meaning of a word (innon-domain specific terms) should be encoded, even if implicitly, in such an invertedrepresentation.
Thus, the use of discrimination-net-like mechanisms for lexical choice, forexample, is not excluded by the considerations of concern here; such networks, however, ought tobe invertable to yield real definitions of lexical content in other than a domain specific way.
(Onthe other hand, discrimination ets impose a rigid order in which choice criteria are examinedregardless of the context under consideration, but such difficulties are not the focus of mycomments here.)
One touchstone for such a system is that it should be capable of absorbing newlexical entries nearly seamlessly, with only a compilation step before such definitions can beefficiently utilized.The views expressed here result from ongoing discussion with Robert Rubinoff, to whom the author is greatly indebted.Any shortcomings are my responsibility, of course.232Current systems by and large fail to represent lexical meaning, and back away from mostissues of lexical choice.
Symptomatic of this lack of concern with words and lexical choice is thestructure of the lexicon in such systems.
Current NLG systems often lack any lexicon per se, asdistinct from a dictionary of the semantic primitives of the "message" domain.
A typical case isthe TEXT system (McKeown, 1985), whose dictionary is exactly a dictionary of primitives, notwords.
Its function is to translate primitives in the message into syntactic fragments that usuallyinclude at least one word each, but it fundamentally encodes tories about how to encode semanticprimitives in very particular configurations for a particular application, not stories about themeanings of words.
To see that this is true, consider the translation of the concept "GUIDED"into a fragment of structure consisting of the adjective "guided" and the noun "projectile".
Notethat the system is entirely unaware of the contribution of the meanings of "guided" and "projectile"to achieve this translation; in this sense, it knows nothing about the meanings of either "guided"or "projectile" per se.
(The lexicon proposed in (Mathiesson, 1981) is an exception to this, andmight allow a full encoding in KL-ONE of the lexical semantics of a word.
However, thestructures that Mathiesson presents encode only selectional restrictions and thematic relations ofthe verb; the examples given do not attempt to encode conceptual meaning.
)A general exception to this is the meaning of "closed class" function words such asdeterminers like "the" and "a" and relative markers like "who" or "what".
Many systems,particularly systems which utilize systemic grammar or one or another unification based approach(e.g.
(Appelt 1985)), build up the choice of closed class items by combining binary features uch as"definite/indefinite" and "singular/plural", resulting in the end in a fully specified description ofone grammatical formative or another.
But note that it is only this subpart of the lexicon whosemeaning can be represented adequately by sets of independently determined binary features.Another issue which highlights the lack of lexical knowledge is the limited extent o whichknowledge of the lexicon of a particular domain used by a generation component of a full NLinterface could be shared with the understanding component.
By and large, existing NLU systemsuse some form of compositional semantics to do analysis, using some restricted form of lexicalmeaning.
Thus, most analysis ystems would construct a representation f the meaning of "guidedprojectile" by modifying the meaning of "projectile" with the meaning of "guided" in classicalfashion.
If such an analysis ystem paralleled TEXT, it would match the entire syntactic structureof "guided projectile" to yield one underlying semantic atom.
In fact, this is exactly the theoreticalposition taken by the PHRAN/PHRED project (Wilensky and Arens, 1981; Jacobs, 1985), whichviews all language understanding and generation as a phrasal process.
This view exactly allowsWilensky and his collaborators to use the same lexicons both for generation and analysis, althoughthe position they currently take is that this work is not be viewed as theoretically motivated, asRubinoff points out (Rubinoff, pc).This view, however, begs several major questions: 1) Why should the same word be used indifferent contexts?
The pure phrasal view denies that words per se have meanings, and views alluses of words within phrases as essenfiaUy idiomatic.
Thus, it would seem unclear why "guided"should be used in a wide range of phrases aU of which are consistent with the view that "guided"has a particular consistent meaning which it contributes to each of these contexts.
If allunderstanding and generation is phrasal, then the contribution of a particular lexical item to eachphrase in which it occurs would be far more idiosyncratic than appears to be the case.In sharp contrast with the trend in generation, a wide variety of current heories of linguisticcompetence are increasingly lexicatist.
These theories increasingly assume that the locus of muchgrammatical knowledge is the lexicon itself, and that much grammatical structure follows fromconstraints on the use of particular lexical items.
This trend is all the more striking because thesetheories, taken together, are widely divergent on most other details of linguistic analysis.233For example, in the Government-Binding framework (Chomsky, 1981), large scale phrasestructure is seen as projected upward from the lexical items themselves, with aspects of syntacticstructure such as case marking deriving from properties of particular lexical items.
In GB, mostgrammatical properties are viewed as properties of words and grammatical formatives; theseproperties play themselves out in accordance to the constituency structure of the grammatical tree,but the properties themselves derive from words.
It is widely suggested, in fact by Stowell(Stowell 1981) and others, that constituency structure itself is derivative on an interaction of moreabstract grammatical properties and the properties of particular lexical items.
While these theoriesby and large fail to explicitly consider issues of meaning deeper than argument structures ofparticular verbs, recent work suggests that many of these properties of "theta-grids" follows fromdeeper semantic generalizations and much work is currently going on in this area (e.g.
(Levin andRappaport 1985), Uackendoff, 1983)).The framework of Lexicalist-Functional Grammar (Bresnan, 1982) began with theobservation that what appeared to be large scale properties of the structural configurations ofsentences could be accounted for by local "pre-compiled" statements about the argument structuresof particular lexical items, in particular verbs.
In LFG, the grammatical structure of a sentencefollows, in large measure, from the mutual satisfaction of elaborate sets of constraints inheritedfrom words, with the constituency structure of secondary importance.Perhaps clearest of all are linguistic theories following from Richard Montague's theories ofnatural language semantics, including both early GPSG (Gazdar, 1982) and recent work incategorial grammar.
In these theories, a very simple grammatical component serves to control thecomputation of a semantic representation which follows by the composition of lambda-expressionsrepresenting word meanings.
In such a framework, all meaning per se derives from lexical itemsand grammatical formatives; syntax merely serves to indicate exactly what lambda-reductionsshould be performed and in what order.As a practical matter, current approaches seem extremely well suited for building generatorsfor particular applications in narrow, well defined domains.
On the other hand, each newapplication requires the construction of an application-specific dictionary of translations from theprimitives of the messages constructed by the application into fragments of English structure.
Inthe long run, if NLG systems are to be both fluent and quickly portable, they must actually knowabout both words and their meanings.ReferencesAppelt, Douglas E., Planning English sentences, Cambridge University Press, Cambridge,UK, 1985.Bresnan, Joan (ed.
), The Memal Representation of Grammatical Relations, MIT Press,Cambridge, MA, 1982.Chomsky, Noam, Lectures on Uovernmem and Binding, Foris, Dordrecht, Holland, 1981.Gazdar, Gerald, "Phrase structure grammar", in Pauline Jacobsen and Geoffrey K.
Pullum,eds., The Nature of Syntactic Representation, Reidel, Dordrecht, Holland, 1982.Jacobs, Paul S., "PHRED: A generator for natural anguage interfaces", Computer ScienceDivision Technical Report UCB/CSD 85/198, UC Berkeley, Berkeley, CA, 1985.Jackendoff, Ray, Semantics and Cognition, MIT Press, Cambridge, MA, 1983.Levin, Beth & Malka Rappaport, "On the Formation of Adjectival Passives", Lexicon ProjectWorking Papers Number 2, Center for Cognitive Science, MIT, Cambridge, MA, 1985.234Mathiesson, C.M.I.M, "A grammar and a lexicon for a text-production system" inProceedings of the 19th Annual Meeting of the Association for Computational Linguistics, Stanford,CA, 1981, pp.
49-56.Kathleen R. McKeown, Text generation, Cambridge University Press, Cambridge, UK, 1985.Stowell, Tim, Origins of Phrase Structure, unpublished Ph.D. Thesis, MIT, Cambridge, MA,1981.Wilensky, Robert, and Yigal Arens, "PHRAN --A Knowledge-based approach to naturallanguage analysis", ERL Laboratory Memorandum UCB/ERL M80/34, UC Berkeley, Berkeley,CA, 1981.235
