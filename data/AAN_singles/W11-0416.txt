Proceedings of the Fifth Law Workshop (LAW V), pages 129?133,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsMAE and MAI: Lightweight Annotation and Adjudication ToolsAmber StubbsDepartment of Computer ScienceBrandeis University MS 018Waltham, Massachusetts, 02454 USAastubbs@cs.brandeis.eduAbstractMAE and MAI are lightweight annotation andadjudication tools for corpus creation.
DTDsare used to define the annotation tags and at-tributes, including extent tags, link tags, andnon-consuming tags.
Both programs are writ-ten in Java and use a stand-alone SQLitedatabase for storage and retrieval of annota-tion data.
Output is in stand-off XML.1 IntroductionThe use of machine learning for natural languageprocessing tasks has been steadily increasing overthe years: text processing challenges such as thoseassociated with the SemEval workshops (Erk andStrapparava, 2010) and the I2B2 medical informat-ics shared tasks (i2b2 team, 2011) are well known,and tools for training and testing algorithms on cor-pora, such as the Natural Language Tool Kit (Birdet al, 2009) and the WEKA tools (Hall et al, 2009)are widely used.However, a key component for training a machinefor a task is having sufficient data for the computerto learn from.
In order to create these corpora, hu-man researchers must define the tasks that they wishto accomplish and find ways to encode the necessaryinformation, usually in some form of XML, thenhave relevant data annotated with XML tags.The necessity of corpus annotation has led to anumber of useful tools, as well as assessments fortool usability and standards for linguistic annotation.A recent survey (Dipper et al, 2004) examined whatattributes an annotation tool should have for it to bemost useful, and the Linguistic Annotation Frame-work (LAF) describes the desired properties of anannotation framework to ensure interoperability andutility (Ide and Romary, 2006).The Multi-purpose Annotation Environment(MAE), and the Multi-document AdjudicationInterface (MAI) were designed to be easy to beginusing, but have enough flexibility to provide astarting point for most annotation tasks.
Bothprograms are written in Java and use a stand-alone SQLite database1 for storage and retrievalof annotation data, and output standoff XMLthat is compliant with the abstract LAF model.Both of these tools are available from http://pages.cs.brandeis.edu/?astubbs/2 Related WorkAs previously mentioned, there are already a num-ber of annotation tools in use?Dipper et al exam-ined five different programs; additionally Knowta-tor (Ogren, 2006), GATE (Cunningham et al, 2010),Callisto (MITRE, 2002), and BAT (Verhagen, 2010)have been used for various annotation tasks; and thelist goes on2.
However, as Kaplan et al noted ina paper about their own annotation software, SLAT2.0 (2010), much annotation software is not generic,either because it was designed for a specific anno-tation task, or designed to be used in a particularway.
BAT, for example, utilizes a layered anno-tation framework, which allows for adjudication ateach step of the annotation process, but this makes1http://www.zentus.com/sqlitejdbc/2See http://annotation.exmaralda.org/index.php/Tools for areasonably up-to-date list of annotation software129tasks difficult to modify and is best suited for usewhen the schema is not likely to change.
GATE wasbuilt primarily as a tool for automated annotation,and Callisto, while excellent for annotating contigu-ous portions of texts, cannot easily create links?it requires the user to create an entire task-specificplug-in.
Knowtator provides links and extent tag-ging, but comes as a plug-in for Prote?ge?3, a levelof overhead that users may find daunting.
Similarly,the Apache UIMA system (Apache, 2006) is welldeveloped and supported but presents a very steeplearning curve for task creation.As for adjudication, while some software hasbuilt-in judgment capabilities (GATE, BAT, Know-tator, and SLAT, for example), that functionalitydoes not stand alone, but rather relies on the annota-tions being done in the same environment.All of the tools mentioned are well-suited for theirpurposes, but it seems that there is room for an anno-tation tool that allows for reasonably complex anno-tation tasks without requiring a lot of time for setup.3 Simple Task CreationOne of the defining factors that Dipper et al (2004)identified in evaluating annotation tools is simplic-ity of use?how long does it take to start annotating?Upon examining various existing annotation tools,they found that there was often a trade-off betweensimplicity and data quality assurance: tools that havean open interface and loose restrictions for tag setstended to have lower quality data output, while toolsthat require a specification could output better data,but took a little longer to get running.MAE and MAI attempt to find a middle groundbetween the two extremes: they require task defini-tions in the form of slightly customized DocumentType Definition (DTD) files, which are used to de-fine the tags and their attributes but are not difficultto create or modify4.There are two types of tags that are primarily usedin annotation: extent tags (sometimes called ?seg-ments?
(Noguchi et al, 2008)) which are used tomark a contiguous portion of text as having a spe-cific characteristic, and link tags, which are used to3http://protege.stanford.edu/4In the future, a GUI will be added to MAE that will makethe DTD creation process easier.create a relationship between two extent tags.
MAEand MAI support both of these tag types, and addi-tionally support non-consuming extent tags, whichcan be useful for having an annotator mark explic-itly whether or not a particular phenomena appearsin the document being annotated.DTD creation is quite simple.
If, for example, anannotator wanted to look at nouns and mark theirtypes, they could define the following:<!ELEMENT NOUN (#PCDATA)><!ATTLIST NOUN type(person|place|thing|other)>The ?#PCDATA?
in the first line informs the soft-ware that NOUN is an extent tag, and the secondline gives NOUN an attribute called ?type?, with thepossible values defined in the list in parenthesis.Creating a link is equally simple:<!ELEMENT ACTION EMPTY ><!ATTLIST ACTION relationship(performs|performed_by)>The ?EMPTY?
marker indicates that the tag is alink, and the attributes and attribute values work thesame way as for extent tags.4 MAEOnce the DTD is created and files are preprocessed,the user loads the DTD and a file into MAE.
Thetext to be annotated appears in the window, and awindow at the bottom of the screen holds a tablefor each tag (see Figure 1).
When a user selectsan extent and creates a tag, some information aboutthe tag is automatically added to the table: the startand end locations of the tag, and the text of the ex-tent.
Additionally, MAE will automatically generatea document-unique ID number for that tag so that itcan easily be referenced in links.The user can then add in any information aboutthe attributes by filling in the table at the bottom ofthe screen.
In the text window, the color of the ex-tent that has been tagged is changed to the color as-sociated with that tag.
If there are multiple tags in alocation, the text is underlined as well.
Highlightingtagged text in the window will also highlight any ta-ble rows associated with that tag, including link tags.This makes it easy for the annotator to see what in-formation has already been added about that text.130Figure 1: TimeML annotation in MAE.Non-consuming tags are created from the menu atthe top of the screen.
Links are created by holdingdown the control key (or the command key on Macs)and clicking on the two tags that will be linked.
Awindow pops up that allows the user to link eitherto the tags at the specified locations, or to any non-consuming tags that have been created in the docu-ment.5 OutputOnce the user is done annotating, they can save theirwork in an XML file.
MAE outputs (and takes as in-put) UTF-8 encoded files, so it can be used to anno-tate any character set that is representable in UTF-8,including Chinese.
The output is compliant with theLAF guidelines (Ide and Romary, 2006).5.1 System testingMAE is currently being used for a variety of annota-tion tasks: medical record annotation, eligibility cri-teria assessment, and for a university course on cor-pus creation.
Annotation tasks in that course rangefrom opinion annotation to tense and aspect in Chi-nese verbs.
It is currently being used on Windows,Mac, and Linux.6 MAIMAI is built on the same back-end code as MAE,making them easily compatible.
Like MAE, usingMAI begins with loading a DTD.
Then the adjudi-cator can load each annotation of a text that theywould like to create a gold standard for.
As eachnew document is added, MAI loads the tag informa-tion for each annotation into the database for quickreference.Once all the files are loaded, the adjudicator se-lects the tag they want to review from the left partof the screen.
The text is then color-coded to reflectthe agreement of the annotators: blue if all the anno-tators agree that a tag of the selected type should beat a location, red if only a subset would place a tagthere, and black for locations where no tag is present(see Figure 2).When text is highlighted by the adjudicator, theinformation about each annotator?s tag and attributesfor that location is filled in on a table to the right ofthe screen.
From there, the annotator can either fillin the values for the gold standard by hand, or copythe values from one annotator directly into the goldstandard column and modifying them as needed.Once the adjudicator is satisfied with the gold stan-dard they can add the annotation to the database by131Figure 2: The extent adjudication table in MAIclicking the ?accept/modify?
button at the bottom ofthe gold standard column.
At this point, MAI willgenerate a new ID for that tag, and the color of theadjudicated font will become green.At the time of this writing, the algorithms for linkand non-consuming tag adjudication have not beenfully worked out for use inside of MAI.
However,once the extent tags have been adjudicated, the an-notator can choose to export the non-consuming tagsand link tags that involve ?approved?
extent tagsinto an XML file, along with the adjudicated ex-tents.
This partially-judged file can then be loadedinto MAE, where it is easier to display and modifyall the relevant information.6.1 System testingAs with MAE, MAI has been used for the variousannotation projects for a course on corpus creation,as well as a medical record annotation task.
Thisprogram is still under development, but so far adju-dications tasks with MAI have proved successful.7 Conclusions and Future WorkWhile MAE and MAI do not represent a new fron-tier in annotation software, I believe that their easeof use, portability, and clean visualization will makethem useful tools for annotation projects that do notwant to invest in the time required to use other exist-ing software, and for adjudicators that want an easyway to fix discrepancies between annotators.
Admit-tedly, tasks involving heirarchical annotations wouldrequire one of the more sophisticated tools that arecurrently available, but there are still many tasks thatdo not require that level of complexity that MAE andMAI can be used for.There is room for improvement in both of theseprograms: fully implementing link adjudication inMAI, allowing for more customization in the visu-alizations would make them more enjoyable to use,and expanding the functionality to make them moreuseful for more tasks (for example, allowing linkswith multiple anchors instead of just two).
BothMAE and MAI are under development, and im-provements to both will be made over the comingmonths.AcknowledgmentsFunding for this project development was providedby NIH grant NIHR21LM009633-02, PI: JamesPustejovskyMany thanks to the annotators who helped meidentify bugs in the software, particularly CorneliaParkes, Cheryl Keenan, BJ Harshfield, and all thestudents in the Brandeis University Spring 2011Computer Science 216 class.132ReferencesApache.
2006.
Unstructured information managementarchitecture.
http://uima.apache.org/.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia Inc, Sebastopol, CA, first edition edition.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan, Niraj Aswani, andIan Roberts, 2010.
Developing Language ProcessingComponents with GATE, 5 edition, July.Stefanie Dipper, Michael Go?tze, and Manfred Stede.2004.
Simple annotation tools for complex annota-tion tasks: an evaluation.
In Proceedings of the LRECWorkshop on XML-based Richly Annotated Corpora,pages 54?62, Lisbon, Portugal.Katrin Erk and Carlo Strapparava.
2010.
Semeval-2.http://semeval2.fbk.eu/semeval2.php.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11(1).i2b2 team.
2011. i2b2 shared task.https://www.i2b2.org/NLP/Coreference/Main.php.accessed Feb. 2011.Nancy Ide and Laurent Romary.
2006.
Representing lin-guistic corpora and their annotations.
In Proceedingsof the Fifth Language Resources and Evaluation Con-ference (LREC.Dain Kaplan, Ryu Iida, and Takenobu Tokunaga.
2010.Slat 2.0: Corpus construction and annotation processmanagement.
In Proceedings of the 16th Annual Meet-ing of The Association for Natural Language Process-ing, pages pp.510 ?
513.MITRE.
2002.
Callisto website.http://callisto.mitre.org/index.html.
accessed Dec. 17,2010.Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008.Multiple purpose annotation using slat - segment andlink-based annotation tool -.
In Proceedings of 2ndLinguistic Annotation Workshop, pages pp.61 ?
64.Philip V. Ogren.
2006.
Knowtator: a prote?ge?
plug-infor annotated corpus construction.
In Proceedings ofthe 2006 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 273?275, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Marc Verhagen.
2010.
The brandeis annotation tool.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odijk,Stelios Piperidis, Mike Rosner, and Daniel Tapias,editors, Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).133
