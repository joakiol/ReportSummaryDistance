A Character-Based Joint Modelfor CIPS-SIGHAN Word Segmentation Bakeoff 2010Kun Wang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Science{kunwang,cqzong}@nlpr.ia.ac.cnKeh-Yih SuBehavior Design Corporationkysu@bdc.com.twAbstractThis paper presents a Chinese WordSegmentation system for the closed trackof CIPS-SIGHAN Word SegmentationBakeoff 2010.
This system adopts acharacter-based joint approach, whichcombines a character-based generativemodel and a character-based discrimina-tive model.
To further improve the cross-domain performance, we use an addi-tional semi-supervised learning proce-dure to incorporate the unlabeled corpus.The final performance on the closedtrack for the simplified-character textshows that our system achieves compa-rable results with other state-of-the-artsystems.1 IntroductionThe character-based tagging approach (Xue,2003) has become the dominant technique forChinese word segmentation (CWS) as it can tol-erate out-of-vocabulary (OOV) words.
In the lastfew years, this method has been widely adoptedand further improved in many previous works(Tseng et al, 2005; Zhang et al, 2006; Jiang etal., 2008).
Among various character-based tag-ging approaches, the character-based joint model(Wang et al, 2010) achieves a good balance be-tween in-vocabulary (IV) words recognition andOOV words identification.In this work, we adopt the character-basedjoint model as our basic system, which combinesa character-based discriminative model and acharacter-based generative model.
The genera-tive module holds a robust performance on IVwords, while the discriminative module canhandle the extra features easily and enhance theOOV words segmentation.
However, the per-formance of out-of-domain text is still not satis-factory as that of in-domain text, while few pre-vious works have paid attention to this problem.To further improve the performance of the ba-sic system in out-of-domain text, we use a semi-supervised learning procedure to incorporate theunlabeled corpora of Literature (Unlabeled-A)and Computer (Unlabeled-B).
The final resultsshow that our system performs well on all fourtesting-sets and achieves comparable segmenta-tion results with other participants.2 Our system2.1 Character-Based Joint ModelThe character-based joint model in our systemcontains two basic components:?
The character-based discriminative model.?
The character-based generative model.The character-based discriminative model(Xue, 2003) is based on a Maximum Entropy(ME) framework (Ratnaparkhi, 1998) and can beformulated as follows:21 1 1 21( ) ( ,nn n kk k kkP t c P t t c +?
?=?? )
(1)Where tk is a member of {Begin, Middle, End,Single} (abbreviated as B, M, E and S from nowon) to indicate the corresponding position ofcharacter ck in its associated word.
For example,the word ????
(Beijing City)?
will be as-signed with the corresponding tags as: ??
/B(North) ?/M (Capital) ?/E (City)?.This discriminative module can flexibly in-corporate extra features and it is implementedwith the ME package1 given by Zhang Le.
Alltraining experiments are done with Gaussianprior 1.0 and 200 iterations.The character-based generative module is acharacter-tag-pair-based trigram model (Wang etal., 2009) and can be expressed as below:111([ , ] ) ([ , ] [ , ] ).nni iiP c t P c t c t ??=??
2i  (2)In our experiments, SRI Language ModelingToolkit2 (Stolcke, 2002) is used to train the gen-erative trigram model with modified Kneser-Neysmoothing (Chen and Goodman, 1998).The character-based joint model combines theabove discriminative module and the generativemodule with log-linear interpolation as follows:1221 2( ) log( ([ , ] [ , ] ))(1 ) log( ( , ))kk kkk k kScore t P c t c tP t t c????+?
?= ?+ ?
?k(3)Where the parameter (0.0 1.0)?
??
?
is theweight for the generative model.
Score(tk) willbe directly used during searching the best se-quence.
We set an empirical value ( 0.3?
= ) tothis model as there is no development-set forvarious domains.2.2 FeaturesIn this work, the feature templates adopted in thecharacter-based discriminative model are verysimple and are listed below:11 12 1 0 1( ) ( 2, 1,0,12);( ) ( 2, 1,0,1);( ) ;( ) ( ) ( ) ( ) ( ) ( )nn na C nb C C nc C Cd T C T C T C T C T C+??
?= ?
?= ?
?2In the above templates, Cn represents a char-acter and the index n indicates the position.
Forexample, when we consider the third character???
in the sequence ??????
?, template (a)results in the features as following: C-2=?, C-1=?, C0=?, C1=?, C2=?, and template (b) gen-erates the features as: C-2C-1=?
?, C-1C0=?
?,1 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html2 http://www.speech.sri.com/projects/srilm/C0C1=?
?, C1C2=?
?, and template (c) givesthe feature C-1C1=?
?.Template (d) is the feature of character type.Five types classes are defined: dates (??
?, ???,??
?, the Chinese character for ?year?, ?month?and ?day?
respectively) represents class 0; for-eign alphabets represent class 1; Arabic andChinese numbers represent class 2; punctuationrepresents class 3 and other characters representclass 4.
For example, when we consider thecharacter ???
in the sequence ????
?Q?,the feature T C  willbe set to ?20341?.2 1 0 1 2( ) ( ) ( ) ( ) ( )T C T C T C T C?
?When training the character-based discrimina-tive module, we convert all the binary featuresinto real-value features, and set the real-value ofC0 to be 2.0, the value of C-1C0 and C0C1 to be3.0, and the values of all other features to be 1.0.This method sounds a little strange because it isequal to duplicate some features for the maxi-mum entropy training.
However, it effectivelyimproves the performance in our previous works.2.3 Restrictions in constructing latticeAs the closed track allows the participants to usethe character type information, we add some re-strictions to our system when constructing thecharacter-tag lattice.
When we consider a char-acter in the sequence, the type information ofboth the previous and the next character wouldbe taken into account.
The restrictions are list asfollows:z If the previous, the current and the nextcharacters are all English or numbers, wewould fix the current tag to be ?M?
;z If the previous and the next characters areboth English or numbers, while the currentcharacter is a connective symbol such as ?-?,?/?, ?_?, ?\?
etc., we would also fix the cur-rent tag to be ?M?
;z Otherwise, all four tags {B, E, M, S} wouldbe given to the current character.It is shown that in the Computer domain thesesimple restrictions not only greatly reduce thenumber of words segmented, but also speed upthe system.Domain Mark OOV Rate R P F1 ROOV RIVLiterature A 0.069 0.937 0.937 0.937 0.652 0.958Computer B 0.152 0.941 0.940 0.940 0.757 0.974Medicine C 0.110 0.930 0.917 0.923 0.674 0.961Finance D 0.087 0.957 0.956 0.957 0.813 0.971Table 1: Official segmentation results of our system.Algorithm 1: Semi-Supervised LearningGiven:z Labeled training corpus: Lz Unlabeled training corpus: U1: Use L to train a segmenter S ;  02: Use S  to segment the unlabeled corpus Uand then get labeled corpus U ;003: for i  to K  do = 14: Add U  to L and get a new corpus Li;i?1Use Li to train a new segmenter Si; 5:6: Use Si to segment the unlabeled corpusand then get labeled corpus Ui; U7:     if convergence criterion meets8:          break8: end forOutput: the last segmenter S  K2.4 Semi-Supervised LearningIn the last decade, Chinese word segmentationhas been improved significantly and gets a highprecision rate in performance.
However, the per-formance for out-of-domain text is still unsatis-factory at the present.
Also, few works have paidattention to the cross-domain problem in Chi-nese word segmentation task so far.Self-training and Co-training are two simplesemi-supervised learning methods to incorporateunlabeled corpus (Zhu, 2006).
In this work, weuse an iterative self-training method to incorpo-rate the unlabeled data.
A segmenter is firsttrained with the labeled corpus.
Then this seg-menter is used to segment the unlabeled data.Then the predicted data is added to the originaltraining corpus as a new training-set.
The seg-menter will be re-trained and the procedure re-peated.
To simplify the task, we fix the weight0.3?
=  for the generative module of our jointmodel in the training iterations.
The procedure isshown in Algorithm 1.
The iterations will not beended until the similarity of two segmentationresults Ui?1 and Ui reach a certain level.
Here weused F-score to measure the similarity between?1 and Ui: treat Ui?1 as the benchmark, Ui as atesting-set.
From our observation, this methodconverges quickly in only 3 or 4 iterations forboth Literature and Computer corpora.Ui3 Experiments and Discussion3.1 ResultsIn this CIPS-SIGHAN bakeoff, we only partici-pate the closed track for simplified-character text.There are two kinds of training corpora:z Labeled corpus from News Domainz Unlabeled corpora from Literature Do-main (Unlabeled-A) and Computer Do-main (Unlabeled-B).Also, the testing corpus covers four domains:Literature (Testing-A), Computer (Testing-B),Medicine (Testing-C) and Finance (Testing-D).As there are only two unlabeled corpora forDomain A and B, we thus adopt different strate-gies for each testing-set:z Testing-A: Character-Based Joint Modelwith semi-supervised learning, trainingon Labeled corpus and Unlabeled-A;z Testing-B: Character-Based Joint Modelwith semi-supervised learning, trainingon Labeled corpus and Unlabeled-B;z Testing-C and D: Character-Based JointModel, training on Labeled corpus;Table 1 shows that our system achieves F-scores for various testing-sets: 0.937 (A), 0.940(B), 0.923 (C) and 0.957 (D), which are compa-rable with other systems.
Among those four test-ing domains, our system performs unsatisfactor-ily on Testing-C (Medicine) even the OOV rateof this domain is not the highest.
There are pos-sible reasons for this result: (1) Semi-supervisedlearning is not conducted for this domain; (2) thestatistical property between News and Medicineare significantly different.Domain Model F1 ROOVJ + R + S 0.937 0.652J + S 0.937 0.646J + R 0.936 0.646AJ 0.936 0.642J + R + S 0.940 0.757J + S 0.931 0.721J + R 0.938 0.744BJ 0.927 0.699J + R 0.923 0.674 CJ 0.923 0.674J + R 0.957 0.813DJ 0.954 0.786Table 2: Performance of various approachesJ: Baseline, the character-based joint modelR: Adding restrictions in constructing latticeS: Conduct Semi-Supervised Learning3.2 DiscussionThe aim of restrictions in constructing lattice isto improve the performance of English and nu-merical expressions, both of which appear fre-quently in Computer and Finance domain.Therefore, the improvements gained from theserestrictions are significantly in these two do-mains (as shown in Table 2).Besides, the adopted semi-supervised learningprocedure improves the performance in DomainA and B., but the improvement is not significant.Semi-supervised learning aims to incorporatelarge amounts of unlabeled data.
However, thesize of unlabeled corpora provided here is toosmall.
The semi-supervised learning procedure isexpected to be more effective if a large amountof unlabeled data is available.4 ConclusionOur system is based on a character-based jointmodel, which combines a generative module anda discriminative module.
In addition, we applieda semi-supervised learning method to the base-line approach to incorporate the unlabeled cor-pus.
Our system achieves comparable perform-ance with other participants.
However, cross-domain performance is still not satisfactory andfurther study is needed.AcknowledgementThe research work has been partially funded bythe Natural Science Foundation of China underGrant No.
60975053, 90820303 and 60736014,the National Key Technology R&D Programunder Grant No.
2006BAH03B02, and also theHi-Tech Research and Development Program(?863?
Program) of China under Grant No.2006AA010108-4 as well.ReferencesStanley F. Chen and Joshua Goodman, 1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, HarvardUniversity Center for Research in ComputingTechnology.Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu,2008.
A Cascaded Linear Model for Joint ChineseWord Segmentation and Part-of-Speech Tagging.In Proceedings of ACL, pages 897-904.Adwait Ratnaparkhi, 1998.
Maximum entropy mod-els for natural language ambiguity resolution.
Uni-versity of Pennsylvania.Andreas Stolcke, 2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Proc-essing, pages 311-318.Huihsin Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky and Christopher Manning, 2005.
AConditional Random Field Word Segmenter forSighan Bakeoff 2005.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, pages 168-171.Kun Wang, Chengqing Zong and Keh-Yih Su, 2009.Which is more suitable for Chinese word segmen-tation, the generative model or the discriminativeone?
In Proceedings of the 23rd Pacific Asia Con-ference on Language, Information and Computa-tion (PACLIC23), pages 827-834.Kun Wang, Chengqing Zong and Keh-Yih Su, 2010.A Character-Based Joint Model for Chinese WordSegmentation.
To appear in COLING 2010.Nianwen Xue, 2003.
Chinese Word Segmentation asCharacter Tagging.
Computational Linguistics andChinese Language Processing, 8 (1).
pages 29-48.Ruiqiang Zhang, Genichiro Kikui and EiichiroSumita, 2006.
Subword-based Tagging for Confi-dence-dependent Chinese Word Segmentation.
InProceedings of the COLING/ACL, pages 961-968.Xiaojin Zhu, 2006.
Semi-supervised learning litera-ture survey.
Technical Report 1530, Computer Sci-ences, University of Wisconsin-Madison.
