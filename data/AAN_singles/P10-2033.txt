Proceedings of the ACL 2010 Conference Short Papers, pages 178?183,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImproving Arabic-to-English Statistical Machine Translationby Reordering Post-verbal Subjects for AlignmentMarine Carpuat Yuval Marton Nizar HabashColumbia UniversityCenter for Computational Learning Systems475 Riverside Drive, New York, NY 10115{marine,ymarton,habash}@ccls.columbia.eduAbstractWe study the challenges raised by Ara-bic verb and subject detection and re-ordering in Statistical Machine Transla-tion (SMT).
We show that post-verbal sub-ject (VS) constructions are hard to trans-late because they have highly ambiguousreordering patterns when translated to En-glish.
In addition, implementing reorder-ing is difficult because the boundaries ofVS constructions are hard to detect accu-rately, even with a state-of-the-art Arabicdependency parser.
We therefore proposeto reorder VS constructions into SV or-der for SMT word alignment only.
Thisstrategy significantly improves BLEU andTER scores, even on a strong large-scalebaseline and despite noisy parses.1 IntroductionModern Standard Arabic (MSA) is a morpho-syntactically complex language, with differentphenomena from English, a fact that raises manyinteresting issues for natural language processingand Arabic-to-English statistical machine transla-tion (SMT).
While comprehensive Arabic prepro-cessing schemes have been widely adopted forhandling Arabic morphology in SMT (e.g., Sa-dat and Habash (2006), Zollmann et al (2006),Lee (2004)), syntactic issues have not receivedas much attention by comparison (Green etal.
(2009), Crego and Habash (2008), Habash(2007)).
Arabic verbal constructions are par-ticularly challenging since subjects can occur inpre-verbal (SV), post-verbal (VS) or pro-dropped(?null subject?)
constructions.
As a result, trainingdata for learning verbal construction translationsis split between the different constructions andtheir patterns; and complex reordering schemasare needed in order to translate them into primarilypre-verbal subject languages (SVO) such as En-glish.These issues are particularly problematic inphrase-based SMT (Koehn et al, 2003).
Standardphrase-based SMT systems memorize phrasaltranslation of verb and subject constructions as ob-served in the training bitext.
They do not cap-ture any generalizations between occurrences inVS and SV orders, even for the same verbs.
Inaddition, their distance-based reordering modelsare not well suited to handling complex reorder-ing operations which can include long distancedependencies, and may vary by context.
Despitethese limitations, phrase-based SMT systems haveachieved competitive results in Arabic-to-Englishbenchmark evaluations.1 However, error analysisshows that verbs are still often dropped or incor-rectly translated, and subjects are split or garbledin translation.
This suggests that better syntacticmodeling should further improve SMT.We attempt to get a better understanding oftranslation patterns for Arabic verb constructions,particularly VS constructions, by studying theiroccurrence and reordering patterns in a hand-aligned Arabic-English parallel treebank.
Ouranalysis shows that VS reordering rules are notstraightforward and that SMT should thereforebenefit from direct modeling of Arabic verb sub-ject translation.
In order to detect VS construc-tions, we use our state-of-the-art Arabic depen-dency parser, which is essentially the CATIBEXbaseline in our subsequent parsing work in Mar-ton et al (2010), and is further described there.
Weshow that VS subjects and their exact boundariesare hard to identify accurately.
Given the noisein VS detection, existing strategies for source-sidereordering (e.g., Xia and McCord (2004), Collinset al (2005), Wang et al (2007)) or using de-1http://www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease/currentArabic.html178Table 1: How are Arabic SV and VS translated inthe manually word-aligned Arabic-English paral-lel treebank?
We check whether V and S are trans-lated in a ?monotone?
or ?inverted?
order for allVS and SV constructions.
?Overlap?
representsinstances where translations of the Arabic verband subject have some English words in common,and are not monotone nor inverted.gold reordering all verbs %SV monotone 2588 98.2SV inverted 15 0.5SV overlap 35 1.3SV total 2638 100VS monotone 1700 27.3VS inverted 4033 64.7VS overlap 502 8.0VS total 6235 100pendency parses as cohesion constraints in decod-ing (e.g., Cherry (2008); Bach et al (2009)) arenot effective at this stage.
While these approacheshave been successful for language pairs such asGerman-English for which syntactic parsers aremore developed and relevant reordering patternsmight be less ambiguous, their impact potential onArabic-English translation is still unclear.In this work, we focus on VS constructionsonly, and propose a new strategy in order to bene-fit from their noisy detection: for the word align-ment stage only, we reorder phrases detected asVS constructions into an SV order.
Then, forphrase extraction, weight optimization and decod-ing, we use the original (non-reordered) text.
Thisapproach significantly improves both BLEU andTER on top of strong medium and large-scalephrase-based SMT baselines.2 VS reordering in gold Arabic-EnglishtranslationWe use the manually word-aligned parallelArabic-English Treebank (LDC2009E82) to studyhow Arabic VS constructions are translated intoEnglish by humans.
Given the gold Arabic syn-tactic parses and the manual Arabic-English wordalignments, we can determine the gold reorder-ings for SV and VS constructions.
We extract VSrepresentations from the gold constituent parsesby deterministic conversion to a simplified depen-dency structure, CATiB (Habash and Roth, 2009)(see Section 3).
We then check whether the En-glish translations of the Arabic verb and the Ara-bic subject occur in the same order as in Arabic(monotone) or not (inverted).
Table 1 summa-rizes the reordering patterns for each category.
Asexpected, 98% of Arabic SV are translated in amonotone order in English.
For VS constructions,the picture is surprisingly more complex.
Themonotone VS translations are mostly explainedby changes to passive voice or to non-verbal con-structions (such as nominalization) in the Englishtranslation.In addition, Table 1 shows that verb subjects oc-cur more frequently in VS order (70%) than in SVorder (30%).
These numbers do not include pro-dropped (?null subject?)
constructions.3 Arabic VS construction detectionEven if the SMT system had perfect knowledgeof VS reordering, it has to accurately detect VSconstructions and their spans in order to applythe reordering correctly.
For that purpose, weuse our state-of-ther-art parsing model, which isessentially the CATIBEX baseline model in Mar-ton et al (2010), and whose details we summa-rize next.
We train a syntactic dependency parser,MaltParser v1.3 with the Nivre ?eager?
algorithm(Nivre, 2003; Nivre et al, 2006; Nivre, 2008) onthe training portion of the Penn Arabic Treebankpart 3 v3.1, hereafter PATB3 (Maamouri et al,2008; Maamouri et al, 2009).
The training / de-velopment split is the same as in Zitouni et al(2006).
We convert the PATB3 representation intothe succinct CATiB format, with 8 dependencyrelations and 6 POS tags, which we then extendto a set of 44 tags using regular expressions ofthe basic POS and the normalized surface wordform, similarly to Marton et al (2010), followingHabash and Roth (2009).
We normalize Alif Maq-sura to Ya, and Hamzated Alifs to bare Alif, as iscommonly done in Arabic SMT.For analysis purposes, we evaluate our subjectand verb detection on the development part ofPATB3 using gold POS tags.
There are variousways to go about it.
We argue that combined de-tection statistics of constructions of verbs and theirsubjects (VATS), for which we achieve an F-scoreof 74%, are more telling for the task at hand.22We divert from the CATiB representation in that a non-matrix subject of a pseudo verb (An and her sisters) is treatedas a subject of the verb that is under the same pseudo verb.This treatment of said subjects is comparable to the PATB?s.179These scores take into account the spans of boththe subject and the specific verb it belongs to, andpotentially reorder with.
We also provide statisticsof VS detection separately (F-score 63%), sincewe only handle VS here.
This low score can beexplained by the difficulty in detecting the post-verbal subject?s end boundary, and the correct verbthe subject belongs to.
The SV construction scoresare higher, presumably since the pre-verbal sub-ject?s end is bounded by the verb it belongs to.
SeeTable 2.Although not directly comparable, our VSscores are similar to those of Green et al (2009).Their VS detection technique with conditionalrandom fields (CRF) is different from ours in by-passing full syntactic parsing, and in only detect-ing maximal (non-nested) subjects of verb-initialclauses.
Additionally, they use a different train-ing / test split of the PATB data (parts 1, 2 and 3).They report 65.9% precision and 61.3% F-score.Note that a closer score comparison should takeinto account their reported verb detection accuracyof 98.1%.Table 2: Precision, Recall and F-scores for con-structions of Arabic verbs and their subjects, eval-uated on our development part of PATB3.construction P R FVATS (verbs & their subj.)
73.84 74.37 74.11VS 66.62 59.41 62.81SV 86.75 61.07 71.68VNS (verbs w/ null subj.)
76.32 92.04 83.45verbal subj.
exc.
null subj.
72.46 60.18 65.75verbal subj.
inc. null subj.
73.97 74.50 74.23verbs with non-null subj.
91.94 76.17 83.31SV or VS 72.19 59.95 65.504 Reordering Arabic VS for SMT wordalignmentBased on these analyses, we propose a newmethod to help phrase-based SMT systems dealwith Arabic-English word order differences due toVS constructions.
As in related work on syntacticreordering by preprocessing, our method attemptsto make Arabic and English word order closer toeach other by reordering Arabic VS constructionsinto SV.
However, unlike in previous work, the re-ordered Arabic sentences are used only for wordalignment.
Phrase translation extraction and de-coding are performed on the original Arabic wordorder.
Preliminary experiments on an earlier ver-sion of the large-scale SMT system described inSection 6 showed that forcing reordering of allVS constructions at training and test time doesnot have a consistent impact on translation qual-ity: for instance, on the NIST MT08-NW test set,TER slightly improved from 44.34 to 44.03, whileBLEU score decreased from 49.21 to 49.09.Limiting reordering to alignment allows the sys-tem to be more robust and recover from incorrectchanges introduced either by incorrect VS detec-tion, or by incorrect reordering of a correctly de-tected VS.
Given a parallel sentence (a, e), weproceed as follows:1. automatically tag VS constructions in a2.
generate new sentence a?
= reorder(a) byreordering Arabic VS into SV3.
get word alignment wa?
on new sentence pair(a?, e)4. using mapping from a to a?, get correspond-ing word alignment wa = unreorder(wa?
)for the original sentence pair (a, e)5 Experiment set-upWe use the open-source Moses toolkit (Koehn etal., 2007) to build two phrase-based SMT systemstrained on two different data conditions:?
medium-scale the bitext consists of 12Mwords on the Arabic side (LDC2007E103).The language model is trained on the Englishside of the large bitext.?
large-scale the bitext consists of severalnewswire LDC corpora, and has 64M wordson the Arabic side.
The language model istrained on the English side of the bitext aug-mented with Gigaword data.Except from this difference in training data, thetwo systems are identical.
They use a standardphrase-based architecture.
The parallel corpus isword-aligned using the GIZA++ (Och and Ney,2003), which sequentially learns word alignmentsfor the IBM1, HMM, IBM3 and IBM4 models.The resulting alignments in both translation di-rections are intersected and augmented using thegrow-diag-final-and heuristic (Koehn et al, 2007).Phrase translations of up to 10 words are extractedin the Moses phrase-table.
We apply statisticalsignificance tests to prune unreliable phrase-pairs180and score remaining phrase-table entries (Chen etal., 2009).
We use a 5-gram language model withmodified Kneser-Ney smoothing.
Feature weightsare tuned to maximize BLEU on the NIST MT06test set.For all systems, the English data is tokenizedusing simple punctuation-based rules.
The Arabicside is segmented according to the Arabic Tree-bank (PATB3) tokenization scheme (Maamouri etal., 2009) using the MADA+TOKAN morpholog-ical analyzer and tokenizer (Habash and Rambow,2005).
MADA-produced Arabic lemmas are usedfor word alignment.6 ResultsWe evaluate translation quality using both BLEU(Papineni et al, 2002) and TER (Snover et al,2006) scores on three standard evaluation testsets from the NIST evaluations, which yield morethan 4400 test sentences with 4 reference transla-tions.
On this large data set, our VS reorderingmethod remarkably yields statistically significantimprovements in BLEU and TER on the mediumand large SMT systems at the 99% confidencelevel (Table 3).Results per test set are reported in Table 4.
TERscores are improved in all 10 test configurations,and BLEU scores are improved in 8 out of the 10configurations.
Results on the MT08 test set showthat improvements are obtained both on newswireand on web text as measured by TER (but notBLEU score on the web section.)
It is worth notingthat consistent improvements are obtained even onthe large-scale system, and that both baselines arefull-fledged systems, which include lexicalized re-ordering and large 5-gram language models.Analysis shows that our VS reordering tech-nique improves word alignment coverage (yield-ing 48k and 330k additional links on the mediumand large scale systems respectively).
This resultsin larger phrase-tables which improve translationquality.7 Related workTo the best of our knowledge, the only other ap-proach to detecting and using Arabic verb-subjectconstructions for SMT is that of Green et al(2009) (see Section 3), which failed to improveArabic-English SMT.
In contrast with our reorder-ing approach, they integrate subject span informa-tion as a log-linear model feature which encour-Table 3: Evaluation on all test sets: on the totalof 4432 test sentences, improvements are statisti-cally significant at the 99% level using bootstrapresampling (Koehn, 2004)system BLEU r4n4 (%) TER (%)medium baseline 44.35 48.34+ VS reordering 44.65 (+0.30) 47.78 (-0.56)large baseline 51.45 42.45+ VS reordering 51.70 (+0.25) 42.21 (-0.24)ages a phrase-based SMT decoder to use phrasaltranslations that do not break subject boundaries.Syntactically motivated reordering for phrase-based SMT has been more successful on languagepairs other than Arabic-English, perhaps due tomore accurate parsers and less ambiguous reorder-ing patterns than for Arabic VS. For instance,Collins et al (2005) apply six manually definedtransformations to German parse trees which im-prove German-English translation by 0.4 BLEUon the Europarl task.
Xia and McCord (2004)learn reordering rules for French to English trans-lations, which arguably presents less syntactic dis-tortion than Arabic-English.
Zhang et al (2007)limit reordering to decoding for Chinese-EnglishSMT using a lattice representation.
Cherry (2008)uses dependency parses as cohesion constraints indecoding for French-English SMT.For Arabic-English phrase-based SMT, the im-pact of syntactic reordering as preprocessing isless clear.
Habash (2007) proposes to learn syntac-tic reordering rules targeting Arabic-English wordorder differences and integrates them as deter-ministic preprocessing.
He reports improvementsin BLEU compared to phrase-based SMT limitedto monotonic decoding, but these improvementsdo not hold with distortion.
Instead of apply-ing reordering rules deterministically, Crego andHabash (2008) use a lattice input to represent alter-nate word orders which improves a ngram-basedSMT system.
But they do not model VS construc-tions explicitly.Most previous syntax-aware word alignmentmodels were specifically designed for syntax-based SMT systems.
These models are oftenbootstrapped from existing word alignments, andcould therefore benefit from our VS reordering ap-proach.
For instance, Fossum et al (2008) reportimprovements ranging from 0.1 to 0.5 BLEU onArabic translation by learning to delete alignment181Table 4: VS reordering improves BLEU and TER scores in almost all test conditions on 5 test sets, 2metrics, and 2 MT systemsBLEU r4n4 (%)test set MT03 MT04 MT05 MT08nw MT08wbmedium baseline 45.95 44.94 48.05 44.86 32.05+ VS reordering 46.33 (+0.38) 45.03 (+0.09) 48.69 (+0.64) 45.06 (+0.20) 31.96 (-0.09)large baseline 52.3 52.45 54.66 52.60 39.22+ VS reordering 52.63 (+0.33) 52.34 (-0.11) 55.29 (+0.63) 52.85 (+0.25) 39.87 (+0.65)TER (%)test set MT03 MT04 MT05 MT08nw MT08wbmedium baseline 48.77 46.45 45.00 47.74 58.02+ VS reordering 48.31 (-0.46) 46.10 (-0.35) 44.29 (-0.71) 47.11 (-0.63) 57.30 (-0.72)large baseline 43.33 40.42 39.15 41.81 52.05+ VS reordering 42.95 (-0.38) 40.40 (-0.02) 38.75 (-0.40) 41.51 (-0.30) 51.86 (-0.19)links if they degrade their syntax-based translationsystem.
Departing from commonly-used align-ment models, Hermjakob (2009) aligns Arabic andEnglish content words using pointwise mutual in-formation, and in this process indirectly uses En-glish sentences reordered into VS order to collectcooccurrence counts.
The approach outperformsGIZA++ on a small-scale translation task, but theimpact of reordering alone is not evaluated.8 Conclusion and future workWe presented a novel method for improving over-all SMT quality using a noisy syntactic parser: weuse these parses to reorder VS constructions intoSV for word alignment only.
This approach in-creases word alignment coverage and significantlyimproves BLEU and TER scores on two strongSMT baselines.In subsequent work, we show that matrix (main-clause) VS constructions are reordered much morefrequently than non-matrix VS, and that limit-ing reordering to matrix VS constructions forword alignment further improves translation qual-ity (Carpuat et al, 2010).
In the future, we plan toimprove robustness to parsing errors by using notjust one, but multiple subject boundary hypothe-ses.
We will also investigate the integration of VSreordering in SMT decoding.AcknowledgementsThe authors would like to thank Mona Diab, Owen Ram-bow, Ryan Roth, Kristen Parton and Joakim Nivre for help-ful discussions and assistance.
This material is based uponwork supported by the Defense Advanced Research ProjectsAgency (DARPA) under GALE Contract No HR0011-08-C-0110.
Any opinions, findings and conclusions or recommen-dations expressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.ReferencesNguyen Bach, Stephan Vogel, and Colin Cherry.
2009.
Co-hesive constraints in a beam search phrase-based decoder.In Proceedings of the 10th Meeting of the North AmericanChapter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 1?4.Marine Carpuat, Yuval Marton, and Nizar Habash.
2010.
Re-ordering matrix post-verbal subjects for arabic-to-englishsmt.
In Proceedings of the Conference Traitement Au-tomatique des Langues Naturelles (TALN).Boxing Chen, George Foster, and Roland Kuhn.
2009.Phrase translation model enhanced with association basedfeatures.
In Proceedings of MT-Summit XII, Ottawa, On-tario, September.Colin Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In Proceedings of the 46thAnnual Meeting of the Association for Computational Lin-guistics (ACL), pages 72?80, Columbus, Ohio, June.Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005.Clause restructuring for statistical machine translation.
InProceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages 531?540,Ann Arbor, MI, June.Josep M. Crego and Nizar Habash.
2008.
Using shallow syn-tax information to improve word alignment and reorderingfor SMT.
In Proceedings of the Third Workshop on Statis-tical Machine Translation, pages 53?61, June.Victoria Fossum, Kevin Knight, and Steven Abney.
2008.Using syntax to improve word alignment precision forsyntax-based machine translation.
In Proceedings of theThird Workshop on Statistical Machine Translation, pages44?52.Spence Green, Conal Sathi, and Christopher D. Manning.2009.
NP subject detection in verb-initial Arabic clauses.182In Proceedings of the Third Workshop on ComputationalApproaches to Arabic Script-based Languages (CAASL3).Nizar Habash and Owen Rambow.
2005.
Arabic Tokeniza-tion, Part-of-Speech Tagging and Morphological Disam-biguation in One Fell Swoop.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 573?580, Ann Arbor, Michigan,June.Nizar Habash and Ryan Roth.
2009.
CATiB: The ColumbiaArabic treebank.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 221?224, Suntec, Singa-pore, August.
Association for Computational Linguistics.Nizar Habash.
2007.
Syntactic preprocessing for statisti-cal machine translation.
In Proceedings of the MachineTranslation Summit (MT-Summit), Copenhagen.Ulf Hermjakob.
2009.
Improved word alignment with statis-tics and linguistic heuristics.
In Proceedings of the 2009Conference on Empirical Methods in Natural LanguageProcessing, pages 229?237, Singapore, August.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHLT/NAACL-2003, Edmonton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Annual Meeting of theAssociation for Computational Linguistics (ACL), demon-stration session, Prague, Czech Republic, June.Philipp Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proceedings of the 2004Conference on Empirical Methods in Natural LanguageProcessing (EMNLP-2004), Barcelona, Spain, July.Young-Suk Lee.
2004.
Morphological analysis for statisticalmachine translation.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, pages 57?60, Boston, MA.Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008.Enhancing the arabic treebank: a collaborative effort to-ward new annotation guidelines.
In Proceedings of theSixth International Language Resources and Evaluation(LREC?08), Marrakech, Morocco.Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-deche, Wigdan Mekki, Sondos Krouna, and BasmaBouziri.
2009.
The penn arabic treebank part 3 version3.1.
Linguistic Data Consortium LDC2008E22.Yuval Marton, Nizar Habash, and Owen Rambow.
2010.
Im-proving arabic dependency parsing with lexical and in-flectional morphological features.
In Proceedings of the11th Meeting of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL) workshopon Statistical Parsing of Morphologically Rich Languages(SPMRL), Los Angeles.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-Parser: A Data-Driven Parser-Generator for DependencyParsing.
In Proceedings of the Conference on LanguageResources and Evaluation (LREC).Joakim Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of the 8th Interna-tional Conference on Parsing Technologies (IWPT), pages149?160, Nancy, France.Joakim Nivre.
2008.
Algorithms for Deterministic Incre-mental Dependency Parsing.
Computational Linguistics,34(4).Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?52.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a method for automatic evaluation ofmachine translation.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics.Fatiha Sadat and Nizar Habash.
2006.
Combination of arabicpreprocessing schemes for statistical machine translation.In Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, pages 1?8,Morristown, NJ, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In Pro-ceedings of AMTA, pages 223?231, Boston, MA.Chao Wang, Michael Collins, and Philipp Koehn.
2007.
Chi-nese syntactic reordering for statistical machine transla-tion.
In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 737?745.Fei Xia and Michael McCord.
2004.
Improving a statisticalmt system with automatically learned rewrite patterns.
InProceedings of COLING 2004, pages 508?514, Geneva,Switzerland, August.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.
Chunk-level reordering of source language sentences with auto-matically learned rules for statistical machine translation.In Human Language Technology Conf.
/ North AmericanChapter of the Assoc.
for Computational Linguistics An-nual Meeting, Rochester, NY, April.Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006.Maximum Entropy Based Restoration of Arabic Diacrit-ics.
In Proceedings of COLING-ACL, the joint conferenceof the International Committee on Computational Linguis-tics and the Association for Computational Linguistics,pages 577?584, Sydney, Australia.Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.2006.
Bridging the inflection morphology gap for ara-bic statistical machine translation.
In Proceedings of theHuman Language Technology Conference of the NAACL,Companion Volume: Short Papers, pages 201?204, NewYork City, USA.183
