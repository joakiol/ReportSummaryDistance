Tutorial Abstracts of ACL 2012, page 5,Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational LinguisticsDeep Learning for NLP (without Magic)Richard Socher Yoshua Bengio?
Christopher D. Manningrichard@socher.org bengioy@iro.umontreal.ca, manning@stanford.eduComputer Science Department, Stanford University?
DIRO, Universite?
de Montre?al, Montre?al, QC, Canada1 AbtractMachine learning is everywhere in today?s NLP, butby and large machine learning amounts to numericaloptimization of weights for human designed repre-sentations and features.
The goal of deep learningis to explore how computers can take advantage ofdata to develop features and representations appro-priate for complex interpretation tasks.
This tuto-rial aims to cover the basic motivation, ideas, mod-els and learning algorithms in deep learning for nat-ural language processing.
Recently, these methodshave been shown to perform very well on variousNLP tasks such as language modeling, POS tag-ging, named entity recognition, sentiment analysisand paraphrase detection, among others.
The mostattractive quality of these techniques is that they canperform well without any external hand-designed re-sources or time-intensive feature engineering.
De-spite these advantages, many researchers in NLP arenot familiar with these methods.
Our focus is oninsight and understanding, using graphical illustra-tions and simple, intuitive derivations.
The goal ofthe tutorial is to make the inner workings of thesetechniques transparent, intuitive and their results in-terpretable, rather than black boxes labeled ?magichere?.The first part of the tutorial presents the basics ofneural networks, neural word vectors, several simplemodels based on local windows and the math andalgorithms of training via backpropagation.
In thissection applications include language modeling andPOS tagging.In the second section we present recursive neuralnetworks which can learn structured tree outputs aswell as vector representations for phrases and sen-tences.
We cover both equations as well as applica-tions.
We show how training can be achieved by amodified version of the backpropagation algorithmintroduced before.
These modifications allow the al-gorithm to work on tree structures.
Applications in-clude sentiment analysis and paraphrase detection.We also draw connections to recent work in seman-tic compositionality in vector spaces.
The princi-ple goal, again, is to make these methods appear in-tuitive and interpretable rather than mathematicallyconfusing.
By this point in the tutorial, the audiencemembers should have a clear understanding of howto build a deep learning system for word-, sentence-and document-level tasks.The last part of the tutorial gives a generaloverview of the different applications of deep learn-ing in NLP, including bag of words models.
We willprovide a discussion of NLP-oriented issues in mod-eling, interpretation, representational power, and op-timization.5
