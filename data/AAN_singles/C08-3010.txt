Coling 2008: Companion volume ?
Posters and Demonstrations, pages 181?184Manchester, August 2008A Linguistic Knowledge Discovery Tool:Very Large Ngram Database Search with Arbitrary WildcardsSatoshi SekineNew York University715 Broadway, 7th floor, New York, NY 10003sekine@cs.nyu.eduAbstractIn this paper, we will describe a search toolfor a huge set of ngrams.
The tool supportsqueries with an arbitrary number of wild-cards.
It takes a fraction of a second for asearch, and can provide the fillers of thewildcards.
The system runs on a singleLinux PC with reasonable size memory (lessthan 4GB) and disk space (less than 400GB).This system can be a very useful tool forlinguistic knowledge discovery and otherNLP tasks.1 IntroductionCurrently, NLP research is shifting towards se-mantic analysis.
In order to understand what asentence means, we require substantial back-ground knowledge which must be gathered inadvance.
Building such knowledge is not aneasy task.
This is the so-called ?knowledge bot-tleneck?
problem, which was one of the majorreasons for the failure of much AI research inthe 1980's.
However, now, the circumstanceshave quite changed.
We have an almost unlim-ited amount of text and machine power has dras-tically improved.
Using these fortunate assets,research on knowledge discovery in NLP isbooming.
The work by (Hearst 92) (Collins andSinger 99) (Brin 99) (Hasegawa et al 04) areonly a few examples of this research direction.Notice that most of these methods use local con-text.
For example, a lexico-syntactic pattern,like ?NP such as NP?
can extract hyponym rela-tionships (Hearst 92), and contexts between twonamed entities can indicate a relationship be-tween those names (Hasegawa et al 04).?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Un-ported license (http://creativecommons.org/ li-censes/by-nc-sa/3.0/).
Some rights reserved.It is quite natural to believe that the larger thecorpus, the larger and the more reliable the dis-covered knowledge can be.
However, it leads toproblems in terms of speed and machine power.In order to solve these problems, some peopleuse commercial search engines (Chklovski andPantel 04).
However, using such search engineshas serious problems: 1) Only a limited numberof results available, 2) Only a limited number ofaccesses permitted, 3) Mysterious ranking func-tion, 4) Unstable results over a long time period,5) Slow, as it is over the internet.
Another ideato overcome this difficulty is to create one's own(maybe smaller) search engine (Cafarella andEtzioni 05) (Shinzato et al 08).
Although creat-ing one's own search engine has advantages (oneof which is the freedom to design the form ofthe query; such as POS and dependency), it is ahuge, expensive task; not everybody can affordto make a search engine.
More seriously, noteverybody can use it as he/she may want.In this paper, we will propose an alternativesolution which should enable researchers withmodest resources to conduct research using hugecorpora for knowledge discovery.
It is an ngramsearch tool with the following requirements.Requirements1.
It searches for ngrams which include an ar-bitrary number of wildcards, such as ?
* suchas * and?, ?Mr.
* said?, ?from * to * by *?or ?
* attack by * * on *?.2.
It returns the fillers of the wildcards as wellas ngram frequencies3.
It produces the results in a fraction of a sec-ond (for most reasonable queries)4.
It runs on a single PC5.
It needs only a reasonable amount of mem-ory (4GB) for processing6.
It needs only a reasonable amount of diskspace (400GB) for indexing 108-109 ngrams1812 Algorithm OverviewThere are two reasonable choices for the searchalgorithm.
One is ?inverted indexing?
(used by?lucene?
and others) and the other is ?trie?.
Weused trie.
Using inverted indexing, it is easy tocreate an index at the cost of runtime speed.
Weprefer an algorithm which requires more com-plicated indexing in order to achieve the speedin searching.
Trie is an indexing tree structure inwhich each node represents a symbol (in ourcase, words) and each link represents the se-quence of the symbols (in our case n-gram).Searching can be done by traversing the tree,which is usually done in time constant in thesize of the corpus.
However, it is important tomention that the trie structure is order sensitive.For example, if the query includes wildcards,such as ?Mr.
* * said yesterday?, searching thetrie is not an optimal solution.One naive solution is to create a search sys-tem (or a trie) for each possible combination ofwildcards.
For example, for the query pattern?Mr.
* * said yesterday?, we should prepare asearch system for modified ngrams which havethe first, fourth and fifth words of the originalngrams as the first three words.
For ngrams oflength N, the number of possible combinationsof literals and wildcards is 2N.
In theory, if wemake that many search systems, we can solvethe problem.
However, the number of searchsystems is too large considering the number ofngrams we aim to handle (Table 1).
Althoughwe applied two implementation techniques toreduce the size of index (which will be de-scribed in the next section), it is still likely thatwe could not satisfy requirement #6.We solved the problem by using a singlesearch system for different kinds of search pat-terns, reducing the number of needed searchsystems significantly.
It can be observed that apattern with wildcards at suffix positions can besearched using the same trie used for patternswithout those wildcards.
Also, we don?t alwaysneed to start the trie by indexing the first word.If we build an alternative trie which starts byindexing the second word, we can cover morepatterns with fewer tries.
For example, using thetrie constructed to search for 5-grams ?DEABC?
(We will call this a ?trie pattern?
: each letterrepresents a literal, with A representing the firsttoken in the original ngram), four ?search pat-terns?
(i.e.
ngram pattern used in the queries),?AB*DE?, ?A**DE?, ?***DE?
and ?
***D*?,can be searched efficiently.We found that the minimum number of triepatterns needed to cover all possible search pat-terns of length N is N/2 C N. We have con-structed minimal sets of patterns for all N up to9.
Once the system receives a query with one ormore wildcards, it finds the trie pattern whichcovers the search pattern of the query.3 ImplementationWe also implemented two ideas to reduce thesize of the index.
One is related to a commontechnique to reduce the size of trie nodes by de-leting the index of unique suffixes.
In addition,we don?t store the remaining data within the trie.We just store the ngram ID at the node in orderto further reduce the size of the index.
Becausethere are many search systems, storing thengram data in a single master database saves alot of space.
When the user wants to see thewords of an ngram which was identified by thesearch, the system retrieves the ngram from themaster database using the ngram ID.
The benefitof this technique is quite large (more than 50%reduction of index size in 9gram), as manyngrams have long unique suffixes.The other idea is based on the fact that oncethe ngram data is provided, no update will berequested (i.e.
insertion or replacement proce-dure in the trie is not necessary).
We can elimi-nate the pointers to the parents and the siblingsfor each node, which contributes to about 30%additional reduction in the index size.Because the tries are very large, we dividethem into segments (128 segments for 9-gramsand 118 for 5-grams), so that an individual trieindex segment is small enough to fit in a mem-ory of modest size (requirement #5).
Each seg-ment contains a lexicographically contiguoussequence of ngrams.
Furthermore, we use?mmap?
to get the index into memory from thedisk, so that only the portions of the trie seg-ment that are actually used will be loaded.System FlowWe will briefly describe the system flow1) First, all the words in the input query arelooked up in the dictionary.
If there are out ofvocabulary words, then there is no ngram whichmatches the query.
If all words are known, findthe appropriate trie pattern for the input querybased on the locations of wildcards.2) Then the appropriate segment(s) of the triedata is found for the search query.
It was done182by searching the table of segment index sortedin lexicographic order.3) Now, the search in the trie is performed.Note that there are 16,128 tries for 9 -gram (128segments for 126 trie patterns).
If the searchends with an internal node, there is more thanone matched ngram.
If the search ends with aterminal node, just one ngram matches.
If thetrie ends before the end of query, you have toretrieve the ngram from the master database andmatch the retrieved ngram against the query.4) Once one or several ngram IDs are identi-fied as matched ngram(s), the system will outputthe information.
There are three output modes.a) Only instance and type frequency are printed.b) Only ngram IDs are printed.
These two typescan be achieved quickly without consulting thengram master database.
c) Ngram instances areprinted using the ngram master database.4 Experiments and Evaluation4.1 Ngram dataWe implemented this using two ngram data sets.Google 1T Web 5gramThis is a part of the ngram data provided byGoogle through LDC (Web 1T).
The data wasgenerated from approximately 1 trillion wordtokens of publicly accessible Web pages.9-grams from 82 years of newspaperFor knowledge discovery purposes, 5-gramsare generally unsatisfactory.
A 5-gram can onlycover 2 words of context on each side of a singleword term.
So we extracted 9-grams from anumber of newspaper corpora available to us.Including NANTC: LATWP(94-97), NYT,REUFF, REUTE, WSJ (94-96), BBN GigaWordcorpus (news archive only): BBC(99-06), Peo-ple Daily, Taipei Times, The Hindu (00-06),Arab News, Gulf News, India Times (01-06),AQUAINT corpus: APW, NYT (98-00), Xinhua(96-00), CSR corpus: WSJ (87-94).These corpora are cleaned up by severalmethods, because some of them have article du-plications/minor variants and some of them con-tain many non-sentences.
We use a simplemethod to reduce such noise, which is to ?uniq?all the sentences for each year of each newspa-per and count each distinct sentence only once.This is not a perfect solution, but it reduces a lotof noise to an amount almost unnoticeable forthe ngram search result.
The statistics of the dataare shown in Table 1.Corpus Google 1T 82 yrs.
NewsOriginal Text 1 T words 1.8 G wordsNgram 5-gram 9-gramThreshold 40 2#of ngrams 1,176,470,663 119,456,373# of patterns 10 126# of nodes 1.4G x 10 160M x 126Index size 277GB 322GBTable 1 Statistics of the data4.2 ExampleObviously, the tool is useful for knowledge dis-covery tasks for hyponym relations, binary rela-tions, name extraction, relations between namesand so on.
It is also useful to extract more spe-cific relations, such as ?what kind of attack oc-curred by whom on what/when?
by searchingfor ?
* attack on * * by * **?.
Figure 1 shows asnapshot of the tool?s output on this query.Figure 1 Snapshot of the output for ?
* attack on * * by * * *?183Importantly, because of the speed, we can mod-ify the query in an interactive manner.
Thespeed is helpful for batch processing, too, whenwe need to search millions of patterns.4.3 EvaluationThe speed evaluation was conducted using9grams.
We randomly selected 1017 9gramsfrom the original data to form a test set, and alsocreated another test set in which 1 to 3 words arerandomly replaced by wildcards in those 9grams.We measure the average time to find the ngrams.In the case of a query with wildcards, wechecked if the original ngram is found among allreturned ngrams to verify the accuracy of thetool.
The system runs in three output modes (weuse a file output rather than stdout).
ProducingNgram output takes a long time if the number ofngrams is very large, so we limited the numberof ngrams to be printed to 100.
In the experi-ment, the number of returned ngrams rangedfrom 1 to 100 with an average of 1.26.
The re-sult is shown in Table 2 and the times are givenin milliseconds.Print mode Freq.
IDs NgramOriginal 19 19 20With wildcards 19 19 29Table 2 Speed of search (Newspaper 9gram)5 Related Work and DiscussionOne of the most related works is the CMU-Cambridge Statistical LM toolkit (CMU-Cambridge).
It?s a tool to search ngrams inSpeech fields.
However, it does not handlewildcards, which is important for knowledgediscovery purposes.
There are several ngramtools in the field of genome sequence analysis(MUMmer) (REPuter).
However, the size of thealphabets are quite different (handful of genomebases vs. about 1.08 million words), and theirmain purpose is to discover similarity or repeti-tion.
These systems are not directly useful forlinguistic knowledge discovery purposes.
As adocument retrieval tool, there is a public domainsearch engine, such as ?lucene?
(Lucene).However, its primary purpose is document re-trieval and the inverted index algorithm can?thandle well very frequent terms.
Searching isrelatively slow.It should be noted that creating the index is ahuge task.
It took about 2 months using five4GB-memory machines.
However, it took onlyabout one week with 64GB-memory machine.As for the future direction, we are improvingthe tool to show the original sentences fromwhich the ngram was extracted.
We have someevidence that a longer context is needed for theknowledge discovery purpose.This tool is merely a tool, but we believe it?sa very powerful tool for linguistic knowledgediscovery among other related objectives.
Thenext step we would like to take is to apply thistool to find linguistic knowledge for NLP appli-cations.AcknowledgementsThis research was supported in part by the Na-tional Science Foundation under Grant IIS-00325657.
This paper does not necessarily re-flect the position of the U.S. Government.
Wewould like to thank our colleagues at New YorkUniversity, who provided useful suggestions anddiscussions, including Prof. Grishman.ReferencesCMU-Cambridge Statistical LM toolkit homepage:http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.htmlLucene homepage: http://lucene.apache.org/MUMmer hompage: http://mummer.sourceforge.net/REPuter homepage: http://bibiserv.techfak.uni-bielefeld.de/reputer/Web 1T 5-gram.
by Google.
LDC Catalog No.:LDC2006T13M.
J. Cafarella and O. Etzioni.
?A Search Engine forNatural Language Applications?.
2005.
In Proc.
ofWorld Wide Web Conference.S.
Brin.
?Extracting Patterns and Relations from theWorld Wide Web?.
1998.
In Proc.
of Workshopon Web and DataBase-98.T.
Chklovski and P. Pantel.
?VerbOcean: Mining theWeb for Fine-Grained Semantic Verb Relations?.2004.
In Proc.
of EMNLP-04.
pp.
33-40.M.
Collins and Y.
Singer.
?Unsupervised Models forNamed Entity Classification?.
1998.
In Proc.
ofEMNLP-99.M.
Hearst.
?Automatic Acquisition of Hyponymsfrom Large Text Corpora?.
1992.
In Proc.
ofCOLING-92.T.
Hasegawa, S. Sekine and R. Grishman ?Discover-ing Relations among Named Entities from LargeCorpora?.
2004.
In Proc.
of ACL-04.K.
Shinzato, T. Shibata, D. Kawahara, C. Hashimotoand S. Kurohashi.
?TSUBAKI: An Open SearchEngine Infrastructure for Developing New Infor-mation Access Methodology?, 2008.
In Proc.
ofthe 3rd IJCNLP-08.184
