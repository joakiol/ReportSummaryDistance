Coling 2010: Poster Volume, pages 1247?1255,Beijing, August 2010A Comparison of Models for Cost-Sensitive Active LearningKatrin Tomanek and Udo HahnJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t Jenahttp://www.julielab.deAbstractActive Learning (AL) is a selective sam-pling strategy which has been shown tobe particularly cost-efficient by drasticallyreducing the amount of training data to bemanually annotated.
For the annotationof natural language data, cost efficiencyis usually measured in terms of the num-ber of tokens to be considered.
This mea-sure, assuming uniform costs for all to-kens involved, is, from a linguistic per-spective at least, intrinsically inadequateand should be replaced by a more ade-quate cost indicator, viz.
the time it takesto manually label selected annotation ex-amples.
We here propose three differ-ent approaches to incorporate costs intothe AL selection mechanism and evaluatethem on the MUC7T corpus, an extensionof the MUC7 newspaper corpus that con-tains such annotation time information.Our experiments reveal that using a cost-sensitive version of semi-supervised AL,up to 54% of true annotation time can besaved compared to random selection.1 IntroductionActive Learning (AL) is a selective sampling strat-egy for determining those annotation exampleswhich are particularly informative for classifiertraining, while discarding those that are alreadyeasily predictable for the classifier given previoustraining experience.
While the efficiency of ALhas already been shown for many NLP tasks basedon measuring the number of tokens or sentencesthat are saved in comparison to random sampling(e.g., Engelson and Dagan (1996), Tomanek et al(2007) or Settles and Craven (2008)), it is obviousthat just counting tokens under the assumption ofuniform annotation costs for each token is empir-ically questionable, from a linguistic perspective,at least.As an alternative, we here explore annotationcosts that incur for AL based on an empiricallymore plausible cost metric, viz.
the time it takesto annotate selected linguistic examples.
We in-vestigate three approaches to incorporate costsinto the AL selection mechanism by modifyingthe standard (fully supervised) mode of AL anda non-standard semi-supervised one according tocost considerations.
The empirical backbone ofthis comparison is constituted by MUC7T , a re-annotation of a part of the MUC7 newspapercorpus that contains annotation time information(Tomanek and Hahn, 2010).2 Active LearningUnlike random sampling, AL is a selective sam-pling technique where the learner is in control ofthe data to be chosen for training.
By design, theintention behind AL is to reduce annotation costs,usually considered as the amount of labeled train-ing material required to achieve a particular targetperformance of the model.
The latter is yieldedby querying labels only for those examples whichare assumed to have a high training utility.
In thissection, we introduce different AL frameworks ?the default, fully supervised AL approach (Sec-tion 2.1), as well as a semi-supervised variant ofit (Section 2.2).
In Section 2.3 we then proposethree methods how these approaches to AL can bemade cost-sensitive without further modifications.12472.1 Fully Supervised AL (FuSAL)As we consider AL for the NLP task of NamedEntity Recognition (NER), some design decisionshave to be made.
Firstly, the selection granular-ity is set to complete sentences ?
a reasonable lin-guistic annotation unit which still allows for fairlyprecise selection.
Second, a batch of examples in-stead of a single example is selected per AL iter-ation to reduce the computational overhead of thesampling process.We base our approach to AL on ConditionalRandom Fields (CRFs), which we employ as baselearners (Lafferty et al, 2001).
For observationsequences ~x = (x1, .
.
.
, xn) and label sequences~y = (y1, .
.
.
, yn), a linear-chain CRF is defined asP?
(~y|~x) =1Z?
(~x)?n?i=1expk?j=1?jfj(yi?1, yi, ~x, i)where Z?
(~x) is the normalization factor, and kfeature functions fj(?)
with feature weights ?
=(?1, .
.
.
, ?k) appear.The core of any AL approach is a utility func-tion u(p, ?)
which estimates the informativenessof each example p, a complete sentence p = (~x),drawn from the pool P of all unlabeled examples,for model induction.
For our experiments, we em-ploy two alternative utility functions which haveproduced the best results in previous experiments(Tomanek, 2010, Chapter 4).
The first utility func-tion is based on the confidence of a CRF model ?in the predicted label sequence ~y?
which is givenby the probability distribution P?(~y?|~x).
The util-ity function based on this probability boils downtouLC(p, ?)
= 1?
P?
(~y ?|~x)so that sentences for which the predicted label se-quence ~y?
has a low probability is granted a highutility.
Instead of calculating the model?s con-fidence on the complete sequence, we might al-ternatively calculate the model?s confidence in itspredictions on single tokens.
To obtain an overallconfidence for the complete sequence, the aver-age over the single token-confidence values can becomputed by the marginal probability P?
(yi|~x).Now that we are calculating the confidence on thetoken level, we might also obtain the performanceof the second best label and calculate the marginbetween the first and second best label as a con-fidence score so that the final utility function isobtained byuMA(p, ?)
= ?1nn?i=1(maxy?inYP?
(yi = y?|~x)?maxy??inYy?
6=y??P?
(yi = y?
?|~x))Algorithm 1 formalizes our AL framework.Depending on the utility function, the best b ex-amples are selected per round, manually labeled,and then added to the set of labeled data L whichfeeds the classifier for the next training round.Algorithm 1 NER-specific AL FrameworkGiven:b: number of examples to be selected in each iterationL: set of labeled examples l = (~x, ~y) ?
Xn ?
YnP: set of unlabeled examples p = (~x) ?
XnT (L): a learning algorithmu(p, ?
): utility functionAlgorithm:loop until stopping criterion is met1.
learn model: ?
?
T (L)2. sort p ?
P: let S ?
(p1, .
.
.
, pm) : u(pi, ?)
?u(pi+1, ?
), i ?
[1,m], p ?
P3.
select b examples pi with highest utility from S: B ?
{p1, .
.
.
, pb}, b ?
m, pi ?
S4.
query labels for all p ?
B: B?
?
{l1, .
.
.
, lb}5.
L ?
L ?
B?, P ?
P \ Breturn L?
?
L and ??
?
T (L?
)The specification is still not cost-sensitive as theselection of examples depends only on the utilityfunction.
Using uLC will result in a reduction ofthe number of examples (i.e., sentences) selectedirrespective of the sentence length so that a modellearns the most from it.
As a result, we observedthat the selected sentences are quite long whichmight even cause higher annotation costs per sen-tence (Tomanek, 2010, Chapter 4).
As for uMAthere is at least a slight normalization sensitiveto costs since the sum over all token-level utilityscores is normalized by the length of the selectedsentence.12482.2 Semi-supervised AL (SeSAL)Tomanek and Hahn (2009) extendeded this stan-dard fully supervised AL framework by a semi-supervised variant (SeSAL).
The selection of sen-tences is performed in a standard manner, i.e.,similarly to the procedure in Algorithm 1.
How-ever, once selected, rather than manually annotat-ing the complete sentence, only (uncertain) sub-sequences of each selected sentence are manuallylabeled, while the remaining (certain) ones are au-tomatically annotated using the current version ofthe classifier.After the selection of an informative examplep = (~x) with ~x = (x1, .
.
.
, xn), the subsequences~x?
= (xa, .
.
.
, xb), 1 ?
a ?
b ?
n, with low localuncertainty have to be identified.
For reasons ofsimplicity, only sequences of length 1, i.e., singletokens, are considered.
For a token xi from a se-lected sequence ~x the model?s confidence C?
(y?i )in label y?i is estimated.
Token-level confidencefor a CRF is calculated as the marginal probabil-ity so thatC?
(y?i ) = P?
(yi = y?i |~x)where y?i specifies the label at the respective posi-tion of the predicted label sequence ~y ?
(the onewhich is obtained by the Viterbi algorithm).
IfC?
(y?i ) exceeds a confidence threshold t, y?i is as-signed as the putatively correct label.
Otherwise,manual annotation of this token is required.Employing SeSAL, savings of over 80 % of thetokens compared to random sampling are reportedby Tomanek and Hahn (2009).
Even when com-pared to FuSAL, still 60 % of the number of to-kens are eliminated.
A crucial question, however,not answered in these experiments, is whether thismethod actually reduces the overall annotation ex-penses in time rather than just in the number of to-kens.
Also SeSAL does not incorporate labelingcosts in the selection process.2.3 Cost-Sensitive AL (CoSAL)In this section, we turn to an extension of FuSALand SeSAL which incorporates cost sensitivityinto the AL selection process (CoSAL).
Threedifferent approaches of CoSAL will be explored.The challenge we now face is that two contradic-tory criteria ?
utility and costs ?
have to be bal-anced.2.3.1 Cost-Constrained SamplingCoSAL can be realized in the most straight-forward way by simply constraining the samplingto a particular maximum cost cmax per example.Therefore, in a pre-processing step all examplesp ?
P for which cost(p) > cmax are removed fromP .
The unmodified NER-specific AL frameworkcan then be applied.An obvious shortcoming of Cost-ConstrainedSampling (CCS) is that it precludes any form ofcompensation between utility and costs.
Thus, anexceptionally useful example with a cost factorslightly above cmax will be rejected.
Another crit-ical issue is how to fix cmax.
If chosen too low,the pre-filtering of P results in a much too strongrestriction of selection options when only few ex-amples remain inside P .
If chosen too high, thecost constraint becomes ineffective.2.3.2 Linear Rank CombinationA general solution to fit different criteria intoa single one is by way of linear combination.If, however, different units of measurement areused, a transformation function for the alignmentof benefit, or utility, and costs must be found.
Thiscan be difficult to determine.
In our scenario, ben-efits measured by utility scores and costs mea-sured in seconds are clearly incommensurable.
Asit is not immediately evident how to express utilityin monetary terms (or vice versa), we transformutility and cost information into ranks R(u(p, ?))andR?
(cost(p)) instead.
As for utility, higher util-ity leads to higher ranks.
As for costs, lower costslead to higher ranks.
The linear rank combination(LRK) is defined as?LRK(~v(p)) = ?R(u(p, ?))+(1??)R?
(cost(p))where ?
is a weighting term.
In a CoSAL sce-nario, where utility is the primary criterion, ?
>0.5 seems a reasonable choice.
Alternatively, ascosts and utility are contradictory, allowing equalinfluence for both criteria, as with ?
= 0.5, itmay be difficult to find appropriate examples ina medium-sized corpus.
Thus, the choice of ?
de-pends on size and diversity with respect to combi-nations of utility and costs within P .12492.3.3 Benefit-Cost RatioOur third approach to CoSAL is based on theBenefit-Cost Ratio (BCR).
Given equal units ofmeasurement for benefits and costs, the benefit-cost ratio indicates whether a scenario is profitable(ratio > 1).
BCR can also be applied when unitsare incommensurable and a transformation func-tion is available, as is the case for the combinationof utility and cost.
This holds as long as bene-fit and costs can be expressed in the same unitsby a linear transformation function, i.e., u(p, ?)
=?
?
cost(p) + b.
If such a transformation functionexists, one can refrain from finding proper valuesfor the above variables b and ?
and instead calcu-late BCR as?BCR(p) =u(p, ?
)cost(p)Since annotation costs are usually expressed ona linear scale, this is also required for utility, ifwe want to use BCR.
But when utility is basedon model confidence as we do it here, this prop-erty gets lost.1 Hence a non-linear transforma-tion function is needed to fit the scales of utilityand costs.
Assuming a linear relationship betweenutility and costs, BCR has already been appliedby Haertel et al (2008) and Settles et al (2008).Our approach provides a crucial extension as weexplicitly consider scenarios where such a linearrelationship is not given and a non-linear transfor-mation function is required instead.In a direct comparison of LRK with BCR, LRKmay be used when such a transformation functionwould be needed but is unknown and hard to find.Choosing LRK over BCR is also motivated byfindings in the context of data fusion in informa-tion retrieval where Hsu and Taksa (2005) remarkthat, given incommensurable units and scales, onewould do better when ranks rather than the actualscores or values were combined.3 ExperimentsIn the following, we study possible benefits ofCoSAL, relative to FuSAL and SeSAL, in the1Though normalized to [0, 1], confidence estimates, es-pecially for sequence classification, are often not on a linearscale so that confidence values that are twice as high do notnecessarily mean that the benefit in training a model on suchan example is doubled.light of real annotation times as a cost measure(instead of the standard, yet inadequate one, viz.the number of tokens being selected).
Such timingdata is available in the MUC7T corpus (Tomanekand Hahn, 2010), a re-annotation of the MUC7corpus containing the ENAMEX types (persons,locations, and organizations) and a time stamp re-flecting the time it took annotators to decide oneach entity type.
The MUC7T corpus contains3,113 sentences (76,900 tokens).The results we report on are averaged over 20independent runs.
For each run, we split theMUC7T corpus randomly into a pool to selectfrom (90%) and an evaluation set (10%).
AL wasstarted from a random seed set of 20 sentences.As utility scores to estimate benefits we applieduMA and uLC as defined in Section 2.1.The plots in the following sections depict costsin terms of annotation time (in seconds) relativeto annotation quality (expressed via F1-scores).Learning curves are only shown for early AL it-erations.
Later on, in the convergence phase, dueto the two conflicting criteria now considered si-multaneously, selection options become more andmore scarce so that CoSAL necessarily performssub-optimally.3.1 Parametrization of CoSAL ApproachesPreparatory experiments were run to analyze howdifferent parameters affected different CoSAL set-tings.
For the CCS and LRK experiments, weused the uLC utility function.For CCS, we tested three cmax values, viz.
7.5,10, and 15, to determine the maximum perfor-mance attainable on MUC7T when only examplesbelow the chosen threshold were included.
Ourchoices of the maximum were based on the dis-tributions of annotation times over the sentences(see Figure 1) where 7.5s marks the 75% quantileand 15s is just above the 90% quantile.
For 7.5s,we peaked at Fmax = 0.84, for 10s at Fmax =0.86, and for 15s at Fmax = 0.88.
Figure 2(top) shows the learning curves of CoSAL withCCS and different cmax values.
With cmax = 15,as could be expected from the boxplot in Fig-ure 1, no difference can be observed comparedto cost-insensitive FuSAL.
CCS with lower val-ues for cmax stagnates at the maximum perfor-1250secondsfrequency0 5 10 15 20 25 300200400600800Figure 1: Distribution of annotation times per sen-tence in MUC7T .mance reported above, but still improves uponcost-insensitive FuSAL in early AL iterations.At some point in time all economical exam-ples, with costs below cmax but high utility, havebeen consumed from the corpus.
Even in a cor-pus much larger than MUC7T this effect will onlyoccur with some delay.
Indeed, any choice of a re-strictive value for cmax will cause similar exhaus-tion effects.
Unfortunately, it is unclear how totune cmax suitably in a real-life annotation sce-nario where pretests for maximum performancefor a particular cmax are not possible.
For furtherexperiments, we chose cmax = 10.For LRK, we tested three different weights ?,viz.
0.5, 0.75, and 0.9.
Figure 2 (bottom) showstheir effects on the learning curves.
Similar ten-dencies as for cmax for CCS can be observed.With ?
= 0.9, CoSAL does not fall below defaultFuSAL, at least in the observed range.
A lowerweight of ?
= 0.75 results in larger improve-ments in earlier AL iterations but then falls backto FuSAL and in later AL iterations (not shownhere) even below FuSAL.
If the time parameteris granted too much influence, as with ?
= 0.5,performance even drops to random selection level.This might also be due to corpus exhaustion.
Forfurther experiments, we chose ?
= 0.75 becauseof its potential to improve upon FuSAL in earlyiterations.For BCR with uMA, we change this utility func-tion to n ?
uMA to compensate for the normaliza-1000 2000 3000 4000 5000 60000.700.750.800.85parameter test for CCSsecondsF?scoreCCS 15sCCS 10sCCS 7.5sFuSAL : uLCRS1000 2000 3000 4000 5000 60000.700.750.800.85parameter test for LRKsecondsF?scoreLRK 0.9LRK 0.75LRK 0.5FuSAL : uLCRSFigure 2: Different parameter settings for CCSand LRK based on FuSAL with uLC as utilityfunction.
FuSAL: uLC refers to cost-insensitiveFuSAL, CCS and LRK to the cost-sensitive ver-sions of FuSAL with the respective parameters.tion by token length which is otherwise alreadycontained in uMA(n is the length of the respectivesentence).
For uLC, the preparatory experimentsalready showed that this utility function does notbehave on a linear scale.
This is so because uLC isbased on P?
(~y|~x) for confidence estimation of thecomplete label sequence ~y.
Hence, a uLC scoretwice as high does not indicate doubled benefit forclassifier training.
Thus, we need a non-linear cal-ibration function to transform uLC into a properutility estimator on a linear scale so that BCR canbe applied.To determine such a non-linear calibrationfunction, the true benefit of an example p would1251l lllllllllllllllllllllllllll llllllll llllllllllllllllll llllllllllllllllll lll lllllllllll llllllllll lllllllllllllllllllllllll ll lllllllllll llllllllllllll lllll llll llllllllllllllllll lllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllll lllll lllllllllllllllllllllllllll0.5 0.6 0.7 0.8 0.9 1.002468uLCn?u MAcorr: 0.6494l lllllllllllllllllllllllllll llllllll llllllllllllllllllllllllllllllllll lll llllllllllllllllllllllllllllllllllllllllll llll lllllllllllllllllll llll lllllllll lll llllllllll lllllllllll llllllllllllllllllllllllllllllllllllll lllll llllllllllllllllll0e+00 1e+08 2e+08 3e+08 4e+08 5e+0802468e?
?uLCn?u MAcorr: 0.8959Figure 3: Scatter plots for (a) uLC versus n?uMA and (b) e?
?uLC versus n?uMAbe needed.
In the absence of such informa-tion, we consider n ?
uMA as a good approxima-tion.
To identify the relationship between uLC andn ?
uMA, we trained a model on a random subsam-ple from P ?
?
P and used this model to obtainthe scores for uLC and n ?
uMA for each examplefrom the test set T .2 Figure 3 (left) shows a scat-ter plot of these scores which provides ample evi-dence that the relationship between uLC and ben-efit is indeed non-linear.
As calibration functionfor uLC we propose f(p) = e??uLC(p).
Experi-mentally, we determined ?
= 20 as a good value.Figure 3 (right) reveals that e?
?uLC(p) is a betterutility estimator; the correlation with n ?
uMA isnow corr = 0.8959 and the relationship is closeto being linear.In Figure 4, learning curves for BCR with theutility function uLC and the calibrated functione?
?uLC(p) are compared.
BCR with the uncali-brated utility function uLC fails miserably (theperformance falls even below random selection).This adds credibility to our claim that while uLCmay be appropriate for ranking examples (as forstandard, cost-insensitive AL), it is inappropriatefor estimating true benefit/utility which is neededwhen costs are to be incorporated with the BCRmethod.
BCR with the calibrated utility e?
?uLC(p),in contrast, outperforms cost-insensitive FuSAL.For further experiments with BCR, we either ap-ply n?uMA or e?
?uLC(p) as utility functions.2We experimented with different sizes forP ?, with almostidentical results.1000 2000 3000 4000 5000 60000.700.750.800.85parameter test for BCRsecondsF?scoreBCR : e20?uLCBCR : uLCFuSAL : uLCRSFigure 4: Different parameter settings for BCR3.2 Comparison of CoSAL ApproachesWe compared all three approaches to CoSAL inthe parametrization chosen above for the utilityfunctions uMA and uLC.
Learning curves areshown in Figure 5.
Improvements over cost-insensitive AL are only achieved in early AL iter-ations up to 2,500s (for CoSAL based on uMA) or4,000s (for CoSAL based on uLC) of annotationtime.
This exclusiveness of early improvementscan be explained by the size of the corpus and, bythis, the limited number of good selection options.Since AL selects with respect to two conflictingcriteria, the pool P should be much larger to in-crease the chance for examples that are favorablewith respect to both criteria.12520 1000 2000 3000 4000 5000 60000.700.750.800.850.90utility function : uMAsecondsF?scoreCCS (10s)LRK (0.75)BCR : n ?
uMAFuSAL : uMARS0 1000 2000 3000 4000 5000 60000.700.750.800.850.90utility function : uLCsecondsF?scoreCCS (10s)LRK (0.75)BCR : e20?uLCFuSAL : uLCRSFigure 5: Comparison of CoSAL approaches for the utility functions uMA and uLC.
Baseline given byrandom selection (RS) and standard FuSAL with either uMA or uLC.Improvements for CoSAL based on uLC aregenerally higher than for uMA.
Moreover, cost-insensitive AL based on uLC does not exhibit anynormalization where, in contrast, uMA is normal-ized at least to the number of tokens per example.In CoSAL, both uLC and uMA are normalized bycosts, which is methodologically a more substan-tial enhancement for uLC than for uMA.For CoSAL based on uMA we cannot proclaima clear winner among the different approaches.All three CoSAL approaches improve upon cost-insensitive AL.
For CoSAL based on uLC, LRKperforms best, while CCS and BCR perform simi-larly well.
Given this result, we might prefer LRKor CCS over BCR.
A disadvantage of the first twoapproaches is that they require corpus-specific pa-rameters which may be difficult to find for a newlearning problem for which no data for experi-mentation is at hand.
Though not the best per-former, BCR does not require further parametriza-tion and appears more appropriate for real-life an-notation projects ?
as long as utility is an appro-priate estimator for benefit.
CoSAL with BCR hasalready been studied by Settles et al (2008).
Theyalso applied a utility function based on sequence-confidence estimation which presumably, as withour uLC utility function, is not a good benefit esti-mator.
The fact that Settles et al did not explicitlytreat this issue might explain why cost-sensitiveAL based on BCR often performed worse thancost-insensitive AL in their experiments.3.3 CoSAL Applied to SeSALWe looked at a cost-sensitive version of SeSAL byapplying the cost-sensitive FuSAL approach to-gether with BCR and the transformation functionfor the utility as discussed above.
On top of thisselection, we ran the standard SeSAL approach ?only tokens below a confidence threshold were se-lected for annotation.
The following experimentsare all based on the uLC utility function (and thetransformation function of it).Figure 6 depicts learning curves for cost-insensitive and cost-sensitive SeSAL and FuSALwhich reveal that cost-sensitive SeSAL consid-0 1000 2000 3000 4000 5000 60000.700.750.800.850.90secondsF?scoreSeSAL BCRFuSAL BCRSeSALFuSALRSFigure 6: Cost-sensitive (BCR variants) vs. cost-insensitive FuSAL and SeSAL with uLC as utilityfunction.1253erably outperforms cost-sensitive FuSAL.
Cost-sensitive SeSAL attains a target performance ofF=0.85 with only 2806s, while cost-sensitiveFuSAL needs 3410s, and random selection con-sumes over 6060s.
Thus, cost-sensitive SeSALhere reduces true annotation time by about 54 %compared to random selection, whereas cost-sensitive FuSAL reduces annotation time by only44 %.4 Related WorkAlthough the reduction of data acquisition coststhat result from human labeling efforts have al-ways been the main driver for AL studies, cost-sensitive AL is a new branch of AL.
In an earlystudy on cost metrics for AL, Becker and Osborne(2005) examined whether AL, while decreasingthe sample size on the one hand, on the otherhand increased annotation efforts.
For a real-world AL annotation project, they demonstratedthat the actual sampling efficiency measure foran AL approach depends on the cost metric be-ing applied.
In a companion paper, Hachey et al(2005) studied how sentences selected by AL af-fected the annotators?
performance both in termsof the time needed and the annotation accuracyachieved.
They found that selectively sampled ex-amples are, on the average, more difficult to anno-tate than randomly sampled ones.
This observa-tion, for the first time, questioned the widespreadassumption that all annotation examples can be as-signed a uniform cost factor.Making a standard AL approach cost-sensitiveby normalizing utility in terms of annotation timehas been proposed before by Haertel et al (2008),Settles et al (2008), and Donmez and Carbonell(2008).
CoSAL based on the net-benefit (costssubtracted from utility) was proposed by Vijaya-narasimhan and Grauman (2009) for object recog-nition in images and Kapoor et al (2007) for voicemessage classification.5 ConclusionsWe investigated three approaches to incorporatethe notion of cost into the AL selection mecha-nism, including a fixed maximal cost budget perexample, a linear rank combination to express net-benefit, and a benefit-cost ratio.
The cost metricwe applied was the time needed by human codersfor annotating particular annotation examples.Among the three approaches to cost-sensitiveAL, we see a slight advantage for benefit cost ra-tios in real-world settings because they do not re-quire additional corpus-specific parametrization,once a proper calibration function is found.Another observation is that advantages ofthe three cost-sensitive AL models over cost-insensitive ones consistently occur only in earlyiteration rounds ?
a result we attribute to corpusexhaustion effects since cost-sensitive AL selectsfor two criteria (utility and cost) and thus requiresa extremely large pool to be able to pick up reallyadvantageous examples.
Consequently, appliedto real-world annotation settings where the poolsmay be extremely large, we expect cost-sensitiveapproaches to be even more effective in terms ofthe reduction of annotation time.To be applicable in real-world scenarios, anno-tation costs which, in our experiments, were di-rectly traceable in the MUC7T corpus have to beestimated since they are not known prior to anno-tation.
In Tomanek et al (2010), we investigatedthe reading behavior during named entity annota-tion using eye-tracking technology.
With the in-sights gained from this study on crucial factors in-fluencing annotation time we were able to inducesuch a much needed predictive model of annota-tion costs.
In future work, we plan to incorporatethis empirically founded cost model into our ap-proaches to cost-sensitive AL and to investigatewhether our positive findings can be reproducedwith estimated costs as well.AcknowledgementsThis work was partially funded by the EC withinthe CALBC (FP7-231727) project.ReferencesBecker, Markus and Miles Osborne.
2005.
A two-stage method for active learning of statistical gram-mars.
In IJCAI?05 ?
Proceedings of the 19th Inter-national Joint Conference on Artificial Intelligence,pages 991?996.
Edinburgh, Scotland, UK, July 31 -August 5, 2005.1254Donmez, Pinar and Jaime Carbonell.
2008.
Proactivelearning: Cost-sensitive active learning with mul-tiple imperfect oracles.
In CIKM?08 ?
Proceed-ing of the 17th ACM conference on Informationand Knowledge Management, pages 619?628.
NapaValley, CA, USA, October 26-30, 2008.Engelson, Sean and Ido Dagan.
1996.
Minimizingmanual annotation cost in supervised training fromcorpora.
In ACL?96 ?
Proceedings of the 34th An-nual Meeting of the Association for ComputationalLinguistics, pages 319?326.
Santa Cruz, CA, USA,June 24-27, 1996.Hachey, Ben, Beatrice Alex, and Markus Becker.2005.
Investigating the effects of selective samplingon the annotation task.
In CoNLL?05 ?
Proceed-ings of the 9th Conference on Computational Natu-ral Language Learning, pages 144?151.
Ann Arbor,MI, USA, June 29-30, 2005.Haertel, Robbie, Kevin Seppi, Eric Ringger, and JamesCarroll.
2008.
Return on investment for activelearning.
In Proceedings of the NIPS 2008 Work-shop on Cost-Sensitive Machine Learning.
Whistler,BC, Canada, December 13, 2008.Hsu, Frank and Isak Taksa.
2005.
Comparing rank andscore combination methods for data fusion in infor-mation retrieval.
Information Retrieval, 8(3):449?480.Kapoor, Ashish, Eric Horvitz, and Sumit Basu.
2007.Selective supervision: Guiding supervised learningwith decision-theoretic active learning.
In IJCAI?07?
Proceedings of the 20th International Joint Con-ference on Artifical Intelligence, pages 877?882.Hyderabad, India, January 6-12, 2007.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML?01 ?
Proceedings of the18th International Conference on Machine Learn-ing, pages 282?289.
Williamstown, MA, USA, June28 - July 1, 2001.Settles, Burr and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In EMNLP?08 ?
Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 1069?1078.
Waikiki, Hon-olulu, Hawaii, USA, October 25-27, 2008.Settles, Burr, Mark Craven, and Lewis Friedland.2008.
Active learning with real annotation costs.
InProceedings of the NIPS 2008 Workshop on Cost-Sensitive Machine Learning.
Whistler, BC, Canada,December 13, 2008.Tomanek, Katrin and Udo Hahn.
2009.
Semi-supervised active learning for sequence labeling.
InACL/IJCNLP?09 ?
Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of Natu-ral Language Processing, pages 1039?1047.
Singa-pore, August 2-7, 2009.Tomanek, Katrin and Udo Hahn.
2010.
Annotationtime stamps: Temporal metadata from the linguisticannotation process.
In LREC?10 ?
Proceedings ofthe 7th International Conference on Language Re-sources and Evaluation.
La Valletta, Malta, May 17-23, 2010.Tomanek, Katrin, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus constructionwhich cuts annotation costs and maintains cor-pus reusability of annotated data.
In EMNLP-CoNLL?07 ?
Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Language Learning,pages 486?495.
Prague, Czech Republic, June 28-30, 2007.Tomanek, Katrin, Udo Hahn, Steffen Lohmann, andJu?rgen Ziegler.
2010.
A cognitive cost model of an-notations based on eye-tracking data.
In ACL?10 ?Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics.
Uppsala,Sweden, July 11-16, 2010.Tomanek, Katrin.
2010.
Resource-Aware Annotationthrough Active Learning.
Ph.D. thesis, TechnicalUniversity of Dortmund.Vijayanarasimhan, Sudheendra and Kristen Grauman.2009.
What?s it going to cost you?
predicting ef-fort vs. informativeness for multi-label image anno-tations.
CVPR?09 ?
Proceedings of the 2009 IEEEComputer Vision and Pattern Recognition Confer-ence.1255
