A One Pass Decoder Design For Large VocabularyRecognitionJ .
J .
Odel l ,  V. Va l tchev,  P.C.
Wood land ,  S. J .
YoungCambr idge  Un ivers i ty  Eng ineer ing  Depar tmentT rumpington  Street ,  Cambr idge ,  CB2 1PZ, Eng landABSTRACTTo achieve reasonable accuracy in large vocabulary speechrecognition systems, it is important o use detailed acous-tic models together with good long span language models.For example, in the Wall Street Journal (WSJ) task bothcross-word triphones and a trigram language model are neces-sary to achieve state-of-the-art performance.
However, whenusing these models, the size of a pre-compiled recognitionnetwork can make a standard Viterbi search infeasible andhence, either multiple-pass or asynchronous stack decodingschemes are typically used.
In tl:fis paper, we show that time-synchronous one-pass decoding using cross-word triphonesand a trigram language model can be implemented using adynamically built tree-structured network.
This approachavoids the compromises inherent in using fast-matches or pre-liminary passes and is relatively efficient in implementation.It was included in the HTK large vocabulary speech recog-nition system used for the 1993 ARPA WSJ evaluation andexperimental results are presented for that task.1.
INTRODUCTIONHidden Markov Models (HMMs) have been used suc-cessfully in a wide variety of recognition tasks rangingfrom small isolated word systems assisted by heavilyconstrained grammars to very large vocabulary uncon-strained continuous peech systems.
Part of the successof HMMs is due to the existence of computationally ef-ficient algorithms for both the training of the models(the Baum-Welch algorithm) and for the decoding of un-known utterances (the Viterbi algorithm).
However, asrecognition tasks have become more complex, the decod-ing process has become more difficult due to the increas-ing size of network needed in a conventional Viterbi de-coder.
In particular, using cross-word triphones or longspan language models can lead to order of magnitudeincreases in the size of a static network.A variety of schemes have been proposed to reduce thecomputation required for recognition \[2,7\].
Most makeuse of fast-matches or preliminary passes using simpli-fied acoustic or linguistic models to constrain the searchspace for a final pass that uses the most detailed andaccurate models available.
Unfortunately, these pre-liminary matches can introduce errors that subsequentpasses are unable to correct.
If the first pass could usethe best acoustic and language models available it wouldallow greater constraints to be placed on the search spacewithout increasing the error rate.
This paper describesa scheme which allows this through the use of a dy-namically built tree-structured network.
This approachavoids the compromises inherent in using fast-matchesor preliminary passes and is both simple and efficient oimplement.The remainder of this paper is organised as follows.
Sec-tion 2 discusses the main features of conventional time-synchronous Viterbi decoding and some of the ways inwhich it can be improved.
In section 3, a one-pass de-coder that implements a beam-pruned Viterbi searchthrough a tree-structured dynamic network is then de-scribed.
Section 4 presents ome experimental resultson the Wall Street Journal task and, finally, section 5presents our conclusions on this work.2.
V ITERBI  DECODING2.1.
The  Standard  V i te rb i  SearchThe standard method of implementing a Viterbi searchfor decoding speech into words is to build a re-entrantnetwork of HMM phone instances.
An instance of eachword in the vocabulary is then made from a concatenatedsequence of phone instances and the end of each word in-stance is linked, according to a language model or gram-mar, back to the start of the word instances.
Decodinguses a time-synchronous Viterbi search through this net-work in which partial state/frame alignment paths aremaintained and extended in parallel using the Viterbicriterion (i.e.
the principle of dynamic programming).In our work, each path is denoted by a token so thatpath extension can be represented simply by propagat-ing tokens through the network \[12\].A complete Viterbi search is admissible and will notmake any search errors whereby the decoder fails to cor-rectly find the most likely sequence of models for a givenutterance.
Unfortunately, because of the size of thesenetworks, a complete search is not computationally fea-sible even for moderately sized tasks.
Consequently it is405Language Model Acoustic Model0 ?, ,?,0- -Figure 1: A Static Network for Viterbi Decoding with aBack-off Bigram Language Modelnecessary to reduce (or prune) the search space to speed-up the search.
A common strategy for doing this is touse a beam search in which only paths whose likelihoodfalls within a fixed beam width of the mostly likely pathare considered for extension.
As an example of this kindof approach, Fig.
1 shows a typical static network struc-ture for word recognition using monophone models anda bigram-backoff language model \[4\].2 .2 .
The  L imi ta t ions  o f  S ta t i c  NetworksRecent uses of HMMs in very large vocabulary tasks havebegun to show the deficiencies of the static network ar-chitecture.
Such systems typically have a large numberof context-dependent models and they use unconstrainedlong span statistical language models.
As is clear fromFig.
1, the number of individual phone instances caleslinearly with the size of the vocabulary whilst the num-ber of possible cross links, for the bigram case, scaleswith the square of the number of words.
For the trigramcase, matters are even worse and each word pair thathas trigram language model probabilities for subsequentwords must be explicitly represented in the network.
De-pending on the size of the language model, this couldresult in a dramatic increase in the size of the network.Similarly, increasing the number of phone models usedcan lead to correspondingly large increases in the size ofnetwork needed to decode them.
For example, if cross-word context dependent triphone models are used duringrecognition, the size of the recognition etwork increasessubstantially.
Rather than having a single model tran-script for each word, the initial (and final) phone mustbe replaced by one of a set of models.
The size of theset depends on which phones may occur at the end ofthe preceding (or start of the following) word.
Normallythis is around the same size as the phone set and so anaverage word will require almost 100 model instances.To give a specific example, the Dragon Wall Street Jour-nal Pronunciation Lexicon Version 2.0 dictionary con-tains 19979 words with 21875 pronunciations.
Using aset of 44 distinct phone models, this leads to a total of air-proximately 150,000 phone instances.
If cross-word tri-phones are used, the number of required phone instancesrises to around 1,800,000.
Finally, the standard WSJ20k trigram language model has approximately 740,000word-pairs with trigram probabilities.
If this is used ina static network structure, the number of needed phoneinstances rises to around 24,000,000.2 .3 .
S tack  DecodersMany of the pressures leading to increased network sizeresult directly from the breadth-first nature of the time-synchronous Viterbi search.
An obvious alternative istherefore to use a depth-first scheme.
For example, stackdecoders have been used with some success \[5,9\].
In astack decoder, hypotheses are advanced one word at atime and hence long span language models and contextdependency can be supported without the need for largenetworks.
However, the indeterminacy of word bound-aries in continuous peech and the variety of possibleright contexts at the ends of words means that, in prac-tice, a stack decoder must use a preliminary fast-matchto reduce the number of path extensions considered.This is a major drawback since fast-matches can be aserious source of search errors.2 .4 .
Reduc ing  Computat ionIf a time-synchronous beam search is to be used, thenthe pruning strategy needs to be as effective as possible.In practical systems, variable width and multiple beamschemes are more effective than using a single fixed beamwidth\[I,6\].
For example, a higher degree of uncertaintyexists at the start of words than at the end.
Hence, usingan additional word-end beam can result in substantiallyfewer active models without significantly increasing theerror rate.The use of such an additional beam can also be justifiedon the basis of the language models employed.
Althoughthe probabilities in a language model may vary by sev-eral orders of magnitude, the probability of a particularword actually varies much less.
For example, in the stan-dard 5k closed vocabulary bigram WSJ language model,the highest and the lowest probabilities vary by a factorof 107 .
However, on average over 99.5% of the proba-bilities for a particular word lie within a factor of 100of each other.
This is due to the very heavy reliance onthe back-off component of the language model.
It meansthat few search errors will be introduced by only propa-gating word-end tokens for which the likelihood is within406a factor of 100 or so of the most likely word-end token.This implies that the word-end beam width can be muchnarrower than the width of the normal beam.2 .5 .
T ree  S t ructur ingSince the uncertainty in decoding speech is much higherat the start of words than at the ends, it follows that themajority of the computation is expended on the first fewphones of each word \[8\].
For very large vocabularies, atree structured network in which the words that sharecommon initial phonesequences share model instances,achieves the dual aim of reducing the size of the networkas well as the computation required to decode it.
Hence,a tree-structured organisation is highly desirable.When using a tree-structured organisation, it is impor-tant to ensure that, although word identity is not ex-plicitly known until the end of the word is reached, theapplication of the language model is not delayed untilthen.
If this is not done, the relaxation of the constraintson the search can offset the computational savings andwill require the use of larger beam widths to avoid searcherrors \[3\].3.
A ONE-PASS DECODERFrom the discussion in the previous section, it is clearthat the key features of a successful single-pass decod-ing scheme are the ability to incorporate cross-word tri-phones and long span language models whilst keepingboth the computational time and space requirementswithin acceptable bounds.
To do this, it is clearly nec-essary to tree-structure the recognition etwork and toapply tight and efficient pruning.
To make this possi-ble, the concept of a static re-entrant network must beabandoned.
Instead, a non re-entrant ree-structurednetwork must be used with a new copy of the tree beingreplicated at every word end.
To make this fit in avail-able memory, the network must be grown dynamicallyon-the-fly and once phone instances fall outside of thebeam, the corresponding nodes must be reclaimed.This section describes uch a decoder.
It uses the tokenpassing paradigm to implement a beam pruned Viterbisearch through a dynamic tree structured network ofHMM instances.3 .1 .
Network  S t ructureDue to the tree-structured nature of the network and thepossibility that two words may have exactly the samephonetic realisation (and will therefore share all theirmodels) it is necessary to have some point at which theidentity of a word becomes unique.
Consequently therecognition etwork consists of two types of nodes.?
HMM instances.
These represent an actual phonefrom the dictionary and are linked to a physicalHMM (the identity of which may depend on the con-text).
The HMM is used to calculate the acousticlikelihood of that phone instance and the networknode holds the tokens that store the associated like-lihoods and paths.?
Word-ends.
These are linked to a particular wordand are the points where the language model likeli-hoods are added to the acoustic likelihoods.The HMM instances are connected in a simple tree struc-tured network in which each model has a specific prede-cessor but may have many followers.
Word-end nodesare also linked to each other when token recombinationor domination can occur(see below).
Fig 2 shows a frag-ment of a typical network configuration.Each node in the network has an associated languagemodel probability.
This is added to the token likelihoodto give the combined likelihood that is used for pruningpurposes.
At word-ends, the language model can pro-vide the exact probablitiy for the word given its history.However, HMM phone instances can be shared by manywords and hence only an approximation for the languagemodel probability can be used within a word.
Therefore,until the word identity is uniquely defined, the highestlanguage model probability of all words that share theinstance is used.
This guarantees that the probabilityused is always an exact upper bound on the actual prob-ability and this helps to minimise search since it cannever increase through a word.
Using the exact upperbound allows the tightest beam widths to be used with-out introducing search errors.
The overhead for dynam-ically constructing the network and for using the exactlanguage model for calculating the upper bound on like-lihoods is relatively small and rarely exceeds 20% of thetotal computational load.3 .2 .
Recombinat ion  and  DominanceWhen a static network is used for recognition, tokens re-combine at the start of each word and only one survivesand is propagated into the word.
These recombinationpoints are where the Viterbi criterion is applied to decidewhich of the set of tokens is part of the maximum likeli-hood path.
In general, three conditions must be fulfilledto allow tokens to recombine?
The following network must be identical in struc-ture.?
Corresponding network nodes must have the sameacoustic likelihoods.407iNEAR SPOKEFigure 2: A Fragment of a Tree Structured Network?
Corresponding network nodes must have the samelanguage model likelihoods.As word-ends are created, they are linked to any existingword-ends that meet these conditions to form dominancechains.
The Viterbi criterion means that only the mostlikely token in the word-end nodes on one of these chainswill form part of the most likely path.
Due to the factthat the network is dynamically constructed, there aretwo ways in which this can be applied.?
Recombination.
The word-end nodes on a domi-nance chain share a single set of successor nodes.The most likely token on the dominance chain isthen propagated into the following models as in astandard Viterbi search.
Note that this implies thatthe token itself must contain traceback informationsince it may have come from one of a number ofpaths.?
Dominance.
Each word-end on the dominance chaincan have its own set of successor nodes.
Howeveronly the most likely word-end on the chain is allowedto create any successor nodes.
Thus, each networknode will have a unique history and so tracebackinformation can be held at the network level ratherthan at token level.At first it may seem that token recombination is themost sensible course since domination can lead to thecreation of multiple copies of network structure when asingle one would suffice.
In practice, however, this hap-pens very rarely and holding the traceback informationat the network level rather than in each token meansthat each HMM instance can be more compact.
Thereis thus a trade-off between memory usage and compu-tation required.
In practice, using domination ratherthan recombination leads to around 5-10% more activemodels (and hence computation) but results in a 10-20%reduction in the memory required to hold the network.The token recombination method does have one distinctadvantage.
As explained later in this paper, it is possibleto produce a lattice of word hypotheses rather than thesingle best sentence for very little extra computationaleffort if each token contains traceback information.3.3.
Network Construct ionThe network is constructed ynamically and nodes areonly created when they will fall into the beam andare destroyed as soon as they leave the beam.
In thiscase, pruning becomes doubly important since it con-trols not only the amount of computation used but alsothe amount of memory required.Network growth occurs during model-external tokenpropagation.
The network is extended if the combinedlikelihood of a token has no following node and fallswithin the beam.
Since the combined likelihood of thetoken after propagation i to the newly created node willdepend on the language model likelihood of that node,this combined likelihood is calculated in advance to pre-vent network nodes being created unnecessarily.
Theadditional computation involved in doing this is muchless than the memory creation/disposal overhead wouldotherwise be.When nodes are constructed, the identity of the phoneand the word in which it occurs, together with its context(at both the phone and the word level), the positionof surrounding word boundaries and the gender of thespeaker may all be used to select which HMM will beused for that node.
This allows the use of function wordspecific, position dependent, gender dependent and long408distance context dependent phonetic models.Pruning is at the model level and occurs in several ways.?
Blocking.
Tokens that fall outside of the beamare blocked to prevent network growth from occur-ring.
Word-end nodes have their own separate beamwhich is also checked before network growth from aword-end node occurs.?
Erasure.
A node that falls outside the beam andwhich has no predecessors is erased.
The space allo-cated to the node is freed, the node is removed fromthe network and will never be re-created.?
Deletion.
A node that falls outside of the beamand which has no followers is deleted.
This involvesfreeing the node in such a way that it may be re-created if it comes back into the beam.?
Halting.
A node that falls outside the beam whichhas predecessors and followers is halted.
The nodeis not removed from the network (since it wouldbe difficult to re-create and link back into the cor-rect place) but is marked as inactive.
Internal tokenpropagation does not occur for inactive nodes andso, although it is using memory, it requires littlecomputation.All other nodes fall within the beam and are active.
Bothinternal and external token propagation will occur forthese models.
The computational load varies approxi-mately linearly with the number of active models.3 .4 .
N -Best  Lat t i cesDuring decoding there are multiple copies of each wordactive and it is therefore possible to generate multiplesentence hypotheses with little additional computation.This is implemented by linking the tokens that occurin chained word-end nodes and propagating the mostlikely token (which is the head of this list) into followingnodes exactly as before.
The multiple hypotheses canbe recovered by descending the list at each boundary atthe end of the utterance.
This will not generate xactsolutions for any but the best path since it implicitlyassumes that the start time of each word is independentof all words before its immediate predecessor.
However,it has been shown that this is a reasonable assumptionand has little effect on overall lattice accuracy \[10\].3 .5 .
One-Pass  A lgor i thmThe above one-pass decoder strategy has been imple-mented in a simple three step algorithm:-?
Create a single sentence_start node for each gender-dependent model set.?
For each frame of input- Prune from the network all models for whichthe combined acoustic and language modellikelihood falls outside of the beam.- Perform token propagation within each HMMinstance and find the top of the beam for thenext time step.- Perform token propagation between odes ex-tending the network when this is necessary.?
Find the most likely sentence_end ode and trace-back to find the resulting word sequence.4.
EXPERIMENTAL  RESULTSExperiments have been performed on both 5k and 20kWall Street Journal tasks.
The WSJ systems used train-ing data from the SI-84 and SI-284 test sets, and the pro-nunciations from the Dragon Wall Street Journal Pro-nunciation Lexicon Version 2.0 together with the stan-dard bigram and trigram language models supplied byMIT Lincoln Labs.
Some locally generated additions andcorrections to the dictionary were used and the stressmarkings were ignored resulting in 44 phones plus si-lence.
Data preparation used the HTK Hidden MarkovModel Toolkit \[13\].
All speech models had three emittingstates and a left-to-right opology and used continuousdensity mixture Gaussian output probability distribu-tions tied at the state level using phonetic decision trees\[14\].
The decoder enforced silence at the start and endof sentences and allowed optional silence between words.These systems achieved the lowest error rates reportedfor the November 1993 WSJ evaluations on the H1-C2,H2-C1 and H2-P0 and the second lowest error rate onH1-P0.
Further details about these systems can be foundin \[11\].Table 1 gives details of decoder performance for the var-ious tasks.
All figures quoted are for the beam widthsused in the evaluation tests.
The required computationscales with the number of active models per frame (andthe number of frames in the test set) and on an HP735decoding the 5k gender dependent cross-word systemsrequired approximately 10 minutes per sentence whilstthe 20k systems took about 15 minutes per sentence (onaverage).
As the table shows, the computation requireddoes not depend on the potential network size since theload for the trigram case is generally less than the corre-sponding bigram case.
This shows that the early appli-cation of knowledge can be used to constrain the searchin order to offset the computational costs of using the409SystemTypeWord Internal GICross Word GDTrainingDataSI84SI84Task5k Bigram5k BigramNumber of States,Models & Triphonesper gender3701 / 8087 / 143443820 / 15303 / 35633PotentialNetwork Sizeper gender20k Bigram40,000400,000Average Numberof Active Modelsper frame960021900Cross Word GD SI284 5k Bigram 7558 / 22978 / 35633 400,000 23400Cross Word GD SI284 5k Trigram 7558 / 22978 / 35633 5,000,000 19800Cross Word GD SI284 i 30700Cross Word CD1,800,00024,000,0007558 / 22978/544577558 / 22978 / 54457 20k Trigram SI284 29300WordErrorRate12.58.76.84.914.412.7Table 1: System characteristics for various WSJ tasks.knowledge.
In the bigram case, no reliance is made onthe back-off nature of the language model and the com-putational load will not therefore change when the sizeof the language model is increased.5.
CONCLUSIONSOne-pass decoding has many advantages over multi-passdecoding if it can be accomplished with a similar amountof computation.
This paper has described a methodof decoding continuous peech using context-dependenthidden Markov models and long span language modelsin a single pass.
The decoder is relatively simple and ef-ficient and should scale well with increasing size of boththe vocabulary and the language models.6.
ACKNOWLEDGEMENTSThe WSJ pronunciation dictionary was provided byDragon Systems Inc. J. Odell is funded by a SERC stu-dentship and part of this work was funded by SERCgrant GR/J10204.References1.
Alleva F, Hon H, Huang X, Hwang M, Rosenfeld It,Weide R (1993).
Applying SPHINX-H to the DARPAWall Street Journal CSR Task.
Proc.
DARPA Speechand Natural Language Workshop 1992, pp 393-398.2.
Alleva F, Huang X, Hwang M-Y (1993).
An ImprovedSearch Algorithm Using Incremental Knowledge /orContinuous Speech Recognition.
Proc.
ICASSP'93, VolII, pp.307-310.
Minneapolis.3.
Aubert X, Dugast C, Ney H, Steinbiss V (1994).Large Vocabulary Continuous Speech Recogniiton ofWall Street Journal Data.
Proc.
ICASSP'94 fortlw~om-ing.
Adelaide.4.
Austin S, Peterson P, Plaeeway P, Schwartz R, Vander-grift J (1990).
Towards a Real-Time Spoken LanguageSystem Using Commercial Hardware.
Proc.
DARPASpeech and Natural Language Workshop 1990, pp 72-77.5.
Jelinek F, Bahl LR, Mercer RL.
Design of a Linguis-tic Statistical Decoder for the Recognition of ContinuousSpeech.
IEEE Trans Information Theory, Vol 21, No 3,pp250-256, 19756.
Lacouture R, Normandin Y (1993).
Efficient Lexical Ac-cess Strategies.
Proc.
Eurospeech'93, Vol III, pp.
1537-1540.
Berlin.7.
Murveit H, Butzberger J, Digalakis V, Weintraub M(1993).
Large-Vocabulary Dictation Using SRI's Deci-pher Speech Recognition System: Progressive SearchTechniques.
Proc.
ICASSP'93, Vol II, pp.319-322.
Min-neapolis.8.
Ney H, Haeb-Umbach R, Tran B-H & Oerder M. (1992).Improvements in Beam Search for 10000-Word Contin-uous Speech Recognition.
Proc.
ICASSP'92, Vol I, pp.9-12.
San Francisco.9.
Paul D, Necioglu B (1993).
The Lin-coln Large- Vocabulary Stack-Decoder HMM CSR.
Proc.ICASSP'93, Vol II, pp.660-663.
Minneapolis.10.
Schwartz R, Austin S (1990).
Efficient, High-Performance Algorithms for N-Best Search.
Proc.DARPA Speech and Natural Language Workshop 1990,pp 6-11.11.
Woodland PC, Odell J J, Valtchev V, Young SJ (1994).Large Vocabulary Continuous Speech Recognition UsingHTK.
Proc.
ICASSP'94 forthcoming, Adelaide.12.
Young S J, Russell NH, Thornton JHS (1989) To-ken Passing: A Simple Conceptual Model \[for Con-nected Speech Recognition Systems.
Technical ReportCUED/F-INFENG/TR38, Cambridge University Engi-neering Dept.13.
Young SJ (1993).
The HTK Hidden Markov ModelToolkit: Design and Philosophy.
TR 152, CambridgeUniversity Engineering Dept, Speech Group.14.
Young S J, Odell J J, Woodland PC (1994).
Tree-basedState Tying for High Accuracy Acoustic Modelling.
Proc.ARPA Human Language Technology Workshop 1994.410
