Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 482?486,Dublin, Ireland, August 23-24, 2014.RoBox: CCG with Structured Perceptron for Supervised SemanticParsing of Robotic Spatial CommandsKilian EvangUniversity of Groningenk.evang@rug.nlJohan BosUniversity of Groningenjohan.bos@rug.nlAbstractWe use a Combinatory Categorial Gram-mar (CCG) parser with a structured per-ceptron learner to address Shared Task 6of SemEval-2014, Supervised SemanticParsing of Robotic Spatial Commands.Our system reaches an accuracy of 79%ignoring spatial context and 87% usingthe spatial planner, showing that CCG cansuccessfully be applied to the task.1 IntroductionWhen interpreting utterances, humans use worldknowledge whereas most semantic parsers to daterely purely on linguistic clues.
Shared Task 6 inthe SemEval 2014 campaign for semantic evalua-tion aims to integrate reasoning about microworldswith semantic parsing.
In this task, a systemis given an instruction for a robot and has toproduce an executable semantic representation inRobot Control Language (Dukes, 2013a, RCL).The Robot Commands Treebank (Dukes, 2013b)is used for training and evaluation.
We partici-pated in this shared task with a system rooted inCombinatory Categorial Grammar (CCG).
In par-ticular, we were interested in finding out whetherexisting techniques for automatically deriving cat-egorial grammars with semantics could be movedeasily to the new domain of robot commandsand integrated with the provided spatial reasoningcomponent.
In this paper we outline our methodand present the results for this shared task.12 Extracting a CCG from RCLCCGs (Steedman, 2001) use a small set of atomicconstituent categories such as S (sentence), NPThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1Our code is available at http://www.let.rug.nl/evang/RoBox.zip(noun phrase) or PP (prepositional phrase).
Con-stituents that take other constituents as argumentshave complex categories describing their combi-natory potential.
For example, an intransitive En-glish verb has category S\NP , meaning that itforms a sentence by combining with an NP to itsleft.
Similarly, modifiers also have complex cate-gories.
For example, a pre-sentential adverb mighthave category S/S because it combines with asentence to its right to form a modified sentence.The combinatory rules that license these exam-ple combinations are called backward applicationand forward application.
They and other combi-natory rules also allow for constituents to be asso-ciated with semantic expressions, and specify howto form a combined semantic expression for thederived larger constituent.In this section, we describe a process that takesan RCL corpus as input and produces a set of CCGlexical entries, i.e.
natural-language words pairedwith categories and semantic expressions.
Thegoal is for these lexical entries to produce the cor-rect semantics under CCG combinatory rules alsofor unseen robotic commands.2.1 Transforming the TreesRCL expressions are rooted ordered trees whosenodes are labeled with tags.
We will write them inthe form (t:h) where t is the root tag and h is thesequence of subtrees of the root?s children.
Leavesare abbreviated as just their tags.
In each trainingexample, each pre-terminal (parent of a leaf) canbe aligned to one or more words in the correspond-ing natural language expression.
An example isshown in Figure 1.
Since the alignments to wordsare not crossing, we can interpret the RCL tree asa phrase structure tree for the sentence and use thealgorithm of (Hockenmaier and Steedman, 2007)to translate it to CCG.
We extend the algorithmwith a semantic step that makes sure the deriva-tions would produce the original RCL expressions.482(event :(action :move)(entity :(color :green)(type :prism))(destination :(spatial -relation :(relation :within)(entity :(indicator :back)(indicator :left)(type :corner))))eventaction:hmovemove theentity:ccolor:agreengreentype:hprismpyramiddestination:cspatial-relation:hrelation:hwithinin theentity:cindicator:abackbottomindicator:aleftlefttype:hcornercornereventevent/destination(event/destination)/entity:(event : (action : move))move theentityentity/entity:(color : green)greenentity:(entity : (type : prism))pyramiddestinationspatial-relationspatial-relation/entity:(spatial-relation : (relation : within))in theentityentity/entity:(indicator : back)bottomentityentity/entity:(indicator : left)leftentity(entity : (type : corner))cornerFigure 1: Top: an RCL expression.
Middle: its representation as a tree diagram.
Internal nodes areannotated with constituent types.
Pre-terminals are aligned to words in a corresponding natural-languageexpression.
Bottom: result of the CCG transformation.The procedure is as follows:1.
Determine constituent types.
We treataction , relation and type constituents as heads,entitys and destinations as complements (i.e.arguments) and cardinals, colors, indicators,measures and spatial -relations as adjuncts (i.e.modifiers).
For sequence nodes that have multipleevent children, we treat the first as head and therest as adjuncts.
A corresponding constituent typelabel h, a or c is added to the label of each internalnode (cf.
Figure 1, middle).2.
Assign lexical semantics.
To the label ofeach pre-terminal, add an RCL expression whichis a copy of a connected subgraph of the tree itself(without the constituent type labels).
For a-typeand c-type pre-terminals, the subgraph includesonly the pre-terminal and its daughter.
For h-typepre-terminals the parent is also included, as wellas any subtrees with root tag id or reference-idthe parent may have.To illustrate, the label ofthe action:h node in our example becomes ac-tion:h:(event : (action : move)), and color:a be-comes color:a:(color :green).
The leaves are nowno longer needed, so we remove them.3.
Add sequence nodes.
If the root is taggedsequence, add an additional node tagged sequencebetween each child and the root.4.
Binarize the tree.
Each local tree withmore than two daughters is binarized by insert-ing dummy nodes, provisionally labeled C : hwhere C is the tag of the parent.
Left adjuncts(such as the first indicator in Figure 1) are splitoff first, followed by right adjuncts (such as thedestination in Figure 1), left complements andright complements.5.
Assign CCG categories.
Starting from theroot, the tag of each node is replaced by a CCGcategory.
For simplicity, we directly use RCL tagsas atomic categories rather than mapping them tostandard CCG categories:The root gets its tag (event or sequence) as cat-egory.c-type nodes get their tag as category.
Their sib-ling gets category P/T if it is on the left and P\Tif it is on the right, where T is the tag of the c-type node and P is the category of the parent.
Forexample, the destination node in Figure 1 getsdestination as category, and its left sibling there-483fore gets event/destination because the parent?scategory is event .a-type nodes such as the two indicators in Fig-ure 1 get category P/P if they are on the leftof their sibling and P\P if they are on its right,where P is the category of their parent.
The sib-ling gets category P .Nodes without siblings get their tag as category.Constituent type labels are dropped.
The resultfor our example is shown at the bottom of Figure 1.2.2 The LexiconFor each leaf in the transformed corpus that isaligned to one or more words, a lexical item is ex-tracted containing the words, category and RCL.For single-word items, we also add part-of-speechtags, obtained using the C&C POS tagger (Curranand Clark, 2003), to reduce overgeneration.
Ex-amples of lexical items are:?
?block/NN?
` entity : (entity : (type :(block))??
?on, top, of?
` spatial-relation/entity :(spatial -relation : (relation : above))2.3 Combinatory RulesGiven the extracted lexical items, the corpusderivations are licensed by standard CCG rules(Steedman, 2001), using a modified semantics thatkeeps things simple and ensures that the semanticsof (most) intermediate constituents are themselvesRCL subexpressions, which is important for inter-facing with the spatial planner during parsing.
Themost important two rules are forward and back-ward application:(X/Y ):f Y :g ?
X :FAPP(X/Y, f, g) (>)Y :g (X\Y ):f ?
X :BAPP(X\Y, g, f) (<)where FAPP and BAPP are defined as follows:FAPP(X/Y,a, (t:h)) = (t:ah) if X = YFAPP(C, (t:h), c) = (t:hc) otherwiseBAPP(X\Y, (t:h),a) = (t:ha) if X = YBAPP(C, c, (t:h)) = (t:ch) otherwiseIn words, the semantics of the adjunct or comple-ment is added as a subtree under the root of thesemantics of the head.We also use a restricted form of the CCG ruleforward composition to form chains of entity ad-juncts:(entity/entity):a (entity/entity):b?
(entity/entity):ab (>B)This is motivated by our use of the spatial plan-ner.
Without forward composition, we would, e.g.,not be able to build a constituent with the seman-tics (entity : (color : green)(color : red)(type :cube-group)) in the context of a stack consistingof green and red cubes, but no stack consistingexclusively of red cubes ?
the planner would fil-ter out the intermediate constituent with semantics(entity :(color :red)(type :cube-group)).Finally, we use type-changing rules, whichis standard practice in CCG parsing (Clark andCurran, 2007; Zettlemoyer and Collins, 2007).They are automatically extracted from the trainingdata.
Some of them account for unary productionswithin RCL expressions by introducing an addi-tional internal node, such as the destination nodein Figure 1.
For example:sp-relation:h?destination:(destination :h) (?1)Others account for RCL leaves that are not linkedto any words.
For example, the RCL expressionfor the command take the light blue prism from theblue cube renders the from-phrase as an adjunctto the prism node: (spatial -relation : (relation :above)(entity :(color :blue)(type :cube))), whereabove is not linked.
Rules like the following dealwith this by not only introducing an internal node,but also a branch leading to the unlinked leaf:entity:h?
entity/entity:(sp-relation :(relation :above)h) (?2)2.4 AnaphoraAnaphora are marked in RCL entity expressionsby the subexpression (id : 1) for antecedent en-tities and (reference-id : 1) for anaphoric enti-ties.
The latter have the special type reference,in which case they are typically linked to the wordit, or type-reference, in which case they are typi-cally linked to the word one, as in the yellow one.More than one anaphoric relation in a command,and thus, other IDs than 1, are possible, but ex-tremely rare.
We do not explicitly try to resolve484anaphora, but merely generate versions both withand without the id subexpression for each entitylexical item seen in training as an antecedent.
Wethen rely on the parser and spatial planner to find aparse with the correct item marked as antecedent.If the spatial planner rejects a subexpression be-cause it contains an unknown reference ID, we ac-cept it anyway because the expression can latercombine with another one that contains the an-tecedent.
However, at the level of complete parses,those containing a reference-id expression but noid expression ?
or vice versa ?
are rejected.
As aheuristic, we also reject parses where reference-idprecedes id because we found this to be a notice-able source of errors, and no cataphora in the train-ing data.3 Training and DecodingFollowing (Zettlemoyer and Collins, 2007), weuse a CKY CCG parser in combination with sim-ple perceptron updates: iterate over the trainingcorpus T times, for each sentence producing allparses.
Each parse is characterized by a num-ber of features and scored using a global weightvector.
The weight vector is updated by sub-tracting the feature vector of the highest-scoringparse and adding the feature vector of the highest-scoring correct parse.
No update is performed ifthe highest-scoring parse is correct, or no correctparse was found.
Since for the present task thetraining data already induces a lexicon, we treatthe lexicon as fixed and perform no lexical update.We parallelize training using iterative parametermixing (McDonald et al., 2010) with 12 shards.3.1 Semantically Empty and UnknownWordsThe parser initially considers each contiguous sub-sequence of words in the sentence and adds allmatching lexical items to the chart.
In order toallow for words that are not linked to the seman-tics, we simply add two additional lexical itemsto the chart for each word w in the sentence:?w?
` X/X : nil and ?w?
` X\X : nil whereX is a variable that can be bound to any categoryduring rule application.
We modify the combina-tory rules above to require that at least one of theinput items has non-nil semantics and to use thatas output semantics if the other is nil .In decoding, the parser also has to deal withwords not seen in training.
For one, there are thenil items, so it is possible to treat the unknownwords as semantically empty.
In addition, we lookat other single-word lexical items with the samePOS tag and generate corresponding lexical itemsfor the unknown word on the fly, hoping that fea-tures and the spatial planner will guide the parserto the right choice.
To limit the search space, thisis currently only done for nouns since we foundthe greatest lexical variance to occur with them.3.2 FeaturesEach chart edge is characterized by the followinglocal features:?
each lexical item w ` c:s used.?
each instance of a combinatory rule used, e.g.>.?
?p, c, s?
for each lexical item used where pis the POS tag (or empty for multiwords).This allows to learn correlations between cat-egory/semantics pairs and particular parts ofspeech, primarily for unknown words.?
each instance of a type-changing rule used,together with the semantic head word of theconstituent it roots, e.g.
?
?1, in?.
This helpsto learn not to use type-changing rules wherethey don?t make sense.
E.g.
the wordsquares often heads entity descriptions thattype-change into measure phrases but theword cube doesn?t.?
the root tag of the semantics of each con-stituent, together with the word to its immedi-ate left, e.g.
?destination, from?.
This exam-ple feature is indicative of typical erroneousparses where spatial adjuncts correspondingto from-phrases are misparsed as destinationcomplements.
The word from provides astrong clue against such a parse but would beignored without such a feature because it isnot aligned to any RCL node.?
the root tag of the semantics of each con-stituent, together with the first word in it, e.g.
?spatial -relation, above?.3.3 The Spatial PlannerThe spatial planner provided together with thetreebank provides access to the context in whicheach command is to be interpreted, consisting ofa current arrangement of bodies on a board and in485the gripper of the robot being instructed.
It can tellus for some RCL subexpressions, chiefly entitydescriptions, whether they ?make sense?
giventhe context.
For example, if the parser builds anedge with semantics (entity : (type :cube)(color :red)) but there is no red cube anywhere on theboard, we can immediately reject the edge (pro-vided no negations or hypothetical descriptionsare used, which is the case for the commandsin this task) and thereby avoid errors and reducethe search space.
The planner also helps resolveattachment ambiguities early: in the commandput the prism on the cube, a constituent with se-mantics (entity : (type :prism)(spatial -relation :(relation :above)(entity :(type :cube)))) is a pos-sible but incorrect parse.
If we are lucky enoughthat no prism is actually sitting on a cube in themicroworld, the planner will weed it out.We have not yet explored making the fullestpossible use of the spatial planner for checking thevalidity of event or sequence expressions, whichwould involve simulating changing the state of theworld as a sequence of event instructions is car-ried out.
Currently we only filter out initial eventinstructions with action drop for scenes in whichthere is nothing initially in the robot?s gripper tobe dropped.
RCL requires the action move hereinstead, a distinction which is often not made inthe natural language commands.4 Experiments and ResultsWe carried out two experiments, one using the spa-tial planner and one not using it.
In each case, wetrained on training examples shorter than 16 wordsto speed up training and evaluated on the full testset.
In both training and decoding, a beam searchstrategy keeps only the 60 highest-scoring edgesper chart cell.
The weights of non-nil lexical itemswere initialized to 1, those of nil items to 0.5, allother feature weights to 0.
The number of trainingepochs T was set to 3.
These values were chosenexperimentally using 80% of the training data andanother 10% for testing.Of the 909 test sentences, 720 (79.21%) wereparsed exactly correctly when not using the plan-ner, and 789 (86.80%) when using it, making thirdplace among the six participating systems.
Theresult shows that standard CCG-based techniquesfor semantic parsing can be successfully applied tothe domain of robotic spatial commands and profitfrom the integration of a spatial planner.A preliminary analysis suggests most errors arerelated to pronoun ellipsis, the ambiguous wordone, anaphora or attachment ambiguity.
We be-lieve some further careful feature engineering andextended use of the spatial planner could go a greatlength to improve accuracy further.ReferencesStephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.James R. Curran and Stephen Clark.
2003.
Inves-tigating GIS and smoothing for maximum entropytaggers.
In Proceedings of the 11th Meeting of theEuropean Chapter of the Association for Compu-tational Linguistics (EACL-03), pages 91?98, Bu-dapest, Hungary.Kais Dukes.
2013a.
Semantic annotation of roboticspatial commands.
In Language and TechnologyConference (LTC), Poznan, Poland.Kais Dukes.
2013b.
Train robots: A dataset for natu-ral language human-robot spatial interaction throughverbal commands.
In International Conference onSocial Robotics (ICSR).
Embodied Communicationof Goals and Intentions Workshop, Bristol, UnitedKingdom.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:a corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 456?464, Stroudsburg, PA,USA.Mark Steedman.
2001.
The Syntactic Process.
TheMIT Press.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for pars-ing to logical form.
In In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL-2007), pages678?687.486
