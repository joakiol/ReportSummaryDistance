Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 163?171,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsAutomatically Building Training Examples for Entity ExtractionMarco PennacchiottiYahoo!
LabsSunnyvale, CA, USApennac@yahoo-inc.comPatrick PantelMicrosoft ResearchRedmond, WA, USAppantel@microsoft.comAbstractIn this paper we present methods for automat-ically acquiring training examples for the taskof entity extraction.
Experimental evidenceshow that: (1) our methods compete with acurrent heavily supervised state-of-the-art sys-tem, within 0.04 absolute mean average pre-cision; and (2) our model significantly out-performs other supervised and unsupervisedbaselines by between 0.15 and 0.30 in abso-lute mean average precision.1 IntroductionEntity extraction is a fundamental task in NLP andrelated applications.
It is broadly defined as the taskof extracting entities of a given semantic class fromtexts (e.g., lists of actors, musicians, cities).
Searchengines such as Bing, Yahoo, and Google collectlarge sets of entities to better interpret queries (Tanand Peng, 2006), to improve query suggestions (Caoet al, 2008) and to understand query intents (Hu etal., 2009).
In response, automated techniques forentity extraction have been proposed (Pas?ca, 2007;Wang and Cohen, 2008; Chaudhuri et al, 2009; Pan-tel et al, 2009).There is mounting evidence that combiningknowledge sources and information extraction sys-tems yield significant improvements over applyingeach in isolation (Pas?ca et al, 2006; Mirkin et al,2006).
This intuition is explored by the Ensem-ble Semantics (ES) framework proposed by Pennac-chiotti and Pantel (2009), which outperforms pre-vious state-of-the-art systems.
A severe limitationof this type of extraction system is its reliance oneditorial judgments for building large training setsfor each semantic class to be extracted.
This isparticularly troublesome for applications such asweb search that require large numbers of semanticclasses in order to have a sufficient coverage of factsand objects (Tan and Peng, 2006).
Hand-craftingtraining sets across international markets is often in-feasible.
In an exploratory study we estimated thata pool of editors would need roughly 300 workingdays to complete a basic set of 100 English classesusing the ES framework.
Critically needed are meth-ods for automatically building training sets that pre-serve the extraction quality.In this paper, we propose simple and intuitivelyappealing solutions to automatically build trainingsets.
Positive and negative training sets for a tar-get semantic class are acquired by leveraging: i)?trusted?
sources such as structured databases (e.g.,IMDB or Wikipedia for acquiring a list of Actors);ii) automatically constructed semantic lexicons; andiii) instances of semantic classes other than the tar-get class.
Our models focus on extracting trainingsets that are large, balanced, and representative ofthe unlabeled data.
These models can be used in anyextraction setting, where ?trusted?
sources of knowl-edge are available: Today, the popularity of struc-tured and semi-structured sources such as Wikipediaand internet databases, makes this approach widelyapplicable.
As an example, in this paper we showthat our methods can be successfully adapted andused in the ES framework.
This gives us the possi-bility to test the methods on a large-scale entity ex-traction task.
We replace the manually built trainingdata in the the ES model with the training data built163by our algorithms.
We show by means of a large em-pirical study that our algorithms perform nearly asgood as the fully supervised ES model, within 4% inabsolute mean average precision.
Further, we com-pare the performance of our method against bothPas?ca et al (2006) and Mirkin et al (2006), show-ing 17% and 15% improvements in absolute meanaverage precision, respectively.The main contributions of this paper are:?
We propose several general methods forautomatically acquiring labeled training data;we show that they can be used in a large-scaleextraction framework, namely ES; and?
We show empirical evidence on a large-scaleentity extraction task that our system usingautomatically labeled training data performsnearly as well as the fully-supervised ESmodel, and that it significantly outperformsstate-of-the-art systems.2 Automatic Acquisition of Training DataSupervised machine learning algorithms requiretraining data that is: (1) balanced and large enoughto correctly model the problem at hand (Kubat andMatwin, 1997; Japkowicz and Stephen, 2002); and(2) representative of the unlabeled data to decode,i.e., training and unlabeled instances should be ide-ally drawn from the same distribution (Blumer et al,1989; Blum and Langley, 1997).
If these two prop-erties are not met, various learning problems, suchas overfitting, can drastically impair predictive ac-curacy.
To address the above properties, a commonapproach is to select a subset of the unlabeled data(i.e., the instances to be decoded), and manually la-bel them to build the training set.In this section we propose methods to automatethis task by leveraging the multitude of structuredknowledge bases available on the Web.Formally, given a target class c, our goal isto implement methods to automatically build atraining set T (c), composed of both positive andnegative examples, respectively P (c) and N(c);and to apply T (c) to classify (or rank) a setof unlabeled data U(c), by using a learningalgorithm.
For example, in entity extraction,given the class Actors, we might have P (c) ={Brad Pitt, Robert De Niro} and N(c) ={Serena Williams, Rome, Robert Demiro}.Below, we define the components of a typicalknowledge acquisition system as in the ES frame-work, where our methods can be applied :Sources.
Textual repositories of information, ei-ther structured (e.g., Freebase), semi-structured(e.g., HTML tables) or unstructured (e.g., a we-bcrawl).
Information sources serve as inputs to theextraction system, either for the Knowledge Extrac-tors to generate candidate instances, or for the Fea-ture Generators to generate features (see below).Knowledge Extractors (KE).
Algorithms re-sponsible for extracting candidate instances such asentities or facts.
Extractors fall into two categories:trusted and untrusted.
Trusted extractors execute onstructured sources where the contents are deemed tobe highly accurate.
Untrusted extractors execute onunstructured or semi-structured sources and gener-ally generate high coverage but noisy knowledge.Feature Generators.
Methods that extract evi-dence (features) of knowledge in order to decidewhich extracted candidate instances are correct.Ranker.
A module for ranking the extracted in-stances using the features generated by the featuregenerators.
In supervised ML-based rankers, labeledtraining instances are required to train the model.Our goal here is to automatically label training in-stances thus avoiding the editorial costs.2.1 Acquiring Positive ExamplesTrusted positives: Candidate instances for a classc that are extracted by a trusted Knowledge Extrac-tor (e.g., a wrapper induction system over IMDB),tend to be mostly positive examples.
A basic ap-proach to acquiring a set of positive examples is thento sample from the unlabeled set U(c) as follows:P (c) = {i ?
U(c) : (?KEi|KEi is trusted} (1)where KEi is a knowledge extractor that extractedinstance i.The main advantage of this method is that P (c) isguaranteed to be highly accurate, i.e., most instancesare true positives.
On the downside, instances inP (c) are not necessarily representative of the un-trusted KEs.
This can highly impact the perfor-mance of the learning algorithm, which could over-fit the training data on properties that are specific to164the trusted KEs, but that are not representative of thetrue population to be decoded (which is largely com-ing from untrusted KEs).We therefore enforce that the instances in P (c)are extracted not only from a trusted KE, but alsofrom any of the untrusted extractors:P (c) = {i ?
U(c) :?KEi|KEi is trusted ?
?KEj |KEj is untrusted}(2)External positives: This method selects the set ofpositive examples P (c) from an external repository,such as an ontology, a database, or an automati-cally harvested source.
The main advantage of thismethod is that such resources are widely availablefor many knowledge extraction tasks.
Yet, the riskis that P (c) is not representative of the unlabeled in-stances U(c), as they are drawn from different pop-ulations.2.1.1 Acquiring Negative ExamplesAcquiring negative training examples is a muchmore daunting task (Fagni and Sebastiani, 2007).The main challenge is to select a set which is a goodrepresentative of the unlabeled negatives in U(c).Various strategies can be adopted, ranging fromselecting near-miss examples to acquiring genericones, each having its own pros and cons.
Belowwe propose our methods, some building on previouswork described in Section 5.Near-class negatives: This method selects N(c)from the population U(C) of the set of classes Cwhich are semantically similar to c. For example, inentity extraction, the classes Athletes, Directorsand Musicians are semantically similar to the classActors, while Manufacturers and Products aredissimilar.
Similar classes allow us to select negativeexamples that are semantic near-misses for the classc.
The hypothesis is the following:A positive instance extracted for a class similarto the target class c, is likely to be a near-missincorrect instance for c.To model this hypothesis, we acquire N(c) fromthe set of instances having the following two restric-tions:1.
The instance is most likely correct for C2.
The instance is most likely incorrect for cNote that restriction (1) alone is not sufficient, as aninstance of C can be at the same time also instanceof c. For example, given the target class Actors, theinstance ?Woody Allen?
?
Directors, is not a goodnegative example for Actors, since Woody Allen isboth a director and an actor.In order to enforce restriction (1), we select onlyinstances that have been extracted by a trusted KEof C, i.e., the confidence of them being positive isvery high.
To enforce (2), we select instances thathave never been extracted by any KE of c. Moreformally, we define N(c) as follows:N(c) =?ci?CP (ci) \ U(c) (3)The main advantage of this method is that it acquiresnegatives that are semantic near-misses of the tar-get class, thus allowing the learning algorithm to fo-cus on these borderline cases (Fagni and Sebastiani,2007).
This is a very important property, as mostincorrect instances extracted by unsupervised KEsare indeed semantic near-misses.
On the downside,the extracted examples are not representative of thenegative examples of the target class c, since theyare drawn from two different distributions.Generic negatives: This method selects N(c)from the population U(C) of all classes C differentfrom the target class c, i.e., both classes semanticallysimilar and dissimilar to c. The method is very sim-ilar to the one above, apart from the selection of C,which now includes any class different from c. Theunderlying hypothesis is the following:A positive instance extracted for a class differentfrom the target class c, is likely to be an incorrectinstance for c.This method acquires negatives that are both seman-tic near-misses and far-misses of the target class.The learning algorithm is then able to focus both onborderline cases and on clear-cut incorrect cases, i.e.the hypothesis space is potentially larger than for thenear-class method.
On the downside, the distribu-tion of c and C are very different.
By enlarging thepotential hypothesis space, the risk is then again tocapture hypotheses that overfit the training data onproperties which are not representative of the truepopulation to be decoded.165Same-class negatives: This method selects theset of negative examples N(c) from the populationU(c).
The driving hypothesis is the following:If a candidate instance for a class c has been ex-tracted by only one KE and this KE is untrusted,then the instance is likely to be incorrect, i.e., anegative example for c.The above hypothesis stems from an intuitive obser-vation common to many ensemble-based paradigms(e.g., ensemble learning in Machine Learning): themore evidence you have of a given fact, the higher isthe probability of it being actually true.
In our case,the fact that an instance has been extracted by onlyone untrusted KE, provides weak evidence that theinstance is correct.
N(c) is defined as follows:N(c) = {i ?
U(c) : ?!
KEi ?KEi is untrusted}(4)The main advantage of this method is that the ac-quired instances in N(c) are good representatives ofthe negatives that will have to be decoded, i.e., theyare drawn from the same distribution U(c).
This al-lows the learning algorithm to focus on the typicalproperties of the incorrect examples extracted by thepool of KEs.A drawback of this method is that instances inN(c) are not guaranteed to be true negatives.
It fol-lows that the final training set may be noisy.
Twomain strategies can be applied to mitigate this prob-lem: (1) Use a learning algorithm which is robust tonoise in the training data; and (2) Adopt techniquesto automatically reduce or eliminate noise.
We hereadopt the first solution, and leave the second as apossible avenue for future work, as described in Sec-tion 6.
In Section 4 we demonstrate the amount ofnoise in our training data, and show that its impactis not detrimental for the overall performance of thesystem.3 A Use Case: Entity ExtractionEntity extraction is a fundamental task in NLP(Cimiano and Staab, 2004; McCarthy and Lehnert,2005) and web search (Chaudhuri et al, 2009; Huet al, 2009; Tan and Peng, 2006), responsible forextracting instances of semantic classes (e.g., ?BradPitt?
and ?Tom Hanks?
are instances of the class Ac-tors).
In this section we apply our methods for auto-matically acquiring training data to the ES entity ex-traction system described in Pennacchiotti and Pan-tel (2009).1The system relies on the following three knowl-edge extractors.
KEtrs: a ?trusted?
databasewrapper extractor acquiring entities from sourcessuch as Yahoo!
Movies, Yahoo!
Music and Yahoo!Sports, for extracting respectively Actors, Musiciansand Athletes.
KEpat: an ?untrusted?
pattern-basedextractor reimplementing Pas?ca et al?s (2006) state-of-the-art web-scale fact extractor.
KEdis: an ?un-trusted?
distributional extractor implementing a vari-ant of Pantel et al?s (2009).The system includes four feature generators,which compute a total of 402 features of varioustypes extracted from the following sources: (1) abody of 600 million documents crawled from theWeb at Yahoo!
in 2008; (2) one year of web searchqueries issued to Yahoo!
Search; (3) all HTML innertables extracted from the above web crawl; (4) anofficial Wikipedia dump from February 2008, con-sisting of about 2 million articles.The system adopts as a ranker a supervisedGradient Boosted Decision Tree regression model(GBDT) (Friedman, 2001).
GBDT is generally con-sidered robust to noisy training data, and hence is agood choice given the errors introduced by our auto-matic training set construction algorithms.3.1 Training Data AcquisitionThe positive and negative components of the trainingset for GBDT are built using the methods presentedin Section 2, as follows:Trusted positives (Ptrs and Pcls): According toEq.
2, we acquire a set of positive instances Pclsas a random sample of the instances extracted byboth KEtrs and either: KEdis, KEpat or both ofthem.
As a basic variant, we also experiment withthe simpler definition in Eq.
1, i.e.
we acquire a setof positive instances Ptrs as a random sample of theinstances extracted by the trusted extractor KEtrs,irrespective of KEdis and KEpat.External positives (Pcbc): Any external repositoryof positive examples would serve here.
In our spe-1We here give a summary description of our implementationof that system.
Refer to the original paper for more details.166cific implementation, we select a set of positive ex-amples from the CBC repository (Pantel and Lin,2002).
CBC is a word clustering algorithm thatgroups instances appearing in similar textual con-texts.
By manually analyzing the cluster membersin the repository created by CBC, it is easy to pick-up the cluster(s) representing a target class.Same-class negatives (Ncls): We select a set ofnegative instances as a random sample of the in-stances extracted by only one extractor, which canbe either of the two untrusted ones, KEdis orKEpat.Near-class negatives (Noth): We select a set ofnegative instances, as a random sample of the in-stances extracted by any of our three extractors for aclass different than the one at hand.
We also enforcethe condition that instances in Noth must not havebeen extracted for the class at hand.Generic negatives (Ncbc): We automatically se-lect as generic negatives a random sample of in-stances appearing in any CBC cluster, except thosecontaining at least one member of the class at hand(i.e., containing at least one instance extracted byone of our KEs for the given class).4 Experimental EvaluationIn this section, we report experiments comparingthe ranking performance of our different methodsfor acquiring training data presented in Section 3,to three different baselines and a fully supervisedupper-bound.4.1 Experimental SetupWe evaluate over three semantic classes: Actors(movie, tv and stage actors); Athletes (profes-sional and amateur); Musicians (singers, musicians,composers, bands, and orchestras), so to comparewith (Pennacchiotti and Pantel, 2009).
Ranking per-formance is tested over the test set described in theabove paper, composed of 500 instances, randomlyselected from the instances extracted by KEpat andKEdis for each of the classes2.We experiment with various instantiations of theES system, each trained on a different training set2We do not test over instances extracted by KEtrs, as theydo not go though the decoding phaseobtained from our methods.
The different system in-stantiations (i.e., different training sets) are reportedin Table 1 (Columns 1-3).
Each training set consistsof 500 positive examples, and 500 negative exam-ples.As an upper bound, we use the ES system, wherethe training consists of 500 manually annotated in-stances (Pman and Nman), randomly selected fromthose extracted by the KEs.
This allows us to di-rectly check if our automatically acquired trainingsets can compete to the human upper-bound.
Wealso compare to the following baselines.Baseline 1: An unsupervised rule-based ES sys-tem, assigning the lowest score to instances ex-tracted by only one KE, when the KE is untrusted;and assigning the highest score to any other instance.Baseline 2: An unsupervised rule-based ES sys-tem, adopting as KEs the two untrusted extractorsKEpat and KEdis, and a rule-based Ranker that as-signs scores to instances according to the sum oftheir normalized confidence scores.Baseline 3: An instantiation of our ES system,trained on Pman and Nman.
The only differ-ence with the upper-bound is that it uses only twofeatures, namely the confidence score returned byKEdis and KEpat.
This instantiation implementsthe system presented in (Mirkin et al, 2006).For evaluation, we use average precision (AP), astandard information retrieval measure for evaluat-ing ranking algorithms:AP (L) =?|L|i=1 P (i) ?
corr(i)?|L|i=1 corr(i)(5)where L is a ranked list produced by a system, P (i)is the precision of L at rank i, and corr(i) is 1 if theinstance at rank i is correct, and 0 otherwise.In order to accurately compute statistical signifi-cance, we divide the test set in 10-folds, and com-pute the AP mean and variance obtained over the10-folds.
For each configuration, we perform therandom sampling of the training set five times, re-building the model each time, to estimate the vari-ance when varying the training sampling.4.2 Experimental ResultsTable 1 reports average precision (AP) results fordifferent ES instantiations, separately on the three167System Training Set AP MAPPositives Negatives Actors Athletes MusiciansBaseline1 (unsup.)
- - 0.562 0.535 0.437 0.511Baseline2 (unsup.)
- - 0.676 0.664 0.576 0.639Baseline3 (sup.)
Pman Nman 0.715 0.697 0.576 0.664Upper-bound (full-sup.)
Pman Nman 0.860?
0.901?
0.786?
0.849?S1.
Pcls Noth 0.751?
0.880?
0.642 0.758?S2.
Pcls Ncbc 0.734?
0.854?
0.644 0.744?S3.
Pcls Ncls 0.842?
0.806?
0.770?
0.806?S4.
Pcls Noth + Ncbc 0.756?
0.853?
0.693?
0.767?S5.
Pcls Ncls + Noth 0.835?
0.807?
0.763?
0.802?S6.
Pcls Ncls + Ncbc 0.838?
0.822?
0.768?
0.809?S7.
Pcls Ncls + Noth + Ncbc 0.838?
0.818?
0.764?
0.807?Table 1: Average precision (AP) results of systems using different training sets, compared to two usupervised Base-lines, a supervised Baseline, and a fully supervised upper-bound system.
?
indicates statistical significance at the 0.95level wrt all Baselines.
?
indicates statistical significance at the 0.95 level wrt Baseline1 and Baseline 2. ?
indicatesstatistical significance at the 0.95 level wrt Baseline1.classes; and the mean average precision (MAP)computed across the classes.
We report results us-ing Pcls as positive training, and varying the neg-ative training composition3.
Systems S1-S3 use asingle method to build the negatives.
Systems S4-S6 combine two methods (250 examples from onemethod, 250 from the other), and S7 combines allthree methods.
Table 3 reports additional basic re-sults when varying the positive training set compo-sition, and fixing the best performing negative set(namely Ncls).Table 1 shows that all systems outperform thebaselines in MAP, with 0.95 statistical significance,but S2 which is not significant wrt Baseline 3.
S6 isthe best performing system, achieving 0.809 MAP,only 4% below the supervised upper-bound (statis-tically insignificant at the 0.95 level).
These resultsindicate that our methods for automatically acquir-ing training data are highly effective and competitivewith manually crafted training sets.A class-by-class analysis reveals similar behav-ior for Actors and Musicians.
For these two classes,the best negative set is Ncls (system S3), achievingalone the best AP (respectively 0.842 and 0.770 forActors and Musicians, 2.1% and 1.6% points belowthe upper-bound).
Noth and Ncbc show a lower ac-curacy, more than 10% below Ncls.
This suggestthat the most promising strategy for automatically3For space limitation we cannot report exhaustively all com-binations.Negative set False NegativesActors Athletes MusiciansNcls 5% 45% 30%Noth 0% 10% 10%Ncbc 0% 0% 15%Table 2: Percentage of false negatives in different types ofnegative sets, across the three experimented classes (esti-mations over a random sample of 20 examples per class).acquiring negative training data is to collect exam-ples from the target class, as they guarantee to bedrawn from the same distribution as the instances tobe decoded.
The use of near- and far-misses is stillvaluable (AP results are still better than the base-lines), but less effective.Results for Athletes give different evidence: thebest performing negative set is Noth, performingsignificantly better than Ncls.
To investigate thiscontrasting result, we manually picked 20 exam-ples from Ncls, Noth and Ncbc for each class, andchecked their degree of noise, i.e., how many falsenegatives they contain.
Table 2 reports the results:these numbers indicate that the Ncls is very noisyfor the Athletes class, while it is more clean for theother two classes.
This suggests that the learningalgorithm, while being robust enough to cope withthe small noise in Ncls for Actors and Musicians, itstarts to diverge when too many false negatives arepresented for training, as it happens for Athletes.False negatives in Ncls are correct instances ex-tracted by one untrusted KE alone.
The results in168Table 2 indicates that our untrusted KEs are moreaccurate in extracting instances for Athletes than forthe other classes: accurate enough to make our train-ing set too noisy, thus decreasing the performanceof S3 wrt S1 and S2.
This indicates that the effec-tiveness of Ncls decreases when the accuracy of theuntrusted KEs is higher.A good strategy to avoid the above problem is topair Ncls with another negative set, either Ncbc orNoth, as in S5 and S6, respectively.
Then, whenthe above problem is presented, the learning algo-rithm can rely on the other negative set to com-pensate some for the noise.
Indeed, when addingNcbc to Ncls (system S6) the accuracy over Athletesimproves, while the overall performance across allclasses (MAP) is kept constant wrt the system usingNcls (S3).It is interesting that in Table 2, Ncbc and Noth alsohave a few false negatives.
An intrinsic analysis re-veals that these are either: (1) Incorrect instancesof the other classes that are actual instances of thetarget class; (2) Correct instances of other classesthat are also instances of the target class.
Case (1) iscaused by errors of KEs for the other classes (e.g.,erroneously extracting ?Matthew Flynt?
as a Musi-cian).
Case (2) covers cases in which instances areambiguous across classes, for example ?Kerry Tay-lor?
is both an Actor and a Musician.
This observa-tion is still surprising, since Eq.
3 explicitly removesfrom Ncbc and Noth any correct instance of the tar-get class extracted by the KEs.
The presence of falsenegatives is then due to the low coverage of the KEsfor the target class, e.g.
the KEs were not able to ex-tract ?Matthew Flynt?
and ?Kerry Taylor?
as actors.Correlations.
We computed the Spearman corre-lation coefficient r among the rankings producedby the different system instantiations, to verifyhow complementary the information enclosed in thetraining sets are for building the learning model.Among the basic systems S1?
S3, the highest cor-relation is between S1 and S2 (r = 0.66 in aver-age across all classes), which is expected, since theyboth apply the principle of acquiring negative ex-amples from classes other than the target one.
S3exhibits lower correlation with both S1 and S2, re-spectively r = 0.57 and r = 0.53, suggesting that itis complementary to them.
Also, the best system S6System Training Set AP MAPPos.
Neg.
Act.
Ath.
Mus.S3.
Pcls Ncls 0.842 0.806 0.770 0.806S8.
Ptrs Ncls 0.556 0.779 0.557 0.631S9.
Pcbc Ncls 0.633 0.521 0.561 0.571Table 3: Comparative average precision (AP) results forsystems using different positive sets as training data.Figure 1: Average precision of system S6 with differenttraining sizes.has higher correlation with S3 (r = 0.94) than withS2 (r = 0.62), indicating that in the combination ofNcls and Ncbc, most of the model is built on Ncls.Varying the positive training.
Table 3 reports re-sults when fixing the negative set to the best per-forming Ncls, and exploring the use of other posi-tive sets.
As expected Pcls largely outperforms Ptrs,confirming that removing the constraint in Eq.
2 andusing the simpler Eq.
1 makes the training set unrep-resentative of the unlabeled population.
A similarobservation stands for Pcbc.
These results indicatethat having a good trusted KE, or even an externalresource of positives, is effective only when select-ing from the training set examples that are also ex-tracted by the untrusted KEs.Varying the training size.
In Figure 1 we reportan analysis of the AP achieved by the best perform-ing System (S6), when varying the training size, i.e.,changing the cardinality of Pcls and Ncls + Ncbc.The results show that a relatively small-sized train-ing set offers good performance, the plateau beingreached already with 500 training examples.
Thisis an encouraging result, showing that our methodscan potentially be applied also in cases where fewexamples are available, e.g., for rare or not well-represented classes.1695 Related WorkMost relevant are efforts in semi-supervised learn-ing.
Semi-supervised systems use both labeled andunlabeled data to train a machine learning system.Most common techniques are based on co-trainingand self-training.
Co-training uses a small set of la-beled examples to train two classifiers at the sametime.
The classifiers use independent views (i.e.
?conditionally independent?
feature sets) to repre-sent the labeled examples.
After the learning phase,the most confident predictions of each classifieron the unlabeled data are used to increase the la-beled set of the other.
These two phases are re-peated until a stop condition is met.
Co-traininghas been successfully applied to various applica-tions, such as statistical parsing (Sarkar, 2001) andweb pages classification (Yarowsky, 1998).
Self-training techniques (or bootsrapping) (Yarowsky,1995) start with a small set of labeled data, and it-eratively classify unlabeled data, selecting the mostconfident predictions as additional training.
Self-training has been applied in many NLP tasks, suchas word sense disambiguation (Yarowsky, 1995) andrelation extraction (Hearst, 1992).
Unlike typicalsemi-supervised approaches, our approach reducesthe needed amount of labeled data not by acting onthe learning algorithm itself (any algorithm can beused in our approach), but on the method to acquirethe labeled training data.Our work also relates to the automatic acquisi-tion of labeled negative training data.
Yangarber etal.
(2002) propose a pattern-based bootstrapping ap-proach for harvesting generalized names (e.g., dis-eases, locations), where labeled negative examplesfor a given class are taken from positive seed exam-ples of ?competing?
classes (e.g.
examples of dis-eases are used as negatives for locations).
The ap-proach is semi-supervised, in that it requires somemanually annotated seeds.
The study shows thatusing competing categories improves the accuracyof the system, by avoiding sematic drift, which isa common cause of divergence in boostrapping ap-proaches.
Similar approaches are used among othersin (Thelen and Riloff, 2002) for learning semanticlexicons, in (Collins and Singer, 1999) for named-entity recognition, and in (Fagni and Sebastiani,2007) for hierarchical text categorization.
Some ofour methods rely on the same intuition describedabove, i.e., using instances of other classes as nega-tive training examples.
Yet, the ES framework al-lows us to add further restrictions to improve thequality of the data.6 ConclusionWe presented simple and general techniques for au-tomatically acquiring training data, and then testedthem in the context of the Ensemble Semanticsframework.
Experimental results show that ourmethods can compete with supervised systems us-ing manually crafted training data.
It is our hope thatthese simple and easy-to-implement methods can al-leviate some of the cost of building machine learn-ing architectures for supporting open-domain infor-mation extraction, where the potentially very largenumber of classes to be extracted makes infeasiblethe use of manually labeled training data.There are many avenues for future work.
Al-though our reliance on high-quality knowledgesources is not an issue for many head classes, itposes a challenge for tail classes such as ?wine con-noisseurs?, where finding alternative sources of highprecision samples is important.
We also plan to ex-plore techniques to automatically identify and elim-inate mislabeled examples in the training data asin (Rebbapragada and Brodley, 2007), and relax theboolean assumption of trusted/untrusted extractorsinto a graded one.
Another important issue regardsthe discovery of ?near-classes?
for collecting near-classes negatives: we plan to automate this step byadapting existing techniques as in (McIntosh, 2010).Finally, we plan to experiment on a larger set ofclasses, to show the generalizability of the approach.Our current work focuses on leveraging auto-learning to create an extensive taxonomy of classes,which will constitute the foundation of a very largeknowledge-base for supporting web search.ReferencesAvrim L. Blum and Pat Langley.
1997.
Selection of rel-evant features and examples in machine learning.
Ar-tificial Intelligence, 97:245?271.A.
Blumer, A. Ehrenfeucht, D. Haussler, and M.K.
War-muth.
1989.
Proceedings of ltc-07.
Journal of ACM,36:929?965.170Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao,Enhong Chen, and Hang Li.
2008.
Context-awarequery suggestion by mining click-through and sessiondata.
In Proceedings of KDD-08, pages 875?883.Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.2009.
Exploiting web search to generate synonyms forentities.
In Proceedings of WWW-09, pages 151?160.Philipp Cimiano and Steffen Staab.
2004.
Learning bygoogling.
SIGKDD Explorations, 6(2):24?34.M.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
In Proceedings ofWVLC/EMNLP-99, pages 100?110.Tiziano Fagni and Fabrizio Sebastiani.
2007.
On the se-lection of negative examples for hierarchical text cate-gorization.
In Proceedings of LTC-07, pages 24?28.Jerome H. Friedman.
2001.
Greedy function approxima-tion: A gradient boosting machine.
Annals of Statis-tics, 29(5):1189?1232.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofCOLING-92, pages 539?545.Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun, andZheng Chen.
2009.
Understanding user?s query intentwith Wikipedia.
In Proceedings of WWW-09, pages471?480.N.
Japkowicz and S. Stephen.
2002.
The class imbalanceproblem: A systematic study.
Intelligent Data Analy-sis, 6(5).M.
Kubat and S. Matwin.
1997.
Addressing the curseof inbalanced data sets: One-side sampleing.
In Pro-ceedings of the ICML-1997, pages 179?186.
MorganKaufmann.Joseph F. McCarthy and Wendy G Lehnert.
2005.
Usingdecision trees for coreference resolution.
In Proceed-ings of IJCAI-1995, pages 1050?1055.Tara McIntosh.
2010.
Unsupervised discovery of nega-tive categories in lexicon bootstrapping.
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 356?365, Mas-sachusetts, USA.
Association for Computational Lin-guistics.Shachar Mirkin, Ido Dagan, and Maayan Geffet.
2006.Integrating pattern-based and distributional similaritymethods for lexical entailment acquisition.
In Pro-ceedings of ACL/COLING-06, pages 579?586.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Organizing and search-ing the world wide web of facts - step one: The one-million fact extraction challenge.
In Proceedings ofAAAI-06, pages 1400?1405.Marius Pas?ca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of CIKM-07, pages 683?690, New York, NY,USA.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of KDD-02, pages613?619.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP-09.Marco Pennacchiotti and Patrick Pantel.
2009.
Entityextraction via ensemble semantics.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 238?247, Singapore.Association for Computational Linguistics.Umaa Rebbapragada and Carla E. Brodley.
2007.
Classnoise mitigation through instance weighting.
In Pro-ceedings of the 18th European Conference on MachineLearning.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In NAACL-2001.Bin Tan and Fuchun Peng.
2006.
Unsupervised querysegmentation using generative language models andwikipedia.
In Proceedings of WWW-06, pages 1400?1405.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 214?221, Philadelphia, PA, USA.
As-sociation for Computational Linguistics.Richard C. Wang and William W. Cohen.
2008.
Itera-tive set expansion of named entities using the web.
InICDM ?08: Proceedings of the 2008 Eighth IEEE In-ternational Conference on Data Mining, pages 1091?1096, Washington, DC, USA.
IEEE Computer Society.Roman Yangarber, Winston Lin, and Ralph Grishman.2002.
Unsupervised learning of generalized names.In COLING-2002.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of ACL-1996, pages 189?196.David Yarowsky.
1998.
Combining labeled and unla-beled data with co-training.
In Proceedings of theWorkshop on Computational Learning Theory, pages92?100.171
