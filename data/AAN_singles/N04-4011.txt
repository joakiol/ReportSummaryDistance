Performance Evaluation and Error Analysis for Multimodal ReferenceResolution in a Conversation SystemJoyce Y. Chai          Zahar Prasov              Pengyu HongDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48864jchai@cse.msu.edu,  prasovza@cse.msu.eduDepartment of StatisticsHarvard UniversityCambridge, MA 02138hong@stat.harvard.eduAbstractMultimodal reference resolution is a processthat automatically identifies what users referto during multimodal human-machineconversation.
Given the substantial work onmultimodal reference resolution; it is importantto evaluate the current state of the art,understand the limitations, and identifydirections for future improvement.
Weconducted a series of user studies to evaluate thecapability of reference resolution in amultimodal conversation system.
This paperanalyzes the main error sources during real-timehuman-machine interaction and presents keystrategies for designing robust multimodalreference resolution algorithms.1 Introduction*Multimodal systems enable users to interact withcomputers through multiple modalities such as speech,gesture, and gaze (Bolt 1980; Cassell et al, 1999; Cohen etal., 1996; Chai et al, 2002; Johnston et al, 2002).
Oneimportant aspect of building multimodal systems is for thesystem to understand the meanings of multimodal userinputs.
A key element of this understanding process isreference resolution.
Reference resolution is a process thatfinds the most proper referents to referring expressions.
Toresolve multimodal references, many approaches havebeen developed, from the use of a focus space model (Nealet al, 1998), a centering framework (Zancanaro et al1997), contextual factors (Huls et al, 1995); to recentapproaches using unification (Johnston, 1998), finite statemachines (Johnston and Bangalore 2000), and context-based rules (Kehler 2000).Given the substantial work in this area; it is importantto evaluate the state of the art, understand the limitations,* This work was supported by grant IIS-0347548 from theNational Science Foundation and grant IRGP-03-42111 fromMichigan State University.and identify directions for future improvement.
Weconducted a series of user studies to evaluate the capabilityof reference resolution in a multimodal conversationsystem.
In particular, this paper examines two importantaspects: (1) algorithm requirements for handling a varietyof references, and (2) technology requirements forachieving good real-time performance.
In the followingsections, we first give a brief description of our system.Then we analyze the main error sources during real-timehuman-machine interaction and discuss the key strategiesfor designing robust reference resolution algorithms.2 System DescriptionWe implemented a multimodal conversation system tostudy multimodal user referring behavior and to evaluatereference resolution algorithms.
Users can use both speechand manual gestures (e.g., point and circle) to interact witha map-based graphic interface to find information aboutreal estate properties.As shown in Figure 1, our system applies a semanticfusion approach that combines the semantic informationidentified from each modality.
A key characteristic of thesystem is that, in addition to fusing information fromdifferent modalities, our system systematicallyincorporates information from the conversation context(e.g., the focus of attention from prior conversation), theSpeech Input Gesture InputSpeechRecognizerNLParserGestureRecognizerMultimodal Interpreter(Graph-based ReferenceResolution Component)ConversationManagerPresentationManagerConversationContextVisualContextDomainContextMultimedia OutputFigure 1:  Overview of the systemvisual context (e.g., objects on the screen that are in thevisual focus), and the domain context (i.e., the domainkmgrespreeinreregohcpisaGpRremd3Wpgthsyathvoice from each subject was trained individually tominimize speech recognition errors.esteef,ee.senowledge).The reference resolution approach is based on a graph-atching algorithm.
Specifically, two attribute relationalraphs are used (Tsai and Fu, 1979).
One graph is calledferring graph that captures referring expressions fromeech utterances.
Each node, corresponding to oneferring expression, consists of the semantic informationxtracted from the expression and the temporalformation when the expression is uttered.
Each edgepresents the semantic and temporal relation between twoferring expressions.
The second graph is called referentraph that represents all potential referents (includingbjects selected by the gesture, objects in the conversationistory, and objects in the visual focus).
Each nodeaptures the semantic and temporal information about aotential referent (e.g., the time when the potential referentselected by a gesture).
Each edge captures the semanticnd temporal relations between two potential referents.iven these graph representations, the reference resolutionroblem becomes a graph-matching problem (Gold andangarajan, 1996).
The goal is to find a match between the3.1 Performance EvaluationTable 1 summarizes the referring behavior observed in thstudies and the performance of the system.
The columnindicate whether there was no gesture, one gesture (poinor circle), or multiple gestures involved in the input.
Throws indicate the type of referring expressions in thspeech utterances.
Each table entry shows the systemperformance on resolving a particular combination ospeech and gesture inputs.
For example, the entry at <S2G4> indicates that 35 inputs consist of demonstrativsingular noun phrases (as the referring expressions) and asingle circle gesture.
Out of these inputs, 27 were correctlyrecognized and eight were incorrectly recognized by thspeech recognizer.
Out of the 27 correctly recognizedinputs, 26 were correctly assigned referents by the systemOut of the eight incorrectly recognized inputs, referencein two inputs were correctly resolved.Consistent with earlier findings (Kehler 2000), thmajority of user references were simple which onlyferring graph and the referent graph that achieves theaximum compatibility between the two graphs.
Theetails of this approach are described in (Chai et al, 2004).Performance Evaluation and Analysise conducted several user studies to evaluate theerformance of real time reference resolution using theraph-based approach.
Eleven subjects participated inese studies.
Each of them was asked to interact with thestem using both speech and gestures (point and circle) toccomplish five tasks.
For example, one task was to finde least expensive house in the most populated town.
Theinvolved one referring expression and one gesture asshown in Table 1 (i.e., S1 to S8, with column G2 and G4).However, we have also found that 14% (31/219) of theinputs were complex, which involved multiple referringexpressions from speech utterances (see the row S9).
Someof these inputs did not have any accompanied gesture (e.g.,<S9, G1>).
Some were accompanied by one gesture (e.g.,<S9, G4>) or multiple gestures (e.g., <S9, G3> and <S9,G5>).
The referents to these referring expressions couldcome from user?s gestures, or from the conversationcontext, or from the graphic display.
To resolve these typesof references, the graph-based approach is effective bysimultaneously considering the semantic, temporal, and1(1), 1(0)001(1), 1(0)000S5:(these|those)(num)*(adj)*(ones)*|them129(111), 90(26)15(9), 16(1)2(0) 12(4)4(2), 0(0)7(7), 2(1)7(4), 12(5)22(21), 14(3)64(61), 27(11)7(6), 6(1)TotalNum4(4), 11(0)9(7), 5(0)63(54), 36(8)7(4), 3(2)39(37),29(14)7(5), 6(2)Total Num0(0), 3(0)8(6), 5(0)3(1), 7(1)4(2), 0(0)00(0), 1(0)S9: multiple expressions0(0), 3(0)00(0), 3(0)0(0), 3(2)1(0), 3(2)1(0), 0(0)S8: proper nouns002(0), 0(0)01(1), 0(0)1(1), 0(0)S7: empty expression006(6), 0(0)01(1), 1(0)0(0), 1(1)S6: here|there004(2), 4(1)02(2), 6(3)1(0), 2(1)S4: it|this|that| (this|that|the)(adj)*one3(3), 2(0)019(18), 12(3)000S3: (these|those)(num)*(adj)*Ns1(1), 2(0)1(1), 0(0)27(26), 8(2)3(2), 0(0)29(28), 16(9)3(3), 1(0)S2: (this|that) (adj*) N0(0), 1(0)01(0), 1(1)05(5), 3(0)1(1), 1(0)S1: the (adj)*(N | Ns)G6Points andCirclesG5MultipleCirclesG4One CircleG3MultiplePointsG2One PointG1NoGestureTable 1: Performance evaluation of the graph-matching approach to multimodal reference resolution.
In each entry form?a(b), c(d)?,  ?a?
indicates the number of inputs in which the referring expressions were correctly recognized by the speechrecognizer; ?b?
indicates the number of inputs in which the referring expressions were  correctly recognized and were cor-rectly resolved; ?c?
indicates the number of inputs in which the referring expressions were not correctly recognized; ?d?indicates the number of inputs in which the referring expressions were not correctly recognized, but were correctly resolved.The sum of ?a?
and ?c?
gives the total number of inputs with a particular combination of speech and gesture.contextual constraints.3.2 Error AnalysisAs shown in Table 1, out of the total 219 inputs, 137 inputshad their referents correctly identified (A complex inputwith multiple referring expressions was consideredcorrectly resolved only if the referents to all the referringexpressions were correctly identified).
For the remaining82 inputs in which the referents were not correctlyidentified, the errors mainly came from five sources assummarized in Table 2.A poor performance in speech recognition is a majorerror source.
Although we have trained each user?s voiceindividually, the speech recognition rate is still very low.Only 59% (129/219) of inputs had correctly recognizedreferring expressions.
This is partly due to the fact thatmore than half of our subjects are non-native speakers.Fusing inputs from multiple modalities together cansometimes compensate for the recognition errors (Oviatt1996).
Among 90 inputs in which referring expressionswere incorrectly recognized, 26 of them were correctlyassigned referents due to the mutual disambiguation.However, poor speech recognition still accounted for 55%of the total errors.
A mechanism to reduce the recognitionerrors, especially by utilizing information from othermodalities will be important to provide a robust solutionfor real time multimodal reference resolution.The second source of errors (20% of the total errors)came from insufficient language understanding, especiallythe out-of-vocabularies.
For example, ?area?
was not inour vocabulary.
So the additional semantic constraintexpressed by ?area?
was not captured.
Therefore, thesystem could not identify whether a house or a town wasreferred when the user uttered ?this area?.
It is importantfor the system to have a capability of acquire knowledge(e.g., vocabulary) dynamically by utilizing informationfrom other modalities and the interaction context.Furthermore, the errors also came from a lack ofunderstanding of spatial relations (as in ?the house justclose to the red one?)
and superlatives (as in ?the mostexpensive house?).
Algorithms to align visual features toresolve spatial references as described in (Gorniak and Roy2003) are desirable.Among all errors, 13% came from unsynchronizedinputs.
Currently, we use an idle status (i.e., 2 seconds withno input from either speech or gesture) as the boundary todelimit an interaction turn.
There are two types of out ofsynchronization.
The first type is unsynchronized inputsfrom the user (such as a big pause between speech andgesture) and the other comes from the underlying systemimplementation.
The system captures speech inputs andgesture inputs from two different servers through TCP/IPprotocol.
A communication delay sometimes split onesynchronized input into two separate turns of inputs (i.e.,one turn was speech input alone and the other turn wasgesture input alone).
A better engineering mechanism tosynchronize inputs is desired.The disfluencies from the users also accounted forabout 7% of the total errors.
Recent findings indicated thatgesture patterns could be used as an additional source toidentify different types of speech disfluencies duringhuman-human conversation (Chen et al, 2002).
Asexpected, speech disfluencies did not occur that much inour studies.
Based on our limited cases, we found thatgesture patterns could be indicators of speech disfluencieswhen they did occur.
For example, if a user says ?show methe red house (point to house A), the green house (stillpoint to the house A)?, then the behavior of pointing to thesame house with different speech description usuallyindicates a repair.
Furthermore, gestures also involvedisfluencies, for example, repeatedly pointing to an objectis a gesture repetition.
Failure in identifying thesedisfluencies caused problems with reference resolution.
Itis important to have a mechanism that can identify thesedisfluencies using multimodal information.The remaining 5% errors came from theimplementation of our approach in order to reduce thecomplexity of graph matching.
Currently, the referentgraph only consists of potential referents from gestures,objects from the prior conversation, and the objects in thevisual focus (i.e., highlighted on the screen).
Therefore, itis insufficient to handle cases where users only use propernames (without any gestures) to refer to objects visible onthe screen.From the error analysis, we learned that variations inuser inputs (e.g., variations in vocabulary andsynchronization patterns), disfluencies in speech utterances,and even small changes in the input quality or theenvironment could seriously impair the real-timeperformance.
The future research effort should be devotedto developing adaptive approaches for reference resolutionto deal with unexpected inputs (e.g., inputs that are outsideof system knowledge).3.Th ies inde rencere andlete dings(O tudy,ge wereut % ofca sturean rred.Fu poral5%Others7%Disfluency13%Out of synchronization20%Language understanding errors55%Speech recognition errorsPercentageTable 2: The distribution of error sources 3 Design Strategiese evaluation also indicates three important strategsigning effective algorithms for multimodal refesolution.
The first strategy concerns with how to hmporal relations.
Consistent with the previous finviatt et al 1997), in most cases (85%) in our sstures occurred before the referring expressionstered.
However, we did find some exceptions.
In 7ses, there was no overlap between speech and ged speech were uttered before gestures occurthermore, one user could have different tembehavior at different stages in one interaction.
In our study,five users exhibited varied temporal alignment during theinteraction.
Therefore, to accommodate different temporalvariations, incorporating relative temporal relationsbetween different modalities based on temporal closenessis preferred over incorporating absolute temporal relationsor temporal orders.Second, in a multimodal conversation, the potentialobjects referred to by a user could come from differentsources.
They could be the objects gestured at, objects inthe visual focus (e.g., highlighted), objects visible on thescreen, or objects mentioned in a prior conversation.
It isimportant for reference resolution algorithms tosimultaneously combine semantic, temporal, andcontextual constraints.
This is particularly important forcomplex inputs that involve multiple referring expressionsand multiple gestures as described earlier.Third, depending on the interface design and theunderlying architecture for multimodal systems, differenttypes of uncertainties occur during the process of inputinterpretation.
For example, in our interface, each houseicon is built on top of the town icon.
Therefore, a pointinggesture could result in several possible objects.
Once atouch screen is used, a finger point may result in differentpossibilities.
Furthermore, most systems like ours arebased on the pipelined architecture as shown in Figure1.The pipelined processes can potentially lose lowprobability information (e.g., recognized alternatives withlow probabilities) that could be very crucial whenincorporated with other modalities and the interactioncontext.
Therefore, it is important to retain information atdifferent levels and systematically incorporate theimprecise information.4 ConclusionThis paper presents an evaluation of graph-basedmultimodal reference resolution in a conversational system.The evaluation indicates that, the real-time performance islargely dependent on speech recognition performance,language processing capability, disfluency detection fromboth speech and gesture, as well as the system engineeringissues.
Furthermore, the studies identify three importantstrategies for robust multimodal reference resolutionalgorithms: (1) using relative temporal constraints basedon temporal closeness, (2) combining temporal, semantic,and contextual constraints simultaneously, and (3)incorporating imprecise information.
A successfulapproach will need to consider both algorithmicrequirements and technology limitations.AcknowledgementThe authors would like to thank Keith Houck andMichelle Zhou at IBM T. J. Watson Research Center fortheir support in developing the system, and the anonymousreviewers for their helpful comments and suggestions.ReferencesBolt, R.A. 1980.
Put that there: Voice and Gesture at theGraphics Interface.
Computer Graphics14(3): 262-270.Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L.,Chang, K., Vilhjalmsson, H. and Yan, H. 1999.
Embodi-ment in Conversational Interfaces: Rea.
In Proceedings ofthe CHI'99 Conference, pp.
520-527.
Pittsburgh, PA.Chai, J. Y., Hong, P., and Zhou, M. X.
2004.
A probabilisticapproach to reference resolution in multimodal user inter-faces, Proceedings of 9th International Conference on Intel-ligent User Interfaces (IUI): 70-77.
Madeira, Portugal,January.Chai, J., Pan, S., Zhou, M., and Houck, K. 2002.
Context-based Multimodal Interpretation in Conversational Systems.Fourth International Conference on Multimodal Interfaces.Chen, L., Harper, M. and Quek, F. 2002.
Gesture patternsduring speech repairs.
Proceedings of International Con-ference on Multimodal Interfaces (ICMI).Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.,Smith, I., Chen, L., and Clow, J.
1996.
Quickset: Multimo-dal Interaction for Distributed Applications.
Proceedings ofACM Multimedia, pp.
31?
40.Gold, S. and Rangarajan, A.
1996.
A graduated assignmentalgorithm for graph-matching.
IEEE Trans.
Pattern Analy-sis and Machine Intelligence, vol.
18, no.
4.Gorniak, P. and Roy, D. 2003.Grounded Semantic Composi-tion for Visual Scenes.
Journal of Artificial IntelligenceResearch.Huls, C., Bos, E., and Classen, W. 1995.
Automatic ReferentResolution of Deictic and Anaphoric Expressions.
Compu-tational Linguistics, 21(1):59-79.Johnston, M. 1998.
Unification-based Multimodal parsing,Proceedings of COLING-ACL.Johnston, M. and Bangalore, S. 2000.
Finite-state multimodalparsing and understanding.
Proceedings of COLING.Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen,P., Walker, M., Whittaker, S., and Maloor, P. 2002.MATCH: An Architecture for Multimodal Dialog Systems,in Proceedings of ACL.Kehler, A.
2000.
Cognitive Status and Form of Reference inMultimodal Human-Computer Interaction, Proceedings ofAAAI.Neal, J. G., Thielman, C. Y.,  Dobes, Z. Haller, S. M., andShapiro, S. C. 1998.
Natural Language with IntegratedDeictic and Graphic Gestures.
Intelligent User Interfaces,M.
Maybury and W. Wahlster (eds.
), 38-51.Oviatt, S., DeAngeli, A., and Kuhn, K. 1997.
Integration andSynchronization of Input Modes during Multimodal Hu-man-Computer Interaction, In Proceedings of Conferenceon Human Factors in Computing Systems: CHI '97.Tsai, W.H.
and Fu, K.S.
1979.
Error-correcting isomorphismof attributed relational graphs for pattern analysis.
IEEETransactions on Systems, Man and Cybernetics, vol.
9, pp.757?768.Zancanaro, M., Stock, O., and Strapparava, C. 1997.
Multi-modal Interaction for Information Access: Exploiting Co-hesion.
Computational Intelligence 13(7):439-464.
