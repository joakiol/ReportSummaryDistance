Proceedings of the 43rd Annual Meeting of the ACL, pages 379?386,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Semantic Approach to IE Pattern InductionMark Stevenson and Mark A. GreenwoodDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, UKmarks,m.greenwood@dcs.shef.ac.ukAbstractThis paper presents a novel algorithm forthe acquisition of Information Extractionpatterns.
The approach makes the assump-tion that useful patterns will have simi-lar meanings to those already identifiedas relevant.
Patterns are compared usinga variation of the standard vector spacemodel in which information from an on-tology is used to capture semantic sim-ilarity.
Evaluation shows this algorithmperforms well when compared with apreviously reported document-centric ap-proach.1 IntroductionDeveloping systems which can be easily adapted tonew domains with the minimum of human interven-tion is a major challenge in Information Extraction(IE).
Early IE systems were based on knowledge en-gineering approaches but suffered from a knowledgeacquisition bottleneck.
For example, Lehnert et al(1992) reported that their system required around1,500 person-hours of expert labour to modify fora new extraction task.
One approach to this problemis to use machine learning to automatically learn thedomain-specific information required to port a sys-tem (Riloff, 1996).
Yangarber et al (2000) proposedan algorithm for learning extraction patterns for asmall number of examples which greatly reduced theburden on the application developer and reduced theknowledge acquisition bottleneck.Weakly supervised algorithms, which bootstrapfrom a small number of examples, have the advan-tage of requiring only small amounts of annotateddata, which is often difficult and time-consumingto produce.
However, this also means that thereare fewer examples of the patterns to be learned,making the learning task more challenging.
Pro-viding the learning algorithm with access to addi-tional knowledge can compensate for the limitednumber of annotated examples.
This paper presentsa novel weakly supervised algorithm for IE patterninduction which makes use of the WordNet ontology(Fellbaum, 1998).Extraction patterns are potentially useful for manylanguage processing tasks, including question an-swering and the identification of lexical relations(such as meronomy and hyponymy).
In addition, IEpatterns encode the different ways in which a pieceof information can be expressed in text.
For exam-ple, ?Acme Inc. fired Jones?, ?Acme Inc. let Jonesgo?, and ?Jones was given notice by his employers,Acme Inc.?
are all ways of expressing the same fact.Consequently the generation of extraction patterns ispertinent to paraphrase identification which is cen-tral to many language processing problems.We begin by describing the general process of pat-tern induction and an existing approach, based onthe distribution of patterns in a corpus (Section 2).We then introduce a new algorithm which makes useof WordNet to generalise extraction patterns (Sec-tion 3) and describe an implementation (Section 4).Two evaluation regimes are described; one based onthe identification of relevant documents and anotherwhich aims to identify sentences in a corpus which379are relevant for a particular IE task (Section 5).
Re-sults on each of these evaluation regimes are thenpresented (Sections 6 and 7).2 Extraction Pattern LearningWe begin by outlining the general process of learn-ing extraction patterns, similar to one presented by(Yangarber, 2003).1.
For a given IE scenario we assume the exis-tence of a set of documents against which thesystem can be trained.
The documents areunannotated and may be either relevant (con-tain the description of an event relevant to thescenario) or irrelevant although the algorithmhas no access to this information.2.
This corpus is pre-processed to generate the setof all patterns which could be used to representsentences contained in the corpus, call this setS.
The aim of the learning process is to identifythe subset of S representing patterns which arerelevant to the IE scenario.3.
The user provides a small set of seed patterns,Sseed, which are relevant to the scenario.
Thesepatterns are used to form the set of currentlyaccepted patterns, Sacc, so Sacc ?
Sseed.
Theremaining patterns are treated as candidates forinclusion in the accepted set, these form the setScand(= S ?
Sacc).4.
A function, f , is used to assign a score toeach pattern in Scand based on those whichare currently in Sacc.
This function as-signs a real number to candidate patterns so?
c  Scand, f(c, Sacc) 7?
<.
A set of highscoring patterns (based on absolute scores orranks after the set of patterns has been orderedby scores) are chosen as being suitable for in-clusion in the set of accepted patterns.
Theseform the set Slearn.5.
The patterns in Slearn are added to Sacc andremoved from Scand, so Sacc ?
Sacc ?
Slearnand Scand ?
Sacc ?
Slearn6.
If a suitable set of patterns has been learnedthen stop, otherwise go to step 42.1 Document-centric approachA key choice in the development of such an algo-rithm is step 4, the process of ranking the candidatepatterns, which effectively determines which of thecandidate patterns will be learned.
Yangarber et al(2000) chose an approach motivated by the assump-tion that documents containing a large number ofpatterns already identified as relevant to a particu-lar IE scenario are likely to contain further relevantpatterns.
This approach, which can be viewed as be-ing document-centric, operates by associating confi-dence scores with patterns and relevance scores withdocuments.
Initially seed patterns are given a maxi-mum confidence score of 1 and all others a 0 score.Each document is given a relevance score based onthe patterns which occur within it.
Candidate pat-terns are ranked according to the proportion of rele-vant and irrelevant documents in which they occur,those found in relevant documents far more than inirrelevant ones are ranked highly.
After new patternshave been accepted all patterns?
confidence scoresare updated, based on the documents in which theyoccur, and documents?
relevance according to theaccepted patterns they contain.This approach has been shown to successfully ac-quire useful extraction patterns which, when addedto an IE system, improved its performance (Yangar-ber et al, 2000).
However, it relies on an assump-tion about the way in which relevant patterns are dis-tributed in a document collection and may learn pat-terns which tend to occur in the same documents asrelevant ones whether or not they are actually rele-vant.
For example, we could imagine an IE scenarioin which relevant documents contain a piece of in-formation which is related to, but distinct from, theinformation we aim to extract.
If patterns expressingthis information were more likely to occur in rele-vant documents than irrelevant ones the document-centric approach would also learn the irrelevant pat-terns.Rather than focusing on the documents matchedby a pattern, an alternative approach is to rank pat-terns according to how similar their meanings areto those which are known to be relevant.
Thissemantic-similarity approach avoids the problemwhich may be present in the document-centric ap-proach since patterns which happen to co-occur inthe same documents as relevant ones but have dif-ferent meanings will not be ranked highly.
We nowgo on to describe a new algorithm which implementsthis approach.3803 Semantic IE Pattern LearningFor these experiments extraction patterns consist ofpredicate-argument structures, as proposed by Yan-garber (2003).
Under this scheme patterns consistof triples representing the subject, verb and object(SVO) of a clause.
The first element is the ?se-mantic?
subject (or agent), for example ?John?
is aclausal subject in each of these sentences ?John hitBill?, ?Bill was hit by John?, ?Mary saw John hitBill?, and ?John is a bully?.
The second element isthe verb in the clause and the third the object (pa-tient) or predicate.
?Bill?
is a clausal object in thefirst three example sentences and ?bully?
in the finalone.
When a verb is being used intransitively, thepattern for that clause is restricted to only the firstpair of elements.The filler of each pattern element can be eithera lexical item or semantic category such as per-son name, country, currency values, numerical ex-pressions etc.
In this paper lexical items are rep-resented in lower case and semantic categories arecapitalised.
For example, in the pattern COM-PANY+fired+ceo, fired and ceo are lexicalitems and COMPANY a semantic category whichcould match any lexical item belonging to that type.The algorithm described here relies on identify-ing patterns with similar meanings.
The approachwe have developed to do this is inspired by thevector space model which is commonly used inInformation Retrieval (Salton and McGill, 1983)and language processing in general (Pado and La-pata, 2003).
Each pattern can be represented asa set of pattern element-filler pairs.
For exam-ple, the pattern COMPANY+fired+ceo consistsof three pairs: subject COMPANY, verb firedand object ceo.
Each pair consists of either alexical item or semantic category, and pattern ele-ment.
Once an appropriate set of pairs has been es-tablished a pattern can be represented as a binaryvector in which an element with value 1 denotes thatthe pattern contains a particular pair and 0 that itdoes not.3.1 Pattern SimilarityThe similarity of two pattern vectors can be com-pared using the measure shown in Equation 1.
Here~a and~b are pattern vectors, ~bT the transpose of~b andPatterns Matrix labelsa.
chairman+resign 1. subject chairmanb.
ceo+quit 2. subject ceoc.
chairman+comment 3. verb resign4.
verb quit5.
verb commentSimilarity matrix Similarity values1 0.95 0 0 00.95 1 0 0 00 0 1 0.9 0.10 0 0.9 1 0.10 0 0.1 0.1 1sim(~a,~b) = 0.925sim(~a, ~c) = 0.55sim(~b, ~c) = 0.525Figure 1: Similarity scores and matrix for an exam-ple vector space formed from three patternsW a matrix that lists the similarity between each ofthe possible pattern element-filler pairs.sim(~a,~b) = ~aW~bT|~a||~b|(1)The semantic similarity matrix W contains infor-mation about the similarity of each pattern element-filler pair stored as non-negative real numbers and iscrucial for this measure.
Assume that the set of pat-terns, P , consists of n element-filler pairs denotedby p1, p2, ...pn.
Each row and column of W rep-resents one of these pairs and they are consistentlylabelled.
So, for any i such that 1 ?
i ?
n, row i andcolumn i are both labelled with pair pi.
If wij is theelement of W in row i and column j then the valueof wij represents the similarity between the pairs piand pj .
Note that we assume the similarity of twoelement-filler pairs is symmetric, so wij = wji and,consequently, W is a symmetric matrix.
Pairs withdifferent pattern elements (i.e.
grammatical roles)are automatically given a similarity score of 0.
Di-agonal elements of W represent the self-similaritybetween pairs and have the greatest values.Figure 1 shows an example using three patterns,chairman+resign, ceo+quit and chair-man+comment.
This shows how these patterns arerepresented as vectors and gives a sample semanticsimilarity matrix.
It can be seen that the first pairof patterns are the most similar using the proposedmeasure.The measure in Equation 1 is similar to the cosinemetric, commonly used to determine the similarityof documents in the vector space model approach381to Information Retrieval.
However, the cosine met-ric will not perform well for our application since itdoes not take into account the similarity between el-ements of a vector and would assign equal similarityto each pair of patterns in the example shown in Fig-ure 1.1 The semantic similarity matrix in Equation 1provides a mechanism to capture semantic similar-ity between lexical items which allows us to identifychairman+resign and ceo+quit as the mostsimilar pair of patterns.3.2 Populating the MatrixIt is important to choose appropriate values for theelements of W .
We chose to make use of the re-search that has concentrated on computing similar-ity between pairs of lexical items using the WordNethierarchy (Resnik, 1995; Jiang and Conrath, 1997;Patwardhan et al, 2003).
We experimented withseveral of the measures which have been reportedin the literature and found that the one proposed byJiang and Conrath (1997) to be the most effective.The similarity measure proposed by Jiang andConrath (1997) relies on a technique developed byResnik (1995) which assigns numerical values toeach sense in the WordNet hierarchy based uponthe amount of information it represents.
These val-ues are derived from corpus counts of the words inthe synset, either directly or via the hyponym rela-tion and are used to derive the Information Content(IC) of a synset c thus IC(c) = ?
log(Pr(c)).
Fortwo senses, s1 and s2, the lowest common subsumer,lcs(s1, s2), is defined as the sense with the highestinformation content (most specific) which subsumesboth senses in the WordNet hierarchy.
Jiang andConrath used these elements to calculate the seman-tic distance between a pair or words, w1 and w2, ac-cording to this formula (where senses(w) is the set1The cosine metric for a pair of vectors is given by the cal-culation a.b|a||b| .
Substituting the matrix multiplication in the nu-merator of Equation 1 for the dot product of vectors ~a and ~bwould give the cosine metric.
Note that taking the dot productof a pair of vectors is equivalent to multiplying by the identitymatrix, i.e.
~a.~b = ~aI ~bT .
Under our interpretation of the simi-larity matrix, W , this equates to each pattern element-filler pairbeing identical to itself but not similar to anything else.of all possible WordNet senses for word w):ARGMAXs1  senses(w1),s2  senses(w2)IC(s1)+IC(s2)?2?IC(lcs(s1, s2))(2)Patwardhan et al (2003) convert this distancemetric into a similarity measure by taking its mul-tiplicative inverse.
Their implementation was usedin the experiments described later.As mentioned above, the second part of a patternelement-filler pair can be either a lexical item or asemantic category, such as company.
The identifiersused to denote these categories, i.e.
COMPANY, donot appear in WordNet and so it is not possible todirectly compare their similarity with other lexicalitems.
To avoid this problem these tokens are man-ually mapped onto the most appropriate node in theWordNet hierarchy which is then used for similar-ity calculations.
This mapping process is not partic-ularly time-consuming since the number of namedentity types with which a corpus is annotated is usu-ally quite small.
For example, in the experimentsdescribed in this paper just seven semantic classeswere sufficient to annotate the corpus.3.3 Learning AlgorithmThis pattern similarity measure can be used to createa weakly supervised approach to pattern acquisitionfollowing the general outline provided in Section 2.Each candidate pattern is compared against the setof currently accepted patterns using the measure de-scribed in Section 3.1.
We experimented with sev-eral techniques for ranking candidate patterns basedon these scores, including using the best and aver-age score, and found that the best results were ob-tained when each candidate pattern was ranked ac-cording to its score when compared against the cen-troid vector of the set of currently accepted patterns.We also experimented with several schemes for de-ciding which of the scored patterns to accept (a fulldescription would be too long for this paper) result-ing in a scheme where the four highest scoring pat-terns whose score is within 0.95 of the best patternare accepted.Our algorithm disregards any patterns whose cor-pus occurrences are below a set threshold, ?, sincethese may be due to noise.
In addition, a second382threshold, ?, is used to determine the maximumnumber of documents in which a pattern can occursince these very frequent patterns are often too gen-eral to be useful for IE.
Patterns which occur in morethan ?
?C, where C is the number of documents inthe collection, are not learned.
For the experimentsin this paper we set ?
to 2 and ?
to 0.3.4 ImplementationA number of pre-processing stages have to be ap-plied to documents in order for the set of patterns tobe extracted before learning can take place.
Firstly,items belonging to semantic categories are identi-fied by running the text through the named entityidentifier in the GATE system (Cunningham et al,2002).
The corpus is then parsed, using a ver-sion of MINIPAR (Lin, 1999) adapted to processtext marked with named entities, to produce depen-dency trees from which SVO-patterns are extracted.Active and passive voice is taken into account inMINIPAR?s output so the sentences ?COMPANYfired their C.E.O.?
and ?The C.E.O.
was fired byCOMPANY?
would yield the same triple, COM-PANY+fire+ceo.
The indirect object of ditran-sitive verbs is not extracted; these verbs are treatedlike transitive verbs for the purposes of this analysis.An implementation of the algorithm describedin Section 3 was completed in addition to an im-plementation of the document-centric algorithm de-scribed in Section 2.1.
It is important to mentionthat this implementation is not identical to the onedescribed by Yangarber et al (2000).
Their systemmakes some generalisations across pattern elementsby grouping certain elements together.
However,there is no difference between the expressiveness ofthe patterns learned by either approach and we donot believe this difference has any effect on the re-sults of our experiments.5 EvaluationVarious approaches have been suggested for theevaluation of automatic IE pattern acquisition.Riloff (1996) judged the precision of patternslearned by reviewing them manually.
Yangarber etal.
(2000) developed an indirect method which al-lowed automatic evaluation.
In addition to learninga set of patterns, their system also notes the rele-vance of documents based on the current set of ac-cepted patterns.
Assuming the subset of documentsrelevant to a particular IE scenario is known, it ispossible to use these relevance judgements to de-termine how accurately a given set of patterns candiscriminate the relevant documents from the irrele-vant.
This evaluation is similar to the ?text-filtering?sub-task used in the sixth Message UnderstandingConference (MUC-6) (1995) in which systems wereevaluated according to their ability to identify thedocuments relevant to the extraction task.
The doc-ument filtering evaluation technique was used to al-low comparison with previous studies.Identifying the document containing relevant in-formation can be considered as a preliminary stageof an IE task.
A further step is to identify the sen-tences within those documents which are relevant.This ?sentence filtering?
task is a more fine-grainedevaluation and is likely to provide more informationabout how well a given set of patterns is likely toperform as part of an IE system.
Soderland (1999)developed a version of the MUC-6 corpus in whichevents are marked at the sentence level.
The set ofpatterns learned by the algorithm after each iterationcan be compared against this corpus to determinehow accurately they identify the relevant sentencesfor this extraction task.5.1 Evaluation CorpusThe evaluation corpus used for the experiments wascompiled from the training and testing corpus usedin MUC-6, where the task was to extract informationabout the movements of executives from newswiretexts.
A document is relevant if it has a filled tem-plate associated with it.
590 documents from a ver-sion of the MUC-6 evaluation corpus described bySoderland (1999) were used.After the pre-processing stages described in Sec-tion 4, the MUC-6 corpus produced 15,407 patterntokens from 11,294 different types.
10,512 patternsappeared just once and these were effectively dis-carded since our learning algorithm only considerspatterns which occur at least twice (see Section 3.3).The document-centric approach benefits from alarge corpus containing a mixture of relevant and ir-relevant documents.
We provided this using a subsetof the Reuters Corpus Volume I (Rose et al, 2002)which, like the MUC-6 corpus, consists of newswire383COMPANY+appoint+PERSONCOMPANY+elect+PERSONCOMPANY+promote+PERSONCOMPANY+name+PERSONPERSON+resignPERSON+departPERSON+quitTable 1: Seed patterns for extraction tasktexts.
3000 documents relevant to the managementsuccession task (identified using document meta-data) and 3000 irrelevant documents were used toproduce the supplementary corpus.
This supple-mentary corpus yielded 126,942 pattern tokens and79,473 types with 14,576 of these appearing morethan once.
Adding the supplementary corpus to thedata set used by the document-centric approach ledto an improvement of around 15% on the documentfiltering task and over 70% for sentence filtering.
Itwas not used for the semantic similarity algorithmsince there was no benefit.The set of seed patterns listed in Table 1 are in-dicative of the management succession extractiontask and were used for these experiments.6 Results6.1 Document FilteringResults for both the document and sentence filter-ing experiments are reported in Table 2 which listsprecision, recall and F-measure for each approachon both evaluations.
Results from the document fil-tering experiment are shown on the left hand sideof the table and continuous F-measure scores forthe same experiment are also presented in graphi-cal format in Figure 2.
While the document-centricapproach achieves the highest F-measure of eithersystem (0.83 on the 33rd iteration compared against0.81 after 48 iterations of the semantic similarity ap-proach) it only outperforms the proposed approachfor a few iterations.
In addition the semantic sim-ilarity approach learns more quickly and does notexhibit as much of a drop in performance after it hasreached its best value.
Overall the semantic sim-ilarity approach was found to be significantly bet-ter than the document-centric approach (p < 0.001,Wilcoxon Signed Ranks Test).Although it is an informative evaluation, the doc-ument filtering task is limited for evaluating IE pat-0 20 40 60 80 100 120Iteration0.400.450.500.550.600.650.700.750.80F-measureSemantic SimilarityDocument-centricFigure 2: Evaluating document filtering.tern learning.
This evaluation indicates whether theset of patterns being learned can identify documentscontaining descriptions of events but does not pro-vide any information about whether it can find thoseevents within the documents.
In addition, the set ofseed patterns used for these experiments have a highprecision and low recall (Table 2).
We have foundthat the distribution of patterns and documents inthe corpus means that learning virtually any patternwill help improve the F-measure.
Consequently, webelieve the sentence filtering evaluation to be moreuseful for this problem.6.2 Sentence FilteringResults from the sentence filtering experiment areshown in tabular format in the right hand side ofTable 22 and graphically in Figure 3.
The seman-tic similarity algorithm can be seen to outperformthe document-centric approach.
This difference isalso significant (p < 0.001, Wilcoxon Signed RanksText).The clear difference between these results showsthat the semantic similarity approach can indeedidentify relevant sentences while the document-centric method identifies patterns which match rel-evant documents, although not necessarily relevantsentences.2The set of seed patterns returns a precision of 0.81 for thistask.
The precision is not 1 since the pattern PERSON+resignmatches sentences describing historical events (?Jones resignedlast year.?)
which were not marked as relevant in this corpusfollowing MUC guidelines.384Document Filtering Sentence FilteringNumber of Document-centric Semantic similarity Document-centric Semantic similarityIterations P R F P R F P R F P R F0 1.00 0.26 0.42 1.00 0.26 0.42 0.81 0.10 0.18 0.81 0.10 0.1820 0.75 0.68 0.71 0.77 0.78 0.77 0.30 0.29 0.29 0.61 0.49 0.5440 0.72 0.96 0.82 0.70 0.93 0.80 0.40 0.67 0.51 0.47 0.64 0.5560 0.65 0.96 0.78 0.68 0.96 0.80 0.32 0.70 0.44 0.42 0.73 0.5480 0.56 0.96 0.71 0.61 0.98 0.76 0.18 0.71 0.29 0.37 0.89 0.52100 0.56 0.96 0.71 0.58 0.98 0.73 0.18 0.73 0.28 0.28 0.92 0.42120 0.56 0.96 0.71 0.58 0.98 0.73 0.17 0.75 0.28 0.26 0.95 0.41Table 2: Comparison of the different approaches over 120 iterations0 20 40 60 80 100 120Iteration0.150.200.250.300.350.400.450.500.55F-measureSemantic SimilarityDocument-centricFigure 3: Evaluating sentence filtering.The precision scores for the sentence filtering taskin Table 2 show that the semantic similarity al-gorithm consistently learns more accurate patternsthan the existing approach.
At the same time itlearns patterns with high recall much faster than thedocument-centric approach, by the 120th iterationthe pattern set covers almost 95% of relevant sen-tences while the document-centric approach coversonly 75%.7 DiscussionThe approach to IE pattern acquisition presentedhere is related to other techniques but uses differ-ent assumptions regarding which patterns are likelyto be relevant to a particular extraction task.
Eval-uation has showed that the semantic generalisa-tion approach presented here performs well whencompared to a previously reported document-centricmethod.
Differences between the two approachesare most obvious when the results of the sentencefiltering task are considered and it seems that this isa more informative evaluation for this problem.
Thesemantic similarity approach has the additional ad-vantage of not requiring a large corpus containing amixture of documents relevant and irrelevant to theextraction task.
This corpus is unannotated, and somay not be difficult to obtain, but is nevertheless anadditional requirement.The best score recorded by the proposed algo-rithm on the sentence filtering task is an F-measureof 0.58 (22nd iteration).
While this result is lowerthan those reported for IE systems based on knowl-edge engineering approaches these results should beplaced in the context of a weakly supervised learningalgorithm which could be used to complement man-ual approaches.
These results could be improved bymanual filtering the patterns identified by the algo-rithm.The learning algorithm presented in Section 3 in-cludes a mechanism for comparing two extractionpatterns using information about lexical similarityderived from WordNet.
This approach is not re-stricted to this application and could be applied toother language processing tasks such as question an-swering, paraphrase identification and generation oras a variant of the vector space model commonlyused in Information Retrieval.
In addition, Sudoet al (2003) proposed representations for IE pat-terns which extends the SVO representation usedhere and, while they did not appear to significantlyimprove IE, it is expected that it will be straightfor-ward to extend the vector space model to those pat-385tern representations.One of the reasons for the success of the approachdescribed here is the appropriateness of WordNetwhich is constructed on paradigmatic principles,listing the words which may be substituted for oneanother, and is consequently an excellent resourcefor this application.
WordNet is also a genericresource not associated with a particular domainwhich means the learning algorithm can make useof that knowledge to acquire patterns for a diverserange of IE tasks.
This work represents a step to-wards truly domain-independent IE systems.
Em-ploying a weakly supervised learning algorithm re-moves much of the requirement for a human anno-tator to provide example patterns.
Such approachesare often hampered by a lack of information but theadditional knowledge in WordNet helps to compen-sate.AcknowledgementsThis work was carried out as part of the RE-SuLT project funded by the EPSRC (GR/T06391).Roman Yangarber provided advice on the re-implementation of the document-centric algorithm.We are also grateful for the detailed comments pro-vided by the anonymous reviewers of this paper.ReferencesH.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: an Architecture for Devel-opment of Robust HLT.
In Proceedings of the 40thAnniversary Meeting of the Association for Computa-tional Linguistics (ACL-02), pages 168?175, Philadel-phia, PA.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and some of its Applications.
MIT Press,Cambridge, MA.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings of International Conference on Research in Com-putational Linguistics, Taiwan.W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff,and S. Soderland.
1992.
University of Massachusetts:Description of the CIRCUS System used for MUC-4.In Proceedings of the Fourth Message UnderstandingConference (MUC-4), pages 282?288, San Francisco,CA.D.
Lin.
1999.
MINIPAR: a minimalist parser.
In Mary-land Linguistics Colloquium, University of Maryland,College Park.MUC.
1995.
Proceedings of the Sixth Message Under-standing Conference (MUC-6), San Mateo, CA.
Mor-gan Kaufmann.S.
Pado and M. Lapata.
2003.
Constructing semanticspace models from parsed corpora.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03), pages 128?135, Sap-poro, Japan.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.
Us-ing measures of semantic relatedness for word sensedisambiguation.
In Proceedings of the Fourth Inter-national Conferences on Intelligent Text Processingand Computational Linguistics, pages 241?257, Mex-ico City.P.
Resnik.
1995.
Using Information Content to evalu-ate Semantic Similarity in a Taxonomy.
In Proceed-ings of the 14th International Joint Conference on Ar-tificial Intelligence (IJCAI-95), pages 448?453, Mon-treal, Canada.E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In Thirteenth NationalConference on Artificial Intelligence (AAAI-96), pages1044?1049, Portland, OR.T.
Rose, M. Stevenson, and M. Whitehead.
2002.
TheReuters Corpus Volume 1 - from Yesterday?s news totomorrow?s language resources.
In LREC-02, pages827?832, La Palmas, Spain.G.
Salton and M. McGill.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill, New York.S.
Soderland.
1999.
Learning Information ExtractionRules for Semi-structured and free text.
MachineLearning, 31(1-3):233?272.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03), pages 224?231.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic acquisition of domainknowledge for information extraction.
In Proceed-ings of the 18th International Conference on Compu-tational Linguistics (COLING 2000), pages 940?946,Saarbru?cken, Germany.R.
Yangarber.
2003.
Counter-training in the discovery ofsemantic patterns.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics (ACL-03), pages 343?350, Sapporo, Japan.386
