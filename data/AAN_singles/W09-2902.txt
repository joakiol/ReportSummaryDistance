Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9?16,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPRe-examining Automatic Keyphrase Extraction Approachesin Scientific ArticlesSu Nam KimCSSE dept.University of Melbournesnkim@csse.unimelb.edu.auMin-Yen KanDepartment of Computer ScienceNational University of Singaporekanmy@comp.nus.edu.sgAbstractWe tackle two major issues in automatickeyphrase extraction using scientific arti-cles: candidate selection and feature engi-neering.
To develop an efficient candidateselection method, we analyze the natureand variation of keyphrases and then se-lect candidates using regular expressions.Secondly, we re-examine the existing fea-tures broadly used for the supervised ap-proach, exploring different ways to en-hance their performance.
While mostother approaches are supervised, we alsostudy the optimal features for unsuper-vised keyphrase extraction.
Our researchhas shown that effective candidate selec-tion leads to better performance as evalua-tion accounts for candidate coverage.
Ourwork also attests that many of existing fea-tures are also usable in unsupervised ex-traction.1 IntroductionKeyphrases are simplex nouns or noun phrases(NPs) that represent the key ideas of the document.Keyphrases can serve as a representative summaryof the document and also serve as high quality in-dex terms.
It is thus no surprise that keyphraseshave been utilized to acquire critical informationas well as to improve the quality of natural lan-guage processing (NLP) applications such as doc-ument summarizer(Da?vanzo and Magnini, 2005),information retrieval (IR)(Gutwin et al, 1999) anddocument clustering(Hammouda et al, 2005).In the past, various attempts have been made toboost automatic keyphrase extraction performancebased primarily on statistics(Frank et al, 1999;Turney, 2003; Park et al, 2004; Wan and Xiao,2008) and a rich set of heuristic features(Barkerand Corrnacchia, 2000; Medelyan and Witten,2006; Nguyen and Kan, 2007).
In Section 2, wegive a more comprehensive overview of previousattempts.Current keyphrase technology still has muchroom for improvement.
First of all, although sev-eral candidate selection methods have been pro-posed for automatic keyphrase extraction in thepast (e.g.
(Frank et al, 1999; Park et al, 2004;Nguyen and Kan, 2007)), most of them do not ef-fectively deal with various keyphrase forms whichresults in the ignorance of some keyphrases as can-didates.
Moreover, no studies thus far have donea detailed investigation of the nature and varia-tion of manually-provided keyphrases.
As a con-sequence, the community lacks a standardized listof candidate forms, which leads to difficulties indirect comparison across techniques during evalu-ation and hinders re-usability.Secondly, previous studies have shown the ef-fectiveness of their own features but not manycompared their features with other existing fea-tures.
That leads to a redundancy in studies andhinders direct comparison.
In addition, existingfeatures are specifically designed for supervisedapproaches with few exceptions.
However, thisapproach involves a large amount of manual labor,thus reducing its utility for real-world application.Hence, unsupervised approach is inevitable in or-der to minimize manual tasks and to encourageutilization.
It is a worthy study to attest the re-liability and re-usability for the unsupervised ap-proach in order to set up the tentative guideline forapplications.This paper targets to resolve these issues ofcandidate selection and feature engineering.
Inour work on candidate selection, we analyze thenature and variation of keyphrases with the pur-pose of proposing a candidate selection methodwhich improves the coverage of candidates thatoccur in various forms.
Our second contributionre-examines existing keyphrase extraction features9reported in the literature, in terms of their effec-tiveness and re-usability.
We test and comparethe usefulness of each feature for further improve-ment.
In addition, we assess how well these fea-tures can be applied in an unsupervised approach.In the remaining sections, we describe anoverview of related work in Section 2, our propos-als on candidate selection and feature engineeringin Section 4 and 5, our system architecture anddata in Section 6.
Then, we evaluate our propos-als, discuss outcomes and conclude our work inSection 7, 8 and 9, respectively.2 Related WorkThe majority of related work has been carriedout using statistical approaches, a rich set ofsymbolic resources and linguistically-motivatedheuristics(Frank et al, 1999; Turney, 1999; Barkerand Corrnacchia, 2000; Matsuo and Ishizuka,2004; Nguyen and Kan, 2007).
Features used canbe categorized into three broad groups: (1) docu-ment cohesion features (i.e.
relationship betweendocument and keyphrases)(Frank et al, 1999;Matsuo and Ishizuka, 2004; Medelyan and Wit-ten, 2006; Nguyen and Kan, 2007), and to lesser,(2) keyphrase cohesion features (i.e.
relationshipamong keyphrases)(Turney, 2003) and (3) termcohesion features (i.e.
relationship among compo-nents in a keyphrase)(Park et al, 2004).The simplest system is KEA (Frank et al,1999; Witten et al, 1999) that uses TF*IDF (i.e.term frequency * inverse document frequency) andfirst occurrence in the document.
TF*IDF mea-sures the document cohesion and the first occur-rence implies the importance of the abstract orintroduction which indicates the keyphrases havea locality.
Turney (2003) added the notion ofkeyphrase cohesion to KEA features and Nguyenand Kan (2007) added linguistic features suchas section information and suffix sequence.
TheGenEx system(Turney, 1999) employed an inven-tory of nine syntactic features, such as length inwords and frequency of stemming phrase as aset of parametrized heuristic rules.
Barker andCorrnacchia (2000) introduced a method basedon head noun heuristics that took three features:length of candidate, frequency and head noun fre-quency.
To take advantage of domain knowledge,Hulth et al (2001) used a hierarchically-organizeddomain-specific thesaurus from Swedish Parlia-ment as a secondary knowledge source.
TheTextract (Park et al, 2004) also ranks the can-didate keyphrases by its judgment of keyphrases?degree of domain specificity based on subject-specific collocations(Damerau, 1993), in addi-tion to term cohesion using Dice coefficient(Dice,1945).
Recently, Wan and Xiao (2008) extractsautomatic keyphrases from single documents, uti-lizing document clustering information.
The as-sumption behind this work is that the documentswith the same or similar topics interact with eachother in terms of salience of words.
The authorsfirst clustered the documents then used the graph-based ranking algorithm to rank the candidates ina document by making use of mutual influences ofother documents in the same cluster.3 Keyphrase AnalysisIn previous study, KEA employed the index-ing words as candidates whereas others such as(Park et al, 2004; Nguyen and Kan, 2007) gen-erated handcrafted regular expression rules.
How-ever, none carefully undertook the analysis ofkeyphrases.
We believe there is more to be learnedfrom the reference keyphrases themselves by do-ing a fine-grained, careful analysis of their formand composition.
Note that we used the articlescollected from ACM digital library for both ana-lyzing keyphrases as well as evaluating methods.See Section 6 for data in detail.Syntactically, keyphrases can be formed by ei-ther simplex nouns (e.g.
algorithm, keyphrase,multi-agent) or noun phrases (NPs) which can be asequence of nouns and their auxiliary words suchas adjectives and adverbs (e.g.
mobile network,fast computing, partially observable Markov de-cision process) despite few incidences.
They canalso incorporate a prepositional phrase (PP) (e.g.quality of service, policy of distributed caching).When keyphrases take the form of an NP with anattached PP (i.e.
NPs in of-PP form), the preposi-tion of is most common, but others such as for, in,via also occur (e.g.
incentive for cooperation, in-equality in welfare, agent security via approximatepolicy, trade in financial instrument based on log-ical formula).
The patterns above correlate wellto part-of-speech (POS) patterns used in modernkeyphrase extraction systems.However, our analysis uncovered additional lin-guistic patterns and alternations which other stud-ies may have overlooked.
In our study we alsofound that keyphrases also occur as a simple con-10Criteria RulesFrequency (Rule1) Frequency heuristic i.e.
frequency ?
2 for simplex words vs. frequency ?
1 for NPsLength (Rule2) Length heuristic i.e.
up to length 3 for NPs in non-of-PP form vs. up to length 4 for NPs in of-PP form(e.g.
synchronous concurrent program vs. model of multiagent interaction)Alternation (Rule3) of-PP form alternation(e.g.
number of sensor = sensor number, history of past encounter = past encounter history)(Rule4) Possessive alternation(e.g.
agent?s goal = goal of agent, security?s value = value of security)Extraction (Rule5) Noun Phrase = (NN |NNS|NNP |NNPS|JJ |JJR|JJS)?
(NN |NNS|NNP |NNPS)(e.g.
complexity, effective algorithm, grid computing, distributed web-service discovery architecture)(Rule6) Simplex Word/NP IN Simplex Word/NP(e.g.
quality of service, sensitivity of VOIP traffic (VOIP traffic extracted),simplified instantiation of zebroid (simplified instantiation extracted))Table 1: Candidate Selection Rulesjunctions (e.g.
search and rescue, propagation anddelivery), and much more rarely, as conjunctionsof more complex NPs (e.g.
history of past en-counter and transitivity).
Some keyphrases appearto be more complex (e.g.
pervasive document editand management system, task and resource allo-cation in agent system).
Similarly, abbreviationsand possessive forms figure as common patterns(e.g.
belief desire intention = BDI, inverse docu-ment frequency = (IDF); Bayes?
theorem, agent?sdominant strategy).A critical insight of our work is that keyphrasescan be morphologically and semantically altered.Keyphrases that incorporate a PP or have an un-derlying genitive composition are often easily var-ied by word order alternation.
Previous studieshave used the altered keyphrases when forming inof-PP form.
For example, quality of service canbe altered to service quality, sometimes with lit-tle semantic difference.
Also, as most morpho-logical variation in English relates to noun num-ber and verb inflection, keyphrases are subject tothese rules as well (e.g.
distributed system 6= dis-tributing system, dynamical caching 6= dynamicalcache).
In addition, possessives tend to alternatewith of-PP form (e.g.
agent?s goal = goal of agent,security?s value = value of security).4 Candidate SelectionWe now describe our proposed candidate selectionprocess.
Candidate selection is a crucial step forautomatic keyphrase extraction.
This step is corre-lated to term extraction study since top Nth rankedterms become keyphrases in documents.
In pre-vious study, KEA employed the indexing wordsas candidates whereas others such as (Park et al,2004; Nguyen and Kan, 2007) generated hand-crafted regular expression rules.
However, nonecarefully undertook the analysis of keyphrases.
Inthis section, before we present our method, we firstdescribe the detail of keyphrase analysis.In our keyphrase analysis, we observed thatmost of author assigned keyphrase and/or readerassigned keyphrase are syntactically more of-ten simplex words and less often NPs.
Whenkeyphrases take an NP form, they tend to be a sim-ple form of NPs.
i.e.
either without a PP or withonly a PP or with a conjunction, but few appear asa mixture of such forms.
We also noticed that thecomponents of NPs are normally nouns and adjec-tives but rarely, are adverbs and verbs.
As a re-sult, we decided to ignore NPs containing adverbsand verbs in this study as our candidates since theytend to produce more errors and to require morecomplexity.Another observation is that keyphrases contain-ing more than three words are rare (i.e.
6% in ourdata set), validating what Paukkeri et al (2008)observed.
Hence, we apply a length heuristic.
Ourcandidate selection rule collects candidates up tolength 3, but also of length 4 for NPs in of-PPform, since they may have a non-genetive alter-nation that reduces its length to 3 (e.g.
perfor-mance of distributed system = distributed systemperformance).
In previous studies, words occur-ring at least twice are selected as candidates.
How-ever, during our acquisition of reader assignedkeyphrase, we observed that readers tend to collectNPs as keyphrases, regardless of their frequency.Due to this, we apply different frequency thresh-olds for simplex words (>= 2) and NPs (>= 1).Note that 30% of NPs occurred only once in ourdata.Finally, we generated regular expression rulesto extract candidates, as presented in Table 1.
Ourcandidate extraction rules are based on those inNguyen and Kan (2007).
However, our Rule6for NPs in of-PP form broadens the coverage of11possible candidates.
i.e.
with a given NPs in of-PP form, not only we collect simplex word(s),but we also extract non-of-PP form of NPs fromnoun phrases governing the PP and the PP.
Forexample, our rule extracts effective algorithm ofgrid computing as well as effective algorithm andgrid computing as candidates while the previousworks?
rules do not.5 Feature EngineeringWith a wider candidate selection criteria, the onusof filtering out irrelevant candidates becomes theresponsibility of careful feature engineering.
Welist 25 features that we have found useful in ex-tracting keyphrases, comprising of 9 existing and16 novel and/or modified features that we intro-duce in our work (marked with ?).
As one ofour goals in feature engineering is to assess thesuitability of features in the unsupervised setting,we have also indicated which features are suitableonly for the supervised setting (S) or applicable toboth (S, U).5.1 Document CohesionDocument cohesion indicates how important thecandidates are for the given document.
The mostpopular feature for this cohesion is TF*IDF butsome works have also used context words to checkthe correlation between candidates and the givendocument.
Other features for document cohesionare distance, section information and so on.
Wenote that listed features other than TF*IDF are re-lated to locality.
That is, the intuition behind thesefeatures is that keyphrases tend to appear in spe-cific area such as the beginning and the end of doc-uments.F1 : TF*IDF (S,U) TF*IDF indicates doc-ument cohesion by looking at the frequency ofterms in the documents and is broadly used in pre-vious work(Frank et al, 1999; Witten et al, 1999;Nguyen and Kan, 2007).
However, a disadvan-tage of the feature is in requiring a large corpusto compute useful IDF.
As an alternative, con-text words(Matsuo and Ishizuka, 2004) can alsobe used to measure document cohesion.
From ourstudy of keyphrases, we saw that substrings withinlonger candidates need to be properly counted, andas such our method measures TF in substrings aswell as in exact matches.
For example, grid com-puting is often a substring of other phrases such asgrid computing algorithm and efficient grid com-puting algorithm.
We also normalize TF with re-spect to candidate types: i.e.
we separately treatsimplex words and NPs to compute TF.
To makeour IDFs broadly representative, we employed theGoogle n-gram counts, that were computedover terabytes of data.
Given this large, genericsource of word count, IDF can be incorporatedwithout corpus-dependent processing, hence suchfeatures are useful in unsupervised approachesas well.
The following list shows variations ofTF*IDF, employed as features in our system.?
(F1a) TF*IDF?
(F1b*) TF including counts of substrings?
(F1c*) TF of substring as a separate feature?
(F1d*) normalized TF by candidate types(i.e.
simplex words vs. NPs)?
(F1e*) normalized TF by candidate types asa separate feature?
(F1f*) IDF using Google n-gramF2 : First Occurrence (S,U) KEA used the firstappearance of the word in the document(Frank etal., 1999; Witten et al, 1999).
The main ideabehind this feature is that keyphrases tend to oc-cur in the beginning of documents, especially instructured reports (e.g., in abstract and introduc-tion sections) and newswire.F3 : Section Information (S,U) Nguyen andKan (2007) used the identity of which specificdocument section a candidate occurs in.
This lo-cality feature attempts to identify key sections.
Forexample, in their study of scientific papers, theauthors weighted candidates differently dependingon whether they occurred in the abstract, introduc-tion, conclusion, section head, title and/or refer-ences.F4* : Additional Section Information (S,U)We first added the related work or previous workas one of section information not included inNguyen and Kan (2007).
We also propose and testa number of variations.
We used the substringsthat occur in section headers and reference titlesas keyphrases.
We counted the co-occurrence ofcandidates (i.e.
the section TF) across all key sec-tions that indicates the correlation among key sec-tions.
We assign section-specific weights as in-dividual sections exhibit different propensities forgenerating keyphrases.
For example, introduction12contains the majority of keyphrases while the ti-tle or section head contains many fewer due to thevariation in size.?
(F4a*) section, ?related/previous work??
(F4b*) counting substring occurring in keysections?
(F4c*) section TF across all key sections?
(F4d*) weighting key sections according tothe portion of keyphrases foundF5* : Last Occurrence (S,U) Similar to dis-tance in KEA , the position of the last occurrenceof a candidate may also imply the importance ofkeyphrases, as keyphrases tend to appear in thelast part of document such as the conclusion anddiscussion.5.2 Keyphrase CohesionThe intuition behind using keyphrase cohesion isthat actual keyphrases are often associated witheach other, since they are semantically related totopic of the document.
Note that this assumptionholds only when the document describes a single,coherent topic ?
a document that represents a col-lection may be first need to be segmented into itsconstituent topics.F6* : Co-occurrence of Another Candidatein Section (S,U) When candidates co-occur inseveral key sections together, then they are morelikely keyphrases.
Hence, we used the number ofsections that candidates co-occur.F7* : Title overlap (S) In a way, titles also rep-resent the topics of their documents.
A large col-lection of titles in the domain can act as a prob-abilistic prior of what words could stand as con-stituent words in keyphrases.
In our work, as weexamined scientific papers from computer science,we used a collection of titles obtained from thelarge CiteSeer1 collection to create this feature.?
(F7a*) co-occurrence (Boolean) in title col-location?
(F7b*) co-occurrence (TF) in title collectionF8 : Keyphrase Cohesion (S,U) Turney (2003)integrated keyphrase cohesion into his system bychecking the semantic similarity between top Nranked candidates against the remainder.
In the1It contains 1.3M titles from articles, papers and reports.original work, a large, external web corpus wasused to obtain the similarity judgments.
As wedid not have access to the same web corpus andall candidates/keyphrases were not found in theGoogle n-gram corpus, we approximated this fea-ture using a similar notion of contextual similarity.We simulated a latent 2-dimensional matrix (simi-lar to latent semantic analysis) by listing all candi-date words in rows and their neighboring words(nouns, verbs, and adjectives only) in columns.The cosine measure is then used to compute thesimilarity among keyphrases.5.3 Term CohesionTerm cohesion further refines the candidacy judg-ment, by incorporating an internal analysis of thecandidate?s constituent words.
Term cohesionposits that high values for internal word associa-tion measures correlates indicates that the candi-date is a keyphrase (Church and Hanks, 1989).F9 : Term Cohesion (S,U) Park et al (2004)used in the Dice coefficient (Dice, 1945)to measure term cohesion particularly for multi-word terms.
In their work, as NPs are longer thansimplex words, they simply discounted simplexword cohesion by 10%.
In our work, we vary themeasure of TF used in Dice coefficient,similar to our discussion earlier.?
(F9a) term cohesion by (Park et al, 2004),?
(F9b*) normalized TF by candidate types(i.e.
simplex words vs. NPs),?
(F9c*) applying different weight by candi-date types,?
(F9d*) normalized TF and different weight-ing by candidate types5.4 Other FeaturesF10 : Acronym (S) Nguyen and Kan (2007) ac-counted for the importance of acronym as a fea-ture.
We found that this feature is heavily depen-dent on the data set.
Hence, we used it only forN&K to attest our candidate selection method.F11 : POS sequence (S) Hulth and Megyesi(2006) pointed out that POS sequences ofkeyphrases are similar.
It showed the distinctivedistribution of POS sequences of keyphrases anduse them as a feature.
Like acronym, this is alsosubject to the data set.13F12 : Suffix sequence (S) Similar to acronym,Nguyen and Kan (2007) also used a candidate?ssuffix sequence as a feature, to capture the propen-sity of English to use certain Latin derivationalmorphology for technical keyphrases.
This fea-ture is also a data dependent features, thus used insupervised approach only.F13 : Length of Keyphrases (S,U) Barker andCorrnacchia (2000) showed that candidate lengthis also a useful feature in extraction as well as incandidate selection, as the majority of keyphrasesare one or two terms in length.6 System and DataTo assess the performance of the proposed candi-date selection rules and features, we implementeda keyphrase extraction pipe line.
We start withraw text of computer science articles convertedfrom PDF by pdftotext.
Then, we parti-tioned the into section such as title and sectionsvia heuristic rules and applied sentence segmenter2, ParsCit3(Councill et al, 2008) for refer-ence collection, part-of-speech tagger4 and lem-matizer5(Minnen et al, 2001) of the input.
Af-ter preprocessing, we built both supervised andunsupervised classifiers using Naive Bayes fromthe WEKA machine learning toolkit(Witten andFrank, 2005), Maximum Entropy6, and simpleweighting.In evaluation, we collected 250 papers fromfour different categories7 of the ACM digital li-brary.
Each paper was 6 to 8 pages on average.In author assigned keyphrase, we found manywere missing or found as substrings.
To rem-edy this, we collected reader assigned keyphraseby hiring senior year undergraduates in computerscience, each whom annotated five of the paperswith an annotation guideline and on average, tookabout 15 minutes to annotate each paper.
The fi-nal statistics of keyphrases is presented in Table2 where Combined represents the total number ofkeyphrases.
The numbers in () denotes the num-ber of keyphrases in of-PP form.
Found means the2http://www.eng.ritsumei.ac.jp/asao/resources/sentseg/3http://wing.comp.nus.edu.sg/parsCit/4http://search.cpan.org/dist/Lingua-EN-Tagger/Tagger.pm5http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html6http://maxent.sourceforge.net/index.html7C2.4 (Distributed Systems), H3.3 (Information Searchand Retrieval), I2.11 (Distributed Artificial Intelligence-Multiagent Systems) and J4 (Social and Behavioral Sciences-Economics)number of author assigned keyphrase and readerassigned keyphrase found in the documents.Author Reader CombinedTotal 1252 (53) 3110 (111) 3816 (146)NPs 904 2537 3027Average 3.85 (4.01) 12.44 (12.88) 15.26 (15.85)Found 769 2509 2864Table 2: Statistics in Keyphrases7 EvaluationThe baseline system for both the supervised andunsupervised approaches is modified N&K whichuses TF*IDF, distance, section information andadditional section information (i.e.
F1-4).
Apartfrom baseline , we also implemented basicKEA and N&K to compare.
Note that N&K is con-sidered a supervised approach, as it utilizes fea-tures like acronym, POS sequence, and suffix se-quence.Table 3 and 4 shows the performance of our can-didate selection method and features with respectto supervised and unsupervised approaches usingthe current standard evaluation method (i.e.
exactmatching scheme) over top 5th, 10th, 15th candi-dates.BestFeatures includes F1c:TF of substring asa separate feature, F2:first occurrence, F3:sectioninformation, F4d:weighting key sections, F5:lastoccurrence, F6:co-occurrence of another candi-date in section, F7b:title overlap, F9a:term co-hesion by (Park et al, 2004), F13:length ofkeyphrases.
Best-TF*IDF means using all bestfeatures but TF*IDF.In Tables 3 and 4, C denotes the classifier tech-nique: unsupervised (U) or supervised using Max-imum Entropy (S)8.In Table 5, the performance of each feature ismeasured using N&K system and the target fea-ture.
+ indicates an improvement, - indicates aperformance decline, and ?
indicates no effector unconfirmed due to small changes of perfor-mances.
Again, supervised denotes MaximumEntropy training and Unsupervised is our unsu-pervised approach.8 DiscussionWe compared the performances over our candi-date selection and feature engineering with sim-ple KEA , N&K and our baseline system.
In eval-uating candidate selection, we found that longer8Due to the page limits, we present the best performance.14Method Features C Five Ten FifteenMatch Precision Recall Fscore Match Precising Recall Fscore Match Precision Recall FscoreAll KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%Candidates S 0.79 15.84% 5.19% 7.82% 1.39 13.88% 9.09% 10.99% 1.84 12.24% 12.03% 12.13%N&K S 1.32 26.48% 8.67% 13.06% 2.04 20.36% 13.34% 16.12% 2.54 16.93% 16.64% 16.78%baseline U 0.92 18.32% 6.00% 9.04% 1.57 15.68% 10.27% 12.41% 2.20 14.64% 14.39% 14.51%S 1.15 23.04% 7.55% 11.37% 1.90 18.96% 12.42% 15.01% 2.44 16.24% 15.96% 16.10%Length<=3 KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%Candidates S 0.81 16.16% 5.29% 7.97% 1.40 14.00% 9.17% 11.08% 1.84 12.24% 12.03% 12.13%N&K S 1.40 27.92% 9.15% 13.78% 2.10 21.04% 13.78% 16.65% 2.62 17.49% 17.19% 17.34%baseline U 0.92 18.4% 6.03% 9.08% 1.58 15.76% 10.32% 12.47% 2.20 14.64% 14.39% 14.51%S 1.18 23.68% 7.76% 11.69% 1.90 19.00% 12.45% 15.04% 2.40 16.00% 15.72% 15.86%Length<=3 KEA U 0.01 0.24% 0.08% 0.12% 0.05 0.52% 0.34% 0.41% 0.07 0.48% 0.47% 0.47%Candidates S 0.83 16.64% 5.45% 8.21% 1.42 14.24% 9.33% 11.27% 1.87 12.45% 12.24% 12.34%+ Alternation N&K S 1.53 30.64% 10.04% 15.12% 2.31 23.08% 15.12% 18.27% 2.88 19.20% 18.87% 19.03%baseline U 0.98 19.68% 6.45% 9.72% 1.72 17.24% 11.29% 13.64% 2.37 15.79% 15.51% 15.65%S 1.33 26.56% 8.70% 13.11% 2.09 20.88% 13.68% 16.53% 2.69 17.92% 17.61% 17.76%Table 3: Performance on Proposed Candidate SelectionFeatures C Five Ten FifteenMatch Prec.
Recall Fscore Match Prec.
Recall Fscore Match Prec.
Recall FscoreBest U 1.14 .228 .747 .113 1.92 .192 .126 .152 2.61 .174 .171 .173S 1.56 .312 .102 .154 2.50 .250 .164 .198 3.15 .210 .206 .208Best U 1.14 .228 .74 .113 1.92 .192 .126 .152 2.61 .174 .171 .173w/o TF*IDF S 1.56 .311 .102 .154 2.46 .246 .161 .194 3.12 .208 .204 .206Table 4: Performance on Feature EngineeringA Method Feature+ S F1a,F2,F3,F4a,F4d,F9aU F1a,F1c,F2,F3,F4a,F4d,F5,F7b,F9a- S F1b,F1c,F1d,F1f,F4b,F4c,F7a,F7b,F9b-d,F13U F1d,F1e,F1f,F4b,F4c,F6,F7a,F9b-d?
S F1e,F10,F11,F12U F1bTable 5: Performance on Each Featurelength candidates play a role to be noises so de-creased the overall performance.
We also con-firmed that candidate alternation offered the flexi-bility of keyphrases leading higher candidate cov-erage as well as better performance.To re-examine features, we analyzed the impactof existing and new features and their variations.First of all, unlike previous studies, we found thatthe performance with and without TF*IDF did notlead to a large difference which indicates the im-pact of TF*IDF was minor, as long as other fea-tures are incorporated.
Secondly, counting sub-strings for TF improved performance, while ap-plying term weighting for TF and/or IDF did notimpact on the performance.
We estimated thecause that many of keyphrases are substrings ofcandidates and vice versa.
Thirdly, section in-formation was also validated to improve perfor-mance, as in Nguyen and Kan (2007).
Extend-ing this logic, modeling additional section infor-mation (related work) and weighting sections bothturned out to be useful features.
Other localityfeatures were also validated as helpful: both firstoccurrence and last occurrence are helpful as itimplies the locality of the key ideas.
In addi-tion, keyphrase co-occurrence with selected sec-tions was proposed in our work and found empiri-cally useful.
Term cohesion (Park et al, 2004) is auseful feature although it has a heuristic factor thatreduce the weight by 10% for simplex words.
Nor-mally, term cohesion is subject to NPs only, henceit needs to be extended to work with multi-wordNPs as well.
Table 5 summarizes the reflectionson each feature.As unsupervised methods have the appeal of notneeding to be trained on expensive hand-annotateddata, we also compared the performance of super-vised and unsupervised methods.
Given the fea-tures initially introduced for supervised learning,unsupervised performance is surprisingly high.While supervised classifier produced a matchingcount of 3.15, the unsupervised classifier obtains acount of 2.61.
We feel this indicates that the exist-ing features for supervised methods are also suit-able for use in unsupervised methods, with slightlyreduced performance.
In general, we observed thatthe best features in both supervised and unsuper-vised methods are the same ?
section informationand candidate length.
In our analysis of the im-pact of individual features, we observed that mostfeatures affect performance in the same way forboth supervised and unsupervised approaches, asshown in Table 5.
These findings indicate that al-though these features may be been originally de-signed for use in a supervised approach, they arestable and can be expected to perform similar inunsupervised approaches.159 ConclusionWe have identified and tackled two core issuesin automatic keyphrase extraction: candidate se-lection and feature engineering.
In the area ofcandidate selection, we observe variations and al-ternations that were previously unaccounted for.Our selection rules expand the scope of possiblekeyphrase coverage, while not overly expandingthe total number candidates to consider.
In ourre-examination of feature engineering, we com-piled a comprehensive feature list from previousworks while exploring the use of substrings in de-vising new features.
Moreover, we also attested toeach feature?s fitness for use in unsupervised ap-proaches, in order to utilize them in real-world ap-plications with minimal cost.10 AcknowledgementThis work was partially supported by a National ResearchFoundation grant, Interactive Media Search (grant # R 252000 325 279), while the first author was a postdoctoral fellowat the National University of Singapore.ReferencesKen Barker and Nadia Corrnacchia.
Using noun phraseheads to extract document keyphrases.
In Proceedings ofthe 13th Biennial Conference of the Canadian Society onComputational Studies of Intelligence: Advances in Arti-ficial Intelligence.
2000.Regina Barzilay and Michael Elhadad.
Using lexical chainsfor text summarization.
In Proceedings of the ACL/EACL1997 Workshop on Intelligent Scalable Text Summariza-tion.
1997, pp.
10?17.Kenneth Church and Patrick Hanks.
Word associationnorms, mutual information and lexicography.
In Proceed-ings of ACL.
1989, 76?83.Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit:An open-source CRF reference string parsing package.
InProceedings of LREC.
2008, 28?30.Ernesto DA?vanzo and Bernado Magnini.
A Keyphrase-Based Approach to Summarization:the LAKE System atDUC-2005.
In Proceedings of DUC.
2005.F.
Damerau.
Generating and evaluating domain-orientedmulti-word terms from texts.
Information Processing andManagement.
1993, 29, pp.43?447.Lee Dice.
Measures of the amount of ecologic associationsbetween species.
Journal of Ecology.
1945, 2.Eibe Frank and Gordon Paynter and Ian Witten and CarlGutwin and Craig Nevill-manning.
Domain SpecificKeyphrase Extraction.
In Proceedings of IJCAI.
1999,pp.668?673.Carl Gutwin and Gordon Paynter and Ian Witten and CraigNevill-Manning and Eibe Frank.
Improving browsing indigital libraries with keyphrase indexes.
Journal of Deci-sion Support Systems.
1999, 27, pp.81?104.Khaled Hammouda and Diego Matute and Mohamed Kamel.CorePhrase: keyphrase extraction for document cluster-ing.
In Proceedings of MLDM.
2005.Annette Hulth and Jussi Karlgren and Anna Jonsson andHenrik Bostrm and Lars Asker.
Automatic Keyword Ex-traction using Domain Knowledge.
In Proceedings of CI-CLing.
2001.Annette Hulth and Beata Megyesi.
A study on automaticallyextracted keywords in text categorization.
In Proceedingsof ACL/COLING.
2006, 537?544.Mario Jarmasz and Caroline Barriere.
Using semantic sim-ilarity over tera-byte corpus, compute the performance ofkeyphrase extraction.
In Proceedings of CLINE.
2004.Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg.Finding Topic Words for Hierarchical Summarization.
InProceedings of SIGIR.
2001, pp.
349?357.Y.
Matsuo and M. Ishizuka.
Keyword Extraction from a Sin-gle Document using Word Co-occurrence Statistical Infor-mation.
In International Journal on Artificial IntelligenceTools.
2004, 13(1), pp.
157?169.Olena Medelyan and Ian Witten.
Thesaurus based automatickeyphrase indexing.
In Proceedings of ACM/IEED-CSjoint conference on Digital libraries.
2006, pp.296?297.Guido Minnen and John Carroll and Darren Pearce.
Appliedmorphological processing of English.
NLE.
2001, 7(3),pp.207?223.Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extrac-tion in Scientific Publications.
In Proceeding of ICADL.2007, pp.317-326.Youngja Park and Roy Byrd and Branimir Boguraev.
Auto-matic Glossary Extraction Beyond Terminology Identifi-cation.
In Proceedings of COLING.
2004, pp.48?55.Mari-Sanna Paukkeri and Ilari Nieminen and Matti Pollaand Timo Honkela.
A Language-Independent Approachto Keyphrase Extraction and Evaluation.
In Proceedingsof COLING.
2008.Peter Turney.
Learning to Extract Keyphrases from Text.In National Research Council, Institute for InformationTechnology, Technical Report ERB-1057.
1999.Peter Turney.
Coherent keyphrase extraction via Web min-ing.
In Proceedings of IJCAI.
2003, pp.
434?439.Xiaojun Wan and Jianguo Xiao.
CollabRank: towards a col-laborative approach to single-document keyphrase extrac-tion.
In Proceedings of COLING.
2008.Ian Witten and Gordon Paynter and Eibe Frank and CarGutwin and Graig Nevill-Manning.
KEA:Practical Au-tomatic Key phrase Extraction.
In Proceedings of ACMDL.
1999, pp.254?256.Ian Witten and Eibe Frank.
Data Mining: Practical Ma-chine Learning Tools and Techniques.
Morgan Kauf-mann, 2005.Yongzheng Zhang and Nur Zinchir-Heywood and Evange-los Milios.
Term based Clustering and Summarization ofWeb Page Collections.
In Proceedings of Conference ofthe Canadian Society for Computational Studies of Intel-ligence.
2004.16
