Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58?65Manchester, August 2008An Agreement Measure for Determining Inter-Annotator Reliability ofHuman Judgements on Affective TextPlaban Kr.
Bhowmick, Pabitra Mitra, Anupam BasuDepartment of Computer Science and Engineering,Indian Institute of Technology, Kharagpur, India ?
721302{plaban,pabitra,anupam}@cse.iitkgp.ernet.inAbstractAn affective text may be judged to be-long to multiple affect categories as it mayevoke different affects with varying degreeof intensity.
For affect classification oftext, it is often required to annotate textcorpus with affect categories.
This taskis often performed by a number of hu-man judges.
This paper presents a newagreement measure inspired by Kappa co-efficient to compute inter-annotator relia-bility when the annotators have freedomto categorize a text into more than oneclass.
The extended reliability coefficienthas been applied to measure the quality ofan affective text corpus.
An analysis ofthe factors that influence corpus quality hasbeen provided.1 IntroductionThe accuracy of a supervised machine learningtask primarily depends on the annotation quality ofthe data, that is used for training and cross valida-tion.
Reliability of annotation is a key requirementfor the usability of an annotated corpus.
Inconsis-tency or noisy annotation may lead to the degrada-tion of performances of supervised learning algo-rithms.
The data annotated by a single annotatormay be prone to error and hence an unreliable one.This also holds for annotating an affective corpus,which is highly dependent on the mental state ofthe subject.
The recent trend in corpus develop-ment in NLP is to annotate corpus by more thanone annotators independently.
In corpus statistics,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.the corpus reliability is measured by coefficient ofagreement.
The coefficients of agreement are ap-plied to corpus for various goals like measuring re-liability, validity and stability of corpus (Artsteinand Poesio, 2008).Jacob Cohen (Cohen, 1960) introduced Kappastatistics as a coefficient of agreement for nom-inal scales.
The Kappa coefficient measures theproportion of observed agreement over the agree-ment by chance and the maximum agreement at-tainable over chance agreement considering pair-wise agreement.
Later Fleiss (Fleiss, 1981) pro-posed an extension to measure agreement in ordi-nal scale data.Cohen?s Kappa has been widely used in vari-ous research areas.
Because of its simplicity androbustness, it has become a popular approach foragreement measurement in the area of electron-ics (Jung, 2003), geographical informatics (Hagen,2003), medical (Hripcsak and Heitjan, 2002), andmany more domains.There are other variants of Kappa like agree-ment measures (Carletta, 1996).
Scott?s pi (Scott,1955) was introduced to measure agreement in sur-vey research.
Kappa and pi measures differ in theway they determine the chance related agreements.pi-like coefficients determine the chance agreementamong arbitrary coders, while ?-like coefficientstreats the chance of agreement among the coderswho produced the reliability data (Artstein andPoesio, 2008).One of the drawbacks of pi and Kappa like coef-ficients except Fleiss?
Kappa (Fleiss, 1981) is thatthey treat all kinds of disagreements in the samemanner.
Krippendorff?s ?
(Krippendorff, 1980) isa reliability measure which treats different kind ofdisagreements separately by introducing a notionof distance between two categories.
It offers a way58to measure agreement in nominal, interval, ordinaland ratio scale data.Reliability assessment of corpus is an impor-tant issue in corpus driven natural language pro-cessing and the existing reliability measures havebeen used in various corpus development tasks.For example, Kappa coefficient has been usedin developing parts of speech corpus (Mieskesand Strube, 2006), dialogue act tagging effortslike MapTask (Carletta et al, 1997) and Switch-board (Stolke et al, 1997), subjectivity taggingtask (Bruce and Wiebe, 1999) and many more.The pi and ?
coefficients measure the reliabil-ity of the annotation task where a data item canbe annotated with one category.
(Rosenberg andBinkowski, 2004) puts an effort towards measur-ing corpus reliability for multiply labeled datapoints.
In this measure, the annotators are allowedto mark one data point with at most two classes,one of which is primary and other is secondary.This measure was used to determine the reliabilityof a email corpus where emails are assigned withprimary and secondary labels from a set of emailtypes.Affect recognition from text is a recent andpromising subarea of natural language process-ing.
The task is to classify text segments into ap-propriate affect categories.
The supervised ma-chine learning techniques, which requires a reli-able annotated corpus, may be applied for solv-ing the problem.
In general, a blend of emotionsis common in both verbal and non-verbal com-munication.
Unlike conventional annotation taskslike POS corpus development, where one data itemmay belong to only one category, in affective textcorpus, a data item may be fuzzy and may belongto multiple affect categories.
For example, the fol-lowing sentence may belong to disgust and sadcategory since it may evoke both the emotions todifferent degrees of intensity.A young married woman was burnt todeath allegedly by her in-laws for dowry.This property makes the existing agreement mea-sures inapplicable for determining agreement inemotional corpus.
Craggs and Wood (2004)adopted a categorical scheme for annotating emo-tion in affective text dialogue.
They claimed to ad-dress the problem of agreement measurement forthe data set where one data item may belong tomore than one category using an extension of Krip-pendorff?s ?.
But the details of the extension is yetto be disseminated.In this paper, we propose a new agreement mea-sure for multiclass annotation which we denote byAm.
The new measure is then applied to an affec-tive text corpus to?
Assess Reliability: To test whether the corpuscan be used for developing computational af-fect recognizer.?
Determine Gold Standard: To define a goldstandard that will be used to test the accuracyof the affect recognizer.In section 2, we describe the affective text cor-pus and the annotation scheme.
In section 3, wepropose a new reliability measure (Am) for mul-ticlass annotated data.
In section 4, we providean algorithm to determine gold standard data fromthe annotation and in section 5, we discuss aboutapplying Ammeasure to the corpus developed byus and some observations related to the annotation.2 Affective Text Corpus and AnnotationSchemeThe affective text corpus collected by us consistsof 1000 sentences extracted from Times of Indianews archive1.
The sentences were collected fromheadlines as well as articles belonging to political,social, sports and entertainment domain.Selection of affect categories is a very crucialand important decision problem due to the follow-ing reasons.?
The affect categories should be applicable tothe considered genre.?
The affect categories should be identifiablefrom language.?
The categories should be unambiguous.We shall try to validate these points based on theresults obtained, after applying the our extendedmeasure on the text corpus with respect to a set ofselected basic emotional categories.Basic emotions are those for which the respec-tive expressions across culture, ethnicity, age, sex,social structure are invariant (Ortony and Turner,1990).
But unfortunately, there is a long per-sistent debate among the psychologists regarding1http://timesofindia.indiatimes.com/archive.cms59the number of basic emotional categories (Ortonyand Turner, 1990).
One of the theories behindthe basic emotions is that they are biologicallyprimitive because they possess evolutionary signif-icance related to the basic needs for the survival ofthe species (Plutchik, 1980).
The universality ofrecognition of emotions from distinctive facial ex-pressions is an indirect technique to establish thebasic emotions (Darwin, 1965).Six basic affect categories (Ekman, Friesen andEllsworth, 1982) have been considered in emotionrecognition from speech (Song et al, 2004), fa-cial expression (Pantic and Rothkrantz, 2000).
Ourannotation scheme considers six basic emotions,namely, Anger, Disgust, Fear, Happiness, Sadness,Surprise as specified by Ekman for affect recogni-tion in text corpus.The annotation scheme considers the followingpoints:?
Two types are sentences are collected for an-notation.?
Direct Affective Sentence: Here, theagent present in the sentence is experi-encing a set of emotions, which are ex-plicit in the sentence.
For example, inthe following sentence Indian support-ers are the agents experiencing a disgustemotion.Indian supporters are disgustedabout players?
performances inthe World Cup.?
Indirect Affective Sentence: Here, thereader of the sentence is experiencing aset of emotions.
In the following sen-tence, the reader is experiencing a dis-gust emotion because the event of ac-cepting bribe, is an indecent act carriedout by responsible agents like Top offi-cials.Top officials are held for accept-ing bribe from a poor villager.?
A sentence may trigger multiple emotions si-multaneously.
So, one annotator may classifya sentence to more than one affective cate-gories.?
For each emotion, the keywords that triggerthe particular emotion are marked.?
For each emotion, the events or objects thattrigger the concerned emotion are marked.Here, we aim at measuring the agreement in an-notation.
The focus is to measure the agreementin annotation pattern rather than the agreement inindividual emotional classes.3 Proposed Agreement MeasureTo overcome the shortcomings of existing relia-bility measures mentioned earlier, we propose Ammeasure, which is an agreement measure for cor-pus annotation task considering multiclass classifi-cation.
We present the notion of agreement below.3.1 Notion of Paired AgreementIn order to allow for multiple labels, we calculateagreement between all the pairs of possible labels.Let C1 and C2 be two affect categories, e.g., angerand disgust.
Let <C1, C2> denote the categorypair.
An annotator?s assignment of labels can berepresented as a pair of binary choices for each cat-egory pair <C1, C2>, namely, < 0, 0 >, < 0, 1 >,< 1, 0 >, and < 1, 1 >.
It should be noted that theproposed metric considers the non-inclusion in acategory by an annotator pair as an agreement.For an item, two annotators U1 and U2 are saidto agree on <C1, C2> if the following conditionshold.U1.C1 = U2.C1U1.C2 = U2.C2where Ui.Cjsignifies that the value for Cjfor an-notator Uiand the value may either be 1 or 0.
Forexample, if one coder marks an item with angerand another with disgust, they would disagree onthe pairs that include these labels, but still agreethat the item does not express happiness and sad-ness.3.2 AmAgreement MeasureWith the notion of paired agreement discussed ear-lier, the observed agreement(Po) is the proportionof items the annotators agreed on the categorypairs and the expected agreement(Pe) is the pro-portion of items for which agreement is expectedby chance when the items are randomly.
Follow-ing the line of Cohen?s Kappa (Cohen, 1960), Amis defined as the proportion of agreement after ex-pected or chance agreement is removed from con-sideration and is given byAm=Po?
Pe1?
Pe(1)60When Poequals Pe, Amvalue is computed tobe 0, which signifies no non-random agreementamong the annotators.
An Amvalue of 1, theupper limit of Am, indicates a perfect agreementamong the annotators.
We define Poand Peasfollows.Observed Agreement (Po):Let I be the number of items, C is the number ofcategories and U is the number of annotators andS be the set of all category pairs with cardinality(C2).
The total agreement on a category pair pfor an item i is nip, the number of annotator pairswho agree on p for i.The average agreement on a category pair p foran item i is nipdivided by the total number of an-notator pairs and is given byPip=1(U2)nip(2)The average agreement for the item i is the meanof Pipover all category pairs and is given byPi=1(C2)(U2)?p?Snip(3)The observed agreement is the average agreementover all the item and is given byPo=1II?i=1Pi=1I(C2)(U2)I?i=1?p?Snip(4)=4IC(C ?
1)U(U ?
1)I?i=1?p?SnipExpected Agreement (Pe):The expected agreement is defined as the agree-ment among the annotators when they assign theitems to a set of categories randomly.
However,since we are considering the agreement on cate-gory pairs, we consider the expected agreementto be the expectation that the annotators agree ona category pair.
For a category pair, four possibleassignment combinations constitute a set which isgiven byG = {[0 0], [0 1], [1 1]}.It is to be noted that the combinations [0 1] and [10] are clubbed to one element as they are symmet-ric to each other.
Let ?P (pg|u) be the overall pro-portion of items assigned with assignment combi-nation g ?
G to category pair p ?
S by annotatoru and npgube the total number of assignments ofitems by annotator u with assignment combinationg to category pair p. Then ?P (pg|u) is given by?P (pg|u) =npguI(5)For an item, the probability that two arbitrarycoders agree with the same assignment combina-tion in a category pair is the joint probability ofindividual coders making this assignments inde-pendently.
For two annotators uxand uythe jointprobability is given by ?P (pg|ux)?P (pg|uy).
Theprobability that two arbitrary annotators agree ona category pair p with assignment combination gis the average over all annotator pairs belonging toW , the set of annotator pairs and is given by?P (pg) =1(U2)?
(ux,uy)?W?P (pg|ux)?P (pg|uy)(6)The probability that two arbitrary annotators agreeon a category pair for all assignment combinationsis given by?P (p) =?pg?G?P (pg) (7)The chance agreement is calculated by takingaverage over all category pairs.Pe=1(C2)?p?S?P (p) (8)The Ammeasure may be calculated based on theexpressions of Poand Peas given in Equation 4and Equation 8 to compute the reliability of anno-tation with respect to multiclass annotation.4 Gold Standard DeterminationGold standard data is used as a reference data setfor various goals like?
Building reliable classifier61?
Determine the performance of a classifierTo attach a set of labels to a data item in the goldstandard data, we assign the majority decisionlabel to an item.
Let nObe the number of annota-tors, who have assigned an item i into category Cand n?annotators have decided not to assign thesame item into that category.
Then i is assigned toC if nO> n?
; otherwise it is not assigned to thatcategory.Algorithm 1: Algorithm for determining goldstandard dataInput: Set of I items annotated into Ccategories by U annotatorsOutput: Gold standard dataforeach annotator u ?
U do?u?
0;endforeach item i ?
I doforeach category c ?
C do?
= set of annotators who haveassigned i in category c;?
= set of annotators who have notassigned i in category c;if cardinality(?)>cardinality(?)
thenassign label c to i;?j?
?j+ 1 where j ?
?
;endelse if cardinality(?)<cardinality(?
)thendo not assign label c to i;?j?
?j+ 1 where j ?
?
;endelse if???
>???
thenassign label c to i;endendendIf nO= n?, then we resolve the tie based on theperformances of the annotators in previous assign-ments.
We assign an expert coder index(?)
to eachannotator and it is updated based on the agreementof their judgments over the corpus.
There are twocases when the ?
values are incremented?
If the item is assigned to a category in the goldstandard data, the ?
values are incrementedfor those annotators who have assigned theitem into that category.?
If the item is not assigned to a category inthe gold standard data, the ?
values are in-cremented for those annotators who have notassigned the item into that category.If nOand n?are equal for an item, we make useof the ?
values for deciding upon the assignment ofthe item to the category in concern.
We assign theitem into that category if the combined ?
values ofthe annotators who have assigned the item into thatcategory is greater than the combined ?
values ofthe annotators who have not assigned the item intothat category, i.e.,nO?i=1?i>n?
?j=1?jThe algorithm for determining gold standarddata is given in Algorithm 1.5 Experimental ResultsWe applied the proposed Ammeasure to estimatethe quality of the affective corpus described in sec-tion 2.
Below we present the annotation experi-ment followed by some relevant analysis.5.1 Annotation ExperimentTen human judges with the same social back-ground participated in the study, assigning affec-tive categories to sentences independently of oneanother.
The annotators were provided with theannotation instructions and they were trained withsome sentences not belonging to the corpus.
Theannotation was performed with the help of a webbased annotation interface2.
The corpus consistsof 1000 sentences.
Three of judges were able tocomplete the task within 20 days.
In this paper,we report the result of applying the measure withdata provided by three annotators without consid-ering the incomplete annotations.
Distribution ofthe sentences across the affective categories for thethree judges is given in Figure 1.5.2 Analysis of Corpus QualityThe corpus was evaluated in terms of the proposedmeasure.
Some of the relevant observations arepresented below.?
Agreement Value: Different agreement val-ues related to Ammeasure are given in Ta-ble 1.
We present Amvalues for all the anno-tator pairs in Table 2.2http://www.mla.iitkgp.ernet.in/Annotation/index.php62Figure 1: Distribution of sentences for threejudges.Agreement AmValueObserved Agreement(Po) 0.878Chance Agreement(Pe) 0.534Am0.738Table 1: Agreement values for the affective textcorpus.Annotator Pair PoPeAmValue1-2 0.858 0.526 0.7021-3 0.868 0.54 0.7132-3 0.884 0.531 0.752Table 2: Annotator pairwise Amvalues.?
Agreement Study: Table 3 provides the dis-tribution of the sentences against individualobserved agreement values.
It is observedObserved Agreement No.
of Sentences0.0 < A0?
0.2 140.2 < A0?
0.4 730.4 < A0?
0.7 1980.7 < A0?
1.0 715Table 3: Distribution of the sentences over ob-served agreement.that 71.5% of the corpus belongs to [0.7 1.0]range of observed agreement and among thisbulk portion of the corpus, the annotators as-sign 78.6% of the sentences into a single cat-egory.
This is due to the existence of a domi-nant emotion in a sentence and in most of thecases, the sentence contains enough clues todecode it.
For the non-dominant emotions ina sentence, ambiguity has been found whiledecoding.?
Disagreement Study: In Table 4, we presentthe category wise disagreement for all the an-notator pairs.
From the disagreement table itis evident that the categories with maximumnumber of disagreements are anger, disgustand fear.
The emotions which are close toeach other in the evaluation-activation spaceare inherently ambiguous.
For example,anger and disgust are close to each other inthe evaluation-activation space.
So, ambigu-ity between these categories will be highercompared to other pairs.
If [a b] is the pair, wecount the number of cases where one annota-tor categorized one item into [a -] pattern andother annotator classified the same item into[- b] pattern.
In Table 5, we provide the con-fusion between two affective categories for allannotator pairs.
This confusion matrix is asymmetric one.
So, we have provided onlythe upper triangular matrix.In Figure 2, we provide ambiguity counts ofthe affective category pairs.
It can be ob-Figure 2: Category pair wise disagreement(A=Anger, D=Disgust, F=Fear, H=Happiness,S=Sadness and Su=Surprise).served that anger, disgust and fear are asso-ciated with three topmost ambiguous pairs.5.3 Gold Standard for Affective Text CorpusTo determine the gold standard corpus, we haveapplied majority decision label based approachdiscussed in section 4 on the judgements providedby only three annotators.
However, as the num-ber of annotators is much less in the current study,the determined gold standard corpus may not have63Anger Disgust Fear Happiness Sadness Surprise1-2 68 94 74 64 74 451-3 74 86 105 57 54 452-3 65 49 58 22 50 20Total 207 229 273 143 178 110Table 4: Categorywise disagreement for the annotator pairs.Anger Disgust Fear Happiness Sadness SurpriseAnger - 39 28 11 22 7Disgust - - 28 6 24 13Fear - - - 2 24 12Happiness - - - - 18 8Sadness - - - - - 9Surprise - - - - - -Table 5: Confusion matrix for category pairs.much significance.
Here, we report the resultof applying the gold standard determination algo-rithm on the data provided by three annotators.The distribution of sentences over the affective cat-egories is depicted in Figure 3.Figure 3: Distribution of sentences in gold stan-dard corpus.6 Conclusion and Future WorkMeasuring the reliability of the affective text cor-pus where one single item may be classified intomore than one single category is a complex task.In this paper, we have provided a new coefficientto measure reliability in multiclass annotation taskby incorporating pairwise agreement in affectiveclass pairs.
The measure yields an agreement value0.72, when applied to an annotated corpus pro-vided by three users.
This considerable agreementvalue indicates that the affect categories consid-ered for annotation may be applicable to the newsgenre.We are in process of collecting annotated corpusfrom more annotators which will ensure a statisti-cally significant result.
According to the disagree-ment study presented in section 5.2, confusionsbetween specific emotions is most likely betweencategories which are adjacent in the activation-evaluation space.
The models of annotator agree-ment which use weights for different types of dis-agreement will be interesting for future study.
Thedirect and indirect affective sentences have notbeen treated separately in this study.
The algo-rithm for determination of gold standard requiresmore details investigation as simple majority vot-ing may not be sufficient for highly subjective datalike emotion.AcknowledgementPlaban Kr.
Bhowmick is partially supported byMicrosoft Corporation, USA and Media Lab Asia,India.
The authors are thankful to the reviewers fortheir detailed suggestions regarding the work.ReferencesArtstein, Ron and Massimo Poesio.
2008.
Inter-coderAgreement for Computational Linguistics.
Compu-tational Linguistics.Bruce, Rebecca F. and Janyce M. Wiebe 1999.
Rec-64ognizing Subjectivity: A Case Study of Manual Tag-ging.
Natural Language Engineering.
1(1):1-16.Carletta, Jean.
1996.
Assessing Agreement on Classi-fication Tasks: The Kappa Statistic.
ComputationalLinguistics.
22(21):249-254.Carletta, Jean, Isard .A, Isard S., Jacqueline C. Kowtko,Gwyneth D. Sneddon, and Anne H. Anderson.
1997.The Reliability of a Dialogue Structure CodingScheme.
Computational Linguistics.
23(1):13-32.Cohen, Jacob.
1960.
A Coefficient of Agreementfor Nominal Scales.
Educational and PsychologicalMeasurement.
20(1):37-46.Craggs Richard and Mary M. Wood.
2004.
A Categori-cal Annotation Scheme for Emotion in the LinguisticContent of Dialogue.
Tutorial and Research Work-shop, Affective Dialogue Systems.
Kloster Irsee, 89-100.Darwin, Charles.
1965.
The Expression of Emotions inMan and Animals.. Chicago: University of ChicagoPress.
(Original work published 1872)Ekman, Paul., Friesen W. V., and Ellsworth P. 1982.What Emotion Categories or Dimensions can Ob-servers Judge from Facial Behavior?
Emotion inthe human face, Cambridge University Press.
pages39-55, New York.Fleiss, Joseph L. 1981.
Statistical Methods for Ratesand Proportions.
Wiley.
second ed., New York.Hagen-Zanker, Alex.
2003.
Fuzzy Set Approach toAssessing Similarity of Categorical Maps.
Interna-tional Journal for Geographical Information Science.17(3):235-249.Hripcsak, George and Daniel F. Heitjan.
2002.
Mea-suring Agreement in Medical Informatics Reliabil-ity Studies.
Journal of Biomedical Informatics.35(2):99-110.Jung, Ho-Won.
2003.
Evaluating Interrater Agreementin SPICE-based Assessments.
Computer Standards& Interfaces.
25(5):477-499.Krippendorff, Klaus 1980.
Content Analysis: An Intro-duction to its Methodology.
Sage Publications.
Bev-erley Hills, CA.Mieskes, Margot and Michael Strube.
2006.
Part-of-Speech Tagging of Transcribed Speech.
Proceedingsof International Conference on Language Resourcesand Evaluation.
GENOAOrtony, Andrew and Terence J. Turner.
1990.
What?sBasic About Basic Emotions?.
Psychological Re-view.
97(3):315-331.Pantic, Maja and Leon Rothkrantz.
2000.
AutomaticAnalysis of Facial Expressions: The State of the Art.IEEE Transactions on Pattern Analysis and MachineIntelligence.
22(12):1424-1445.Plutchik, Robert 1980.
A General PsychoevolutionaryTheory of Emotion.
Emotion: Theory, research, andexperience: Vol.
1.
Theories of emotion.
AcademicPress, New York, 3-33.Rosenberg, Andrew, and Ed Binkowski.
2004.
Aug-menting the Kappa Statistic to Determine Interanno-tator Reliability for Multiply Labeled Data Points.In Proceedings of North American Chapter of theAssociation for Computational Linguistics.
Boston,77-80.Scott, William A.
1955.
Reliability of Content Anal-ysis: The Case of Nominal Scale Coding.
PublicOpinion Quarterly.
19(3):321-325.Song, Mingli, Chun Chen, Jiajun Bu, and Mingyu You.2004.
Speech Emotion Recognition and Intensity Es-timation.
Internation Conference on ComputationalScience and its Applications.
Perugia, 406-413.Stolcke A., Ries K., Coccaro N., Shriberg E., Bates R.,Jurafsky .D, Taylor P., Martin C. Van-Ess-Dykema,and Meteer .M.
1997.
Dialogue Act Modelingfor Automatic Tagging and Recognition of Con-versational Speech.
Computational Linguistics.26(3):339-371.65
