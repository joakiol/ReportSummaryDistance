The Basque Country University system: English and Basque tasksEneko AgirreIXA NLP GroupBasque Country UniversityDonostia, Spaineneko@si.ehu.esDavid MartinezIXA NLP GroupBasque Country UniversityDonostia, Spaindavidm@si.ehu.esAbstractOur group participated in the Basque and En-glish lexical sample tasks in Senseval-3.
Alanguage-specific feature set was defined forBasque.
Four different learning algorithms wereapplied, and also a method that combined theiroutputs.
Before submission, the performanceof the methods was tested for each task on theSenseval-3 training data using cross validation.Finally, two systems were submitted for eachlanguage: the best single algorithm and the bestensemble.1 IntroductionOur group (BCU, Basque Country University),participated in the Basque and English lexicalsample tasks in Senseval-3.
We applied 4 differ-ent learning algorithms (Decision Lists, NaiveBayes, Vector Space Model, and Support VectorMachines), and also a method that combinedtheir outputs.
These algorithms were previouslytested and tuned on the Senseval-2 data for En-glish.
Before submission, the performance ofthe methods was tested for each task on theSenseval-3 training data using 10 fold cross val-idation.
Finally, two systems were submittedfor each language, the best single algorithm andthe best ensemble in cross-validation.The main difference between the Basque andEnglish systems was the feature set.
A richset of features was used for English, includ-ing syntactic dependencies and domain infor-mation, extracted with different tools, and alsofrom external resources like WordNet Domains(Magnini and Cavaglia?, 2000).
The features forBasque were different, as Basque is an agglu-tinative language, and syntactic information isgiven by inflectional suffixes.
We tried to rep-resent this information in local features, relyingon the analysis of a deep morphological analyzerdeveloped in our group (Aduriz et al, 2000).In order to improve the performance of the al-gorithms, different smoothing techniques weretested on the English Senseval-2 lexical sam-ple data (Agirre and Martinez, 2004), and ap-plied to Senseval-3.
These methods helped toobtain better estimations for the features, andto avoid the problem of 0 counts Decision Listsand Naive Bayes.This paper is organized as follows.
The learn-ing algorithms are first introduced in Section 2,and Section 3 describes the features applied toeach task.
In Section 4, we present the exper-iments performed on training data before sub-mission; this section also covers the final config-uration of each algorithm, and the performanceobtained on training data.
Finally, the officialresults in Senseval-3 are presented and discussedin Section 5.2 Learning AlgorithmsThe algorithms presented in this section rely onfeatures extracted from the context of the targetword to make their decisions.The Decision List (DL) algorithm is de-scribed in (Yarowsky, 1995b).
In this algorithmthe sense with the highest weighted feature is se-lected, as shown below.
We can avoid undeter-mined values by discarding features that have a0 probability in the divisor.
More sophisticatedsmoothing techniques have also been tried (cf.Section 4).arg maxkw(sk, fi) = log(Pr(sk|fi)?j =kPr(sj|fi))The Naive Bayes (NB) algorithm is basedon the conditional probability of each sensegiven the features in the context.
It also re-quires smoothing.arg maxkP (sk)?mi=1P (fi|sk)For the Vector Space Model (V) algo-rithm, we represent each occurrence context asa vector, where each feature will have a 1 or 0Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsvalue to indicate the occurrence/absence of thefeature.
For each sense in training, one cen-troid vector is obtained.
These centroids arecompared with the vectors that represent test-ing examples, by means of the cosine similarityfunction.
The closest centroid is used to assignits sense to the testing example.
No smooth-ing is required to apply this algorithm, but it ispossible to use smoothed values.Regarding Support Vector Machines(SVM) we utilized SVM-Light (Joachims,1999), a public distribution of SVM.
Linear ker-nels were applied, and the soft margin (C) wasestimated per each word (cf.
Section 4).3 Features3.1 Features for EnglishWe relied on an extensive set of features ofdifferent types, obtained by means of differenttools and resources.
The features used can begrouped in four groups:Local collocations: bigrams and trigramsformed with the words around the target.
Thesefeatures are constituted with lemmas, word-forms, or PoS tags1.
Other local featuresare those formed with the previous/posteriorlemma/word-form in the context.Syntactic dependencies: syntactic depen-dencies were extracted using heuristic patterns,and regular expressions defined with the PoStags around the target2.
The following rela-tions were used: object, subject, noun-modifier,preposition, and sibling.Bag-of-words features: we extract thelemmas of the content words in the whole con-text, and in a ?4-word window around the tar-get.
We also obtain salient bigrams in the con-text, with the methods and the software de-scribed in (Pedersen, 2001).Domain features: The WordNet Domainsresource was used to identify the most relevantdomains in the context.
Following the relevanceformula presented in (Magnini and Cavaglia?,2000), we defined 2 feature types: (1) the mostrelevant domain, and (2) a list of domains abovea predefined threshold3.
Other experiments us-ing domains from SUMO, the EuroWordNet1The PoS tagging was performed with the fnTBLtoolkit (Ngai and Florian, 2001).2This software was kindly provided by DavidYarowsky?s group, from Johns Hopkins University.3The software to obtain the relevant domains waskindly provided by Gerard Escudero?s group, from Uni-versitat Politecnica de Catalunyatop-ontology, and WordNet?s Semantic Fieldswere performed, but these features were dis-carded from the final set.3.2 Features for BasqueBasque is an agglutinative language, and syn-tactic information is given by inflectional suf-fixes.
The morphological analysis of the text isa necessary previous step in order to select in-formative features.
The data provided by thetask organization includes information aboutthe lemma, declension case, and PoS for the par-ticipating systems.
Our group used directly theoutput of the parser (Aduriz et al, 2000), whichincludes some additional features: number, de-terminer mark, ambiguous analyses and ellipticwords.
For a few examples, the morphologicalanalysis was not available, due to parsing errors.In Basque, the determiner, the number andthe declension case are appended to the last el-ement of the phrase.
When defining our fea-ture set for Basque, we tried to introduce thesame knowledge that is represented by featuresthat work well for English.
We will describeour feature set with an example: for the phrase?elizaren arduradunei?
(which means ?to thedirectors of the church?)
we get the followinganalysis from our analyzer:eliza |-ren |arduradun |-eichurch |of the |director |to the +pl.The order of the words is the inverse in En-glish.
We extract the following information foreach word:elizaren:Lemma: eliza (church)PoS: nounDeclension Case: genitive (of)Number: singularDeterminer mark: yesarduradunei:Lemma: arduradun (director)PoS: nounDeclension Case: dative (to)Number: pluralDeterminer mark: yesWe will assume that eliza (church) is thetarget word.
Words and lemmas are shownin lowercase and the other information in up-percase.
As local features we defined differenttypes of unigrams, bigrams, trigrams and awindow of ?4 words.
The unigrams were con-structed combining word forms, lemmas, case,number, and determiner mark.
We defined 4kinds of unigrams:Uni wf0 elizarenUni wf1 eliza SING+DETUni wf2 eliza GENITIVEUni wf3 eliza SING+DET GENITIVEAs for English, we defined bigrams based onword forms, lemmas and parts-of-speech.
Butin order to simulate the bigrams and trigramsused for English, we defined different kinds offeatures.
For word forms, we distinguished twocases: using the text string (Big wf0), or usingthe tags from the analysis (Big wf1).
The wordform bigrams for the example are shown below.In the case of the feature type ?Big wf1?, theinformation is split in three features:Big wf0 elizaren arduraduneiBig wf1 eliza GENITIVEBig wf1 GENITIVE arduradun PLUR+DETBig wf1 arduradun PLUR+DET DATIVESimilarly, depending on the use of the de-clension case, we defined three kinds of bigramsbased on lemmas:Big lem0 eliza arduradunBig lem1 eliza GENITIVEBig lem1 GENITIVE arduradunBig lem1 arduradun DATIVEBig lem2 eliza GENITIVEBig lem2 arduradun DATIVEThe bigrams constructed using Part-of-speech are illustrated below.
We included thedeclension case as if it was another PoS:Big pos -1 NOUN GENITIVEBig pos -1 GENITIVE NOUNBig pos -1 NOUN DATIVETrigrams are built similarly, by combining theinformation from three consecutive words.
Wealso used as local features all the content wordsin a window of ?4 words around the target.
Fi-nally, as global features we took all the con-tent lemmas appearing in the context, whichwas constituted by the target sentence and thetwo previous and posterior sentences.One difficult case to model in Basque is the el-lipsis.
For example, the word ?elizakoa?
means?the one from the church?.
We were able toextract this information from our analyzer andwe represented it in the features, using a specialsymbol in place of the elliptic word.4 Experiments on training dataThe algorithms that we applied were first testedon the Senseval-2 lexical sample task for En-glish.
The best versions were then evaluated by10 fold cross-validation on the Senseval-3 data,both for Basque and English.
We also used thetraining data in cross-validation to tune the pa-rameters, such as the smoothed frequencies, orthe soft margin for SVM.
In this section we willdescribe first the parameters of each method(including the smoothing procedure), and thenthe cross-validation results on the Senseval-3training data.4.1 Methods and ParametersDL: On Senseval-2 data, we observed thatDL improved significantly its performance witha smoothing technique based on (Yarowsky,1995a).
For our implementation, the smoothedprobabilities were obtained by grouping the ob-servations by raw frequencies and feature types.As this method seems sensitive to the featuretypes and the amount of examples, we tested3 DL versions: DL smooth (using smoothedprobabilities), DL fixed (replacing 0 counts with0.1), and DL discard (discarding features ap-pearing with only one sense).NB: We applied a simple smoothing methodpresented in (Ng, 1997), where zero counts arereplaced by the probability of the given sensedivided by the number of examples.V: The same smoothing method used for NBwas applied for vectors.
For Basque, two ver-sions were tested: as the Basque parser can re-turn ambiguous analyses, partial weights are as-signed to the features in the context, and we canchose to use these partial weights (p), or assignthe full weight to all features (f).SVM: No smoothing was applied.
We esti-mated the soft margin using a greedy process incross-validation on the training data per eachword.Combination: Single voting was used,where each system voted for its best rankedsense, and the most voted sense was chosen.More sophisticate schemes like ranked voting,were tried on Senseval-2 data, but the resultsdid not improve.
We tested combinations ofthe 4 algorithms, leaving one out, and the twobest.
The best results were obtained combining3 methods (leave one out).Method Recallvector 73,9SVM 73,5DL smooth 69,4NB 69,4DL fixed 65,6DL discard 65,4MFS 57,1Table 1: Single systems (English) in cross-validation, sorted by recall.Combination RecallSVM-vector-DL smooth-NB 73,2SVM-vector-DL fixed-NB 72,7SVM-vector-DL smooth 74,0SVM-vector-DL fixed 73,8SVM-vector-NB 73,6SVM-DL smooth-NB 72,4SVM-DL fixed-NB 71,3SVM-vector 73,1Table 2: Combined systems (English) in cross-validation, best recall in bold.Method RecallSVM 71,1NB 68,5vector(f) 66,8DL smooth 65,9DL fixed 65,2vector(p) 65,0DL discard 60,7MFS 53,0Table 3: Single systems (Basque) in cross-validation, sorted by recall.Combination RecallSVM-vector-DL smooth-NB 70,6SVM-vector-DL fixed-NB 71,1SVM-vector-DL smooth 70,6SVM-vector-DL fixed 70,8SVM-vector-NB 71,1SVM-DL smooth-NB 70,2SVM-DL fixed-NB 70,5SVM-vector 69,0SVM-NB 69,8Table 4: Combined systems (Basque) in cross-validation, best recall in bold.
Only vector(f)was used for combination.4.2 Results on English Training DataThe results using cross-validation on theSenseval-3 data are shown in Table 1 for singlesystems, and in Table 2 for combined methods.All the algorithms have full-coverage (for En-glish and Basque), therefore the recall and theprecision are the same.
The most frequent sense(MFS) baseline is also provided, and it is easilybeaten by all the algorithms.We have to note that these figures are consis-tent with the performance we observed in theSenseval-2 data, where the vector method isthe best performing single system, and the bestcombination is SVM-vector-DL smooth.
Thereis a small gain when combining 3 systems, whichwe expected would be higher.
We submitted thebest single system, and the best combination forthis task.4.3 Results on Basque Training DataThe performance on the Senseval-3 Basquetraining data is given in Table 1 for single sys-tems, and in Table 2 for combined methods.
Inthis case, the vector method, and DL smoothobtain lower performance in relation to othermethods.
This can be due to the type of fea-tures used, which have not been tested as ex-tensively as for English.
In fact, it could hap-pen that some features contribute mostly noise.Also, the domain tag of the examples, whichcould provide useful information, was not used.There is no improvement when combining dif-ferent systems, and the result of the combina-tion of 4 systems is unusually high in relationto the English experiments.
We also submit-ted two systems for this task: the best singlemethod in cross-validation (SVM), and the best3-method combination (SVM-vector-NB).5 Results and ConclusionsTable 5 shows the performance obtained by oursystems and the winning system in the Senseval-3 evaluation.
We can see that we are very closeto the best algorithms in both languages.The recall of our systems is 1.2%-1.9% lowerthan cross-validation for every system and task,which is not surprising when we change the set-ting.
The combination of methods is useful forEnglish, where we improve the recall in 0.3%,reaching 72.3%.
The difference is statisticallysignificant according to McNemar?s test.However, the combination of methods doesnot improve the results in the the Basque task,where the SVM method alone provides betterTask Code Method Rec.Eng.
Senseval-3 Best ?
72,9Eng.
BCU combSVM-vector-DL smooth 72,3Eng.
BCU-english vector 72,0Basq.
Senseval-3 Best ?
70,4Basq.
BCU-basque SVM 69,9Basq.
BCU-Basque comb SVM-vector-NB69,5Table 5: Official results for the English andBasque lexical tasks (recall).results (69.9% recall).
In this case the differenceis not significant applying McNemar?s test.Our disambiguation procedure shows a sim-ilar behavior on the Senseval-2 and Senseval-3data for English (both in cross-validation andin the testing part), where the ensemble worksbest, followed by the vector model.
This didnot apply to the Basque dataset, where somealgorithms seem to perform below the expecta-tions.
For future work, we plan to study betterthe Basque feature set and include new features,such as domain tags.Overall, the ensemble of algorithms providesa more robust system for WSD, and is able toachieve state-of-the-art performance.6 AcknowledgementsWe wish to thank both David Yarowsky?s group,from Johns Hopkins University, and Gerard Es-cudero?s group, from Universitat Politecnica deCatalunya, for providing us software for the ac-quisition of features.
This research has beenpartially funded by the European Commission(MEANING IST-2001-34460).ReferencesI.
Aduriz, E. Agirre, I. Aldezabal, I. Alegria,X.
Arregi, J. Arriola, X. Artola, K. Gojenola,A.
Maritxalar, K. Sarasola, and M. Urkia.2000.
A word-grammar based morphologicalanalyzer for agglutinative languages.
In Pro-ceedings of the International Conference onComputational Linguistics COLING, Saar-brucken, Germany.Eneko Agirre and David Martinez.
2004.Smoothing and word sense disambiguation.(submitted).T.
Joachims.
1999.
Making large?scale SVMlearning practical.
In Advances in KernelMethods ?
Support Vector Learning, pages169?184, Cambridge, MA.
MIT Press.Bernardo Magnini and Gabriela Cavaglia?.
2000.Integrating subject field codes into WordNet.In Proceedings of the Second InternationalLREC Conference, Athens, Greece.Hwee Tou Ng.
1997.
Exemplar-based wordsense disambiguation: Some recent improve-ments.
In Proceedings of the Second EMNLPConference.
ACL, Somerset, New Jersey.Grace Ngai and Radu Florian.
2001.Transformation-based learning in the fastlane.
Proceedings of the Second Conferenceof the NAACL, Pittsburgh, PA, USA.Ted Pedersen.
2001.
A decision tree of bi-grams is an accurate predictor of word sense.Proceedings of the Second Meeting of theNAACL, Pittsburgh, PA.David Yarowsky.
1995a.
Three machine learn-ing algorithms for lexical ambiguity resolu-tion.
In PhD thesis, Department of Com-puter and Information Sciences, University ofPennsylvania.David Yarowsky.
1995b.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings of the 33rd An-nual Meeting of the Association for Compu-tational Linguistics (ACL), pages 189?196,Cambridge, MA.
