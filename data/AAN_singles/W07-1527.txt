Proceedings of the Linguistic Annotation Workshop, pages 168?175,Prague, June 2007. c?2007 Association for Computational LinguisticsExperiments with an Annotation Scheme for a Knowledge-rich Noun PhraseInterpretation SystemRoxana GirjuUniversity of Illinois at Urbana-Champaigngirju@uiuc.eduAbstractThis paper presents observations on our ex-perience with an annotation scheme that wasused in the training of a state-of-the-art nounphrase semantic interpretation system.
Thesystem relies on cross-linguistic evidencefrom a set of five Romance languages: Span-ish, Italian, French, Portuguese, and Roma-nian.
Given a training set of English nounphrases in context along with their transla-tions in the five Romance languages, ouralgorithm automatically learns a classifica-tion function that is later on applied to un-seen test instances for semantic interpreta-tion.
As training and test data we used twotext collections of different genre: Europarland CLUVI.
The training data was annotatedwith contextual features based on two state-of-the-art classification tag sets.1 IntroductionLinguistically annotated corpora are valuable re-sources for both theoretical and computational lin-guistics.
They have played an important role in anyaspect of natural language processing research, fromsupervised learning to evaluation, and have beenused in many applications such as Syntactic and Se-mantic Parsing, Information Extraction, and Ques-tion Answering.A long-term research topic in linguistics, compu-tational linguistics1, and artificial intelligence has1In the past few years at many workshops, tutorials, andcompetitions this research topic has received considerable inter-been the semantic interpretation of noun phrases(NPs).
The basic problem is simple to define: givena noun phrase constructed out of a pair of conceptsexpressed by words or phrases, c1 ?
c2, one rep-resenting the head and the other the modifier, de-termine the semantic relationship between the twoconcepts.
For example, a compound family estateshould be interpreted as the estate OWNED BY thefamily; an NP such as dress of silk should be inter-preted as denoting a dress MADE FROM silk.
Theproblem, while simple to state is hard to solve.
Thereason is that the meaning of these constructions ismost of the time ambiguous or implicit.Currently, the best-performing English NP inter-pretation methods in computational linguistics fo-cus mostly on two consecutive noun instances (nouncompounds) and are either (weakly) supervised,knowledge-intensive (Rosario and Hearst, 2001),(Rosario et al, 2002), (Moldovan et al, 2004),(Pantel and Pennacchiotti, 2006), (Pennacchiotti andPantel, 2006), (Kim and Baldwin, 2006), (Snow etal., 2006), (Girju et al, 2005; Girju et al, 2006),or use statistical models on large collections of un-labeled data (Berland and Charniak, 1999), (Lap-ata and Keller, 2004), (Nakov and Hearst, 2005),(Turney, 2006).
Unlike unsupervised models, su-pervised knowledge-rich approaches rely heavily onlarge sets of annotated training data.
For example,we previously showed (Girju et al, 2006) that, forest from the computational linguistics community: Workshopon Multiword Expressions at COLING/ACL 2006, 2004, 2003;Computational Lexical Semantics Workshop at ACL 2004; Tu-torial on Knowledge Discovery from Text at ACL 2003; Sharedtask on Semantic Role Labeling at CONLL 2005, 2004 and atSENSEVAL 2005.168the task of automatic detection of part-whole rela-tions, our system?s learning curve reached a plateauat 74% F-measure when trained on approximatively10,000 positive and negative examples.Interpreting NPs correctly requires various typesof information from world knowledge to complexcontext features.
Since the training data needs to beas accurate as possible, many of such features aremanually identified and annotated.
Thus, the anno-tation process is an important task that requires notonly considerable amount of time, but also experi-ence with various annotation schemas and tools, anda good understanding of the research topic.
More-over, the extension of the noun phrase interpretationtask to other natural languages brings forward newannotation issues.This paper presents observations on our experi-ence with an annotation scheme that was used in thetraining of a state-of-the-art noun phrase semanticinterpretation system (Girju, 2007).
The system re-lies on cross-linguistic evidence from a set of fiveRomance languages: Spanish, Italian, French, Por-tuguese, and Romanian.
Given a training set of En-glish noun phrases in context along with their trans-lations in the five Romance languages, our algo-rithm automatically learns a classification functionthat is later on applied to unseen test instances forsemantic interpretation.
As training and test datawe used two text collections of different genre: Eu-roparl2 and CLUVI3.
The training data was anno-tated with contextual features based on two state-of-the-art classification tag sets: Lauer?s set of 8 prepo-sitions (Lauer, 1995) and our list of 22 semantic re-lations.
The system achieved an accuracy of 77.9%(Europarl) and 74.31% (CLUVI).The paper is organized as follows.
Section 2presents a summary of linguistic considerations ofnoun phrases.
In Section 3 we describe the list of se-mantic interpretation categories used along with ob-servations regarding their distribution on the two dif-2http://www.isi.edu/koehn/europarl/This corpus contains over 20 million words in eleven officiallanguages of the European Union covering the proceedings ofthe European Parliament from 1996 to 2001.3CLUVI - Linguistic Corpus of the University of Vigo Par-allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/.
CLUVI is an opentext repository of parallel corpora of contemporary oral andwritten texts in some of the Romance languages, such as Gali-cian, French, Spanish, Portuguese, Basque parallel text collec-tions.ferent cross-lingual corpora.
Section 4 presents thedata used along with observations on corpus annota-tion and inter-annotator agreement.
Finally, Section5 offers some discussion and conclusions.2 Linguistic considerations of nounphrasesThe automatic discovery of semantic relations muststart with a thorough understanding of the linguisticaspects of the underlying relations.
These consider-ations are not only employed as features in the su-pervised noun phrase interpretation model, but theyare also used in the annotation process.Noun phrases can be compositional when theirmeaning is derived from the meaning of the con-stituent nouns (e.g., door knob ?
PART-WHOLE,kiss in the morning ?
TEMPORAL), or idiosyn-cratic, when the meaning is a matter of conven-tion (e.g., soap opera, sea lion).
NPs can also ex-press metaphorical names (eg, ladyfinger), propernames (e.g., John Doe), and binomial (dvandva)compounds in which neither noun is the head (e.g.,player-coach).NPs can also be classified into synthetic (verbal)and root (non-verbal) constructions.
It is widely held(Levi, 1978), (Selkirk, 1982) that the modifier nounof a synthetic noun compound, for example, may beassociated with a theta-role of the verbal head.
Forinstance, in truck driver, the noun truck satisfies theTHEME relation associated with the direct object inthe corresponding argument structure of the verb todrive.Studied cross-linguistically, noun phrases can ex-press variations from one language to another.
Forexample, English compounds of the form N1 N2(e.g., wood stove) usually translate in Romance lan-guages as N2 P N1 (e.g., four a?
bois (French) ?stove at/to wood).
Romance languages have veryfew N N compounds and they are of limited se-mantic categories, such as TYPE (e.g., legge quadro(Italian) ?
framework law).
Moreover, while En-glish N N compounds are right-headed (e.g., frame-work/modifier law/head), Romance compounds areleft-headed (e.g., legge/head quadro/modifier).For this research we focus only on English?Romance compositional noun phrases of the typeN N and N P N and disregard metaphorical and169proper names.
In the following section we presenttwo different state-of-the-art classification sets usedin NP interpretation.3 Lists of semantic classification relationsAlthough researchers (Downing, 1977), (Jespersen,1954) argued that noun compounds, and NPs in gen-eral, encode an infinite set of semantic relations,many agree (Finin, 1980), (Levi, 1978) there is alimited number of relations that occur with high fre-quency in these constructions.
However, the num-ber and the level of abstraction of these frequentlyused semantic categories are not agreed upon.
Theycan vary from a few prepositions (Lauer, 1995) tohundreds and even thousands more specific seman-tic relations (Finin, 1980).
The more abstract thecategories, the more noun phrases are covered, butalso the more room for variation as to which cat-egory a phrase should be assigned.
Lauer (Lauer,1995), for example, considers a set of eight prepo-sitions as semantic classification categories that canlink the head and the modifier nouns in a noun com-pound: of, for, with, in, on, at, about, and from.However, according to this classification, the nouncompound love story, for instance, can be classifiedboth as story of love and story about love.
The mainproblem with these abstract categories is that muchof the meaning of individual compounds is lost, andsometimes there is no way to decide whether a formis derived from one category or another.
On theother hand, lists of very specific semantic relationsare difficult to build as they usually contain a verylarge number of predicates, such as the list of allpossible verbs that can link the noun constituents.Finin (Finin, 1980), for example, uses semantic cat-egories such as ?dissolved in?
to build interpreta-tions of compounds such as ?salt water?
and ?sugarwater?.In this research we experiment with two sets ofsemantic classification categories defined at differ-ent abstraction levels.
The first is a core set of 22 se-mantic relations (22 SRs), set which was identifiedby us from the linguistics literature and from vari-ous experiments after many iterations over a periodof time (Moldovan and Girju, 2003)4.
We proved4There are also other lists of semantic relations used by theresearch community (e.g., (Barker and Szpakowicz, 1998)), butempirically that this set is encoded by noun ?
nounpairs in noun phrases and is a subset of our largerlist of 35 semantic relations.
This list, presentedin Table 1 along with examples and semantic ar-gument frames, is general enough to cover a largemajority of text semantics while keeping the seman-tic relations to a manageable number.
A semanticargument frame is defined for each semantic rela-tion and indicates the position of each semantic ar-gument in the underlying relation.
For example,?Arg1 is part of (whole) Arg2?
identifies the part(Arg1) and the whole (Arg2) entities of this rela-tion.
This representation is important since it allowsto distinguish between different arrangements of thearguments for given relation instances.
For exam-ple, most of the time, in N N compounds Arg1 pre-cedes Arg2, while in N P N constructions the po-sition is reversed (Arg2 P Arg1).
However, thisis not always the case as shown by N N instancessuch as ?ham/Arg1 sandwich/Arg2?
and ?door/Arg2knob/Arg1?.
These argument frames were intro-duced to provide consistent guide to the annotatorsto easily test the goodness-of-fit of the relations.The second set is Lauer?s list of 8 prepositions andcan be applied only to noun?noun compounds.
Weselected these two state-of-the-art sets as they areof different size and contain semantic classificationcategories at different levels of abstraction.
Lauer?slist is more abstract and, thus capable of encoding alarge number of noun compound instances found ina corpus, while our list contains finer grained seman-tic categories.
Details about the coverage of thesesemantic lists on the two different corpora (Europarland CLUVI), how well they solve the interpretationproblem of noun phrases, and the mapping from onelist to another are provided in a companion paper(Girju, 2007).4 The dataFor a better understanding of the semantic relationsencoded by N N and N P N instances, we analyzedthe semantic behavior of these constructions on alarge cross-linguistic corpora of examples.
Our in-tention is to answer questions such as:(1) What syntactic constructions are used totranslate the English instances to the target Ro-they overlap considerably with our list of 22-SR.170No.
Semantic Default argument frame ExamplesRelations1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg22 KINSHIP Arg1 IS IN KINSHIP REL.
WITH Arg2 the boy#1/Arg1?s sister#1/Arg23 PROPERTY Arg2 IS PROPERTY OF Arg1 lubricant#1/Arg1 viscosity#1/Arg24 AGENT Arg1 IS AGENT OF Arg2 investigation#2/Arg2 of the crew#2/Arg15 TEMPORAL Arg2 IS TEMPORAL LOCATION OF Arg1 morning#1/Arg2 news#3/Arg16 DEPICTION-DEPICTED Arg1 DEPICTS Arg2 a picture#1Arg1 of the nice#1/Arg27 PART-WHOLE Arg2 IS PART OF (whole) Arg1 faces#1/Arg2 of children#1/Arg18 HYPERNYMY (IS-A) Arg2 IS A Arg1 daisy#1/Arg2 flower#1/Arg19 CAUSE Arg1 CAUSES Arg2 scream#1/Arg2 of pain#1/Arg110 MAKE/PRODUCE Arg1 PRODUCES Arg2 chocolate#2/Arg2 factory#1/Arg111 INSTRUMENT Arg2 IS INSTRUMENT OF Arg1 laser#1/Arg2 treatment#1/Arg112 LOCATION Arg2 IS LOCATED IN Arg1 castle#1/Arg2 in the desert#1/Arg113 PURPOSE Arg2 IS PURPOSE OF Arg1 cough#1/Arg2 syrup#1/Arg114 SOURCE Arg2 IS SOURCE OF Arg1 grapefruit#2/Arg2 oil#3/Arg115 TOPIC Arg2 IS TOPIC OF Arg1 weather#1/Arg2 report#2/Arg216 MANNER Arg2 IS MANNER OF Arg1 performance#3/Arg1 with passion#1/Arg217 MEANS Arg2 IS MEANS OF Arg1 bus#1/Arg2 service#1/Arg118 EXPERIENCER Arg1 IS EXPERIENCER OF Arg2 the girl#1/Arg1?s fear#1/Arg219 MEASURE Arg2 IS MEASURE OF Arg1 cup#2/Arg2 of sugar#1/Arg120 RESEMBLANCE/TYPE Arg2 RESEMBLES OR IS A TYPE OF Arg1 framework#1/Arg1 law#2/Arg221 THEME Arg2 IS THEME OF Arg1 acquisition#1/Arg1 of stock#1/Arg222 BENEFICIARY Arg1 IS BENEFICIARY OF Arg2 reward#1/Arg2 for the finder#1/Arg1OTHERS altar#1 boys#1Table 1: The set of 22 semantic relations along with examples interpreted in context and the semanticargument frame.mance languages and vice-versa?
(cross-linguisticsyntactic mapping),(2) What semantic relations do these construc-tions encode?
(cross-linguistic semantic mapping),(3) What is the corpus distribution of the seman-tic relations per each syntactic construction?, andfinally(4) What is the role of English and Romanceprepositions in the NP interpretation?Thus, we collected the data from two text col-lections with different distributions and of differentgenre, Europarl and CLUVI.The Europarl text collectionEuroparl is a parallel corpora of over 20 millionwords in eleven official languages of the Euro-pean Union covering the proceedings of the Eu-ropean Parliament from 1996 to 2001.
The cor-pus was assembled by combining four of the bilin-gual sentence-aligned corpora made public as partof the freely available Europarl corpus.
Specifi-cally, the Spanish-English, Italian-English, French-English and Portuguese-English corpora were au-tomatically aligned based on exact matches of En-glish translations.
Then, only those English sen-tences which appeared verbatim in all four languagepairs were considered.
The resulting English cor-pus contained 10,000 sentences which were syntac-tically parsed (Charniak, 2000).
From these we ex-tracted the first 3,000 NP instances (N N: 48.82%and N P N: 51.18%).The CLUVI text collectionCLUVI (Linguistic Corpus of the University ofVigo) is an open text repository of parallel cor-pora of contemporary oral and written languages,resource that besides Galician also contains literarytext collections in other Romance languages.
We fo-cused only on the English-Portuguese and English-Spanish literary parallel texts from the works ofJohn Steinbeck, H. G. Wells, J. Salinger, amongothers.
Using the CLUVI search interface we cre-ated a sentence-aligned parallel corpus of 2,800English-Spanish and English-Portuguese sentences.The English versions were automatically parsed af-ter which each N N and N P N instance thus iden-tified was manually mapped to the correspondingtranslations.
The resulting corpus contains 2,200English instances with a distribution of 26.77% N Nand 73.23% N P N.1714.1 Corpus annotationFor each corpus, each NP instance was presentedseparately to two experienced annotators5 in a webinterface in context along with the English sentenceand its translations.
Since the corpora do not coversome of the languages (Romanian in Europarl andCLUVI, and Italian and French in CLUVI), threeother native speakers of these languages and flu-ent in English provided the translations which wereadded to the list.WordNet sensesThe two computational semantics annotators hadto tag each English constituent noun with its cor-responding WordNet sense6.
If the word was notfound in WordNet the instance was not considered.Tagging each noun constituent with the corre-sponding WordNet sense in context is important notonly as a feature employed in the training models,but also as guidance for the annotators to select theright semantic relation.
For instance, in the fol-lowing sentences, daisy flower expresses a PART-WHOLE relation in (1) and a IS-A relation in (2) de-pending on the sense of the noun flower (cf.
Word-Net 2.1: flower#2 is a ?reproductive organ of an-giosperm plants especially one having showy or col-orful parts?, while flower#1 is ?a plant cultivated forits blooms or blossoms?).
(1) ?Usually, more than one daisy#1 flower#2grows on top of a single stem.?
(2) ?Try them with orange or yellow flowers ofred-hot poker, solidago or other late daisy#1flowers#1, such as rudbeckias and heliopsis.
?In cases where noun senses were not enough forrelation selection, the annotators had to rely on alarger context provided by the sentence and its trans-lations as shown below.Semantic argument frameThe annotators were also asked to identify the trans-lation phrases, tag each instance with the corre-sponding semantic relation, and identify the seman-tic arguments Arg1 and Arg2 in the semantic argu-ment frame of the corresponding relation.5The annotators have extensive expertise in computationalsemantics and are fluent in at least two of the Romance lan-guages considered for this task.6For the purpose of this research we used WordNet 2.1.Thus, since the order of the semantic argumentsin an NP is not fixed (Girju et al, 2005), the an-notators were presented with the semantic argu-ment frame for each of the 22 semantic relationsand were asked to tag the NP instances accord-ingly.
For example, in PART-WHOLE instances suchas chair/Arg2 arm/Arg1 the part arm follows thewhole chair, while in button/Arg1 shirt/Arg2 the or-der is reversed.Translation instancesIn the annotation process the annotators were askedto identify and use, if necessary, the five correspond-ing translations as additional information in select-ing the semantic relation.
Since only N N and N P Nnoun phrase constructions were considered, the an-notators had to discard those instances encoded bydifferent syntactic constructions in the Romance lan-guages.For instance, the context provided by the EuroparlEnglish sentence in (3) below does not give enoughinformation for the disambiguation of the Englishnoun phrase ?judgment of the presidency?
whichcan mean either AGENT or THEME.
The annotatorshad to rely on the Romance translations in order toidentify the correct meaning in context (in this caseTHEME): valoracio?n sobre la Presidencia (Es.
), avissur la pre?sidence (Fr.
), giudizio sulla Presidenza(It.
), veredicto sobre a Preside?ncia (Port.
), evalu-area Presendit?iei (Ro.)7.(3)En.
: ?If you do , our final judgment of theSpanish presidency will be even morepositive than it has been so far.?Es.
: ?Si se hace, nuestra valoracio?n sobrela Presidencia espan?ola del Consejo sera?au?n mucho ma?s positiva de lo que es hastaahora.?Fr.
: ?Si cela arrive, notre avis sur lapre?sidence espagnole du Conseil seraencore beaucoup plus positif que ce n?estde?ja` le cas.?It.
: ?Se ci riuscira` il nostro giudizio sullaPresidenza spagnola sara` ancora piu`positivo di quanto non sia stato finora.?7En.
means English, Es.
?
Spanish, Fr.
?
French, It.
?Italian, Port.
?
Portuguese, and Ro.
?
Romanian.172Port.
: ?Se isso acontecer, o nosso veredictosobre a Preside?ncia espanhola sera?
aindamuito mais positivo do que o actual.?Ro.
: ?Daca?
are loc, evaluarea Pres?edint?ieispaniole va fi ??nca?
mai pozitiva?
deca?tpa?na?
acum.
?Semantic relationsWhenever the annotators found an example encod-ing a semantic relation or a preposition paraphraseother than those provided or they didn?t know whatinterpretation to give, they had to tag it as OTHER-SR and OTHER-PP, respectively .
For example, inthe CLUVI sentences (4) and (5) below, the nounphrases melody of the pearl and cry of death (the cryannouncing death) were tagged as OTHER-SR sincehere the context of the sentences does not indicatethe association between the two nouns.
Moreover,noun compound instances such as the corner boxand knowledge searches were tagged as OTHER-PP(box in the corner, searches after knowledge).
(3) LPE-284: ?And because the need was greatand the desire was great, the little secretmelody of the pearl that might be wasstronger this morning.?
(En.
)(4) LPE-1582: ?And then Kino?s brain clearedfrom its red concentration and he knew thesound - the keening, moaning, rising hyster-ical cry from the little cave in the side of thestone mountain, the cry of death.?
(En.
)Moreover, most of the time one instance wastagged with one semantic relation, and respectivelypreposition paraphrase, but there were also situa-tions in which an example could belong to morethan one classification category in the same con-text.
For example, Texas city is tagged as PART-WHOLE/PLACE-AREA, but also as a LOCATION re-lation using the 22-SR classification category, andrespectively as of, from, in based on the 8-PP cat-egory (e.g., city of Texas, city from Texas, andcity in Texas).
Other instances, however, can en-code a total of three semantic relations in a par-ticular context.
One such instance is cup#2 ofhot chocolate#1 in example (6) below, which wastagged in CLUVI as MEASURE/OTHER(CONTENT-CONTAINER)/LOC.
Sense #2 of cup in WordNetrefers to ?the quantity the cup will hold?
(cf.
Word-Net 2.1), thus mostly indicating a MEASURE rela-tion.
(5) 557-AGU: ?Wouldn?t you like a cup of hotchocolate before you go??
(En.
)However, since most hot beverages (such as tea,coffee, and chocolate) are served in cups, it standsto reason that the instance can be easily paraphrasedas a cup holding hold chocolate.
Although our cur-rent NP interpretation system (Girju, 2007) doesnot differentiate between LOCATION and CONTENT-CONTAINER (as other researchers (Tyler and Evans,2003)8, we consider CONTENT-CONTAINER as aspecial type of LOCATION), we capture them in ourannotation scheme.Other examples of multiple annotations areMEASURE/PART-WHOLE (e.g., an abundance ofbuildings, a bunch of guys), Overall, 0.5% Europarland 6.9% CLUVI instances were tagged with morethan one semantic relation, and almost all noun com-pound instances were tagged with more than onepreposition.Thus, the annotated instances used in the cor-pus analysis and system training phases havethe following format: <NPEn ;NPEs; NPIt;NPFr; NPPort; NPRo; target>.
The word tar-get is one of the 23 (22 + OTHER) semanticrelations or one of the eight prepositions con-sidered.
For example, <judgment#2/Arg1 ofpresidency#2/Arg2; valoracio?n sobre la Presiden-cia; avis sur la pre?sidence; giudizio sulla Pres-idenza; veredicto sobre a Preside?ncia; evaluareaPres?edint?iei; THEME>.4.2 Inter-annotator agreementThe annotators?
agreement was measured usingKappa statistics, one of the most frequently usedmeasure of inter-annotator agreement for classifica-tion tasks: K = Pr(A)?Pr(E)1?Pr(E) , where Pr(A) is theproportion of times the annotators agree and Pr(E)is the probability of agreement by chance.
The Kcoefficient is 1 if there is a total agreement amongthe annotators, and 0 if there is no agreement otherthan that expected to occur by chance.8(Tyler and Evans, 2003) cite child language acquisitionstudies which show there is a strong cognitive relationship be-tween LOCATION and CONTENT-CONTAINER.173The Kappa values obtained on each corpus areshown in Table 2.
We also computed the numberof pairs that were tagged with OTHER by both an-notators for each semantic relation and prepositionparaphrase, over the number of examples classifiedin that category by at least one of the judges.
For thenoun compound instances that encoded more thanone classification category, the agreement was doneon one of the relations only.The agreement obtained for the Europarl corpusis higher than the one for CLUVI on both classifica-tion sets.
This is partially explained by the distribu-tion of semantic relations in both corpora.
Overall,the K coefficient shows a fair to good level of agree-ment for the corpus data on the set of 22-SRs, tak-ing into consideration the task difficulty.
The levelof agreement for the prepositional paraphrases wasmuch higher.
All these can be explained by the in-structions the annotators received prior to the anno-tation and by their expertise in lexical semantics.Corpus Classification Kappa Agreementtag sets N N N P N OTHEREuroparl 8-PP 0.80 N/A 91%22-SR 0.61 0.67 78%CLUVI 8-PP 0.77 N/A 86%22-SR 0.56 0.58 69%Table 2: The inter-annotator agreement on the NP annotationon the two corpora.
For the noun compound instances that en-coded more than one semantic classification category, the agree-ment was done on one of the relations only.
?N/A?
means notapplicable.13.05% of Europarl9 and 1.9% of CLUVI in-stances that could not be tagged with Lauer?s prepo-sitions were included in OTHER-PP category.
About99% of the Europarl N N instances encode TYPE re-lations (e.g., framework law), while in CLUVI mostof them were TYPE (e.g., nightmare sensation), fol-lowed by OTHER-SR (e.g., altar boys), and IS-A(e.g., Winchester carbine).From the initial corpus we considered those En-glish instances that had all the translations encodedby N N and N P N. Out of these, we selected only1,023 Europarl and 1,008 CLUVI instances encodedby N N and N P N in all languages considered andresulted after agreement10.
We split the corpora us-9Only 5.70% of the TYPE instances in the Europarl corpuswere unique.10The annotated corpora resulted in this research are avail-able at http://apfel.ai.uiuc.edu.ing a 8:2 training - test ratio and used it to train andtest our system.
Details about the experiments andthe results obtained are presented in (Girju, 2007).5 Discussion and conclusionsIn this paper we presented some observations on ourexperience with an annotation scheme that was usedin the training of a state-of-the-art noun phrase se-mantic interpretation system.
These observationsare defined in the framework of a larger project.
Thisproject is to investigate various linguistic issues anddevelop specific language models for the interpreta-tion of noun phrase constructions in Germanic, Ro-mance, and other classes of languages.Our approach to NP interpretation, and thus an-notation procedure, is novel in several ways.
Wedefine the problem in a cross-linguistic frameworkand provide empirical observations on various an-notation issues based on a set of two different cor-pora using two state-of-the-art classification tag sets:Lauer?s prepositions and our list of 22 relations.The linguistic implications are also important tomention here.
The annotation investigations done inthis research provide new insights into the researchtopic at hand, the semantic interpretation of nounphrases, in particular and the identification of se-mantic relations between nominals (irrespective ofthe syntactic constructions that link the two nouns),in general.
One such linguistic aspect is the impor-tance of context for this task.
Sometimes, the localcontext of the noun phrase is not enough to disam-biguate the underlying instances.
For this, the anno-tators need to relay on world and domain specificknowledge and the entire context of the sentence,or consider a larger context window (from a simpleparagraph including the sentence, to the discourse ofthe text) as shown below in (6), (7), and (8).
In (6)and (7), for example, neither the context of the sen-tence, nor the context of their paragraph provide themeaning of the NPs.
Many of the CLUVI instancestagged as OTHER-SR (such as the music of the pearlin (6)), are naming phrases ?
they were defined onlyonce in the text collection and later on mentioned torefer to the initial concept.In (8), on the other hand, the meaning of theNP the destruction of the Palestinian Authority isTHEME and not AGENT as might be considered bydefault.174(6) LPE-390: ?And the music of the pearlrose like a chorus of trumpets in his ears.?
(CLUVI)(7) ?Mr President, the violent destruction of theState of Israel.?
(Europarl)(8) ?The spread of the settlements, the seizingof land, the curfews, the Palestinians im-prisoned in their own villages, the summaryexecutions, the ambulances prevented fromreaching their destinations, the women giv-ing birth at check points, the destruction ofthe Palestinian Authority: these are not mis-takes or accidents.?
(Europarl)6 AcknowledgmentsWe would like to thank all the people who helpedwith the corpus creation and annotation, and thosewith whom we had nice discussions about vari-ous semantic relations.
Without them this researchwouldn?t have been possible: Archna Bhatia, Gus-tavo Cavallin, Brian Drexler, Matt Garley, TaniaIonin, Matt Niemi, Dustin Parr, and Chris Struven.And last, but not least we like to thank the reviewersfor their useful comments.ReferencesK.
Barker and S. Szpakowicz.
1998.
Semi-automatic recogni-tion of noun modifier relationships.
In the Proceedings ofthe Association for Computational Linguistics / Conferenceon Computational Linguistics.M.
Berland and E. Charniak.
1999.
Finding Parts in Very LargeCorpora.
In the Proceedings of the Association for Compu-tational Linguistics (ACL), University of Maryland.E.
Charniak.
2000.
A Maximum-entropy-inspired Parser.
Inthe Proceedings of the North American Chapter of the As-sociation for Computational Linguistics (NAACL), Seattle,Washington.P.
Downing.
1977.
On the Creation and Use of English Com-pound Nouns.
Language, 53(4):810?842.T.
W. Finin.
1980.
The Semantic Interpretation of CompoundNominals.
Ph.D. thesis, University of Illinois at Urbana-Champaign.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.
Onthe semantics of noun compounds.
Computer Speech andLanguage, 19(4):479?496.R.
Girju, A. Badulescu, and D. Moldovan.
2006.
Automaticdiscovery of part-whole relations.
Computational Linguis-tics, 32(1).R.
Girju.
2007.
Improving the interpretation of noun phraseswith cross-linguistic information.
In the Proceedings of theAssociation for Computational Linguistics (ACL), Prague.O.
Jespersen.
1954.
A Modern English Grammar on HistoricalPrinciples.
London.S.
N. Kim and T. Baldwin.
2006.
In the Proceedings of the As-sociation for Computational Linguistics, Sydney, Australia.M.
Lapata and F. Keller.
2004.
The Web as a baseline: Evaluat-ing the performance of unsupervised Web-based models fora range of NLP tasks.
In the Proceedings of the Human Lan-guage Technology Conference / North American Chapter ofthe Association of Computational Linguistics (HLT-NAACL).M.
Lauer.
1995.
Corpus statistics meet the noun compound:Some empirical results.
In the Proceedings of Associationfor Computational Linguistics (ACL), Cambridge, Mass.J.
Levi.
1978.
The Syntax and Semantics of Complex Nominals.Academic Press, New York.D.
Moldovan and R. Girju.
2003.
Knowledge discovery fromtext.
In the Tutorial Proceedings of the Association for Com-putational Linguistics (ACL), Sapporo, Japan.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.2004.
Models for the semantic classification of nounphrases.
In the Proceedings of the HLT/NAACL Workshopon Computational Lexical Semantics, Boston, MA.P.
Nakov and M. Hearst.
2005.
Search engine statistics be-yond the n-gram: Application to noun compo und bracket-ing.
In the Proceedings of the Computational Natural Lan-guage Learning Conference.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Leverag-ing generic patterns for automatically harvesting semanticrelations.
In the Proceedings of the International Confer-ence for Computational Linguistics (COLING/ACL), Syd-ney, Australia.M.
Pennacchiotti and P. Pantel.
2006.
Ontologizing semanticrelations.
In the Proceedings of Conference on Computa-tional Linguistics / Association for Computational Linguis-tics (COLING/ACL-06), Sydney, Australia.
Association forComputational Linguistics.B.
Rosario and M. Hearst.
2001.
Classifying the semantic re-lations in noun compounds.
In the Proceedings of the 2001EMNLP Conference.B.
Rosario, M. Hearst, and C. Fillmore.
2002.
The descent ofhierarchy, and selection in relational semantics.
In the Pro-ceedings of the Association for Computational Linguistics.E.
Selkirk.
1982.
Syntax of words.
In Linguistic Inquiry Mono-graph.
MIT Press.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic taxonomyinduction from heterogenous evidence.
In the Proceedingsof the Conference on Computational Linguistics / Associa-tion for Computational Linguistics (COLING-ACL), Sydney,Australia.P.
Turney.
2006.
Expressing implicit semantic relations with-out supervision.
In the Proceedings of the Conference onComputational Linguistics / Association for ComputationalLinguistics (COLING/ACL), Sydney, Australia.A.
Tyler and V. Evans.
2003.
Spatial Experience, Lexical Struc-ture and Motivation: The Case of In.
In G. Radden and K.Panther.
Studies in Linguistic Motivation.
Berlin and NewYork: Mouton de Gruyter.175
