Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683?1692,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMinimum Risk Training for Neural Machine TranslationShiqi Shen?, Yong Cheng#, Zhongjun He+, Wei He+, Hua Wu+, Maosong Sun?, Yang Liu??
?State Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing, China#Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China+Baidu Inc., Beijing, China{vicapple22, chengyong3001}@gmail.com, {hezhongjun, hewei06, wu hua}@baidu.com, {sms, liuyang2011}@tsinghua.edu.cnAbstractWe propose minimum risk training forend-to-end neural machine translation.Unlike conventional maximum likelihoodestimation, minimum risk training is ca-pable of optimizing model parameters di-rectly with respect to arbitrary evaluationmetrics, which are not necessarily differ-entiable.
Experiments show that our ap-proach achieves significant improvementsover maximum likelihood estimation on astate-of-the-art neural machine translationsystem across various languages pairs.Transparent to architectures, our approachcan be applied to more neural networksand potentially benefit more NLP tasks.1 IntroductionRecently, end-to-end neural machine transla-tion (NMT) (Kalchbrenner and Blunsom, 2013;Sutskever et al, 2014; Bahdanau et al, 2015)has attracted increasing attention from the com-munity.
Providing a new paradigm for machinetranslation, NMT aims at training a single, largeneural network that directly transforms a source-language sentence to a target-language sentencewithout explicitly modeling latent structures (e.g.,word alignment, phrase segmentation, phrase re-ordering, and SCFG derivation) that are vital inconventional statistical machine translation (SMT)(Brown et al, 1993; Koehn et al, 2003; Chiang,2005).Current NMT models are based on the encoder-decoder framework (Cho et al, 2014; Sutskeveret al, 2014), with an encoder to read and encodea source-language sentence into a vector, fromwhich a decoder generates a target-language sen-tence.
While early efforts encode the input into a?Corresponding author: Yang Liu.fixed-length vector, Bahdanau et al (2015) advo-cate the attention mechanism to dynamically gen-erate a context vector for a target word being gen-erated.Although NMT models have achieved results onpar with or better than conventional SMT, they stillsuffer from a major drawback: the models are op-timized to maximize the likelihood of training datainstead of evaluation metrics that actually quantifytranslation quality.
Ranzato et al (2015) indicatetwo drawbacks of maximum likelihood estimation(MLE) for NMT.
First, the models are only ex-posed to the training distribution instead of modelpredictions.
Second, the loss function is defined atthe word level instead of the sentence level.In this work, we introduce minimum risk train-ing (MRT) for neural machine translation.
Thenew training objective is to minimize the expectedloss (i.e., risk) on the training data.
MRT has thefollowing advantages over MLE:1.
Direct optimization with respect to evalu-ation metrics: MRT introduces evaluationmetrics as loss functions and aims to mini-mize expected loss on the training data.2.
Applicable to arbitrary loss functions: ourapproach allows arbitrary sentence-level lossfunctions, which are not necessarily differen-tiable.3.
Transparent to architectures: MRT does notassume the specific architectures of NMT andcan be applied to any end-to-end NMT sys-tems.While MRT has been widely used in conven-tional SMT (Och, 2003; Smith and Eisner, 2006;He and Deng, 2012) and deep learning based MT(Gao et al, 2014), to the best of our knowledge,this work is the first effort to introduce MRT1683into end-to-end NMT.
Experiments on a variety oflanguage pairs (Chinese-English, English-French,and English-German) show that MRT leads to sig-nificant improvements over MLE on a state-of-the-art NMT system (Bahdanau et al, 2015).2 BackgroundGiven a source sentence x = x1, .
.
.
,xm, .
.
.
,xMand a target sentence y = y1, .
.
.
,yn, .
.
.
,yN,end-to-end NMT directly models the translationprobability:P (y|x;?)
=N?n=1P (yn|x,y<n;?
), (1)where ?
is a set of model parameters and y<n=y1, .
.
.
,yn?1is a partial translation.Predicting the n-th target word can be modeledby using a recurrent neural network:P (yn|x,y<n;?)
?
exp{q(yn?1, zn, cn,?
)}, (2)where znis the n-th hidden state on the targetside, cnis the context for generating the n-th tar-get word, and q(?)
is a non-linear function.
Cur-rent NMT approaches differ in calculating znandcnand defining q(?).
Please refer to (Sutskever etal., 2014; Bahdanau et al, 2015) for more details.Given a set of training examples D ={?x(s),y(s)?
}Ss=1, the standard training objectiveis to maximize the log-likelihood of the trainingdata:?
?MLE= argmax?{L(?
)}, (3)whereL(?)
=S?s=1logP (y(s)|x(s);?)
(4)=S?s=1N(s)?n=1logP (y(s)n|x(s),y(s)<n;?).
(5)We use N(s)to denote the length of the s-th targetsentence y(s).The partial derivative with respect to a modelparameter ?iis calculated as?L(?)?
?i=S?s=1N(s)?n=1?P (y(s)n|x(s),y(s)<n;?)/?
?iP (y(s)n|x(s),y(s)<n;?).
(6)Ranzato et al (2015) point out that MLEfor end-to-end NMT suffers from two drawbacks.First, while the models are trained only on thetraining data distribution, they are used to generatetarget words on previous model predictions, whichcan be erroneous, at test time.
This is referred toas exposure bias (Ranzato et al, 2015).
Second,MLE usually uses the cross-entropy loss focus-ing on word-level errors to maximize the proba-bility of the next correct word, which might hardlycorrelate well with corpus-level and sentence-levelevaluation metrics such as BLEU (Papineni et al,2002) and TER (Snover et al, 2006).As a result, it is important to introduce newtraining algorithms for end-to-end NMT to includemodel predictions during training and optimizemodel parameters directly with respect to evalu-ation metrics.3 Minimum Risk Training for NeuralMachine TranslationMinimum risk training (MRT), which aims tominimize the expected loss on the training data,has been widely used in conventional SMT (Och,2003; Smith and Eisner, 2006; He and Deng,2012) and deep learning based MT (Gao et al,2014).
The basic idea is to introduce evaluationmetrics as loss functions and assume that the opti-mal set of model parameters should minimize theexpected loss on the training data.Let ?x(s),y(s)?
be the s-th sentence pair in thetraining data and y be a model prediction.
We usea loss function ?
(y,y(s)) to measure the discrep-ancy between the model prediction y and the gold-standard translation y(s).
Such a loss functioncan be negative smoothed sentence-level evalua-tion metrics such as BLEU (Papineni et al, 2002),NIST (Doddington, 2002), TER (Snover et al,2006), or METEOR (Lavie and Denkowski, 2009)that have been widely used in machine translationevaluation.
Note that a loss function is not param-eterized and thus not differentiable.In MRT, the risk is defined as the expected losswith respect to the posterior distribution:R(?)
=S?s=1Ey|x(s);?[?
(y,y(s))](7)=S?s=1?y?Y(x(s))P (y|x(s);?)?
(y,y(s)), (8)where Y(x(s)) is a set of all possible candidatetranslations for x(s).1684?
(y,y(s)) P (y|x(s);?
)y1?1.0 0.2 0.3 0.5 0.7y2?0.3 0.5 0.2 0.2 0.1y3?0.5 0.3 0.5 0.3 0.2Ey|x(s);?[?
(y,y(s))] ?0.50 ?0.61 ?0.71 ?0.83Table 1: Example of minimum risk training.
x(s)is an observed source sentence, y(s)is its correspondinggold-standard translation, and y1, y2, and y3are model predictions.
For simplicity, we suppose that thefull search space contains only three candidates.
The loss function ?
(y,y(s)) measures the differencebetween model prediction and gold-standard.
The goal of MRT is to find a distribution (the last column)that correlates well with the gold-standard by minimizing the expected loss.The training objective of MRT is to minimizethe risk on the training data:?
?MRT= argmin?{R(?)}.
(9)Intuitively, while MLE aims to maximize thelikelihood of training data, our training objective isto discriminate between candidates.
For example,in Table 1, suppose the candidate set Y(x(s)) con-tains only three candidates: y1, y2, and y3.
Ac-cording to the losses calculated by comparing withthe gold-standard translation y(s), it is clear thaty1is the best candidate, y3is the second best, andy2is the worst: y1> y3> y2.
The right half ofTable 1 shows four models.
As model 1 (column3) ranks the candidates in a reverse order as com-pared with the gold-standard (i.e., y2> y3> y1),it obtains the highest risk of ?0.50.
Achievinga better correlation with the gold-standard thanmodel 1 by predicting y3> y1> y2, model 2(column 4) reduces the risk to ?0.61.
As model3 (column 5) ranks the candidates in the same or-der with the gold-standard, the risk goes down to?0.71.
The risk can be further reduced by con-centrating the probability mass on y1(column 6).As a result, by minimizing the risk on the trainingdata, we expect to obtain a model that correlateswell with the gold-standard.In MRT, the partial derivative with respect to amodel parameter ?iis given by?R(?)??i=S?s=1Ey|x(s);?[?
(y,y(s))?N(s)?n=1?P (yn|x(s),y<n;?)/?
?iP (yn|x(s),y<n;?)].
(10)Since Eq.
(10) suggests there is no need to dif-ferentiate ?
(y,y(s)), MRT allows arbitrary non-differentiable loss functions.
In addition, our ap-proach is transparent to architectures and can beapplied to arbitrary end-to-end NMT models.Despite these advantages, MRT faces a majorchallenge: the expectations in Eq.
(10) are usu-ally intractable to calculate due to the exponentialsearch space of Y(x(s)), the non-decomposabilityof the loss function ?
(y,y(s)), and the contextsensitiveness of NMT.To alleviate this problem, we propose to onlyuse a subset of the full search space to approxi-mate the posterior distribution and introduce a newtraining objective:?R(?)
=S?s=1Ey|x(s);?,?[?
(y,y(s))](11)=S?s=1?y?S(x(s))Q(y|x(s);?, ?)?
(y,y(s)), (12)where S(x(s)) ?
Y(x(s)) is a sampled subset ofthe full search space, and Q(y|x(s);?, ?)
is a dis-tribution defined on the subspace S(x(s)):Q(y|x(s);?, ?)
=P (y|x(s);?)??y?
?S(x(s))P (y?|x(s);?)?.
(13)Note that ?
is a hyper-parameter that controls thesharpness of the Q distribution (Och, 2003).Algorithm 1 shows how to build S(x(s)) bysampling the full search space.
The sampled sub-set initializes with the gold-standard translation(line 1).
Then, the algorithm keeps sampling a tar-get word given the source sentence and the partialtranslation until reaching the end of sentence (lines3-16).
Note that sampling might produce dupli-cate candidates, which are removed when building1685Input: the s-th source sentence in the training data x(s), the s-th target sentence in the training data y(s), the set ofmodel parameters ?, the limit on the length of a candidate translation l, and the limit on the size of sampledspace k.Output: sampled space S(x(s)).1 S(x(s))?
{y(s)}; // the gold-standard translation is included2 i?
1;3 while i ?
k do4 y?
?
; // an empty candidate translation5 n?
1;6 while n ?
l do7 y ?
P (yn|x(s),y<n;?
); // sample the n-th target word8 y?
y ?
{y};9 if y = EOS then10 break; // terminate if reach the end of sentence11 end12 n?
n+ 1;13 end14 S(x(s))?
S(x(s)) ?
{y};15 i?
i+ 1;16 endAlgorithm 1: Sampling the full search space.the subspace.
We find that it is inefficient to forcethe algorithm to generate exactly k distinct candi-dates because high-probability candidates can besampled repeatedly, especially when the probabil-ity mass highly concentrates on a few candidates.In practice, we take advantage of GPU?s parallelarchitectures to speed up the sampling.1Given the sampled space, the partial derivativewith respect to a model parameter ?iof?R(?)
isgiven by??R(?)?
?i= ?S?s=1Ey|x(s);?,?
[?P (y|x(s);?)/?
?iP (y|x(s);?)?(?(y,y(s))?Ey?|x(s);?,?[?(y?,y(s))])].
(14)Since |S(x(s))|  |Y(x(s))|, the expectationsin Eq.
(14) can be efficiently calculated by ex-plicitly enumerating all candidates in S(x(s)).
Inour experiments, we find that approximating thefull space with 100 samples (i.e., k = 100) worksvery well and further increasing sample size doesnot lead to significant improvements (see Section4.3).1To build the subset, an alternative to sampling is com-puting top-k translations.
We prefer sampling to comput-ing top-k translations for efficiency: sampling is more effi-cient and easy-to-implement than calculating k-best lists, es-pecially given the extremely parallel architectures of GPUs.4 Experiments4.1 SetupWe evaluated our approach on three transla-tion tasks: Chinese-English, English-French, andEnglish-German.
The evaluation metric is BLEU(Papineni et al, 2002) as calculated by themulti-bleu.perl script.For Chinese-English, the training data consistsof 2.56M pairs of sentences with 67.5M Chinesewords and 74.8M English words, respectively.
Weused the NIST 2006 dataset as the validation set(hyper-parameter optimization and model selec-tion) and the NIST 2002, 2003, 2004, 2005, and2008 datasets as test sets.For English-French, to compare with the resultsreported by previous work on end-to-end NMT(Sutskever et al, 2014; Bahdanau et al, 2015;Jean et al, 2015; Luong et al, 2015b), we usedthe same subset of the WMT 2014 training cor-pus that contains 12M sentence pairs with 304MEnglish words and 348M French words.
The con-catenation of news-test 2012 and news-test 2013serves as the validation set and news-test 2014 asthe test set.For English-German, to compare with theresults reported by previous work (Jean et al,2015; Luong et al, 2015a), we used the same sub-set of the WMT 2014 training corpus that contains4M sentence pairs with 91M English words and87M German words.
The concatenation of news-test 2012 and news-test 2013 is used as the valida-tion set and news-test 2014 as the test set.168605101520253035400  5  10  15  20  25  30  35  40BLEUscoreTraining time (hours)?=5?10-3?=1?10-4?=1?10-1Figure 1: Effect of ?
on the Chinese-English vali-dation set.We compare our approach with two state-of-the-art SMT and NMT systems:1.
MOSES (Koehn and Hoang, 2007): a phrase-based SMT system using minimum error ratetraining (Och, 2003).2.
RNNSEARCH (Bahdanau et al, 2015): anattention-based NMT system using maxi-mum likelihood estimation.MOSES uses the parallel corpus to train aphrase-based translation model and the targetpart to train a 4-gram language model using theSRILM toolkit (Stolcke, 2002).2The log-linearmodel Moses uses is trained by the minimum errorrate training (MERT) algorithm (Och, 2003) thatdirectly optimizes model parameters with respectto evaluation metrics.RNNSEARCH uses the parallel corpus to trainan attention-based neural translation model usingthe maximum likelihood criterion.On top of RNNSEARCH, our approach replacesMLE with MRT.
We initialize our model with theRNNsearch50 model (Bahdanau et al, 2015).
Weset the vocabulary size to 30K for Chinese-Englishand English-French and 50K for English-German.The beam size for decoding is 10.
The defaultloss function is negative smoothed sentence-levelBLEU.4.2 Effect of ?The hyper-parameter ?
controls the smoothness ofthe Q distribution (see Eq.
(13)).
As shown in2It is possible to exploit larger monolingual corpora forboth MOSES and RNNSEARCH (Gulcehre et al, 2015; Sen-nrich et al, 2015).
We leave this for future work.05101520253035400  10  20  30  40  50  60BLEUscoreTraining time (hours)k=100k=50k=25Figure 2: Effect of sample size on the Chinese-English validation set.criterion loss BLEU TER NISTMLE N/A 30.48 60.85 8.26?sBLEU 36.71 53.48 8.90MRT sTER 30.14 53.83 6.02?sNIST 32.32 56.85 8.90Table 2: Effect of loss function on the Chinese-English validation set.Figure 1, we find that ?
has a critical effect onBLEU scores on the Chinese-English validationset.
While ?
= 1 ?
10?1deceases BLEU scoresdramatically, ?
= 5 ?
10?3improves translationquality significantly and consistently.
Reducing?
further to 1 ?
10?4, however, results in lowerBLEU scores.
Therefore, we set ?
= 5?
10?3inthe following experiments.4.3 Effect of Sample SizeFor efficiency, we sample k candidate translationsfrom the full search space Y(x(s)) to build anapproximate posterior distribution Q (Section 3).Figure 2 shows the effect of sample size k onthe Chinese-English validation set.
It is clear thatBLEU scores consistently rise with the increase ofk.
However, we find that a sample size larger than100 (e.g., k = 200) usually does not lead to signi-ficant improvements and increases the GPU mem-ory requirement.
Therefore, we set k = 100 in thefollowing experiments.4.4 Effect of Loss FunctionAs our approach is capable of incorporating evalu-ation metrics as loss functions, we investigate theeffect of different loss functions on BLEU, TER168705101520253035400  50  100  150  200  250  300BLEUscoreTraining time (hours)MRTMLEFigure 3: Comparison of training time on theChinese-English validation set.and NIST scores on the Chinese-English valida-tion set.
As shown in Table 2, negative smoothedsentence-level BLEU (i.e, ?sBLEU) leads to sta-tistically significant improvements over MLE (p <0.01).
Note that the loss functions are all defined atthe sentence level while evaluation metrics are cal-culated at the corpus level.
This discrepancy mightexplain why optimizing with respect to sTER doesnot result in the lowest TER on the validation set.As ?sBLEU consistently improves all evaluationmetrics, we use it as the default loss function inour experiments.4.5 Comparison of Training TimeWe used a cluster with 20 Telsa K40 GPUs to trainthe NMT model.
For MLE, it takes the clusterabout one hour to train 20,000 mini-batches, eachof which contains 80 sentences.
The training timefor MRT is longer than MLE: 13,000 mini-batchescan be processed in one hour on the same cluster.Figure 3 shows the learning curves of MLE andMRT on the validation set.
For MLE, the BLEUscore reaches its peak after about 20 hours andthen keeps going down dramatically.
Initializingwith the best MLE model, MRT increases BLEUscores dramatically within about 30 hours.3Af-terwards, the BLEU score keeps improving grad-ually but there are slight oscillations.4.6 Results on Chinese-English Translation4.6.1 Comparison of BLEU ScoresTable 3 shows BLEU scores on Chinese-Englishdatasets.
For RNNSEARCH, we follow Luong3Although it is possible to initialize with a randomizedmodel, it takes much longer time to converge.010203040500  10  20  30  40  50  60  70BLEUscoreInput sentence lengthMRTMLEMosesFigure 4: BLEU scores on the Chinese-Englishtest set over various input sentence lengths.01020304050607080900  10  20  30  40  50  60  70Output sentencelengthInput sentence lengthMosesMRTMLEFigure 5: Comparison of output sentences lengthson the Chinese-English test set.et al (2015b) to handle rare words.
We findthat introducing minimum risk training into neu-ral MT leads to surprisingly substantial improve-ments over MOSES and RNNSEARCH with MLEas the training criterion (up to +8.61 and +7.20BLEU points, respectively) across all test sets.
Allthe improvements are statistically significant.4.6.2 Comparison of TER ScoresTable 4 gives TER scores on Chinese-Englishdatasets.
The loss function used in MRT is?sBLEU.
MRT still obtains dramatic improve-ments over MOSES and RNNSEARCH with MLEas the training criterion (up to -10.27 and -8.32TER points, respectively) across all test sets.
Allthe improvements are statistically significant.4.6.3 BLEU Scores over Sentence LengthsFigure 4 shows the BLEU scores of translationsgenerated by MOSES, RNNSEARCH with MLE,1688System Training MT06 MT02 MT03 MT04 MT05 MT08MOSES MERT 32.74 32.49 32.40 33.38 30.20 25.28RNNSEARCHMLE 30.70 35.13 33.73 34.58 31.76 23.57MRT 37.34 40.36 40.93 41.37 38.81 29.23Table 3: Case-insensitive BLEU scores on Chinese-English translation.System Training MT06 MT02 MT03 MT04 MT05 MT08MOSES MERT 59.22 62.97 62.44 61.20 63.44 62.36RNNSEARCHMLE 60.74 58.94 60.10 58.91 61.74 64.52MRT 52.86 52.87 52.17 51.49 53.42 57.21Table 4: Case-insensitive TER scores on Chinese-English translation.MLE vs. MRT< = >evaluator 1 54% 24% 22%evaluator 2 53% 22% 25%Table 5: Subjective evaluation of MLE and MRTon Chinese-English translation.and RNNSEARCH with MRT on the Chinese-English test set with respect to input sentencelengths.
While MRT consistently improves overMLE for all lengths, it achieves worse translationperformance for sentences longer than 60 words.One reason is that RNNSEARCH tends to pro-duce short translations for long sentences.
Asshown in Figure 5, both MLE and MRE gen-erate much shorter translations than MOSES.This results from the length limit imposed byRNNSEARCH for efficiency reasons: a sentencein the training set is no longer than 50 words.
Thislimit deteriorates translation performance becausethe sentences in the test set are usually longer than50 words.4.6.4 Subjective EvaluationWe also conducted a subjective evaluation to vali-date the benefit of replacing MLE with MRT.
Twohuman evaluators were asked to compare MLEand MRT translations of 100 source sentences ran-domly sampled from the test sets without know-ing from which system a candidate translation wasgenerated.Table 5 shows the results of subjective evalua-tion.
The two human evaluators made close judge-ments: around 54% of MLE translations are worsethan MRE, 23% are equal, and 23% are better.4.6.5 Example TranslationsTable 6 shows some example translations.
Wefind that MOSES translates a Chinese string ?yiwei fuze yu pingrang dangju da jiaodao de qianguowuyuan guanyuan?
that requires long-distancereordering in a wrong way, which is a notoriouschallenge for statistical machine translation.
Incontrast, RNNSEARCH-MLE seems to overcomethis problem in this example thanks to the capa-bility of gated RNNs to capture long-distance de-pendencies.
However, as MLE uses a loss func-tion defined only at the word level, its translationlacks sentence-level consistency: ?chinese?
oc-curs twice while ?two senate?
is missing.
By opti-mizing model parameters directly with respect tosentence-level BLEU, RNNSEARCH-MRT seemsto be able to generate translations more consis-tently at the sentence level.4.7 Results on English-French TranslationTable 7 shows the results on English-French trans-lation.
We list existing end-to-end NMT systemsthat are comparable to our system.
All these sys-tems use the same subset of the WMT 2014 train-ing corpus and adopt MLE as the training crite-rion.
They differ in network architectures and vo-cabulary sizes.
Our RNNSEARCH-MLE systemachieves a BLEU score comparable to that of Jeanet al (2015).
RNNSEARCH-MRT achieves thehighest BLEU score in this setting even with a vo-cabulary size smaller than Luong et al (2015b)and Sutskever et al (2014).
Note that our ap-proach does not assume specific architectures andcan in principle be applied to any NMT systems.4.8 Results on English-German TranslationTable 8 shows the results on English-Germantranslation.
Our approach still significantly out-1689Source meiguo daibiao tuan baokuo laizi shidanfu daxue de yi wei zhongguozhuanjia , liang ming canyuan waijiao zhengce zhuli yiji yi wei fuze yupingrang dangju da jiaodao de qian guowuyuan guanyuan .Reference the us delegation consists of a chinese expert from the stanford university, two senate foreign affairs policy assistants and a former state departmentofficial who was in charge of dealing with pyongyang authority .MOSES the united states to members of the delegation include representatives fromthe stanford university , a chinese expert , two assistant senate foreign policyand a responsible for dealing with pyongyang before the officials of the statecouncil .RNNSEARCH-MLE the us delegation comprises a chinese expert from stanford university , achinese foreign office assistant policy assistant and a former official who isresponsible for dealing with the pyongyang authorities .RNNSEARCH-MRT the us delegation included a chinese expert from the stanford university ,two senate foreign policy assistants , and a former state department officialwho had dealings with the pyongyang authorities .Table 6: Example Chinese-English translations.
?Source?
is a romanized Chinese sentence, ?Refer-ence?
is a gold-standard translation.
?MOSES?
and ?RNNSEARCH-MLE?
are baseline SMT and NMTsystems.
?RNNSEARCH-MRT?
is our system.System Architecture Training Vocab BLEUExisting end-to-end NMT systemsBahdanau et al (2015) gated RNN with searchMLE30K 28.45Jean et al (2015) gated RNN with search 30K 29.97Jean et al (2015) gated RNN with search + PosUnk 30K 33.08Luong et al (2015b) LSTM with 4 layers 40K 29.50Luong et al (2015b) LSTM with 4 layers + PosUnk 40K 31.80Luong et al (2015b) LSTM with 6 layers 40K 30.40Luong et al (2015b) LSTM with 6 layers + PosUnk 40K 32.70Sutskever et al (2014) LSTM with 4 layers 80K 30.59Our end-to-end NMT systemsthis workgated RNN with search MLE 30K 29.88gated RNN with search MRT 30K 31.30gated RNN with search + PosUnk MRT 30K 34.23Table 7: Comparison with previous work on English-French translation.
The BLEU scores are case-sensitive.
?PosUnk?
denotes Luong et al (2015b)?s technique of handling rare words.System Architecture Training BLEUExisting end-to-end NMT systemsJean et al (2015) gated RNN with searchMLE16.46Jean et al (2015) gated RNN with search + PosUnk 18.97Jean et al (2015) gated RNN with search + LV + PosUnk 19.40Luong et al (2015a) LSTM with 4 layers + dropout + local att.
+ PosUnk 20.90Our end-to-end NMT systemsthis workgated RNN with search MLE 16.45gated RNN with search MRT 18.02gated RNN with search + PosUnk MRT 20.45Table 8: Comparison with previous work on English-German translation.
The BLEU scores are case-sensitive.1690performs MLE and achieves comparable resultswith state-of-the-art systems even though Luonget al (2015a) used a much deeper neural network.We believe that our work can be applied to theirarchitecture easily.Despite these significant improvements, themargins on English-German and English-Frenchdatasets are much smaller than Chinese-English.We conjecture that there are two possible rea-sons.
First, the Chinese-English datasets containfour reference translations for each sentence whileboth English-French and English-German datasetsonly have single references.
Second, Chinese andEnglish are more distantly related than English,French and German and thus benefit more fromMRT that incorporates evaluation metrics into op-timization to capture structural divergence.5 Related WorkOur work originated from the minimum risk train-ing algorithms in conventional statistical machinetranslation (Och, 2003; Smith and Eisner, 2006;He and Deng, 2012).
Och (2003) describes asmoothed error count to allow calculating gradi-ents, which directly inspires us to use a param-eter ?
to adjust the smoothness of the objectivefunction.
As neural networks are non-linear, ourapproach has to minimize the expected loss onthe sentence level rather than the loss of 1-besttranslations on the corpus level.
Smith and Eis-ner (2006) introduce minimum risk annealing fortraining log-linear models that is capable of grad-ually annealing to focus on the 1-best hypothe-sis.
He et al (2012) apply minimum risk trainingto learning phrase translation probabilities.
Gaoet al (2014) leverage MRT for learning continu-ous phrase representations for statistical machinetranslation.
The difference is that they use MRTto optimize a sub-model of SMT while we are in-terested in directly optimizing end-to-end neuraltranslation models.The Mixed Incremental Cross-Entropy Rein-force (MIXER) algorithm (Ranzato et al, 2015)is in spirit closest to our work.
Building onthe REINFORCE algorithm proposed by Williams(1992), MIXER allows incremental learning andthe use of hybrid loss function that combines bothREINFORCE and cross-entropy.
The major dif-ference is that Ranzato et al (2015) leverage rein-forcement learning while our work resorts to mini-mum risk training.
In addition, MIXER only sam-ples one candidate to calculate reinforcement re-ward while MRT generates multiple samples tocalculate the expected risk.
Figure 2 indicates thatmultiple samples potentially increases MRT?s ca-pability of discriminating between diverse candi-dates and thus benefit translation quality.
Our ex-periments confirm their finding that taking evalu-ation metrics into account when optimizing modelparameters does help to improve sentence-leveltext generation.More recently, our approach has been suc-cessfully applied to summarization (Ayana et al,2016).
They optimize neural networks for head-line generation with respect to ROUGE (Lin,2004) and also achieve significant improvements,confirming the effectiveness and applicability ofour approach.6 ConclusionIn this paper, we have presented a framework forminimum risk training in end-to-end neural ma-chine translation.
The basic idea is to minimizethe expected loss in terms of evaluation metricson the training data.
We sample the full searchspace to approximate the posterior distribution toimprove efficiency.
Experiments show that MRTleads to significant improvements over maximumlikelihood estimation for neural machine trans-lation, especially for distantly-related languagessuch as Chinese and English.In the future, we plan to test our approach onmore language pairs and more end-to-end neuralMT systems.
It is also interesting to extend mini-mum risk training to minimum risk annealing fol-lowing Smith and Eisner (2006).
As our approachis transparent to loss functions and architectures,we believe that it will also benefit more end-to-endneural architectures for other NLP tasks.AcknowledgmentsThis work was done while Shiqi Shen and YongCheng were visiting Baidu.
Maosong Sun andHua Wu are supported by the 973 Program(2014CB340501 & 2014CB34505).
Yang Liu issupported by the National Natural Science Foun-dation of China (No.61522204 and No.61432013)and the 863 Program (2015AA011808).
This re-search is also supported by the Singapore NationalResearch Foundation under its International Re-search Centre@Singapore Funding Initiative andadministered by the IDM Programme.1691ReferencesAyana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun.2016.
Neural headline generation with minimumrisk training.
arXiv:1604.01904.Dzmitry Bahdanau, KyungHyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
In Proceed-ings of ICLR.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguisitics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder-decoderfor statistical machine translation.
In Proceedingsof EMNLP.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of HLT.Jianfeng Gao, Xiaodong He, Wen tao Yih, and Li Deng.2014.
Learning continuous phrase representationsfor translation modeling.
In Proceedings of ACL.Caglar Gulcehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
Onusing monolingual corpora in neural machine trans-lation.
arXiv:1503.03535.Xiaodong He and Li Deng.
2012.
Maximum expectedbleu training of phrase and lexicon translation mod-els.
In Proceedings of ACL.Sebastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015.
On using very large tar-get vocabulary for neural machine translation.
InProceedings of ACL.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedingsof EMNLP.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of EMNLP.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT-NAACL.Alon Lavie and Michael Denkowski.
2009.
Themereor metric for automatic evaluation of machinetranslation.
Machine Translation.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proceedings of ACL.Minh-Thang Luong, Hieu Pham, and Christopher DManning.
2015a.
Effective approaches to attention-based neural machine translation.
In Proceedings ofEMNLP.Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, OriolVinyals, and Wojciech Zaremba.
2015b.
Address-ing the rare word problem in neural machine trans-lation.
In Proceedings of ACL.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL.Marc?Aurelio Ranzato, Sumit Chopra, Michael Auli,and Wojciech Zaremba.
2015.
Sequencelevel training with recurrent neural networks.arXiv:1511.06732v1.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015.
Improving neural machine translation modelswith monolingual data.
arXiv:1511.06709.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Pro-ceedings of ACL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA.Andreas Stolcke.
2002.
Srilm - am extensible lan-guage modeling toolkit.
In Proceedings of ICSLP.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of NIPS.Ronald J. Willams.
1992.
Simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
Machine Learning.1692
