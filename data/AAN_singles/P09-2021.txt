Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 81?84,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPRealistic Grammar Error Simulation using Markov LogicSungjin LeePohang University of Science andTechnologyPohang, Koreajunion@postech.ac.krGary Geunbae LeePohang University of Science andTechnologyPohang, Koreagblee@postech.ac.krAbstractThe development of Dialog-Based Computer-Assisted Language Learning (DB-CALL) sys-tems requires research on the simulation oflanguage learners.
This paper presents a newmethod for generation of grammar errors, animportant part of the language learner simula-tor.
Realistic errors are generated via MarkovLogic, which provides an effective way tomerge a statistical approach with expert know-ledge about the grammar error characteristicsof language learners.
Results suggest that thedistribution of simulated grammar errors gen-erated by the proposed model is similar to thatof real learners.
Human judges also gave con-sistently close judgments on the quality of thereal and simulated grammar errors.1 IntroductionSecond Language Acquisition (SLA) researchershave claimed that feedback provided during con-versational interaction facilitates the acquisitionprocess.
Thus, interest in developing Dialog-Based Computer Assisted Language Learning(DB-CALL) systems is rapidly increasing.
How-ever, developing DB-CALL systems takes a longtime and entails a high cost in collecting learnerV?data.
Also, evaluating the systems is not a trivialtask because it requires numerous languagelearners with a wide range of proficiency levelsas subjects.While previous studies have considered usersimulation in the development and evaluation ofspoken dialog systems (Schatzmann et al, 2006),they have not yet simulated grammar errors be-cause those systems were assumed to be used bynative speakers, who normally produce fewgrammar errors in utterances.
However, as tele-phone-based information access systems becomemore commonly available to the general public,the inability to deal with non-native speakers isbecoming a serious limitation since, at least forsome applications, (e.g.
tourist information, le-gal/social advice) non-native speakers representa significant portion of the everyday user popula-tion.
Thus, (Raux and Eskenazi, 2004) conducteda study on adaptation of spoken dialog systemsto non-native users.
In particular, DB-CALL sys-tems should obviously deal with grammar errorsbecause language learners naturally commit nu-merous grammar errors.
Thus grammar error si-mulation should be embedded in the user simula-tion for the development and evaluation of suchsystems.In Foster?s (2007) pioneering work, she de-scribed a procedure which automatically intro-duces frequently occurring grammatical errorsinto sentences to make ungrammatical trainingdata for a robust parser.
However the algorithmcannot be directly applied to grammar error gen-eration for language learner simulation for sever-al reasons.
First, it either introduces one error persentence or none, regardless of how many wordsof the sentence are likely to generate errors.Second, it determines which type of error it willcreate only by relying on the relative frequenciesof error types and their relevant parts of speech.This, however, can result in unrealistic errors.
Asexemplified in Table 1, when the algorithm triesto create an error by deleting a word, it wouldprobably omit the word ?go?
because verb is oneof the most frequent parts of speech omitted re-sulting in an unrealistic error like the first simu-lated output.
However, Korean/Japanese lan-guage learners of English tend to make subject-verb agreement errors, omission errors of thepreposition of prepositional verbs, and omissionerrors of articles because their first languagedoes not have similar grammar rules so that theymay be slow on the uptake of such constructs.Thus, they often commit errors like the secondsimulated output.81This paper develops an approach to statisticalgrammar error simulation that can incorporatethis type of knowledge about language learners?error characteristics and shows that it does in-deed result in realistic grammar errors.
The ap-proach is based on Markov logic, a representa-tion language that combines probabilistic graphi-cal models and first-order logic (Richardson andDomingos, 2006).
Markov logic enables concisespecification of very complex models.
Efficientopen-source Markov logic learning and inferencealgorithms were used to implement our solution.We begin by describing the overall process ofgrammar error simulation and then briefly re-viewing the necessary background in Markovlogic.
We then describe our Markov Logic Net-work (MLN) for grammar error simulation.
Fi-nally, we present our experiments and results.2 Overall process of grammar error si-mulationThe task of grammar error simulation is to gen-erate an ill-formed sentence when given a well-formed input sentence.
The generation procedureinvolves three steps: 1) Generating probabilityover error types for each word of the well-formed input sentence through MLN inference 2)Determining an error type by sampling the gen-erated probability for each word 3) Creating anill-formed output sentence by realizing the cho-sen error types (Figure 1).3 Markov LogicMarkov logic is a probabilistic extension of finitefirst-order logic (Richardson and Domingos,2006).
An MLN is a set of weighted first-orderclauses.
Together with a set of constants, it de-fines a Markov network with one node perground atom and one feature per ground clause.The weight of a feature is the weight of the first-order clause that originated it.
The probability ofa state x in such a network is given by ?(?)
=(1/?)
???
(?
??
??
(?)?
), where ?
is a normali-zation constant, ??
is the weight of the ?th clause, ??
=  1 if the ?th clause is true, and ??
=  0 oth-erwise.Markov logic makes it possible to compactlyspecify probability distributions over complexrelational domains.
We used the learning andinference algorithms provided in the open-sourceAlchemy package (Kok et al, 2006).
In particu-lar, we performed inference using the beliefpropagation algorithm (Pearl, 1988), and genera-tive weight learning.4 An MLN for Grammar Error Simula-tionThis section presents our MLN implementationwhich consists of three components: 1) Basicformulas based on parts of speech, which arecomparable to Foster?s method 2) Analytic for-mulas drawn from expert knowledge obtained byerror analysis on a learner corpus 3) Error limit-ing formulas that penalize statistical model?sover-generation of nonsense errors.4.1 Basic formulasError patterns obtained by error analysis, whichmight capture a lack or an over-generalization ofknowledge of a particular construction, cannotexplain every error that learners commit.
Be-cause an error can take the form of a perfor-mance slip which can randomly occur due tocarelessness or tiredness, more general formulasare needed as a default case.
The basic formulasare represented by the simple rule:y ???????
?, ?, +???
?
?)????????
?, ?, +??
)where all free variables are implicitly universallyquantified.
The ?+?
?, +???
notation signifiesthat the MLN contains an instance of this rule foreach (part of speech, error type) pair.
The evi-Input sentenceHe wants to go to a movie theaterUnrealistic simulated outputHe wants to to a movie theaterRealistic simulated outputHe want go to movie theaterTable 1: Examples of simulated outputsFigure 1: An example process of grammar error simulation82dence predicate in this case is ??)????
?, ?, ??
),which is true iff the ?th position of the sentence ?has the part of speech ??.
The query predicate is ?)????????
?, ?, ??).
It is true iff the ?th positionof the sentence ?
has the error type ?
?, and infer-ring it returns the probability that the word atposition ?
would commit an error of type ?
?.4.2 Analytic formulasOn top of the basic formulas, analytic formulasadd concrete knowledge of realistic error charac-teristics of language learners.
Error analysis andlinguistic differences between the first languageand the second language can identify variouserror sources for each error type.
We roughlycategorize the error sources into three groups forexplanation: 1) Over-generalization of the rulesof the second language 2) Lack of knowledge ofsome rules of the second language 3) Applyingrules and forms of the first language into thesecond language.Often, English learners commit pluralizationerror with irregular nouns.
This is because theyover-generalize the pluralization rule, i.e.
attach-ing ?s/es?, so that they apply the rule even to ir-regular  nouns such  as ?fish?
and ?feet?
etc.
Thischaracteristic is captured by the simple formula:y ????????????????????
?, ??
?
???????
?, ?, ?????
?)????????
?, ?, ?_?????_?
)where ????????????????????
?, ??
is true iff the ?th word of the sentence ?
is an irregular pluraland N_NUM_SUB is the abbreviation for substi-tution by noun number error.One trivial error caused by a lack of know-ledge of the second language is using the singu-lar noun form for weekly events:y ?????
?, ?
?
1, ???
?
????????
?, ???
???????
?, ?, ?????
?)????????
?, ?, ?_?????_?
)where ?????
?, ?
?
1, ???
is true iff the ?
?
1thword is ?on?
and ????????
?, ??
is true iff the ?th word of the sentence ?
is a noun describingday like Sunday(s).
Another example is use ofplurals behind ?every?
due to the ignorance that anoun modified by ?every?
should be singular:y ?????
?, ?
?, ??????
?
??????????????
?, ?
?, ????
?)????????
?, ?
?, ?_?????_?
)where ??????????????
?, ?
?, ???
is true iff the ?
?th word is the determiner of the ?
?th word.An example of errors by applying the rules ofthe first language is that Korean/Japanese oftenallows omission of the subject of a sentence; thus,they easily commit the subject omission error.The following formula is for the case:y ????????
?, ???
?)????????
?, ?,?_???_???
)where ????????
?, ??
is true iff the ?th word is thesubject and N_LXC_DEL is the abbreviation fordeletion by noun lexis error.14.3 Error limiting formulasA number of elementary formulas explicitlystated as hard formulas prevent the MLN fromgenerating improbable errors that might resultfrom over-generations of the statistical model.For example, a verb complement error should nothave a probability at the words that are not com-plements of a verb:y !
???????????????
?, ?
?, ????
!
?)????????
?, ?
?, ?_???_???
).where ?!?
denotes logically ?not?
and ?.?
at theend signifies that it is a hard formula.
Hard formu-las are given maximum weight during inference.
???????????????
?, ?
?, ???
is true iff the ??
thword is a complement of the verb at the ?
?th po-sition and V_CMP_SUB is the abbreviation forsubstitution by verb complement error.5 ExperimentsExperiments used the NICT JLE Corpus, whichis speech samples from an English oral profi-ciency interview test, the ACTFL-ALC StandardSpeaking Test (SST).
167 of the files are errorannotated.
The error tagset consists of 47 tagsthat are described in Izumi (2005).
We appendedstructural type of errors (substitution, addition,deletion) to the original error types becausestructural type should be determined when creat-ing an error.
For example, V_TNS_SUB consistsof the original error type V_TNS (verb tense) andstructural type SUB (substitution).
Level-specific language learner simulation was accom-plished by dividing the 167 error annotated filesinto 3 level groups: Beginner(level1-4), Interme-diate(level5-6), Advanced(level7-9).The grammar error simulation was comparedwith real learnerV?
errors and the baseline modelusing only basic formulas comparable to Foster?salgorithm, with 10-fold cross validations per-formed for each group.
The validation resultswere added together across the rounds to com-pare the number of simulated errors with thenumber of real errors.
Error types that occurredless than 20 times were excluded to improve re-liability.
Result graphs suggest that the distribu-tion of simulated grammar errors generated bythe proposed model using all formulas is similarto that of real learners for all level groups and the1Because space is limited, all formulas can be found athttp://isoft.postech.ac.kr/ges/grm_err_sim.mln83proposed model outperforms the baseline modelusing only the basic formulas.
The Kullback-Leibler divergences, a measure of the differencebetween two probability distributions, were alsomeasured for quantitative comparison.
For alllevel groups, the Kullback-Leibler divergence ofthe proposed model from the real is less than thatof the baseline model (Figure 2).Two human judges verified the overall realismof the simulated errors.
They evaluated 100 ran-domly chosen sentences consisting of 50 sen-tences each from the real and simulated data.
Thesequence of the test sentences was mixed so thatthe human judges did not know whether thesource of the sentence was real or simulated.They evaluated sentences with a two-level scale(0: Unrealistic, 1: Realistic).
The result showsthat the inter evaluator agreement (kappa) ismoderate and that both judges gave relativelyclose judgments on the quality of the real andsimulated data (Table 2).6 Summary and Future WorkThis paper introduced a somewhat new researchtopic, grammar error simulation.
Expert know-ledge of error characteristics was imported tostatistical modeling using Markov logic, whichprovides a theoretically sound way of encodingknowledge into probabilistic first order logic.Results indicate that our method can make anerror distribution more similar to the real errordistribution than the baseline and that the qualityof simulated sentences is relatively close to thatof real sentences in the judgment of human eva-luators.
Our future work includes adding moreexpert knowledge through error analysis to in-crementally improve the performance.
Further-more, actual development and evaluation of aDB-CALL system will be arranged so that wemay investigate how much the cost of collectingdata and evaluation would be reduced by usinglanguage learner simulation.AcknowledgementThis research was supported by the MKE (Ministry ofKnowledge Economy), Korea, under the ITRC (In-formation Technology Research Center) support pro-gram supervised by the IITA (Institute for Informa-tion Technology Advancement) (IITA-2009-C1090-0902-0045).ReferencesFoster, J.
2007.
Treebanks Gone Bad: Parser evalua-tion and retraining using a treebank of ungrammat-ical sentences.
IJDAR, 10(3-4), 129-145.Izumi, E et al 2005.
Error Annotation for Corpus ofJapanese Learner English.
In Proc.
InternationalWorkshop on Linguistically Interpreted CorporaKok, S. et al 2006.
The Alchemy system for statistic-al relational AI.
http://alchemy.cs.washington.edu/.Pearl, J.
1988.
Probabilistic Reasoning in IntelligentSystems Morgan Kaufmann.Raux, A. and Eskenazi, M. 2004.
Non-Native Users inthe Let's Go!!
Spoken Dialogue System: Dealingwith Linguistic Mismatch, HLT/NAACL.Richardson, M. and Domingos, P. 2006.
Markov logicnetworks.
Machine Learning, 62(1):107-136.Schatzmann, J. et al 2006.
A survey of statistical usersimulation techniques for reinforcement-learningof dialogue management strategies, The Know-ledge Engineering ReviewVol?Advanced Level:DKL (Real || Proposed)=0.068, DKL (Real || Baseline)=0.122Intermediate Level:DKL (Real || Proposed)=0.075, DKL (Real || Baseline)=0.142Beginner Level:DKL (Real || Proposed)=0.075, DKL (Real || Baseline)=0.092Figure 2: Comparison between the distributions of thereal and simulated dataHuman 1 Human 2 Average KappaReal 0.84 0.8 0.82 0.46Simulated 0.8 0.8 0.8 0.5Table 2: Human evaluation results84
