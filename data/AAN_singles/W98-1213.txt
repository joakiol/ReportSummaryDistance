IIIIIIIIIIIIIIIIIAutomatically generating hypertext in newspaper articles by computingsemantic relatednessStephen J. GreenMicrosoft  Research InstituteSchool  o f  Mathematics,  Physics,  Comput ing  and Electronics*Macquar ie  Univers i tySydney, NSW 2109Austral iasj green@mri .mq.edu.auAbstractWe discuss an automatic method for the construction ofhypertext links within and between ewspaper articles.The method comprises three steps: determining the lexicalchains in a text, building links between the paragraphs ofarticles, and building links between articles.
Lexical chainscapture the semantic relations between words that occurthroughout a text.
Each chain is a set of related words thatcaptures a portion of the cohesive structure of a text.
Byconsidering the distribution of chains within an article, wecan build links between the paragraphs.
By computing thesimilarity of the chains contained in two different articles,we can decide whether or not to place a link between them.We also describe the results of an evaluation performed totest he methodology.1 IntroductionA survey, reported in Outing (1996), found that therewere 1,115 commercial newspaper online services world-wide, 94% of which were on the World-Wide Web(WWW).
Of these online newspapers, 73% are in NorthAmerica.
Outing predicted that the number of newspa-pers online would increase to more than 2,000 by the endof 1997.The problem is that these services are not making fulluse of the hypertext capabilities of the WWW.
The usermay be able to navigate to a particular article in the cur-rent edition of  an online paper by using hypertext links,but they must then read the entire article to find the in-formation that interests them.
These databases are "shal-low" hypertexts; the documents hat are being retrievedare dead ends in the hypertext, rather than offering start-ing points for explorations.
In order to truly reflect hehypertext nature of the Web, links should to be placedwithin and between the documents.As Westland (1991) has pointed out, manually creat-ing and maintaining the sets of links needed for a large-scale hypertext is prohibitively expensive.
This is espe-cially true for newspapers, given the volume of articlesWork done at the Department of Computer Science of the Univer-sity of Torontoproduced every day.
This could certainly account for thestate of current WWW newspaper fforts.
Aside from thetime-and-money aspects of building such large hypertextsmanually, humans are inconsistent inassigning hypertextlinks between the paragraphs of documents (Ellis et al,1994; Green, 1997).
That is, different linkers disagreewith each other as to where to insert hypertext links intoa document.The cost and inconsistency of manually constructedhypertexts does not necessarily mean that large-scale hy-pertexts can never be built.
It is well known in the IRcommunity that humans are inconsistent in assigning in-dex terms to documents, but this has not hindered theconstruction of automatic indexing systems intended tobe used for very large collections of documents.
Simi-larly, we can turn to automatically constructed hypertextsto address the issues of cost and inconsistency.In this paper, we will describe a novel method forbuilding hypertext links within and between ewspaperarticles.
We have selected newspaper articles for twomain reasons.
First, as we stated above, there is a grow-ing number of services devoted to providing this informa-tion in a hypertext environment.
Second, many newspa-per articles have a standard structure that we can exploitin building hypertext links.Most of the proposed methods for automatic hypertextconstruction rely on term repetition.
The underlying phi-losophy of these systems is that texts that are related willtend to use the same terms.
Our system is based on lexi-cal chaining and the philosophy that texts that are relatedwill tend to use related terms.2 Lexical chainsA lexical chain (Morris and Hirst, 1991) is a sequence ofsemantically related words in a text.
For example, ifa textcontained the words apple and fruit, they would appear ina chain together, since apple is a kind of fruit.
Each wordin a text may appear in only one chain, but a documentwill contain many chains, each of which captures a por-tion of the cohesive structure of the document.
CohesionGreen 101 Automatically generating hypertextStephen J.
Green (1998) Automatically generating hypertext in newspaper articles by computing semantic relatedness.
InD.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Languageis what, as Halliday and Hasan (1976) put it, helps a text"hang together as a whole".
The lexical chains containedin a text will tend to delineate the parts of the text that are"about" the same thing.
Morris and Hirst showed that theorganization of  the lexical chains in a document mirrors,in some sense, the discourse structure of that document.The lexical chains in a text can be identified using anylexical resource that relates words by their meaning.
Ourcurrent lexical chainer (based on the one described by St-Onge, 1995) uses the WordNet database (Beckwith et al,199 I).
The WordNet database is composed of  synonymsets or synsets.
Each synset contains one or more wordsthat have the same meaning.
A word may appear in manysynsets, depending on the number of senses that it has.Synsets can be connected to each other by several dif-ferent ypes of  links that indicate different relations.
Forexample, two synsets can be connected by a hypernymlink, which indicates that the words in the source synsetare instances of  the words in the target synset.For the purposes of chaining, each type of link betweenWordNet synsets is assigned a direction of up, down, orhorizontal.
Upward links correspond to generalization:for example, an upward link from apple to fruit indicatesthat fruit is more general than apple.
Downward linkscorrespond to specialization: for example, a link fromfruit to apple would have a downward irection.
Hori-zontal inks are very specific specializations.
For exam-ple, the antonymy relation in WordNet is given a direc-tion of  horizontal, since it specializes the sense of a wordvery accurately, that is, if a word and its antonym appearin a text, the two words are very likely being used in thesenses that are antonyms.Given these types of links, three kinds of relations arebuilt between words:Extra strong An exwa strong relation is said to exist be-tween repetitions of the same word: i.e., term repe-tition.Strong A strong relation is said to exist between wordsthat are in the same WordNet synset (i.e., words thatare synonymous).
Strong relations are also said toexist between words that have synsets connected bya single horizontal link or words that have synsetsconnected by a single IS-A or INCLUDES relation.Regular A regular elation is said" to exist between twowords when there is at least one allowable pathbetween a synset containing the first word and asynset containing the second word in the WordNetdatabase.
A path is allowable if it is short (less thann links, where n is typically 3 or 4) and adheres tothree rules:1.
No other direction may precede an upwardlink.2.
No more than one change of direction is al-lowed.3.
A horizontal link may be used to move froman upward to a downward irection.When a word is processed uring chaining, it is ini-tially associated with all of the synsets of which it is amember.
When the word is added to a chain, the chainerattempts to find connections between the synsets associ-ated with the new word and the synsets associated withwords that are already in the chain.
Synsets that canbe connected are retained and all others are discarded.The result of this processing is that, as the chains arebuilt, the words in the chains are progressively sense-disambiguated.
When an article has been chained, a de-scription of the chains contained in the document is writ-ten to a file.
Table 1 shows some of the chains that wererecovered from an article about he trend towards "virtualparenting" (Shellenbarger, 1995).
In this table, the num-bers in parentheses show the number of occurrences of  aparticular word.The process of  lexical chaining is not perfect, but ifwe wish to process articles quickly, then we must ac-cept some errors or at least bad decisions.
In our sam-ple article, for example, chain 1 is a conglomeration fwords that would have better been separated into differ-ent chains.
This is a side effect of the current implemen-tation of the lexical chainer, but even with these difficul-ties, we are able to perform useful tasks.
We expect oaddress ome of these problems in subsequent versionsof the chainer, hopefully with no loss in efficiency.3 Building links within an article3.1 Analyzing the iexicai chainsNewspaper articles are written so that one may stop read-ing at the end of any paragraph and feel as though onehas read a complete unit.
For this reason, it is natural tochoose to use paragraphs as the nodes in our hypertext.Table 1 showed the lexical chains recovered from a newsarticle about he trend towards "virtual parenting".
Figure1 shows the second and eighth paragraphs of this articlewith the words that participate in lexical chains taggedwith their chain numbers.
We will use this particular arti-cle to illustrate the process of building intra-article links.The first step in the process is to determine how im-portant each chain is to each paragraph in an article.
Wejudge the importance of a chain by calculating the frac-tion of the content words of the paragraph t at are in thatchain.
We refer to this fraction as the density of that chainin that paragraph.
The density of chain c in paragraph p,dc,p, is defined as:dc,p ~ Wc,pwpGreen 102 Automatically generating hypertextIIIIIIIIIIIIIIIIIIIII///l///////////IIlTable I:Word Synworking (5) 40755ground (I) 58279field (1) 57992antarctica (I) 58519michigan (I) 57513feed (I) 53429chain (I) 57822hazard (1) 77281risk ( 1 ) 77281young (2) 24623need (1) 58548parent (7) 62334kid (3) 60256child (1) 60256baby (1) 59820wife (1) 63852adult (I) 59073traveller (3) 59140substitute (1) 63327backup (1) 63327computer(l) 60118Some lexical chains from the virtual parenting article.410C Wordexpert(I)mark ( 1 )worker (I)speaker (1)advertiser (I)entrepreneur (I)engineer (1)sitter (I)consultant (2)management_consultant ( I )man (1)flight_attendant (I)folk (1)family (4)management (2)professor (i)conference (1)meeting (I)school (I)university (I)company (!
)Sya59108602705914563258596436088959101598275964461903619O263356543625436255578626385537255371552615529954918C Word12 giving (I)pushing (!
)push (1)high-tech (2)19 planning (1)arranging ( 1 )21 good_night(l)wish (l)22 phone (2)cellular.phone (I)fax (2)gear (1)joint (2)junction (1)network (I)system (2)audiotape (1)gadget (I)23 feel (I)kissing (I)Syn19911200012000 I1995723089231274807448061400173380835302320303657436604372473219639983324282280822806Although no one is pushing 12 virtual-reality headgear 16 as a substitute I for parents I, many Itechnical ad campaigns 13 are promoting cellular phones ~,  faxes ~ , computers I and pagers to"l working I parents !
as a way of  bridging separations 17 from their kids I .
A recent promotion 13 by A T & T and Residence 2 Inns 7 in the United States 6, for example 3, suggests that business 3 travellers I with young j children use video 3 and audio tapes ~,  voice 3 mail 3, videophones and E-mail to stay 3 connected, including kissing ~ the kids I good night 21 by phone 22.More advice 3from advertisers t: Business  travellers I can dine with their kids t by speakerLphone or "tuck them in" by cordless phone z2.
Separately, a management  I0 newsletter 24 rec-ommends faxing your child I when you have to break 17 a promise 3 to be home 2 or giving 12 ayoung I child I a beeper to make him feel ~ more secure when left "s alone.F igure 1: Two port ions o f  a text tagged wi th  chain numbers .where wc,p is the number of  words from chain c thatappear in paragraph p and w v is the number of contentwords (i.e., words that are not stop words) in p. For ex-ample, if we consider paragraph two of our sample arti-cle, we see that there are 9 words from chain 1.
We alsonote that there are 48 content words in the paragraph.
So,in this case the density of chain 1 in paragraph I, dr,z, is9 4-g = 0.19.The result of these calculations i that each paragraphin the article has associated with it a vector of chain den-sities, with an element for each of the chains in the article.Table 2 shows these chain density vectors for the chainsshown in table I.
Note that an empty element indicates adensity of 0.3.2 Determining paragraph linksAs we said earlier, the parts of a document that are aboutthe same thing, and therefore related, will tend to containthe same lexical chains.
Given the chain density vectorsthat we described above, we need to develop amethod todetermine the similarity of  the sets of chains contained ineach paragraph.
The second stage of paragraph linking,therefore, is to compute the similarity between the para-graphs of the article by computing the similarity betweenthe chain density vectors representing them.
We can com-pute these similarities using any one of 16 similarity co-efficients that we have taken from Ellis et al (1994).This similarity is computed for each pair of chain den-sity vectors, giving us a symmetric p x p matrix of simi-laritie s, where p is the number of paragraphs in the arti-cle.
From this matrix we can calculate the mean and thestandard eviation of the paragraph similarities.The next step is to decide which paragraphs should belinked, on the basis of  the similarities computed in theprevious tep.
We make this decision by looking at howthe similarity of two paragraphs compares to the meanparagraph similarity across the entire article.
Each sim-ilarity between two paragraphs i and j, si,j, is convertedGreen 103 Automatically generating hypertextTable 2: Some chain density vectors for the virtual parenting article.Chain1410121921222310.140.07Chain Words 8Content 14Density 0.57Paragraph2 3 4 5 6 7 8 / 9 100.19 0.07 0.16 0.28 0.18 0.10 0.25 \[ 0.24 0.130.11 0.05 0.03 0.030.07 0.05 0.11 0.04 0.030.02 0.04 0.05 0.04 0.030.04 0.06I10.330.02 0.050.08 0.04 0.05 0.I1 0.07 0.07 0.08 0.030.02 0.0430 15 15 10 15 16 19 20 15 648 27 19 18 28 29 28 38 30 90.62 0.56 0.79 0.56 0.54 0.55 0.68 0.53 0.5'0 0.67Table 3: Adjacency matrix for the virtual parenting arti-cle.Par12345678910111 2 3 4 5 6 7 8 9 10 110 0 0 0 0 0 0 0 0 0 00 0 0 1 0 0 I I 1 00 0 0 0 0 0 0 0 00 0 0 0 0 0 0 00 0 0 1 I 0 00 0 0 0 1 00 0 0 1 00 1 0 00 0 I0 00to a z-score, zi,j.
If  two paragraphs are more similar thana threshold given in terms of a number of standard e-viations, then a link is placed between them.
The resultis a symmetric adjacency matrix where a 1 indicates thata link should be placed between two paragraphs.
Figure3 shows the adjacency matrix that is produced when a z-score threshold of 1.0 is used to compute the links for ourvirtual parenting example.Once we have decided which paragraphs should belinked, we need to be able to produce a representationof the hypertext that can be used for browsing.
In thecurrent system, there are two ways to output he HTMLrepresentation f an article.
The first simply displays allof the links that were computed uring the last stage ofthe process described above.
The second is more compli-cated, showing only some of the links.
The idea is thatlinks between physically adjacent paragraphs should beomitted so that they do not clutter the hypertext.4 Building links between articlesWhile it is useful to be able to build links within articles,for a large scale hypertext, links also need to be placedbetween articles.
You will recall from section 2 that theoutput of the lexical chainer is a list of chains, each chainconsisting of one or more words.
Each word in a chainhas associated with it one or more synsets.
These synsetsindicate the sense of the word as it is being used in thischain.
An example of the kind of output produced bythe ehainer is shown in table 4, which shows a portion ofthe chains extracted from an article (Gadd, 1995b) aboutcuts in staffat children's aid societies due to a reductionin provincial grants.
Table 5 shows a portion of  anotherset of chains, this time from an article (Gadd, 1995a) de-scribing the changes in child-protection agencies, due inpart to budget cuts.It seems quite clear that these two articles are related,and that we would like to place a link from one to theother.
It is also clear that the words in these two articlesdisplay both of  the linguistic factors that affect IR per-formance, namely synonymy and polysemy.
For exam-ple, the first set of  chains contains the word abuse, whilethe second set contains the synonym altreatment.
Sim-ilarly, the first set of chains includes the word kid, whilethe second contains child.
The word abuse in the first ar-ticle has been disambiguated by the lexieal chainer intothe "cruel or inhuman treatment" sense, as has the wordmaltreatment from the second article.
We once again notethat the lexieal chaining process is not perfect: for exam-ple, both texts contain the word abuse, but it has beend.isambiguated into different senses--  in the first article,it is meant in the sense of "ill-treatment", while in thesecond it is meant in the sense of "verbal abuse".Although the articles share a large number of  words,by missing the synonyms or by making incorrect (or no)judgments about different senses, atraditional IR systemmight miss the relation between these documents or rankthem as less related than they really are.
Aside from theproblems of synonymy and polysemy, we can see thatthere are also more-distant relations between the words ofthese two articles.
For example, the second set of  chainsGreen 104 Automatically generating hypertextII1Ii11l1II1IIIIIiIIIIl//IIlIII/III//ITable 4: Some lexical chains from an articlWord Syn C Wordsociety (7) 54351 annual (1)group (I) 19698 5 ontario (I)mother(l) 62088 canadian (I)parent (4) 62334kid (1) 60256 burlington (1)recruit (!)
62769 union (3)employee (2) 60862 10 saying (1)worker (2) 59145 interview (2)computer(l) 60118 27 try(1)teen-ager (2) 59638 seeking (1)provincial (3) 62386 acting (1)face (I) 59111 services (I)spokesman (I) 63287 work (3)insolvent (I) 59869 risk (2)about cuts in children's aid societies.Syn C Word Syn'64656 care (I) 2220456918 social_work (l) 2418058424 slowdown (1) 2364059296 abuse (3) 2121457612 child..abuse ( l ) 2121557424 neglect ( 1 ) 2123550294 28 living (I) 7562~50268 standing (I) 7557322561 complaint ( I ) 7627022571 agency (I) 7578621759 stress (1) 7679921922 7690621919 32 executive_director (2) 6092222613 manager (l) 59634Table 5: Some lexical chains fromWord Syn C Wordwit (I) 48647 guardian (1)play ( I ) 48668 official (I)abuse (4) 48430 worker (1)cut (4) 48431 neighbour (1)criticism (1) 48406 youngster (1)recommendation (I) 48310 kid (2)case (1) 48682 natural (1)problem (I) 48680 lawyer (2)question (3) 48679 professional (I)child ( 1 O) 60256 prostitute ( 1)parent (9) 62334 provincial (2)mother (3) 62088 welfare_worker (1)daughter (1) 60587 lorelei (1)foster.home (I) 54374 god (I)society (5) 54351 4 protection (2)at_home (i) 55170 care (5)social (1) 55184 preservation (2)function (1) 55154 judgment (I)expert (3) 59108 act (1)human (1) 19677 behaviour (I)related article.Syn5909962223591456215260255602556213961725626366266062386632206183358615226722272122676228811969724235C Wordmaking (1)calling (I)services (2)prevention (l)supply (1)providing (3)maltrea~'nent (2)child.abuse (2)investigation (I)research (I)investigating ( 1 )work (1)aid (9)social.work ( 1 )risk (1)dispute (1)intervention (1)fail (1)Syn24236230762191121922236832359623596212142121522142221432214221885222042418022613240512431719811contains the word maltreatment while the first set con-tains the related word child abuse (a kind of maltreat-ment) as well as the repetition of child abuse.We can build these inter-article links by determiningthe similarity of the two sets of chains contained in twoarticles.
In essence, we wish to perform a kind of cross-document chaining.4.1 Synset weight vectorsWe can represent each document in a database by twovectors.
Each vector will have an element for each synsetin WordNet.
An element in the first vector will containa weight based on the number of occurrences of that par-ticular synset in the words of the chains contained in thedocument.
An element in the second vector will containa weight based on the number of occurrences ofthat par-ticular synset when it is one link away from a synset as-sociated with a word in the chains.
We will call thesevectors the member and linked synset vectors, or simplythe member and linked vectors, respectively.The weight of a particular synset in a particular docu-ment is not based solely on the frequency of that synsetin the document, but also on how frequently that term ap-pears throughout the database.
The synsets that are themost heavily weighted in a document are the ones thatappear frequently in that document but infrequently inthe entire database.
The weights are calculated using thestandard ff-idf weighting function:Wik =- sf ik" log(N/nk)~/Y~= t (sf ij) 2.
(log(N lnj) )2where sfik is the frequency of synset k in document i, Nis the size of the document collection, n, is the numberof documents in the collection that contain synset k, ands is the number of synsets in all documents.
Note thatthis equation incorporates the normalization of the synsetweight vectors.The weights are calculated independently for the mem-ber and linked vectors.
We do this because the linkedvectors introduce a large number of synsets that do notnecessarily appear in the original chains of an article, andshould therefore not influence the frequency counts of themember synsets.
Thus, we make a distinction betweenGreen 105 Automatically generating hypertextstrong links that occur due to synonymy, and strong linksthat occur due to IS-A or INCLUDES relations.
The simi-larity between two documents, DI and/32, is then deter-mined by calculating three cosine similarities:1.
The similarity of the member vectors of DI and/)2;2.
The similarity of the member vector of Dl andlinked vector olD2; and3.
The similarity of the linked vector of Di and themember vector of D2.Clearly, the first similarity measure (the member-member similarity) is the most important, as it will cap-ture extra-strong relations as well as strong relations be-tween synonymous words.
The last two measures (themember-linked similarities) are less important as theycapture strong relations that occur between synsets thatare one link away from each other.
If we enforce athresh-old on these measures of relatedness, then we ensure thatthere are several connections between two articles, sinceeach element of the vectors will contribute only a smallpart of the overall similarity.4.2 Building inter-article finksOnce we have built a set of synset weight vectors for acollection of documents, the process of building links be-tween articles is relatively simple.
Given an article thatwe wish to build links from, we can compute the simi-larity between the article's ymet weight vectors and thevectors of all other documents.
Documents whose mem-ber vectors exceed a given threshold of similarity willhave a link placed between them.
Our preliminary workshows that a threshold of 0.15 will include most relateddocuments while excluding many unrelated ocuments.This is almost exactly the methodology used in vector-space IR systems uch as SMART, with the differencebeing that for each pair of documents we are calculatingthree separate similarity measures.
The best way to copewith these multiple measurements seems to be to rankrelated ocuments by the sum of the three similarities.The sum of the three similarities can lie, theoretically,anywhere between 0 and 3.
In practice, the sum is usuallyless than 1.
For example, the average sum of the threesimilarities when running the vectors of a single articleagainst 5,592 other articles is 0.039.5 EvaluationIn the evaluation that we conducted, the basic questionthat we asked was: Is our hypertext linking methodologysuperior to other methodologies that have been proposed(e.g., that of Allan, 1995)?
The obvious way to answerthe question was to test whether the links generated byour methodology lead to better performance when theywere used in the context of an appropriate IR task.We selected a question-answering task for our study.We made this choice because it appears that this kindof task is well suited to the browsing methodology thathypertext links are meant o support.
This kind of taskis also useful because it can be performed easily usingonly hypertext browsing.
This is necessary because intheinterface used for our experiment, no query engine wasprovided for the subjects.We used the "Narrative" section of three TREC topics(Harman, 1994) to build three questions for our subjectsto answer.
There were approximately 1996 documentsthat were relevant to the topics from which these ques-tions were created.
We read these documents and pre-pared lists of answers for the questions.
Our test databaseconsisted of these articles combined randomly with ap-proximately 29,000 other articles elected randomly fromthe TREC corpus.
The combination of these articles pro-vided us with a database that was large enough for areasonable evaluation and yet small enough to be easilymanageable.5.1 The test systemWe considered two possible methods for generating inter-article hypertext links.
The first is our own method, de-scribed above.
The second method uses a vector space IRsystem called Managing Gigabytes (MG) (Witten et al,1994) to generate links by calculating a document simi-laxity that is based strictly on term repetition.
We used theMG system to generate links in a way very similar to thatpresented in Allan (1995).
For simplicity's sake, we willcall the links generated by our technique HT links and thelinks generated by the MG system MG links.Figure 2 shows the interface of the test system used.The main part of the screen showed the text of a singlearticle.
The subjects could navigate through the articleby using the intra-article links, a scroll bar, or the pageup and down keys.
The Previous Article and Next Articlebuttons could be used for navigating through the set of ar-ticles that had been visited and the Back button returnedthe user to the point from which an intra-article link wastaken.
Each search began on a "starter" page that con-mined the text of the appropriate TREC topic as the "ar-ticle" and the list of articles related to the topic shown(this was computed by using the text of the topic as theinitial "query" to the database).
Subjects were expectedto traverse the links, writing down whatever answers theycould find.At each stage during a subject's browsing, a set ofinter-article inks was generated by combining the set ofI-IT links and the set of MG links.
By using this strat-egy, the subjects "vote" for the system that hey prefer bychoosing the links generated by that system.
Of course,the subjects are not aware of which system generated thelinks that they are following - -  they can only decide toGreen 106 Automatically generating hypertext1111II11II1IIIIII111II1IIIIi lI II II II II II II II II II II II II II II II!I IFile Article HelpPrevious ArticleI Next ArticleBackI Jurno toReloted II Arh'clet=.Here  is  the Headline of the Art ic leHere is a subheadingThe text 0t the arlJcle thal you're viewing goes here.
If you're looking atit and you decide that it's relevant to the query that you're trying toans',tc, r, then you should write down the answer!?
Here is a link that will,.. ?
This is another rink...HeadlineHere is the headline of an article that you can jump to.Try clicking on me to jump to a new article!Figure 2: The interface of the evaluation system.follow a link by considering the article headlines dis-played as anchors.
We can, however, determine whichsystem they "voted" for by considering their success inanswering the questions they were asked.
If we can showthat their success was greater when they followed moreI-IT links, then we can say that they have "voted" for thesuperiority of HT links.
A similar methodology has beenused previously by Nordhausen et al (1991) in their com-parison of human and machine-generated hypertext links.The two sets of inter-article links can be combined bysimply taking the unique links from each set, that is, thelinks that we take are those that appear in only one ofthe sets of links.
Of Course, we would expect he twomethods to have many links in common, but it is diffi-cult to tell how these links should be counted in the "vot-ing" procedure.
By leaving them out, we test the differ-ences between the methods rather than their similarities.Of course, by excluding the links that the methods agreeon we are reducing the ability of the subjects to find an-swers to the questions that we have posed for them.
Infact, we found that nearly 40% of the links found werefound by both methods.
It does seem, however, that theusers could find enough answers to give some interestingresults.5.2 Experimental resultsThe number of both inter- and intra-articte links followedwas, on average, quite small and variable (full data aregiven in Green, 1997).
The number of correct answersfound was also low and variable, which we believe is duepartly to the methodology and partly to the time restric-tions placed on the searches (15 minutes).
On average,the subjects howed a slight bias for HT links, choosing47.9% MG links and 52.1% HT links.
This is interesting,especially in light of the fact that, for all the articles thesubjects visited, 50.4% of the links available were MGlinks, while 49.6% were HT links.
A paired t-test, how-ever indicates that this difference is not significant.For the remainder of the discussion, we will use thevariable LHT tO refer to the number of HT links that asubject followed, LMG to refer to the number of MG linksfollowed, and L/ to refer to the number of intra-articlelinks followed.
The variable Ans will refer to the numberof correct answers that a subject found.
We can combineLHr and LMG into a ratio, LR = ~u-'~G" If LR > 1, then a" W .
M~ .
subject folio ed more HT links than MG hnks.
An inter-esting question to ask is: did subjects with significantlyhigher values for LR find more answers?
With 23 subjectseach answering 3 questions, we have 69 values for LR.
Ifwe sort these values in decreasing order and divide theresulting list at the median, we have two groups with asignificant difference in LR.
An unpaired t-test hen tellsus that the differences in Ans for these two groups aresignificant at the 0.
I level.So it seems that there may be some relationship be-tween the number and kinds of links that a subject fol-lowed and his or her success in finding answers to thequestions pose.
We can explore this relationship usingtwo different regression analyses, one incorporating onlyinter-article links and another incorporating both inter-and intra-article links.
These analyses will express therelationship between the number of links followed andthe number of correct answers found.5.2.1 Inter-article linksA model incorporating only the inter-article links thatour subjects followed gives us the following equation:Green 107 Automatically generating hypertextIIIIilIIIIIIIIIIIIIIIIIIIIIIIIII1614121086I I IA- -  t 44 A?
4 ?- -  ?
AA  4.
4&A~ A ?
?
?4 A ?0 " ' | "  I ,0 1 2I I I, DataAria = 3.65 + 0.56- LR/JI I I I3 4 5 6LRFigure 3: Data and regression line for a two-dimensional model.we can see a set of subjects (the High Web group) whofound significantly more answers and followed signifi-cantly more I-IT links, indicating the advantage of HTlinks over MG links.5.2.4 Viewed answersIn the analyses that we've performed to this point, wehave been using the number of correct answers that thesubjects provided as our dependent variable.
Part of  thereason we are using this dependent variable is that thesubjects were limited in the amount of time that theycould spend on each search, and so they could only find acertain umber of answers, no matter how many answersthere were to find.
We can mitigate this effect by intro-ducing a new dependent variable, Ansv, or the number of.viewed answers.The number of viewed answers for a particular ques-tion is simply the number of answers that were containedin articles that a subject visited while attempting to an-swer a question.
These answers need not have been writ-ten down.
We are merely saying that, given more time,the subjects might have been able to read the article morefully and find these answers.
This idea is analogous to theuse of judged and viewed recall by Golovchinsky (1997)in his studies.When we consider Ansi, as our dependent variable, themodel for the High Web group is still not significant, andthere is still a high probability that the coefficient of L/is 0.
For our Low Web group, who followed signifi-cantly more intra-article links than the High Web group,the model that results is significant and has the followingequation:Ansv = 0.58.L,~r + 0.21 .LMG + 0.21 "L1 (R 2 = 0.41)Table 9:model using viewed answers.Parameter Value tLtcr 0.58 4.37LMG 0.21 1.62L!
0.21 2.1995% confidence intervals for coefficients in ap Low High0.00 0.31 0.850.06 -0.05 0.470.02 0.01 0.40Table 9 shows the 95% confidence intervals for thismodel.
We see that the coefficient of Lt is always pos-itive, indicating some effect on Ansv from intra-articlelinks.
We also see that the probability that this coeffi-cient is 0 is less than 0.02.
We note, however, that forthis model we earmot claim that the coefficient of LHr isalways greater than the coefficient of LMG.
This is nottoo surprising in light of the fact that he High Web groupchose significantly more HT links than did the Low Webgroup.6 Conclusions and future workOur evaluation shows that we cannot reject our null hy-pothesis that here is no difference in the two methods forgenerating inter-article links.
Having said this, we candemonstrate a partition of the subjects uch that the onlysignificant differences between them are the number ofHT links followed and the number of answers found.
Fur-thermore, we determined that he probability of obtainingresults such as these by chance is less than 0.1.
Our in-ability to achieve asignificant result may be due to severalimplementation factors, described in Green (1997).
Thus,we conclude that we need to replicate the experiment inorder to gain further information about the relationshipbetween the two kinds of inter-article links.Green 109 Automatically generating hypertext
