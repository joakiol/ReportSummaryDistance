Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 17?23Manchester, August 2008Toward an Underspecifiable Corpus Annotation SchemeYuka TateisiDepartment of Informatics, Kogakuin University1-24-2 Nishi-shinjuku, Shinjuku-ku, Tokyo, 163-8677, Japanyucca@cc.kogakuin.ac.jpAbstractThe Wall Street Journal corpora providedfor the Workshop on Cross-Frameworkand Cross-Domain Parser EvaluationShared Task are investigated in order tosee how the structures that are difficultfor an annotator of dependency structureare encoded in the different schemes.Non-trivial differences among theschemes are found.
The paper also inves-tigates the possibility of merging the in-formation encoded in the different cor-pora.1 BackgroundThis paper takes a look at several annotationschemes related to dependency parsing, from theviewpoint of a corpus annotator.
The dependencystructure is becoming a common criterion forevaluating parsers in biomedical text mining(Clegg and Shepherd, 2007; Pyssalo et al,2007a), since their purpose in using parsers are toextract predicate-argument relations, which areeasier to access from dependency than constitu-ency structure.
One obstacle in applying depend-ency-based evaluation schemes to parsers forbiomedical texts is the lack of a manually anno-tated corpus that serves as a gold-standard.Aforementioned evaluation works used corporaautomatically converted to the Stanford depend-ency scheme (de Marneffe et al, 2006) fromgold-standard phrase structure trees in the PennTreebank (PTB) (Marcus et al, 1993) format.However, the existence of errors in the automaticconversion procedure, which are not well-?
2008.
Licensed under the Creative Commons Attribu-tion-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.documented, makes the suitability of the result-ing corpus for parser evaluation questionable,especially in comparing PTB-based parsers andparsers based on other formalisms such as CCGand HPSG (Miyao et al, 2007).
To overcome theobstacle, we have manually created a depend-ency-annotated corpus in the biomedical fieldusing the Rasp Grammatical Relations (Briscoe2006) scheme (Tateisi et al, 2008).
In the anno-tation process, we encountered linguistic phe-nomena for which it was difficult to decide theappropriate relations to annotate, and that moti-vated the investigation of the sample corporaprovided for the Workshop on Cross-Frameworkand Cross-Domain Parser Evaluation SharedTask1, in which the same set of sentences takenfrom the Wall Street Journal section from PennTreebank is annotated with different schemes.The process of corpus annotation is assigninga label from a predefined set to a substring of thetext.
One of the major problems in the process isthe annotator's lack of confidence in decidingwhich label should be annotated to the particularsubstring of the text, thus resulting in the incon-sistency of annotation.
The lack of confidenceoriginates from several reasons, but typical situa-tions can be classified into two types:1) The annotator can think of two or moreways to annotate the text, and cannot decidewhich is the best way.
In this case, the annotationscheme has more information than the annotatorhas.
For example, the annotation guideline ofPenn Treebank (Bies et al 1995) lists alterna-tives for annotating structures involving nullconstituents that exist in the Treebank.2) The annotator wants to annotate a certaininformation that cannot be expressed properlywith the current scheme.
This is to say, the anno-tator has more information than the scheme canexpress.1 http://www-tsujii.is.s.u-tokyo.ac.jp/pe08-st/17For example, Tateisi et al(2000) report that, inthe early version of the GENIA corpus,  somecases of inter-annotator discrepancy occur be-cause the class of names to be assigned (e.g.PROTEIN) is too coarse-grained for annotators,and the result led to a finer-graded classification(e.g.
PROTEIN-FAMILY, PROTEIN-COMPLEX) of names in the published versionof GENIA (Kim et al, 2003).In practice, the corpus designers deal withthese problems by deciding how to annotate thequestionable cases, and describing them in theguidelines, often on an example-by-example ba-sis.
Still, these cases are sources of errors whenthe decision described in the guideline is againstthe intuition of the annotator.If the scheme allows the annotator to annotatethe exact amount of information that (s)he has,(s)he would not be uncertain about how to anno-tate the information.
However, because the in-formation that an annotator has varies from anno-tator to annotator it is not practical to define ascheme for each annotator.
Moreover, the result-ing corpus would not be very useful, for a corpusshould describe a "common standard" that isagreed by (almost) everyone.One solution would be to design a scheme thatis as information-rich as possible, in the way thatit can be "underspecified" to the amount of theinformation that an annotator has.
When the cor-pus is published, the annotation can be reducedto the "most-underspecified" level to ensure theuniformity and consistency of annotations, that is,to the level that all the annotators involved canagree (or the corpus can be published as-is withunderspecification left to the user).
For example,annotators may differ in decision about whetherthe POS of "human" in the phrase "human anno-tator" is an NN (common noun) or a JJ (adjec-tive), but everyone would agree that it is not, forexample, a VBN (past participle of a verb).
Inthat case, the word can be annotated with an un-derspecified label like "NN or JJ".
The PennTreebank POS corpus (Santrini, 1990) allowssuch underspecification (NN|JJ).
In the depend-ency structure annotation, Grammatical Relations(Briscoe 2006), for example, allows underspeci-fication of dependency types by defining theclass hierarchy of dependency types.
The under-specified annotation is obviously better than dis-carding the annotation because of inconsistency,for the underspecified annotation have muchmore information than nothing at all, and canassure consistency over the entire corpus.Defining an underspecification has another use.There are corpora in similar but differentschemes, for a certain linguistic aspect (e.g.
syn-tactic structure) based on formalisms suited forthe application that the developers have in mind.That makes the corpus difficult for the use out-side the group involved in the development ofthe corpus.
In addition to the difficulty of usingthe resources across the research groups, the ex-istence of different formalisms is an obstacle forusers of NLP systems to compare and evaluatethe systems.
One scheme may receive a de factostatus, as is the case with the Penn Treebank, butit is still unsuitable for applications that requirethe information not encoded in the formalisms orto compare systems based on widely differentformalisms (e.g., CCG or HPSG in the case ofsyntactic parsing).If some common aspects are extracted fromthe schemes based on different formalisms, thecorpus annotated with the (common) scheme willbe used as a standard for (coarse-grained) evalua-tion and comparison between systems based ondifferent formalisms.
If an information-richscheme can be underspecified into a "common"level, the rich information in the corpus will beused locally for the system development and the"common" information can be used by peopleoutside the developers' group.
The key issue forestablishing the "common" level would be toprovide the systematic way to underspecify theindividual scheme.In this paper, the schemes of dependency cor-pora provided for the Shared Task are comparedon the problematic linguistic phenomena encoun-tered in annotating biomedical abstracts, in orderto investigate the possibility of making the "com-mon, underspecified" level of annotation.
Thecompared schemes are mainly CONLL sharedtask structures (CONLL) 1 , Rasp GrammaticalRelations (GR) , PARC 700 dependency struc-tures (PARC)2 and Stanford dependency struc-tures (Stanford; de Marneffe et al 2006),  withpartial reference to UTokyo HPSG Treebankpredicate-argument structures (HPSG; Miyao2006) and CCGBank predicate-argument struc-tures (CCG; Hockenmaier and Steedman 2005).2 UnderspecificationIn dependency annotation, two types of informa-tion are annotated to sentences.1 http://www.yr-bcn.es/conll2008/2 http://www2.parc.com/isl/groups/nltt/fsbank/triplesdoc.html18?
Dependency structure: what is dependenton what?
Dependency type: how the dependentdepends on the headFor the latter information, schemes like GR andStanford incorporates the hierarchy ofdependency types and allows systematicunderspecification but that does not totally solvethe problem.
A case of GR is addressed later.
Iftype hierarchy over different schemes can beestablished, it helps cross-scheme comparison.For the former information, in cases where someinformation in a corpus is omitted in another (e.g.head percolation), the corpus with lessinformation is considered as theunderspecification of the other, but when adifferent structure is assigned, there is nomechanism to form the underspecified structureso far proposed.
In the following section, thesample corpora are investigated trying to find thedifference in annotation, especially of thestructural difference.3 How are problematic structures en-coded in the sample corpora?The Wall Street Journal corpora provided for theshared task is investigated in order to look for thestructures that the annotator of our dependencycorpus commented as difficult, and to see howthey are encoded in the different schemes.
Thesubsections describe the non-trivial differencesamong the annotation schemes that are found.The subsections also discuss the underspecifiableannotation where possible.3.1 Multi-word TermsThe structure inside multi-word terms, or morebroadly, noun-noun sequence in general, havebeen left unannotated in Penn Treebank, and thelater schemes follow the decision.
Here, under-specification is realized in practice.
In depend-ency schemes where dependency is encoded by aset of binary relations, the last element of theterm is regarded as a head, and the rest of theelement of the term is regarded as dependent onthe last.
In the PARC annotation, proper nameslike "Los Angeles" and "Alex de Castro" aretreated as one token.However, there are noun sequences in whichthe head is clearly not the last token.
For exam-ple, there are a lot of names in the biomedicalfield where a subtype is specified (e.g.
HumanImmunodeficiency Virus Type I).
If the sequenceis considered as a name (of a type of virus in thisexample), it may be reasonable to assign a flatstructure to it, wherever the head is.
On theother hand, a flat structure is not adequate foranalyzing a structure like "Human Immunodefi-ciency Virus Type I and Type II".
Thus it isconventional to assign to a noun phrase "a flatstructure unless coordination is involved" in thebiomedical corpora, e.g., GENIA and Bioinfer(Pyssalo et al, 2007b).
However, adopting thisconvention can expose the corpus to a risk thatthe instances of a same name can be analyzeddifferently depending on context.Human Immunodeficiency Virus TypeI is a ...id(name0, Human ImmunodeficiencyVirus Type I)id(name1, Human ImmunodeficiencyVirus)id(name2, Type I)concat(name0, name1, name2)subject(is, name0)Human Immunodeficiency Virus TypeI and Type IIid(name3, Type II)conj(coord0, name2)conj(coord0, name3)conj_form(coord0, and)A possible solution is to annotate a certainnoun sequence as a term with a non-significantinternal structure, and where needed, the internalstructure may be annotated independently of theoutside structure.
The PARC annotation can beregarded as doing this kind of annotation bytreating a multi-word term as token and totallyignore the internal structure.
Going a step further,using IDs to the term and sub-terms, the internalstructure of a term  can be annotated, and thewhole term or a subcomponent can be used out-side, retaining the information where the se-quence refers to parts of the same name.
For ex-ample, Figure 1 is a PARC-like annotation usingname-IDs, where id(ID, name) is for assigningan ID to a name or a part of a name, and name0,name1, name2, and name3 are IDs for "Hu-man Immunodeficiency Virus Type I", "HumanImmunodeficiency Virus", "Type I", "Type II",and "Human Immunodeficiency Virus Type II"respectively, and concat(a, b, c) means thatstrings b and c is concatenated to make string a.adjunct(name1, coord0)Figure 1.
PARC-like annotation with explicitannotation of names193.2 CoordinationThe example above suggests that the coordina-tion is a problematic structure.
In our experience,coordination structures, especially ones with el-lipsis, were a major source of annotation incon-sistency.
In fact, there are significant differencesin the annotation of coordination in the samplecorpora, as shown in the following subsections.What is the head?Among the schemes used in the sample corpora,CCG does not explicitly annotate the coordina-tion but encodes them as if the coordinated con-stituents exist independently 3 .
The remainingschemes may be divided into determination ofthe head of coordination.?
GR, PARC, and HPSG makes the coor-dinator (and, etc) the head?
CONLL and Stanford makes the preced-ing component the headFor example, in the case with "makes and dis-tributes", the former group encodes the relationinto two binary relations where "and" is the head(of both), and "makes" and "distributes" are thedependent on "and".
In the latter group, CONLLencodes the coordination into two binary rela-tions: one is the relation where "makes" is thehead and "and" is the dependant and anotherwhere "and" is the head and "distributes" is thedependent.
In Stanford scheme, the coordinatoris encoded into the type of relation (conj_and)where "makes" is the head and "distributes" isthe dependent.
As for the CCG scheme, the in-formation that the verbs are coordinated by "and"is totally omitted.
The difference of policy onhead involves structural discrepancy where un-derspecification does not seem easy.Distribution of the dependentsAnother difference is in the treatment of depend-ents on the coordinated head.
For example, thefirst sentence of the corpus can be simplified to"Bell makes and distributes products".
The sub-ject and object of the two verbs are shared:"Bell" is the subject of "makes" and "distributes",and "products" is their direct object.
The subject3 Three kinds of files for annotating sentence structures areprovided in the original CCGbank corpus: the human-readable corpus files, the machine-readable derivation files,and the predicate-argument structure files.The coordinators are marked in the human-readable corpusfiles, but not in the predicate-argument structure files fromwhich the sample corpus for the shared task was derived.is treated as dependent on the coordinator in GR,dependent on the coordinator as well as bothverbs in PARC 4 , dependent on both verbs inHPSG and Stanford (and CCG), and dependenton "makes" in CONLL.
As for the object, "prod-ucts" is treated as dependent on the coordinatorin GR and PARC, dependent on both verbs inHPSG (and CCG), and dependent on "makes" inCONLL and Stanford.
The Stanford scheme uni-formly treats subject and object differently: Thesubject is distributed among the coordinatedverbs, and the object is treated as dependent onthe first verb only.A different phenomenon was observed fornoun modifiers.
For example, semantically,"electronic, computer and building products" inthe first sentence should be read as "electronicproducts and computer products and buildingproducts" not as "products that have electronicand computer and building nature".
That is, thecoordination should be read distributively.
Thedistinction between distributive and non-distributive reading is necessary for applicationssuch as information extraction.
For example, inthe biomedical text, it must be determinedwhether "CD4+ and CD8+ T cells" denotes "Tcells expressing CD4 and T cells expressingCD8" or "T cells expressing both CD4 and CD8".Coordinated noun modifier is treated differ-ently among the corpora.
The coordinated adjec-tives are dependent on the noun (like in non-distributive reading) in GR, CONLL, and PARC,while the adjectives are treated as separately de-pendent on the noun in Stanford and HPSG (andCCG).
In the PARC scheme, there is a relationnamed coord_level denoting the syntactictype of the coordinated constituents.
For example,in the annotation of the first sentence of the sam-ple corpus ("...electronic, computer and buildingproducts"), coord_level(coord~19, AP)denotes that the coordinated constituents are AP,as syntactically speaking adjectives are coordi-nated.
It seems that distributed and non-distributed readings (semantics) are not distin-guished.It can be said that GR and others are annotat-ing syntactic structure of the dependency whileHPSG and others annotate more semantic struc-4 According to one of the reviewers this is an error in thedistributed version of the PARC corpus that is the result ofthe automatic conversion.
The correct structure is the one inwhich the subject is only dependent on both verbs but noton the coordinator (an example is parc_23.102 inhttp://www2.parc.com/isl/groups/nltt/fsbank/parc700-2006-05-30.fdsc); the same would hold of the object.20ture.
Ideally, the mechanism for encoding thesyntactic and semantic structure separately on thecoordination should be provided, with an optionto decide whether one of them is left unanno-tated.For example, the second example shown inFigure 1 ("Human Immunodeficiency VirusType I and Type II") can be viewed as a coordi-nation of two modifiers ("Type I" and "Type II")syntactically, and as a coordination of two names("Human Immunodeficiency Virus Type I" and"Human Immunodeficiency Virus Type II") se-mantically.
Taking this into consideration, thestructure shown in Figure 1 can be enhanced intothe one shown in Figure 2 where conj_sem isfor representing the semantic value of coordina-tion, and coord0_S denotes that the dependen-cies are related semantically to coord0.
Provid-ing two relations that work as cood_level inthe PARC scheme, one for the syntactic level andthe other for the semantic level, may be anothersolution: if a parallel of coord_level, say,coord_level_sem, can be used in addition toencode the semantically coordinated constituents,distributive reading of "electronic, computer andbuilding products" mentioned above may be ex-pressed by coord_level_sem(coord~19,NP)indicating that it is a noun phrases withshared head that are coordinated.Human Immunodeficiency Virus TypeI and Type IIid(name0, Human ImmunodeficiencyVirus Type I)id(name1, Human ImmunodeficiencyVirus)id(name2, Type I)concat(name0, name1, name2)id(name3, Type II)id(name4, Human ImmunodeficiencyVirus Type II)concat(name4, name1, name3)conj(coord0, name2)conj(coord0, name3)conj_form(coord0, and)adjunct(name1, coord0)conj_sem(coord0_S, name0)conj_sem(coord0_S, name4)Figure 2.
Annotation of coordinated names onsyntactic and semantic levelsCoordinatorTwo ways of expressing the coordination be-tween three items are found in the corpora: re-taining the surface form or not.cotton , soybeans and riceeggs and butter and milkFor example, the structures for the two phrasesabove are different in the CONLL corpus whileothers ignore the fact that the former uses acomma while "and" is used in the latter.
That is,the CONLL scheme encodes the surface struc-ture, while others encode the deeper structure, forsemantically the comma in the former examplemeans "and".
The difference can be captured byretrieving the surface form of the sentences in thecorpora that ignore the surface structure.
How-ever, encoding surface form and deeper structurewould help to capture maximal information andto compare the structures across different annota-tions more smoothly.3.3 Prepositional phrasesAnother major source of inconsistency involvedprepositional phrases.
The PP-attachment prob-lem (where the PP should be attached) is a prob-lem traditionally addressed in parsing, but in thecase of dependency, the type of attachment alsobecomes a problem.Where is the head?The focus of the PP-attachment problem is thehead where the PP should attach.
In some cases,athe correct place to attach can be determinedfrom the broader context in which the problem-atic sentence appears, and in some other casesthe attachment ambiguity is "benign" in the sensethat there is little or no difference in meaningcaused by the difference in the attachment site.However, in highly specialized domain like bio-medical papers, annotators of grammatical struc-tures do not always have full access to the mean-ing, and occasionally, it is not easy to decidewhere to attach the PP, whether the ambiguity isbenign, etc.
Yet, it is not always that the annota-tor of a problematic sentence has no informationat all: the annotator cannot usually choose fromthe few candidates selected by the (partial) un-derstanding of the sentence, and not from all pos-sible sites the PP can syntactically attach.No schemes provided for the task allow the list-ing of possible candidates of the phrases where aPP can attach (as allowed in the case of PennTreebank POS corpus).
As with the POS, ascheme for annotating ambiguous attachmentshould be incorporated.
This can be more easilyrealized for dependency annotation, where thestructure of a sentence is decomposed into list of21local dependencies, than treebank annotation,where the structure is annotated as a whole.
Sim-ply listing the possible dependencies, with a flagfor ambiguity, should work for the purpose.
Pref-erably, the flag encodes the information aboutwhether the annotator thinks the ambiguity isbenign, i.e.
the annotator believes that the ambi-guity does not affect the semantics significantly.Complement or ModifierIn dependency annotation, the annotator mustdecide whether the PP dependent of a verb or averbal noun is an obligatory complement or anoptional modifier.
External resources (e.g.
dic-tionary) can be used for common verbs, but fortechnical verbs such resources are not yet widelyavailable, and collecting and investigating a largeset of actual use of the verbal is not an easy task.Dependency types for encoding PP-attachmentare varied among the schemes.
Schemes such asCONLL and Stanford do not distinguish betweencomplements and modifiers, and they just anno-tate the relation that the phrase "attaches as a PP".HPSG in theory can distinguish complementsand modifiers, but in the actual corpus, all PPsappear as modifiers5.
GR does not mark the typeof the non-clausal modifying phrase but distin-guish PP-complements (iobj), nominal com-plements (dobj) and modifiers.
PARC has moredistinction of attachment type (e.g.
obj, obl,adjunct).If the inconsistency problem involving thetype of PP attachment lies in the distinction be-tween complements and modifiers, treatment ofCONLL and Stanford looks better than that ofGR and PARC.
However, an application mayrequire the distinction (a candidate of such appli-cation is relation information extraction usingpredicate-argument structure) so that analysiswith the schemes that cannot annotate such dis-tinction at all is not suitable for such kind of ap-plications.
On the other hand, GR does havetype-underspecification (Briscoe 2006) but theargument (complement) - modifier distinction isat the top level of the hierarchy and underspecifi-cation cannot be done without discarding the in-formation that the dependent is a PP.A dependent of a verbal has two aspects ofdistinction: complement/modifier and grammati-cal category (whether it is an NP, a PP, an AP,etc).
The mechanism for encoding these aspectsseparately should be provided, with an option to5 The modifier becomes a head in HPSG and in CCG unlikeother formalisms.decide if one is left unannotated.
A possible an-notation scheme using IDs is illustrated in Figure3, where type of dependency and type of the de-pendent are encoded separately.
A slash indicatesthe alternatives from which to choose one (ormore, in ambiguous cases).Dependency(ID, verb, dependent)Dependent_type(ID, MOD/ARG)Dependent_form(ID, PP/NP/AP/...)Figure 3: An illustration of attachment to a ver-bal head4 Toward a Unified SchemeThe observation suggests that, for difficult lin-gustic phenomena, different aspects of the phe-nomena are annotated by different schemes.
Italso suggests that there are at least two problemsin defining the type of dependencies: one is theconfusion of the level of analysis, and another isthat several aspects of dependency are encodedinto one label.The confusion of the level of analysis meansthat, as seen in the case of coordination, the syn-tactic-level analysis and semantic-level analysisreceive the same or similar label across theschemes.
In each scheme only one level of analy-sis is provided, but it is not always explicit whichlevel is provided in a particular scheme.
Thus, itis inconvenient and annoying for an annotatorwho wants to annotate the other level or bothlevels at once.As seen in the case of PP-dependents ofverbals, because different aspects, or features, areencoded in one label, type-underspecificationbecomes a less convenient mechanism.
If labelsare properly decomposed into a set of featurevalues, and a hierarchy of values is provided foreach feature, the annotation labels can be moreflexible and it is easier for an annotator to choosea label that can encode the desired information.The distinction of syntax/semantics (or there maybe more levels) can be incorporated into one ofthe features.
Other possible features include thegrammatical categories of head and dependent,argument/modifier distinction, and  role of argu-ments or modifiers like the one annotated inPropbank (Palmer et al, 2005).Decomposing labels into features have anotheruse.
It would make the mapping between onescheme and another more transparent.As the dependency structure of a sentence isencoded into a list of local information in de-22pendency schemes, it can be suggested that tak-ing the union of the annotation of differentschemes can achieve the encoding of the unionof information that the individual schemes canencode, except for conflicting representationssuch as the head of coordinated structures, andthe head of modifiers in HPSG.
If the currentlabels are decomposed into features, it wouldenable one to take non-redundant union of in-formation, and mapping from the union to a par-ticular scheme would be more systematic.
Inmany cases listed in the previous section, indi-vidual schemes could be obtained by systemati-cally omitting some relations in the union, andcommon information among the schemes (thestructures that all of the schemes concerned canagree) could be retrieved by taking the intersec-tion of annotations.
An annotator can annotatethe maximal information (s)he knows within theframework of the union, and mapped into thepredefined scheme when needed.Also, providing a mechanism for annotatingambiguity should be provided.
As for depend-ency types the type hierarchy of features de-scribed above can help.
As for the ambiguity ofattachment site and others that involve the prob-lem of what is dependent on what, listing of pos-sible candidates with a flag of ambiguity canhelp.AcknowledgmentsI am grateful for the anonymous reviewers forsuggestions and comments.ReferencesBies, Ann, Mark Ferguson, Karen Katz, Robert Mac-Intyre, Victoria Tredinnick, Grace Kim, Mary AnnMarcinkiewicz, and Britta Schasberger , 1995.Bracketing Guidelines for Treebank II Style PennTreebank Project.
Technical report, University ofPennsylvania.Briscoe, Ted.
2006.
An introduction to tag sequencegrammars and the RASP system parser.
TechnicalReport (UCAM-CL-TR-662), Cambridge Univer-sity Computer Laboratory.Clegg, Andrew B. and Adrian J Shepherd.
2007.Benchmarking natural-language parsers for bio-logical applications using dependency graphs.BMC Bioinformatics 8:24.Hockenmaier, Julia and Mark Steedman.
2005.CCGbank: User?s Manual, Technical Report (MS-CIS-05-09), University of Pennsylvania.Kim, J-D., Ohta, T.,  Teteisi Y., Tsujii, J.
(2003).GENIA corpus - a semantically annotated corpusfor bio-textmining.
Bioinformatics.
19(suppl.
1), pp.i180-i182.de Marneffe, Marie-Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.Proceedings of LREC 2006, Genoa, Italy.Miyao, Yusuke.
From Linguistic Theory to SyntacticAnalysis: Corpus-Oriented Grammar Developmentand Feature Forest Model.
2006.
PhD Thesis, Uni-versity of Tokyo.Miyao, Yusuke, Kenji Sagae, Jun'ichi Tsujii.
2007.Towards Framework-Independent Evaluation ofDeep Linguistic Parsers.
In Proceedings of Gram-mar Engineering across Frameworks, Stanford,California, USA, pp.
238-258.Palmer, Martha, Paul Kingsbury, Daniel Gildea.
2005.
"The Proposition Bank: An Annotated Corpus ofSemantic Roles".
Computational Linguistics 31(1): 71?106.Pyysalo, Sampo, Filip Ginter, Veronika Laippala,Katri Haverinen, Juho Heimonen, and Tapio Sala-koski.
2007a.
On the unification of syntactic anno-tations under the Stanford dependency scheme: Acase study on BioInfer and GENIA.
Proceedings ofBioNLP Workshop at ACL 2007, Prague, CzechRepublic .Pyysalo, Sampo, Filip Ginter, Juho Heimonen, JariBj?rne, Jorma Boberg, Jouni J?rvinen and TapioSalakoski.
2007b.
BioInfer: a corpus for informa-tion extraction in the biomedical domain.
BMCBioinformatics 8:50.Santorini, Beatrice.
1990.
Part-of-Speech TaggingGuidelines for the Penn Treebank Project.
Techni-cal report, University of Pennsylvania.Tateisi, Yuka, Ohta, Tomoko, Nigel Collier, ChikashiNobata and Jun'ichi Tsujii.
2000.
Building an An-notated Corpus from Biology Research Papers.
Inthe Proceedings of COLING 2000 Workshop onSemantic Annotation and Intelligent Content.
Lux-embourg.
pp.
28-34.Tateisi,Yuka, Yusuke Miyao, Kenji Sagae, Jun'ichiTsujii.
2008.
GENIA-GR: a Grammatical RelationCorpus for Parser Evaluation in the BiomedicalDomain.
In the Proceedings of the Sixth Interna-tional Language Resources and Evaluation(LREC'08).
Marrakech, Morocco.23
