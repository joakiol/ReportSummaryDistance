Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062?1072,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSpectral Unsupervised Parsing with Additive Tree MetricsAnkur P. ParikhSchool of Computer ScienceCarnegie Mellon Universityapparikh@cs.cmu.eduShay B. CohenSchool of InformaticsUniversity of Edinburghscohen@inf.ed.ac.ukEric P. XingSchool of Computer ScienceCarnegie Mellon Universityepxing@cs.cmu.eduAbstractWe propose a spectral approach for un-supervised constituent parsing that comeswith theoretical guarantees on latent struc-ture recovery.
Our approach is grammar-less ?
we directly learn the bracketingstructure of a given sentence without us-ing a grammar model.
The main algorithmis based on lifting the concept of additivetree metrics for structure learning of la-tent trees in the phylogenetic and machinelearning communities to the case wherethe tree structure varies across examples.Although finding the ?minimal?
latent treeis NP-hard in general, for the case of pro-jective trees we find that it can be foundusing bilexical parsing algorithms.
Empir-ically, our algorithm performs favorablycompared to the constituent context modelof Klein and Manning (2002) without theneed for careful initialization.1 IntroductionSolutions to the problem of grammar inductionhave been long sought after since the early days ofcomputational linguistics and are interesting bothfrom cognitive and engineering perspectives.
Cog-nitively, it is more plausible to assume that chil-dren obtain only terminal strings of parse trees andnot the actual parse trees.
This means the unsu-pervised setting is a better model for studying lan-guage acquisition.
From the engineering perspec-tive, training data for unsupervised parsing existsin abundance (i.e.
sentences and part-of-speechtags), and is much cheaper than the syntacticallyannotated data required for supervised training.Most existing solutions treat the problem of un-supervised parsing by assuming a generative pro-cess over parse trees e.g.
probabilistic contextfree grammars (Jelinek et al, 1992), and the con-stituent context model (Klein and Manning, 2002).Learning then reduces to finding a set of parame-ters that are estimated by identifying a local max-imum of an objective function such as the likeli-hood (Klein and Manning, 2002) or a variant of it(Smith and Eisner, 2005; Cohen and Smith, 2009;Headden et al, 2009; Spitkovsky et al, 2010b;Gillenwater et al, 2010; Golland et al, 2012).
Un-fortunately, finding the global maximum for theseobjective functions is usually intractable (Cohenand Smith, 2012) which often leads to severe lo-cal optima problems (but see Gormley and Eisner,2013).
Thus, strong experimental results are oftenachieved by initialization techniques (Klein andManning, 2002; Gimpel and Smith, 2012), incre-mental dataset use (Spitkovsky et al, 2010a) andother specialized techniques to avoid local optimasuch as count transforms (Spitkovsky et al, 2013).These approaches, while empirically promising,generally lack theoretical justification.On the other hand, recently proposed spectralmethods approach the problem via restriction ofthe PCFG model (Hsu et al, 2012) or matrix com-pletion (Bailly et al, 2013).
These novel perspec-tives offer strong theoretical guarantees but are notdesigned to achieve competitive empirical results.In this paper, we suggest a different approach,to provide a first step to bridging this theory-experiment gap.
More specifically, we approachunsupervised constituent parsing from the per-spective of structure learning as opposed to pa-rameter learning.
We associate each sentence withan undirected latent tree graphical model, which isa tree consisting of both observed variables (corre-sponding to the words in the sentence) and an ad-ditional set of latent variables that are unobservedin the data.
This undirected latent tree is then di-rected via a direction mapping to give the finalconstituent parse.In our framework, parsing reduces to finding thebest latent structure for a given sentence.
How-ever, due to the presence of latent variables, struc-ture learning of latent trees is substantially morecomplicated than in observed models.
As before,one solution would be local search heuristics.Intuitively, however, latent tree models en-code low rank dependencies among the observedvariables permitting the development of ?spec-1062tral?
methods that can lead to provably correctsolutions.
In particular we leverage the con-cept of additive tree metrics (Buneman, 1971;Buneman, 1974) in phylogenetics and machinelearning that can create a special distance met-ric among the observed variables as a functionof the underlying spectral dependencies (Choi etal., 2011; Song et al, 2011; Anandkumar et al,2011; Ishteva et al, 2012).
Additive tree met-rics can be leveraged by ?meta-algorithms?
suchas neighbor-joining (Saitou and Nei, 1987) andrecursive grouping (Choi et al, 2011) to provideconsistent learning algorithms for latent trees.Moreover, we show that it is desirable to learnthe ?minimal?
latent tree based on the tree metric(?minimum evolution?
in phylogenetics).
Whilethis criterion is in general NP-hard (Desper andGascuel, 2005), for projective trees we find that abilexical parsing algorithm can be used to find anexact solution efficiently (Eisner and Satta, 1999).Unlike in phylogenetics and graphical models,where a single latent tree is constructed for all thedata, in our case, each part of speech sequence isassociated with its own parse tree.
This leads to asevere data sparsity problem even for moderatelylong sentences.
To handle this issue, we presenta strategy that is inspired by ideas from kernelsmoothing in the statistics community (Zhou et al,2010; Kolar et al, 2010b; Kolar et al, 2010a).This allows principled sharing of samples fromdifferent but similar underlying distributions.We provide theoretical guarantees on the re-covery of the correct underlying latent tree andcharacterize the associated sample complexity un-der our technique.
Empirically we evaluate ourmethod on data in English, German and Chi-nese.
Our algorithm performs favorably to Kleinand Manning?s (2002) constituent-context model(CCM), without the need for careful initialization.In addition, we also analyze CCM?s sensitivity toinitialization, and compare our results to Seginer?salgorithm (Seginer, 2007).2 Learning Setting and ModelIn this section, we detail the learning setting and aconditional tree model we learn the structure for.2.1 Learning SettingLet w = (w1, ..., w`) be a vector of words corre-sponding to a sentence of length `.
Each wiis rep-resented by a vector in Rpfor p ?
N. The vectoris an embedding of the word in some space, cho-VBD DT NNVBD DT NNFigure 2: Candidate constituent parses for x = (VBD, DT, NN)(left-correct, right -incorrect)sen from a fixed dictionary that maps word typesto Rp.
In addition, let x = (x1, ..., x`) be the as-sociated vector of part-of-speech (POS) tags (i.e.xiis the POS tag of wi).In our learning algorithm, we assume that ex-amples of the form (w(i),x(i)) for i ?
[N ] ={1, .
.
.
, N} are given, and the goal is to predicta bracketing parse tree for each of these examples.The word embeddings are used during the learn-ing process, but the final decoder that the learningalgorithm outputs maps a POS tag sequence x toa parse tree.
While ideally we would want to usethe word information in decoding as well, much ofthe syntax of a sentence is determined by the POStags, and relatively high level of accuracy can beachieved by learning, for example, a supervisedparser from POS tag sequences.Just like our decoder, our model assumes thatthe bracketing of a given sentence is a functionof its POS tags.
The POS tags are generatedfrom some distribution, followed by a determin-istic generation of the bracketing parse tree.
Then,latent states are generated for each bracket, andfinally, the latent states at the yield of the bracket-ing parse tree generate the words of the sentence(in the form of embeddings).
The latent states arerepresented by vectors z ?
Rmwhere m < p.2.2 IntuitionFor intuition, consider the simple tag sequencex = (VBD, DT, NN).
Two candidate constituentparse structures are shown in Figure 2 and the cor-rect one is boxed in green (the other in red).
Re-call that our training data contains word phrasesthat have the tag sequence x e.g.
w(1)=(hit, the, ball), w(2)= (ate, an, apple).Intuitively, the words in the above phrases ex-hibit dependencies that can reveal the parse struc-ture.
The determiner (w2) and the direct object(w3) are correlated in that the choice of deter-miner depends on the plurality of w3.
However,the choice of verb (w1) is mostly independent ofthe determiner.
We could thus conclude that w2and w3should be closer in the parse tree than w11063The be ar ate the fish?1 , ?2 , ?3 , ?4 , ?5 , ?1, ?2, ?3?
= (??,?
?, ??
?, ??,??)?(?
)((DT NN) (VBD (DT NN)))w 1 w 2 w 3z 3z 1w 4 w 5z 2w 1 w 2 w 3z 3z 1w 4 w 5z 2Figure 1: Example for the tagsequence (DT, NN, VBD, DT, NN)showing the overview of ourapproach.
We first learn a undi-rected latent tree for the se-quence (left).
We then ap-ply a direction mapping hdirtodirect the latent tree (center).This can then easily be con-verted into a bracketing (right).andw2, giving us the correct structure.
Informally,the latent state z corresponding to the (w2, w3)bracket would store information about the plural-ity of z, the key to the dependence betweenw2andw3.
It would then be reasonable to assume that w2and w3are independent given z.2.3 A Conditional Latent Tree ModelFollowing this intuition, we propose to model thedistribution over the latent bracketing states andwords for each tag sequence x as a latent treegraphical model, which encodes conditional inde-pendences among the words given the latent states.Let V := {w1, ..., w`, z1, ..., zH}, with wirep-resenting the word embeddings, and zirepresent-ing the latent states of the bracketings.
Then, ac-cording to our base model it holds that:p(w, z|x) =H?i=1p(zi|pix(zi), ?
(x))?`(x)?i=1p(wi|pix(wi), ?
(x)) (1)where pix(?)
returns the parent node index of theargument in the latent tree corresponding to tagsequence x.1If z is the root, then pix(z) = ?.All the wiare assumed to be leaves while all theziare internal (i.e.
non-leaf) nodes.
The param-eters ?
(x) control the conditional probability ta-bles.
We do not commit to a certain parametricfamily, but see more about the assumptions wemake about ?
in ?3.2.
The parameter space is de-noted ?.
The model assumes a factorization ac-cording to a latent-variable tree.
The latent vari-ables can incorporate various linguistic properties,such as head information, valence of dependencybeing generated, and so on.
This information isexpected to be learned automatically from data.Our generative model deterministically maps aPOS sequence to a bracketing via an undirected1At this point, pi refers to an arbitrary direction of theundirected tree u(x).latent-variable tree.
The orientation of the tree isdetermined by a direction mapping hdir(u), whichis fixed during learning and decoding.
This meansour decoder first identifies (given a POS sequence)an undirected tree, and then orients it by applyinghdiron the resulting tree (see below).Define U to be the set of undirected latent treeswhere all internal nodes have degree exactly 3 (i.e.they correspond to binary bracketing), and in addi-tion hdir(u) for any u ?
U is projective (explainedin the hdirsection).
In addition, let T be the setof binary bracketings.
The complete generativemodel that we follow is then:?
Generate a tag sequence x = (x1, .
.
.
, x`)?
Decide on u(x) ?
U , the undirected latent treethat x maps to.?
Set t ?
T by computing t = hdir(u).?
Set ?
?
?
by computing ?
= ?(x).?
Generate a tuple v = (w1, .
.
.
, w`, z1, ..., zH)where wi?
Rp, zj?
Rmaccording to Eq.
1.See Figure 1 (left) for an example.The Direction Mapping hdir.
Generating abracketing via an undirected tree enables us tobuild on existing methods for structure learningof latent-tree graphical models (Choi et al, 2011;Anandkumar et al, 2011).
Our learning algorithmfocuses on recovering the undirected tree basedfor the generative model that was described above.This undirected tree is converted into a directedtree by applying hdir.
The mapping hdirworks inthree steps:?
It first chooses a top bracket ([1, R ?
1], [R, `])where R is the mid-point of the bracket and ` isthe length of the sentence.?
It marks the edge ei,jthat splits the tree accord-ing to the top bracket as the ?root edge?
(markedin red in Figure 1(center))?
It then creates t from u by directing the tree out-ward from ei,jas shown in Figure 1(center)1064The resulting t is a binary bracketing parse tree.As implied by the above definition of hdir, se-lecting which edge is the root can be interpretedas determining the top bracket of the constituentparse.
For example, in Figure 1, the top bracketis ([1, 2], [3, 5]) = ([DT, NN], [VBD, DT, NN]).
Notethat the ?root?
edge ez1,z2partitions the leavesinto precisely this bracketing.
As indicated in theabove section, we restrict the set of undirectedtrees to be those such that after applying hdirtheresulting t is projective i.e.
there are no crossingbrackets.
In ?4.1, we discuss an effective heuristicto find the top bracket without supervision.3 Spectral Learning Algorithm based onAdditive Tree MetricsOur goal is to recover t ?
T for tag sequence xusing the data D = [(w(i),x(i))]Ni=1.
To get an in-tuition about the algorithm, consider a partition ofthe set of examplesD intoD(x) = {(w(i),x(i)) ?D|x(i)= x}, i.e.
each section in the partition hasan identical sequence of part of speech tags.
As-sume for this section |D(x)| is large (we addressthe data sparsity issue in ?3.4).We can then proceed by learning how to map aPOS sequence x to a tree t ?
T (through u ?
U)by focusing only on examples in D(x).Directly attempting to maximize the likelihoodunfortunately results in an intractable optimiza-tion problem and greedy heuristics are often em-ployed (Harmeling and Williams, 2011).
Insteadwe propose a method that is provably consistentand returns a tree that can be mapped to a bracket-ing using hdir.If all the variables were observed, then theChow-Liu algorithm (Chow and Liu, 1968) couldbe used to find the most likely tree structure u ?U .
The Chow-Liu algorithm essentially computesthe distances among all pairs of variables (the neg-ative of the mutual information) and then finds theminimum cost tree.
However, the fact that the ziare latent variables makes this strategy substan-tially more complicated.
In particular, it becomeschallenging to compute the distances among pairsof latent variables.
What is needed is a ?special?distance function that allows us to reverse engineerthe distances among the latent variables given thedistances among the observed variables.
This isthe key idea behind additive tree metrics that arethe basis of our approach.In the following sections, we describe the keysteps to our method.
?3.1 and ?3.2 largely describeexisting background on additive tree metrics andlatent tree structure learning, while ?3.3 and ?3.4discuss novel aspects that are unique to our prob-lem.3.1 Additive Tree MetricsLet u(x) be the true undirected tree of sentence xand assume the nodes V to be indexed by [M ] ={1, .
.
.
,M} such that M = |V| = H + `.
Fur-thermore, let v ?
V refer to a node in the undi-rected tree (either observed or latent).
We assumethe existence of a distance function that allows usto compute distances between pairs of nodes.
Forexample, as we see in ?3.2 we will define the dis-tance d(i, j) to be a function of the covariance ma-trix E[viv>j|u(x), ?(x)].
Thus if viand vjare bothobserved variables, the distance can be directlycomputed from the data.Moreover, the metrics we construct are suchthat they are tree additive, defined below:Definition 1 A function du(x) : [M ]?
[M ]?
R isan additive tree metric (Erd?os et al, 1999) for theundirected tree u(x) if it is a distance metric,2andfurthermore, ?i, j ?
[M ] the following relationholds:du(x)(i, j) =?
(a,b)?pathu(x)(i,j)du(x)(a, b) (2)where pathu(x)(i, j) is the set of all the edges inthe (undirected) path from i to j in the tree u(x).As we describe below, given the tree structure,the additive tree metric property allows us to com-pute ?backwards?
the distances among the latentvariables as a function of the distances among theobserved variables.Define D to be the M ?
M distance matrixamong the M variables, i.e.
Dij= du(x)(i, j).LetDWW, DZW(equal toD>WZ), andDZZindi-cate the word-word, latent-word and latent-latentsub-blocks of D respectively.
In addition, sinceu(x) is assumed to be known from context, wedenote du(x)(i, j) just by d(i, j).Given the fact that the distance between a pairof nodes is a function of the random variablesthey represent (according to the true model), onlyDWWcan be empirically estimated from data.However, if the underlying tree structure is known,then Definition 1 can be leveraged to computeDZZand DZWas we show below.2This means that it satisfies d(i, j) = 0 if and only ifi = j, the triangle inequality and is also symmetric.1065v j v ie i , j(a)v ie i , jv j(b)Figure 3: Two types of edges in general undirected latenttrees.
(a) leaf edge, (b) internal edgeWe first show how to compute d(i, j) for all i, jsuch that i and j are adjacent to each other in u(x),based only on observed nodes.
It then follows thatthe other elements of the distance matrix can becomputed based on Definition 1.
To show how tocompute distances between adjacent nodes, con-sider the two cases: (1) (i, j) is a leaf edge; (2)(i, j) is an internal edge.Case 1 (leaf edge, figure 3(a)) Assume withoutloss of generality that j is the leaf and i is an in-ternal latent node.
Then i must have exactly twoother neighbors a ?
[M ] and b ?
[M ].
Let Adenote the set of nodes that are closer to a thani and similarly let B denote the set of nodes thatare closer to b than i.
Let A?and B?denote allthe leaves (word nodes) in A and B respectively.Then using path additivity (Definition 1), it can beshown that for any a??
A?, b??
B?it holds that:d(i, j) =12(d(j, a?)
+ d(j, b?)?
d(a?, b?))
(3)Note that the right-hand side only depends ondistances between observed random variables.Case 2 (internal edge, figure 3(b)) Both i andj are internal nodes.
In this case, i has exactlytwo other neighbors a ?
[M ] and b ?
[M ], andsimilarly, j has exactly other two neighbors g ?
[M ] and h ?
[M ].
Let A denote the set of nodescloser to a than i, and analogously for B, G, andH .
Let A?, B?, G?, and H?refer to the leaves inA,B,G, and H respectively.
Then for any a?
?A?, b??
B?, g??
G?, and h??
H?it can beshown that:d(i, j) =14(d(a?, g?)
+ d(a?, h?)
+ d(b?, g?
)+d(b?, h?)?
2d(a?, b?)?
2d(g?, h?
))(4)Empirically, one can obtain a more robust em-pirical estimate?d(i, j) by averaging over all validchoices of a?, b?in Eq.
3 and all valid choices ofa?, b?, g?, h?in Eq.
4 (Desper and Gascuel, 2005).3.2 Constructing a Spectral Additive MetricIn constructing our distance metric, we begin withthe following assumption on the distribution inEq.
1 (analogous to the assumptions made inAnandkumar et al, 2011).Assumption 1 (Linear, Rank m, Means)E[zi|pix(zi),x] = A(zi|zpix(zi),x)pix(zi) ?i ?
[H]where A(zi|pix(zi),x) ?
Rm?mhas rank m.E[wi|pix(wi),x] = C(wi|pix(wi),x)pix(wi) ?i ?
[`(x)]where C(wi|pix(wi),x) ?
Rp?mhas rank m.Also assume that E[ziz>i|x] has rank m ?i ?
[H].Note that the matrices A and C are a directfunction of ?
(x), but we do not specify a modelfamily for ?(x).
The only restriction is in the formof the above assumption.
If wiand ziwere dis-crete, represented as binary vectors, the above as-sumption would correspond to requiring all con-ditional probability tables in the latent tree to haverankm.
Assumption 1 allows for the wito be highdimensional features, as long as the expectationrequirement above is satisfied.
Similar assump-tions are made with spectral parameter learningmethods e.g.
Hsu et al (2009), Bailly et al (2009),Parikh et al (2011), and Cohen et al (2012).Furthermore, Assumption 1 makes it explicitthat regardless of the size of p, the relationshipsamong the variables in the latent tree are restrictedto be of rank m, and are thus low rank since p >m.
To leverage this low rank structure, we proposeusing the following additive metric, a normalizedvariant of that in Anandkumar et al (2011):dspectral(i, j) = ?
log ?m(?x(i, j))+12log ?m(?x(i, i)) +12log ?m(?x(j, j)) (5)where ?m(A) denotes the product of the top msingular values of A and ?x(i, j) := E[viv>j|x],i.e.
the uncentered cross-covariance matrix.We can then show that this metric is additive:Lemma 1 If Assumption 1 holds then, dspectralisan additive tree metric (Definition 1).A proof is in the supplementary for completeness.From here, we use d to denote dspectral, since thatis the metric we use for our learning algorithm.10663.3 Recovering the Minimal ProjectiveLatent TreeIt has been shown (Rzhetsky and Nei, 1993) thatfor any additive tree metric, u(x) can be recoveredby solving arg minu?Uc(u) for c(u):c(u) =?
(i,j)?Eud(i, j).
(6)where Euis the set of pairs of nodes which areadjacent to each other in u and d(i, j) is computedusing Eq.
3 and Eq.
4.Note that the metric d we use in defining c(u)is based on the expectations from the true distri-bution.
In practice, the true distribution is un-known, and therefore we use an approximation forthe distance metric?d.
As we discussed in ?3.1all elements of the distance matrix are functionsof observable quantities if the underlying tree u isknown.
However, only the word-word sub-blockDWWcan be directly estimated from the datawithout knowledge of the tree structure.This subtlety makes solving the minimizationproblem in Eq.
6 NP-hard (Desper and Gascuel,2005) if u is allowed to be an arbitrary undirectedtree.
However, if we restrict u to be in U , as we doin the above, then maximizing c?
(u) over U can besolved using the bilexical parsing algorithm fromEisner and Satta (1999).
This is because the com-putation of the other sub-blocks of the distancematrix only depend on the partitions of the nodesshown in Figure 3 into A, B, G, and H , and noton the entire tree structure.Therefore, the procedure to find a bracketingfor a given POS tag x is to first estimate the dis-tance matrix sub-block?DWWfrom raw text data(see ?3.4), and then solve the optimization prob-lem arg minu?Uc?
(u) using a variant of the Eisner-Satta algorithm where c?
(u) is identical to c(u) inEq.
6, with d replaced with?d.Summary.
We first defined a generative modelthat describes how a sentence, its sequence of POStags, and its bracketing is generated (?2.3).
Firstan undirected u ?
U is generated (only as a func-tion of the POS tags), and then u is mapped toa bracketing using a direction mapping hdir.
Wethen showed that we can define a distance met-ric between nodes in the undirected tree, such thatminimizing it leads to a recovery of u.
This dis-tance metric can be computed based only on thetext, without needing to identify the latent infor-mation (?3.2).
If the true distance metric is known,Algorithm 1 The learning algorithm for find-ing the latent structure from a set of examples(w(i),x(i)), i ?
[N ].Inputs: Set of examples (w(i),x(i)) for i ?
[N ],a kernel K?
(j, k, j?, k?|x,x?
), an integer mData structures: For each i ?
[N ], j, k ?`(x(i)) there is a (uncentered) covariance matrix?
?x(i)(j, k) ?
Rp?p, and a distance?dspectral(j, k).Algorithm:(Covariance estimation) ?i ?
[N ], j, k ?
`(x(i))?
Let Cj?,k?|i?
= w(i?)j?(w(i?)k?
)>, kj,k,j?,k?,i,i?=K?
(j, k, j?, k?|x(i),x(i?))
and `i?= `(x(i?
)),and estimate each p?
p covariance matrix as:?
?x(j, k) =?Ni?=1?`i?j?=1?`i?k?=1kj,k,j?,k?,i,i?Cj?,k?|i??Ni?=1?`i?j?=1?`i?k?=1kj,k,j?,k?,i,i??
Compute?dspectral(j, k) ?j, k ?
`(x(i)) usingEq.
5.
(Uncover structure) ?i ?
[N ]?
Find u?
(i)= arg minu?Uc?
(u), and for the ithexample, return the structure hdir(u?
(i)).with respect to the true distribution that generatesthe words in a sentence, then u can be fully recov-ered by optimizing the cost function c(u).
How-ever, in practice the distance metric must be esti-mated from data, as discussed below.3.4 Estimation of d from Sparse DataWe now address the data sparsity problem, in par-ticular that D(x) can be very small, and thereforeestimating d for each POS sequence separately canbe problematic.3In order to estimate d from data, we need to es-timate the covariance matrices ?x(i, j) (for i, j ?
{1, .
.
.
, `(x)}) from Eq.
5.To give some motivation to our solu-tion, consider estimating the covariancematrix ?x(1, 2) for the tag sequencex = (DT1, NN2, VBD3, DT4, NN5).
D(x) maybe insufficient for an accurate empirical es-3This data sparsity problem is quite severe ?
for example,the Penn treebank (Marcus et al, 1993) has a total numberof 43,498 sentences, with 42,246 unique POS tag sequences,averaging |D(x)| to be 1.04.1067timate.
However, consider another sequencex?= (RB1, DT2, NN3, VBD4, DT5, ADJ6, NN7).Although x and x?are not identical, it is likelythat ?x?
(2, 3) is similar to ?x(1, 2) because thedeterminer and the noun appear in similar syn-tactic context.
?x?
(5, 7) also may be somewhatsimilar, but ?x?
(2, 7) should not be very similarto ?x(1, 2) because the noun and the determinerappear in a different syntactic context.The observation that the covariance matricesdepend on local syntactic context is the main driv-ing force behind our solution.
The local syntacticcontext acts as an ?anchor,?
which enhances or re-places a word index in a sentence with local syn-tactic context.
More formally, an anchor is a func-tion G that maps a word index j and a sequence ofPOS tags x to a local context G(j,x).
The anchorwe use is G(j,x) = (j, xj).
Then, the covariancematrices ?x are estimated using kernel smooth-ing (Hastie et al, 2009), where the smoother testssimilarity between the different anchors G(j,x).The full learning algorithm is given in Figure 1.The first step in the algorithm is to estimate thecovariance matrix block?
?x(i)(j, k) for each train-ing example x(i)and each pair of preterminal po-sitions (j, k) in x(i).
Instead of computing thisblock by computing the empirical covariance ma-trix for positions (j, k) in the data D(x), the al-gorithm uses all of the pairs (j?, k?)
from all ofN training examples.
It averages the empiricalcovariance matrices from these contexts using akernel weight, which gives a similarity measurefor the position (j, k) in x(i)and (j?, k?)
in an-other example x(i?).
?
is the kernel ?bandwidth?,a user-specified parameter that controls how in-clusive the kernel will be with respect to exam-ples in D (see ?
4.1 for a concrete example).
Notethat the learning algorithm is such that it ensuresthat?
?x(i)(j, k) =??x(i?
)(j?, k?)
if G(j,x(i)) =G(j?,x(i?))
and G(k,x(i)) = G(k?,x(i?
)).Once the empirical estimates for the covariancematrices are obtained, a variant of the Eisner-Sattaalgorithm is used, as mentioned in ?3.3.3.5 Theoretical GuaranteesOur main theoretical guarantee is that Algorithm 1will recover the correct tree u ?
U with high prob-ability, if the given top bracket is correct and ifwe obtain enough examples (w(i),x(i)) from themodel in ?2.
We give the theorem statement be-low.
The constants lurking in the O-notation andthe full proof are in the supplementary.Denote ?x(j, k)(r)as the rthsingu-lar value of ?x(j, k).
Let ??
(x) :=minj,k?`(x) min(?x(j, k)(m)).Theorem 1 Define u?
as the estimated tree for tagsequence x and u(x) as the correct tree.
Let4(x) := minu?
?U :u?6=u(x)(c(u(x))?
c(u?
))/(8|`(x)|)Assume thatN ?
O??m2log(p2`(x)2?)min(??
(x)24(x)2, ??(x)2)?x(?)2?
?Then with probability 1?
?, u?
= u(x).where ?x(?
), defined in the supplementary, is afunction of the underlying distribution over the tagsequences x and the kernel bandwidth ?.Thus, the sample complexity of our approachdepends on the dimensionality of the latent andobserved states (m and p), the underlying singu-lar values of the cross-covariance matrices (??
(x))and the difference in the cost of the true tree com-pared to the cost of the incorrect trees (4(x)).4 ExperimentsWe report results on three different languages: En-glish, German, and Chinese.
For English we usethe Penn treebank (Marcus et al, 1993), with sec-tions 2?21 for training and section 23 for finaltesting.
For German and Chinese we use the Ne-gra treebank and the Chinese treebank respectivelyand the first 80% of the sentences are used fortraining and the last 20% for testing.
All punc-tuation from the data is removed.4We primarily compare our method to theconstituent-context model (CCM) of Klein andManning (2002).
We also compare our method tothe algorithm of Seginer (2007).4.1 Experimental SettingsTop bracket heuristic Our algorithm requiresthe top bracket in order to direct the latent tree.In practice, we employ the following heuristic tofind the bracket using the following three steps:?
If there exists a comma/semicolon/colon at in-dex i that has at least a verb before i and botha noun followed by a verb after i, then return([0, i ?
1], [i, `(x)]) as the top bracket.
(Pickthe rightmost comma/semicolon/colon if multi-ple satisfy the criterion).4We make brief use of punctuation for our top bracketheuristic detailed below before removing it.1068Length CCM CCM-U CCM-OB CCM-UB?
10 72.5 57.1 58.2 62.9?
15 54.1 36 24 23.7?
20 50 34.7 19.3 19.1?
25 47.2 30.7 16.8 16.6?
30 44.8 29.6 15.3 15.2?
40 26.3 13.5 13.9 13.8Table 1: Comparison of different CCM variants on English(training).
U stands for universal POS tagset, OB stands forconjoining original POS tags with Brown clusters and UBstands for conjoining universal POS tags with Brown clusters.The best setting is just the vanilla setting, CCM.?
Otherwise find the first non-participle verb (sayat index j) and return ([0, j ?
1], [j, `(x)]).?
If no verb exists, return ([0, 1], [1, `(x)]).Word embeddings As mentioned earlier, eachwican be an arbitrary feature vector.
For all lan-guages we use Brown clustering (Brown et al,1992) to construct a log(C) + C feature vectorwhere the first log(C) elements indicate whichmergable cluster the word belongs to, and the lastC elements indicate the cluster identity.
For En-glish, more sophisticated word embeddings areeasily obtainable, and we experiment with neuralword embeddings Turian et al (2010) of length50.
We also explored two types of CCA embed-dings: OSCCA and TSCCA, given in Dhillon etal.
(2012).
The OSCCA embeddings behaved bet-ter, so we only report its results.Choice of kernel For our experiments, we usethe kernelK?
(j, k, j?, k?|x,x?
)= max{0, 1??
(j, k, j?, k?|x,x?)?
}where ?
denotes the user-specified bandwidth,and ?
(j, k, j?, k?|x,x?)
=|j ?
k| ?
|j??
k?||j ?
k|+ |j??
k?|ifx(j) = x(j?)
and x(k?)
= x(k), and sign(j ?k) = sign(j??
k?)
(and?
otherwise).The kernel is non-zero if and only if the tags atposition j and k in x are identical to the ones inposition j?and k?in x?, and if the direction be-tween j and k is identical to the one between j?and k?.
Note that the kernel is not binary, as op-posed to the theoretical kernel in the supplemen-tary material.
Our experiments show that using anon-zero value different than 1 that is a functionof the distance between j and k compared to thedistance between j?and k?does better in practice.Choice of data For CCM, we found that if thefull dataset (all sentence lengths) is used in train-ing, then performance degrades when evaluatingon sentences of length ?
10.
We therefore restrictthe data used with CCM to sentences of length?
`, where ` is the maximal sentence length beingevaluated.
This does not happen with our algo-rithm, which manages to leverage lexical informa-tion whenever more data is available.
We thereforeuse the full data for our method for all lengths.We also experimented with the original POStags and the universal POS tags of Petrov et al(2011).
Here, we found out that our methoddoes better with the universal part of speech tags.For CCM, we also experimented with the origi-nal parts of speech, universal tags (CCM-U), thecross-product of the original parts of speech withthe Brown clusters (CCM-OB), and the cross-product of the universal tags with the Brown clus-ters (CCM-UB).
The results in Table 1 indicatethat the vanilla setting is the best for CCM.Thus, for all results, we use universal tags forour method and the original POS tags for CCM.We believe that our approach substitutes the needfor fine-grained POS tags with the lexical informa-tion.
CCM, on the other hand, is fully unlexical-ized.Parameter Selection Our method requires twoparameters, the latent dimension m and the band-width ?.
CCM also has two parameters, the num-ber of extra constituent/distituent counts used forsmoothing.
For both methods we chose the bestparameters for sentences of length ` ?
10 on theEnglish Penn Treebank (training) and used thisset for all other experiments.
This resulted inm = 7, ?
= 0.4 for our method and 2, 8 forCCM?s extra constituent/distituent counts respec-tively.
We also tried letting CCM choose differ-ent hyperparameters for different sentence lengthsbased on dev-set likelihood, but this gave worseresults than holding them fixed.4.2 ResultsTest I: Accuracy Table 2 summarizes our re-sults.
CCM is used with the initializer proposedin Klein and Manning (2002).5NN, CC, and BCindicate the performance of our method for neuralembeddings, CCA embeddings, and Brown clus-tering respectively, using the heuristic for hdirde-5We used the implementation available athttp://tinyurl.com/lhwk5n6.1069` English German ChineseNN-O NN CC-O CC BC-O BC CCM BC-O BC CCM BC-O BC CCMtrain?
10 70.9 69.2 70.4 68.7 71.1 69.3 72.5 64.6 59.9 62.6 64.9 57.3 46.1?
20 55.1 53.5 53.2 51.6 53.0 51.5 50 52.7 48.7 47.9 51.4 46 22.4?
40 46.1 44.5 43.6 41.9 43.3 41.8 26.3 46.7 43.6 19.8 42.6 38.6 15test?
10 69.2 66.7 68.3 65.5 68.9 66.1 70.5 66.4 61.6 64.7 58.0 53.2 40.7?
15 60.3 58.3 58.6 56.4 58.6 56.5 53.8 57.5 53.5 49.6 54.3 49.4 35.9?
20 54.1 52.3 52.3 50.3 51.9 50.2 50.4 52.8 49.2 48.9 49.7 45.5 20.1?
25 50.8 49.0 48.6 46.6 48.3 46.6 47.4 50.0 46.8 45.6 46.7 42.7 17.8?
30 48.1 46.3 45.6 43.7 45.4 43.8 44.9 48.3 45.4 21.9 44.6 40.7 16.1?
40 45.5 43.8 43.0 41.1 42.7 41.1 26.1 46.9 44.1 20.1 42.2 38.6 14.3Table 2: F1bracketing measure for the test sets and train sets in three languages.
NN, CC, and BC indicate the performance ofour method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for hdirdescribedin ?
4.1.
NN-O, CC-O, and BC-O indicate that the oracle (i.e.
true top bracket) was used for hdir.051 01 52 02 53 03 520- 30 31- 40 41- 50 51- 60 61- 70 71- 80FrequencyBracketing F1CCM Random Restarts (Length <= 10)Figure 4: Histogram showing performance of CCM across100 random restarts for sentences of length ?
10.scribed in ?
4.1.
NN-O, CC-O, and BC-O indicatethat the oracle (i.e.
true top bracket) was used forhdir.
For our method, test set results can be ob-tained by using Algorithm 1 (except the distancesare computed using the training data).For English, while CCM behaves better forshort sentences (` ?
10), our algorithm is morerobust with longer sentences.
This is especiallynoticeable for length ?
40, where CCM breaksdown and our algorithm is more stable.
We findthat the neural embeddings modestly outperformthe CCA and Brown cluster embeddings.The results for German are similar, except CCMbreaks down earlier at sentences of ` ?
30.
ForChinese, our method substantially outperformsCCM for all lengths.
Note that CCM performsvery poorly, obtaining only around 20% accu-racy even for sentences of ` ?
20.
We didn?thave neural embeddings for German and Chinese(which worked best for English) and thus onlyused Brown cluster embeddings.For English, the disparity between NN-O (ora-cle top bracket) and NN (heuristic top bracket) israther low suggesting that our top bracket heuris-tic is rather effective.
However, for German andChinese note that the ?BC-O?
performs substan-tially better, suggesting that if we had a better topbracket heuristic our performance would increase.Test II: Sensitivity to initialization The EM al-gorithm with the CCM requires very careful ini-tialization, which is described in Klein and Man-ning (2002).
If, on the other hand, random ini-tialization is used, the variance of the performanceof the CCM varies greatly.
Figure 4 shows a his-togram of the performance level for sentences oflength ?
10 for different random initializers.
Asone can see, for some restarts, CCM obtains ac-curacies lower than 30% due to local optima.
Ourmethod does not suffer from local optima and thusdoes not require careful initialization.Test III: Comparison to Seginer?s algorithmOur approach is not directly comparable toSeginer?s because he uses punctuation, while weuse POS tags.
Using Seginer?s parser we wereable to get results on the training sets.
On English:75.2% (` ?
10), 64.2% (` ?
20), 56.7% (` ?
40).On German: 57.8% (` ?
10), 45.0% (` ?
20), and39.9% (` ?
40).
On Chinese: 56.6% (` ?
10),45.1% (` ?
20), and 38.9% (` ?
40).Thus, while Seginer?s method performs betteron English, our approach performs 2-3 points bet-ter on German, and both methods give similar per-formance on Chinese.5 ConclusionWe described a spectral approach for unsu-pervised constituent parsing that comes withtheoretical guarantees on latent structure recovery.Empirically, our algorithm performs favorably tothe CCM of Klein and Manning (2002) withoutthe need for careful initialization.Acknowledgements: This work is supportedby NSF IIS1218282, NSF IIS1111142, NIHR01GM093156, and the NSF Graduate ResearchFellowship Program under Grant No.
0946825(NSF Fellowship to APP).1070ReferencesA.
Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade,L.
Song, and T. Zhang.
2011.
Spectral methodsfor learning multivariate latent tree structure.
arXivpreprint arXiv:1107.1283.R.
Bailly, F. Denis, and L. Ralaivola.
2009.
Gram-matical inference as a principal component analysisproblem.
In Proceedings of ICML.R.
Bailly, X. Carreras, F. M. Luque, and A. Quattoni.2013.
Unsupervised spectral learning of WCFGas low-rank matrix completion.
In Proceedings ofEMNLP.P.
F. Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra,and J.C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational linguistics,18(4):467?479.O.
P. Buneman.
1971.
The recovery of trees from mea-sures of dissimilarity.
Mathematics in the archaeo-logical and historical sciences.P.
Buneman.
1974.
A note on the metric properties oftrees.
Journal of Combinatorial Theory, Series B,17(1):48?50.M.J.
Choi, V. YF Tan, A. Anandkumar, and A.S. Will-sky.
2011.
Learning latent tree graphical mod-els.
The Journal of Machine Learning Research,12:1771?1812.C.
K. Chow and C. N. Liu.
1968.
ApproximatingDiscrete Probability Distributions With DependenceTrees.
IEEE Transactions on Information Theory,IT-14:462?467.S.
B. Cohen and N. A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in un-supervised grammar induction.
In Proceedings ofHLT-NAACL.S.
B. Cohen and N. A. Smith.
2012.
Empirical riskminimization for probabilistic grammars: Samplecomplexity and hardness of learning.
Computa-tional Linguistics, 38(3):479?526.S.
B. Cohen, K. Stratos, M. Collins, D. P. Foster, andL.
Ungar.
2012.
Spectral learning of latent-variablePCFGs.
In Proceedings of ACL.R.
Desper and O. Gascuel.
2005.
The minimum evo-lution distance-based approach to phylogenetic in-ference.
Mathematics of evolution and phylogeny,pages 1?32.P.
S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar.2012.
Two step cca: A new spectral method for es-timating vector models of words.
In Proceedings ofICML.J.
Eisner and G. Satta.
1999.
Efficient parsing forbilexical context-free grammars and head automatongrammars.
In Proceedings of ACL.P.
Erd?os, M. Steel, L. Sz?ekely, and T. Warnow.
1999.A few logs suffice to build (almost) all trees: Part ii.Theoretical Computer Science, 221(1):77?118.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, andB.
Taskar.
2010.
Sparsity in dependency grammarinduction.
In Proceedings of ACL.K.
Gimpel and N.A.
Smith.
2012.
Concavity and ini-tialization for unsupervised dependency parsing.
InProceedings of NAACL.D.
Golland, J. DeNero, and J. Uszkoreit.
2012.
Afeature-rich constituent context model for grammarinduction.
In Proceedings of ACL.M.
Gormley and J. Eisner.
2013.
Nonconvex globaloptimization for latent-variable models.
In Proceed-ings of ACL.S.
Harmeling and C. KI Williams.
2011.
Greedylearning of binary latent trees.
Pattern Analysisand Machine Intelligence, IEEE Transactions on,33(6):1087?1097.T.
Hastie, R. Tibshirani, and J. Friedman.
2009.
TheElements of Statistical Learning: Data Mining, In-ference, and Prediction.
Springer Series in Statis-tics.
Springer Verlag.W.
P. Headden, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing withricher contexts and smoothing.
In Proceedings ofNAACL-HLT.D.
Hsu, S. Kakade, and T. Zhang.
2009.
A spectralalgorithm for learning hidden Markov models.
InProceedings of COLT.D.
Hsu, S. M. Kakade, and P. Liang.
2012.
Identi-fiability and unmixing of latent parse trees.
arXivpreprint arXiv:1206.3137.M.
Ishteva, H. Park, and L. Song.
2012.
Unfoldinglatent tree structures using 4th order tensors.
arXivpreprint arXiv:1210.1258.F.
Jelinek, J. D. Lafferty, and R. L. Mercer.
1992.
Ba-sic methods of probabilistic context free grammars.Springer.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proceedings of ACL.M.
Kolar, A. P. Parikh, and E. P. Xing.
2010a.
Onsparse nonparametric conditional covariance selec-tion.
In Proceedings of ICML.M.
Kolar, L. Song, A. Ahmed, and E. P. Xing.
2010b.Estimating time-varying networks.
The Annals ofApplied Statistics, 4(1):94?123.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguis-tics, 19:313?330.1071A.P.
Parikh, L. Song, and E.P.
Xing.
2011.
A spectralalgorithm for latent tree graphical models.
In Pro-ceedings of ICML.S.
Petrov, D. Das, and R. McDonald.
2011.
A univer-sal part-of-speech tagset.
ArXiv:1104.2086.A.
Rzhetsky and M. Nei.
1993.
Theoretical founda-tion of the minimum-evolution method of phyloge-netic inference.
Molecular Biology and Evolution,10(5):1073?1095.N.
Saitou and M. Nei.
1987.
The neighbor-joiningmethod: a new method for reconstructing phylo-genetic trees.
Molecular biology and evolution,4(4):406?425.Y.
Seginer.
2007.
Fast unsupervised incremental pars-ing.
In Proceedings of ACL.N.
A. Smith and J. Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.In Proceedings of ACL.L.
Song, A.P.
Parikh, and E.P.
Xing.
2011.
Kernelembeddings of latent tree graphical models.
In Pro-ceedings of NIPS.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010a.From baby steps to leapfrog: how less is more inunsupervised dependency parsing.
In Proceedingsof NAACL.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.Manning.
2010b.
Viterbi training improves un-supervised dependency parsing.
In Proceedings ofCoNLL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2013.Breaking out of local optima with count transformsand model recombination: A study in grammar in-duction.
In Proceedings of EMNLP.J.
P. Turian, L.-A.
Ratinov, and Y. Bengio.
2010.
Wordrepresentations: A simple and general method forsemi-supervised learning.
In Proceedings of ACL.S.
Zhou, J. Lafferty, and L. Wasserman.
2010.
Timevarying undirected graphs.
Machine Learning,80(2-3):295?319.1072
