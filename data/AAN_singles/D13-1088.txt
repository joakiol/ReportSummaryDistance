Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879?883,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsRussian Stress Prediction using Maximum Entropy RankingKeith Hall Richard SproatGoogle, IncNew York, NY, USA{kbhall,rws}@google.comAbstractWe explore a model of stress predictionin Russian using a combination of lo-cal contextual features and linguistically-motivated features associated with theword?s stem and suffix.
We frame thisas a ranking problem, where the objec-tive is to rank the pronunciation with thecorrect stress above those with incorrectstress.
We train our models using a simpleMaximum Entropy ranking framework al-lowing for efficient prediction.
An empir-ical evaluation shows that a model com-bining the local contextual features andthe linguistically-motivated non-local fea-tures performs best in identifying bothprimary and secondary stress.1 IntroductionIn many languages, one component of accu-rate word pronunciation prediction is predict-ing the placement of lexical stress.
While insome languages (e.g.
Spanish) the lexical stresssystem is relatively simple, in others (e.g.
En-glish, Russian) stress prediction is quite compli-cated.
Much as with other work on pronuncia-tion prediction, previous work on stress assign-ment has fallen into two camps, namely systemsbased on linguistically motivated rules (Church,1985, for example) and more recently data-driven techniques where the models are deriveddirectly from labeled training data (Dou et al2009).
In this work, we present a machine-learned system for predicting Russian stresswhich incorporates both data-driven contextualfeatures as well as linguistically-motivated wordfeatures.2 Previous Work on StressPredictionPronunciation prediction, of which stress pre-diction is a part, is important for many speechapplications including automatic speech recog-nition, text-to-speech synthesis, and translit-eration for, say, machine translation.
Whilethere is by now a sizable literature on pro-nunciation prediction from spelling (oftentermed ?grapheme-to-phoneme?
conversion),work that specifically focuses on stress predic-tion is more limited.
One of the best-knownearly pieces of work is (Church, 1985), whichuses morphological rules and stress patterntemplates to predict stress in novel words.
An-other early piece of work is (Williams, 1987).The work we present here is closer in spirit todata-driven approaches such as (Webster, 2004;Pearson et al 2000) and particularly (Dou etal., 2009), whose features we use in the workdescribed below.3 Russian Stress PatternsRussian stress preserves many features of Indo-European accenting patterns (Halle, 1997).
Inorder to know the stress of a morphologicallycomplex word consisting of a stem plus a suf-fix, one needs to know if the stem has an accent,and if so on what syllable; and similarly for thesuffix.
For words where the stem is accented,879acc unacc postaccDat Sg ???'???
?'?????
?????
'?gor?oxu g?orodu korolj?uDat Pl ???'????
?????'??
?????'?
?gor?oxam gorod?am korolj?am?pea?
?town?
?king?Table 1: Examples of accented, unaccented andpostaccented nouns in Russian, for dative singularand plural forms.this accent overrides any accent that may oc-cur on the suffix.
With unaccented stems, ifthe suffix has an accent, then stress for thewhole word will be on the suffix; if there isalso no stress on the suffix, then a default ruleplaces stress on the first syllable of the word.In addition to these patterns, there are alsopostaccented words, where accent is placed uni-formly on the first syllable of the suffix ?
aninnovation of East and South Slavic languages(Halle, 1997).
These latter cases can be handledby assigning an accent to the stem, indicatingthat it is associated with the syllable after thestem.
Some examples of each of these classes,from (Halle, 1997, example 11), are given inTable 1.
According to Halle (1997), consid-ering just nouns, 91.6% are accented (on thestem), 6.6% are postaccented and 0.8% are un-accented, with about 1.0% falling into otherpatterns.Stress placement in Russian is important forspeech applications since over and above thephonetic effects of stress itself (prominence, du-ration, etc.
), the position of stress strongly in-fluences vowel quality.
To take an exampleof the lexically unaccented noun ?????
gorod?city?, the genitive singular ?'?????
g?oroda/g"Or@d@/ contrasts with the nominative plural?????'?
gorod?a /g@r2d"a/.
All non-stressed/a/ are reduced to schwa ?
or by most ac-counts if before the stressed syllable to /2/; see(Wade, 1992).The stress patterns of Russian suggest thatuseful features for predicting stress might in-clude (string) prefix and suffix features of theword in order to capture properties of the stem,since some stems are (un)accented, or of thesuffix, since some suffixes are accented.4 Maximum Entropy RankersSimilarly to Dou et al(2009), we frame thestress prediction problem as a ranking problem.For each word, we identify stressable vowels andgenerate a set of alternatives, each represent-ing a different primary stress placement.
Somewords also have secondary stress which, if it oc-curs, always occurs before the primary stressedsyllable.
For each primary stress alternative,we generate all possible secondary stressed al-ternatives, including an alternative that has nosecondary stress.
(In the experiments reportedbelow we actually consider two conditions: onewhere we ignore secondary stress in trainingand evaluation; and one where we include it.
)Formally, we model the problem using a Max-imum Entropy ranking framework similar tothat presented in Collins and Koo (2005).
Foreach example, xi, we generate the set of possiblestress patterns Yi.
Our goal is to rank the itemsin Yi such that all of the valid stress patternsY?i are above all of the invalid stress patterns.Our objective function is the likelihood, L ofthis conditional distribution:L =?ip(Y?i |Yi, xi) (1)logL =?ilog p(Y?i |Yi, xi) (2)=?ilog?y?
?Y?ie?k ?kfk(y?,x)Z(3)Z is defined as the sum of the conditional like-lihood over all hypothesized stress predictionsfor example xi:Z =?y??
?Yie?k ?kfk(y?
?,x) (4)The objective function in Equation 3 can beoptimized using a gradient-based optimization.In our case, we use a variety of stochastic gra-dient descent (SGD) which can be parallelizedfor efficient training.During training, we provide all plausibly cor-rect primary stress patterns as the positive set880Y?i .
At prediction-time, we evaluate all possi-ble stress predictions and pick the one with thehighest score under the trained model ?:argmaxy?
?Yip(y?|Yi) = argmaxy?
?Yi?k?kfk(y?, x) (5)The primary motivation for using MaximumEntropy rather the ranking-SVM is for efficienttraining and inference.
Under the above Max-imum Entropy model, we apply a linear modelto each hypothesis (i.e., we compute the dot-product) and sort according to this score.
Thismakes inference (prediction) fast in comparisonto the ranking SVM-based approach proposedin Dou et al(2009).All experiments presented in this paper usedthe Iterative Parameter Mixtures distributedSGD training optimizer (Hall et al 2010).
Un-der this training approach, per-iteration aver-aging has a regularization-like effect for sparsefeature spaces.
We also experimented with L1-regularization, but it offered no additional im-provements.5 FeaturesThe features used in (Dou et al 2009) arebased on trigrams consisting of a vowel letter,the preceding consonant letter (if any) and thefollowing consonant letter (if any).
Attachedto each trigram is the stress level of the tri-gram?s vowel ?
1, 2 or 0 (for no stress).
Forthe English word overdo with the stress pattern2-0-1, the basic features would be ov:2, ver:0,and do:1.
Notating these pairs as si : ti, wheresi is the triple, ti is the stress pattern and i isthe position in the word, the complete featureset is given in Table 2, where the stress pat-tern for the whole word is given in the last rowas t1t2...tN .
Dou and colleagues use an SVM-based ranking approach, so they generated fea-tures for all possible stress assignments for eachword, assigning the highest rank to the correctassignment.
The ranker was then trained toassociate feature combinations to the correctranking of alternative stress possibilities.Given the discussion in Section 3, plausibleadditional features are all prefixes and suffixesSubstring si, tisi, i, tiContext si1, tisi1si, tisi+1, tisisi+1, tisi1sisi+1, tiStress Pattern t1t2...tNTable 2: Features used in (Dou et al 2009, Table 2).vowel ?,?,?,?,?,?,?,?,?stop ?,?,?,?,?,?nasal ?,?fricative ?,?,?,?,?,?,?hard/soft ?,?yo ?semivowel ?,?liquid ?,?affricate ?,?Table 3: Abstract phonetic classes used for con-structing ?abstract?
versions of a word.
Note thatetymologically, and in some ways phonologically, ?v behaves like a semivowel in Russian.of the word, which might be expected to bettercapture some of the properties of Russian stresspatterns discussed above, than the much morelocal features from (Dou et al 2009).
In thiscase for all stress variants of the word we collectprefixes of length 1 through the length of theword, and similarly for suffixes, except that forthe stress symbol we treat that together withthe vowel it marks as a single symbol.
Thus forthe word gorod?a, all prefixes of the word wouldbe g, go, gor, goro, gorod, gorod?a.In addition, we include prefixes and suffixesof an ?abstract?
version of the word where mostconsonants and vowels have been replaced bya phonetic class.
The mappings for these areshown in Table 3.Note that in Russian the vowel ?
/jO/ is al-ways stressed, but is rarely written in text: itis usually spelled as ?, whose stressed pronun-cation is /(j)E/.
Since written ?
is in generalambiguous between ?
and ?, when we computestress variants of a word for the purpose of rank-881ing, we include both variants that have ?
and?.6 DataOur data were 2,004,044 fully inflected wordswith assigned stress expanded from Zaliznyak?sGrammatical Dictionary of the Russian Lan-guage (Zaliznyak, 1977).
These were split ran-domly into 1,904,044 training examples and100,000 test examples.
The 100,000 test ex-amples obviously contain no forms that werefound in the training data, but most of themare word forms that derive from lemmata fromwhich some training data forms are also de-rived.
Given the fact that Russian stress is lex-ically determined as outlined in Section 3, thisis perfectly reasonable: in order to know howto stress a form, it is often necessary to haveseen other words that share the same lemma.Nonetheless, it is also of interest to know howwell the system works on words that do notshare any lemmata with words in the trainingdata.
To that end, we collected a set of 248forms that shared no lemmata with the train-ing data.
The two sets will be referred to in thenext section as the ?shared lemmata?
and ?noshared lemmata?
sets.7 ResultsTable 4 gives word accuracy results for the dif-ferent feature combinations, as follows: Dou etal?s features (Dou et al 2009); our affix fea-tures; our affix features plus affix features basedon the abstract phonetic class versions of words;Dou et al features plus our affix features; Douet al features plus our affix features plus theabstract affix features.When we consider only primary stress (col-umn 2 in Table 4, for the shared-lemmata testdata, Dou et al features performed the worstat 97.2% accuracy, with all feature combina-tions that include the affix features performingat the same level, 98.7%.
For the no-shared-lemmata test data, using Dou et al featuresalone achieved an accuracy of 80.6%.
The affixfeatures alone performed worse, at 79.8%, pre-sumably because it is harder for them to gener-Features 1 stress 1+2 stressshared lemmataDou et al.972 0.965Aff 0.987 0.985Aff+Abstr Aff 0.987 0.985Dou et alff 0.987 0.986Dou et alff+Abstr Aff 0.987 0.986no shared lemmataDou et al.806 0.798Aff 0.798 0.782Aff+Abstr 0.810 0.790Dou et alff 0.823 0.810Dou et alff+Abstr Aff 0.839 0.815Table 4: Word accuracies for various feature combi-nations for both shared lemmata and no-shared lem-mata conditions.
The second column reports resultswhere we consider only primary stress, the third col-umn results where we also predict secondary stress.alize to unseen cases, but using the abstract af-fix features increased the performance to 81.0%,better than that of using Dou et al featuresalone.
As can be seen combining Dou et alfeatures with various combinations of the affixfeatures improved the performance further.For primary and secondary stress prediction(column 3 in the table), the results are over-all degraded for most conditions but otherwisevery similar in terms of ranking of the fea-tures to what we find with primary stress alone.Note though that for the shared-lemmata con-dition the results with affix features are almostas good as for the primary-stress-only case,whereas there is a significant drop in perfor-mance for the Dou et alfeatures.
For theno-shared-lemmata condition, Dou et als fea-tures fare rather better compared to the affixfeatures.
On the other hand there is a sub-stantial benefit to combining the features, asthe results for ?Dou et alff?
and ?Dou etal+Aff+Abstr Aff?
show.
Note that in theno-shared-lemmata condition, there is only oneword that is marked with a secondary stress,and that stress is actually correctly predictedby all methods.
Much of the difference betweenthe Dou et alfeatures and the affix conditioncan be accounted for by three cases involvingthe same root, which the affix condition misas-882signs secondary stress to.For the shared-lemmata task however therewere a substantial number of differences, asone might expect given the nature of the fea-tures.
Comparing just the Dou et alfea-tures and the all-features condition, system-atic benefit for the all-features condition wasfound for secondary stress assignment for pro-ductive prefixes where secondary stress is typ-ically found.
For example, the prefix ????(?aero-?)
as in ?`???????'????
(?aerodynam-ics?)
typically has secondary stress.
This is usu-ally missed by the Dou et alfeatures, but isuniformly correct for the all-features condition.Since the no-shared-lemmata data set issmall, we tested significance using two permu-tation tests.
The first computed a distribu-tion of scores for the test data where succes-sive single test examples were removed.
Thesecond randomly permuted the test data 248times, after each random permutation, remov-ing the first ten examples, and computing thescore.
Pairwise t-tests between all conditionsfor the primary-stress-only and for the primaryplus secondary stress predictions, were highlysignificant in all cases.We also experimented with a postaccent fea-ture to model the postaccented class of nounsdescribed in Section 3.
For each prefix of theword, we record whether the following vowelis stressed or unstressed.
This feature yieldedonly very slight improvements, and we do notreport these results here.8 DiscussionIn this paper we have presented a MaximumEntropy ranking-based approach to Russianstress prediction.
The approach is similar inspirit to the SVM-based ranking approach pre-sented in (Dou et al 2009), but incorporatesadditional affix-based features, which are moti-vated by linguistic analyses of the problem.
Wehave shown that these additional features gen-eralize better than the Dou et alfeatures incases where we have seen a related form of thetest word, and that combing the additional fea-tures with the Dou et alfeatures always yieldsan improvement.ReferencesKenneth Church.
1985.
Stress assignment in letterto sound rules for speech synthesis.
In Associ-ation for Computational Linguistics, pages 246?253.Michael Collins and Terry Koo.
2005.
Discrim-inative reranking for natural language parsing.Computational Linguistics, 31:25?69, March.Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,and Grzegorz Kondrak.
2009.
A ranking ap-proach to stress prediction for letter-to-phonemeconversion.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Nat-ural Language Processing of the AFNLP, pages118?126, Suntec, Singapore, August.
Associationfor Computational Linguistics.Keith B.
Hall, Scott Gilpin, and Gideon Mann.2010.
Mapreduce/bigtable for distributed opti-mization.
In Neural Information Processing Sys-tems Workshop on Leaning on Cores, Clusters,and Clouds.Morris Halle.
1997.
On stress and accent in Indo-European.
Language, 73(2):275?313.Steve Pearson, Roland Kuhn, Steven Fincke, andNick Kibre.
2000.
Automatic methods for lexicalstress assignment and syllabification.
In Interna-tional Conference on Spoken Language Process-ing, pages 423?426.Terence Wade.
1992.
A Comprehensive RussianGrammar.
Blackwell, Oxford.Gabriel Webster.
2004.
Improving letter-to-pronunciation accuracy with automaticmorphologically-based stress prediction.
InInternational Conference on Spoken LanguageProcessing, pages 2573?2576.Briony Williams.
1987.
Word stress assignmentin a text-to-speech synthesis system for BritishEnglish.
Computer Speech and Language, 2:235?272.Andrey Zaliznyak.
1977.
Grammaticheskij slovar?russkogo jazyka.
Russkiy Yazik, Moscow.883
