Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 126?135,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLow-Rank Regularization for Sparse Conjunctive Feature Spaces:An Application to Named Entity ClassificationAudi PrimadhantyUniversitat Polit`ecnica de Catalunyaprimadhanty@cs.upc.eduXavier Carreras Ariadna QuattoniXerox Research Centre Europexavier.carreras@xrce.xerox.comariadna.quattoni@xrce.xerox.comAbstractEntity classification, like many otherimportant problems in NLP, involveslearning classifiers over sparse high-dimensional feature spaces that resultfrom the conjunction of elementary fea-tures of the entity mention and its context.In this paper we develop a low-rank reg-ularization framework for training max-entropy models in such sparse conjunctivefeature spaces.
Our approach handles con-junctive feature spaces using matrices andinduces an implicit low-dimensional rep-resentation via low-rank constraints.
Weshow that when learning entity classifiersunder minimal supervision, using a seedset, our approach is more effective in con-trolling model capacity than standard tech-niques for linear classifiers.1 IntroductionMany important problems in NLP involve learn-ing classifiers over sparse high-dimensional fea-ture spaces that result from the conjunction of el-ementary features.
For example, to classify an en-tity in a document, it is standard to exploit featuresof the left and right context in which the entity oc-curs as well as spelling features of the entity men-tion itself.
These sets of features can be groupedinto vectors which we call elementary feature vec-tors.
In our example, there will be one elementaryfeature vector for the left context, one for the rightcontext and one for the features of the mention.Observe that, when the elementary vectors consistof binary indicator features, the outer product ofany pair of vectors represents all conjunctions ofthe corresponding elementary features.Ideally, we would like to train a classifier thatcan leverage all conjunctions of elementary fea-tures, since among them there might be somethat are discriminative for the classification task athand.
However, allowing for such expressive highdimensional feature space comes at a cost: datasparsity becomes a key challenge and controllingthe capacity of the model is crucial to avoid over-fitting the training data.The problem of data sparsity is even more se-vere when the goal is to train classifiers with min-imal supervision, i.e.
small training sets.
For ex-ample, in the entity classification setting we mightbe interested in training a classifier using only asmall set of examples of each entity class.
Thisis a typical scenario in an industrial setting, wheredevelopers are interested in classifying entities ac-cording to their own classification schema and canonly provide a handful of examples of each class.A standard approach to control the capacity of alinear classifier is to use `1or `2regularization onthe parameter vector.
However, this type of regu-larization does not seem to be effective when deal-ing with sparse conjunctive feature spaces.
Themain limitation is that `1and `2regularization cannot let the model give weight to conjunctions thathave not been observed at training.
Without suchability it is unlikely that the model will generalizeto novel examples, where most of the conjunctionswill be unseen in the training set.Of course, one could impose a strong prior onthe weight vector so that it assigns weight to un-seen conjunctions, but how can we build such aprior?
What kind of reasonable constraints can weput on unseen conjunctions?Another common approach to handle high di-mensional conjunctive feature spaces is to manu-ally design the feature function so that it includes126only a subset of ?relevant?
conjunctions.
But de-signing such a feature function can be time con-suming and one might need to design a new fea-ture function for each classification task.
Ide-ally, we would have a learning algorithm that doesnot require such feature engineering and that itcan automatically leverage rich conjunctive fea-ture spaces.In this paper we present a solution to this prob-lem by developing a regularization frameworkspecifically designed for sparse conjunctive fea-ture spaces.
Our approach results in a more effec-tive way of controlling model capacity and it doesnot require feature engineering.Our strategy is based on:?
Employing tensors to define the scoring func-tion of a max-entropy model as a multilinearform that computes weighted inner productsbetween elementary vectors.?
Forcing the model to induce low-dimensionalembeddings of elementary vectors via low-rank regularization on the tensor parameters.The proposed regularization framework is basedon a simple conceptual trick.
The standard ap-proach to handle conjunctive feature spaces inNLP is to regard the parameters of the linearmodel as long vectors computing an inner prod-uct with a high dimensional feature representationthat lists explicitly all possible conjunctions.
In-stead, the parameters of our the model will be ten-sors and the compatibility score between an inputpattern and a class will be defined as the sum ofmultilinear functions over elementary vectors.We then show that the rank1of the tensor has avery natural interpretation.
It can be seen as theintrinsic dimensionality of a latent embedding ofthe elementary feature vectors.
Thus by impos-ing a low-rank penalty on the tensor parameterswe are encouraging the model to induce a low-dimensional projection of the elementary featurevectors .
Using the rank itself as a regularizationconstraint in the learning algorithm would resultin a non-convex optimization.
Instead, we followa standard approach which is to use the nuclearnorm as a convex relaxation of the rank.In summary the main contributions of this paperare:1There are many ways of defining the rank of a tensor.
Inthis paper we matricize tensors into matrices and use the rankof the resulting matrix.
Matricization is also referred to asunfolding.?
We develop a new regularization frame-work for training max-entropy models inhigh-dimensional sparse conjunctive featurespaces.
Since the proposed regularization im-plicitly induces a low dimensional embed-ding of feature vectors, our algorithm canalso be seen as a way of implicitly learninga latent variable model.?
We present a simple convex learning al-gorithm for training the parameters of themodel.?
We conduct experiments on learning entityclassifiers with minimal supervision.
Our re-sults show that the proposed regularizationframework is better for sparse conjunctivefeature spaces than standard `2and `1reg-ularization.
These results make us concludethat encouraging the max-entropy model tooperate on a low-dimensional space is an ef-fective way of controlling the capacity of themodel an ensure good generalization.2 Entity Classification with Log-linearModelsThe formulation we develop in this paper appliesto any prediction task whose inputs are some formof tuple.
We focus on classification of entity men-tions, or entities in the context of a sentence.
For-mally, our input objects are tuples x = ?l, e, r?consisting of an entity e, a left context l and a rightcontext r. The goal is to classify x into one entityclass in the set Y .We will use log-linear models of the form:Pr(y | x; ?)
=exp{s?
(x, y)}?y?exp{s?
(x, y?
)}(1)where s?
: X ?
Y ?
R is a scoring function ofentity tuples with a candidate class, and ?
are theparameters of this function, to be specified below.In the literature it is common to employ afeature-based linear model.
That is, one defines afeature function ?
: X ?
{0, 1}nthat representsentity tuples in an n-dimensional binary featurespace2, and the model has a weight vector for eachclass, ?
= {wy}y?Y.
Then s?
(x, y) = ?
(x) ?wy.2In general, all models in this paper accept real-valuedfeature functions.
But we focus on binary indicator featuresbecause in practice these are the standard type of features inNLP classifiers, and the ones we use here.
In fact, in this pa-per we develop feature spaces based on products of elemen-tary feature functions, in which case the resulting representa-tions correspond to conjunctions of the elementary features.1273 Low-rank Entity Classification ModelsIn this section we propose a specific family ofmodels for classifying entity tuples.3.1 A Low-rank Model of Left-RightContextsWe start from the observation that when repre-senting tuple objects such as x = ?l, e, r?
withfeatures, we often depart from a feature represen-tation of each element of the tuple.
Hence, let?land ?rbe two feature functions representingleft and right contexts, with binary dimensions d1and d2respectively.
For now, we will define amodel that ignores the entity mention e and makespredictions using context features.
It is naturalto define conjunctions of left and right features.Hence, in its most general form, one can definea matrix Wy?
Rd1?d2for each class, such that?
= {Wy}y?Yand the score is:s?
(?l, e, r?, y) = ?l(l)>Wy?r(r) .
(2)Note that this corresponds to a feature-basedlinear model operating in the product space of ?land ?r, that is, the score has one term for each pairof features:?i,j?l(l)[i] ?r(r)[j] Wy[i, j].
Notealso that it is trivial to include elementary featuresof ?land ?r, in addition to conjunctions, by havinga constant dimension in each of the two represen-tations set to 1.In all, the model in Eq.
(2) is very expressive,with the caveat that it can easily overfit the data,specially when we work only with a handful of la-beled examples.
The standard way to control thecapacity of a linear model is via `1or `2regular-ization.Regarding our parameters as matrices allows usto control the capacity of the model via regulariz-ers that favor parameter matrices with low rank.To see the effect of these regularizers, considerthat Wyhas rank k, and let Wy= Uy?yV>ybe the singular value decomposition, where Uy?Rd1?kand Vy?
Rd2?kare orthonormal projec-tions and ?y?
Rk?kis a diagonal matrix of sin-gular values.
We can rewrite the score function ass?
(?l, e, r?, y) = (?l(l)>Uy) ?y(V>y?r(r)) .
(3)In words, the rank k is the intrinsic dimensionalityof the inner product behind the score function.
Alow-rank regularizer will favor parameter matricesthat have low intrinsic dimensionality.
Below wedescribe a convex optimization for low-rank mod-els using nuclear norm regularization.3.2 Adding Entity FeaturesThe model above classifies entities based only onthe context.
Here we propose an extension to makeuse of features of the entity.
Let T be a set of pos-sible entity feature tags, i.e.
tags that describe anentity, such as ISCAPITALIZED, CONTAINSDIG-ITS, SINGLETOKEN, .
.
.
Let ?ebe a feature func-tion representing entities.
For this case, to simplifyour expression, we will use a set notation and de-note by ?e(e) ?
T the set of feature tags that de-scribe e. Our model will be defined with one pa-rameter matrix per feature tag and class label, i.e.?
= {Wt,y}t?T ,y?Y.
The model form is:s?
(?l, e, r?, y) =?t??e(e)?l(l)>Wt,y.?r(r).
(4)3.3 Learning with Low-rank ConstraintsIn this section we describe a convex procedure tolearn models of the above form that have low rank.We will define an objective that combines a lossand a regularization term.Our first observation is that our parameters area tensor with up to four axes, namely left and rightcontext representations, entity features, and entityclasses.
While a matrix has a clear definition ofrank, it is not the case for general tensors, andthere exist various definitions in the literature.
Thetechnique that we use is based on matricization ofthe tensor, that is, turning the tensor into a matrixthat has the same parameters as the tensor but or-ganized in two axes.
This is done by partitioningthe tensor axes into two sets, one for matrix rowsand another for columns.
Once the tensor has beenturned into a matrix, we can use the standard def-inition of matrix rank.
A main advantage of thisapproach is that we can make use of standard rou-tines like singular value decomposition (SVD) todecompose the matricized tensor.
This is the mainreason behind our choice.In general, different ways of partitioning thetensor axes will lead to different notions of intrin-sic dimensions.
In our case we choose the left con-text axes as the row dimension, and the rest of axesas the column dimension.3In this section, we will3In preliminary experiments we tried variations, such ashaving right prefixes in the columns, and left prefixes, entitytags and classes in the rows.
We only observer minor, non-significant variations in the results.128denote as W the matricized version of the param-eters ?
of our models.The second observation is that minimizing therank of a matrix is a non-convex problem.
Wemake use of a convex relaxation based on the nu-clear norm (Srebro and Shraibman, 2005).
Thenuclear norm4of a matrix W, denoted ?W?
?, isthe sum of its singular values: ?W?
?=?i?i,iwhere W = U?V>is the singular value decom-position of W. This norm has been used in severalapplications in machine learning as a convex sur-rogate for imposing low rank, e.g.
(Srebro et al,2004).Thus, the nuclear norm is used as a regularizer.With this, we define our objective as follows:argminWL(W) + ?R(W) , (5)where L(W) is a convex loss function,R(W) is aregularizer, and ?
is a constant that trades off errorand capacity.
In experiments we will compare nu-clear norm regularization with `1and `2regulariz-ers.
In all cases we use the negative log-likelihoodas loss function, denoting the training data as D:L(W) =?(?l,e,r?,y)?D?
log Pr(y | ?l, e, r?
;W) .
(6)To solve the objective in Eq.
(5) we use a simpleoptimization scheme known as forward-backwardsplitting (FOBOS) (Duchi and Singer, 2009).
Ina series of iterations, this algorithm performs agradient update followed by a proximal projec-tion of the parameters.
Such projection dependson the regularizer used: for `1it thresholds the pa-rameters; for `2it scales them; and for nuclear-norm regularization it thresholds the singular val-ues.
This means that, for nuclear norm regulariza-tion, each iteration requires to decompose W us-ing SVD.
See (Madhyastha et al, 2014) for detailsabout this optimization for a related application.4 Related WorkThe main aspect of our approach is the use ofa spectral penalty (i.e., the rank) to control thecapacity of multilinear functions parameterizedby matrices or tensors.
Quattoni et al (2014)used nuclear-norm regularization to learn latent-variable max-margin sequence taggers.
Mad-hyastha et al (2014) defined bilexical distribu-4Also known as the trace norm.tions parameterized by matrices which result lex-ical embeddings tailored for a particular linguis-tic relation.
Like in our case, the low-dimensionallatent projections in these papers are learned im-plicitly by imposing low-rank constraints on thepredictions of the model.Lei et al (2014) also use low-rank tensor learn-ing in the context of dependency parsing, wherelike in our case dependencies are represented byconjunctive feature spaces.
While the motivationis similar, their technical solution is different.
Weuse the technique of matricization of a tensor com-bined with a nuclear-norm relaxation to obtain aconvex learning procedure.
In their case they ex-plicitly look for a low-dimensional factorization ofthe tensor using a greedy alternating optimization.Also recently, Yao et al (2013) have framedentity classification as a low-rank matrix comple-tion problem.
The idea is based on the fact that iftwo entities (in rows) have similar descriptions (incolumns) they should have similar classes.
Thelow-rank structure of the matrix defines intrin-sic representations of entities and feature descrip-tions.
The same idea was applied to relation ex-traction (Riedel et al, 2013), using a matrix ofentity pairs times descriptions that corresponds toa matricization of an entity-entity-description ten-sor.
Very recently Singh et al (2015) explored al-ternative ways of applying low-rank constraints totensor-based relation extraction.Another aspect of this paper is training entityclassification models using minimal supervision,which has been addressed by multiple works inthe literature.
A classical successful approachfor this problem is to use co-training (Blum andMitchell, 1998): learn two classifiers that use dif-ferent views of the data by using each other?s pre-dictions.
In the same line, Collins and Singer(1999) trained entity classifiers by bootstrapingfrom an initial set of seeds, using a boosting ver-sion of co-training.
Seed sets have also been ex-ploited by graphical model approaches.
Haghighiand Klein (2006) define a graphical model that issoft-constrained such that the prediction for an un-labeled example agrees with the labels of seedsthat are distributionally similar.
Li et al (2010)present a Bayesian approach to expand an initialseed set, with the goal of creating a gazetteer.Another approach to entity recognition that, likein our case, learns projections of contextual fea-tures is the method by Ando and Zhang (2005).129Class Nb Mentions10-30 Seed 10-30 40-120 640-1920 AllPER clinton, dole, arafat, yeltsin, wasim akram, lebed, dutroux, waqar you-nis, mushtaq ahmed, croft334 747 3,133 6,516LOC u.s., england, germany, britain, australia, france, spain, pakistan, italy,china1,384 2,885 5,812 6,159ORG reuters, u.n., oakland, puk, osce, cincinnati, eu, nato, ajax, honda 295 699 3,435 5,271MISC russian, german, british, french, dutch, english, israeli, european, iraqi,australian611 1326 3,085 3,205O year, percent, thursday, government, police, results, tuesday, soccer,president, monday, friday, people, minister, sunday, division, week,time, state, market, years, officials, group, company, saturday, match,at, world, home, august, standings5,326 11,595 31,071 36,673Table 1: For each entity class, the seed of entities for the 10-30 set, together with the number of mentionsin the training data that involve entities in the seed for various sizes of the seeds.They define a set of auxiliary tasks, which can besupervised using unlabeled data, and find a projec-tion of the data that works well as input represen-tation for the auxiliary tasks.
This representationis then used for the target task.More recently Neelakantan and Collins (2014)presented another approach to gazetteer expansionusing an initial seed.
A novel aspect is the useof Canonical Correlation Analysis (CCA) to com-pute embeddings of entity contexts, that are usedby the named entity classifier.
Like in our case,their method learns a compressed representationof contexts that helps prediction.5 ExperimentsIn this section we evaluate our regulariza-tion framework for training models in high-dimensional sparse conjunctive feature spaces.
Werun experiments on learning entity classifiers withminimal supervision.
We focus on classification ofunseen entities to highlight the ability of the reg-ularizer to generalize over conjunctions that arenot observed at training.
We simulate minimalsupervision using the CoNLL-2003 Shared Taskdata (Tjong Kim Sang and De Meulder, 2003), andcompare the performance to `1and `2regularizers.5.1 Minimal Supervision TaskWe use a minimal supervision setting where weprovide the algorithm a seed of entities for eachclass, that is, a list of entities that is representativefor that class.
The assumption is that any men-tion of an entity in the seed is a positive examplefor the corresponding class.
Given unlabeled dataand a seed of entities for each class, the goal isto learn a model that correctly classifies mentionsof entities that are not in the seed.
In addition tostandard entity classes, we also consider a specialnon-entity class, which is part of the classificationbut is excluded from evaluation.Note that named entity classification for unseenentities is a challenging problem.
Even in the stan-dard fully-supervised scenario, when we measurethe performance of state-of-the-art methods on un-seen entities, the F1 values are in the range of 60%.This represents a significant drop with respect tothe standard metrics for named entity recognition,which consider all entity mentions of the test setirrespective of whether they appear in the trainingdata or not, and where F1 values at 90% levels areobtained (e.g.
(Ratinov and Roth, 2009)).
Thissuggests that part of the success of state-of-the-artmodels is in storing known entities together withtheir type (in the form of gazetteers or directly inlexicalized parameters of the model).5.2 SettingWe use the CoNLL-2003 English data, which isannotated with four types: person (PER), location(LOC), organization (ORG), and miscellaneous(MISC).
In addition, the data is tagged with parts-of-speech (PoS), and we compute word clustersrunning the Brown clustering algorithm (Brown etal., 1992) on the words in the training set.We consider annotated entity phrases as candi-date entities, and all single nouns that are not partof an entity as candidate non-entities (O).
Bothcandidate entities and non-entities will be referredto as candidates in the remaining of this section.We lowercase all candidates and remove the am-130Features WindowBag-of-words N-gramsLexical Cluster Lexical ClusterElementary features of left and right contexts1 13.63 14.59 13.63 14.592 15.49 13.86 13.08 13.543 12.18 14.45 12.14 13.28Only full conjunctions of left and right contexts1 12.90 13.75 12.90 13.752 8.59 8.85 12.31 12.433 8.57 10.59 10.15 10.49Elementary features and all conjunctions of left and right contexts1 15.30 16.98 15.30 16.982 13.26 12.89 14.28 15.333 11.87 11.54 13.94 13.15Table 2: Average-F1 of classification of unseen entity candidates on development data, using the 10-30training seed and `2regularization, for different conjunctive spaces (elementary only, full conjunctions,all).
Bag-of-words elementary features contain all clusters/PoS in separate windows to the left and tothe right of the candidate.
N-grams elementary features contain all n-grams of clusters/PoS in separateleft and right windows (e.g.
for size 3 it includes unigrams, bigrams and trigrams on each side).biguous ones (i.e., those with more than one labelin different mentions).5To simulate a minimal supervision, we createsupervision seeds by picking the n most frequenttraining candidates for entity types, and the mmost frequent candidate non-entities.
We createseeds of various sizes n-m, namely 10-30, 40-120,640-1920, as well as all of the candidates.
Foreach seed, the training set consists of all trainingmentions that involve entities in the seed.
Table 1shows the smaller seed, as well as the number ofmentions for each seed size.For evaluation we use the development and testsections of the data, but we remove the instancesof candidates in the training data (i.e., that are inthe all seed).
We do not remove instances that areambiguous in the tests.6As evaluation metric weuse the average F1 score computed over all entitytypes, excluding the non-entity type.5In the CoNLL-2003 English training set, only 235 can-didates are ambiguous out of 13,441 candidates, i.e.
less than2%.
This suggests that in this data the difficulty behind thetask is in recognizing and classifying unseen entities, and notin disambiguating known entities in a certain context.6After removing the ambiguous candidates from the train-ing data, and removing candidates seen in the training fromthe development and test sets, this is the number of mentions(and number of unique candidates in parenthesis) in the dataused in our experiments:training dev.
testPER 6,516 (3,489) 1,040 (762) 1,342 (925)LOC 6,159 ( 987) 176 (128) 246 (160)ORG 5,271 (2,149) 400 (273) 638 (358)MISC 3,205 ( 760) 177 (142) 213 (152)O 36,673 (5,821) 951 (671) 995 (675)5.3 Context RepresentationsWe refer to context as the sequence of tokens be-fore (left context) and after (right context) a can-didate mention in a sentence.
Different classifierscan be built using different representations of thecontexts.
For example we can change the windowsize of the context sequence (i.e., for a windowsize of 1 we only use the last token before the men-tion and the first token after the mention).
We cantreat the left and right contexts independently ofeach other, we can treat them as a unique combi-nation, or we can use both.
We can also choose touse the word form of a token, its PoS tag, a wordcluster, or a combination of these.Table 2 compares different context represen-tations and their performance in classifying un-seen candidates using maximum-entropy classi-fiers trained with Mallet (McCallum, 2002) with`2regularization, using the 10-30 seed.
We usethe lexical representation (the word itself) and aword cluster representation of the context tokensand use a window size of one to three.
We usetwo types of features: bag-of-words features (1-grams of tokens in the specified window) and n-gram features (with n smaller or equal to the win-dow size).
The performance of using word clustersis comparable, and sometimes better, to using lexi-cal representations.
Moreover, using a longer win-dow, in this case, does not necessarily result in bet-ter performance.7In the rest of the experiments7Our learner and feature configuration, using `2regular-ization, obtains state-of-the-art results on the standard evalu-13110-30 40-120 640-1920 All0406014.2317.5827.4128.8814.1217.7227.1828.5425.1128.2538.342.81Seed setAVG-F1(%)L1L2NN(a) Only full conjunctions of left-right contexts (cluster),window size = 110-30 40-120 640-1920 All0406017.3920.0532.7338.0117.420.7232.7638.9528.933.6742.7253.65Seed setAVG-F1(%)L1L2NN(b) Only full conjunctions of entity tags and left-right contexts(cluster), window size = 110-30 40-120 640-1920 All06028.5725.339.5245.0428.4825.9140.4544.236.874156.1660.94Seed setAVG-F1(%)L1L2NN(c) Elementary features and all conjunctions of entity tags andleft-right contexts (cluster), window size = 110-30 40-120 640-1920 All06028.3324.4139.9544.0328.9224.6240.5444.0935.5837.4556.7459.46Seed setAVG-F1(%)L1L2NN(d) Elementary features and all conjunctions of entity tags andleft-right contexts (cluster), window size = 210-30 40-120 640-1920 All0206030.2127.0539.7444.6729.6727.0739.8445.1637.141.1457.761.13Seed setAVG-F1(%)L1L2NN(e) Elementary features and all conjunctions of entity tags andleft-right contexts (cluster & PoS), window size = 110-30 40-120 640-1920 All06028.2128.0437.2142.8328.6527.6239.3444.138.2237.1154.5658.43Seed setAVG-F1(%)L1L2NN(f) Elementary features and all conjunctions of entity tags andleft-right contexts (cluster & PoS), window size = 2Figure 1: Average F1 of classification of unseen entity candidates on development data, with respect tothe size of the seed.
NN refers to models with nuclear norm regularization, L1 and L2 refer to `1and`2regularization.
Each plot corresponds to a different conjunctive feature space with respect to windowsize (1 or 2), context representation (cluster with/out PoS), using entity features or not, and combiningor not full conjunctions with lower-order conjunctions and elementary features.?
cap=1, cap=0: whether the first letter of the entity candidate is uppercase, or not?
all-low=1, all-low=0: whether all letters of the candidate are lowercase letters, or not?
all-cap1=1, all-cap1=0: whether all letters of the candidate are uppercase letters, or not?
all-cap2=1, all-cap2=0: whether all letters of the candidate are uppercase letters and periods, or not?
num-tokens=1, num-tokens=2, num-tok>2: whether the candidate consists of one token, two or more?
dummy: a tag that holds for any entity candidate, used to capture context features aloneTable 3: The 12 entity tags used to represent entity candidates.
The tags all-cap1 and all-cap2 are from(Neelakantan and Collins, 2014).132PER LOC ORG MISCAVGF1PREC REC F1 PREC REC F1 PREC REC F1 PREC REC F110-30`165.69 65.40 65.55 15.38 23.58 18.62 59.33 19.44 29.28 23.36 30.05 26.28 34.93`165.54 64.80 65.17 15.12 23.17 18.30 60.82 18.50 28.37 23.30 30.52 26.42 34.56NN 72.41 74.52 73.45 14.89 21.55 17.61 49.09 21.16 17.61 31.40 38.03 34.40 38.7640-120`172.16 44.07 54.72 13.38 40.24 20.08 48.89 31.19 38.09 22.03 35.68 27.24 35.03`271.75 44.89 55.23 13.61 41.87 20.54 49.39 31.50 38.47 21.64 30.99 25.48 34.93NN 75.16 61.33 67.54 13.08 20.73 16.04 49.03 35.74 41.34 29.97 47.42 36.73 40.41640-1920 `179.52 62.27 69.85 23.59 44.31 30.79 55.78 47.65 51.39 19.81 30.05 23.88 43.98`278.62 65.55 71.49 26.55 43.50 32.97 60.19 49.06 54.06 21.73 31.92 25.86 46.10NN 80.73 80.55 80.64 51.91 44.31 47.81 53.82 54.08 53.95 29.14 51.17 37.14 54.88All`175.58 72.48 74.00 32.84 36.18 34.43 57.28 46.24 51.17 27.93 29.11 28.51 47.03`276.59 70.77 73.57 34.21 36.99 35.55 57.79 50.00 53.61 28.93 32.86 30.77 48.37NN 73.83 90.84 81.46 64.96 36.18 46.48 72.11 44.98 55.41 37.20 43.66 40.17 55.88Table 4: Results on the test for models trained with different sizes of the seed, using the parametersand features that obtain the best evaluation results the development set.
NN refers to nuclear normregularization, L1 and L2 refer to `1and `2regularization.
Only test entities unseen at training areconsidered.
Avg.
F1 is over PER, LOC, ORG and MISC, excluding O.we will use the elementary features that are morepredictive and compact: clusters and PoS tags inwindows of size at most 2.5.4 Comparing RegularizersWe compare the performance of models trainedusing the nuclear norm regularizer with modelstrained using `1and `2regularizers.
To train eachmodel, we validate the regularization parameterand the number of iterations on development data,trying a wide range of values.
The best performingconfiguration is then used for the comparison.Figure 1 shows results on the development setfor different feature sets.
We started representingcontext using cluster labels, as it is the most com-pact representation obtaining good results in pre-liminary experiments.
We tried several conjunc-tions: a conjunction of the left and right context,as well as conjunctions of left and right contextsand features of the candidate entity.
We also triedall different conjunction combinations of the con-texts and the candidate entity features, as well asadding PoS tags to represent contexts.
To repre-sent an entity candidate we use standard traits ofthe spelling of the mention, such as capitalization,ation.
Using our richest feature set, the model obtains 76.76of accuracy in the development, for the task of classifing enti-ties with correct boundaries.
If we add features capturing thefull entity and its tokens, then the accuracy is 87.63, whichis similar to state-of-the-art performance (the best results inliterature typically exploit additional gazetteers).
Since ourevaluation focuses on unknown entities, our features do notinclude information about the word tokens of entites.1 2 3 4 5 678 9 10203040506070020406028.2243.9350.1954.2458.1461.7361.13dimensionAVG-F1(%)Figure 2: Avg.
F1 on development for increasingdimensions, using the low-rank model in Figure 1etrained with all seeds.the existence of symbols, as well as the number oftokens in the candidate.
See Table 3 for the defini-tion of the features describing entity candidates.We observe that for most conjunction settingsour regularizer performs better than the `1and`2regularizers.
Using the best model from eachregularizer, we evaluated on the test set.
Table4 shows the test results.
For all seed sets, thenuclear norm regularizer obtains the best aver-age F1 performance.
This shows that encourag-ing the max-entropy model to operate on a low-dimensional space is effective.
Moreover, Figure2 shows model performance as a function of thenumber of dimensions of the intrinsic projection.The model obtains a good performance even ifonly a few intrinsic dimensions are used.Figure 3 shows the parameter matrix of the low-133O PER LOC ORG MISCClusterPoS(a) Full parameter matrix of the low-rank model.
The ticks in x-axis indicate the space for different entity types, while the ticksin y-axis indicate the space for different prefix context representations.all?cap1=1 all?cap1=0 all?cap2=1 all?cap2=0 all?low=1 all?low=0 nb?tok=1 nb?tok=2 nb?tok>2 cap=1 cap=0 dummyprefix=NNP(b) The subblock for PER entity type and PoS representation of the prefixes.
The ticks in x-axis indicate the space of the entityfeatures used, while the tick in y-axis indicates an example of a frequently observed prefix for this entity type.Figure 3: Parameter matrix of the low-rank model in Figure 1f trained with the 10-30 seed, with respect toobservations of the associated features in training and development.
Non-white conjunctions correspondto non-zero weights: black is for conjunctions seen in both the training and development sets; blue is forthose seen in training but not in the development; red indicates that the conjunctions were observed onlyin the development; yellow is for those not observed in training nor development.rank model in Figure 1f trained with the 10-30seed, with respect to observed features in trainingand development data.
Many of the conjunctionsof the development set were never observed in thetraining set.
Our regularizer framework is able topropagate weights from the conjunctive featuresseen in training to unseen conjunctive features thatare close to each other in the projected space (theseare the yellow and red cells in the matrix).
In con-trast, `1and `2regularization techniques can notput weight on unseen conjunctions.6 ConclusionWe have developed a low-rank regularizationframework for training max-entropy models insparse conjunctive feature spaces.
Our formula-tion is based on using tensors to parameterize clas-sifiers.
We control the capacity of the model usingthe nuclear-norm of a matricization of the tensor.Overall, our formulation results in a convex proce-dure for training model parameters.We have experimented with these techniques inthe context of learning entity classifiers.
Com-pared to `1and `2penalties, the low-rank modelobtains better performance, without the need tomanually specify feature conjunctions.
In ouranalysis, we have illustrated how the low-rank ap-proach can assign non-zero weights to conjunc-tions that were unobserved at training, but are sim-ilar to observed conjunctions with respect to thelow-dimensional projection of their elements.We have used matricization of a tensor to defineits rank, using a fixed transformation of the tensorinto a matrix.
Future work should explore how tocombine efficiently different transformations.AcknowledgementsWe thank Gabriele Musillo and the anonymous re-viewers for their helpful comments and sugges-tions.
This work has been partially funded by theSpanish Government through the SKATER project(TIN2012-38584-C06-01) and an FPI predoctoralgrant for Audi Primadhanty.134ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
J. Mach.
Learn.
Res.,6:1817?1853, December.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of the Eleventh Annual Conference onComputational Learning Theory, COLT?
98, pages92?100, New York, NY, USA.
ACM.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
InJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora.John Duchi and Yoram Singer.
2009.
Efficient onlineand batch learning using forward backward splitting.Journal of Machine Learning Research, 10:2899?2934.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Main Conference on Human Language Tech-nology Conference of the North American Chap-ter of the Association of Computational Linguistics,HLT-NAACL ?06, pages 320?327, Stroudsburg, PA,USA.
Association for Computational Linguistics.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1381?1391, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Xiao-Li Li, Lei Zhang, Bing Liu, and See-KiongNg.
2010.
Distributional similarity vs. pu learn-ing for entity set expansion.
In Proceedings of theACL 2010 Conference Short Papers, ACLShort ?10,pages 359?364, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Pranava Swaroop Madhyastha, Xavier Carreras, andAriadna Quattoni.
2014.
Learning Task-specificBilexical Embeddings.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 161?171, Dublin, Ireland, August.
Dublin City Univer-sity and Association for Computational Linguistics.Andrew K. McCallum.
2002.
Mallet: A machinelearning for language toolkit.Arvind Neelakantan and Michael Collins.
2014.Learning dictionaries for named entity recognitionusing minimal supervision.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 452?461, Gothenburg, Sweden, April.
Association forComputational Linguistics.Ariadna Quattoni, Borja Balle, Xavier Carreras, andAmir Globerson.
2014.
Spectral regularization formax-margin sequence tagging.
In Tony Jebara andEric P. Xing, editors, Proceedings of the 31st Inter-national Conference on Machine Learning (ICML-14), pages 1710?1718.
JMLR Workshop and Con-ference Proceedings.Lev Ratinov and Dan Roth.
2009.
Design chal-lenges and misconceptions in named entity recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 147?155, Boulder, Colorado,June.
Association for Computational Linguistics.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 74?84, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Sameer Singh, Tim Rockt?aschel, and Sebastian Riedel.2015.
Towards Combined Matrix and Tensor Fac-torization for Universal Schema Relation Extraction.In NAACL Workshop on Vector Space Modeling forNLP (VSM).Nathan Srebro and Adi Shraibman.
2005.
Rank, trace-norm and max-norm.
In Learning Theory, pages545?560.
Springer Berlin Heidelberg.Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.2004.
Maximum-margin matrix factorization.
InAdvances in neural information processing systems,pages 1329?1336.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 SharedTask: Language-Independent Named Entity Recog-nition.
In Proceedings of the Seventh Conference onNatural Language Learning at HLT-NAACL 2003,pages 142?147.Limin Yao, Sebastian Riedel, and Andrew McCallum.2013.
Universal schema for entity type prediction.In Proceedings of the 2013 Workshop on AutomatedKnowledge Base Construction, AKBC ?13, pages79?84, New York, NY, USA.
ACM.135
