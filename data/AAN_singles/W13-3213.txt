Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 110?118,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsAnswer Extraction by Recursive Parse Tree DescentChristopher MalonNEC Laboratories America4 Independence WayPrinceton, NJ 08540malon@nec-labs.comBing BaiNEC Laboratories America4 Independence WayPrinceton, NJ 08540bbai@nec-labs.comAbstractWe develop a recursive neural network (RNN)to extract answers to arbitrary natural languagequestions from supporting sentences, by train-ing on a crowdsourced data set (to be releasedupon presentation).
The RNN defines featurerepresentations at every node of the parse treesof questions and supporting sentences, whenapplied recursively, starting with token vectorsfrom a neural probabilistic language model.
Incontrast to prior work, we fix neither the typesof the questions nor the forms of the answers;the system classifies tokens to match a sub-string chosen by the question?s author.Our classifier decides to follow each parse treenode of a support sentence or not, by classify-ing its RNN embedding together with those ofits siblings and the root node of the question,until reaching the tokens it selects as the an-swer.
A novel co-training task for the RNN,on subtree recognition, boosts performance,along with a scheme to consistently handlewords that are not well-represented in the lan-guage model.
On our data set, we surpass anopen source system epitomizing a classic ?pat-tern bootstrapping?
approach to question an-swering.1 IntroductionThe goal of this paper is to learn the syntax used to an-swer arbitrary natural language questions.
If the kindsof questions were fixed but the supporting sentenceswere open, this would be a kind of relation extraction orslot-filling.
If the questions were open but the support-ing information was encoded in a database, this wouldbe a kind of semantic parsing.In spite of many evaluation sets, no suitable data setfor learning to answer questions has existed before.Data sets such as TREC (Dang et al 2008) do notidentify supporting sentences or even answers unlessa competing system submitted an answer and a humanverified it.
Exceeding the capabilities of current sys-tems is difficult by training on such labels; any newlydiscovered answer is penalized as wrong.
The JeopardyArchive (Schmidt, 2013) offers more than 200,000 an-swer/question pairs, but no pointers to information thatsupports the solutions.Believing that it is impossible to learn to answerquestions, QA systems in TREC tended to measuresyntactic similarity between question and candidate an-swer, or to map the question into an enumerated set ofpossible question types.
For the pre-determined ques-tion types, learning could be achieved, not from the QAdata itself, but from pattern bootstrapping (Brin, 1998)or distant supervision against an ontology like Freebase(Mintz et al 2009).
These techniques lose precision;Riedel et al(2010) found the distant supervision as-sumption was violated on 31% of examples aligningFreebase relations to text from The New York Times.We introduce a new, crowdsourced dataset, TurkQA,to enable question answering to be learned.
TurkQAconsists of single sentences, each with several crowd-sourced questions.
The answer to each question isgiven as a substring of the supporting sentence.
Forexample,James Hervey (February 26, 1714 - De-cember 25, 1758), English divine, was bornat Hardingstone, near Northampton, andwas educated at the grammar school ofNorthampton, and at Lincoln College, Ox-ford.could have questions like ?Where did James Hervey at-tend school as a boy??
with answers like ?the grammarschool of Northampton.?
Our approach has yielded al-most 40,000 such questions, and easily scales to manymore.
Since the sentence containing the answer has al-ready been located, the machine?s output can be judgedwithout worrying about missing labels elsewhere in thecorpus.
Token-level ground truth forces the classifier toisolate the relevant information.To meet this challenge, we develop a classifier thatrecursively classifies nodes of the parse tree of a sup-porting sentence.
The positively classified nodes arefollowed down the tree, and any positively classifiedterminal nodes become the tokens in the answer.
Fea-ture representations are dense vectors in a continuousfeature space; for the terminal nodes, they are the wordvectors in a neural probabilistic language model (like(Bengio and Ducharme, 2001)), and for interior nodes,they are derived from children by recursive applicationof an autoencoder.110The contributions of this paper are: a data set forlearning to answer free-form questions; a top-downsupervised method using continuous word features inparse trees to find the answer; and a co-training taskfor training a recursive neural network that preservesdeep structural information.2 Related WorkMost submissions to TREC, TAC, and CLEF (Forneret al 2008) QA workshops rely on a large pipelineof modules, emphasizing feature development, patternbootstrapping, and distant supervision.
However, somerecent work has introduced new learning techniques forquestion answering.Restricting the forms of the questions, Poon andDomingos (2009) present a question-answering systemfor simple questions about biomedical text, by unsu-pervised semantic parsing (USP), using Markov logic.Because of its dependence on semantic role labeling(SRL), USP can only extract limited kinds of infor-mation from a sentence?s syntax.
Particularly, USPhas been programmed to use information from justfive dependencies: NN, AMOD, PREP_OF, NUM, andAPPOS.
The system handles only questions of twoforms: ?What VERB OBJ??
and ?What does SUBJVERB?
?Liang et al(2011) offers a compositional focus onsemantic parsing.
He has implemented a question-answering system for geography and job databases,learning to transform natural language questions intoSQL queries.
Liang is able to take many words as ver-batim analogues of table columns (e.g.
?city?
triggers asearch on a city column), but our task requires learningsuch associations in natural language (?city?
to a placenamed entity), and less attention to Boolean composi-tional semantics.We have not seen recursive neural networks (RNN)applied to QA yet, but Socher has developed applica-tions to paraphrase (Socher et al 2011a) and sentimentanalysis (Socher et al 2011b).
Relying either on dy-namic pooling or the root feature alone, these methodsdo not use the full information of the input graphs.3 TurkQA: a scalable, crowdsourceddata setThe TurkQA data set consists of 13,424 problem sets.Each problem set consists of the first sentence of aWikipedia article, which we call a support sentence,and four questions, written by workers from Ama-zon Mechanical Turk.1 (Occasionally, due to a faultyheuristic, two or three consecutive sentences at the be-ginning of the article are taken.)
Each of the four ques-tions is answered by a phrase in the support sentence, oryes/no.
At least two short answer questions must existin each problem set, and their answers are selected by1https://www.mturk.comtheir authors as contiguous, non-overlapping substringsof the support sentence.Over 600 workers contributed.
The quality of thequestions was ensured by rigorous constraints on theinput: no pronouns could be used; all words from thequestion had to be in the dictionary or the support sen-tence; the same phrase could not be used as the answerfor multiple questions.
We requested that anyone whounderstood English should be able to understand thequestion just by reading the support sentence, withoutany background knowledge.
As we took the supportsentences from the start of an article, references to priortext should not occur.At first we reviewed submissions by hand, but aswe found that 96% of the problem sets were accept-able (and a higher percentage of the questions), we ap-proved most submissions automatically.
Thus we ex-pect the data acquisition technique to be scalable witha budget.A possible drawback of our data acquisition is so-called back-formulation: a tendency of question writ-ers to closely match the syntax of the supporting sen-tence when writing questions.
This drawback was ob-served in TREC 8, and caused TREC organizers tochange data set construction for later conferences bystarting with questions input to a search engine, andthen localize supporting sentences, rather than startingwith the support (Voorhees, 2000).
In actuality, manyTurkQA question writers introduced their own word-ing and asked questions with more qualifications thana typical search engine query.
They even asked 100?why?
questions.4 Recursive neural networksIn their traditional form (Pollack, 1990), autoencodersconsist of two neural networks: an encoder E to com-press multiple input vectors into a single output vec-tor, and a decoder D to restore the inputs from thecompressed vector.
Through recursion, autoencodersallow single vectors to represent variable length datastructures.
Supposing each terminal node t of a rootedtree T has been assigned a feature vector ~x(t) ?
Rn,the encoder E is used to define n-dimensional featurevectors at all remaining nodes.
Assuming for simplic-ity that T is a binary tree, the encoder E takes theform E : Rn ?
Rn ?
Rn.
Given children c1 andc2 of a node p, the encoder assigns the representation~x(p) = E(~x(c1), ~x(c2)).
Applying this rule recur-sively defines vectors at every node of the tree.The decoder and encoder may be trained together tominimize reconstruction error, typically Euclidean dis-tance.
Applied to a set of trees T with features alreadyassigned at their terminal nodes, autoencoder trainingminimizes:Lae =?t?T?p?N(t)?ci?C(p)||~x?(ci)?
~x(ci)||, (1)where N(t) is the set of non-terminal nodes of tree111Algorithm 1: Auto-encoders co-trained for subtree recognition by stochastic gradient descentData: E : Rn ?
Rn ?
Rn a neural network (encoder)Data: S : Rn ?
Rn ?
R2 a neural network for binary classification (subtree or not)Data: D : Rn ?
Rn ?
Rn a neural network (decoder)Data: T a set of trees T with features ~x(t) assigned to terminal nodes t ?
TResult: Weights of E and D trained to minimize a combination of reconstruction and subtree recognitionerrorbeginwhile stopping criterion not satisfied doRandomly choose T ?
Tfor p in a postorder depth first traversal of T doif p is not terminal thenLet c1, c2 be the children of pCompute ~x(p) = E(~x(c1), ~x(c2))Let (~x?
(c1), ~x?
(c2)) = D(~x(p))Compute reconstruction loss LR = ||~x?(c1)?
~x(c1)||2 + ||~x?(c2)?
~x(c2)||2Choose a random q ?
T such that q is a descendant of pLet cq1, cq2 be the children of q, if they existCompute S(~x(p), ~x(q)) = S(E(~x(c1), ~x(c2)), E(~x(cq1), ~x(cq2)))Compute cross-entropy loss L1 = h(S(~x(p), ~x(q)), 1)if p is not the root of T thenChoose a random r ?
T such that r is not a descendant of pLet cr1, cr2 be the children of r, if they existCompute cross-entropy loss L2 = h(S(~x(p), ~x(r)), 0)elseLet L2 = 0Compute gradients of LR + L1 + L2 with respect to weights of E, D, and S, fixing ~x(c1),~x(c2), ~x(cq1), ~x(cq2), ~x(cr1), and ~x(cr2).Update parameters of E, D, and S by backpropagationt, C(p) = c1, c2 is the set of children of node p,and (~x?
(c1), ~x?
(c2)) = D(E(~x(c1), ~x(c2))).
This losscan be trained with stochastic gradient descent (Bottou,2004).However, there have been some perennial concernsabout autoencoders:1.
Is information lost after repeated recursion?2.
Does low reconstruction error actually keep the in-formation needed for classification?Socher attempted to address the first of these con-cerns in his work on paraphrase with deep unfoldingrecursive autoencoders (Socher et al 2011a), whereeach node is penalized for reconstruction errors manylevels down an input tree, not just the reconstruction ofits immediate descendants.
Beyond five levels, Socherobserved many word-choice errors on decoding in-put sentences.
Socher?s work on sentiment analysis(Socher et al 2011b) focused on the second concern,by co-training on desired sentence classification, alongwith the usual reconstruction objective, at every leveldown to the terminal nodes.
Of course, this had theside effect of imputing sentence-level sentiment labelsto words where it was not really relevant.As an alternative, we propose subtree recognitionas a semi-supervised co-training task for any recurrentneural network on tree structures.
This task can be de-fined just as generally as reconstruction error.
Whileaccepting that some information will be lost as we goup the tree, the co-training objective encourages the en-coder to produce representations that can answer basicquestions about the presence or absence of descendantsfar below.Subtree recognition is a binary classification prob-lem concerning two nodes x and y of a tree T ; we traina neural network S to predict whether y is a descen-dant of x.
The neural network S should produce twooutputs, corresponding to log probabilities that the de-scendant relation is satisfied.
In our experiments, wetake S (as we do E and D) to have one hidden layer.We train the outputs S(x, y) = (z0, z1) to minimize thecross-entropy functionh((z0, z1), j) = ?
log(ezjez0 + ez1)for j = 0, 1.
(2)so that z0 and z1 estimate log likelihoods that the de-scendant relation is satisfied.Our algorithm for training the subtree classifier ispresented in Algorithm 1.
We use SENNA software(Collobert et al 2011) to compute parse trees for sen-tences.
Training on a corpus of 64,421 Wikipedia sen-tences and testing on 20,160, we achieve a test error112rate of 3.2% on pairs of parse tree nodes that are sub-trees, for 6.9% on pairs that are not subtrees (F1 =.95), with .02 mean squared reconstruction error.5 Features for question and answer dataApplication of the recursive neural network begins withfeatures from the terminal nodes (the tokens).
Thesefeatures come from the language model of SENNA(Collobert et al 2011), the Semantic Extraction Neu-ral Network Architecture.
Originally, neural proba-bilistic language models associated words with learnedfeature vectors so that a neural network could predictthe joint probability function of word sequences (Ben-gio and Ducharme, 2001).
SENNA?s language modelis co-trained on many syntactic tagging tasks, witha semi-supervised task in which valid sentences areto be ranked above sentences with random word re-placements.
Through the ranking and tagging tasks,this model learned embeddings of each word in a 50-dimensional space.
Besides this learned representa-tions, we encode capitalization and SENNA?s predic-tions of named entity and part of speech tags with ran-dom vectors associated to each possible tag, as shownin Figure 1.
The dimensionality of these vectors is cho-sen roughly as the logarithm of the number of pos-sible tags.
Thus every terminal node obtains a 61-dimensional feature vector.We modify the basic RNN construction of Section 4to obtain features for interior nodes.
Since interior treenodes are tagged with a node type, we encode the pos-sible node types in a six-dimensional vector and makeE and D work on triples (ParentType, Child 1, Child2), instead of pairs (Child 1, Child 2).WordCapitalName EntityPOS x Parsing"The"x1: 261-dim vector "cat""sat"E ...x8: 261-dim vectorEx2: 261-dim vectorx11: 261-dim vectorEx10: 261-dim vector50 1 4 6200Padding 6-dimNPEncodersigmoidLinear2x261+62002002616-dimVP50 1 4 6200 ...LinearFigure 1: Recursive autoencoder to assign features tonodes of the parse tree of, ?The cat sat on the mat.
?Note that the node types (e.g.
?NP?
or ?VP?)
of internalnodes, and not just the children, are encoded.Also, parse trees are not necessarily binary, so we bi-narize by right-factoring.
Newly created internal nodesare labeled as ?SPLIT?
nodes.
For example, a nodewith children c1, c2, c3 is replaced by a new node withthe same label, with left child c1 and newly createdright child, labeled ?SPLIT,?
with children c2 and c3.Vectors from terminal nodes are padded with 200 ze-ros before they are input to the autoencoder.
We dothis so that interior parse tree nodes have more roomto encode the information about their children, as theoriginal 61 dimensions may already be filled with in-formation about just one word.The feature construction is identical for the questionand the support sentence.5.1 Modeling unknown wordsMany QA systems derive powerful features from exactword matches.
In our approach, we trust that the classi-fier will be able to match information from autoencoderfeatures of related parse tree branches, if it needs to.But our neural language probabilistic language modelis at a great disadvantage if its features cannot charac-terize words outside its original training set.Since Wikipedia is an encyclopedia, it is common forsupport sentences to introduce entities that do not ap-pear in the dictionary of 100,000 most common wordsfor which our language model has learned features.
Inthe support sentenceJean-Bedel Georges Bokassa, CrownPrince of Central Africa was born on the 2ndNovember 1975 the son of Emperor BokassaI of the Central African Empire and his wifeCatherine Denguiade, who became Empresson Bokassa?s accession to the throne.both Bokassa and Denguiade are uncommon, and donot have learned language model embeddings.
SENNAtypically replaces these words with a fixed vector asso-ciated with all unknown words, and this works fine forsyntactic tagging; the classifier learns to use the contextaround the unknown word.
However, in a question-answering setting, we may need to read Denguiadefrom a question and be able to match it with Den-guiade, not Bokassa, in the support.Thus we extend the language model vectors with arandom vector associated to each distinct word.
Therandom vectors are fixed for all the words in the orig-inal language model, but a new one is generated thefirst time any unknown word is read.
For known words,the original 50 dimensions give useful syntactic and se-mantic information.
For unknown words, the newly in-troduced dimensions facilitate word matching withoutdisrupting predictions based on the original 50.6 Convolutions inside treesWe extract answers from support sentences by classify-ing each token as a word to be included in the answer ornot.
Essentially, this decision is a tagging problem onthe support sentence, with additional features requiredfrom the question.Convolutional neural networks efficiently classifysequential (or multi-dimensional) data, with the abilityto reuse computations within a sliding frame tracking113NP) , ADJP ,English divineNPNP ( -NPJames Hervey February 26 , 1714 December 25 , 1758NPFigure 2: Labeling nodes to be followed to select ?December 25, 1758.?NP ) , ADJP ,P E NPngl igshdvhl e( , -a-m DgHgrsgh ey ,-Fe m y ( a b -u -- -ay2 -eF- F Fe 6 2 NP FF 174c58$-y %5n57-m m- -amu&E')&Fe &mu &E')&2 &mu &E')&FF &munFF( &E')&muu &E')&muum(NPFigure 3: Assembling features from the question, the parent, and siblings, to decide whether to follow node 33the item to be classified (Waibel et al 1989).
Con-volving over token sequences has achieved state-of-the-art performance in part-of-speech tagging, namedentity recognition, and chunking, and competitive per-formance in semantic role labeling and parsing, usingone basic architecture (Collobert et al 2011).
More-over, at classification time, the approach is 200 timesfaster at POS tagging than next-best systems such as(Shen et al 2007) and 100 times faster at semantic rolelabeling (Koomen et al 2005).Classifying tokens to answer questions involves notonly information from nearby tokens, but long rangesyntactic dependencies.
In most work utilizing parsetrees as input, a systematic description of the wholeparse tree has not been used.
Some state-of-the-artsemantic role labeling systems require multiple parsetrees (alternative candidates for parsing the same sen-tence) as input, but they measure many ad-hoc featuresdescribing path lengths, head words of prepositionalphrases, clause-based path features, etc., encoded in asparse feature vector (Pradhan et al 2005).By using feature representations from our RNN andperforming convolutions across siblings inside the tree,instead of token sequences in the text, we can utilizethe parse tree information in a more principled way.
Westart at the root of the parse tree and select branches tofollow, working down.
At each step, the entire questionis visible, via the representation at its root, and we de-cide whether or not to follow each branch of the supportsentence.
Ideally, irrelevant information will be cut atthe point where syntactic information indicates it is nolonger needed.
The point at which we reach a termi-nal node may be too late to cut out the correspondingword; the context that indicates it is the wrong answermay have been visible only at a higher level in the parsetree.
The classifier must cut words out earlier, thoughwe do not specify exactly where.Our classifier uses three pieces of information to de-cide whether to follow a node in the support sentenceor not, given that its parent was followed:1.
The representation of the question at its root2.
The representation of the support sentence at theparent of the current node3.
The representations of the current node and aframe of k of its siblings on each side, in the orderinduced by the order of words in the sentence114Algorithm 2: Training the convolutional neural network for question answeringData: ?, a set of triples (Q,S, T ), with Q a parse tree of a question, S a parse tree of a support sentence, andT ?
W(S) a ground truth answer substring, and parse tree features ~x(p) attached by the recursiveautoencoder for all p ?
Q or p ?
SLet n = dim ~x(p)Let h be the cross-entropy loss (equation (2))Data: ?
:(R3n)2k+1?
R2 a convolutional neural network over frames of size 2k + 1, with parameters to betrained for question-answeringResult: Parameters of ?
trainedbeginwhile stopping criterion not satisfied doRandomly choose (Q,S, T ) ?
?Let q, r = root(Q), root(S)Let X = {r} (the set of nodes to follow)Let A(T ) ?
S be the set of ancestor nodes of T in Swhile X 6= ?
doPop an element p from Xif p is not terminal thenLet c1, .
.
.
, cm be the children of pLet ~xj = ~x(cj) for j ?
{1, .
.
.
,m}Let ~xj = ~0 for j /?
{1, .
.
.
,m}for i=1, .
.
.
m doLet t = 1 if ci ?
A(T ), or 0 otherwiseLet vci = ?i+kj=i?k (~xj ?
~x(p)?
~x(q))Compute the cross-entropy loss h (?
(vci) , t)if exp(?h (?
(vci) , 1)) >12 thenLet X = X ?
{ci} (the network predicts ci should be followed)Update parameters of ?
by backpropagationEach of these representations is n-dimensional.
Theconvolutional neural network concatenates them to-gether (denoted by ?)
as a 3n-dimensional feature ateach node position, and considers a frame enclosingk siblings on each side of the current node.
The CNNconsists of a convolutional layer mapping the 3n inputsto an r-dimensional space, a sigmoid function (such astanh), a linear layer mapping the r-dimensional spaceto two outputs, and another sigmoid.
We take k = 2and r = 30 in the experiments.Application of the CNN begins with the children ofthe root, and proceeds in breadth first order throughthe children of the followed nodes.
Sliding the CNN?sframe across siblings allows it to decide whether to fol-low adjacent siblings faster than a non-convolutionalclassifier, where the decisions would be computedwithout exploiting the overlapping features.
A fol-lowed terminal node becomes part of the short answerof the system.The training of the question-answering convolu-tional neural network is detailed in Algorithm 2.
Onlyvisited nodes, as predicted by the classifier, are used fortraining.
For ground truth, we say that a node should befollowed if it is the ancestor of some token that is partof the desired answer.
For example, to select the deathdate ?December 25, 1758?
from the support sentence(displayed on page one) about James Hervey, nodes ofthe tree would be attached ground truth values accord-ing to the coloring in Figure 2.
At classification time,some unnecessary (negatively labeled) nodes may befollowed without mistaking the final answer.For example, when deciding whether to follow the?NP?
in node 33 on the third row of Figure 3, the clas-sifier would see features of node 32 (NP) and node 8(?-?)
on its left, its own features, and nothing on itsright.
Since there are no siblings to the right of node33, zero vectors, used for padding, would be placed inthe two empty slots.
To each of these feature vectors,features from the parent and the question root would beconcatenated.The combination of recursive autoencoders withconvolutions inside the tree affords flexibility and gen-erality.
The ordering of children would be immea-surable by a classifier relying on path-based featuresalone.
For instance, our classifier may consider abranch of a parse tree as in Figure 2, in which the birthdate and death date have isomorphic connections to therest of the parse tree.
It can distinguish them by theordering of nodes in a parenthetical expression (see ex-amples in Table 2).115System Short Answer Short Answer Short Answer MC MC MCPrecision Recall F1 Precision Recall F1Main 58.9% 27.0% .370 79.9% 38.8% .523No subtree recognition 48.9% 18.6% .269 71.7% 27.8% .400No unknown 66.8% 19.7% .305 84.2% 29.0% .431word embeddingsSmaller training 40.6% 16.7% .236 65.5% 20.4% .311(1,333 questions)OpenEphyra 52.2% 13.1% .209 73.6% 32.0 % .446Table 1: Performance on TurkQA test set, for short answer and multiple choice (MC) evaluations.7 ExperimentsFrom the TurkQA data, we disregard the yes/no ques-tions, and obtain 12,916 problem sets with 38,083 shortanswer questions for training, and 508 problem setswith 1,488 short answer questions for testing.Because there may be several ways of stating a shortanswer, short answer questions in other data sets aretypically judged by humans.
In TurkQA, because an-swers must be extracted as substrings, we can approx-imate the machine?s correctness by considering the to-ken classification error against the substring originallychosen by the Turk worker.
Of course, answer vali-dation strategies could be used to clean up the shortanswers?for instance, require that they are contigu-ous substrings (as is guaranteed by the task)?but wedid not employ them here, so as not to obfuscate theperformance of the substring extraction system itself.Inevitably, some token misclassification will occur be-cause question writers choose more or less completeanswers (?in Nigeria?
or just ?Nigeria?
).Table 6 shows the performance of our main algo-rithm, evaluated both as short-answer and as multiplechoice.
The short answer results describe the main set-ting, which formulates answer extraction as token clas-sification.
The multiple choice results come from con-sidering all the short answers in the problem sets asalternative choices, and comparing the classifier?s out-puts averaged over the words in each response to selectthe best, or skip if no average is positive.
(Thus, allquestions in a single problem set have the same set ofchoices.)
Although the multiple choice setting is lesschallenging, it helps us see how much of the short an-swer error may be due to finding poor answer bound-aries as opposed to the classifier being totally misled.On more than half of the 1,488 test questions, no an-swer at all is selected, so that multiple choice precisionremains high even with low recall.As one baseline method, we took the OpenEphyraquestion answering system, an open source projectled by Carnegie Mellon University, which evolvedout of submissions to TREC question answering con-tests (Ko et al 2007), bypassing its retrieval mod-ule to simply use our support sentence.
In con-trast to our system, OpenEphyra?s question analy-sis module is trained to map questions to one of afixed number of answer types, such as PERCENTAGE,or PROPER_NAME.PERSON.FIRST_NAME, and uti-lizes a large database of answer patterns for these types.In spite of OpenEphyra?s laborious pattern coding, oursystem performs 17% better on a multiple choice basis,and 77% better on short answers, the latter likely be-cause OpenEphyra?s answer types cover shorter stringsthan the Turks?
answers.The results show the impact of several of our algo-rithmic contributions.
If the autoencoder is trained onlyon reconstruction error and not subtree recognition,the F1 score for token classification drops from .370(58.9% precision, 27.0% recall) to .269 (48.9% preci-sion, 18.6% recall).
Without extended embeddings todifferentiate unknown words, F1 is only .305 (66.8%precision, 19.7% recall).
We are encouraged that in-creasing the amount of data contributes 50% to the F1score (from only F1=.236 training on 1,333 questions),as it suggests that the power of our algorithms is notsaturated while picking up the simplest features.Table 2 gives examples of questions in the test set,together with the classifier?s selection from the supportsentence.8 DiscussionWe have developed a recursive neural network architec-ture capable of using learned representations of wordsand syntax in a parse tree structure to answer freeform questions about natural language text.
Usingmeaning representations of the question and support-ing sentences, our approach buys us freedom from ex-plicit rules, question and answer types, and exact stringmatching.Certainly retrieval is important in a full-fledgedquestion answering system, whereas our classifier per-forms deep analysis after candidate supporting sen-tences have been identified.
Also, multi-sentencedocuments would require information to be linkedamong coreferent entities.
Despite these challenges,we present our system in the belief that strong QA tech-nologies should begin with a mastery of the syntax ofsingle sentences.
A computer cannot be said to have acomplete knowledge representation of a sentence untilit can answer all the questions a human can ask aboutthat sentence.116Question: What is the name of the British charity based in Haringey in north London?Support: Based in Haringey in North London, Exposure is a Britishcharity which enables children and young people from all backgrounds,including disadvantaged groups and those from areas of deprivation,to participate and achieve their fullest potential in the media.Selection: ExposureCorrect Answer: ExposureQuestion: What was Robert Person?s profession?Support: Robert Person was a professional baseball pitcher whoplayed 9 seasons in Major League Baseball: two for the New York Mets, two and ahalf for the Toronto Blue Jays, three and a half for the PhiladelphiaPhillies, and only pitched 7 games for the Boston Red Sox in the last yearof his career.Selection: baseball pitcherCorrect Answer: baseball pitcherQuestion: How many seasons did Robert Person play in the Major League?Support: Robert Person was a professional baseball pitcher whoplayed 9 seasons in Major League Baseball: two for the New York Mets, two and ahalf for the Toronto Blue Jays, three and a half for the PhiladelphiaPhillies, and only pitched 7 games for the Boston Red Sox in the last yearof his career.Selection: 9Correct Answer: 9 seasonsQuestion: What sea does Mukka have a shore on?Support: Mukka is suburb of Mangalore city on the shore of Arabian sea .It is located to north of NITK, Surathkal campus on National Highway 17 .There is a beach in Mukka which has escaped public attention.Selection: Arabian seaCorrect Answer: Arabian seaQuestion: What genre was Lights Out?Support: Lights Out was an extremely popular American old-time radioprogram, an early example of a network series devoted mostly to horror andthe supernatural, predating Suspense and Inner Sanctum.Selection: horror supernaturalCorrect Answer: horror and the supernaturalQuestion: Where is the Arwa Group?Support: The Arwa Group is a set of three Himalayan peaks, named ArwaTower, Arwa Crest, and Arwa Spire, situated in the Chamoli district ofUttarakhand state, in northern India.Selection: the Chamoli district Uttarakhand state northern IndiaCorrect Answer: the Chamoli district of Uttarakhand stateQuestion: What year did Juan Bautista Segura die in?Support: Juan Bautista Quiros Segura (1853 - 1934) was president ofCosta Rica for two weeks, from August 20 to September 2, 1919, following theresignation of Federico Tinoco.Selection: 1934Correct Answer: 1934Question: What state is the Oregon School Activities Association in?Support: The Oregon School Activities Association, or OSAA, is anon-profit, board-governed organization that regulates high school athleticsand competitive activities in Oregon, providing equitable competition amongstits members, both public and private.Selection: OSAACorrect Answer: OregonTable 2: Example results of main classifier on TurkQA test set.117ReferencesY.
Bengio and R. Ducharme.
2001.
A neural proba-bilistic language model.
In Advances in NIPS, vol-ume 13.L.
Bottou.
2004.
Stochastic learning.
In O. Bousquetand U. von Luxburg, editors, Advanced Lectures onMachine Learning, Lecture Notes in Artificial Intel-ligence, LNAI 3176, pages 146?168.
Springer.S.
Brin.
1998.
Extracting patterns and relations fromthe World Wide Web.
In Proceedings World WideWeb and Databases International Workshop (LNCS1590), pages 172?183.
Springer.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.H.
T. Dang, D. Kelly, and J. Lin.
2008.
Overview ofthe TREC 2007 question answering track.
In NISTSpecial Pub.
500-274: The Sixteenth Text RetrievalConference (TREC 2007).P.
Forner, A. Penas, E. Agirre, I. Alegria, C. Forascu,N.
Moreau, P. Osenova, P. Prokopidis, P. Rocha,B.
Sacaleanu, R. Sutcliffe, and E. T. K. Sang.
2008.Overview of the Clef 2008 Multilingual QuestionAnswering Track.
In CLEF.J.
Ko, E. Nyberg, and L. Luo Si.
2007.
A probabilisticgraphical model for joint answer ranking in questionanswering.
In Proceedings of the 30th ACM SIGIRConference.P.
Koomen, V. Punyakanok, D. Roth, and W. Yih.2005.
Generalized inference with multiple seman-tic role labeling systems.
In Proceedings of CoNLL.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InACL.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.Distant supervision for relation extraction withoutlabeled data.
In Proceedings of ACL-IJCNLP, pages1003?1011.J.
B. Pollack.
1990.
Recursive distributed representa-tions.
Artficial Intelligence, 46.H.
Poon and P. Domingos.
2009.
Unsupervised se-mantic parsing.
In Proceedings of ACL.S.
Pradhan, K. Hacioglu, W. Ward, J. H. Martin, andD.
Jurafsky.
2005.
Semantic role chunking com-bining complementary syntactic views.
In Confer-ence on Computational Natural Language Learning(CoNLL), pages 217?220.S.
Riedel, L. Yao, and A. McCallum.
2010.
Modelingrelations and their mentions without labeled text.
InProceedings of ECML-PKDD.R.
Schmidt.
2013.
The fan-created archive ofJeopardy!
games and players.
http://www.j-archive.com.
Accessed: April 26, 2013.L.
Shen, G. Satta, and A. K. Joshi.
2007.
Guided learn-ing for bidirectional sequence classification.
In Pro-ceedings of ACL.R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011a.
Dynamic pooling and un-folding recursive autoencoders for paraphrase detec-tion.
In Advances in NIPS.R.
Socher, J. Pennington, E. Huang, A. Y. Ng, andC.
D. Manning.
2011b.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.In Proceedings of EMNLP.E.
M. Voorhees.
2000.
Overview of the TREC-9 Ques-tion Answering track.
In NIST Special Pub.
500-249: The Ninth Text Retrieval Conference (TREC 9),pages 71?79.A.
Waibel, T. Hanazawa, G. Hinton, K. Shikano, andK.
J. Lang.
1989.
Phoneme recognition usingtime-delay neural networks.
IEEE Trans.
Acoustics,Speech, and Signal Processing, 37(3):328?339.118
