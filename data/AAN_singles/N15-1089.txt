Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 882?891,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEarly Gains Matter: A Case for Preferring Generative over DiscriminativeCrowdsourcing ModelsPaul Felt, Eric Ringger, Kevin Seppi, Kevin Black, Robbie HaertelBrigham Young UniversityProvo, UT 84602, USA{paul felt,kevin black}@byu.edu, {ringger,kseppi}@cs.byu.edu, robbie.haertel@gmail.comAbstractIn modern practice, labeling a dataset of-ten involves aggregating annotator judgmentsobtained from crowdsourcing.
State-of-the-art aggregation is performed via inference onprobabilistic models, some of which are data-aware, meaning that they leverage features ofthe data (e.g., words in a document) in addi-tion to annotator judgments.
Previous worklargely prefers discriminatively trained condi-tional models.
This paper demonstrates thata data-aware crowdsourcing model incorpo-rating a generative multinomial data modelenjoys a strong competitive advantage overits discriminative log-linear counterpart in thetypical crowdsourcing setting.
That is, thegenerative approach is better except when theannotators are highly accurate in which casesimple majority vote is often sufficient.
Ad-ditionally, we present a novel mean-field vari-ational inference algorithm for the generativemodel that significantly improves on the previ-ously reported state-of-the-art for that model.We validate our conclusions on six text clas-sification datasets with both human-generatedand synthetic annotations.1 IntroductionThe success of supervised machine learning has cre-ated an urgent need for manually-labeled trainingdatasets.
Crowdsourcing allows human label judg-ments to be obtained rapidly and at relatively lowcost.
Micro-task markets such as Amazon?s Me-chanical Turk and CrowdFlower have popularizedcrowdsourcing by reducing the overhead required todistribute a job to a community of annotators (the?crowd?).
However, crowdsourced judgments oftensuffer from high error rates.
A common solution tothis problem is to obtain multiple redundant humanjudgments, or annotations,1relying on the obser-vation that, in aggregate, the ability of non-expertsoften rivals or exceeds that of experts by averag-ing over individual error patterns (Surowiecki, 2005;Snow et al, 2008; Jurgens, 2013).For the purposes of this paper a crowdsourcingmodel is a model that infers, at a minimum, classlabels y based on the evidence of one or more im-perfect annotations a.
A common baseline methodaggregates annotations by majority vote but by sodoing ignores important information.
For exam-ple, some annotators are more reliable than others,and their judgments ought to be weighted accord-ingly.
State-of-the-art crowdsourcing methods for-mulate probabilistic models that account for suchside information and then apply standard inferencetechniques to the task of inferring ground truth la-bels from imperfect annotations.Data-aware crowdsourcing models additionallyaccount for the features x comprising each data in-stance (e.g., words in a document).
The data can bemodeled generatively by proposing a joint distribu-tion p(y,x,a).
However, because of the challengeof accurately modeling complex data x, most previ-ous work uses a discriminatively trained conditionalmodel p(y,a|x), hereafter referred to as a discrim-inative model.
As Ng and Jordan (2001) explain,maximizing conditional log likelihood is a compu-1We use the term annotation to identify human judgmentsand distinguish them from gold standard class labels.882tationally convenient approximation to minimizinga discriminative 0-1 loss objective, giving rise to thecommon practice of referring to conditional modelsas discriminative.Contributions.
This paper challenges the popu-lar preference for discriminative data models in thecrowdsourcing literature by demonstrating that intypical crowdsourcing scenarios a generative modelenjoys a strong advantage over its discriminativecounterpart.
We conduct, on both real and syn-thetic annotations, the first empirical comparisonof structurally comparable generative and discrim-inative crowdsourcing models.
The comparison ismade fair by developing similar mean-field varia-tional inference algorithms for both models.
Thegenerative model is considerably improved by ourvariational algorithm compared with the previouslyreported state-of-the-art for that model.2 Previous WorkDawid and Skene (1979) laid the groundwork formodern annotation aggregation by proposing theitem-response model: a probabilistic crowdsourcingmodel p(y,a|?)
over document labels y and annota-tions a parameterized by confusion matrices ?
foreach annotator.
A growing body of work extendsthis model to account for such things as correlationamong annotators, annotator trustworthiness, itemdifficulty, and so forth (Bragg et al, 2013; Hovy etal., 2013; Passonneau and Carpenter, 2013; Paster-nack and Roth, 2010; Smyth et al, 1995; Welinder etal., 2010; Whitehill et al, 2009; Zhou et al, 2012).Of the crowdsourcing models that are data-aware,most model the data discriminatively (Carroll et al,2007; Liu et al, 2012; Raykar et al, 2010; Yan etal., 2014).
A smaller line of work models the datageneratively (Lam and Stork, 2005; Simpson andRoberts, In Press).
We are aware of no papers thatcompare a generative crowdsourcing model with asimilar discriminative model.
In the larger context ofsupervised machine learning, Ng and Jordan (2001)observe that generative models parameters tend toconverge with fewer training examples than theirdiscriminatively trained counterparts, but to lowerasymptotic performance levels.
This paper exploresthose insights in the context of crowdsourcing mod-els.3 ModelsAt a minimum, a probabilistic crowdsourcing modelpredicts ground truth labels y from imperfect anno-tations a (i.e., argmaxyp(y|a)).
In this section we re-view the specifics of two previously-proposed data-aware crowdsourcing models.
These models are bestunderstood as extensions to a Bayesian formulationof the item-response model that we will refer to asITEMRESP.
ITEMRESP, illustrated in Figure 1a, isdefined by the joint distributionp(?
,?,y,a) (1)= p(?)[?j?J?k?Kp(?jk)]?i?Np(yi|?
)?j?Jp(ai j|?j,yi)where J is the set of annotators, K is the set of classlabels, N is the set of data instances in the corpus, ?is a stochastic vector in which ?kis the probabilityof label class k, ?jis a matrix of stochastic vectorrows in which ?jkk?is the probability that annotatorj annotates with k?items whose true label is k, yiisthe class label associated with the ith instance in thecorpus, and ai jkis the number of times that instancei was annotated by annotator j with label k. Thefact that ai jis a count vector allows for the generalcase where annotators express their uncertainty overmultiple class values.
Also, ?
?
Dirichlet(b(?)),?jk?
Dirichlet(b(?
)jk), yi|?
?
Categorical(?
), andai j|yi,?j?
Multinomial(?jyi,Mi) where Miis thenumber of times annotator j annotated instance i.We need not define a distribution over Mibecause inpractice Mi= |ai j|1is fixed and known during pos-terior inference.
A special case of this model formu-lates ai jas a categorical distribution assuming thatannotators will provide at most one annotation peritem.
All hyperparameters are designated b and aredisambiguated with a superscript (e.g., the hyperpa-rameters for p(?)
are b(?)).
When ITEMRESP pa-rameters are set with uniform ?
values and diagonalconfusion matrices ?
, majority vote is obtained.Inference in a crowdsourcing model involves acorpus with an annotated portion NA= {i : |ai|1> 0}and also potentially an unannotated portion NU={i : |ai|1= 0}.
ITEMRESP can be written asp(?,y,a) = p(?,yA,yU,a) where yA= {yi: i ?
NA}and yU= {yi: i ?
NU}.
However, because ITEM-RESP has no model of the data x, it receives no ben-efit from unannotated data NU.883??jkb(?)b(?
)yiai ji ?
Nk ?
Kj ?
Jj ?
J(a) ITEMRESP?jk?kb(?
)?yixiai ji ?
Nk ?
Kj ?
Jj ?
Jk ?
K(b) LOGRESP??jk?kb(?)b(?)b(?
)yixiai ji ?
Nk ?
Kj ?
Jj ?
Jk ?
K(c) MOMRESPFigure 1: Directed graphical model depictions of the models discussed in this paper.
Round nodes arevariables with distributions.
Rectangular nodes are hyperparameters (without distributions).
Shaded nodeshave known values (although some a values may be unobserved).3.1 Log-linear data model (LOGRESP)One way to make ITEMRESP data-aware is byadding a discriminative log-linear data component(Raykar et al, 2010; Liu et al, 2012).
For short, werefer to this model as LOGRESP, illustrated in Fig-ure 1b.
Concretely,p(?,?
,y,a|x) =[?j?J?k?Kp(?jk)](2)?k?Kp(?k)?i?Np(yi|xi,?
)?j?Jp(ai j|?j,yi)where xi fis the value of feature f in data instancei (e.g., a word count in a text classification prob-lem), ?k fis the probability of feature f occurringin an instance of class k, ?k?
Normal(0,?
), andyi|xi,?
?
LogLinear(xi,?).
That is, p(yi|xi,?)
=exp[?Tyixi]/?kexp[?Tkxi].In the special case that each ?jis the identity ma-trix (each annotator is perfectly accurate), LOGRESPreduces to a multinomial logistic regression model.Because it is a conditional model, LOGRESP lacksany built-in capacity for semi-supervised learning.3.2 Multinomial data model (MOMRESP)An alternative way to make ITEMRESP data-awareis by adding a generative multinomial data compo-nent (Lam and Stork, 2005; Felt et al, 2014).
We re-fer to the model as MOMRESP, shown in Figure 1c.p(?
,?,?
,y,x,a) = p(?)[?j?J?k?Kp(?jk)](3)?k?Kp(?k)?i?Np(yi|?)p(xi|yi,?
)?j?Jp(ai j|?j,yi)where ?k fis the probability of feature f occurringin an instance of class k, ?k?
Dirichlet(b(?
)k), xi?Multinomial(?yi,Ti), and Tiis a number-of-trials pa-rameter (e.g., for text classification Tiis the numberof words in document i).
Ti= |xi|1is observed dur-ing posterior inference p(?
,?,?
,y|x,a).Because MOMRESP is fully generative overthe data features x, it naturally performs semi-supervised learning as data from unannotated in-stances NUinform inferred class labels yAof an-notated instances via ?
.
This can be seen by ob-serving that p(x) terms prevent terms involvingyUfrom summing out of the marginal distribu-tion p(?
,?,?
,yA,x,a)=?yUp(?
,?,?
,yA,yU,x,a)=p(?
,?,?
,yA,xA,a)?yUp(yU|?
)p(xU|yU).When N = NU(the unsupervised setting) the pos-terior distribution p(?
,?,?
,yU|x,a) = p(?
,?
,yU|x)is a mixture of multinomials clustering model.Otherwise, the model resembles a semi-supervisedna?
?ve Bayes classifier (Nigam et al, 2006).
How-ever, na?
?ve Bayes is supervised by trustworthy labelswhereas MOMRESP is supervised by imperfect an-notations mediated by inferred annotator error char-acteristic ?
.
In the special case that ?
is the identitymatrix (each annotator is perfectly accurate), MOM-RESP reduces to a possibly semi-supervised na?
?ve884Bayes classifier where each annotation is a fullytrusted label.3.3 A Generative-Discriminative PairMOMRESP and LOGRESP are a generative-discriminative pair, meaning that they belong to thesame parametric model family but with parametersfit to optimize joint likelihood and conditional likeli-hood, respectively.
This relationship is seen via theequivalence of the conditional probability of LOG-RESP pL(y,a|x) and the same expression accord-ing to MOMRESP pM(y,a|x).
For simplicity in thisderivation we omit priors and consider ?
, ?
, and ?to be known values.
ThenpM(y,a|x) =p(y)p(x|y)p(a|y)?y??a?
p(y?)p(x|y?)p(a?|y?)(4)=p(y)p(x|y)?y?
p(y?)p(x|y?)?
p(a|y) (5)=exp[ewTyx+z]?kexp[ewTkx+z]?
p(a|y) (6)= pL(y,a|x) (7)Equation 4 follows from Bayes Rule and conditionalindependence in the model.
In Equation 5 p(a?|y)sums to 1.
The first term of Equation 6 is the pos-terior p(y|x) of a na?
?ve Bayes classifier, known tohave the same form as a logistic regression classi-fier where parameters w and z are constructed from?
and ?
.24 Mean-field Variational Inference (MF)In this section we present novel mean-field (MF)variational algorithms for LOGRESP and MOM-RESP.
Note that Liu et al (2012) present (in an ap-pendix) variational inference for LOGRESP based onbelief propagation (BP).
They do not test their algo-rithm for LOGRESP; however, their comparison ofMF and BP variational inference for the ITEMRESPmodel indicates that the two flavors of variationalinference perform very similarly.
Our MF algorithmfor LOGRESP has not been designed with the ideaof outperforming its BP analogue, but rather withthe goal of ensuring that the generative and discrim-inative model use the same inference algorithm.
We2http://cs.cmu.edu/?tom/mlbook/NBayesLogReg.pdfgives a proof of this property in the continuous case and hintsabout the discrete case proof.expect that we would achieve the same results if ourcomparison used variational BP algorithms for bothMOMRESP and LOGRESP, although such an addi-tional comparison is beyond the scope of this work.Broadly speaking, variational approaches to pos-terior inference transform inference into an opti-mization problem by searching within some familyof tractable approximate distributions Q for the dis-tribution q ?
Q that minimizes distributional diver-gence from an intractable target posterior p?.
In par-ticular, under the mean-field assumption we confineour search to distributions Q that are fully factorized.4.1 LOGRESP InferenceWe approximate LOGRESP?s posteriorp?(?,?
,y|x,a) using the fully factorized approxima-tion q(?,?
,y) =[?j?kq(?jk)]?kq(?k)?iq(yi).Approximate marginal posteriors q are disam-biguated by their arguments.Algorithm.
Initialize each q(yi) to the em-pirical distribution observed in the annotations ai.The Kullback-Leibler divergence KL(q||p?)
is min-imized by iteratively updating each variational dis-tribution in the model as follows:q(?jk) ??k??K?b(?
)jkk?+?i?Nai jk?q(yi=k)?1jkk?= Dirichlet(?(?
)jk)q(?k) ?
exp[?Tk?
?1?k+?i?Nq(yi= k)?Tkxi]q(yi) ??k?Kexp[?j?J?k?
?Kai jk?Eq(?jk)[log?jkk?
]+?f?Fxi fEq(?k)[?k f]]1(yi=k)??k?K?(y)1(yi=k)ik=Categorical(?
(y)i)Approximate distributions are updated by calcu-lating variational parameters ?(?
), disambiguated bya superscript.
Because q(?jk) is a Dirichlet distri-bution the term Eq(?jk)[log?jkk?]
appearing in q(yi)is computed analytically as ?(?(?)jkk?)??(?k?
?(?)jkk?
)where ?
is the digamma function.The distribution q(?k) is a logistic normal distri-bution.
This means that the expectations Eq(?k)[?k f]that appear in q(yi) cannot be computed analyti-cally.
Following Liu et al (2012), we approxi-mate the distribution q(?k) with the point estimate885?
?k= argmax?kq(?k) which can be calculated usingexisting numerical optimization methods for log-linear models.
Such maximization can be under-stood as embedding the variational algorithm insideof an outer EM loop such as might be used to tunehyperparameters in an empirical Bayesian approach(where ?
are treated as hyperparameters).4.2 MOMRESP InferenceMOMRESP?s posterior p?(y,?
,?,?
|x,a) is ap-proximated with the fully factorized distributionq(y,?
,?,?)
= q(?)[?j?kq(?jk)]?kq(?k)?iq(yi).Algorithm.
Initialize each q(yi) to the em-pirical distribution observed in the annotations ai.The Kullback-Leibler divergence KL(q||p?)
is min-imized by iteratively updating each variational dis-tribution in the model as follows:q(?)
??k?K?b(?
)k+?i?Nq(yi=k)?1k= Dirichlet(?(?
))q(?jk) ??k??K?b(?
)jkk?+?i?Nai jk?q(yi=k)?1jkk?= Dirichlet(?(?
)jk)q(?k) ??f?F?b(?
)k f+?i?Nxi fq(yi=k)?1k f= Dirichlet(?(?
)k)q(yi) ??k?Kexp[?j?J?k?
?Kai jk?Eq(?jk)[log?jkk?
]+Eq(?k)[log?k]+?f?Fxi fEq(?k)[log?k f]]1(yi=k)??k?K?(y)1(yi=k)ik=Categorical(?
(y)i)Approximate distributions are updated by calcu-lating the values of variational parameters ?(?
), dis-ambiguated by a superscript.
The expectations oflog terms in the q(yi) update are all with respect toDirichlet distributions and so can be computed ana-lytically as explained previously.4.3 Model priors and implementation detailsComputing a lower bound on the log likelihoodshows that in practice the variational algorithms pre-sented above converge after only a dozen or soupdates.
We compute argmax?kq(?k) for LOG-RESP using the L-BFGS algorithm as implementedin MALLET (McCallum, 2002).
We choose unin-formed priors b(?
)k= 1 for MOMRESP and identitymatrix ?
= 1 for LOGRESP.
We set b(?
)k f= 0.1 forMOMRESP to encourage sparsity in per-class worddistributions.
Liu et al (2012) argue that a uniformprior over the entries of each confusion matrix ?jcanlead to degenerate performance.
Accordingly, weset the diagonal entries of each b(?
)jto a higher valueb(?
)jkk=1+?K+?and off-diagonal entries to a lower valueb(?
)jkk?=1K+?with ?
= 2.Both MOMRESP and LOGRESP are given full ac-cess to all instances in the dataset, annotated andunannotated.
However, as explained in Section 3.1,LOGRESP is conditioned on the data and thus isstructurally unable to make use of unannotated data.We experimented briefly with self-training for LOG-RESP but it had little effect.
With additional effortone could likely settle on a heuristic scheme that al-lowed LOGRESP to benefit from unannotated data.However, since such an extension is external to themodel itself, it is beyond the scope of this work.5 Experiments with Simulated AnnotatorsModels which learn from error-prone annotationscan be challenging to evaluate in a systematic way.Simulated annotations allow us to systematicallycontrol annotator behavior and measure the perfor-mance of our models in each configuration.5.1 Simulating AnnotatorsWe simulate an annotator by corrupting ground truthlabels according to that annotator?s accuracy param-eters.
Simulated annotators are drawn from the an-notator quality pools listed in Table 1.
Each rowis a named pool and contains five annotators A1?A5, each with a corresponding accuracy parameter(the number five is chosen arbitrarily).
In the poolsHIGH, MED, and LOW, annotator errors are dis-tributed uniformly across the incorrect classes.
Be-cause there are no patterns among errors, these set-tings approximate situations in which annotators areultimately in agreement about the task they are do-ing, although some are better at it than others.
TheHIGH pool represents a corpus annotation projectwith high quality annotators.
In the MED and LOWpools annotators are progressively less reliable.The CONFLICT annotator pool in Table 1 is spe-cial in that annotator errors are made systemati-cally rather than uniformly.
Systematic errors are886WEBKB0.40.60.81.0MED0  1  2  3Number of annotated instances x 1,000Accuracy algorithmMomResp+MFMomResp+GibbsMomResp InferenceR520.00.20.40.6LOW0 2 4 6 8Number of annotated instances x 1,000Accuracy algorithmLogResp+MFLogResp+EMLogResp InferenceNumber of annotated instances ?1000.Figure 2: Mean field (MF) variational inference outperforms previous inference methods for both models.Left: MOMRESP with MF (MOMRESP+MF) versus with Gibbs sampling (MOMRESP+Gibbs) on the We-bKB dataset using annotators from the MED pool.
Right: LOGRESP with MF (LOGRESP+MF) versus withEM (LOGRESP+EM) on the Reuters52 dataset using annotators from the LOW pool.produced at simulation time by constructing a per-annotator confusion matrix (similar to ?j) whose di-agonal is set to the desired accuracy setting, andwhose off-diagonal row entries are sampled froma symmetric Dirichlet distribution with parameter0.1 to encourage sparsity and then scaled so thateach row properly sums to 1.
These draws froma sparse Dirichlet yield consistent error patterns.The CONFLICT pool approximates an annotationproject where annotators understand the annotationguidelines differently from one another.
For the sakeof example, annotator A5 in the CONFLICT settingwill annotate documents with the true class B as Bexactly 10% of the time but might annotate B as C85% of the time.
On the other hand, annotator A4might annotate B as D most of the time.
We chooselow agreement rates for CONFLICT to highlight acase that violates majority vote?s assumption that an-notators are basically in agreement.A1 A2 A3 A4 A5HIGH 90 85 80 75 70MED 70 65 60 55 50LOW 50 40 30 20 10CONFLICT 50?
40?
30?
20?
10?Table 1: For each simulated annotator quality pool(HIGH, MED, LOW, CONFLICT), annotators A1-A5 are assigned an accuracy.
?
indicates that errorsare systematically in conflict as described in the text.5.2 Datasets and FeaturesWe simulate the annotator pools from Table1 on each of six text classification datasets.The datasets 20 Newsgroups, WebKB, Cade12,Reuters8, and Reuters52 are described by Cardoso-Cachopo (2007).
The LDC-labeled Enron emailsdataset is described by Berry et al (2001).
Eachdataset is preprocessed via Porter stemming and byremoval of the stopwords from MALLET?s stop-word list.
Features occurring fewer than 5 times inthe corpus are discarded.
Features are fractionallyscaled so that |xi|1is equal to the average documentlength since document scaling has been shown to bebeneficial for multinomial document models (Nigamet al, 2006).Each dataset is annotated according to the follow-ing process: an instance is selected at random (with-out replacement) and annotated by three annotatorsselected at random (without replacement).
Becauseannotation simulation is a stochastic process, eachsimulation is repeated five times.5.3 Validating Mean-field Variational InferenceFigure 2 compares mean-field variational inference(MF) with alternative inference algorithms from pre-vious work.
For variety, the left and right plots arecalculated over arbitrarily chosen datasets and an-notator pools, but these trends are representative ofother settings.
MOMRESP using MF is comparedwith MOMRESP using Gibbs sampling estimatingp(y|x,a) from several hundred samples (an improve-ment to the method used by Felt et al (2014)).887CONFLICT LOW MED HIGH0.40.60.81.00  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15Number of annotated instances x 1,000AccuracyalgorithmMomRespLogRespMajority20 NewsgroupsCONFLICT LOW MED HIGH0.40.60.81.00  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15Number of annotated instances x 1,000TestAccuracyalgorithmMomRespLogResp20 NewsgroupsFigure 3: Top row: Inferred label accuracy on three-deep annotations.
A majority vote baseline is shown forreference.
Bottom row: Generalization accuracy on a test set.
Majority vote is not shown since it does notgenerate test set predictions.
Each column uses the indicated simulated annotator pool.MOMRESP benefits significantly from MF.
We sus-pect that this disparity could be reduced via hyper-parameter optimization as indicated by Asuncion etal.
(2009).
However, that investigation is beyond thescope of the current work.
LOGRESP using MF iscompared with LOGRESP using expectation maxi-mization (EM) as in (Raykar et al, 2010).
LOG-RESP with MF displays minor improvements overLOGRESP with EM.
This is consistent with the mod-est gains that Liu et al (2012) reported when com-paring variational and EM inference for the ITEM-RESP model.5.4 Discriminative (LOGRESP) versusGenerative (MOMRESP)We run MOMRESP and LOGRESP with MF infer-ence on the cross product of datasets and annotatorpools.
Inferred label accuracy on items that havebeen annotated is the primary task of crowdsourc-ing; we track this measure accordingly.
However,the ability of these models to generalize on unanno-tated data is also of interest and allows better com-parison with traditional non-crowdsourcing models.Figure 3 plots learning curves for each annotatorpool on the 20 Newsgroups dataset; results on otherdatasets are summarized in Table 2.
The first row ofFigure 3 plots the accuracy of labels inferred fromannotations.
The second row of Figure 3 plots gen-eralization accuracy using the inferred model param-eters ?
(and ?
in the case of MOMRESP) on held-outtest sets with no annotations.
The generalization ac-curacy curves of MOMRESP and LOGRESP may becompared with those of na?
?ve Bayes and logistic re-gression, respectively.
Recall that in the special casewhere annotations are both flawless and trusted (viadiagonal confusion matrices ?)
then MOMRESP andLOGRESP simplify to semi-supervised na?
?ve Bayesand logistic regression classifiers, respectively.Notice that MOMRESP climbs more steeply thanLOGRESP in all cases.
This observation is in keep-ing with previous work in supervised learning.
Ngand Jordan (2001) argue that generative and discrim-inative models have complementary strengths: gen-erative models tend to have steeper learning curvesand converge in terms of parameter values afteronly logn training examples, whereas discriminativemodels tend to achieve higher asymptotic levels butconverge more slowly after n training examples.
Thesecond row of Figure 3 shows that even after trainingon three-deep annotations over the entire 20 news-groups dataset, LOGRESP?s data model does not ap-proach its asymptotic level of performance.
The888early steep slope of the generative model is moredesirable in this setting than the eventually superiorperformance of the discriminative model given largenumbers of annotations.
Figure 4 additionally plotsMOMRESPA, a variant of MOMRESP deprived ofall unannotated documents, showing that the earlygenerative advantage is not attributable entirely tosemi-supervision.The generative model is more robust to annotationnoise than the discriminative model, seen by com-paring the LOW, MED, and HIGH columns in Fig-ure 3.
This robustness is significant because crowd-sourcing tends to yield noisy annotations, makingthe LOW and MED annotator pools of greatest prac-tical interest.
This assertion is borne out by an ex-periment with CrowdFlower, reported in Section 6.To validate that LOGRESP does, indeed, asymp-totically surpass MOMRESP we ran inferenceon datasets with increasing annotation depths.Crossover does not occur until 20 Newsgroups is an-notated nearly 12-deep for LOW, 5-deep for MED,and 3.5-deep (on average) for HIGH.
Additionally,for each combination of dataset and annotator poolexcept those involving CONFLICT, by the timeLOGRESP surpasses MOMRESP, the majority votebaseline is extremely competitive with LOGRESP.The CONFLICT setting is the exception to this rule:CONFLICT annotators are particularly challengingfor majority vote since they violate the implicit as-sumption that annotators are basically aligned withthe truth.
The CONFLICT setting is of practicalinterest only when annotators have dramatic deep-seated differences of opinion about what various la-bels should mean.
For most crowdsourcing projectsthis issue may be avoided with sufficient up-frontorientation of the annotators.
For reference, in Fig-ure 4 we show that a less extreme variant of CON-FLICT behaves more similarly to LOW.Table 2 reports the percent of the dataset thatmust be annotated three-deep before LOGRESP?s in-ferred label accuracy surpasses that of MOMRESP.Crossover tends to happen later when annotationquality is low and earlier when annotator quality ishigh.
Cases reported as NA were too close to call;that is, the dominating algorithm changed depend-ing on the random run.Unsurprisingly, MOMRESP is not well suited toall classification datasets.
The 0% entries in TableCONFLICT_MILD0.40.60.81.00  5 10 15Number of annotated instances x 1,000AccuracyalgorithmMomRespMomRespALogRespMajority20 NewsgroupsFigure 4: Inferred label accuracy for a variant ofthe CONFLICT annotator pool in which the off-diagonals of each annotator confusion matrix aredrawn from a Dirichlet parameterized by 1 ratherthan 0.1.
Also adds the algorithm MOMRESPA toshow the effect of removing MOMRESP?s access tounannotated documents.2 mean that LOGRESP dominates the learning curvefor that annotator pool and dataset.
These cases arelikely the result of the MOMRESP model makingthe same strict inter-feature independence assump-tions as na?
?ve Bayes, rendering it tractable and ef-fective for many classification tasks but ill-suited fordatasets where features are highly correlated or fortasks in which class identity is not informed by doc-ument vocabulary.
The CADE12 dataset, in particu-lar, is known to be challenging.
A supervised na?
?veBayes classifier achieves only 57% accuracy on thisdataset (Cardoso-Cachopo, 2007).
We would expectMOMRESP to perform similarly poorly on sentimentclassification data.
Although we assert that gener-ative models are inherently better suited to crowd-sourcing than discriminative models, a sufficientlystrong mismatch between model assumptions anddata can negate this advantage.6 Experiments with Human AnnotatorsIn the previous section we used simulations to con-trol annotator error.
In this section we relax that con-trol.
To assess the effect of real-world annotation er-ror on MOMRESP and LOGRESP, we selected 1000instances at random from 20 Newsgroups and paidannotators on CrowdFlower to annotate them withthe 20 Newsgroups categories, presented as human-readable names (e.g., ?Atheism?
for alt.atheism).Annotators were allowed to express uncertainty by889CONFLICT LOW MED HIGH20 News 21% X X XWebKB NA X X 0%Reuters8 NA X X XReuters52 X X X XCADE12 0% X 0% 0%Enron X X X 18%Table 2: The percentage of the dataset that must beannotated (three-deep) before the generative modelMOMRESP is surpassed by LOGRESP.
Xindicatesthat MOMRESP dominates the entire learning curve;0% indicates that LOGRESP dominates.
NA indi-cates high variance cases that were too close to call.selecting up to three unique categories per docu-ment.
During the course of a single day we gath-ered 7,265 annotations, with each document havinga minimum of 3 and a mean of 7.3 annotations.3Fig-ure 5 shows learning curves for the CrowdFlowerannotations.
The trends observed previously are un-changed.
MOMRESP enjoys a significant advantagewhen relatively few annotations are available.
Pre-sumably LOGRESP would still dominate if we wereable to explore later portions of the curve or curveswith greater annotation depth.0.40.50.60.70  2  4  6Number of human annotations x 1,000AccuracyalgorithmMomRespLogRespMajorityCrowdflower AnnotationsFigure 5: Inferred label accuracy on annotationsgathered from CrowdFlower over a subset of 1000instances of the 20 Newsgroups dataset.
At the lastplotted point there are 7,265/1,000 ?
7.3 annota-tions per instance.3This dataset and the scripts that produced it are availablevia git at git://nlp.cs.byu.edu/plf1/crowdflower-newsgroups.git7 Conclusions and Future WorkWe have argued that generative models are bettersuited than discriminative models to the task of an-notation aggregation since they tend to be morerobust to annotation noise and to approach theirasymptotic performance levels with fewer annota-tions.
Also, in settings where a discriminative modelwould usually shine, there are often enough annota-tions that a simple baseline of majority vote is suffi-cient.In support of this argument, we developedcomparable mean-field variational inference fora generative-discriminative pair of crowdsourcingmodels and compared them on both crowdsourcedand synthetic annotations on six text classificationdatasets.
In practice we found that on classificationtasks for which generative models of the data workreasonably well, the generative model greatly out-performs its discriminative log-linear counterpart.The generative multinomial model we employedmakes inter-feature independence assumptions illsuited to some classification tasks.
Document topicmodels (Blei, 2012) could be used as the basisof a more sophisticated generative crowdsourcingmodel.
One might also transform the data to makeit more amenable to a simple model using docu-ments assembled from distributed word represen-tations (Mikolov et al, 2013).
Finally, althoughwe expect these results to generalize, we have onlyexperimented with text classification.
Similar ex-periments could be performed on other commonlycrowdsourced tasks such as sequence labeling.AcknowledgmentsWe thank Alex Smola and the anonymous reviewersfor their insightful comments.
This work was sup-ported by the collaborative NSF Grant IIS-1409739(BYU) and IIS-1409287 (UMD).References[Asuncion et al2009] A. Asuncion, M. Welling,P.
Smyth, and Y. W. Teh.
2009.
On smoothingand inference for topic models.
In Proceedings of theTwenty-Fifth Conference on Uncertainty in ArtificialIntelligence, pages 27?34.
AUAI Press.
[Berry et al2001] M. W. Berry, M. Browne, and890B.
Signer.
2001.
Topic annotated Enron email dataset.
Linguistic Data Consortium, Philadelphia.
[Blei2012] D. Blei.
2012.
Probabilistic topic models.Communications of the ACM, 55(4):77?84.
[Bragg et al2013] J. Bragg, Mausam, and D. Weld.
2013.Crowdsourcing multi-label classification for taxon-omy creation.
In First AAAI Conference on HumanComputation and Crowdsourcing.
[Cardoso-Cachopo2007] A. Cardoso-Cachopo.
2007.Improving Methods for Single-label Text Categoriza-tion.
Ph.D. thesis, Universidade Tecnica de Lisboa.
[Carroll et al2007] J. Carroll, R. Haertel, P. McClanahan,E.
Ringger, and K. Seppi.
2007.
Modeling the anno-tation process for ancient corpus creation.
In Proceed-ings of ECAL 2007, pages 25?42.
Charles University.
[Dawid and Skene1979] A.P.
Dawid and A.M. Skene.1979.
Maximum likelihood estimation of observererror-rates using the EM algorithm.
Applied Statistics,pages 20?28.
[Felt et al2014] P. Felt, R. Haertel, E. Ringger, andK.
Seppi.
2014.
MomResp: A Bayesian model formulti-annotator document labeling.
In Proceedings ofLREC.
[Hovy et al2013] D. Hovy, T. Berg-Kirkpatrick,A.
Vaswani, and E. Hovy.
2013.
Learning whom totrust with MACE.
In Proceedings of HLT-NAACL2013, pages 1120?1130.
[Jurgens2013] D. Jurgens.
2013.
Embracing ambigu-ity: A comparison of annotation methodologies forcrowdsourcing word sense labels.
In Proceedings ofNAACL-HLT 2013, pages 556?562.
[Lam and Stork2005] C. P. Lam and D. G. Stork.
2005.Toward optimal labeling strategy under multiple unre-liable labelers.
In AAAI Spring Symposium: Knowl-edge Collection from Volunteer Contributors.
[Liu et al2012] Q. Liu, J. Peng, and A. Ihler.
2012.
Vari-ational inference for crowdsourcing.
In NIPS, pages692?700.
[McCallum2002] Andrew Kachites McCallum.
2002.Mallet: A machine learning for language toolkit.http://mallet.cs.umass.edu.
[Mikolov et al2013] T. Mikolov, I. Sutskever, K. Chen,G.
Corrado, and J.
Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.
[Ng and Jordan2001] A. Ng and M. Jordan.
2001.
Ondiscriminative vs. generative classifiers: A comparisonof logistic regression and naive Bayes.
NIPS, 14:841?848.
[Nigam et al2006] K. Nigam, A. McCallum, andT.
Mitchell.
2006.
Semi-supervised text classificationusing EM.
Semi-Supervised Learning, pages 33?56.
[Passonneau and Carpenter2013] R. Passonneau andB.
Carpenter.
2013.
The benefits of a model ofannotation.
In Proceedings of the 7th Linguistic Anno-tation Workshop and Interoperability with Discourse,pages 187?195.
Citeseer.
[Pasternack and Roth2010] J. Pasternack and D. Roth.2010.
Knowing what to believe (when you alreadyknow something).
In COLING, Beijing, China.
[Raykar et al2010] V. Raykar, S. Yu, L. Zhao,G.
Valadez, C. Florin, L. Bogoni, and L. Moy.2010.
Learning from crowds.
The Journal of MachineLearning Research, 11:1297?1322.
[Simpson and RobertsIn Press] E. Simpson andS.
Roberts.
In Press.
Bayesian methods for in-telligent task assignment in crowdsourcing systems.In Decision Making: Uncertainty, Imperfection,Deliberation and Scalability.
Springer.
[Smyth et al1995] P. Smyth, U. Fayyad, M. Burl, P. Per-ona, and P. Baldi.
1995.
Inferring ground truth fromsubjective labelling of Venus images.
NIPS, pages1085?1092.
[Snow et al2008] R. Snow, B. O?Connor, D. Jurafsky,and A. Ng.
2008.
Cheap and fast?but is it good?
:Evaluating non-expert annotations for natural lan-guage tasks.
In Proceedings of EMNLP.
ACL.
[Surowiecki2005] J. Surowiecki.
2005.
The Wisdom ofCrowds.
Random House LLC.
[Welinder et al2010] P. Welinder, S. Branson, P. Perona,and S. Belongie.
2010.
The multidimensional wisdomof crowds.
In NIPS, pages 2424?2432.
[Whitehill et al2009] J. Whitehill, P. Ruvolo, T. Wu,J.
Bergsma, and J. Movellan.
2009.
Whose voteshould count more: Optimal integration of labels fromlabelers of unknown expertise.
NIPS, 22:2035?2043.
[Yan et al2014] Y. Yan, R. Rosales, G. Fung, R. Subra-manian, and J. Dy.
2014.
Learning from multiple an-notators with varying expertise.
Machine Learning,95(3):291?327.
[Zhou et al2012] D. Zhou, J. Platt, S. Basu, and Y. Mao.2012.
Learning from the wisdom of crowds by mini-max entropy.
In NIPS, volume 25, pages 2204?2212.891
