Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 231?235,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsCoreference Resolution Evaluation for Higher Level ApplicationsDon TuggenerUnversity of ZurichInstitute of Computational Linguisticstuggener@cl.uzh.chAbstractThis paper presents an evaluation frame-work for coreference resolution geared to-wards interpretability for higher-level ap-plications.
Three application scenariosfor coreference resolution are outlined andmetrics for them are devised.
The metricsprovide detailed system analysis and aimat measuring the potential benefit of usingcoreference systems in preprocessing.1 IntroductionCoreference Resolution is often described as animportant preprocessing step for higher-level ap-plications.
However, the commonly used coref-erence evaluation metrics (MUC, BCUB, CEAF,BLANC) treat coreference as a generic clusteringproblem and perform cluster similarity measuresto evaluate coreference system outputs.
Mentionsare seen as unsorted generic items rather than lin-early ordered linguistic objects (Chen and Ng,2013).
This makes it arguably hard to interpretthe scores and assess the potential benefit of usinga coreference system as a preprocessing step.Therefore, this paper proposes an evaluationframework for coreference systems which aims atbridging the gap between coreference system de-velopment, evaluation, and higher level applica-tions.
For this purpose, we outline three typesof application scenarios which coreference resolu-tion can benefit and devise metrics for them whichare easy to interpret and provide detailed systemoutput analysis based on any available mentionfeature.2 Basic ConceptsLike other coreference metrics, we adapt the con-cepts of Recall and Precision from evaluation inInformation Retrieval (IR) to compare mentionsin a system output (the response) to the anno-tated mentions in a gold standard (the key).
Tostay close to the originally clear definitions of Re-call and Precision in IR, Recall is aimed at iden-tifying how many of the annotated key mentionsare correctly resolved by a system, and Precisionwill measure the correctness of the returned sys-tem mentions.However, if we define Recall astptp+fn, the de-nominator will not include key mentions that havebeen put in the wrong coreference chain, and willnot denote all mentions in the key.
Therefore,borrowing nomenclature from (Durrett and Klein,2013), we introduce an additional error class,wrong linkage (wl), which signifies key mentionsthat have been linked to incorrect antecedents.
Re-call can then be defined astptp+wl+fnand Precisionastptp+wl+fp.
Recall then extends over all key men-tions, and Precision calculation includes all sys-tem mentions.Furthermore, including wrong linkage in theRecall equation prevents it from inflating com-pared to Precision when a large number of keymentions are incorrectly resolved.
Evaluationis also sensitive to the anaphoricity detectionproblem.
For example, an incorrectly resolvedanaphoric ?it?
pronoun is counted as wrong link-age and thus also affects Recall, while a resolvedpleonastic ?it?
pronoun is considered a false posi-tive which is only penalized by Precision.
Besidethe ?it?
pronoun, this is of particular relevance fornoun markables, as determining their referentialstatus is a non-trivial subtask in coreference res-olution.As we evaluate each mention individually, weare able to measure performance regarding anyfeature type of a mention, e.g.
PoS, number, gen-der, semantic class etc.
We will focus on men-tion types based on PoS tags (i.e.
pronouns, nounsetc.
), as they are often the building blocks of coref-erence systems.
Furthermore, mention type based231performance analysis is informative for higher-level applications, as they might be specifically in-terested in certain mention types.3 Application ScenariosNext, we will outline three higher-level applica-tion types which consume coreference and deviserelevant metrics for them.3.1 Models of entity distributionsThe first application scenario subsumes modelsthat investigate distributions and patterns of en-tity occurrences in discourse.
For example, Cen-tering theory (Grosz et al., 1995) and the thereofderived entity grid model (Barzilay and Lapata,2008; Elsner and Charniak, 2011) record transi-tions of grammatical functions that entities occurwith in coherent discourse.
These models canbenefit from coreference resolution if entities arepronominalized or occur as a non-string matchingnominal mentions.Another application which tracks sequences ofentity occurrences is event sequence modeling.Such models investigate prototypical sequences ofevents to derive event schemes or templates of suc-cessive events (Lee et al., 2012; Irwin et al., 2011;Kuo and Chen, 2007).
Here, coreference res-olution can help link pronominalized argumentsof events to their previous mention and, thereby,maintain the event argument sequence.The outlined applications in this scenario pri-marily rely on the identification of correct andgapless sequences of entity occurrences.
We canapproximate this requirement in a metric by re-quiring the immediate antecedent of a mention ina response chain to be the immediate antecedentof that mention in the key chain.Note that this restriction deems mentions as in-correct, if they skip an antecedent but are resolvedto another antecedent in the correct chain.
For ex-ample, given a key [A-B-C-D], mention D in a re-sponse [A-B-D] would not be considered correct,as the immediate antecedent is not the same as inthe key.
The original sequence of the entity?s oc-currence is broken between mention B and D inthe response, as mention C is missing.We use the following algorithm (table 1) to cal-culate Recall and Precision for evaluating imme-diate antecedents.
Let K be the key and S be thesystem response.
Let e be an entity denoted by mnmentions.01 for ek?
K:02 for mi?
ek?
i > 0:03 if ?
?es,mj: (es?
S ?mj?
es?mj= mi?
?predecessor(mj, es)) ?
fn++04 elif ?es,mj: (es?
S ?mj?
es?mj= mi?predecessor(mi, ek) = predecessor(mj, es))?
tp++05 else wl++06 for es?
S:07 for mi?
es?
i > 0:08 if ?
?ek,mj: (ek?
K ?mj?
ek?mj= mi?
?predecessor(mj, ek)) ?
fp++Table 1: Algorithm for calculating Recall and Pre-cision.We traverse the key K and each entity ekinit1.
We evaluate each mention miin ek, except forthe first one (line 2), as we investigate coreferencelinks.
If no response chain exists that containsmiand its predecessor, we count mias a falsenegative (line 3).
This condition subsumes thecase where miis not in the response, and the casewhere miis the first mention of a response chain.In the latter case, the system has deemed mito benon-anaphoric (i.e.
the starter of a chain), while itis anaphoric in the key2.
We check whether theimmediate predecessor of miin the key chain ekis also the immediate predecessor of mjin the re-sponse chain es(line 4).
If true, we count mias atrue positive, or as wrong linkage otherwise.We traverse the response chains to detect spu-rious system mentions, i.e.
mentions not in thekey, and count them as false positives, i.e.
non-anaphoric markables that have been resolved bythe system (lines 6-8).
Here, we also count men-tions in the response, which have no predecessorin a key chain, as false positives.
If a mentionin the response chain is the chain starter in a keychain, it means that the system has falsely deemedit to be anaphoric and we regard it as a false posi-tive3.3.2 Inferred local entitiesThe second application scenario relies on corefer-ence resolution to infer local nominal antecedents.For example, in Summarization, a target sentencemay contain a pronoun which should be replacedby a nominal antecedent to avoid ambiguities andensure coherence in the summary.
Machine Trans-1We disregard singleton entities, as it is not clear whatbenefit a higher level application could gain from them.2(Durrett and Klein, 2013) call this error false new (FN).3This error is called false anaphoric (FA) by (Durrett andKlein, 2013).232lation can benefit from pronoun resolution in lan-guage pairs where nouns have grammatical gen-der.
In such language pairs, the gender of a pro-noun antecedent has to be retrieved in the sourcelanguage in order to insert the pronoun with thecorrect gender in the target language.In these applications, it is not sufficient to linkpronouns to other pronouns of the same corefer-ence chain because they do not help infer the un-derlying entity.
Therefore, in our metric, we re-quire the closest preceding nominal antecedent ofa mention in a response chain to be an antecedentin the key chain.The algorithm for calculation of Recall and Pre-cision is similar to the one in table 1.
We modifylines 3 and 4 to require the closest nominal an-tecedent of miin the response chain esto be anantecedent of mjin the corresponding key chainek, where mj= mi, i.e.:?mh?
es: is closest noun(mh,mi) ?
?ek,mj,ml: (ek?
K ?
mj?
ek?
mj=mi?ml?
ek?
l < j ?ml= mh) ?
tp++Note that we cannot process chains without anominal mention in this scenario4.
Therefore, weskip evaluation for such ek?
K. We still wantto find incorrectly inferred nominal antecedents ofanaphoric mentions, i.e.
mentions in es?
S thathave been assigned a nominal antecedent in the re-sponse but have none in the key and count them aswrong linkage, as they infer an incorrect nominalantecedent.
Therefore, we traverse all es?
S andadd to the algorithm:?mi?
es: ?is noun(mi) ?
?mh?
es:is noun(mh) ?
?ek,mj: (ek?
K ?
mj?ek?mj= mi?
??ml?
ek: is noun(ml)) ?wl++3.3 Finding contexts for a specific entityThe last scenario we consider covers applicationsthat are primarily query driven.
Such applicationssearch for references to a given entity and analyzeor extract its occurrence contexts.
For example,Sentiment Analysis searches large text collectionsfor occurrences of a target entity and then derivespolarity information from its contexts.
Biomedicalrelation mining looks for interaction contexts ofspecific genes or proteins etc.4We found that 476 of 4532 key chains (10.05%) do notcontain a nominal mention.
Furthermore, we do not treatcataphora (i.e.
pronouns at chain start) in this scenario.
Wefound that 241 (5.31%) of the key chains start with cataphoricpronouns.For these applications, references to relevant en-tities have to be accessible by queries.
For ex-ample, if a sentiment system investigates polaritycontexts of the entity ?Barack Obama?, given akey chain [Obama - the president - he], a responsechain [the president - he] is not sufficient, becausethe higher level application is not looking for in-stances of the generic ?president?
entity.Therefore, we determine an anchor mention foreach coreference chain which represents the mostlikely unique surface form an entity occurs with.As a simple approximation, we choose the firstnominal mention of a coreference chain to be theanchor of the entity, because first mentions of enti-ties introduce them to discourse and are, therefore,generally informative, unambiguous, semanticallyextensive and are likely to contain surface forms ahigher level application will query.Entity Detection01 for ek?
K:02 if ?mn?
ek: is noun(mn)?
manchor= determine anchor(ek)03 if ?manchor?
?es?
S : manchor?
es?
tp++04 else ?
fn++05 for es?
S:06 if ?mn?
es: is noun(mn)?
manchor= determine anchor(es)07 if ??ek?
K : manchor?
ek?
fp++Entity Mentions01 for ek?
K : ?manchor?
?es?
S : manchor?
es:02 for mi?
ek:03 if mi?
es?
tp++04 else?
fn++05 for mi?
es:06 if mi?
?
ek?
fp++Table 2: Algorithm for calculating Recall and Pre-cision using anchor mentions.To calculate Recall and Precision, we aligncoreference chains in the responses to those in thekey via their anchors and then measure how many(in)correct references to that anchor the corefer-ence systems find (table 2).
We divide evaluationinto entity detection (ED), which measures howmany of the anchor mentions a system identifies.We then measure the quality of the entity men-tions (EM) for only those entities which have beenaligned through their anchors.The quality of the references to the anchor men-tions are not directly comparable between sys-tems, as their basis is not the same if the num-ber of aligned anchors differs.
Therefore, we cal-culate the harmonic mean of entity detection andentity mentions to enable direct system compari-233son.
Where applicable, we obtain the named en-tity class of the entity and measure performancefor each such class.4 EvaluationWe apply our metrics to three available corefer-ence systems, namely the Berkley system (Dur-rett and Klein, 2013), the IMS system (Bj?orkelundand Farkas, 2012), and the Stanford system (Leeet al., 2013) and their responses for the CoNLL2012 shared task test set for English (Pradhan etal., 2012).
Tables 3 and 4 report the results.Immediate antecedent Inferred antecedentR P F R P FBERK (Durrett and Klein, 2013)NOUN 45.06 47.06 46.04 55.54 60.37 57.85PRP 67.66 64.87 66.24 48.92 53.62 51.16PRP$ 74.49 74.32 74.41 61.95 66.80 64.28TOTAL 56.60 56.91 56.76 52.94 58.04 55.37IMS (Bj?orkelund and Farkas, 2012)NOUN 38.01 43.09 40.39 46.90 54.96 50.61PRP 69.06 68.64 68.85 43.04 57.42 49.20PRP$ 72.57 72.11 72.34 51.51 63.54 56.90TOTAL 53.55 57.55 55.48 45.27 56.47 50.25STAN (Lee et al., 2013)NOUN 38.51 42.92 40.60 50.03 57.62 53.56PRP 65.55 61.09 63.25 36.67 45.97 40.80PRP$ 66.12 65.70 65.91 40.64 52.38 45.77TOTAL 51.70 52.69 52.19 43.01 51.73 46.97Table 3: Antecedent based evaluationWe note that the system ranking based on theMELA score5is retained by our metrics.
MELArates the Berkley system best (61.62), followed bythe IMS system (57.42), and then the Stanford sys-tem (55.69).Beside detailed analysis based on PoS tags, ourmetrics reveal interesting nuances.
Somewhat ex-pectedly, noun resolution is worse when the imme-diate antecedent is evaluated, than if the next nom-inal antecedent is analyzed.
Symmetrically in-verse, pronouns achieve higher scores when theirdirect antecedent is measured, as compared towhen the next nominal antecedent has to be cor-rect.Our evaluation shows that the IMS systemachieves a higher score for pronouns than theBerkley system when immediate antecedents aremeasured and has a higher Precision for pronounsregarding the inferred antecedents.
The Berkleysystem performs best mainly due to Recall.
Fore.g.
personal pronouns (PRP), Berkley has the5MUC+BCUB+CEAFE3following counts for the inferred antecedents:tp=2687, wl=1935, fn=871, fp=389, while IMSshows tp=2243, wl=1376, fn=1592, fp=287.
Thisindicates that the IMS Recall is lower because ofthe high false negative count, rather than being dueto too many wrong linkages.Finally, table 4 suggests that the IMS systemsperforms significantly worse in the PERSON classthan the other systems and is outperformed by theStanford system in the ORG class, but performsbest in the GPE class.R P F F?PERSON (18.69%)BERKED 64.02 75.88 69.4567.11EM 63.60 66.29 64.92IMSED 45.66 51.69 48.4852.74EM 47.67 73.45 57.82STANED 56.33 59.74 57.9861.61EM 53.84 84.37 65.73GPE (13.28%)BERKED 73.21 77.36 75.2375.71EM 69.89 83.73 76.19IMSED 73.51 74.17 73.8476.21EM 69.94 90.04 78.73STANED 70.24 76.62 73.2975.24EM 68.44 88.81 77.30ORG (9.63%)BERKED 62.78 67.13 64.8867.62EM 66.87 74.78 70.60IMSED 44.98 54.30 49.2056.85EM 57.26 81.66 67.32STANED 49.68 58.56 53.7559.41EM 57.25 79.05 66.41TOTAL (100%)BERKED 58.65 53.19 55.7963.41EM 72.65 74.28 73.45IMSED 47.16 42.66 44.8055.24EM 65.88 79.40 72.01STANED 48.62 41.40 44.7255.27EM 65.66 80.48 72.32Table 4: Anchor mention based evaluation5 ConclusionWe have presented a simple evaluation frameworkfor coreference evaluation with higher level ap-plications in mind.
The metrics allow specificperformance measurement regarding different an-tecedent requirements and any mention feature,such as PoS type, lemma, or named entity class,which can aid system development and compari-son.
Furthermore, the metrics do not alter systemrankings compared to the commonly used evalua-tion approach6.6The scorers are freely available on our website:http://www.cl.uzh.ch/research/coreferenceresolution.html234ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Com-put.
Linguist., 34(1):1?34, March.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Joint Conference on EMNLP andCoNLL - Shared Task, CoNLL ?12, pages 49?55,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Chen Chen and Vincent Ng.
2013.
Linguisticallyaware coreference evaluation metrics.
In Proceed-ings of the 6th International Joint Conference onNatural Language Processing, pages 1366?1374.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, Seattle, Washington, Oc-tober.
Association for Computational Linguistics.Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 125?129, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Barbara J. Grosz, Scott Weinstein, and Aravind K.Joshi.
1995.
Centering: a framework for modelingthe local coherence of discourse.
Comput.
Linguist.,21(2):203?225, June.Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.2011.
Narrative schema as world knowledge forcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, CONLL Shared Task?11, pages 86?92, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.June-Jei Kuo and Hsin-Hsi Chen.
2007.
Cross-document event clustering using knowledge miningfrom co-reference chains.
Inf.
Process.
Manage.,43(2):327?343, March.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entityand event coreference resolution across documents.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4).Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof the Sixteenth Conference on Computational Natu-ral Language Learning (CoNLL 2012), Jeju, Korea.235
