Proceedings of ACL-08: HLT, pages 870?878,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGeneralized Expectation Criteria for Semi-Supervised Learning ofConditional Random FieldsGideon S. MannGoogle Inc.76 Ninth AvenueNew York, NY 10011Andrew McCallumDepartment of Computer ScienceUniversity of Massachusetts140 Governors DriveAmherst, MA 01003AbstractThis paper presents a semi-supervised train-ing method for linear-chain conditional ran-dom fields that makes use of labeled featuresrather than labeled instances.
This is accom-plished by using generalized expectation cri-teria to express a preference for parameter set-tings in which the model?s distribution on un-labeled data matches a target distribution.
Weinduce target conditional probability distribu-tions of labels given features from both anno-tated feature occurrences in context and ad-hoc feature majority label assignment.
Theuse of generalized expectation criteria allowsfor a dramatic reduction in annotation timeby shifting from traditional instance-labelingto feature-labeling, and the methods presentedoutperform traditional CRF training and othersemi-supervised methods when limited humaneffort is available.1 IntroductionA significant barrier to applying machine learningto new real world domains is the cost of obtainingthe necessary training data.
To address this prob-lem, work over the past several years has exploredsemi-supervised or unsupervised approaches to thesame problems, seeking to improve accuracy withthe addition of lower cost unlabeled data.
Tradi-tional approaches to semi-supervised learning areapplied to cases in which there is a small amount offully labeled data and a much larger amount of un-labeled data, presumably from the same data source.For example, EM (Nigam et al, 1998), transduc-tive SVMs (Joachims, 1999), entropy regularization(Grandvalet and Bengio, 2004), and graph-basedaddress          :            *number*           oak            avenue          rent             $ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS RENT RENTTraditional Full Instance LabelingADDRESSaddress : *number* oak avenue rent $ ....CONTACT..
( please include the address of this rental )ADDRESS... pm .
address : *number* marie street sausalito ...ADDRESS..
laundry .
address : *number* macarthur blvd ....Feature LabelingConditionalDistributionof LabelsGiven Word=addressADDRESSCONTACTFigure 1: Top: Traditional instance-labeling in which se-quences of contiguous tokens are annotated as to theircorrect label.
Bottom: Feature-labeling in which non-contiguous feature occurrences in context are labeled forthe purpose of deriving a conditional probability distribu-tion of labels given a particular feature.methods (Zhu and Ghahramani, 2002; Szummer andJaakkola, 2002) have all been applied to a limitedamount of fully labeled data in conjunction with un-labeled data to improve the accuracy of a classifier.In this paper, we explore an alternative approachin which, instead of fully labeled instances, thelearner has access to labeled features.
These fea-tures can often be labeled at a lower-cost to the hu-man annotator than labeling entire instances, whichmay require annotating the multiple sub-parts of asequence structure or tree.
Features can be labeledeither by specifying the majority label for a partic-ular feature or by annotating a few occurrences ofa particular feature in context with the correct label(Figure 1).To train models using this information we use870generalized expectation (GE) criteria.
GE criteriaare terms in a training objective function that as-sign scores to values of a model expectation.
Inparticular we use a version of GE that prefers pa-rameter settings in which certain model expectationsare close to target distributions.
Previous work hasshown how to apply GE criteria to maximum en-tropy classifiers.
In section 4, we extend GE crite-ria to semi-supervised learning of linear-chain con-ditional random fields, using conditional probabilitydistributions of labels given features.To empirically evaluate this method we compareit with several competing methods for CRF train-ing, including entropy regularization and expectedgradient, showing that GE provides significant im-provements.
We achieve competitive performancein comparison to alternate model families, in partic-ular generative models such as MRFs trained withEM (Haghighi and Klein, 2006) and HMMs trainedwith soft constraints (Chang et al, 2007).
Finally, inSection 5.3 we show that feature-labeling can lead todramatic reductions in the annotation time that is re-quired in order to achieve the same level of accuracyas traditional instance-labeling.2 Related WorkThere has been a significant amount of work onsemi-supervised learning with small amounts offully labeled data (see Zhu (2005)).
However therehas been comparatively less work on learning fromalternative forms of labeled resources.
One exam-ple is Schapire et al (2002) who present a methodin which features are annotated with their associatedmajority labels and this information is used to boot-strap a parameterized text classification model.
Un-like the model presented in this paper, they requiresome labeled data in order to train their model.This type of input information (features + major-ity label) is a powerful and flexible model for spec-ifying alternative inputs to a classifier, and has beenadditionally used by Haghighi and Klein (2006).
Inthat work, ?prototype?
features?words with theirassociated labels?are used to train a generativeMRF sequence model.
Their probability model canbe formally described as:p?
(x,y) =1Z(?
)exp(?k?kFk(x,y)).Although the partition function must be computedover all (x,y) tuples, learning via EM in this modelis possible because of approximations made in com-puting the partition function.Another way to gather supervision is by meansof prior label distributions.
Mann and McCallum(2007) introduce a special case of GE, label regular-ization, and demonstrate its effectiveness for train-ing maximum entropy classifiers.
In label regu-larization, the model prefers parameter settings inwhich the model?s predicted label distribution on theunsupervised data match a target distribution.
Notethat supervision here consists of the the full distribu-tion over labels (i.e.
conditioned on the maximumentropy ?default feature?
), instead of simply the ma-jority label.
Druck et al (2007) also use GE with fulldistributions for semi-supervised learning of maxi-mum entropy models, except here the distributionsare on labels conditioned on features.
In Section 4we describe how GE criteria can be applied to CRFsgiven conditional probability distributions of labelsgiven features.Another recent method that has been proposed fortraining sequence models with constraints is Changet al (2007).
They use constraints for approximateEM training of an HMM, incorporating the con-straints by looking only at the top K most-likelysequences from a joint model of likelihood and theconstraints.
This model can be applied to the combi-nation of labeled and unlabeled instances, but cannotbe applied in situations where only labeled featuresare available.
Additionally, our model can be easilycombined with other semi-supervised criteria, suchas entropy regularization.
Finally, their model is agenerative HMM which cannot handle the rich, non-independent feature sets that are available to a CRF.There have been relatively few different ap-proaches to CRF semi-supervised training.
One ap-proach has been that proposed in both Miller et al(2004) and Freitag (2004), uses distributional clus-tering to induce features from a large corpus, andthen uses these features to augment the feature spaceof the labeled data.
Since this is an orthogonalmethod for improving accuracy it can be combinedwith many of the other methods discussed above,and indeed we have obtained positive preliminaryexperimental results with GE criteria (not reportedon here).871Another method for semi-supervised CRF train-ing is entropy regularization, initially proposed byGrandvalet and Bengio (2004) and extended tolinear-chain CRFs by Jiao et al (2006).
In this for-mulation, the traditional label likelihood (on super-vised data) is augmented with an additional term thatencourages the model to predict low-entropy labeldistributions on the unlabeled data:O(?
;D,U) =?dlog p?(y(d)|x(d))?
?H(y|x).This method can be quite brittle, since the minimalentropy solution assigns all of the tokens the samelabel.1 In general, entropy regularization is fragile,and accuracy gains can come only with precise set-tings of ?.
High values of ?
fall into the minimalentropy trap, while low values of ?
have no effect onthe model (see (Jiao et al, 2006) for an example).When some instances have partial labelings (i.e.labels for some of their tokens), it is possible to trainCRFs via expected gradient methods (Salakhutdinovet al, 2003).
Here a reformulation is presented inwhich the gradient is computed for a probability dis-tribution with a marginalized hidden variable, z, andobserved training labels y:?L(?)
=???
?zlog p(x, y, z; ?
)=?zp(z|y, x)fk(x, y, z)?
?z,y?p(z, y?|x; ?
)fk(x, y, z).In essence, this resembles the standard gradient forthe CRF, except that there is an additional marginal-ization in the first term over the hidden variable z.This type of training has been applied by Quattoniet al (2007) for hidden-state conditional randomfields, and can be equally applied to semi-supervisedconditional random fields.
Note, however, that la-beling variables of a structured instance (e.g.
to-kens) is different than labeling features?being bothmore coarse-grained and applying supervision nar-rowly only to the individual subpart, not to all placesin the data where the feature occurs.1In the experiments in this paper, we use ?
= 0.001, whichwe tuned for best performance on the test set, giving an unfairadvantage to our competitor.Finally, there are some methods that use auxil-iary tasks for training sequence models, though theydo not train linear-chain CRFs per se.
Ando andZhang (2005) include a cluster discovery step intothe supervised training.
Smith and Eisner (2005)use neighborhoods of related instances to figure outwhat makes found instances ?good?.
Although thesemethods can often find good solutions, both are quitesensitive to the selection of auxiliary information,and making good selections requires significant in-sight.23 Conditional Random FieldsLinear-chain conditional random fields (CRFs) are adiscriminative probabilistic model over sequences xof feature vectors and label sequences y = ?y1..yn?,where |x| = |y| = n, and each label yi has s dif-ferent possible discrete values.
This model is anal-ogous to maximum entropy models for structuredoutputs, where expectations can be efficiently calcu-lated by dynamic programming.
For a linear-chainCRF of Markov order one:p?
(y|x) =1Z(x)exp(?k?kFk(x,y)),where Fk(x,y) =?i fk(x, yi, yi+1, i),and the partition function Z(x) =?y exp(?k ?kFk(x,y)).
Given training dataD =?(x(1),y(1))..
(x(n),y(n))?, the model is tra-ditionally trained by maximizing the log-likelihoodO(?
;D) =?d log p?
(y(d)|x(d)) by gradient ascentwhere the gradient of the likelihood is:???kO(?
;D) =?dFk(x(d),y(d))??d?yp?
(y|x(d))Fk(x(d),y).The second term (the expected counts of the featuresgiven the model) can be computed in a tractableamount of time, since according to the Markov as-2Often these are more complicated than picking informativefeatures as proposed in this paper.
One example of the kind ofoperator used is the transposition operator proposed by Smithand Eisner (2005).872sumption, the feature expectations can be rewritten:?yp?
(y|x)Fk(x,y) =?i?yi,yi+1p?
(yi, yi+1|x)fk(x, yi, yi+1, i).A dynamic program (the forward/backward algo-rithm) then computes in time O(ns2) all the neededprobabilities p?
(yi, yi+1), where n is the sequencelength, and s is the number of labels.4 Generalized Expectation Criteria forConditional Random FieldsPrior semi-supervised learning methods have aug-mented a limited amount of fully labeled data witheither unlabeled data or with constraints (e.g.
fea-tures marked with their majority label).
GE crite-ria can use more information than these previousmethods.
In particular GE criteria can take advan-tage of conditional probability distributions of la-bels given a feature (p(y|fk(x) = 1)).
This in-formation provides richer constraints to the modelwhile remaining easily interpretable.
People havegood intuitions about the relative predictive strengthof different features.
For example, it is clear thatthe probability of label PERSON given the featureWORD=JOHN is high, perhaps around 0.95, whereas for WORD=BROWN it would be lower, perhaps0.4.
These distributions need not be not estimatedwith great precision?it is far better to have the free-dom to express shades of gray than to be force intoa binary supervision signal.
Another advantage ofusing conditional probability distributions as prob-abilistic constraints is that they can be easily esti-mated from data.
For the feature INITIAL-CAPITAL,we identify all tokens with the feature, and thencount the labels with which the feature co-occurs.GE criteria attempt to match these conditionalprobability distributions by model expectations onunlabeled data, encouraging, for example, the modelto predict that the proportion of the label PERSONgiven the word ?john?
should be .95 over all of theunlabeled data.In general, a GE (generalized expectation) crite-rion (McCallum et al, 2007) expresses a preferenceon the value of a model expectation.
One kind ofpreference may be expressed by a distance function?, a target expectation f?
, data D, a function f , anda model distribution p?, the GE criterion objectivefunction term is ?(f?
, E[f(x)]).
For the purposesof this paper, we set the functions to be conditionalprobability distributions and set ?
(p, q) = D(p||q),the KL-divergence between two distributions.3 Forsemi-supervised training of CRFs, we augment theobjective function with the regularization term:O(?
;D,U) =?dlog p?(y(d)|x(d))?
?k ?k2?2?
?D(p?||p??
),where p?
is given as a target distribution andp??
= p??
(yj |fm(x, j) = 1)=1Um?x?Um?j?p?
(y?j |x),with the unnormalized potentialq??
= q??
(yj |fm(x, j) = 1) =?x?Um?j?p?
(y?j |x),where fm(x, j) is a feature that depends only onthe observation sequence x, and j?
is defined as{j : fm(x, j) = 1}, and Um is the set of sequenceswhere fm(x, j) is present for some j.4Computing the GradientTo compute the gradient of the GE criteria,D(p?||p??
), first we drop terms that are constant withrespect to the partial derivative, and we derive thegradient as follows:???k?lp?
log q??
=?lp?q?????kq??=?lp?q???x?U?j????kp?(yj?
= l|x)=?lp?q???x?U?j??y?j????kp?(yj?
= l,y?j?
|x),where y?j = ?y1..(j?1)y(j+1)..n?.
The last step fol-lows from the definition of the marginal probability3We are actively investigating different choices of distancefunctions which may have different generalization properties.4This formulation assumes binary features.873P (yj |x).
Now that we have a familiar form in whichwe are taking the gradient of a particular label se-quence, we can continue:=?lp?q???x?U?j??y?j?p?(yj?
= l,y?j?
|x)Fk(x,y)??lp?q???x?U?j??y?j?p?(yj?
= l,y?j?
|x)?y?p?(y?|x)Fk(x,y)=?lp?q??
?x?U?i?yi,yi+1fk(x, yi, yi+1, i)?j?p?
(yi, yi+1, yj?
= l|x)??lp?q??
?x?U?i?yi,yi+1fk(x, yi, yi+1, i)p?
(yi, yi+1|x)?j?p?(yj?
= l|x).After combining terms and rearranging we arrive atthe final form of the gradient:=?x?U?i?yi,yi+1fk(x, yi, yi+1, i)?lp?q???(?j?p?
(yi, yi+1, yj?
= l|x)?p?
(yi, yi+1|x)?j?p?(yj?
= l|x)).Here, the second term is easily gathered from for-ward/backward, but obtaining the first term is some-what more complicated.
Computing this termnaively would require multiple runs of constrainedforward/backward.
Here we present a more ef-ficient method that requires only one run of for-ward/backward.5 First we decompose the prob-ability into two parts:?j?
p?
(yi, yi+1, yj?
=l|x) =?ij=1 p?
(yi, yi+1, yj = l|x)I(j ?
j?)
+?Jj=i+1 p?
(yi, yi+1, yj = l|x)I(j ?
j?).
Next, weshow how to compute these terms efficiently.
Simi-lar to forward/backward, we build a lattice of inter-mediate results that then can be used to calculate the5(Kakade et al, 2002) propose a related method that com-putes p(y1..i = l1..i|yi+1 = l).quantity of interest:i?j=1p?
(yi, yi+1, yj = l|x)I(j ?
j?
)= p(yi, yi+1|x)?
(yi, l)I(i ?
j?)+i?1?j=1p?
(yi, yi+1, yj = l|x)I(j ?
j?
)= p(yi, yi+1|x)?
(yi, l)I(i ?
j?)+???yi?1i?1?j=1p?
(yi?1, yi, yj = l|x)I(j ?
j?)??p?
(yi+1|yi,x).For efficiency,?yi?1?i?1j=1 p?
(yi?1, yi, yj =l|x)I(j ?
j?)
is saved at each stage in the lat-tice.
?Jj=i+1 p?
(yi?1, yi, yj = l|x)I(j ?
j?)
canbe computed in the same fashion.
To compute thelattices it takes time O(ns2), and one lattice must becomputed for each label so the total time is O(ns3).5 Experimental ResultsWe use the CLASSIFIEDS data provided by Grenageret al (2005) and compare with results reportedby HK06 (Haghighi and Klein, 2006) and CRR07(Chang et al, 2007).
HK06 introduced a set of 33features along with their majority labels, these arethe primary set of additional constraints (Table 1).As HK06 notes, these features are selected usingstatistics of the labeled data, and here we used sim-ilar features here in order to compare with previousresults.
Though in practice we have found that fea-ture selection is often intuitive, recent work has ex-perimented with automatic feature selection usingLDA (Druck et al, 2008).
For some of the exper-iments we also use two sets of 33 additional fea-tures that we chose by the same method as HK06,the first 33 of which are also shown in Table 1.
Weuse the same tokenization of the dataset as HK06,and training/test/unsupervised sets of 100 instanceseach.
This data differs slightly from the tokenizationused by CRR07.
In particular it lacks the newlinebreaks which might be a useful piece of information.There are three types of supervised/semi-supervised data used in the experiments.
Labeledinstances are the traditional or conventionally874Label HK06: 33 Features 33 Added FeaturesCONTACT *phone* call *time please appointment moreFEATURES kitchen laundry parking room new largeROOMMATES roommate respectful drama i bit meanRESTRICTIONS pets smoking dog no sorry catsUTILITIES utilities pays electricity water garbage includedAVAILABLE immediately begin cheaper *month* now *ordinal*0SIZE *number*1*1 br sq *number*0*1 bedroom bathPHOTOS pictures image link *url*long click photosRENT *number*15*1 $ month deposit lease rentNEIGHBORHOOD close near shopping located bart downtownADDRESS address carlmont ave san *ordinal*5 #Table 1: Features and their associated majority label.Features for each label were chosen by the method de-scribed in HK06 ?
top frequency for that label and nothigher frequency for any other label.+ SVD featuresHK06 53.7% 71.5%CRF + GE/Heuristic 66.9% 68.3%Table 2: Accuracy of semi-supervised learning methodswith majority labeled features alone.
GE outperformsHK06 when neither model has access to SVD features.When SVD features are included, HK06 has an edge inaccuracy.labeled instances used for estimation in traditionalCRF training.
Majority labeled features are fea-tures annotated with their majority label.6 Labeledfeatures are features m where the distributionp(yi|fm(x, i)) has been specified.
In Section 5.3 weestimate these distributions from isolated labeledtokens.We evaluate the system in two scenarios: (1) withfeature constraints alone and (2) feature constraintsin conjunction with a minimal amount of labeled in-stances.
There is little prior work that demonstratesthe use of both scenarios; CRR07 can only be ap-plied when there is some labeled data, while HK06could be applied in both scenarios though there areno such published experiments.5.1 Majority Labeled Features OnlyWhen using majority labeled features alone, it canbe seen in Table 2 that GE is the best performingmethod.
This is important, as it demonstrates thatGE out of the box can be used effectively, withouttuning and extra modifications.6While HK06 and CRR07 require only majority labeled fea-tures, GE criteria use conditional probability distributions of la-bels given features, and so in order to apply GE we must decideon a particular distribution for each feature constraint.
In sec-tions 5.1 and 5.2 we use a simple heuristic to derive distribu-tions from majority label information: we assign .99 probabil-ity to the majority label of the feature and divide the remainingprobability uniformly among the remainder of the labels.Labeled Instances10 25 100supervised HMM 61.6% 70.0% 76.3%supervised CRF 64.6% 72.9% 79.4%CRF+ Entropy Reg.
67.3% 73.7% 79.5%CRR07 70.9% 74.8% 78.6%+ inference constraints 74.7% 78.5% 81.7%CRF+GE/Heuristic 72.6% 76.3% 80.1%Table 3: Accuracy of semi-supervised learning meth-ods with constraints and limited amounts of trainingdata.
Even though CRR07 uses more constraints and re-quires additional development data for estimating mix-ture weights, GE still outperforms CRR07 when that sys-tem is run without applying constraints during inference.When these constraints are applied during test-time infer-ence, CRR07 has an edge over the CRF trained with GEcriteria.In their original work, HK06 propose a methodfor generating additional features given a set of ?pro-totype?
features (the feature constraints in Table 1),which they demonstrate to be highly effective.
Intheir method, they collect contexts around all wordsin the corpus, then perform a SVD decomposition.They take the first 50 singular values for all words,and then if a word is within a thresholded distanceto a prototype feature, they assign that word a newfeature which indicates close similarity to a proto-type feature.
When SVD features such as these aremade available to the systems, HK06 has a higheraccuracy.7 For the remainder of the experiments weuse the SVD feature enhanced data sets.8We ran additional experiments with expected gra-dient methods but found them to be ineffective,reaching around 50% accuracy on the experimentswith the additional SVD features, around 20% lessthan the competing methods.5.2 Majority Labeled Features and LabeledInstancesLabeled instances are available, the technique de-scribed in CRR07 can be used.
While CRR07 isrun on the same data set as used by HK06, a directcomparison is problematic.
First, they use additionalconstraints beyond those used in this paper and those7We generated our own set of SVD features, so they mightnot match exactly the SVD features described in HK06.8One further experiment HK06 performs which we do notduplicate here is post-processing the label assignments to betterhandle field boundaries.
With this addition they realize another2.5% improvement.875used by HK06 (e.g.
each contiguous label sequencemust be at least 3 labels long)?so their results can-not be directly compared.
Second, they require addi-tional training data to estimate weights for their softconstraints, and do not measure how much of thisadditional data is needed.
Third, they use a slightlydifferent tokenization procedure.
Fourth, CRR07uses different subsets of labeled training instancesthan used here.
For these reasons, the comparisonbetween the method presented here and CRR07 can-not be exact.The technique described in CRR07 can be appliedin two ways: constraints can be applied during learn-ing, and they can also be applied during inference.We present comparisons with both of these systemsin Table 3.
CRFs trained with GE criteria consis-tently outperform CRR07 when no constraints areapplied during inference time, even though CRR07has additional constraints.
When the method inCRR07 is applied with constraints in inference time,it is able to outperform CRFs trained with GE.
Wetried adding the additional constraints described inCRR07 during test-time inference in our system, butfound no accuracy improvement.
After doing errorinspection, those additional constraints weren?t fre-quently violated by the GE trained method, whichalso suggests that adding them wouldn?t have a sig-nificant effect during training either.
It is possiblethat for GE training there are alternative inference-time constraints that would improve performance,but we didn?t pursue this line of investigation asthere are benefits to operating within a formal prob-abilistic model, and eschewing constraints appliedduring inference time.
Without these constraints,probabilistic models can be combined easily withone another in order to arrive at a joint model, andadding in these constraints at inference time compli-cates the nature of the combination.5.3 Labeled Features vs.
Labeled InstancesIn the previous section, the supervision signal wasthe majority label of each feature.9 Given a featureof interest, a human can gather a set of tokens thathave this feature and label them to discover the cor-9It is not clear how these features would be tagged with ma-jority label in a real use case.
Tagging data to discover the ma-jority label could potentially require a large number of taggedinstances before the majority label was definitively identified.AccuracyTokens0.450.50.550.60.650.70.750.80.8510  100  1000  10000  100000Traditional Instance Labeling33 Labeled Features66 Labeled Features99 Labeled FeaturesCRR07 + inference time constraintsFigure 2: Accuracy of supervised and semi-supervisedlearning methods for fixed numbers of labeled tokens.Training a GE model with only labeled features sig-nificantly outperforms traditional log-likelihood trainingwith labeled instances for comparable numbers of labeledtokens.
When training on less than 1500 annotated to-kens, it also outperforms CRR07 + inference time con-straints, which uses not only labeled tokens but additionalconstraints and development data for estimating mixtureweights.Labeled Instances0 10 25 100HK06 71.5% - - -GE/Heuristic 68.3% 72.6% 76.3% 80.1%GE/Sampled 73.0% 74.6% 77.2% 80.5%Table 4: Accuracy of semi-supervised learning methodscomparing the effects of (1) a heuristic for setting con-ditional distributions of labels given features and (2) es-timating this distributions via human annotation.
WhenGE is given feature distributions are better than the sim-ple heuristic it is able to realize considerable gains.relation between the feature and the labels.10 Whilethe resulting label distribution information could notbe fully utilized by previous methods (HK06 andCRR07 use only the majority label of the word), itcan, however, be integrated into the GE criteria byusing the distribution from the relative proportionsof labels rather than a the previous heuristic distri-bution.
We present a series of experiments that testthe advantages of this annotation paradigm.To simulate a human labeler, we randomly sam-ple (without replacement) tokens with the particu-lar feature in question, and generate a label usingthe human annotations provided in the data.
Thenwe normalize and smooth the raw counts to obtain a10In this paper we observe a 10x speed-up by using isolatedlabeled tokens instead of a wholly labeled instances?so evenif it takes slightly longer to label isolated tokens, there will stillbe a substantial gain.876conditional probability distribution over labels givenfeature.
We experiment with samples of 1, 2,5, 10,100 tokens per feature, as well as with all availablelabeled data.
We sample instances for labeling ex-clusively from the training and development data,not from the testing data.
We train a model using GEwith these estimated conditional probability distri-butions and compare them with corresponding num-bers of tokens of traditionally labeled instances.Training from labeled features significantly out-performs training from traditional labeled instancesfor equivalent numbers of labeled tokens (Figure2).
With 1000 labeled tokens, instance-labelingachieves accuracy around 65%, while labeling 33features reaches 72% accuracy.11 To achieve thesame level of performance as traditional instance la-beling, it can require as much as a factor of ten-foldfewer annotations of feature occurrences.
For exam-ple, the accuracy achieved after labeling 257 tokensof 33 features is 71% ?
the same accuracy achievedonly after labeling more than 2000 tokens in tradi-tional instance-labeling.12Assuming that labeling one token in isolationtakes the same time as labeling one token in asequence, these results strongly support a newparadigm of labeling in which instead of annotat-ing entire sentences, the human instead selects somekey features of interest and labels tokens that havethis feature.
Particularly intriguing is the flexibilityour scenario provides for the selection of ?featuresof interest?
to be driven by error analysis.Table 4 compares the heuristic method describedabove against sampled conditional probability distri-butions of labels given features13.
Sampled distribu-tions yield consistent improvements over the heuris-tic method.
The accuracy with no labeled instances(73.0%) is better than HK06 (71.5%), which demon-strates that the precisely estimated feature distribu-tions are helpful for improving accuracy.Though accuracy begins to level off with distri-11Labeling 99 features with 1000 tokens reaches nearly 76%.12Accuracy at one labeled token per feature is much worsethan accuracy with majority label information.
This due to thenoise introduced by sampling, as there is the potential for a rel-atively rare label be sampled and labeled, and thereby train thesystem on a non-canonical supervision signal.13Where the tokens labeled is the total available number inthe data, roughly 2500 tokens.00.20.40.60.810  2  4  6  8  10  12ProbabilityLabel  00.20.40.60.810  2  4  6  8  10  12ProbabilityLabelFigure 3: From left to right: distributions (with standarderror) for the feature WORD=ADDRESS obtained fromsampling, using 1 sample per feature and 10 samples perfeature.
Labels 1, 2, 3, and 9 are (respectively) FEA-TURES, CONTACT, SIZE, and ADDRESS.
Instead of moreprecisely estimating these distributions, it is more benefi-cial to label a larger set of features.butions over the original set of 33 labeled features,we ran additional experiments with 66 and 99 la-beled features, whose results are also shown in Fig-ure 2.14 The graph shows that with an increasednumber of labeled features, for the same numbersof labeled tokens, accuracy can be improved.
Thereason behind this is clear?while there is some gainfrom increased precision of probability estimates (asthey asymptotically approach their ?true?
values asshown in Figure 3), there is more information to begained from rougher estimates of a larger set of fea-tures.
One final point about these additional featuresis that their distributions are less peaked than theoriginal feature set.
Where the original feature setdistribution has entropy of 8.8, the first 33 added fea-tures have an entropy of 22.95.
Surprisingly, evenambiguous feature constraints are able to improveaccuracy.6 ConclusionWe have presented generalized expectation criteriafor linear-chain conditional random fields, a newsemi-supervised training method that makes use oflabeled features rather than labeled instances.
Pre-vious semi-supervised methods have typically usedad-hoc feature majority label assignments as con-straints.
Our new method uses conditional proba-bility distributions of labels given features and candramatically reduce annotation time.
When thesedistributions are estimated by means of annotatedfeature occurrences in context, there is as much asa ten-fold reduction in the annotation time that is re-quired in order to achieve the same level of accuracyover traditional instance-labeling.14Also note that for less than 1500 tokens of labeling, the 99labeled features outperform CRR07 with inference time con-straints.877ReferencesR.
K. Ando and T. Zhang.
2005.
A framework for learn-ing predictive structures from multiple tasks and unla-beled data.
JMLR, 6.M.-W. Chang, L. Ratinov, and D. Roth.
2007.
Guidingsemi-supervision with constraint-driven learning.
InACL.G.
Druck, G. Mann, and A. McCallum.
2007.
Lever-aging existing resources using generalized expectationcriteria.
In NIPS Workshop on Learning Problem De-sign.G.
Druck, G. S. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In SIGIR.D.
Freitag.
2004.
Trained named entity recognition usingdistributional clusters.
In EMNLP.Y.
Grandvalet and Y. Bengio.
2004.
Semi-supervisedlearning by entropy minimization.
In NIPS.T.
Grenager, D. Klein, and C. Manning.
2005.
Unsuper-vised learning of field segmentation models for infor-mation extraction.
In ACL.A.
Haghighi and D. Klein.
2006.
Prototype-driver learn-ing for sequence models.
In NAACL.F.
Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schu-urmans.
2006.
Semi-supervised conditional randomfields for improved sequence segmentation and label-ing.
In COLING/ACL.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InICML.S.
Kakade, Y-W. Teg, and S.Roweis.
2002.
An alternateobjective function for markovian fields.
In ICML.G.
Mann and A. McCallum.
2007.
Simple, robust, scal-able semi-supervised learning via expectation regular-ization.
In ICML.A.
McCallum, G. S. Mann, and G. Druck.
2007.
Gener-alized expectation criteria.
Computer science techni-cal note, University of Massachusetts, Amherst, MA.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Nametagging with word clusters and discriminative training.In ACL.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.1998.
Learning to classify text from labeled and un-labeled documents.
In AAAI.A.
Quattoni, S. Wang, L-P. Morency, M. Collins, andT.
Darrell.
2007.
Hidden-state conditional randomfields.
In PAMI.H.
Raghavan, O. Madani, and R. Jones.
2006.
Activelearning with feedback on both features and instances.JMLR.R.
Salakhutdinov, S. Roweis, and Z. Ghahramani.
2003.Optimization with em and expectation-conjugate-gradient.
In ICML.R.
Schapire, M. Rochery, M. Rahim, and N. Gupta.2002.
Incorporating prior knowledge into boosting.In ICML.N.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In ACL.Martin Szummer and Tommi Jaakkola.
2002.
Partiallylabeled classification with markov random walks.
InNIPS, volume 14.X.
Zhu and Z. Ghahramani.
2002.
Learning from labeledand unlabeled data with label propagation.
TechnicalReport CMU-CALD-02-107, CMU.X.
Zhu.
2005.
Semi-supervised learning lit-erature survey.
Technical Report 1530, Com-puter Sciences, University of Wisconsin-Madison.http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.878
