Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 38?44,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSemantic Analysis and Helpfulness Prediction of Textfor Online Product ReviewsYinfei YangAmazon Inc.Seattle, WA 98121yangyin7@ gmail.comYaowei YanDept.
of Electrical & Computer EngineeringUniversity of AkronAkron, OH 44325-3904yy28@ uakron.eduMinghui QiuAlibaba GroupHangzhou, China 311121minghuiqiu@ gmail.comForrest Sheng BaoDept.
of Electrical & Computer EngineeringUniversity of AkronAkron, OH 44325-3904forrest.bao@ gmail.comAbstractPredicting the helpfulness of product re-views is a key component of many e-commerce tasks such as review rankingand recommendation.
However, previouswork mixed review helpfulness predictionwith those outer layer tasks.
Using non-text features, it leads to less transferablemodels.
This paper solves the problemfrom a new angle by hypothesizing thathelpfulness is an internal property of text.Purely using review text, we isolate re-view helpfulness prediction from its outerlayer tasks, employ two interpretable se-mantic features, and use human scoringof helpfulness as ground truth.
Experi-mental results show that the two seman-tic features can accurately predict helpful-ness scores and greatly improve the per-formance compared with using featurespreviously used.
Cross-category test fur-ther shows the models trained with seman-tic features are easier to be generalizedto reviews of different product categories.The models we built are also highly inter-pretable and align well with human anno-tations.1 IntroductionProduct reviews have influential impact to onlineshopping as consumers tend to read product re-views when finalizing purchase decisions (Duan etal., 2008).
However, a popular product usually hastoo many reviews for a consumer to read.
There-fore, reviews need to be ranked and recommendedto consumers.
In particular, review helpfulnessplays a critical role in review ranking and recom-mendation (Ghose and Ipeirotis, 2011; Mudambiand Schuff, 2010; Danescu-Niculescu-Mizil et al,2009).
The simple question ?Was this review help-ful to you??
increases an estimated $2.7B revenueto Amazon.com annually1.However, existing literature solves helpfulnessprediction together with its outer layer task, thereview ranking (Kim et al, 2006; O?Mahony andSmyth, 2010; Liu et al, 2008; Martin and Pu,2014).
Those studies use features not contribut-ing to helpfulness, such as date (Liu et al, 2008),or features making the model less transferable,such as product type (Mudambi and Schuff, 2010).Models built in these ways are also difficult to in-terpret from linguistic perspective.Therefore, it is necessary to isolate review help-fulness prediction from its outer layer tasks andformulate it as a new problem.
In this way, mod-els can be more robust and generalizable.
Beyondpredicting whether a review is helpful, we can alsounderstand why it is helpful.
In our approach, theresults can also facilitate many other tasks, such asreview summarization (Xiong and Litman, 2014)and sentiment extraction (Hu and Liu, 2004).Recent NLP studies reveal the connection be-tween text style and its properties, include read-ability (Agichtein et al, 2008), informative-ness (Yang and Nenkova, 2014) and trustworthi-ness (Pasternack and Roth, 2011) of text.
Hence,we hypothesize that helpfulness is also an under-lying property of text.To understand the essence of review text, weleverage existing linguistic and psychological dic-tionaries and represent reviews in semantic dimen-sions.
Two semantic features that are new to solv-ing this problem, LIWC (Pennebaker et al, 2007)and INQUIRER (Stone et al, 1962), are employedin this work.
The intuition behind is that peopleusually embed semantic meanings, such as emo-tion and reasoning, into text.
For example, the re-1http://www.uie.com/articles/magicbehindamazon/38view ?With the incredible brightness of the main LED, thislight is visible from a distance on a sunny day at noon.
ismore helpful than the review ?I ordered an iPad, Ireceived an iPad.
I got exactly what I ordered which makesme satisfied.
Thanks!?
because the former mentionsuser experience and functionality of the productwhile the latter has emotional statements only.Previous work approximates the ground truth ofhelpfulness from users?
votes using ?X of Y ap-proach?
: if X of Y users think a review is help-ful, then the helpfulness score of the review isthe ratio X/Y .
However, not many reviews havestatistically abundant votes, i.e., a very small Y .Fewer than 20% of the reviews in Amazon ReviewDataset (McAuley and Leskovec, 2013) have atleast 5 votes (Table 1) while only 0.44% have 100+votes.
In addition, the review voting itself may bebiased (Danescu-Niculescu-Mizil et al, 2009; Caoet al, 2011).
Therefore, we proactively recruitedhuman annotators and let them score the helpful-ness of reviews in our dataset.We model the problem of predicting reviewhelpfulness score as a regression problem.
Ex-perimental results show that it is feasible to usetext-only features to accurately predict helpful-ness scores.
The two semantic features signifi-cantly outperform baseline features used in previ-ous work.
In cross-category test, the two semanticfeatures show good transferability.
To interpret themodels, we analyze the semantic features and findthat Psychological Process plays an important rolein review text helpfulness.
Words reflecting think-ing and understanding are more related to helpfulreviews while emotional words are not.
Lastly, wevalidate the models trained on ?X of Y approach?data on human annotated data and achieve highlycorrelated prediction.2 DatasetTwo subsets of reviews are constructed from Ama-zon Review Dataset (McAuley and Leskovec,2013), which includes nearly 35 million reviewsfrom Amazon.com between 1995 and 2013.
Asubset of 696,696 reviews from 4 categories:Books, Home (home and kitchen), Outdoors andElectronics, are chosen in this research.
For eachcategory, we select the top 100 products with themost reviews and then include all reviews relatedto the selected products for analysis.
Each reviewcomes with users?
helpfulness votes and hencehelpfulness score can be approximated using ?Xof Y approach.?
Finally, 115,880 reviews, each ofwhich has at least 5 votes, form the automatic la-beled dataset (Table 1).Table 1: Number of Reviews for Each CategoryCategoryTotal numberof reviewsNumber of reviewswith at least 5 votes, se-lected for experimentsBooks391,666 81,014 (20.7%)Home116,194 13,331 (11.5%)Outdoors52,838 6,158 (11.7%)Electronics135,998 15,377 (11.3%)Overall696,696 115,880 (16.6%)In addition, we also create the human labeleddataset.
As mentioned earlier, the X of Y ap-proach may not be a good approximation to help-fulness.
A better option is human scoring.
Werandomly select 400 reviews outside of the au-tomatic labeled dataset, 100 from each category.Eight students annotated these reviews in a fash-ion similar to that in (Bard et al, 1996) by as-signing real-value scores (?
[0, 100]) to each re-view.
Review text was the only information givento them.
The average helpfulness score of allvalid annotations is used as the ground truth foreach review.
We have released the human annota-tion data at https://sites.google.com/site/forrestbao/acl_data.tar.bz2 .3 FeaturesDriven by the hypothesis that helpfulness is an un-derlying feature of text itself, we consider text-based features only.
Features used in previous re-lated work, namely Structure (STR) (Kim et al,2006; Xiong and Litman, 2011), Unigram (Kim etal., 2006; Xiong and Litman, 2011; Agarwal et al,2011) and GALC emotion (Martin and Pu, 2014),are considered as baselines.We then introduce two semantic features LIWCand General Inquirer (INQUIRER) for easy map-ping from text to human sense, including emo-tions, writing styles, etc.
Our rationale for thetwo semantic features is that a helpful review in-cludes opinions, analyses, emotions and personalexperiences, etc.
These two features have beenproven effective in other semantic analysis tasksand hence we are here giving them a try for study-ing review helpfulness.
We leave the study of us-ing more sophisticated features like syntactic anddiscourse representations to future work.
All fea-tures except UGR are independent of training data.STR Following the (Xiong and Litman, 2011),we use the following structural features: totalnumber of tokens, total number of sentences, av-erage length of sentences, number of exclamationmarks, and the percentage of question sentences.39UGR Unigram feature has been demonstratedas a very reliable feature for review helpfulnessprediction in previous work.
We build a vocab-ulary with all stopwords and non-frequent words(df < 3) removed.
Each review is represented bythe vocabulary with tf ?
idf weighting for eachappeared term.GALC (Geneva Affect Label Coder) (Scherer,2005) proposes to recognize 36 effective statescommonly distinguished by words.
Similar to(Martin and Pu, 2014), we construct a featurevector with the number of occurrences of eachemotion plus one additional dimension for non-emotional words.LIWC (Linguistic Inquiry and Word Count)(Pennebaker et al, 2007) is a dictionary whichhelps users to determine the degree that any textuses positive or negative emotions, self-referencesand other language dimensions.
Each word inLIWC is assigned 1 or 0 for each language dimen-sion.
For each review, we sum up the values of allwords for each dimension.
Eventually each reviewis represented by a histogram of language dimen-sions.
We employ the LIWC2007 English dictio-nary which contains 4,553 words with 64 dimen-sions in our experiments.INQUIRER General Inquirer (Stone et al,1962) is a dictionary in which words are groupedin categories.
It is basically a mapping tool whichmaps each word to some semantic tags, e.g., ab-surd is mapped to tags NEG and VICE.
The dic-tionary contains 182 categories and a total of 7,444words.
Like for LIWC representation, we computethe histogram of categories for each review.4 ExperimentsUp to this point, we are very interested in firstwhether a prediction model learned for one cat-egory can be generalized to a new category, andsecond what elements make a review helpful.
Inother words, we want to know the robustness ofour approach and the underlying reasons.In this section we will evaluate the effectivenessof each of the features as well as the combinationof them.
For convenience, we use FusionSemanticto denote the combination of GALC, LIWC andINQUIRER, and FusionAllto denote the combi-nation of all features.
Because STR and UGR arewidely used in previous work, we use them as twobaselines.
GALC has been introduced for this taskas an emotion feature before, so we use it as thethird baseline.
STR, URG and GALC are used as3 baselines.
For predicting helpfulness scores, weuse SVM regressor with RBF kernel provided byLibSVM (Chang and Lin, 2011).Two kinds of labels are used: automatic labelsobtained in ?X of Y approach?
from votes, andhuman labels made by human annotators.
Per-formance is evaluated by Root Mean Square Er-ror (RMSE) and Pearson?s correlation coefficients.Ten-fold cross-validation is performed for all ex-periments.4.1 Results using Automatic LabelsBefore studying the transferability of models, wefirst need to make sure that models work well onreviews of products of the same category.4.1.1 RMSERMSE and correlation coefficient using automaticlabels are given in Table 2 and Table 3 respec-tively.
Each row corresponds to the model trainedby a feature or a combination of features, whileeach column corresponds to one product category.The lowest RMSE achieved using every single fea-ture in each category is marked in bold.The two newly employed semantic features,LIWC and INQUIRER, have 8% lower RMSEon average than UGR, the best baseline feature.FusionAllhas the best overall RMSE, rangingfrom 0.200 to 0.265.
FusionSemantichas the sec-ond best performance on average.
It achieves thelowest RMSE in Books category.Table 2: RMSE (the lower the better) using auto-matic labelsBooks Home Outdoors Electro.
AverageSTR 0.239 0.289 0.314 0.307 0.287UGR 0.242 0.260 0.284 0.286 0.268GALC 0.266 0.290 0.310 0.308 0.365LIWC 0.188 0.256 0.279 0.278 0.250INQUIRER 0.193 0.248 0.274 0.273 0.247FusionSemantic0.187 0.248 0.272 0.268 0.244FusionAll0.200 0.247 0.261 0.265 0.243Table 3: Correlation coefficients (the higher thebetter) using automatic labels.
All correlations arehighly significant, with p < 0.001.Books Home Outdoors ElectronicsSTR 0.500 0.280 0.333 0.351UGR 0.507 0.467 0.458 0.471GALC 0.239 0.216 0.255 0.274LIWC 0.742 0.439 0.424 0.475INQUIRER 0.720 0.487 0.455 0.498FusionSemantic0.744 0.490 0.467 0.527FusionAll0.682 0.525 0.535 0.5394.1.2 Correlation CoefficientIn line with RMSE measurements, the seman-tic feature based models outperform the baseline40features in terms of correlation coefficient (Ta-ble 3).
In each category, the highest correla-tion coefficient is achieved by using LIWC orINQUIRER, with only one exception (Outdoors).The two fusion models further improve the re-sults.
FusionSemantichas the highest coefficientsin Books category while FusionAllhas the highestcoefficients in other 3 categories.4.2 Cross Category TestOne motivation of introducing semantic featuresis that, unlike UGR which is category-dependent,they can be more transferable.
To validate thetransferability of semantic features, we performcross category test by using the model trained fromone category to predict the helpfulness scores ofreviews in other categories.
GALC is excluded inthis analysis due to its poor performance earlier.Training Category BooksHomeOutdoorElectronicsTesting CategoryBooksHomeOutdoorElectronicsCorrelationCoefficient0.00.20.40.60.81.0INQUIRERLIWCSTRUGRFigure 1: Normalized cross-category correlationcoefficientsModel transferability from Category A to Cate-gory B cannot be measured simply by the perfor-mance when using A as the training set and B asthe test set.
Instead, it should be compared rela-tively with the performance when using A as boththe training and test sets.
There are 4 categoriesin our dataset, and the performances on the 4 cate-gories vary (Tables 2 and 3).
In order to provide afair comparison, we normalize cross-category cor-relation coefficients by the corresponding same-category ones, i.e., cross-category correlation co-efficient / correlation coefficient on training cate-gory.
For example, the 3 cross-category correla-tion coefficients of using Books category as train-ing set are all normalized by the correlation coef-ficient when using Books as both training and testsets earlier.
A normalized correlation coefficientof 0 means the prediction on the test category israndom, and thus the model has no transferabil-ity, while 1 means as accurate as predicting on thetraining category, and thus the model is fully trans-ferable.Results on transferrability are visualized in Fig-ure 1 with same-category correlation coefficientsignored as they are always 1.
Correlation coef-ficients of 4 features are clustered for each pairof training and testing categories and are color-coded.It is shown that INQUIRER and STR are twobest features in cross category test, leading in mostof the category pairs.
LIWC follows, achieving atleast 70% of the same-category correlation coeffi-cients in most cases.
The UGR feature, however,performs poorly in this test.
In most cases, the cor-relation coefficients have been halved, comparedwith same-category results.According to the results, we can conclude thatsemantic features are accurate and transferable,UGR is accurate but is not transferable, and STRis transferable but not accurate enough (Figure 2).Accurate TransferableUGR STRLIWCandINQUIRERFigure 2: Classification of features based on ex-perimental results4.3 What Makes a Review Helpful: ASemantic InterpretationLIWC and INQUIRER not only have better per-formances than previously used features but alsoprovide us a good semantic interpretation to whatmakes a review helpful.
We analyze the correla-tion coefficients between helpfulness and each lan-guage dimension in the two dictionaries.
The top 5language dimensions that are mostly correlated tohelpfulness from LIWC and INQUIRER are givenin Figure 3.The top 5 dimensions from LIWC are: Rel-ativ (Relativity), Time, Incl (Inclusive), Posemo(Positive Emotion), and Cogmech (Cognitive Pro-cesses).
All of them belong to Psychological Pro-cesses categories in LIWC, indicating that peopleare more thoughtful when writing a helpful review.The top 5 dimensions from INQUIRER are:Vary, Begin, Exert, Vice and Undrst.
Words with41Vary, Begin or Exert tags belong to process orchange words, such as start, happen and break.Vice tag contains words indicating an assess-ment of moral disapproval or misfortune.Undrst(Understated) tag contains words indicating de-emphasis and caution in these realms, which oftenreflects the lack of emotional expressiveness.
Ac-cordingly, we can infer that consumers perfer crit-ical reviews with personal experience and a lackof emotion.Figure 3: Language dimensions with highest cor-relation coefficients.
Top: LIWC?s; Bottom: IN-QUIRER?s.The discovery that helpful reviews are less emo-tional is consistent with the weak performance ofGALC (Tables 2, 3 and 4), which is emotion fo-cused.
However, we notice that one of the top5 dimensions in LIWC, PosEmo, is an emotionalfeature.
This is partially because some words ap-pear in both emotional and rational expressions,such as LIWC PosEmo words: love, nice, sweet.For example, the sentence ?I used to love linksys, butmy experience with several of their products makes me se-riously think that their quality is suspect?
is a rationalstatement.
But the word ?love?
appears in it.4.4 Prediction Results on Human LabelsA better ground truth for helpfulness is human rat-ing.
We further evaluate the prediction models onhuman annotated data to evaluate whether the pre-dictions indeed align with human perceptions ofreview helpfulness by reading text only.The model we built indeed aligns with humanperceptions of review helpfulness when text is theonly data.
Table 4 shows the correlation coef-ficients between the predicted scores and humanannotated scores.
INQUIRER is the best feature,leading in 3 of 4 categories.
It is followed by UGRand LIWC, which show comparable results.Table 4: Correlation coefficients between pre-dicted scores and human annotation, *: p < 0.001.Books Home Outdoors ElectronicsSTR 0.539* 0.522* 0.471* 0.635*UGR 0.607* 0.560* 0.579* 0.626*GALC 0.214 0.405* 0.156 0.418*LIWC 0.524* 0.553* 0.517* 0.702*INQUIRER 0.620* 0.662* 0.620* 0.676*FusionSemantic0.556* 0.680* 0.569* 0.603*FusionAll0.610* 0.801* 0.698* 0.768*For FusionAllmodels, correlation coefficientsare about or over 0.7 in 3 of 4 categories, indi-cating the successful prediction.
The only excep-tion is on Books category.
We notice that reviewsin Books are more subjective.
Therefore, in Booksreviews, consumers are more influenced by factorsoutside of the text, e.g., personal preference on thebook.
In this case, the approximate scores used intraining may not reflect the real text helpfulness.This observation echoes with our speculation thatthe ?X of Y approach?
may not always be a goodapproximation for helpfulness due to the subjec-tivity.
We will leave the analysis to this as a futurework.5 ConclusionIn this paper, we formulate a new problem whichis an important component of many tasks aboutonline product reviews: predicting the helpfulnessof review text.
We hypothesize that helpfulnessis an underlying property of text and isolate help-fulness prediction from its outer layer problems,such as review ranking.
Introducing two seman-tic features, which have been shown effective inother NLP tasks, we achieve more accurate andtransferable prediction than using features used inexisting related work.
The ground truth is pro-vided by votes on massive Amazon product re-views.
We further explore a semantic interpreta-tion to reviews?
helpfulness that helpful reviewsexhibit more reasoning and experience and lessemotion.
The results are further validated on hu-man scoring to helpfulness.42References[Agarwal et al2011] Deepak Agarwal, Bee-ChungChen, and Bo Pang.
2011.
Personalized recom-mendation of user comments via factor models.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 571?582, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Agichtein et al2008] Eugene Agichtein, CarlosCastillo, Debora Donato, Aristides Gionis, andGilad Mishne.
2008.
Finding high-quality contentin social media.
In Proceedings of the 2008 Interna-tional Conference on Web Search and Data Mining,WSDM ?08, pages 183?194.
ACM.
[Bard et al1996] Ellen Gurman Bard, Dan Robertson,and Antonella Sorace.
1996.
Magnitude estimationof linguistic acceptability.
Language, 72(1):pp.
32?68.
[Cao et al2011] Qing Cao, Wenjing Duan, and QiweiGan.
2011.
Exploring determinants of voting forthe ?helpfulness?
of online user reviews: A text min-ing approach.
Decis.
Support Syst., 50(2):511?521,January.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A library for sup-port vector machines.
ACM Transactions on In-telligent Systems and Technology, 2:27:1?27:27.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Danescu-Niculescu-Mizil et al2009] CristianDanescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinberg, and Lillian Lee.
2009.
Howopinions are received by online communities: Acase study on amazon.com helpfulness votes.
InProceedings of the 18th International Conferenceon World Wide Web, WWW ?09, pages 141?150,New York, NY, USA.
ACM.
[Duan et al2008] Wenjing Duan, Bin Gu, and An-drew B. Whinston.
2008.
The dynamics of on-line word-of-mouth and product sales-an empiricalinvestigation of the movie industry.
Journal of Re-tailing, 84:233242.
[Ghose and Ipeirotis2011] A. Ghose and P.G.
Ipeirotis.2011.
Estimating the helpfulness and economic im-pact of product reviews: Mining text and reviewercharacteristics.
volume 23, pages 1498?1512, Oct.[Hu and Liu2004] Minqing Hu and Bing Liu.
2004.Mining opinion features in customer reviews.
InProceedings of the 19th National Conference onArtifical Intelligence, AAAI?04, pages 755?760.AAAI Press.
[Kim et al2006] Soo-Min Kim, Patrick Pantel, TimChklovski, and Marco Pennacchiotti.
2006.
Au-tomatically assessing review helpfulness.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?06,pages 423?430, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Liu et al2008] Yang Liu, Xiangji Huang, Aijun An,and Xiaohui Yu.
2008.
Modeling and predictingthe helpfulness of online reviews.
In Proceedings ofthe 2008 Eighth IEEE International Conference onData Mining, ICDM ?08, pages 443?452, Washing-ton, DC, USA.
IEEE Computer Society.
[Martin and Pu2014] Lionel Martin and Pearl Pu.
2014.Prediction of helpful reviews using emotions extrac-tion.
In Twenty-Eighth AAAI Conference on Artifi-cial Intelligence, AAAI ?14.
[McAuley and Leskovec2013] Julian McAuley andJure Leskovec.
2013.
Hidden factors and hiddentopics: Understanding rating dimensions withreview text.
In Proceedings of the 7th ACM Confer-ence on Recommender Systems, RecSys ?13, pages165?172, New York, NY, USA.
ACM.
[Mudambi and Schuff2010] Susan M. Mudambi andDavid Schuff.
2010.
What makes a helpful on-line review?
a study of customer reviews on ama-zon.com.
MIS Quarterly, pages 185?200.
[O?Mahony and Smyth2010] Michael P. O?Mahonyand Barry Smyth.
2010.
Using readability teststo predict helpful product reviews.
In Adaptivity,Personalization and Fusion of Heterogeneous Infor-mation, RIAO ?10, pages 164?167, Paris, France,France.
LE CENTRE DE HAUTES ETUDESINTERNATIONALES D?INFORMATIQUE DOC-UMENTAIRE.
[Pasternack and Roth2011] Jeff Pasternack and DanRoth.
2011.
Making better informed trust deci-sions with generalized fact-finding.
In Proceedingsof the Twenty-Second International Joint Conferenceon Artificial Intelligence - Volume Three, IJCAI?11,pages 2324?2329.
AAAI Press.
[Pennebaker et al2007] J. W. Pennebaker, Roger J.Booth, and M. E. Francis.
2007.
Linguistic inquiryand word count: Liwc.
[Scherer2005] Klaus R. Scherer.
2005.
What are emo-tions?
and how can they be measured?
Social Sci-ence Information, 44(4):695?729.
[Stone et al1962] P. J.
Stone, R. F. Bales, J.
Z. Namen-wirth, and D. M. Ogilvie.
1962.
The general in-quirer: a computer system for content analysis andretrieval based on the sentence as a unit of informa-tion.
In Behavioral Science, pages 484?498.
[Xiong and Litman2011] Wenting Xiong and DianeLitman.
2011.
Automatically predicting peer-review helpfulness.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies:Short Papers - Volume 2, HLT ?11, pages 502?507,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.43[Xiong and Litman2014] Wenting Xiong and DianeLitman.
2014.
Empirical analysis of exploitingreview helpfulness for extractive summarization ofonline reviews.
In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics: Technical Papers, pages 1985?1995.
[Yang and Nenkova2014] Yinfei Yang and AniNenkova.
2014.
Detecting information-densetexts in multiple news domains.
In Proceedingsof Twenty-Eighth AAAI Conference on ArtificialIntelligence.44
