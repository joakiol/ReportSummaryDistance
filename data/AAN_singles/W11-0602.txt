Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 10?19,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsA Bayesian belief updating model of phonetic recalibration and selectiveadaptationDave Kleinschmidt1 and T. Florian Jaeger1,2Departments of 1Brain and Cognitive Sciences and 2Computer ScienceUniversity of RochesterRochester, NY, USA{dkleinschmidt,fjaeger}@bcs.rochester.eduAbstractThe mapping from phonetic categories toacoustic cue values is highly flexible, andadapts rapidly in response to exposure.
Thereis currently, however, no theoretical frame-work which captures the range of this adap-tation.
We develop a novel approach to mod-eling phonetic adaptation via a belief-updatingmodel, and demonstrate that this model natu-rally unifies two adaptation phenomena tradi-tionally considered to be distinct.1 IntroductionIn order to understand speech, people map a contin-uous, acoustic signal onto discrete, linguistic cate-gories, such as words.
Despite a long history of re-search, no invariant mapping from acoustic featuresto underlying linguistic units has yet been found.Some of this lack of invariance is due to randomfactors, such as errors in production and percep-tion, but much is due to systematic factors, such asdifferences between speakers, dialects/accents, andspeech conditions.The human speech perception system appears todeal with the lack of invariance in two ways: by stor-ing separate, speaker-, group-, or context-specificrepresentations of the same categories (Goldinger,1998), and by rapidly adapting phonetic categoriesto acoustic input.
Even though a person?s inven-tory of native language phonetic categories is gen-erally fixed from an early age (Werker and Tees,1984), the mapping between these categories andtheir acoustic realizations is flexible.
Listeners adaptrapidly to foreign-accented speech (Bradlow andBent, 2008) and acoustically distorted speech (Daviset al, 2005), showing increased comprehension af-ter little exposure.
Such adaptation results in tem-porary and perhaps speaker-specific changes in pho-netic categorization (Norris et al, 2003; Vroomen etal., 2007; Kraljic and Samuel, 2007).To our knowledge, there is no theoretical frame-work which explains the range and specific pat-terns of adaptation of phonetic categories.
In thispaper, we propose a novel framework for under-standing phonetic category adaptation?rational be-lief updating?and develop a computational modelwithin this framework which straightforwardly ex-plains two types of phonetic category adaptationwhich are traditionally considered to be separate.While phonetic category adaptation has not thusfar been described in this way, it nevertheless showsmany hallmarks of rational inference under uncer-tainty (Jacobs and Kruschke, 2010).
When there isanother possible explanation for strange pronunci-ations (e.g.
the speaker has a pen in her mouth),listeners do not show any adaptation (Kraljic etal., 2008).
Listeners are more willing to gener-alize features of a foreign accent to new talkersif they were exposed to multiple talkers initially,rather than a single talker (Bradlow and Bent, 2008).Listeners also show rational patterns of generaliza-tions of perceptual learning for specific phoneticcontrasts, generalizing to new speakers only whenthe adapted phonetic categories of the old and newspeakers share similar acoustic cue values (Kraljicand Samuel, 2007).While it is not conclusive, the available evidencesuggests that listeners update their beliefs about pho-101100 1680F2 at closure (Hz)/aba/ /ada/Proportion'ba' responsesProportion/b/responses/aba/ /ada/01Proportion'ba' responses/aba/ /ada/Figure 1: Left: approximate distribution of acoustic cue values for /aba/ and /ada/ stimuli from Vroomen et al(2007).
Right: exposure to acoustically ambiguous /aba/ tokens results in recalibration of the /aba/ category, withthe classification boundary shifting towards /ada/ (center-right), while exposure to unambiguous /aba/ tokens resultsin selective adaptation of the /aba/ category, where the classification boundary shifts towards /aba/ (far right).netic categories based on experience in a rationalway.
We propose that Bayesian belief updatingcan provide a principled computational frameworkfor understanding rapid adaptation of phonetic cate-gories as optimal inference under uncertainty.
Sucha framework has the appeal of being successfully ap-plied in other domains (Brenner et al, 2000; Fineet al, 2010).
In addition, rational models have alsobeen used within the domain of speech perception tomodel acquisition of phonetic categories (Vallabhaet al, 2007; Feldman et al, 2009a; McMurray et al,2009), the perceptual magnet effect (Feldman et al,2009b), and how various cues to the same phoneticcontrast can be combined (Toscano and McMurray,2010).2 The Phenomena: Perceptualrecalibration and selective adaptationThe flexibility of phonetic categories has beendemonstrated through studies which manipulate thedistribution of acoustic cues associated with a par-ticular category.
These studies take advantage ofthe natural variability of acoustic cues.
Take, forexample, the consonants /b/ and /d/.
These twoconsonants can be distinguished largely on the ba-sis of the trajectory of the second formant beforeand after closure (Iskarous et al, 2010).
Like allacoustic-phonetic cues, there is natural variability inthe F2 locus for productions of each category (de-picted schematically in Figure 1, left).
Listeners re-act to subtle changes in the distributions of acous-tic cues, and adjust their phonetic categories for avariety of contrasts and manipulations (Kraljic andSamuel, 2006).
In this paper, we model the effectsof the two most common types of manipulation stud-ied thus far, which produce opposite changes in pho-netic classification.The first of these is repeated exposure to acousti-cally ambiguous tokens, which results in a change inclassification termed ?perceptual learning?
(Norriset al, 2003) or ?perceptual recalibration?
(Bertelsonet al, 2003) in which the initially-ambiguous tokenbecomes an accepted example of one phonetic cate-gory.
Such ambiguous cue values are not uncommonbecause of the natural variability in normal speech.It is thus possible to generate a synthetic production/?/ which is acoustically intermediate between /b/and /d/, and which is phonetically ambiguous in theabsence of other cues but nevertheless sounds like aplausible production.
When paired with another cuewhich implies /b/, subjects reliably classify /?/ as/b/.
Disambiguating information could be providedby a video of a talker producing /b/ (Vroomen et al,2007), or a word such as a?out, where a /b/ has beenreplaced with /?/ (Norris et al, 2003).
When /?/ isrepeatedly paired in this way with information bias-ing a /b/ interpretation, subjects begin to interpret/?/ as /b/ in general, classifying more items on a/b/-to-/d/ continuum as /b/ (Figure 1, center-right,red curve).A second manipulation is repeated exposure tothe same, acoustically unambiguous token.
Re-peated exposure to /b/ causes ?selective adaptation?of this category, where listeners are less likely to11classify items as /b/, indicated by a shift in the /b/-/d/ classification boundary towards /b/ (Figure 1,far-right).Traditionally, recalibration and selective adapta-tion have been analyzed as separate processes,driven by separate underlying mechanisms(Vroomen et al, 2004), since they arise underdifferent circumstances and produce oppositeeffects on classification.
They also show differenttime courses.
Vroomen et al (2007) found that,on the one hand, strong recalibration effects occurafter just a few exposures to ambiguous tokens, butfade with further exposure (Figure 3, upper curve).On the other, selective adaptation is present after afew exposures to unambiguous tokens, but growssteadily stronger with further exposure (Figure 3,lower curve).We will show that these two superficially differ-ent adaptation phenomena are actually closely re-lated, and will provide a unified account by appeal-ing to principles of Bayesian belief updating.
Theseprinciples are used to construct two models.
Thefirst, a unimodal model, treats phonetic categoriesas distributions over acoustic cue dimensions.
Thesecond, a multimodal model, treats phonetic cate-gories as distributions over phonetic cue dimensions,which integrate information from both audio and vi-sual cues.
Both models capture the general effectdirections of selective adaptation and recalibration,but only the multimodal model captures their dis-tinct time courses.The next section provides a high-level descrip-tions of these models, and how they might describethe selective adaptation and recalibration data ofVroomen et al (2007).
Section 4 describes this dataand the methods used to collect it in more details.Section 5 describes the general modeling frame-work, how it was fit to the data, and the results, andSection 6 describes the multimodal model and its fitto the data.3 Phonetic category adaptation via beliefupdatingIn our proposed framework, the listener?s classifica-tion behavior can be viewed as arising from their be-liefs about the distribution of acoustic cues for eachphonetic category.
Specifically, as we will developlll lllll llllll llllllllll llll llllllll lll llll llllllll lllll llll llll llllllll llllllllllllllllllllllll llllllll ll l llllllllllllll lllllllllll llllllllllllllll llllllllllllll lllllllll lllll ll ll llllllll llllllFigure 2: An incremental belief-updating model for pho-netic recalibration and selective adaptation.
These distri-butions correspond to the classification functions in Fig-ure 1.
Left: ambiguous stimuli labeled as /b/ cause ashift of the /b/ category towards those stimuli.
Right:repeated unambiguous stimuli correspond to a narrowerdistribution than expected.more rigorously below, the probability of classify-ing a given token x (which is the value of either anacoustic cue or a multimodal, phonetic cue) as /b/is proportional to the relative likelihood of the cuevalue x arising from /b/ (relative to the overall like-lihood of observing tokens like x, regardless of cat-egory).
Thus, changes in the listener?s beliefs aboutthe distribution of cue values of category /b/ will re-sult in changes in their willingness to classify tokensas /b/.A belief-updating model accounts for recalibra-tion and selective adaptation in the following way.When, on the one hand, a listener encounters manytokens that they consider to be /b/ but which are allacoustically intermediate between /b/ and /d/, theywill change their beliefs about the distribution of/b/, shifting it to better align with these ambiguouscue values (Figure 2, left).
This results in increasedcategorization of items on a /b/-to-/d/ continuumas /b/, since the range on the continuum over whichthe likelihood associated with /b/ is higher than thatof /d/ is extended.On the other hand, when a listener encountersrepeated, tightly-clustered and highly prototypical/b/ productions, they update their beliefs about thedistribution of /b/ to reflect that /b/ productionsare more precise than they previously believed (Fig-ure 2, right).
They consequently assign lower likeli-hood to intermediate, ambiguous cue values for /b/,causing them to classify fewer /b/-/d/ continuumitems as /b/.Modeling the time course of selective adaptation12Exposures/b/?/d/differencescore?0.50.00.50 50 100 150 200 250Acoustic stimulusambiguousunambiguousFigure 3: The results of Vroomen et al (2007), show-ing the build-up time course of selective adaptation (as afunction of unambiguous exposure trials) and recalibra-tion (as a function of ambiguous exposure trials).is straightforward: the more observations are made,the narrower the distribution becomes, and the morethe classification boundary shifts towards the adapt-ing category.
However, modeling the time courseof recalibration, as measured by Vroomen et al(2007), is more complicated.
Recalibration comeson quickly, but fades gradually with many expo-sures (Figure 3).
As discussed below in Section 5.3,the unimodal model cannot account for this pattern,because it consideres the acoustically-similar expo-sure and test stimuli the same.
The multimodalmodel, by integrating audio and visual cues to formthe adapting percept, dissociates the adapting stim-ulus from the test stimuli and does not suffer fromthis problem.
It is thus in principle capable of re-producing the empirical time course of recalibrationobserved by Vroomen et al (2007).
In practice, thismodel does indeed provide a good qualitative fit tohuman data, as discussed in Section 6.4 Behavioral data: Vroomen et al (2007)Vroomen et al (2007) investigated the time courseof adaptation to audio-visual speech stimuli.
In eachblock, subjects were repeatedly exposed to a sin-gle type of stimulus.
The visual stimulus was either/aba/ or /ada/, and the audio stimulus was either anunambiguous match of the visual stimulus or was anambiguous production.
Throughout exposure, sub-jects were tested with unimodal acoustic test stimuliin order to measure the effect of exposure thus far.
?j ?jxi ciNM?j ?
Normal(?0j ,?
)?j ?
Gamma(?,?
)ci ?
Categorical(pi)xi ?
Normal(?ci ,?ci)Figure 1: Graphical model for MOG observations with independent priors oncomponent parameters.
Categories are indexed by j and observations are in-dexed by i.?j ?jxi ciNM?j ?
Normal(?0j ,?
?j)?j ?
Gamma(?,?
)ci ?
Categorical(pi)xi ?
Normal(?ci ,?ci)Figure 2: Graphical model for MOG with Normal-Gamma prior on componentparameters.
Categories are indexed by j = 1 .
.
.N and observations are indexedby i = 1 .
.
.M .1Figure 4: Graphical model for the mixture of Gaussianswith n rmal-gamma prior model.
See text for descrip-tion.The overall effect of exposure to unambiguous stim-uli was computed by comparing classification be-tween unambiguous-/b/ and unambiguous-/d/ ex-posure, and likewise for the effect of exposure toambiguous stimuli.The acoustic stimuli used in exposure and testwere drawn from a nine-item continuum (denotedx = 1, .
.
.
, 9) from /aba/ to /ada/, formed by ma-nipulating the second formant frequency before andafter the stop consonant (Vroomen et al, 2004).
Themost /aba/-like item x = 1 was synthesized us-ing the formant values from a normal /aba/ pro-duction, and the most /ada/-like item x = 9 wasderived from an /ada/ production.
The maximallyambiguous item was determined for each subject viaa labeling function (percent-/aba/ classification foreach token) derived from pre-test classification data(98 trials from across the entire continuum).
Allsubjects?
maximally ambiguous tokens were one ofx = 4, 5 or 6.Each exposure block consisted of 256 repetitionsof the bimodal exposure stimulus.
After 1, 2, 4, 8,16, 32, 64, 128, and 256 exposure trials subjectscompleted a test block, of six classification trials.They were asked to classify as /aba/ or /ada/ thethree most ambiguous stimuli from the continuum(the most ambiguous stimulus and the two neigh-boring stimuli) twice each.
For each ambiguity con-dition, the aggregate effect of exposure across cat-egories was a difference score, calculated by sub-tracting the percent /aba/-classification after /d/-exposure from the percent after /b/-exposure.
This/b/-/d/ difference score, as a function of cumulativeexposure trials, is plotted in Figure 3.135 The unimodal modelWe implemented an incremental belief-updatingmodel using a mixture of Gaussians as the underly-ing model of phonetic categories (Figure 4), whereeach phonetic category j = 1 .
.
.M corresponds toa normal distribution over percepts x with mean ?jand precision (inverse-variance) ?j (e.g.
Figure 1,left).p(xi | ci) = N (?ci , ?ci) (1)The listener?s beliefs about phonetic categoriesare captured by additionally assigning probabilitydistributions to the means ?j and precisions ?jof each phonetic category.
The prior distributionp(?j , ?j) represents the listener?s beliefs before ex-posure to the experimental stimuli, and the posteriorp(?j , ?j |X) captures the listener?s beliefs after ex-posure to stimuli X from category j.
These two dis-tributions are related via Bayes?
Rule:p(?j , ?j |X) ?
p(X |?j , ?j)p(?j , ?j) (2)In order to quantitatively evaluate such a model,the form of the prior distributions needs to be spec-ified.
A natural prior to use in this case is known asa Normal-Gamma prior.1 This prior factorizes thejoint prior intop(?j , ?j) = p(?j |?j)p(?j)p(?j |?j) = N (?0j , ?
?j)p(?j) = G(?, ?
)where N (?0j , ?
?j) is a Normal distribution withmean ?0j and precision ?
?j , and G(?, ?)
is a Gammadistribution with shape ?
and rate ?
(Figure 4).5.1 Identifying individual subjects?
priorbeliefsIn order to pick the most ambiguous token for eachsubject, Vroomen et al (2007) collected calibrationdata from their subjects, which consisted of 98 two-alternative forced choice trials on acoustic tokensspanning the entire /aba/-to-/ada/ continuum.
As1It is natural in that the Normal-Gamma distribution is theconjugate prior for a Gaussian distribution where there is someuncertainty about both the mean and the precision.
Using theconjugate prior ensures that the posterior distribution has thesame form as the prior.revealed by this pre-test data, each subject?s pho-netic categories are different, and so we chose to es-timate the prior beliefs about the nature of the expo-sure categories on a subject-by-subject basis.
We fiteach subject?s classification function using logisticregression.
The logistic function is closely relatedto the distribution over category labels given obser-vations in a mixture of Gaussians model.
Specifi-cally, when there are only two categories (as in ourcase), the probability that an observation at x will belabeled c1 is2p(c1 |x) =p(x | c1)p(c1)p(x | c1)p(c1) + p(x | c2)p(c2)(3)Further assuming that the categories have equal pre-cision ?
and equal prior probability p(c1) = p(c2) =0.53, this reduces to a logistic function of the formp(c1 |x) = (1 + exp(?gx+ b))?1, whereg = (?1 ?
?2)?
and b = (?21 ?
?22)?Even when b and g can be estimated from the sub-ject?s pre-test data, one additional degree of freedomneeds to be fixed, and we chose to fix the distancebetween the means, ?1??2.
Given these values, thevalues for (?1 + ?2)/2 (the middle of the subject?scontinuum) and ?
can be calculated using?1 + ?22 =bg and ?
=g?1 ?
?2(4)We chose to use ?1 ?
?2 = 8, the length of theacoustic continuum, which stretches from x = 1(derived from a natural /aba/) to x = 9 (from a nat-ural /ada/).
This is roughly equivalent to assumingthat all subjects would accept these tokens as goodproductions of /aba/ and /ada/, which indeed theydo (Vroomen et al, 2004).So far, we have accounted for the expected val-ues of category means and precisions.
The strengthof these prior beliefs, however, has yet to be speci-fied, and unfortunately there is no way to estimatethis based on the pre-test data of Vroomen et al(2007).
The model parameters corresponding to the2Here we are abusing notation a bit by using c1 as a short-hand for c = 1.3This assumption is not strictly necessary, but for this pre-liminary model we chose to make it in order to keep the modelas simple as possible.14subject?s confidence in their prior beliefs are ?
and?
for the means and variances, respectively.
Giventhe specific form of the prior we use here, these twoparameters are closely related to the number of ob-servations that are required to modify the subject?sbelief about a phonetic category (Murphy, 2007).5.2 Model fittingIn order to evaluate the performance of this modelrelative to human subjects, four simulations wererun per subject, corresponding to the four condi-tions used by Vroomen et al (2007): ambiguous /d/and /b/, and unambiguous /d/ and /b/.
For eachsubject, the hyper-parameters (?0j , ?, ?, ?)
were setaccording to the methods described above: valueswere chosen for the free parameters ?
and ?, and ?and ?0j were set based on the subject?s pre-test data.To model the effect of n exposure trials in a givencondition, the stimuli used by Vroomen et al (2007)were input into the model in the following way.
Forambiguous blocks, the observations X were n repe-titions of that subject?s most ambiguous token, andfor unambiguous blocks they were n repetitions ofthe x = 1 for /b/ or x = 9 for /d/.
For /b/ ex-posure blocks, the category labels C were set to 1,and for /d/ they were set to 2, corresponding to thedisambiguating effect of the visual cues.For each subject, condition, and number of expo-sures, the posterior distribution over category meansand precisions p(?j , ?j |X,C) was sampled usingnumerical MCMC techniques.4To compare the simulation results with the testdata of Vroomen et al (2007), it was neces-sary to find the classification function, p(ctest =1 |xtest, X), which is the probability that acoustictest stimulus xtest will be categorized as /b/ (ctest =1) given the training dataX .
Based on (3), it sufficesto find the predictive distributionsp(xtest | ctest = 1, X)=?
?p(xtest |?1, ?1)p(?1, ?1 |X)d?1d?1and, analogously, p(xtest | ctest = 2, X).
These in-4Specifically, 1 000 samples for each parameter were ob-tained after burn-in using JAGS, an open-source implemen-tation of the BUGS language for Gibbs sampling of graph-ical models: https://sourceforge.net/projects/mcmc-jags/b/?/d/differencescore?0.50.00.50 50 100 150 200 250Exposures/b/?/d/differencescore?0.50.00.50 50 100 150 200 250Figure 5: Overall fit of the acoustic-only (top, R2 =0.14) and bimodal model (bottom R2 = 0.67).
Solidlines correspond to the best fit averaged over subjects, anddashed lines correspond to empirical difference scores,with shaded regions corresponding to the 95% confidenceinterval on the empirical subject means.tegrals can be approximated numerically, by averag-ing over the individual likelihoods corresponding toeach individual pair of means and variances drawnfrom the posterior p(?j , ?j |X).Once this labeling function is obtained, the de-pendent measure used by Vroomen et al (2007)?average percentage categorized as /b/?can becalculated, by averaging the value of p(ctest =1 |xtest, X) for the test stimuli xtest used byVroomen et al (2007).
These were the subject?smaximally ambiguous stimulus (x = 4, 5 or 6, de-pending on the subject), and its two neighbors on thecontinuum.
The difference score used by Vroomenet al (2007) was computed by subtracting the aver-age probability of /b/ classification after /b/ (c = 1)exposure from the probability of /b/ classificationafter /d/ (c = 2) exposure.
The best fitting con-fidence parameters ?
and ?
were those which mini-mized mean squared error between the empirical andmodel difference scores.15llllllll llllllllll ll lllllllllllllllllll ll llllllllllllll lllllll lllllllllllllllll lllllllllllllll lllllllllllllllllllllllllll ll llll llll lllllllllllllllllllllllllllllll ll ll llllllllllllllll ll llllllllllll lllll lllllllllllllllllllllllllllllllllllll lllllllllllllllll lll lllllll llllllll lllllllllllllllllll l lllllllllllllllllllllllllllllllllll lllllllllllllllllllllllll lllllllll lll llllllllllllll lllllllllll llllll l lllll llllllllllll lll lllllllllll lllProportion'ba' responses/aba/ /ada/01Proportion/b/responsesFigure 6: When audio and visual cues are integrated before categorization, a small number of ambiguous tokens stillproduces a shift in the category mean, and thus recalibration (left, bright red).
However, a large number of ambiguoustokens produces both a shift of the category mean and an increase in precision (center-right, dark blue).
If the audio-visual percept is located away from the maximally ambiguous middle region of the continuum, this can result in anextinction of the initial recalibration effect with increasing exposure (far right).5.3 ResultsFigure 5, top panel shows the results of the unimodalmodel.
While this model clearly captures the direc-tion of the effects caused by ambiguous and unam-biguous exposure, it fails to account for a significantqualitative feature of the human data: the rise andthen fall of the recalibration effect (red line).The reason for this is that the audio componentof the audio-visual exposure stimuli is identical tothe maximally ambiguous (audio-only) test stimu-lus.
Under this model, the probability with which astimulus is classified as /b/ is proportional to thelikelihood assigned to that cue value by category/b/, relative to the total likelihood assigned by /b/and /d/.
In addition, under rational belief updatingthe likelihood assigned to the exposure stimulus?
cuevalue will always increase with more exposure.
Inthe unimodal model the cue dimension is only au-ditory (with the visual information in the exposurestimuli only being used to assign category labels),and so to the unimodal model the ambiguous expo-sure stimuli and the ambiguous test stimuli are ex-actly the same.
Thus, the probability that the teststimuli will be categorized as the exposure categoryincreases monotonically with further exposure.6 The multimodal modelThe unimodal model assumes that the cue dimen-sions which phonetic categories are defined overare acoustic, incorporating information from othermodalities only indirectly.
This assumption is al-most certainly wrong, based on work on audio-visual speech, which shows strong and pervasivecross-modal interactions (McGurk and MacDonald,1976; Bejjanki et al, 2011).
Indeed, Bertelson et al(2003) report strong effects of the visual cue usedby Vroomen et al (2007): subjects were at chancein discriminating acoustically ambiguous versus un-ambiguous bimodal tokens when the visual cuematched.The multimodal model replaces the acoustic per-cept x in the unimodal model with a phonetic per-cept which integrates information from audio andvisual cues.
Under reasonably general assumptions,information from auditory and visual cues to thesame phonetic dimension can be optimally com-bined by a simple weighted sum x = waxa +wvxv,where the weights wa and wv sum to 1 and are pro-portional to the reliability of the auditory and visualcues (Ernst and Banks, 2002; Knill and Saunders,2003; Jacobs, 2002; Toscano and McMurray, 2010).Such optimal linear cue-combination can be in-corporated into our model in an approximate wayby replacing x with a weighted sum of the con-tinuum values for the auditory and visual tokensx = wxa + (1 ?
w)xv.
In the unambiguous con-ditions, there is no mismatch between these values(xa, xv = 1 for /aba/ trials and 9 for /ada/ tri-als), and behavior is the same.
In the ambiguous tri-als, however, the combination of visual and auditorycues creates a McGurk illusion, and pulls the ob-served stimulus?now located on a phonetic /aba/-/ada/ continuum rather than an acoustic one?away16Exposures/b/?/d/differencescore?1.0?0.50.00.51.0?1.0?0.50.00.51.0?1.0?0.50.00.51.01102050 150 2502112150 150 2503122550 150 2504132650 150 2505142750 150 2506152850 150 2507162950 150 2508173050 150 2509193150 150 250Figure 7: Best model fit for each individual subject.
Dashed lines are empirical difference scores (shaded regions are95% confidence intervals) and solid lines are the best-fitting model for that subject.
Mean R2 = 0.57, SE= 0.04.from the maximally ambiguous test stimuli, whichare still located at the middle of the continuum, be-ing audio-only.
This allows recalibration to dom-inate early, as the mean of the adapted categorymoves towards the adapting percept, but be reversedlater, as the precision increases with further expo-sure percepts, all tightly clustered around the new,intermediate mean (Figure 6).To be optimal, w must be the relative reliability(precision) of audio cues relative to visual cues, butin this preliminary model it is treated as a free pa-rameter, between 0 and 1, and fit to each subject?stest data individually, in the same way as the confi-dence parameters ?
and ?.The best fitting models?
predictions are shown av-eraged across subjects in Figure 5 (bottom panel).Unlike the unimodal model, the multimodal modelclearly captures the initial rise and later fall of recal-ibration for ambiguous stimuli, and captures a fairamount of the variation between subjects (Figure 7).7 DiscussionThe Bayesian belief updating model developed inthis paper, which takes into account cross-modal cueintegration, provides a good qualitative fit to boththe overall direction and detailed time-course of twovery different types of adaptation of phonetic cat-egories, recalibration and selective adaptation, asstudied by Vroomen et al (2007).
This constitutesa first step towards a novel theoretical frameworkfor understanding the flexibility that characterizesthe mapping between phonetic categories to acoustic(and other) cues.
There is a large number of modelswhich adhere to the basic principles outlined here,and we have investigated only two of the simplestones in order to show that, firstly, selective adapta-tion and recalibration can be considered the productof the same underlying inferential process, and sec-ondly, this process likely occurs at the level of mul-timodal phonetic percepts.One of the most striking findings from this work,which space precludes discussing in depth, is thatall subjects?
data is fit best when the strength of theprior beliefs is quite low, corresponding to a fewhundred or thousand prior examples, which is manyorders of magnitude less than the number of /b/sand /d/s a normal adult has encountered in their life.Why should this number be so low?
The answerlies in the fact that phonetic adaptation is often ex-tremely specific, at the level of a single speaker orsituation.
In the future, we plan to model these pat-terns of specificity and generalization (Kraljic and17Samuel, 2007; Kraljic and Samuel, 2006) via hier-archical extensions of the current model, with con-nected mixtures of Gaussians for phonetic categoriesthat vary in predictable ways between groups ofspeakers.Besides being a principled, mathematical frame-work, Bayesian belief updating and the broaderframework of rational inference under uncertaintyalso provides a good framework for understandinghow and why multiple cues are combined in pho-netic categorization (Toscano and McMurray, 2010;Jacobs, 2002).
Finally, this approach is similar inspirit and in its mathematical formalisms to modelswhich treat the acquisition of phonetic categories asstatistical inference, where the number of categoriesneeds to be inferred, as well as the means and preci-sions of those categories (Vallabha et al, 2007; Feld-man et al, 2009a).
It is also similar to recent workon syntactic adaptation (Fine et al, 2010), and thusconstitutes a central part of an emerging paradigmfor understanding language as inference and learn-ing under uncertain conditions.AcknowledgementsWe would like to thank Jean Vroomen for gener-ously making the raw data from Vroomen et al(2007) available.This work was partially funded by NSF GrantBCS-0844472 and an Alfred P. Sloan Fellowship toTFJ.ReferencesVikranth Rao Bejjanki, Meghan A Clayards, David CKnill, and Richard N Aslin.
2011.
Cue Integra-tion in Categorical Tasks : Insights from Audio-VisualSpeech Perception.
PLoS ONE, in press.Paul Bertelson, Jean Vroomen, and Be?atrice de Gelder.2003.
Visual recalibration of auditory speech identifi-cation: a McGurk aftereffect.
Psychological Science,14(6):592?597, November.Ann R Bradlow and Tessa Bent.
2008.
Perceptual adap-tation to non-native speech.
Cognition, 106(2):707?29, February.Naama Brenner, William Bialek, and Rob de Ruyter VanSteveninck.
2000.
Adaptive Rescaling MaximizesInformation Transmission.
Neuron, 26(3):695?702,June.Matthew H Davis, Ingrid S Johnsrude, Alexis Hervais-Adelman, Karen Taylor, and Carolyn McGettigan.2005.
Lexical information drives perceptual learningof distorted speech: evidence from the comprehensionof noise-vocoded sentences.
Journal of experimentalpsychology.
General, 134(2):222?41, May.Marc O Ernst and Martin S Banks.
2002.
Humans in-tegrate visual and haptic information in a statisticallyoptimal fashion.
Nature, 415(6870):429?33.Naomi H Feldman, Thomas L Griffiths, and James LMorgan.
2009a.
Learning phonetic categories bylearning a lexicon.
Proceedings of the 31st AnnualConference of the Cognitive Science Society, pages2208?2213.Naomi H Feldman, Thomas L Griffiths, and James LMorgan.
2009b.
The influence of categories on per-ception: explaining the perceptual magnet effect asoptimal statistical inference.
Psychological review,116(4):752?82, October.Alex B Fine, Ting Qian, T Florian Jaeger, and Robert AJacobs.
2010.
Is there syntactic adaptation in lan-guage comprehension?
In ACL Workshop on Cog-nitive Modeling and Computational Linguistics, pages18?26.Stephen D Goldinger.
1998.
Echoes of echoes?
Anepisodic theory of lexical access.
Psychological re-view, 105(2):251?79, April.Khalil Iskarous, Carol A Fowler, and D H Whalen.
2010.Locus equations are an acoustic expression of articu-lator synergy.
The Journal of the Acoustical Society ofAmerica, 128(4):2021?32, October.Robert A Jacobs and John K Kruschke.
2010.
Bayesianlearning theory applied to human cognition.
Wiley In-terdisciplinary Reviews: Cognitive Science, pages n/a?n/a, May.Robert A Jacobs.
2002.
What determines visual cue re-liability?
Trends in cognitive sciences, 6(8):345?350,August.David C Knill and Jeffrey A Saunders.
2003.
Do hu-mans optimally integrate stereo and texture informa-tion for judgments of surface slant?
Vision Research,43(24):2539?2558, November.Tanya Kraljic and Arthur G Samuel.
2006.
Generaliza-tion in perceptual learning for speech.
Psychonomicbulletin & review, 13(2):262?8, April.Tanya Kraljic and Arthur G Samuel.
2007.
Perceptualadjustments to multiple speakers.
Journal of Memoryand Language, 56(1):1?15, January.Tanya Kraljic, Arthur G Samuel, and Susan E Brennan.2008.
First impressions and last resorts: how listenersadjust to speaker variability.
Psychological science : ajournal of the American Psychological Society / APS,19(4):332?8, April.Harry McGurk and John MacDonald.
1976.
Hearing lipsand seeing voices.
Nature, 264(5588):746?748.18Bob McMurray, Richard N Aslin, and Joseph C Toscano.2009.
Statistical learning of phonetic categories: in-sights from a computational approach.
DevelopmentalScience, 12(3):369?78, April.Kevin P Murphy.
2007.
Conjugate Bayesian analysis ofthe Gaussian distribution.
Technical report, Universityof British Columbia.Dennis Norris, James M McQueen, and Anne Cutler.2003.
Perceptual learning in speech.
Cognitive Psy-chology, 47(2):204?238, September.Joseph C Toscano and Bob McMurray.
2010.
Cue in-tegration with categories: Weighting acoustic cues inspeech using unsupervised learning and distributionalstatistics.
Cognitive science, 34(3):434?464, April.Gautam K Vallabha, James L McClelland, Ferran Pons,Janet F Werker, and Shigeaki Amano.
2007.
Unsuper-vised learning of vowel categories from infant-directedspeech.
Proceedings of the National Academy of Sci-ences of the United States of America, 104(33):13273?8, August.Jean Vroomen, Sabine van Linden, Mirjam Keetels,Be?atrice de Gelder, and Paul Bertelson.
2004.
Se-lective adaptation and recalibration of auditory speechby lipread information: dissipation.
Speech Commu-nication, 44(1-4):55?61, October.Jean Vroomen, Sabine van Linden, Be?atrice de Gelder,and Paul Bertelson.
2007.
Visual recalibration andselective adaptation in auditory-visual speech percep-tion: Contrasting build-up courses.
Neuropsychologia,45(3):572?7, February.Janet F Werker and Richard C Tees.
1984.
Cross-language speech perception: Evidence for perceptualreorganization during the first year of life.
Infant Be-havior and Development, 7(1):49?63, January.19
