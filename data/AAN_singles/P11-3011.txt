Proceedings of the ACL-HLT 2011 Student Session, pages 58?63,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsOptimistic BacktrackingA Backtracking Overlay for Deterministic Incremental ParsingGisle Ytrest?lDepartment of InformaticsUniversity of Oslogisley@ifi.uio.noAbstractThis paper describes a backtracking strategyfor an incremental deterministic transition-based parser for HPSG.
The method couldtheoretically be implemented on any othertransition-based parser with some adjust-ments.
In this paper, the algorithm is evaluatedon CuteForce, an efficient deterministic shift-reduce HPSG parser.
The backtracking strat-egy may serve to improve existing parsers, orto assess if a deterministic parser would bene-fit from backtracking as a strategy to improveparsing.1 IntroductionIncremental deterministic parsing has received in-creased awareness over the last decade.
Process-ing linguistic data linearly is attractive both froma computational and a cognitive standpoint.
Whilethere is a rich research tradition in statistical parsing,the predominant approach derives from chart pars-ing and is inherently non-deterministic.A deterministic algorithm will incrementally ex-pand a syntactic/semantic derivation as it reads theinput sentence one word/token at the time.
There area number of attractive features to this approach.
Thetime-complexity will be linear when the algorithm isdeterministic, i.e.
it does not allow for later changesto the partial derivation, only extensions to it.
For anumber of applications, e.g.
speech recognition, theability to process input on the fly per word, and notper sentence, can also be vital.
However, there areinherent challenges to an incremental parsing algo-rithm.
Garden paths are the canonical example ofsentences that are typically misinterpret due to anearly incorrect grammatical assumption.
(1) The horse raced past the barn fell.The ability to reevaluate an earlier grammatical as-sumption is disallowed by a deterministic parser.Optimistic Backtracking is an method designed tolocate the incorrect parser decision in an earlierstage if the parser reaches an illegal state, i.e.
a statein which a valid parse derivation cannot be retrieved.The Optimistic Backtracking method will try to lo-cate the first incorrect parsing decision made by theparser, and replace this decision with the correcttransition, and resume parsing from this state.2 Related WorkIncremental deterministic classifier-based parsingalgorithms have been studied in dependency pars-ing (Nivre and Scholz, 2004; Yamada and Mat-sumoto, 2003) and CFG parsing (Sagae and Lavie,2005).
Johansson and Nugues (2006) describea non-deterministic implementation to the depen-dency parser outlined by Nivre and Scholz (2004),where they apply an n-best beam search strategy.For a highly constrained unification-based for-malism like HPSG, a deterministic parsing strategycould frequently lead to parse failures.
Ninomiyaet al (2009) suggest an algorithm for determinis-tic shift-reduce parsing in HPSG.
They outline twobacktracking strategies for HPSG parsing.
Their ap-proach allows the parser to enter an old state if pars-ing fails or ends with non-sentential success, basedon the minimal distance between the best candidate58and the second best candidate in the sequence oftransitions leading up to the current stage.
Furtherconstraints may be added, i.e.
restricting the numberof states the parser may backtrack.
This algorithmis expanded by using a beam-thresholding best-firstsearch algorithm, where each state in the parse has astate probability defined by the product of the prob-abilities of the selecting actions that has been takento reach the state.3 CuteForceOptimistic Backtracking is in this paper used toevaluate CuteForce, an incremental deterministicHPSG parser currently in development.
Simi-lar to MaltParser (Nivre et al, 2007), it employsa classifier-based oracle to guide the shift-reduceparser that incrementally builds a syntactic/semanticHPSG derivation defined by LinGO English Re-source Grammar (ERG) (Flickinger, 2000).Parser Layout CuteForce has a more complextransition system than MaltParser in order to facil-itate HPSG parsing.
The sentence input buffer ?
isa list of tuples with token, part-of-speech tags andHPSG lexical types (i.e.
supertags (Bangalore andJoshi, 1999)).Given a set of ERG rules R and a sentence buffer?, a parser configuration is a tuple c = (?, ?, ?, pi)where:?
?
is a stack of ?active?
edges1?
?
is a list of tuples of word forms W ,part of speech tags POS and lexicaltypes LT derived from a sentence x =((W1, POS1, LT1), ...(Wn, POSn, LTn)).?
?
is the current input position in ??
pi is a stack of passive edges instantiating aERG ruleThe stack of passive edges pi makes up the fullHPSG representation of the input string if the stringis accepted.1An ?active?
edges in our sense is a hypothesis of an ap-plication of a binary rule where the left daughter is known (anelement of pi), and the specific binary ERG rule and the rightdaughter is yet to be found.Transition System The shift-reduce parser hasfour different transitions, two of which are param-eterized with a unary or binary ERG rule, which areadded to the passive edges, hence building the HPSGstructure.
The four transitions are:?
ACTIVE ?
(adds an active edge to stack ?, andincrements ?)?
UNIT(R1) ?
(adds unary passive edge to pi in-stantiating unary ERG rule (R1))?
PASSIVE(R2) ?
(pops ?
and adds binary pas-sive edge to pi instantiating binary ERG rule(R2))?
ACCEPT ?
(terminates the parse of the sen-tence.
pi represents the HPSG derivation of thesentence)Derivation Example Figure 1 is a derivation ex-ample from Redwoods Treebank (Oepen et al,2002).
We note that the tree derivation consistsof unary and binay productions, corresponding tothe UNIT(R1) and PASSIVE(R2) parser transitions.Further, the pre-terminal lexical types have a le suf-fix, and are provided together with the terminal wordform in the input buffer for the parser.sb-hd mc csp-hd n cd - prt-div le?some?aj-hdn norm cv j-nb-pas-tr dlrv pas odlrv np* le?specialized?n ms ilrn - m le?software?hd-cmp u cv vp mdl-p le?can?hd-cmp u cv n3s-bse ilrv np* le?narrate?hdn bnp cnp-hdn cpd chdn bnp-pn cw hyphen plrn - pl le?RSS-?w period plrn pl olrn - mc le?feeds.
?Figure 1: HPSG derivation from Redwoods Treebank.Parsing Configuration Mode CuteForce can op-erate in three different oracle configurations: HPSGUnification mode, CFG approximation mode andunrestricted mode.In HPSG Unification mode, the parser validatesthat no oracle decisions lead to an invalid HPSGderivation.
All UNIT and PASSIVE transitions are59an implicit unification.
For each parsing stage, theparsing oracle returns a ranked list of transitions.The highest-ranked transition not violating a unifi-cation constraint will be executed.
If no transitionyields a valid unification, parsing fails for the givensentence.In CFG mode, a naive CFG approximation of theERG is employed to guide the oracle.
The CFG ap-proximation consists of CFG rules harvested fromthe treebanks used in training the parser ?
for thispurpose we have used existing Redwoods treebanksused in training, and augmented with derivationsfrom WikiWoods, in total 300,000 sentences.
EachERG rule instantiation, using the identifiers shownin Figure 1 as non-terminal symbols, will be treatedas a CFG rule, and each parser action will be val-idated against the set of CFG rules.
If the parseraction yields a CFG projection not found among thevalid CFG rules in the CFG approximation, the CFGfilter will block this transition.
If the parser arrivesat a state where the CFG filter blocks all further tran-sitions, parsing fails.In unrestricted mode, the oracle chooses the high-est scoring transition without any further restrictionsimposed.
In this setting, the parser typically reachesclose to 100 % coverage ?
the only sentences notcovered in this setting are instances where the parserenters an infinite unit production loop.
Hence, wewill only evaluate the parser in CFG and Unificationmode in this paper.4 Optimistic BacktrackingOptimistic Backtracking can be added as an overlayto a transition-based parser in order to evaluate theparser in non-deterministic mode.
The overlay hasa linear time-complexity.
This backtracking methodis, to the best of our knowledge, the only method thatapplies ranking rather than some probability-basedalgorithm for backtracking.
This aspect is criticalfor classification-based parsing oracles that do notyield a probability score in the ranking of candidatetransitions.Treating backtracking as a ranking problem hasseveral attractive features.
It may combine globaland local syntactic and semantic information relatedto each candidate transition, contrary to a probabilis-tic approach that only employs the local transitionprobability.
Utilizing global information also seemsmore sound from a human point of view.
Considersentence (1), it?s first when the second verb (fell) isencountered that we would re-evaluate our originalassumption, namely that raced may not be the headverb of the sentence.
That fell indeed is a verb issurely relevant information for reconsidering racedas the head of a relative clause.When the parser halts, the backtracker will rankeach transition produced up until the point of fail-ure according to which transition is most likely to bethe first incorrect transition.
When the best scoringtransition is located, the parser will backtrack to thisposition, and replace this transition with the pars-ing oracle?s second-best scoring transition for thiscurrent parsing state.
If the parser later comes toanother halt, only the transitions occurring after thefirst backtrack will be subject to change.
Hence, thebacktracker will always assume that its last back-track was correct (thus being Optimistic).
Havingallowed the parser to backtrack unrestrictedly, wecould theoretically have reached close to 100 %coverage, but the insights of parsing incrementallywould have become less pronounced.The search space for the backtracker is n ?
mwhere n is the number of candidate transitions, andm is the total number of parser transitions.
In Op-timistic Backtracking we disregard the m dimensionaltogether by always choosing the second-best tran-sition candidate ranked by the parsing oracle, as-suming that the second-ranked transition in the givenstate actually was the correct transition.
Hence wereduce the search-space to the n-dimension.
In thispaper, using CuteForce as HPSG parser, this as-sumption holds in about 80-90 % of the backtracksin CFG mode, in HPSG Unification mode this num-ber is somewhat lower.4.1 BaselineAs a baseline for identifying the incorrect transition,we use a strategy inspired by Ninomiya et al (2009),namely to pick the candidate transition with the min-imal probability difference between the best and thesecond best transition candidate.
However, since wedo not have true probability, a pseudo-probabilityis computed by taking the dot product of the fea-ture vector and weight-vector for each best-scoring(P) and second-best scoring (P2) candidate transi-60tion, and use the proportion of the second-best scoreover the joint probability of the best and second-bestscoring transition: P2P+P2In our development test set of 1794 sentences, weran the parser in CFG and HPSG unification modein deterministic and non-deterministic mode.
Thebaseline results are found in Table 1 (CFG-BL) andTable 2 (UNI-BL).
In CFG mode (Table 1), we ob-tain a 51.2 % reduction in parsing failure.
In unifica-tion mode (Table 2) the parser is much more likelyto fail, as the parse derivations are guaranteed tobe a valid HPSG derivation.
Baseline backtrackingyields a mere 10 % reduction in parsing failures.4.2 Feature ModelEach candidate transition is mapped to a featurevector that provides information about the transi-tion.
The task for the ranker is to identify the firstincorrect transition in the sequence of transitions.The feature model used by the ranker employs fea-tures that can roughly be divided in three.
First, thetransition-specific features provide information onthe nature of the candidate transition and surround-ing transitions.
Here we also have features related tothe pseudo-probability of the transition (provided bythe parsing oracle), and the oracle score distance be-tween the best-scoring and second-best scoring tran-sition for each given state.
Secondly we have fea-tures related to the last token that was processed bythe parser before it reached an invalid state, and theinformation on the incomplete HPSG derivation thatwas built at that state.
These features are used incombination with local transition-specific features.Third, we have features concerning the preliminaryHPSG derivation in the actual state of the transition.Feature Types The list of transitions T = t0, t1, ...tn comprises the candidate transitions that are sub-ject to backtracking upon parsing failure.
The fea-ture types used by the backtracker includes:?
the pseudo-probability of the best scoring (P)and second best scoring (P2) transition?
the transition category of the current transition?
the probability proportion of the second bestscoring transition over the joint probabilityP2P+P2?
the transition number in the list of applicablecandidates, and the number of remaining tran-sitions, relative to the list of candidates?
the last lexical tag and part-of-speech tag thatwere processed before parsing failure?
the head category of the HPSG derivation andthe left daughter unification candidate for theHPSG derivation in the current position?
the lexical tag relative to the current position inthe bufferThe backtracker is trained as a linear SVM us-ing SVM rank (Joachims, 2006).
Totally, the featurevector maps 24 features for each transition, includ-ing several combinations of the feature types above.5 EvaluationIn this paper we trained CuteForce with data fromRedwoods Treebank, augmented with derivationsfrom WikiWoods (Flickinger et al, 2010).
The testset contains a random sample of 1794 sentencesfrom the Redwoods Treebank (which was excludedfrom the training data), with an average length of 14tokens.
Training data for the backtracker is extractedby parsing derivations from WikiWoods determin-istically, and record transition candidates each timeparsing fails, labeling the correct backtracking can-didate, backtrack to this point, and resume parsingfrom this state.5.1 ResultsThe first column (CFG-NB and UNI-NB) in Table 1and 2 indicates the scores when the parser is run indeterministic mode, i.e.
without backtracking.
Thesecond and third column contain results for baselineand Optimistic backtracking.
Coverage refers to theproportion of sentences that received a parse.
Pre-cision refers to the backtracker?s precision with re-spect to identifying the incorrect transition amongthe candidate transitions.
?
BT Cand is the aver-age number of candidate transitions the backtrackerranks when trying to predict the incorrect transition,and ?
BT Cand,1st is the number of candidates atthe initial point-of-failure.
Exact Matches is the to-tal number of parse derivations which are identicalto the gold standard.For Ms per Sent (milliseconds per sentence) itshould be said that the code is not optimized, es-61pecially with respect to the HPSG unification algo-rithm2.
How the figures relate to one another shouldhowever give a good indication on how the compu-tational costs vary between the different configura-tions.CFG -NB CFG -BL CFG -OptCoverage 0.754 0.880 0.899Precision N/A 0.175 0.235?BT Cand N/A 26.1 30.6?BT Cand,1st N/A 51.5 51.5Exact Matches 727 746 742Ms per Sent 10.7 45.0 72.5Table 1: Results ?
CFG modeUNI -NB UNI -BL UNI -OptCoverage 0.574 0.598 0.589Precision N/A 0.183 0.206?BT Cand N/A 12.89 20.12?BT Cand,1st N/A 51.6 51.6Exact Matches 776 777 776Ms per Sent 1801.4 5519.1 5345.2Table 2: Results ?
HPSG unification mode5.2 CFG approximationThe number of failed sentences is greatly reducedwhen backtracking is enabled.
Using baseline back-tracking, the reduction is 51.2 %, whereas Opti-mistic backtracking has a 59.1 % reduction in parsefailures.
Further, Optimistic Backtracker performssubstantially better than baseline in identifying in-correct transitions.The average number of candidate transitionsranged from 26 to 30 for the baseline and Optimisticbacktracking strategy.
It?s interesting to observe thateven with a success rate of about 1/5 in identifyingthe incorrect transition, the coverage is still greatlyincreased.
That backtracking manages to recoverso many sentences that initially failed, even if itdoes not manage to identify the incorrect transition,would seem to indicate that even when mistaken, thebacktracker is producing a good prediction.
On theother hand, the exact match score does not improvethe same way as the coverage, this is directly related2Specifically, the current unification back-end preformsnon-destructive unification, i.e.
it does not take advantage ofthe deterministic nature of CuteForceto the fact that the backtracker still has relatively lowprecision, as only a perfect prediction would leavethe parser capable of deriving an exact match.The success rate of about 0.23 in picking the in-correct transition in a set of in average 30 candidatesindicates that treating the backtracking as a rankingproblem is promising.
The precision rate in itself ishowever relatively low, which serves as an indica-tion of the difficulty of this task.5.3 HPSG UnificationIn unification mode the we see no substantive dif-ference between deterministic mode, and baselineand Optimistic backtracking, and practically no im-provement in the quality of the parses produced.In Table 2 we see that the only striking differencebetween the figures for the parser in backtrackingmode and deterministic mode is the efficiency ?
thetime consumption is increased by approximately afactor of 3.5.4 ConclusionThe findings in this paper are specific to CuteForce.It is however very likely that the results would besimilar for other deterministic HPSG parsers.In CFG mode, the number of failed parses aremore than halved compared to deterministic mode.It is likely that further increase could be obtained byrelaxing constraints in the Optimistic algorithm.In Unification mode, we experienced only a slightincrease in coverage.
By relaxing the Optimisticconstraints, the time-complexity would go up.
Con-sidering how little the parser benefited from back-tracking in unification mode with Optimistic con-straints, it seems implausible that the parser willimprove considerably without a heavy relaxation ofthe constraints in the Optimistic algorithm.
If do-ing so, the attractive features of the parser?s inher-ently deterministic nature will be overshadowed bya very large number of backtracks at a heavy compu-tational cost.
Hence, it?s hard to see that such a semi-deterministic approach could have any advantagesover other non-deterministic HPSG parsers neitherin computational cost, performance or on a cogni-tive level.62AcknowledgementsThe author would like to thank Stephan Oepen (Uni-versity of Oslo) and Joakim Nivre (Uppsala Uni-versity) for their valued input and inspiring feed-back during the writing of this paper, and in thePhD project.
Experimentation and engineering wasmade possible through access to the TITAN high-performance computing facilities at the Universityof Oslo (UiO), and we are grateful to the ScientificComputation staff at UiO, as well as to the Norwe-gian Metacenter for Computational Science.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, pages 237?265.Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.2010.
Wikiwoods: Syntacto-semantic annotationfor english wikipedia.
In Proceedings of the Sev-enth conference on International Language Resourcesand Evaluation (LREC?10).
European Language Re-sources Association (ELRA).Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6 (1):15 ?
28.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, pages 217?226.
ACM.Richard Johansson and Pierre Nugues.
2006.
Investi-gating multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, pages 206?210.
Associationfor Computational Linguistics.Takashi Ninomiya, Nobuyuki Shimizu, Takuya Mat-suzaki, and Hiroshi Nakagawa.
2009.
Deterministicshift-reduce parsing for unification-based grammarsby using default unification.
In Proceedings of the12th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 603?611.Association for Computational Linguistics.Joakim Nivre and Mario Scholz.
2004.
Determinis-tic dependency parsing of English text.
In Proceed-ings of the 20th international conference on Computa-tional Linguistics.
Association for Computational Lin-guistics.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Stephan Oepen, Kristina Toutanova, Stuart Shieber, ChrisManning, Dan Flickinger, and Thorsten Brants.
2002.The LinGO Redwoods treebank.
Motivation and pre-liminary applications.
In Proceedings of the 19th In-ternational Conference on Computational Linguistics.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnology, pages 125?132.
Association for Compu-tational Linguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal dependency analysis with support vector machines.In Proceedings of the 8th International Workshop onParsing Technologies, pages 195?206.63
