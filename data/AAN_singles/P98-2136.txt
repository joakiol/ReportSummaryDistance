Confirmation in Multimodal SystemsDavid R. McGee, Philip R. Cohen and Sharon OviattCenter for Human-Computer Communication,Department ofComputer Science and EngineeringOregon Graduate InstituteP.O.
Box 91000, Portland, Oregon 97291-1000\[ dmcgee, pcohen, oviatt }@cse.ogi.eduABSTRACTSystems that attempt to understand natural human inputmake mistakes, even humans.
However, humans avoidmisunderstandings by confirming doubtful input.Multimodal systems--those that combine simultaneousinput from more than one modality, for example speechand gesture--have historically been designed so thatthey either equest confwmation of speech, their primarymodality, or not at all.
Instead, we experimented withdelaying confirmation until after the speech and gesturewere combined into a complete multimodal command.In controlled experiments, ubjects achieved morecommands per minute at a lower error rate when thesystem delayed confirmation, than compared to whensubjects confirmed only speech.
In addition, this style oflate confirmation meets the user's expectation thatconfirmed commands should be executable.KEYWORDS: multimodal, confirmation, uncertainty,disambiguation"Mistakes are inevitable in dialog...In practice, conversationbreaks down almost instantly in the absence of a facility torecognize and repair errors, ask clarification questions, giveconfinnatior~ and perform disambiguatimt \[ 1 \]"INTRODUCr IONWe claim that multimodal systems \[2, 3\] that issuecommands based on speech and gesture input should notrequest confirmation of words or ink.
Rather, thesesystems should, when there is doubt, requestconfirmation of their understanding of the combinedmeaning of each coordinated language act.
The purposeof any confirmation act, after all, is to reach agreementon the ovemU meaning of each command.
To test theseclaims we have extended our multirn~ial map system,QuickSet \[4, 5\], so that it can be tuned to requestcortfL,'mafion either before or after integration ofmodalities.
Using QuickSet, we have conducted anempirical study that indicates agreement about thecorrectness of commands can be reached quicker ifconfirmation is delayed until after blending.
This paperdescribes QuickSet, our experiences with it, anexperiment that compares early and late confirmationstrategies, the results of that experiment, and ourconclusions.Command-driven conversational systems need toidentify hindrances to accurate understanding andexecution of commands in order to avoidmiscornmunication.
These hindrances can arise from atleast hree sources:Unce~k of confidence in interpretation fthe input,Ambi~y ~ly  in~ons  of inr~ andInp.as/bah'y--~ inability to perf~n the co,~, ~d.Suppose that we use a recognition system that interpretsnatural human input \[6\], that is capable of multimodalinteraction \[2, 3\], and that will let users place simulatedmilitary units and related objects on a map.
When weuse this system, our words and stylus movements aresimultaneously recognized, interpreted, and blendedtogether.
A user calls out the names of objects, such as'~OMEO ONE EAGLE," while marking the map with agesture.
If the system is confident of its recognition ofthe input, it might interpret his command in thefollowing manner:, a unit should be placed on the map atthe specified location.
Another equally likelyinterpretation, looking only at the results of speechrecognition, might be to select an existing "ROMEO ONEEAGLE."
Since this multimodal system is performingrecognition, uncertainty inevitably exists in therecognizer's hypotheses.
"ROMEO ONE ~_&GLE" maynot be recognized with a high degree of confidence.
Itmay not even be the most likely hypothesis.One way to disambiguate he hypotheses i with themultimodal language specification itself, the way weallow modalities to combine.
Since different modalitiestend to capture complementary information \[7-9\], wecan leverage this facility by combining ambiguous823spoken interpretations with disimilar gestures.
Forexample, we might specify that selection gestures(circling) combine with the ambiguous peech fromabove to produce a selection command.
Another way ofdisambiguating the spoken utterance is to enforce aprecondition for the command: for example, for theselection command to be possible the object mustalready exist on the map.
Thus, under such aprecondition, if "Ro~o ONE F_~Cr.~."
is not alreadypresent on the map, the user cannot select it.
We callthese techniques multimodal disambiguation techniques.Regardless, if a system receives input that it findsuncertain, ambiguous, or infeasible, or if its effect mightbe profound, risky, costly, or irreversible, it may want toverify its interpretation f the command with the user.For example, a system prepared to execute thecommand "DESTROY ALL DATA" should give thespeaker a chance to change or correct he command.Otherwise, the cost of such errors is task-dependent a dcan be immeasurable \[6, 10\].Therefore, we claim that conversational systems houldbe able to request the user to confirm the command, ashumans tend to do \[11-14\].
Such confirmations are used"to achieve common grounar' in human-human dialogue\[15\].
On their way to achieving common ground,participants attempt o minimize their collaborativeeffort, "the work that both do from the initiation of \[acommand\] to its completion."
\[15\] Herein we willfurther define collaborative effort in terms of work in acommand-based collaborative dialogue, where anincrease in the rate at which commands can besuccessfully performed corresponds toa reduction i  thecollaborative effort.
We know that confirmations are animportant way to reduce miscommunication \[13, 16,17\], and thus collaborative effort.
In fact, the more likelymiscommunication, the more frequently peopleintroduce confirmations \[ 16, 17\].To ensure that common ground is achieved,miscommunication is avoided, and collaborative effort isreduced, system designers must determine when andhow confirmations ought to be requested.
Should aconfirmation occur for each modality or shouldconfmmtion be delayed until the modalities have beenblended?
Choosing to confirm speech and gestureseparately, or speech alone (as many contemporarymultimodal systems do), might simplify the process ofconfirmation.
For example, confirmations could beperformed irnrnediately after recognition of one or bothmodalities.
However, we will show that collaborativeeffort can be reduced if multirnodal systems delayconfirmation until after blending.1 MOTIVAT IONHistorically, multimodal systems have either notconfLrmed input \[18-22\] or confLrmed only the primarymodality of such systems--speech.
T is is reasonable,considering the evolution of multimodal systems fromtheir speech-based roots.
Observations of QuickSetprototypes last year, however, showed that simplyconfirming the results of speech recognition was oftenproblematic---users had the expectation that whenever acommand was conf~ it would be executed.
Weobserved that confwming speech prior to multimodalintegration led to three possible cases where thisexpectation might not be met: ambiguous gestures, non-meaningful speech, and delayed confinmtion.The first problem with speech-only confirmation wasthat the gesture recognizer produced results that wereoften ambiguous.
For example, recognition of the ink inFigure 1 could result in confusion.
The arc (left) in thefigure provides ome semantic ontent, but it may beincomplete.
The user may have been selectingsomething or she may have been creating an area, line,or route.
On the other hand, the circle-like gesture(middle) might not be designating an area or specifyinga selection; it might be indicating a circuitous route orline.
Without more information from other modalities, itis difficult o guess the hutentions behind these gestures.OOcFigure 1.
Ambiguous GesturesFigure 1 demonstrates how, oftentimes, it is difficult todetermine which interpretation is correct.
Some gesturescan be assumed to be fully specified by themselves (atright, an editor's mark meaning "cut").
However, mostrely on complementary input for completeinterpretation.
If the gesture recognizer misinterprets thegesture, failure will not occur until integration.
Thespeech ypothesis might not combine with any of thegesture hypotheses.
Also, earlier versions of our speechrecognition agent were limited to a single recognitionhypothesis and one that might not even be syntactically824correct, in which case integration would always fail.Finally, the confirmation act itself could delay the arrivalof speech into the process of multimodal integration.
Ifthe user chose to correct he speech recognition outputor to delay confirmation for any other eason, integrationitself could fail due to sensitivity in the multimodalarchitecture.In all three cases, users were asked to confirm acommand that could not be xecuted.
An importantlesson learned from these observations i that whenconfirming a command, users think they are givingapproval; thus, they expect hat the command can beexecuted without hindrance.
Due to these earlyobservations, we wished to determine whether delayingconfirmation until after modalities have combinedwould enhance the human-computer dialogue inmultimodal systems.
Therefore, we hypothesize thatlate-stage confirmations will lead to three improvementsin dialogue.
First, because late-stage systems can bedesigned to present only feasible commands forconfirmation, blended inputs that fail to produce afeasible command can be immediately flagged as a non-understanding and presented to the user as such, ratherthan as a possible command.
Second, because ofmultimodal disambiguation, misunderstandings can bereduced, and therefore the number of conversationaltums required to reach mutual understanding can bereduced as well.
Finally, a reduction in turns combinedwith a reduction in time spent will lead to reducing the"collaborative effort" in the dialogue.
To examine ourhypotheses, we designed an experiment using QuickSetto determine if late-stage confmmtions enhance human-computer conversational performance.2 QUICKSETThis section describes QuickSet, a suite of agents formultimodal human-computer communication [4, 5].2.1 A Mulfi.Agem ArchitectureUnderneath the QuickSet suite of agents lies adistributed, blackboard-based, multi-agent architecturebased on the Open Agent Architecture' [23].
Theblackboard acts as a repository of shared informationand facilitator.
The agents rely on it for brokering,rre.ssage distribution, and notification.'
qlac Open Agent Architecture is atmde~ of SRI International.2.2 The QuickSet AgentsThe following section briefly summarizes theresponsibilities of each agent, their interaction, and theresults of their computation.2.2.1 User InterfaceThe user draws on and speaks to the interface (seeFigure 2 for a snapshot f the interface) to place objectson the map, assign attributes and behaviors to them,and ask questions about them.Figure 2.
Quicl~t Early Confmmtion Mode2.2.2 Gesture RecognitionThe gesture recognition agent recognizes gestures fromstrokes drawn on the map.
Along with coordinatevalues, each stroke from the user interface providescontextual information about objects touched orencircled by the stroke.
Recognition results are an n-bestlist (top n-ranked) of interpretations.
The interpretationsare encoded as typed feature structures [5], whichrepresent each of the potential semantic contributions ofthe gesture.
This list is then passed to the multimodalintegrator.2.2.3 Speech RecognitionThe Whisper speech recognition engine from MicrosoftCorp.
[24] drives the speech recognition agent.
It offersspeaker-independent, continuous recognition i  close toreal time.
QuickSet relies upon a context-free domaingrammar, specifically designed for each application, toconstrain the speech recognizer.
The speech recognizer825agent's output is also an n-best list of hypotheses andtheir probability estimates.
These results are passed onfor natural language interpretation.2.2.4 Natural Language InterpretationThe natural anguage interpretation agent parses theoutput of the speech recognizer attempting to providemeaningful semantic interpretations based on a domain-specific grammar.
This process may introduce furtherambiguity; that is, more hypotheses.
The results ofparsing are, again, in the form of an n-best list of typedfeature structures.
When complete, the results of naturallanguage interpretation are passed to the integrator formultimodal integration.2.2.5 Multimodal IntegrationThe multimodal integration agent accepts typed featurestructures from the gesture and natural languageinterpretation agents, and unifies them \[5\].
The processof integration ensures that modes combine according toa multimodal language specification, and that they meetcertain multimodal timing and command-specificconstraints.
These constraints place limits on whendifferent input can occur, thus reducing errors \[7\].
If afterunification and constraint satisfaction, there is more thanone completely specified command, the agent thencomputes the joint probabilities for each and passes thefeature structure with the highest to the bridge.
If, on theother hand, no completely specified command exists, arrr.ssage is sent to the user interface, asking it to informthe user of the non-understanding.2.2.6 Bridge to Application SystemsThe bridge agent acts as a single message-basedinterface to domain applications.
When it receives afeature structure, it sends a message to the appropriateapplications, requesting that hey execute the command.3 CONFIRMATION STRATEGIESQuickset supports two modes of confmnation: early,which uses the speech recognition hypothesis; and late,which renders the confirmation act graphically using theentire integrated multimodal command.
These twomodes are detailed in the following subsections.3.1 Early ConfirmationUnder the early confirmation strategy (see Figure 3),speech and gesture are immediately passed to theirrespective r cognizers (la and lb).
Electronic ink is usedfor immediate visual feedback of the gesture input.
Thehighest-scoring speech-recognition hypothesis isreturned to the user interface and displayed forconfirmation (2).
Gesture recognition results areforwarded to the integrator after processing (4).Figure 3.
Early Confirmation Message FlowAfter confirmation of the speech, Quickset passes theselected sentence to the parser (3) and the process ofintegration follows (4).
If, during confirmation, thesystem fails to present the correct spoken interpretation,users are given the choice of selecting it from a pop-upmenu or respeaking the command (see Figure 2).3.2 Late ConfirmationIn order to meet he user's expectations, it was proposedthat confmmtions occur after integration of themultimodal inputs.
Notice that in Figure 4, as opposed toFigure 3, no confirmation act impedes input as itprogresses towards integration, thus eliminating thetiming issues of prior Quickset architectures.Figure 4.
Late Confirmation Message FlowFigure 5 is a snapshot of QuickSet in late confirmationmode.
The user is indicating the placement ofcheckpoints on the terrain.
She has just touched the mapwith her pen, while saying "YELLOW" to name the nextcheckpoint.
In response, QuickSet has combined thegesture with the speech and graphically presented the826logical consequence of the command: a checkpoint icon(which looks like an upside-down pencil).~~, ,o~ ........................ ~ ~,~,:: :u~:~l  ~:~.
.
,.. ,~...~,.~ .
................ ~ ~ .
.
.
.
.
.
.
.
.
.
.
.
.
~ !~,~ , ;~>~:~!
~':~,~, |!lv~me 5.
Qui~Set in Late Confmamllon ModeTo confu'm or disconfima n object in either mode, theuser can push either the SEND (checkrnark) or the E~,S~.
(eraser) buttons, respectively.
Altematively, to confn-rnthe command in late confirmation mode, the user canrely on implicit confirmation, wherein QuickSet reatsnon-contradiction as a confirrnation [25-27].
In otherwords, if the user proceeds to the next command, sheimplicitly confLrrns the previous command.4 EXPERIMENTAL  METHODThis section describes this experiment, i s design, andhow data were collected and evaluated.4.1 Subjects, Tasks, and ProcedureEight subjects, 2 male and 6 female adults, half with acomputer science background and half without, wererecruited from the OGI campus and asked to spend onehour using a prototypical system for disaster escueplanning.During training, subjects received a set of writteninstructions that described how users could interact withthe system.
Before each task, subjects received oralinstructions regarding how the system would requestconfirmations.
The subjects were equipped withmicrophone and pen, and asked to perform 20 typicalcommands as practice prior to data collection.
Theyperformed these cornrnands in one of the twoconfLrmation modes.
After they had completed eitherthe flood or the f'Lre scenario, the other scenario wasintroduced and the remaining cortfirmation mode wasexplained.
At this time, the subject was given a chanceto practice commands in the new confirmation mode,and then conclude the experiment.4.2 Research Design and Data CaptureThe research design was within-subjects with a singlefactor, confirmation mode, and repeated measures.
Eachof the eight subjects completed one fire-fighting and oneflood-control rescue task, composed of approximatelythe same number and types of commands, for a strictrecipe of about 50 multimodal commands.
Wecounterbalanced the order of confm'nation mode andtask, resulting in four different ask and confwmationmode orderings.4.3 Transcript Preparation and CodingThe QuickSet user interface was videotaped andmicrophone input was recorded while each of thesubjects interacted with the system.
The followingdependent measures were coded from the videotapedsessions: time to complete each task, and the number ofcommands and repairs.4.3.1 7qme to complete taskThe total elapsed time in minutes and seconds taken tocomplete each task was rrr.asured: from the first contactof the pen on the interface until the task was complete.4.3.2 Commands, repairs, turnsThe number of commands attempted for each task wastabulated.
Some subjects skipped commands, and mosttended to add commands to each task, typically tonavigate on the map (e.g., "PAN" and "ZOOM").
If thesystem misunderstood, the subjects were asked toattempt a command up to three times (repair), thenproceed to the next one.
Completely unsuccessfulcommands and the time spent on them, includingrepairs, were factored out of this study (1% of allcommands).
The number of turns to complete ach taskis the sum of the total number of commands attemptedand any repairs.4.3.3 Derived MeasuresSeveral treasures were derived from the dependentrrmasures.
Turns per command (tpc) describes howmany turns it takes to successfully complete acommand.
Turns per minute (tpm) measures the speedwith which the user interacts.
A multirnodal error ratewas calculated based on how often repairs were827necessary.
Commands per m/nute (cpm) represents herate at which the subject is able to issue successfulcommands, estimating the collaborative effort.5 RESULTS0,P'l~me(min.
)tpctpmError ratecpmMeansEarly Late13.5 10.71.2 1.14.5 5.320% 14%3.8 4.8One-tailed t-test (df=7)t = 2.802,p<0.011t= 1.759, p< 0.061t = -4.00, p < O.O03t= 1.90, p< 0.05t= -3.915, p< 0.003These results how that when comparing late with earlyconfirmation: 1) subjects complete commands in fewerturns (the error rate and tpc are reduced, resulting in a30% error reduction); 2) they complete tums at a fasterrate (tpm is increased by 21%); and 3) they completemore commands in less time (cpm is increased by 26%).These results confirm all of our predictions.6 D ISCUSSIONThere are two likely reasons why late confLrmationoutperforms early confLrmation: implicit confirmationand multirnodal disambiguation.
Heisterkamp theorizedthat implicit confLrmation could reduce the number ofturns in dialogue \[25\].
Rudnicky proved in a speech-only digit-entry system that implicit confirmationimproved throughput when compared to explicitconfirmation \[27\], and our results confirm their findings.Lavie and colleagues have shown the usefulness of late-stage disambiguafion, during which speech-understanding systems pass multiple interpretationsthrough the system, using context in the final stages ofprocessing to disambiguate he recognition hypotheses\[28\].
However, we have demonstrated and empiricallyshown the advantage in combining these two strategiesin a multirnodal system.It can be argued that implicit confirmation is equivalentto being able to undo the last command, as somemultimodal systems allow \[3\].
However, commands thatare infeasible, profound, risky, costly, or irreversible aredifficult o undo.
For this reason, we argue that implicitconfirmation is often superior to the option of undoingthe previous command.
Implicit confirmation, whencombined with late confirmation, contributes to asmoother, faster, and more accurate collaborationbetween human and computer.7 CONCLUSIONSWe have developed a system that meets the followingexpectation: when the proposition being confirmed is acommand, it should be one that the system believes canbe executed.
To meet his expectation and increase theconversational performance of multimodal systems, wehave argued that confirmations should occur late in thesystem's understanding process, at a point after blendinghas enhanced its understanding.
This research hascompared two strategies: one in which confirmation isperformed immediately after speech recognition, andone in which it is delayed until after multimodalintegration.
The comparison shows that lateconfirmation reduces the time to perform mapmanipulation tasks with a multimodal interface.
Userscan interact faster and complete commands in fewertums, leading to a reduction i  collaborative effort.A direction for future research is to adopt a strategy fordetermining whether a confirmation is necessary \[29,30\], rather than confu'rning every utterance, andmeasuring this strategy's effectiveness.ACKNOWLEDGEMENTSThis work is supported in part by the InformationTechnology and Information Systems offices of DARPAunder contract number DABT63-95-C-007, and in partby ONR grant number N00014-95-I-1164.
It has beendone in collaboration with the US Navy's NCCOSCRDT&E Division (NRaD).
Thanks to the faculty, staff,and students who contributed tothis research, includingJoshua Clow, Peter Heeman, Michael Johnston, IraSmith, Stephen Sutton, and Karen Ward.
Special thanksto Donald Hanley for his insightful editorial commentand friendship.
Finally, sincere thanks to the people whovolunteered to participate as subjects in this research.REFERENCES\[1\] D. Perlis and K. Purang, "Conversational adequacy:Mistakes are the essence," in Proceedings of Workshop onDetecting, Repairing, and Preventing Human-MachineMiscommu ication, AAAI96, 1996.\[2\] R. Bolt, "Put-That-There: Voice and gesture at thegraphics interface," Computer Graphics, vol.
14, pp.
262-270,1980.\[3\] M. T. Vo and C. Wood, "Building an ApplicationFramework for Speech and Pen Input Integration inMulfirnodal Learning Interfaces," in Proceedings of IEEEInternational Conference on Acoustics, Speech, and SignalProcessing, ICASSP96, Atlanta, GA, 1996.828\[4\] E R. Cohen, M. Johnston, D. McGee, I. Smith, J. Pittman,L.
Chen, and J. Clow, "Mulfimodal interaction for distributedinteractive simulation," in Proceedings of InnovativeApplications of Artificial Intelligence Conference, IAAI97,Menlo Park, CA, 1997.\[5\] M. Johnston, E R. Cohen, D. McGee, S. L. Oviatt, J. A.Pittman, and I. Smith, "Unification-based multimodalintegration," in Proceedings of 35th Annual Meeting of theAssociation for Computational linguistics, ACL 97, Madrid,Spain, 1997.\[6\] J.
1L Rhyne and C. G Wolf, 'L-'hapter 7: Recognition-based user interfaces," in Advances in Human-ComputerInteraction, vol.
4, H. R. Hanson and D. Hix, Eds., pp.
191-250, 1992.\[7\] S. Oviatt, A. DeAngeli, and K. Kuhn, 'qntegration andsynchronization of input modes during multimodal human-computer interaction," in Proceedings of Conference onHuman Factors in Computing Systems, CHIPT, pp.
415-422,Atlanta, GA, 1997.\[8\] E Lefebvre, G Duncan, and E Poirier, "Speaking withcomputers: A multimodal approach," in Proceedings ofEUROSPEECH93 Conference, pp.
1665-1668, Berlin,Germany, 1993.\[9\] P. Morin and J. Junqua, "Habitable interaction i  goal-oriented multimodal dialogue systems," in Proceedings ofEUROSPEECH93 Conference, pp.
1669-1672, Berlin,Germany, 1993.\[ 10\] L. Hirschman and C. Pao, "I'he cost of errors in a spokenlanguage system," in Proceedings of EUROSPEECH93Conference, pp.
1419-1422, Berlin, Germany, 1993.\[11\] H. Clark and D. W'tikes-Gibbs, 'Referring as acollaborative process," Cognition, vol.
13, pp.
259-294, 1986.\[12\] P. R. Cohen and H. J. Levesque, "Confirmations and jointaction," in Proceedings of International Joint Conference onArtificial Intelligence, pp.
951-957, 1991.\[13\] D. G Novick and S. Sutton, "An empirical model ofacknowledgment for spoken-language systems," inProceedings of 32nd Annual Meeting of the Association forComputational Linguistics, ACL94, pp.
96-101, Las Cruces,New Mexico, 1994.\[14\] D. Tmum, "A Computational Theory of Grounding inNatural language Conversation," Computer ScienceDeparmaent, University of Rochester, Rochester, NY, Ph.D.1994.\[15\] H. H. Clark and E. E Schaefer, '~.ontributing todiscourse," Cognitive Science, vol.
13, pp.
259-294, 1989.\[16\] S. L. Oviatt, P. 1L Cohen, and A. M. Podlozny, "Spokenlanguage and performance during interpretation," inProceedings oflntemational Conference on Spoken LanguageProcessing, ICSLPgO, pp.
1305-1308, Kobe, Japan, 1990.\[17\] S. L. Oviatt and P. IL Cohen, "Spoken language ininterpreted telephone dialogues," Computer Speech andLanguage, vol.
6, pp.
277-302, 1992.\[18\] G Ferguson, J. Allen, and B. Miller, 'if'he design andimplementation f the TRAINS-96 system: A prototype mixed-initiative planning assistant," University of Rochester,Rochester, NY, TRAINS Technical Note 96-5, October 19961996.\[19\] G Ferguson, J. Allen, and B. Miller, 'q'RAINS-95:Towards a mixed-initiative planning assistant," inProceedingsof Third Conference on Artificial Intelligence PlanningSystems, AIPSP6, pp.
70-77, 1996.\[20\] D. Goddeau, E. BriU, J.
Glass, C. Pao, M. Phillips, J.Polifroni, S. Seneff, and V.. Zue, "GAI.AXY: A Human-language Interface to On-Line Travel Information," inProceedings ofInternational Conference on Spoken LanguageProcessing, ICSLP 94, pp.
707-710, Yokohama, Japan, 1994.\[21\] IL Lau, G Flammia, C. Pao, and V. Zue, "WebGALAXY:Spoken language access to information space from yourfavorite browser," Massachusetts Institute of Technology,Cambridge, MA, URLhttp'gwww.sls.lcs.mit.edu/SLSPublications.html, December1997 1997.\[22\] V. Zue, "Navigating the information superhighway usingspoken language interfaces," IEEE Expert, pp.
39-43, 1995.\[23\] P. R. Cohen, A. Cheyer, M. Wang, and S. C. Baeg, "Anopen agent architecture," in Proceedings ofAAA11994 SpringSyml~sium onSoftware Agents, pp.
1-8, 1994.\[24\] X. Huang, A. Acero, E AUeva, M.-Y.
Hwang, L. Jiang,and M. Mahajan, "Microsott Windows Highly IntelligentSpeech Recognizer.
Whisper," in Proceedings of IEEEInternational Conference on Acoustics, Speech, and SignalProcessing, ICASSP95, 1995.\[25\] P. Heisterkamp, "Ambiguity and uncertainty in spokendialogue," in Proceedings of EUROSPEECH93 Conference,pp.
1657-1660, Berlin, Germany, 1993.\[26\] Y. Takebayashi, 'L-'hapter 14: Integration of understandingand synthesis functions for multimedia interfaces," inMultimedia interface design, M. M. Blatmer and R. B.Dannenberg, Eds.
New York, NY: ACM Press, pp.
233-256,1992.\[27\] A. I. Rudnicky and A. G Hauptmann, "Chapter 10:Multimodal interaction in speech systems," in MultimediaInterface Design, M. M. Blattner and R. B. Dannenberg, Eds.New York, NY: ACM Press, pp.
147-171, 1992.\[28\] A. Lavie, L. Levin, Y. Qu, A. Waibel, and D. Gates,"Dialogue processing in a conversational speech translationsystem," in Proceedings of International Conference onSpoken Language Processing, ICSLP 96, pp.
554-557, 1996.\[29\] R. W. Smith, "An evaluation of swategies for selectiveutterance verification for spoken natural language dialog," inProceedings ofFifth Conference on Applied Natural LanguageProcessing, ANId~96, pp.
41-48, 1996.\[30\] Y. N'fimi and Y. Kobayashi, "A dialog control strategybased on the reliability of speech recognition," inProceedingsof International Conference on Spoken Language Processing,ICSLP96, pp.
534-537, 1996.829
