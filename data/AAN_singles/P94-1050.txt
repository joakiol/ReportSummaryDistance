AN AUTOMATIC  METHOD OF F INDING TOP ICBOUNDARIESJeffrey C. Reynar*Depar tment  of Computer  and In format ion  Sc ienceUn ivers i ty  of  Pennsy lvan iaPh i lade lph ia ,  Pennsy lvan ia ,  USAj c reynar@unag i .c i s .upenn.eduAbstractThis article outlines a new method of locating discourseboundaries based on lexical cohesion and a graphicaltechnique called dotplotting.
The application of dot-plotting to discourse segmentation can be performed ei-ther manually, by examining a graph, or automatically,using an optimization algorithm.
The results of two ex-periments involving automatically locating boundariesbetween a series of concatenated documents are pre-sented.
Areas of application and future directions forthis work are also outlined.In t roduct ionIn general, texts are "about" some topic.
That is, thesentences which compose a document contribute infor-mation related to the topic in a coherent fashion.
In allbut the shortest exts, the topic will be expounded uponthrough the discussion of multiple subtopics.
Whetherthe organization of the text is hierarchical in nature,as described in (Grosz and Sidner, 1986), or linear, asexamined in (Skorochod'ko, 1972), boundaries betweensubtopics will generally exist.In some cases, these boundaries will be explicit andwill correspond to paragraphs, or in longer texts, sec-tions or chapters.
They can also be implicit.
Newspa-per articles often contain paragraph demarcations, butless frequently contain section markings, even thoughlengthy articles often address the main topic by dis-cussing subtopics in separate paragraphs or regions ofthe article.Topic boundaries are useful for several different asks.Hearst and Plaunt (1993) demonstrated their usefulnessfor information retrieval by showing that segmentingdocuments and indexing the resulting subdocumentsimproves accuracy on an information retrieval task.Youmans (1991) showed that his text segmentation al-gorithm could be used to manually find scene bound-aries in works of literature.
Morris and Hirst (1991) at-*The author would like to thank Christy Doran, Ja-son Eisner, A1 Kim, Mark Liberman, Mitch Marcus, MikeSchultz and David Yarowsky for their helpful comments andacknowledge the support of DARPA grant No.
N0014-85-K0018 and ARO grant No.
DAAL 03~89-C0031 PRI.tempted to confirm the theories of discourse structureoutlined in (Grosz and Sidner, 1986) using informationfrom a thesaurus.
In addition, Kozima (1993) specu-lated that segmenting text along topic boundaries maybe useful for anaphora resolution and text summariza-tion.This paper is about an automatic method of findingdiscourse boundaries based on the repetition of lexi-cal items.
Halliday and Hasan (1976) and others haveclaimed that the repetition of lexical items, and in par-ticular content-carrying lexical items, provides coher-ence to a text.
This observation has been used implic-itly in several of the techniques described above, butthe method presented here depends exclusively on it.Methodo logyChurch (1993) describes a graphical method, called doL-plotting, for aligning bilingual corpora.
This methodhas been adapted here for finding discourse boundaries.The dotplot used for discovering topic boundaries i cre-ated by enumerating the lexical items in an article andplotting points which correspond to word repetitions.For example, if a particular word appears at word po-sitions x and y in a text, then the four points corre-sponding to the cartesian product of the set containingthese two positions with itself would be plotted.
Thatis, (x, x), (x, y), (y, x) and (y, y) would be plotted onthe dotplot.Prior to creating the dotplot, several filters are ap-plied to the text.
First, since closed-class words carrylittle semantic weight, they are removed by filteringbased on part of speech information.
Next, the remain-ing words are lemmatized using the morphological nal-ysis software described in (Karp et al, 1992).
Finally,the lemmas are filtered to remove a small number ofcommon words which are regarded as open-class by thepart of speech tag set, but which contribute little to themeaning of the text.
For example, forms of the verbsBE and HAVE are open class words, but are ubiquitousin all types of text.
Once these steps have been taken,the dotplot is created in the manner described above.
Asample dotplot of four concatenated Wall Street Jour-nal articles is shown in figure 1.
The real boundaries3313.00 - - .
.
.
.
.
.
.
.
.
.
.
.
: : .
.
.
.
.
~-'" .,.
.
.
.
.
.
--z .~o-  .
.
j .
:...:..'.:".
'a":: .
.
.
.
: .-... :.
:_-~z~o " " .
:  " :  ' -: ..... : " ~::; ~:~: .
.. ".
.
.
.
.
, ... .., .. : ... .. :~ ~ ;-~..';;-;.
; .z~-  . '
" '~ ' :  : !
.
, .
.~=, .
.
; .
; .
/~ .
,~; .
; ; , .
: .
.
.
.
:  .-'.. - ' :  .
.
-.
.
.
.
.
: : :  - ~ ~!
'~: :~~-~!
;  _~-~-  ~ .
.
.
.
.
~i."
" ( " .
j  " ' f  .
: ' . '
" .
i~ i i !
i i i - i "~" ' " '~  ?.
~...:i?
"2 : ;  .
. '
- .
. '
:  L "" --o=o-" .
;~: ,  . "
. "
.
: -~: .h  , .
-" " ", /':."
":'"" :.
'i-" .
.~"  .
V" ~: .~ .
,  " ' . "
?
? "
? '
- .
- .o.~o - -~: ~,~'~. "
i : " '%!
=i~: ": ".
:" ?
.
.
.
- ,  .,~.
".
~: 7" .".
_0.~-  ; .
.7 ;  '::if:" :: ::~ " :~ :'~ : ,- " " ?:.
:::-.
i:~ -?
~-  ~; ~i..~i~?
:,::~~5:.,-V : ~": "" " .
?- " -I I I I I I I0.~ o2o I,o0 I~  2Oo 2,50 3.~Xx l~Figure 1: The dotplot of four concatenated Wall StreetJournal articles.vz lO  "~~0.~ - -demity550.O0 - -  /~~00.00 - -  /450.00 - -  j - -40020 - -  /15It00 - -  ~15o.O0 --0.00 0.5o 1.00 I~  2.O0 2.$0 3.0OXx l~Figure 2: The outside density plot of the same fourarticles.between documents are located at word positions 1085,2206 and 2863.The word position in the file increases as values in-crease along both axes of the dotplot.
As a result, thediagonal with slope equal to one is present since eachword in the text is identical to itself.
The gaps in thisline correspond to points where words have been re-moved by one of the filters.
Since the repetition of lexi-cal items occurs more frequently within regions of a textwhich are about the same topic or group of topics, thevisually apparent squares along the main diagonal ofthe plot correspond to regions of the text.
Regions aredelimited by squares because of the symmetry presentin the dotplot.Although boundaries may be identified visually usingthe dotplot, the plot itself is unnecessary for the dis-covery of boundaries.
The reason the regions along thediagonal are striking to the eye is that they are denser.This fact leads naturally to an algorithm based on max-imizing the density of the regions within squares alongthe diagonal, which in turn corresponds to minimizingthe density of the regions not contained within thesesquares.
Once the densities of areas outside these re-gions have been computed, the algorithm begins by se-lecting the boundary which results in the lowest outsidedensity.
Additional boundaries are added until eitherthe outside density increases or a particular numberof boundaries have been added.
Potential boundariesare selected from a list of either sentence boundaries orparagraph boundaries, depending on the experiment.More formally, let n be the length of the concatena-tion of articles; let m be the number of unique tokens(after lemmatization and removal of words on the stoplist); let B be a list of boundaries, initialized to containonly the boundary corresponding to the beginning ofthe series of articles, 0.
Maintain B in ascending order.Let i be a potential boundary; let P = B (3 {i), alsosorted in ascending order; let Vx,y be a vector contain-ing the word counts associated with word positions xthrough y in the concatenation.
Now, find the i suchthat the equation below is minimized.
Repeat this min-imization, inserting i into B, until the desired numberof boundaries have been located.I 'lj=2The dot product in the equation reveals the similar-ity between this method and Heart and Plaunt's (1993)work which was done in a vector-space framework.
Thecrucial difference lies in the global nature of this equa-tion.
Their algorithm placed boundaries by comparingneighboring regions only, while this technique compareseach region with all other regions.A graph depicting the density of the regions not en-closed in squares along the diagonal is shown in figure2.
The y-coordinate on this graph represents the den-sity when a boundary is placed at the correspondinglocation on the x-axis.
These data are derived fromthe dotplot shown in figure 1.
Actual boundaries corre-spond to the most extreme minima--those at positions1085, 2206 and 2863.Resu l t sSince determining where topic boundaries belong is asubjective task, (Passoneau and Litman, 1993), the pre-liminary experiments conducted using this algorithminvolved discovering boundaries between concatenatedarticles.
All of the articles were from the Wall StreelJournal and were tagged in conjunction with the PennTreebank project, which is described in (Marcus el al.,1993).
The motivation behind this experiment is thatnewspaper articles are about sufficiently different top-ics that discerning the boundaries between them shouldserve as a baseline measure of the algorithm's effective-ness.332Expt.
1 Expt.
2# of exact matches 271 106:#: of close matches 196 55# of extra boundaries 1085 38# of missed boundaries 43 355Precision 0.175 0.549Precision counting close 0.300 0.803Recall 0.531 0.208Recall counting close 0.916 0.304Table 1: Results of two experiments.The results of two experiments in which between twoand eight randomly selected Wall Street Journal arti-cles were concatenated are shown in table 1.
Both ex-periments were performed on the same data set whichconsisted of 150 concatenations of articles containing atotal of 660 articles averaging 24.5 sentences in length.The average sentence length was 24.5 words.
The differ-ence between the two experiments was that in the firstexperiment, boundaries were placed only at the ends ofsentences, while in the second experiment, they wereonly placed at paragraph boundaries.
Tuning the stop-ping criteria parameters in either method allows im-provements in precision to be traded for declines in re-call and vice versa.
The first experiment demonstratesthat high recall rates can be achieved and the secondshows that high precision can also be achieved.In these tests, a minimum separation between bound-aries was imposed to prevent documents from beingrepeatedly subdivided around the location of one ac-tual boundary.
For the purposes of evaluation, an exactmatch is one in which the algorithm placed a boundaryat the same position as one existed in the collection ofarticles.
A missed boundary is one for which the algo-rithm found no corresponding boundary.
If a boundarywas not an exact match, but was within three sentencesof the correct location, the result was considered a closematch.
Precision and recall scores were computed bothincluding and excluding the number of close matches.The precision and recall scores including close matchesreflect the admission of only one close match per ac-tual boundary.
It should be noted that some of theextra boundaries found may correspond to actual shiftsin topic and may not be superfluous.Future  WorkThe current implementation of the algorithm relies onpart of speech information to detect closed class wordsand to find sentence boundaries.
However, a largercommon word list and a sentence boundary recognitionalgorithm could be employed to obviate the need fortags.
Then the method could be easily applied to largeamounts of text.
Also, since the task of segmentingconcatenated documents i quite artificial, the approachshould be applied to finding topic boundaries.
To thisend, the algorithm's output should be compared to thesegmentations produced by human judges and the sec-tion divisions authors insert into some forms of writing,such as technical writing.
Additionally, the segment in-formation produced by the algorithm should be usedin an information retrieval task as was done in (Hearstand Plaunt, 1993).
Lastly, since this paper only exam-ined flat segmentations, work needs to be done to seewhether useful hierarchical segmentations can be pro-duced.ReferencesChurch, Kenneth Ward.
Char_align: A Program forAligning Parallel Texts at the Character Level.
Pro-ceedings of the 31st Annual Meeting of the Associa-tion for Computational Linguistics, 1993.Grosz, Barbara J. and Candace L. Sidner.
Attention,Intentions and the Structure of Discourse.
Computa-tional Linguistics, Volume 12, Number 3, 1986.Halliday, Michael and Ruqaiya Hasan.
Cohesion in En-glish.
New York: Longman Group, 1976.Hearst, Marti A. and Christian Plaunt.
Subtopic Struc-turing for Full-Length Document Access.
Proceed-ings of the Special Interest Group on Information Re-trieval, 1993.Karp, Daniel, Yves Schabes, Martin Zaidel and DaniaEgedi.
A Freely Available Wide Coverage Morpho-logical Analyzer for English.
Proceedings of the 15thInternational Conference on Computational Linguis-tics, 1992.Kozima, Hideki.
Text Segmentation Based on Similar-ity Between Words.
Proceedings of the 31st AnnualMeeting of the Association for Computational Lin-guistics, 1993.Marcus, Mitchell P., Beatrice Santorini and Mary AnnMarkiewicz.
Building a Large Annotated Corpus ofEnglish: The Penn Treebank.
Computational Lin-guistics, Volume 19, Number 2, 1993.Morris, Jane and Graeme Hirst.
Lexical Cohesion Com-puted by Thesaural Relations as an Indicator of theStructure of Text.
Computational Linguistics, Vol-ume 17, Number 1, 1991.Passoneau, Rebecca J. and Diane J. Litman.
Intention-Based Segmentation: Human Reliability and Corre-lation with Linguistic Cues.
Proceedings of the 31stAnnual Meeting of the Association for ComputationalLinguistics, 1993.Skorochod'ko, E.F. Adaptive Method of Automatic Ab-stracting and Indexing.
Information Processing, Vol-ume 71, 1972.Youmans, Gilbert.
A New Tool for Discourse Analy-sis: The Vocabulary-Management Profile.
Language,Volume 67, Number 4, 1991.333
