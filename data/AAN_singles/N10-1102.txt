Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 693?696,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLanguage identification of names with SVMsAditya Bhargava and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{abhargava,kondrak}@cs.ualberta.caAbstractThe task of identifying the language of textor utterances has a number of applications innatural language processing.
Language iden-tification has traditionally been approachedwith character-level language models.
How-ever, the language model approach cruciallydepends on the length of the text in ques-tion.
In this paper, we consider the problemof language identification of names.
We showthat an approach based on SVMs with n-gramcounts as features performs much better thanlanguage models.
We also experiment withapplying the method to pre-process transliter-ation data for the training of separate models.1 IntroductionThe task of identifying the language of text or utter-ances has a number of applications in natural lan-guage processing.
Font Llitjo?s and Black (2001)show that language identification can improve theaccuracy of letter-to-phoneme conversion.
Li etal.
(2007) use language identification in a translit-eration system to account for different semantictransliteration rules between languages when the tar-get language is Chinese.
Huang (2005) improves theaccuracy of machine transliteration by clustering histraining data according to the source language.Language identification has traditionally beenapproached using character-level n-gram languagemodels.
In this paper, we propose the use of sup-port vector machines (SVMs) for the language iden-tification of very short texts such as proper nouns.We show that SVMs outperform language modelson two different data sets consisting of personalnames.
Furthermore, we test the hypothesis that lan-guage identification can improve transliteration bypre-processing the source data and training separatemodels using a state-of-the-art transliteration sys-tem.2 Previous workN -gram approaches have proven very popular forlanguage identification in general.
Cavnar and Tren-kle (1994) apply n-gram language models to generaltext categorization.
They construct character-levellanguage models using n-grams up to a certain max-imum length from each class in their training cor-pora.
To classify new text, they generate an n-gramfrequency profile from the text and then assign it tothe class having the most similar language model,which is determined by summing the differences inn-gram ranks.
Given 14 languages, text of 300 char-acters or more, and retaining the 400 most commonn-grams up to length 5, they achieve an overall accu-racy of 99.8%.
However, the accuracy of the n-gramapproach strongly depends on the length of the texts.Kruengkrai et al (2005) report that, on a languageidentification task of 17 languages with average textlength 50 bytes, the accuracy drops to 90.2%.
WhenSVMs were used for the same task, they achieved99.7% accuracy.Konstantopoulos (2007) looks particularly at thetask of identifying the language of proper nouns.
Hefocuses on a data set of soccer player names comingfrom 13 possible national languages.
He finds thatusing general n-gram language models yields an av-erage F1 score of only 27%, but training the modelsspecifically to these smaller data gives significantlybetter results: 50% average F1 score for last names693only, and 60% for full names.On the other hand, Li et al (2007) report somegood results for single-name language identificationusing n-gram language models.
For the task of sepa-rating single Chinese, English, and Japanese names,they achieve an overall accuracy of 94.8%.
One rea-son that they do better is because of the smaller num-ber of classes.
We can further see that the languagesin question are very dissimilar, making the problemeasier; for example, the character ?x?
appears onlyin the list of Chinese names, and the bigram ?kl?
ap-pears only in the list of English names.3 Language identification with SVMsRather than using language models to determine thelanguage of a name, we propose to count charac-ter n-gram occurrences in the given name, for n upto some maximum length, and use these counts asthe features in an SVM.
We choose SVMs becausethey can take a large number of features and learn toweigh them appropriately.
When counting n-grams,we include space characters at the beginning andend of each word, so that prefixes and suffixes arecounted appropriately.
In addition to n-gram counts,we also include word length as a feature.In our initial experiments, we tested several dif-ferent kernels.
The kernels that performed the bestwere the linear, sigmoid, and radial basis function(RBF) kernels.
We tested various maximum n-gramlengths; Figure 1 shows the accuracy of the linearkernel as a function of maximum n-gram length.Polynomial kernels, a substring match?count stringkernel, and a string kernel based on the edit distanceall performed poorly in comparison.
We also exper-imented with other modifications such as normaliz-ing the feature vectors, and decreasing the weightsof frequent n-gram counts to avoid larger countsdominating smaller counts.
Since the effects werenegligible, we exclude these results from this paper.In our experiments, we used the LIBLINEAR(Fan et al, 2008) package for the linear kernel andthe LIBSVM (Chang and Lin, 2001) package for theRBF and sigmoid kernels.
We discarded any peri-ods and parentheses, but kept apostrophes and hy-phens, and we converted all letters to lower case.We removed very short names of length less thantwo.
For all data sets, we held out 10% of the data40 50 60 70 801  2  3  4  5  6Accuracy (%) Maximum n-gram lengthFigure 1: Cross-validation accuracy of the linear kernelon the Transfermarkt full names corpus.as the test set.
We then found optimal parametersfor each kernel type using 10-fold cross-validationon the remaining training set.
This yielded optimummaximum n-gram lengths of four for single namesand five for full names.
Using the optimal parame-ters, we constructed models from the entire trainingdata and then tested the models on the held-out testset.4 Intrinsic evaluationWe used two corpora to test our SVM-based ap-proach: the Transfermarkt corpus of soccer playernames, and the Chinese-English-Japanese (CEJ)corpus of first names and surnames.
These corporaare described in further detail below.4.1 Transfermarkt corpusThe Transfermarkt corpus (Konstantopoulos, 2007)consists of European soccer player names annotatedwith one of 13 possible national languages, with sep-arate lists provided for last names and full names.Diacritics were removed in order to avoid trivializ-ing the task.
There are 14914 full names, with aver-age length 14.8, and 12051 last names, with averagelength 7.8.
It should be noted that these data arenoisy; the fact that a player plays for a certain na-tion?s team does not necessarily indicate that his orher name is of that nation?s language.
For example,Dario Dakovic was born in Bosnia but plays for theAustrian national team; his name is therefore anno-tated as German.Table 1 shows our results on the Transfermarktcorpus.
Because Konstantopoulos (2007) providesonly F1 scores, we used his scripts to generate newresults using language models and calculate the ac-curacy instead, which allows us to be consistent withour tests on other data sets.
Our results show that us-694Method Last names Full namesLanguage models 44.7 54.2Linear SVM 56.4 79.9RBF SVM 55.7 78.9Sigmoid SVM 56.2 78.7Table 1: Language identification accuracy on the Trans-fermarkt corpus.
Language models have n = 5.ing SVMs clearly outperforms using language mod-els on the Transfermarkt corpus; in fact, SVMs yieldbetter accuracy on last names than language modelson full names.
Differences between kernels are notstatistically significant.4.2 CEJ corpusThe CEJ corpus (Li et al, 2007) provides a com-bined list of first names and surnames, each classi-fied as Chinese, English, or Japanese.
There are atotal of 97115 names with an average length of 7.6characters.
This corpus was used for the semantictransliteration of personal names into Chinese.We found that the RBF and sigmoid kernels werevery slow?presumably due to the large size of thecorpus?so we tested only the linear kernel.
Table 2shows our results in comparison to those of languagemodels reported in (Li et al, 2007); we reduce theerror rate by over 50%.5 Application to machine transliterationMachine transliteration is one of the primary poten-tial applications of language identification becausethe language of a word often determines its pronun-ciation.
We therefore tested language identificationto see if results could indeed be improved by usinglanguage identification as a pre-processing step.5.1 DataThe English-Hindi corpus of names (Li et al, 2009;MSRI, 2009) contains a test set of 1000 names rep-resented in both the Latin and Devanagari scripts.We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them.1 Wediscarded those names that fell into both categories1Our tagged data are available online at http://www.cs.ualberta.ca/?ab31/langid/.Method Ch.
Eng.
Jap.
AllLang.
model 96.4 89.9 96.5 94.8Linear SVM 99.0 94.8 97.6 97.6Table 2: Language identification accuracy on the CEJcorpus.
Language models have n = 4.(e.g.
?Maya?)
as well as those that we could notconfidently classify.
In total, we discarded 95 ofthese names, and randomly selected 95 names fromthe training set that we could confidently classify tocomplete our corpus of 1000 names.
Of the 1000names, 546 were classified as being of Indian originand the remaining 454 were classified as being ofnon-Indian origin; the names have an average lengthof 7.0 characters.We trained our language identification approachon 900 names, with the remaining 100 names serv-ing as the test set.
The resulting accuracy was 80%with the linear kernel, 84% with the RBF kernel,and 83% with the sigmoid kernel.
In this case, theperformance of the RBF kernel was found to be sig-nificantly better than that of the linear kernel accord-ing to the McNemar test with p < 0.05.5.2 Experimental setupWe tested a simple method of combining languageidentification with transliteration.
We use a lan-guage identification model to split the training, de-velopment, and test sets into disjoint classes.
Wetrain a transliteration model on each separate class,and then combine the results.Our transliteration system was DIRECTL (Ji-ampojamarn et al, 2009).
We trained the languageidentification model over the entire set of 1000tagged names using the parameters from above.
Be-cause these names comprised most of the test setand were now being used as the training set for thelanguage identification model, we swapped variousnames between sets such that none of the words usedfor training the language identification model werein the final transliteration test set.Using this language identification model, we splitthe data.
After splitting, the ?Indian?
training, de-velopment, and testing sets had 5032, 575, and 483words respectively while the ?non-Indian?
sets had11081, 993, and 517 words respectively.6955.3 ResultsSplitting the data and training two separate mod-els yielded a combined top-1 accuracy of 46.0%, ascompared to 47.0% achieved by a single translitera-tion model trained over the full data; this differenceis not statistically significant.
Somewhat counter-intuitively, using language identification as a pre-processing step for machine transliteration yields noimprovement in performance for our particular dataand transliteration system.While it could be argued that our language identi-fication accuracy of 84% is too low to be useful here,we believe that the principal reason for this perfor-mance decrease is the reduction in the amount ofdata available for the training of the separate mod-els.
We performed an experiment to confirm thishypothesis: we randomly split the full data into twosets, matching the sizes of the Indian and non-Indiansets.
We then trained two separate models and com-bined the results; this yielded a top-1 accuracy of41.5%.
The difference between this and the 46.0%result above is statistically significant with p < 0.01.From this we conclude that the reduction in data sizewas a significant factor in the previously describednull result, and that language identification does pro-vide useful information to the transliteration system.In addition, we believe that the transliteration systemmay implicitly leverage the language origin infor-mation.
Whether a closer coupling of the two mod-ules could produce an increase in accuracy remainsan open question.6 ConclusionWe have proposed a novel approach to the task oflanguage identification of names.
We have shownthat applying SVMs with n-gram counts as fea-tures outperforms the predominant approach basedon language models.
We also tested language identi-fication in one of its potential applications, machinetransliteration, and found that a simple method ofsplitting the data by language yields no significantchange in accuracy, although there is an improve-ment in comparison to a random split.In the future, we plan to investigate other methodsof incorporating language identification in machinetransliteration.
Options to explore include the useof language identification probabilities as features inthe transliteration system (Li et al, 2007), as well assplitting the data into sets that are not necessarilydisjoint, allowing separate transliteration models tolearn from potentially useful common information.AcknowledgementsWe thank Sittichai Jiampojamarn for his assistancewith the DIRECTL transliteration system, ShaneBergsma for his advice, and Stasinos Konstantopou-los for providing us with his scripts and data.
Thisresearch was supported by the Natural Sciences andEngineering Research Council of Canada.ReferencesW.
B. Cavnar and J. M. Trenkle.
1994.
N-gram-basedtext categorization.
In Proc.
of the Third Annual Sym-posium on Document Analysis and Information Re-trieval, pages 161?175.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: a li-brary for support vector machines.
Software availableat http://www.csie.ntu.edu.tw/?cjlin/libsvm.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for large lin-ear classification.
Journal of Machine Learning Re-search, 9:1871?1874.A.
Font Llitjo?s and A. W. Black.
2001.
Knowledge oflanguage origin improves pronunciation accuracy ofproper names.
In Proc.
of Eurospeech, pages 1919?1922.F.
Huang.
2005.
Cluster-specific named entity transliter-ation.
In Proc.
of HLT-EMNLP, pages 435?442.S.
Jiampojamarn, A. Bhargava, Q. Dou, K. Dwyer, andG.
Kondrak.
2009.
DirecTL: a language independentapproach to transliteration.
In Proc.
of ACL-IJCNLPNamed Entities Workshop, pages 28?31.S.
Konstantopoulos.
2007.
What?s in a name?
In Proc.of RANLP Computational Phonology Workshop.C.
Kruengkrai, P. Srichaivattana, V. Sornlertlamvanich,and H. Isahara.
2005.
Language identification basedon string kernels.
In Proc.
of International Symposiumon Communications and Information Technologies.H.
Li, K. C. Sim, J.-S. Kuo, and M. Dong.
2007.
Seman-tic transliteration of personal names.
In Proc.
of ACL,pages 120?127.H.
Li, A. Kumaran, V. Pervouchine, and M. Zhang.
2009.Report of NEWS 2009 machine transliteration sharedtask.
In Proc.
of Named Entities Workshop: SharedTask on Transliteration, pages 1?18.MSRI, 2009.
Microsoft Research India.
http://research.microsoft.com/india.696
