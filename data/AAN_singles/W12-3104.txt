Proceedings of the 7th Workshop on Statistical Machine Translation, pages 59?63,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsImproving AMBER, an MT Evaluation MetricBoxing Chen, Roland Kuhn and George FosterNational Research Council Canada283 Alexandre-Tach?
Boulevard, Gatineau (Qu?bec), Canada J8X 3X7{Boxing.Chen, Roland.Kuhn, George.Foster}@nrc.caAbstractA recent paper described a new machinetranslation evaluation metric, AMBER.
Thispaper describes two changes to AMBER.
Thefirst one is incorporation of a new orderingpenalty; the second one is the use of thedownhill simplex algorithm to tune theweights for the components of AMBER.
Wetested the impact of the two changes, usingdata from the WMT metrics task.
Each of thechanges by itself improved the performance ofAMBER, and the two together yielded evengreater improvement, which in some caseswas more than additive.
The new version ofAMBER clearly outperforms BLEU in termsof correlation with human judgment.1 IntroductionAMBER is a machine translation evaluation metricfirst described in (Chen and Kuhn, 2011).
It is de-signed to have the advantages of BLEU (Papineniet al, 2002), such as nearly complete languageindependence and rapid computability, while at-taining even higher correlation with human judg-ment.
According to the paper just cited: ?It can bethought of as a weighted combination of dozens ofcomputationally cheap features based on word sur-face forms for evaluating MT quality?.
Many re-cently defined machine translation metrics seek toexploit deeper sources of knowledge than areavailable to BLEU, such as external lexical andsyntactic resources.
Unlike these and like BLEU,AMBER relies entirely on matching surface formsin tokens in the hypothesis and reference, thus sac-rificing depth of knowledge for simplicity andspeed.In this paper, we describe two improvements toAMBER.
The first is a new ordering penalty called?v?
developed in (Chen et al, 2012).
The secondremedies a weakness in the 2011 version ofAMBER  by carrying out automatic, rather thanmanual, tuning of this metric?s free parameters; wenow use the simplex algorithm to do the tuning.2 AMBERAMBER is the product of a score and a penalty, asin Equation (1); in this, it resembles BLEU.
How-ever, both the score part and the penalty part aremore sophisticated than in BLEU.
The score part(Equation 2) is enriched by incorporating theweighted average of n-gram precisions (AvgP), theF-measure derived from the arithmetic averages ofprecision and recall (Fmean), and the arithmeticaverage of F-measure of precision and recall foreach n-gram (AvgF).
The penalty part is aweighted product of several different penalties(Equation 3).
Our original AMBER paper (Chenand Kuhn, 2011) describes the ten penalties used atthat time; two of these penalties, the normalizedSpearman?s correlation penalty and the normalizedKendall?s correlation penalty, model word reorder-ing.penaltyscoreAMBER ?=                 (1)AvgFFmeanAvgPscore??
?+?+?=)1( 2121????
(2)?==Piwiipenpenalty1(3)where 1?
and 2?
are weights of each score com-ponent; wi is the weight of each penalty peni.59In addition to the more complex score and pen-alty factors, AMBER differs from BLEU in twoother ways:?
Not only fixed n-grams, but three differentkinds of flexible n-grams, are used in com-puting scores and penalties.?
The AMBER score can be computed withdifferent types of text preprocessing, i.e.different combinations of several text pre-processing techniques: lowercasing, to-kenization, stemming, word splitting, etc.
8types were tried in (Chen and Kuhn, 2011).When using more than one type, the finalscore is computed as an average over runs,one run per type.
In the experiments re-ported below, we averaged over two typesof preprocessing.3 Improvements to AMBER3.1   Ordering penalty vWe use a simple matching algorithm (Isozaki etal., 2010) to do 1-1 word alignment between thehypothesis and the reference.After word alignment, represent the reference bya list of normalized positions of those of its wordsthat were aligned with words in the hypothesis, andrepresent the hypothesis by a list of positions forthe corresponding words in the reference.
For bothlists, unaligned words are ignored.
E.g., let P1 =reference, P2 = hypothesis:P1: 11p21p31p41p  ?ip1  ?np1P2: 12p22p32p42p  ?ip2  ?np2Suppose we haveRef: in the winter of 2010 , I visited ParisHyp: I visited Paris in 2010 ?s winterThen we obtainP1: 1 2 3 4 5 6  (the 2nd word ?the?, 4thword ?of?
and 6th word ?,?
in the referenceare not aligned to any word in thehypothesis.
Thus, their positions are not inP1, so the positions of the matching words?in winter 2010 I visited Paris?
are nor-malized to 1 2 3 4 5 6)P2: 4 5 6 1 3 2 (the word ??s?
wasunaligned).The ordering metric v is computed from twodistance measures.
The first is absolutepermutation distance:?=?=niii ppPPDIST121211 ||),(               (4)Let2/)1(),(1 2111 +?= nnPPDIST?
(5)v1 ranges from 0 to 1; a larger value means moresimilarity between the two permutations.
Thismetric is similar to Spearman?s ?
(Spearman,1904).
However, we have found that ?
punisheslong-distance reordering too heavily.
For instance,1?is more tolerant than ?
of the movement of?recently?
in this example:Ref: Recently , I visited ParisHyp: I visited Paris recentlyP1: 1 2 3 4P2: 2 3 4 1Its 2.0-1 1)4(16)9116(1?==?+++?
; however, its4.0-1 1)/24(4 3111 == + +++1v .Inspired by HMM word alignment (Vogel et al,1996), our second distance measure is based onjump width.
This punishes only once a sequence ofwords that moves a long distance with the internalword order conserved, rather than on every word.In the following, only two groups of words havemoved, so the jump width punishment is light:Ref: In the winter of 2010, I visited ParisHyp: I visited Paris in the winter of 2010The second distance measure is?=????
?=niiiii ppppPPDIST1122111212 |)()(|),(   (6)where we set 001 =p  and 002 =p .
Let1),(1 2 2122?
?=nPPDISTv                     (7)As with v1, v2 is also from 0 to 1, and larger valuesindicate more similar permutations.
The orderingmeasure vs is the harmonic mean of v1 and v2 (Chenet al, 2012):)11(2 21 /v/v/vs +=.
(8)In (Chen et al, 2012) we found this to be slightlymore effective than the geometric mean.
vs in (8) iscomputed at segment level.
We compute documentlevel ordering vD with a weighted arithmetic mean:60?
?==?= ls sls ssDRlenRlenvv11)()((9)where l is the number of segments of thedocument, and len(R) is the length of the referenceafter text preprocessing.
vs is the segment-levelordering penalty.Recall that the penalty part of AMBER is theweighted product of several component penalties.In the original version of AMBER, there were 10component penalties.
In the new version, v is in-corporated as an additional, 11th weighted penaltyin (3).
Thus, the new version of AMBER incorpo-rates three reordering penalties: Spearman?scorrelation, Kendall?s correlation, and v. Note thatv is also incorporated in a tuning metric we recent-ly devised (Chen et al, 2012).3.2   Automatic tuningIn (Chen and Kuhn, 2011), we manually set the 17free parameters of AMBER (see section 3.2 of thatpaper).
In the experiments reported below, wetuned the 18 free parameters ?
the original 17 plusthe ordering metric v described in the previous sec-tion - automatically, using the downhill simplexmethod of (Nelder and Mead, 1965) as describedin (Press et al, 2002).
This is a multidimensionaloptimization technique inspired by geometricalconsiderations that has shown good performance ina variety of applications.4 ExperimentsThe experiments are carried out on WMT metrictask data: specifically, the WMT 2008, WMT2009, WMT 2010, WMT 2011 all-to-English, andEnglish-to-all submissions.
The languages ?all?(?xx?
in Table 1) include French, Spanish, Germanand Czech.
Table 1 summarizes the statistics forthese data sets.Set Year Lang.
#system #sent-pairTest1 2008 xx-En 43 7,804Test2 2009 xx-En 45 15,087Test3 2009 en-Ex 40 14,563Test4 2010 xx-En 53 15,964Test5 2010 en-xx 32 18,508Test6 2011 xx-En 78 16,120Test7 2011 en-xx 94 23,209Table 1: Statistics of the WMT dev and test sets.We used 2008 and 2011 data as dev sets, 2009and 2010 data as test sets.
Spearman?s rankcorrelation coefficient ?
was employed to measurecorrelation of the metric with system-level humanjudgments of translation.
The human judgmentscore was based on the ?Rank?
only, i.e., howoften the translations of the system were rated asbetter than those from other systems (Callison-Burch et al, 2008).
Thus, BLEU and the new ver-sion of AMBER were evaluated on how well theirrankings correlated with the human ones.
For thesegment level, we followed (Callison-Burch et al,2010) in using Kendall?s rank correlationcoefficient ?.In what follows, ?AMBER1?
will denote a vari-ant of AMBER as described in (Chen and Kuhn,2011).
Specifically, it is the variant AMBER(1,4) ?that is, the variant in which results are averagedover two runs with the following preprocessing:1.
A run with tokenization and lower-casing2.
A run in which tokenization and lower-casing are followed by the word splitting.Each word with more than 4 letters is seg-mented into two sub-words, with one beingthe first 4 letters and the other the last 2 let-ters.
If the word has 5 letters, the 4th letterappears twice: e.g., ?gangs?
becomes?gang?
+ ?gs?.
If the word has more than 6letters, the middle part is thrown away.The second run above requires some explana-tion.
Recall that in AMBER, we wish to avoid useof external resources such as stemmers and mor-phological analyzers, and we aim at maximal lan-guage independence.
Here, we are doing a kind of?poor man?s morphological analysis?.
The firstfour letters of a word are an approximation of itsstem, and the last two letters typically carry at leastsome information about number, gender, case, etc.Some information is lost, but on the other hand,when we use the metric for a new language (or atleast, a new Indo-European language) we knowthat it will extract at least some of the informationhidden inside morphologically complex words.The results shown in Tables 2-4 compare thecorrelation of variants of AMBER with humanjudgment; Table 5 compares the best version ofAMBER (AMBER2) with BLEU.
For instance, tocalculate segment-level correlations using61Kendall?s ?, we carried out 33,071 paired compari-sons for out-of-English and 31,051 paired compar-isons for into-English.
The resulting ?
wascalculated per system, then averaged for each con-dition (out-of-English and into-English) to obtainone out-of-English value and one into-English val-ue.First, we compared the performance ofAMBER1 with a version of AMBER1 that in-cludes the new reordering penalty v, at the systemand segment levels.
The results are shown in Table2.
The greatest impact of v is on ?out of English?
atthe segment level, but none of the results are par-ticularly impressive.AMBER1 +v ChangeInto-EnSystem0.860 0.862 0.002(+0.2%)Into-EnSegment0.178 0.180 0.002(+1.1%)Out-of-EnSystem0.637 0.637 0(0%)Out-of-EnSegment0.167 0.170 0.003(+1.8%)Table 2: Correlation with human judgment forAMBER1 vs. (AMBER1 including v).Second, we compared the performance of manu-ally tuned AMBER1 with AMBER1 whose param-eters were tuned by the simplex method.
Thetuning was run four times on the dev set, once foreach possible combination of into/out-of Englishand system/segment level.
Table 3 shows the re-sults on the test set.
This change had a greater im-pact, especially on the segment level.AMBER1 +Simplex ChangeInto-EnSystem0.860 0.862 0.002(+0.2%)Into-EnSegment0.178 0.184 0.006(+3.4%)Out-of-EnSystem0.637 0.637 0(0%)Out-of-EnSegment0.167 0.182 0.015(+9.0%)Table 3: Correlation with human judgment forAMBER1 vs. simplex-tuned AMBER1.Then, we compared the performance ofAMBER1 with AMBER1 that contains v and thathas been tuned by the simplex method.
We willdenote the new version of AMBER containingboth changes ?AMBER2?.
It will be seen fromTable 4 that AMBER2 is a major improvementover AMBER1 at the segment level.
In the case of?into English?
at the segment level, the impact ofthe two changes seems to have been synergistic:adding together the percentage improvements dueto v and simplex from Tables 2 and 3, one wouldhave expected an improvement of 4.5% for bothchanges together, but the actual improvement was6.2%.
Furthermore, there was no improvement atthe system level for ?out of English?
when eitherchange was tried separately, but there was a smallimprovement when the two changes were com-bined.AMBER1 AMBER2 ChangeInto-EnSystem0.860 0.870 0.010(+1.2%)Into-EnSegment0.178 0.189 0.011(+6.2%)Out-of-EnSystem0.637 0.642 0.005(+0.8%)Out-of-EnSegment0.167 0.184 0.017(+10.2%)Table 4: Correlation with human judgment forAMBER1 vs. AMBER2.Of course, the most important question is: doesthe new version of AMBER (AMBER2) performbetter than BLEU?
Table 5 answers this question(the version of BLEU used here was smoothedBLEU (mteval-v13a)).
There is a clear advantagefor AMBER2 over BLEU at both the system andsegment levels, for both ?into English?
and ?out ofEnglish?.BLEU AMBER2 ChangeInto-EnSystem0.773 0.870 0.097(+12.5%)Into-EnSegment0.154 0.189 0.035(+22.7%)Out-of-EnSystem0.574 0.642 0.068(+11.8%)Out-of-EnSegment0.149 0.184 0.035(+23.5%)Table 5: Correlation with human judgment forBLEU vs. AMBER2.625 ConclusionWe have made two changes to AMBER, a metricdescribed in (Chen and Kuhn, 2011).
In our exper-iments, the new version of AMBER was shown tobe an improvement on the original version in termsof correlation with human judgment.
Furthermore,it outperformed BLEU by about 12% at the systemlevel and about 23% at the segment level.A good evaluation metric is not necessarily agood tuning metric, and vice versa.
In parallel withour work on AMBER for evaluation, we have alsobeen exploring a machine translation tuning metriccalled PORT (Chen et al, 2012).
AMBER andPORT differ in many details, but they share thesame underlying philosophy: to exploit surfacesimilarities between hypothesis and referenceseven more thoroughly than BLEU does, rather thanto invoke external resources with richer linguisticknowledge.
So far, the results for PORT have beenjust as encouraging as the ones for AMBER re-ported here.ReferenceC.
Callison-Burch, P. Koehn, C. Monz, K. Peterson, M.Przybocki and O. Zaidan.
2010.
Findings of the 2010Joint Workshop on Statistical Machine Translationand Metrics for Machine Translation.
In Proceedingsof WMT.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz andJ.
Schroeder.
2008.
Further Meta-Evaluation of Ma-chine Translation.
In Proceedings of WMT.B.
Chen, R. Kuhn, and S. Larkin.
2012.
PORT:  a Preci-sion-Order-Recall MT Evaluation Metric for Tuning.Accepted for publication in Proceedings of ACL.B.
Chen and R. Kuhn.
2011.
AMBER: a ModifiedBLEU, Enhanced Ranking Metric.
In Proceedings ofthe Sixth Workshop on Statistical Machine Transla-tion, Edinburgh, Scotland.H.
Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada.2010.
Automatic Evaluation of Translation Qualityfor Distant Language Pairs.
In Proceedings ofEMNLP.J.
Nelder and R. Mead.
1965.
A simplex method forfunction minimization.
Computer Journal V. 7, pages308?313.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of ACL.W.
Press, S. Teukolsky, W. Vetterling and B. Flannery.2002.
Numerical Recipes in C++.
Cambridge Uni-versity Press, Cambridge, UK.C.
Spearman.
1904.
The proof and measurement of as-sociation between two things.
In American Journal ofPsychology, V. 15, pages 72?101.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM basedword alignment in statistical translation.
In Proceed-ings of COLING.63
