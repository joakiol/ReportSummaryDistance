Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701?709,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPMine the Easy, Classify the Hard:A Semi-Supervised Approach to Automatic Sentiment ClassificationSajib Dasgupta and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{sajib,vince}@hlt.utdallas.eduAbstractSupervised polarity classification systemsare typically domain-specific.
Buildingthese systems involves the expensive pro-cess of annotating a large amount of datafor each domain.
A potential solutionto this corpus annotation bottleneck is tobuild unsupervised polarity classificationsystems.
However, unsupervised learningof polarity is difficult, owing in part to theprevalence of sentimentally ambiguous re-views, where reviewers discuss both thepositive and negative aspects of a prod-uct.
To address this problem, we pro-pose a semi-supervised approach to senti-ment classification where we first mine theunambiguous reviews using spectral tech-niques and then exploit them to classifythe ambiguous reviews via a novel com-bination of active learning, transductivelearning, and ensemble learning.1 IntroductionSentiment analysis has recently received a lotof attention in the Natural Language Processing(NLP) community.
Polarity classification, whosegoal is to determine whether the sentiment ex-pressed in a document is ?thumbs up?
or ?thumbsdown?, is arguably one of the most popular tasksin document-level sentiment analysis.
Unliketopic-based text classification, where a high accu-racy can be achieved even for datasets with a largenumber of classes (e.g., 20 Newsgroups), polarityclassification appears to be a more difficult task.One reason topic-based text classification is easierthan polarity classification is that topic clusters aretypically well-separated from each other, result-ing from the fact that word usage differs consid-erably between two topically-different documents.On the other hand, many reviews are sentimentallyambiguous for a variety of reasons.
For instance,an author of a movie review may have negativeopinions of the actors but at the same time talkenthusiastically about how much she enjoyed theplot.
Here, the review is ambiguous because shediscussed both the positive and negative aspects ofthe movie, which is not uncommon in reviews.
Asanother example, a large portion of a movie re-view may be devoted exclusively to the plot, withthe author only briefly expressing her sentiment atthe end of the review.
In this case, the review isambiguous because the objective material in thereview, which bears no sentiment orientation, sig-nificantly outnumbers its subjective counterpart.Realizing the challenges posed by ambiguousreviews, researchers have explored a number oftechniques to improve supervised polarity classi-fiers.
For instance, Pang and Lee (2004) train anindependent subjectivity classifier to identify andremove objective sentences from a review prior topolarity classification.
Koppel and Schler (2006)use neutral reviews to help improve the classi-fication of positive and negative reviews.
Morerecently, McDonald et al (2007) have investi-gated a model for jointly performing sentence- anddocument-level sentiment analysis, allowing therelationship between the two tasks to be capturedand exploited.
However, the increased sophistica-tion of supervised polarity classifiers has also re-sulted in their increased dependence on annotateddata.
For instance, Koppel and Schler needed tomanually identify neutral reviews to train their po-larity classifier, and McDonald et al?s joint modelrequires that each sentence in a review be labeledwith polarity information.Given the difficulties of supervised polarityclassification, it is conceivable that unsupervisedpolarity classification is a very challenging task.Nevertheless, a solution to unsupervised polarityclassification is of practical significance.
One rea-son is that the vast majority of supervised polarity701classification systems are domain-specific.
Hence,when given a new domain, a large amount of an-notated data from the domain typically needs to becollected in order to train a high-performance po-larity classification system.
As Blitzer et al (2007)point out, this data collection process can be ?pro-hibitively expensive, especially since product fea-tures can change over time?.
Unfortunately, toour knowledge, unsupervised polarity classifica-tion is largely an under-investigated task in NLP.Turney?s (2002) work is perhaps one of the mostnotable examples of unsupervised polarity clas-sification.
However, while his system learns thesemantic orientation of phrases in a review in anunsupervised manner, such information is used toheuristically predict the polarity of a review.At first glance, it may seem plausible to applyan unsupervised clustering algorithm such as k-means to cluster the reviews according to their po-larity.
However, there is reason to believe that sucha clustering approach is doomed to fail: in the ab-sence of annotated data, an unsupervised learneris unable to identify which features are relevantfor polarity classification.
The situation is furthercomplicated by the prevalence of ambiguous re-views, which may contain a large amount of irrel-evant and/or contradictory information.In light of the difficulties posed by ambiguousreviews, we differentiate between ambiguous andunambiguous reviews in our classification processby addressing the task of semi-supervised polar-ity classification via a ?mine the easy, classify thehard?
approach.
Specifically, we propose a novelsystem architecture where we first automaticallyidentify and label the unambiguous (i.e., ?easy?
)reviews, then handle the ambiguous (i.e., ?hard?
)reviews using a discriminative learner to bootstrapfrom the automatically labeled unambiguous re-views and a small number of manually labeled re-views that are identified by an active learner.It is worth noting that our system differs fromexisting work on unsupervised/active learning intwo aspects.
First, while existing unsupervisedapproaches typically rely on clustering or learn-ing via a generative model, our approach distin-guishes between easy and hard instances and ex-ploits the strengths of discriminative models toclassify the hard instances.
Second, while exist-ing active learners typically start with manually la-beled seeds, our active learner relies only on seedsthat are automatically extracted from the data.
Ex-perimental results on five sentiment classificationdatasets demonstrate that our system can gener-ate high-quality labeled data from unambiguousreviews, which, together with a small number ofmanually labeled reviews selected by the activelearner, can be used to effectively classify ambigu-ous reviews in a discriminative fashion.The rest of the paper is organized as follows.Section 2 gives an overview of spectral cluster-ing, which will facilitate the presentation of ourapproach to unsupervised sentiment classificationin Section 3.
We evaluate our approach in Section4 and present our conclusions in Section 5.2 Spectral ClusteringIn this section, we give an overview of spectralclustering, which is at the core of our algorithmfor identifying ambiguous reviews.2.1 MotivationWhen given a clustering task, an important ques-tion to ask is: which clustering algorithm shouldbe used?
A popular choice is k-means.
Neverthe-less, it is well-known that k-means has the majordrawback of not being able to separate data pointsthat are not linearly separable in the given featurespace (e.g, see Dhillon et al (2004)).
Spectralclustering algorithms were developed in responseto this problem with k-means clustering.
The cen-tral idea behind spectral clustering is to (1) con-struct a low-dimensional space from the original(typically high-dimensional) space while retainingas much information about the original space aspossible, and (2) cluster the data points in this low-dimensional space.2.2 AlgorithmAlthough there are several well-known spectralclustering algorithms in the literature (e.g., Weiss(1999), Meila?
and Shi (2001), Kannan et al(2004)), we adopt the one proposed by Ng et al(2002), as it is arguably the most widely used.
Thealgorithm takes as input a similarity matrix S cre-ated by applying a user-defined similarity functionto each pair of data points.
Below are the mainsteps of the algorithm:1.
Create the diagonal matrix G whose (i,i)-th entry is the sum of the i-th row of S,and then construct the Laplacian matrix L =G?1/2SG?1/2.2.
Find the eigenvalues and eigenvectors of L.7023.
Create a new matrix from the m eigenvectorsthat correspond to the m largest eigenvalues.14.
Each data point is now rank-reduced to apoint in the m-dimensional space.
Normal-ize each point to unit length (while retainingthe sign of each value).5.
Cluster the resulting data points using k-means.In essence, each dimension in the reduced spaceis defined by exactly one eigenvector.
The rea-son why eigenvectors with large eigenvalues areretained is that they capture the largest variance inthe data.
Therefore, each of them can be thoughtof as revealing an important dimension of the data.3 Our ApproachWhile spectral clustering addresses a major draw-back of k-means clustering, it still cannot be ex-pected to accurately partition the reviews due tothe presence of ambiguous reviews.
Motivated bythis observation, rather than attempting to clusterall the reviews at the same time, we handle them indifferent stages.
As mentioned in the introduction,we employ a ?mine the easy, classify the hard?approach to polarity classification, where we (1)identify and classify the ?easy?
(i.e., unambigu-ous) reviews with the help of a spectral cluster-ing algorithm; (2) manually label a small numberof ?hard?
(i.e., ambiguous) reviews selected by anactive learner; and (3) using the reviews labeledthus far, apply a transductive learner to label theremaining (ambiguous) reviews.
In this section,we discuss each of these steps in detail.3.1 Identifying Unambiguous ReviewsWe begin by preprocessing the reviews to be clas-sified.
Specifically, we tokenize and downcaseeach review and represent it as a vector of uni-grams, using frequency as presence.
In addition,we remove from the vector punctuation, numbers,words of length one, and words that occur in asingle review only.
Finally, following the com-mon practice in the information retrieval commu-nity, we remove words with high document fre-quency, many of which are stopwords or domain-specific general-purpose words (e.g., ?movies?
inthe movie domain).
A preliminary examinationof our evaluation datasets reveals that these words1For brevity, we will refer to the eigenvector with the n-thlargest eigenvalue simply as the n-th eigenvector.typically comprise 1?2% of a vocabulary.
The de-cision of exactly how many terms to remove fromeach dataset is subjective: a large corpus typicallyrequires more removals than a small corpus.
To beconsistent, we simply sort the vocabulary by doc-ument frequency and remove the top 1.5%.Recall that in this step we use spectral clusteringto identify unambiguous reviews.
To make use ofspectral clustering, we first create a similarity ma-trix, defining the similarity between two reviewsas the dot product of their feature vectors, but fol-lowing Ng et al (2002), we set its diagonal entriesto 0.
We then perform an eigen-decomposition ofthis matrix, as described in Section 2.2.
Finally,using the resulting eigenvectors, we partition thelength-normalized reviews into two sets.As Ng et al point out, ?different authors stilldisagree on which eigenvectors to use, and how toderive clusters from them?.
To create two clusters,the most common way is to use only the secondeigenvector, as Shi and Malik (2000) proved thatthis eigenvector induces an intuitively ideal par-tition of the data ?
the partition induced by theminimum normalized cut of the similarity graph2,where the nodes are the data points and the edgeweights are the pairwise similarity values of thepoints.
Clustering in a one-dimensional space istrivial: since we have a linearization of the points,all we need to do is to determine a threshold forpartitioning the points.
A common approach is toset the threshold to zero.
In other words, all pointswhose value in the second eigenvector is positiveare classified as positive, and the remaining pointsare classified as negative.
However, we found thatthe second eigenvector does not always induce apartition of the nodes that corresponds to the min-imum normalized cut.
One possible reason is thatShi and Malik?s proof assumes the use of a Lapla-cian matrix that is different from the one used byNg et al To address this problem, we use the firstfive eigenvectors: for each eigenvector, we (1) useeach of its n elements as a threshold to indepen-dently generate n partitions, (2) compute the nor-malized cut value for each partition, and (3) findthe minimum of the n cut values.
We then selectthe eigenvector that corresponds to the smallest ofthe five minimum cut values.Next, we identify the ambiguous reviews from2Using the normalized cut (as opposed to the usual cut)ensures that the size of the two clusters are relatively bal-anced, avoiding trivial cuts where one cluster is empty andthe other is full.
See Shi and Malik (2000) for details.703the resulting partition.
To see how this is done,consider the example in Figure 1, where the goalis to produce two clusters from five data points.
( 1 1 1 0 01 1 1 0 00 0 1 1 00 0 0 1 10 0 0 1 1) (?0.6983 0.7158?0.6983 0.7158?0.9869 ?0.1616?0.6224 ?0.7827?0.6224 ?0.7827)Figure 1: Sample data and the top two eigenvec-tors of its LaplacianIn the matrix on the left, each row is the featurevector generated for Di, the i-th data point.
By in-spection, one can identify two clusters, {D1,D2}and {D4,D5}.
D3 is ambiguous, as it bears re-semblance to the points in both clusters and there-fore can be assigned to any of them.
In the ma-trix on the right, the two columns correspond tothe top two eigenvectors obtained via an eigen-decomposition of the Laplacian matrix formedfrom the five data points.
As we can see, the sec-ond eigenvector gives us a natural cluster assign-ment: all the points whose corresponding valuesin the second eigenvector are strongly positive willbe in one cluster, and the strongly negative pointswill be in another cluster.
Being ambiguous, D3 isweakly negative and will be assigned to the ?neg-ative?
cluster.
Before describing our algorithm foridentifying ambiguous data points, we make twoadditional observations regarding D3.First, if we removed D3, we could easily clus-ter the remaining (unambiguous) points, since thesimilarity graph becomes more disconnected aswe remove more ambiguous data points.
Thequestion then is: why is it important to producea good clustering of the unambiguous points?
Re-call that the goal of this step is not only to iden-tify the unambiguous reviews, but also to annotatethem as POSITIVE or NEGATIVE, so that they canserve as seeds for semi-supervised learning in alater step.
If we have a good 2-way clustering ofthe seeds, we can simply annotate each cluster (bysampling a handful of its reviews) rather than eachseed.
To reiterate, removing the ambiguous datapoints can help produce a good clustering of theirunambiguous counterparts.Second, as an ambiguous data point, D3 can inprinciple be assigned to any of the two clusters.According to the second eigenvector, it should beassigned to the ?negative?
cluster; but if feature#4 were irrelevant, it should be assigned to the?positive?
cluster.
In other words, the ability todetermine the relevance of each feature is crucialto the accurate clustering of the ambiguous datapoints.
However, in the absence of labeled data,it is not easy to assess feature relevance.
Even iflabeled data were present, the ambiguous pointsmight be better handled by a discriminative learn-ing system than a clustering algorithm, as discrim-inative learners are more sophisticated, and canhandle ambiguous feature space more effectively.Taking into account these two observations, weaim to (1) remove the ambiguous data points whileclustering their unambiguous counterparts, andthen (2) employ a discriminative learner to labelthe ambiguous points in a later step.The question is: how can we identify theambiguous data points?
To do this, we ex-ploit an important observation regarding eigen-decomposition.
In the computation of eigenvalues,each data point factors out the orthogonal projec-tions of each of the other data points with whichthey have an affinity.
Ambiguous data points re-ceive the orthogonal projections from both thepositive and negative data points, and hence theyhave near-zero values in the pivot eigenvectors.Given this observation, our algorithm uses theeight steps below to remove the ambiguous pointsin an iterative fashion and produce a clustering ofthe unambiguous points.1.
Create a similarity matrix S from the datapoints D.2.
Form the Laplacian matrix L from S.3.
Find the top five eigenvectors of L.4.
Row-normalize the five eigenvectors.5.
Pick the eigenvector e for which we get theminimum normalized cut.6.
Sort D according to e and remove ?
points inthe middle of D (i.e., the points indexed from|D|2 ?
?2 + 1 to|D|2 +?2 ).7.
If |D| = ?, goto Step 8; else goto Step 1.8.
Run 2-means on e to cluster the points in D.This algorithm can be thought of as the oppo-site of self-training.
In self-training, we iterativelytrain a classifier on the data labeled so far, use itto classify the unlabeled instances, and augmentthe labeled data with the most confidently labeledinstances.
In our algorithm, we start with an ini-tial clustering of all of the data points, and theniteratively remove the ?
most ambiguous pointsfrom the dataset and cluster the remaining points.Given this analogy, it should not be difficult to seethe advantage of removing the data points in an it-erative fashion (as opposed to removing them in a704single iteration): the clusters produced in a giveniteration are supposed to be better than those inthe previous iterations, as subsequent clusteringsare generated from less ambiguous points.
In ourexperiments, we set ?
to 50 and ?
to 500.3Finally, we label the two clusters.
To do this,we first randomly sample 10 reviews from eachcluster and manually label each of them as POS-ITIVE or NEGATIVE.
Then, we label a cluster asPOSITIVE if more than half of the 10 reviews fromthe cluster are POSITIVE; otherwise, it is labeledas NEGATIVE.
For each of our evaluation datasets,this labeling scheme always produces one POSI-TIVE cluster and one NEGATIVE cluster.
In the restof the paper, we will refer to these 500 automati-cally labeled reviews as seeds.A natural question is: can this algorithm pro-duce high-quality seeds?
To answer this question,we show in the middle column of Table 1 the label-ing accuracy of the 500 reviews produced by ouriterative algorithm for our five evaluation datasets(see Section 4.1 for details on these datasets).
Tobetter understand whether it is indeed beneficialto remove the ambiguous points in an iterativefashion, we also show the results of a version ofthis algorithm in which we remove all but the 500least ambiguous points in just one iteration (seethe rightmost column).
As we can see, for threedatasets (Movie, Kitchen, and Electronics), theaccuracy is above 80%.
For the remaining two(Book and DVD), the accuracy is not particularlygood.
One plausible reason is that the ambiguousreviews in Book and DVD are relatively tougherto identify.
Another reason can be attributed tothe failure of the chosen eigenvector to capture thesentiment dimension.
Recall that each eigenvectorcaptures an important dimension of the data, andif the eigenvector that corresponds to the minimumnormalized cut (i.e., the eigenvector that we chose)does not reveal the sentiment dimension, the re-sulting clustering (and hence the seed accuracy)will be poor.
However, even with imperfectly la-beled seeds, we will show in the next section howwe exploit these seeds to learn a better classifier.3.2 Incorporating Active LearningSpectral clustering allows us to focus on a smallnumber of dimensions that are relevant as far ascreating well-separated clusters is concerned, but3Additional experiments indicate that the accuracy of ourapproach is not sensitive to small changes to these values.Dataset Iterative Single StepMovie 89.3 86.5Kitchen 87.9 87.1Electronics 80.4 77.6Book 68.5 70.3DVD 66.3 65.4Table 1: Seed accuracies on five datasets.they are not necessarily relevant for creating po-larity clusters.
In fact, owing to the absence of la-beled data, unsupervised clustering algorithms areunable to distinguish between useful and irrelevantfeatures for polarity classification.
Nevertheless,being able to distinguish between relevant and ir-relevant information is important for polarity clas-sification, as discussed before.
Now that we havea small, high-quality seed set, we can potentiallymake better use of the available features by train-ing a discriminative classifier on the seed set andhaving it identify the relevant and irrelevant fea-tures for polarity classification.Despite the high quality of the seed set, the re-sulting classifier may not perform well when ap-plied to the remaining (unlabeled) points, as thereis no reason to believe that a classifier trainedsolely on unambiguous reviews can achieve ahigh accuracy when classifying ambiguous re-views.
We hypothesize that a high accuracy canbe achieved only if the classifier is trained on bothambiguous and unambiguous reviews.As a result, we apply active learning (Cohnet al, 1994) to identify the ambiguous reviews.Specifically, we train a discriminative classifier us-ing the support vector machine (SVM) learning al-gorithm (Joachims, 1999) on the set of unambigu-ous reviews, and then apply the resulting classifierto all the reviews in the training folds4 that are notseeds.
Since this classifier is trained solely on theunambiguous reviews, it is reasonable to assumethat the reviews whose labels the classifier is mostuncertain about (and therefore are most informa-tive to the classifier) are those that are ambigu-ous.
Following previous work on active learningfor SVMs (e.g., Campbell et al (2000), Schohnand Cohn (2000), Tong and Koller (2002)), we de-fine the uncertainty of a data point as its distancefrom the separating hyperplane.
In other words,4Following Dredze and Crammer (2008), we performcross-validation experiments on the 2000 labeled reviews ineach evaluation dataset, choosing the active learning pointsfrom the training folds.
Note that the seeds obtained in theprevious step were also acquired using the training folds only.705points that are closer to the hyperplane are moreuncertain than those that are farther away.We perform active learning for five iterations.In each iteration, we select the 10 most uncertainpoints from each side of the hyperplane for humanannotation, and then re-train a classifier on all ofthe points annotated so far.
This yields a total of100 manually labeled reviews.3.3 Applying Transductive LearningGiven that we now have a labeled set (composedof 100 manually labeled points selected by activelearning and 500 unambiguous points) as well asa larger set of points that are yet to be labeled(i.e., the remaining unlabeled points in the train-ing folds and those in the test fold), we aim totrain a better classifier by using a weakly super-vised learner to learn from both the labeled andunlabeled data.
As our weakly supervised learner,we employ a transductive SVM.To begin with, note that the automatically ac-quired 500 unambiguous data points are not per-fectly labeled (see Section 3.1).
Since these unam-biguous points significantly outnumber the manu-ally labeled points, they could undesirably domi-nate the acquisition of the hyperplane and dimin-ish the benefits that we could have obtained fromthe more informative and perfectly labeled activelearning points otherwise.
We desire a system thatcan use the active learning points effectively and atthe same time is noise-tolerant to the imperfectlylabeled unambiguous data points.
Hence, insteadof training just one SVM classifier, we aim to re-duce classification errors by training an ensembleof five classifiers, each of which uses all 100 man-ually labeled reviews and a different subset of the500 automatically labeled reviews.Specifically, we partition the 500 automaticallylabeled reviews into five equal-sized sets as fol-lows.
First, we sort the 500 reviews in ascendingorder of their corresponding values in the eigen-vector selected in the last iteration of our algorithmfor removing ambiguous points (see Section 3.1).We then put point i into set Li mod 5.
This ensuresthat each set consists of not only an equal numberof positive and negative points, but also a mix ofvery confidently labeled points and comparativelyless confidently labeled points.
Each classifier Ciwill then be trained transductively, using the 100manually labeled points and the points in Li as la-beled data, and the remaining points (including allpoints in Lj , where i 6= j) as unlabeled data.After training the ensemble, we classify eachunlabeled point as follows: we sum the (signed)confidence values assigned to it by the five ensem-ble classifiers, labeling it as POSITIVE if the sumis greater than zero (and NEGATIVE otherwise).Since the points in the test fold are included in theunlabeled data, they are all classified in this step.4 Evaluation4.1 Experimental SetupFor evaluation, we use five sentiment classifica-tion datasets, including the widely-used movie re-view dataset [MOV] (Pang et al, 2002) as well asfour datasets that contain reviews of four differ-ent types of product from Amazon [books (BOO),DVDs (DVD), electronics (ELE), and kitchen ap-pliances (KIT)] (Blitzer et al, 2007).
Each datasethas 2000 labeled reviews (1000 positives and 1000negatives).
We divide the 2000 reviews into 10equal-sized folds for cross-validation purposes,maintaining balanced class distributions in eachfold.
It is important to note that while the test foldis accessible to the transductive learner (Step 3),only the reviews in training folds (but not their la-bels) are used for the acquisition of seeds (Step 1)and the selection of active learning points (Step 2).We report averaged 10-fold cross-validation re-sults in terms of accuracy.
Following Kamvar et al(2003), we also evaluate the clusters produced byour approach against the gold-standard clusters us-ing Adjusted Rand Index (ARI).
ARI ranges from?1 to 1; better clusterings have higher ARI values.4.2 Baseline SystemsRecall that our approach uses 100 hand-labeled re-views chosen by active learning.
To ensure a faircomparison, each of our three baselines has ac-cess to 100 labeled points chosen from the train-ing folds.
Owing to the randomness involved inthe choice of labeled data, all baseline results areaveraged over ten independent runs for each fold.Semi-supervised spectral clustering.
We im-plemented Kamvar et al?s (2003) semi-supervisedspectral clustering algorithm, which incorporateslabeled data into the clustering framework in theform of must-link and cannot-link constraints.
In-stead of computing the similarity between eachpair of points, the algorithm computes the similar-ity between a point and its k most similar pointsonly.
Since its performance is highly sensitive to706Accuracy Adjusted Rand IndexSystem Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.022 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.023 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.034 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.015 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.046 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets.k, we tested values of 10, 15, .
.
., 50 for k and re-ported in row 1 of Table 2 the best results.
As wecan see, accuracy ranges from 56.2% to 67.3%,whereas ARI ranges from 0.02 to 0.12.Transductive SVM.
We employ as our secondbaseline a transductive SVM5 trained using 100points randomly sampled from the training foldsas labeled data and the remaining 1900 points asunlabeled data.
Results of this baseline are shownin row 2 of Table 3.
As we can see, accuracyranges from 57.3% to 68.7% and ARI ranges from0.02 to 0.14, which are significantly better thanthose of semi-supervised spectral learning.Active learning.
Our last baseline implementsthe active learning procedure as described in Tongand Koller (2002).
Specifically, we begin by train-ing an inductive SVM on one labeled examplefrom each class, iteratively labeling the most un-certain unlabeled point on each side of the hyper-plane and re-training the SVM until 100 points arelabeled.
Finally, we train a transductive SVM onthe 100 labeled points and the remaining 1900 un-labeled points, obtaining the results in row 3 of Ta-ble 1.
As we can see, accuracy ranges from 58%to 68.9%, whereas ARI ranges from 0.03 to 0.14.Active learning is the best of the three baselines,presumably because it has the ability to choose thelabeled data more intelligently than the other two.4.3 Our ApproachResults of our approach are shown in rows 4?6 ofTable 2.
Specifically, rows 4 and 5 show the re-sults of the SVM classifier when it is trained onthe labeled data obtained after the first step (unsu-pervised extraction of unambiguous reviews) andthe second step (active learning), respectively.
Af-ter the first step, our approach can already achieve5All the SVM classifiers in this paper are trained usingthe SVMlight package (Joachims, 1999).
All SVM-relatedlearning parameters are set to their default values, except intransductive learning, where we set p (the fraction of unla-beled examples to be classified as positive) to 0.5 so that thesystem does not have any bias towards any class.comparable results to the best baseline.
Per-formance increases substantially after the secondstep, indicating the benefits of active learning.Row 6 shows the results of transductive learn-ing with ensemble.
Comparing rows 5 and 6,we see that performance rises by 0.7%-2.9% forall five datasets after ?ensembled?
transduction.This could be attributed to (1) the unlabeled data,which may have provided the transductive learnerwith useful information that are not accessible tothe other learners, and (2) the ensemble, which ismore noise-tolerant to the imperfect seeds.4.4 Additional ExperimentsTo gain insight into how the design decisions wemade in our approach impact performance, weconducted the following additional experiments.Importance of seeds.
Table 1 showed that forall but one dataset, the seeds obtained throughmultiple iterations are more accurate than thoseobtained in a single iteration.
To envisage the im-portance of seeds, we conducted an experimentwhere we repeated our approach using the seedslearned in a single iteration.
Results are shown inthe first row of Table 3.
In comparison to row 6 ofTable 2, we can see that results are indeed betterwhen we bootstrap from higher-quality seeds.To further understand the role of seeds, we ex-perimented with a version of our approach thatbootstraps from no seeds.
Specifically, we usedthe 500 seeds to guide the selection of active learn-ing points, but trained a transductive SVM usingonly the active learning points as labeled data (andthe rest as unlabeled data).
As can be seen in row2 of Table 3, the results are poor, suggesting thatour approach yields better performance than thebaselines not only because of the way the activelearning points were chosen, but also because ofcontributions from the imperfectly labeled seeds.We also experimented with training a transduc-tive SVM using only the 100 least ambiguousseeds (i.e., the points with the largest unsigned707Accuracy Adjusted Rand IndexSystem Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.052 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.013 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.074 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.045 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.036 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.227 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets.second eigenvector values) in combination withthe active learning points as labeled data (and therest as unlabeled data).
Note that the accuracy ofthese 100 least ambiguous seeds is 4?5% higherthan that of the 500 least ambiguous seeds shownin Table 1.
Results are shown in row 3 of Table 3.As we can see, using only 100 seeds turns out to beless beneficial than using all of them via an ensem-ble.
One reason is that since these 100 seeds arethe most unambiguous, they may also be the leastinformative as far as learning is concerned.
Re-member that SVM uses only the support vectors toacquire the hyperplane, and since an unambiguousseed is likely to be far away from the hyperplane,it is less likely to be a support vector.Role of ensemble learning To get a better ideaof the role of the ensemble in the transductivelearning step, we used all 500 seeds in combina-tion with the 100 active learning points to train asingle transductive SVM.
Results of this experi-ment (shown in row 4 of Table 3) are worse thanthose in row 6 of Table 2, meaning that the en-semble has contributed positively to performance.This should not be surprising: as noted before,since the seeds are not perfectly labeled, using allof them without an ensemble might overwhelm themore informative active learning points.Passive learning.
To better understand the roleof active learning in our approach, we replaced itwith passive learning, where we randomly picked100 data points from the training folds and usedthem as labeled data.
Results, shown in row 5 ofTable 3, are averaged over ten independent runsfor each fold.
In comparison to row 6 of Table 2,we see that employing points chosen by an activelearner yields significantly better results than em-ploying randomly chosen points, which suggeststhat the way the points are chosen is important.Using more active learning points.
An interest-ing question is: how much improvement can weobtain if we employ more active learning points?In row 6 of Table 3, we show the results when theexperiment in row 6 of Table 2 was repeated using500 active learning points.
Perhaps not surpris-ingly, the 400 additional labeled points yield a 4?11% increase in accuracy.
For further comparison,we trained a fully supervised SVM classifier usingall of the training data.
Results are shown in row7 of Table 3.
As we can see, employing only 500active learning points enables us to almost reachfully-supervised performance for three datasets.5 ConclusionsWe have proposed a novel semi-supervised ap-proach to polarity classification.
Our key ideais to distinguish between unambiguous, easy-to-mine reviews and ambiguous, hard-to-classify re-views.
Specifically, given a set of reviews, weapplied (1) an unsupervised algorithm to identifyand classify those that are unambiguous, (2) anactive learner that is trained solely on automati-cally labeled unambiguous reviews to identify asmall number of prototypical ambiguous reviewsfor manual labeling, and (3) an ensembled trans-ductive learner to train a sophisticated classifieron the reviews labeled so far to handle the am-biguous reviews.
Experimental results on five sen-timent datasets demonstrate that our ?mine theeasy, classify the hard?
approach, which only re-quires manual labeling of a small number of am-biguous reviews, can be employed to train a high-performance polarity classification system.We plan to extend our approach by exploringtwo of its appealing features.
First, none of thesteps in our approach is designed specifically forsentiment classification.
This makes it applica-ble to other text classification tasks.
Second, ourapproach is easily extensible.
Since the semi-supervised learner is discriminative, our approachcan adopt a richer representation that makes useof more sophisticated features such as bigrams ormanually labeled sentiment-oriented words.708AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on an earlier draft of the pa-per.
This work was supported in part by NSFGrant IIS-0812261.ReferencesJohn Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the ACL, pages 440?447.Colin Campbell, Nello Cristianini, , and Alex J. Smola.2000.
Query learning with large margin classifiers.In Proceedings of ICML, pages 111?118.David Cohn, Les Atlas, and Richard Ladner.
1994.Improving generalization with active learning.
Ma-chine Learning, 15(2):201?221.Inderjit Dhillon, Yuqiang Guan, and Brian Kulis.
2004.Kernel k-means, spectral clustering and normalizedcuts.
In Proceedings of KDD, pages 551?556.Mark Dredze and Koby Crammer.
2008.
Active learn-ing with confidence.
In Proceedings of ACL-08:HLTShort Papers (Companion Volume), pages 233?236.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scholkopf andAlexander Smola, editors, Advances in Kernel Meth-ods - Support Vector Learning, pages 44?56.
MITPress.Sepandar Kamvar, Dan Klein, and Chris Manning.2003.
Spectral learning.
In Proceedings of IJCAI,pages 561?566.Ravi Kannan, Santosh Vempala, and Adrian Vetta.2004.
On clusterings: Good, bad and spectral.
Jour-nal of the ACM, 51(3):497?515.Moshe Koppel and Jonathan Schler.
2006.
The im-portance of neutral examples for learning sentiment.Computational Intelligence, 22(2):100?109.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Proceedings ofthe ACL, pages 432?439.Marina Meila?
and Jianbo Shi.
2001.
A random walksview of spectral segmentation.
In Proceedings ofAISTATS.Andrew Ng, Michael Jordan, and Yair Weiss.
2002.On spectral clustering: Analysis and an algorithm.In Advances in NIPS 14.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe ACL, pages 271?278.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86.Greg Schohn and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProceedings of ICML, pages 839?846.Jianbo Shi and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 22(8):888?905.Simon Tong and Daphne Koller.
2002.
Support vec-tor machine active learning with applications to textclassification.
Journal of Machine Learning Re-search, 2:45?66.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the ACL, pages417?424.Yair Weiss.
1999.
Segmentation using eigenvectors: Aunifying view.
In Proceedings of ICCV, pages 975?982.709
