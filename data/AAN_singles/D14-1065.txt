Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 600?609,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSemantic-Based Multilingual Document Clustering via Tensor ModelingSalvatore Romeo, Andrea TagarelliDIMES, University of CalabriaArcavacata di Rende, Italysromeo@dimes.unical.ittagarelli@dimes.unical.itDino IencoIRSTEA, UMR TETISMontpellier, FranceLIRMMMontpellier, Francedino.ienco@irstea.frAbstractA major challenge in document clustering re-search arises from the growing amount of textdata written in different languages.
Previ-ous approaches depend on language-specificsolutions (e.g., bilingual dictionaries, sequen-tial machine translation) to evaluate documentsimilarities, and the required transformationsmay alter the original document semantics.
Tocope with this issue we propose a new docu-ment clustering approach for multilingual cor-pora that (i) exploits a large-scale multilingualknowledge base, (ii) takes advantage of themulti-topic nature of the text documents, and(iii) employs a tensor-based model to deal withhigh dimensionality and sparseness.
Resultshave shown the significance of our approachand its better performance w.r.t.
classic docu-ment clustering approaches, in both a balancedand an unbalanced corpus evaluation.1 IntroductionDocument clustering research was initially focused onthe development of general purpose strategies to groupunstructured text data.
Recent studies have started de-veloping new methodologies and algorithms that takeinto account both linguistic and topical characteristics,where the former include the size of the text and thetype of language, and the latter focus on the commu-nicative function and targets of the documents.A major challenge in document clustering researcharises from the growing amount of text data that arewritten in different languages, also due to the increasedpopularity of a number of tools for collaboratively edit-ing through contributors across the world.
Multilingualdocument clustering (MDC) aims to detect clusters in acollection of texts written in different languages.
Thiscan aid a variety of applications in cross-lingual infor-mation retrieval, including statistical machine transla-tion and corpora alignment.Existing approaches to MDC can be divided in twobroad categories, depending on whether a parallel cor-pus rather than a comparable corpus is used (Kumar etal., 2011c).
A parallel corpus is typically comprisedof documents with their related translations (Kim etal., 2010).
These translations are usually obtainedthrough machine translation techniques based on a se-lected anchor language.
Conversely, a comparable cor-pus is a collection of multilingual documents writtenover the same set of classes (Ni et al., 2011; Yo-gatama and Tanaka-Ishii, 2009) without any restric-tion about translation or perfect correspondence be-tween documents.
To mine this kind of corpus, externalknowledge is employed to map concepts or terms froma language to another (Kumar et al., 2011c; Kumaret al., 2011a), which enables the extraction of cross-lingual document correlations.
In this case, a majorissue lies in the definition of a cross-lingual similaritymeasure that can fit the extracted cross-lingual correla-tions.
Also, from a semi-supervised perspective, otherworks attempt to define must-link constraints to de-tect cross-lingual clusters (Yogatama and Tanaka-Ishii,2009).
This implies that, for each different dataset, theset of constraints needs to be redefined; in general, thefinal results can be negatively affected by the quantityand the quality of involved constraints (Davidson et al.,2006).To the best of our knowledge, existing clustering ap-proaches for comparable corpora are customized for asmall set (two or three) of languages (Montalvo et al.,2007).
Most of them are not generalizable to manylanguages as they employ bilingual dictionaries andthe translation is performed sequentially consideringonly pairs of languages.
Therefore, the order in whichthis process is done can seriously impact the results.Another common drawback concerns the way mostof the recent approaches perform their analysis: thevarious languages are analyzed independently of eachother (possibly by exploiting external knowledge likeWikipedia to enrich documents (Kumar et al., 2011c;Kumar et al., 2011a)), and then the language-specificresults are merged.
This two-step analysis howevermay fail in profitably exploiting cross-language infor-mation from the multilingual corpus.Contributions.
We address the problem of MDCby proposing a framework that features three key ele-ments, namely: (1) to model documents over a unifiedconceptual space, with the support of a large-scale mul-tilingual knowledge base; (2) to decompose the mul-tilingual documents into topically-cohesive segments;and (3) to describe the multilingual corpus under amulti-dimensional data structure.600The first key element prevents loss of informationdue to the translation of documents from different lan-guages to a target one.
It enables a conceptual represen-tation of the documents in a language-independent waypreserving the content semantics.
BabelNet (Navigliand Ponzetto, 2012a) is used as multilingual knowl-edge base.
To the extent of our knowledge, this is thefirst work in MDC that exploits BabelNet.The second key element, document segmentation,enables us to simplify the document representationaccording to their multi-topic nature.
Previous re-search has demonstrated that a segment-based ap-proach can significantly improve document clusteringperformance (Tagarelli and Karypis, 2013).
More-over, the conceptual representation of the documentsegments enables the grouping of linguistically dif-ferent (portions of) documents into topically coherentclusters.The latter aspect is leveraged by the third key ele-ment of our proposal, which relies on a tensor-basedmodel (Kolda and Bader, 2009) to effectively handlethe high dimensionality and sparseness in text.
Ten-sors are considered as a multi-linear generalization ofmatrix factorizations, since all dimensions or modesare retained thanks to multi-linear structures which canproduce meaningful components.
The applicability oftensor analysis has recently attracted growing atten-tion in information retrieval and data mining, includingdocument clustering (e.g., (Liu et al., 2011; Romeoet al., 2013)) and cross-lingual information retrieval(e.g., (Chew et al., 2007)).The rest of the paper is organized as follows.
Sec-tion 2 provides an overview of BabelNet and basic no-tions on tensors.
We describe our proposal in Section 3.Data and experimental settings are described in Sec-tion 4, while results are presented in Section 5.
Wesummarize our main findings in Section 6, finally Sec-tion 7 concludes the paper.2 Background2.1 BabelNetBabelNet (Navigli and Ponzetto, 2012a) is a multilin-gual semantic network obtained by linking Wikipediawith WordNet, that is, the largest multilingual Web en-cyclopedia and the most popular computational lex-icon.
The linking of the two knowledge bases wasperformed through an automatic mapping of WordNetsynsets and Wikipages, harvesting multilingual lexi-calization of the available concepts through human-generated translations provided by the Wikipedia inter-language links or through machine translation tech-niques.
The result is an encyclopedic dictionary con-taining concepts and named entities lexicalized in 50different languages.Multilingual knowledge in BabelNet is representedas a labeled directed graph in which nodes are conceptsor named entities and edges connect pairs of nodesthrough a semantic relation.
Each edge is labeled with arelation type (is-a, part-of, etc.
), while each node corre-sponds to a BabelNet synset, i.e., a set of lexicalizationsof a concept in different languages.BabelNet can be accessed and easily integrated intoapplications by means of a Java API provided by thetoolkit described in (Navigli and Ponzetto, 2012b).The toolkit also provides functionalities for graph-based WSD in a multilingual context.
Given an in-put set of words, a semantic graph is built by lookingfor related synset paths and by merging all them in aunique graph.
Once the semantic graph is built, thegraph nodes can be scored with a variety of algorithms.Finally, this graph with scored nodes is used to rank theinput word senses by a graph-based approach.2.2 Tensor model representationA tensor is a multi-dimensional array T ?<I1?I2?????IM.
The number of dimensions M , alsoknown as ways or modes, is called order of the ten-sor, so that a tensor with order M is also said a M -way or M -order tensor.
A higher-order tensor (i.e., atensor with order three or higher) is denoted by bold-face calligraphic letters, e.g., T ; a matrix (2-way ten-sor) is denoted by boldface capital letters, e.g., U;a vector (1-way tensor) is denoted by boldface low-ercase letters, e.g., v. The generic entry (i1, i2, i3)of a third-order tensor T is denoted by ti1i2i3, withi1?
[1..I1], i2?
[1..I2], i3?
[1..I3].A one-dimensional fragment of tensor, defined byvarying one index and keeping the others fixed, is a1-way tensor called fiber.
A third-order tensor hascolumn, row and tube fibers.
Analogously, a two-dimensional fragment of tensor, defined by varying twoindices and keeping the rest fixed, is a 2-way tensorcalled slice.
A third-order tensor has horizontal, lateraland frontal slices.The mode-m matricization of a tensor T , denotedby T(m), is obtained by arranging the mode-m fibersas columns of a matrix.
A third-order tensor T ?<I1?I2?I3is all-orthogonal if?i1i2ti1i2?ti1i2?=?i1i3ti1?i3ti1?i3=?i2i3t?i2i3t?i2i3= 0 when-ever ?
6= ?.
The mode-m product of a tensor T ?<I1?I2????
?IMwith a matrix U ?
<J?Im, denoted byT ?mU, is a tensor of dimension I1?
.
.
.
Im?1?J ?
Im+1?
?
?
?
?
IMand can be expressed in termsof matrix product as Y = T ?mU, whose mode-mmatricization is Y(m)= UT(m).3 Our Proposal3.1 Multilingual Document ClusteringframeworkWe are given a collection of multilingual documentsD =?Ll=1Dl, where each Dl= {dli}Nli=1represents asubset of documents written in the same language, withN =?Ll=1Nl= |D|.
Our framework can be appliedto any multilingual document collection regardless ofthe languages, and can deal with balanced as well as601Algorithm 1 SeMDocT (Segment-based MultiLingualDocument Clustering via Tensor Modeling)Input: A collection of multilingual documents D, the num-ber k of segment clusters, the number of tensorial com-ponents r.Output: A document clustering solution C over D.1: Apply a text segmentation algorithm over each of thedocuments in D to produce a collection of document seg-ments S. /* Section 3.1.1 */2: Represent S in either a bag-of-words (BoW) or a bag-of-synsets (BoS) space.
/* Section 3.1.2 */3: Apply any document clustering algorithm on S to obtaina segment clustering CS= {Csi}ki=1.
/* Section 3.1.2 */4: Represent CSin either a bag-of-words (BoW) or a bag-of-synsets (BoS) space.
/* Section 3.1.3 */5: Model S as a third-order tensor T ?
<I1?I2?I3 , withI1= |D|, I2= |F|, and I3= k. /* Section 3.1.4 */6: Decompose the tensor using a Truncated HOSVD./* Section 3.1.4 */7: Apply a document clustering algorithm on the mode-1factor matrix to obtain the final clusters of documentsC = {Ci}Ki=1.
/* Section 3.1.5 */unbalanced corpora.
Therefore, no restriction is givenon both the number L of languages and the distributionof documents over the languages (i.e., NiQ Nj, withi, j = 1..L, i 6= j).Real-world documents often span multiple topics.We assume that each document in D is relatively longto be comprised of smaller textual units, or segments,each of which can be considered cohesive w.r.t.
a topicover the document.
This represents a key aspect inour framework as it enables the use of a tensor modelto conveniently address the multi-faceted nature of thedocuments.Our overall framework, named SeMDocT (Segment-based MultiLingual Document Clustering via TensorModeling), is shown in Algorithm 1.
In the following,we shall describe in details each of the steps involvedin SeMDocT.3.1.1 Computing within-document segmentsText segmentation is concerned with the fragmentationof an input text into multi-paragraph, contiguous anddisjoint blocks that represent subtopics.
Regardless ofthe presence of logical structure clues in the document,linguistic criteria (Beeferman et al., 1999) and statis-tical similarity measures (Hearst, 1997; Choi et al.,2001; Cristianini et al., 2001) have been mainly used todetect subtopic boundaries between segments.
A com-mon assumption is that terms that discuss a subtopictend to co-occur locally, and a switch to a new subtopicis detected by the ending of co-occurrence of a givenset of terms and the beginning of the co-occurrence ofanother set of terms.Our SeMDocT does not depend on a specific algo-rithmic choice to perform text segmentation; in thiswork, we refer to the classic TextTiling (Hearst, 1997),which is the exemplary similarity-block-based methodfor text segmentation.3.1.2 Inducing document segment clustersThe result of the previous step is a collection of doc-ument segments, henceforth denoted as S. Each seg-ment in S is represented as a vector of feature oc-currences, where a feature can be either lexical or se-mantic.
This corresponds to two alternative represen-tation models: the standard bag-of-words (henceforthBoW), whereby features correspond to lemmatized,non-stopword terms, and the obtained feature spaceresults from the union of the vocabularies of the dif-ferent languages; and bag-of-synsets (henceforth BoS),whereby features correspond to BabelNet synsets.
Weshall devote Section 3.2 to a detailed description of ourproposed BoS representation.The segment collection S is given in input to a doc-ument clustering algorithm to produce a clustering ofthe segments CS= {Csi}ki=1.
The obtained clustersof segments can be disjoint or overlapping.
Again, ourSeMDocT is parametric to the clustering algorithm aswell; here, we resort to a state-of-the-art clustering al-gorithm, namely Bisecting K-Means (Steinbach et al.,2000), which is widely known to produce high-quality(hard) clustering solutions in high-dimensional, largedatasets (Zhao and Karypis, 2004).
Note howeverthat it requires as input the number of clusters.
Tocope with this issue, we adopt the method describedin (Salvador and Chan, 2004), which explores how thewithin-cluster cohesion changes by varying the numberof clusters.
The number of clusters for which the slopeof the plot changes drastically is chosen as a suitablevalue for the clustering algorithm.3.1.3 Segment-cluster based representationUpon the segment clustering, each document is repre-sented by its segments assigned to possibly multiplesegment clusters.
Therefore, we derive a document-feature matrix for each of the k segment clusters.
Thefeatures correspond either to the BoW or BoS model,according to the choice made for the segment represen-tation.Let us denote with F the feature space for all seg-ments in S. Given a segment cluster Cs, the cor-responding document-feature matrix is constructed asfollows.
The representation of each document d ?
Dw.r.t.
Csis a vector of length |F| that results from thesum of the feature vectors of the d?s segments belong-ing to Cs.
Moreover, in order to weight the appearanceof a document in a cluster based on its segment-basedportion covered in the cluster, the document vector of dw.r.t.
Csis finally obtained by multiplying the sum ofthe segment-vectors by a scalar representing the portionof d?s features that appear in the segments belonging toCs.
The document-feature matrix of Csresulting fromthe previous step is finally normalized by column.3.1.4 Tensor model and decompositionThe document-feature matrices corresponding to the ksegment-clusters are used to form a third-order tensor.602Tfeatures(terms,synsets)documentsFigure 1: The third-order tensor model for the repre-sentation of a multilingual document collection basedon segment clusters.Our third-order tensor model is built by arrangingas frontal slices the k segment-cluster matrices.
Theresulting tensor will be of the form T ?
<I1?I2?I3,with I1= |D|, I2= |F|, and I3= k. The proposedtensor model is sketched in Fig.
1.The resulting tensor is decomposed through a Trun-cated Higher Order SVD (T-HOSVD) (Lathauwer etal., 2000) in order to obtain a low-dimensional rep-resentation of the segment-cluster-based representationof the document collection.
The T-HOSVD can be con-sidered as an extension of the Truncated Singular ValueDecomposition (T-SVD) to the case of three or moredimensions.
For a third-order tensor T ?
<I1?I2?I3the T-HOSVD is expressed asT ?
X ?1U(1)?2U(2)?3U(3)where U(m)= [u(m)1u(m)2. .
.u(m)rm] ?
<Im?rm(m =1, 2, 3) are orthogonal matrices, rmIm, and the coretensorX ?
<r1?r2?r3is an all-orthogonal and orderedtensor.
T-HOSVD can be computed in two steps:1.
For m ?
{1, 2, 3}, compute the unfolded ma-trices T(m)from T and related standard SVD:T(m)= U(m)S(m)V(m).
The orthogonal matrixU(m)contains the leading left singular vectors ofT(m).2.
Compute the core tensor X using the inversionformula: X = T ?1U(1)T?2U(2)T?3U(3)T.Note that, since T-HOSVD is computed by means of3 standard matrix T-SVDs, its computational cost canbe reduced by using fast and efficient SVD algorithms.Moreover, the ability of T-HOSVD in effectively cap-turing the variation in each of the modes independentlyfrom the other ones, is particularly important to alle-viate the problem of concentration of distances, thusmaking T-HOSVD well-suited to clustering purposes.In this work, in order to obtain a final clustering so-lution of the documents, we will consider the mode-1factor matrix U(1)of the T-HOSVD.3.1.5 Document clusteringThe mode-1 factor matrix is provided in input to a clus-tering method to obtain a final organization of the doc-uments into K clusters, i.e., C = {Ci}Ki=1.
Note thatthere is no principled relation between the numberK offinal document clusters and k. However, K is expectedto reflect the number of topics of interest for the docu-ment collection.
Also, possibly but not necessarily, thesame clustering algorithm used for the segment cluster-ing step (i.e., Bisecting K-Means) can be employed forthis step.3.2 Bag-of-synset representationIn the BoS model, our objective is to represent the doc-ument segments in a conceptual feature space insteadof the traditional term space.
Since we deal with mul-tilingual documents, this task clearly relies on the mul-tilingual lexical knowledge base functionalities of Ba-belNet.
Conceptual features will hence correspond toBabelNet synsets.The segment collection S is subject to a two-stepprocessing phase.
In the first step, each segment isbroken down into a set of lemmatized and POS-taggedsentences, in which each word is replaced with re-lated lemma and associated POS-tag.
Let us denotewith ?w,POS(w)?
a lemma and associated POS-tagoccurring in any sentence sen of the segment.
In thesecond step, a WSD method is applied to each pair?w,POS(w)?
to detect the most appropriate Babel-Net synset ?wfor ?w,POS(w)?
contextually to sen.The WSD algorithm is carried out in such a way thatall words from all languages are disambiguated overthe same concept inventory, producing a language-independent feature space for the whole multilingualcorpus.
Each segment is finally modeled as a |BS|-dimensional vector of BabelNet synset frequencies, be-ing BS the set of retrieved BabelNet synsets.As previously discussed in Section 2.1, BabelNetprovides WSD algorithms for multilingual corpora.More specifically, the authors in (Navigli and Ponzetto,2012b) suggest to use the Degree algorithm (Navigliand Lapata, 2010), as it showed to yield highly com-petitive performance in a multilingual context as well.Note that the Degree algorithm, given a semantic graphfor the input context, simply selects the sense of the tar-get word with the highest vertex degree.
Clearly, othergraph-based methods for (unsupervised) WSD, partic-ularly PageRank-style methods (e.g., (Mihalcea et al.,2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsat-saronis et al., 2010)), can be plugged in to address themultilingual WSD task based on BabelNet.
An investi-gation of the performance of existing WSD algorithmsfor a multilingual context is however out of the scopeof this paper.4 Evaluation MethodologyIn order to evaluate our proposal we need a multilin-gual comparable document collection with annotated603RCV2 Topics English French ItalianBalanced CorpusC15 - PERFORMANCE 850 850 850C18 - OWNERSHIP CHANGES 850 850 850E11 - ECONOMIC PERFORMANCE 850 850 850E12 - MONETARY/ECONOMIC 850 850 850M11 - EQUITY MARKETS 850 850 850M13 - MONEY MARKETS 850 850 850Total 5 100 5 100 5 100Unbalanced CorpusC15 - PERFORMANCE 850 850 0C18 - OWNERSHIP CHANGES 850 850 0E11 - ECONOMIC PERFORMANCE 0 850 850E12 - MONETARY/ECONOMIC 850 0 850M11 - EQUITY MARKETS 0 850 850M13 - MONEY MARKETS 850 0 850Total 3 400 3 400 3 400Table 1: Number of documents for each topic and lan-guage.Statistics Balanced Corpus Unbalanced Corpus# of docs 15 300 10 200# of terms 58 825 44 535# of synsets 16 395 14 339BoW Density 1.5E-3 2.0E-3BoS Density 2.6E-3 3.1E-3Table 2: Main characteristics of the corpora.topics.
For this reason, we used Reuters Corpus Volume2 (RCV2), a multilingual corpus containing news arti-cles in thirteen language.1In the following, we presentthe corpus characteristics and competing methods usedin our analysis.4.1 Data preparationWe consider a subset of the RCV2 corpus correspond-ing to three languages: English, French and Italian.It covers six different topics, i.e., different labels ofthe RCV2 TOPICS field.
Topics are chosen accord-ing with their coverage in the different languages.The language-specific documents were lemmatized andPOS-tagged through the Freeling library (Padr?o andStanilovsky, 2012) in order to obtain a suitable rep-resentation for the WSD process.To assess the robustness of our proposal, we de-sign two different scenarios.
The first (Balanced Cor-pus) is characterized by a completely balanced dataset.Each language covers all topics and for each pair lan-guage/topic the same number of documents is selected.The second scenario corresponds to an UnbalancedCorpus.
Starting from the balanced corpus, we re-moved for each topic all the documents belonging toone language.
In this way, we obtained a corpus inwhich each topic is covered by only two of the threelanguages.Main characteristics of both evaluation corpora arereported in Table 1 and Table 2.
In the latter table,we report the number of documents, number of terms,number of synsets and the dataset density for bothrepresentations.
To quantify the density of each cor-1http://trec.nist.gov/data/reuters/reuters.htmlRCV2 Topics English French ItalianC15 - PERFORMANCE 3.41 3.67 3.27C18 - OWNERSHIP CHANGES 3.20 3.32 2.40E11 - ECONOMIC PERFORMANCE 4.89 3.17 2.07E12 - MONETARY/ECONOMIC 5.22 3.69 2.05M11 - EQUITY MARKETS 4.29 2.94 2.15M13 - MONEY MARKETS 3.31 3.12 2.10Table 3: Average number of document segments, foreach topic and language.English French ItalianRCV2 avg BoS avg BoW avg BoS avg BoW avg BoS avg BoWTopics seg.
leng.
seg.
leng.
seg.
leng.
seg.
leng.
seg.
leng.
seg.
leng.C15 21.76 36.32 11.54 34.92 10.58 37.75C18 20.94 36.87 10.94 35.62 11.24 41.20E11 22.90 37.24 11.47 34.73 11.96 38.60E12 22.70 37.70 11.50 37.44 12.59 43.63M11 22.04 36.83 10.91 32.76 11.57 42.39M13 22.22 36.97 11.34 34.75 11.72 39.36Table 4: Average length of document segment in theBoW and BoS spaces, for each topic and language.pus/representation combination, we counted the non-zero entries of the induced document-synset matrix (al-ternatively, document-term matrix) and we divided thisvalue by the size of such matrix.
This number pro-vides an estimate about the density/sparseness of eachdataset.
Lower values indicate more sparse data.
Wecan note that BoS model yields more dense datasets forboth Balanced Corpus and Unbalanced Corpus.As our proposal explicitly models document seg-ments, we also report statistics, considering both topicsand languages, related to the average number of seg-ments per document (Table 3), and the average lengthof segments per document (Table 4).
The latter statisticis computed separately for BoW and BoS representa-tions.
We made this distinction because a term cannothave a mapping to a synset, or it can be mapped to morethan one synset in the BoS space during the WSD pro-cess (Section 3.2).Looking at the average number of segments per doc-ument in Table 3, it can be noted that English docu-ments contain, for all topics, a larger number of seg-ments.
This means that English documents are gener-ally richer than the ones in the other languages.
Ital-ian language corresponds to the smallest documents,each of them containing between 2 and 3.2 segmentson average.
A sharper difference appears in the MONE-TARY/ECONOMIC topic for which English documentscontain 5.2 segments, while the Italian ones are com-posed, on average, by only 2 segments.Table 4 shows the average length of segments perdocument for both space representations.
Generally,segments in the BoS representation are smaller than thecorresponding segments in the BoW space.
More in de-tail, if we consider the ratio between the segment lengthin BoS and the one in BoW, this ratio is around 2/3 forthe English language, while for both French and Ital-ian it varies between 1/4 and 1/3.
This disequilibriumis induced by the multilingual concept coverage of Ba-belNet, as stated by its authors (Navigli and Ponzetto,6042012a), (Navigli and Ponzetto, 2012b).
In particular,the WSD process tightly depends from the concept cov-erage supplied from the language-specific knowledgebase.4.2 Competing methods and settingsWe compare our SeMDocT with two standard ap-proaches, namely Bisecting K-Means (Steinbach et al.,2000), and Latent Semantic Analysis (LSA)-based doc-ument clustering (for short, LSA).
Given a number Kof desired clusters, Bisecting K-Means produces a K-way clustering solution by performing a sequence ofK-1 repeated bisections based on standard K-Meansalgorithm.
This process continues until the number Kof clusters is found.
LSA performs a decomposition ofthe document collection matrix through Singular ValueDecomposition in order to extract a more concise anddescriptive representation of the documents.
After thisstep, Bisecting K-Means is applied over the new docu-ment space to get the final document clustering.All the three methods, SeMDocT, Bisecting K-Means and LSA are coupled with either BoS or BoWrepresentation models.
The comparison between BoSand BoW representations allows us to evaluate thepresumed benefits that can be derived by exploitingsynsets instead of terms for the multilingual documentclustering task.Both SeMDocT and LSA require the number of com-ponents as input; as concerns specifically SeMDocT,we varied r1(cf.
Section 3.1.4) from 2 to 30, with in-crements of 2.
To determine the number of segmentclusters k, we employed an automatic way as discussedin Section 3.1.2.
By varying k from 2 to 40, for Bal-anced Corpus and Unbalanced Corpus, respectively,the values of k obtained were 22 and 23 under BoS,and 25 and 11 under BoW.As concerns the step of text segmentation, TextTilingrequires the setting of some interdependent parameters,particularly the size of the text unit to be compared andthe number of words in a token sequence.
We used thesetting suggested in (Hearst, 1997) and also confirmedin (Tagarelli and Karypis, 2013), i.e., 10 for the textunit size and 20 for the token-sequence size.4.3 Assessment criteriaPerformance of the different methods are evaluated us-ing two standard clustering validation criteria, namelyF-Measure and Rand Index.Given a document collection D, let ?
= {?j}Hj=1and C = {Ci}Ki=1denote a reference classificationand a clustering solution for D, respectively.
The lo-cal precision and the local recall of a cluster Ciw.r.t.a class ?jare defined as Pij= |Ci?
?j|/|Ci| andRij= |Ci?
?j|/|?j|, respectively.
F-Measure (FM) iscomputed as follows (Steinbach et al., 2000):F =H?j=1|?j||D|maxi=1...K{Fij}where Fij= 2PijRij/(Pij+Rij).Rand Index (RI) (Rand, 1971) measures the percent-age of decisions that are correct, penalizing false pos-itive and false negative decisions during clustering.
Ittakes into account the following quantities: TP, i.e., thenumber of pairs of documents that are in the same clus-ter in C and in the same class in ?
; TN, i.e., the numberof pairs of documents that are in different clusters inC and in different classes in ?
; FN, i.e., the number ofpairs of documents that are in different clusters in C andin the same class in ?
; and FP, i.e., the number of pairsof documents that are in the same cluster in C and indifferent classes in ?.
Rand Index is hence defined as:RI =TP + TNTP + TN + FP + FNNote that for each method, results were averagedover 30 runs and the number of final document clustersK was set equal to the number of topics in the docu-ment collection (i.e., 6).5 ResultsWe present here our main experimental results.
We firstprovide a comparative evaluation of our SeMDocT withthe competing methods, on both balanced and unbal-anced corpus evaluation cases.
Then we provide a perlanguage analysis focusing on SeMDocT.5.1 Evaluation with competing methodsEvaluation on balanced corpus.
Figure 2 shows FMand RI results obtained by the various methods coupledwith the two document representations on the BalancedCorpus.
Several remarks stand out.
First, the BoSspace positively influences the performance of all theemployed approaches.
This is particularly evident forBisecting K-Means and LSA that clearly benefit fromthis kind of representation.
The former almost doublesits performance in terms of FM and significantly im-proves its result w.r.t.
RI.
LSA shows improvementsin both cases.
SeMDocT-BoS generally outperformsall the competitors for both FM and RI when the num-ber of components is greater than 16.
Note that, underthe BoW model, SeMDocT-BoW still outperforms theother methods.Evaluation on unbalanced corpus.
Figure 3 reportsresults for the Unbalanced Corpus.
Also in this eval-uation, the best performances for all the methods arereached using the BoS representation.
SeMDocT-BoSshows similar behavior according to the two measures.It always outperforms the competitors considering anumber of components greater than or equal to 12.More precisely, SeMDocT-BoW obtains a gain of 0.047and 0.103 in terms of FM and 0.006 and 0.058 interms of RI, w.r.t.
LSA-BoW and Bisecting K-Means-BoW, respectively.
Similarly, SeMDocT-BoS obtainsimprovements of 0.05 in terms of FM w.r.t.
both BoS605ll lll l l l l l l l l l l2 6 10 14 18 22 26 300.10.20.30.40.50.6no.
of componentsF?measurel SeMDocT?BoSSeMDocT?BoWLSA?BoSLSA?BoWBisKMeans?BoSBisKMeans?BoW(a)lll lll ll l l l l l l l2 6 10 14 18 22 26 300.50.60.70.8no.
of componentsRandindexl SeMDocT?BoSSeMDocT?BoWLSA?BoSLSA?BoWBisKMeans?BoSBisKMeans?BoW(b)Figure 2: Average F-Measure (a) and Rand Index (b)on the Balanced Corpus using BoW and BoS documentrepresentation and varying the number of componentsfor both SeMDocT and LSA.competitors, while in terms of RI the differences in per-formance are 0.012 and 0.019 for LSA-BoS and Bisect-ing K-Means-BoS, respectively.5.2 Per language evaluation of SeMDocT-BoSStarting from the clustering solutions produced bySeMDocT-BoS in both balanced and unbalanced cases,for each language we extracted a language-specific pro-jection of the clustering.
After that, we computed theclustering validation criteria according to language spe-cific solutions to quantify how well the clustering resultfits each specific language.
The results of this experi-ment are reported in Fig.
4 and Fig.
5.On the Balanced Corpus, SeMDocT-BoS showscomparable performance for English and French docu-ments, while it behaves slightly worse for Italian texts.This trend is highlighted for both clustering evaluationcriteria.
Inspecting the results for the Unbalanced Cor-pus, we observe a different trend.
Results obtained forthe English texts are generally better than the resultsfor the French and Italian documents.
For this bench-mark, SeMDocT-BoS obtains similar results for docu-llllll l l l l l l l ll2 6 10 14 18 22 26 300.20.30.40.50.6no.
of componentsF?measurel SeMDocT?BoSSeMDocT?BoWLSA?BoSLSA?BoWBisKMeans?BoSBisKMeans?BoW(a)lll ll l l l l l l l l l l2 6 10 14 18 22 26 300.50.60.70.8no.
of componentsRandindexl SeMDocT?BoSSeMDocT?BoWLSA?BoSLSA?BoWBisKMeans?BoSBisKMeans?BoW(b)Figure 3: Average F-Measure (a) and Rand Index (b)on the Unbalanced Corpus using BoW and BoS docu-ment representation and varying the number of compo-nents for both SeMDocT and LSA.BoW BoS avg # synsetsDataset Language size size per term (?
)BalancedEnglish 29 999 12 065 0.4021French 17 826 5 310 0.2978Italian 16 951 4 471 0.2637UnbalancedEnglish 19 432 10 387 0.5345French 14 439 4 431 0.3068Italian 14 743 4 012 0.2721Table 5: Statistics by language.ments written in French and in Italian.We gained an insight into the above discussed perfor-mance behaviors by computing some additional statis-tics that we report in Table 5: for each language andeach dataset, the size of the term and synset dictionar-ies and the average number of synsets per lemma (?
)we retrieved through BabelNet according to the relatedcorpus.
More in detail, ?
is the ratio between the BoSand the BoW dictionaries.
This quantity roughly eval-uates how many synsets are produced per term duringthe multilingual WSD process (Section 3.2).
As we canobserve, this value is always smaller than one, whichmeans that not all the terms have a corresponding map-ping to a synset.
The ?
ratio can explain the discrep-606l llll l l l l ll l lll2 6 10 14 18 22 26 300.40.50.60.7no.
of componentsF?measurel EnglishFrench Italian(a)lllll l l l l l ll ll l2 6 10 14 18 22 26 300.40.50.60.70.8no.
of componentsRandindexl EnglishFrench Italian(b)Figure 4: Average F-Measure (a) and Rand Index (b)for language specific solutions on the Balanced Corpusobtained by SeMDocT-BoS.ancy in (language-specific) performances in the twoscenarios.
In particular, the difference in the ?
statis-tic between English and the other languages is moreevident for the Unbalanced Corpus (i.e., 0.23 betweenEnglish and French), while it is lower for the BalancedCorpus (around 0.1).
The relatively large gap in ?
be-tween the first and the second language (respectively,English and French) for the Unbalanced Corpus re-duces the relative gap between the second and the thirdlanguages (respectively, French and Italian) while thistrend is less marked for the Balanced Corpus as ?
rangeis narrower.
In summary, we can state that our frame-work works well if BabelNet knowledge base providesa good coverage of the terms in the analyzed language.Experimental evidence shows that, if this condition ismet, SeMDocT-BoS provides better clustering resultsw.r.t.
the competing approaches.5.3 Runtime of tensor decompositionAs previously discussed, T-HOSVD of a third-ordertensor can be computed through three standard SVDs.Furthermore, for clustering purposes, we consideredonly the mode-1 factor matrix of the decomposition.lll lll l l l l lll ll2 6 10 14 18 22 26 300.40.50.60.70.8no.
of componentsF?measurel EnglishFrench Italian(a)lll ll l ll l l lll ll2 6 10 14 18 22 26 300.40.50.60.70.8no.
of componentsRandindexl EnglishFrench Italian(b)Figure 5: Average F-Measure (a) and Rand Index (b)for language specific solutions on the Unbalanced Cor-pus obtained by SeMDocT-BoS.To compute the SVD, we used the svds() function ofMATLAB R2012b, which is based on an iterative algo-rithm.2Experiments were carried out on an Intel CoreI7-3610QM platform with 16GB DDR RAM.Figure 6 shows the execution time of the SVD overthe mode-1 matricization of our tensor for the BalancedCorpus, by varying the number of components, for bothBoW and BoS representation models.
As it can be ob-served, in both cases the runtime is linear in the numberof components.
However, the SVD computation in theBoS setting is one order of magnitude faster than timeperformance in the BoW setting.
This is mainly due toa large difference in size between the feature spaces ofBoW and BoS (cf.
Table 2), since the selected num-ber of segment clusters (k) was nearly the same (25 forBoW, and 22 for BoS).
Therefore, by providing a morecompact feature space, BoS clearly allows for a muchless expensive SVD computation for our tensor decom-position.2http://www.mathworks.it/it/help/matlab/ref/svds.html607l l lll l l ll ll l lll2 6 10 14 18 22 26 30020406080100120no.
of componentstime(sec.
)l Bag of WordsBag of SynsetsFigure 6: Time performance of SVD over the mode-1matricization of the Balanced Corpus tensor.6 DiscussionOur work paves the way for the use of a multilingualknowledge base to deal with the multilingual documentclustering task.
Here we sum up our main findings.SeMDocT vs. LSA.
LSA achieved its best resultsfor a number of components generally smaller than theone for which SeMDocT obtained its maximum.
Thisis due to the initial information that the two methodssummarize.
LSA tries to capture the variation of theinitial document-term (alternatively, document-synset)matrix representing the texts in a lower space, whereasSeMDocT does the same starting from a richer repre-sentation of the documents (i.e., a third-order tensormodel).
For this reason, SeMDocT tends to employrelatively more components in order to summarize thedocuments content; however, a number of componentsbetween 16 and 30 is generally enough to ensure goodperformance of SeMDocT.
Moreover, in most cases,the highest performance results by SeMDocT are betterthan the highest performances of LSA.BoS vs.
BoW.
Our results have highlighted thebetter quality in multilingual clustering supplied bysynsets compared with the one provided by terms.
BoSproduces a smaller representation space over whichdocuments are projected, but it is enough rich to wellcapture the documents content.
In particular, BoS ben-efits from the WSD process that is able to discriminatethe same term w.r.t.
the context in which it appears.BabelNet.
BabelNet is a recent project that supportsmany different languages.
As the intention of the au-thors is to enrich this resource, in the future our frame-work will benefit of this fact.
Moreover, our frameworkcan deal with documents written in many different lan-guages as they are represented through the same space;the only constraint is related to the available languagesupport in BabelNet.
On the other hand, we point outthat any other multilingual knowledge base and WSDtools can in principle be integrated in our framework.7 ConclusionIn this paper we proposed a new approach for multi-lingual document clustering.
Our key idea lies in thecombination of a tensor-based model with a bag-of-synsets description, which enables a common space toproject multilingual document collections.
We evalu-ated our approach w.r.t.
standard document clusteringmethods, using both term and synset representations.Results have shown the benefits deriving from the useof a multilingual knowledge base in the analysis ofcomparable corpora, and also shown the significanceof our approach in both a balanced and an unbalancedcorpus evaluation.
Our tensor-based representation oftopically-segmented multilingual documents can alsobe applied to cross-lingual information retrieval or mul-tilingual document categorization.ReferencesEneko Agirre and Aitor Soroa.
2009.
Personaliz-ing PageRank for Word Sense Disambiguation.
InProc.
of the International Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL), pages 33?41.Doug Beeferman, Adam L. Berger, and John D. Laf-ferty.
1999.
Statistical Models for Text Segmenta-tion.
Machine Learning, 34(1-3):177?210.Peter A. Chew, Brett W. Bader, Tamara G. Kolda, andAhmed Abdelali.
2007.
Cross-language informationretrieval using PARAFAC2.
In Proc.
of the ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD), pages 143?152.Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Jo-hanna Moore.
2001.
Latent Semantic Analysis forText Segmentation.
In Proc.
of the InternationalConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 109?117.Nello Cristianini, John Shawe-Taylor, and HumaLodhi.
2001.
Latent Semantic Kernels.
In Proc.
ofthe International Conference on Machine Learning(ICML), pages 66?73.Ian Davidson, Kiri Wagstaff, and Sugato Basu.
2006.Measuring constraint-set utility for partitional clus-tering algorithms.
In Proc.
of the European Confer-ence on Principles and Practice of Knowledge Dis-covery in Databases (PKDD), pages 115?126.Dino Ienco, C?eline Robardet, Ruggero G. Pensa, andRosa Meo.
2013.
Parameter-less co-clustering forstar-structured heterogeneous data.
Data Miningand Knowledge Discovery, 26(2):217?254.Marti A. Hearst.
1997.
TextTiling: Segmenting Textinto Multi-Paragraph Subtopic Passages.
Computa-tional Linguistics, 23(1):33?64.Young-Min Kim, Massih-Reza Amini, Cyril Goutte,and Patrick Gallinari.
2010.
Multi-view clustering608of multilingual documents.
In Proc.
of the ACM SI-GIR International Conference on Research and De-velopment in Information Retrieval (SIGIR), pages821?822.Tamara G. Kolda and Brett W. Bader.
2009.
TensorDecompositions and Applications.
SIAM Review,51(3):455?500.N.
Kiran Kumar, G. S. K. Santosh, and VasudevaVarma.
2011.
A language-independent approach toidentify the named entities in under-resourced lan-guages and clustering multilingual documents.
InProc.
of the International Conference of the Cross-Language Evaluation Forum (CLEF), pages 74?82.N.
Kiran Kumar, G. S. K. Santosh, and VasudevaVarma.
2011.
Effectively mining Wikipedia for clus-tering multilingual documents.
In Proc.
of the Inter-national Conference on Applications of Natural Lan-guage to Information Systems (NLDB), pages 254?257.N.
Kiran Kumar, G. S. K. Santosh, and VasudevaVarma.
2011.
Multilingual document clustering us-ing Wikipedia as external knowledge.
In Proc.
of theInformation Retrieval Facility Conference (IRFC),pages 108?117.Lieven De Lathauwer, Bart De Moor, and Joos Vande-walle.
2000.
A Multilinear Singular Value Decom-position.
SIAM Journal on Matrix Analysis and Ap-plications, 21(4):1253?1278.Xinhai Liu, Wolfgang Gl?anzel, and Bart De Moor.2011.
Hybrid clustering of multi-view data viaTucker-2 model and its application.
Scientometrics,88(3):819?839.Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
2004.PageRank on Semantic Networks, with Applicationto Word Sense Disambiguation.
In Proc.
of the In-ternational Conference on Computational Linguis-tics (COLING).Soto Montalvo, Raquel Mart?
?nez-Unanue, ArantzaCasillas, and V?
?ctor Fresno.
2007.
Multilingualnews clustering: Feature translation vs. identifica-tion of cognate named entities.
Pattern RecognitionLetters, 28(16):2305?2311.Roberto Navigli and Mirella Lapata.
2010.
An Exper-imental Study of Graph Connectivity for Unsuper-vised Word Sense Disambiguation.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(4):678?692.Roberto Navigli and Simone P. Ponzetto.
2012.
Babel-Net: The Automatic Construction, Evaluation andApplication of a Wide-Coverage Multilingual Se-mantic Network.
Artificial Intelligence, 193:217-250.Roberto Navigli and Simone P. Ponzetto.
2012.
Mul-tilingual WSD with Just a Few Lines of Code: TheBabelNet API.
In Proc.
of the System Demonstra-tions of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 67?72.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2011.
Cross lingual text classification by miningmultilingual topics from Wikipedia.
In Proc.
of theACM International Conference on Web Search andWeb Data Mining (WSDM), pages 375?384.Llu?
?s Padr?o and Evgeny Stanilovsky.
2012.
FreeL-ing 3.0: Towards Wider Multilinguality.
In Proc.
ofthe Language Resources and Evaluation Conference(LREC).William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the Ameri-can Statistical Association, 66:846?850.Salvatore Romeo, Andrea Tagarelli, Francesco Gullo,and Sergio Greco.
2013.
A Tensor-based ClusteringApproach for Multiple Document Classifications.
InProc.
of the International Conference on PatternRecognition Applications and Methods (ICPRAM),pages 200?205.Stan Salvador and Philip Chan.
2004.
Determining theNumber of Clusters/Segments in Hierarchical Clus-tering/Segmentation Algorithms.
In Proc.
of the In-ternational Conference on Tools with Artificial Intel-ligence (ICTAI), pages 576?584.Michael Steinbach, George Karypis, and Vipin Kumar.2000.
A Comparison of Document Clustering Tech-niques.
In Proc.
of the KDD Workshop on Text Min-ing.Andrea Tagarelli and George Karypis.
2013.
Asegment-based approach to clustering multi-topicdocuments.
Knowledge and Information Systems,34(3):563?595.George Tsatsaronis, Iraklis Varlamis, and KjetilN?rv?ag.
2010.
SemanticRank: Ranking Keywordsand Sentences Using Semantic Graphs.
In Proc.
ofthe International Conference on Computational Lin-guistics (COLING), pages 1074?1082.Chih-Ping Wei, Christopher C. Yang, and Chia-MinLin.
2008.
A Latent Semantic Indexing-based Ap-proach to Multilingual Document Clustering.
Deci-sion Support Systems, 45(3):606?620.Eric Yeh, Daniel Ramage, Christopher D. Manning,Eneko Agirre, and Aitor Soroa.
2009.
WikiWalk:Random walks on Wikipedia for Semantic Related-ness.
In Proc.
of the ACL Workshop on Graph-basedMethods for Natural Language Processing, pages41?49.Dani Yogatama and Kumiko Tanaka-Ishii.
2009.
Mul-tilingual spectral clustering using document similar-ity propagation.
In Proc.
of the International Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 871?879.Ying Zhao and George Karypis.
2004.
Empirical andTheoretical Comparison of Selected Criterion Func-tions for Document Clustering.
Machine Learning,55(3):311?331.609
