Evaluating Question-Answering Techniques in ChineseXiaoyan Li  and  W. Bruce CroftComputer Science DepartmentUniversity of Massachusetts, Amherst, MA{xiaoyan, croft}@cs.umass.eduABSTRACTAn important first step in developing a cross-lingualquestion answering system is to understand whethertechniques developed with English text will also work withother languages, such as Chinese.
The Marsha Chinesequestion answering system described in this paper usestechniques similar to those used in the English systemsdeveloped for TREC.
Marsha consists of three maincomponents: the query processing module, the Hanquerysearch engine, and the answer extraction module.
It alsocontains some specific techniques dealing with Chineselanguage characteristics, such as word segmentation andordinals processing.
Evaluation of the system is done usinga method based on the TREC question-answering track.The results of the evaluation show that the performance ofMarsha is comparable to some English question answeringsystems in TREC 8 track.
An English language version ofMarsha further indicates that the heuristics used areapplicable to the English question answering task.KeywordsQuestion-Answering (QA); Search engine; multilingualretrieval, Chinese QA.1.
IntroductionA number of techniques for ?question answering?
haverecently been evaluated both in the TREC environment(Voorhees and Harman, 1999) and in the DARPA TIDESprogram.
In the standard approach to information retrieval,relevant text documents are retrieved in response to aquery.
The parts of those documents that may contain themost useful information or even the actual answer to thequery are typically indicated by highlighting occurrences ofquery words in the text.
In contrast, the task of a question-answering system is to identify text passages containing therelevant information and, if possible, extract the actualanswer to the query.
Question answering has a long historyin natural language processing, and Salton?s first book(Salton, 1968) contains a detailed discussion of therelationship between information retrieval and question-answering systems.
The focus in recent research has beenon extracting answers from very large text databases andmany of the techniques use search technology as a majorcomponent.
A significant number of the queries used ininformation retrieval experiments are questions, forexample, TREC topic 338 ?What adverse effects havepeople experienced while taking aspirin repeatedly??
andtopic 308 ?What are the advantages and/or disadvantages oftooth implants??
In question-answering experiments, thequeries tend to be more restricted questions, where answersare likely to be found in a single text passage, for example,TREC question-answering question 11 ?Who was PresidentCleveland?s wife??
and question 14 ?What country is thebiggest producer of Tungsten?
?The TREC question-answering experiments have, to date,used only English text.
As the first step towards our goal ofcross-lingual question answering, we investigated whetherthe general approaches to question answering that havebeen used in English will also be effective for Chinese.Although it is now well known that statistical informationretrieval techniques are effective in many languages, earlierresearch, such as Fujii and Croft (1993, 1999), was helpfulin pointing out which techniques were particularly usefulfor languages like Japanese.
This research was designed toprovide similar information for question answering.
In thenext section, we describe the components of the Chinesequestion answering system (Marsha) and the algorithm usedto determine answers.
In section 3, we describe anevaluation of the system using queries obtained fromChinese students and the TREC-9 Chinese cross-lingualdatabase (164,779 documents from the Peoples Daily andthe Xing-Hua news agencies in the period 1991-1995).2.
Overview of the Marsha QuestionAnswering SystemThe Chinese question-answering system consists of threemain components.
These are the query processing module,the Hanquery search engine, and the answer extractionmodule.
The query processing module recognizes knownquestion types and formulates queries for the search engine.The search engine retrieves candidate texts from a largedatabase.
The answer extraction module identifies textpassages that are likely to contain answers and extractsanswers, if possible, from these passages.
This systemarchitecture is very similar to other question-answeringsystems described in the literature.More specifically, the query processing module carries outthe following steps:(1) The query is matched with templates to decide thequestion type and the ?question words?
in the query.
Wedefine 9 question types.
Most of these correspond to typicalnamed entity classes used in information extraction systems.For each question type, there are one or more templates.Currently there are 170 templates.
If more than onetemplate matches the question, we pick the longest match.For example, a question may include?
?(how manydollars).
Then both (how many dollars) and (how many) will match the question.
In this case, we willpick and assign ?MONEY?
to the question type.The following table gives examples for each question type:TEMPLATE QUESTIONTYPETRANSLATIONPERSON which personLOCATION which cityORGANIZATIONwhat organizationDATE what dateTIME what timeMONEY how many dollarsPERCENTAGE what is thepercentageNUMBER how manyOTHER what is the meaningof(2) Question words are removed from the query.
This is aform of ?stop word?
removal.
Words like ?
?(which person) are removed from the query since they areunlikely to occur in relevant text.
(3) Named entities in the query are marked up using BBN?sIdentiFinder system.
A named entity is kept as a word aftersegmentation.
(5) The query is segmented to identify Chinese words.
(6) Stop words are removed.
(7) The query is formulated for the Hanquery search engine.Hanquery is the Chinese version of Inquery (Broglio,Callan and Croft, 1996) and uses the Inquery querylanguage that supports the specification of a variety ofevidence combination methods.
To support questionanswering, documents containing most of the query wordswere strongly preferred.
If the number of query words leftafter the previous steps is greater than 4, then the operator#and (a probabilistic AND) is used.
Otherwise, theprobabilistic passage operator #UWn (unordered window)is used.
The parameter n is set to twice the number of wordsin the query.Hanquery is used to retrieve the top 10 ranked documents.The answer extraction module then goes through thefollowing steps:(8) IdentiFinder is used to mark up named entities in thedocuments.
(9) Passages are constructed from document sentences.
Weused passages based on sentence pairs, with a 1-sentenceoverlap.
(10) Scores are calculated for each passage.
The score isbased on five heuristics:?
First Rule:Assign 0 to a passage if no expected name entity is present.?
Second Rule:Calculate the number of match words in a passage.Assign 0 to the passage if the number of matching words isless than the threshold.
Otherwise, the score of this passageis equal to the number of matching words (count_m).The threshold is defined as follows:threshold = count_q   if count_q<4threshold = count_q/2.0+1.0  if 4<=count_q<=8threshold = count_q/3.0+2.0  if count_q>8count_q is the number of words in the query.?
Third Rule:Add 0.5 to score if all matching words are within onesentence.?
Fourth Rule:Add 0.5 to score if all matching words are in the same orderas they are in the original question.?
Fifth Rule:score = score + count_m/(size of matching window)(11) Pick the best passage for each document and rank them.
(12) Extract the answer from the top passage:Find all candidates according to the question type.
Forexample, if the question type is LOCATION, then eachlocation marked by IdentiFinder is an answer candidate.
Ananswer candidate is removed if it appears in the originalquestion.
If no candidate answer is found, no answer isreturned.Calculate the average distance between an answer candidateand the location of each matching word in the passage.Pick the answer candidate that has the smallest averagedistance as the final answer.3.
Evaluating the SystemWe used 51 queries to do the initial evaluation of thequestion-answering system.
We selected 26 queries from240 questions collected from Chinese students in ourdepartment, because only these had answers in the testcollection.
The other 25 queries were constructed by eitherreformulating a question or asking a slightly differentquestion.
For example, given the question ?which city is thebiggest city in China??
we also generated the questions?where is the biggest city in China??
and ?which city is thebiggest city in the world?
?.The results for these queries were evaluated in a similar, butnot identical way to the TREC question-answering track.An ?answer?
in this system corresponds to the 50 byteresponses in TREC and passages are approximatelyequivalent to the 250 byte TREC responses.For 33 of 51 queries, the system suggested answers.
24 ofthe 33 were correct.
For these 24, the ?reciprocal rank?
is 1,since only the top ranked passage is used to extract answers.Restricting the answer extraction to the top ranked passagealso means that the other 27 queries have reciprocal rankvalues of 0.
In TREC, the reciprocal ranks are calculatedusing the highest rank of the correct answer (up to 5).
In ourcase, using only the top passage means that the meanreciprocal rank of 0.47 is a lower bound for the result of the50 byte task.As an example, the question ??
(Which city is the biggest city in China?
), the answerreturned is  (Shanghai).
In the top ranked passage,?China?
and ?Shanghai?
are the two answer candidates thathave the smallest distances.
?Shanghai?
is chosen as thefinal answer since ?China?
appears in the original question.As an example of an incorrect response, the question ??
(In which year did Jun Xie defeat a Russian player andwin the world chess championship for the first time?
)produced an answer of  (today).
There were twocandidate answers in the top passage, ?October 18?
and?today?.
Both were marked as DATE by Identifinder, but?today?
was closer to the matching words.
This indicatesthe need for more date normalization and better entityclassification in the system.For 44 queries, the correct answer was found in the top-ranked passage.
Even if the other queries are given areciprocal rank of 0, this gives a mean reciprocal rank of0.86 for a task similar to the 250 byte TREC task.
In fact,the correct answer for 4 other queries was found in the top5 passages, so the mean reciprocal rank would be somewhathigher.
For 2 of the remaining 3 queries, Hanquery did notretrieve a document in the top 10 that contained an answer,so answer extraction could not work.4.
Further ImprovementsThese results, although preliminary, are promising.
Wehave made a number of improvements in the new version(v2) of the system.
Some of these are described in thissection.One of the changes is designed to improve the system?sability to extract answers for the questions that ask for anumber.
A number recognizer was developed to recognizenumbers in Chinese documents.
The numbers here arenumbers other than DATE, MONEY and PERCENTAGEthat are recognized by IdentiFinder.
The version ofIdentiFinder used in our system can only mark up seventypes of name entities and this limits the system?s ability toanswer other types of questions.
The number recognizer isthe first example of the type of refinement to named entityrecognition that must be done for better performance.An example of a question requiring a numeric answer is:?
?
(What is the number ofClinton?s presidency?)?.
This question could be answeredin Marsha v2 by extracting the marked up number from thebest passage in the answer extraction part, while Marsha v1could only return the top 5 passages that were likely to havethe answer to this question.Another improvement relates to the best matching windowof a passage.
The size of the matching window in eachpassage is an important part of calculating the belief scorefor the passage.
Locating the best matching window is alsoimportant in the answer-extraction processing because thefinal answer picked is the candidate that has the smallestaverage distance from the matching window.
The bestmatching window of a passage here is the window that hasthe most query words in it and has the smallest windowsize.
In the previous version of our system, we onlyconsider the first occurrence of each query word in apassage and index the position accordingly.
The matchingwindow is thus from the word of the smallest index to theword of the largest index in the passage.
It is only a roughapproximation of the best matching window though itworks well for many of the passages.
In the second versionof Marsha, we developed a more accurate algorithm tolocate the best matching window of each passage.
Thischange helped Marsha v2 find correct answers for somequestions that previously failed.
The following is anexample of such a question.For the question ?
?
(How many people in the United States are below thepoverty line?
)?The best passage is as follows:?
?This passage has two occurrences of query word ?
?.In v1, the first occurrence of ?
?
is treated as the start ofthe matching window, whereas the second occurrence isactually the start of the best matching window.
There aretwo numbers ?
?
(more than 2 million) and ??
(33.585 million) in the passage.
The rightanswer ?
?
(33.585 million) is nearer to thebest matching window and  ?
?
(more than 2million) is nearer to the estimated matching window.Therefore, the right answer can be extracted after correctlylocating the best matching window.The third improvement is with the scoring strategies ofpassages.
Based on the observation that the size of the bestmatching window of a passage plays a more important rolethan the order of the query words in a passage, we adjustedthe score bonus for same order satisfaction from 0.5 to0.05.
This adjustment makes a passage with a smallermatching window get a higher belief score than a passagethat satisfies the same order of query words but has a biggermatching window.
As an example, consider the question:?
?
(Who was the first president inthe United States?
)?.Passage 1 is the passage that has the right answer ?
?.Passage 1.?
#pn: #pm:#xh:5#lm: #ti: #au:#rw: #rw: #rw:?Passage 2.?, ?Passage 1 and Passage 2 both have all query words.
Thesize of the best matching window in Passage 1 is smallerthan that in Passage 2 while query words in Passage 2 havethe same order as that in the question.
The scoring strategyin Marsha v2 selects Passage 1 and extracts the correctanswer while Marsha v1 selected Passage 2.Special processing of ordinals has also been considered inMarsha v2.
Ordinals in Chinese usually start with theChinese character " " and are followed by a cardinal.
It isbetter to retain ordinals as single words during the querygeneration in order to retrieve better relevant documents.However, the cardinals (part of the ordinals in Chinese) in apassage are marked up by the number recognizer for theymight be answer candidates for questions asking for anumber.
Thus ordinals in Chinese need special care in a QAsystem.
In Marsha v2, ordinals appearing in a question arefirst retained as single words for the purpose of generating agood query and then separated in the post processing afterrelevant documents are retrieved to avoid answercandidates being ignored.5.
Comparison with English QuestionAnswering SystemsSome techniques used in Marsha are similar to thetechniques in English question answering systemsdeveloped by other researchers.
The template matching inMarsha for deciding the type of expected answer for aquestion is basically the same as the one used in theGuruQA (Prager et al, 2000) except that the templatesconsist of Chinese word patterns instead of English wordpatterns.
Marsha has the ability of providing answers toeight types of questions: PERSON, LOCATION,ORGANIZATION, DATE, TIME, MONEY,PERCENTAGE, and NUMBER.
The first seven typescorrespond to the named entities from IdentiFinderdeveloped by BBN.
We developed a Chinese number-recognizer ourselves which marks up numbers in thepassages as answer candidates for questions asking for anumber.
The number could be represented as a digitnumber or Chinese characters.
David A.
Hull used a propername tagger ThingFinder developed at Xerox in hisquestion answering system.
Five of the answer typescorrespond to the types of proper names from ThingFinder(Hull, 1999).
The scoring strategy in Marsha is similar tothe computation of score for an answer window in theLASSO QA system (Moldovan et al, 1999) in terms of thefactors considered in the computation.
Factors such as thenumber of matching words in the passage, whether allmatching words in the same sentence, and whether thematching words in the passage have the same order as theyare in the question are common to LASSO and Marsha.We have also implemented an English language version ofMarsha.
The system implements the answer classesPERSON, ORGANIZATION, LOCATION, and DATE.Queries are generated in the same fashion as Marsha.
Ifthere are any phrases in the input query (named entitiesfrom IdentiFinder, quoted strings) these are added to anInquery query in a #N operator all inside a #sum operator.For example:Question: "Who is the author of "Bad Bad LeroyBrown"Inquery query: #sum( #uw8(author Bad Bad LeroyBrown) #6(Bad Bad Leroy Brown))Where N is number of terms + 1 for named entities, andnumber of terms + 2 for quoted phrases.
If a query retrievesno documents, a ?back off?
query uses #sum over the queryterms, with phrases dropped.
The above would become#sum(author Bad Bad Leroy Brown).The system was tested against the TREC9 questionanswering evaluation questions.
The mean reciprocal rankover 682/693 questions was 0.300 with 396 questions goingunanswered.
The U.Mass.
TREC9 (250 byte) run had ascore of 0.367.
Considering only the document retrieval, wefind a document containing an answer for 471 of thequestions, compared to 477 for the official TREC9 runwhich used expanded queries.
This indicates that theMarsha heuristics have applicability to the English questionanswering task and are not limited to the Chinese questionanswering task.6.
Summary and Future WorkThe evaluations on Marsha, although preliminary, indicatethat techniques developed for question answering in Englishare also effective in Chinese.
In future research, we plan tocontinue to improve these techniques and carry out morecareful evaluations to establish whether there are anysignificant differences in the question-answering taskbetween these two languages.The evaluation of the English version of Marsha indicatesthat the Marsha heuristics work well in English as well as inChinese.
We now plan to incorporate these techniques in across-lingual question-answering system for English andChinese.
By using two systems with similar questionprocessing strategies, we hope to exploit the querytemplates to produce accurate question translations.We have also started to develop a probabilistic model ofquestion answering using the language model approach(Ponte and Croft, 1998).
This type of model will beessential for extending the capability of QA systems beyonda few common query forms.AcknowledgementsThis material is based on work supported in part by theLibrary of Congress and Department of Commerce undercooperative agreement number EEC-9209623 and in partby SPAWARSYSCEN-SD grant number N66001-99-1-8912.Any opinions, findings and conclusions orrecommendations expressed in this material are theauthor(s) and do not necessarily reflect those of thesponsor.We also want to express out thanks to people at CIIR fortheir help.
Special thanks to David Fisher who implementedthe English language version of Marsha, and Fangfang Fengfor his valuable discussions on Chinese related researchissues.7.
ReferencesBroglio, J., Callan, J.P. and Croft, W.B.
?Technical Issuesin Building an Information Retrieval System for Chinese,?CIIR Technical Report IR-86, Computer ScienceDepartment, University of Massachusetts, Amherst, (1996).H.
Fujii and W.B.
Croft, ?A Comparison of IndexingTechniques for Japanese Text Retrieval,?
Proceedings ofSIGIR 93, 237-246, (1993).H.
Fujii and W.B.
Croft, ?Comparing the performance ofEnglish and Japanese text databases?, in S. Armstrong et al(eds.
), Natural Language Processing using Very LargeCorpora, 269-282, Kluwer, (1999).
(This paper firstappeared in a 1994 workshop)G. Salton, Automatic Information Organization andRetrieval, McGraw-Hill, (1968).E.
Voorhees and D. Harman (eds.
), The 7th Text RetrievalConference (TREC-7), NIST Special Publication 500-242,(1999).Ponte, J. and Croft, W.B.
"A Language Modeling Approachto Information Retrieval," in the Proceedings of SIGIR 98,pp.
275-281(1998).Moldovan, Dan et al ?LASSO: A Tool for Surfing theAnswer Net,?
in the proceedings of TREC-8, pp 175-183.
(1999).Hull, David A., ?Xerox TREC-8 Questio Answering TrackReport,?
in the proceedings of TREC-8, pp743.Prager, John, Brown, Eric, and Coden, Anni,?Question_Answering by Predictive Annotation,?
in theproceedings of SIGIR 2000.
