Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212?216,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLatent Semantic Matching: Application to Cross-language TextCategorization without Alignment InformationTsutomu Hirao and Tomoharu Iwata and Masaaki NagataNTT Communication Science Laboratories, NTT Corporation2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan{hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jpAbstractUnsupervised object matching (UOM) isa promising approach to cross-languagenatural language processing such as bilin-gual lexicon acquisition, parallel corpusconstruction, and cross-language text cat-egorization, because it does not requirelabor-intensive linguistic resources.
How-ever, UOM only finds one-to-one corre-spondences from data sets with the samenumber of instances in source and targetdomains, and this prevents us from ap-plying UOM to real-world cross-languagenatural language processing tasks.
To al-leviate these limitations, we proposes la-tent semantic matching, which embedsobjects in both source and target lan-guage domains into a shared latent topicspace.
We demonstrate the effectivenessof our method on cross-language text cat-egorization.
The results show that ourmethod outperforms conventional unsu-pervised object matching methods.1 IntroductionUnsupervised object matching is a method forfinding one-to-one correspondences between ob-jects across different domains without knowledgeabout the relation between the domains.
Kernel-ized sorting (Novi et al, 2010) and canonical cor-relation analysis based methods (Haghighi et al,2008; Tripathi et al, 2010) are two such exam-ples of unsupervised object matching, which havebeen shown to be quite useful for cross-languagenatural language processing (NLP) tasks.
One ofthe most important properties of the unsupervisedobject matching is that it does not require any lin-guistic resources which connects between the lan-guages.
This distinguishes it from other cross-language NLP methods such as machine transla-tion based and projection based approaches (Du-mais et al, 1996; Gliozzo and Strapparava, 2005;Platt et al, 2010), which we need bilingual dictio-naries or parallel sentences.When we apply unsupervised object matchingmethods to cross-language NLP tasks, there aretwo critical problems.
The first is that they onlyfind one-to-one matching.
The second is they re-quire the same size of source- and target-data.
Forexample, the correct translation of a word is notalways unique.
French words ?maison?, ?appart-ment?
and ?domicile?
can be regarded as transla-tion of an English word ?home?.
In addition, En-glish vocabulary size is not equal to that of French.These discussions motivate us to introduce ashared space in which both source and target do-main objects will reside.
If we can obtain sucha shared space, we can match objects within thespace, because we can use standard distance met-rics on this space.
This will also enable us to usevarious kinds of non-strict matching.
For exam-ple, k-nearest objects in the source domain will beretrieved for a query object in the target domain.In this paper, we propose a simple but effectivemethod to find the shared space by assuming thattwo languages have common latent topics, whichwe call latent semantic matching.
With latent se-mantic matching, we first find latent topics in twodomains independently.
Then, the topics in twodomains are aligned by kernelized sorting, and ob-jects are embedded in a shared latent topic space.Latent topic representations are successfully usedin a wide range of NLP tasks, such as informationretrieval and text classification, because they rep-resent intrinsic information of documents (Deer-wester et al, 1990).
By matching latent topics,we can find relation between source and target do-mains, and additionally we can handle differentnumbers of objects in two domains.We compared latent semantic matching withconventional unsupervised object matching meth-212ods on the task of cross-language text categoriza-tion, i.e.
classifying target side unlabeled docu-ments by label information obtained from sourceside documents.
The results show that, with moresource side documents, our method achieved thehighest classification accuracy.2 Related workMany cross-language text processing methodshave been proposed that require correspondencesbetween source and target languages.
For exam-ple, (Dumais et al, 1996) proposed cross-linguallatent semantic indexing, and (Platt et al, 2010)employed oriented principle component analysisand canonical correlation analysis (CCA).
Theyconcatenate the document pairs (source documentand its translation) obtained from a document-level parallel corpus.
They then apply multi-variate analysis to acquire the translingual projec-tion.
There are extensions of latent Dirichlet alo-cation (LDA) (Blei et al, 2003) for cross-languageanalysis, such as multilingual topic models (Boyd-Graber and Blei, 2009), joint LDA (Jagadeeshand Daume III, 2010) and multilingual LDA (Xi-aochuan et al, 2011).
They require a bilingual dic-tionary or document-level parallel corpora.Unsupervised object matching methods havebeen proposed recently (Novi et al, 2010;Haghighi et al, 2008; Yamada and Sugiyama,2011).
These methods are promising in terms oflanguage portability because they do not requireexternal language resources.
(Novi et al, 2010)proposed kernelized sorting (KS); it finds one-to-one correspondences between objects in differentdomains by permuting a set to maximize the de-pendence between two sets.
Here, the Hilbert-Schmidt independence criterion is used for mea-suring dependence.
(Djuric et al, 2012) proposedconvex kernelized sorting as an extension of KS.
(Yamada and Sugiyama, 2011) proposed least-squares object matching which maximizes thesquared-loss mutual information between matchedpairs.
(Haghighi et al, 2008) proposed anotherframework, matching CCA (MCCA), based on aprobabilistic interpretation of CCA (Bach and Jor-dan, 2005).
MCCA simultaneously finds latentvariables that represent correspondences and la-tent features so that the latent features of corre-sponding examples exhibit the maximum correla-tion.
However, these unsupervised object match-ing methods have limitations.
They require thatthe source and target domains have the same datasize, and they find one-to-one correspondences.There are critical weaknesses of these methodswhen we attempt to apply them to real worldcross-language NLP applications.3 Latent Semantic MatchingWe propose latent semantic matching to find ashared latent space by assuming that two lan-guages have common latent topics.
Our methodconsists of following four steps: (1) for bothsource and target domains, we map the documentsto a K-dimensional latent topic space indepen-dently, (2) we find the one-to-one correspondencesbetween topics across source and target domainsby unsupervised object matching, (3) we permutetopics of the target side according to the corre-spondences, while fixing the topics of the sourceside, and (4) finally, we map documents in thesource and target domains to a shared latent spaceby using permuted and fixed topics.3.1 Topic Extraction as Dimension ReductionSuppose that we have N documents in the sourcedomain.
sn=(sni)Ii=1 is the nth document rep-resented as a multi-dimensional column vector inthe domain, i.e.
each document is represented asa bag-of-words vector.
Here, each element of thevectors indicates the TF?IDF score of the corre-sponding word in the document.
I is the size of thefeature set, i.e., the vocabulary size in the sourcedomain.
Also, we have M documents in the tar-get domain.
tm=(tmj)Jj=1 is the mth documentrepresented as a multi-dimensional vector.
J isthe vocabulary size in the target domain.
Thus,the data set in the source domain is represented byan I ?
N matrix, S=(s1, ?
?
?
, sN ), the data setin the target is represented by a J ?
M matrix,T=(t1, ?
?
?
, tM ).We factorize these matrices using nonnegativematrix factorization (Lee and Seung, 2000) to findtopics as follows:S ?WSHS , (1)T ?WTHT .
(2)WS is an I?K matrix that represents a set of top-ics, i.e.
each column vector denotes word weightsfor each topic.
HS is a K ?
N matrix that de-notes a set of latent semantic representations ofdocuments in the source domain, i.e.
each row213??????
WS HS?
* = 0 0 1 00 1 0 00 0 0 11 0 0 0ment.
I is the size of feature set, i.e., the size of vocabulary inthe source domain.
Also, we have M documents in a targetdomain.
tm = (tmj)Jj=1 is the m-th document representedas a multi-dimensional vector.
J is the size of vocabulary inthe target domain.
Thus, the data set in the source domain isrepresented as the I ?N matrix, S, the data set in the targetis represented as the J ?M matrix, T .Here, we assume that these matrices are approximated asthe product of low rank matrices as follows:S ?
WSHS , (1)T ?
WTHT (2)WS is I?K matrix, which represents a set of topic propor-tions in the source domain, i.e., each column vector denotestopic proportion.
HS is K ?
N matrix, which denotes a setof documents in the K-dimensional latent space which cor-responds to the source domain, i.e., each row vector denotesthe document in the latent space.
The k(1 ?
k ?
K)-th basisin the latent space corresponds to the k-th topic proportion.WT is I ?
K matrix, which represents a set of topic pro-portions in the target domain.
HT is K ?
N matrix, whichdenotes a set of documents in the latent topic space with di-mentionaly K. K is less than I , J .
In this paper, we employNon-negative Matrix Factorization (NMF) [Lee and Seung,2000] to factorize the original matrices.According to the factorization of the original matrices, wecan map the documents in the source and target domain tolatent topic space with dimentionaly K, independently.3.2 Finding Optimal Topic Alignments byUnsupervised Object MatchingTo connect the different latent space, the basis of the spacehave to be aligned each other.
That is, topic proportion ex-tracted from the source language must be aligned that fromthe target language.
This is reasonable consideration becausewe can assume the same latent concept for both language.For example, a topic proportion obtained from English docu-ments can be aligned a topic proportion obtained from Frenchdocuments.
For all k and k?, k-th column vector in WS arealigned k?-th column vector in WT .However, we can not measure similarity between the topicproportions because we do not have any language resourcessuch as dictionary.
Therefore, we utilize unsupervised ob-ject matching method to find one-to-one correspondences be-tween topic proportions.
In this paper, we employ KernelizedSorting (KS) [Novi et al, 2010].
Of cource, we can replaceKS to another unsupervised object matching sush as MCCA[Haghighi et al, 2008], LSOM [Yamada and Sugiyama,2011].KS finds the best one-to-one matching by followings:pi?
= argmaxpi?
?K tr(G?SpiTG?Tpi),s.t.
pi1K = 1K and piT1K = 1K .
(3)pi is K?K matrix which represents one-to-one correspon-dence between topic proportion, i.e., piij = 1 indicates i-thtopic proportion in the source language corresponds to j-thone of the target language.
?
indicates set of all possibleK ?
K matrices which store one-to-one corresponrence.
GdenotesK?K kernel matrix obtained from topic proportion,Gij = K(WTi,:,W:,j), and G?
is the centerd matrix of G. K(, )is a kernel function.
1K is K-dimensional column vector ofall ones.
pi?
is obtained by iterative procedure.
According topi?, we can permutate the basis of the latent space obtainedfrom source language.
See fig hoge.S ?
WSHS .
(4)On the other hand, we can directly fomulate objective func-tion of unsupervised mapping.
If the topic proportions arealigned each other, the correlation matrix (or gram matrix)obtained from source language is proportional to one fromtarget language:||GS ?
?GT ||2 = 0.
(5)?
denotes the hyperparameter for tuning the socore range be-tween two gram matrices.By minimize the error of the matrix factorization (equa-tion (1),(2)) and the difference between correlation matrices(equation (6)), the objective function is defined as follow:E = ?S ?WSHS?2+ ?T ?WTHT ?2+ ?||GS ?
?GT ||2.
(6)?
is cost parameter between first, second argu-ment and third argument.
The optimal parameters(WS,WT ,HS,HT ) are obtained by minimizing theobjective function.
To mimimize the objective, gradient de-scend can be used.
but However that is not convex function,we only obtained local optimal.
Thefore, we employed abovetwo step procedure?????
?This objective function is not convex.
That meanswe can only obtain local optimal parameters.
By min-imizing equation (6), we can obtain a set of parameter(WS,WT ,HS,HT ) for unsupervised mapping.
we couldbe employed gradient based algorithm but, as the first step,we employ former two step optimization procedure.3.3 Cross-lingual Text Categorization viaUnsupervised Mappingm-th document in the target domain (tm) is mapped to thesource domain as follows,s(tm) = HT$:,mWS .
(7)Here, HT :,m denotes the m-th column vector of HT , s(tm)is I dimentional vector.When each document in the source domain has a classlabel yn, we can train a classifier on the training data set{sn, yn}Nn=1.
Therefore, the class label of the mapped docu-ment in the target domain s(tm) is assigned by the classifier.In the later experiments, we employ k(= 10)-NN as a classi-fier.WT HTMI NJ KKKTSTFigure 1: Topic alignments.vector denotes an embedding of a document in theK-dimensional latent space.
Similarly, WT is anI ?K matrix that represents a set of topics in thetarget domain, and HT is a K ?
M matrix thatdenotes a set of latent semantic representations oftarget documents.
K is less than I and J .By factorizing the original matrices, we can in-dependently map the documents in the source andtarget domains to the latent topic spaces whose di-mensionality is K.3.2 Finding Optimal Topic Alignments byUnsupervised Object MatchingTo connect the different latent spaces, topics ex-tracted from the source language must be alignedto one from the target language.
This is reasonablebecause we can assume that both languages sharethe same latent concept.However, we cannot quantify the similarity be-tween the topics because we do not have any ex-ternal language resources such as a dictionary.Therefore, we utilize unsupervised object match-ing method to find one-to-one correspondencesbetween topics.
In this paper, we employ kernel-ized sorting (KS) (Novi et al, 2010).
KS finds thebest one-to-one matching as follows:pi?
= argmaxpi??Ktr(GSpi?GTpi),s.t.
pi1K=1K and pi?1K=1K .
(3)Here, pi is a K?K matrix that represents the one-to-one correspondence between topics, i.e.
piij=1indicates that the ith topic in the source languagecorresponds to the jth one of the target language.Overall AverageKS 0.252 ?
0.112CKS 0.249 ?
0.033LSOM 0.278 ?
0.086LSM(300) 0.298 ?
0.077LSM(600) 0.359 ?
0.062Table 1: Average accuracy over all language pairs?K indicates the set of all possible matrices stor-ing one-to-one correspondences.
G denotes theK ?
K kernel matrix obtained from topic pro-portion, Gij=K(W?i,: ,W:,j), and G is the centeredmatrix of G. K(, ) is a kernel function.
1K is aK-dimensional column vector of all ones.
pi?
isobtained by iterative procedure.According to pi?, we obtain permuted matrices,WT=WTpi?
and HT=pi?
?HT , and the productof permuted matrices is the same with that of un-permuted matrices as follows:T ?WTHT=WTHT .
(4)Fig.
1 shows the topic alignment procedure.Since documents from both domains are repre-sented in a shared latent space, we can directly cal-culate the similarity between the nth document inthe source domain and the mth document in thetarget domain based on HT :,m (mth column vec-tor of HT ) and HS:,n (nth column vector of HS).4 Cross-language Text Categorizationvia Latent Semantic MatchingCross-language text categorization is the task ofexploiting labeled documents in the source lan-guage (e.g.
English) to classify documents inthe target language (e.g.
French).
Suppose wehave training data set {sn, yn}Nn=1 in the sourcelanguage domain.
yn ?
Y is the class labelfor the nth document.
We can train a classifierin the K-dimensional latent space with data set{H?S:,n, yn}Nn=1.
H?S:,n is the projected vector ofsn.
Also, the mth document in the target languagedomain tm is projected into the latent space asH?T :,m. Here, the documents in both domains areprojected into the same size latent space and thebasis vectors of the spaces are aligned.
Therefore,we can classify a document in the target domaintm by a classifier trained with {H?S:,n, yn}Nn=1.214BooksEnglish Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, MurphyGerman Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, KlinikElectronicsEnglish SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MPGerman *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einlietKitchenEnglish Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tampGerman ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun , Bolognese, prieren, Testsieger, TopfMusicEnglish Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knockingGerman Norah, mini, ?Little, ?Rome, ?Come, Gardot, Lana, listenings , dreamlike, digipakWatchEnglish watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonableGerman Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,Konsumbereiche, Schwingungen/StdTable 2: Examples of aligned latent topics5 Experimental Evaluation5.1 Experimental SettingsWe compared our method, latent semantic match-ing (LSM), with three unsupervised object match-ing methods: Kernelized Sorting (KS), ConvexKernelized Sorting (CKS), Least-Squares ObjectMatching (LSOM).
We set the number of the la-tent topics K to 100 and employed the k-nearestneighbor method (k=10) as the classifier.For, KS, CKS and LSOM, we find the one-to-one correspondence between documents in thesource language and documents in the target lan-guage.
Then, we assign class labels of the targetdocuments according to the correspondence.In order to build a corpus with various lan-guage pairs for evaluation, we crawled productreviews from Amazon U.S., German, France andJapan with five categories: ?Books?, ?Electronics?,?Music?, ?Kitchen?, ?Watch?.
The corpus is nei-ther sentence level parallel nor comparable.
Foreach category, we randomly select 60 documentsas the test data (M=300) for all methods and 60documents as the training data (N=300) for KS,CKS, LSOM and LSM(300).
We also comparedlatent semantic matching with 120 training docu-ments for each category (N=600), and called thismethod LSM(600).
Note that since KS, CKS andLSOM require that the data sizes are the same forsource and target domains, they cannot use train-ing data more than test data.
To avoid local opti-mum solutions of NMF, we executed our methodswith 100 different initialization values and chosethe solution that achieved the best objective func-tion of KS.5.2 Results and DiscussionTable 1 shows average accuracies with standarddivision over all language pairs.
From the table,classification accuracy of all methods significantlyoutperformed random classifier (accuracy=0.2).The results showed the effectiveness of both un-supervised object matching and latent semanticmatching.
When comparing LSM(300) with KS,CKS and LSOM, LSM(300) obtained better re-sults than these unsupervised object matchingmethods.
The result supports the effectiveness ofthe latent topic matching.
Moreover, LSM(600)achieved the highest accuracy.
There are large dif-ferences between LSM(600) and the others.
Thisresult implies not only the effectiveness of the la-tent topic matching but also increasing the numberof source side documents (labeled training data)contributes to improving classification accuracy.This is natural in terms of supervised learning butonly our method can deal with source side docu-ments that are larger in number.Table 2 shows examples of latent topics inEnglish and German extracted and aligned byLSM(600).
We can see that some author names,words related to camera, and cooking equipmentappear in ?Books?, ?Electronics?
and ?Kitchen?topics, respectively.
Similarity, there are someartists?
names in ?Music?
and watch brands in?Watch?.2156 ConclusionAs an extension of unsupervised object matching,this paper proposed latent semantic matching thatconsiders the shared latent space between two lan-guage domains.
To generate such a space, top-ics of the target space are permuted by exploit-ing unsupervised object matching.
We can mea-sure distances between objects by standard met-rics, which enable us retrieving k-nearest objectsin the source domain for a query object in the tar-get domain.
This is a significant advantage overconventional unsupervised object matching meth-ods.
We used Amazon review corpus to demon-strate the effectiveness of our method on cross-language text categorization.
The results showedthat our method outperformed conventional objectmatching methods with the same number of train-ing samples.
Moreover, our method achieved evenhigher performance by utilizing more documentsin the source domain.AcknowledgementsThe authors would like to thank Nemanja Djuricfor providing code for Convex Kernelized Sortingand the three anonymous reviewers for thoughtfulsuggestions.ReferencesFrancis Bach and Michael Jordan.
2005.
A probabilis-tic interpretation of canonical correlation analysis.Technical report, Department of Statistics, Univer-sity of California, Berkeley.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
JMLR, 3(Jan.):993?1022.Jordan Boyd-Graber and David Blei.
2009.
Multilin-gual topic model for unaligned text.
In Proc.
of the25th UAI, pages 75?82.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Nemanja Djuric, Mihajlo Grbovic, and SlobodanVucetic.
2012.
Convex kernelized sorting.
In Proc.of the 26th AAAI, pages 893?899.Susan Dumais, Lanauer Thomas, and Michael Littman.1996.
Automatic cross-linguistic information re-trieval using latent semantic indexing.
In Proc.of the Workshop on Cross-Linguistic InformationRetieval in SIGIR, pages 16?23.Alfio Gliozzo and Carlo Strapparava.
2005.
Cross lan-guage text categorization by acquiring multilingualdomain models from comparable corpora.
In Proc.of the ACL Workshop on Building and Using Paral-lel Texts, pages 9?16.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proc.
of ACL-08:HLT, pages 771?779.Jagarlamudi Jagadeesh and Hal Daume III.
2010.
Ex-tracting multilingual topics from unaligned corpora.In Proc of the 32nd ECIR, pages 444?456.Daniel Lee and Sebastian Seung.
2000.
Algorithmfor non-negative matrix factorization.
In Advancesin Neural Information Processing Systems 13, pages556?562.Quadrianto Novi, Smola Alexander, Song Le, andTuytelaars Tinne.
2010.
Kernelized sorting.
IEEETrans.
on Pattern Analysis and Machine Intelli-gence, 32(10):1809?1821.Jhon Platt, Kristina Toutanova, andWen-tau Yih.
2010.Translingual document representation from discrim-inative projections.
In Proc.
of the 2010 Conferenceon EMNLP, pages 251?261.Abhishek Tripathi, Arto Klami, and Sami Virpioja.2010.
Bilingual sentence matching using kernelCCA.
In Proc.
of the 2010 IEEE InternationalWorkshop on MLSP, pages 130?135.Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and ChenZheng.
2011.
Cross lingual text classification bymining multilingual topics from wikipedia.
In Proc.of the 4th WSDM, pages 375?384.Makoto Yamada and Masashi Sugiyama.
2011.
Cross-domain object matching with model selection.
InProc.
of the 14th AISTATS, pages 807?815.216
