Semantic Role Labeling for Coreference ResolutionSimone Paolo Ponzetto and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlp/AbstractExtending a machine learning based coref-erence resolution system with a featurecapturing automatically generated infor-mation about semantic roles improves itsperformance.1 IntroductionThe last years have seen a boost of work devotedto the development of machine learning basedcoreference resolution systems (Soon et al, 2001;Ng & Cardie, 2002; Kehler et al, 2004, inter alia).Similarly, many researchers have explored tech-niques for robust, broad coverage semantic pars-ing in terms of semantic role labeling (Gildea &Jurafsky, 2002; Carreras & Ma`rquez, 2005, SRLhenceforth).This paper explores whether coreference reso-lution can benefit from SRL, more specifically,which phenomena are affected by such informa-tion.
The motivation comes from the fact that cur-rent coreference resolution systems are mostly re-lying on rather shallow features, such as the dis-tance between the coreferent expressions, stringmatching, and linguistic form.
On the other hand,the literature emphasizes since the very begin-ning the relevance of world knowledge and infer-ence (Charniak, 1973).
As an example, considera sentence from the Automatic Content Extraction(ACE) 2003 data.
(1) A state commission of inquiry into the sinking of theKursk will convene in Moscow on Wednesday, theInterfax news agency reported.
It said that the divingoperation will be completed by the end of next week.It seems that in this example, knowing that the In-terfax news agency is the AGENT of the reportpredicate, and It being the AGENT of say, couldtrigger the (semantic parallelism based) inferencerequired to correctly link the two expressions, incontrast to anchoring the pronoun to Moscow.SRL provides the semantic relationships thatconstituents have with predicates, thus allowingus to include document-level event descriptive in-formation into the relations holding between re-ferring expressions (REs).
This layer of semanticcontext abstracts from the specific lexical expres-sions used, and therefore represents a higher levelof abstraction than predicate argument statistics(Kehler et al, 2004) and Latent Semantic Analy-sis used as a model of world knowledge (Klebanov& Wiemer-Hastings, 2002).
In this respect, thepresent work is closer in spirit to Ji et al (2005),who explore the employment of the ACE 2004 re-lation ontology as a semantic filter.2 Coreference Resolution Using SRL2.1 Corpora UsedThe system was initially prototyped using theMUC-6 and MUC-7 data sets (Chinchor & Sund-heim, 2003; Chinchor, 2001), using the standardpartitioning of 30 texts for training and 20-30 textsfor testing.
Then, we developed and tested thesystem with the ACE 2003 Training Data cor-pus (Mitchell et al, 2003)1.
Both the Newswire(NWIRE) and Broadcast News (BNEWS) sectionswhere split into 60-20-20% document-based par-titions for training, development, and testing, andlater per-partition merged (MERGED) for systemevaluation.
The distribution of coreference chainsand referring expressions is given in Table 1.2.2 Learning AlgorithmFor learning coreference decisions, we used aMaximum Entropy (Berger et al, 1996) model.Coreference resolution is viewed as a binary clas-sification task: given a pair of REs, the classifierhas to decide whether they are coreferent or not.First, a set of pre-processing components includ-1We used the training data corpus only, as the availabilityof the test data was restricted to ACE participants.143BNEWS NWIRE#coref ch.
#pron.
#comm.
nouns #prop.
names #coref ch.
#pron.
#comm.
nouns #prop.
namesTRAIN.
587 876 572 980 904 1037 1210 2023DEVEL 201 315 163 465 399 358 485 923TEST 228 291 238 420 354 329 484 712Table 1: Partitions of the ACE 2003 training data corpusing a chunker and a named entity recognizer isapplied to the text in order to identify the nounphrases, which are further taken as REs to be usedfor instance generation.
Instances are created fol-lowing Soon et al (2001).
During testing theclassifier imposes a partitioning on the availableREs by clustering each set of expressions labeledas coreferent into the same coreference chain.2.3 Baseline System FeaturesFollowing Ng & Cardie (2002), our baseline sys-tem reimplements the Soon et al (2001) system.The system uses 12 features.
Given a pair of can-didate referring expressions REi and REj the fea-tures are computed as follows2.
(a) Lexical featuresSTRING MATCH T if REi and REj have thesame spelling, else F.ALIAS T if one RE is an alias of the other; elseF.
(b) Grammatical featuresI PRONOUN T if REi is a pronoun; else F.J PRONOUN T if REj is a pronoun; else F.J DEF T if REj starts with the; else F.J DEM T if REj starts with this, that, these, orthose; else F.NUMBER T if both REi and REj agree in num-ber; else F.GENDER U if REi or REj have an undefinedgender.
Else if they are both defined and agreeT; else F.PROPER NAME T if both REi and REj areproper names; else F.APPOSITIVE T if REj is in apposition withREi; else F.(c) Semantic featuresWN CLASS U if REi or REj have an undefinedWordNet semantic class.
Else if they both havea defined one and it is the same T; else F.2Possible values are U(nknown), T(rue) and F(alse).
Notethat in contrast to Ng & Cardie (2002) we classify ALIAS asa lexical feature, as it solely relies on string comparison andacronym string matching.
(d) Distance featuresDISTANCE how many sentences REi and REjare apart.2.4 Semantic Role FeaturesThe baseline system employs only a limitedamount of semantic knowledge.
In particular, se-mantic information is limited to WordNet seman-tic class matching.
Unfortunately, a simple Word-Net semantic class lookup exhibits problems suchas coverage and sense disambiguation3, whichmake the WN CLASS feature very noisy.
As aconsequence, we propose in the following to en-rich the semantic knowledge made available to theclassifier by using SRL information.In our experiments we use the ASSERTparser (Pradhan et al, 2004), an SVM based se-mantic role tagger which uses a full syntacticanalysis to automatically identify all verb predi-cates in a sentence together with their semanticarguments, which are output as PropBank argu-ments (Palmer et al, 2005).
It is often the casethat the semantic arguments output by the parserdo not align with any of the previously identifiednoun phrases.
In this case, we pass a semantic rolelabel to a RE only in case the two phrases share thesame head.
Labels have the form ?ARG1 pred1 .
.
.ARGn predn?
for n semantic roles filled by aconstituent, where each semantic argument labelARGi is always defined with respect to a predicatelemma predi.
Given such level of semantic infor-mation available at the RE level, we introduce twonew features4.I SEMROLE the semantic role argument-predicate pairs of REi.3Following the system to be replicated, we simplymapped each RE to the first WordNet sense of the head noun.4During prototyping we experimented unpairing the ar-guments from the predicates, which yielded worse results.This is supported by the PropBank arguments always beingdefined with respect to a target predicate.
Binarizing the fea-tures ?
i.e.
do REi and REj have the same argument orpredicate label with respect to their closest predicate?
?
alsogave worse results.144MUC-6 MUC-7original R P F1 R P F1Soon et al 58.6 67.3 62.3 56.1 65.5 60.4duplicatedbaseline 64.9 65.6 65.3 55.1 68.5 61.1Table 2: Results on MUCJ SEMROLE the semantic role argument-predicate pairs of REj .For the ACE 2003 data, 11,406 of 32,502 auto-matically extracted noun phrases were tagged with2,801 different argument-predicate pairs.3 Experiments3.1 Performance MetricsWe report in the following tables the MUCscore (Vilain et al, 1995).
Scores in Table 2 arecomputed for all noun phrases appearing in eitherthe key or the system response, whereas Tables 3and 4 refer to scoring only those phrases which ap-pear in both the key and the response.
We discardtherefore those responses not present in the key,as we are interested here in establishing the upperlimit of the improvements given by SRL.We also report the accuracy score for all threetypes of ACE mentions, namely pronouns, com-mon nouns and proper names.
Accuracy is thepercentage of REs of a given mention type cor-rectly resolved divided by the total number of REsof the same type given in the key.
A RE is saidto be correctly resolved when both it and its directantecedent are in the same key coreference class.In all experiments, the REs given to the clas-sifier are noun phrases automatically extracted bya pipeline of pre-processing components (i.e.
PoStagger, NP chunker, Named Entity Recognizer).3.2 ResultsTable 2 compares the results between our du-plicated Soon baseline and the original system.The systems show a similar performance w.r.t.
F-measure.
We speculate that the result improve-ments are due to the use of current pre-processingcomponents and another classifier.Tables 3 and 4 show a comparison of the per-formance between our baseline system and theone incremented with SRL.
Performance improve-ments are highlighted in bold.
The tables showthat SRL tends to improve system recall, ratherthan acting as a ?semantic filter?
improving pre-cision.
Semantic roles therefore seem to trigger aR P F1 Ap Acn Apnbaseline 54.5 88.0 67.3 34.7 20.4 53.1+SRL 56.4 88.2 68.8 40.3 22.0 52.1Table 4: Results ACE (merged BNEWS/NWIRE)Feature Chi-squareSTR MATCH 1.0J SEMROLE 0.2096ALIAS 0.1852I SEMROLE 0.1594SEMCLASS 0.1474DIST 0.1107GENDER 0.1013J PRONOUN 0.0982NUMBER 0.0578I PRONOUN 0.0489APPOSITIVE 0.0397PROPER NAME 0.0141DEF NP 0.0016DEM NP 0.0Table 5: ?2 statistic for each featureresponse in cases where more shallow features donot seem to suffice (see example (1)).The RE types which are most positively affectedby SRL are pronouns and common nouns.
On theother hand, SRL information has a limited or evenworsening effect on the performance on propernames, where features such as string matching andalias seem to suffice.
This suggests that SRL playsa role in pronoun and common noun resolution,where surface features cannot account for complexpreferences and semantic knowledge is required.3.3 Feature EvaluationWe investigated the contribution of the differentfeatures in the learning process.
Table 5 showsthe chi-square statistic (normalized in the [0, 1] in-terval) for each feature occurring in the trainingdata of the MERGED dataset.
SRL features showa high ?2 value, ranking immediately after stringmatching and alias, which indicates a high corre-lation of these features to the decision classes.The importance of SRL is also indicated by theanalysis of the contribution of individual featuresto the overall performance.
Table 6 shows the per-formance variations obtained by leaving out eachfeature in turn.
Again, it can be seen that remov-ing both I and J SEMROLE induces a relativelyhigh performance degradation when compared toother features.
Their removal ranks 5th out of12, following only essential features such as stringmatching, alias, pronoun and number.
Similarlyto Table 5, the semantic role of the anaphor rankshigher than the one of the antecedent.
This re-145BNEWS NWIRER P F1 Ap Acn Apn R P F1 Ap Acn Apnbaseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.7 23.1 55.6+SRL 50.9 86.1 64.0 36.8 14.3 45.7 58.3 86.9 69.8 38.0 25.8 55.8Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)Feature(s) removed ?
F1all features 68.8STR MATCH ?21.02ALIAS ?2.96I/J PRONOUN ?2.94NUMBER ?1.63I/J SEMROLE ?1.50J SEMROLE ?1.26APPOSITIVE ?1.20GENDER ?1.13I SEMROLE ?0.74DIST ?0.69WN CLASS ?0.56DEF NP ?0.57DEM NP ?0.50PROPER NAME ?0.49Table 6: ?
F1 from feature removallates to the improved performance on pronouns, asit indicates that SRL helps for linking anaphoricpronouns to preceding REs.
Finally, it shouldbe noted that SRL provides much more solid andnoise-free semantic features when compared to theWordNet class feature, whose removal induces al-ways a lower performance degradation.4 ConclusionIn this paper we have investigated the effectsof using semantic role information within a ma-chine learning based coreference resolution sys-tem.
Empirical results show that coreference res-olution can benefit from SRL.
The analysis of therelevance of features, which had not been previ-ously addressed, indicates that incorporating se-mantic information as shallow event descriptionsimproves the performance of the classifier.
Thegenerated model is able to learn selection pref-erences in cases where surface morpho-syntacticfeatures do not suffice, i.e.
pronoun resolution.We speculate that this contrasts with the disap-pointing findings of Kehler et al (2004) since SRLprovides a more fine grained level of informationwhen compared to predicate argument statistics.As it models the semantic relationship that a syn-tactic constituent has with a predicate, it carries in-directly syntactic preference information.
In addi-tion, when used as a feature it allows the classifierto infer semantic role co-occurrence, thus induc-ing deep representations of the predicate argumentrelations for learning in coreferential contexts.Acknowledgements: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by aKTF grant (09.003.2004).ReferencesBerger, A., S. A. Della Pietra & V. J. Della Pietra (1996).
Amaximum entropy approach to natural language process-ing.
Computational Linguistics, 22(1):39?71.Carreras, X.
& L. Ma`rquez (2005).
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
InProc.
of CoNLL-05, pp.
152?164.Charniak, E. (1973).
Jack and Janet in search of a theoryof knowledge.
In Advance Papers from the Third Inter-national Joint Conference on Artificial Intelligence, Stan-ford, Cal., pp.
337?343.Chinchor, N. (2001).
Message Understanding Conference(MUC) 7.
LDC2001T02, Philadelphia, Penn: LinguisticData Consortium.Chinchor, N. & B. Sundheim (2003).
Message Understand-ing Conference (MUC) 6.
LDC2003T13, Philadelphia,Penn: Linguistic Data Consortium.Gildea, D. & D. Jurafsky (2002).
Automatic labeling of se-mantic roles.
Computational Linguistics, 28(3):245?288.Ji, H., D. Westbrook & R. Grishman (2005).
Using semanticrelations to refine coreference decisions.
In Proc.
HLT-EMNLP ?05, pp.
17?24.Kehler, A., D. Appelt, L. Taylor & A. Simma (2004).
The(non)utility of predicate-argument frequencies for pro-noun interpretation.
In Proc.
of HLT-NAACL-04, pp.
289?296.Klebanov, B.
& P. Wiemer-Hastings (2002).
The role ofwor(l)d knowledge in pronominal anaphora resolution.
InProceedings of the International Symposium on ReferenceResolution for Natural Language Processing, Alicante,Spain, 3?4 June, 2002, pp.
1?8.Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dod-dington, R. Grishman, A. Meyers, A. Brunstain, L. Ferro& B. Sundheim (2003).
TIDES Extraction (ACE) 2003Multilingual Training Data.
LDC2004T09, Philadelphia,Penn.
: Linguistic Data Consortium.Ng, V. & C. Cardie (2002).
Improving machine learning ap-proaches to coreference resolution.
In Proc.
of ACL-02,pp.
104?111.Palmer, M., D. Gildea & P. Kingsbury (2005).
The proposi-tion bank: An annotated corpus of semantic roles.
Com-putational Linguistics, 31(1):71?105.Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-sky (2004).
Shallow semantic parsing using support vectormachines.
In Proc.
of HLT-NAACL-04, pp.
233?240.Soon, W. M., H. T. Ng & D. C. Y. Lim (2001).
A ma-chine learning approach to coreference resolution of nounphrases.
Computational Linguistics, 27(4):521?544.Vilain, M., J. Burger, J. Aberdeen, D. Connolly &L. Hirschman (1995).
A model-theoretic coreference scor-ing scheme.
In Proceedings of the 6th Message Under-standing Conference (MUC-6), pp.
45?52.146
