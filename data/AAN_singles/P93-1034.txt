PART-OF-SPEECH INDUCTION FROM SCRATCHHinr ich  Sch i i t zeCenter  for the Study of Language and In format ionVentura HallStanford, CA 94305-4115schuetze~csl i .s tanford.eduAbst rac tThis paper presents a method for inducingthe parts of speech of a language and part-of-speech labels for individual words from alarge text corpus.
Vector representations forthe part-of-speech of a word are formed fromentries of its near lexical neighbors.
A dimen-sionality reduction creates a space represent-ing the syntactic ategories of unambiguouswords.
A neural net trained on these spa-tial representations classifies individual con-texts of occurrence of ambiguous words.
Themethod classifies both ambiguous and unam-biguous words correctly with high accuracy.INTRODUCTIONPart-of-speech information about individual wordsis necessary for any kind of syntactic and higherlevel processing of natural anguage.
While it iseasy to obtain lists with part of speech labels forfrequent English words, such information is notavailable for less common languages.
Even for En-glish, a categorization f words that is tailored to aparticular genre may be desired.
Finally, there arerare words that need to be categorized even if fre-quent words are covered by an available lectronicdictionary.This paper presents a method for inducing theparts of speech of a language and part-of-speechlabels for individual words from a large text cor-pus.
Little, if any, language-specific knowledge isused, so that it is applicable to any language inprinciple.
Since the part-of-speech representationsare derived from the corpus, the resulting catego-rization is highly text specific and doesn't containcategories that are inappropriate for the genre inquestion.
The method is efficient enough for vo-cabularies of tens of thousands of words thus ad-dressing the problem of coverage.The problem of how syntactic ategories can beinduced is also of theoretical interest in languageacquisition and learnability.
Syntactic categoryinformation is part of the basic knowledge aboutlanguage that children must learn before they canacquire more complicated structures.
It has beenclaimed that "the properties that the child candetect in the input - such as the serial positionsand adjacency and co-occurrence r lations amongwords - are in general inguistically irrelevant.
"(Pinker 1984) It will be shown here that relativeposition of words with respect o each other is suf-ficient for learning the major syntactic ategories.In the first part of the derivation, two iterationsof a massive linear approximation f cooccurrencecounts categorize unambiguous words.
Then aneural net trained on these words classifies indi-vidual contexts of occurrence of ambiguous words.An evaluation suggests that the method classi-fies both ambiguous and unambiguous words cor-rectly.
It differs from previous work in its effi-ciency and applicability to large vocabularies; andin that linguistic knowledge is only used in thevery last step so that theoretical ssumptions thatdon't hold for a language or sublanguage have min-imal influence on the classification.The next two sections describe the linear ap-proximation and a birecurrent neural network forthe classification of ambiguous words.
The lastsection discusses the results.CATEGORY SPACEThe goal of the first step of the induction is to com-pute a multidimensional real-valued space, calledcategory space, in which the syntactic ategory ofeach word is represented by a vector.
Proximity inthe space is related to similarity of syntactic at-egory.
The vectors in this space will then be usedas input and target vectors for the connectionistnet.The vector space is bootstrapped by collectingrelevant distributional information about words.The 5,000 most frequent words in five months ofthe New York Times News Service (June through251October 1990) were selected for the experiments.For each pair of these words < wi, w i >, the num-ber of occurrences of wi immediately to the left ofwj (hi,j), the number of occurrences of wi immedi-ately to the right ofwj (cij), the number of occur-rences of wl at a distance of one word to the left ofwj (ai,j), and the number of occurrences ofwi at adistance of one word to the right of wj (d/ j )  werecounted.
The four sets of 25,000,000 counts werecollected in the 5,000-by-5,000 matrices B, C, A,and D, respectively.
Finally these four matriceswere combined into one large 5,000-by-20,000 ma-trix as shown in Figure 1.
The figure also showsfor two words where their four cooccurrence ountsare located in the 5,000-by-20,000 matrix.
In theexperiments, w3000 was resistance and ~/24250 wastheaters.
The four marks in the figure, the posi-tions of the counts 1:13000,4250, b3000,4250, e3000,4250,and d3000,4~50, indicate how often resistance oc-curred at positions -2 ,  -1 ,  1, and 2 with respectto theaters.These 20,000-element rows of the matrix couldbe used directly to compute the syntactic similar-ity between individual words: The cosine of theangle between the vectors of a pair of words is ameasure of their similarity.
I However, computa-tions with such large vectors are time-consuming.Therefore a singular value decomposition was per-formed on the matrix.
Fifteen singular values werecomputed using a sparse matrix algorithm fromSVDPACK (Berry 1992).
As a result, each of the5,000 words is represented by a vector of real num-bers.
Since the original 20,000-component vectorsof two words (corresponding to rows in the ma-trix in Figure 1) are similar if their collocationsare similar, the same holds for the reduced vectorsbecause the singular value decomposition finds thebest least square approximation for the 5,000 orig-inal vectors in a 15-dimensional space that pre-serves imilarity between vectors.
See (Deerwesteret al 1990) for a definition of SVD and an appli-cation to a similar problem.Close neighbors in the 15-dimensional spacegenerally have the same syntactic ategory as canbe seen in Table 1.
However, the problem with thismethod is that it will not scale up to a very largenumber of words.
The singular value decomposi-tion has a time complexity quadratic in the rankof the matrix, so that one can only treat a smallpart of the total vocabulary of a large corpus.Therefore, an alternative set of features was con-sidered: classes of words in the 15-dimensionalspace.
Instead of counting the number of occur-rences of individual words, we would now count1The cosine between two vectors corresponds tothe normalized correlation coefficient: cos(c~(~,ff)) =the number of occurrences of members of wordclasses.
2 The space was clustered with Buckshot, alinear-time clustering algorithm described in (Cut-ting et al 1992).
Buckshort applies a high-qualityquadratic lustering algorithm to a random sam-ple of size v/k-n, where k is the number of desiredcluster centers and n is the number of vectors tobe clustered.
Each of the remaining n - ~ vec-tors is assigned to the nearest cluster center.
Thehigh-quality quadratic clustering algorithm usedwas truncated group average agglomeration (Cut-ting et al 1992).Clustering algorithms generally do not con-struct groups with just one member.
But thereare many closed-class words such as auxiliaries andprepositions that shouldn't be thrown togetherwith the open classes (verbs, nouns etc.).
There-fore, a list of 278 closed-class words, essentially thewords with the highest frequency, was set aside.The remaining 4722 words were classified into 222classes using Buckshot.The resulting 500 classes (278 high-frequencywords, 222 clusters) were used as features in thematrix shown in Figure 2.
Since the number offeatures has been greatly reduced, a larger num-ber of words can be considered.
For the secondmatrix all 22,771 words that occurred at least 100times in 18 months of the New York Times NewsService (May 1989 - October 1990) were selected.Again, there are four submatrices, correspondingto four relative positions.
For example, the entriesaij  in the A part of the matrix count how oftena member of class i occurs at a distance of oneword to the left of word j.
Again, a singular valuedecomposition was performed on the matrix, thistime 10 singular values were computed.
(Note thatin the first figure the 20,000-element rows of thematrix are reduced to 15 dimensions whereas inthe second matrix the 2,000-element columns arereduced to 10 dimensions.
)Table 2 shows 20 randomly selected words andtheir nearest neighbors in category space (in orderof proximity to the head word).
As can be seenfrom the table, proximity in the space is a goodpredictor of similar syntactic ategory.
The near-est neighbors of athlete, clerk, declaration, anddome are singular nouns, the nearest neighborsof bowers and gibbs are family names, the near-est neighbors of desirable and sole are adjectives,and the nearest neighbors of financings are plu-ral nouns, in each case without exception.
Theneighborhoods of armaments, cliches and luxuries(nouns), and b'nai and northwestern (NP-initialmodifiers) fail to respect finer grained syntactic2Cf.
(Brown et al 1992) where the same idea ofimproving generalization and accuracy by looking atword classes instead of individual words is used.2524250A+IB+IC+ID+I3000 3000 3000 3000Figure 1: The setup o f thematr ix fo r the  first singular value decomposition.Table 1: Ten random and three selected words and their nearest neighbors in category space 1.wordaccompaniedalmostcausingclassesdirectorsgoaljapaneserepresentthinkyorknearest neighborssubmitted banned financed eveloped authorized headed canceled awarded barredvirtually merely formally fully quite officially just nearly only lessreflecting forcing providing creating producing becoming carrying particularlyelections courses payments losses computers performances violations levels picturesprofessionals investigations materials competitors agreements papers transactionsmood roof eye image tool song pool scene gap voicechinese iraqi american western arab foreign european federal soviet indianreveal attend deliver reflect choose contain impose manage stablish retainbelieve wish know realize wonder assume feel say mean betangeles francisco sox rouge kong diego zone vegas inning layerOilmustthrough in at over into with from for by acrosswe you i he she nobody who it everybody there theymight would could cannot will should can may does helps500 features500 features500 features500 featuresABCD22,771 wordsFigure 2: The setup of the matrix for the second singular value decomposition.253Table 2: Twenty random and four selected words and their neigborhoods in category space 2.wordarmamentsathleteb'nalbowersclerkclichescruzdeclarationdesirabledomeequallyfinancingsgibbsluxuriesnorthwesternohsolenearest neighborsturmoil weaponry landmarks coordination prejudices ecrecy brutality unrest harassment \[virus scenario \[ event audience disorder organism candidate procedure pidemicI suffolk sri allegheny cosmopolitan berkshire cuny broward multimedia bovine nytimesjacobs levine cart hahn schwartz adams bucldey dershowitz fitzpatrick peterson \[salesman \] psychologist photographer p eacher mechanic dancer lawyer trooper trainerpests wrinkles outbursts treams icons endorsements I friction unease appraisals lifestylesantonio I' clara pont saud monica paulo rosa mae attorney palmasequence mood profession marketplace concept facade populace downturn moratorium Ire'cognizable I frightening loyal devastating excit!ng troublesome awkward palpableblackout furnace temblor quartet citation chain countdown thermometer shaft II somewhat progressively acutely enormously excessively unnecessarily largely scattered\[ endeavors monopolies raids patrols stalls offerings occupations philosophies religionsadler reid webb jenkins stevens carr lanrent dempsey hayes farrell \[volatility insight hostility dissatisfaction stereotypes competence unease animosity residues \]transportsvividlywalks\[ baja rancho harvard westchester ubs humboldt laguna guinness vero granadagee gosh ah hey I appleton ashton dolly boldface baskin loI lengthy vast monumental rudimentary nonviolent extramarital lingering meager gruesomeI spokesman copyboy staffer barrios comptroller alloy stalks spokeswoman dal spokespersonIskillfully frantically calmly confidently streaming relentlessly discreetly spontaneouslyfloats \[ jumps collapsed sticks stares crumbled peaked disapproved runs crashedclaimsOilmusttheycredits promises \[ forecasts shifts searches trades practices processes supplements controlsthrough from in \[ at by 'within with under against forwill might would cannot could can should won't \[ doesn't maywe \[ i you who nobody he it she everybody theredistinctions, but are reasonable representations ofsyntactic category.
The neighbors of cruz (sec-ond components ofnames), and equally and vividly(adverbs) include words of the wrong category, butare correct for the most part.In order to give a rough idea of the density ofthe space in different locations, the symbol "1" isplaced before the first neighbor in Table 2 thathas a correlation of 0.978 or less with the headword.
As can be seen from the table, the re-gions occupied by nouns and proper names aredense, whereas adverbs and adjectives have moredistant nearest neighbors.
One could attempt ofind a fixed threshold that would separate neigh-bors of the same category from syntactically dif-ferent ones.
For instance, the neighbors of oh witha correlation higher than 0.978 are all interjectionsand the neighbors of cliches within the thresholdregion are all plural nouns.
However, since thedensity in the space is different for different re-gions, it is unlikely that a general threshold for allsyntactic ategories can be found.The neighborhoods of transports and walks arenot very homogeneous.
These two words areambiguous between third person singular presenttense and plural noun.
Ambiguity is a problemfor the vector epresentation scheme used here, be-cause the two components of an ambiguous vectorcan add up in a way that makes it by chance simi-lar to an unambiguous word of a different syntacticcategory.
If we call the distributional vector fi'?
ofwords of category c the profile of category c, andif a word wl is used with frequency c~ in categorycl and with frequency ~ in category c2, then theweighted sum of the profiles (which corresponds toa column for word Wl in Figure 2) may turn outto be the same as the profile of an unrelated thirdcategory c3:This is probably what happened in the cases oftransports and walks.
The neighbors of claimsdemonstrate that there are homogeneous "am-biguous" regions in the space if there are enoughwords with the same ambiguity and the same fre-quency ratio of the categories, lransports andwalks (together with floats, jumps, sticks, stares,and runs) seem to have frequency ratios a/fl dif-ferent from claims, so that they ended up in dif-ferent regions.The last three lines of Table 2 indicate that func-tion words such as prepositions, auxiliaries, andnominative pronouns and quantifiers occupy theirown regions, and are well separated from eachother and from open classes.254A B IRECURRENT NETWORKFOR PART-OF-SPEECHPREDICT IONA straightforward way to take advantage of thevector representations for part of speech catego-rization is to cluster the space and to assign part-of-speech labels to the clusters.
This was donewith Buckshot.
The resulting 200 clusters yieldedgood results for unambiguous words.
However, forthe reasons discussed above (linear combination ofprofiles of different categories) the clustering wasnot very successful for ambiguous words.
There-fore, a different strategy was chosen for assigningcategory labels.
In order to tease apart the differ-ent uses of ambiguous words, one has to go back tothe individual contexts of use.
The connectionistnetwork in Figure 3 was used to analyze individualcontexts.The idea of the network is similar to Elman's re-current networks (Elman 1990, Elman 1991): Thenetwork learns about the syntactic structure of thelanguage bY trying to predict the next word fromits own context units in the previous tep and thecurrent word.
The network in Figure 3 has twonovel features: It uses the vectors from the secondsingular vMue decomposition as input and target.Note that distributed vector representations areideal for connectionist nets, so that a connection-ist model seems most appropriate for the predic-tion task.
The second innovation is that the netis birecurrent.
It has recurrency to the left as wellas to the right.In more detail, the network's input consists ofthe word to the left tn-1, its own left context in theprevious time step c-l,,-1, the word to the righttn+l and its own right context C-rn+l in the nexttime step.
The second layer has the context unitsof the current time step.
These feed into thirtyhidden units h,~ which in turn produce the outputvector o,,.
The target is the current word tn.
Theoutput units are linear, hidden units are sigmoidM.The network was trained stochastically withtruncated backpropagation through time (BPTT,Rumelhart et al 1986, Williams and Peng 1990).For this purpose, the left context units were un-folded four time steps to the left and the right con-text units four time steps to the right as shownin Figure 4.
The four blocks of weights on theconnections to c-in-3, c-ln-~., c-in-l, and c-Inare linked to ensure identical mapping from one"time step" to the next.
The connections on theright side are linked in the same way.
The train-ing set consisted of 8,000 words in the New YorkTimes newswire (from June 1990).
For each train-ing step, four words to the left of the target word(tn_3, tn_2,tn_l, and in) and four words to theright of the target word (tn, tn+l, tn+2, and in+3)FU:qI h.
I.--q,+-z\],+-;71Figure 4: Unfolded birecurrent network in train-ing.were the input to the unfolded network.
The tar-get was the word tn.
A modification of bp fromthe pdp package was used with a learning rate of0.01 for recurrent units, 0.001 for other units andno momentum.After training, the network was applied tothe category prediction tasks described below bychoosing a part of the text without unknownwords, computing all left contexts from left toright, computing all right contexts from right toleft, and finally predicting the desired category ofa word t ,  by using the precomputed contexts c-l,,and c-rn.In order to tag the occurrence of a word, onecould retrieve the word in category space whosevector is closest o the output vector computed bythe network.
However, this would give rise to toomuch variety in category labels.
To illustrate, con-sider the prediction of the category NOUN.
If thenetwork categorizes occurrences of nouns correctlyas being in the region around declaration, then theslightest variation in the output will change thenearest neighbor of the output vector from decla-ration to its nearest neighbors equence or mood(see Table 2).
This would be confusing to the hu-man user of the categorization program.Therefore, the first 5,000 output vectors of thenetwork (from the first day of June 1990), wereclustered into 200 output clusters with Buckshot.Each output cluster was labeled by the two wordsclosest o its centroid.
Table 3 lists labels of someof the output clusters that occurred in the ex-periment described below.
They are easily in-terpretable for someone with minimal linguisticknowledge as the examples how.
For some cat-egories such as HIS_THI~.
one needs to look at acouple of instances to get a "feel" for their mean-255I , t n  (10) II o-(lO) I\[ h. (30)It,,-, (10) I \[ C-In-, (15)I I ~n+'l (15) {Figure 3: The architecture of the birecurrent networkTable 3: The labels of 10 output clusters.output cluster labelexceLdepartprompt_selectcares_sonndsoffice_staffpromotion_traumafamous_talentedpublicly_badlyhis_thepart of speechintransitive verb (base form)transitive verb (base form)3. person sg.
present ensenounnounadjectiveadverbNP-initialing.The syntactic distribution of an individual wordcan now be more accurately determined by thefollowing algorithm:?
compute an output vector for each position inthe text at which the target word occurs.?
for each output vector j do the following:- determine the centroid of the cluster i whichis closest- compute the correlation coefficient of the out-put vector j and the centroid of the outputcluster i.
This is the score si,i for cluster iand vector j .
Assign zero to the scores of theother clusters for this vector: s~,j : -  0, k ~ i?
for each cluster i, compute the final score fi asthe sum of the scores si j  : fi := ~ j  si,j?
normalize the vector of 200 final scores to unitlengthThis algorithm was applied to June 1990.
If fora given word, the sum of the unnormalized finalscores was less than 30 (corresponding to roughly100 occurrences in June), then this word was dis-carded.
Table 4 lists the highest scoring categoriesfor 10 random words and 11 selected ambiguouswords.
(Only categories with a score of at least0.2 are listed.
)The network failed to learn the distinctions be-tween adjectives, intransitive present participlesand past participles in the frame "to-be + \[\] +non-NP'.
For this reason, the adjective close, thepresent participle beginning, and the past partici-ple shot are all classified as belonging to the cate-gory STRUGGLING_TRAVELING.
(Present Partici-ples are successfully discriminated in the frame"to-be + \[\] + NP": see winning in the table, whichis classified as the progressive form of a transitiveverb: HOLDING_PROMISING.)
This is the placewhere linguistic knowledge has to be injected inform of the following two rules:?
If a word in STRUGGLING_TRAVELING is a mor-phological present participle or past participleassign it to that category, otherwise to the cat-egory ADJECTIVE_PREDICATIVE.
* If a word in a noun category is a morpho-logical plural assign it to NOUN_PLURAL, toNOUN_SINGULAR otherwise.With these two rules, all major categoriesare among the first found by the algorithm;in particular the major categories of the am-biguous words better (adjective/adverb), close(verb/adjective), work (noun/base form of verb),hopes (noun/third person singular), beginning(noun/present-participle), shot (noun/past par-ticiple) and 's  ('s/is).
There are two clear errors:GIVEN_TAKING for contain, and RICAN_ADVISORYfor 's, both of rank three in the table.256Tablewordadequateadmitappointconsensuscontaindodgersgeneslanguagelegacythirdsgoodbettercloseworkhospitalbuyhopesbeginningshot'Swinning4: The highest scoring categories for 10 random and 11 selected words.highest scoring categoriesuniversal_martial (0.50)excel_depart (0.88)prompt_select (0.72)office_staff (0.71)gather_propose (0.76)promotion_trauma (0.57)office_staff (0.43)promotion_trauma (0.65)promotion_trauma (0.95)hand_shooting (0.75)famous_talented (0.86)famous_talented (0.65)gather_propose (0.43)exceLdepart (0.72)promotion_trauma (0.75)gather_propose (0.77)promotion_trauma (0.56)promotion_trauma (0.90)hand_shooting (0.54)'s_f~cto (0.54)famous_talented (0.71)struggling_traveling (0.33)gather_propose (0.30)gather_propose (0.65)promotion_trauma (0.43)prompt_select (0.43)yankees_paper (0.52)promotion_trauma (0.75)office_staff (0.57)office_staff (0.22)famous_talented (0.41)his_the (0.34)struggling_traveling (0.42)promotion_trauma (0.51)office_agent (0.40)prompt_select (0.47)cares.sounds (0.53)struggling_travehng (0.34)struggling_traveling (0.45)makes_is (0.40)holding_promising (0.33)several_numerous (0.33)prompt_select (0.20)hand_shooting (0.39)given_taking (0.24)fantasy_ticket (0.48)route_style (0.22)office_agent (0.21)iron_pickup (0.36)_pubhcly_badly (0.27)famous_talented (0.36)remain_want (0.27)fantasy_ticket (0.24)remain_want (0.22)windows_pictures (0.21)promotion_trauma (0.40)rican_advisory (0.~7)iron_pickup (0.29)These results seem promising iven the fact thatthe context vectors consist of only 15 units.
Itseems naive to believe that all syntactic informa-tion of the sequence of words to the left (or to theright) can be expressed in such a small numberof units.
A larger experiment with more hiddenunits for each context vector will hopefully yieldbetter results.D ISCUSSION AND CONCLUSIONBrill and Marcus describe an approach with simi-lar goals in (Brill and Marcus 1992).
Their methodrequires an initial consultation of a native speakerfor a couple of hours.
The method presented heremakes a short consultation of a native speaker nec-essary, however it occurs at the end, as the laststep of category induction.
This has the advantageof avoiding bias in an initial a priori classification.Finch and Chater present an approach to cat-egory induction that also starts out with offsetcounts, proceeds by classifying words on the ba-sis of these counts, and then goes back to the lo-cal context for better results (Finch and Chater1992).
But the mathematical nd computationaltechniques used here seem to be more efficient andmore accurate than Finch and Chater's, and henceapplicable to vocabularies of a more realistic size.An important feature of the last step of the pro-cedure, the neural network, is that the lexicogra-pher or linguist can browse the space of outputvectors for a given word to get a sense of its syn-tactic distribution (for instance uses of better asan adverb) or to improve the classification (for in-stance by splitting an induced category that is toocoarse).
The algorithm can also be used for cate-gorizing unseen words.
This is possible as long asthe words surrounding it are known.The procedure for part-of-speech ategorizationintroduced here may be of interest even for wordswhose part-of-speech labels are known.
The di-mensionality reduction makes the global distribu-tional pattern of a word available in a profile con-sisting of a dozen or so real numbers.
Becauseof its compactness, this profile can be used effi-ciently as an additional source of information forimproving the performance of natural languageprocessing systems.
For example, adverbs maybe lumped into one category in the lexicon of aprocessing system.
But the category vectors ofadverbs that are used in different positions uchas completely (mainly pre~adjectival), normally(mainly pre-verbal) and differently (mainly post-verbal) are different because of their different dis-tributional properties.
This information can beexploited by a parser if the category vectors areavailable as an additional source of information.The model has also implications for languageacquisition.
(Maratsos and Chalkley 1981) pro-pose that the absolute position of words in sen-tences is important evidence in children's learn-ing of categories.
The results presented here showthat relative position is sufficient for learning themajor syntactic ategories.
This suggests that rel-ative position could be important information forlearning syntactic ategories in child language ac-quisition.The basic idea of this paper is to collect a257large amount of distributional information con-sisting of word cooccurrence counts and to com-pute a compact, low-rank approximation.
Thesame approach was applied in (Sch/itze, forth-coming) to the induction of vector epresentationsfor semantic information about words (a differ-ent source of distributional information was usedthere).
Because of the graded information presentin a multi-dimensional space, vector representa-tions are particularly well-suited for integratingdifferent sources of information for disambigua-tion.In summary, the algorithm introduced here pro-vides a language-independent, largely automaticmethod for inducing highly text-specific syntacticcategories for a large vocabulary.
It is to be hopedthat the method for distributional analysis pre-sented here will make it easier for computationaland traditional lexicographers to build dictionar-ies that accurately reflect language use.ACKNOWLEDGMENTSI'm indebted to Mike Berry for SVDPACK andto Marti Hearst, Jan Pedersen and two anony-mous reviewers for very helpful comments.
Thiswork was partially supported by the National Cen-ter for Supercomputing Applications under grantBNS930000N.REFERENCESBerry, Michael W. 1992.
Large-scale sparse singu-lar value computations.
The International Jour-nal of Supercomputer Applications 6(1):13-49.Brill, Eric, and Mitch Marcus.
1992.
Tagging anUnfamiliar Text with Minimal Human Supervi-sion.
In Working Notes of the AAAI Fall Sym-posium on Probabilistic Approaches to NaturalLanguage, ed.
Robert Goldman.
AAAI Press.Brown, Peter F., Vincent J. Della Pietra, Pe-ter V. deSouza, Jenifer C. Lai, and Robert L.Mercer.
1992.
Class-Based n-gram Models ofNatural Language.
Computational Linguistics18(4):467-479.Cutting, Douglas R., Jan O. Pedersen, DavidKarger, and John W. Tukey.
1992.
Scat-ter/Gather: A Cluster-based Approach toBrowsing Large Document Collections.
In Pro-ceedings of SIGIR '92.Deerwester, Scott, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and RichardHarshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society forInformation Science 41(6):391-407.Elman, Jeffrey L. 1990.
Finding Structure inTime.
Cognitive Science 14:179-211.Elman, Jeffrey L. 1991.
Distributed Repre-sentations, Simple Recurrent Networks, andGrammatical Structure.
Machine Learning7(2/3):195-225.Finch, Steven, and Nick Chater.
1992.
Boot-strapping Syntactic Categories Using Statisti-cal Methods.
In Background and Experimentsin Machine Learning of Natural Language, ed.Walter Daelemans and David Powers.
TilburgUniversity.
Institute for Language Technologyand AI.Maratsos, M. P., and M. Chalkley.
1981.
The inter-nal language of children's yntax: the ontogene-sis and representation f syntactic ategories.
InChildren's language, ed.
K. Nelson.
New York:Gardner Press.Pinker, Steven.
1984.
Language Learnability andLanguage Development.
Cambridge MA: Har-vard University Press.Rumelhart, D. E., G. E. Hinton, and R. J.Williams.
1986.
Learning Internal Representa-tions by Error Propagation.
In Parallel Dis-tributed Processing.
Explorations in the Mi-crostructure of Cognition.
Volume I: Founda-tions, ed.
David E. Rumelhart, James L. Mc-Clelland, and the PDP Research Group.
Cam-bridge MA: The MIT Press.Schiitze, Hinrich.
Forthcoming.
Word Space.
InAdvances in Neural Information Processing Sys-tems 5, ed.
Stephen J. Hanson, Jack D. Cowan,and C. Lee Giles.
San Mateo CA: Morgan Kauf-mann.Williams, Ronald J., and Jing Peng.
1990.
An Ef-ficient Gradient-Based Algorithm for On-LineTraining of Recurrent Network Trajectories.Neural Computation 2:490-501.258
