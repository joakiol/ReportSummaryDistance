Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 711?720,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEffective Greedy Inference for Graph-basedNon-Projective Dependency ParsingIlan Tchernowitz Liron YedidsionFaculty of Industrial Engineering and Management, Technion, IIT{ilantc@campus|lirony@ie|roiri@ie}.technion.ac.ilRoi ReichartAbstractExact inference in high-order graph-basednon-projective dependency parsing is in-tractable.
Hence, sophisticated approximationtechniques based on algorithms such as be-lief propagation and dual decomposition havebeen employed.
In contrast, we propose a sim-ple greedy search approximation for this prob-lem which is very intuitive and easy to im-plement.
We implement the algorithm withinthe second-order TurboParser and experimentwith the datasets of the CoNLL 2006 and 2007shared task on multilingual dependency pars-ing.
Our algorithm improves the run time ofthe parser by a factor of 1.43 while losing 1%in UAS on average across languages.
More-over, an ensemble method exploiting the jointpower of the parsers, achieves an average UAS0.27% higher than the TurboParser.1 IntroductionDependency parsing is instrumental in NLP appli-cations, with recent examples in information extrac-tion (Wu and Weld, 2010), word embeddings (Levyand Goldberg, 2014), and opinion mining (Almeidaet al, 2015).
The two main approaches for this taskare graph based (McDonald et al, 2005) and transi-tion based (Nivre et al, 2007).The graph based approach aims to optimize aglobal objective function.
While exact polyno-mial inference algorithms exist for projective pars-ing (Eisner, 1996; McDonald et al, 2005; Carreras,2007; Koo and Collins, 2010, inter alia), high ordernon-projective parsing is NP-hard (McDonald andPereira, 2006).
The current remedy for this comes inthe form of advanced optimization techniques suchas dual decomposition (Martins et al, 2013), LP re-laxations (Riedel et al, 2012), belief propagation(Smith and Eisner, 2008; Gormley et al, 2015) andsampling (Zhang et al, 2014b; Zhang et al, 2014a).The transition based approach (Zhang and Nivre,2011; Bohnet and Nivre, 2012; Honnibal et al,2013; Choi and McCallum, 2013a, inter alia), andthe easy first approach (Goldberg and Elhadad,2010) which extends it by training non-directionalparsers that consider structural information fromboth sides of their decision points, lack a global ob-jective function.
Yet, their sequential greedy solversare fast and accurate in practice.We propose a greedy search algorithm for high-order, non-projective graph-based dependency pars-ing.
Our algorithm is a simple iterative graph-basedmethod that does not rely on advanced optimizationtechniques.
Moreover, we factorize the graph-basedobjective into a sum of terms and show that our basicgreedy algorithm relaxes the global objective by se-quentially optimizing these terms instead of globallyoptimizing their sum.Unlike previous greedy approaches to depen-dency parsing, transition based and non-directional,our algorithm does not require a specialized featureset or a training method that specializes in local deci-sions.
In contrast, it supports global parameter train-ing based on the comparison between an inducedtree and the gold tree.
Hence, it can be integratedinto any graph-based parser.We first present a basic greedy algorithm that re-laxes the global graph-based objective (Section 3).However, as this simple algorithm does not provide a711realistic estimation of the impact of an arc selectionon uncompleted high-order structures in the partialparse forest, it is not competitive with state of theart approximations.
We hence present an advancedversion of our algorithm with an improved arc scoreformulation and show that this simple algorithm pro-vides high quality solutions to the graph-based infer-ence problem (Section 4).Particularly, we implement the algorithm withinthe TurboParser (Martins et al, 2013) and exper-iment (Sections 8 and 9) with the datasets of theCoNLL 2006-2007 shared tasks on multilingual de-pendency parsing (Buchholz and Marsi, 2006; Nils-son et al, 2007).
On average across languages ourparser achieves UAS scores of 87.78% and 89.25%for first and second order parsing respectively, com-pared to respective UAS of 87.98% and 90.26%achieved by the original TurboParser.We further implement (Section 6) an ensemblemethod that integrates information from the outputtree of the original TurboParser and the arc weightslearned by our variant of the parser into our searchalgorithm to generate a new tree.
This yields an im-provement: average UAS of 88.03% and 90.53% forfirst and second parsing, respectively.Despite being greedy, the theoretical runtimecomplexity of our advanced algorithm is not betterthan the best previously proposed approximationsfor our problem (O(nk+1), for nword sentences andk order parsing, Section 5).
In experiments, our al-gorithms improve the runtime of the TurboParser bya factor of up to 2.41.The main contribution of this paper is hence inproviding a simple, intuitive and easy to implementsolution for a long standing problem that has beenaddressed in past with advanced optimization tech-niques.
Besides the intellectual contribution, webelieve this will make high-order graph-based de-pendency parsing accessible to a much broader re-search and engineering community as it substan-tially relaxes the coding and algorithmic proficiencyrequired for the implementation and understandingof parsing algorithms.2 Problem FormulationWe start with a brief definition of the high ordergraph-based dependency parsing problem.
Given ann word input sentence, an input graph G = (V,E)is defined.
The set of vertices is V = {0, ..., n},with the {1, .
.
.
, n} vertices representing the wordsof the sentence, in their order of appearance, and the0 vertex is a specialized root vertex.
The set of arcsis E = {(u, v) : u ?
{0, ..., n}, v ?
{1, ..., n}, u 6=v}, that is, the root vertex has no incoming arcs.We further define a part of order k to be a subsetof E of size k, and denote the set of all parts withparts.
For the special case of k = 1 a part is anarc.
Different works employed different parts sets(e.g.
(Martins et al, 2013; McDonald et al, 2005;Koo and Collins, 2010)).
Generally, most parts setsconsist of arcs connecting vertices either vertically(e.g.
{(u, v), (v, z)} for k = 2) or horizontally (e.g.
{(u, v), (u, z)}, for k = 2).
In this paper we focuson the parts employed by (Martins et al, 2013), astate-of-the-art parser, but our algorithms are gener-ally applicable for any parts set consistent with thisgeneral definition.1In graph-based dependency parsing, each part pis given a score Wp ?
R. A Dependency Tree(DT) T is a subset of arcs for which the followingconditions hold: (1) Every vertex, except for theroot, has an incoming arc: ?v ?
V \ {0} : ?u ?V s.t.
(u, v) ?
T ; (2) No vertex has multiple incom-ing arcs: ?
(u, u?, v) ?
V, (u, v) ?
T ?
(u?, v) /?
T ;and (3) There are no cycles in T .
The score of a DTT is finally defined by:score(T ) =?part?TWpartThe inference problem in this model is to find thehighest scoring DT in the input weighted graph.3 Basic Greedy InferenceWe start with a basic greedy algorithm (Algorithm1), analyze the approximation it provides for thegraph-based objective and its inherent limitations.1More generally, a part is defined by two arc subsets, A andB, such that a part p belongs to a tree T if ?e ?
A : e ?T and ?e ?
B : e /?
T .
In this paper we assume B = ?.Hence, we cannot experiment with the third order TurboParseras in all its third order parts B 6= ?.
Also, when we integrateour algorithms into the second order TurboParser we omit thenextSibling part for whichB 6= ?.
For the original TurboParserto which we compare our results, we do not omit this part as itimproves the parser?s performance.712Algorithm 1 maintains a partial tree data struc-ture, T i, to which it iteratively adds arcs from theinput graph G, one in each iteration, until a depen-dency tree Tn is completed.
For this end, in everyiteration, i, a value, vie, composed of lossie and gainieterms, is computed for every arc e ?
E and the arcwith the lowest vie value is added to T i?1 to createthe extended partial tree T i.Due to the aforementioned conditions on the in-duced dependency tree, every arc that is added toT i?1 yields a set of lostArcs and lostParts that can-not be added to the partial tree in subsequent itera-tions.
The loss value is defined to be:lossie :=?part?lostPartsWpartThat is, every part that contains one or more arcsthat violate the dependency tree conditions for a treethat extends the partial tree T i?1?
{e} is considereda lost part as it will not be included in any tree ex-tending T i?1?{e}.
The loss value sums the weightsof these parts.Likewise, the gain value is the sum of the weightsof all the parts that are added to T i?1 when we adde to it.
Denote this set of parts with Pe := {part :part ?
T i?1 ?
{e}, part /?
T i?1}, then:gainie :=?part?PeWpartFinally, vie is given by:vie = lossie ?
gainieAfter the arc with the minimal value vie is added toT i?1, the arcs that violate the structural constraintson dependency trees are removed from G.An example of an update iteration of the algo-rithm (lines 3-16) is given in Figure 1.
In this ex-ample we consider two types of parts: first-order,arc, parts (ARC) and second-order grandparent parts(GP), consisting of arc pairs, {(g, u), (u, v)}.
Theupper graph shows the partial tree T 2 (solid arcs) aswell as the rest of the graph G (dashed arcs).
Theparts included in T 2 are ARC(0,2), ARC(2,1) andGP[(0,2),(2,1)].
The table contains the weights ofthe parts and the values computed for the arcs dur-ing the third iteration.
The arc that is chosen is (2,3),as it has the minimal v3e value.
Thus, in the bottomroot John walked home0 1 2 3part weight loss3e gain3e v3eARC(0,3) 2 3.5 2 1.5ARC(1,3) 1 5.5 0 5.5ARC(2,3) 1.5 2 3.5 -1.5GP[(0,2)(2,3)] 2 ?
?
?GP[(2,1)(1,3)] -1 ?
?
?root John walked home0 1 2 3Figure 1: An example of an iteration of Algorithm 1(lines 3-16)).
See description in text.graph that corresponds to T 3 all other incoming arcsto vertex 3 are removed.
(in this instance there areno cycle forming arcs).Analysis We now turn to an analysis of therelaxation that Algorithm 1 provides for the globalgraph-based objective.
Recall that our objective initeration i is: viei = min{vie}.
For the inferred treeTn it holds that:?ei?Tnviei ?
?part?GWpart =?part*TnWpart ?
?part?TnWpart ?
?part?GWpart =?
2?
?part?TnWpart +?part*TnWpart ?
?part*TnWpart =?
2?
?part?TnWpartThe first equation holds since ?ei?Tn viei is thesum of all lost parts (parts that are not in Tn) minusall the gained parts (parts in Tn).
Each of these partswas counted exactly once: when the part was addedto the partial tree or when one of its arcs was re-moved from G. The second equation splits the termof?part?GWpart to two sums, one over parts in Tnand the other over the rest.
Since ?part?GWpartand 2 are constants, we get:argminTn(?
?part?TnWpart) = argminTn?ei?TnvieiFrom this argument it follows that our inferencealgorithm performs sequential greedy optimizationover the presented factorization of the graph-based713objective instead of optimizing the sum of terms,and hence the objective, globally.The main limitation of Algorithm 1 is that it doesnot take into account high order parts contributionuntil the part is actually added to T .
For exam-ple, in Figure 1, when the arc (2, 1) is added, thepart GP[(2,1),(1,3)] is getting closer to completion.Yet, this is not taken into account when consideringwhether (2, 1) should be added to the tree or not.
In-cluding this information in the gain and loss valuesof an arc can improve the accuracy of the algorithm,especially in high-order parsing.Algorithm 1 Basic Greedy Inference1: T 0 = {}2: for i ?
1..n do3: for e = (u, v) ?
E do4: Pe := {part ?
parts : part ?
T i?1 ?
{e}, part * T i?1}5: gainie :=?part?Pe Wpart6: incomingSet := {(u?, v) ?
E : u?
6= u}7: cycleSet := {(u?, v?)
?
E : T i?1 ?
{e} ?
(u?, v?)
contains a cycle}8: lostArcs = (incomingSet ?
cycleSet)9: lostParts = {part : ?e ?
lostArcs ?
part}10: lossie :=?part?lostPartsWpart11: vie := lossie ?
gainie12: end for13: ei = (ui, vi) = argmine?{vie?
}14: T i = T i?1 ?
{ei}15: remove from G all incoming arcs to vi16: remove from G all cycle forming arcs w.r.t T i17: end for4 Greedy Inference with Partial PartPredictionsIn order for the algorithm to account for informationabout partial high order parts, we estimate the prob-ability that such parts would be eventually includedin Tn.
Our way to do this (Algorithm 2) is by es-timating these probabilities for arcs and from thesederive parts probabilities.Particularly, for the set of incoming arcs of a ver-tex v, Ev = {e = (u, v) : e ?
E}, a probabilitymeasure pe is computed according to:pe=(u,v) =exp?
?We?e?=(u?,v) exp?
?We?Where ?
is a hyper parameter of the model.
For?
= 0 we get a uniform distribution over all possibleAlgorithm 2 Greedy Inference with Partial Part Pre-dictions1: T 0 = {}2: for i ?
1..n do3: for e = (u, v) ?
E do4: Pe := {part ?
parts : e ?
part}5: gainie :=?part?Pe Wpart?
ppart|(Ti?1 ?
{e})6: incomingSet := {(u?, v) ?
E : u?
6= u}7: cycleSet := {(u?, v?)
?
E : T i?1 ?
{e} ?
(u?, v?)
contains a cycle}8: lostArcs = (incomingSet ?
cycleSet)9: lostParts = {part : ?e ?
lostArcs ?
part}10: lossie :=?part?lostPartsWpart ?
ppart|T i?111: vie := ?
?
lossie ?
(1?
?)?
gainie12: end for13: ei = (ui, vi) = argmine?{vie?
}14: T i = T i?1 ?
{ei}15: remove from G all incoming arcs to vi16: remove from G all cycle forming arcs w.r.t T i17: end forheads of a vertex v, and for large ?
values arcs withlarger weights get higher probabilities.The intuition behind this measure is that arcsmostly compete with other arcs that have the sametarget vertex and hence their weight should be nor-malized accordingly.
Using this measure, we definethe arc-factored probability of a part to be:ppart =?e?partpeAnd the residual probability of a part given an exist-ing partial tree T :ppart|T =ppart?e?
(part?T ) peThese probability measures are used in both thegain and the loss computations (lines 5 and 10 inAlgorithm 2) as follows:gainie :=?part:e?partWpart ?
ppart|(T i?1 ?
{e})lossie :=?part?lostPartsWpart ?
ppart|T i?1Finally, as adding an arc to the dependency sub-tree results in an exclusion of several arcs, the num-ber of lost parts is also likely to be much higherthan the number of gained parts.
In order to com-pensate for this effect, we introduce a balancing714hyper-parameter, ?
?
[0, 1], and change the com-putation of vie (line 10 in Algorithm 2) to be: vie :=?
?
lossie ?
(1?
?)?
gainie.5 Runtime Complexity AnalysisIn this section we provide a sketch of the runtimecomplexity analysis of the algorithm.
Full detailsare in appendix A.
In what follows, we denote themaximal indegree of a vertex with nin.Algorithm 1 Algorithm 1 consists of two nestedloops (lines 2-3) and hence lines 4-11 are repeatedO (n?
|E|) = O(n?n?nin) times.
At each repe-tition, loss (lines 6-10) and gain (lines 4-5) valuesare computed.
Afterwards the graph?s data struc-tures are updated (lines 13-16).
We define datastructures (DSs) that keep our computations effi-cient.
With these DSs the total runtime of lines 4-11is O(nin + min{nk?1in , n2in}).
The DSs are initial-ized in O(|parts| ?
k) time and their total updatetime is O(k ?
|parts|) = O(nk+1in ).
Thus algo-rithm 1 runs in O(|parts| ?
k + n2 ?
nin ?
(nin +min{nk?1in , n2in})) time.Algorithm 2 Algorithm 2 is similar in structure toAlgorithm 1.
The enhanced loss and gain computa-tions take O(nin +min{nk?1in , n2in}) time.
The ini-tialization of the DSs takes O(|parts| ?
k) time andtheir update time is O(nkin ?
k2).
The total runtimeof Algorithm 2 isO(nk+1in ?k+n?(n?nin?
(nin+min{n2in, nk?1in })+nkin?k2)).
For unpruned graphsand k ?
2 this is equivalent to O(nk+1), the theo-retical runtime of the TurboParser?s dual decompo-sition inference algorithm.6 Error PropagationUnlike modern approximation algorithms for ourproblem, our algorithm is greedy and determinis-tic.
That is, in each iteration it selects an arc tobe included in its final dependency tree and this de-cision cannot be changed in subsequent iterations.Hence, our algorithm is likely to suffer from error-propagation.
We propose two solutions to this prob-lem described within Algorithm 2.Beam search In each iteration (lines 3-16) the al-gorithm outputs its |B| best solutions to be subse-quently considered in the next iteration.
That is,lines 4-10 are performed |B| times for each edgee ?
E, one for each of the |B| partial solutions inthe beam, bj ?
B.
For each such solution, we de-note its weight, as calculated by the previous itera-tion of the algorithm with beamV albj .
When evalu-ating vie for an arc e with respect to bj (line 11), weset vi,je = beamV albj+??lossie?(1??
)?gainie.Post-search improvements After Algorithm 2 isexecuted, we perform s iterations of local greedy arcswaps.
That is, for every vertex v, s.t.
(u, v) ?
Tn,we try to switch the arc (u, v) with the arc (u?, v) asfollows.
Let Tnv be the sub tree that is rooted at v,we distinguish between two cases:(1) If u?
/?
Tnv then Tn = Tn \ {(u, v)} ?
{(u?, v)}.
(2) If u?
?
Tnv then let w be the first vertex on thepath from v to u?
(if (v, u?)
?
T then w = u?
):Tn = Tn \ {(u, v), (v, w)} ?
{(u?, v), (u,w)}.After inspecting all possible substitutions, wechoose the one that yields the best increase in thetree score (if such a substitution exists) and performthe substitution.7 Parser CombinationIn our experiments (see below), we implemented ouralgorithms within the TurboParser so that each ofthem, in turn, serves as its inference algorithm.
Indevelopment data experiments with Algorithm 2 wefound that for first order parsing, both our algorithmand the TurboParser predict on average over all lan-guages around 1% of the gold arcs that are not in-cluded in the output of the other algorithm.
For sec-ond order parsing, the corresponding numbers are1.75% (for gold arcs in the output of our algorithmbut not of the original TurboParser) and 4.3% (forthe other way around).
This suggests that an ensem-ble method may improve upon both parsers.We hence introduce a variation of Algorithm 2that accepts a dependency tree To as an input, andbiases its output towards that tree.
As differentparsers usually generate weights on different scales,we do not directly integrate part weights.
Instead,we change the weight of each part part ?
To oforder j, to be Wpart = Wpart + ?j , where ?j isan hyperparameter reflecting our belief in the pre-diction of the other parser on parts of order j. Thechange is applied only at test time, thus integratingtwo pre-trained parsers.7158 Experimental SetupWe implemented our algorithms within the Tur-boParser (Martins et al, 2013)2.
That is, every otheraspect of the parser ?
feature set, pruning algorithm,cost-augmented MIRA training (Crammer et al,2006) etc., is kept fixed but our algorithms replacethe inference algorithms: Chu-Liu-Edmonds ((Ed-monds, 1967), first order) and dual-decomposition(higher order).
We implemented two variants, foralgorithm 1 and 2 respectively, and compare theirresults to those of the original TurboParser.We experiment with the datasets of the CoNLL2006 and 2007 shared task on multilingual depen-dency parsing (Buchholz and Marsi, 2006; Nilssonet al, 2007), for a total of 17 languages.
When alanguage is represented in both sets, we used the2006 version.
We followed the standard train/testsplit of these datasets and, for the 8 languages witha training set of at least 10000 sentences, we ran-domly sampled 1000 sentences from the training setto serve as a development set.
For these languages,we first trained the parser on the training set and thenused the development set for hyperparameter tuning(|B|, s, ?, ?, and ?1, .
.
.
, ?k for k order parsing).34We employ four evaluation measures, where ev-ery measure is computed per language, and we re-port the average across all languages: (1) UnlabeledAttachment Score (UAS); (2) Undirected UAS (U-UAS) - for error analysis purposes; (3) Shared arcs(SARC) - the percentage of arcs shared by the pre-dictions of each of our algorithms and of the origi-nal TurboParser; and (4) Tokens per second (TPS)- for ensemble models this measure includes theTurboParser?s inference time.5 We also report agold(x,y) = (a,b) measure: where a is the percentageof gold standard arcs included in trees produced byalgorithm x but not by y, and b is the correspondingnumber for y and x.
We consider two setups.2https://github.com/andre-martins/TurboParser3|B| = 3, s = 5, ?
?
[0, 2.5], ?
?
[0.2, 0.5], ?1 ?
[0.5, 1.5], ?2 ?
[0.2, 0.3].
Our first order part weights are in[?9, 4], and second order part weights in [?3, 13].4The original TurboParser is trained on the training set ofeach language and tested on its test set, without any further di-vision of the training data to training and development sets.5Run times where computed on an Intel(R) Xeon(R) CPUE5-2697 v3@2.60GHz machine with 20GB RAM memory.Fully Supervised Training In this setup we onlyconsider the 8 languages with a development set.For each language, the parser is trained on the train-ing set and then the hyperparameters are tuned.
Firstwe set the beam size (|B|) and number of improve-ment iterations (s) to 0, and tune the other hyperpa-rameters on the language-specific development set.Then, we tune |B| and s, using the optimal parame-ters of the first step, on the English dev.
set.Minimally Supervised Training Here we con-sider all 17 languages.
For each language we ran-domly sampled 20 training sets of 500 sentencesfrom the original training set, trained a parser oneach set and tested on the original test set.
Resultsfor each language were calculated as the averageover these 20 folds.
The hyper parameters for alllanguages were tuned once on the English develop-ment set to the values that yielded the best averageresults across the 20 training samples.9 ResultsFully Supervised Training Average results forthis setup are presented in table 1 (top).
Unsur-prisingly, UAS for second order parsing with basicgreedy inference (Algorithm 1, BGI) is very low, asthis model does not take information about partialhigh order parts into account in its edge scores.
Wehence do not report more results for this algorithm.The table further reflects the accuracy/runtimetradeoff provided by Algorithm 2 (basic greedy in-ference with partial part predictions, BGI-PP): aUAS degradation of 0.34% and 2.58% for first andsecond order parsing respectively, with a runtimeimprovement by factors of 1.01 and 2.4, respec-tively.
Employing beam search and post search im-provements (BGI-PP+i+b) to compensate for errorpropagation improves UAS but harms the runtimegain: for example, the UAS gap in second orderparsing is 1.01% while the speedup factor is 1.43.As discussed in footnote 1 and Section 11, ouralgorithm does not support the third-order parts ofthe TurboParser.
However, the average UAS ofthe third-order TurboParser is 90.62% (only 0.36%above second order TurboParser) and its TPS is72.12 (almost 5 times slower).The accuracy gaps according to UAS and undi-rected UAS are similar, indicating that the source716Fully supervised Individual Models Ensemble ModelsUAS TPS SARC U-UAS UAS TPS SARC U-UASTurboParser order1 87.98 5621.30 ?
88.82 ?
?
?
?order2 90.26 356.63 ?
90.98 ?
?
?
?BGI order1 83.78 5981.91 90.87 90.87 ?
?
?
?order2 27.54 715.41 27.76 27.77 ?
?
?
?BGI-PP order1 87.64 5680.60 97.15 88.53 88.03 2876.03 99.59 88.84order2 87.68 858.25 92.66 88.73 90.50 249.40 99.54 91.20BGI-PP + i order1 87.76 4648.4 98.10 88.64 87.96 2557.00 99.47 88.80order2 88.98 639.97 94.40 89.81 90.50 297.10 99.43 91.19BGI-PP + i + b order1 87.78 3253.80 98.29 88.73 87.91 2053.00 99.07 88.82order2 89.25 511.47 94.79 90.02 90.53 212.40 99.40 91.21(a) The fully supervised setup.Minimally supervised Individual Models Ensemble ModelsUAS TPS SARC U-UAS UAS TPS SARC U-UASTurboParser order1 78.99 13097.00 ?
80.38 ?
?
?order2 80.52 830.05 ?
81.84 ?
?
?BGI-PP order1 78.76 13848.00 85.36 80.15 79.14 6499.00 87.36 80.50order2 78.80 3089.40 84.59 80.27 80.60 636.30 95.57 81.88BGI-PP + i order1 78.87 11673.00 85.54 80.25 79.24 6516.00 87.55 80.59order2 79.36 2414.00 84.81 80.76 80.67 621.50 95.41 82.16BGI-PP + i + b order1 78.91 4212.50 85.58 80.29 79.29 4349.00 87.61 80.62order2 79.45 1372.70 84.89 80.84 80.69 518.10 95.44 81.96(b) The minimally supervised setup.Table 1: Results for the fully supervised (top table) and minimally supervised (bottom table) setups.
The left columnsection of each table is for individual models while the right column section is for ensemble models (Section 7).
BGI-PP is the basic greedy inference algorithm with partial part predictions, +i indicates post-search improvements and+b indicates beam search (Section 6).
The Tokens per Second (TPS) measure for the ensemble models reports theadditional inference time over the TurboParser inference.
All scores are averaged across individual languages.of differences between the parsers is not arc direc-tionality.
The percentage of arcs shared betweenthe parsers increases with model complexity but isstill as low as 94.79% for BGI-PP+i+b in second or-der parsing.
In this setup, gold(BGI-PP+i+b, Tur-boParser) = (1.6%,2.6%) which supports the devel-opment data pattern reported in Section 6 and furtherjustifies an ensemble approach.The right column section of the table indeedshows consistent improvements of the ensemblemodels over the TurboParser for second order pars-ing: the ensemble models achieve UAS of 90.5-90.53% compared to 90.26% of the TurboParser.Naturally, running the TurboParser alone is faster bya factor of 1.67.
Like for the individual inferencealgorithms, the undirected UAS measure indicatesthat the gain does not come from arc directionalityimprovements.
The ensemble methods share almostall of their arcs with the TurboParser, but in cases ofdisagreement ensembles tend to be more accurate.Table 2 complements our results, providing UASvalues for each of the 8 languages participat-ing in this setup.
The UAS difference betweenBGI+PP+i+b and the TurboParser are (+0.24)-(-0.71) in first order parsing and (+0.18)-(-2.46) insecond order parsing.
In the latter case, combiningthese two models (BGI+PP+i+b+e) yields improve-ments over the TurboParser in 6 out of 8 languages.Minimally Supervised Training Results for thissetup are in table 1 (bottom).
While result pat-terns are very similar to the fully supervised case,two observations are worth mentioning.
First,the percentage of arcs shared by our algorithmsand the original parser is much lower than inthe fully supervised case.
This is true also forshared gold arcs: gold(BGI-PP+b+i,TurboParser) =(4.86%,5.92%) for second order parsing.
This sug-gests that more sophisticated ensemble techniquesmay be useful in this setup.Second, ensemble modeling improves UAS overthe TurboParser also for first order parsing, lead-ing to a gain of 0.3% in UAS for the BGI+i+bensemble (79.29% vs. 78.99%).
As the percent-age of shared arcs between the ensemble mod-els and the TurboParser is particularly low in firstorder parsing, as well as the shared gold arcs717language First Order Second OrderTurboParser BGI-PP BGI-PP BGI-PP TurboParser BGI-PP BGI-PP BGI-PP+ i + b + i + b + e + i + b + i + b + eswedish 87.12 86.35 86.93 87.12 88.65 86.14 87.85 89.29bulgarian 90.66 90.22 90.42 90.66 92.43 89.73 91.50 92.58chinese 84.88 83.89 84.17 84.17 86.53 81.33 85.18 86.59czech 83.53 83.46 83.44 83.44 86.35 84.91 86.26 87.50dutch 88.48 88.56 88.43 88.43 91.30 89.64 90.49 91.34japanese 93.03 93.18 93.27 93.27 93.83 93.78 94.01 94.01catalan 88.94 88.50 88.67 88.93 92.25 89.3 90.46 92.24english 87.18 86.94 86.84 87.18 90.70 86.52 88.24 90.66Table 2: Per language UAS for the fully supervised setup.
Model names are as in Table 1, ?e?
stands for ensemble.Best results for each language and parsing model order are highlighted in bold.
(gold(BGI+i+b,TurboParser) = (4.98%,5.5%)), im-proving the ensemble techniques is a promising fu-ture research direction.10 Related WorkOur work brings together ideas that have been con-sidered in past, although in different forms.Greedy Inference Goldberg and Elhadad (2010)introduced an easy-first, greedy, approach to depen-dency parsing.
Their algorithm adds at each iterationthe best candidate arc, in contrast to the left to rightordering of standard transition based parsers.
Thiswork is extended at (Tratz and Hovy, 2011; Gold-berg and Nivre, ; Goldberg and Nivre, 2013).The easy-first parser consists of a feature set anda specialized variant of the structured perceptrontraining algorithm, both dedicated to greedy infer-ence.
In contrast, we show that a variant of the Tur-boParser that employs Algorithm 2 for inference andis trained with its standard global training algorithm,performs very similarly to the same parser that em-ploys dual decomposition inference.Error Propagation in Deterministic ParsingSince deterministic algorithms are standard intransition-based parsing, the error-propagation prob-lem has been dealt with in that context.
Variousmethods were employed, with beam search being aprominent idea (Sagae and Lavie, 2006; Titov andHenderson, 2007; Zhang and Clark, 2008; Huang etal., 2009; Zhang and Nivre, 2011; Bohnet and Nivre,2012; Choi and McCallum, 2013b, inter alia).Post Search Improvements Several previousworks employed post-search improvements tech-niques.
Like in our case, these techniques improvethe tree induced by an initial, possibly more princi-pled, search technique through local, greedy steps.McDonald and Pereira (2006) proposed to ap-proximate high-order graph-based non-projectiveparsing, by arc-swap iterations over a previously in-duced projective tree.
Levi et al (2016) proposeda post-search improvements method, different thanours, to compensate for errors of their graph-based,undirected inference algorithm.
Finally, Zhang etal.
(2014a) demonstrated that multiple random ini-tialization followed by local improvements with re-spect to a high-order parsing objective result in ex-cellent parsing performance.
Their algorithm, how-ever, shouldbhhb employ hundreds of random ini-tializations in order to provide state-of-the-art re-sults.Ensemble Approaches Finally, several previousworks combined dependency parsers.
These includeNivre and McDonald (2008) who used the outputof one parser to provide features for another, Zhangand Clark (2008) that proposed a beam-search basedparser that combines two parsers into a single sys-tem for training and inference, and Martins et al(2008) that employed stacked learning, in which asecond predictor is trained to improve the perfor-mance of the first.
Our work complements theseworks by integrating information from a pre-trainedTurboParser in our algorithm at test time only.11 DiscussionWe presented a greedy inference approach forgraph-based, high-order, non-projective dependencyparsing.
Our experiments with 17 languages showthat our simple and easy to implement algorithm is adecent alternative for dual-decomposition inference.A major limitation of our algorithm is in-718cluding information from parts that require agiven set of arcs not to be included in the de-pendency tree (footnote 1).
For example, thenextSibling((1, 2), (1, 5)) part of the TurboParserwould fire iff the tree includes the arcs (1, 2) and(1, 5) but not the arcs (1, 3) and (1, 4).In order to account for such parts, we should de-cide how to compute their probabilities and, addi-tionally, at which point they are considered part ofthe tree.
We explored several approaches, but failedto improve our results.
Hence, we did not experi-ment with the third-order TurboParser as all of itsthird-order parts contain ?non-included?
arcs.
Thisis left for future work.A Runtime Complexity AnalysisHere we analyze the complexity of our algorithms,denoting the maximal indegree of a vertex with nin.Algorithm 1 Algorithm 1 consists of two nestedloops (lines 2-k3) and hence lines 4-11 are repeatedO (n?
|E|) = O(n?n?nin) times.
At each repe-tition, loss (lines 6-10) and gain (lines 4-5) valuesare computed.
Afterwards the graph?s data struc-tures are updated (lines 13-16).For every arc that we examine (line 3), there areO(nin) lost arcs, as there are O(nin) incoming arcs(set 1) and O(nin) cycles to break (set 2).
Sinceevery lost arc translates to a set of lost parts, wecan avoid repeating computations by storing the par-tial loss of every arc in a data structure (DS): e ??part:e?partwpart.
Now, instead of summing allthe lost parts, (every edge participates in O(nk?1in )parts,6 thus there are O(nkin) lost parts per addedarc), we can sum only O(nin) partial loss values.However, since some lost parts may contain an arcfrom set 1 and an arc from set 2, we need to sub-tract the values that were summed twice, this can bedone in O(min{nk?1in , n2in}) time by holding a sec-ond DS: e1 ?
e2 ?
?part:e1?part?e2?partwpart.7In order to efficiently compute the gain values, wehold a mapping from arcs to the sum of weights ofparts that can be completed in the current iterationby adding the arc to the tree.
With this DS, gain val-6Assuming that a part is a connected component.7For first order parsing this is not needed; for second orderparsing it is done in O(nin) time.ues can be computed in constant time.
In total, theruntime of lines 4-11 is O(nin +min{nk?1in , n2in}).The DSs are initialized in O(|parts| ?
k) time.Since every part is deleted at most once, and getsupdated (its arcs are added to the tree) at most ktimes, the total DS update time is O(k ?
|parts|) =O(nk+1in ).
Thus algorithm 1 runs in O(|parts|?k+n2 ?
nin ?
(nin +min{nk?1in , n2in})) time.Algorithm 2 Algorithm 2 is similar in structureto Algorithm 1 but the loss and gain computationsare more complex.
To facilitate efficiency, we holdtwo DSs: (a) a mapping from arcs to the sum oflost parts values, which are now wpart ?
Ppart forpart ?
parts; and (b) a mapping from arc pairsto the sum of part values for parts that contain botharcs.
The loss and gain values can be computed, asabove, in O(nin +min{nk?1in , n2in}) time.The initialization of the DSs takes O(|parts|?k)time.
In the i-th iteration we add e = (u, v) toT i, and remove the lostArcs from E. Every lostarc participates in O(nk?1in ) parts, and we need toupdate O(k) entries for each lost part in DS(a) (asthe value of the other arcs of that part should nolonger account for that part?s weight) and O(k2) en-tries in DS (b).
Thus, the total update time of theDSs is O(nkin ?
k2) and the total runtime of Algo-rithm 2 is O(nk+1in ?
k + n ?
(n ?
nin ?
(nin +min{n2in, nk?1in })+nkin?k2)).
For unpruned graphsand k ?
2 this is equivalent to O(nk+1), the theo-retical runtime of the TurboParser?s dual decompo-sition inference algorithm.AcknowledgmentsThe third author was partly supported by a researchgrant from the Microsoft/Technion research centerfor electronic commerce: Context Sensitive Sen-tence Understanding for Natural Language Process-ing.ReferencesMariana SC Almeida, Cla?udia Pinto, Helena Figueira,Pedro Mendes, and Andre?
FT Martins.
2015.
Align-ing opinions: Cross-lingual opinion mining with de-pendencies.
In ACL.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-719ceedings of EMNLP-CoNLL.
Association for Compu-tational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In CoNLL.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In EMNLP-CoNLL.Jinho D Choi and Andrew McCallum.
2013a.Transition-based dependency parsing with selectionalbranching.
In ACL.Jinho D Choi and Andrew McCallum.
2013b.Transition-based dependency parsing with selectionalbranching.
In ACL.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research, 7:551?585.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.Jason Eisner.
1996.
Efficient normal-form parsing forcombinatory categorial grammar.
In ACL.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In NAACL-HLT.Yoav Goldberg and Joakim Nivre.
A dynamic oracle forarc-eager dependency parsing.
In COLING.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the Association for ComputationalLinguistics, 1(Oct):403?414.Matthew Gormley, Mark Dredze, and Jason Eisner.
2015.Approximation-aware dependency parsing by beliefpropagation.
Transactions of the Association for Com-putational Linguistics, 3:489?501.Matthew Honnibal, Yoav Goldberg, and Mark Johnson.2013.
A non-monotonic arc-eager transition systemfor dependency parsing.
In CoNLL.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In EMNLP.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL.Effi Levi, Roi Reichart, and Ari Rappoport.
2016.
Edge-linear first-order dependency parsing with undirectedminimum spanning tree inference.
In ACL.Omer Levy and Yoav Goldberg.
2014.
Neural word em-bedding as implicit matrix factorization.
In NIPS.Andre?
FT Martins, Dipanjan Das, Noah A Smith, andEric P Xing.
2008.
Stacking dependency parsers.
InEMNLP.A.
Martins, M. Almeida, and N. A. Smith.
2013.
Turn-ing on the turbo: Fast third-order non-projective turboparsers.
In ACL.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In EACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In HLT-EMNLP.Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The conll 2007 shared task on dependency parsing.In Proceedings of the CoNLL shared task session ofEMNLP-CoNLL.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In ACL-08: HLT, June.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(02):95?135.Sebastian Riedel, David Smith, and Andrew McCallum.2012.
Parse, price and cut ?
delayed column androw generation for graph based parsers.
In EMNLP-CoNLL.Kenji Sagae and Alon Lavie.
2006.
A best-first prob-abilistic shift-reduce parser.
In Proc.
of the COL-ING/ACL on Main conference poster sessions.David Smith and Jason Eisner.
2008.
Dependency pars-ing by belief propagation.
In EMNLP.Ivan Titov and James Henderson.
2007.
Fast and robustmultilingual dependency parsing with a generative la-tent variable model.
In EMNLP-CoNLL.Stephen Tratz and Eduard Hovy.
2011.
A fast, accu-rate, non-projective, semantically-enriched parser.
InEMNLP.Fei Wu and Daniel S Weld.
2010.
Open informationextraction using wikipedia.
In ACL.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In EMNLP.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InACL.Yuan Zhang, Tao Lei, Regina Barzilay, and TommiJaakkola.
2014a.
Greed is good if randomized: Newinference for dependency parsing.
In EMNLP.Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola,and Amir Globerson.
2014b.
Steps to excellence:Simple inference with refined scoring of dependencytrees.
In ACL.720
