Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 59?67,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsA System for Predicting ICD-10-PCS Codesfrom Electronic Health RecordsMichael Subotin3M Health Information SystemsSilver Spring, MDmsubotin@mmm.comAnthony R. Davis3M Health Information SystemsSilver Spring, MDadavis4@mmm.comAbstractMedical coding is a process of classify-ing health records according to standardcode sets representing procedures and di-agnoses.
It is an integral part of healthcare in the U.S., and the high costs itincurs have prompted adoption of natu-ral language processing techniques for au-tomatic generation of these codes fromthe clinical narrative contained in elec-tronic health records.
The need for effec-tive auto-coding methods becomes evengreater with the impending adoption ofICD-10, a code inventory of greater com-plexity than the currently used code sets.This paper presents a system that predictsICD-10 procedure codes from the clinicalnarrative using several levels of abstrac-tion.
First, partial hierarchical classifica-tion is used to identify potentially rele-vant concepts and codes.
Then, for eachof these concepts we estimate the confi-dence that it appears in a procedure codefor that document.
Finally, confidence val-ues for the candidate codes are estimatedusing features derived from concept confi-dence scores.
The concept models can betrained on data with ICD-9 codes to sup-plement sparse ICD-10 training resources.Evaluation on held-out data shows promis-ing results.1 IntroductionIn many countries reimbursement rules for healthcare services stipulate that the patient encountermust be assigned codes representing diagnosesthat were made for and procedures that were per-formed on the patient.
These codes may be as-signed by general health care personnel or by spe-cially trained medical coders.
The billing codesused in the U.S. include International Statisti-cal Classification of Diseases and Related HealthProblems (ICD) codes, whose version 9 is cur-rently in use and whose version 10 was scheduledfor adoption in October 20141, as well as CurrentProcedural Terminology (CPT) codes.
The samecodes are also used for research, internal book-keeping, and other purposes.Assigning codes to clinical documentation of-ten requires extensive technical training and in-volves substantial labor costs.
This, together withincreasing prominence of electronic health records(EHRs), has prompted development and adoptionof NLP algorithms that support the coding work-flow by automatically inferring appropriate codesfrom the clinical narrative and other informationcontained in the EHR (Chute et al., 1994; Heinzeet al., 2001; Resnik et al., 2006; Pakhomov et al.,2006; Benson, 2006).
The need for effective auto-coding methods becomes especially acute with theintroduction of ICD-10 and the associated increaseof training and labor costs for manual coding.The novelty and complexity of ICD-10 presentsunprecedented challenges for developers of rule-based auto-coding software.
Thus, while ICD-9contains 3882 codes for procedures, the numberof codes defined by the ICD-10 Procedure Cod-ing System (PCS) is greater than 70,000.
Further-more, the organization of ICD-10-PCS is funda-mentally different from ICD-9, which means thatthe investment of time and money that had goneinto writing auto-coding rules for ICD-9 proce-dure codes cannot be easily leveraged in the tran-sition to ICD-10.In turn, statistical auto-coding methods are con-strained by the scarcity of available training datawith manually assigned ICD-10 codes.
While thisproblem will be attenuated over the years as ICD-10-coded data are accumulated, the health care1The deadline was delayed by at least a year while thispaper was in review.59industry needs effective technology for ICD-10computer-assisted coding in advance of the imple-mentation deadline.
Thus, for developers of statis-tical auto-coding algorithms two desiderata cometo the fore: these algorithms should take advantageof all available training data, including documentssupplied only with ICD-9 codes, and they shouldpossess high capacity for statistical generalizationin order to maximize the benefits of training mate-rial with ICD-10 codes.The auto-coding system described here seeksto meet both these requirements.
Rather thanpredicting codes directly from the clinical narra-tive, a set of classifiers is first applied to identifycoding-related concepts that appear in the EHR.We use General Equivalence Mappings (GEMs)between ICD-9 and ICD-10 codes (CMS, 2014)to train these models not only on data with human-assigned ICD-10 codes, but also on ICD-9-codeddata.
We then use the predicted concepts to de-rive features for a model that estimates probabil-ity of ICD-10 codes.
Besides the intermediate ab-straction to concepts, the code confidence modelitself is also designed so as to counteract sparsityof the training data.
Rather than train a separateclassifier for each code, we use a single modelwhose features can generalize beyond individualcodes.
Partial hierarchical classification is used forgreater run-time efficiency.
To our knowledge, thisis the first research publication describing an auto-coding system for ICD-10-PCS.
It is currently de-ployed, in tandem with other auto-coding mod-ules, to support computer-assisted coding in the3MTM360 EncompassTMSystem.The rest of the paper is organized as follows.Section 2 reviews the overall organization of ICD-10-PCS.
Section 4.1 outlines the run-time process-ing flow of the system to show how its componentsfit together.
Section 4.2 describes the concept con-fidence models, including the hierarchical classi-fication components.
Section 4.3 discusses howdata with manually assigned ICD-9 codes is usedto train some of the concept confidence models.Section 4.4 describes the code confidence model.Finally, Section 5 reports experimental results.2 ICD-10 Procedure Coding SystemICD-10-PCS is a set of codes for medical proce-dures, developed by 3M Health Information Sys-tems under contract to the Center for Medicare andMedicaid Services of the U.S. government.
ICD-10-PCS has been designed systematically; eachcode consists of seven characters, and the charac-ter in each of these positions signifies one partic-ular aspect of the code.
The first character des-ignates the ?section?
of ICD-10-PCS: 0 for Med-ical and Surgical, 1 for Obstetrics, 2 for Place-ment, and so on.
Within each section, the sevencomponents, or axes of classification, are intendedto have a consistent meaning; for example in theMedical and Surgical section, the second charac-ter designates the body system involved, the thirdthe root operation, and so on (see Table 1 for alist).
All procedures in this section are thus clas-sified along these axes.
For instance, in a codesuch as 0DBJ3ZZ, the D in the second position in-dicates that the body system involved is the gas-trointestinal system, B in the third position alwaysindicates that the root operation is an excision of abody part, the J in the fourth position indicates thatthe appendix is the body part involved, and the 3 inthe fifth position indicates that the approach is per-cutaneous.
The value Z in the last two axes meansthan neither a device nor a qualifier are specified.Character Meaning1st Section2nd Body System3rd Root Operation4th Body Part5th Approach6th Device7th QualifierTable 1: Character Specification of the Medicaland Surgical Section of ICD-10-PCSSeveral consequences of the compositionalstructure of ICD-10-PCS are especially relevantfor statistical auto-coding methods.On the one hand, it defines over 70,000 codes,many of which are logically possible, but very rarein practice.
Thus, attempts to predict the codes asunitary entities are bound to suffer from data spar-sity problems even with a large training corpus.Furthermore, some of the axis values are formu-lated in ways that are different from how the cor-responding concepts would normally be expressedin a clinical narrative.
For example, ICD-10-PCSuses multiple axes (root opreration, body part, and,in a sense, the first two axes as well) to encodewhat many traditional procedure terms (such asthose ending in -tomy and -plasty) express by a60single word, while the device axis uses genericcategories where a clinical narrative would referonly to specific brand names.
This drastically lim-its how much can be accomplished by matchingcode descriptions or indexes derived from themagainst the text of EHRs.On the other hand, the systematic conceptualstructure of PCS codes and of the codeset as awhole can be exploited to compensate for datasparsity and idiosyncracies of axis definitions byintroducing abstraction into the model.3 Related workThere exists a large literature on automatic clas-sification of clinical text (Stanfill et al., 2010).
Asizeable portion of it is devoted to detecting cate-gories corresponding to billing codes, but most ofthese studies are limited to one or a handful of cat-egories.
This is in part because the use of patientrecords is subject to strict regulation.
Thus, thecorpus used for most auto-coding research up todate consists of about two thousand documents an-notated with 45 ICD-9 codes (Pestian et al., 2007).It was used in a shared task at the 2007 BioNLPworkshop and gave rise to papers studying a va-riety of rule-based and statistical methods, whichare too numerous to list here.We limit our attention to a smaller set of re-search publications describing identification of anentire set of billing codes, or a significant por-tion thereof, which better reflects the role of auto-coding in real-life applications.
Mayo Clinic wasamong the earliest adopters of auto-coding (Chuteet al., 1994), where it was deployed to assigncodes from a customized and greatly expandedversion of ICD-8, consisting of almost 30K diag-nostic codes.
A recently reported version of theirsystem (Pakhomov et al., 2006) leverages a com-bination of example-based techniques and Na?
?veBayes classification over a database of over 20MEHRs.
The phrases representing the diagnoseshave to be itemized as a list beforehand.
In an-other pioneering study, Larkey & Croft (1995) in-vestigated k-Nearest Neighbor, Na?
?ve Bayes, andrelevance feedback on a set of 12K dischargesummaries, predicting ICD-9 codes.
Heinze etal (2000) and Ribeiro-Neto et al (2001) describesystems centered on symbolic computation.
Jianget al (2006) discuss confidence assessment forICD-9 and CPT codes, performed separately fromcode generation.
Medori & Fairon (2010) com-bine information extraction with a Na?
?ve Bayesclassifier, working with a corpus of about 20K dis-charge summaries in French.
In a recent paper,Perotte et al (2014) study standard and hierarchi-cal classification using support vector machines ona corpus of about 20K EHRs with ICD-9 codes.We are not aware of any previous publicationson auto-coding for ICD-10-PCS, and the resultsof these studies cannot be directly compared withthose reported below due to the unique nature ofthis code set.
Our original contributions also in-clude explicit modeling of concepts and the ca-pability to assign previously unobserved codeswithin a machine learning framework.4 Methods4.1 Run-time processing flowWe first describe the basic run-time processingflow of the system, shown in Figure 1.Figure 1: Run-time processing flowIn a na?
?ve approach, one could generate allcodes from the ICD-10-PCS inventory for eachEHR2and estimate their probability in turn, butthis would be too computationally expensive.
In-stead, the hypothesis space is restricted by two-2We use the term EHR generically in this paper.
The sys-tem can be applied at the level of individual clinical docu-ments or entire patient encounters, whichever is appropriatefor the given application.61level hierarchical classification with beam search.First, a set of classifiers estimates the confidenceof all PCS sections (one-character prefixes of thecodes), one per section.
The sections whose con-fidence exceeds a threshold are used to generatecandidate body systems (two-character code pre-fixes), whose confidence is estimated by anotherset of classifiers.
Then, body systems whose con-fidence exceeds a threshold are used to generatea set of candidate codes and the set of conceptsexpressed by these codes.
The probability of ob-serving each of the candidate concepts in the EHRis estimated by a separate classifier.
Finally, theseconcept confidence scores are used to derive fea-tures for a model that estimates the probability ofobserving each of the candidate codes, and thehighest-scoring codes are chosen according to athresholding decision rule.The choice of two hierarchical layers is partiallydetermined by the amount of training data withICD-10 codes available for this study, since manythree-character code prefixes are too infrequent totrain reliable classifiers.
Given more training data,additional hierarchical classification layers couldbe used, which would trade a higher risk of recallerrors against greater processing speed.
The sametrade-off can be negotiated by adjusting the beamsearch threshold.4.2 Concept confidence modelsEstimation of concept confidence ?
including theconfidence of code prefixes in the two hierarchi-cal classification layers ?
is performed by a set ofclassifiers, one per concept, which are trained onEHRs supplied with ICD-10 and ICD-9 procedurecodes.The basis for training the concept models isprovided by a mapping between codes and con-cepts expressed by the codes.
For example, thecode 0GB24ZZ (Excision of Left Adrenal Gland,Percutaneous Endoscopic Approach) expresses,among other concepts, the concept adrenal glandand the more specific concept left adrenal gland.It also expresses the concept of adrenalectomy(surgical removal of one or both of the adrenalglands), which corresponds to the regular expres-sion 0G[BT][234]..Z over ICD-10-PCS codes.We used the code-to-concept mapping describedin Mills (2013), supplemented by some additionalcategories that do not correspond to traditionalclinical concepts.
For example, our set of conceptsincluded entries for the categories of no deviceand no qualifer, which are widely used in ICD-10-PCS.
We also added entries that specified the de-vice axis or the qualifier axis together with the firstthree axes, where they were absent in the originalconcept map, reasoning that the language used toexpress the choice of the device or qualifier can bespecific to particular procedures and body parts.For data with ICD-10-PCS codes, the logic usedto generate training instances is straightforward.Whenever a manually assigned code expresses agiven concept, a positive training instance for thecorresponding classifier is generated.
Negativetraining instances are sub-sampled from the con-cepts generated by hierarchical classification lay-ers for that EHR.
As can be seen from this logic,the precise question that the concept models seekto answer is as follows: given that this particularconcept has been generated by the upstream hier-archical layers, how likely is it that it will be ex-pressed by one of the ICD-10 procedure codes as-signed to that EHR?In estimating concept confidence we do not at-tempt to localize where in the clinical narrativethe given concept is expressed.
Our baselinefeature set is simply a bag of tokens.
We alsoexperimented with other feature types, includingfrequency-based weighting schemes for token fea-ture values and features based on string matches ofUnified Medical Language System (UMLS) con-cept dictionaries.
For the concepts of left and rightwe define an additional feature type, indicatingwhether the token left or right appears more fre-quently in the EHR.
While still rudimentary, thisfeature type is more apt to infer laterality than abag of tokens.A number of statistical methods can be usedto estimate concept confidence.
We use theMallet (McCallum, 2002) implementation of `1-regularized logistic regression, which has showngood performance for NLP tasks in terms of ac-curacy as well as scalability at training and run-time (Gao et al., 2007).4.3 Training on ICD-9 dataIn training concept confidence models on datawith ICD-9 codes we make use of the GeneralEquivalence Mappings (GEMs), a publicly avail-able resource establishing relationships betweenICD-9 and ICD-10 codes (CMS, 2014).
Most cor-respondences between ICD-9 and ICD-10 proce-62dure codes are one-to-many, although other map-ping patterns are also found.
Furthermore, a codein one set can correspond to a combination ofcodes from the other set.
For example, the ICD-9 code for combined heart-lung transplantationmaps to a set of pairs of ICD-10 codes, the firstcode in the pair representing one of three possibletypes of heart transplantation, and the other rep-resenting one of three possible types of bilaterallung transplantation.A complete description of the rules underlyingGEMs and our logic for processing them is beyondthe scope of this paper, and we limit our discussionto the principles underlying our approach.
We firstdistribute a unit probability mass over the ICD-10 codes or code combinations mapped to eachICD-9 code, using logic that reflects the struc-ture of GEMs and distributing probability massuniformly among comparable alternatives.
Fromthese probabilities we compute a cumulative prob-ability mass for each concept appearing in theICD-10 codes.
For example, if an ICD-9 codemaps to four ICD-10 codes over which we dis-tribute a uniform probability distibution, and agiven concept appears in two of them, we assignthe probability of 0.5 to that concept.
For a givenEHR, we assign to each concept the highest prob-ability it receives from any of the codes observedfor the EHR.
Finally, we use the resulting conceptprobabilities to weight positive training instances.Negative instances still have unit weights, sincethey correspond to concepts that can be unequivo-cably ruled out based on the GEMs.4.4 Code confidence modelThe code confidence model produces a confidencescore for candidate codes generated by the hierar-chical classification layers, using features derivedfrom the output of the code confidence modelsdescribed above.
The code confidence model istrained on data with ICD-10 codes.
Whenever acandidate code matches a code assigned by hu-man annotators, a positive training instance is gen-erated.
Otherwise, a negative instance is gener-ated, with sub-sampling.
We report experimentsusing logistic regression with `1and `2regulariza-tion (Gao et al., 2007).The definition of features used in the model re-quires careful attention, because it is in the form ofthe feature space that the proposed model differsfrom a standard one-vs-all approach.
To elucidatethe contrast we may start with a form of the featurespace that would correspond to one-vs-all classi-fication.
This can be achieved by specifying theidentity of a particular code in all feature names.Then, the objective function for logistic regressionwould decompose into independent learning sub-problems, one for each code, producing a collec-tion of one-vs-all classifiers.
There are clear draw-backs to this approach.
If all parameters are re-stricted to a specific code, the training data wouldbe fragmented along the same lines.
Thus, evenif features derived from concepts may seem to en-able generalization, in reality they would in eachcase be estimated only from training instances cor-responding to a single code, causing unnecessarydata sparsity.This shortcoming can be overcome in logisticregression simply by introducing generalized fea-tures, without changing the rest of the model (Sub-otin, 2011).
Thus, in deriving features from scoresof concept confidence models we include onlythose concepts which are expressed by the givencode, but we do not specify the identity of the codein the feature names.
In this way the weights forthese features are estimated at once from traininginstances for all codes in which these concepts ap-pear.
We combine these generalized features withthe code-bound features described earlier.
The lat-ter should help us learn more specific predictorsfor particular procedures, when such predictorsexist in the feature space.While the scores of concept confidence mod-els provide the basis for the feature space of thecode confidence model, there are multiple ways inwhich features can be derived from these scores.The simplest way is to take concept identity (op-tionally specified by code identity) as the fea-ture name and the confidence score as the featurevalue.
We supplement these features with featuresbased on score quantization.
That is, we thresh-old each concept confidence score at several pointsand define binary features indicating whether thescore exceeds each of the thresholds.
For boththese feature types, we generate separate featuresfor predictions of concept models trained on ICD-9 data and concept models trained on ICD-10 datain order to allow the code confidence model tolearn how useful predictions of concept confidencemodels are, depending on the type of their trainingdata.Both the concept confidence models and the63code confidence model can be trained on data withICD-10 codes.
We are thus faced with the ques-tion of how best to use this limited resource.
Thesimplest approach would be to train both types ofmodels on all available training data, but there is aconcern that predictions of the concept models ontheir own training data would not reflect their out-of-sample performance, and this would misleadthe code confidence model into relying on themtoo much.
An alternative approach, often calledstacked generalization (Wolpert, 1992), would beto generate training data for the code confidencemodel by running concept confidence models onout-of-sample data.
We compare the performanceof these approaches below.5 Evaluation5.1 MethodologyWe evaluated the proposed model using a cor-pus of 28,536 EHRs (individual clinical records),compiled to represent a wide variety of clinicalcontexts and supplied with ICD-10-PCS codes bytrained medical coders.
The corpus was annotatedunder the auspices of 3M Health Information Sys-tems for the express purpose of developing auto-coding technology for ICD-10.
There was a totalof 51,082 PCS codes and 5,650 unique PCS codesin the corpus, only 76 of which appeared in morethan 100 EHRs, and 2,609 of which appeared justonce.
Multiple coders worked on some of the doc-uments, but they were allowed to collaborate, pro-ducing what was effectively a single set of codesfor each EHR.
We held out about a thousand EHRsfor development testing and evaluation, each, us-ing the rest for training.
The same corpus, as wellas 175,798 outpatient surgery EHRs with ICD-9procedure codes submitted for billing by a healthprovider were also used to train hierarchical andconcept confidence models.We evaluated auto-coding performance by amodified version of mean reciprocal rank (MRR).MRR is a common evaluation metric for systemswith ranked outputs.
For a set of Q correct out-puts with ranks rankiamong all outputs, standardMRR is computed as:MRR =1QQ?i=11rankiFor example, a MRR value of 0.25 means thatthat the correct answer has rank 4 on average.
Thismetric is designed for tasks where only one of theoutputs can be correct.
When applied directly totasks where more than one output can be correct,MRR unfairly penalizes cases with multiple cor-rect outputs, increasing the rank of some correctoutputs on account of other, higher-ranked outputsthat are also correct.
We modify MRR for our taskby ignoring correct outputs in the rank computa-tions.
In other words, the rank of a correct outputis computed as the number of higher-ranked incor-rect outputs, plus one.
This metric has the advan-tage of summarizing the accuracy of an auto-coderwithout reference to a particular choice of thresh-old, which may be determined by business rules orresearch considerations, as would be the case forprecision and recall.One advantage of regularized logistic regres-sion is that the value of 1 is often a near-optimalsetting for the regularization trade-off parameter.This can save considerable computation time thatwould be required for tuning this parameter foreach experimental condition.
We have previouslyobserved that the value of 1 consistently producednear-optimal results for the `1regularizer in con-cept confidence models and for the `2regularizerin the code confidence models, and we have usedthis setting for all the experiments reported here.For the code confidence model with `1-regularizedlogistic regression we saw a slight improvementwith weaker regularization, and we report the bestresult we obtained for this model below.5.2 ResultsThe results are shown in Table 2.
The top MMRscore of 0.572 corresponds to a micro-averaged F-score of 0.485 (0.490 precision, 0.480 recall) whenthe threshold is chosen to obtain approximatelyequal values for recall and precision3.
The bestresult was obtained when:?
the concept models used bag-of-tokens fea-tures (with the additional laterality featuresdescribed in Section 4.2);?
both concept models trained on ICD-9 dataand those trained on ICD-10 data were used;?
the code confidence model was trained ondata with predictions of concept modelstrained on all of ICD-10 data (i.e., no3To put these numbers into perspective, note that the aver-age accuracy of trained medical coders for ICD-10 has beenestimated to be 63% (HIMSS/WEDI, 2013).64data splitting for stacked generalization wasused);?
the code confidence model used all of the fea-ture types described in Section 4.4;?
the code confidence model used logistic re-gression with `2regularization.We examine the impact of all these choices onsystem performance in turn.Model MRRAll data, all features, `2reg.
0.572Concept model training:Trained on ICD-10 only 0.558Trained on ICD-9 only 0.341Code model features:One-vs-all 0.519No code-bound features 0.553No quantization features 0.560Stacked generalization:half & half data split 0.5015-fold cross-validation 0.539Code model algorithm:`1regularization 0.528Table 2: Evaluation results.
Each row after thefirst correponds to varying one aspect of the modelshown in the first row.
See Section 5.3 for detailsof the experimental conditions.5.3 DiscussionDespite its apparent primitive nature, the bag-of-token feature space for the concept confidencemodels has turned out to provide a remarkablystrong baseline.
Our experiments with frequency-based weighting schemes for the feature valuesand with features derived from text matches fromthe UMLS concept dictionaries did not yield sub-stantial improvements in the results.
Thus, the useof UMLS-based features, obtained using ApacheConceptMapper, yielded a relative improvementof 0.6% (i.e., 0.003 in absolute terms), but at thecost of nearly doubling run-time processing time.Nonetheless, we remain optimistic that more so-phisticated features can benefit performance of theconcept models while maintaining their scalabil-ity.As can be seen from the table, both conceptmodels trained on ICD-9 data and those trained onICD-10 data contributed to the overall effective-ness of the system.
However, the contribution ofthe latter is markedly stronger.
This suggests thatfurther research is needed in finding the best waysof exploiting ICD-9-coded data for ICD-10 auto-coding.
Given that data with ICD-9 codes is likelyto be more readily available than ICD-10 trainingdata in the foreseeable future, this line of investi-gation holds potential for significant gains in auto-coding performance.For the choice of features used in the code con-fidence model, the most prominent contribution ismade by the feature that generalize beyond spe-cific codes, as discussed in Section 4.4.
Addingthese features yields a 10% relative improvementover the set of features equivalent to a one-vs-all model.
In fact, using the generalized featuresalone (see the row marked ?no code-bound fea-tures?
in Table 2) gives a score only 0.02 lowerthan the best result.
As would be expected, gener-alized features are particularly important for codeswith limited training data.
Thus, if we restrictour attention to codes with fewer than 25 traininginstances (which account for 95% of the uniquecodes in our ICD-10 training data), we find thatgeneralized features yielded a 25% relative im-provement over the one-vs-all model (0.247 to0.309).
In contrast, for codes with over 100 train-ing instances (which account for 1% of the uniquecodes, but 36% of the total code volume in ourcorpus) the relative improvement from generalizedfeatures is less than 4% (0.843 to 0.876).
Thesenumbers afford two further observations.
First,the model can be improved dramatically by addinga few dozen EHRs per code to the training cor-pus.
Secondly, there is still much room for re-search in mitigating the effects of data sparsityand improving prediction accuracy for less com-mon codes.
Elsewhere in Table 2 we see thatquantization-based features contribute a modestpredictive value.Perhaps the most surprising result of the seriescame from investigating the options for using theavailable ICD-10 training data, which act as train-ing material both for concept confidence modelsand the code confidence model.
The danger oftraining both type of models on the same corpusis intuitively apparent.
If the training instancesfor the code model are generated by concept mod-els whose training data included the same EHRs,the accuracy of these concept predictions may not65reflect out-of-sample performance of the conceptmodels, causing the code model to rely on themexcessively.The simplest implementation of Wolpert?sstacked generalization proposal, which is intendedto guard against this risk, is to use one part of thecorpus to train one predictive layer and use its pre-dictions on the another part of the corpus to trainthe other layer.
The result in Table 2 (see therow marked ?half & half data split?)
shows thatthe resulting increase in sparsity of the trainingdata for both models leads to a major degradationof the system?s performance, even though at run-time concept models trained on all available dataare used.
We also investigated a cross-validationversion of stacked generalization designed to mit-igate against this fragmentation of training data.We trained a separate set of concept models on thetraining portion of each cross-validation fold, andran them on the held-out portion.
The training setfor the code confidence model was then obtainedby combining these held-out portions.
At run-time, concept models trained on all of the avail-able data were used.
However, as intuitively com-pelling as the arguments motivating this proceduremay be, the results were not competitive with thebaseline approach of using all available trainingdata for all the models.Finally, we found that an `2regularizer per-formed clearly better than an `1regularizer for thecode confidence model, even though we set the `2trade-off constant to 1 and tuned the `1trade-offconstant on the development test set.
This is incontrast to concept confidence models, where weobserved slightly better results with `1regulariza-tion than with `2regularization.6 ConclusionWe have described a system for predicting ICD-10-PCS codes from the clinical narrative con-tained in EHRs.
The proposed approach seeks tomitigate the sparsity of training data with manu-ally assigned ICD-10-PCS codes in three ways:through an intermediate abstraction to clinicalconcepts, through the use of data with ICD-9codes to train concept confidence models, andthrough the use of a code confidence modelwhose parameters can generalize beyond individ-ual codes.
Our experiments show promising re-sults and point out directions for further research.AcknowledgmentsWe would like to thank Ron Mills for provid-ing the crosswalk between ICD-10-PCS codes andclinical concepts; Guoli Wang, Michael Nossal,Kavita Ganesan, Joel Bradley, Edward Johnson,Lyle Schofield, Michael Connor, Jean Stoner andRoxana Safari for helpful discussions relating tothis work; and the anonymous reviewers for theirconstructive criticism.ReferencesSean Benson.
2006.
Computer-assisted Coding Soft-ware Improves Documentation, Coding, Compli-ance, and Revenue.
Perspectives in Health Infor-mation Management, CAC Proceedings, Fall 2006.Centers for Medicare & Medicaid Services.
2014.General Equivalence Mappings.
Documentationfor Technical Users.
Electronically published atcms.gov.Chute CG, Yang Y, Buntrock J.
1994.
An evalua-tion of computer assisted clinical classification algo-rithms.
Proc Annu Symp Comput Appl Med Care.,1994:162?6.Jianfeng Gao, Galen Andrew, Mark Johnson, KristinaToutanova.
2007.
A Comparative Study of Param-eter Estimation Methods for Statistical Natural Lan-guage Processing.
ACL 2007.Daniel T. Heinze, Mark L. Morsch, Ronald E. Shef-fer, Jr., Michelle A. Jimmink, Mark A. Jennings,William C. Morris, and Amy E. W. Morsch.
2000.LifeCodeTM?
A Natural Language Processing Sys-tem for Medical Coding and Data Mining.
AAAIProceedings.Daniel T. Heinze, Mark Morsch, Ronald Sheffer,Michelle Jimmink, Mark Jennings, William Mor-ris, and Amy Morsch.
2001.
LifeCode: A De-ployed Application for Automated Medical Coding.AI Magazine, Vol 22, No 2.HIMSS/WEDI.
2013.
ICD-10 National Pilot Pro-gram Outcomes Report.
Electronically published athimss.org.Yuankai Jiang, Michael Nossal, and Philip Resnik.2006.
How Does the System Know It?s Right?Automated Confidence Assessment for CompliantCoding.
Perspectives in Health Information Man-agement, Computer Assisted Coding ConferenceProceedings, Fall 2006.Leah Larkey and W. Bruce Croft.
1995.
Automatic As-signment of ICD9 Codes To Discharge Summaries.Technical report, Center for Intelligent InformationRetrieval at University of Massachusetts.66Andrew Kachites McCallum.
2002.
MAL-LET: A Machine Learning for Language Toolkit.http://mallet.cs.umass.eduMedori, Julia and Fairon, C?edrick.
2010.
MachineLearning and Features Selection for Semi-automaticICD-9-CM Encoding.
Proceedings of the NAACLHLT 2010 Second Louhi Workshop on Text and DataMining of Health Documents, 2010: 84?89.Ronald E. Mills.
2013.
Methods using multi-dimensional representations of medical codes.
USPatent Application US20130006653.S.V.
Pakhomov, J.D.
Buntrock, and C.G.
Chute.
2006.Automating the assignment of diagnosis codes topatient encounters using example-based and ma-chine learning techniques.
J Am Med Inform Assoc,13(5):516?25.Adler Perotte, Rimma Pivovarov, Karthik Natarajan,Nicole Weiskopf, Frank Wood, No?emie Elhadad .2014.
Diagnosis code assignment: models and eval-uation metrics.
J Am Med Inform Assoc, 21(2):231?7.Pestian, JP, Brew C, Matykiewicz P, Hovermale DJ,Johnson N, Bretonnel Cohen K, and Duch W. 2007.A shared task involving multi-label classificationof clinical free text.
Proceedings ACL: BioNLP,2007:97?104.Philip Resnik, Michael Niv, Michael Nossal, GregorySchnitzer, Jean Stoner, Andrew Kapit, and RichardToren.
2006.
Using intrinsic and extrinsic metricsto evaluate accuracy and facilitation in computer-assisted coding.. Perspectives in Health Informa-tion Management, Computer Assisted Coding Con-ference Proceedings, Fall 2006.Berthier Ribeiro-Neto, Alberto H.F. Laender and Lu-ciano R.S.
de Lima.
2001.
An experimental studyin automatically categorizing medical documents.Journal of the American Society for Information Sci-ence and Technology, 52(5): 391?401.Mary H. Stanfill, Margaret Williams, Susan H. Fenton,Robert A. Jenders, and William R. Hersh.
2010.A systematic literature review of automated clinicalcoding and classification systems.
J Am Med InformAssoc., 17(6): 646?651.Michael Subotin.
2011.
An exponential translationmodel for target language morphology.
ACL 2011.David H. Wolpert.
1992.
Stacked Generalization.Neural Networks, 5:241?259.67
