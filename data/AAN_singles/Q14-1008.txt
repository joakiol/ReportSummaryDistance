Exploring the Role of Stress in Bayesian Word Segmentation using AdaptorGrammarsBenjamin Bo?rschinger1,2 Mark Johnson1,31Department of Computing, Macquarie University, Sydney, Australia2Department of Computational Linguistics, Heidelberg University, Heidelberg, Germany3Santa Fe Institute, Santa Fe, USA{benjamin.borschinger|mark.johnson}@mq.edu.auAbstractStress has long been established as a major cuein word segmentation for English infants.
Weshow that enabling a current state-of-the-artBayesian word segmentation model to take ad-vantage of stress cues noticeably improves itsperformance.
We find that the improvementsrange from 10 to 4%, depending on both theuse of phonotactic cues and, to a lesser ex-tent, the amount of evidence available to thelearner.
We also find that in particular earlyon, stress cues are much more useful for ourmodel than phonotactic cues by themselves,consistent with the finding that children doseem to use stress cues before they use phono-tactic cues.
Finally, we study how the model?sknowledge about stress patterns evolves overtime.
We not only find that our model cor-rectly acquires the most frequent patterns rel-atively quickly but also that the Unique StressConstraint that is at the heart of a previouslyproposed model does not need to be built inbut can be acquired jointly with word segmen-tation.1 IntroductionAmong the first tasks a child language learner has tosolve is picking out words from the fluent speechthat constitutes its linguistic input.1 For English,stress has long been claimed to be a useful cuein infant word segmentation (Jusczyk et al., 1993;Jusczyk et al., 1999b), following the demonstra-1The datasets and software to replicate our experimentsare available from http://web.science.mq.edu.au/?bborschi/tion of its effectiveness in adult speech process-ing (Cutler et al., 1986).
Several studies haveinvestigated the role of stress in word segmenta-tion using computational models, using both neu-ral network and ?algebraic?
(as opposed to ?statis-tical?)
approaches (Christiansen et al., 1998; Yang,2004; Lignos and Yang, 2010; Lignos, 2011; Lig-nos, 2012).
Bayesian models of word segmenta-tion (Brent, 1999; Goldwater, 2007), however, haveuntil recently completely ignored stress.
The soleexception in this respect is Doyle and Levy (2013)who added stress cues to the Bigram model (Gold-water et al., 2009), demonstrating that this leads toan improvement in segmentation performance.
Inthis paper, we extend their work and show how tointegrate stress cues into the flexible Adaptor Gram-mar framework (Johnson et al., 2007).
This allowsus to both start from a stronger baseline model andto investigate how the role of stress cues interactswith other aspects of the model.
In particular, wefind that phonotactic cues to word-boundaries inter-act with stress cues, indicating synergistic effects forsmall inputs and partial redundancy for larger in-puts.
Overall, we find that stress cues add roughly6% token f-score to a model that does not accountfor phonotactics and 4% to a model that already in-corporates phonotactics.
Relatedly and in line withthe finding that stress cues are used by infants be-fore phonotactic cues (Jusczyk et al., 1999a), we ob-serve that phonotactic cues require more input thanstress cues to be used efficiently.
A closer look atthe knowledge acquired by our models shows thatthe Unique Stress Constraint of Yang (2004) can beacquired jointly with segmenting the input instead93Transactions of the Association for Computational Linguistics, 2 (2014) 93?104.
Action Editor: Stefan Riezler.Submitted 12/2013; Published 2/2014.
c?2014 Association for Computational Linguistics.of having to be pre-specified; and that our modelscorrectly identify the predominant stress pattern ofthe input but underestimate the frequency of iambicwords, which have been found to be missegmentedby infant learners.The outline of the paper is as follows.
In Section 2we review prior work.
In Section 3 we introduce ourown models.
In Section 4 we explain our experimen-tal evaluation and its results.
Section 5 discusses ourfindings, and Section 6 concludes and provides somesuggestions for future research.2 Background and related workLexical stress is the ?accentuation of syllableswithin words?
(Cutler, 2005) and has long been ar-gued to play an important role in adult word recog-nition.
Following Cutler and Carter (1987)?s obser-vation that stressed syllables tend to occur at the be-ginnings of words in English, Jusczyk et al.
(1993)investigated whether infants acquiring English takeadvantage of this fact.
Their study demonstratedthat this is indeed the case for 9 month olds, al-though they found no indication of using stressedsyllables as cues for word boundaries in 6 montholds.
Their findings have been replicated and ex-tended in subsequent work (Jusczyk et al., 1999b;Thiessen and Saffran, 2003; Curtin et al., 2005;Thiessen and Saffran, 2007).
A brief summaryof the key findings is as follows: English infantstreat stressed syllables as cues for the beginnings ofwords from roughly 7 months of age, suggesting thatthe role played by stress needs to be acquired, andthat this requires antecedent segmentation by non-stress-based means (Thiessen and Saffran, 2007).They also exhibit a preference for low-pass filteredstress-initial words from this age, suggesting that itis indeed stress and not other phonetic or phono-tactic properties that are treated as a cue for word-beginnings (Jusczyk et al., 1993).
In fact, phontacticcues seem to be used later than stress cues (Jusczyket al., 1999a) and seem to be outweighed by stresscues (Mattys and Jusczyk, 2000).The earliest computational model for word seg-mentation incorporating stress cues we are aware ofis the recurrent network model of Christiansen et al.
(1998) and Christiansen and Curtin (1999).
Theyonly reported a word-token f-score of 44% (roughly,segmentation accuracy: see Section 4), which isconsiderably below the performance of subsequentmodels, making a direct comparison complicated.Yang (2004) introduced a simple incremental algo-rithm that relies on stress by embodying a UniqueStress Constraint (USC) that allows at most a sin-gle stressed syllable per word.
On pre-syllabifiedchild directed speech, he reported a word token f-score of 85.6% for a non-statistical algorithm thatexploits the USC.
While the USC has been arguedto be near-to-universal and follows from the ?cul-minative function of stress?
(Fromkin, 2001; Cutler,2005), the high score Yang reported crucially de-pends on every word token carrying stress, includingfunction words.
More recently, Lignos (2010, 2011,2012) further explored Yang?s original algorithm,taking into account that function words should notbe assumed to possess lexical stress cues.
Whilehis scores are in line with those reported by Yang,the importance of stress for this learner were moremodest, providing a gain of around 2.5% (Lignos,2011).
Also, the Yang/Lignos learner is unable toacquire knowledge about the role stress plays in thelanguage, e.g.
that stress tends to fall on particularpositions within words.Doyle and Levy (2013) extend the Bigrammodel of Goldwater et al.
(2009) by adding stress-templates to the lexical generator.
A stress-templateindicates how many syllables the word has, andwhich of these syllables (if any) are stressed.
Thisallows the model to acquire knowledge about thestress patterns of its input by assigning differentprobabilities to the different stress-templates.
How-ever, Doyle and Levy (2013) do not directly exam-ine the probabilities assigned to the stress-templates;they only report that their model does slightly preferstress-initial words over the baseline model by cal-culating the fraction of stress-initial word types inthe output segmentations of their models.
They alsodemonstrate that stress cues do indeed aid segmen-tation, although their reported gain of 1% in tokenf-score is even smaller than that reported by Lig-nos (2011).
Our own approach differs from theirsin assuming phonemic rather than pre-syllabified in-put (although our model could, trivially, be run onsyllabified input as well) and makes use of Adap-tor Grammars instead of the Goldwater et al.
(2009)Bigram model, providing us with a flexible frame-work for exploring the usefulness of stress in differ-94ent models.Adaptor Grammar (Johnson et al., 2007) isa grammar-based formalism for specifying non-parametric hierarchical models.
Previous work ex-plored the usefulness of, for example, syllable-structure (Johnson, 2008b; Johnson and Goldwa-ter, 2009) or morphology (Johnson, 2008b; Johnson,2008a) in word segmentation.
The closest work toour own is Johnson and Demuth (2010) who investi-gate the usefulness of tones for Mandarin phonemicsegmentation.
Their way of adding tones to a modelof word segmentation is very similar to our way ofincorporating stress.3 ModelsWe give an intuitive description of the mathemati-cal background of Adaptor Grammars in 3.1, refer-ring the reader to Johnson et al.
(2007) for technicaldetails.
The models we examine are derived fromthe collocational model of Johnson and Goldwater(2009) by varying three parameters, resulting in 6models: two baselines that do not take advantage ofstress cues and either do or do not use phonotactics,as described in Section 3.2; and four stress modelsthat differ with respect to the use of phonotactics,and as to whether they embody the Unique StressConstraint introduced by Yang (2004).
We describethese models in section 3.3.3.1 Adaptor GrammarsBriefly, an Adaptor Grammar (AG) can be seen asa probabilistic context-free grammar (PCFG) with aspecial set of adapted non-terminals.
We use un-derlining to distinguish adapted non-terminals (X )from non-adapted non-terminals (Y ).
The distri-bution for each adapted non-terminal X is drawnfrom a Pitman-Yor Process which takes as its base-distribution the tree-distribution over trees rootedin X as defined by the PCFG.
As an effect, eachadapted non-terminal can be seen as having associ-ated with it a cache of previously-generated subtreesthat can be reused without having to be regeneratedusing the individual PCFG rules.
This allows AGs tolearn reusable sub-trees such as words, sequences ofwords, or smaller units such as Onsets and Codas.Thus, while ordinary PCFGs have a finite numberof parameters (one probability for each rule), Adap-tor Grammars in addition have a parameter for everypossible complete tree rooted in any of its adaptednon-terminals, leading to a potentially infinite num-ber of such parameters.
The Pitman-Yor Process in-duces a rich-get-richer dynamics, biasing the modeltowards identifying a small set of units that can bereused as often as possible.
In the case of word seg-mentation, the model will try to identify as compacta lexicon as possible to segment the unsegmentedinput.3.2 Baseline modelsOur starting point is the state-of-the-art AG modelfor word segmentation, Johnson and Goldwater(2009)?s colloc3-syll model, reproduced in Fig-ure 1.2 The model assumes that words are groupedinto larger collocational units that themselves can begrouped into even larger collocational units.
Thisaccounts for the fact that in natural language, thereare strong word-to-word dependencies that need tobe accounted for if severe undersegmentations ofthe form ?is in the?
are to be avoided (Goldwater,2007; Johnson and Goldwater, 2009; Bo?rschinger etal., 2012).
It also uses a language-independent formof syllable structure to constrain the space of possi-ble words.
Finally, this model can learn word-initialonsets and word-final codas.
In a language like En-glish, this ability provides additional cues to word-boundaries as certain onsets are much more likelyto occur word-initially than medially (e.g.
?bl?
in?black?
), and analogously for certain codas (e.g.?dth?
in ?width?
or ?ngth?
in ?strength?
).We define an additional baseline model by replac-ing rules (5) and (6) by (17), and deleting rules (7) to(12).
This removes the model?s ability to use phono-tactic cues to word-boundaries.Word ?
Syll ( Syll ) ( Syll ) ( Syll ) (17)We refer to the model in Figure 1 as the colloc3-phon model, and the model that results from sub-stituting and removing rules as described as thecolloc3-nophon model.
Alternatively, one couldlimit the models ability to capture word-to-word de-pendencies by removing rules (1) to (3).
This results2We follow Johnson and Goldwater (2009) in limiting thelength of possible words to four syllables to speed up runtime.In pilot experiments, this choice did not have a noticeable effecton segmentation performance.95Collocations3 ?
Collocation3+ (1)Collocation3 ?
Collocation2+ (2)Collocation2 ?
Collocation+ (3)Collocation ?
Word+ (4)Word ?
SyllIF (5)Word ?
SyllI ( Syll ) ( Syll ) SyllF (6)SyllIF ?
(OnsetI )RhymeF (7)SyllI ?
(OnsetI )Rhyme (8)SyllF ?
(Onset )RhymeF (9)CodaF ?
Consonant+ (10)RhymeF ?
Vowel (CodaF ) (11)OnsetI ?
Consonant+ (12)Syll ?
(Onset )Rhyme (13)Rhyme ?
Vowel (Coda ) (14)Onset ?
Consonant+ (15)Coda ?
Consonant+ (16)Figure 1: The baseline model.
We use regular-expressionnotation to abbreviate multiple rules.
X {n} stands for upto n repetitions of X , brackets indicate optionality, andX+ stands for one or more repetitions of X .
X indicatesan adapted non-terminal.
Rules that introduce terminalsfor the pre-terminals Vowel , Consonant are omitted.Refer to the main text for an explanation of the grammar.in the colloc-model (Johnson, 2008b) that has previ-ously been found to behave similarly to the Bigrammodel used in Doyle and Levy (2013) (Johnson,2008b; Bo?rschinger et al., 2012).
We performed ex-periments with the colloc-model as well and foundsimilar results to Doyle and Levy (2013) which are,while overall worse, similar in trend to the resultsobtained for the colloc3-models.
For the rest of thepaper, therefore, we will focus on variants of thecolloc3-model.3.3 Stress-based modelsIn order for stress cues to be helpful, the model musthave some way of associating the position of stresswith word-boundaries.
Intuitively, the reason stresshelps infants in segmenting English is that a stressedsyllable is a reliable indicator of the beginning ofa word (Jusczyk et al., 1993).
More generally, ifthere is a (reasonably) reliable relationship betweenthe position of stressed syllables and beginnings (orWord ?
{SSyll | USyll }{1,4} (18)SSyll ?
(Onset )RhymeS (19)USyll ?
(Onset )RhymeU (20)RhymeS ?
Vowel ?
( Coda ) (21)RhymeU ?
Vowel (Coda ) (22)Onset ?
Consonant+ (23)Coda ?
Consonant+ (24)Figure 2: Description of the all-stress-patterns model.
Weuse X {m,n} for ?at least m and at most n repetitions ofX ?
and {X | Y } for ?either X or Y ?.
Stress is asso-ciated with a vowel by suffixing it with the special termi-nal symbol ?
, leading to a distinction between stressed(SSyll ) and unstressed (USyll ) syllables.
A word canconsist of any possible sequence of up to four syllables,as indicated by the regular-expression notation.
By ad-ditionally adding initial and final variants of SSyll andUSyll as in Figure 1, phonotactics can be combined withstress cues.endings) of words, a learner might exploit this rela-tionship.
In a Bayesian model, this intuition can becaptured by modifying the lexical generator, that is,the distribution that generates Word s.Here, changing the lexical generator correspondsto modifying the rules expanding Word .
A straight-forward way to modify it accordingly is to enu-merate all possible sequences of stressed and un-stressed syllables.3 A lexical generator like this isgiven in Figure 2.
In the data, stress cues are rep-resented using a special terminal ?
?
?
that followsa stressed vowel, as illustrated in Figure 3.
In thegrammar, ?
?
?
is constrained to only surface follow-ing a Vowel , rendering a syllable in which it occursstressed (SSyll ).
Syllables that do not contain a ?
?
?are considered unstressed (USyll ).
By performinginference for the probabilities assigned to the dif-ferent expansions of rule (18), our models can, forexample, learn that a bi-syllabic word that is stress-initial (a trochee) is more probable than one that putsstress on the second syllable (an iamb).
This (partly)captures the tendency of English for stress-initialwords and thus provide an additional cue for identi-fying words; and it is exactly the kind of preferenceinfant learners of English seem to acquire (Jusczyk3This is, in essence, also the strategy chosen by Doyle andLevy (2013).96grammar phon stress USCcolloc3-nophoncolloc3-phon ?colloc3-nophon-stress ?colloc3-phon-stress ?
?colloc3-nophon-stress-usc ?
?colloc3-phon-stress-usc ?
?
?Table 1: The different models used in our experiments.?phon?
indicates whether phonotactics are used, ?stress?whether stress cues are used and ?usc?
whether theUnique Stress Constraint is assumed.orthographic the do-gieno-stress dh ah d ao g iystress dh ah d ao * g iyFigure 3: Illustration of the input-representation wechoose.
We indicate primary stress according to the dic-tionary with bold-face in the orthography.
The phonemictranscription uses ARPABET and is produced using anextended version of CMUDict.
Primary stress is indi-cated by inserting the special symbol ?*?
after the vowelof a stressed syllable.et al., 1993).We can combine this lexical generator with thecolloc3-nophon baseline, resulting in the colloc3-nophon-stress model.
We can also add phonotac-tics to the lexical generator in Figure 2 by addinginitial and final variants of SSyll and USyll , anal-ogous to rules (5) to (12) in Figure 1.
This yieldsthe colloc3-phon-stress model.
We can also addthe Unique Stress Constraint (USC) (Yang, 2004)by excluding all variants of rule (18) that generatetwo or more stressed syllables.
For example, whilethe lexical generator for the colloc3-nophon-stressmodel will include the rule Word ?
SSyll SSyll ,the lexical generator embodying the USC lacks thisrule.
We refer to the models that include the USC ascolloc3-nophon-stress-usc and colloc3-phon-stress-usc models.
A compact overview of the six differentmodels is given in Table 1.4 ExperimentsWe evaluate our models on several corpora of childdirected speech.
We first describe the corpora weused, then the experimental methodology employedand finally the experimental results.
As the trend iscomparable across all corpora, we only discuss indetail results obtained on the Alex corpus.
For com-pleteness, however, Table 3 reports the ?standard?evaluation of performing inference over all of thethree corpora.4.1 Corpora and corpus creationFollowing Christiansen et al.
(1998) and Doyle andLevy (2013), we use the Korman corpus (Korman,1984) as one of our corpora.
It comprises child-directed speech for very young infants, aged be-tween 6 and 16 weeks and, like all other cor-pora used in this paper, is available through theCHILDES database (MacWhinney, 2000).
We de-rive a phonemicized version of the corpus usingan extended version of CMUDict (Carnegie MellonUniversity, 2008)4, as we were unable to obtain thestress-annotated version of this corpus used in previ-ous experiments.
The phonemicized version is pro-duced by replacing each orthographic word in thetranscript with the first pronunciation given by thedictionary.
CMUDict also annotates lexical stress,and we use this information to add stress cues to thecorpus.
We only code primary lexical stresses in theinput, ignoring secondary stresses in line with ex-perimental work that indicates that human listenersare capable of reliably distinguishing primary andsecondary stress (Mattys, 2000).
Due to the verylow frequency of words with 3 or more syllables inthese corpora, this choice has very little effect on thenumber of stress cues available in the input.
Our ver-sion of the Korman corpus contains, in total, 11413utterances.
Unlike Christiansen et al.
(1998), Yang(2004), and Doyle and Levy (2013), we follow Lig-nos and Yang (2010) in making the more realistic as-sumption that the 94 mono-syllabic function wordslisted by Selkirk (1984) never surface with lexicalstress.
As function words account for roughly 50%of the tokens but only roughly 5% of the types in ourcorpora, this means that the type and token distribu-tion of stress patterns differs dramatically in all ourcorpora, as can be seen from Table 2.We also added stress information to the Brent-Bernstein-Ratner corpus (Bernstein-Ratner, 1987;Brent, 1999), following the procedure just out-lined.
This corpus is a de-facto standard for evaluat-4http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict.0.7a97Pattern brent korman alexTok Typ Tok Typ Tok TypW+ .48 .07 .47 .08 .44 .05SW?
.49 .86 .49 .86 .52 .87WSW?
.03 .07 .03 .06 .04 .07Other .00 .00 .00 .00 .00 .00Table 2: Relative frequencies for stress patterns for thecorpora used in our study.
X?
stands for 0 or more, X+for one or more repetitions of X , and S for a stressed andW for an unstressed syllable.
Note the stark asymmetrybetween type and token frequencies for unstressed words.Up to two-decimal places, patterns other than the onesgiven have relative frequency 0.00 (frequencies might notsum to 1 as an artefact of rounding to 2 decimal places).ing models of Bayesian word segmentation (Brent,1999; Goldwater, 2007; Goldwater et al., 2009;Johnson and Goldwater, 2009), comprising in total9790 utterances.As our third corpus, we use the Alex portionof the Providence corpus (Demuth et al., 2006;Bo?rschinger et al., 2012).
A major benefit of theProvidence corpus is that the video-recordings fromwhich the transcripts were produced are availablethrough CHILDES alongside the transcripts.
Thiswill allow future work to rely on even more realis-tic stress cues that can be derived directly from theacoustic signal.
While beyond the scope of this pa-per, we believe choosing a corpus that makes richerinformation available will be important for futurework on stress (and other acoustic) cues.
Anothermajor benefit of the Alex corpus is that it provideslongitudinal data for a single infant, rather than be-ing a concatenation of transcripts collected frommultiple children, such as the Korman and the Brent-Bernstein-Ratner corpus.
In total, the Alex corpuscomprises 17948 utterances.Note that despite the differences in age of the in-fants and overall make-up of the corpora, the dis-tribution of stress patterns across the corpora isroughly the same, as shown by Table 2 for the first10,000 utterances of each of the corpora.
This sug-gests that the distribution of stress patterns both at atoken and type level is a robust property of Englishchild-directed speech.4.2 Evaluation procedureThe aim of our experiments is to understand thecontribution of stress cues to the Bayesian wordsegmentation models described in Section 3.
Toget an idea of how input size interacts with this,we look at prefixes of the corpora with increasingsizes (100, 200, 500, 1000, 2000, 5000, and 10,000utterances).
In addition, we are interested in under-standing what kind of stress pattern preferences ourmodels acquire.
For this, we also collect samples ofthe probabilities assigned to the different expansionsof rule (18), allowing us to examine this directly.The standard evaluation of segmentation models in-volves having them segment their input in an un-supervised manner and evaluating performance onhow well they segmented that input.
We addition-ally evaluate the models on a test set for each cor-pus.
Use of a separate test set has previously beensuggested as a means of testing how well the knowl-edge a learner acquired generalizes to novel utter-ances (Pearl et al., 2011), and is required for the kindof comparison across different sizes of input we areinterested in to determine whether there the role ofstress cues interacts with the input size.We create the test-sets by taking the final 1000 ut-terances for each corpus.
These 1000 utterances willbe segmented by the model after it has performedinference on its input, without making any furtherchanges to the lexicon that the model has induced.In other words, the model will have to segment eachof the test utterances using only the lexicon (and anyadditional knowledge about co-occurrences, phono-tactics, and stress) it has acquired from the trainingportion of the corpus during inference.We measure segmentation performance using thestandard metric of token f-score (Brent, 1999) whichis the harmonic mean of token precision and recall.Token f-score provides an overall impression of howaccurate individual word tokens were identified.
Toillustrate, if the gold segmentation is ?the dog?, thesegmentation ?th e dog?
has a token precision of 13(one out of three predicted words is correct); a tokenrecall of 12 (one of the two gold words was correctlyidentified); and a token f-score of 0.4.4.3 InferenceFor inference, we closely follow Johnson and Gold-water (2009): we put vague priors on all the hyper-98p s usc alex korman brenttrain test train test train test.81 .81 .85 .83 .82 .82?
.85 .84 .86 .84 .86 .86?
.86 .87 .87 .86 .86 .87?
?
.88 .88 .88 .87 .87 .87?
?
.87 .88 .87 .88 .86 .87?
?
?
.88 .88 .88 .87 .87 .88Table 3: Token f-scores on both train and test portionsfor all three corpora when inference is performed overthe full corpus.
Note that the benefit of stress is clearerwhen evaluating on the test set, and that overall, perfor-mance of the different models is comparable across allthree corpora.
Models are coded according to the key inTable 1.parameters of our models and run 4 chains for 1000iterations, collecting 20 samples from each chainwith a lag of 10 iterations between each sample af-ter a burn-in of 800 iterations, using both batch-initialization and table-label resampling to ensuregood convergence of the sampler.
We construct asingle segmentation from the posterior samples us-ing their minimum Bayes risk decoding, providing asingle score for each condition.4.4 Experimental conditionsEach of our six models is evaluated on inputs of in-creasing size, starting at 100 and ending at 10,000utterances, allowing us to investigate both how per-formance and ?knowledge?
of the learner varies asa function of input size.
For completeness, we alsoreport the ?standard?
evaluation, i.e.
performance ofour models on all corpora when trained on the entireinput in Table 3.
We will focus our discussion on theresults obtained on the Alex corpus, which are de-picted in Figure 4, where the input size is depictedon the x-axis, and the segmentation f-score for thetest-set on the y-axis.5 DiscussionWe find a clear improvement for the stress-modelsover both the colloc3-nophon and the colloc3-phonmodels.
As can be seen in Table 3, the overalltrend is the same for all three corpora, both whenevaluating on the input and the separate test-set.55We performed Wilcox rank sum tests on the individualscores of the 4 independent chains for each model on the fulltraining data sets and found that the stress-models were alwaysNote how the relative gain for stress is roughly1% higher when evaluating on the test-set; thismight have to do with Jusczyk (1997)?s observa-tion that the advantage of stress ?might be moreevident for relatively unexpected or unfamiliarizedstrings?
(Jusczyk, 1997).
A closer look at Figure 4indicates further interesting differences between thecolloc3-nophon and the colloc3-phon models thatonly become evident when considering different in-put sizes.5.1 Stress cues without phonotacticsFor the colloc3-nophon models, we observe a rel-atively stable improvement by adding stress cuesof 6-7%, irrespective of input size and whether ornot the Unique Stress Constraint (USC) is assumed.The sole exception to this occurs when the learneronly gets to see 100 utterances: in this case, thecolloc-nophon-stress model only shows a 3% im-provement, whereas the colloc3-nophon-stress-uscmodel obtains a boost of roughly 8%.
Noticeableconsistent differences between the colloc3-nophon-stress and colloc3-nophon-stress-usc model, how-ever, all but disappear starting from around 500 ut-terances.
This is somewhat surprising, consideringthat it is the USC that was argued by Yang (2004) tobe key for taking advantage of stress.6We take this behaviour to indicate that evenwith as little evidence as 200 to 500 utterances,a Bayesian ideal learner can effectively infer thatsomething like the USC is true of English.
Thisalso becomes clear when examining how the learn-ers?
preferences for different stress patterns evolveover time, as we do in Section 5.3 below.5.2 Stress cues and phonotacticsOverall, the models including phonotactic cues per-form better than those that do not rely on phono-tactics.
However, the overall gain contributed bystress to the colloc3-phon baseline is smaller, al-significantly more accurate (p < 0.05) than the baseline modelsexcept when evaluating on the training data for the Korman andBrent corpora.6On data in which function words are marked for stress (asin Yang (2004) and Doyle and Levy (2013)), the USC yields ex-tremely high scores across all models, simply because roughlyevery second word is a function word.
Given that this assump-tion is extremely unnatural, we do not take this as an argumentfor the USC.990.650.700.750.800.85100 200 500 1000 2000 5000 10000number of input utterancessegmentation f?scorecolloc3?nophoncolloc3?phoncolloc3?nophon?stresscolloc3?phon?stresscolloc3?nophon?stress?usccolloc3?phon?stress?uscFigure 4: Segmentation performance of the different models, across different input sizes and as evaluated on thetest-set for the Alex corpus.
The no-stress baselines are given in red, the stress-models without the Unique StressConstraint (USC) in green and the ones including the USC in black.
Solid lines indicate models that use, dashed linesmodels that do not use phonotactics.
Refer to the text for discussion.though this seems to depend on the size of the input.While phonotactics by itself appears to be a pow-erful cue, yielding a noticeable 4-5% improvementover the colloc3-nophon baseline, the learner seemsto require at least around 500 utterances before thecolloc3-phon model becomes clearly more accuratethan the colloc3-nophon model.
In contrast, evenfor only 100 utterances stress cues by themselvesprovide a 3% improvement to the colloc3-nophonmodel, indicating that they can be taken advantageof earlier.
While the number of utterances processedby a Bayesian ideal learner is not directly related todevelopmental stages, this observation is consistentwith the psycholinguists?
claim that phonotactics areused by infants for word segmentation after theyhave begun to use stress for segmentation (Jusczyket al., 1999a).Turning to the interaction between stress andphonotactics, we see that there is no consistent ad-vantage of including the USC in the model.
Thisis, in fact, even clearer than for the colloc3-nophonmodel where at least for small inputs of size 100,the USC added almost 5% in performance.
For thecolloc3-phon models, we only observe a 1-2% im-provement by adding the USC up until 500 utter-ances.
This further strengthens the point that even inthe absence of such an innate constraint, a statisti-cal learner can take advantage of stress cues and, aswe show below, actually acquire something like theUSC from the input.The 4% difference between the colloc3-phon-stress / colloc3-phon-stress-usc models to thecolloc3-phon baseline is smaller than the 7% dif-ference between the colloc3-nophon and colloc3-nophon-stress models.
This shows that there is aredundancy between phonotactic and stress cues inlarge amounts of data, as their joint contribution tothe colloc3-nophon baseline is less than the sum oftheir individual contributions at 10,000 utterances,of 4% (for phonotactics) and 7% (for stress).Unlike for the colloc3-nophon models, we alsosee a clear impact of input size.
In particular, at100 utterances the addition of stress cues leads toan 8 ?
10% improvement, depending on whether ornot the USC is assumed, whereas for the colloc3-nophon model we only observed a 3 ?
8% improve-ment.
This is particularly striking when we con-sider that by themselves, the phonotactic cues onlycontribute a 1% improvement to the colloc3-nophonbaseline when trained on the 100 utterance corpus,100indicating a synergistic interaction (rather than re-dundancy) between phonotactics and stress for smallinputs.
This effect disappears starting from around1000 utterances; for inputs of size 1000 and larger,the net-gain of stress drops from roughly 10% to a3?4% improvement.
That is, while we did not noticeany relationship between input size and impact ofstress cues for the colloc3-nophon model, we do seesuch an interaction for the combination of phonotac-tics and stress cues which, taken together, lead to alarger relative gain in performance on smaller inputsthan on large ones.5.3 Acquisition of stress patternsIn addition to acquiring a lexicon, the Bayesianlearner acquires knowledge about the possible stresspatterns of English words.
The fact that this knowl-edge is explicitly represented through the PCFGrules and their probabilities that define the lexi-cal generator allows us to study the generalisationsabout stress the model actually acquires.
WhileDoyle and Levy (2013) suggest carrying out suchan analysis, they restrict themselves to estimatingthe fraction of stress patterns in the segmented out-put.
As shown in Table 2, however, the type andtoken distributions of stress patterns can differ sub-stantially.
We therefore investigate the stress pref-erences acquired by our learner by examining theprobabilities assigned to the different expansions ofrule (18), aggregating the probabilities of the indi-vidual rules into patterns.
For example, the rulesWord ?
SSyll (USyll ){0,3} correspond to thepattern ?Stress on the first syllable?, whereas therules Word ?
USyll {1,4} correspond to the pat-tern ?Unstressed word?.
By computing the respec-tive probabilities, we get the overall probability as-signed by a learner to the pattern.Figure 5 provides this information for several dif-ferent rule patterns.
Additionally, these plots in-clude the empirical type (red dotted) and token pro-portions (red double-dashed) for the input corpus.Note how for the two major patterns, all models suc-cessfully track the type, rather than the token fre-quency, correctly developing a preference for stress-initial over unstressed words, despite the compa-rable token frequency of these two patterns.
Thisis compatible with a recent proposal by Thiessenand Saffran (2007), who argue that infants infer thestress pattern over their lexicon.
For a Bayesianmodel such as ours or Goldwater et al.
(2009)?s,there is no need to pre-specify that the distributionought to be learned over types rather than tokens, asthe models automatically interpolate between typeand token statistics according to the properties oftheir input (Goldwater et al., 2006).
In addition,a Bayesian framework provides a simple answer tothe question of how a learner might identify the roleof stress in its language without already having ac-quired at least some words.
By combining differ-ent kinds of cues, e.g.
distributional, phonotacticand prosodic, in a principled manner a Bayesianlearner can jointly segment its input and learn theappropriate role of each cue, without having to pre-specify specific preferences that might differ acrosslanguages.The iambic rule pattern that puts stress on the sec-ond syllable is much more infrequent on a tokenlevel.
All models track this low token frequency,underestimating the type frequency of this patternby a fair amount.
This suggests that learning thispattern correctly requires considerably more inputthan for the other patterns.
Indeed, the iambic pat-tern is known to pose problems for infants when theystart using stress as an effective cue.
It is only fromroughly 10 months of age that infants successfullysegment iambic words (Jusczyk et al., 1999b).
Notsurprisingly, the USC doesn?t aid in learning aboutthis pattern because it is completely silent on wherestress might fall (and does not noticeably improvesegmentation performance to begin with).Finally, we can also investigate whether themodels that lack the USC nevertheless learn thatwords contain at most one lexically stressed syl-lable.
The bottom-right graph in Figure 5 plotsthe probability assigned by the models to patternsthat violate the USC.
This includes, for example,the rules Word ?
SyllS SyllS and Word ?SyllS SyllU SyllS .
Note how the probabilities as-signed to these rules approaches zero, indicating thatthe learner becomes more certain that there are nowords that contain more than one syllable with lex-ical stress.
As we argued above, this suggests that aBayesian learner can acquire the USC from a mod-est amount of data ?
it will properly infer that theunnatural patterns are simply not supported by theinput.
To summarize, by examining the internal1010.550.600.650.700.750.800.85100 200 500 1000 2000 5000 10000P(Stresson first)0.020.030.040.050.060.07100 200 500 1000 2000 5000 10000number of input utterancesP(Stresson second)0.050.100.150.200.250.300.350.400.45100 200 500 1000 2000 5000 10000P(Unstressedword)0.050.10100 200 500 1000 2000 5000 10000number of input utterancesP(Violates USC)colloc3?nophon?stress      colloc3?phon?stress      colloc3?nophon?stress?usc      colloc3?phon?stress?uscFigure 5: Evolution of the knowledge the learner acquires on the Alex corpus.
The red dotted line indicates theempirical type distribution of a specific pattern, and the double-dashed line the empirical token distribution.
Top-Left:Stress-initial pattern, Top-Right: Unstressed Words, Bottom-Left: Stress-second pattern, Bottom-Right: Patterns thatviolate the USC.state of the Bayesian learners we can characterisehow their knowledge about the stress preferences oftheir languages develops, rather than merely measur-ing how well they perform word segmentation.
Wefind that the iambic pattern that has been observed topose problems for infant learners also is harder forthe Bayesian learner to acquire, arguably due to itsextremely low token-frequency.6 Conclusion and Future WorkWe have presented Adaptor Grammar models ofword segmentation that are able to take advantageof stress cues and are able to learn from phonemicinput.
We find that phonotactics and stress interactin interesting ways, and that stress cues makes a sta-ble contribution to existing word segmentation mod-els, improving their performance by 4-6% token f-score.
We also find that the USC introduced by Yang(2004) need not be prebuilt into a model but can beacquired by a Bayesian learner from the data.
Sim-ilarly, we directly investigate the stress preferencesacquired by our models and find that for stress-initialand unstressed words, they track type rather thantoken frequencies.
The rare stress-second patternseems to require more input to be properly acquired,which is compatible with infant development data.An important goal for future research is to eval-uate segmentation models on typologically differentlanguages and to study the relative usefulness of dif-ferent cues cross-lingually.
For example, languagessuch as French lack lexical stress; it would be inter-esting to know whether in such a case, phonotactic(or other) cues are more important.
Relatedly, recentwork such as Bo?rschinger et al.
(2013) has foundthat artificially created data often masks the com-plexity exhibited by real speech.
This suggests thatfuture work should use data directly derived fromthe acoustic signal to account for contextual effects,rather than using dictionary look-up or other heuris-tics.
In using the Alex corpus, for which good qual-ity audio is available, we have taken a first step inthis direction.102AcknowledgementsThis research was supported by the AustralianResearch Council?s Discovery Projects fundingscheme (project numbers DP110102506 andDP110102593).
We?d like to thank ProfessorDupoux and our other colleagues at the Laboratoirede Sciences Cognitives et Psycholinguistique inParis for hosting us while this research was per-formed, as well as the Mairie de Paris, the fondationPierre Gilles de Gennes, the Ecole des HautesEtudes en Sciences Sociales, the Ecole NormaleSupe?rieure, The Region Ile de France, the EuropeanResearch Council (ERC-2011-AdG-295810 BOOT-PHON), the Agence Nationale pour la Recherche(ANR-2010-BLAN-1901-1 BOOTLANG, ANR-10-IDEX-0001-02 and ANR-10-LABX-0087) andthe Fondation de France.
We?d also like to thankthree anonymous reviewers for helpful commentsand suggestions.ReferencesN.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.Benjamin Bo?rschinger, Katherine Demuth, and MarkJohnson.
2012.
Studying the effect of input size forBayesian word segmentation on the Providence cor-pus.
In Proceedings of the 24th International Con-ference on Computational Linguistics (Coling 2012),pages 325?340.
Coling 2012 Organizing Committee.Benjamin Bo?rschinger, Mark Johnson, and Katherine De-muth.
2013.
A joint model of word segmentationand phonological variation for English word-final /t/-deletion.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1508?1516.
Associationfor Computational Linguistics.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34:71?105.M.
Christiansen and S. Curtin.
1999.
The power of sta-tistical learning: No need for algebraic rules.
In Pro-ceedings of the 21st Annual Conference of the Cogni-tive Science Society.Morten H Christiansen, Joseph Allen, and Mark S Sei-denberg.
1998.
Learning to segment speech usingmultiple cues: A connectionist model.
Language andCognitive Processes, 13(2-3):221?268.Suzanne Curtin, Toben H Mintz, and Morten H Chris-tiansen.
2005.
Stress changes the representationallandscape: Evidence from word segmentation.
Cog-nition, 96(3):233?262.Anne Cutler and David M Carter.
1987.
The predomi-nance of strong initial syllables in the English vocabu-lary.
Computer Speech and Language, 2(3):133?142.Anne Cutler, Jacques Mehler, Dennis Norris, and JuanSegui.
1986.
The syllable?s differing role in the seg-mentation of French and English.
Journal of Memoryand Language, 25(4):385 ?
400.Anne Cutler.
2005.
Lexical stress.
In David B.Pisoni and Robert E. Remez, editors, The Handbookof Speech Perception, pages 264?289.
Blackwell Pub-lishing.K.
Demuth, J. Culbertson, and J.
Alter.
2006.
Word-minimality, epenthesis, and coda licensing in the ac-quisition of English.
Language and Speech, 49:137?174.Gabriel Doyle and Roger Levy.
2013.
Combining mul-tiple information types in Bayesian word segmenta-tion.
In Proceedings of the 2013 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 117?126.
Association for ComputationalLinguistics.Victoria Fromkin, editor.
2001.
Linguistics: An Intro-duction to Linguistic Theory.
Blackwell, Oxford, UK.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466.MIT Press.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54.Sharon Goldwater.
2007.
Nonparametric Bayesian Mod-els of Lexical Acquisition.
Ph.D. thesis, Brown Uni-versity.Mark Johnson and Katherine Demuth.
2010.
Unsu-pervised phonemic Chinese word segmentation usingAdaptor Grammars.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 528?536.
Coling 2010 Organiz-ing Committee.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Amer-ican Chapter of the Association for Computational103Linguistics, pages 317?325.
Association for Compu-tational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor Grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 641?648.
MIT Press, Cambridge, MA.Mark Johnson.
2008a.
Unsupervised word segmentationfor Sesotho using Adaptor Grammars.
In Proceedingsof the Tenth Meeting of ACL Special Interest Groupon Computational Morphology and Phonology, pages20?27.
Association for Computational Linguistics.Mark Johnson.
2008b.
Using Adaptor Grammars toidentify synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, pages 398?406.
Association for ComputationalLinguistics.Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.1993.
Infants?
preference for the predominant stresspatterns of English words.
Child Development,64(3):675?687.Peter W. Jusczyk, E. A. Hohne, and A. Bauman.
1999a.Infants?
sensitivity to allophonic cues for word seg-mentation.
Perception and Psychophysics, 61:1465?1476.Peter W. Jusczyk, Derek M. Houston, and Mary New-some.
1999b.
The beginnings of word segmentation inEnglish-learning infants.
Cognitive Psychology, 39(3-4):159?207.Peter Jusczyk.
1997.
The discovery of spoken language.MIT Press, Cambridge, MA.Myron Korman.
1984.
Adaptive aspects of maternal vo-calizations in differing contexts at ten weeks.
FirstLanguage, 5:44?45.Constantine Lignos and Charles Yang.
2010.
Reces-sion segmentation: simpler online word segmentationusing limited resources.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 88?97.
Association for Com-putational Linguistics.Constantine Lignos.
2011.
Modeling infant word seg-mentation.
In Proceedings of the Fifteenth Conferenceon Computational Natural Language Learning, pages29?38.
Association for Computational Linguistics.Constantine Lignos.
2012.
Infant word segmentation:An incremental, integrated model.
In Proceedings ofthe West Coast Conference on Formal Linguistics 30.Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk: Volume I: Transcription format andprograms, volume II: The database.
ComputationalLinguistics, 26(4):657?657.Sven L Mattys and Peter W Jusczyk.
2000.
Phonotac-tic cues for segmentation of fluent speech by infants.Cognition, 78(2):91?121.Sven L Mattys.
2000.
The perception of primary andsecondary stress in English.
Perception and Psy-chophysics, 62(2):253?265.Lisa Pearl, Sharon Goldwater, and Mark Steyvers.
2011.Online learning mechanisms for Bayesian models ofword segmentation.
Research on Language and Com-putation, 8(2):107?132.Elisabeth O. Selkirk.
1984.
Phonology and Syntax: TheRelation Between Sound and Structure.
MIT Press.Erik D Thiessen and Jenny R Saffran.
2003.
Whencues collide: use of stress and statistical cues to wordboundaries by 7-to 9-month-old infants.
Developmen-tal Psychology, 39(4):706.Erik D Thiessen and Jenny R Saffran.
2007.
Learning tolearn: Infants acquisition of stress-based strategies forword segmentation.
Language Learning and Develop-ment, 3(1):73?100.Carnegie Mellon University.
2008.
The CMU pronounc-ing dictionary, v.0.7a.Charles Yang.
2004.
Universal grammar, statistics orboth?
Trends in Cognitive Sciences, 8(10):451?456.104
