Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100?1111,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Systematic Exploration of Diversity in Machine TranslationKevin Gimpel?
Dhruv Batra?
Chris Dyer?
Gregory Shakhnarovich?
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA?Virginia Tech, Blacksburg, VA 24061, USA?Carnegie Mellon University, Pittsburgh, PA 15213, USACorresponding author: kgimpel@ttic.eduAbstractThis paper addresses the problem of produc-ing a diverse set of plausible translations.
Wepresent a simple procedure that can be usedwith any statistical machine translation (MT)system.
We explore three ways of using di-verse translations: (1) system combination,(2) discriminative reranking with rich features,and (3) a novel post-editing scenario in whichmultiple translations are presented to users.We find that diversity can improve perfor-mance on these tasks, especially for sentencesthat are difficult for MT.1 IntroductionFrom the perspective of user interaction, the idealmachine translator is an agent that reads documentsin one language and produces accurate, high qual-ity translations in another.
This interaction idealhas been implicit in machine translation (MT) re-search since the field?s inception.
It is the waywe interact with commercial MT services (such asGoogle Translate and Microsoft Translator), and theway MT systems are evaluated (Bojar et al 2013).Unfortunately, when a real, imperfect MT systemmakes an error, the user is left trying to guess whatthe original sentence means.Multiple Hypotheses.
In contrast, when we lookat the way other computer systems consume out-put from MT systems (or similarly unreliable tools),we see a different pattern.
In a pipeline settingit is commonplace to propagate not just a single-best output but the M -best hypotheses (Venugopalet al 2008).
Multiple solutions are also used forreranking (Collins, 2000; Shen and Joshi, 2003;Collins and Koo, 2005; Charniak and Johnson,2005), tuning (Och, 2003), minimum Bayes risk de-coding (Kumar and Byrne, 2004), and system com-bination (Rosti et al 2007).
When dealing witherror-prone systems, knowing about alternatives hasbenefits over relying on only a single output (Finkelet al 2006; Dyer, 2010).Need for Diversity.
Unfortunately, M -best lists area poor surrogate for structured output spaces (Finkelet al 2006; Huang, 2008).
In MT, for exam-ple, many translations on M -best lists are extremelysimilar, often differing only by a single punctua-tion mark or minor morphological variation.
Re-cent work has explored reasoning about sets usingpacked representations such as lattices and hyper-graphs (Macherey et al 2008; Tromble et al 2008;Kumar et al 2009), or sampling translations propor-tional to their probability (Chatterjee and Cancedda,2010).
We argue that the implicit goal behind thesetechniques is to better explore the output space byintroducing diversity into the surrogate set.Overview and Contributions.
In this work, we el-evate diversity to a first-class status and directly ad-dress the problem of generating a set of diverse,plausible translations.
We use the recently pro-posed technique of Batra et al(2012), which pro-duces diverse M -best solutions from a probabilisticmodel using a generic dissimilarity function ?
(?, ?
)that specifies how two solutions differ.
Our first con-tribution is a family of dissimilarity functions forMT that admit simple algorithms for generating di-verse translations.
Other contributions are empiri-cal: we show that diverse translations can lead toimprovements for system combination and discrim-inative reranking.
We also perform a novel human1100post-editing evaluation in order to measure whetherdiverse translations can help users make sense ofnoisy MT output.
We find that diverse translationscan help post-editors produce better outputs for sen-tences that are the most difficult for MT.
While wefocus on machine translation in this paper, we notethat our approach is applicable to other structure pre-diction problems in NLP.2 Preliminaries and NotationLet X denote the set of all strings in a source lan-guage.
For an x ?
X, let Yx denote the set of its pos-sible translations y in the target language.
MT mod-els typically include a latent variable that capturesthe derivational structure of the translation process.Regardless of its specific form, we refer to this vari-able as a derivation h ?
Hx, where Hx is the set ofpossible values of h for x. Derivations are coupledwith translations and we define Tx ?
Yx ?
Hx asthe set of possible ?y,h?
pairs for x.We use a linear model with a parameter vector wand a vector ?
(x,y,h) of feature functions on x, y,and h (Och and Ney, 2002).
The translation of x isselected using a simple decision rule:?y?, h??
= argmax?y,h??Txw??
(x,y,h) (1)where we also maximize over the latent variable hfor efficiency.
Translation models differ in the formof Tx and the choice of the feature functions ?.
Inthis paper we focus on phrase-based (Koehn et al2003) and hierarchical phrase-based (Chiang, 2007)models, which include several bilingual and mono-lingual features, including n-gram language models.3 Diversity in Machine TranslationWe now address the task of producing a set of di-verse high-scoring translations.3.1 Generating Diverse TranslationsWe use a recently proposed technique (Batra et al2012) that constructs diverse lists via a greedy itera-tive procedure as follows.
Let y1 be the model-besttranslation (Eq.
1).
On the m-th iteration, the m-thbest (diverse) translation is obtained as ?ym,hm?
=argmax?y,h??Txw??
(x,y,h) +m?1?j=1?j?
(yj ,y) (2)where ?
is a dissimilarity function and ?j is theweight placed on dissimilarity to previous trans-lation j relative to the model score.
Intuitively,we seek a translation that is highly-scoring underthe model while being different (as measured by?)
from all previous translations.
The ?
param-eters determine the trade-off between model scoreand diversity.
We refer to Eq.
(2) as dissimilarity-augmented decoding.The objective in Eq.
(2) is a Lagrangian relax-ation for an intractable constrained objective speci-fying a minimum dissimilarity ?min between trans-lations in the list, i.e., ?
(yj ,y) ?
?min (Batra etal., 2012).
Instead of setting the dissimilarity thresh-old ?min , we set the weights ?j .
While the formu-lation allows for a different ?j for each previous so-lution j, we simply use a single ?
= ?j for all j.This was also done in the experiments in (Batra etal., 2012).Note that if the dissimilarity function factorsacross the parts of the output variables ?y,h?
in thesame way as the features ?, then the same decod-ing algorithm can be used as for Eq.
(1).
We discussdesign choices for ?
next.3.2 Dissimilarity Functions for MTWhen designing a dissimilarity function ?
(?, ?)
forMT, we want to consider variation both in individ-ual word choice and longer-range sentence structure.We also want a function that can be easily incorpo-rated into extant statistical MT systems.
We proposea dissimilarity function that simply counts the num-ber of times any n-gram is present in both transla-tions, then negates.
Letting q = n?
1:?n(y,y?)
= ?|y|?q?i=1|y?|?q?j=1[[yi:i+q = y?j:j+q]] (3)where [[?]]
is the Iverson bracket (1 if input conditionis true, 0 otherwise) and yi:j is the subsequence of yfrom word i to word j (inclusive).Importantly, Eq.
(2) can be solved with no changeto the decoding algorithm.
The dissimilarity termscan simply be incorporated as an additional lan-guage model in ARPA format that sets the log-probability to the negated count for each n-gramin previous diverse translations, and sets to zeroall other n-grams?
log-probabilities and back-offweights.1101The advantage of this dissimilarity function is itssimplicity.
It can be easily used with any transla-tion system that uses n-gram language models with-out any change to the decoder.
Indeed, we use bothphrase-based and hierarchical phrase-based modelsin our experiments below.4 Related WorkMT researchers have recently started to con-sider diversity in the context of system combina-tion (Macherey and Och, 2007).
Most closely-related is work by Devlin and Matsoukas (2012),who proposed a way to generate diverse transla-tions by varying particular ?traits,?
such as transla-tion length, number of rules applied, etc.
Their ap-proach can be viewed as solving Eq.
(2) with a richerdissimilarity function that requires a special-purposedecoding algorithm.
We chose our n-gram dissimi-larity function due to its simplicity and applicabilityto most MT systems without requiring any changeto decoders.Among other work, Xiao et al(2013) used bag-ging and boosting to get diverse system outputs forsystem combination and Cer et al(2013) used mul-tiple identical systems trained jointly with an objec-tive function that encourages the systems to generatecomplementary translations.There is also similarity between our approach andminimum Bayes risk decoding (Kumar and Byrne,2004), variational decoding (Li et al 2009), andother ?consensus?
decoding algorithms (DeNero etal., 2009).
These all seek a single translation thatis most similar on average to the model?s preferredtranslations.
In this way, they try to capture themodel?s range of beliefs in a single translation.
Weinstead seek a set of translations that, when consid-ered as a whole, similarly express the full range ofthe model?s beliefs about plausible translations forthe input.Also related is work on determinantal point pro-cesses (DPPs; Kulesza and Taskar, 2010), an ele-gant probabilistic model over sets of items that nat-urally prefers diverse sets.
DPPs have been ap-plied to summarization (Kulesza and Taskar, 2011)and discovery of topical threads in document collec-tions (Gillenwater et al 2012).
Unfortunately, inthe structured setting, DPPs make severely restric-tive assumptions on the scoring function, while ourframework does not.5 Experimental SetupWe now embark on an extensive empirical evalua-tion of the framework presented above.
We beginby analyzing our diverse sets of translations, show-ing how they differ from standard M -best lists (Sec-tion 6), followed by three tasks that illustrate how di-versity can be exploited to improve translation qual-ity: system combination (Section 7), discrimina-tive reranking (Section 8), and a novel human post-editing task (Section 9).
In the remainder of this sec-tion, we describe details of our experimental setup.5.1 Language Pairs and DatasetsWe use three language pairs: Arabic-to-English(AR?EN), Chinese-to-English (ZH?EN), andGerman-to-English (DE?EN).
For AR?EN andDE?EN, we used a phrase-based model (Koehn etal., 2003) and for ZH?EN we used a hierarchicalphrase-based model (Chiang, 2007).Each language pair has two tuning and one testset: TUNE1 is used for tuning the baseline sys-tems with minimum error rate training (MERT; Och,2003), TUNE2 is used for training system combin-ers and rerankers, and TEST is used for evaluation.There are four references for AR?EN and ZH?ENand one for DE?EN.For AR?EN, we used data provided by the LDCfor the NIST evaluations, which includes 3.3M sen-tences of UN data and 982K sentences from other(mostly news) sources.
Arabic text was prepro-cessed using an HMM segmenter that splits attachedprepositional phrases, personal pronouns, and thefuture marker (Lee et al 2003).
The common stylis-tic sentence-initial w+ (and) clitic was removed.The resulting corpus contained 130M Arabic tokensand 130M English tokens.
We used the NIST MT06test set as TUNE1, a 764-sentence subset of MT05 asTUNE2, and MT08 as TEST.For ZH?EN, we used 303k sentence pairs fromthe FBIS corpus (LDC2003E14).
We segmentedthe Chinese data using the Stanford Chinese seg-menter (Chang et al 2008) in ?CTB?
mode, givingus 7.9M Chinese tokens and 9.4M English tokens.We used the NIST MT02 test set as TUNE1, MT051102as TUNE2, and MT03 as TEST.For DE?EN, we used data released for theWMT2011 shared task (Callison-Burch et al 2011).German compound words were split using a CRFsegmenter (Dyer, 2009).
We used the WMT2010test set as TUNE1, the 2009 test set as TUNE2, andthe 2011 test set as TEST.5.2 Baseline SystemsWe used the Moses MT toolkit (Koehn et al2007; Hoang et al 2009) with default settingsand features for both phrase-based and hierarchi-cal systems.
Word alignment was done usingGIZA++ (Och and Ney, 2003) in both directions,with the grow-diag-final-and heuristic usedto symmetrize the alignments and a max phraselength of 7 used for phrase extraction.Language models used the target side of the paral-lel corpus in each case augmented with 24.8M lines(601M tokens) of randomly-selected sentences fromthe Gigaword v4 corpus (excluding the NY Timesand LA Times).
We used 5-gram models, estimatedusing the SRI Language Modeling toolkit (Stolcke,2002) with modified Kneser-Ney smoothing (Chenand Goodman, 1998).
The minimum count cut-offfor unigrams, bigrams, and trigrams was 1 and thecut-off for 4-grams and 5-grams was 3.
Languagemodel inference used KenLM (Heafield, 2011).Uncased IBM BLEU was used for evaluation (Pa-pineni et al 2002).
MERT was used to train the fea-ture weights for the baseline systems on TUNE1.
Weused the learned parameters to generate M -best anddiverse lists for TUNE2 and TEST to use for subse-quent experiments.5.3 Diverse List GenerationGenerating diverse translations depends on two hy-perparameters: the n-gram order used by the dissim-ilarity function ?n (?3.2) and the ?j weights on thedissimilarity terms in Eq.
(2).
Though our frame-work permits different ?j for each j, we use a sin-gle ?
value for simplicity, as was also done in (Ba-tra et al 2012).
The values of n and ?
were tunedon a 200 sentence subset of TUNE1 separately foreach language pair (which we call TUNE200), so asto maximize the oracle BLEU score of the diverseAR?EN ZH?EN DE?EN1 best 50.1 36.9 21.820 best 54.0 40.3 24.7200 best 57.5 43.8 27.71000 best 59.8 46.4 29.8unique 20 best 56.6 44.1 26.7unique 200 best 59.6 46.4 29.520 diverse 58.5 46.4 28.620 div ?
10 best 61.3 48.7 30.320 div ?
50 best 63.2 50.6 31.6Table 1: Oracle BLEU scores on TEST for various sizesof M -best and diverse lists.
Unique lists were obtainedfrom 1,000-best lists and therefore may not contain thetarget number of unique translations for all sentences.lists.1 We considered n values in {2, 3, .
.
.
, 9} and?
values in {0.005, 0.01, 0.05, 0.1}.
We give detailson optimal values for these hyperparameters whendiscussing particular tasks below.Though simple, our approach is computationallyexpensive as M grows because it requires decodingM times for each sentence.
So, we assumeM ?
20.But we also extract an N -best list for each of the Mdiverse translations.2 Many MT decoders, includingthe phrase-based and hierarchical implementationsin Moses, permit efficient extraction of N -best lists,so we exploit this to obtain larger lists that still ex-hibit diversity.
But we note that these N -best listsfor each diverse solution are not in themselves di-verse; with more computational power or more effi-cient algorithms (Devlin and Matsoukas, 2012) wecould potentially generate larger, more diverse lists.6 Analysis of Diverse ListsWe now characterize our diverse lists by compar-ing them to M -best lists.
Table 1 shows oracleBLEU scores on TEST for M -best lists, unique M -best lists, and diverse lists of several sizes.
To getunique lists, we first generated 1000-best lists, thenretained only the highest-scoring derivation for eachunique translation.
When comparingM -best and di-verse lists of comparable size, the diverse lists al-1Since BLEU does not decompose additively across seg-ments, we chose translations for individual sentences that max-imized BLEU+1 (Lin and Och, 2004), then computed ?oracle?corpus BLEU of these translations.2We did not consider n-grams from previous N -best listswhen computing the dissimilarity function, but only those fromthe previous diverse translations.11030102030405060700-25 25-36 36-47 47-94%BLEU1-best BLEU bin20 best 20 diverseFigure 1: Median, min, and max BLEU+1 of 20-bestand 20-diverse lists for the ZH?EN test set, divided intoquartiles according to the BLEU+1 score of the 1-besttranslation, and averaged across sentences in each quar-tile.
Heights of the bars show median and ?error bars?indicate max and min.ways have higher oracle BLEU.
The differences arelargest when comparing 20-best lists and 20-diverselists, where they range from 4 to 6 BLEU points.When generating these diverse lists, we used then and ?
values that were tuned for each languagepair to maximize oracle BLEU on TUNE200 for the?20 div ?
50 best?
configuration.
The optimal val-ues of n were 6 for ZH?EN and AR?EN and 7 forDE?EN.3 When instead tuning to maximize oracleBLEU for 20-diverse lists, the optimal n stayed at7 for DE?EN, but increased to 7 for AR?EN and 9for ZH?EN.
These values are noticeably larger thann-gram sizes typically used in language modelingand evaluation.
They suggest that for optimal ora-cle BLEU, translations with long-spanning amountsof repeated material should be avoided, while shortoverlapping n-grams are permitted.Figure 1 shows other statistics on TEST forZH?EN.
Plots for AR?EN and DE?EN are quali-tatively similar.
We divided the TEST sentences intoquartiles based on BLEU+1 of the 1-best transla-tions from the baseline system.
We computed themedian, min, and max BLEU+1 on each list and av-eraged over the sentences in each quartile.
As shownin the plot, the ranges of 20-diverse lists subsumethose of 20-best lists, though the medians of diverse3The optimal values of ?
were 0.005 for AR?EN and 0.01for ZH?EN and DE?EN.
Since these values depend on thescale of the weights learned by MERT, they are difficult to in-terpret in isolation.lists drop when the baseline system has high BLEUscore.
This matches intuition: when the baselinesystem is performing well, forcing it to find differenttranslations is likely to result in worse translations.So we may expect diverse lists to be most helpful formore difficult sentences, a point we return to in ourexperiments below.7 System Combination ExperimentsOne way to evaluate the quality of our diverse listsis to use them in system combination, as was sim-ilarly done by Devlin and Matsoukas (2012) andCer et al(2013).
We use the system combinationframework of Heafield and Lavie (2010b), whichhas an open-source implementation (Heafield andLavie, 2010a).4We use our baseline systems (trained on TUNE1)to generate lists for system combination on TUNE2and TEST.
We compareM -best lists, uniqueM -bestlists, and M -diverse lists, with M ?
{10, 15, 20}.5For each choice of list type and M , we trained thesystem combiner on TUNE2 and tested on TEST withthe learned parameters.
System combination hyper-parameters (whether to use feature length normal-ization; the size of the k-best lists generated by thesystem combiner during tuning, k ?
{300, 600})were chosen to maximize BLEU on TUNE200.
Also,we removed the individual features from thedefault feature set because they correspond to in-dividual systems in the combination; they did notseem appropriate for us since our hypotheses allcome from the same system.The results are shown in Table 2.
Like Devlin andMatsoukas (2012), we see no gain from system com-bination using M -best lists.
We see some improve-ment with unique lists, particularly for AR?EN, al-though it is not consistent across M values.
Butwe see larger improvements with diverse lists forAR?EN and ZH?EN.
For these language pairs, our4The implementation uses MERT to tune parameters, but wefound this to be time-consuming and noisy for the larger featuresets.
So we used a structured support vector machine learningframework instead (described in Section 8), using multiple it-erations of learning interleaved with (system combiner) N -bestlist generation, and accumulating N -best lists across iterations.5Dissimilarity hyperparameters n and ?
were again chosento maximize oracle BLEU on TUNE200, separately for each Mand for each language pair.1104AR?EN ZH?EN DE?EN10 15 20 10 15 20 10 15 20baseline (no system combination) 50.1 36.9 21.8M -best 50.2 50.1 50.0 36.7 36.9 37.0 21.7 21.7 21.8unique M -best (from 1000-best list) 50.6 50.0 50.8 37.1 36.9 37.1 21.8 21.9 21.9M -diverse 51.4 51.2 51.2 37.6 37.6 37.5 22.0 21.8 21.6Table 2: System combination results (%BLEU on TEST).
Size of lists is M ?
{10, 15, 20}.
Highest score in eachcolumn is bold.AR?EN ZH?EN DE?ENq1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4baseline 30.1 44.1 55.1 70.0 15.2 28.9 41.0 57.5 5.3 14.4 23.7 40.915-best 30.1 44.6 55.5 68.8 15.9 29.2 40.5 56.8 6.0 15.0 23.6 40.0unique 15-best 30.4 44.7 55.2 68.4 16.7 29.0 41.2 56.6 5.9 14.9 23.8 40.615-diverse 31.3 45.3 57.8 69.1 17.7 30.6 41.7 56.9 7.6 15.2 23.4 39.6Table 3: System combination results (%BLEU on quartiles of TEST, M = 15).
Source sentences were divided intoquartiles (numbered ?qn?)
according to BLEU+1 of the 1-best translations of the baseline system.
Highest score ineach column is bold.gains are similar to those seen by Devlin and Mat-soukas, but use our simpler dissimilarity function.6For DE?EN, results are similar for all settings anddo not show much improvement from system com-bination.In Table 3, we break down the scores accordingto 1-best BLEU+1 quartiles, as done in Figure 1.7In general, we find the largest gains for the low-BLEU translations.
For the two worst BLEU quar-tiles, we see gains of 1.2 to 2.5 BLEU points, whilethe gains shrink or disappear entirely for the bestquartile.
This may be a worthwhile trade-off: alarge improvement in the worst translations may bemore significant to users than a smaller degredationon sentences that are already being translated well.In addition, quality estimation (Specia et al 2011;Bach et al 2011) could be used to automatically de-termine the BLEU quartile for each sentence.
Thensystem combination of diverse translations might beused only when the 1-best translation is predicted tobe of low quality.8 Reranking ExperimentsWe now turn to discriminative reranking, which hasfrequently been used to easily add rich features toa model.
It has been used for MT with varying de-6They reported +0.8 BLEU from system combination forAR?EN, and saw a further +0.5?0.7 from their new features.7Quartile points are: 39, 49, 61 for AR?EN; 25, 36, and 47for ZH?EN; and 14.5, 21.1, and 30.3 for DE?EN.gree of success (Och et al 2004; Shen et al 2004;Hildebrand and Vogel, 2008); some have attributedits mixed results to a lack of diversity in the M -bestlists traditionally used.
We propose diverse lists as away to address this concern.8.1 Learning FrameworkSeveral learning formulations have been proposedfor M -best reranking.
One commonly-used ap-proach in MT is MERT, used in the reranking ex-periments of Och et al(2004) and Hildebrand andVogel (2008), among others.
We experimented withMERT and other algorithms, including pairwiseranking optimization (Hopkins and May, 2011), butwe found best results using the approach of Yadol-lahpour et al(2013), who used a slack-rescaledstructured support vector machine (Tsochantaridiset al 2005) with L2 regularization.
As a sentence-level loss, we used negated BLEU+1.
We used the1-slack cutting-plane algorithm of Joachims et al(2009) for optimization during learning.8 A moredetailed description of the reranker is provided in thesupplementary material.We used 5-fold cross-validation on TUNE2 tochoose the regularization parameter C from the set{0.01, 0.1, 1, 10}.
We selected the value yieldingthe highest average BLEU score across the held-out8Our implementation uses OOQP (Gertz and Wright, 2003)to solve the quadratic program in the inner loop, which usesHSL, a collection of Fortran codes for large-scale scientificcomputation (www.hsl.rl.ac.uk).1105folds.
This value was then used for one final roundof training on the entirety of TUNE2.
Additionally,we tuned the decision to return the parameters atconvergence or those that produced the highest train-ing corpus BLEU score.
Since we use a sentence-level metric during training (BLEU+1) and a corpus-level metric for final evaluation (BLEU), we foundthat it was often better to return parameters that pro-duced the highest training BLEU score.This tuning procedure was repeated for each fea-ture set and for each list type (M -best or diverse).The test set was not used for any of this tuning.8.2 FeaturesIn addition to the features from the baseline models(14 for phrase-based, 8 for hierarchical), we add 36more for reranking:Inverse Model 1 (INVMOD1): We added the ?in-verse?
versions of the three IBM Model 1 featuresdescribed in Section 2.2 of Hildebrand and Vogel(2008).
The first is the probability of the source sen-tence given the translation under IBM Model 1, thesecond replaces the?with a max in the first fea-ture, and the third computes the percentage of wordswhose lexical translation probability falls below athreshold.
We also include versions of the first 2features normalized by the translation length, for atotal of 5 INVMOD1 features.Large LM (LLM): We created a large 4-gram LMby interpolating LMs from the WMT news data, Gi-gaword, Europarl, and the DE?EN news commen-tary (NC) corpus to maximize likelihood of a held-out development set (WMT08 test set).
We used theaverage per-word log-probability as the single fea-ture function in this category.Syntactic LM (SYN): We used the syntactic treeletlanguage model of Pauls and Klein (2012) to com-pute two features: the translation log probability andthe length-normalized log probability.Finite/Non-Finite Verbs (VERB): We ran the Stan-ford part-of-speech (POS) tagger (Toutanova et al2003) on each translation and added four features:the fraction of words tagged as finite/non-finiteverbs, and the fraction of verbs that are finite/non-finite.99Words tagged as MD, VBP, VBZ, and VBD were countedReranking AR?EN ZH?EN DE?ENfeatures best div best div best divN/A (baseline) 50.1 36.9 21.8None 50.5 50.7 37.3 37.1 21.9 21.6+ INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8+ LLM, SYN 50.5 51.1 37.4 37.3 21.7 21.7+ VERB, DISC 50.4 51.3 37.3 37.3 21.9 22.2+ GOOG 50.7 51.3 36.8 37.1 21.9 22.2+ WCLM 51.2 51.8 37.3 37.4 22.2 22.3Table 4: Reranking results (%BLEU on TEST).Discriminative Word/Tag LMs (DISC): For eachlanguage pair, we generated 10,000-best lists forTUNE1 and computed BLEU+1 for each.
Fromthese lists, we estimated 3- and 5-gram LMs,weighting the n-gram counts by the BLEU+1scores.10 We repeated this procedure except using1 minus BLEU+1 as the weight (learning a languagemodel of ?bad?
translations).
This yielded 4 fea-tures.
The procedure was then repeated using POStags instead of words, for 8 features in total.Google 5-Grams (GOOG): Translations were com-pared to the Google 5-gram corpus (LDC2006T13)to compute: the number of 5-grams that matched,the number of 5-grams that missed, and a set ofindicator features that fire if the fraction of 5-grams that matched in the sentence was greater than{0.05, 0.1, 0.2, .
.
.
, 0.9}, for a total of 12 features.Word Cluster LMs (WCLM): Using an imple-mentation provided by Liang (2005), we performedBrown clustering (Brown et al 1992) on 900k En-glish sentences, including the NC corpus and ran-dom sentences from Gigaword.
We clustered wordsthat appeared at least twice, once with 300 clus-ters and again with 1000.
We then replaced wordswith their clusters in a large corpus consisting ofthe WMT news data, Gigaword, and the NC data.An additional cluster label was used for unknownwords.
For each of the clusterings (300 and 1000),we estimated 5- and 7-gram LMs with Witten-Bellsmoothing (Witten and Bell, 1991).
We added 4 fea-tures to the reranker, one for the log-probability ofthe translation under each of the word cluster LMs.as finite verbs, and VB, VBG, and VBN were non-finite verbs.10Before estimating LMs, we projected the sentence weightsso that the min and max per source sentence were 0 and 1.1106List typeFeaturesNone All20 best 50.3 50.6100 best 50.6 50.8200 best 50.4 51.21000 best 50.5 51.2unique 20 best 50.5 51.2unique 100 best 50.6 51.2unique 200 best 50.4 51.320 diverse 50.5 51.120 div ?
5 best 50.6 51.420 div ?
10 best 50.7 51.320 div ?
50 best 50.7 51.8Table 5: List comparison for AR?EN reranking.8.3 ResultsOur results are shown in Table 4.
We report resultsusing the baseline system alone (labeled ?N/A (base-line)?
), and reranking standard M -best lists and ourdiverse lists.
For diverse lists, we use the ?20 div ?50 best?
lists described in Section 5.3, with the tuneddissimilarity hyperparameters reported in Section 6.In the reranking settings, we also report results with-out adding any additional features (the row labeled?None?
).11The remaining rows add features.
For AR?EN,we see the largest gains, both over the baseline aswell as differences betweenM -best lists and diverselists.
When using all features, we achieve a gainof 0.6 BLEU over M -best reranking and 1.7 BLEUpoints over the baseline system.
The difference of0.6 BLEU is consistent across feature subsets.
Wefound the WCLM features to give the largest in-dividual improvement, with the remaining featuresets each contributing a small amount.
For Chineseand German, the gains and individual differences aresmaller.
Nonetheless, diverse lists appear to be morerobust for these language pairs as features are added.In Table 5, we compare several sizes and types oflists for AR?EN reranking both with no additionalfeatures and with the full set.
We see that using 20-diverse lists nearly matches the performance of 200-best lists.
Also, retaining 50-best lists for each di-verse solution improves BLEU by 0.7.11Though such results have not always been reported in priorwork on reranking, we generally found them to improve overthe baseline, presumably because seeing more data improvesgeneralization ability.Trainbest divTest best 51.2 51.7div 50.5 51.8Table 6: Comparing M -best and diverse lists for train-ing/testing (AR?EN, all features).Thus far, when training the reranker on M -bestlists, we tested it on M -best lists, and similarly fordiverse lists.
Table 6 shows what happens with theother two pairings for AR?EN with the full featureset.
When training on diverse lists, we see very lit-tle difference in BLEU whether testing on M -bestor diverse lists.
This has a practical benefit: we canuse (computationally-expensive) diverse lists duringoffline training and then use fast M -best lists at testtime.
When training on M -best lists and testingon diverse lists, we see a substantial drop (51.2 vs50.5).
The reranker may be overfitting to the limitedscope of translations present in typical M -best lists,thereby hindering its ability to correctly rank diverselists at test time.
These results suggest that part ofthe benefit of using diverse lists comes from seeinga larger portion of the output space during training.9 Human Post-Editing ExperimentsWe wanted to determine whether diverse translationscould be helpful to users struggling to understandthe output of an imperfect MT system.
We con-sider a post-editing task in which users are presentedwith translation output without the source sentence,and are asked to improve it.
This setting has beenstudied; e.g., Koehn (2010) presented evidence thatmonolingual speakers could often produce improvedtranslations for this task, occasionally reaching thelevel of an expert translator.Here, we use a novel variation of this task inwhich multiple translations are shown to editors.
Wecompare the use of entries from an M -best list andentries from a diverse list.
Again, the original sourcesentence is not provided.
Our goal is to determinewhether multiple, diverse translations can help usersto more accurately guess the meaning of the originalsentence than entries from a standard M -best list.
Ifso, commercial MT systems might permit users torequest additional diverse translations for those sen-tences whose model-best translations are difficult tounderstand.11079.1 Translation List Post-EditingWe use Amazon Mechanical Turk (MTurk) for thisexperiment.
Workers are shown 3 outputs from anMT system.
They are not shown the original sen-tence, nor are they shown a reference.
Based onthe 3 imperfect translations, they are asked to writea single fluent English translation that best cap-tures the understood meaning.
Half of the time, theworker is shown 3 entries from an M -best list, andthe other half of the time 3 entries from a diverselist.
We then compare the outputs produced underthe two conditions.
The goal is to measure whetherworkers are able to produce translations that arecloser in meaning to the (unseen) references whenshown diverse translations.
We refer to this task asthe EDITING task.To evaluate the outputs, we use a second task inwhich users are shown a reference translation alongwith two outputs from the first task: one createdfrom M -best lists and one from diverse lists.
Work-ers in this task are asked to choose which translationis a better match to the reference in terms of mean-ing, or they can indicate that the translations are ofthe same quality.
We refer to this second task as theEVAL task.9.2 Dissimilarity FunctionsTo generate diverse lists for the EDITING task, weuse the same dissimilarity function as in reranking,but we tune the hyperparameters n and ?
differently.Since our expectation here is that workers may com-bine information from multiple translations to pro-duce a superior output, we are interested in the cov-erage of the translations in the diverse list, ratherthan the oracle BLEU score.We designed a metric based on coverage of entirelists of translations.
It is similar to BLEU+1, except(1) it uses n-gram recalls instead of n-gram preci-sions, (2) there is no brevity penalty term, and (3) itcompares a list to a set of references and any trans-lation in the list can contribute a match of an n-gramin any reference.
Like BLEU, counts are clippedbased on those in the references.
We maximizedthis metric over diverse lists of length 5, for n ?
{2, 3, .
.
.
, 9} and ?
?
{0.005, 0.01, 0.05, 0.1, 0.2}.The optimal values for AR?EN were n = 4 and?
= 0.1, while for ZH?EN they were n = 4 and?
= 0.2.
These n values are smaller than for rerank-ing, and the ?
values are larger.
This suggests that,when maximizing coverage of a small diverse list,more dissimilarity is desired among the translations.9.3 Detailed ProcedureWe focused on AR?EN and ZH?EN for this study.We sampled 200 sentences from their test sets, cho-sen from among those whose reference translationwas between 5 and 25 words.
We generated a unique5-best list for each sentence using our baseline sys-tem (described in Section 5.2) and also generated adiverse list of length 5 using the dissimilarity func-tion ?
with hyperparameters tuned using the proce-dure from the previous section.
We untokenized andtruecased the translations.
We dropped non-ASCIIcharacters because we feared they would confuseour workers.
As a result, workers must contend withmissing words in the output, often proper nouns.Given the 2 lists for each sentence, we sampledtwo integers i, j ?
{2, 3, 4, 5} without replacement.The indices i and j indicate two entries from thelists.
We took translations 1, i, and j from the 5-bestlist and created an EDITING task from them.
We didthe same using entries 1, i, and j from the diverselist.
We repeated this process 3 times for each sen-tence, obtaining 3?
2 = 6 tasks for each, giving usa total of 1,200 EDITING tasks per language pair.The outputs of the EDITING tasks were evaluatedwith EVAL tasks.
For each sentence, we had 3 post-edited outputs generated using entries in 5-best listsand 3 post-edited outputs from diverse lists.
We cre-ated EVAL tasks for all 9 output pairs, for all 200sentences per language pair.
We additionally gaveeach task to three MTurk workers.
This gave us10,800 evaluation judgments for the EVAL task.9.4 ResultsFigure 2 shows the quartile breakdown for judg-ments collected from the EVAL task.
The Y axisrepresents the percentage of judgments for whichbest/diverse outputs were preferred; the missing per-centage for each bin is accounted for by ?same?judgments.We observe an interesting phenomenon.
Overall,there is a slight preference for the post-edited out-puts of M -best entries (?best?)
over those from di-verse translations (?div?
); this preference is clearest11080?34 34?46 46?62 62?94202530354045 Arabic?English% Chosen0?23 23?36 36?50 50?94203040BLEU Bin% ChosenChinese?EnglishbestdivFigure 2: Percentages in which post-edited output givenM -best entries (?best?)
was preferred by human eval-uators as compared to post-edited output given diversetranslations (?div?
), broken down by the BLEU+1 scoreof the 1-best translation for the sentences.
When the base-line system is doing poorly, diversity helps post-editors toproduce better translations.when the baseline system?s 1-best translation had ahigh BLEU score.
However, we see this trend re-versed for sentences in which the baseline system?s1-best translation had a low BLEU score.
In general,when the BLEU score of the baseline system is be-low 35, it is preferable to give diverse translations tousers for post-editing.
But when the baseline systemdoes very well, diverse translations do not contributeanything, and in fact hurt because they may distractusers from the high-quality (and typically very sim-ilar) translations from the 5-best lists.Estimation of the quality of the output (?confi-dence estimation?)
has recently gained interest inthe MT community (Specia et al 2011; Bach etal., 2011; Callison-Burch et al 2012; Bojar et al2013), including specifically for post-editing (Tat-sumi, 2009; Specia, 2011; Koponen, 2012).
Futurework could investigate whether such automatic con-fidence estimation could be used to identify situa-tions in which diverse translations can be helpful foraiding user understanding.10 Future WorkOur dissimilarity function captures diversity in theparticular phrases used by an MT system, but forcertain applications we may prefer other types of di-versity.
Defining the dissimilarity function on POStags or word clusters would help us to capture stylis-tic patterns in sentence structure, as would targetingsyntactic structures in syntax-based translation.A weakness of our approach is its computationalexpense; by contrast, the method of Devlin and Mat-soukas (2012) obtains diverse translations more ef-ficiently by extracting them from a single decodingof an input sentence (albeit with a wide beam).
Weexpect their ideas to be directly applicable to our set-ting in order to get diverse solutions more cheaply.We also plan to explore methods of explicitly target-ing multiple, diverse solutions as part of the searchalgorithm.Finally, M -best lists are currently used to ap-proximate structured spaces for many areas of MT,including tuning (Och, 2003), minimum Bayesrisk decoding (Kumar and Byrne, 2004), andpipelines (Venugopal et al 2008).
Future workcould replace M -best lists with diverse lists in theseand related tasks, whether for MT or other areas ofstructured NLP.AcknowledgmentsWe thank the anonymous reviewers as well as ColinCherry, Kenneth Heafield, Silja Hildebrand, FeiHuang, Dan Klein, Adam Pauls, and Bing Xiang.DB was partially supported by the National ScienceFoundation under Grant No.
1353694.ReferencesN.
Bach, F. Huang, and Y. Al-Onaizan.
2011.
Goodness:A method for measuring machine translation confi-dence.
In Proc.
of ACL.D.
Batra, P. Yadollahpour, A. Guzman-Rivera, andG.
Shakhnarovich.
2012.
Diverse M-best solutionsin Markov random fields.
In Proc.
of ECCV.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2013.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proc.
of WMT.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based N-gram mod-1109els of natural language.
Computational Linguistics,18.C.
Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.2011.
Findings of the 2011 Workshop on StatisticalMachine Translation.
In Proc.
of WMT.C.
Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-cut, and L. Specia.
2012.
Findings of the 2012 Work-shop on Statistical Machine Translation.
In Proc.
ofWMT.D.
Cer, C. D. Manning, and D. Jurafsky.
2013.
Positivediversity tuning for machine translation system com-bination.
In Proc.
of WMT.P.
Chang, M. Galley, and C. D. Manning.
2008.
Opti-mizing Chinese word segmentation for machine trans-lation performance.
In Proc.
of WMT.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProc.
of ACL.S.
Chatterjee and N. Cancedda.
2010.
Minimum errorrate training by sampling the translation lattice.
InProc.
of EMNLP.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report 10-98, Harvard University.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1).M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proc.
of ICML.J.
DeNero, D. Chiang, and K. Knight.
2009.
Fast con-sensus decoding over translation forests.
In Proc.
ofACL.J.
Devlin and S. Matsoukas.
2012.
Trait-based hypoth-esis selection for machine translation.
In Proc.
ofNAACL.C.
Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In Proc.
of HLT-NAACL.C.
Dyer.
2010.
A Formal Model of Ambiguity and its Ap-plications in Machine Translation.
Ph.D. thesis, Uni-versity of Maryland.J.
R. Finkel, C. D. Manning, and A. Y. Ng.
2006.
Solv-ing the problem of cascading errors: ApproximateBayesian inference for linguistic annotation pipelines.In Proc.
of EMNLP.E.
M. Gertz and S. J. Wright.
2003.
Object-oriented soft-ware for quadratic programming.
ACM Transactionson Mathematical Software, 29(1).J.
Gillenwater, A. Kulesza, and B. Taskar.
2012.
Discov-ering diverse and salient threads in document collec-tions.
In Proc.
of EMNLP.K.
Heafield and A. Lavie.
2010a.
Combining machinetranslation output with open source: The CarnegieMellon multi-engine machine translation scheme.
ThePrague Bulletin of Mathematical Linguistics, 93.K.
Heafield and A. Lavie.
2010b.
Voting on n-grams formachine translation system combination.
In Proc.
ofAMTA.K.
Heafield.
2011.
Kenlm: Faster and smaller languagemodel queries.
In Proc.
of WMT.A.
Hildebrand and S. Vogel.
2008.
Combination ofmachine translation systems via hypothesis selectionfrom combined n-best lists.
In Proc.
of AMTA.H.
Hoang, P. Koehn, and A. Lopez.
2009.
A Uni-fied Framework for Phrase-Based, Hierarchical, andSyntax-Based Statistical Machine Translation.
InProc.
of IWSLT.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InProc.
of EMNLP.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In Proc.
of ACL.T.
Joachims, T. Finley, and C. Yu.
2009.
Cutting-plane training of structural SVMs.
Machine Learning,77(1).P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL (demosession).P.
Koehn.
2010.
Enabling monolingual translators: Post-editing vs. options.
In Proc.
of NAACL.M.
Koponen.
2012.
Comparing human perceptions ofpost-editing effort with post-editing operations.
InProc.
of WMT.A.
Kulesza and B. Taskar.
2010.
Structured determinan-tal point processes.
In Proc.
of NIPS.A.
Kulesza and B. Taskar.
2011.
Learning determinantalpoint processes.
In Proc.
of UAI.S.
Kumar and W. Byrne.
2004.
Minimum bayes-riskdecoding for statistical machine translation.
In Proc.of HLT-NAACL.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.Efficient minimum error rate training and minimum1110Bayes-risk decoding for translation hypergraphs andlattices.
In Proc.
of ACL-IJCNLP.Y.
Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.2003.
Language model based Arabic word segmenta-tion.
In Proc.
of ACL.Z.
Li, J. Eisner, and S. Khudanpur.
2009.
Variationaldecoding for statistical machine translation.
In Proc.of ACL.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.C.
Lin and F. J. Och.
2004.
Orange: a method for evalu-ating automatic evaluation metrics for machine trans-lation.
In Proc.
of COLING.W.
Macherey and F. J. Och.
2007.
An empirical studyon computing consensus translations from multiplemachine translation systems.
In Proc.
of EMNLP-CoNLL.W.
Macherey, F. J. Och, I. Thayer, and J. Uszkoreit.
2008.Lattice-based minimum error rate training for statisti-cal machine translation.
In Proc.
of EMNLP.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of ACL.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In HLT-NAACL.F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.A.
Pauls and D. Klein.
2012.
Large-scale syntactic lan-guage modeling with treelets.
In Proc.
of ACL.A.-V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,R.
Schwartz, and B. Dorr.
2007.
Combining outputsfrom multiple machine translation systems.
In HLT-NAACL.L.
Shen and A. K. Joshi.
2003.
An SVM-based votingalgorithm with application to parse reranking.
In Proc.of CoNLL.L.
Shen, A. Sarkar, and F. J. Och.
2004.
Discriminativereranking for machine translation.
In Proc.
of HLT-NAACL.L.
Specia, N. Hajlaoui, C. Hallett, and W. Aziz.
2011.Predicting machine translation adequacy.
In Proc.
ofMT Summit XIII.L.
Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Proc.
ofEAMT.A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP.M.
Tatsumi.
2009.
Correlation between automatic evalu-ation metric scores, post-editing speed, and some otherfactors.
In Proc.
of MT Summit XII.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proc.
of HLT-NAACL.R.
Tromble, S. Kumar, F. J. Och, and W. Macherey.
2008.Lattice Minimum Bayes-Risk decoding for statisticalmachine translation.
In Proc.
of EMNLP.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
JMLR, 6.A.
Venugopal, A. Zollmann, N.A.
Smith, and S. Vogel.2008.
Wider pipelines: N-best alignments and parsesin MT training.
In Proc.
of AMTA.I.
H. Witten and T. C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel eventsin adaptive text compression.
IEEE Transactions onInformation Theory, 37(4).T.
Xiao, J. Zhu, and T. Liu.
2013.
Bagging and boostingstatistical machine translation systems.
Artif.
Intell.,195.P.
Yadollahpour, D. Batra, and G. Shakhnarovich.
2013.Discriminative re-ranking of diverse segmentations.
InProc.
of CVPR.1111
