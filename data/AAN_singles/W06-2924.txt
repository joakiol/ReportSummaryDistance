Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 176?180, New York City, June 2006. c?2006 Association for Computational LinguisticsDependency Parsing by Inference over High-recall Dependency PredictionsSander Canisius, Toine Bogers,Antal van den Bosch, Jeroen GeertzenILK / Computational Linguistics and AITilburg University, P.O.
Box 90153,NL-5000 LE Tilburg, The Netherlands{S.V.M.Canisius,A.M.Bogers,Antal.vdnBosch,J.Geertzen}@uvt.nlErik Tjong Kim SangInformatics InstituteUniversity of Amsterdam, Kruislaan 403NL-1098 SJ Amsterdam, The Netherlandserikt@science.uva.nl1 IntroductionAs more and more syntactically-annotated corporabecome available for a wide variety of languages,machine learning approaches to parsing gain inter-est as a means of developing parsers without havingto repeat some of the labor-intensive and language-specific activities required for traditional parser de-velopment, such as manual grammar engineering,for each new language.
The CoNLL-X shared taskon multi-lingual dependency parsing (Buchholz etal., 2006) aims to evaluate and advance the state-of-the-art in machine learning-based dependency pars-ing by providing a standard benchmark set compris-ing thirteen languages1.
In this paper, we describetwo different machine learning approaches to theCoNLL-X shared task.Before introducing the two learning-based ap-proaches, we first describe a number of baselines,which provide simple reference scores giving somesense of the difficulty of each language.
Next, wepresent two machine learning systems: 1) an ap-proach that directly predicts all dependency relationsin a single run over the input sentence, and 2) a cas-cade of phrase recognizers.
The first approach hasbeen found to perform best and was selected for sub-mission to the competition.
We conclude this paperwith a detailed error analysis of its output for two ofthe thirteen languages, Dutch and Spanish.1The data sets were extracted from various existing tree-banks (Hajic?
et al, 2004; Simov et al, 2005; Simov and Osen-ova, 2003; Chen et al, 2003; Bo?hmova?
et al, 2003; Kromann,2003; van der Beek et al, 2002; Brants et al, 2002; Kawata andBartels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006; CivitTorruella and Mart??
Anton?
?n, 2002; Nilsson et al, 2005; Oflazeret al, 2003; Atalay et al, 2003)2 Baseline approachesGiven the diverse range of languages involved inthe shared task, each having different characteristicsprobably requiring different parsing strategies, wedeveloped four different baseline approaches for as-signing labeled dependency structures to sentences.All of the baselines produce strictly projective struc-tures.
While the simple rules implementing thesebaselines are insufficient for achieving state-of-the-art performance, they do serve a useful role in givinga sense of the difficulty of each of the thirteen lan-guages.
The heuristics for constructing the trees andlabeling the relations used by each of the four base-lines are described below.Binary right-branching trees The first baselineproduces right-branching binary trees.
The first to-ken in the sentence is marked as the top node withHEAD 0 and DEPREL ROOT.
For the rest of thetree, token n ?
1 serves as the HEAD of token n.Figure 1 shows an example of the kind of tree thisbaseline produces.Binary left-branching trees The binary left-branching baseline mirrors the previous baseline.The penultimate token in the sentence is marked asthe top node with HEAD 0 and DEPREL ROOTsince punctuation tokens can never serve as ROOT2.For the rest of the tree, the HEAD of token n is tokenn+1.
Figure 2 shows an example of a tree producedby this baseline.2We simply assume the final token in the sentence to bepunctuation.176Inward-branching trees In this approach, thefirst identified verb3 is marked as the ROOT node.The part of the sentence to the left of the ROOT isleft-branching, the part to the right of the ROOT isright-branching.
Figure 3 shows an example of atree produced by this third baseline.Nearest neighbor-branching trees In our mostcomplex baseline, the first verb is marked as theROOT node and the other verbs (with DEPREL vc)point to the closest preceding verb.
The other to-kens point in the direction of their nearest neighbor-ing verb, i.e.
the two tokens at a distance of 1 froma verb have that verb as their HEAD, the two tokensat a distance of 2 have the tokens at a distance of 1as their head, and so on until another verb is a closerneighbor.
In the case of ties, i.e.
tokens that areequally distant from two different verbs, the token islinked to the preceding token.
Figure 4 clarifies thiskind of dependency structure in an example tree.verb verb punctROOTFigure 1: Binary right-branching tree for an examplesentence with two verbs.verb verb punctROOTFigure 2: Binary left-branching tree for the examplesentence.verb verb punctROOTFigure 3: Binary inward-branching tree for the ex-ample sentence.3We consider a token a verb if its CPOSTAG starts with a?V?.
This is an obviously imperfect, but language-independentheuristic choice.ROOTverb verb punctFigure 4: Nearest neighbor-branching tree for theexample sentence.Labeling of identified relations is done using athree-fold back-off strategy.
From the training set,we collect the most frequent DEPREL tag for eachhead-dependent FORM pair, the most frequent DE-PREL tag for each FORM, and the most frequentDEPREL tag in the entire training set.
The rela-tions are labeled in this order: first, we look up if theFORM pair of a token and its head was present inthe training data.
If not, then we assign it the mostfrequent DEPREL tag in the training data for thatspecific token FORM.
If all else fails we label thetoken with the most frequent DEPREL tag in the en-tire training set (excluding punct4 and ROOT).language baseline unlabeled labeledArabic left 58.82 39.72Bulgarian inward 41.29 29.50Chinese NN 37.18 25.35Czech NN 34.70 22.28Danish inward 50.22 36.83Dutch NN 34.07 26.87German NN 33.71 26.42Japanese right 67.18 64.22Portuguese right 25.67 22.32Slovene right 24.12 19.42Spanish inward 32.98 27.47Swedish NN 34.30 21.47Turkish right 49.03 31.85Table 1: The labeled and unlabeled scores for thebest performing baseline for each language (NN =nearest neighbor-branching).The best baseline performance (labeled and un-labeled scores) for each language is listed in Table1.
There was no single baseline that outperformedthe others on all languages.
The nearest neighborbaseline outperformed the other baselines on fiveof the thirteen languages.
The right-branching and4Since the evaluation did not score on punctuation.177inward-branching baselines were optimal on fourand three languages respectively.
The only languagewhere the left-branching trees provide the best per-formance is Arabic.3 Parsing by inference over high-recalldependency predictionsIn our approach to dependency parsing, a machinelearning classifier is trained to predict (directed) la-beled dependency relations between a head and a de-pendent.
For each token in a sentence, instances aregenerated where this token is a potential dependentof each of the other tokens in the sentence5.
Thelabel that is predicted for each classification caseserves two different purposes at once: 1) it signalswhether the token is a dependent of the designatedhead token, and 2) if the instance does in fact corre-spond to a dependency relation in the resulting parseof the input sentence, it specifies the type of this re-lation, as well.The features we used for encoding instances forthis classification task correspond to a rather simpledescription of the head-dependent pair to be clas-sified.
For both the potential head and dependent,there are features encoding a 2-1-2 window of wordsand part-of-speech tags6; in addition, there are twospatial features: a relative position feature, encodingwhether the dependent is located to the left or to theright of its potential head, and a distance feature thatexpresses the number of tokens between the depen-dent and its head.One issue that may arise when considering eachpotential dependency relation as a separate classifi-cation case is that inconsistent trees are produced.For example, a token may be predicted to be a de-pendent of more than one head.
To recover a validdependency tree from the separate dependency pre-dictions, a simple inference procedure is performed.Consider a token for which the dependency relationis to be predicted.
For this token, a number of clas-sification cases have been processed, each of them5To prevent explosion of the number of classification casesto be considered for a sentence, we restrict the maximum dis-tance between a token and its potential head.
For each language,we selected this distance so that, on the training data, 95% of thedependency relations is covered.6More specifically, we used the part-of-speech tags from thePOSTAG column of the shared task data files.indicating whether and if so how the token is relatedto one of the other tokens in the sentence.
Some ofthese predictions may be negative, i.e.
the token isnot a dependent of a certain other token in the sen-tence, others may be positive, suggesting the tokenis a dependent of some other token.If all classifications are negative, the token is as-sumed to have no head, and consequently no depen-dency relation is added to the tree for this token; thenode in the dependency tree corresponding to thistoken will then be an isolated one.
If one of the clas-sifications is non-negative, suggesting a dependencyrelation between this token as a dependent and someother token as a head, this dependency relation isadded to the tree.
Finally, there is the case in whichmore than one prediction is non-negative.
By defi-nition, at most one of these predictions can be cor-rect; therefore, only one dependency relation shouldbe added to the tree.
To select the most-likely can-didate from the predicted dependency relations, thecandidates are ranked according to the classificationconfidence of the base classifier that predicted them,and the highest-ranked candidate is selected for in-sertion into the tree.For our base classifier we used a memory-basedlearner as implemented by TiMBL (Daelemans etal., 2004).
In memory-based learning, a machinelearning method based on the nearest-neighbor rule,the class for a given test instance is predicted by per-forming weighted voting over the class labels of acertain number of most-similar training instances.As a simple measure of confidence for such a pre-diction, we divide the weight assigned to the major-ity class by the total weight assigned to all classes.Though this confidence measure is a rather ad-hocone, which should certainly not be confused withany kind of probability, it tends to work quite wellin practice, and arguably did so in the context ofthis study.
The parameters of the memory-basedlearner have been optimized for accuracy separatelyfor each language on training and development datasampled internally from the training set.The base classifier in our parser is faced with aclassification task with a highly skewed class dis-tribution, i.e.
instances that correspond to a depen-dency relation are largely outnumbered by those thatdo not.
In practice, such a huge number of nega-tive instances usually results in classifiers that tend178to predict fairly conservatively, resulting in high pre-cision, but low recall.
In the approach introducedabove, however, it is better to have high recall, evenat the cost of precision, than to have high precision atthe cost of recall.
A missed relation by the base clas-sifier can never be recovered by the inference proce-dure; however, due to the constraint that each tokencan only be a dependent of one head, excessive pre-diction of dependency relations can still be correctedby the inference procedure.
An effective method forincreasing the recall of a classifier is down-samplingof the training data.
In down-sampling, instancesbelonging to the majority class (in this case the neg-ative class) are removed from the training data, soas to obtain a more balanced distribution of negativeand non-negative instances.Figure 5 shows the effect of systematically re-moving an increasingly larger part of the negative in-stances from the training data.
First of all, the figureconfirms that down-sampling helps to improve re-call, though it does so at the cost of precision.
Moreimportantly however, it also illustrates that this im-proved recall is beneficial for the performance of thedependency parser.
The shape of the performancecurve of the dependency parser closely follows thatof the recall.
Remarkably, parsing performance con-tinues to improve with increasingly stronger down-sampling, even though precision drops considerablyas a result of this.
This shows that the confidenceof the classifier for a certain prediction is a suffi-ciently reliable indication of the quality of that pre-diction for fixing the over-prediction of dependencyrelations.
Only when the number of negative train-ing instances is reduced to equal the number of pos-itive instances, the performance of the parser is neg-atively affected.
Based on a quick evaluation of var-ious down-sampling ratios on a 90%-10% train-testsplit of the Dutch training data, we decided to down-sample the training data for all languages with a ratioof two negative instances for each positive one.Table 2 lists the unlabeled and labeled attachmentscores of the resulting system for all thirteen lan-guages.4 Cascaded dependency parsingOne of the alternative strategies explored by us wasmodeling the parsing process as a cascaded pair of0204060801002 4 6 8 10Sampling ratioPrecisionRecallSystem LASFigure 5: The effect of down-sampling on precisionand recall of the base classifier, and on labeled ac-curacy of the dependency parser.
The x-axis refersto the number of negative instances for each posi-tive instance in the training data.
Training and test-ing was performed on a 90%-10% split of the Dutchtraining data.basic learners.
This approach is similar to Yamadaand Matsumoto (2003) but we only use their Leftand Right reduction operators, not Shift.
In the firstphase, each learner predicted dependencies betweenneighboring words.
Dependent words were removedand the remaining words were sent to the learners forfurther rounds of processing until all words but onehad been assigned a head.
Whenever crossing linksprevented further assignments of heads to words, thelearner removed the remaining word requiring thelongest dependency link.
When the first phase wasfinished another learner assigned labels to pairs ofwords present in dependency links.Unlike in related earlier work (Tjong Kim Sang,2002), we were unable to compare many differentlearner configurations.
We used two different train-ing files for the first phase: one for predicting thedependency links between adjacent words and onefor predicting all other links.
As a learner, we usedTiMBL with its default parameters.
We evaluateddifferent feature sets and ended up with using words,lemmas, POS tags and an extra pair of features withthe POS tags of the children of the focus word.
Withthis configuration, this cascaded approach achieveda labeled score of 62.99 on the Dutch test data com-pared to 74.59 achieved by our main approach.179language unlabeled labeledArabic 74.59 57.64Bulgarian 82.51 78.74Chinese 82.86 78.37Czech 72.88 60.92Danish 82.93 77.90Dutch 77.79 74.59German 80.01 77.56Japanese 89.67 87.41Portuguese 85.61 77.42Slovene 74.02 59.19Spanish 71.33 68.32Swedish 85.08 79.15Turkish 64.19 51.07Table 2: The labeled and unlabeled scores for thesubmitted system for each of the thirteen languages.5 Error analysisWe examined the system output for two languagesin more detail: Dutch and Spanish.5.1 DutchWith a labeled attachment score of 74.59 and anunlabeled attachment score of 77.79, our submittedDutch system performs somewhat above the averageover all submitted systems (labeled 70.73, unlabeled75.07).
We review the most notable errors made byour system.From a part-of-speech (CPOSTAG) perspective,a remarkable relative amount of head and depen-dency errors are made on conjunctions.
A likelyexplanation is that the tag ?Conj?
applies to both co-ordinating and subordinating conjunctions; we didnot use the FEATS information that made this dis-tinction, which would have likely solved some ofthese errors.Left- and right-directed attachment to heads isroughly equally successful.
Many errors are madeon relations attaching to ROOT; the system appearsto be overgenerating attachments to ROOT, mostlyin cases when it should have generated rightwardattachments.
Unsurprisingly, the more distant thehead is, the less accurate the attachment; especiallyrecall suffers at distances of three and more tokens.The most frequent attachment error is generat-ing a ROOT attachment instead of a ?mod?
(mod-ifier) relation, often occurring at the start of a sen-tence.
Many errors relate to ambiguous adverbs suchas bovendien (moreover), tenslotte (after all), andzo (thus), which tend to occur rather frequently atthe beginning of sentences in the test set, but lessso in the training set.
The test set appears to con-sist largely of formal journalistic texts which typi-cally tend to use these marked rhetorical words insentence-initial position, while the training set is amore mixed set of texts from different genres plusa significant set of individual sentences, often man-ually constructed to provide particular examples ofsyntactic constructions.5.2 SpanishThe Spanish test data set was the only data set onwhich the alternative cascaded approach (72.15) out-performed our main approach (68.32).
A detailedcomparison of the output files of the two systemshas revealed two differences.
First, the amount ofcircular links, a pair of words which have each otheras head, was larger in the analysis of the submittedsystem (7%) than in the cascaded analysis (3%) andthe gold data (also 3%).
Second, the number of rootwords per sentence (always 1 in the gold data) wasmore likely to be correct in the cascaded analysis(70% correct; other sentences had no root) than inthe submitted approach (40% with 20% of the sen-tences being assigned no roots and 40% more thanone root).
Some of these problems might be solvablewith post-processingAcknowledgementsThis research is funded by NWO, the NetherlandsOrganization for Scientific Research under the IMIXprogramme, and the Dutch Ministry for EconomicAffairs?
IOP-MMI programme.ReferencesS.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006.CoNLL-X shared task on multilingual dependency parsing.In Proc.
of the Tenth Conf.
on Computational Natural Lan-guage Learning (CoNLL-X).
SIGNLL.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A.
Van denBosch.
2004.
TiMBL: Tilburg memory based learner, ver-sion 5.1, reference guide.
Technical Report ILK 04-02, ILKResearch Group, Tilburg University.Erik Tjong Kim Sang.
2002.
Memory-based shallow parsing.Journal of Machine Learning Research, 2(Mar):559?594.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
In 8th In-ternational Workshop of Parsing Technologies (IWPT2003).Nancy, France.180
