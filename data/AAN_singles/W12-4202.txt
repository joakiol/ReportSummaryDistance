Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?19,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsLinguistically-Enriched Models for Bulgarian-to-English MachineTranslationRui WangLanguage Technology LabDFKI GmbHSaarbru?cken, Germanyruiwang@dfki.dePetya Osenova and Kiril SimovLinguistic Modelling Department, IICTBulgarian Academy of SciencesSofia, Bulgaria{petya,kivs}@bultreebank.orgAbstractIn this paper, we present our linguistically-enriched Bulgarian-to-English statistical ma-chine translation model, which takes a sta-tistical machine translation (SMT) system asbackbone various linguistic features as fac-tors.
The motivation is to take advantages ofboth the robustness of the SMT system andthe rich linguistic knowledge from morpho-logical analysis as well as the hand-craftedgrammar resources.
The automatic evaluationhas shown promising results and our extensivemanual analysis confirms the high quality ofthe translation the system delivers.
The wholeframework is also extensible for incorporatinginformation provided by different sources.1 IntroductionIncorporating linguistic knowledge into statisticalmodels is an everlasting topic in natural languageprocessing.
The same story happens in the ma-chine translation community.
Along with the suc-cess of statistical machine translation (SMT) models(summarized by Koehn (2010)), various approacheshave been proposed to include linguistic informa-tion, ranging from early work by Wu (1997) to re-cent work by Chiang (2010), from deep transfer-based models (Graham and van Genabith, 2008) tomapping rules at the syntactic level (Galley et al,2004; Liu et al, 2006; Zhang et al, 2008).
Althoughthe purely data-driven approaches achieve signifi-cant results as shown in the evaluation campaigns(Callison-Burch et al, 2011), according to the hu-man evaluation, the final outputs of the SMT sys-tems are still far from satisfactory.Koehn and Hoang (2007) proposed a factoredSMT model as an extension of the traditionalphrase-based SMT model, which opens up an easyway to incorporate linguistic knowledge at the to-ken level.
Birch et al (2007) and Hassan et al(2007) have shown the effectiveness of adding su-pertags on the target side, and Avramidis and Koehn(2008) have focused on the source side, translat-ing a morphologically-poor language (English) to amorphologically-rich language (Greek).
However,all of them attempt to enrich the English part ofthe language pairs being translated.
For the lan-guage pairs like Bulgarian-English, there has notbeen much study on it, mainly due to the lack ofresources, including corpora, preprocessors, etc, onthe Bulgarian part.
There was a system publishedby Koehn et al (2009), which was trained and testedon the European Union law data, but not on otherpopular domains like news.
They reported a veryhigh BLEU score (Papineni et al, 2002) on theBulgarian-English translation direction (61.3).Apart from being morphologically-rich, Bulgar-ian has a number of challenging linguistic phenom-ena to consider, including free word order, long dis-tance dependency, coreference relations, clitic dou-bling, etc.
For instance, the following two sentences:(1) MomchetoBoy-thejher-datgoit-accdavagivesbuketabouquet-thenatomomicheto.girl-the.The boy gives the bouquet to the girl.
(2) MomchetoBoy-thejher-datgoit-accdava.gives.The boy gives it to her.10are difficult for the traditional phrase-based SMTsystem, because the clitic in the first sentence mustnot be translated, while in the second case it is oblig-atory.
Via the semantic analysis (e.g., Minimal Re-cursion Semantics), the clitic information will be in-corporated in the representation of the correspond-ing arguments.In this work, we rely on the linguistic processingto cope with some of these phenomena and improvethe correspondences between the two languages: 1)The lemmatization factors out the difference be-tween word forms and ensures better coverage of theBulgarian-English lexicon.
2) The dependency pars-ing helps to identify the grammatical functions suchas subject, object in sentences with a non-standardword order.
3) The semantic analysis provides a fur-ther abstraction which hides some of the languagespecific features.
Example of the last is the case ofclitic doubling.As for the Bulgarian-to-English translationmodel, we basically ?annotate?
the SMT baselinewith various linguistic features derived from thepreprocessing and hand-crafted grammars.
Thereare three contributions of this work:?
The models trained on a decent amount of par-allel corpora output surprisingly good results,in terms of automatic evaluation metrics.?
The enriched models give us more space for ex-perimenting with different linguistic featureswithout losing the ?basic?
robustness.?
According to our extensive manual analyses,the approach has shown promising results forfuture integration of more knowledge from thecontinued advances of the deep grammars.The rest of the paper will be organized as fol-lows: Section 2 briefly introduces some backgroundof the hand-crafted grammar resources we use andalso some previous related work on transfer-basedMT.
Section 3 describes the linguistic analyses weperform on the Bulgarian text, whose output is usedin the factored SMT model.
We show our exper-iments in Section 4 as well as both automatic anddetailed manual evaluation of the results.
We sum-marize this paper in Section 5 and point out severaldirections for future work.2 Machine Translation with DeepGrammarsOur work is also enlightened by another line of re-search, transfer-based MT models using deep lin-guistic knowledge, which are seemingly differentbut actually very related.
In this section, beforewe describe our model of incorporating linguis-tic knowledge from the hand-crafted grammars, wefirstly introduce the background of such resources aswell as some previous work on MT using them.Our usage of Minimal Recursion Semantic(MRS) analysis of Bulgarian text is inspired by thework on MRS and RMRS (Robust Minimal Recur-sion Semantic) (see (Copestake, 2003) and (Copes-take, 2007)) and the previous work on transfer of de-pendency analyses into RMRS structures describedin (Spreyer and Frank, 2005) and (Jakob et al,2010).
Although being a semantic representation,MRS is still quite close to the syntactic level, whichis not fully language independent.
This requires atransfer at the MRS level, if we want to do trans-lation from the source language to the target lan-guage.
The transfer is usually implemented in theform of rewriting rules.
For instance, in the Nor-wegian LOGON project (Oepen et al, 2004), thetransfer rules were hand-written (Bond et al, 2005;Oepen et al, 2007), which included a large amountof manual work.
Graham and van Genabith (2008)and Graham et al (2009) explored the automatic ruleinduction approach in a transfer-based MT settinginvolving two lexical functional grammars (LFGs)1,which was still restricted by the performance of boththe parser and the generator.
Lack of robustness fortarget side generation is one of the main issues, whenvarious ill-formed or fragmented structures comeout after transfer.
Oepen et al (2007) used theirgenerator to generate text fragments instead of fullsentences, in order to increase the robustness.In our approach, we want to make use of thegrammar resources while keeping the robustness,therefore, we experiment with another way of trans-fer involving information derived from the gram-mars.
In particular, we take a robust SMT systemas our ?backbone?
and then we augment it with deeplinguistic knowledge.
In general, what we are doing1Although their grammars are automatically induced fromtreebanks, the formalism supports rich linguistic information.11is still along the lines of previous work utilizing deepgrammars, but we build a more ?light-weighted?
butyet extensible statistical transfer model.3 Factor-based SMT ModelOur translation model is built on top of the factoredSMT model proposed by Koehn and Hoang (2007),as an extension of the traditional phrase-based SMTframework.
Instead of using only the word formof the text, it allows the system to take a vector offactors to represent each token, both for the sourceand target languages.
The vector of factors can beused for different levels of linguistic annotations,like lemma, part-of-speech, or other linguistic fea-tures, if they can be (somehow) represented as an-notations to each token.The process is quite similar to supertagging (Ban-galore and Joshi, 1999), which assigns ?rich descrip-tions (supertags) that impose complex constraints ina local context?.
In our case, all the linguistic fea-tures (factors) associated with each token form asupertag to that token.
Singh and Bandyopadhyay(2010) had a similar idea of incorporating linguis-tic features, while they worked on Manipuri-Englishbidirectional translation.
Our approach is slightlydifferent from (Birch et al, 2007) and (Hassan et al,2007), who mainly used the supertags on the targetlanguage side, English.
Instead, we primarily ex-periment with the source language side, Bulgarian.This potentially huge feature space provides us withvarious possibilities of using our linguistic resourcesdeveloped within and out of our project.Firstly, the data was processed by the NLP pipefor Bulgarian (Savkov et al, 2012) including a mor-phological tagger, GTagger (Georgiev et al, 2012), alemmatizer and a dependency parser2.
Then we con-sider the following factors on the source languageside (Bulgarian):?
WF ?
word form is just the original text token.?
LEMMA is the lexical invariant of the original wordform.
We use the lemmatizer, which operates onthe output from the POS tagging.
Thus, the 3rd per-son, plural, imperfect tense verb form ?varvyaha?
(?walking-were?, They were walking) is lemmatizedas the 1st person, present tense verb ?varvya?.2We have trained the MaltParser3 (Nivre et al, 2007)on the dependency version of BulTreeBank: http://www.bultreebank.org/dpbtb/.
The trained model achieves85.6% labeled parsing accuracy.?
POS ?
part-of-speech of the word.
We use the po-sitional POS tag set of the BulTreeBank, where thefirst letter of the tag indicates the POS itself, whilethe next letters refer to semantic and/or morphosyn-tactic features, such as: Dm - where ?D?
stands for?adverb?, and ?m?
stand for ?modal?
; Ncmsi - where?N?
stand for ?noun?, ?c?
means ?common?, ?m?
is?masculine?, ?s?
is ?singular?,and ?i?
is ?indefinite?.?
LING ?
other linguistic features derived from thePOS tag in the BulTreeBank tagset.?
DEPREL is the dependency relation between thecurrent word and the parent node.?
HLEMMA is the lemma of the parent node.?
HPOS is the POS tag of the parent node.Here is an example of a processed sentence.
Thesentence is ?spored odita v elektricheskite kompaniipoliticite zloupotrebyavat s dyrzhavnite predpriy-atiya.?
The glosses for the words in the Bulgariansentence are: spored (according) odita (audit-the) v(in) elektricheskite (electrical-the) kompanii (com-panies) politicite (politicians-the) zloupotrebyavat(abuse) s (with) dyrzhavnite (state-the) predpriy-atiya (enterprises).
The translation in the originalsource is : ?electricity audits prove politicians abus-ing public companies.?
The result from the linguisticprocessing are presented in Table 1.As for the deep linguistic knowledge, we also ex-tract features from the semantic analysis ?
MinimalRecursion Semantics (MRS).
MRS is introduced asan underspecified semantic formalism (Copestake etal., 2005).
It is used to support semantic analysesin the English HPSG grammar ERG (Copestake andFlickinger, 2000), but also in other grammar for-malisms like LFG.
The main idea is that the for-malism avoids spelling out the complete set of read-ings resulting from the interaction of scope bearingoperators and quantifiers, instead providing a singleunderspecified representation from which the com-plete set of readings can be constructed.
Here wewill present only basic definitions from (Copestakeet al, 2005).
For more details the cited publicationshould be consulted.An MRS structure is a tuple ?
GT , R, C ?, whereGT is the top handle, R is a bag of EPs (ele-mentary predicates) and C is a bag of handle con-straints, such that there is no handle h that outscopesGT .
Each elementary predicate contains exactlyfour components: 1) a handle which is the label of12No WF Lemma POS Ling DepRel HLemma HPOS1 spored spored R adjunct zloupotrebyavam VP2 odita odit Nc npd prepcomp spored R3 v v R mod odit Nc4 elektricheskite elektricheski A pd mod kompaniya Nc5 kompanii kompaniya Nc fpi prepcomp v R6 politicite politik Nc mpd subj zloupotrebyavam Vp7 zloupotrebyavat zloupotrebyavam Vp tir3p root - -8 s s R indobj zloupotrebyavam Vp9 dyrzhavnite dyrzhaven A pd mod predpriyatie Nc10 predpriyatiya predpriyatie Nc npi prepcomp s RTable 1: The sentence analysis with added head information ?
HLemma and HPOS.No EP EoV EP1 /POS1 EP2 /POS2 EP3 /POS31 spored r e zloupotrebyavam v/Vp odit n/Nc -2 odit n v - - -3 v r e odit n/Nc kompaniya n/Nc -4 elekticheski a e kompaniya n/Nc - -5 kompaniya n v - - -6 politik n v - - -7 zloupotrebyavam v e politik n/Nc - s r/R8 s r e zloupotrebyavam v/Vp predpriyatie n/Nc -9 dyrzhaven a e predpriyatie n/Nc - -10 predpriyatie n v - - -Table 2: Representation of MRS factors for each wordform in the sentence.the EP; 2) a relation; 3) a list of zero or more or-dinary variable arguments of the relation; and 4) alist of zero or more handles corresponding to scopalarguments of the relation (i.e., holes).Robust MRS (RMRS) is introduced as a modifica-tion of MRS which captures the semantics resultingfrom the shallow analysis.
Here the following as-sumption is taken into account: the shallow proces-sor does not have access to a lexicon.
Thus it doesnot have access to the arity of the relations in EPs.Therefore, the representation has to be underspeci-fied with respect to the number of arguments of therelations.
The names of relations are constructed onthe basis of the lemma for each wordform in the textand the main argument for the relation is specified.This main argument could be of two types: referen-tial index for nouns and event for the other parts ofspeech.
Because in this work we are using only theRMRS relation and the type of the main argument asfeatures to the translation model, we will skip herethe explanation of the full RMRS structures and howthey are constructed.As for the factors, we firstly do a match betweenthe surface tokens and the MRS elementary predi-cates (EPs) and then extract the following featuresas extra factors:?
EP ?
the name of the elementary predicate, whichusually indicates an event or an entity semantically.?
EOV indicates the current EP is either an event or areference variable.?
ARGnEP indicates the elementary predicate of theargument which belongs to the predicate.
n is usu-ally from 1 to 3.?
ARGnPOS indicates the POS tag of the argumentwhich belongs to the predicate.Notice that we do not take all the information pro-vided by the MRS, e.g., we throw away the scopalinformation and the other arguments of the relations.Those kinds of information is not straightforward tobe represented in such ?tagging?-style models, whichwill be tackled in the future.The extra information for the example sentenceis represented in Table 2.
All these factors encoded13within the corpus provide us with a rich selection offeatures for different experiments.4 ExperimentsTo run the experiments, we use the phrase-basedtranslation model provided by the open-source sta-tistical machine translation system, Moses4 (Koehnet al, 2007).
For training the translation model,the SETIMES parallel corpus has been used, whichis part of the OPUS parallel corpus5.
As for thechoice of the datasets, the language is more diversein the news articles, compared with other corpora inmore controlled settings, e.g., the JRC-Acquis cor-pus6 used by Koehn et al (2009).We split the corpus into the training set and thetest set by 150,000 and 1,000 sentence pairs re-spectively7.
Both datasets are preprocessed withthe tokenizer and lowercase converter provided byMoses.
Then the procedure is quite standard: Werun GIZA++ (Och and Ney, 2003) for bi-directionalword alignment, and then obtain the lexical trans-lation table and phrase table.
A tri-gram languagemodel is estimated using the SRILM toolkit (Stol-cke, 2002).
For the rest of the parameters we use thedefault setting provided by Moses.Notice that, since on the target language side (i.e.,English) we do not have any other factors than theword form, the factor-based models we use hereonly differentiate from each other in the translationphase, i.e., there is no ?generation?
models involved.4.1 Automatic Evaluation MetricsThe baseline results (non-factored model) under thestandard evaluation metrics are shown in the firstrow of Table 3 in terms of BLEU (Papineni et al,2002) and METEOR (Denkowski and Lavie, 2011).We then design various configurations to test theeffectiveness of different linguistic annotations de-scribed in Section 3.
The detailed configurations weconsidered are shown in the first column of Table 3.The first impression is that the BLEU scores ingeneral are high.
These models can be roughly4http://www.statmt.org/moses/5OPUS ?
an open source parallel corpus, http://opus.lingfil.uu.se/6http://optima.jrc.it/Acquis/7We did not preform MERT (Och, 2003), as it is quite com-putationally heavy for such various configurations.grouped into six categories (separated by doublelines): word form with linguistic features; lemmawith linguistic features; models with dependencyfeatures; MRS elementary predicates (EP) and thetype of the main argument of the predicate (EOV);EP features without word forms; and EP featureswith MRS ARGn features.In terms of the resulting scores, POS and Lemmaseem to be effective features, as Model 2 has thehighest BLEU score and Model 4 the best METEORscore.
Model 3 indicates that linguistic features alsoimprove the performance.
Model 4-6 show the ne-cessity of including the word form as one of thefactors.
Incorporating HLEMMA feature largely de-creases the results due to the vastly increasing vo-cabulary, i.e., aligning and translating bi-grams in-stead of tokens.
Therefore, we did not include theresults in the table.
After replacing the HLEMMAwith HPOS, the result is close to the others (Model8).
Model 9 may also indicate that increasing thenumber of factors does not guarantee performanceenhancement.
The experiments with predicate fea-tures (EP and EOV) from the MRS analyses (Model10-12) show improvements over the baseline con-sistently and using only the MRS features (Model13-14) also delivers descent results.
Concerningthe MRS ARGn features, the models with ARGnEPagain suffer from the sparseness problem as the de-pendency HLEMMA features, but the models withARGnPOS (Model 15-16) achieve better perfor-mance than those with dependency HPOS features.This is mainly because the dependency informationis encoded together with the (syntactically) depen-dent word, while the MRS arguments are groupedaround the semantic heads.So far, incorporating additional linguistic knowl-edge has not shown huge improvement in terms ofstatistical evaluation metrics.
However, this does notmean that the translations delivered are the same.
Inorder to fully evaluate the system, manual analysis isabsolutely necessary.
We are still far from drawing aconclusion at this point, but the automatic evaluationscores already indicate that the system can deliverdecent translation quality consistently.4.2 Manual EvaluationWe manually validated the output for all the modelsmentioned in Table 3.
The guideline includes two14ID Model BLEU 1-gram 2-gram 3-gram 4-gram METEOR1 WF (Baseline) 38.61 69.9 44.6 31.5 22.7 0.38162 WF, POS 38.85 69.9 44.8 31.7 23.0 0.38123 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0 0.38034 LEMMA 37.22 68.8 43.0 30.1 21.5 0.38175 LEMMA, POS 37.49 68.9 43.2 30.4 21.8 0.38126 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8 0.38007 WF, DEPREL 36.87 68.4 42.8 29.9 21.1 0.36278 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7 0.35249 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3 0.361010 WF, POS, EP 38.74 69.8 44.6 31.6 22.9 0.380711 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9 0.380712 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9 0.380213 EP, EOV 37.22 68.5 42.9 30.2 21.6 0.371114 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7 0.369115 EP, EOV, ARGnPOS 36.21 67.4 41.9 29.2 20.9 0.357716 WF, EP, EOV, ARGnPOS 37.37 68.4 43.2 30.3 21.8 0.3641Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000/1,000)aspects of the quality of the translation: Grammati-cality and Content.
Grammaticality can be evaluatedsolely on the system output and Content by compar-ison with the reference translation.
We use a 1-5score for each aspect as follows:Grammaticality1.
The translation is not understandable.2.
The evaluator can somehow guess the meaning, butcannot fully understand the whole text.3.
The translation is understandable, but with some ef-forts.4.
The translation is quite fluent with some minor mis-takes or re-ordering of the words.5.
The translation is perfectly readable and grammati-cal.Content1.
The translation is totally different from the refer-ence.2.
About 20% of the content is translated, missing themajor content/topic.3.
About 50% of the content is translated, with somemissing parts.4.
About 80% of the content is translated, missing onlyminor things.5.
All the content is translated.For the missing lexicons or not-translated Cyril-lic tokens, we ask the evaluators to score 2 for oneCyrillic token and score 1 for more than one tokensin the output translation.
We have two annotatorsachieving the inter-annotator agreement accordingto Cohen?s Kappa (Cohen, 1960) ?
= 0.73 for gram-maticality and ?
= 0.75 for content, both of whichare substantial agreement.
For the conflict cases,we take the average value of both annotators androunded the final score up or down in order to havean integer.The current results from the manual validationare on the basis of randomly sampled 150 sentencepairs.
The numbers shown in Table 4 are the numberof sentences given the corresponding scores.
The?Sum?
column shows the average score of all the out-put sentences by each model and the ?Final?
columnshows the average of the two ?Sum?
scores.The results show that linguistic and semanticanalyses definitely improve the quality of the trans-lation.
Exploiting the linguistic processing onword level ?
LEMMA, POS and LING ?
pro-duces the best result.
However, the model withonly EP and EOV features also delivers very goodresults, which indicates the effectiveness of theMRS features from the deep hand-crafted gram-mars, although incorporating the MRS ARGn fea-tures shows similar performance drops as depen-dency features.
Including more factors in generalreduces the results because of the sparseness effectover the dataset, which is consistent with the au-tomatic evaluation.
The last two rows are shown15ID ModelGrammaticality ContentFinal1 2 3 4 5 Sum 1 2 3 4 5 Sum1 WF (Baseline) 20 47 5 32 46 3.25 20 46 5 23 56 3.33 3.292 WF, POS 20 48 5 37 40 3.19 20 48 5 24 53 3.28 3.243 WF, LEMMA, POS, LING 20 47 6 34 43 3.22 20 47 1 24 58 3.35 3.294 LEMMA 15 34 11 46 44 3.47 15 32 5 33 65 3.67 3.575 LEMMA, POS 15 38 12 51 34 3.34 15 35 9 32 59 3.57 3.456 LEMMA, POS, LING 20 48 5 34 43 3.21 20 48 5 22 55 3.29 3.257 WF, DEPREL 32 48 3 29 38 2.95 32 49 4 14 51 3.02 2.998 WF, DEPREL, HPOS 45 41 7 23 34 2.73 45 41 2 21 41 2.81 2.779 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 2.89 34 48 3 20 45 2.96 2.9210 WF, POS, EP 19 49 4 34 44 3.23 19 49 3 20 59 3.34 3.2911 WF, EP, EOV 20 49 2 41 38 3.19 19 50 4 16 61 3.33 3.2612 WF, POS, LING, EP, EOV 19 49 5 37 40 3.20 19 50 3 24 54 3.29 3.2513 EP, EOV 15 41 10 44 40 3.35 14 38 7 31 60 3.57 3.4614 EP, EOV, LING 20 49 7 38 36 3.14 19 49 7 20 55 3.29 3.2115 EP, EOV, ARGnPOS 23 49 9 34 35 3.06 23 47 8 33 39 3.12 3.0916 WF, EP, EOV, ARGnPOS 34 47 10 30 29 2.82 34 47 10 20 39 2.89 2.85* GOOGLE 0 2 20 52 76 4.35 1 0 9 42 98 4.57 4.46* REFERENCE 0 0 5 51 94 4.59 1 0 5 37 107 4.66 4.63Table 4: Manual evaluation of the grammaticality and the contentfor reference.
?Google?
shows the results of usingthe online translation service provided by http://translate.google.com/ on 06.02.2012.
Thehigh score (very close to the reference translation)may be because our test data are not excluded fromtheir training data.
In future we plan to do the sameevaluation with a larger dataset.Concerning the impact from the linguistic pro-cessing pipeline to the final translation results,Lemma and MRS elementary predicates help at thelevel of rich morphology.
For example, the baselinemodel correctly translates the adjective ?Egyptian?in ?Egyptian Scientists?
(plural), but not in ?Egyp-tian Government, as in the second phrase the adjec-tive has a neutral gender.
Model 4 and Model 13 arecorrect for both.Generally speaking, if we roughly divide the lin-guistic processing pipeline in two categories: statis-tical processing (POS tagger and dependency parser)and rule-based processing (lemmatizer and MRSconstruction), the latter category (almost perfect)highly relies on the former one.
For example, thelemma depends on the word form and the tag, andthe result is unambiguous in more than 98% of themorphological lexicon and in text this is almost100% (because the ambiguous cases are very rare).The errors come mainly from new words and errorsin the tagger.
Similarly, the RMRS rules are goodwhen the parser is correct.
Here, the main problemsare duplications of the ROOT elements and the sub-ject elements, which we plan to fix using heuristicsin the future.4.3 Question-Based EvaluationAlthough the reported manual evaluation in the pre-vious section demonstrates that linguistic knowl-edge improves the translation, we notice that theevaluators tend to give marks at the two ends ofscale, and less in the middle.
Generally, this isbecause the measurement is done on the basis ofthe content that the evaluators extract from the Bul-garian sentence using there own cognitive capacity.Then they start to overestimate or underestimate thetranslation, knowing in advance what has to be trans-lated.
In order to avoid this subjectivity, we designa different manual evaluation in which the evalua-tor does not know the original Bulgarian sentences.Then the evaluation is based only on the content rep-resented within the English translation.In order to do this, we represent the content of theBulgarian sentences as a set of questions that havea list of possible answers, assigned to them.
Duringthe judgement of the content transfer, the evaluators16need to answer these questions.
As the list of an-swers also contains false answers, the evaluators areforced to select the right answer which can be in-ferred from the English translation.The actual questions are created semi-automatically from the dependency analysis ofthe sentences.
We defined a set of rules for genera-tion of the questions on the basis of the dependencyrelations.
For example, if a sentence has only asubject relation presented within the analysis, thequestion will be about who is doing the event.
Ifthe analysis presents subject and direct object, thequestion will be about who is doing something withwhat/whom.
These automatically generated ques-tions are manually investigated and, if necessary,edited.
Also, additional answers are formulated onthe basis of general language knowledge.
The mainidea is that the possible answers are conceptuallyclose to each other, but not in a hypernymy relation.Always there is an answer ?none?.Then the questions are divided into small groupsand distributed to be answered by three evaluatorsin such a way that each question is answered by twoevaluators, but no evaluator answers the whole set ofquestions for a given sentence.
In this way, we tryto minimize the influence of one question to the an-swers of the next questions.
The answers are com-pared to the true answers of the questions for eachgiven sentence.
We evaluated 192 questions for eachmodel and sum up the scores (correctly answeredquestions) in Table 5.This evaluation is more expensive, but we expectthem to be more objective.
As for a related work,(Yuret et al, 2010) used textual entailment to eval-uate different parser outputs.
The way they con-structed the hypotheses is similar to our creation ofquestions (based on dependency relations).
How-ever, they focused on the automatic evaluation andwe adopt it for the manual evaluation.5 Conclusion and Future WorkIn this paper, we report our work on building alinguistically-enriched statistical machine transla-tion model from Bulgarian to English.
Based on ourobservations of the previous approaches on transfer-based MT models, we decide to build a factoredmodel by feeding an SMT system with deep lin-ID Model Score1 WF (Baseline) 1272 WF, POS 1263 WF, LEMMA, POS, LING 1314 LEMMA 1335 LEMMA, POS 1336 LEMMA, POS, LING 1287 WF, DEPREL 1318 WF, DEPREL, HPOS 1209 WF, LEMMA, POS, LING, DEPREL 12410 WF, POS, EP 12511 WF, EP, EOV 12612 WF, POS, LING, EP, EOV 12813 EP, EOV 13814 EP, EOV, LING 12215 EP, EOV, ARGnPOS 13016 WF, EP, EOV, ARGnPOS 121Table 5: Question-based evaluationguistic features.
We perform various experiments onseveral configurations of the system (with differentlinguistic knowledge).
The high BLEU score showsthe high quality of the translation delivered by theSMT baseline; and various manual analyses confirmthe consistency of the system.There are various aspects of the current approachwe can improve: 1) The MRSes are not fully ex-plored yet, although we have considered the mostimportant predicate and argument features.
2) Wewould like to add factors on the target language side(English) as well to fulfill a ?complete?
transfer.
3)Incorporating reordering rules on the Bulgarian sidemay help the alignment and larger language mod-els on the English side should also help improvingthe translation results.
4) Due to the morphologi-cal complexity of the Bulgarian language, the othertranslation direction, from Bulgarian to English, isalso worth investigation in this framework.AcknowledgementsThis work was partially supported by the EuroMa-trixPlus project (IST-231720) funded by the Euro-pean Community?s Seventh Framework Programme.The authors would like to thank Laska Laskova,Stanislava Kancheva and Ivaylo Radev for doing thehuman evaluation of the data.17ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
Enrich-ing morphologically poor languages for statistical ma-chine translation.
In Proceedings of ACL.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2), June.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
Ccg supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshop onStatistical Machine Translation, pages 9?16, Prague,Czech Republic, June.Francis Bond, Stephan Oepen, Melanie Siegel, AnnCopestake, and Dan Flickinger.
2005.
Open sourcemachine translation with DELPH-IN.
In Proceedingsof the Open-Source Machine Translation Workshop atthe 10th Machine Translation Summit, pages 15 ?
22,Phuket, Thailand, September.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar F. Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the 6th Workshop on SMT.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of ACL, pages 1443?1452.J.
Cohen.
1960.
A Coefficient of Agreement for NominalScales.
Educational and Psychological Measurement,20(1):37.Ann Copestake and Dan Flickinger.
2000.
An opensource grammar development environment and broad-coverage english grammar using hpsg.
In Proceedingsof the 2nd International Conference on Language Re-sources and Evaluation, Athens, Greece.Ann Copestake, Dan Flickinger, Carl Pollard, and IvanSag.
2005.
Minimal recursion semantics: An in-troduction.
Research on Language & Computation,3(4):281?332.Ann Copestake.
2003.
Robust minimal recursion seman-tics (working paper).Ann Copestake.
2007.
Applying robust semantics.
InProceedings of the 10th Conference of the Pacific As-socation for Computational Linguistics (PACLING),pages 1?12.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic Metric for Reliable Optimization and Eval-uation of Machine Translation Systems.
In Proceed-ings of the EMNLP 2011 Workshop on Statistical Ma-chine Translation.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InProceedings of HLT-NAACL, Boston, Massachusetts,USA, May.G.
Georgiev, V. Zhikov, P. Osenova, K. Simov, andP.
Nakov.
2012.
Feature-rich part-of-speech taggingfor morphologically complex languages: Applicationto bulgarian.
In EACL 2012.Yvette Graham and Josef van Genabith.
2008.
Packedrules for automatic transfer-rule induction.
In Pro-ceedings of the European Association of MachineTranslation Conference (EAMT 2008), pages 57?65,Hamburg, Germany, September.Y.
Graham, A. Bryl, and J. van Genabith.
2009.
F-structure transfer-based statistical machine translation.In Proceedings of the Lexical Functional GrammarConference, pages 317?328, Cambridge, UK.
CSLIPublications, Stanford University, USA.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proceedings of ACL, Prague, Czech Republic, June.Max Jakob, Marke?ta Lopatkova?, and Valia Kordoni.2010.
Mapping between dependency structures andcompositional semantic representations.
In Proceed-ings of the 7th International Conference on LanguageResources and Evaluation (LREC 2010), pages 2491?2497.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of EMNLP.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL (demo session).P.
Koehn, A. Birch, and R. Steinberger.
2009.
462 ma-chine translation systems for europe.
In Proceedingsof MT Summit XII.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, January.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616.Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: a language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(1):1?41.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1).Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL.Stephan Oepen, Helge Dyvik, Jan Tore L?nning, ErikVelldal, Dorothee Beermann, John Carroll, Dan18Flickinger, Lars Hellan, Janne Bondi Johannessen,Paul Meurer, Torbj?rn Nordga?rd, , and Victoria Rose?n.2004.
Som a?
kapp-ete med trollet?
towards MRS-based norwegian to english machine translation.
InProceedings of the 10th International Conference onTheoretical and Methodological Issues in MachineTranslation, Baltimore, MD.Stephan Oepen, Erik Velldal, Jan Tore L?nning, PaulMeurer, Victoria Rose?n, and Dan Flickinger.
2007.Towards hybrid quality-oriented machine translation?
on linguistics and probabilities in MT.
In Pro-ceedings of the 11th Conference on Theoretical andMethodological Issues in Machine Translation (TMI-07), Skovde, Sweden.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL.Aleksandar Savkov, Laska Laskova, StanislavaKancheva, Petya Osenova, and Kiril Simov.
2012.Linguistic processing pipeline for bulgarian.
InProceedings of LREC, Istanbul, Turkey.Thoudam Doren Singh and Sivaji Bandyopadhyay.2010.
Manipuri-english bidirectional statistical ma-chine translation systems using morphology and de-pendency relations.
In Proceedings of the FourthWorkshop on Syntax and Structure in Statistical Trans-lation, pages 83?91, Beijing, China, August.Kathrin Spreyer and Anette Frank.
2005.
ProjectingRMRS from TIGER Dependencies.
In Proceedings ofthe HPSG 2005 Conference, pages 354?363, Lisbon,Portugal.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.
In Proceedings of the International Con-ference on Spoken Language Processing, volume 2.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Septem-ber.Deniz Yuret, Ayd?n Han, and Zehra Turgut.
2010.Semeval-2010 task 12: Parser evaluation using tex-tual entailments.
In Proceedings of the SemEval-2010Evaluation Exercises on Semantic Evaluation.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proceedings of ACL-HLT, pages 559?567.19
