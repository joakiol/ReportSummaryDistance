Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 236?240,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsEfficient Online Summarization of Microblogging StreamsAndrei OlariuFaculty of Mathematics and Computer ScienceUniversity of Bucharestandrei@olariu.orgAbstractThe large amounts of data generated onmicroblogging services are making sum-marization challenging.
Previous researchhas mostly focused on working in batchesor with filtered streams.
Input data has tobe saved and analyzed several times, in or-der to detect underlying events and thensummarize them.
We improve the effi-ciency of this process by designing an on-line abstractive algorithm.
Processing isdone in a single pass, removing the need tosave any input data and improving the run-ning time.
An online approach is also ableto generate the summaries in real time, us-ing the latest information.
The algorithmwe propose uses a word graph, along withoptimization techniques such as decayingwindows and pruning.
It outperforms thebaseline in terms of summary quality, aswell as time and memory efficiency.1 IntroductionCoined in 2006-2007, the term microblogging isused to describe social networks that allow usersto exchange small elements of content.
Thewidespread use of services like Facebook or Twit-ter means users have access to information that isotherwise unavailable.
Yet, as popular events com-monly generate hundreds of thousands of tweets,following them can be difficult.
Stream summa-rization ?
generating a short text based on a se-quence of posts ?
has been seen as the best ap-proach in solving this problem.This paper introduces Twitter Online WordGraph Summarizer.
TOWGS is the first online ab-stractive summarization algorithm and is capableof state-of-the-art processing speeds.
Most previ-ous algorithms process a stream in batches.
Theyrequire several passes through the data or a feedspecifically filtered for an event.
Batch summa-rization is suitable for small experiments, but itis not capable of efficiently handling thousands oftweets per second.We collect a 3.4 million tweets dataset for eval-uation purposes.
We choose as baseline an algo-rithm designed to summarize related tweets.
Wedetermine a set of important events relative to theinput data.
A group of judges rate the summariesgenerated by both algorithms for the given events.Our solution is not only capable of online summa-rization, but it also outperforms the batch-basedevent-filtered baseline in terms of result quality.The code for our algorithm is available online,along with the summaries, event keywords, rat-ings and tweet IDs: https://github.com/andreiolariu/online-summarizer.2 Related Work2.1 SummarizationWe distinguish two approaches in performingmulti-document summarization: extractive andabstractive.
With the risk of oversimplification,we view extractive summarization as a processof selecting sentences from the documents, whileabstractive summarization generates phrases thatmay not appear in the input data.The extractive approach is usually modeled asan optimization problem (Erkan and Radev, 2004).It can be combined with other techniques, suchas clustering (Silveira and Branco, 2012) or topicmodeling (Li and Li, 2013).Although actually performing word-level ex-traction, we consider word graph summarizationalgorithms abstractive because they are able togenerate summaries not found among the inputsentences.
Word graphs are used in compressingsimilar sentences (Filippova, 2010) or summariz-ing product reviews (Ganesan et al., 2010).A relevant reference for the problem of up-236date summarization is TAC update summarizationtrack (Dang and Owczarzak, 2008).2.2 Summarization on TwitterRegarding summarizing Twitter streams, we no-tice that all approaches are either restricted to spe-cific filtered streams, or combined with event de-tection.Extractive summarization is predominant whenworking with Twitter data.
It was first used forstreams following simple and structured events,such as sports matches (Takamura et al., 2011;Chakrabarti and Punera, 2011; Nichols et al.,2012; Zubiaga et al., 2012).The Phrase Reinforcement algorithm, intro-duced by Sharifi et al.
(2010a; 2010b), extractsfrequently used sequences of words.
It was firstapplied in summarizing topic streams.
Subsequentresearch emphasized evolving topics (Gao et al.,2013) or event decomposition (Olariu, 2013).Other approaches are based on integer linearprogramming (Liu et al., 2011) or LDA (Khan etal., 2013).
Yang et al.
(2012) develop a frame-work for summarization, highlighting its scalabil-ity.
Shou et al.
(2013) introduce Sumblr, capableof cluster-based online extractive summarization.Abstractive summarization is difficult on Twit-ter streams.
It is easily affected by noise or by thelarge variety of tweets.
Olariu (2013) showed thatabstractive summarization is feasible if posts areclustered based on similarity or underlying events.3 Twitter Online Word GraphSummarizer3.1 Building the Word GraphBy employing a word graph, TOWGS doesn?thave to save any of the tweets, like extractive ap-proaches do.
It can also skip the clustering stepapplied by the other online algorithm (Shou et al.,2013), leading to faster summarization.Previous word graph algorithms are based onbigrams.
Words are mapped to nodes in the graph,while an edge is added for each bigram.
When ap-plied to Twitter messages, the results depend onthe similarity of the summarized tweets (Olariu,2013).
A set of related tweets generates a qual-ity summary.
When applied to unrelated tweets,the generated summary lacks any meaning.
Thishappens because event-related signals (in our casebigrams) stand out when analyzing similar tweets,but get dominated by noise (bigrams of commonwords) when analyzing unrelated tweets.We solve this issue by building the wordgraph from trigrams.
In our version, eachnode in the graph is a bigram.
Having a sen-tence (w1, w2, w3, w4), we will first add two spe-cial words (to mark the beginning and end ofthe sentence) and generate the following edges:(S,w1) ?
(w1, w2), (w1, w2) ?
(w2, w3),(w2, w3) ?
(w3, w4) and (w3, w4) ?
(w4, E).Weights are added to nodes and edges in order tostore the count for each bigram or trigram.A negative effect of building the word graphfrom trigrams is that it significantly increases thenumber of nodes, leading to an increase in bothmemory and time.
We approach this issue bypruning the graph.
We implement pruning by pe-riodically going through the whole graph and re-moving edges that were not encountered in theprevious time window.
The length of this hardwindow can be set based on how much memorywe would like to allocate, as well as on the size ofthe soft window introduced in the next subsection.3.2 Word Graph Online UpdatingIn previous work, word graphs are discarded af-ter generating the summary.
For our online sum-marization task, the graph is being constantly up-dated with tweets.
It can also respond, at any time,to queries for generating summaries starting fromgiven keywords.In order to keep the results relevant to what ispopular at query time, we would like the graphto forget old data.
We implement this behaviorby using decaying windows (Rajaraman and Ull-man, 2011).
They are applied not only to graphweights (counts of bigrams and trigrams), but alsoto counts of words and word pair cooccurrences.At each time step (in our case, each second),all counts are multiplied by 1 ?
c, where c is asmall constant.
For example, after one hour (3600seconds), a value of 1 would become 0.48 withc = 0.0002 (given by (1 ?
c)3600) and 0.05 withc = 0.0008.In order to optimize the implementation, we ex-plicitly multiply the counts only when they areread or incremented.
For each record, we keepthe timestamp for its latest update tk.
Knowingthe current timestamp tn, we update the count bymultiplying with (1?
c)tn?tk.The size of the decaying window influences the237results and the memory requirements for TOWGS.A larger window requires less pruning and morememory, while also leading to more general sum-maries.
For example, given a stream of tweetsrelated to a sporting event, summaries generatedover very narrow windows would probably high-light individual goals, touchdowns or penalties.The summary for a two hour window would in-stead capture just the final score.3.3 Generating SummariesGiven a word graph, generating a summary in-volves finding the highest scoring path in thegraph.
That path connects the special words whichmark the beginning and end of each sentence.Since finding the exact solution is unfeasible givenour real time querying scenario, we will employ agreedy search strategy.The search starts by selecting the node (bigram)with the highest weight.
If we are interested insummarizing an event, we select the top rankingbigram containing one of the event?s keywords.At this point, we have a path with one node.We expand it by examining forward and backwardedges and selecting the one that maximizes thescoring function:score(n, e,m, p, k) =c1frequency(n) (1a)+ c2edge score(e,m) (1b)+ c3word score(n, p) (1c)+ c4word score(n, k) (1d)?
c5frequent word pen(n) (1e)?
c6repeated word pen(n) (1f)where p is a path representing a partial summary, nis a node adjacent to one of the path?s endpoints mby edge e and k is a list of keywords related to anevent.
The constants c1through c6determine theinfluence each helper function has on the overallscore.
The node n represents a bigram composedof the words wi(already in the path as part of m)and wo(currently being considered for extendingp).
The helper functions are defined as:frequency(n) = log(Wb[n]) (2a)edge score(e,m) = log(Wt[e]Wb[m])(2b)word score(n, p)=?w?p1|p|log(Wd[w,wo]?Ww[w]Ww[wo])(2c)frequent word pen(n) = log(Ww[wo]) (2d)repeated word pen(n) = 1p(wo) (2e)where Ww[w] is the weight for word w, Wb[m] isthe weight for the bigram represented by node m,Wt[e] is the weight for the trigram represented byedge e and Wd[w,wo] is the weight for the cooc-currences of words w and woin the same tweets.1p(wo) is the indicator function.
In all these cases,weights are counts implemented using decayingwindows (subsection 3.2).The scoring function gives a higher score to fre-quent bigrams (equations 1a and 2a).
In the sametime, individual words are penalized on their fre-quency (equations 1e and 2d).
Such scores favorwords used in specific contexts as opposed to gen-eral ones.
Trigrams are scored relative to bigrams(equations 1b and 2b).
Again, this favors contextspecific bigrams.
The word score function (equa-tion 2c) computes the average correlation betweena word (wofrom the bigram represented by noden) and a set of words.
The set of words is eitherthe current partial summary (equation 1c) or theevent-related keywords (equation 1d).We use logarithms in order to avoid floatingpoint precision errors.4 Evaluation4.1 Corpus and BaselineOur corpus is built using the Twitter Search API.We gathered an average of 485000 tweets perday for a total of seven days, between the 4thand the 10thof November 2013.
This volume oftweets represents around 0.1% of the entire Twit-ter stream.
Because of Twitter?s terms of service,sharing tweets directly is not allowed.
Instead, thesource code we?ve released comes with the tweetIDs needed for rebuilding the corpus.The algorithm chosen as baseline is Multi-Sentence Compression (or MSC), as presented in(Olariu, 2013).
MSC is a batch algorithm forabstractive summarization.
It performs best ongroups of similar tweets, such as the ones relatedto an event.
After receiving a summarization queryfor a set of keywords, the tweets are filtered basedon those keywords.
MSC processes the remainingtweets and generates a word graph.
After buildingthe summary, the graph is discarded.Because it has to store all tweets, MSC is not asmemory-efficient as TOWGS.
It is also not time-efficient.
Each summarization query requires fil-238tering the whole stream and building a new wordgraph.
The advantage MSC has is that it is work-ing with filtered data.
Olariu (2013) has shownhow susceptible word graphs are to noise.4.2 Evaluation ProcedureThe list of 64 events to be summarized was de-termined using a frequency based approach.
Asimple procedure identified words that were usedsignificantly more in a given day compared to abaseline.
The baselines were computed on a setof tweets posted between the 1stand the 3rdofNovember 2013.
Words that often appeared to-gether were grouped, with each group represent-ing a different event.The MSC algorithm received a cluster of postsfor each event and generated summaries of onesentence each.
TOWGS processed the posts as astream and answered to summarization requests.The requests were sent after the peak of each event(at the end of the hour during which that event reg-istered the largest volume of posts).The metrics used for assessing summary qual-ity were completeness (how much information isexpressed in the summary, relative to the eventtweets) and grammaticality.
They were rated ona scale of 1 (lowest) to 5 (highest).We asked five judges to rate the summaries us-ing a custom built web interface.
The judges werenot native English speakers, but they were all pro-ficient.
Three of them were Twitter users.
Whilethe judges were subjective in assessing summaryquality, each one did rate all of the summaries andthe differences between the two algorithms?
rat-ings were consistent across all judges.The constants c1through c6(introduced in sub-section 3.3) were set to 2, 3, 3, 10, 1 and 100,respectively.
These values were manually deter-mined after experimenting with a one day samplenot included in the evaluation corpus.5 ResultsThe average ratings for completeness are very sim-ilar, with a small advantage for TOWGS (4.29 ver-sus MSC?s 4.16).
We believe this is a good result,considering TOWGS doesn?t perform clusteringand summarizes events that account for less than1% of the total volume.
Meanwhile, MSC pro-cesses only the event-related tweets.
The averagerating for grammaticality is significantly higherfor TOWGS (4.30), as compared to MSC (3.78).Figure 1: The ratings distribution by algorithmand metric.While not engineered for speed, our implemen-tation can process a day of data from our corpus(around 485000 tweets) in just under three minutes(using one 3.2 GHz core).
In comparison, Sum-blr (Shou et al., 2013) can process around 30000tweets during the same interval.
TOWGS requiresan average of 0.5 seconds for answering each sum-marization query.
Regarding memory use, pruningkept its value constant.
In our experiments, theamount of RAM used by the algorithm was be-tween 1.5 - 2 GB.The code for TOWGS is available online,along with the summaries, keywords, ratingsand tweet IDs: https://github.com/andreiolariu/online-summarizer.6 ConclusionSummarizing tweets has been a popular researchtopic in the past three years.
Yet developing effi-cient algorithms has proven a challenge, with mostwork focused on small filtered streams.This paper introduces TOWGS, a highly effi-cient algorithm capable of online abstractive mi-croblog summarization.
TOWGS was tested ona seven day 0.1% sample of the entire Twitterstream.
We asked five judges to rate the sum-maries it generated, along with those from a base-line algorithm (MSC).
After aggregating the re-sults, the summaries generated by TOWGS provedto have a higher quality, despite the fact that MSCprocessed just the batches of event-filtered tweets.We also highlighted the state-of-the-art time effi-ciency of our approach.239ReferencesDeepayan Chakrabarti and Kunal Punera.
2011.
Eventsummarization using tweets.
In Proceedings of the5th Int?l AAAI Conference on Weblogs and SocialMedia (ICWSM).Hoa Trang Dang and Karolina Owczarzak.
2008.Overview of the tac 2008 update summarizationtask.
In Proceedings of text analysis conference,pages 1?16.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res., 22(1):457?479,December.Katja Filippova.
2010.
Multi-sentence compression:finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 322?330, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages340?348, Stroudsburg, PA, USA.
Association forComputational Linguistics.Dehong Gao, Wenjie Li, and Renxian Zhang.
2013.Sequential summarization: A new application fortimely updated twitter trending topics.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics, ACL ?13, pages 567?571.
Association for Computational Linguistics.Muhammad Asif Hossain Khan, Danushka Bollegala,Guangwen Liu, and Kaoru Sezaki.
2013.
Multi-tweet summarization of real-time events.
In Social-Com, pages 128?133.
IEEE.Jiwei Li and Sujian Li.
2013.
Evolutionary hierarchi-cal dirichlet process for timeline summarization.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, ACL ?13,pages 556?560.
Association for Computational Lin-guistics.Fei Liu, Yang Liu, and Fuliang Weng.
2011.
Whyis ?sxsw?
trending?
: exploring multiple text sourcesfor twitter topic summarization.
In Proceedings ofthe Workshop on Languages in Social Media, LSM?11, pages 66?75, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.2012.
Summarizing sporting events using twitter.
InProceedings of the 2012 ACM international confer-ence on Intelligent User Interfaces, IUI ?12, pages189?198, New York, NY, USA.
ACM.Andrei Olariu.
2013.
Hierarchical clustering in im-proving microblog stream summarization.
In Pro-ceedings of the 14th international conference onComputational Linguistics and Intelligent Text Pro-cessing - Volume 2, CICLing?13, pages 424?435,Berlin, Heidelberg.
Springer-Verlag.Anand Rajaraman and Jeffrey David Ullman.
2011.Mining of Massive Datasets.
Cambridge UniversityPress, New York, NY, USA.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010a.
Summarizing microblogs automatically.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 685?688, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.Kalita.
2010b.
Experiments in microblog sum-marization.
In Proceedings of the 2010 IEEE Sec-ond International Conference on Social Computing,SOCIALCOM ?10, pages 49?56, Washington, DC,USA.
IEEE Computer Society.Lidan Shou, Zhenhua Wang, Ke Chen, and Gang Chen.2013.
Sumblr: Continuous summarization of evolv-ing tweet streams.
In Proceedings of the 36th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?13,pages 533?542, New York, NY, USA.
ACM.S.B.
Silveira and A. Branco.
2012.
Combining a dou-ble clustering approach with sentence simplificationto produce highly informative multi-document sum-maries.
In Information Reuse and Integration (IRI),2012 IEEE 13th International Conference on, pages482?489.Hiroya Takamura, Hikaru Yokono, and Manabu Oku-mura.
2011.
Summarizing a document stream.
InProceedings of the 33rd European conference onAdvances in information retrieval, ECIR?11, pages177?188, Berlin, Heidelberg.
Springer-Verlag.Xintian Yang, Amol Ghoting, Yiye Ruan, and Srini-vasan Parthasarathy.
2012.
A framework for sum-marizing and analyzing twitter feeds.
In Proceed-ings of the 18th ACM SIGKDD international con-ference on Knowledge discovery and data mining,KDD ?12, pages 370?378, New York, NY, USA.ACM.Arkaitz Zubiaga, Damiano Spina, Enrique Amig?o, andJulio Gonzalo.
2012.
Towards real-time summa-rization of scheduled events from twitter streams.
InProceedings of the 23rd ACM Conference on Hyper-text and Social Media, HT ?12, pages 319?320, NewYork, NY, USA.
ACM.240
