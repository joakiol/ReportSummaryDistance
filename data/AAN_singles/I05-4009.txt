Question Classification using Multiple ClassifiersLI XinComputer ScienceEngineering Dep.FUDAN Univ., Shanghailixin@fudan.edu.cnHUANG Xuan-JingComputer ScienceEngineering Dep.FUDAN Univ., Shanghaixjhuang@fudan.edu.cnWU Li-deComputer ScienceEngineering Dep.FUDAN Univ., Shanghaildwu@fudan.edu.cnAbstractThe Open-domain Question Answeringsystem (QA) has been attached greatattention for its capacity of providingcompact and precise results for Xsers.The question classification is anessential part in the system, affectingthe accuracy of it.
The paper studiesquestion classification through machinelearning approaches, namely, differentclassifiers and multiple classifiercombination method.
By usingcompositive statistic and rule classifiers,and by introducing dependencystructure from Minipar and linguisticknowledge from Wordnet into questionrepresentation, the research shows highaccuracy in question classification.1 IntroductionWith the rapid development of the Internet, thecapacity of textual information has been greatlyimproved.
How to acquire accurate and effectiveinformation has become one of the greatconcerns among Internet users.
Open-DomainQuestion Answering System (QA) has gain greatpopularities among scholars who care the aboveproblem (Li, et al 2002; Moldovan, et al 2003;Zhang, et al 2003), for QA can meet users?demand by offering compact and accurateanswers, rather than text with correspondinganswers, to the questions presented in naturallanguage.
Therefore, it saves users?
great troubleto find out specific facts or figures from largequantity of texts.The study of Question Classification (QC), asa new field, corresponds with the research of QA.QC is an essential part of QA, for to correctlyanswer users?
questions, the system has to knowwhat the users are looking for, and it is QC thatpresents important searching clues for thesystem.
QC can be defined to match a questionto one or several classes in K category so as todetermine the answer type.
Every class presentssome semantic restrictions on the answersearching, which serves QA with variousstrategies in locating the correct answer.The result of QC can also serve QA in theanswer selecting and extract, which influence theperformance of QA directly.
The first reason isthat QC minish searching space.
For example, ifthe system knows that the answer type to thequestion ?Who was the first astronaut to walk inspace??
is a person?s name, it can confine theanswer in the names, rather than every word inthe texts.
The second reason is that QC candetermine the searching strategies andknowledge base QA may need.
For instance, thequestion ?What county is California in??
needsthe name of a country as its answer, so systemneeds the knowledge of countries?
name andname entities tagging to identify and testify theplace name, while the question ?What isTeflon??
expects an answer in a sentence or afragment, in the form of Teflon is <?.
>.
In fact,almost all the QA have the QC module and QCis the one of the most important factors whatdetermines the QA system performance(Moldovan, et al 2003).64At present the studies on QC are mainly basedon the text classification.
Though QC is similarto TC in some aspects, they are clearly distinctin that ?
Question is usually shorter, andcontains less lexicon-based information than text,which brings great trouble to QC.
Therefore toobtain higher classifying accuracy, QC has tomake further analysis of sentences, namely QChas to extend interrogative sentence withsyntactic and semantic knowledge, replacing orextending the vocabulary of the question withthe semantic meaning of every words.In QC, many systems apply machine-learningapproaches (Hovy, et al 2002; Ittycheriah, et al2000; Zhang, et al 2003).
The classification ismade according to the lexical, syntactic featuresand parts of speech.
Machine learning approachis of great adaptability, and 90.0% of classifyingaccuracy is obtained with SVM method and treeKernel as features.
However, there is still theproblem that the classifying result is affected bythe accuracy of syntactic analyzer, which needmanually to determine the weights of differentclassifying features.Some other systems adopting manual-rulemethod make QC, though may have highclassifying accuracy, lack of adaptability,because regulation determination involvesmanual interference to solve the conflictsbetween regulations and to form orderlyarranged rule base.The paper combines statistic and ruleclassifiers, specifically statistics precedingregulation, to classify questions.
With ruleclassifier as supplementary to statistic, theadvantages of respective classifier can be givenfull play to, and therefore the overallperformance of the classifier combination willbe better than the single one.
Moreover, as far asthe QC task is concerned, the paper comparesvarious classifier combinations, statistic-ruleclassifier, voting?
Adaboost and ANN.
Torepresent questions, the paper uses dependencystructure from Minipar (Lin 1998) and linguisticknowledge from Wordnet (Miller 1995; Miller,et al 2003).
In the following parts of the paper,classifying method and features is firstintroduced, and then comparisons are madebetween different type features and betweenfeature combination methods.
The comparisonsare testified in experiments.
The last part of thepaper is about the conclusion of the presentresearch and about the introduction of the furtherwork to be done on this issue.2 Classifying FeaturesIn machine learning method, every questionshould at first be transformed into a featurevector.
Bag-of-word is one typical way oftransforming questions, where every feature isone word in a corpus, whose value can beBoolean, showing whether the word is present inquestions, and which can also be an integer or areal number, showing the presence frequency ofthe word.
In this paper, every question isrepresented as a Boolean vector.1.
Bag-of-word: all lexical items in questions aretaken as classifying features, because stop-wordsuch as ?what?
and ?is?
playing a critical role inQC.2.
Wordnet Synsets: Wordnet was conceived asa machine-readable dictionary.
In Wordnet,word form is represented by word spellings, andthe sense is expressed by Synsets, and everysynset stands for a concept.
Wordnet shows bothlexical and semantic relationships.
The formerexists between word forms, while the latterexists between concepts.
Among varioussemantic relations in Wordnet, we choosehypernyms between nouns as our only concern.The classifying features are the senses of thenouns in the sentences and synsets of theirhypernyms.3.
N-gram: the model is founded on a hypothesisthat the presence of a word is only relevant tothe n words before it.
The frequently used areBi-gram and Tri-gram, and Bi-gram is chosen asthe classifying features in the present research.Compared with word, Bi-gram modelinvestigates two historical records, and reflectsthe partial law of language.
It embodies thefeatures of word order, and therefore it canreflect the theme of the sentence more strongly.4.
Dependency 6tructure: Minipar is a syntacticanalyzer, which can analyze the dependencyrelation of words in sentences.
It describes thesyntactic relationships between words insentences.
Such relation is direction-oriented,semantically rather than spatially, namely oneword governs, or is governed by, anotherconcerning their syntactic relation.
In onesentence (W1W2?Wn), compared withBi-gram, Dependency structure concerns65WiWj ?
but not need limitation j= i+1.Obviously, Dependency Relation goes furtherthan Bi-gram in language understanding.Dependency structure is specified by a list oflabeled tuples.
The format of a labeled tuple is asfollows:label (word  pos  root  governor  rel  exinfo ?)?Label?
is a label assigned to the tuple.
If thetuple represents a word in the sentence, labelshould be the index of the word in the sentence.?Word?
is a word in the input sentence.
?Pos?
isthe part of speech.
?Root?
is the root form.?Governor?
if the label of the governor of word(if it has one), ?rel?
is type of dependencyrelationship, and ?exinfo?
for extra information.Minipar output is represented by the worddependency relationship via ?governor?.
Thoughonly 79% of recall and some word relations failto be analyzed, the accuracy reaches 89%, whichguarantees that a large proportion of dependencyrelations from the output are correct.
And theexperiment proves that Dependency structurehas more classify precision than Bi-gram asclassifying feature.For example, as to the question ?Whichcompany created the Internet browser Mosaic?
?Minipar may produce the following results:E0 (()       fin         C    *   )1 (Which  ~  Det   2  det  (gov company))2 (company  ~   N  E0  whn   (gov fin))3 (created   create   V   E0  i     (gov fin))E2 (()  company  N  3 subj  (gov create)(antecedent 2))?
?According to the tuple, we can getdependency relationships between words insentences.
tuple 1 (Which ~  Det 2  detgov company) shows us the det relationshipbetween ?which?
and ?company?
in the sentence.Therefore, we can get a words-pair?whichcompany?
, and likewise other five pairs ofwords can be obtained ?
?company create???
the Mosaic ?
?
(Internet Mosaic) ?
(browser Mosaic) ?
(create Mosaic), whichwill be the item of vector represented thequestion.3 Classifying Method Description3.1 Support Vector Machine (SVM)SVM is a kind of machine learning approachbased on statistic learning theory.
SVM arelinear functions of the form f (x) = <w?x> +b,where <w?x> is the inner product between theweight vector w and the input vector x. TheSVM can be used as a classifier by setting theclass to 1 if f(x) > 0 and to -1 otherwise.
Themain idea of SVM is to select a hyperplane thatseparates the positive and negative exampleswhile maximizing the minimum margin, wherethe margin for example xi is yi f(x) and yi ?>-1,1] is the target output.
This corresponds tominimizing <w?w> subject to yi (<w?x> +b) ?for all i.
Large margin classifiers are known tohave good generalization properties.
Anadaptation of the LIBSVM implementation(Chang, et al 2001) is used in the following.Four type of kernel function linear, polynomial,radial basis function, and sigmoid are providedby LIBSVM .3.2 SVM-TBL QC AlgorithmTBL has been a part of NLP since Eric Brill?sbreakthrough paper in 1995(Brill 1995), whichhas been as effective as any other approach onthe Part-of-Speech Tagging problem.
TBL is atrue machine learning technique.
Given a taggedtraining corpus, it produces a sequence of rulesthat serves as a model of the training data.
Then,to derive the appropriate tags, each rule may beapplied, in order, to each instance in an untaggedcorpus.TBL generates all of the potential rules thatwould make at least one tag in the trainingcorpus correct.
For each potential rule, itsimprovement score is defined to be the numberof correct tags in the training corpus afterapplying the rule minus the number of correcttags in the training corpus before applying therule.
The potential rule with the highestimprovement score is output as the next rule inthe final model and applied to the entire trainingcorpus.
This process repeats (using the updatedtags on the training corpus), producing one rulefor each pass through the training corpus until norule can be found with an improvement scorethat surpasses some predefined threshold.
In66practice, threshold values of 1 or 2 appear to beeffective.Therefore, we present compositive QCapproach with rule and statistic learning.
At first,questions are represented by Bag-of-word,Wordnet Synsets, Bi-gram, and Dependencystructure, and are classified by the same samplesand same SVM.
Then output of SVM istransformed to the input of TBL, and thus everysample in TBL training data is featured byfour-dimensioned vectors, from which a new isobtained as training data of TBL.
When theerrors produced in initial marking process arecorrected in TBL to the greatest extent, afinal-classifier is produced as follows (Figure1).Figure1 SVM-TBL QC AlgorithmTBL is composed of three parts: unannotatedtext, transformation templates, and objectivefunction.
In the experiment, unannotated text isobtained from SVM.
The transformationtemplates define the space of transformations;here is combination of SVM output.
Suppose wehave k basic classifiers, and each classifier mayput questions into N types, then wehave rule templates.Objective function is the precision of classifier.kkkkk NCNCNC  22114 Results and AnalysisThe research adopts the same UIUC data andclassifying system as (Zhang, et al 2003) shows.There are about 5,500 labeled questionsrandomly divided into 5 training sets of sizes1,000, 2,000, 3,000, 4,000 and 5,500respectively.
The testing set contains 500questions from the TREC10 QA track.
Onlycoarse category is test.4.1 SVM Classifying Result We experiment the QC by SVM with four kernelfunction, and the following table (Table1) is theillustration of classifying accuracy by usingsingle-kind classifying feature.It is shown that as to the four type features, nomatter what Kernel is used, using Dependencyrelation feature have more precision than othersand feature of Synsets is better than Bag-of-word.Therefore it is safe to draw the conclusion thatSynsets and dependency relationship are helpful torepresent questions.
Among the four Kernelfunction, Liner has the best classifying precision.That is why we use Liner in the followingexperiment.Num.
of TrainingKernel & feature 1000 2000 3000 4000 5500Bag-of-word 79.6 81.2 83.4 85.8 84.8Wordnet 77.8 83.8 85.2 86.4 86.8Bi-gram 73.6 80.6 83.2 87.4 88.6LinerDependency 82.0 86.8 87.2 88.4 89.2Bag-of-word 52.4 69.2 66.0 61.4 62.6Wordnet 48.4 69.8 70.0 68.8 73.2Bi-gram 27.6 49.2 46.4 49.6 50.8polynomialDependency 73.0 78.8 81.8 82.4 85.2Bag-of-word 68.8 73.2 80.2 81.4 83.6Wordnet 69.0 73.2 79.8 80.2 81.0Bi-gram 62.2 70.2 76.0 80.0 81.2RBFDependency 72.8 78.8 81.0 83.2 85.0Bag-of-word 65.6 74.2 77.0 78.2 80.2Wordnet 74.2 82.6 83.4 83.8 84.4Bi-gram 68.6 74.4 79.8 83.2 84.8SigmoidDependency 75.2 78.0 82.4 83.4 85.2Table1.
Four kernel function QuestionClassifying Accuracy (%)4.2 Result of SVM multi-kind-featureclassification    1XPEHURI7UDLQLQJ([DPSOHV3UHFLVLRQ%DJRIZRUG:RUGQHW%LJUDP'HSHQGHQF\690Figure 2.
Multi-type FeatureCategory 1SVM1Feature1 Feature2 Feature3SVM2 SVM3Category 2 Category 3DATATBL67A question can be represented directly as a vectorwith multi-kind-features: Bag of Word,Dependency Structure, Synonym and Bi-gram.Figure2 provides an accuracy comparison of theresults derived from classification with fourfeatures and classification with only one kindfeature.
Experimental result indicates that, resultsfrom classification with four type features do notexcel the best classification precision with only onefeature.4.3 Using Adaboost to combine severalclassification resultsMulti-classifier combination is often used toobtain better classification results.
Adaboost(Schapire 1997; Schapire 1999) is an effectiveclassifier combination method.
Yet in questionclassification training, chances of samples to befaultily classified are slim.
Therefore, greateraccuracy on classification can hardly be realizedwith Boost.4.4 Using BP to combine several classifiersWe have also tried to use nerve network tocombine the output results of 4 classifiers.
Webuild a BP network with 4 input nodes and 1output node.
The number of hidden nodeschosen comes from the empirical formula:m=sqrt(nl), whose ?m?
indicates hidden nodes,?n?
input nodes, and ?l?
output nodes.
Thus, thenumber of hidden layer nodes is ?2?.Figure3shows, when training samples are relatively less,classification accuracy of BP is greatercompared to that of single-feature classifier, butnot in cases where the number of samplesincreases.    1XPEHURI7UDLQLQJ([DPSOHV3UHFLVLRQ%DJRIZRUG:RUGQHW%LJUDP'HSHQGHQF\$11Figure 3.
ANN combine several classifiers4.5 Using the method of voting to combineseveral classifiers    1XPEHURI7UDLQLQJ([DPSOHV3UHFLVLRQ%DJRIZRUG:RUGQHW%LJUDP'HSHQGHQF\927(Figure 4.
Voting combine several classifiersThrough the method of voting, we can also get thecombination results, according to the class labeloutputted by SVM with different type features.Experimental results are given in Figure 4.
We maysee that, due to the rule of ?more votes winning?
invoting, when there are a number of not-so-accurateclassifiers, the accuracy of voting can not competewith the greatest accuracy of a single classifier.4.6 Using TBL method to combine severalclassification results    1XPEHURI7UDLQLQJ([DPSOHVSULFLVRQ%DJRIZRUG:RUGQHW%LJUDP'HSHQGHQF\7%/Figure 5.
TBL combine several classifiersFigure 5 displays the accuracy of a number ofclassification results in TBL combination.
In ourexperiment, we construct 5 test-training sets,using 5500 sentences in UIUC.
Eachtest-training set has 1000 stochastically chosenquestions as its test set, and the other 4500 as itstraining.
The TBL training set is built upon theSVM classification results from the test set.
Incomparison with the method to voting, TBL usesthe conversion rule to fully rectify the errors of68initial tagger.
Therefore, TBL classification willnot produce results inferior to the best results ofra y.BagOfWord_2 $$$$ _3=_1_0 $$ _#=_0?
?the first class.ready been tagged as 3, itill be put in class 2.initial tagging.We obtain from the experiment all together251 conversion rules, the foremost ones ofwhich are listed as follows.
From these ruleswhich come from TBL training, we may alsodeduce that, TBL makes use of, firstly, theresults of the most accurate classifier (parser),and secondly, the results of other classifiers,especially those of dependency structurerectified by Bi-gram results.
It puts the accuracyof SVM single-feature classification into full useto secure greater accu c1.
Parser_2 $$ _#=_22.
Parser_3 $$ _#=_33.
Parser_1 $$ _#=_14.
Parser_5 $$ _#=_55.
Parser_4 $$ _#=_46.
Parser_3 && Bigram_2 && Synset_2 &&_3=_27.
Parser_3 && Bigram8?.
ParserRule 1 shows that: in cases where DependencyStructure is adopted as the feature, when theclassification result is 1 and the question is notclassified, the question belongs toRule 2, 3, 4, and 5 is similar to 1.Rules 6, 7 involve classification results frommultiple classifiers.
Rule 6 indicates that, ifsentence is placed in 3 when DependencyStructure is adopted as feature, in class 2, whenBi-gram or Synset or Bag-of-Word is adopted,and questions have alw    1XPEHURI7UDLQLQJ([DPSOHSUHFLVRQ7%/$11690927(Figure 6.
Different combine methodmulti-type-features to representquestions directly.igure 7.
Using dependency structure or notlso promote precision,with a percentage of 1.8.weigFigure 6 gives us the classification results of 500questions of Trec10 in different method ofcombination.
It can be seen that, TBL combinationof classifiers is better than voting and ANN; TBLand SVM working together is better than SVMclassification using    1XPEHURI7UDLQLQJ([DPSOHV3UHFLVRQ7%/6906907%/FFigure7 provides a comparison ofclassification accuracy between TBL combiningmulti-classifier and SVM directly using severaltype features, in conditions of adopting or notadopting Dependency Structure as feature.
TBL-and SVM- both mean classifier not adopting.The results show: Using such method of QC asblending ?statistics?
and ?rules?, that is, theaccuracy of classification is 1.6% greater thanthat of not using TBL; adopting DependencyStructure as feature can a4.7 Result AnalysisCompared to (Zhang, et al 2003) using ?treekernel?
as the classification feature, this thesisadopts the ?statistics and rules blended?
methodin QC (?statistics first and rules next?
), liftingthe precision of classification to 1.4% higherthan it used to be.
Moreover, it also avoids theproblem of artificial selection in different featurehting that appearing in =KDQJ?VSDSHU.Tests using the ?statistics and rulesblended?
pattern of question classificationunfold that, 34.1% of faulty classification ofsentences arouses from the using of improperstatistical methods.
The manifest of this is thatall the SVM classifiers with 4 features placequestions into class ?i?, while they actuallybelong to class ?j?.
Classification features thathave relatively big differences are needed to69work as basic classifier to improve the finalresult.
And also, 31.8% of the faultyclassification stems from the fact that, there areno corresponding rules in the rule sets derivedfrom TBL training, so that the rule sets cannotcorrect the errors caused by wrong statisticalmethods.
This may because our question corpusis Limited, and therefore, some of theclassification combinations never even appear.5 Conclusionsto further theresearch on QC using Wordnet.Britagging."
ComputationalChlable at:Hothe COLING-2002 Conference.Itty-9Li,ComputationalLinEvaluation ofMimmunications of the ACMMiUniversity's Cognitive ScienceMons on Information SystemsSchourteenthSchnalZhaonRetrieval (SIGIR?03).
Toronto, Canada: ACM.4&, an important module in the 4$ system, canconduct answer choosing and selection.
Thisthesis experiment several different methods inQC, and studies features like the DependencyStructure, Wordnet Synsets, Bag-of-Word, andBi-gram.
It also analyzes a number of kernelfunctions and the influence of different ways ofclassifier combination, such as Voting,Adaboost?ANN and TBL, on the precision ofQC.
Adopting the ?statistics and rules blended?method of question classification (?statistics firstand rules next?)
and using language informationsuch as the Synset from Wordnet and thedependency structure of Minipar as classificationfeatures promote the accuracy of questionclassification.
TBL combination multi-classifiermethod can be extended, easily.
As long as newclassifying algorithm or new feature set is found,the classifying result from them can betransformed to rule set, which can lead to furtherclassifying function.
Wordnet has provided uswith semantic relation, examples, explanation,etc.
The present study only investigates thesemantic relation of hyponymy.
There are stillmuch to be done in the futureReferencell, E. 1995.
"Transformation-based error-drivenlearning and natural language processing: a casestudy part-of-speechLinguistics 2:543-565.ang, C.-C. and Lin, C.-J.
2001. "
LIBSVM: alibrary for support vector machines."
avaihttp://www.csie.ntu.edu.tw/~cjlin/libsvmvy, E., Hermjakob, U., Lin, C.-Y.
andRavichandran, D. 2002.
"Using Knowledge toFacilitate Factoid Answer Pinpointing."
In,Proceedings ofTaipei, Taiwan.cheriah, A., Franz, M., Zhu, W.-J.
and Ratnaparkhi,A.
2000.
"IBM?s Statistical Question AnsweringSystem."
In, Proceedings of the TRECConference.
Gaithersburg, MD: NIST, p. 229.X.
and Roth, D. 2002.
"Learning QuestionClassifiers."
In, Proceedings of the 19thInternational Conference onLinguistics, (COLING?02).
Taipei., D. 1998.
"Dependency-based Evaluation ofMINIPAR."
In Workshop on theParsing Systems.
Granada, Spain.ller, G. 1995.
"WordNet: A Lexical Database forEnglish."
Co38(11):39-41.ller, G. and McDonnell, J. S. 2003.
"Wordnet 2.0.
"PrincetonLaboratory.ldovan, D., PASCA, M. and HARABAGIU, S.2003.
"Performance Issues and Error Analysis inan Open-Domain Question Answering System.
"ACM TransactioVol.21:133-154.apire, R. E. 1997.
"Using output codes to boostmulticlass learning problems."
In MachineLearning: Proceedings of the FInternational Conference.
pp.
313-321.apire, R. E. 1999.
"Theoretical views of boostingand applications."
In 10th InternatioConference on Algorithmic Learning Theory.ng, D. and Lee, W. S. 2003.
"QuestionClassification using Support Vector Machines."
In,Proceedings of ACM SIGIR Conference onResearch and Development in Informati70
