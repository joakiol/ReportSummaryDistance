Proceedings of the Workshop on BioNLP: Shared Task, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsOverview of BioNLP?09 Shared Task on Event ExtractionJin-Dong Kim?
Tomoko Ohta?
Sampo Pyysalo?
Yoshinobu Kano?
Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan?School of Computer Science, University of Manchester, Manchester, UK?National Centre for Text Mining, University of Manchester, Manchester, UK{jdkim,okap,smp,kano,tsujii}@is.s.u-tokyo.ac.jpAbstractThe paper presents the design and implemen-tation of the BioNLP?09 Shared Task, andreports the final results with analysis.
Theshared task consists of three sub-tasks, each ofwhich addresses bio-molecular event extrac-tion at a different level of specificity.
The datawas developed based on the GENIA event cor-pus.
The shared task was run over 12 weeks,drawing initial interest from 42 teams.
Ofthese teams, 24 submitted final results.
Theevaluation results are encouraging, indicatingthat state-of-the-art performance is approach-ing a practically applicable level and revealingsome remaining challenges.1 IntroductionThe history of text mining (TM) shows that sharedtasks based on carefully curated resources, suchas those organized in the MUC (Chinchor, 1998),TREC (Voorhees, 2007) and ACE (Strassel et al,2008) events, have significantly contributed to theprogress of their respective fields.
This has also beenthe case in bio-TM.
Examples include the TREC Ge-nomics track (Hersh et al, 2007), JNLPBA (Kim etal., 2004), LLL (Ne?dellec, 2005), and BioCreative(Hirschman et al, 2007).
While the first two ad-dressed bio-IR (information retrieval) and bio-NER(named entity recognition), respectively, the last twofocused on bio-IE (information extraction), seekingrelations between bio-molecules.
With the emer-gence of NER systems with performance capable ofsupporting practical applications, the recent interestof the bio-TM community is shifting toward IE.Similarly to LLL and BioCreative, theBioNLP?09 Shared Task (the BioNLP task, here-after) also addresses bio-IE, but takes a definitivestep further toward finer-grained IE.
While LLL andBioCreative focus on a rather simple representationof relations of bio-molecules, i.e.
protein-proteininteractions (PPI), the BioNLP task concerns thedetailed behavior of bio-molecules, characterized asbio-molecular events (bio-events).
The difference infocus is motivated in part by different applicationsenvisioned as being supported by the IE methods.For example, BioCreative aims to support curationof PPI databases such as MINT (Chatr-aryamontriet al, 2007), for a long time one of the primary tasksof bioinformatics.
The BioNLP task aims to supportthe development of more detailed and structureddatabases, e.g.
pathway (Bader et al, 2006) or GeneOntology Annotation (GOA) (Camon et al, 2004)databases, which are gaining increasing interestin bioinformatics research in response to recentadvances in molecular biology.As the first shared task of its type, the BioNLPtask aimed to define a bounded, well-defined bio-event extraction task, considering both the actualneeds and the state of the art in bio-TM technologyand to pursue it as a community-wide effort.
Thekey challenge was in finding a good balance betweenthe utility and the feasibility of the task, which wasalso limited by the resources available.
Special con-sideration was given to providing evaluation at di-verse levels and aspects, so that the results can drivecontinuous efforts in relevant directions.
The pa-per discusses the design and implementation of theBioNLP task, and reports the results with analysis.1Type Primary Args.
Second.
Args.Gene expression T(P)Transcription T(P)Protein catabolism T(P)Phosphorylation T(P) SiteLocalization T(P) AtLoc, ToLocBinding T(P)+ Site+Regulation T(P/Ev), C(P/Ev) Site, CSitePositive regulation T(P/Ev), C(P/Ev) Site, CSiteNegative regulation T(P/Ev), C(P/Ev) Site, CSiteTable 1: Event types and their arguments.
The type of thefiller entity is specified in parenthesis.
The filler entityof the secondary arguments are all of Entity type whichrepresents any entity but proteins: T=Theme, C=Cause,P=Protein, Ev=Event.2 Task settingTo focus efforts on the novel aspects of the eventextraction task, is was assumed that named entityrecognition has already been performed and the taskwas begun with a given set of gold protein anno-tation.
This is the only feature of the task settingthat notably detracts from its realism.
However,given that state-of-the-art protein annotation meth-ods show a practically applicable level of perfor-mance, i.e.
88% F-score (Wilbur et al, 2007), webelieve the choice is reasonable and has several ad-vantages, including focus on event extraction and ef-fective evaluation and analysis.2.1 Target event typesTable 1 shows the event types addressed in theBioNLP task.
The event types were selected fromthe GENIA ontology, with consideration given totheir importance and the number of annotated in-stances in the GENIA corpus.
The selected eventtypes all concern protein biology, implying that theytake proteins as their theme.
The first three typesconcern protein metabolism, i.e.
protein productionand breakdown.
Phosphorylation is a representa-tive protein modification event, and Localization andBinding are representative fundamental molecularevents.
Regulation (including its sub-types, Posi-tive and Negative regulation) represents regulatoryevents and causal relations.
The last five are uni-versal but frequently occur on proteins.
For the bio-logical interpretation of the event types, readers arereferred to Gene Ontology (GO) and the GENIA on-tology.The failure of p65 translocation to the nucleus .
.
.T3 (Protein, 40-46)T2 (Localization, 19-32)E1 (Type:T2, Theme:T3, ToLoc:T1)T1 (Entity, 15-18)M1 (Negation E1)Figure 1: Example event annotation.
The protein an-notation T3 is given as a starting point.
The extractionof annotation in bold is required for Task 1, T1 and theToLoc:T1 argument for Task 2, and M1 for Task 3.As shown in Table 1, the theme or themes of allevents are considered primary arguments, that is, ar-guments that are critical to identifying the event.
Forregulation events, the entity or event stated as thecause of the regulation is also regarded as a primaryargument.
For some event types, other argumentsdetailing of the events are also defined (SecondaryArgs.
in Table 1).From a computational point of view, the eventtypes represent different levels of complexity.
Whenonly primary arguments are considered, the first fiveevent types require only unary arguments, and thetask can be cast as relation extraction between apredicate (event trigger) and an argument (Protein).The Binding type is more complex in requiring thedetection of an arbitrary number of arguments.
Reg-ulation events always take a Theme argument and,when expressed, also a Cause argument.
Note that aRegulation event may take another event as its themeor cause, a unique feature of the BioNLP task com-pared to other event extraction tasks, e.g.
ACE.2.2 RepresentationIn the BioNLP task, events are expressed using threedifferent types of entities.
Text-bound entities (t-entities hereafter) are represented as text spans withassociated class information.
The t-entities includeevent triggers (Localization, Binding, etc), proteinreferences (Protein) and references to other entities(Entity).
A t-entity is represented by a pair, (entity-type, text-span), and assigned an id with the pre-fix ?T?, e.g.
T1?T3 in Figure 1.
An event is ex-pressed as an n-tuple of typed t-entities, and hasa id with prefix ?E?, e.g.
E1.
An event modifi-cation is expressed by a pair, (predicate-negation-or-speculation, event-id), and has an id with prefix?M?, e.g.
M1.2Item Training Devel.
TestAbstract 800 150 260Sentence 7,449 1,450 2,447Word 176,146 33,937 57,367Event 8,597 / 8,615 1,809 / 1,815 3,182 / 3,193Table 2: Statistics of the data sets.
For events,Task1/Task2 shown separately as secondary argumentsmay introduce additional differentiation of events.2.3 SubtasksThe BioNLP task targets semantically rich event ex-traction, involving the extraction of several differentclasses of information.
To facilitate evaluation ondifferent aspects of the overall task, the task is di-vided to three sub-tasks addressing event extractionat different levels of specificity.Task 1.
Core event detection detection of typed,text-bound events and assignment of given pro-teins as their primary arguments.Task 2.
Event enrichment recognition of sec-ondary arguments that further specify theevents extracted in Task 1.Task 3.
Negation/Speculation detection detectionof negations and speculation statementsconcerning extracted events.Task 1 serves as the backbone of the shared task andis mandatory for all participants.
Task 2 involves therecognition of Entity type t-entities and assignmentof those as secondary event arguments.
Task 3 ad-dresses the recognition of negated or speculativelyexpressed events without specific binding to text.
Anexample is given in Fig.
1.3 Data preparationThe BioNLP task data were prepared based on theGENIA event corpus.
The data for the training anddevelopment sets were derived from the publiclyavailable event corpus (Kim et al, 2008), and thedata for the test set from an unpublished portion ofthe corpus.
Table 2 shows statistics of the data sets.For data preparation, in addition to filtering outirrelevant annotations from the original GENIA cor-pus, some new types of annotation were added tomake the event annotation more appropriate for thepurposes of the shared task.
The following sectionsdescribe the key changes to the corpus.3.1 Gene-or-gene-product annotationThe named entity (NE) annotation of the GENIAcorpus has been somewhat controversial due to dif-ferences in annotation principles compared to otherbiomedical NE corpora.
For instance, the NE an-notation in the widely applied GENETAG corpus(Tanabe et al, 2005) does not differentiate proteinsfrom genes, while GENIA annotation does.
Suchdifferences have caused significant inconsistency inmethods and resources following different annota-tion schemes.
To remove or reduce the inconsis-tency, GENETAG-style NE annotation, which weterm gene-or-gene-product (GGP) annotation, hasbeen added to the GENIA corpus, with appropriaterevision of the original annotation.
For details, werefer to (Ohta et al, 2009).
The NE annotation usedin the BioNLP task data is based on this annotation.3.2 Argument revisionThe GENIA event annotation was made based onthe GENIA event ontology, which uses a loose typ-ing system for the arguments of each event class.For example, in Figure 2(a), it is expressed thatthe binding event involves two proteins, TRAF2and CD40, and that, in the case of CD40, its cy-toplasmic domain takes part in the binding.
With-out constraints on the type of theme arguments,the following two annotations are both legitimate:(Type:Binding, Theme:TRAF2, Theme:CD40)(Type:Binding, Theme:TRAF2,Theme:CD40 cytoplasmic domain)The two can be seen as specifying the same eventat different levels of specificity1.
Although both al-ternatives are reasonable, the need to have consis-tent training and evaluation data requires a consis-tent choice to be made for the shared task.Thus, we fix the types of all non-eventprimary arguments to be proteins (specificallyGGPs).
For GENIA event annotations involvingthemes other than proteins, additional argumenttypes were introduced, for example, as follows:1In the GENIA event annotation guidelines, annotators areinstructed to choose the more specific alternative, thus the sec-ond alternative for the example case in Fig.
2(a).3(a)TRAF2 is a ?
which binds to the CD40 cytoplasmic domainGGP GGP PDR(b)HMG-I binds to GATA motifsGGP DDR(c)alpha B2 bound the PEBP2 site within the GM-CSF promoterGGP GGPDDR DDRFigure 2: Entity annotation to example sentencesfrom (a) PMID10080948, (b) PMID7575565, and (c)PMID7605990 (simplified).
(a)Ah receptor recognizes the B cell transcription factor, BSAP(b)Grf40 binds to linker for activation of T cells (LAT)(c)expression of p21(WAF1/CIP1) and p27(KIP1)(d)included both p50/p50 and p50/p65 dimers(e)IL-4 Stat, also known as Stat6Figure 3: Equivalent entities in example sentences from(a) PMID7541987 (simplified), (b) PMID10224278, (c)PMID10090931, (d) PMID9243743, (e) PMID7635985.
(Type:Binding, Theme1:TRAF2, Theme2:CD40,Site2:cytoplasmic domain)Note that the protein, CD40, and its domain, cyto-plasmic domain, are associated by argument num-bering.
To resolve issues related to the mappingbetween proteins and related entities systematically,we introduced partial static relation annotation forrelations such as Part-Whole, drawing in part onsimilar annotation of the BioInfer corpus (Pyysaloet al, 2007).
For details of this part of the revisionprocess, we refer to (Pyysalo et al, 2009).Figure 2 shows some challenging cases.
In (b),the site GATA motifs is not identified as an argumentof the binding event, because the protein containingit is not stated.
In (c), among the two sites (PEBP2site and promoter) of the gene GM-CSF, only themore specific one, PEBP2, is annotated.3.3 Equivalent entity referencesAlternative names for the same object are fre-quently introduced in biomedical texts, typicallythrough apposition.
This is illustrated in Figure 3(a),where the two expressions B cell transcription fac-tor and BSAP are in apposition and refer to thesame protein.
Consequently, in this case the fol-lowing two annotations represent the same event:(Type:Binding, Theme:Ah receptor,Theme:B cell transcription factor)(Type:Binding, Theme:Ah receptor, Theme:BSAP)In the GENIA event corpus only one of these is an-notated, with preference given to shorter names overlonger descriptive ones.
Thus of the above exam-ple events, the latter would be annotated.
How-ever, as both express the same event, in the sharedtask evaluation either alternative was accepted ascorrect extraction of the event.
In order to im-plement this aspect of the evaluation, expressionsof equivalent entities were annotated as follows:Eq (B cell transcription factor, BSAP)The equivalent entity annotation in the revised GE-NIA corpus covers also cases other than simple ap-position, illustrated in Figure 3.
A frequent case inbiomedical literature involves use of the slash sym-bol (?/?)
to state synonyms.
The slash symbol isambiguous as it is used also to indicate dimerizedproteins.
In the case of p50/p50, the two p50 areannotated as equivalent because they represent thesame proteins at the same state.
Note that althoughrare, also explicitly introduced aliases are annotated,as in Figure 3(e).4 EvaluationFor the evaluation, the participants were given thetest data with gold annotation only for proteins.
Theevaluation was then carried out by comparing theannotation predicted by each participant to the goldannotation.
For the comparison, equality of anno-tations is defined as described in Section 4.1.
Theevaluation results are reported using the standardrecall/precision/f-score metrics, under different cri-teria defined through the equalities.4.1 Equalities and Strict matchingEquality of events is defined as follows:Event Equality equality holds between any twoevents when (1) the event types are the same,(2) the event triggers are the same, and (3) thearguments are fully matched.4A full matching of arguments between two eventsmeans there is a perfect 1-to-1 mapping between thetwo sets of arguments.
Equality of individual argu-ments is defined as follows:Argument Equality equality holds between anytwo arguments when (1) the role types are thesame, and (2-1) both are t-entities and equalityholds between them, or (2-2) both are eventsand equality holds between them.Due to the condition (2-2), event equality is definedrecursively for events referring to events.
Equalityof t-entities is defined as follows:T-entity Equality equality holds between any twot-entities when (1) the entity types are the same,and (2) the spans are the same.Any two text spans (beg1, end1) and (beg2, end2),are the same iff beg1 = beg2 and end1 = end2.Note that the event triggers are also t-entities thustheir equality is defined by the t-entity equality.4.2 Evaluation modesVarious evaluation modes can be defined by varyingequivalence criteria.
In the following, we describethree fundamental variants applied in the evaluation.Strict matching The strict matching mode requiresexact equality, as defined in section 4.1.
As someof its requirements may be viewed as unnecessarilyprecise, practically motivated relaxed variants, de-scribed in the following, are also applied.Approximate span matching The approximatespan matching mode is defined by relaxing therequirement for text span matching for t-entities.Specifically, a given span is equivalent to a goldspan if it is entirely contained within an extensionof the gold span by one word both to the left andto the right, that is, beg1 ?
ebeg2 and end1 ?eend2, where (beg1, end1) is the given span and(ebeg2, eend2) is the extended gold span.Approximate recursive matching In strict match-ing, for a regulation event to be correct, the events itrefers to as theme or cause must also be be strictlycorrect.
The approximate recursive matching modeis defined by relaxing the requirement for recursiveevent matching, so that an event can match evenif the events it refers to are only partially correct.Event Release dateAnnouncement Dec 8Sample data Dec 15Training data Jan 19 ?
21, Feb 2 (rev1), Feb 10 (rev2)Devel.
data Feb 7Test data Feb 22 ?
Mar 2Submission Mar 2 ?
Mar 9Table 3: Shared task schedule.
The arrows indicate achange of schedule.Specifically, for partial matching, only Theme argu-ments are considered: events can match even if re-ferred events differ in non-Theme arguments.5 ScheduleThe BioNLP task was held for 12 weeks, from thesample data release to the final submission.
It in-cluded 5 weeks of system design period with sam-ple data, 6 weeks of system development period withtraining and development data, and a 1 week test pe-riod.
The system development period was originallyplanned for 5 weeks but extended by 1 week due tothe delay of the training data release and the revi-sion.
Table 3 shows key dates of the schedule.6 Supporting ResourcesTo allow participants to focus development effortson novel aspects of event extraction, we preparedpublicly available BioNLP resources readily avail-able for the shared task.
Several fundamentalBioNLP tools were provided through U-Compare(Kano et al, 2009)2, which included tools for to-kenization, sentence segmentation, part-of-speechtagging, chunking and syntactic parsing.Participants were also provided with the syntacticanalyses created by a selection of parsers.
We ap-plied two mainstream Penn Treebank (PTB) phrasestructure parsers: the Bikel parser3, implementingCollins?
parsing model (Bikel, 2004) and trainedon PTB, and the reranking parser of (Charniakand Johnson, 2005) with the self-trained biomed-ical parsing model of (McClosky and Charniak,2008)4.
We also applied the GDep5, native de-pendency parser trained on the GENIA Treebank2http://u-compare.org/3http://www.cis.upenn.edu/?dbikel/software.html4http://www.cs.brown.edu/?dmcc/biomedical.html5http://www.cs.cmu.edu/?sagae/parser/gdep/5NLP TaskTeam Task Org Word Chunking Parsing Trigger Argument Ext.
ResourcesUTurku 1-- 3C+2BI Porter MC SVM SVM (SVMlight)JULIELab 1-- 1C+2L+2B OpenNLP OpenNLP GDep Dict+Stat SVM(libSVM) UniProt, Mesh,Porter ME(Mallet) GOA, UMLSConcordU 1-3 3C Stanford Stanford Dict+Stat Rules WordNet, VerbNet,UMLSUT+DBCLS 12- 2C Porter MC Dict MLN(thebeast)CCGVIBGhent 1-3 2C+1B Porter, Stanford Dict SVM(libSVM)UTokyo 1-- 3C GTag GDep, Dict ME(liblinear) UIMAEnjuUNSW 1-- 1C+1B GDep CRF Rules WordNet, MetaMapUZurich 1-- 3C LingPipe, LTChunk Pro3Gres Dict RulesMorphaASU+HU+BU 123 6C+2BI Porter BioLG, Dict Rules LuceneCharniak RulesCam 1-- 3C Porter RASP Dict RulesUAntwerp 12- 3C GTag GDep MBL MBL(TiMBL)RulesUNIMAN 1-- 4C+2BI Porter GDep Dict, CRF SVM MeSH, GOGTag RulesSCAI 1-- 1C RulesUAveiro 1-- 1C+1L NooJ NooJ Rules BioLexiconUSzeged 1-3 3C+1B GTag Dict, VSM C4.5(WEKA) BioScopeRulesNICTA 1-3 4C GTag ERG CRF(CRF++) Rules JULIECNBMadrid 12- 2C+1B Porter, GTag CBRGTag RulesCCP-BTMG 123 7C LingPipe LingPipe OpenDMAP LingPipe, CM Rules GO, SO, MIO,UIMACIPS-ASU 1-- 3C MontyTagger Custom Stanford CRF(ABNER) Rules,NB(WEKA)UMich 1-- 2C Stanford MC Dict SVM(SVMlight)PIKB 1-- 5C+2B MIRA MIRAKoreaU 1-- 5C GTag GDep Rules, ME ME WSJTable 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,CBR=Case-Based Reasoning, CM=ConceptMapper.
(Tateisi et al, 2005), and a version of the C&C CCGdeep parser6 adapted to biomedical text (Rimell andClark, 2008).The text of all documents was segmented and to-kenized using the GENIA Sentence Splitter and theGENIA Tagger, provided by U-Compare.
The samesegmentation was enforced for all parsers, whichwere run using default settings.
Both the native out-put of each parser and a representation in the popularStanford Dependency (SD) format (de Marneffe etal., 2006) were provided.
The SD representation wascreated using the Stanford tools7 to convert from thePTB scheme, the custom conversion introduced by(Rimell and Clark, 2008) for the C&C CCG parser,and a simple format-only conversion for GDep.7 Results and Discussion7.1 ParticipationIn total, 42 teams showed interest in the shared taskand registered for participation, and 24 teams sub-6http://svn.ask.it.usyd.edu.au/trac/candc/wiki7http://nlp.stanford.edu/software/lex-parser.shtmlmitted final results.
All 24 teams participated in theobligatory Task 1, six in each of Tasks 2 and 3, andtwo teams completed all the three tasks.Table 4 shows a profile of the 22 final teams,excepting two who wished to remain anonymous.A brief examination on the team organization (theOrg column) shows a computer science background(C) to be most frequent among participants, withless frequent participation from bioinformaticians(BI), biologists (B) and liguists (L).
This may beattributed in part to the fact that the event extrac-tion task required complex computational modeling.The role of computer scientists may be emphasizedin part due to the fact that the task was novel to mostparticipants, requiring particular efforts in frame-work design and implementation and computationalresources.
This also suggests there is room for im-provement from more input from biologists.7.2 Evaluation resultsThe final evaluation results of Task 1 are shown inTable 5.
The results on the five event types involv-6Team Simple Event Binding Regulation AllUTurku 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95JULIELab 59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66ConcordU 49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62UT+DBCLS 55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35VIBGhent 54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54UTokyo 45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88UNSW 45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92UZurich 44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09Cam 39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80UAntwerp 41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58UNIMAN 50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35SCAI 43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26UAveiro 43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38Team 24 41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10USzeged 47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21NICTA 31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29CNBMadrid 50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15CCP-BTMG 28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66CIPS-ASU 39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74UMich 52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28PIKB 26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25Team 09 27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04KoreaU 20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31Table 5: Evaluation results of Task 1 (recall / precision / f-score).Team All Site for Phospho.
(56) AtLoc & ToLoc (65) All Second Args.UT+DBCLS 35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52UAntwerp 21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00Team 24 22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66 21.54 / 66.67 / 32.56 30.10 / 76.62 / 43.22CCP-BTMG 13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96CNBMadrid 25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57 32.31 / 47.73 / 38.53 50.00 / 09.71 / 16.27Table 6: Evaluation results for Task 2.ing only a single primary theme argument are shownin one merged class, ?Simple Event?.
The broad per-formance range (31% ?
70%) indicates even the ex-traction of simple events is not a trivial task.
How-ever, the top-ranked systems show encouraging per-formance, achieving or approaching 70% f-score.The performance ranges for Binding (5% ?
44%)and Regulation (1% ?
40%) events show their ex-traction to be clearly more challenging.
It is in-teresting that while most systems show better per-formance for binding over regulation events, thesystems [ConcordU] and [UT+DBCLS] are betterfor regulation, showing somewhat reduced perfor-mance for Binding events.
This is in particular con-trast to the following two systems, [ViBGhent] and[UTokyo], which show far better performance forBinding than Regulation events.
As one possibleexplanation, we find that the latter two differentiatebinding events by their number of themes, while theformer two give no specific treatment to multi-themebinding events.
Such observations and comparisonsare a clear benefit of a community-wide shared task.Table 6 shows the evaluation results for the teamswho participated in Task 2.
The ?All?
column showsthe overall performance of the systems for Task 2,while the ?All Second Args.?
column shows theperformance of finding only the secondary argu-ments.
The evaluation results show considerabledifferences between the criteria.
For example, thesystem [Team 24] shows performance comparableto the top ranked system in finding secondary argu-ments, although its overall performance for Task 2is more limited.
Table 6 also shows the three sys-tems, [UT+DBCLS], [Team 24] and [CNBMadrid],7Team Negation SpeculationConcordU 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27VIBGhent 10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24NICTA 05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30USzeged 05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95Table 7: Evaluation results for Task 3.010203040506002/18 02/21 02/24 02/27 03/02 03/05 03/08daily averageFigure 4: Scatterplot of the evaluation results on the de-velopment data during the system development period.show performance at a practical level in particular infinding specific sites of phosphorylation.As shown in Table 7, the performance range forTask 3 is very low although the representation of thetask is as simple as the simple events.
We attributethe reason to the fact that Task 3 is the only task ofwhich the annotation is not bound to textual clue,thus no text-bound annotation was provided.Figure 4 shows a scatter plot of the performanceof the participating systems during the system devel-opment period.
The performance evaluation comesfrom the log of the online evaluation system on thedevelopment data.
It shows the best performanceand the average performance of the participatingsystems were trending upwards up until the dead-line of final submission, which indicates there is stillmuch potential for improvement.7.3 EnsembleTable 8 shows experimental results of a system en-semble using the final submissions.
For the ex-periments, the top 3?10 systems were chosen, andthe output of each system treated as a weightedvote8.
Three weighting schemes were used; ?Equal?weights each vote equally; ?Averaged?
weights each8We used the ?ensemble?
function of U-Compare.Ensemble Equal Averaged Event TypeTop 3 53.19 53.19 54.08Top 4 54.34 54.34 55.21Top 5 54.77 55.03 55.10Top 6 55.13 55.77 55.96Top 7 54.33 55.45 55.73Top 10 52.79 54.63 55.18Table 8: Experimental results of system ensemble.vote by the overall f-score of the system; ?EventType?
weights each vote by the f-score of the sys-tem for the specific event type.
The best score,55.96%, was obtained by the ?Event Type?
weight-ing scheme, showing a 4% unit improvement overthe best individual system.
While using the finalscores for weighting uses data that would not beavailable in practice, similar weighting could likelybe obtained e.g.
using performance on the devel-opment data.
The experiment demonstrates that anf-score better than 55% can be achieved simply bycombining the strengths of the systems.8 ConclusionMeeting with the community-wide participation, theBioNLP Shared Task was successful in introducingfine-grained event extraction to the domain.
Theevaluation results of the final submissions from theparticipants are both promising and encouraging forthe future of this approach to IE.
It has been revealedthat state-of-the-art performance in event extractionis approaching a practically applicable level for sim-ple events, and also that there are many remain-ing challenges in the extraction of complex events.A brief analysis suggests that the submitted datatogether with the system descriptions are rich re-sources for finding directions for improvements.
Fi-nally, the experience of the shared task participantsprovides an invaluable basis for cooperation in fac-ing further challenges.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan)and Genome Network Project (MEXT, Japan).8ReferencesGary D. Bader, Michael P. Cary, and Chris Sander.
2006.Pathguide: a Pathway Resource List.
Nucleic AcidsResearch., 34(suppl 1):D504?506.Daniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4):479?511.Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-vian Lee, Emily Dimmer, John Maslen, David Binns,Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.2004.
The Gene Ontology Annotation (GOA)Database: sharing knowledge in Uniprot with GeneOntology.
Nucl.
Acids Res., 32(suppl 1):D262?266.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 173?180.Andrew Chatr-aryamontri, Arnaud Ceol, Luisa Montec-chi Palazzi, Giuliano Nardelli, Maria Victoria Schnei-der, Luisa Castagnoli, and Gianni Cesareni.
2007.MINT: the Molecular INTeraction database.
NucleicAcids Research, 35(suppl 1):D572?574.Nancy Chinchor.
1998.
Overview of MUC-7/MET-2.In Message Understanding Conference (MUC-7) Pro-ceedings.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC?06),pages 449?454.William Hersh, Aaron Cohen, Ruslenm Lynn, , andPhoebe Roberts.
2007.
TREC 2007 Genomics trackoverview.
In Proceeding of the Sixteenth Text RE-trieval Conference.Lynette Hirschman, Martin Krallinger, and Alfonso Va-lencia, editors.
2007.
Proceedings of the SecondBioCreative Challenge Evaluation Workshop.
CNIOCentro Nacional de Investigaciones Oncolo?gicas.Yoshinobu Kano, William Baumgartner, Luke McCro-hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,and Jun?ichi Tsujii.
2009.
U-Compare: share andcompare text mining tools with UIMA.
Bioinformat-ics.
To appear.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introductionto the bio-entity recognition task at JNLPBA.
In Pro-ceedings of the International Joint Workshop on Nat-ural Language Processing in Biomedicine and its Ap-plications (JNLPBA), pages 70?75.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromlterature.
BMC Bioinformatics, 9(1):10.David McClosky and Eugene Charniak.
2008.
Self-Training for Biomedical Parsing.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics - Human Language Technolo-gies (ACL-HLT?08), pages 101?104.Claire Ne?dellec.
2005.
Learning Language in Logic -Genic Interaction Extraction Challenge.
In J. Cussensand C. Ne?dellec, editors, Proceedings of the 4th Learn-ing Language in Logic Workshop (LLL05), pages 31?37.Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, andJun?ichi Tsujii.
2009.
Incorporating GENETAG-styleannotation to GENIA corpus.
In Proceedings of Nat-ural Language Processing in Biomedicine (BioNLP)NAACL 2009 Workshop.
To appear.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2007.
BioInfer: A corpus for informationextraction in the biomedical domain.
BMC Bioinfor-matics, 8(50).Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, andJun?ichi Tsujii.
2009.
Static Relations: a Piecein the Biomedical Information Extraction Puzzle.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop.
Toappear.Laura Rimell and Stephen Clark.
2008.
Porting alexicalized-grammar parser to the biomedical domain.Journal of Biomedical Informatics, To Appear.Stephanie Strassel, Mark Przybocki, Kay Peterson, ZhiyiSong, and Kazuaki Maeda.
2008.
Linguistic Re-sources and Evaluation Techniques for Evaluation ofCross-Document Automatic Content Extraction.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-ten, and John Wilbur.
2005.
Genetag: a tagged cor-pus for gene/protein named entity recognition.
BMCBioinformatics, 6(Suppl 1):S3.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun?ichi Tsujii.
2005.
Syntax Annotation for the GE-NIA corpus.
In Proceedings of the IJCNLP 2005,Companion volume, pages 222?227.Ellen Voorhees.
2007.
Overview of TREC 2007.
InThe Sixteenth Text REtrieval Conference (TREC 2007)Proceedings.John Wilbur, Lawrence Smith, and Lorraine Tanabe.2007.
BioCreative 2.
Gene Mention Task.
InL.
Hirschman, M. Krallinger, and A. Valencia, editors,Proceedings of Second BioCreative Challenge Evalu-ation Workshop, pages 7?16.9
