Lexical Chains and Sliding Locality Windows in Content-basedText Similarity DetectionThade Nahnsen, ?zlem Uzuner, Boris KatzComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA 02139{tnahnsen,ozlem,boris}@csail.mit.eduAbstractWe present a system to determinecontent similarity of documents.Our goal is to identify pairs of bookchapters that are translations of thesame original chapter.
Achievingthis goal requires identification ofnot only the different topics in thedocuments but also of the particularflow of these topics.Our approach to contentsimilarity evaluation employs n-grams of lexical chains andmeasures similarity using thecosine of vectors of n-grams oflexical chains, vectors of tf*idf-weighted keywords, and vectors ofunweighted lexical chains(unigrams of lexical chains).
Ourresults show that n-grams ofunordered lexical chains of lengthfour or more are particularly usefulfor the recognition of contentsimilarity.1   IntroductionThis paper addresses the problem of determiningcontent similarity between chapters of literarynovels.
We aim to determine content similarityeven when book chapters contain more than onetopic by resolving exact content matches ratherthan finding similarities in dominant topics.Our solution to this problem relies on lexicalchains extracted from WordNet [6].2   Related WorkLexical Chains (LC) represent lexical itemswhich are conceptually related to each other, forexample, through hyponymy or synonymyrelations.
Such conceptual relations havepreviously been used in evaluating cohesion,e.g., by Halliday and Hasan [2, 3].
Barzilayand Elhadad [1] used lexical chains for textsummarization; they identified importantsentences in a document by retrieving strongchains.
Silber and McCoy [7] extended thework of Barzilay and Elhadad; they developedan algorithm that is linear in time and space forefficient identification of lexical chains in largedocuments.
In this algorithm, Silber and McCoyfirst created a text representation in the form ofmetachains, i.e., chains that capture all possiblelexical chains in the document.
After creatingthe metachains, they used a scoring algorithm toidentify the lexical chains that are most relevantto the document, eliminated unnecessaryoverhead information from the metachains, andselected the lexical chains representing thedocument.
Our method for building lexicalchains follows this algorithm.N-gram based language models, i.e., modelsthat divide text into n-word (or n-character)strings, are frequently used in natural languageprocessing.
In plagiarism detection, the overlapof n-grams between two documents has beenused to determine whether one documentplagiarizes another [4].
In general, n-gramscapture local relations.
In our case, they capturelocal relations between lexical chains andbetween concepts represented by these chains.Three main streams of research in contentsimilarity detection are: 1) shallow, statisticalanalysis of documents, 2) analysis of rhetoricalrelations in texts [5], and 3) deep syntactic150analysis [8].
Shallow methods do not includemuch linguistic information and provide a veryrough model of content while approaches thatuse syntactic analysis generally requiresignificant computation.
Our approach strikes acompromise between these two extremes: it usesthe linguistic knowledge provided in WordNetas a way of making use of low-cost linguisticinformation for building lexical chains that canhelp detect content similarity.3   Lexical Chains in Content SimilarityDetection3.1   CorpusThe experiments in this paper were performedon a corpus consisting of chapters fromtranslations of four books (Table 1) that cover avariety of topics.
Many of the chapters fromeach book deal with similar topics; therefore,fine-grained content analysis is required toidentify chapters that are derived from the sameoriginal chapter.#translationsTitle #chapters2 20,000 Leagues under the Sea 473 Madame Bovary 352 The Kreutzer Sonata 282 War and Peace 365Table 1: Corpus3.2   Computing Lexical ChainsOur approach to calculating lexical chains usesnouns, verbs, and adjectives present inWordNetV2.0.
We first extract such wordsfrom each chapter in the corpus and representeach chapter as a set of these word instances {I1,?, In}.
Each instance of each of these wordshas a set of possible interpretations, IN, inWordNet.
These interpretations are either thesynsets or the hypernyms of the instances.Given these interpretations, we apply a slightlymodified version of the algorithm by Silber andMcCoy [7] to automatically disambiguatenouns, verbs, and adjectives, i.e., to select thecorrect interpretation, for each instance.
Silberand McCoy?s algorithm computes all of thescored metachains for all senses of each word inthe document and attributes the word to themetachain to which it contributes the most.During this process, the algorithm computes thecontribution of a word to a given chain byconsidering 1) the semantic relations betweenthe synsets of the words that are members of thesame metachain, and 2) the distance betweentheir respective instances in the discourse.
Ourapproach uses these two parameters, with minormodifications.
Silber and McCoy measuredistance in terms of paragraphs on prose text;we measure distance in terms of sentences inorder to handle both dialogue and prose text.Figure 1: Intermediate representation aftereliminating words that are not nouns, verbs, oradjectives and after identifying lexical chains(represented by WordNet synset IDs).
Note that{kitchen, bathroom} are represented by the samesynset ID which corresponds to the synset ID oftheir common hypernym ?room?.
{kitchen,bathroom} is a lexical chain.
Ties are broken infavor of hypernyms.Following Silber and McCoy, we allowdifferent types of conceptual relations tocontribute differently to each lexical chain, i.e.,the contribution of each word to a lexical chainis dependent on its semantic relation to the chain(see Table 2).
After scoring, concepts that aredominant in the text segment are identified andeach word is represented by only the WordNetID of the synset (or the hypernym/hyponym set)that best fits its local context.
Figure 1 gives anexample of the resulting intermediaterepresentation, corresponding to theinterpretation, S, found for each word instance,I, that can be used to represent each chapter, C,where C = {S1, ?, Sm}.LexicalsemanticrelationDistance <=6 sentencesDistance >6 sentencesSame word 1 0Hyponym 0.5 0Hypernym 0.5 0Sibling 0.2 0Table 2: Contribution to lexical chainsOriginal document (underlined words are representedwith lexical chains):The furniture in the kitchen seems beautiful, but the bathroomseems untidy.Intermediate representation (lexical chains):03281101   03951013   02071636   00218842   0395101302071636   023367181513.3 Determining the Locality WindowAfter computing the lexical chains, we created arepresentation for text by substituting the correctlexical chain for each noun, verb, and adjectivein each document.
We omitted the remainingparts of speech from the documents (see Figure1 for sample intermediate representation).
Weobtained ordered and unordered n-grams oflexical chains from this representation.Ordered n-grams consist of n consecutivelexical chains extracted from text.
These orderedn-grams preserve the original order of the lexicalchains in the text.
Corresponding unordered n-grams disregard this order.
The resulting textrepresentation is T = {gram1, gram2, ?, gramn},where grami = [lc1, ?,  lcn], where lci ?
{I1, ?,Ik} (the chains that represent Chapter C).
Theelements in grami may be sorted or unsorted,depending on the selected method.
N-grams areextracted from text using sliding localitywindows and provide what we call ?attributevectors?.
The attribute vector for ordered n-grams has the form C = {(e1, ?, en), (e2, ?,en+1), ?, (em-n, ?, em)} where (e1, ?, en) is anordered n-gram and em is the last lexical chain inthe chapter.
For unordered n-grams, theattribute vector has the form C = {sort[(e1, ?,en)], sort[(e2, ?, en+1)], ?, sort[(em-n, ?, em)]}where sort[?]
indicates alphabetical sorting ofchains (rather than the actual order in which thechains appear in the text).We evaluated similarity between pairs ofbook chapters using the cosine of the attributevectors of n-grams of lexical chains (slidinglocality windows of width n).
We varied thewidth of the sliding locality windows from twoto five elements.4   EvaluationWe used cosine similarity as the distance metric,computed the cosine of the angle between thevectors of pairs of documents in the corpus, andranked the pairs based on this score.
Weidentified the top n most similar pairs (alsoreferred to as ?selection level of n?)
andconsidered them to be similar in content.We calculated similarity between pairs ofdocuments in several different ways, evaluatedthese approaches with the standard informationretrieval measures, i.e., precision, recall, and f-measure, and compared our results with twobaselines.
The first baseline measured thesimilarity of documents with tf*idf-weightedkeywords; the second used the cosine ofunweighted lexical chains (unigrams of lexicalchains).The corpus of parallel translations providesdata that can be used as ground truth for contentsimilarity; corresponding chapters from differenttranslations of the same original title areconsidered similar in content, i.e., chapter 1 oftranslation 1 of Madame Bovary is similar incontent to chapter 1 of translation 2 of MadameBovary.Figure 2 shows the f-measure of differentmethods for measuring similarity between pairsof chapters using ordered lexical chains,unordered lexical chains, and baselines.
Thesegraphs present the results when the top 100?1,600 most similar pairs in the corpus areconsidered similar in content and the rest areconsidered dissimilar (selection level of 100?1,600).
The total number of chapter pairs isapproximately 1,000,000.
Of these, 1,080 (475unique chapters with 2 or 3 translations each)are considered similar for evaluation purposes.The results indicate that four similaritymeasures gave the best performance.
Thesewere tri-grams, quadri-grams, penta-grams, andhexa-grams of unordered lexical chains.
Thepeak f-measure at the selection level of 1,100chapter pairs was 0.981.
Chi squared testsperformed on the f-measures (when the top1,100 pairs were considered similar) weresignificant at p = 0.001.Closer analysis of the graphs in Figure 2shows that, at the optimal selection level, n-grams of ordered lexical chains of length greaterthan four significantly outperformed the baselineat p = 0.001 while n-grams of ordered lexicalchains of length less than or equal to four aresignificantly outperformed by the baseline at thesame p. A similar observation cannot be madefor the n-grams of unordered lexical chains; forthese n-grams, the performance degradationappears at n = 7, i.e., the corresponding curveshave a steeper negative incline than the baseline.After the cut-off point of 1,100 chapter pairs,the performance of all algorithms declines.
Thisis due to the evaluation method we have chosen:although the cut-off for similarity judgement canbe increased, the number of chapters that are infact similar does not change and at high cut-offvalues many dissimilar pairs are consideredsimilar, leading to degradation in performance.152Figures 2a and 2b show that some of thelexical chain representations do not outperformthe tf*idf-weighted baseline.
A comparison ofFigures 2a and 2b shows that, for n < 5, n-gramsof ordered lexical chains perform worse than n-grams of unordered lexical chains.
Thisindicates that between different translations ofthe same book the order of chains changessignificantly, but that the chains withincontiguous regions (locality windows) of thetexts remain similar.Interestingly, ordered n-grams of length 3 to 5perform significantly better than unordered n-grams of the same length.
This implies that,during translation, the order of the contentwords does not change enormously for three tofive lexical chain elements.
Allowing flexibleorder for the lexical chains (i.e., unorderedlexical chains) in these n-grams therefore hurtsperformance by allowing many false positives.However, for longer n-grams to be successful,the order of the lexical chains has to be flexible.Figure 2: F-Measure.F-M e as u r e  vs .
C h ap te r s  Se le cte d  (Un o r d e r e d  N-Gr am s )00,10 ,20 ,30 ,40 ,50 ,60 ,70 ,80 ,91100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600C h ap te r s  Se le cte dF-Measureu2gram/LC u3gram/LC u4gram/LC u5gram/LC u6gram/LCu7gram/LC tf *id f c os ine(a) F-Measure: Unordered n-grams vs. the baselinesF- M e a s u r e  v s .
C h a p t e r s  S e le c t e d  ( O r d e r e d  N-G r a m s )00 ,10 ,20 ,30 ,40 ,50 ,60 ,70 ,80 ,91100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600C h a p te r s  S e le c t e dF-Measuretf * id f c o s in e 4g ram/LC 5g ram/LC 6g ram/LC7g ram/LC 2g ram/LC 3g ram/LC(b) F-Measure: Ordered n-grams vs. the baselinesngram/LC ?
unordered n-grams of lexical chains are used in the attribute vectorungram/LC ?
ordered n-grams of lexical chains are used in the attribute vectortf*idf ?
tf*idf weighted words are used in the attribute vectorcosine ?
the standard information retrieval measure; words are used in the attribute vector1535   Future WorkCurrently, our similarity measures do notemploy any weighting scheme for n-grams, i.e.,every n-gram is given the same weight.
Forexample, the n-gram ?be it as it has been?
inlexical chain form corresponds to synsets for thewords be, have and be.
The trigram of theselexical chains does not convey significantmeaning.
On the other hand, the n-gram ?thelawyer signed the heritage?
is converted into thetrigram of lexical chains of lawyer, sign, andheritage.
This trigram is more meaningful thanthe trigram be have be, but in our scheme bothtrigrams will get the same weight.
As a result,two documents that share the trigram be have bewill look as similar as two documents that sharelawyer sign heritage.
This problem can beaddressed in two possible ways: using a ?stopword?
list to filter such expressions completelyor giving different weights to n-grams based onthe number of their occurrences in the corpus.6   ConclusionWe have presented a system that extendsprevious work on lexical chains to contentsimilarity detection.
This system employslexical chains and sliding locality windows, andevaluates similarity using the cosine of n-gramsof lexical chains and tf*idf weighted keywords.The results indicate that lexical chains areeffective for detecting content similaritybetween pairs of chapters corresponding to thesame original in a corpus of parallel translations.References1.
Barzilay, R., Elhadad, M. 1999.
Using lexicalchains for text summarization.
In: InderjeetMani and Mark T. Maybury, eds., Advancesin AutomaticText Summarization, pp.
111?121.
Cambridge/MA, London/England: MITPress.2.
Halliday, M. and Hasan, R. 1976.
Cohesion inEnglish.
Longman, London.3.
Halliday, M. and Hasan, R. 1989.
Language,context, and text.
Oxford University Press,Oxford, UK.4.
Lyon, C., Malcolm, J. and Dickerson, B.2001.
Detecting Short Passages of SimilarText in Large Document Collections, InProceedings of the 2001 Conference onEmpirical Methods in Natural LanguageProcessing, pp.118-125.5.
Marcu, D. 1997.
The Rhetorical Parsing,Summarization, and Generation of NaturalLanguage Texts (Ph.D. dissertation).
Univ.
ofToronto.6.
Miller, G., Beckwith, R., Felbaum, C., Gross,D., and Miller, K. 1990.
Introduction toWordNet: An online lexical database.
J.Lexicography, 3(4), pp.
235-244.7.
Silber, G. and McCoy, K. 2002.
Efficientlycomputed lexical chains as an intermediaterepresentation for automatic textsummarization.
Computational Linguistics,28(4).8.
Uzuner, O., Davis, R., Katz, B.
2004.
UsingEmpirical Methods for Evaluating Expressionand Content Similarity.
In: Proceedings of the37th Hawaiian International Conference onSystem Sciences (HICSS-37).
IEEEComputer Society.154
