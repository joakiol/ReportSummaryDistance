Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 989?998,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsStarting From Scratch in Semantic Role LabelingMichael ConnorUniversity of Illinoisconnor2@uiuc.eduYael GertnerUniversity of Illinoisygertner@cyrus.psych.uiuc.eduCynthia FisherUniversity of Illinoiscfisher@cyrus.psych.uiuc.eduDan RothUniversity of Illinoisdanr@illinois.eduAbstractA fundamental step in sentence compre-hension involves assigning semantic rolesto sentence constituents.
To accomplishthis, the listener must parse the sentence,find constituents that are candidate argu-ments, and assign semantic roles to thoseconstituents.
Each step depends on priorlexical and syntactic knowledge.
Wheredo children learning their first languagesbegin in solving this problem?
In this pa-per we focus on the parsing and argument-identification steps that precede Seman-tic Role Labeling (SRL) training.
Wecombine a simplified SRL with an un-supervised HMM part of speech tagger,and experiment with psycholinguistically-motivated ways to label clusters resultingfrom the HMM so that they can be usedto parse input for the SRL system.
Theresults show that proposed shallow rep-resentations of sentence structure are ro-bust to reductions in parsing accuracy, andthat the contribution of alternative repre-sentations of sentence structure to suc-cessful semantic role labeling varies withthe integrity of the parsing and argument-identification stages.1 IntroductionIn this paper we present experiments with an au-tomatic system for semantic role labeling (SRL)that is designed to model aspects of human lan-guage acquisition.
This simplified SRL system isinspired by the syntactic bootstrapping theory, andby an account of syntactic bootstrapping knownas ?structure-mapping?
(Fisher, 1996; Gillette etal., 1999; Lidz et al, 2003).
Syntactic bootstrap-ping theory proposes that young children use theirvery partial knowledge of syntax to guide sen-tence comprehension.
The structure-mapping ac-count makes three key assumptions: First, sen-tence comprehension is grounded by the acquisi-tion of an initial set of concrete nouns.
Nouns arearguably less dependent on prior linguistic knowl-edge for their acquisition than are verbs; thus chil-dren are assumed to be able to identify the refer-ents of some nouns via cross-situational observa-tion (Gillette et al, 1999).
Second, these nouns,once identified, yield a skeletal sentence structure.Children treat each noun as a candidate argument,and thus interpret the number of nouns in the sen-tence as a cue to its semantic predicate-argumentstructure (Fisher, 1996).
Third, children representsentences in an abstract format that permits gener-alization to new verbs (Gertner et al, 2006).The structure-mapping account of early syn-tactic bootstrapping makes strong predictions, in-cluding predictions of tell-tale errors.
In the sen-tence ?Ellen and John laughed?, an intransitiveverb appears with two nouns.
If young chil-dren rely on representations of sentences as sim-ple as an ordered set of nouns, then they shouldhave trouble distinguishing such sentences fromtransitive sentences.
Experimental evidence sug-gests that they do: 21-month-olds mistakenly in-terpreted word order in sentences such as ?The girland the boy kradded?
as conveying agent-patientroles (Gertner and Fisher, 2006).Previous computational experiments with asystem for automatic semantic role labeling(BabySRL: (Connor et al, 2008)) showed thatit is possible to learn to assign basic semanticroles based on the shallow sentence representa-tions proposed by the structure-mapping view.Furthermore, these simple structural features wererobust to drastic reductions in the integrity ofthe semantic-role feedback (Connor et al, 2009).These experiments showed that representations ofsentence structure as simple as ?first of two nouns?are useful, but the experiments relied on perfect989knowledge of arguments and predicates as a startto classification.Perfect built-in parsing finesses two problemsfacing the human learner.
The first problem in-volves classifying words by part-of-speech.
Pro-posed solutions to this problem in the NLP andhuman language acquisition literatures focus ondistributional learning as a key data source (e.g.,(Mintz, 2003; Johnson, 2007)).
Importantly,infants are good at learning distributional pat-terns (Gomez and Gerken, 1999; Saffran et al,1996).
Here we use a fairly standard HiddenMarkov Model (HMM) to generate clusters ofwords that occur in similar distributional contextsin a corpus of input sentences.The second problem facing the learner ismore contentious: Having identified clusters ofdistributionally-similar words, how do childrenfigure out what role these clusters of words shouldplay in a sentence interpretation system?
Someclusters contain nouns, which are candidate ar-guments; others contain verbs, which take argu-ments.
How is the child to know which are which?In order to use the output of the HMM tagger toprocess sentences for input to an SRL model, wemust find a way to automatically label the clusters.Our strategies for automatic argument and pred-icate identification, spelled out below, reflect coreclaims of the structure-mapping theory: (1) Themeanings of some concrete nouns can be learnedwithout prior linguistic knowledge; these concretenouns are assumed based on their meanings to bepossible arguments; (2) verbs are identified, notprimarily by learning their meanings via observa-tion, but rather by learning about their syntacticargument-taking behavior in sentences.By using the HMM part-of-speech tagger in thisway, we can ask how the simple structural fea-tures that we propose children start with stand upto reductions in parsing accuracy.
In doing so, wemove to a parser derived from a particular theoret-ical account of how the human learner might clas-sify words, and link them into a system for sen-tence comprehension.2 ModelWe model language learning as a Semantic RoleLabeling (SRL) task (Carreras and Ma`rquez,2004).
This allows us to ask whether a learner,equipped with particular theoretically-motivatedrepresentations of the input, can learn to under-stand sentences at the level of who did what towhom.
The architecture of our system is similarto a previous approach to modeling early languageacquisition (Connor et al, 2009), which is itselfbased on the standard architecture of a full SRLsystem (e.g.
(Punyakanok et al, 2008)).This basic approach follows a multi-stagepipeline, with each stage feeding in to the next.The stages are: (1) Parsing the sentence, (2) Iden-tifying potential predicates and arguments basedon the parse, (3) Classifying role labels for eachpotential argument relative to a predicate, (4) Ap-plying constraints to find the best labeling of ar-guments for a sentence.
In this work we attemptto limit the knowledge available at each stage tothe automatic output of the previous stage, con-strained by knowledge that we argue is availableto children in the early stages of language learn-ing.In the parsing stage we use an unsupervisedparser based on Hidden Markov Models (HMM),modeling a simple ?predict the next word?
parser.Next the argument identification stage identifiesHMM states that correspond to possible argu-ments and predicates.
The candidate argumentsand predicates identified in each input sentence arepassed to an SRL classifier that uses simple ab-stract features based on the number and order ofarguments to learn to assign semantic roles.As input to our learner we use samples ofnatural child directed speech (CDS) from theCHILDES corpora (MacWhinney, 2000).
Duringinitial unsupervised parsing we experiment withincorporating knowledge through a combinationof statistical priors favoring a skewed distributionof words into classes, and an initial hard cluster-ing of the vocabulary into function and contentwords.
The argument identifier uses a small setof frequent nouns to seed argument states, relyingon the assumptions that some concrete nouns canbe learned as a prerequisite to sentence interpreta-tion, and are interpreted as candidate arguments.The SRL classifier starts with noisy largely un-supervised argument identification, and receivesfeedback based on annotation in the PropBankstyle; in training, each word identified as an argu-ment receives the true role label of the phrase thatword is part of.
This represents the assumptionthat learning to interpret sentences is naturally su-pervised by the fit of the learner?s predicted mean-ing with the referential context.
The provision990of perfect ?gold-standard?
feedback over-estimatesthe real child?s access to this supervision, but al-lows us to investigate the consequences of noisyargument identification for SRL performance.
Weshow that even with imperfect parsing, a learnercan identify useful abstract patterns for sentenceinterpretation.
Our ultimate goal is to ?close theloop?
of this system, by using learning in the SRLsystem to improve the initial unsupervised parseand argument identification.The training data were samples of parentalspeech to three children (Adam, Eve, andSarah; (Brown, 1973)), available via CHILDES.The SRL training corpus consists of parental utter-ances in samples Adam 01-20 (child age 2;3 - 3;1),Eve 01-18 (1;6 - 2;2), and Sarah 01-83 (2;3 - 3;11).All verb-containing utterances without symbolsindicating disfluencies were automatically parsedwith the Charniak parser (Charniak, 1997), anno-tated using an existing SRL system (Punyakanoket al, 2008) and then errors were hand-corrected.The final annotated sample contains about 16,730propositions, with 32,205 arguments.3 Unsupervised ParsingAs a first step of processing, we feed the learnerlarge amounts of unlabeled text and expect it tolearn some structure over this data that will facil-itate future processing.
The source of this textis child directed speech collected from variousprojects in the CHILDES repository1.
We re-moved sentences with fewer than three words ormarkers of disfluency.
In the end we used 160thousand sentences from this set, totaling over 1million tokens and 10 thousand unique words.The goal of the parsing stage is to give thelearner a representation permitting it to generalizeover word forms.
The exact parse we are after isa distributional and context-sensitive clustering ofwords based on sequential processing.
We chosean HMM based parser for this since, in essencethe HMM yields an unsupervised POS classifier,but without names for states.
An HMM trainedwith expectation maximization (EM) is analogousto a simple process of predicting the next word in astream and correcting connections accordingly foreach sentence.1We used parts of the Bloom (Bloom, 1970; Bloom,1973), Brent (Brent and Siskind, 2001), Brown (Brown,1973), Clark (Clark, 1978), Cornell, MacWhin-ney (MacWhinney, 2000), Post (Demetras et al, 1986)and Providence (Demuth et al, 2006) collections.With HMM we can also easily incorporate ad-ditional knowledge during parameter estimation.The first (and simplest) parser we used was anHMM trained using EM with 80 hidden states.The number of hidden states was made relativelylarge to increase the likelihood of clusters corre-sponding to a single part of speech, while preserv-ing some degree of generalization.Johnson (2007) observed that EM tends to cre-ate word clusters of uniform size, which doesnot reflect the way words cluster into parts ofspeech in natural languages.
The addition of pri-ors biasing the system toward a skewed alloca-tion of words to classes can help.
The secondparser was an 80 state HMM trained with Varia-tional Bayes EM (VB) incorporating Dirichlet pri-ors (Beal, 2003).2In the third and fourth parsers we experi-ment with enriching the HMM POS-tagger withother psycholinguistically plausible knowledge.Words of different grammatical categories dif-fer in their phonological as well as in their dis-tributional properties (e.g., (Kelly, 1992; Mon-aghan et al, 2005; Shi et al, 1998)); combiningphonological and distributional information im-proves the clustering of words into grammaticalcategories.
The phonological difference betweencontent and function words is particularly strik-ing (Shi et al, 1998).
Even newborns can cate-gorically distinguish content and function words,based on the phonological difference between thetwo classes (Shi et al, 1999).
Human learners maytreat content and function words as distinct classesfrom the start.To implement this division into function andcontent words3, we start with a list of functionword POS tags4 and then find words that appearpredominantly with these POS tags, using taggedWSJ data (Marcus et al, 1993).
We allocated afixed number of states for these function words,and left the rest of the states for the rest of thewords.
This amounts to initializing the emissionmatrix for the HMM with a block structure; wordsfrom one class cannot be emitted by states al-located to the other class.
This trick has beenused before in speech recognition work (Rabiner,2We tuned the prior using the same set of 8 value pairssuggested by Gao and Johnson (2008), using a held out set ofPOS-tagged CDS to evaluate final performance.3We also include a small third class for punctuation,which is discarded.4TO,IN,EX,POS,WDT,PDT,WRB,MD,CC,DT,RP,UH9911989), and requires far fewer resources than thefull tagging dictionary that is often used to intel-ligently initialize an unsupervised POS classifier(e.g.
(Brill, 1997; Toutanova and Johnson, 2007;Ravi and Knight, 2009)).Because the function and content word preclus-tering preceded parameter estimation, it can becombined with either EM or VB learning.
Al-though this initial split forces sparsity on the emis-sion matrix and allows more uniform sized clus-ters, Dirichlet priors may still help, if word clus-ters within the function or content word subsetsvary in size and frequency.
The third parser wasan 80 state HMM trained with EM estimation,with 30 states pre-allocated to function words;the fourth parser was the same except that it wastrained with VB EM.3.1 Parser Evaluation3.23.43.63.844.24.44.64.855.2100  1000  10000  100000  1e+06VariationofInformationTraining SentencesEMVBEM+FunctVB+FunctFigure 1: Unsupervised Part of Speech results, match-ing states to gold POS labels.
All systems use 80 states, andcomparison is to gold labeled CDS text, which makes up asubset of the HMM training data.
Variation of Information isan information-theoretic measure summing mutual informa-tion between tags and states, proposed by (Meila?, 2002), andfirst used for Unsupervised Part of Speech in (Goldwater andGriffiths, 2007).
Smaller numbers are better, indicating lessinformation lost in moving from the HMM states to the goldPOS tags.
Note that incorporating function word precluster-ing allows both EM and VB algorithms to achieve the sameperformance with an order of magnitude fewer sentences.We first evaluate these parsers (the first stageof our SRL system) on unsupervised POS tag-ging.
Figure 1 shows the performance of the foursystems using Variation of Information to mea-sure match between gold states and unsupervisedparsers as we vary the amount of text they receive.Each point on the graph represents the average re-sult over 10 runs of the HMM with different sam-ples of the unlabeled CDS.
Another common mea-sure for unsupervised POS (when there are morestates than tags) is a many to one greedy mappingof states to tags.
It is known that EM gives a bettermany to one score than VB trained HMM (John-son, 2007), and likewise we see that here: withall data EM gives 0.75 matching, VB gives 0.74,while both EM+Funct and VB+Funct reach 0.80.Adding the function/content word split to theHMM structure improves both EM and VB esti-mation in terms of both tag matching accuracy andinformation.
However, these measures look at theparser only in isolation.
What is more important tous is how useful the provided word clusters are forfuture semantic processing.
In the next sectionswe use the outputs of our four parsers to identifyarguments and predicates.4 Argument IdentificationThe unsupervised parser provides a state label foreach word in each sentence; the goal of the ar-gument identification stage is to use these statesto label words as potential arguments, predicatesor neither.
As described in the introduction, corepremises of the structure-mapping account offerroutes whereby we could label some HMM statesas argument or predicate states.The structure-mapping account holds that sen-tence comprehension is grounded in the learningof an initial set of nouns.
Children are assumedto identify the referents of some concrete nounsvia cross-situational learning (Gillette et al, 1999;Smith and Yu, 2008).
Children then assume, byvirtue of the meanings of these nouns, that they arecandidate arguments.
This is a simple form of se-mantic bootstrapping, requiring the use of built-inlinks between semantics and syntax to identify thegrammatical type of known words (Pinker, 1984).We use a small set of known nouns to transformunlabeled word clusters into candidate argumentsfor the SRL: HMM states that are dominated byknown names for animate or inanimate objects areassumed to be argument states.Given text parsed by the HMM parser and alist of known nouns, the argument identifier pro-ceeds in multiple steps as illustrated in figure 2.The first stage identifies as argument states thosestates that appear at least half the time in the train-ing data with known nouns.
This use of a seedlist and distributional clustering is similar to Proto-type Driven Learning (Haghighi and Klein, 2006),except we are only providing information on onespecific class.992Algorithm ARGUMENT STATE IDENTIFICATIONINPUT: Parsed Text T = list of (word, state) pairsSet of concrete nouns NOUTPUT: Set of argument states AArgument count likelihood ArgLike(s, c)Identify Argument StatesLet freq(s) = |{(?, s) ?
T}|Let freqN (s) = |{(w, s) ?
T |w ?
N}|For each s:If freqN (s) ?
freq(s)/2Add s to ACollect Per Sentence Argument Count statisticsFor each Sentence S ?
T :Let Arg(S) = |{(w, s) ?
S|s ?
A}|For (w, s) ?
S s.t.
s /?
AIncrement ArgCount(s, Arg(S))For each s /?
A, and argument count c:ArgLike(s, c) = ArgCount(s, c)/freq(s)(a) Argument IdentificationAlgorithm PREDICATE STATE IDENTIFICATIONINPUT: Parsed Sentence S = list of (word, state) pairsSet of argument states ASentence Argument Count ArgLike(s, c)OUTPUT: Most likely predicate (v, sv)Find Number of arguments in sentenceLet Arg(S) = |{(w, s) ?
S|s ?
A}|Find Non-argument state in sentence most likelyto appear with this number of arguments(v, sv) = argmax(w,s)?SArgLike(s, Arg(S))(b) Predicate IdentificationFigure 2: Argument identification algorithm.
This is a twostage process: argument state identification based on statis-tics collected over entire text and per sentence predicate iden-tification.As a list of known nouns we collected all thosenouns that appear three times or more in the childdirected speech training data and judged to be ei-ther animate or inanimate nouns.
The full set of365 nouns covers over 93% of noun occurencesin our data.
In upcoming sections we experimentwith varying the number of seed nouns used fromthis set, selecting the most frequent set of nouns.Reflecting the spoken nature of the child directedspeech, the most frequent nouns are pronouns,but beyond the top 10 we see nouns naming peo-ple (?daddy?, ?ursula?)
and object nouns (?chair?,?lunch?
).What about verbs?
A typical SRL model iden-tifies candidate arguments and tries to assign rolesto them relative to each verb in the sentence.
Inprinciple one might suppose that children learnthe meanings of verbs via cross-situational ob-servation just as they learn the meanings of con-crete nouns.
But identifying the meanings ofverbs is much more troublesome.
Verbs?
mean-ings are abstract, therefore harder to identify basedon scene information alone (Gillette et al, 1999).As a result, early vocabularies are dominated bynouns (Gentner, 2006).
On the structure-mappingaccount, learners identify verbs, and begin to de-termine their meanings, based on sentence struc-ture cues.
Verbs take noun arguments; thus, learn-ers could learn which words are verbs by detect-ing each verb?s syntactic argument-taking behav-ior.
Experimental evidence provides some supportfor this procedure: 2-year-olds keep track of thesyntactic structures in which a new verb appears,even without a concurrent scene that provides cuesto the verb?s semantic content (Yuan and Fisher,2009).We implement this behavior by identifying aspredicate states the HMM states that appear com-monly with a particular number of previouslyidentified arguments.
First, we collect statisticsover the entire HMM training corpus regardinghow many arguments are identified per sentence,and which states that are not identified as argu-ment states appear with each number of argu-ments.
Next, for each parsed sentence that servesas SRL input, the algorithm chooses as the mostlikely predicate the word whose state is most likelyto appear with the number of arguments found inthe current input sentence.
Note that this algo-rithm assumes exactly one predicate per sentence.Implicitly, the argument count likelihood dividespredicate states up into transitive and intransitivepredicates based on appearances in the simple sen-tences of CDS.4.1 Argument Identification EvaluationFigure 3 shows argument and predicate identifi-cation accuracy for each of the four parsers whenprovided with different numbers of known nouns.The known word list is very skewed with its mostfrequent members dominating the total noun oc-currences in the data.
The ten most frequentwords5 account for 60% of the total noun occur-rences.
We achieve the different occurrence cov-erage numbers of figure 3 by using the most fre-quent N words from the list that give the specificcoverage6.
Pronouns refer to people or objects,but are abstract in that they can refer to any personor object.
The inclusion of pronouns in our list of5you, it, I, what, he, me, ya, she, we, her6N of 5, 10, 30, 83, 227 cover 50%, 60%, 70%, 80%,90% of all noun occurrences9930.20.30.40.50.60.70.80.45  0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95F1%Noun Occurences CoveredEMVBEM+FunctVB+FunctFigure 3: Effect of number of concrete nouns for seedingargument identification with various unsupervised parsers.Argument identification accuracy is computed against true ar-gument boundaries from hand labeled data.
The upper set ofresults show primary argument (A0-4) identification F1, andbottom lines show predicate identification F1.known nouns represents the assumption that tod-dlers have already identified pronouns as referen-tial terms.
Even 19-month-olds assign appropri-ately different interpretations to novel verbs pre-sented in simple transitive versus intransitive sen-tences with pronoun arguments (?He?s kraddinghim!?
vs. ?He?s kradding!?
; (Yuan et al, 2007)).In ongoing work we experiment with other meth-ods of identifying seed nouns.Two groups of curves appear in figure 3: theupper group shows the primary argument iden-tification accuracy and the bottom group showsthe predicate identification accuracy.
We evaluatecompared to gold tagged data with true argumentand predicate boundaries.
The primary argument(A0-4) identification accuracy is the F1 value, withprecision calculated as the proportion of identifiedarguments that appear as part of a true argument,and recall as the proportion of true arguments thathave some state identified as an argument.
F1 iscalculated similarly for predicate identification, asone state per sentence is identified as the predicate.As shown in figure 3, argument identification F1is higher than predicate identification (which is tobe expected, given that predicate identification de-pends on accurate arguments), and as we add moreseed nouns the argument identification improves.Surprisingly, despite the clear differences in un-supervised POS performance seen in figure 1, thedifferent parsers do not yield very different argu-ment and predicate identification.
As we will seein the next section, however, when the argumentsidentified in this step are used to train SRL clas-sifier, distinctions between parsers reappear, sug-gesting that argument identification F1 masks sys-tematic patterns in the errors.5 Testing SRL PerformanceFinally, we used the results of the previous pars-ing and argument-identification stages in traininga simplified SRL classifier (BabySRL), equippedwith sets of features derived from the structure-mapping account.
For argument classification weused a linear classifier trained with a regularizedperceptron update rule (Grove and Roth, 2001).In the results reported below the BabySRL didnot use sentence-level inference for the final clas-sification, every identified argument is classifiedindependently; thus multiple nouns can have thesame role.
In what follows, we compare the per-formance of the BabySRL across the four parsers.We evaluated SRL performance by testing theBabySRL with constructed sentences like thoseused for the experiments with children describedin the Introduction.
All test sentences contained anovel verb, to test the model?s ability to general-ize.We examine the performance of four versionsof the BabySRL, varying in the features used torepresent sentences.
All four versions includelexical features consisting of the target argumentand predicate (as identified in the previous steps).The baseline model has only these lexical features(Lexical).
Following Connor et al (2008; 2009),the key feature type we propose is noun patternfeatures (NounPat).
Noun pattern features indi-cate how many nouns there are in the sentence andwhich noun the target is.
For example, in ?Youdropped it!
?, ?you?
has a feature active indicatingthat it is the first of two nouns, while ?it?
has a fea-ture active indicating that it is the second of twonouns.
We compared the behavior of noun pat-tern features to another simple representation ofword order, position relative to the verb (VerbPos).In the same example sentence, ?you?
has a featureactive indicating that it is pre-verbal; for ?it?
a fea-ture is active indicating that it is post-verbal.
Afourth version of the BabySRL (Combined) usedboth NounPat and VerbPos features.We structured our tests of the BabySRL to testthe predictions of the structure-mapping account.
(1) NounPat features will improve the SRL?s abil-ity to interpret simple transitive test sentencescontaining two nouns and a novel verb, relative994to a lexical baseline.
Like 21-month-old chil-dren (Gertner et al, 2006), the SRL should inter-pret the first noun as an agent and the second asa patient.
(2) Because NounPat features representword order solely in terms of a sequence of nouns,an SRL equipped with these features will make theerrors predicted by the structure-mapping accountand documented in children (Gertner and Fisher,2006).
(3) NounPat features permit the SRL toassign different roles to the subjects of transitiveand intransitive sentences that differ in their num-ber of nouns.
This effect follows from the natureof the NounPat features: These features partitionthe training data based on the number of nouns,and therefore learn separately the likely roles ofthe ?1st of 1 noun?
and the ?1st of 2 nouns?.These patterns contrast with the behavior of theVerbPos features: When the BabySRL was trainedwith perfect parsing, VerbPos promoted agent-patient interpretations of transitive test sentences,and did so even more successfully than Noun-Pat features did, reflecting the usefulness of po-sition relative to the verb in understanding Englishsentences.
In addition, VerbPos features elimi-nated the errors with two-noun intransitive sen-tences.
Given test sentences such as ?You andMommy krad?, VerbPos features represented bothnouns as pre-verbal, and therefore identified bothas likely agents.
However, VerbPos features didnot help the SRL assign different roles to thesubjects of simple transitive and intransitive sen-tences: ?Mommy?
in ?Mommy krads you?
and?Mommy krads?
are both represented simply aspre-verbal.To test the system?s predictions on transitive andintransitive two noun sentences, we constructedtwo test sentence templates: ?A krads B?
and ?Aand B krad?, where A and B were replaced withfamiliar animate nouns.
The animate nouns wereselected from all three children?s data in the train-ing set and paired together in the templates suchthat all pairs are represented.Figure 4 shows SRL performance on test sen-tences containing a novel verb and two animatenouns.
Each plot shows the proportion of test sen-tences that were assigned an agent-patient (A0-A1) role sequence; this sequence is correct fortransitive sentences but is an error for two-nounintransitive sentences.
Each group of bars showsthe performance of the BabySRL trained using oneof the four parsers, equipped with each of our fourfeature sets.
The top and bottom panels in Figure 4differ in the number of nouns provided to seed theargument identification stage.
The top row showsperformance with 10 seed nouns (the 10 most fre-quent nouns, mostly animate pronouns), and thebottom row shows performance with 365 concrete(animate or inanimate) nouns treated as known.Relative to the lexical baseline, NounPat featuresfared well: they promoted the assignment of A0-A1 interpretations to transitive sentences, acrossall parser versions and both sets of known nouns.Both VB estimation and the content-function wordsplit increased the ability of NounPat features tolearn that the first of two nouns was an agent, andthe second a patient.
The NounPat features alsopromote the predicted error with two-noun intran-sitive sentences (Figures 4(b), 4(d)).
Despite therelatively low accuracy of predicate identificationnoted in section 4.1, the VerbPos features did suc-ceed in promoting an A0A1 interpretation for tran-sitive sentences containing novel verbs relative tothe lexical baseline.
In every case the performanceof the Combined model that includes both Noun-Pat and VerbPos features exceeds the performanceof either NounPat or VerbPos alone, suggestingboth contribute to correct predictions for transitivesentences.
However, the performance of VerbPosfeatures did not improve with parsing accuracy asdid the performance of the NounPat features.
Moststrikingly, the VerbPos features did not eliminatethe predicted error with two-noun intransitive sen-tences, as shown in panels 4(b) and 4(d).
TheCombined model predicted an A0A1 sequence forthese sentences, showing no reduction in this errordue to the participation of VerbPos features.Table 1 shows SRL performance on the sametransitive test sentences (?A krads B?
), comparedto simple one-noun intransitive sentences (?Akrads?).
To permit a direct comparison, the tablereports the proportion of transitive test sentencesfor which the first noun was assigned an agent(A0) interpretation, and the proportion of intran-sitive test sentences with the agent (A0) role as-signed to the single noun in the sentence.
Here wereport only the results from the best-performingparser (trained with VB EM, and content/functionword pre-clustering), compared to the same clas-sifiers trained with gold standard argument iden-tification.
When trained on arguments identifiedvia the unsupervised POS tagger, noun patternfeatures promoted agent interpretations of tran-995Two Noun Transitive, % Agent First One Noun Intransitive, % Agent PredictionLexical NounPat VerbPos Combine Lexical NounPat VerbPos CombineVB+Funct 10 seed 0.48 0.61 0.55 0.71 0.48 0.57 0.56 0.59VB+Funct 365 seed 0.22 0.64 0.41 0.74 0.23 0.33 0.43 0.41Gold Arguments 0.16 0.41 0.69 0.77 0.17 0.18 0.70 0.58Table 1: SRL result comparison when trained with best unsupervised argument identifier versus trained with gold arguments.Comparison is between agent first prediction of two noun transitive sentences vs. one noun intransitive sentences.
The unsu-pervised arguments lead the classifier to rely more on noun pattern features; when the true arguments and predicate are knownthe verb position feature leads the classifier to strongly indicate agent first in both settings.00.10.20.30.40.50.60.70.8EM VB EM+Funct VB+Funct Gold%A0A1LexicalNounPatVerbPosCombine(a) Two Noun Transitive Sentence, 10 seed nouns00.10.20.30.40.50.60.70.8EM VB EM+Funct VB+Funct Gold%A0A1LexicalNounPatVerbPosCombine(b) Two Noun Intransitive Sentence, 10 seed nouns00.10.20.30.40.50.60.70.8EM VB EM+Funct VB+Funct Gold%A0A1LexicalNounPatVerbPosCombine(c) Two Noun Transitive Sentence, 365 seed nouns00.10.20.30.40.50.60.70.8EM VB EM+Funct VB+Funct Gold%A0A1LexicalNounPatVerbPosCombine(d) Two Noun Intransitive Sentence, 365 seed nounsFigure 4: SRL classification performance on transitive and intransitive test sentences containing two nouns and a novelverb.
Performance with gold-standard argument identification is included for comparison.
Across parses, noun pattern featurespromote agent-patient (A0A1) interpretations of both transitive (?You krad Mommy?)
and two-noun intransitive sentences(?You and Mommy krad?
); the latter is an error found in young children.
Unsupervised parsing is less accurate in identifyingthe verb, so verb position features fail to eliminate errors with two-noun intransitive sentences.sitive subjects, but not for intransitive subjects.This differentiation between transitive and intran-sitive sentences was clearer when more knownnouns were provided.
Verb position features, incontrast, promote agent interpretations of subjectsweakly with unsupervised argument identification,but equally for transitive and intransitive.Noun pattern features were robust to increasesin parsing noise.
The behavior of verb positionfeatures suggests that variations in the identifiabil-ity of different parts of speech can affect the use-fulness of alternative representations of sentencestructure.
Representations that reflect the posi-tion of the verb may be powerful guides for un-derstanding simple English sentences, but repre-sentations reflecting only the number and order ofnouns can dominate early in acquisition, depend-ing on the integrity of parsing decisions.6 Conclusion and Future WorkThe key innovation in the present work is thecombination of unsupervised part-of-speech tag-ging and argument identification to permit learn-ing in a simplified SRL system.
Children do not996have the luxury of treating part-of-speech taggingand semantic role labeling as separable tasks.
In-stead, they must learn to understand sentencesstarting from scratch, learning the meanings ofsome words, and using those words and their pat-terns of arrangement into sentences to bootstraptheir way into more mature knowledge.We have created a first step toward modelingthis incremental process.
We combined unsuper-vised parsing with minimal supervision to begin toidentify arguments and predicates.
An SRL clas-sifier used simple representations built from theseidentified arguments to extract useful abstract pat-terns for classifying semantic roles.
Our resultssuggest that multiple simple representations ofsentence structure could co-exist in the child?s sys-tem for sentence comprehension; representationsthat will ultimately turn out to be powerful guidesto role identification may be less powerful early inacquisition because of the noise introduced by theunsupervised parsing.The next step is to ?close the loop?, using higherlevel semantic feedback to improve the earlier ar-gument identification and parsing stages.
Per-haps with the help of semantic feedback the sys-tem can automatically improve predicate identifi-cation, which in turn allows it to correct the ob-served intransitive sentence error.
This approachwill move us closer to the goal of using initial sim-ple structural patterns and natural observation ofthe world (semantic feedback) to bootstrap moreand more sophisticated representations of linguis-tic structure.AcknowledgmentsThis research is supported by NSF grant BCS-0620257 and NIH grant R01-HD054448.ReferencesM.J.
Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, GatsbyComputational Neuroscience Unit, University Col-lege London.L.
Bloom.
1970.
Language development: Form andfunction in emerging grammars.
MIT Press, Cam-bridge, MA.L.
Bloom.
1973.
One word at a time: The use ofsingle-word utterances before syntax.
Mouton, TheHague.M.R.
Brent and J.M.
Siskind.
2001.
The role of expo-sure to isolated words in early vocabulary develop-ment.
Cognition, 81:31?44.E.
Brill.
1997.
Unsupervised learning of disambigua-tion rules for part of speech tagging.
In NaturalLanguage Processing Using Very Large Corpora.Kluwer Academic Press.R.
Brown.
1973.
A First Language.
Harvard Univer-sity Press, Cambridge, MA.X.
Carreras and L. Ma`rquez.
2004.
Introduction tothe CoNLL-2004 shared tasks: Semantic role label-ing.
In Proceedings of CoNLL-2004, pages 89?97.Boston, MA, USA.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
NationalConference on Artificial Intelligence.E.V.
Clark.
1978.
Awwareness of language: Some ev-idence from what children say and do.
In R. J. A.Sinclair and W. Levelt, editors, The child?s concep-tion of language.
Springer Verlag, Berlin.M.
Connor, Y. Gertner, C. Fisher, and D. Roth.
2008.Baby srl: Modeling early language acquisition.
InProc.
of the Annual Conference on ComputationalNatural Language Learning (CoNLL), pages xx?yy,Aug.M.
Connor, Y. Gertner, C. Fisher, and D. Roth.2009.
Minimally supervised model of early lan-guage acquisition.
In Proc.
of the Annual Confer-ence on Computational Natural Language Learning(CoNLL), Jun.M.
Demetras, K. Post, and C. Snow.
1986.
Feedbackto first-language learners.
Journal of Child Lan-guage, 13:275?292.K.
Demuth, J. Culbertson, and J.
Alter.
2006.
Word-minimality, epenthesis, and coda licensing in the ac-quisition of english.
Language & Speech, 49:137?174.C.
Fisher.
1996.
Structural limits on verb mapping:The role of analogy in children?s interpretation ofsentences.
Cognitive Psychology, 31:41?81.Jianfeng Gao and Mark Johnson.
2008.
A compar-ison of bayesian estimators for unsupervised hid-den markov model pos taggers.
In Proceedings ofEMNLP-2008, pages 344?352.D.
Gentner.
2006.
Why verbs are hard to learn.
InK.
Hirsh-Pasek and R. Golinkoff, editors, Actionmeets word: How children learn verbs, pages 544?564.
Oxford University Press.Y.
Gertner and C. Fisher.
2006.
Predicted errors inearly verb learning.
In 31st Annual Boston Univer-sity Conference on Language Development.997Y.
Gertner, C. Fisher, and J. Eisengart.
2006.
Learningwords and rules: Abstract knowledge of word or-der in early sentence comprehension.
PsychologicalScience, 17:684?691.J.
Gillette, H. Gleitman, L. R. Gleitman, and A. Led-erer.
1999.
Human simulations of vocabulary learn-ing.
Cognition, 73:135?176.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proceedings of 45th Annual Meeting ofthe Association of Computational Linguists, pages744?751.R.
Gomez and L. Gerken.
1999.
Artificial grammarlearning by 1-year-olds leads to specific and abstractknowledge.
Cognition, 70:109?135.A.
Haghighi and D. Klein.
2006.
Prototype-drivelearning for sequence models.
In Proceedings ofNAACL-2006, pages 320?327.Mark Johnson.
2007.
Why doesnt em find good hmmpos-taggers?
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 296?305.M.H.
Kelly.
1992.
Using sound to solve syntac-tic problems: The role of phonology in grammat-ical category assignments.
Psychological Review,99:349?364.J.
Lidz, H. Gleitman, and L. R. Gleitman.
2003.
Un-derstanding how input matters: verb learning and thefootprint of universal grammar.
Cognition, 87:151?178.B.
MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Third Edition.
Lawrence Elr-baum Associates, Mahwah, NJ.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330, June.Marina Meila?.
2002.
Comparing clusterings.
Techni-cal Report 418, University of Washington StatisticsDepartment.T.
Mintz.
2003.
Frequent frames as a cue for grammat-ical categories in child directed speech.
Cognition,90:91?117.P.
Monaghan, N. Chater, and M.H.
Christiansen.
2005.The differential role of phonological and distribu-tional cues in grammatical categorisation.
Cogni-tion, 96:143?182.S.
Pinker.
1984.
Language learnability and languagedevelopment.
Harvard University Press, Cambridge,MA.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The im-portance of syntactic parsing and inference in se-mantic role labeling.
Computational Linguistics,34(2).L.
R. Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?285.Sujith Ravi and Kevin Knight.
2009.
Minimizedmodels for unsupervised part-of-speech tagging.
InProceedings of the Joint Conferenceof the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP).J.R.
Saffran, R.N.
Aslin, and E.L. Newport.
1996.
Sta-tistical learning by 8-month-old infants.
Science,274:1926?1928.Rushen Shi, James L. Morgan, and Paul Allopenna.1998.
Phonological and acoustic bases for ear-liest grammatical category assignment: a cross-linguistic perspective.
Journal of Child Language,25(01):169?201.Rushen Shi, Janet F. Werker, and James L. Morgan.1999.
Newborn infants?
sensitivity to perceptualcues to lexical and grammatical words.
Cognition,72(2):B11 ?
B21.L.B.
Smith and C. Yu.
2008.
Infants rapidly learnword-referent mappings via cross-situational statis-tics.
Cognition, 106:1558?1568.Kiristina Toutanova and Mark Johnson.
2007.
Abayesian lda-based model for semi-supervised part-of-speech tagging.
In Proceedings of NIPS.S.
Yuan and C. Fisher.
2009.
?really?
she blickedthe baby??
: Two-year-olds learn combinatorial factsabout verbs by listening.
Psychological Science,20:619?626.S.
Yuan, C. Fisher, Y. Gertner, and J. Snedeker.
2007.Participants are more than physical bodies: 21-month-olds assign relational meaning to novel tran-sitive verbs.
In Biennial Meeting of the Society forResearch in Child Development, Boston, MA.998
