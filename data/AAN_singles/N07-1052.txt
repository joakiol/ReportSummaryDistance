Proceedings of NAACL HLT 2007, pages 412?419,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsApproximate Factoring for A?
SearchAria Haghighi, John DeNero, Dan KleinComputer Science DivisionUniversity of California Berkeley{aria42, denero, klein}@cs.berkeley.eduAbstractWe present a novel method for creating A?
esti-mates for structured search problems.
In our ap-proach, we project a complex model onto multiplesimpler models for which exact inference is effi-cient.
We use an optimization framework to es-timate parameters for these projections in a waywhich bounds the true costs.
Similar to Klein andManning (2003), we then combine completion es-timates from the simpler models to guide searchin the original complex model.
We apply our ap-proach to bitext parsing and lexicalized parsing,demonstrating its effectiveness in these domains.1 IntroductionInference tasks in NLP often involve searching foran optimal output from a large set of structured out-puts.
For many complex models, selecting the high-est scoring output for a given observation is slow oreven intractable.
One general technique to increaseefficiency while preserving optimality is A?
search(Hart et al, 1968); however, successfully using A?search is challenging in practice.
The design of ad-missible (or nearly admissible) heuristics which areboth effective (close to actual completion costs) andalso efficient to compute is a difficult, open prob-lem in most domains.
As a result, most work onsearch has focused on non-optimal methods, suchas beam search or pruning based on approximatemodels (Collins, 1999), though in certain cases ad-missible heuristics are known (Och and Ney, 2000;Zhang and Gildea, 2006).
For example, Klein andManning (2003) show a class of projection-based A?estimates, but their application is limited to modelswhich have a very restrictive kind of score decom-position.
In this work, we broaden their projection-based technique to give A?
estimates for modelswhich do not factor in this restricted way.Like Klein and Manning (2003), we focus onsearch problems where there are multiple projec-tions or ?views?
of the structure, for example lexicalparsing, in which trees can be projected onto eithertheir CFG backbone or their lexical attachments.
Weuse general optimization techniques (Boyd and Van-denberghe, 2005) to approximately factor a modelover these projections.
Solutions to the projectedproblems yield heuristics for the original model.This approach is flexible, providing either admissi-ble or nearly admissible heuristics, depending on thedetails of the optimization problem solved.
Further-more, our approach allows a modeler explicit controlover the trade-off between the tightness of a heuris-tic and its degree of inadmissibility (if any).
We de-scribe our technique in general and then apply it totwo concrete NLP search tasks: bitext parsing andlexicalized monolingual parsing.2 General ApproachMany inference problems in NLP can be solvedwith agenda-based methods, in which we incremen-tally build hypotheses for larger items by combiningsmaller ones with some local configurational struc-ture.
We can formalize such tasks as graph searchproblems, where states encapsulate partial hypothe-ses and edges combine or extend them locally.1 Forexample, in HMM decoding, the states are anchoredlabels, e.g.
VBD[5], and edges correspond to hiddentransitions, e.g.
VBD[5] ?
DT[6].The search problem is to find a minimal cost pathfrom the start state to a goal state, where the pathcost is the sum of the costs of the edges in the path.1In most complex tasks, we will in fact have a hypergraph,but the extension is trivial and not worth the added notation.412( aa?)?
( bb?
)( ba?)?
( cb?
)1( aa?)?
( bb?
)( bb?)?
( cc?
)1( aa?)?
( bb?
)( ab?)?
( bc?
)1( aa?)?
( bb?
)( ab?)?
( bc?
)1Local Configurationsa' ?
b' b'  c'a ?
bb ?
c3.0 4.03.02.02.01.02.01.0Factored Cost MatrixOriginal Cost Matrix3.0 4.03.02.0a ?
bb ?
ca' ?
b' b' ?
c'c(a ?
b)c(b ?
c)c(a' ?
b')c(a' ?
b')3.0 4.03.02.02.01.02.01.0Factored Cost MatrixOriginal Cost Matrix3.0 5.03.02.0a ?
bb ?
ca' ?
b' b' ?
c'c(a ?
b)c(b ?
c)c(a' ?
b')c(a' ?
b')(a) (b) (c)Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row andcolumn of the cell).
In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration.
Thebottom matrix represents our factored estimates, where each entry is the sum of configuration projections.
For this example, theactual cost matrix can be decomposed exactly into two projections.
In (c), the top cost matrix cannot be exactly decomposed alongtwo dimensions.
Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost.Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.For probabilistic inference problems, the cost of anedge is typically a negative log probability which de-pends only on some local configuration type.
Forinstance, in PCFG parsing, the (hyper)edges refer-ence anchored spans X[i, j], but the edge costs de-pend only on the local rule type X ?
Y Z.
We willuse a to refer to a local configuration and use c(a)to refer to its cost.
Because edge costs are sensi-tive only to local configurations, the cost of a pathis?a c(a).
A?
search requires a heuristic function,which is an estimate h(s) of the completion cost, thecost of a best path from state s to a goal.In this work, following Klein and Manning(2003), we consider problems with projections or?views,?
which define mappings to simpler state andconfiguration spaces.
For instance, suppose that weare using an HMM to jointly model part-of-speech(POS) and named-entity-recognition (NER) tagging.There might be one projection onto the NER com-ponent and another onto the POS component.
For-mally, a projection pi is a mapping from states tosome coarser domain.
A state projection inducesprojections of edges and of the entire graph pi(G).We are particularly interested in search problemswith multiple projections {pi1, .
.
.
, pi`} where eachprojection, pii, has the following properties: its stateprojections induce well-defined projections of thelocal configurations pii(a) used for scoring, and theprojected search problem admits a simpler infer-ence.
For instance, the POS projection in our NER-POS HMM is a simpler HMM, though the gainsfrom this method are greater when inference in theprojections have lower asymptotic complexity thanthe original problem (see sections 3 and 4).In defining projections, we have not yet dealt withthe projected scoring function.
Suppose that thecost of local configurations decomposes along pro-jections as well.
In this case,c (a) =?`i=1ci(a) , ?a ?
A (1)where A is the set of local configurations and ci(a)represents the cost of configuration a under projec-tion pii.
A toy example of such a cost decomposi-tion in the context of a Markov process over two-partstates is shown in figure 1(b), where the costs of thejoint transitions equal the sum of costs of their pro-jections.
Under the strong assumption of equation(1), Klein and Manning (2003) give an admissibleA?
bound.
They note that the cost of a path decom-poses as a sum of projected path costs.
Hence, thefollowing is an admissible additive heuristic (Felneret al, 2004),h(s) =?`i=1h?i (s) (2)where h?i (s) denote the optimal completion costs inthe projected search graph pii(G).
That is, the com-pletion cost of a state bounds the sum of the comple-tion costs in each projection.In virtually all cases, however, configuration costswill not decompose over projections, nor would weexpect them to.
For instance, in our joint POS-NERtask, this assumption requires that the POS and NER413transitions and observations be generated indepen-dently.
This independence assumption underminesthe motivation for assuming a joint model.
In thecentral contribution of this work, we exploit the pro-jection structure of our search problem without mak-ing any assumption about cost decomposition.Rather than assuming decomposition, we proposeto find scores ?
for the projected configurationswhich are pointwise admissible:?`i=1?i(a) ?
c(a), ?a ?
A (3)Here, ?i(a) represents a factored projection cost ofpii(a), the pii projection of configuration a. Givenpointwise admissible ?i?s we can again apply theheuristic recipe of equation (2).
An example offactored projection costs are shown in figure 1(c),where no exact decomposition exists, but a point-wise admissible lower bound is easy to find.Claim.
If a set of factored projection costs{?1, .
.
.
, ?`} satisfy pointwise admissibility, thenthe heuristic from (2) is an admissible A?
heuristic.Proof.
Assume a1, .
.
.
, ak are configurations usedto optimally reach the goal from state s.
Then,h?
(s) =kXj=1c(aj) ?kXj=1X`i=1?i(aj)=X`i=1kXj=1?i(aj)!
?X`i=1h?i (s) = h(s)The first inequality follows from pointwise admis-sibility.
The second inequality follows because eachinner sum is a completion cost for projected problempii and therefore h?i (s) lower bounds it.
Intuitively,we can see two sources of slack in such projectionheuristics.
First, there may be slack in the pointwiseadmissible scores.
Second, the best paths in the pro-jections will be overly optimistic because they havebeen decoupled (see figure 5 for an example of de-coupled best paths in projections).2.1 Finding Factored Projections forNon-Factored CostsWe can find factored costs ?i(a) which are point-wise admissible by solving an optimization problem.We think of our unknown factored costs as a blockvector ?
= [?1, .., ?`], where vector ?i is composedof the factored costs, ?i(a), for each configurationa ?
A.
We can then find admissible factored costsby solving the following optimization problem,minimize????
(4)such that, ?a = c(a)?
?`i=1?i(a), ?a ?
A?a ?
0, ?a ?
AWe can think of each ?a as the amount by whichthe cost of configuration a exceeds the factored pro-jection estimates (the pointwise A?
gap).
Requiring?a ?
0 insures pointwise admissibility.
Minimiz-ing the norm of the ?a variables encourages tighterbounds; indeed if ???
= 0, the solution correspondsto an exact factoring of the search problem.
In thecase where we minimize the 1-norm or ?-norm, theproblem above reduces to a linear program, whichcan be solved efficiently for a large number of vari-ables and constraints.2Viewing our procedure decision-theoretically, byminimizing the norm of the pointwise gaps we areeffectively choosing a loss function which decom-poses along configuration types and takes the formof the norm (i.e.
linear or squared losses).
A com-plete investigation of the alternatives is beyond thescope of this work, but it is worth pointing out thatin the end we will care only about the gap on entirestructures, not configurations, and individual config-uration factored costs need not even be pointwise ad-missible for the overall heuristic to be admissible.Notice that the number of constraints is |A|, thenumber of possible local configurations.
For manysearch problems, enumerating the possible configu-rations is not feasible, and therefore neither is solv-ing an optimization problem with all of these con-straints.
We deal with this situation in applying ourtechnique to lexicalized parsing models (section 4).Sometimes, we might be willing to trade searchoptimality for efficiency.
In our approach, we canexplicitly make this trade-off by designing an alter-native optimization problem which allows for slack2We used the MOSEK package (Andersen and Andersen,2000).414in the admissibility constraints.
We solve the follow-ing soft version of problem (4):minimize??
?+?+ C????
(5)such that, ?a = c(a)?
?`i=1?i(a), ?a ?
Awhere ?+ = max{0, ?}
and ??
= max{0,??
}represent the componentwise positive and negativeelements of ?
respectively.
Each ?
?a > 0 representsa configuration where our factored projection esti-mate is not pointwise admissible.
Since this situa-tion may result in our heuristic becoming inadmis-sible if used in the projected completion costs, wemore heavily penalize overestimating the cost by theconstant C.2.2 Bounding Search ErrorIn the case where we allow pointwise inadmissibil-ity, i.e.
variables ?
?a , we can bound our search er-ror.
Suppose ?
?max = maxa?A ?
?a and that L?
isthe length of the longest optimal solution for theoriginal problem.
Then, h(s) ?
h?
(s) + L??
?max,?s ?
S. This ?-admissible heuristic (Ghallab andAllard, 1982) bounds our search error by L??
?max.33 Bitext ParsingIn bitext parsing, one jointly infers a synchronousphrase structure tree over a sentence ws and itstranslation wt (Melamed et al, 2004; Wu, 1997).Bitext parsing is a natural candidate task for ourapproximate factoring technique.
A synchronoustree projects monolingual phrase structure trees ontoeach sentence.
However, the costs assigned bya weighted synchronous grammar (WSG) G donot typically factor into independent monolingualWCFGs.
We can, however, produce a useful surro-gate: a pair of monolingual WCFGs with structuresprojected by G and weights that, when combined,underestimate the costs of G.Parsing optimally relative to a synchronous gram-mar using a dynamic program requires time O(n6)in the length of the sentence (Wu, 1997).
This highdegree of complexity makes exhaustive bitext pars-ing infeasible for all but the shortest sentences.
In3This bound may be very loose if L is large.contrast, monolingual CFG parsing requires timeO(n3) in the length of the sentence.3.1 A?
ParsingAlternatively, we can search for an optimal parseguided by a heuristic.
The states in A?
bitext pars-ing are rooted bispans, denoted X [i, j] :: Y [k, l].States represent a joint parse over subspans [i, j] ofws and [k, l] of wt rooted by the nonterminals X andY respectively.Given a WSG G, the algorithm prioritizes a state(or edge) e by the sum of its inside cost ?G(e) (thenegative log of its inside probability) and its outsideestimate h(e), or completion cost.4 We are guaran-teed the optimal parse if our heuristic h(e) is nevergreater than ?G(e), the true outside cost of e.We now consider a heuristic combining the com-pletion costs of the monolingual projections of G,and guarantee admissibility by enforcing point-wiseadmissibility.
Each state e = X [i, j] :: Y [k, l]projects a pair of monolingual rooted spans.
Theheuristic we propose sums independent outside costsof these spans in each monolingual projection.h(e) = ?s(X [i, j]) + ?t(Y [k, l])These monolingual outside scores are computed rel-ative to a pair of monolingual WCFG grammars Gsand Gt given by splitting each synchronous ruler =(X(s)Y(t))?(?
??
?
)into its components pis(r) = X?
??
and pit(r) =Y???
and weighting them via optimized ?s(r) and?t(r), respectively.5To learn pointwise admissible costs for the mono-lingual grammars, we formulate the following opti-mization problem:6minimize?,?s,?t??
?1such that, ?r = c(r)?
[?s(r) + ?t(r)]for all synchronous rules r ?
G?s ?
0, ?t ?
0, ?
?
04All inside and outside costs are Viterbi, not summed.5Note that we need only parse each sentence (monolin-gually) once to compute the outside probabilities for every span.6The stated objective is merely one reasonable choiceamong many possibilities which require pointwise admissibil-ity and encourage tight estimates.415ijklSourceTargetijklSourceTargetijklSourceTarget?
?Cost under Gt Cost under GSynchronized completionscored by original modelSynchronized completionscored by factored modelMonolingual completionsscored by factored modelCost under GsFigure 2: The gap between the heuristic (left) and true comple-tion cost (right) comes from relaxing the synchronized problemto independent subproblems and slack in the factored models.Figure 2 diagrams the two bounds that enforce theadmissibility of h(e).
For any outside cost ?G(e),there is a corresponding optimal completion struc-ture o under G, which is an outer shell of a syn-chronous tree.
o projects monolingual completionsos and ot which have well-defined costs cs(os) andct(ot) under Gs and Gt respectively.
Their sumcs(os) + ct(ot) will underestimate ?G(e) by point-wise admissibility.Furthermore, the heuristic we compute underesti-mates this sum.
Recall that the monolingual outsidescore ?s(X [i, j]) is the minimal costs for any com-pletion of the edge.
Hence, ?s(X [i, j]) ?
cs(os)and ?t(X [k, l]) ?
ct(ot).
Admissibility follows.3.2 ExperimentsWe demonstrate our technique using the syn-chronous grammar formalism of tree-to-tree trans-ducers (Knight and Graehl, 2004).
In each weightedrule, an aligned pair of nonterminals generates twoordered lists of children.
The non-terminals in eachlist must align one-to-one to the non-terminals in theother, while the terminals are placed freely on eitherside.
Figure 3(a) shows an example rule.Following Galley et al (2004), we learn a gram-mar by projecting English syntax onto a foreign lan-guage via word-level alignments, as in figure 3(b).7We parsed 1200 English-Spanish sentences usinga grammar learned from 40,000 sentence pairs ofthe English-Spanish Europarl corpus.8 Figure 4(a)shows that A?
expands substantially fewer stateswhile searching for the optimal parse with our op-7The bilingual corpus consists of translation pairs with fixedEnglish parses and word alignments.
Rules were scored by theirrelative frequencies.8Rare words were replaced with their parts of speech to limitthe memory consumption of the parser.(a)?NP(s)NP(t)?
?NN(s)1 NNS(s)2NNS(t)2 de NN(t)1!
(b)TranslationsystemssometimesworksistemastraduccionfuncionanavecesdeNNSNNNPNNSNNNPRB VBSFigure 3: (a) A tree-to-tree transducer rule.
(b) An exampletraining sentence pair that yields rule (a).timization heuristic.
The exhaustive curve showsedge expansions using the null heuristic.
The in-termediate result, labeled English only, used onlythe English monolingual outside score as a heuris-tic.
Similar results using only Spanish demonstratethat both projections contribute to parsing efficiency.All three curves in figure 4 represent running timesfor finding the optimal parse.Zhang and Gildea (2006) offer a different heuris-tic for A?
parsing of ITG grammars that provides aforward estimate of the cost of aligning the unparsedwords in both sentences.
We cannot directly applythis technique to our grammar because tree-to-treetransducers only align non-terminals.
Instead, wecan augment our synchronous grammar model to in-clude a lexical alignment component, then employboth heuristics.
We learned the following two-stagegenerative model: a tree-to-tree transducer generatestrees whose leaves are parts of speech.
Then, thewords of each sentence are generated, either jointlyfrom aligned parts of speech or independently givena null alignment.
The cost of a complete parse un-der this new model decomposes into the cost of thesynchronous tree over parts of speech and the costof generating the lexical items.Given such a model, both our optimization heuris-tic and the lexical heuristic of Zhang and Gildea(2006) can be computed independently.
Crucially,the sum of these heuristics is still admissible.
Re-sults appear in figure 4(b).
Both heuristics (lexi-cal and optimization) alone improve parsing perfor-mance, but their sum opt+lex substantially improvesupon either one.416(a) 0501001502005 7 9 11 13 15Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveLexicalOptimizationOpt+Lex0501001502005 7 9 11 13 15Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveEnglish OnlyOptimization(b) 0501001502005 7 9 1 13 15Sent ce lengthAvg.
EdgesPopped(in thousands) ExhaustiveLexicalOptimizationOpt+Lex0501001502005 7 9 1 13 15Sent ce lengthAvg.
EdgesPopped(in thousands) ExhaustiveEnglish OnlyOptimizationFigure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efficiency.4 Lexicalized ParsingWe next apply our technique to lexicalized pars-ing (Charniak, 1997; Collins, 1999).
In lexical-ized parsing, the local configurations are lexicalizedrules of the form X[h, t] ?
Y [h?, t?]
Z[h, t], whereh, t, h?, and t?
are the head word, head tag, ar-gument word, and argument tag, respectively.
Wewill use r = X ?
Y Z to refer to the CFG back-bone of a lexicalized rule.
As in Klein and Man-ning (2003), we view each lexicalized rule, `, ashaving a CFG projection, pic(`) = r, and a de-pendency projection, pid(`) = (h, t, h?, t?
)(see fig-ure 5).9 Broadly, the CFG projection encodes con-stituency structure, while the dependency projectionencodes lexical selection, and both projections areasymptotically more efficient than the original prob-lem.
Klein and Manning (2003) present a factoredmodel where the CFG and dependency projectionsare generated independently (though with compati-ble bracketing):P (Y [h, t]Z[h?, t?]
| X[h, t]) = (6)P (Y Z|X)P (h?, t?|t, h)In this work, we explore the following non-factoredmodel, which allows correlations between the CFGand dependency projections:P (Y [h, t]Z[h?, t?]
| X[h, t]) = P (Y Z|X, t, h) (7)P (t?|t, Z, h?, h) P (h?|t?, t, Z, h?, h)This model is broadly representative of the suc-cessful lexicalized models of Charniak (1997) and9We assume information about the distance and direction ofthe dependency is encoded in the dependency tuple, but we omitit from the notation for compactness.Collins (1999), though simpler.104.1 Choosing Constraints and HandlingUnseen DependenciesIdeally we would like to be able to solve the op-timization problem in (4) for this task.
Unfortu-nately, exhaustively listing all possible configura-tions (lexical rules) yields an impractical number ofconstraints.
We therefore solve a relaxed problem inwhich we enforce the constraints for only a subsetof the possible configurations, A?
?
A.
Once westart dropping constraints, we can no longer guaran-tee pointwise admissibility, and therefore there is noreason not to also allow penalized violations of theconstraints we do list, so we solve (5) instead.To generate the set of enforced constraints, wefirst include all configurations observed in the goldtraining trees.
We then sample novel configurationsby choosing (X,h, t) from the training distributionand then using the model to generate the rest of theconfiguration.
In our experiments, we ended up with434,329 observed configurations, and sampled thesame number of novel configurations.
Our penaltymultiplier C was 10.Even if we supplement our training set with manysample configurations, we will still see new pro-jected dependency configurations at test time.
It istherefore necessary to generalize scores from train-ing configurations to unseen ones.
We enrich ourprocedure by expressing the projected configurationcosts as linear functions of features.
Specifically, wedefine feature vectors fc(r) and fd(h, t, h?t?)
overthe CFG and dependency projections, and intro-10All probability distributions for the non-factored model areestimated by Witten-Bell smoothing (Witten and Bell, 1991)where conditioning lexical items are backed off first.417SXXXXXXNPSaaa!!
!NPNPDTThesePPNPHHHNNSstocksNPPPRBeventuallyVPSVBDreopenedreopened-VBDhhhhhhhh""((((((((These-DTThesestocks-NNSstocksreopened-VBDPPPPeventually-RBeventuallyreopened-VBDreopenedS, reopened-VBDhhhhhhhhhh((((((((((NPS , stocks-NNSbb""DTTheseNNSstocksADVPS , eventually-RBRBeventuallyVPS , reopened-VBDVBDreopenedActual Cost: 18.7Best Projected CFG Cost: 4.1 Best Projected Dep.
Cost: 9.5 CFG Projection Cost : 6.9Dep.
Projection Cost: 11.1(a) (b) (c)Figure 5: Lexicalized parsing projections.
The figure in (a) is the optimal CFG projection solution and the figure in (b) is theoptimal dependency projection solution.
The tree in (c) is the optimal solution for the original problem.
Note that the sum of theCFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.duce corresponding weight vectors wc and wd.
Theweight vectors are learned by solving the followingoptimization problem:minimize?,wc,wd?
?+?2 + C???
?2 (8)such that, wc ?
0, wd ?
0?` = c(`)?
[wTc fc(r) + wTd fd(h, t, h?, t?
)]for ` = (r, h, t, h?, t?)
?
A?Our CFG feature vector has only indicator featuresfor the specific rule.
However, our dependency fea-ture vector consists of an indicator feature of the tu-ple (h, t, h?, t?)
(including direction), an indicator ofthe part-of-speech type (t, t?)
(also including direc-tion), as well as a bias feature.4.2 Experimental ResultsWe tested our approximate projection heuristic ontwo lexicalized parsing models.
The first is the fac-tored model of Klein and Manning (2003), givenby equation (6), and the second is the non-factoredmodel described in equation (7).
Both modelsuse the same parent-annotated head-binarized CFGbackbone and a basic dependency projection whichmodels direction, but not distance or valence.11In each case, we compared A?
using our approxi-mate projection heuristics to exhaustive search.
Wemeasure efficiency in terms of the number of ex-panded hypotheses (edges popped); see figure 6.12In both settings, the factored A?
approach substan-tially outperforms exhaustive search.
For the fac-11The CFG and dependency projections correspond to thePCFG-PA and DEP-BASIC settings in Klein and Manning(2003).12All models are trained on section 2 through 21 of the En-glish Penn treebank, and tested on section 23.tored model of Klein and Manning (2003), we canalso compare our reconstructed bound to the knowntight bound which would result from solving thepointwise admissible problem in (4) with all con-straints.
As figure 6 shows, the exact factoredheuristic does outperform our approximate factoredheuristic, primarily because of many looser, backed-off cost estimates for unseen dependency tuples.
Forthe non-factored model, we compared our approxi-mate factored heuristic to one which only bounds theCFG projection as suggested by Klein and Manning(2003).
They suggest,?c(r) = min`?A:pic(`)=rc(`)where we obtain factored CFG costs by minimizingover dependency projections.
As figure 6 illustrates,this CFG only heuristic is substantially less efficientthan our heuristic which bounds both projections.Since our heuristic is no longer guaranteed to beadmissible, we evaluated its effect on search in sev-eral ways.
The first is to check for search errors,where the model-optimal parse is not found.
In thecase of the factored model, we can find the optimalparse using the exact factored heuristic and compareit to the parse found by our learned heuristic.
In ourtest set, the approximate projection heuristic failedto return the model optimal parse in less than 1% ofsentences.
Of these search errors, none of the costswere more than 0.1% greater than the model optimalcost in negative log-likelihood.
For the non-factoredmodel, the model optimal parse is known only forshorter sentences which can be parsed exhaustively.For these sentences up to length 15, there were nosearch errors.
We can also check for violations ofpointwise admissibility for configurations encoun-418(a)0501001502005 10 15 20 25 30 35 40Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveCFG OnlyApprox.
Factored0501001502005 10 15 20 25 30 35 40Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveApprox.
FactoredExact Factored(b)0501001502005 10 15 20 25 30 35 40Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveCFG OnlyApprox.
Factored0501001502005 10 15 20 25 30 35 40Sentence lengthAvg.
EdgesPopped(in thousands) ExhaustiveApprox.
FactoredExact FactoredFigure 6: Edges popped by exhaustive versus factored A?
search.
The chart in (a) is using the factored lexicalized model fromKlein and Manning (2003).
The chart in (b) is using the non-factored lexicalized model described in section 4.tered during search.
For both the factored and non-factored model, less than 2% of the configurationsscored by the approximate projection heuristic dur-ing search violated pointwise admissibility.While this is a paper about inference, we alsomeasured the accuracy in the standard way, on sen-tences of length up to 40, using EVALB.
The fac-tored model with the approximate projection heuris-tic achieves an F1 of 82.2, matching the performancewith the exact factored heuristic, though slower.
Thenon-factored model, using the approximate projec-tion heuristic, achieves an F1 of 83.8 on the test set,which is slightly better than the factored model.13We note that the CFG and dependency projectionsare as similar as possible across models, so the in-crease in accuracy is likely due in part to the non-factored model?s coupling of CFG and dependencyprojections.5 ConclusionWe have presented a technique for creating A?
es-timates for inference in complex models.
Our tech-nique can be used to generate provably admissibleestimates when all search transitions can be enumer-ated, and an effective heuristic even for problemswhere all transitions cannot be efficiently enumer-ated.
In the future, we plan to investigate alterna-tive objective functions and error-driven methods forlearning heuristic bounds.Acknowledgments We would like to thank theanonymous reviewers for their comments.
Thiswork is supported by a DHS fellowship to the first13Since we cannot exhaustively parse with this model, wecannot compare our F1 to an exact search method.author and a Microsoft new faculty fellowship to thethird author.ReferencesE.
D. Andersen and K. D. Andersen.
2000.
The MOSEK in-terior point optimizer for linear programming.
In H. Frenket al, editor, High Performance Optimization.
Kluwer Aca-demic Publishers.Stephen Boyd and Lieven Vandenberghe.
2005.
Convex Opti-mization.
Cambridge University Press.Eugene Charniak.
1997.
Statistical parsing with a context-freegrammar and word statistics.
In National Conference on Ar-tificial Intelligence.Michael Collins.
1999.
Head-driven statistical models for nat-ural language parsing.Ariel Felner, Richard Korf, and Sarit Hanan.
2004.
Additivepattern database heuristics.
JAIR.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.Malik Ghallab and Dennis G. Allard.
1982.
A??
- an efficientnear admissible heuristic search algorithm.
In IJCAI.P.
Hart, N. Nilsson, and B. Raphael.
1968.
A formal basis forthe heuristic determination of minimum cost paths.
In IEEETransactions on Systems Science and Cybernetics.
IEEE.Dan Klein and Christopher D. Manning.
2003.
Factored A*search for models over sequences and trees.
In IJCAI.Kevin Knight and Jonathan Graehl.
2004.
Training tree trans-ducers.
In HLT-NAACL.I.
Dan Melamed, Giorgio Satta, and Ben Wellington.
2004.Generalized multitext grammars.
In ACL.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In ACL.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel events inadaptive text compression.
IEEE.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
Comput.
Linguist.Hao Zhang and Daniel Gildea.
2006.
Efficient search for inver-sion transduction grammar.
In EMNLP.419
