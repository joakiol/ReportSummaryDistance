Generating Referring Expressions in Open DomainsAdvaith Siddharthan Ann CopestakeComputer Science Department Computer LaboratoryColumbia University University of Cambridgeas372@cs.columbia.edu aac10@cl.cam.ac.ukAbstractWe present an algorithm for generating referringexpressions in open domains.
Existing algorithmswork at the semantic level and assume the avail-ability of a classification for attributes, which isonly feasible for restricted domains.
Our alterna-tive works at the realisation level, relies on Word-Net synonym and antonym sets, and gives equiva-lent results on the examples cited in the literatureand improved results for examples that prior ap-proaches cannot handle.
We believe that ours isalso the first algorithm that allows for the incremen-tal incorporation of relations.
We present a novelcorpus-evaluation using referring expressions fromthe Penn Wall Street Journal Treebank.1 IntroductionReferring expression generation has historicallybeen treated as a part of the wider issue of gener-ating text from an underlying semantic representa-tion.
The task has therefore traditionally been ap-proached at the semantic level.
Entities in the realworld are logically represented; for example (ignor-ing quantifiers), a big brown dog might be repre-sented as big1(x) ?
brown1(x) ?
dog1(x), wherethe predicates big1, brown1 and dog1 represent dif-ferent attributes of the variable (entity) x.
The taskof referring expression generation has traditionallybeen framed as the identification of the shortest log-ical description for the referent entity that differen-tiates it from all other entities in the discourse do-main.
For example, if there were a small brown dog(small1(x) ?
brown1(x) ?
dog1(x)) in context, theminimal description for the big brown dog would bebig1(x) ?
dog1(x)1.This semantic framework makes it difficult to ap-ply existing referring expression generation algo-rithms to the many regeneration tasks that are im-portant today; for example, summarisation, open-ended question answering and text simplification.Unlike in traditional generation, the starting point in1The predicate dog1 is selected because it has a distin-guished status, referred to as type in Reiter and Dale (1992).One such predicate has to to be present in the description.these tasks is unrestricted text, rather than a seman-tic representation of a small domain.
It is difficultto extract the required semantics from unrestrictedtext (this task would require sense disambiguation,among other issues) and even harder to constructa classification for the extracted predicates in themanner that existing approaches require (cf., ?2).In this paper, we present an algorithm for generat-ing referring expressions in open domains.
We dis-cuss the literature and detail the problems in apply-ing existing approaches to reference generation toopen domains in ?2.
We then present our approachin ?3, contrasting it with existing approaches.
Weextend our approach to handle relations in ?3.3 andpresent a novel corpus-based evaluation on the PennWSJ Treebank in ?4.2 Overview of Prior ApproachesThe incremental algorithm (Reiter and Dale, 1992)is the most widely discussed attribute selectionalgorithm.
It takes as input the intended refer-ent and a contrast set of distractors (other enti-ties that could be confused with the intended refer-ent).
Entities are represented as attribute value ma-trices (AVMs).
The algorithm also takes as inputa *preferred-attributes* list that contains, in orderof preference, the attributes that human writers useto reference objects.
For example, the preferencemight be {colour, size, shape...}.
The algorithmthen repeatedly selects attributes from *preferred-attributes* that rule out at least one entity in thecontrast set until all distractors have been ruled out.It is instructive to look at how the incremental al-gorithm works.
Consider an example where a largebrown dog needs to be referred to.
The contrast setcontains a large black dog.
These are representedby the AVMs shown below.?
?type dogsize largecolour brown???
?type dogsize largecolour black?
?Assuming that the *preferred-attributes* list is[size, colour, ...], the algorithm would first com-pare the values of the size attribute (both large),disregard that attribute as not being discriminating,compare the values of the colour attribute and re-turn the brown dog.Subsequent work on referring expression genera-tion has expanded the logical framework to allowreference by negation (the dog that is not black)and references to multiple entities (the brown orblack dogs) (van Deemter, 2002), explored differentsearch algorithms for finding the minimal descrip-tion (e.g., Horacek (2003)) and offered differentrepresentation frameworks like graph theory (Krah-mer et al, 2003) as alternatives to AVMs.
However,all these approaches are based on very similar for-malisations of the problem, and all make the follow-ing assumptions:1.
A semantic representation exists.2.
A classification scheme for attributes exists.3.
The linguistic realisations are unambiguous.4.
Attributes cannot be reference modifying.All these assumptions are violated when we movefrom generation in a very restricted domain to re-generation in an open domain.
In regenerationtasks such as summarisation, open-ended questionanswering and text simplification, AVMs for enti-ties are typically constructed from noun phrases,with the head noun as the type and pre-modifiersas attributes.
Converting words into semantic la-bels would involve sense disambiguation, addingto the cost and complexity of the analysis module.Also, attribute classification is a hard problem andthere is no existing classification scheme that can beused for open domains like newswire; for example,WordNet (Miller et al, 1993) organises adjectivesas concepts that are related by the non-hierarchicalrelations of synonymy and antonymy (unlike nounsthat are related through hierarchical links such ashyponymy, hypernymy and metonymy).
In addi-tion, selecting attributes at the semantic level isrisky because their linguistic realisation might beambiguous and many common adjectives are pol-ysemous (cf., example 1 in ?3.1).
Reference modi-fication, which has not been considered in the refer-ring expression generation literature, raises furtherissues; for example, referring to an alleged mur-derer as the murderer is potentially libellous.In addition to the above, there is the issue of over-lap between values of attributes.
The case of sub-sumption (for example, that the colour red sub-sumes crimson and the type dog subsumes chi-huahua) has received formal treatment in the liter-ature; Dale and Reiter (1995) provide a find-best-value function that evaluates tree-like hierarchiesof values.
As mentioned earlier, such hierarchi-cal knowledge bases do not exist for open domains.Further, a treatment of subsumption is insufficient,and degrees of intersection between attribute valuesalso require consideration.
van Deemter (2000) dis-cusses the generation of vague descriptions whenentities have gradable attributes like size; for ex-ample, in a domain with four mice sized 2, 5, 7and 10cm, it is possible to refer to the large mouse(the mouse sized 10cm) or the two small mice (themice sized 2 and 5cm).
However, when applying re-ferring expression generation to regeneration taskswhere the representation of entities is derived fromtext rather than a knowledge base, we have to con-sider the case where the grading of attributes is notexplicit.
For example, we might need to comparethe attribute dark with black, light or white.In contrast to previous approaches, our algorithmworks at the level of words, not semantic labels, andmeasures the relatedness of adjectives (lexicalisedattributes) using the lexical knowledge base Word-Net rather than a semantic classification.
Our ap-proach also addresses the issue of comparing inter-sective attributes that are not explicitly graded, bymaking novel use of the synonymy and antonymylinks in WordNet.
Further, it treats discriminatingpower as only one criteria for selecting attributesand allows for the easy incorporation of other con-siderations such as reference modification (?5).3 The Lexicalised Approach3.1 Quantifying Discriminating PowerWe define the following three quotients.Similarity Quotient (SQ)We define similarity as transitive synonymy.
Theidea is that if X is a synonym of Y and Y is a syn-onym of Z, then X is likely to be similar to Z. Thedegree of similarity between two adjectives dependson how many steps must be made through WordNetsynonymy lists to get from one to the other.Suppose we need to find a referring expressionfor e0.
For each adjective aj describing e0, we cal-culate a similarity quotient SQj by initialising it to0, forming a set of WordNet synonyms S1 of aj ,forming a synonymy set S2 containing all the Word-Net synonyms of all the adjectives in S1 and form-ing S3 from S2 similarly.
Now for each adjectivedescribing any distractor, we increment SQj by 4 ifit is present in S1, by 2 if it is present in S2, and by 1if it is present in S3.
SQj now measures how similaraj is to other adjectives describing distractors.Contrastive Quotient (CQ)Similarly, we define contrastive in terms ofantonymy relationships.
We form the set C1 ofstrict WordNet antonyms of aj .
The set C2 con-sists of strict WordNet antonyms of members of S1and WordNet synonyms of members of C1.
C3 issimilarly constructed from S2 and C2.
We now ini-tialise CQj to zero and for each adjective describingeach distractor, we add w =?
{4, 2, 1} to CQj , de-pending on whether it is a member of C1, C2 or C3.CQj now measures how contrasting aj is to otheradjectives describing distractors.Discriminating Quotient (DQ)An attribute that has a high value of SQ has baddiscriminating power.
An attribute that has a highvalue of CQ has good discriminating power.
Wecan now define the Discriminating Quotient (DQ)as DQ = CQ ?
SQ.
We now have an order (de-creasing DQs) in which to incorporate attributes.This constitutes our *preferred* list.
We illustratethe benefits of our approach with two examples.Example 1: The Importance of LexicalisationPrevious referring expression generation algorithmsignore the issue of realising the logical descriptionfor the referent.
The semantic labels are chosensuch that they have a direct correspondence withtheir linguistic realisation and the realisation is thusconsidered trivial.
Ambiguity and syntacticallyoptional arguments are ignored.
To illustrate oneproblem this causes, consider the two entitiesbelow:e1 e2?
?type presidentage oldtenure current???
?type presidentage youngtenure past?
?If we followed the strict typing system usedby previous algorithms, with *preferred*={age,tenure}, to refer to e1 we would compare theage attributes and rule out e2 and generate theold president.
This expression is ambiguous sinceold can also mean previous.
Models that selectattributes at the semantic level will run into troublewhen their linguistic realisations are ambiguous.In contrast, our algorithm, given flattened attributelists:e1 e2[head presidentattrib old, current][head presidentattrib young, past]successfully picks the current president as currenthas a higher DQ (2) than old (0):attribute distractor CQ SQ DQold e2{young, past} 4 4 0current e2{young, past} 2 0 2In this example, old is a WordNet antonym of youngand a WordNet synonym of past.
Current is aWordNet synonym of present, which is a WordNetantonym of past.
Note that WordNet synonym andantonym links capture the implicit gradation in thelexicalised values of the age and tenure attributes.Example 2: Naive IncrementalityTo illustrate another problem with the original in-cremental algorithm, consider three dogs: e1(a bigblack dog), e2(a small black dog) and e3(a tinywhite dog).Consider using the original incremental algo-rithm to refer to e1 with *preferred*={colour,size}.
The colour attribute black rules out e3.We then we have to select the size attribute big aswell to rule out e2, thus generating the sub-optimalexpression the big black dog.
Here, the use of apredetermined *preferred* list fails to capture whatis obvious from the context: that e1 stands out notbecause it is black, but because it is big.In our approach, for each of e1?s attributes, wecalculate DQ with respect to e2 and e3:attribute distractor CQ SQ DQbig e2{small, black} 4 0 4big e3{tiny, white} 2 0 2black e2{small, black} 1 4 -3black e3{tiny, white} 2 1 1Overall, big has a higher discriminating power(6) than black (-2) and rules out both e2 and e3.We therefore generate the big dog.
Our incremen-tal approach thus manages to select the attribute thatstands out in context.
This is because we constructthe *preferred* list after observing the context.
Wediscuss this issue further in the next section.
Noteagain that WordNet antonym and synonym linkscapture the gradation in the lexicalised size andcolour attributes.
However, this only works wherethe gradation is along one axis; in particular, thisapproach will not work for colours in general, andcannot be used to deduce the relative similarity be-tween yellow and orange as compared to, say, yel-low and blue.3.2 Justifying our AlgorithmThe psycholinguistic justification for the incremen-tal algorithm (IA) hinges on two premises:1.
Humans build referring expressions incrementally.2.
There is a preferred order in which humans selectattributes (e.g., colour>shape>size...).Our algorithm is also incremental.
However, itdeparts significantly from premise 2.
We assumethat speakers pick out attributes that are distinctivein context (cf., example 2, previous section).
Aver-aged over contexts, some attributes have more dis-criminating power than others (largely because ofthe way we visualise entities) and premise 2 is anapproximation to our approach.We now quantify the extra effort we are makingto identify attributes that ?stand out?
in a given con-text.
Let N be the maximum number of entities inthe contrast set and n be the maximum number ofattributes per entity.
The table below compares thecomputational complexity of an optimal algorithm(such as Reiter (1990)), our algorithm and the IA.Incremental Algo Our Algorithm Optimal AlgoO(nN) O(n2N) O(n2N )Both the IA and our algorithm are linear in thenumber of entities N .
This is because neither al-gorithm allows backtracking; an attribute, once se-lected, cannot be discarded.
In contrast, an opti-mal search requires O(2N ) comparisons.
As ouralgorithm compares each attribute of the discoursereferent to every attribute of every distractor, it isquadratic in n. The IA compares each attribute ofthe discourse referent to only one attribute per dis-tractor and is linear in n. Note, however, that valuesfor n of over 4 are rare.3.3 RelationsSemantically, attributes describe an entity (e.g., thesmall grey dog) and relations relate an entity toother entities (e.g., the dog in the bin).
Relationsare troublesome because in relating an entity eo toe1, we need to recursively generate a referring ex-pression for e1.
The IA does not consider relationsand the referring expression is constructed out of at-tributes alone.
The Dale and Haddock (1991) algo-rithm allows for relational descriptions but involvesexponential global search, or a greedy search ap-proximation.
To incorporate relational descriptionsin the incremental framework would require a clas-sification system which somehow takes into accountthe relations themselves and the secondary entitiese1 etc.
This again suggests that the existing algo-rithms force the incrementality at the wrong stagein the generation process.
Our approach computesthe order in which attributes are incorporated afterobserving the context, by quantifying their utilitythrough the quotient DQ.
This makes it easy forus to extend our algorithm to handle relations, be-cause we can compute DQ for relations in muchthe same way as we did for attributes.We illustratethis for prepositions.3.4 Calculating DQ for RelationsSuppose the referent entity eref contains a relation[prepo eo] that we need to calculate the three quo-tients for (cf., figure 1 for representation of rela-tions in AVMs).
We consider each entity ei in thecontrast set for eref in turn.
If ei does not have aprepo relation then the relation is useful and we in-crement CQ by 4.
If ei has a prepo relation thentwo cases arise.
If the object of ei?s prepo rela-tion is eo then we increment SQ by 4.
If it is noteo, the relation is useful and we increment CQ by4.
This is an efficient non-recursive way of com-puting the quotients CQ and SQ for relations.
Wenow discuss how to calculate DQ.
For attributes,we defined DQ = CQ ?
SQ.
However, as the lin-guistic realisation of a relation is a phrase and nota word, we would like to normalise the discriminat-ing power of a relation with the length of its lin-guistic realisation.
Calculating the length involvesrecursively generating referring expressions for theobject of the preposition, an expensive task that wewant to avoid unless we are actually using that rela-tion in the final referring expression.
We thereforeinitially approximate the length as follows.
The re-alisation of a relation [prepo eo] consists of prepo,a determiner and the referring expression for eo.
Ifnone of eref?s distractors have a prepo relation thenwe only require the head noun of eo in the refer-ring expression and length = 3.
In this case, therelation is sufficient to identify both entities; for ex-ample, even if there were multiple bins in figure 1,as long as only one dog is in a bin, the referencethe dog in the bin succeeds in uniquely referencingboth the dog and the bin.
If n distractors of erefcontain a prepo relation with a non-eo object that isdistractor for eo, we set length = 3 + n. This is anestimate for the word length of the realised relationthat assumes one extra attribute for distinguishingeo from each distractor.
Normalisation by estimatedlength is vital; if eo requires a long description, therelations?s DQ should be small so that shorter pos-sibilities are considered first in the incremental pro-cess.
The formula for DQ for relations is thereforeDQ = (CQ ?
SQ)/length.This approach can also be extended to allow forrelations such as comparatives which have syntac-tically optional arguments (e.g., the earlier ight vsthe ight earlier than UA941) which are not allowedfor by approaches which ignore realisation.3.5 The Lexicalised Context-Sensitive IAOur lexicalised context-sensitive incremental algo-rithm (below) generates a referring expression forEntity.
As it recurses, it keeps track of entities it hasused up in order to avoid entering loops like the dogin the bin containing the dog in the bin.... To gener-ate a referring expression for an entity, the algorithmcalculates the DQs for all its attributes and approxi-mates the DQs for all its relations (2).
It then formsthe *preferred* list (3) and constructs the referringexpression by adding elements of *preferred* tillthe contrast set is empty (4).
This is straightfor-ward for attributes (5).
For relations (6), it needs torecursively generate the prepositional phrase first.It checks that it hasn?t entered a loop (6a), gener-ates a new contrast set for the object of the relation(6(a)i), recursively generates a referring expressionfor the object of the preposition (6(a)ii), recalculatesDQ (6(a)iii) and either incorporates the relation inthe referring expression or shifts the relation downthe *preferred* list (6(a)iv).
This step ensures thatan initial mis-estimation in the word length of a re-lation doesn?t force its inclusion at the expense ofshorter possibilities.
If after incorporating all at-tributes and relations, the contrast set is still non-empty, the algorithm returns the best expression itcan find (7).set generate-ref-exp(Entity, ContrastSet, UsedEntities)1.
IF ContrastSet = [] THEN RETURN {Entity.head}2.
Calculate CQ, SQ and DQ for each attribute andrelation of Entity (as in Sec 3.1 and 3.4)3.
Let *preferred* be the list of attributes/ relationssorted in decreasing order of DQs.
FOR each ele-ment (Mod) of *preferred* DO steps 4, 5 and 64.
IF ContrastSet = [] THEN RETURN RefExp ?{Entity.head}5.
IF Mod is an Attribute THEN(a) LET RefExp = {Mod} ?
RefExp(b) Remove from ContrastSet, any entities Modrules out6.
IF Mod is a Relation [prepi ei] THEN(a) IF ei ?
UsedEntities THENi.
Set DQ = ??ii.
Move Mod to the end of *preferred*ELSEi.
LET ContrastSet2 be the set of non-ei en-tities that are the objects of prepi rela-tions in members of ContrastSetii.
LET RE = generate-referring-exp(ei,ContrastSet2, {ei}?UsedEntities)iii.
recalculate DQ using length = 2 +length(RE)iv.
IF position in *preferred* is loweredTHEN re-sort *preferred*ELSE(?)
SET RefExp = RefExp ?{[prepi|determiner|RE]}(?)
Remove from ContrastSet, anyentities that Mod rules out7.
RETURN RefExp ?
{Entity.head}An Example Trace:We now trace the algorithm above as it generates areferring expression for d1 in figure 1.call generate-ref-exp(d1,[d2],[])?
step 1: ContrastSet is not empty?
step 2: DQsmall = ?4, DQgrey = ?4DQ[in b1] = 4/3, DQ[near d2] = 4/4?
step 3: *preferred* = [[in b1], [near d2], small,grey]d2d1b1d1???
?head dogattrib [small,grey]in b1near d2????d2???
?head dogattrib [small,grey]outside b1near d1????b1??
?head binattrib [large, steel]containing d1near d2??
?Figure 1: AVMs for two dogs and a bin?
Iteration 1 ?
mod = [in b1]?
step 6(a)i: ContrastSet2 = []?
step 6(a)ii: call generate-ref-exp(b1,[],[d1])?
step 1: ContrastSet = []return {bin}?
step 6(a)iii: DQ[in b1] = 4/3?
step 6(a)iv?
: RefExp = {[in, the, {bin}]}?
step 6(a)iv?
: ContrastSet = []?
Iteration 2 ?
mod = [near d2]?
step 4: ContrastSet = []return {[in the {bin}], dog}The algorithm presented above is designed to re-turn the shortest referring expression that uniquelyidentifies an entity.
If the scene in figure 1 were clut-tered with bins, the algorithm would still refer to d1as the dog in the bin as there is only one dog that isin a bin.
The user gets no help in locating the bin.If helping the user locate entities is important to thediscourse plan, we need to change step 6(a)(ELSE)iso that the contrast set includes all bins in context,not just bins that are objects of in relations of dis-tractors of d1.3.6 Compound NominalsOur analysis so far has assumed that attributes areadjectives.
However, many nominals introducedthrough relations can also be introduced in com-pound nominals, for example:1. a church in Paris ?
a Paris church2.
a novel by Archer ?
an Archer novel3.
a company from London ?
a London companyThis is an important issue for regeneration appli-cations, where the AVMs for entities are constructedfrom text rather than a semantic knowledge base(which could be constructed such that such casesare stored in relational form, though possibly withan underspecified relation).
We need to augment ouralgorithm so that it can compare AVMs like:[head churchin[head Paris ]]and[head churchattrib [Paris]]Formally, the algorithm for calculating SQ andCQ for a nominal attribute anom of entity eo is:FOR each distractor ei of eo DO1.
IF anom is similar to any nominal attribute of eiTHEN SQ = SQ + 42.
IF anom is similar to the head noun of the objectof any relation of ei THEN(a) SQ = SQ + 4(b) flatten that relation for ei, i.e., add the at-tributes of the object of the relation to the at-tribute list for eiIn step 2, we compare a nominal attribute anomof eo to the head noun of the object of a relationof ei.
If they are similar, it is likely that any at-tributes of that object might help distinguish eo fromei.
We then add those attributes to the attribute listof ei.
Now, if SQ is non-zero, the nominal at-tribute anom has bad discriminating power and weset DQ = ?SQ.
If SQ = 0, then anom has gooddiscriminating power and we set DQ = 4.We also extend the algorithm for calculating DQfor a relation [prepj ej ] of eo as follows:1.
IF any distractor ei has a nominal attribute anomTHEN(a) IF anom is similar to the head of ej THENi.
Add all attributes of eo to the attribute listand calculate their DQs2.
calculate DQ for the relation as in section 3.4We can demonstrate how this approach works us-ing entities extracted from the following sentence(from the Wall Street Journal):Also contributing to the firmness in copper, theanalyst noted, was a report by Chicago pur-chasing agents, which precedes the full pur-chasing agents report that is due out today andgives an indication of what the full report mighthold.Consider generating a referring expression for eowhen the distractor is e1:eo =???
?head reportby?
?head agentsattrib [Chicago,purchasing]?????
?e1 =[head reportattributes [full, purchasing, agents]]The distractor the full purchasing agents reportcontains the nominal attribute agents.
To comparereport by Chicago purchasing agents with full pur-chasing agents report, our algorithm flattens the for-mer to Chicago purchasing agents report.
Our algo-rithm now gives:DQagents = ?4, DQpurchasing = ?4,DQChicago = 4, DQby Chicago purchasing agents = 4/4We thus generate the referring expression theChicago report.
This approach takes advantage ofthe flexibility of the relationships that can hold be-tween nouns in a compound: although examples canbe devised where removing a nominal causes un-grammaticality, it works well enough empirically.To generate a referring expression for e1 (fullpurchasing agents report) when the distractor iseo(report by Chicago purchasing agents), our algo-rithm again flattens eo to obtain:DQagents = ?4, DQpurchasing = ?4DQfull = 4The generated referring expression is the full report.This is identical to the referring expression used inthe original text.4 EvaluationAs our algorithm works in open domains, we wereable to perform a corpus-based evaluation using thePenn WSJ Treebank (Marcus et al, 1993).
Our eval-uation aimed to reproduce existing referring expres-sions (NPs with a definite determiner) in the PennTreebank by providing our algorithm as input:1.
The first mention NP for that reference.2.
The contrast set of distractor NPsFor each referring expression (NP with a definitedeterminer) in the Penn Treebank, we automaticallyidentified its first mention and all its distractors in afour sentence window, as described in ?4.1.
We thenused our program to generate a referring expres-sion for the first mention NP, giving it a contrast-set containing the distractor NPs.
Our evaluationcompared this generated description with the orig-inal WSJ reference that we had started out with.Our algorithm was developed using toy examplesand counter-examples constructed by hand, and thePenn Treebank was unseen data for this evaluation.4.1 Identifying Antecedents and DistractorsFor every definite noun phrase NPo in the PennTreebank, we shortlisted all the noun phrases NPiin a discourse window of four sentences (the twopreceding sentences, current sentence and the fol-lowing sentence) that had a head noun identical toor a WordNet synonym of the head noun of NPo.We compared the set of attributes and relationsfor each shortlisted NPi that preceded NPo in thediscourse window with that of NPo.
If the attributesand relations set of NPi was a superset of that ofNPo, we assumed that NPo referred to NPi andadded NPi to an antecedent set.
We added all otherNPi to the contrast set of distractors.Similarly, we excluded any noun phrase NPi thatappeared in the discourse after NPo whose attributesand relations set was a subset of NPo?s and addedthe remaining NPi to the contrast set.
We then se-lected the longest noun phrase in the antecedent setto be the antecedent that we would try and generatea referring expression from.The table below gives some examples of distrac-tors that our program found using WordNet syn-onyms to compare head nouns:Entity Distractorsfirst half-free Soviet vote fair elections in the GDRmilitary construction bill fiscal measuresteep fall in currency drop in market stockpermanent insurance death benefit coverage4.2 ResultsThere were 146 instances of definite descriptions inthe WSJ where the following conditions (that ensurethat the referring expression generation task is non-trivial) were satisfied:1.
The definite NP (referring expression) contained atleast one attribute or relation.2.
An antecedent was found for the definite NP.3.
There was at least one distractor NP in the dis-course window.In 81.5% of these cases, our program returned areferring expression that was identical to the oneused in the WSJ.
This is a surprisingly high accu-racy, considering that there is a fair amount of vari-ability in the way human writers use referring ex-pressions.
For comparison, the baseline of repro-ducing the antecedent NP performed at 48%2.Some errors were due to non-recognition of mul-tiword expessions in the antecedent (for example,our program generated care product from personalcare product).
In many of the remaining error cases,it was difficult to decide whether what our pro-gram generated was acceptable or wrong.
For ex-ample, the WSJ contained the referring expressionthe one-day limit, where the automatically detectedantecedent was the maximum one-day limit for the2We are only evaluating content selection (the nouns andpre- and post-modifiers) and ignore determiner choice.S&P 500 stock-index futures contract and the auto-matically detected contrast set was:{the five-point opening limit for the contract,the 12-point limit, the 30-point limit, the in-termediate limit of 20 points}Our program generated the maximum limit, wherethe WSJ writer preferred the one-day limit.5 Further Issues5.1 Reference Modifying AttributesThe analysis thus far has assumed that all at-tributes modify the referent rather than the refer-ence to the referent.
However, for example, if e1is an alleged murderer, the attribute alleged mod-ifies the reference murderer rather than the refer-ent e1 and referring to e1 as the murderer wouldbe factually incorrect.
Logically e1 could be rep-resented as (alleged1(murderer1))(x), rather thanalleged1(x) ?
murderer1(x).
This is no longerfirst-order, and presents new difficulties for the tra-ditional formalisation of the reference generationproblem.
One (inelegant) solution would be to in-troduce a new predicate allegedMurderer1(x).A working approach in our framework would beto add a large positive weight to the DQs of refer-ence modifying attributes, thus forcing them to beselected in the referring expression.5.2 Discourse Context and SalienceThe incremental algorithm assumes the availabilityof a contrast set and does not provide an algorithmfor constructing and updating it.
The contrast set, ingeneral, needs to take context into account.
Krah-mer and Theune (2002) propose an extension to theIA which treats the context set as a combination of adiscourse domain and a salience function.
The blackdog would then refer to the most salient entity in thediscourse domain that is both black and a dog.Incorporating salience into our algorithm isstraightforward.
As described earlier, we computethe quotients SQ and CQ for each attribute or re-lation by adding an amount w ?
{4, 2, 1} to therelevant quotient based on a comparison with the at-tributes and relations of each distractor.
We can in-corporate salience by weighting w with the salienceof the distractor whose attribute or relation we areconsidering.
This will result in attributes and rela-tions with high discriminating power with regard tomore salient distractors getting selected first in theincremental process.5.3 Discourse PlansIn many situations, attributes and relations serve dif-ferent discourse functions.
For example, attributesmight be used to help the hearer identify an entitywhile relations might serve to help locate the en-tity.
This needs to be taken into account when gen-erating a referring expression.
If we were gener-ating instructions for using a machine, we mightwant to include both attributes and relations; so toinstruct the user to switch on the power, we mightsay switch on the red button on the top-left corner.This would help the user locate the switch (on thetop-left corner) and identify it (red).
If we werehelping a chef find the salt in a kitchen, we mightwant to use only relations because the chef knowswhat salt looks like.
The salt behind the corn akeson the shelf above the fridge is in this context prefer-able to the white powder.
If the discourse plan thatcontrols generation requires our algorithm to pref-erentially select relations or attributes, it can add apositive amount ?
to their DQs.
Then, the resultantformula is DQ = (CQ ?
SQ)/length + ?, wherelength = 1 for attributes and by default ?
= 0 forboth relations and attributes.6 Conclusions and Future WorkWe have described an algorithm for generating re-ferring expressions that can be used in any domain.Our algorithm selects attributes and relations thatare distinctive in context.
It does not rely on theavailability of an adjective classification scheme anduses WordNet antonym and synonym lists instead.It is also, as far as we know, the first algorithm thatallows for the incremental incorporation of relationsand the first that handles nominals.
In a novel eval-uation, our algorithm successfully generates identi-cal referring expressions to those in the Penn WSJTreebank in over 80% of cases.In future work, we plan to use this algorithm aspart of a system for generation from a database ofuser opinions on products which has been automat-ically extracted from newsgroups and similar text.This is midway between regeneration and the clas-sical task of generating from a knowledge base be-cause, while the database itself provides structure,many of the field values are strings correspondingto phrases used in the original text.
Thus, our lexi-calised approach is directly applicable to this task.7 AcknowledgementsThanks are due to Kees van Deemter and threeanonymous ACL reviewers for useful feedback onprior versions of this paper.This document was generated partly in the con-text of the Deep Thought project, funded underthe Thematic Programme User-friendly InformationSociety of the 5th Framework Programme of the Eu-ropean Community (Contract N IST-2001-37836)ReferencesRobert Dale and Nicholas Haddock.
1991.
Gen-erating referring expressions involving relations.In Proceedings of the 5th Conference of the Eu-ropean Chapter of the Association for Compu-tational Linguistics (EACL?91), pages 161?166,Berlin, Germany.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gen-eration of referring expressions.
Cognitive Sci-ence, 19:233?263.Helmut Horacek.
2003.
A best-first search algo-rithm for generating referring expressions.
InProceedings of the 11th Conference of the Eu-ropean Chapter of the Association for Compu-tational Linguistics (EACL?03), pages 103?106,Budapest, Hungary.Emiel Krahmer and Marie?t Theune.
2002.
Efficientcontext-sensitive generation of referring expres-sions.
In Kees van Deemter and Rodger Kib-ble, editors, Information Sharing: Givenness andNewness in Language Processing, pages 223?264.
CSLI Publications, Stanford,California.Emiel Krahmer, Sebastiaan van Erk, and Andre?Verleg.
2003.
Graph-based generation of re-ferring expressions.
Computational Linguistics,29(1):53?72.Mitchell Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large naturallanguage corpus of English: The Penn Treebank.Computational Linguistics, 19:313?330.George A. Miller, Richard Beckwith, Christiane D.Fellbaum, Derek Gross, and Katherine Miller.1993.
Five Papers on WordNet.
Technical report,Princeton University, Princeton, N.J.Ehud Reiter.
1990.
The computational complex-ity of avoiding conversational implicatures.
InProceedings of the 28th Annual Meeting of Asso-ciation for Computational Linguistics (ACL?90),pages 97?104, Pittsburgh, Pennsylvania.Ehud Reiter and Robert Dale.
1992.
A fast al-gorithm for the generation of referring expres-sions.
In Proceedings of the 14th InternationalConference on Computational Linguistics (COL-ING?92), pages 232?238, Nantes, France.Kees van Deemter.
2000.
Generating vague de-scriptions.
In Proceedings of the 1st Interna-tional Conference on Natural Language Genera-tion (INLG?00), pages 179?185, Mitzpe Ramon,Israel.Kees van Deemter.
2002.
Generating referring ex-pressions: Boolean extensions of the incrementalalgorithm.
Computational Linguistics, 28(1):37?52.
