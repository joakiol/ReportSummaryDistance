Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43?48,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultilingual Semantic Role LabelingAnders Bjo?rkelund Love Hafdell Pierre NuguesDepartment of Computer Science, Lund UniversityS-221 00 Lund, Swedenfte04abj@student.lth.selove hafdell@hotmail.comPierre.Nugues@cs.lth.seAbstractThis paper describes our contribution to thesemantic role labeling task (SRL-only) of theCoNLL-2009 shared task in the closed chal-lenge (Hajic?
et al, 2009).
Our system con-sists of a pipeline of independent, local clas-sifiers that identify the predicate sense, the ar-guments of the predicates, and the argumentlabels.
Using these local models, we carriedout a beam search to generate a pool of candi-dates.
We then reranked the candidates usinga joint learning approach that combines the lo-cal models and proposition features.To address the multilingual nature of the data,we implemented a feature selection procedurethat systematically explored the feature space,yielding significant gains over a standard setof features.
Our system achieved the secondbest semantic score overall with an average la-beled semantic F1 of 80.31.
It obtained thebest F1 score on the Chinese and German dataand the second best one on English.1 IntroductionIn this paper, we describe a three-stage analysis ap-proach that uses the output of a dependency parserand identifies the arguments of the predicates in asentence.
The first stage consists of a pipeline ofindependent classifiers.
We carried out the pred-icate disambiguation with a set of greedy classi-fiers, where we applied one classifier per predicatelemma.
We then used a beam search to identifythe arguments of each predicate and to label them,yielding a pool of candidate propositions.
The sec-ond stage consists of a reranker that we applied tothe candidates using the local models and proposi-tion features.
We combined the score of the greedyclassifiers and the reranker in a third stage to selectthe best candidate proposition.
Figure 1 shows thesystem architecture.We evaluated our semantic parser on a set of sevenlanguages provided by the organizers of the CoNLL-2009 shared task: Catalan and Spanish (Taule?
etal., 2008), Chinese (Palmer and Xue, 2009), Czech(Hajic?
et al, 2006), English (Surdeanu et al, 2008),German (Burchardt et al, 2006), and Japanese(Kawahara et al, 2002).
Our system achieved anaverage labeled semantic F1 of 80.31, which cor-responded to the second best semantic score over-all.
After the official evaluation was completed, wediscovered a fault in the training procedure of thereranker for Spanish.
The revised average labeledsemantic F1 after correction was 80.80.2 SRL PipelineThe pipeline of classifiers consists of a predicatedisambiguation (PD) module, an argument identi-fication module (AI), and an argument classifica-tion (AC) module.
Aside from the lack of a pred-icate identification module, which was not needed,as predicates were given, this architecture is identi-cal to the one adopted by recent systems (Surdeanuet al, 2008), as well as the general approach withinthe field (Gildea and Jurafsky, 2002; Toutanova etal., 2005).We build all the classifiers using the L2-regularized linear logistic regression from the LIB-LINEAR package (Fan et al, 2008).
The packageimplementation makes models very fast to train and43N candidatesN candidatesRerankerLocal features + proposition featuresGlobal modelLinear combination of modelsLocal classifier pipelineSense disambiguationgreedy searchArgument identificationbeam searchArgument labelingbeam searchRerankedcandidatesFigure 1: System architecture.use for classification.
Since models are logistic, theyproduce an output in the form of probabilities thatwe use later in the reranker (see Sect.
3).2.1 Predicate DisambiguationWe carried out a disambiguation for all the lem-mas that had multiple senses in the corpora and wetrained one classifier per lemma.
We did not use thepredicate lexicons and we considered lemmas with aunique observed sense as unambiguous.English required a special processing as the sensenomenclature overlapped between certain nominaland verbal predicates.
For instance, the nominalpredicate plan.01 and the verbal predicate plan.01do not correspond to the same semantic frame.Hence, we trained two classifiers for each lemmaplan that could be both a nominal and verbal predi-cate.Table 1: Feature sets for predicate disambiguation.ca ch cz en ge spPredWord ?
?
?PredPOS ?
?PredDeprel ?
?
?PredFeats ?
?
?PredParentWord ?
?
?
?
?PredParentPOS ?
?
?PredParentFeats ?
?DepSubCat ?
?
?
?
?ChildDepSet ?
?
?
?
?
?ChildWordSet ?
?
?
?
?
?ChildPOSSet ?
?
?
?
?2.2 Argument Identification and ClassificationWe implemented the argument identification andclassification as two separate stages, because it en-abled us to apply and optimize different feature setsin each step.
Arguments were identified by meansof a binary classifier.
No pruning was done, eachword in the sentence was considered as a potentialargument to all predicates of the same sentence.Arguments were then labeled using a multiclassclassifier; each class corresponding to a certain la-bel.
We did not apply any special processing withmultiple dependencies in Czech and Japanese.
In-stead, we concatenated the composite labels (i.e.double edge) to form unique labels (i.e.
single edge)having their own class.2.3 Identification and Classification FeaturesFor the English corpus, we used two sets of featuresfor the nominal and the verbal predicates both in theAI and AC steps.
This allowed us to create differentclassifiers for different kinds of predicates.
We ex-tended this approach with a default classifier catch-ing predicates that were wrongly tagged by the POStagger.
For both steps, we used the union of the twofeature sets for this catch-all class.We wanted to employ this procedure with the twoother languages, Czech and Japanese, where predi-cates had more than one POS type.
As feature selec-tion (See Sect.
2.4) took longer than expected, par-ticularly in Czech due to the size of the corpus andthe annotation, we had to abandon this idea and wetrained a single classifier for all POS tags in the AIand AC steps.For each data set, we extracted sets of featuressimilar to the ones described by Johansson andNugues (2008).
We used a total of 32 features thatwe denote with the prefixes: Pred-, PredParent-,Arg-, Left-, Right-, LeftSibling-, and RightSibling-for, respectively, the predicate, the parent of thepredicate, the argument, the leftmost and rightmostdependents of the argument, and the left and right44Table 2: Feature sets for argument identification and classification.Argument identification Argument classificationca ch cz en ge ja sp ca ch cz en ge ja spPredWord ?
N ?PredPOS N ?
?
V ?PredLemma N ?
?
?
?
N,V ?
?PredDeprelSense ?
?
V ?
?
?
?
N,V ?
?
?PredFeats ?
?
?PredParentWord V ?
V ?PredParentPOS V V ?PredParentFeats ?DepSubCat ?
?ChildDepSet ?
?
?
?
V ?
?
?ChildWordSet N ?
?ChildPOSSet ?
?
N ?ArgWord ?
?
N,V ?
?
?
?
?
?
N,V ?
?
?ArgPOS ?
?
N,V ?
?
?
?
?
?
N,V ?ArgFeats ?
?
?
?
?ArgDeprel ?
?
?
V ?
?
?
?
?
V ?
?DeprelPath ?
?
?
N,V ?
?
?
?
?
V ?POSPath ?
?
?
N,V ?
?
?
?
V ?
?Position ?
N,V ?
?
?
?
N,V ?
?LeftWord ?
?
?
?
N ?
?LeftPOS ?
?
VLeftFeats ?
?
?RightWord ?
N ?
?
N,V ?RightPOS N ?
?
N,V ?RightFeats ?
?LeftSiblingWord ?
?
?
?
N ?LeftSiblingPOS ?
?
?
?
N,V ?LeftSiblingFeats ?
?
?RightSiblingWord ?
?
V ?
?
?
?
?
?RightSiblingPOS ?
?RightSiblingFeats ?sibling of the argument.
The suffix of these namescorresponds to the column name of the CoNLL for-mat, except Word which corresponds to the Formcolumn.
Additional features are:?
Sense: the value of the Pred column, e.g.plan.01.?
Position: the position of the argument with re-spect to the predicate, i.e.
before, on, or after.?
DepSubCat: the subcategorization frame of thepredicate, e.g.
OBJ+OPRD+SUB.?
DeprelPath: the path from predicate to argu-ment concatenating dependency labels with thedirection of the edge, e.g.
OBJ?OPRD?SUB?.?
POSPath: same as DeprelPath, but depen-dency labels are exchanged for POS tags, e.g.NN?NNS?NNP?.?
ChildDepSet: the set of dependency labels ofthe children of the predicate, e.g.
{OBJ, SUB}.?
ChildPOSSet: the set of POS tags of the chil-dren of the predicate, e.g.
{NN, NNS}.?
ChildWordSet: the set of words (Form) of thechildren of the predicate, e.g.
{fish, me}.452.4 Feature SelectionWe selected the feature sets using a greedy forwardprocedure.
We first built a set of single features and,to improve the separability of our linear classifiers,we paired features to build bigrams.
We searchedthe space of feature bigrams using the same proce-dure.
See Johansson (2008, page 83), for a com-plete description.
We intended to carry out a cross-validation search.
Due to the lack of time, we re-sorted to using 80% of the training set for trainingand 20% for evaluating the features.
Table 2 con-tains the complete list of single features we used.We omitted the feature bigrams.Feature selection turned out to be a massive task.It took us three to four weeks searching the featurespaces, yet in most cases we were forced to interruptthe selection process after a few bigram features inorder to have our system ready in time.
This meansthat our feature sets can probably be further opti-mized.When the training data was initially released,we used the exact feature set from Johansson andNugues (2008) to compute baseline results on thedevelopment set for all the languages.
After featureselection, we observed an increase in labeled seman-tic F1 close to 10% in most languages.2.5 Applying Beam SearchThe AI module proceeds left to right consideringeach word as an argument of the current predicate.The current partial propositions are scored by com-puting the product of the probabilities of all thewords considered so far.
After each word, the cur-rent pool of partial candidates is reduced to the beamsize, k, and at the end of the sentence, the top k scor-ing propositions are passed on to the AC module.Given k unlabeled propositions, the AC moduleapplies a beam search on each of these propositionsindependently.
This is done in a similar manner,proceeding from left to right among the identifiedarguments, keeping the l best labelings in its beam,and returning the top l propositions, when all iden-tified arguments have been processed.
This yieldsn = k ?
l complete propositions, unless one of theunlabeled propositions has zero arguments, in whichcase we have n = (k ?
1) ?
l + 1.The probability of a labeled proposition accordingto the local pipeline is given by PLocal = PAI ?PAC , where PAI and PAC is the output probabilityfrom the AI and AC modules, respectively.
In thecase of empty propositions, PAC was set to 1.3 Global RerankerWe implemented a global reranker followingToutanova et al (2005).
To generate training ex-amples for the reranker, we trained m AI and ACclassifiers by partitioning the training set in m partsand using m ?
1 of these parts for each AI and ACclassifier, respectively.We applied these AI and AC classifiers on the partof the corpus they were not trained on and we thengenerated the top n propositions for each predicate.We ran the CoNLL evaluation script on the proposi-tions and we marked the top scoring one(s) as pos-itive.
We marked the others negative.
If the correctproposition was not in the pool of candidates, weadded it as an extra positive example.
We used thesepositive and negative examples as training data forthe global reranker.3.1 Reranker FeaturesWe used all the features from the local pipeline forall the languages.
We built a vector where the AIfeatures were prefixed with AI- and the AC featuresprefixed with lab?, where lab was any of the argu-ment labels.We added one proposition feature to the concate-nation of local features, namely the sequence of coreargument labels, e.g.
A0+plan.01+A1.
In Catalanand Spanish, we considered all the labels prefixed byarg0, arg1, arg2, or arg3 as core labels.
In Chineseand English, we considered only the labels A0, A1,A2, A3, and A4.
In Czech, German, and Japanese,we considered all the labels as core labels.Hence, the total size of the reranker vector spaceis |AI| + |L| ?
|AC| + |G|, where |AI| and |AC|denotes the size of the AI and AC vector spaces, re-spectively, |L| corresponds to the number of labels,and |G| is the size of additional global features.We ran experiments with the grammaticalvoice that we included in the string represent-ing the sequence of core argument labels, e.g.A1+plan.01/Passive+A0.
The voice was derived byhand-crafted rules in Catalan, English, German, and46Spanish, and given in the Feat column in Czech.However, we did not notice any significant gain inperformance.
The hand-crafted rules use lexicalforms and dependencies, which we believe classi-fiers are able to derive themselves using the localmodel features.
This also applies to Czech, as Pred-Feats was a feature used in the local pipeline, bothin the AI and AC steps.3.2 Weighting the ModelsIn Sect.
2.5, we described how the pipeline was usedto generate the top n propositions, each with its ownlocal probability PLocal.
Similar to softmax, we nor-malized these local probabilities by dividing each ofthem by their total sum.
We denote this normalizedprobability by P ?Local.
The reranker gives a proba-bility on the complete proposition, PReranker.
Weweighted these probabilities and chose the proposi-tion maximizing PF inal = (P ?Local)?
?
PReranker.This is equivalent to a linear combination of the logprobabilities.3.3 Parameters UsedFor the submission to the CoNLL 2009 Shared Task,we set the beam widths to k = l = 4, yielding can-didate pools of size n = 13 or n = 16 (See Sec-tion 2.5).
We used m = 5 for training the rerankerand ?
= 1 for combining the local model with thereranker.4 ResultsOur system achieved the second best semantic score,all tasks, with an average labeled semantic F1 of80.31.
It obtained the best F1 score on the Chineseand German data and the second best on English.Our system also reached the third rank in the out-of-domain data, all tasks, with a labeled semantic F1 of74.38.
Post-evaluation, we discovered a bug in theSpanish reranker model causing the poor results inthis language.
After correcting this, we could reacha labeled semantic F1 of 79.91 in Spanish.
Table 3shows our official results in the shared task as wellas the post-evaluation update.We also compared the performance of a greedystrategy with that of a global model.
Table 4 showsthese figures with post-evaluation figures in Spanish.Table 5 shows the training time, parsing time, andthe parsing speed in predicates per second.
Thesefigures correspond to complete execution time ofparsing, including loading models into memory, i.e.a constant overhead, that explains the low parsingspeed in German.
We implemented our system tobe flexible for easy debugging and testing variousideas.
Optimizing the implementation would reduceexecution times significantly.Table 3: Summary of submitted results: closed challenge,semantic F1.
* denotes the post-evaluation results ob-tained for Spanish after a bug fix.Unlabeled LabeledCatalan 93.60 80.01Chinese 84.76 78.60Czech 92.63 85.41English 91.17 85.63German 92.13 79.71Japanese 83.45 76.30Spanish 92.69 76.52Spanish* 93.76 79.91Average 90.06 80.31Average* 90.21 80.80Table 4: Improvement of reranker.
* denotes the post-evaluation results obtained for Spanish after a bug fix.Greedy Reranker GainCatalan 79.54 80.01 0.47Chinese 77.84 78.60 0.76Czech 84.99 85.41 0.42English 84.44 85.63 1.19German 79.01 79.71 0.70Japanese 75.61 76.30 0.69Spanish 79.28 76.52 -2.76Spanish* 79.28 79.91 0.63Average 80.10 80.31 0.21Average* 80.10 80.80 0.705 ConclusionWe have built and described a streamlined and ef-fective semantic role labeler that did not use anylexicons or complex linguistic features.
We used ageneric feature selection procedure that keeps lan-guage adaptation minimal and delivers a relativelyeven performance across the data sets.
The system is47Table 5: Summary of training and parsing times on an Apple Mac Pro, 3.2 GHz.Training Parsing (Greedy) Speed (Greedy) Parsing (Reranker) Speed (Reranker)(min) (min:sec) (pred/sec) (min:sec) (pred/sec)Catalan 46 1:10 71 1:21 62Chinese 139 2:35 79 3:45 55Czech 299 18:47 40 33:49 22English 421 6:25 27 8:51 20German 15 0:21 26 0:22 25Japanese 48 0:37 84 1:02 50Spanish 51 1:15 69 1:47 48robust and can handle incorrect syntactic parse treeswith a good level of immunity.
While input parsetrees in Chinese and German had a labeled syntac-tic accuracy of 78.46 (Hajic?
et al, 2009), we couldreach a labeled semantic F1 of 78.60 and 79.71 inthese languages.
We also implemented an efficientglobal reranker in all languages yielding a 0.7 av-erage increase in labeled semantic F1.
The rerankerstep, however, comes at the expense of parsing timesincreased by factors ranging from 1.04 to 1.82.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In Proceedings of theShared Task Session of CoNLL-2008.Richard Johansson.
2008.
Dependency-based SemanticAnalysis of Natural-language Text.
Ph.D. thesis, LundUniversity, December 5.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and JoakimNivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008).Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semantic rolelabeling.
In Proceedings of ACL-2005.48
