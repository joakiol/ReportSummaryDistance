Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 311?321,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsConnotation Frames: A Data-Driven InvestigationHannah Rashkin Sameer Singh Yejin ChoiComputer Science & EngineeringUniversity of Washington{hrashkin, sameer, yejin}@cs.washington.eduAbstractThrough a particular choice of a predicate(e.g., ?x violated y?
), a writer can subtlyconnote a range of implied sentiment andpresupposed facts about the entities x andy: (1) writer?s perspective: projecting xas an ?antagonist?
and y as a ?victim?, (2)entities?
perspective: y probably dislikesx, (3) effect: something bad happened to y,(4) value: y is something valuable, and (5)mental state: y is distressed by the event.We introduce connotation frames as a rep-resentation formalism to organize theserich dimensions of connotation using typedrelations.
First, we investigate the fea-sibility of obtaining connotative labelsthrough crowdsourcing experiments.
Wethen present models for predicting the con-notation frames of verb predicates basedon their distributional word representationsand the interplay between different typesof connotative relations.
Empirical resultsconfirm that connotation frames can be in-duced from various data sources that reflecthow language is used in context.
We con-clude with analytical results that show thepotential use of connotation frames for an-alyzing subtle biases in online news media.1 IntroductionPeople commonly express their opinions throughsubtle and nuanced language (Thomas et al, 2006;Somasundaran and Wiebe, 2010).
Often, throughseemingly objective statements, the writer can in-fluence the readers?
judgments toward an event andtheir participants.
Even by choosing a particularpredicate, the writer can indicate rich connotativeinformation about the entities that interact throughthe predicate.
More specifically, through a simpleWriter: ?Agent violates theme.
?Writer+----=+- Reader-==Agent ThemeP(w !agent) P(w!
theme)P(agent!
theme)E(agent) E(theme)V(theme)V(agent)E(theme)S ag nt)S(agent)E ) Perspective: the writer is  sympathetic  towards the themePerspective:  the writer portrays the agent as being antagonistic Value: the theme must be valuable+Effect: the agent is not really affected by the violationState: the theme will be unhappyState: the agent feels indifferentEffect: the theme has been hurtValue: not clear if agent is valuableFigure 1: An example connotation frame of ?violate?
as aset of typed relations: perspective P(x ?
y), effect E(x),value V(x), and mental state S(x).statement such as ?x violated y?, the writer canconvey:(1) writer?s perspective: the writer is projectingx as an ?antagonist?
and y as a ?victim?, elic-iting negative perspective from readers towardx (i.e., blaming x) and positive perspective to-ward y (i.e., sympathetic or supportive towardy).
(2) entities?
perspective: y most likely feels neg-atively toward x as a result of being violated.
(3) effect: something bad happened to y.
(4) value: y is something valuable, since it doesnot make sense to violate something worthless.In other words, the writer is presupposing apositive value of y as a fact.
(5) mental state: y is most likely unhappy aboutthe outcome.11To be more precise, y is most likely in a negative state311Verb Subset of Typed Relations Example Sentences L/Rsuffer P(w ?
agent) = +P(w ?
theme) = ?P(agent?
theme) = ?E(agent) = ?V(agent) = +S(agent) = ?The story begins in Illinois in 1987, when a 17-year-old girl suffered a botched abortion.Rguard P(w ?
agent) = +P(w ?
theme) = +P(agent?
theme) = +E(theme) = +V(theme) = +S(theme) = +In August, marshals guarded 25 clinics in 18cities.Luphold P(w ?
theme) = +P(agent?
theme) = +E(theme) = +V(theme) = +A hearing is scheduled to make a decision onwhether to uphold the clinic?s suspension.RTable 1: Example typed relations (perspective P(x?
y), effect E(x), value V(x), and mental state S(x)).Not all typed relations are shown due to space constraints.
The example sentences demonstrate the usageof the predicates in left [L] or right [R] leaning news sources.Even though the writer might not explicitly stateany of the interpretation [1-5] above, the readerswill be able interpret these intentions as a part oftheir comprehension.
In this paper, we present anempirical study of how to represent and induce theconnotative interpretations that can be drawn froma verb predicate, as illustrated above.We introduce connotation frames as a represen-tation framework to organize the rich dimensionsof the implied sentiment and presupposed facts.Figure 1 shows an example of a connotation framefor the predicate violate.
We define four differenttyped relations: P(x ?
y) for perspective of xtowards y, E(x) for effect on x, V(x) for value ofx, and S(x) for mental state of x.
These relation-ships can all be either positive (+), neutral (=), ornegative (-).Our work is the first study to investigate framesas a representation formalism for connotativemeanings.
This contrasts with previous com-putational studies and resource development forframe semantics, where the primary focus was al-most exclusively on denotational meanings of lan-guage (Baker et al, 1998; Palmer et al, 2005).
Ourformalism draws inspirations from the earlier workof frame semantics, however, in that we investi-gate the connection between a word and the relatedworld knowledge associated with the word (Fill-more, 1976), which is essential for the readers tointerpret many layers of the implied sentiment andpresupposed value judgments.We also build upon the extensive amount of lit-erature in sentiment analysis (Pang and Lee, 2008;Liu and Zhang, 2012), especially the recent emerg-ing efforts on implied sentiment analysis (Fenget al, 2013; Greene and Resnik, 2009), entity-entity sentiment inference (Wiebe and Deng, 2014),assuming it is an entity that can have a mental state.opinion role induction (Wiegand and Ruppenhofer,2015) and effect analysis (Choi and Wiebe, 2014).However, our work is the first to organize variousaspects of the connotative information into coher-ent frames.More concretely, our contributions are threefold:(1) a new formalism, model, and annotated datasetfor studying connotation frames from large-scalenatural language data and statistics, (2) new data-driven insights into the dynamics among differenttyped relations within each frame, and (3) an ana-lytic study showing the potential use of connotationframes for analyzing subtle biases in journalism.The rest of the paper is organized as follows: in?2, we provide the definitions and data-driven in-sights for connotation frames.
In ?3, we introducemodels for inducing the connotation frames, fol-lowed by empirical results, annotation studies, andanalysis on news media in ?4.
We discuss relatedwork in ?5 and conclude in ?6.2 Connotation FrameGiven a predicate v, we define a connotation frameF(v) as a collection of typed relations and their po-larity assignments: (i) perspective Pv(ai?
aj):a directed sentiment from the entity aito the entityaj, (ii) value Vv(ai): whether aiis presupposed tobe valuable, (iii) effect Ev(ai): whether the eventdenoted by the predicate v is good or bad for theentity ai, and (iv) mental state Sv(ai): the likelymental state of the entity aias a result of the event.We assume that each typed relation can have one ofthe three connotative polarities ?
{+,?,=}, i.e.,positive, negative, or neutral.
Our goal in this paperis to focus on the general connotation of the predi-cate considered out of context.
We leave contextualinterpretation of connotation as future work.Table 1 shows examples of connotation frame312Verb x?s role P(w?
?)
Left-leaning Sources Right-leaning Sourcesaccuseagent - Putin, Progressives, Limbaugh, Gingrich activist, U.S., protestor, Chaveztheme + official, rival, administration, leader Romney, Iran, Gingrich, regimeattackagent - McCain, Trump, Limbaugh Obama, campaign, Biden, Israeltheme + Gingrich, Obama, policy citizen, Zimmermancriticizeagent - Ugandans, rival, Romney, Tyson Britain, passage, Obama, Maddowtheme + Obama, Allen, Cameron, Congress Pelosi, Romey, GOP, RepublicansTable 2: Media Bias in Connotation Frames: Obama, for example, is portrayed as someone who attacksor criticizes others by the right-leaning sources, whereas the left-leaning sources portray Obama as thevictim of harsh acts like ?attack?
and ?criticize?.relations for the verbs suffer, guard, and uphold,along with example sentences.
For instance, for theverb suffer, the writer is likely to have a positiveperspective towards the agent (e.g., being support-ive or sympathetic toward the ?17-year-old girl?
inthe example shown on the right) and a negativeperspective towards the theme (e.g., being negativetowards ?botched abortion?
).2.1 Data-driven MotivationSince the meaning of language is ultimately contex-tual, the exact connotation will vary depending onthe context of each utterance.
Nonetheless, therestill are common shifts or biases in the connota-tive polarities, as we found from two data-drivenanalyses.First, we looked at words from the SubjectivityLexicon (Wilson et al, 2005) that are used in theargument positions of a small selection of predi-cates in Google Syntactic N-grams (Goldberg andOrwant, 2013).
For this analysis, we assumed thatthe word in the subject position is the agent whilethe object is the theme.
We found 64% of thewords in the agent position of suffer are positive,and 94% of the words in the theme position arenegative, which is consistent with the polarities ofthe writer?s perspective towards these arguments,as shown in Table 1.
For guard, 57% of the sub-jects and 76% of the objects are positive, and in thecase of uphold, 56% of the subjects and 72% of theobjects are positive.We also investigated how media bias can po-tentially be analyzed through connotation frames.From the Stream Corpus 2014 dataset (KBA, 2014),we selected all articles from news outlets withknown political biases,2and compared how they2The articles come from 30 news sources indicated byothers as exhibiting liberal or conservative leanings (Mitchellet al, 2014; Center for Media and Democracy, 2013; Centerfor Media and Democracy, 2012; HWC Library, 2011)use polarised words such as ?accuse?, ?attack?, and?criticize?
differently in light of P(w ?
agent)and P(w ?
theme) relations of the connota-tion frames.
Table 2 shows interesting contrasts.Obama, for example, is portrayed as someone whoattacks or criticizes others according to the right-leaning sources, whereas the left-leaning sourcesportray Obama as the victim of harsh acts like ?at-tack?
or ?criticize?.3Furthermore, by knowing theperspective relationships P(w ?
ai) associatedwith a predicate, we can make predictions abouthow the left-leaning and right-leaning sources feelabout specific people or issues.
For example, be-cause left-leaning sources frequently use McCain,Trump, and Limbaugh in the subject position ofattack, we might predict that these sources have anegative sentiment towards these entities.2.2 Dynamics between Typed RelationsGiven a predicate, the polarity assignments of typedrelations are interdependent.
For example, if thewriter feels positively towards the agent but nega-tively towards the theme, then it is likely that theagent and the theme do not feel positively towardseach other.
This insight is related to that of Wiebeand Deng (2014), but differs in that the polaritiesare predicate-specific and do not rely on knowledgeof prior sentiment towards the arguments.
This andother possible interdependencies are summarized inTable 3.
These interdependencies serve as generalguidelines of what properties we expect to dependon one another, especially in the case where the po-larities are non-neutral.
We will promote these in-ternal consistencies in our factor graph model (?3)as soft constraints.There also exist other interdependencies that wewill use to simplify our task.
First, the directed3That is, even if someone truly deserves criticism fromObama, left-learning sources would choose slightly differentwordings to avoid a potentially harsh cast on Obama.313Perspective Triad: If A is positive towards B, and B is positive towards C, then we expect A is also positive towardsC.
Similar dynamics hold for the negative case.Pw?a1= ?
(Pw?a2?
Pa1?a2)Perspective ?
Effect: If a predicate has a positive effect on the Subject, then we expect that the interaction betweenthe Subject and Object was positive.
Similar dynamics hold for the negative case and for other perspective relations.Ea1= Pa2?a1Perspective ?
Value: If A is presupposed as valuable, then we expect that the writer also views A positively.
Similardynamics hold for the negative case.Va1= Pw?a1Effect ?
Mental State: If the predicate has a positive effect on A, then we expect that A will gain a positive mentalstate.
Similar dynamics hold for the negative case.Sa1= Ea1Table 3: Potential Dynamics among Typed Relations: we propose models that parameterize these dynamicsusing log-linear models (frame-level model in ?3).sentiments between the agent and the theme arelikely to be reciprocal, or at least do not directlyconflict with + and ?
simultaneously.
Therefore,we assume that P(a1?
a2) = P(a2?
a1) =P(a1?
a2), and we only measure for these binaryrelationships going in one direction.
In addition, weassume the predicted4perspective from the readerr to an argument P(r ?
a) is likely to be the sameas the implied perspective from the writer w to thesame argument P(w ?
a).
So, we only try tolearn the perspective of the writer.
Lifting theseassumptions will be future work.For simplicity, our model only explores the po-larities involving the agent and the theme roles.
Wewill assume that these roles are correlated to thesubject and object positions, and henceforth referto them as the ?Subject?
and ?Object?
of the event.3 Modeling Connotation FramesOur task is essentially that of lexicon induction(Akkaya et al, 2009; Feng et al, 2013) in thatwe want to induce the connotation frames of pre-viously unseen verbs.
For each predicate, we infera connotation frame composed of 9 relationshipaspects that represent: perspective {P(w ?
o),P(w ?
s), P(s?
o)}, effect {E(o), E(s)}, value{V(o), V(s)}, and mental state {S(o), S(s)} po-larities.We propose two models: an aspect-level modelthat makes the prediction for each typed relationindependently based on the distributional represen-tation of the context in which the predicate appears(?3.1), and a frame-level model that makes the pre-4Surely different readers can and will form varying opin-ions after reading the same text.
Here we concern with themost likely perspective of the general audience, as a result ofreading the text.Node MeaningPerspective of Writer towards SubjectEffect on SubjectValue of SubjectMental State of SubjectFigure 2: A factor graph for predicting the polari-ties of the typed relations that define a connotationframe for a given verb predicate.
The factor graphalso includes unary factors (?emb), which we leftout for brevity.diction over the connotation frame collectively inconsideration the dynamics between typed relations(?3.2).3.1 Aspect-LevelOur aspect-level model predicts labels for each ofthese typed relations separately.
As input, we usethe 300-dimensional dependency-based word em-beddings from Levy and Goldberg (2014).
For eachaspect, there is a separate MaxEnt (maximum en-tropy) classifier used to predict the label of that as-pect on a given word-embedding, which is treatedas a 300 dimensional input vector to the classi-fier.
The MaxEnt classifiers learn their weightsusing LBFGS on the training data examples withre-weighting of samples to maximize for the bestaverage F1 score.3143.2 Frame-LevelNext we present a factor graph model (Figure 2)of the connotation frames that parameterize thedynamics between typed relations.
Specifically,for each verb predicate,5the factor graph contains9 nodes representing the different aspects of theconnotation frame.
All these variables take polarityvalues from the set {?,=,+}.We define Yi:= {Pwo,Pws,Pso, Eo, Es,Vo,Vs,So,Ss} as the set of relational aspects for theithverb.
The factor graph for Yi, is illustrated inFigure 2, and we will describe the factor potentialsin more detail in the rest of this section.
The proba-bility of an assignment of polarities to the nodes inYiis:P (Yi) ?
?PV(Pws,Vs) ?PV(Pwo,Vo)?PE(Pso, Es) ?PE(Pso, Eo)?ES(Es,Ss) ?ES(Eo,So)?PT(Pwo,Pws,Pso)?y?Yi?emb(y)Embedding Factors We include unary factors onall nodes to represent the results of the aspect-levelclassifier.
Incorporating this knowledge as factors,as opposed to fixing the variables as observed, af-fords us the flexibility of representing noise in thelabels as soft evidence.
The potential function ?embis a log-linear function of a feature vector f, whichis a one-hot feature vector representing the polarityof a node (+,?,or =).
For example, with the noderepresenting the value of the object (Vo):?emb(Vo) = ewVo?f(Vo)The potential ?embis defined similarly for the other8 remaining nodes.
All weights were learned usingstochastic gradient descent (SGD) over trainingdata.Interdependency Factors We include interde-pendency factors to promote the properties definedby the dynamics between relations (?2.2).
The po-tentials for Perspective Triad, Perspective-Value,Perspective-Effect, and Effect-State Relationships(?PT, ?PV, ?PE, ?ESrespectively) are all definedusing log-linear functions of one-hot feature vec-tors that encode the combination of polarities ofthe neighboring nodes.
The potential for ?PTistherefore:?PT(Pwo,Pws,Pso) = ewPT?f(Pwo,Pws,Pso)5We consider only verb predicates here.And we define the potentials for ?PV, ?PE, and ?ESfor subject nodes as:?PV(Pws,Vs) = ewPV,s?f(Pws,Vs)?PE(Pso, Es) = ewPE,s?f(Pso,Es)?ES(Es,Ss) = ewES,s?f(Es,Ss)and we define the potentials for the object nodessimilarly.
As with the unary seed factors, weightswere learned using SGD over training data.Belief Propagation We use belief propagationto induce the connotation frames of previously un-seen verbs.
In the belief propagation algorithm,messages are iteratively passed between the nodesto their neighboring factors and vice versa.
Eachmessage ?, containing a scalar for each valuex ?
{?, 0,+}, is defined from each node v toa neighboring factor a as follows:?v?a(x) ??a?
?N(v) a?a?
?v(x)and from each factor a to a neighboring node v as:?a?v??x?,x?v=x?(x?)?v?
?N(a) v?v??a(x?v?
)At the conclusion of message passing, the probabil-ity of a specific polarity associated with node v be-ing equal to x is proportional to?a?N(v)?a?v(x).Our factor graph does not contain any loops, so weare able to perform exact inference.4 ExperimentsWe first describe crowd-sourced annotations (?4.1),then present the empirical results of predicting con-notation frames (?4.2), and conclude with qualita-tive analysis of a large corpus (?4.3).4.1 Data and CrowdsourcingIn order to understand how humans interpret conno-tation frames, we designed an Amazon MechanicalTurk (AMT) annotation study.
We gathered a set oftransitive verbs commonly used in the New YorkTimes corpus (Sandhaus, 2008), selecting the 2400verbs that are used more than 200 times in the cor-pus.
Of these, AMT workers annotated the 1000most frequently used verbs.Annotation Design In a pilot annotation exper-iment, we found that annotators have difficultythinking about subtle connotative polarities whenshown predicates without any context.
Therefore,315we designed the AMT task to provide a genericcontext as follows.
We first split each verb predi-cate into 5 separate tasks that each gave workers adifferent generic sentence using the verb.
To cre-ate generic sentences, we used Google SyntacticN-grams (Goldberg and Orwant, 2013) to come upwith a frequently seen Subject-Verb-Object tuplewhich served as a simple three-word sentence withgeneric arguments.
For each of the 5 sentences, weasked 3 annotators to answer questions like ?Howdo you think the Subject feels about the event de-scribed in this sentence??
In total, each verb has15 annotations aggregated over 5 different genericsentences containing the verb.In order to help the annotators, some of the ques-tions also allowed annotators to choose sentimentusing additional classes for ?positive or neutral?or ?negative or neutral?
for when they were lessconfident but still felt like a sentiment might ex-ist.
When taking inter-annotator agreement, wecount ?positive or neutral?
as agreeing with either?positive?
or ?neutral?
classes.Annotator agreement Table 4 shows agreementsand data statistics.
The non-conflicting (NC) agree-ment only counts opposite polarities as disagree-ment.6From this study, we can see that non-expertannotators are able to see these sort of relationshipsbased on their understanding of how language isused.
From the NC agreement, we see that annota-tors do not frequently choose completely oppositepolarities, indicating that even when they disagree,their disagreements are based on the degree of con-notations rather than the polarity itself.
The averageKrippendorff alpha for all of the questions posedto the workers is 0.25, indicating stronger than ran-dom agreement.
Considering the subtlety of theimplicit sentiments that we are asking them to an-notate, it is reasonable that some annotators willpick up on more nuances than others.
Overall, thepercent agreement is encouraging that the connota-tive relationships are visible to human annotators.Aggregating Annotations We aggregated overcrowdsourced labels (fifteen annotations per verb)to create a polarity label for each aspect of a verb.7Final distributions of the aggregated labels are6Annotators were asked yes/no questions related to Value,so this does not have a corresponding NC agreement score.7We take the average to obtain scalar value between[?1., 1.]
for each aspect of a verb?s connotation frame.
Forsimplicity, we cutoff the ranges of negative, neutral and pos-itive polarities as [?1,?0.25), [?0.25, 0.25] and (0.25, 1],respectively.Aspect% Agreement DistributionStrict NC % + % -P(w ?
o) 75.6 95.6 36.6 4.6P(w ?
s) 76.1 95.5 47.1 7.9P(s?
o) 70.4 91.9 45.8 5.0E(o) 52.3 94.6 50.3 20.24E(s) 53.5 96.5 45.1 4.7V(o) 65.2 - 78.64 2.7V(s) 71.9 - 90.32 1.4S(o) 79.9 98.0 12.8 14.5S(s) 70.4 92.5 50.72 8.6Table 4: Label Statistics: % Agreement refers to pairwiseinter-annotator agreement.
The strict agreement counts agree-ment over 3 classes (?positive or neutral?
was counted asagreeing with either + or neutral), while non-conflicting (NC)agreement also allows agreements between neutral and -/+ (nodirect conflicts).
Distribution shows the final class distributionof -/+ labels created by averaging annotations.included in the right-hand columns of Table 4.Notably, the distributions are skewed toward pos-itive and neutral labels.
The most skewed conno-tation frame aspect is the value V(x) which tendsto be positive, especially for the subject argument.This makes some intuitive sense since, as the sub-ject actively causes the predicate event to occur,they most likely have some intrinsic potential to bevaluable.
An example of a verb where the subjectwas labelled as not valuable is ?contaminate?.
Inthe most generic case, the writer is using contami-nate to frame the subject as being worthless (andeven harmful) with regards to the other event par-ticipants.
For example, in the sentence ?his touchcontaminated the food,?
it is clear that the writerconsiders ?his touch?
to be of negative value in thecontext of how it impacts the rest of the event.4.2 Connotation Frame PredictionUsing our crowdsourced labels, we randomly di-vided the annotated verbs into training, dev, andheld-out test sets of equal size (300 verbs each).For evaluation we measured average accuracy andF1 score over the 9 different Connotation Framerelationship types for which we have annotations:P(w ?
o), P(w ?
s), P(s ?
o), V(o), V(s),E(o), E(s), S(o), and S(s).Baselines To show the non-trivial challenge oflearning Connotation Frames, we include a simplemajority-class baselines.
The MAJORITY classi-fier assigns each of the 9 relationships the labelof the majority of that relationship type found inthe training data.
Some of these relationships (inparticular, the Value of subject/object) have skewed316distributions, so we expect this classifier to achievea much higher accuracy than random but a muchlower overall F1 score.Additionally, we add a GRAPH PROP baselinethat is comparable to algorithms like graph prop-agation or label propagation which are often usedfor (sentiment) lexicon induction.
We use a factorgraph with nodes representing the polarity of eachtyped relation for each verb.
Binary factors con-nect nodes representing a particular type of relationfor two similar verbs (e.g.
P(w ?
o) for verbspersuade and convince).
These binary factors havehand-tuned potentials that are proportional to thecosine similarity of the verbs?
embeddings, encour-aging similar verbs to have the same polarity forthe various relational aspects.
We use words in thetraining data as the seed set and use loopy beliefpropagation to propagate polarities from knownnodes to the unknown relationships.Finally, we use a 3-NEAREST NEIGHBOR base-line that labels relationships for a verb based onthe predicate?s 300-dimensional word embeddingrepresentation, using the same embeddings as inour aspect-level.
3-NEAREST NEIGHBOR labelseach verb using the polarities of the three closestverbs found in the training set.
The most similarverbs are determined using the cosine similaritybetween word embeddings.Results As shown in Table 5, aspect-level andframe-level models consistently outperform allthree baselines ?
MAJORITY, 3-NN, GRAPHPROP in the development set across the differenttypes of relationships.
In particular, the improvedF1 scores show that these models are able to per-form better across all three classes of labels evenin the most skewed cases.
The frame-level modelalso frequently improves the F1 scores of the la-bels from what they were in the aspect-level model.The summarized comparison of the classifiers?
per-formance test set is shown in Table 6.
As withthe development set, aspect-level and frame-levelare both able to outperform the baselines.
Fur-thermore, the frame-level formulation is able tomake improvement over the results of the aspect-level classification, indicating that the modelling ofinter-dependencies between relationships did helpcorrect some of the mistakes made.One point of interest about the frame-level re-sults is whether the learned weights over the consis-tency factors match our initial intuitions about inter-dependencies between relationships.
The weights(a) wembfor P(s?
o)(b) P(w ?
o): -(c) P(w ?
o): =(d) P(w ?
o): +Figure 3: Learned weights of embedding factor forthe perspective of subject to object and the weightsthe perspective triad (PT) factor.
Red is for weightsthat are more positive, whereas blue are more neg-ative.learned in our algorithm do tell us something in-teresting about the degree to which these inter-dependencies are actually found in our data.We show the heat maps for some of the learnedweights in Figure 3.
In 3a, we show the weights ofone of the embedding factors, and how the polari-ties are more strongly weighted when they matchthe relation-level output.
In the rest of the figure,we show the weights for the other perspective rela-tionships when P(w ?
o) is negative (3b), neutral(3c), and positive (3d), respectively.
Based on theexpected interdependencies, when P(w ?
o) : ?,the model should favor P(w ?
s) 6= P(s ?
o)and when P(w ?
o) : +, the model should favorP(w ?
s) = P(s?
o).
Our model does, in fact,learn a similar trend, with slightly higher weightsalong these two diagonals in the maps 3b and 3d.Interestingly, when P(w ?
o) is neutral, weightsslightly prefer for the other two perspectives to re-semble one another, but with highest weights beingwhen other perspectives are also neutral.4.3 Analysis of a Large News CorpusUsing the connotation frame, we present measuredimplied sentiment in online journalism.Data From the Stream Corpus (KBA, 2014),we select 70 million news articles.
We extractsubject-verb-object relations for this subset us-ing the direct dependencies between noun phrases317Aspect Algorithm Acc.
Avg F1P(w ?
o)Majority 56.52 24.07Graph Prop 59.53 50.203-nn 62.88 47.93Aspect-Level 67.56 56.18Frame-Level 67.56 56.18P(w ?
s)Majority 49.83 22.17Graph Prop 52.84 42.933-nn 55.18 45.88Aspect-Level 60.54 60.72Frame-Level 61.87 63.07P(s?
o)Majority 49.83 22.17Graph Prop 52.17 46.573-nn 56.52 52.94Aspect-Level 63.21 61.70Frame-Level 63.88 62.56E(o)Majority 48.83 21.87Graph Prop 54.85 51.403-nn 55.18 51.53Aspect-Level 64.21 63.63Frame-Level 65.22 64.67E(s)Majority 49.83 22.17Graph Prop 52.17 35.563-nn 54.85 42.63Aspect-Level 62.54 53.82Frame-Level 63.88 56.81V(o)Majority 79.60 29.55Graph Prop 71.91 35.103-nn 76.25 39.09Aspect-Level 75.92 45.45Frame-Level 76.25 48.13V(s)Majority 89.30 31.45Graph Prop 84.62 38.823-nn 85.62 38.45Aspect-Level 87.96 48.06Frame-Level 87.96 48.06S(o)Majority 71.91 27.89Graph Prop 69.90 55.573-nn 72.91 59.26Aspect-Level 81.61 72.85Frame-Level 81.61 72.85S(s)Majority 50.84 22.47Graph Prop 48.83 35.403-nn 54.85 45.51Aspect-Level 61.54 53.88Frame-Level 61.54 53.88Table 5: Detailed breakdown of results on the de-velopment set using accuracy and average F1 overthe three class labels (+,-,=).Algorithm Acc.
Avg F1Graph Prop 58.81 41.463-nn 63.71 47.30Aspect-Level 67.93 53.17Frame-Level 68.26 53.50Table 6: Performance on the test set.
Results areaveraged over the different aspects.1.0 0.5 0.0 0.5 1.0Democrat1.00.50.00.51.0Republicanlawsuitsfundingbudget dealtax proposalabortionelephant mccainnancy pelosidelaying tacticsbacklashbiasmitt romneynrathe proposaljudicial nomineesstate departmentbill clintontheir principlesaidbig businessobamacaremarketrenominationgay marriagetraditionhealth care billbusinessthe pipeline tax cutsprinciplessmall businessesveto threatboehnerthe dream actgeorge w. bushideabusinessesbudget proposaltax increasespropositionspalinbarack obamathe allegationsenvironmentconstitutionkerrytax dealjobs billsmedicaregop leadershiphealthfloor voteunionsbudget cutsgun controlFigure 4: Average sentiment of Democrats and Re-publicans (as subjects) to selected nouns (as theirobjects), aggregated over a large corpus using thelearned lexicon (?4.2).
The line indicates identi-cal sentiments, i.e.
Republicans are more positivetowards the nouns that are above the line.and verbs as identified by the BBN Serif sys-tem, obtaining 1.2 billion unique tuples of theform (url,subject,verb,object,count).We also ex-tracted subject-verb-object tuples from news arti-cles found in the Annotated English Gigaword Cor-pus (Napoles et al, 2012), which contains nearly10 million articles.
From the Gigaword corpus weextracted a further 120 million unique tuples.Estimating Entity Polarities Using connotationframes, we can also measure entity-to-entity sen-timent at a large scale.
Figure 4, for example,presents the polarity of entities ?Democrats?
and?Republicans?
towards a selected set of nouns, bycomputing the average estimated polarity (usingour lexicon) over triples where one of these entitiesappears as part of the subject (e.g.
?Democrats?
or?Republican party?).
Apart from nouns that bothentities are positive (?business?, ?constitution?)
ornegative (?the allegations?,?veto threat?)
towards,we can also see interesting examples in whichDemocrats feel more positively (below the line:?nancy pelosi?, ?unions?, ?gun control?, etc.)
andones where Republicans are more positive (?thepipeline?, ?gop leaders?, ?budget cuts?, etc.)
Also,both entities are neutral towards ?idea?
and ?theproposal?, which probably owes to the fact thatideas or proposals can be good or bad for eitherentity depending on the context.3185 Related WorkMost prior work on sentiment lexicons focusedon the overall polarity of words without takinginto account their semantic arguments (Wilson etal., 2005; Baccianella et al, 2010; Wiebe et al,2005; Velikovich et al, 2010; Kaji and Kitsure-gawa, 2007; Kamps et al, 2004; Takamura etal., 2005; Adreevskaia and Bergler, 2006).
Sev-eral recent studies began exploring more specificand nuanced aspects of sentiment such as connota-tion (Feng et al, 2013), good and bad effects (Choiand Wiebe, 2014), and evoked sentiment (Moham-mad and Turney, 2010).
Drawing inspirations fromthem, we present connotation frames as a unifyingrepresentation framework to encode the rich di-mensions of implied sentiment, presupposed valuejudgements, and effect evaluation, and propose afactor graph formulation that captures the interplayamong different types of connotation relations.Goyal et al (2010a; 2010b) investigated howcharacters (protagonists, villains, victims) in chil-dren?s stories are affected by certain predicates,which is related to the effect relations studied in thiswork.
While Klenner et al (2014) similarly investi-gated the relation between the polarity of the verbsand arguments, our work introduces new perspec-tive types and proposes a unified representation andinference model.
Wiegand and Ruppenhofer (2015)also looked at perspective-based relationships in-duced by verb predicates with a focus on opinionroles.
Building on this concept, our frameworkalso incorporates information about the perspec-tives?
polarities as well as information about othertyped relations.
There have been growing interestsfor modeling framing (Greene and Resnik, 2009;Hasan and Ng, 2013), biased language (Recasenset al, 2013) and ideology detection (Yano et al,2010).
All these tasks are relatively less studied,and we hope our connotation frame lexicon will beuseful for them.Sentiment inference rules have been exploredby the recent work of Wiebe and Deng (2014) andDeng and Wiebe (2014).
In contrast, we makea novel conceptual connection between inferredsentiments and frame semantics, organized as con-notation frames, and present a unified model that in-tegrates different aspects of the connotation frames.Finally, in a broader sense, what we study as con-notation frames draws a connection to schema andscript theory (Schank and Abelson, 1975).
Unlikemost prior work that focused on directly observableactions (Chambers and Jurafsky, 2009; Frermannet al, 2014; Bethard et al, 2008), we focus onimplied sentiments that are framed by predicateverbs.6 ConclusionIn this paper, we presented a novel system ofconnotative frames that define a set of impliedsentiment and presupposed facts for a predi-cate.
Our work also empirically explores differ-ent methods of inducing and modelling these con-notation frames, incorporating the interplay be-tween relations within frames.
Our work sug-gests new research avenues on learning connota-tion frames, and their applications to deeper under-standing of social and political discourse.
All thelearned connotation frames and annotations will beshared at http://homes.cs.washington.edu/?hrashkin/connframe.html.AcknowledgementsWe thank the anonymous reviewers for many in-sightful comments.
We also thank members ofUW NLP for discussions and support.
This mate-rial is based upon work supported by the NationalScience Foundation Graduate Research FellowshipProgram under Grant No.
DGE-1256082.
Thework is also supported in part by NSF grants IIS-1408287, IIS-1524371 and gifts by Google andFacebook.ReferencesAlina Adreevskaia and Sabine Bergler.
2006.
Miningwordnet for fuzzy sentiment: Sentiment tag extrac-tion from wordnet glosses.
In 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 209?216.Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.Subjectivity word sense disambiguation.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, volume 2,pages 190?199.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10).Collin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 17th international conference on Compu-tational linguistics, volume 1, pages 86?90.319Steven Bethard, William J Corvey, Sara Klingenstein,and James H Martin.
2008.
Building a corpusof temporal-causal structure.
In Proceedings ofthe Sixth International Conference on Language Re-sources and Evaluation (LREC?08).Center for Media and Democracy.
2012.Sourcewatch: Conservative news outlets.http://www.sourcewatch.org/index.php/Conservative_news_outlets.Center for Media and Democracy.
2013.Sourcewatch: Liberal news outlets.
http://www.sourcewatch.org/index.php/Liberal_news_outlets.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, volume 2 of ACL ?09,pages 602?610.Yoonjung Choi and Janyce Wiebe.
2014.
+/-effectwordnet: Sense-level lexicon acquisition foropinion inference.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1181?1191.
Associa-tion for Computational Linguistics, October.Lingjia Deng and Janyce Wiebe.
2014.
Sentimentpropagation via implicature constraints.
In Pro-ceedings of the Conference of the European Chap-ter of the Association for Computational Linguistics(EACL).Song Feng, Jun Seok Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dashof sentiment beneath the surface meaning.
In Pro-ceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), vol-ume 1, pages 1774?1784.
Association for Compu-tational Linguistics.Charles J. Fillmore.
1976.
Frame semantics and thenature of language.
In In Annals of the New YorkAcademy of Sciences: Conference on the Origin andDevelopment of Language and Speech, volume 280,pages 2032.Lea Frermann, Ivan Titov, and Manfred Pinkal.
2014.A hierarchical bayesian model for unsupervised in-duction of script knowledge.
In Proceedings of theConference of the European Chapter of the Associa-tion for Computational Linguistics.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In Second Joint Conference onLexical and Computational Semantics (*SEM), vol-ume 1, pages 241?247, June.Amit Goyal, Ellen Riloff, and Hal Daum?e, III.
2010a.Automatically producing plot unit representationsfor narrative text.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 77?86.Amit Goyal, Ellen Riloff, Hal Daum?e III, and NathanGilbert.
2010b.
Toward plot units: Automatic affectstate analysis.
In Proceedings of HLT/NAACL Work-shop on Computational Approaches to Analysis andGeneration of Emotion in Text (CAET).Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 503?511.Kazi Saidul Hasan and Vincent Ng.
2013.
Frame se-mantics for stance classification.
Proceedings of theSeventeenth Conference on Computational NaturalLanguage Learning (CONLL), pages 124?132.HWC Library.
2011.
Consider the Source: A ResourceGuide to Liberal, Conservative, and NonpartisanPeriodicals.
www.ccc.edu/colleges/washington/departments/Documents/PeriodicalsPov.pdf.
Compiled by HWCLibrarians in January 2011.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive col-lection of html documents.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL), pages1075?1083.Jaap Kamps, Maarten Marx, Robert J Mokken, andMaarten De Rijke.
2004.
Using wordnet to mea-sure semantic orientations of adjectives.
In Pro-ceedings of the Fourth International Conference onLanguage Resources and Evaluation(LREC?04), vol-ume 4, pages 1115?1118.TREC KBA.
2014.
Knowledge Base Accelera-tion Stream Corpus.
http://trec-kba.org/kba-stream-corpus-2014.shtml.Manfred Klenner, Michael Amsler, and Nora Hollen-stein.
2014.
Verb polarity frames: a new resourceand its application in target-specific polarity classi-fication.
In Proceedings of KONVENS 2014, pages106?115.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 302?308.Bing Liu and Lei Zhang.
2012.
A survey of opinionmining and sentiment analysis.
In Mining text data,pages 415?463.
Springer.320Amy Mitchell, Jeffrey Gottfried, JocelynKiley, and Katerina Eva Matsa.
2014.Political Polarization & Media Habits.www.journalism.org/2014/10/21/political-polarization-media-habits/.Produced by Pew Research Center in October, 2014.Saif M Mohammad and Peter D Turney.
2010.
Emo-tions evoked by common words and phrases: Usingmechanical turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Gener-ation of Emotion in Text, pages 26?34.
Associationfor Computational Linguistics.Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Pro-ceedings of the Joint Workshop on Automatic Knowl-edge Base Construction and Web-scale KnowledgeExtraction, pages 95?100.
Association for Computa-tional Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational linguistics,31(1):71?106.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Marta Recasens, Cristian Danescu-Niculescu-Mizil,and Dan Jurafsky.
2013.
Linguistic models for an-alyzing and detecting biased language.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1650?1659.Evan Sandhaus.
2008.
The new york times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,6(12):e26752.Roger C Schank and Robert P Abelson.
1975.
Scripts,plans, and knowledge.
Yale University.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.
Associationfor Computational Linguistics.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of wordsusing spin model.
In Proceedings of 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages327?335.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The viabil-ity of web-derived polarity lexicons.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 777?785.Janyce Wiebe and Lingjia Deng.
2014.
An account ofopinion implicatures.
CoRR, abs/1404.6491.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language resources and evalua-tion, 39(2-3):165?210.Michael Wiegand and Josef Ruppenhofer.
2015.
Opin-ion holder and target extraction based on the in-duction of verbal categories.
Proceedings of the2015 Conference on Computational Natural Lan-guage Learning (CoNLL), page 215.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on human language technology and empiri-cal methods in natural language processing, pages347?354.Tae Yano, Philip Resnik, and Noah A. Smith.
2010.Shedding (a thousand points of) light on biased lan-guage.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, CSLDAMT ?10,pages 152?158.321
