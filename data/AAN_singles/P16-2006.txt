Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 32?37,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIncremental Parsing with Minimal Features Using Bi-Directional LSTMJames Cross and Liang HuangSchool of Electrical Engineering and Computer ScienceOregon State UniversityCorvallis, Oregon, USA{crossj,liang.huang}@oregonstate.eduAbstractRecently, neural network approaches forparsing have largely automated the combi-nation of individual features, but still relyon (often a larger number of) atomic fea-tures created from human linguistic intu-ition, and potentially omitting importantglobal context.
To further reduce fea-ture engineering to the bare minimum, weuse bi-directional LSTM sentence repre-sentations to model a parser state withonly three sentence positions, which au-tomatically identifies important aspects ofthe entire sentence.
This model achievesstate-of-the-art results among greedy de-pendency parsers for English.
We also in-troduce a novel transition system for con-stituency parsing which does not requirebinarization, and together with the abovearchitecture, achieves state-of-the-art re-sults among greedy parsers for both En-glish and Chinese.1 IntroductionRecently, neural network-based parsers have be-come popular, with the promise of reducing theburden of manual feature engineering.
For ex-ample, Chen and Manning (2014) and subsequentwork replace the huge amount of manual fea-ture combinations in non-neural network efforts(Nivre et al, 2006; Zhang and Nivre, 2011) byvector embeddings of the atomic features.
How-ever, this approach has two related limitations.First, it still depends on a large number of care-fully designed atomic features.
For example, Chenand Manning (2014) and subsequent work such asWeiss et al (2015) use 48 atomic features fromZhang and Nivre (2011), including select third-order dependencies.
More importantly, this ap-proach inevitably leaves out some nonlocal in-formation which could be useful.
In particular,though such a model can exploit similarities be-tween words and other embedded categories, andlearn interactions among those atomic features, itcannot exploit any other details of the text.We aim to reduce the need for manual inductionof atomic features to the bare minimum, by us-ing bi-directional recurrent neural networks to au-tomatically learn context-sensitive representationsfor each word in the sentence.
This approach al-lows the model to learn arbitrary patterns from theentire sentence, effectively extending the general-ization power of embedding individual words tolonger sequences.
Since such a feature representa-tion is less dependent on earlier parser decisions,it is also more resilient to local mistakes.With just three positional features we can builda greedy shift-reduce dependency parser that is onpar with the most accurate parser in the publishedliterature for English Treebank.
This effort is sim-ilar in motivation to the stack-LSTM of Dyer et al(2015), but uses a much simpler architecture.We also extend this model to predict phrase-structure trees with a novel shift-promote-adjoinsystem tailored to greedy constituency parsing,and with just two more positional features (defin-ing tree span) and nonterminal label embeddingswe achieve the most accurate greedy constituencyparser for both English and Chinese.2 LSTM Position Featuresf1;b1w1;t1f2;b2w2;t2f3;b3w3;t3f4;b4w4;t4f5;b5w5;t5Figure 1: The sentence is modeled with an LSTMin each direction whose input vectors at each timestep are word and part-of-speech tag embeddings.32The central idea behind this approach is exploitingthe power of recurrent neural networks to let themodel decide what apsects of sentence context areimportant to making parsing decisions, rather thanrelying on fallible linguistic information (whichmoreover requires leaving out information whichcould be useful).
In particular, we model an in-put sentence using Long Short-Term Memory net-works (LSTM), which have made a recent resur-gence after being initially formulated by Hochre-iter and Schmidhuber (1997).The input at each time step is simply a vectorrepresenting the word, in this case an embeddingfor the word form and one for the part-of-speechtag.
These embeddings are learned from randominitialization together with other network param-eters in this work.
In our initial experiments, weused one LSTM layer in each direction (forwardand backward), and then concatenate the outputat each time step to represent that sentence posi-tion: that word in the entire context of the sen-tence.
This network is illustrated in Figure 1.h1f21;b21f11;b11w1;t1h2f22;b22f12;b12w2;t2h3f23;b23f13;b13w3;t3h4f24;b24f14;b14w4;t4h5f25;b25f15;b15w5;t5Figure 2: In the 2-Layer architecture, the outputof each LSTM layer is concatenated to create thepositional feature vector.It is also common to stack multiple such LSTMlayers, where the output of the forward and back-ward networks at one layer are concatenated toform the input to the next.
We found that parsingperformance could be improved by using two bi-directional LSTM layers in this manner, and con-catenating the output of both layers as the posi-tional feature representation, which becomes theinput to the fully-connected layer.
This architec-input: w0.
.
.
wn?1axiom ?, 0?
: ?shift?S, j?
: A?S|j, j + 1?
: Aj < nrex?S|s1|s0, j?
: A?S|s0, j?
: A ?
{s1xs0}goal ?s0, n?
: AFigure 3: The arc-standard dependency parsingsystem (Nivre, 2008) (reyomitted).
Stack S isa list of heads, j is the start index of the queue,and s0and s1are the top two head indices on S.dependency constituencypositional s1, s0, q0s1, s0, q0, s1.left, s0.leftlabels - s0.
{left, right, root, head}s1.
{left, right, root, head}Table 1: Feature templates.
Note that, remarkably,even though we do labeled dependency parsing,we do not include arc label as features.ture is shown in Figure 2.Intuitively, this represents the sentence positionby the word in the context of the sentence up tothat point and the sentence after that point in thefirst layer, as well as modeling the ?higher-order?interactions between parts of the sentence in thesecond layer.
In Section 5 we report results us-ing only one LSTM layer (?Bi-LSTM?)
as well aswith two layers where output from each layer isused as part of the positional feature (?2-Layer Bi-LSTM?
).3 Shift-Reduce Dependency ParsingWe use the arc-standard system for dependencyparsing (see Figure 4).
By exploiting the LSTMarchitecture to encode context, we found that wewere able to achieve competitive results using onlythree sentence-position features to model parserstate: the head word of each of the top two treeson the stack (s0and s1), and the next word on thequeue (q0); see Table 1.The usefulness of the head words on the stackis clear enough, since those are the two words thatare linked by a dependency when taking a reduceaction.
The next incoming word on the queue isalso important because the top tree on the stackshould not be reduced if it still has children whichhave not yet been shifted.
That feature thus allows33input: w0.
.
.
wn?1axiom ?, 0?
: ?shift?S, j?
?S | j, j + 1?j < npro(X)?S | t, j?
?S | X(t), j?adjx?S | t | X(t1...tk), j?
?S | X(t, t1...tk), j?goal ?s0, n?Figure 4: Our shift-promote-adjoin system forconstituency parsing (adjyomitted).the model to learn to delay a right-reduce until thetop tree on the stack is fully formed, shifting in-stead.3.1 Hierarchical ClassificationThe structure of our network model after com-puting positional features is fairly straightforwardand similar to previous neural-network parsing ap-proaches such as Chen and Manning (2014) andWeiss et al (2015).
It consists of a multilayerperceptron using a single ReLU hidden layer fol-lowed by a linear classifier over the action space,with the training objective being negative log soft-max.We found that performance could be improved,however, by factoring out the decision over struc-tural actions (i.e., shift, left-reduce, or right-reduce) and the decision of which arc label to as-sign upon a reduce.
We therefore use separateclassifiers for those decisions, each with its ownfully-connected hidden and output layers but shar-ing the underlying recurrent architecture.
Thisstructure was used for the results reported in Sec-tion 5, and it is referred to as ?Hierarchical Ac-tions?
when compared against a single action clas-sifier in Table 3.4 Shift-Promote-AdjoinConstituency ParsingTo further demonstrate the advantage of our ideaof minimal features with bidirectional sentencerepresentations, we extend our work from depen-dency parsing to constituency parsing.
However,the latter is significantly more challenging than theformer under the shift-reduce paradigm because:SVPNP5sportsNNS63likeVBP47NP1IPRP29 81shift (I)6pro (NP)2pro (NP)7adjy3shift (like)8pro (S)4pro (VP)9adjx5shift (sports)Figure 5: Shift-Promote-Adjoin parsing example.Upward and downward arrows indicate promoteand (sister-)adjunction actions, respectively.?
we also need to predict the nonterminal labels?
the tree is not binarized (with many unaryrules and more than binary branching rules)While most previous work binarizes the con-stituency tree in a preprocessing step (Zhu etal., 2013; Wang and Xue, 2014; Mi and Huang,2015), we propose a novel ?Shift-Promote-Adjoin?
paradigm which does not require any bi-nariziation or transformation of constituency trees(see Figure 5).
Note in particular that, in ourcase only the Promote action produces a new treenode (with a non-terminal label), while the Ad-join action is the linguistically-motivated ?sister-adjunction?
operation, i.e., attachment (Chiang,2000; Henderson, 2003).
By comparison, in pre-vious work, both Unary-X and Reduce-L/R-X ac-tions produce new labeled nodes (some of whichare auxiliary nodes due to binarization).
Thus ourparadigm has two advantages:?
it dramatically reduces the number of possi-ble actions, from 3X + 1 or more in previ-ous work to 3 + X , where X is the numberof nonterminal labels, which we argue wouldsimplify learning;?
it does not require binarization (Zhu et al,2013; Wang and Xue, 2014) or compressionof unary chains (Mi and Huang, 2015)There is, however, a more closely-related ?shift-project-attach?
paradigm by Henderson (2003).For the example in Figure 5 he would use the fol-lowing actions:shift(I), project(NP), project(S), shift(like),project(VP), shift(sports), project(NP), attach,attach.34The differences are twofold: first, our Promote ac-tion is head-driven, which means we only promotethe head child (e.g., VP to S) whereas his Projectaction promotes the first child (e.g., NP to S); andsecondly, as a result, his Attach action is alwaysright-attach whereas our Adjoin action could be ei-ther left or right.
The advantage of our method isits close resemblance to shift-reduce dependencyparsing, which means that our constituency parseris jointly performing both tasks and can produceboth kinds of trees.
This also means that we usehead rules to determine the correct order of goldactions.We found that in this setting, we did needslightly more input features.
As mentioned, nodelabels are necessary to distinguish whether a treehas been sufficiently promoted, and are helpful inany case.
We used 8 labels: the current and im-mediate predecessor label of each of the top twostacks on the tree, as well as the label of the left-and rightmost adjoined child for each tree.
We alsofound it helped to add positional features for theleftmost word in the span for each of those trees,bringing the total number of positional features tofive.
See Table 1 for details.5 Experimental ResultsWe report both dependency and constituency pars-ing results on both English and Chinese.All experiments were conducted with minimalhyperparameter tuning.
The settings used forthe reported results are summarized in Table 6.Networks parameters were updated using gradi-ent backpropagation, including backpropagationthrough time for the recurrent components, usingADADELTA for learning rate scheduling (Zeiler,2012).
We also applied dropout (Hinton et al,2012) (with p = 0.5) to the output of each LSTMlayer (separately for each connection in the case ofthe two-layer network).We tested both types of parser on the Penn Tree-bank (PTB) and Penn Chinese Treebank (CTB-5),with the standard splits for each of training, de-velopment, and test sets.
Automatically predictedpart of speech tags with 10-way jackknifing wereused as inputs for all tasks except for Chinese de-pendency parsing, where we used gold tags, fol-lowing the traditions in literature.5.1 Dependency Parsing: English & ChineseTable 2 shows results for English Penn Tree-bank using Stanford dependencies.
Despite theminimally designed feature representation, rela-tively few training iterations, and lack of pre-computed embeddings, the parser performed onpar with state-of-the-art incremental dependencyparsers, and slightly outperformed the state-of-the-art greedy parser.The ablation experiments shown in the Table 3indicate that both forward and backward contextsfor each word are very important to obtain strongresults.
Using only word forms and no part-of-speech input similarly degraded performance.ParserDev TestUAS LAS UAS LASC & M 2014 92.0 89.7 91.8 89.6Dyer et al 2015 93.2 90.9 93.1 90.9Weiss et al 2015 - - 93.19 91.18+ Percept./Beam - - 93.99 92.05Bi-LSTM 93.31 91.01 93.21 91.162-Layer Bi-LSTM 93.67 91.48 93.42 91.36Table 2: Development and test set results for shift-reduce dependency parser on Penn Treebank usingonly (s1, s0, q0) positional features.Parser UAS LASBi-LSTM Hierarchical?93.31 91.01?
- Hierarchical Actions 92.94 90.96?
- Backward-LSTM 91.12 88.72?
- Forward-LSTM 91.85 88.39?
- tag embeddings 92.46 89.81Table 3: Ablation studies on PTB dev set (wsj22).
Forward and backward context, and part-of-speech input were all critical to strong performace.Figure 6 compares our parser with that of Chenand Manning (2014) in terms of arc recall for var-ious arc lengths.
While the two parsers performsimilarly on short arcs, ours significantly outpe-forms theirs on longer arcs, and more interestinglyour accuracy does not degrade much after length6.
This confirms the benefit of having a globalsentence repesentation in our model.Table 4 summarizes the Chinese dependencyparsing results.
Again, our work is competitivewith the state-of-the-art greedy parsers.351 2 3 4 5 6 7 8 9 10 11 12 13 14 >14Arc Length0.750.800.850.900.95RecallBi-LSTM (this work)Chen and ManningFigure 6: Recall on dependency arcs of variouslengths in PTB dev set.
The Bi-LSTM parser isparticularly good at predicting longer arcs.ParserDev TestUAS LAS UAS LASC & M 2014 84.0 82.4 83.9 82.4Dyer et al 2015 87.2 85.9 87.2 85.7Bi-LSTM 85.84 85.24 85.53 84.892-Layer Bi-LSTM 86.13 85.51 86.35 85.71Table 4: Development and test set results for shift-reduce dependency parser on Penn Chinese Tree-bank (CTB-5) using only (s1, s0, q0) position fea-tures (trained and tested with gold POS tags).5.2 Constituency Parsing: English & ChineseTable 5 compares our constituency parsing re-sults with state-of-the-art incremental parsers.
Al-though our work are definitely less accurate thanthose beam-search parsers, we achieve the highestaccuracy among greedy parsers, for both Englishand Chinese.1,2Parser bEnglish Chinesegreedy beam greedy beamZhu et al (2013) 16 86.08 90.4 75.99 85.6Mi & Huang (05) 32 84.95 90.8 75.61 83.9Vinyals et al (05) 10 - 90.5 - -Bi-LSTM - 89.75 - 79.44 -2-Layer Bi-LSTM - 89.95 - 80.13 -Table 5: Test F-scores for constituency parsing onPenn Treebank and CTB-5.1The greedy accuracies for Mi and Huang (2015) are fromHaitao Mi, and greedy results for Zhu et al (2013) come fromduplicating experiments with code provided by those authors.2The parser of Vinyals et al (2015) does not use an ex-plicit transition system, but is similar in spirit since generat-ing a right bracket can be viewed as a reduce action.Dependency ConstituencyEmbeddingsWord (dims) 50 100Tags (dims) 20 100Nonterminals (dims) - 100Pretrained No NoNetwork detailsLSTM units (each direction) 200 200ReLU hidden units 200 / decision 1000TrainingTraining epochs 10 10Minibatch size (sentences) 10 10Dropout (LSTM output only) 0.5 0.5L2 penalty (all weights) none 1?
10?8ADADELTA ?
0.99 0.99ADADELTA  1?
10?71?
10?7Table 6: Hyperparameters and training settings.6 Related WorkBecause recurrent networks are such a natural fitfor modeling languages (given the sequential na-ture of the latter), bi-directional LSTM networksare becoming increasingly common in all sortsof linguistic tasks, for example event detection inGhaeini et al (2016).
In fact, we discovered aftersubmission that Kiperwasser and Goldberg (2016)have concurrently developed an extremely similarapproach to our dependency parser.
Instead of ex-tending it to constituency parsing, they also applythe same idea to graph-based dependency parsing.7 ConclusionsWe have presented a simple bi-directional LSTMsentence representation model for minimal fea-tures in both incremental dependency and incre-mental constituency parsing, the latter using anovel shift-promote-adjoint algorithm.
Experi-ments show that our method are competitive withthe state-of-the-art greedy parsers on both parsingtasks and on both English and Chinese.AcknowledgmentsWe thank the anonymous reviewers for comments.We also thank Taro Watanabe, Muhua Zhu, andYue Zhang for sharing their code, Haitao Mi forproducing greedy results from his parser, andAshish Vaswani and Yoav Goldberg for discus-sions.
The authors were supported in part byDARPA FA8750-13-2-0041 (DEFT), NSF IIS-1449278, and a Google Faculty Research Award.36ReferencesDanqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Empirical Methods in Natural LanguageProcessing (EMNLP).David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree-adjoining grammar.
InProc.
of ACL.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
arXiv preprint arXiv:1505.08075.Reza Ghaeini, Xiaoli Z. Fern, Liang Huang, and PrasadTadepalli.
2016.
Event nugget detection withforward-backward recurrent neural networks.
InProc.
of ACL.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Pro-ceedings of NAACL.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.CoRR, abs/1207.0580.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Eliyahu Kiperwasser and Yoav Goldberg.
2016.
Sim-ple and accurate dependency parsing using bidi-rectional LSTM feature representations.
CoRR,abs/1603.04351.Haitao Mi and Liang Huang.
2015.
Shift-reduce con-stituency parsing with dynamic programming andpos tag lattice.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proc.
of LREC.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Gram-mar as a foreign language.
In Advances in NeuralInformation Processing Systems, pages 2755?2763.Zhiguo Wang and Nianwen Xue.
2014.
Joint postagging and transition-based constituent parsing inchinese with non-local features.
In Proceedings ofACL.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of ACL.Matthew D. Zeiler.
2012.
ADADELTA: an adaptivelearning rate method.
CoRR, abs/1212.5701.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL, pages 188?193.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of ACL2013.37
