Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 24?29,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 06: Word-Sense Disambiguation of PrepositionsKen LitkowskiCL Research9208 Gue RoadDamascus, MD 20872ken@clres.comOrin Hargraves5130 Band Hall Hill RoadWestminster, MD 21158orinhargraves@googlemail.comAbstractThe SemEval-2007 task to disambiguateprepositions was designed as a lexical sampletask.
A set of over 25,000 instances wasdeveloped, covering 34 of the most frequentEnglish prepositions, with two-thirds of theinstances for training and one-third as the testset.
Each instance identified a preposition to betagged in a full sentence taken from theFrameNet corpus (mostly from the BritishNational Corpus).
Definitions from the OxfordDictionary of English formed the senseinventories.
Three teams participated, with allachieving supervised results significantlybetter than baselines, with a high fine-grainedprecision of 0.693.
This level is somewhatsimilar to results on lexical sample tasks withopen class words, indicating that significantprogress has been made.
The data generated inthe task provides ample opportunitites forfurther investigations of preposition behavior.1 IntroductionThe SemEval-2007 task to disambiguate prepositionswas designed as a lexical sample task to investigatethe extent to which an important  closed class ofwords could be disambiguated.
In addition, becausethey are a closed class, with stable senses, therequisite datasets for this task are enduring and canbe used as long as the problem of prepositiondisambiguation remains.
The data used in this taskwas developed in The Preposition Project (TPP,Litkowski & Hargraves (2005) and Litkowski &Hargraves (2006)),1with further refinements to fitthe requirements of a SemEval task.In the following sections, we first describe themotivations for a preposition disambiguation task.Next, we describe the development of the datasetsused for the task, i.e., the instance sets and the senseinventories.
We describe how the task was performedand how it was evaluated (essentially using the samescoring methods as previous Senseval lexical sampletasks).
We present the results obtained from theparticipating teams and provide an initial analysis ofthese results.
Finally, we identify several furthertypes of analyses that will provide further insightsinto the characterization of preposition behavior.2 MotivationPrepositions are a closed class, meaning that thenumber of prepositions remains relatively constantand that their meanings are relatively stable.
Despitethis, their treatment in computational linguistics hasbeen somewhat limited.
In the Penn Treebank, onlytwo types of prepositions are recognized (IN(locative, temporal, and manner) and TO (direction))(O?Hara, 2005).
Prepositions are viewed as functionwords that occur with high frequency and thereforecarry little meaning.
A task to disambiguateprepositions would, in the first place, allow thislimited treatment to be confronted more fully.Preposition behavior has been the subject ofmuch research, too voluminous to cite here.
Threerecent workshops on prepositions have beensponsored by the ACL-SIGSEM: Toulouse in 2003,Colchester in 2005, and Trento in 2006.
For the mostpart, these workshops have focused on individualprepositions, with various investigations of moregeneralized behavior.
The SemEval prepositiondisambiguation task provides a vehicle to examinewhether these behaviors are substantiated with awell-defined set of corpus instances.Prepositions assume more importance when they1http://www.clres.com/prepositions.html.24are considered in relation to verbs.
While linguistictheory focuses on subjects and objects as importantverb arguments, quite frequently there is anadditional oblique argument realized in aprepositional phrase.
But with the focus on the verbs,the prepositional phrases do not emerge as havingmore than incidental importance.
However, withinframe semantics (Fillmore, 1976), prepositions riseto a greater prominence; frequently, two or threeprepositional phrases are identified as constitutingframe elements.
In addition, frame semantic analysesindicate the possibility of a greater number ofprepositional phrases acting as adjuncts (particularlyidentifying time and location frame elements).
Whilelinguistic theories may identify only one or twoprepositions associated with an argument of a verb,frame semantic analyses bring in the possibility of agreater variety of prepositions introducing the sametype of frame element.
The prepositiondisambiguation task provides an opportunity toexamine this type of variation.The question of prepositional phrase attachmentis another important issue.
Merlo & Esteve Ferrer(2006) suggest that this problem is a four-waydisambiguation task, depending on the properties ofnouns and verbs and whether the prepositionalphrases are arguments or adjuncts.
Their analysisrelied on Penn Treebank data.
Further insights maybe available from the finer-grained data available inthe preposition disambiguation task.Another important thread of investigationconcerning preposition behavior is the task ofsemantic role (and perhaps semantic relation)labeling (Gildea & Jurafsky, 2002).
This task hasbeen the subject of a previous Senseval task(Automatic Semantic Role Labeling, Litkowski(2004)) and two shared tasks on semantic rolelabeling in the Conference on Natural LanguageLearning (Carreras & Marquez (2004) and Carreras& Marquez (2005)).
In addition, three other tasks inSemEval-2007 (semantic relations between nominals,task 4; temporal relation labeling, task 15; and framesemantic structure extraction, task 19) address issuesof semantic role labeling.
Since a great proportion ofthese semantic roles are realized in prepositionalphrases, this gives greater urgency to understandingpreposition behavior.Despite the predominant view of prepositions asfunction words carrying little meaning, this view isnot borne out in dictionary treatment of theirdefinitions.
To all appearances, prepositions exhibitdefinitional behavior similar to that of open classwords.
There is a reasonably large number of distinctprepositions and they show a range of polysemoussenses.
Thus, with a suitable set of instances, theymay be amenable to the same types of analyses asopen class words.3 Preparation of DatasetsThe development of the datasets for the prepositiondisambiguation task grew directly out of TPP.
Thisproject essentially articulates the corpus selection, thelexicon choice, and the production of the goldstandard.
The primary objective of TPP is tocharacterize each of 847 preposition senses for 373prepositions (including 220 phrasal prepositions with309 senses)2with a semantic role name and thesyntactic and semantic properties of its complementand attachment point.
The preposition senseinventory is taken from the Oxford Dictionary ofEnglish (ODE, 2004).33.1 Corpus DevelopmentFor a particular preposition, a set of instances isextracted from the FrameNet database.4FrameNetwas chosen since it provides well-studied sentencesdrawn from the British National Corpus (as well asa limited set of sentences from other sources).
Sincethe sentences to be selected for frame analysis weregenerally chosen for some open class verb or noun,these sentences would be expected to provide no biaswith respect to prepositions.
In addition, the use ofthis resource makes available considerableinformation for each sentence in its identification of2The number of prepositions and the number of sensesis not fixed, but has changed during the course of theproject, as will become clear.3TPP does not include particle senses of such words asin or over (or any other particles) used with verbs tomake phrasal verbs.
In this context, phrasal verbs areto be distinguished from verbs that select a preposition(such as on in rely on), which may be characterized asa collocation.4http://framenet.icsi.berkeley.edu/25frame elements, their phrase type, and theirgrammatical function.
The FrameNet data was alsomade accessible in a form (FrameNet Explorer)5tofacilitate a lexicographer?s examination ofpreposition instances.Each sentence in the FrameNet data is labeledwith a subcorpus name.
This name is generallyintended only to capture some property of a set ofinstances.
In particular, many of these subcorpusnames include a string ppprep and this identificationwas used for the selection of instances.
Thus,searching the FrameNet corpus for subcorporalabeled ppof or ppafter would yield sentencescontaining a prepositional phrase with a desiredpreposition.
This technique was used for manycommon prepositions, yielding 300 to 4500instances.
The technique was modified forprepositions with fewer instances.
Instead, allsentences having a phrase beginning with a desiredpreposition were selected.The number of sentences eventually used in theSemEval task is shown in Table 1.
More than 25,000instances for 34 prepositions were tagged in TPP andused for the SemEval-2007 task.3.2 Lexicon DevelopmentAs mentioned above, ODE (and its predecessor, theNew Oxford Dictionary of English (NODE, 1997))was used as the sense inventory for the prepositions.ODE is a corpus-based, lexicographically-drawnsense inventory, with a two-level hierarchy,consisting of a set of core senses and a set ofsubsenses (if any) that are semantically related to thecore sense.
The full set of information, both printedand in electronic form, containing additionallexicographic information, was made publiclyavailable for TPP, and hence, the SemEvaldisambiguation task.The sense inventory was not used as absolute andfurther information was added during TPP.
Thelexicographer (Hargraves) was free to add senses,particularly as the corpus evidence provided by theFrameNet data suggested.
The process of refining thesense inventory was performed as the lexicographerassigned a sense to each instance.
While engaged inthis sense assignment, the lexicographer accumulatedan understanding of the behavior of the preposition,assigning a name to each sense (characterizing itssemantic type), and characterizing the syntactic andsemantic properties of the preposition complementand its point of attachment or head.
Each sense wasalso characterized by its syntactic function and itsmeaning, identifying the relevant paragraph(s) whereit is discussed in Quirk et al(1985).After sense assignments were completed, the setof instances for each preposition was analyzedagainst the FrameNet database.
In particular, theFrameNet frames and frame elements associated witheach sense was identified.
The set of sentences wasprovided in SemEval format in an XML file with thepreposition tagged as <head>, along with an answerkey (also identifying the FrameNet frame and frameelement).
Finally, using the FrameNet frame andframe element of the tagged instances, syntacticalternation patterns (other syntactic forms in whichthe semantic role may be realized) are provided foreach FrameNet target word for each sense.All of the above information was combined intoa preposition database.6For SemEval-2007, entriesfor the target prepositions were combined into anXML file as the ?Definitions?
to be used as the senseinventory, where each sense was given a uniqueidentifier.
All prepositions for which a set ofinstances had been analyzed in TPP were included.These 34 prepositions are shown in Table 1 (below,beyond, and near were used in the trial set).3.3 Gold Standard ProductionUnlike previous Senseval lexical sample tasks,tagging was not performed as a separate step.
Rather,sense tagging was completed as an integral part ofTPP.
Funding was unavailable to perform additionaltagging with other lexicographers and the appropriateinterannotator agreement studies have not yet beencompleted.
At this time, only qualitative assessmentsof the tagging can be given.As indicated, the sense inventory for eachpreposition evolved as the lexicographer examined5Available for the Windows operating system athttp://www.clres.com for those with access to theFrameNet data.6The full database is viewable in the Online TPP(http://www.clres.com/cgi-bin/onlineTPP/find_prep.cgi).26the set of FrameNet instances.
Multiple sources (suchas Quirk et al) and lexicographic experience wereimportant components of the sense tagging.
Thetagging was performed without any deadlines andwith full adherence to standard lexicographicprinciples.
Importantly, the availability of theFrameNet corpora facilitated the sense assignment,since many similar instances were frequentlycontiguous in the instance set (e.g., associated withthe same target word and frame).Another important factor suggesting higherquality in the sense assignment is the quality of thesense inventory.
Unlike previous Senseval lexicalsample tasks, the sense inventory was developedusing lexicographic principles and was quite stable.In arriving at the sense inventory, the lexicographerwas able to compare ODE with its predecessorNODE, noting in most cases that the senses had notchanged or had changed in only minor ways.Finally, the lexicographer had little difficulty inmaking sense assignments.
The sense distinctionswere well enough drawn that there was relativelylittle ambiguity given a sentence context.
Thelexicographer was not constrained to selecting onesense, but could tag a preposition with multiplesenses as deemed necessary.
Out of 25,000 instances,only 350 instances received multiple senses.4 Task Organization and EvaluationThe organization followed standard SemEval(Senseval) procedures.
The data were prepared inXML, using Senseval DTDs.
That is, each instancewas labeled with an instance identifier as an XMLattribute.
Within the <instance> tag, the FrameNetsentence was labeled as the <context> and includedone item, the target preposition, in the <head> tag.The FrameNet sentence identifier was used as theinstance identifier, enabling participants to make useof other FrameNet data.
Unlike lexical sample tasksfor open class words, only one sentence was providedas the context.
Although no examination of whetherthis is sufficient context for prepositions, it seemslikely that all information necessary for prepositiondisambiguation is contained in the local context.A trial set of three prepositions was provided (thethree smallest instance sets that had been developed).For each of the remaining 34 prepositions, the datawas split in a ratio of two to one between training andtest data.
The training data included the senseidentifier.
Table 1 shows the total number ofinstances for each preposition, along with the numberin the training and the test sets.Answers were submitted in the standard Sensevalformat, consisting of the lexical item name, theinstance identifier, the system sense assignments, andoptional comments.
Although participants were notrestricted to selecting only one sense, all did so anddid not provide either multiple senses or weighting ofdifferent senses.
Because of this, a simple Perl scriptwas used to score the results, giving precision, recall,and F-score.7The answers were also scored using thestandard Senseval scoring program, which records aresult for ?attempted?
rather than F-score, withprecision interpreted as percent of attemptedinstances that are correct and recall as percent oftotal instances that are correct.8Table 1 reports thestandard SemEval recall, while Tables 2 and 3 usethe standard notions of precision and recall.5 ResultsTables 2 and 3 present the overall fine-grained andcoarse-grained results, respectively, for the threeparticipating teams (University of Melbourne, Ko?University, and Instituto Trentino di Cultura, IRST).The tables show the team designator, and the resultsover all prepositions, giving the precision, the recall,and the F-score.
The table also shows the results fortwo baselines.
The FirstSense baseline selects thefirst sense of each preposition as the answer (underthe assumption that the senses are organizedsomewhat according to prominence).
The FreqSensebaseline selects the most frequent sense from thetraining set.
Table 1 shows the fine-grained recallscores for each team for each preposition.
Table 1also shows the entropy and perplexity for eachpreposition, based on the data from the training sets.7Precision is the percent of total correct instances andrecall is the percent of instances attempted, so that anF-score can be computed.8The standard SemEval (Senseval) scoring program,scorer2, does not work to compute a coarse-grainedscore for the preposition instances, since senses arenumbers such as ?4(2a)?
and not alphabetic.27Table 2.
Fine-Grained Scores(All Prepositions - 8096 Instances)Team Prec Rec FMELB-YB 0.693 1.000 0.818KU 0.547 1.000 0.707IRST-BP 0.496 0.864 0.630FirstSense 0.289 1.000 0.449FreqSense 0.396 1.000 0.568Table 3.
Coarse-Grained Scores(All Prepositions - 8096 Instances)Team Prec Rec FMELB-YB 0.755 1.000 0.861KU 0.642 1.000 0.782IRST-BP 0.610 0.864 0.715FirstSense 0.441 1.000 0.612FreqSense 0.480 1.000 0.649As can be seen, all participating teams performedsignificantly better than the baselines.
Additionalimprovements occurred at the coarse grain, althoughthe differences are not dramatically higher.All participating teams used supervised systems,using the training data for their submissions.
TheUniversity of Melbourne used a maximum entropysystem using a wide variety of syntactic and semanticfeatures.
Ko?
University used a statistical languagemodel (based on Google ngram data) to measure thelikelihood of various substitutes for various senses.IRST-BP used Chain Clarifying Relationships, inwhich contextual lexical and syntactic features ofrepresentative contexts are used for learning sensediscriminative patterns.
Further details on theirmethods are available in their respective papers.6 DiscussionExamination of the detailed results by preposition inTable 1 shows that performance is inversely relatedto polysemy.
The greater number of senses leads toreduced performance.
The first sense heuristic has acorrelation of -0.64; the most frequent sense heuristichas a correlation of -0.67. the correlations forMELB, KU, and IRST are -0.40, -0.70, and -0.56,respectively.
The scores are also negativelycorrelated with the number of test instances.
Thecorrelations are -0.34 and -0.44 for the first senseand the most frequent sense heuristics.
For thesystems, the scores are -0.17, -0.48, and -0.39 forMelb, KU, and IRST.The scores for each preposition are stronglynegatively correlated with entropy and perplexity, asfrequently observed in lexical sample disambiguation.For MELB-YB and IRST-BP, the correlation withentropy is about -0.67, while for KU, the correlationis -0.885.
For perplexity, the correlation is -0.55 forMELB-YB, -0.62 for IRST-ESP , and -0.82 for KU.More detailed analysis is required to examine theperformance for each preposition, particularly for themost frequent prepositions (of, in, from, with, to, for,on, at, into, and by).
Performance on theseprepositions ranged from fairly good to mediocre torelatively poor.
In addition, a comparison of thevarious attributes of the TPP sense information withthe different performances might be fruitful.
Little ofthis information was used by the various systems.7 ConclusionsThe SemEval-2007 preposition disambiguation taskcan be considered successful, with results that can beexploited in general NLP tasks.
In addition, the taskhas generated considerable information for furtherexamination of preposition behavior.ReferencesXavier Carreras and Lluis Marquez.
2004.Introduction to the CoNLL-2004 Shared Task:Semantic Role Labeling.
In: Proceedings ofCoNLL-2004.Xavier Carreras and Lluis Marquez.
2005.Introduction to the CoNLL-2005 Shared Task:Semantic Role Labeling.
In: Proceedings ofCoNLL-2005.Charles Fillmore.
1976.
Frame Semantics and theNature of Language.
Annals of the New YorkAcademy of Sciences, 280: 20-32.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
ComputationalLinguistics, 28 (3), 245-288.Kenneth C. Litkowski.
2004.
Senseval-3 Task:Automatic Labeling of Semantic Roles.
InSenseval-3: Third International Workshop on theEvaluation of Systems for the Semantic Analysis ofText.
ACL.
9-12.Kenneth C. Litkowski & Orin Hargraves.
2005.
ThePreposition Project.
In: ACL-SIGSEM Workshopon the Linguistic Dimensions of Prepositions andtheir Use in Computational Linguistic Formalisms28and Applications, University of Essex -Colchester, United Kingdom.
171-179.Kenneth C. Litkowski.& Orin Hargraves.
2006.Coverage and Inheritance in The PrepositionProject.
In: Proceedings of the Third ACL-SIGSEM Workshop on Prepositions.
Trento, Italy.ACL.
89-94.Paola Merlo and Eva Esteve Ferrer.
2006.
The Notionof Argument in Prepositional Phrase Attachment.Computational Linguistics, 32 (3), 341-377.The New Oxford Dictionary of English.
1998.
(J.Pearsall, Ed.).
Oxford: Clarendon Press.Thomas P. O?Hara.
2005.
Empirical Acquisition ofConceptual Distinctions via DictionaryDefinitions.
Ph.D. Thesis.
New Mexico State .The Oxford Dictionary of English.
2003.
(A.Stevension and C. Soanes, Eds.).
Oxford:Clarendon Press.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,& Jan Svartik.
(1985).
A comprehensive grammarof the English language.
London: Longman.Table 1.
SemEval-2007 Preposition DisambiguationPrepostition Senses Ent PerpNumber of InstancesFine-Grained RecallParticipating Teams BaselinesTotal Training Test Melb KU IRSTFirstSenseFreqSenseabout 6 0.63 1.54 1074 710 364 0.885 0.934 0.780 0.885 0.885above 9 1.80 3.49 71 48 23 0.652 0.522 0.565 0.043 0.609across 3 0.23 1.17 470 319 151 0.960 0.960 0.914 0.960 0.960after 11 2.15 4.44 156 103 53 0.472 0.585 0.585 0.434 0.434against 10 1.89 3.69 287 195 92 0.880 0.793 0.826 0.446 0.435along 4 0.30 1.23 538 365 173 0.954 0.954 0.936 0.954 0.954among 4 1.55 2.93 150 100 50 0.660 0.680 0.620 0.300 0.300around 6 2.05 4.13 490 335 155 0.561 0.535 0.381 0.155 0.452as 2 0.00 1.00 258 174 84 1.000 1.000 0.988 1.000 1.000at 12 2.38 5.21 1082 715 367 0.790 0.662 0.646 0.425 0.425before 4 1.33 2.51 67 47 20 0.600 0.850 0.800 0.450 0.450behind 9 1.31 2.47 206 138 68 0.662 0.676 0.471 0.662 0.662beneath 6 1.22 2.33 85 57 28 0.714 0.679 0.750 0.571 0.571beside 3 0.00 1.00 91 62 29 1.000 1.000 1.000 1.000 1.000between 9 2.11 4.31 313 211 102 0.814 0.765 0.892 0.422 0.422by 22 2.53 5.77 758 510 248 0.730 0.556 0.391 0.000 0.371down 5 1.18 2.26 485 332 153 0.654 0.647 0.680 0.438 0.438during 2 1.00 2.00 120 81 39 0.769 0.564 0.667 0.615 0.385for 15 2.84 7.17 1429 951 478 0.573 0.395 0.456 0.036 0.238from 16 2.85 7.21 1784 1206 578 0.642 0.415 0.512 0.279 0.279in 15 2.81 7.01 2085 1397 688 0.561 0.436 0.494 0.362 0.362inside 5 1.63 3.10 105 67 38 0.579 0.579 0.605 0.368 0.526into 10 2.14 4.41 901 604 297 0.616 0.539 0.586 0.290 0.451like 7 1.26 2.40 391 266 125 0.856 0.808 0.592 0.120 0.768of 20 3.14 8.80 4482 3004 1478 0.681 0.374 0.144 0.000 0.205off 7 1.16 2.23 237 161 76 0.658 0.776 0.408 0.171 0.763on 25 3.42 10.68 1313 872 441 0.624 0.469 0.351 0.218 0.206onto 3 0.60 1.52 175 117 58 0.879 0.879 0.776 0.879 0.879over 17 2.52 5.73 298 200 98 0.510 0.510 0.480 0.010 0.327round 8 2.31 4.95 263 181 82 0.610 0.512 0.000 0.037 0.378through 16 2.71 6.54 649 441 208 0.524 0.538 0.481 0.322 0.495to 17 2.43 5.38 1755 1183 572 0.745 0.579 0.558 0.322 0.322towards 6 0.71 1.63 316 214 102 0.931 0.873 0.833 0.873 0.873with 18 3.05 8.27 1769 1191 578 0.699 0.455 0.635 0.149 0.249Total 332 24653 16557 8096 0.693 0.547 0.496 0.289 0.39629
