Practical Grammar-Based NLG from ExamplesDavid DeVault and David Traum and Ron ArtsteinUSC Institute for Creative Technologies13274 Fiji WayMarina del Rey, CA 90292{devault,traum,artstein}@ict.usc.eduAbstractWe present a technique that opens upgrammar-based generation to a wider rangeof practical applications by dramatically re-ducing the development costs and linguis-tic expertise that are required.
Our methodinfers the grammatical resources needed forgeneration from a set of declarative exam-ples that link surface expressions directly tothe application?s available semantic represen-tations.
The same examples further serve tooptimize a run-time search strategy that gener-ates the best output that can be found within anapplication-specific time frame.
Our methodoffers substantially lower development coststhan hand-crafted grammars for application-specific NLG, while maintaining high outputquality and diversity.1 IntroductionThis paper presents a new example-based genera-tion technique designed to reduce the developmentcosts and linguistic expertise needed to integrate agrammar-based generation component into an ex-isting application.
We believe this approach willbroaden the class of applications in which grammar-based generation may feasibly be deployed.In principle, grammar-based generation offerssignificant advantages for many applications, whencompared with simpler template-based or cannedtext output solutions, by providing productive cov-erage and greater output variety.
However, realiz-ing these advantages can require significant devel-opment costs (Busemann and Horacek, 1998).One possible strategy is to exploit a wide-coverage realizer that aims for applicability in mul-tiple application domains (White et al, 2007; Cahilland van Genabith, 2006; Zhong and Stent, 2005;Langkilde-Geary, 2002; Langkilde and Knight,1998; Elhadad, 1991).
These realizers providea sound wide-coverage grammar (or robust wide-coverage language model) for free, but demand aspecific input format that is otherwise foreign toan existing application.
Unfortunately, the devel-opment burden of implementing the translation be-tween the system?s available semantic representa-tions and the required input format can be quite sub-stantial (Busemann and Horacek, 1998).
Indeed, im-plementing the translation might require as much ef-fort as would be required to build a simple customgenerator; cf.
(Callaway, 2003).
Thus, there cur-rently are many applications where using a wide-coverage generator remains impractical.Another strategy is for system builders to handcraft an application-specific grammar for genera-tion.
This approach can be initially attractive tosystem builders because it allows syntactic cover-age and semantic modeling to be tailored directlyto application needs.
However, writing grammati-cal rules by hand ultimately requires a painstaking,time-consuming effort by a developer who has de-tailed linguistic knowledge as well as detailed appli-cation knowledge.
Further, the resulting coverage isinevitably limited to the set of linguistic construc-tions that have been selected for careful modeling.A third strategy is to use an example-based ap-proach (Wong and Mooney, 2007; Stone, 2003;Varges and Mellish, 2001) in which the connection77between available application semantic representa-tions and desired output utterances is specified byexample.
Example-based approaches aim to allowsystem builders to specify a productive generationcapacity while leaving the representations and rea-soning that underlie that productive capacity mostlyimplicit in a set of training examples.
This method-ology insulates system builders from the detailed ex-pertise and technical infrastructure needed to imple-ment the productive capacity directly, and has madeexample-based approaches attractive not only in textgeneration but also in related areas such as concate-native speech synthesis and motion capture basedanimation; see, e.g., (Stone et al, 2004).The technique we present in this paper is a newexample-based approach to specifying application-specific text generation.
As in other hand-craftedand example-based approaches, our technique al-lows syntactic coverage and semantic modeling tofollow the needs and available semantic representa-tions in an application.
One contribution of our tech-nique is to relieve the generation content author ofthe burden of manual syntactic modeling by lever-aging an off-the-shelf parser; defects in the syntaxprovided by the parser are effectively overcome us-ing a machine learning technique.
Additionally, ourtechnique organizes the authoring task in a way thatrelieves the generation author of carefully modelingthe connections between particular syntactic con-structions and available semantic representations.Together, we argue, these features dramaticallyreduce the linguistic expertise and other develop-ment costs that are required to integrate a grammar-based generation component into an existing system.In a case study application, we show that our ap-proach allows an application developer who lacksdetailed linguistic knowledge to extend grammaticalcoverage at an expense of less than one minute peradditional lexical entry.2 Case Study: Doctor PerezOur approach has been tested as a replacement forthe generation component of interactive virtual hu-mans used for social training purposes (Swartout etal., 2006).
Virtual humans are embodied conversa-tional agents that play the role of people in simula-tions or games.
The case study we present in thispaper is the generation of output utterances for aparticular virtual human, Doctor Perez, who is de-signed to teach negotiation skills in a multi-modal,multi-party, non-team dialogue setting (Traum et al,2008).
The human trainee who talks to the doctorplays the role of a U.S. Army captain named Cap-tain Kirk.
The design goals for Doctor Perez createa number of requirements for a practical NLG com-ponent.
We briefly summarize these requirementshere; see (DeVault et al, 2008) for more details.Doctor Perez has a relatively rich internal mentalstate including beliefs, goals, plans, and emotions.He uses an attribute-value matrix (AVM) semanticrepresentation to describe an utterance as a set ofcore speech acts and other dialogue acts.
Speechacts generally have semantic contents that describepropositions and questions about states and actionsin the domain.
To facilitate interprocess communi-cation, and statistical processing, this AVM structureis linearized into a ?frame?
of key values in whicheach non-recursive terminal value is paired with apath from the root to the final attribute.
Figure 1shows a typical frame.
See (Traum, 2003) for addi-tional details and examples of this representation.While only hundreds of frames currently arise inactual dialogues, the number of potential frames isorders of magnitude larger, and it is difficult to pre-dict in advance which frames might occur.
The ut-terances that realize these frames need to take a va-riety of syntactic forms, including simple declar-ative sentences, various modal constructions relat-ing to hypothetical actions or plans, yes/no and wh-questions, and abbreviated dialogue forms such aselliptical clarification and repair requests, ground-ing, and turn-taking utterances.
Highly fluent out-put is not a necessity for this character, since Doc-tor Perez is designed to simulate a non-native En-glish speaker.
However, in order to support com-pelling real-time conversational interaction and ef-fective training, the generation module must be ableto identify an utterance for Doctor Perez to usewithin approximately 200ms on modern hardware.Finally, the development team for Doctor Perez?slanguage capabilities includes approximately 10programmers, testers, linguists, and computationallinguists.
Wherever possible, it is better if any de-veloper can improve any aspect of Doctor Perez?slanguage processing; e.g., if a programmer discov-78ers a bug or disfluency in the NLG output, it is betterif she can fix it directly rather than requiring a (com-putational) linguist to do so.3 Technical ApproachOur approach builds on recently developed tech-niques in statistical parsing, lexicalized syntax mod-eling, generation with lexicalized grammars, andsearch optimization to automatically construct allthe resources needed for a high-quality run-timegeneration component.
In particular, we leverage theincreasing availability of off-the-shelf parsers suchas (Charniak, 2001; Charniak, 2005) to automati-cally (or semi-automatically) assign syntactic anal-yses to a set of suggested output sentences.
Wethen draw on lexicalization techniques for statisticallanguage models (Magerman, 1995; Collins, 1999;Chiang, 2000; Chiang, 2003) to induce a probabilis-tic, lexicalized tree-adjoining grammar that supportsthe derivation of all the suggested output sentences,and many others besides.The final step is to use the training examples tolearn an effective search policy so that our run-timegeneration component can find good output sen-tences in a reasonable time frame.
In particular, weuse variants of existing search optimization (Daum?and Marcu, 2005) and ranking algorithms (Collinsand Koo, 2005) to train our run-time component tofind good outputs within a specified time window;see also (Stent et al, 2004; Walker et al, 2001).
Theresult is a run-time component that treats generationas an anytime search problem, and is thus suitablefor applications in which a time/performance trade-off is necessary (such as real-time dialogue).3.1 Specification of Training ExamplesEach training example in our approach speci-fies a target output utterance (string), its syn-tax, and a set of links between substrings withinthe utterance and system semantic representa-tions.
Formally, a training example takes the form(u, syntax(u), semantics(u)).
We will illustratethis format using the training example in Figure 1.
Inthis example, the generation content author suggeststhe output utterance u = we don?t have medicalsupplies here captain.
Each utterance u is accom-panied by syntax(u), a syntactic analysis in PennTreebank format (Marcus et al, 1994).
In the fig-ure, we show two alternative syntactic analyses thatmight be specified: one is the uncorrected output ofthe Charniak parser on this sentence, and the othera hand-corrected version of that parse; we evaluatethe utility of this hand correction in Section 4.To represent the meaning of utterances, our ap-proach assumes that the system provides some setM = {m1, ...,mj} of semantic representations.The meaning of any individual utterance is thenidentified with some subset of M .
For Doctor Perez,M comprises the 232 distinct key-value pairs thatappear in the system?s various generation frames.
Inthis example, the utterance?s meaning is captured bythe 8 key-value pairs indicated in the figure.Our approach requires the generation contentauthor to link these 8 key-value pairs to con-tiguous surface expressions within the utterance.The technique is flexible about which surface ex-pressions are chosen (e.g.
they need not corre-spond to constituent boundaries); however, they doneed to be compatible with the way the syntacticanalysis tokenizes the utterance, as follows.
Lett(u) = ?t1, ..., tn?
be the terminals in the syn-tactic analysis, in left-to-right order.
Formally,semantics(u) = {(s1,M1), ..., (sk,Mk)}, wheret(u) = s1@ ?
?
?
@sk (with @ denoting concatena-tion), and where Mi ?
M for all i ?
1..k. In thisexample, the surface expression we don?t, which to-kenizes as ?we,do,n?t?, is connected to key-valuesthat indicate a negative polarity assertion.This training example format has two features thatare crucial to our approach.
First, the semantics ofan utterance is specified independently of its syntax.This greatly reduces the amount of linguistic exper-tise a generation content author needs to have.
Italso allows making changes to the underlying syn-tax without having to re-author the semantic links.Second, the assignment of semantic representa-tions to surface expressions must span the entire ut-terance.
No words or expressions can be viewed as?meaningless?.
This is essential because, otherwise,the semantically motivated search algorithm used ingeneration has no basis on which to include thoseparticular expressions when it constructs its outpututterance.
Many systems, including Doctor Perez,lack some of the internal representations that wouldbe necessary to specify semantics down to the lex-79Utterance we don?t have medical supplies here captainSyntaxcat: SA?
?cat: S?
?cat: NP?
?pos: PRP?
?wecat: VP?
?pos: AUX?
?dopos: RB?
?n?tcat: VP?
?pos: AUX?
?havecat: NP?
?pos: JJ?
?medicalpos: NNS?
?suppliescat: ADVP?
?pos: RB?
?herecat: NP?
?pos: NN?
?captaincat: SA?
?cat: S?
?cat: S?
?cat: NP?
?pos: PRP?
?wecat: VP?
?pos: AUX?
?dopos: RB?
?n?tcat: VP?
?pos: AUX?
?havecat: NP?
?pos: JJ?
?medicalpos: NNS?
?suppliescat: VP?
?cat: ADVP?
?pos: RB?
?herepos: VBP?
?captain(corrected Charniak parse) or (uncorrected Charniak parse)Semanticswe do n?t .
.
.
.
.
.
.
{have .
.
.
.
.
.
.
.
.
.
.
.
.medical supplies .
.here .
.
.
.
.
.
.
.
.
.
.
.
.captain .
.
.
.
.
.
.
.??
?semantic framespeech-act.action = assertspeech-act.content.polarity = negativespeech-act.content.attribute = resourceAttributespeech-act.content.value = medical-suppliesspeech-act.content.object-id = marketaddressee = captain-kirkdialogue-act.addressee = captain-kirkspeech-act.addressee = captain-kirkFigure 1: A generation training example for Doctor Perez.
If uncorrected syntax is used, the generation content authoronly writes the utterance and the links to the semantic frame.ical level.
An important feature of our approach isthat it allows an arbitrary semantic granularity to beemployed, by mapping the representations availablein the system to appropriate multi-word chunks.3.2 Automatic Grammar InductionWe adopt essentially the probabilistic tree-adjoininggrammar (PTAG) formalism and grammar induc-tion technique of (Chiang, 2003).
Our approachmakes three modifications, however.
First, whileChiang?s model includes both full adjunction andsister adjunction operations, our grammar has onlysister adjunction (left and right), exactly as in theTAGLET grammar formalism of (Stone, 2002).
Sec-ond, to support lexicalization at an arbitrary gran-ularity, we allow Chiang?s tree templates to be as-sociated with more than one lexical anchor.
Third,to unify syntactic and semantic reasoning in search,we augment lexical anchors with semantic informa-tion.
Formally, wherever Chiang?s model has a lex-ical anchor w, ours has a pair (?w1, ..., wn?,M ?
),where M ?
?
M is connected to lexical anchors?w1, ..., wn?
by the generation content author, as inFigure 1.
The result is that the derivation probabili-ties the grammar assigns depend not only on the im-plicated syntactic structures and lexical anchors butalso on the senses of those lexical anchors in appli-cation terms.We induce our grammar from training exam-ples such as Figure 1 using heuristics to assignderivations to the examples, exactly as in (Chiang,2003).
The process proceeds in two stages.
Inthe first stage, a collection of rules is used to au-tomatically ?decorate?
the training syntax with anumber of features.
These include deciding thelexical anchor(s) for each non-terminal constituentand assigning complement/adjunct status for non-terminals which are not on their parent?s lexical-ization path; see (Magerman, 1995; Chiang, 2003;Collins, 1999).
In addition, we deterministically addfeatures to improve several grammatical aspects, in-cluding (1) enforcing verb inflectional agreement inderived trees, (2) enforcing consistency in the finite-ness of VP and S complements, and (3) restrictingsubject/direct object/indirect object complements toplay the same grammatical role in derived trees.In the second stage, the complements and ad-juncts in the decorated trees are incrementally re-80syntax:cat: SA?
?fin: other,??
cat: Scat: NP,??
apr: VBP,apn: other?
?pos: PRP?
?wefin: yes,??
cat: VPapn: other,??
pos: VBPdopos: RB?
?n?tfin: yes,??
cat: VP,gra: obj1?
?fin: yes,??
cat: VP,gra: obj1?
?pos: VBP?
?havecat: NP,??
gra: obj1operations: initial tree compsemantics: speech-act.action = assertspeech-act.content.polarity = negativespeech-act.content.attribute = resourceAttributesyntax:cat: NP,??
apr: VBP,gra: obj1,??
apn: otherpos: JJ?
?medicalpos: NNS?
?suppliescat: ADVP,??
gra: adjpos: RB?
?herecat: NP,??
apr: VBZ,gra: adj,??
apn: 3pspos: NN?
?captainoperations: comp left/right adjunction left/right adjunctionsemantics: speech-act.content.value =medical-suppliesspeech-act.content.object-id =marketaddressee = captain-kirkdialogue-act.addressee = captain-kirkspeech-act.addressee = captain-kirkFigure 2: The linguistic resources inferred from the training example in Figure 1.moved, yielding the reusable linguistic resources inthe grammar, as illustrated in Figure 2, as well asthe maximum likelihood estimates needed to com-pute operation probabilities.Our approach uses this induced grammar to treatgeneration as a search problem: given a desired se-mantic representation M ?
?
M , use the grammarto incrementally construct an output utterance u thatexpressesM ?.
We treat generation as anytime searchby accruing multiple goal states up until a specifiedtimeout (for Doctor Perez: 200ms) and returning alist of alternative outputs ranked by their derivationprobabilities.3.3 Optimizing the Search StrategyThe search space created by a grammar induced inthis way is too large to be searched exhaustively inmost applications.
The solution we have developedis a beam search strategy that uses weighted featuresto rank alternative grammatical expansions at eachstep.
In particular, the beam size and structure is op-timized so that, with high probability, the beam canbe searched exhaustively before the timeout.1 Thesecond step of automated processing, then, is a train-ing problem of finding weighted features such that1For Doctor Perez, we use a wider beam for initial trees,since the Doctor?s semantic representation is particularly im-poverished at the level of main verbs.
At search depths > 1, weuse beam size 1 (i.e.
greedy search).for every training problem, nodes that lead to goodgeneration output are ranked highly enough by thosefeatures to make it into the beam.We use domain-independent rules to automati-cally define a set of features that could be heuris-tically useful for a given induced grammar.
Theseinclude features for various syntactic structures andoperations, numbers of undesired and desired mean-ings of different types added by an expansion,derivation probabilities, etc.
(For Doctor Perez,this yields about 600 features.)
Our training algo-rithm is based on the search optimization algorithmof (Daum?
and Marcu, 2005), which updates fea-ture weights when mistakes are made during searchon training examples.
For the weight update step,we use the boosting approach of (Collins and Koo,2005), which performs feature selection and iden-tifies weight values that improve the ranking of al-ternative derivation steps when mistakes are madeduring search.
We discuss the resulting success rateand quality in the next section.4 Cost/Benefit AnalysisThe motivation that underlies our technical approachis to reduce the development costs and linguistic ex-pertise needed to develop a grammar-based genera-tion component for an existing system.
In this sec-tion, we assess the progress we have made by ana-81lyzing the use of our approach for Doctor Perez.Method.
We began with a sample of 220 in-stances of frames that Doctor Perez?s dialogue man-ager had requested of the generation component inprevious dialogues with users.
Each frame was as-sociated with a hand-authored target output utter-ance.
We then constructed two alternative trainingexamples, in the format specified in Section 3.1, foreach frame.
One example had uncorrected outputof the Charniak parser for the syntax, and anotherhad hand-corrected parser output (see Figure 1).
Theconnections between surface expressions and framekey-value pairs were identical in both uncorrectedand corrected training sets.We then built two generators using the two setsof training examples.
We used 90% of the data fortraining and held out 10% for testing.
The genera-tors sometimes failed to find a successful utterancewithin the 200ms timeout.
For example, the successrate of the version of our generator trained on uncor-rected syntax was 96.0% for training examples and81.8% for test examples.Quality of generated output.
To assess outputquality, 5 system developers rated each of 494 utter-ances, in relation to the specific frame for which itwas produced, on a single 1 (?very bad?)
to 5 (?verygood?)
scale.
The 494 utterances included all of thehand-authored (suggested) utterances in the trainingexamples.
They also included all the top-ranked ut-terances that were successfully generated by the twogenerators.
We asked our judges to make an over-all assessment of output quality, incorporating bothaccuracy and fluency, for the Doctor Perez charac-ter.
Judges were blind to the conditions under whichutterances were produced.
We discuss additional de-tails of this rating task in (DeVault et al, 2008).The judges achieved a reliability of ?
= 0.708(Krippendorff, 1980); this value shows that agree-ment is well above chance, and allows for tentativeconclusions.
We ran a small number of plannedcomparisons on these ratings.
Surprisingly, wefound no significant difference between generatedoutput trained on corrected and uncorrected syntax(t(29) = 0.056, p > 0.9 on test items, t(498) =?1.1, p > 0.2 on all items).2 However, we did2The distribution of ratings across utterances is not normal;to validate our results we accompanied each t-test by a non-parametric Wilcoxon rank sum test, and significance always fellHand-authored (N = 1099)Generated:Training input (N = 949)Test input (N = 90)RatingFrequency(%)01020304050601 2 3 4 5Figure 3: Observed rating frequencies for hand-authoredvs.
generated utterances (uncorrected syntax).find that hand-authored utterances (mean rating 4.4)are significantly better (t(388) = 5.9, p < 0.001)than generated utterances (mean rating 3.8 for un-corrected syntax).
These ratings are depicted in Fig-ure 3.
While the figure suggests a slight reduction inquality for generated output for test frames vs. train-ing frames, we did not find a significant differencebetween the two (t(19) = 1.4228, p > 0.15).Variety of generated output.
In general our any-time algorithm delivers a ranked list of alternativeoutputs.
While in this initial study our judges ratedonly the highest ranked output generated for eachframe, we observed that many of the lower rankedoutputs are of relatively high quality.
For example,Figure 4 shows a variety of alternative outputs thatwere generated for two of Doctor Perez?s trainingexamples.
Many of these outputs are not present ashand-authored utterances (for any frame); this illus-trates the potential of our approach to provide a va-riety of alternative outputs or paraphrases, which insome applications may be useful even for meaningsfor which an example utterance is hand-authored.Figure 5 shows the overall distribution in the numberof outputs returned for Doctor Perez.Development costs.
The development costs in-cluded implementation of the approach and specifi-cation of Doctor Perez?s training set.
Implementa-in the same general range.82Rank Time (ms) Novel?1 16 no the clinic is up to standard captain2 94 no the clinic is acceptable captain (hand-authored for this input)3 78 yes the clinic should be in acceptable condition captain4 16 yes the clinic downtown is currently acceptable captain5 78 yes the clinic should agree in an acceptable condition captain1 94 no there are no medical supplies downtown captain2 172 no we don?t have medical supplies downtown captain3 125 yes well captain i do not think downtown there are medical supplies4 16 yes i do not think there are medical supplies downtown captainFigure 4: All the utterances generated (uncorrected syntax) for two examples.
Rank is determined by derivationprobability.
Outputs marked as novel are different from any suggested output for any training example.Number of successful outputsFrequency(%)01020300 1 2 3 4 5 6 7 8 9Figure 5: Variety of outputs for each input.tion required an effort of approximately six personmonths.
The developer who carried out the imple-mentation initially had no familiarity with the Doc-tor Perez domain, so part of this time was spent un-derstanding Doctor Perez and his available seman-tic representations.
The bulk of the developmenttime was spent implementing the grammar inductionand training processes.
Grammar induction includedimplementing the probabilistic grammar model andwriting about 40 rules that are used to extract gram-matical entries from the training examples.
Of these40 rules, only 3 are specific to Doctor Perez.3 Theremainder are broadly applicable to syntactic anal-yses in Penn Treebank format, and thus we expectthey would transfer to applications of our approachin other domains.
Similarly, the training algorithmsare entirely domain neutral and could be expected totransfer well to additional domains.Specification of Doctor Perez?s training data took3These 3 rules compensate for frequent errors in Charniakparser output for the words captain, elder, and imam, which areoften used to signal the addressee of Doctor Perez?s utterances.about 6 hours, or about 1.6 minutes per training ex-ample.
This time included hand correction of syn-tactic analyses generated by the Charniak parser anddefinition of semantic links between surface expres-sions and frame key-value pairs.
Since we foundthat hand-correcting syntax does not improve out-put quality, this 1.6 minutes/example figure over-estimates the authoring time required by our ap-proach.
The remaining work lies in defining the se-mantic links.
For Doctor Perez, approximately halfof the semantic links were automatically assignedwith simple ad hoc scripts.4 The semantic linkingprocess might be further sped up through a stream-lined authoring interface offering additional automa-tion, or even using a machine learning approach tosuggest appropriate links.Linguistic expertise required.
Since we foundthat hand-correcting syntax does not improve outputquality, a developer who wishes to exploit our ap-proach may use the Charniak parser to supply thesyntactic model for the domain.
Thus, while onedeveloper with linguistic expertise is required to im-plement the approach, anybody on the applicationteam can contribute by hand authoring additional ut-terances and defining semantic links.
The benefit ofthis authoring effort is the ability to generate highquality output for many novel semantic inputs.Cost/benefit.
The grammar induced from the 198training examples (with uncorrected syntax) con-tains 426 lexical entries of the type depicted in Fig-ure 2.
These 426 lexical entries were produced auto-matically from about 6 hours worth of authoring ef-4Time to compose these scripts is included in the 1.6 min-utes/example.83fort together with domain-neutral algorithms.
Thistranslates to a rate of grammar expansion of lessthan 1 minute per lexical entry, on average, for thissmall application-specific grammar.
This constitutesa dramatic improvement over our previous experi-ence hand-crafting grammars.
It would be challeng-ing for an expert to specify a lexical entry such asthose in Figure 2 in under one minute (and probablyimpossible for someone lacking detailed linguisticknowledge).
In our experience, however, the bulkof development lies in additional time spent con-sidering and investigating possible interactions be-tween lexical entries in generation.
Our techniquehelps with both problems: the grammar inductionstreamlines the specification of lexical entries, andthe training removes the need for a developer tomanually trace through the various complex inter-actions between lexical entries during generation.5 LimitationsCurrently, we do not support semantic links fromnon-contiguous expressions, which means a desiredoutput like ?we rely heavily on medical supplies?would be difficult to annotate if rely...on correspondsto a single semantic representation.
This is not an in-trinsic limitation to our general approach, but rathera simplification in our initial implementation.As discussed in Section 3.2, our grammar induc-tion process adds syntactic features related to verbinflection, finiteness, and grammatical role to the in-ferred lexical entries.
Such features improve the flu-ency and accuracy of output derived with the gram-mar.
While we believe such features can always beassigned using domain-independent rules, develop-ing these rules requires linguistic expertise, and itis likely that additional rules and features (not yetimplemented) would improve coverage of linguisticphenomena such as control verbs, various kinds ofcoordination, and relative clauses, inter alia.A more entrenched limitation of our approachis its assumption that the generator does not needcontext as a separate input.
This means, for ex-ample, that our approach cannot generate referringexpressions (by selecting disambiguating semanticproperties); rather, all semantic properties must bepre-selected and included in the generation request.Generation of anaphoric expressions is also limited,since contextual ambiguities are not considered.6 Related WorkTo our knowledge, this is the first implementedgeneration technique that does all three of the fol-lowing: directly interfaces to existing applicationsemantic representations, infers a phrase structuregrammar from examples, and does not require hand-authored syntax as input.
(Varges and Mellish,2001) also aims to reduce the authoring burden ofdomain-specific generation; however, they seem touse a special purpose semantic annotation ratherthan pre-existing application semantics, and theirtask is defined in terms of the Penn Treebank, sohand-authored syntax is used as input.
(Wong andMooney, 2007) also interfaces to existing applica-tion semantics, and does not require hand-authoredsyntax as input.
Their technique infers a syn-chronous grammar in which the hierarchical linguis-tic analysis is isomorphic to the hierarchy in the ap-plication semantics, and differs from phrase struc-ture.
It would be interesting to compare their out-put quality with ours; their automated alignment ofwords to semantics might also provide a way to fur-ther reduce the authoring burden of our approach.7 Conclusion and Future WorkWe have presented a new example-based approachto specifying text generation for an existing appli-cation.
We have used a cost/benefit analysis to ar-gue that our approach offers productive coverageand high-quality output with less linguistic expertiseand lower development costs than building a hand-crafted grammar.
In future work, we will evaluateour approach in additional application settings, andstudy the performance of our approach as the sizeand scope of the training set grows.AcknowledgmentsThanks to our anonymous reviewers, Arno Hartholt,Susan Robinson, Thomas Russ, Chung-chieh Shan,andMatthew Stone.
This work was sponsored by theU.S.
Army Research, Development, and Engineer-ing Command (RDECOM), and the content does notnecessarily reflect the position or the policy of theGovernment, and no official endorsement should beinferred.84ReferencesS.
Busemann and H. Horacek.
1998.
A flexible shallowapproach to text generation.
In Proceedings of INLG,pages 238?247.Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-based generation using automatically acquiredLFG approximations.
In ACL, pages 1033?1040.C.
B. Callaway.
2003.
Evaluating coverage for largesymbolic NLG grammars.
Proceedings of IJCAI.E.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In ACL, pages 124?131, Morristown,NJ, USA.
Association for Computational Linguistics.E.
Charniak.
2005. ftp://ftp.cs.brown.edu/pub/nlparser/parser05Aug16.tar.gz.D.
Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InACL ?00: Proceedings of the 38th Annual Meetingon Association for Computational Linguistics, pages456?463, Morristown, NJ, USA.
Association for Com-putational Linguistics.D.
Chiang.
2003.
Statistical parsing with an automat-ically extracted tree adjoining grammar.
In R. Bod,R.
Scha, and K. Sima?an, editors, Data Oriented Pars-ing, pages 299?316.
CSLI Publications, Stanford.M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?70.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. dissertation, Uni-versity of Pennsylvania.H.
Daum?
and D. Marcu.
2005.
Learning as searchoptimization: approximate large margin methods forstructured prediction.
In ICML ?05: Proceedings ofthe 22nd international conference on Machine learn-ing, pages 169?176, New York, NY, USA.
ACM.David DeVault, David Traum, and Ron Artstein.
2008.Making grammar-based generation easier to deploy indialogue systems.
In Ninth SIGdial Workshop on Dis-course and Dialogue (SIGdial).M.
Elhadad.
1991.
FUF: the universal unifier user man-ual version 5.0.
Technical Report CUCS-038-91.K.
Krippendorff, 1980.
Content Analysis: An Introduc-tion to Its Methodology, chapter 12, pages 129?154.Sage, Beverly Hills, CA.I.
Langkilde and K. Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InCOLING-ACL, pages 704?710.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.D.
M. Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,pages 276?283, Morristown, NJ, USA.
Association forComputational Linguistics.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguistics,19(2):313?330.A.
Stent, R. Prasad, and M. Walker.
2004.
Trainable sen-tence planning for complex information presentationin spoken dialog systems.
In ACL.Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Ro-driguez, Adrian Stere, Alyssa Lees, and Chris Bregler.2004.
Speaking with hands: creating animated con-versational characters from recordings of human per-formance.
ACM Trans.
Graph., 23(3):506?513.M.
Stone.
2002.
Lexicalized grammar 101.
In ACLWorkshop on Tools and Methodologies for TeachingNatural Language Processing.Matthew Stone.
2003.
Specifying generation of referringexpressions by example.
In AAAI Spring Symposiumon Natural Language Generation in Spoken and Writ-ten Dialogue, pages 133?140.W.
Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,J.
Rickel, and D. Traum.
2006.
Toward virtual hu-mans.
AI Mag., 27(2):96?108.D.
R. Traum, W. Swartout, J. Gratch, and S. Marsella.2008.
A virtual human dialogue model for non-teaminteraction.
In L. Dybkjaer and W. Minker, editors,Recent Trends in Discourse and Dialogue.
Springer.D.
Traum.
2003.
Semantics and pragmatics of questionsand answers for dialogue agents.
In proceedings of theInternational Workshop on Computational Semantics,pages 380?394, January.Sebastian Varges and Chris Mellish.
2001.
Instance-based natural language generation.
In NAACL, pages1?8.M.
Walker, O. Rambow, and M. Rogati.
2001.
Spot:A trainable sentence planner.
In Proceedings of theNorth American Meeting of the Association for Com-putational Linguistics.M.
White, R. Rajkumar, and S. Martin.
2007.
To-wards broad coverage surface realization with CCG.In Proc.
of the Workshop on Using Corpora for NLG:Language Generation and Machine Translation (UC-NLG+MT).Yuk Wah Wong and Raymond Mooney.
2007.
Genera-tion by inverting a semantic parser that uses statisticalmachine translation.
In Proceedings of NAACL-HLT,pages 172?179.H.
Zhong and A. Stent.
2005.
Building surface realiz-ers automatically from corpora using general-purposetools.
In Proc.
Corpus Linguistics ?05 Workshop onUsing Corpora for Natural Language Generation.85
