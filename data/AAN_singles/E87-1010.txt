PATI'ERN RECOGNITION APPLIED TOTHE ACQUISITION OF A GRAMMATICAL CLASSIFICATION SYSTEMFROM UNRESTRICTED ENGLISH TEXTEric Steven Atwell and Nicos Frixou DrakosArtificial Intelligence GroupDepartment of Computer StudiesLeeds University, Leeds LS2 9JT, U.K.(EARN/BITNET: eric%leeds.ai@ac.uk)ABSTRACTWithin computational linguistics, the use of statisticalpattern matching is generally restricted to speech processing.We have attempted to apply statistical techniques to discovera grammatical c assification system from a Corpus of 'raw'English text.
A discovery procedure is simpler for a simplerlanguage model; we assume a first-order Markov model,which (surprisingly) is shown elsewhere to be sufficient forpractical applications.
The extraction of the parameters of astandard Markov model is theoretically straightforward;however, the huge size of the standard model for a NaturalLanguage renders it incomputahle in reasonable time.
Wehave explored various constrained models to reducecomputation, which have yielded results of varying success.Pattern recognition and NLPIn the area of language-related computational research,there is a perceived ichotomy between, on the one hand,"Natural Language" research dealing principally withsyntactic and other analysis of typed text, and on the otherhand, "Speech Processing" research dealing with synthesis,recognition, and understanding of speech signals.
Thisdistinction is nut based merely on a difference of inputand/or output media, but seems also to correlate to noticeabledifferences in assumptions and techniques used in research.One example is in the use of statistical pattern recognitiontechniques: these are used in a wide variety of computer-based research areas, and many speech researchers take it forgranted that such methods are part of their stock in trade.
Incontrast, statistical pattern recognition is hardly ever evenconsidered as a technique to be used in "Natural Language"text analysis.
One reason for this is that speech researchersdeal with "real", "unrestricted" data (speech samples),whereas much NLP research deals with highly restrictedlanguage data, such as examples intuited by theoreticians, orsimplified English as allowed by a dialogue system, sach asa Natural Language Database Query system.Chomsky (57) did much to discredit the use ofrepresentative text samples or Corpora in syntactic research;he dismissed both statistics and semantics as being of no useto syntacticians: "Despite the undeniable interest andimportance of semantic and statistical studies of language,they appear to have no direct relevance to the problem ofdetermining or characterizing the set of grammaticalutterances" (Chomsky 57 p.17).
Subsequent research inComputational Linguistics has shown that Semantics is farmore relevant and important than Chomsky gave credit for.Phenomenal advances in computer power and capabilitiesmean that we can now try statistical pattern recognitiontechniques which would have been incomputable inChomsky's early days.
Therefore, we felt that the case forCorpus-based statistical Pattern Recognition techniquesshould be reopened.
Specifically, we have investigated thepossibility of using Pattern Recognition techniques for theacquisition of a grammatical classification system fromUnrestricted English text.Corpus LinguisticsA Corpus of English text samples can constitute adefinitive source of data in the description of linguisticconstructs or strnctures.
Computational linguists may usetheir intuitions about the English language to devise agrammar of English (or of some part of the Englishlanguage), and then cite example sentences from the Corpusas evidence for their grammar (or counter-evidence againstsomeone else's grammar).
Going one stage further,computational linguists may use data from a Corpus as asource of inspiration at the earlier stage of devising the rulesof the grammar, relying as little as possible on intuitionsabout English grammatical structures (see, for example,(Leech, Garside & AtweU 83a)).
With appropriate softwaretools to extract relevant sentences from the computerisedCorpus, the process of providing evidence for (or against) aparticular grammar might in theory be largely mechanisedAnother way to use data from a Corpus for inspiration is tomanually draw parse-trees on top of example sentences takenfrom the Corpus, without explicitly formulating a56corresponding Context-Free or other rewrite-rule grammar.These trees could then be used as a set of examples for agrammar-rule xtraction program, since every subtree ofmother and immediate daughters corresponds to a phrase-structure rewrite rule; such an experiment is described byAtwell (forthcoming b).However, the linguists must still use their expertise intheoretical linguistics to devise the roles for the grammar andthe grammatical categories used in these roles.
Tocompletely automate the process of devising a grammar forEnglish (or some other language), the computer systemwould have to "know" about theories of grammar, how tochoose an appropriate model (e.g.
context-free rules,Generalized Phrase Structure Grammar, transition etwork,or Markov process), and how to go about devising a set ofroles in the chosen formalism which actually produces theset of sentences in the Corpus (and doesn't produce (toomany) other sentences).Chomsky (1957), in discussing the goals of linguistictheory, considered the possibility of a discovery procedurefor grammars, that is, a mechanical method for constructinga grammar, given a corpus of utterances.
His conclusionwas: "I think it is very questionable that this goal isattainable in any interesting way".
Since then, linguists haveproposed various different grammatical formalisms or modelsfor the description of natural anguages, and there has beenno general consensus amongst expert linguists as to the'best' model.
If even human experts can't agree on thisissue, Chomak-y was probably right in thinking itunreasonable to expect a machine, even an 'intelligent'expert system, to he able to choose which theory or model tostart from.Constrained iscovery proceduresHowever, it may still be possible to devise a discoveryprocedure if we constrain the computer system to a specificgrammatical model.
The problem is simplified further if weconstrain the input to the discovery procedure, to carefullychosen example sentences (and possibly counter-examplenon-sentences).
This is the approach used, for example, byBerwick (85); his system extracted grammar mles in aformalism based on that of Marcus's PARSIFAL (Marcus80) from fairly simple example sentences, and managed toacquire "approximately 70% of the parsing rules originallyhand-written for \[Marcus's\] parser".
Unfortunately, it is notat all clear that such a system could be generalised to dealwith Unrestricted English text, including deviant, idiomaticand even ill-formed sentences found in a Corpus of 'real'language data.
This is the kind of problem best suited tostatistical pattern matching methods.The plausibility of a truly general discovery procedure,capable of working with unrestricted input, increases if wecan use a very simple model to describe the language inquestion.
Chomsky believed that English could only bedescribed by a phrase structure grammar augmented withtransformations, and clearly a discovery procedure fordevising Transformational Generative grammars from aCorpus would have to be extremely complex and 'clever'.More recently, (Gazdar et al85) and others have argued thata less powerful mechanism such as a variant of phrasestructure grammar is sufficient to describe English syntax.
Adiscovery procedure for phrase structure grammars would besimpler than one for TG grammars because phrase structuregrammars are simpler (more constrained) than TG grammars.CLAWSFor the more limited task of assigning part-of-speechlabels to words, (Leech, Garside & AtweU 83b), (Atwell 83)and (Atweii, Leech & Garside 84) showed that an evensimpler model, a first-order Markov model, will suffice.This model was used by CLAWS, the Constituent-Likelihood Automatic Word-tagging System, to assigngrammatical wordclass (part-of-speech) markers to words inthe LOB Corpus.
The LOB Corpus is a collection of 500British English text samples, each of just over 2000 words,totalling over a million words in all; it is available in severalformats (with or without word-tags associated with eachword) from the Norwegian Computing Centre for theHumanities, Bergen University (see (lohansson et al78),(lohansson et al86)).
The Markovian CLAWS was able toassign the correct ag to c96% of words in the LOB Corpus,leaving only a small residual of problematic constructs to beanalysed manually (see (Atwell 81, 82)).
Although CLAWSdoes not yield a full grammatical parse of input sentences,this level of analysis is still useful for some applications; forexample, Atwell (83, 86?)
showed that the first-orderMarkov model could be used in detecting rammatical errorsin ill-formed input English texL The main components ofthe first order Markov model or grammar used by CLAWSwere ;i) a set of 133 grammatical c ass labels or TAGS, e.g.NN (singular common oun) or J JR (comparative adjective)ii) a 133"133 tag-pair matrix, giving the frequency ofcooccurrence of every possible pair of tags (the mwsums orcolumnsums giving frequencies of individual tags)iii) a wordlist associating each word with a list ofpossible tags (with some indication of relative frequency ofeach tag where a word has more than one), supplememed bya suffixlist, prefixlist, and other default routines to deal withinput words not found in the wordlist57iv) a set of formulae to use in calculating likelihood-in-context, o disambiguate word-tags in tagging new text.The last item, the formulae underlying the CLAWSsystem (see (Atwell 83)), constitutes the Markovianmathematical model, and it is too much to ask of any expertsystem to devise or extract this from data.
At least intheory, the first three components could be automaticallyextracted from sample text WHICH HAS ALREADY BEENTAGGED, providing there is enough of it (in particular,there should be many examples of each word in the wordlist,to ensure relative tag likelihoods are accurate).
However, thisis effectively "learning by example": the tagged textsconstitute xamples of correct analyses, and the programextracting word-tag and tag-pair frequencies could be said tobe "learning" the parameters of a Markov model compatiblewith the example data.
Such a learning system is not a trulygeneralised discovery procedure.
Ideally, we would like to beable to extract the parameters of a compatible Markov modelfrom RAW, untagged text.RUNNEWTAGSETStatistical patXem recognition techniques have been usedin many fields of scientific omputing for data classificationand pattern detection.
In a typical application, there will bea large number of data records, each of which will have afairly complex internal structure; the task is to somehowgroup together sets of data records with 'similar' internalstructures, and/or to note types of internal structures whichoccur frequently in data records.
For example, a speechpattern recognition system is 'trained' with repeatedexamples of each word in its vocabulary to recognise thestereotypical structure of the given speech signal, and thenwhen given a 'new' sound it must classify it in terms of the'known' patterns.
In attempting to devise a grarranaticaiclassification system for words in text, a record consists ofthe word itself, and its grammatical context A reasonablylarge sample of text such as the million-word LOB Corpuscorresponds to a huge amount of data if the 'grammaticalcontext' considered with each word is very large.
Thesimplest model is to assume that only the single wordimmediately to the left and/or right of each TARGET wordis important in the context; and even this oversimplificationof context entails vast amounts of processing.If we assume that each word can belong to one and onlyone word*class, then whenever two words tend to occur inthe same set of immediate (lexical) contexts, they willprobably belong to the s~Lme word*class.
This idea wastested using a suite of programs called RUNNEWTAGSETto group words in a c200,000-word subsection of the LOBCorpus into word*classes.
The system only attempted toclassify wordforms which occurred a hundred times or more,the minimum sample size for lexical collocation analysissuggested by Sinclair et al(70).
All possible pairings of onewordfurm with another wordform (wl,w2) were compared: ifthe immediate lexical contexts in which wl occurred weresignificantly similar to the immediate contexts of w2, the twowere deemed to belong to the same word*class, and the twocontext-sets were merged.
A threshold was used to test"significant similarity"; initially, only words which occurredvery frequently in the same contexts were classified together,but then the threshold was lowered in stages, allowing lessand less similar context-sets to be merged at each stage.Unfortunately, the 200,000-word sample turned out to befar too small for conclusive results: even in a sample of thissize, only 175 words occur 1(30 times or more.
However,this program run took several weeks, so it was impractical totry a much larger text sample.
There were some promisingtrends; for example, at the initial threshold level, <willshould could must may might>, <in for on by at during>, <iswas>, <had has:,, <it he there>, <they we>, <but if whenwhile>, <make take>, <end use point question>, and <sensenumber> were grouped into word-classes on the basis oftheir immediate lexical contexts, and in subsequentreductions of the threshold these classes were enlarged andnew classes were added.
However, even if the mammothcomputing requirements could be met, this approach toautomatic generation of a tagset or word*classification systemis unlikely to be wholely successful because it tries to assignevery word to one and only one word*class, whereasintuitively many words can have more than one possible tag.For example, this technique will tend to form three separateclasses for nouns, verbs, and words which can function inboth ways.
For further details of the RUNNEWTAGSETexperiment, see (Atwell 86a, 86b).Baker's algorithmBaker (75, 79) gives a technique which might in theorysolve this problem.
Baker showed that if we assume that alanguage is generated by a Markov process, then it istheoretically possible, given a sufficiently large sample ofdata, to automatically calculate the parameters of a Markovmodel compatible with the data.
Baker's method wasproposed as a technique for automatic training of theparameters of a model of an acoustic processor, but it couldin theory be applied to the syntactic description of text.
InBaker's technique, the principle parameters of the Markovmodel were two matrices, a(i,j) and b(i,j,k).
For the word-tagging application, i and j correspond to tags, while kcorresponds to a word; a(i,j) is the probability of tag i beingfollowed by tag j, and b(i,j,k) is the probability of a wordwith tag i being followed by the word k with tag j. a(i,j) isthe direct equivalent of the tag-pair matrix in the CLAWSmodel above, b(i,j,k) is analogous to the wordlist, except58that the information associated with each word is moredetailed: instead of just a relative frequency for each tag thatcan appear with the word, there is a frequency for everypossible pair of <previous tag - this tag>.
Baker's model ismathematically equivalent to the one used in CLAWS; and ithas the advantage that if the true matrices a(i,j) and b(i,j,k)are not known, then they can be calculated by analysing rawtext.
We start with initial estimates for each value, and thenuse an iterative procedure to repeatedly improve on theseestimates of a(i,j) and b(i,j,k).Unfortunately, although this grammar discovery proceduremight work in theory, the amount of computation i practicerams out to be vast We must iteratively estimate alikelihood for every <tag-tag> pair for a(i,j), and for everypossible <tag-tag-word> triple for h(i,j,k).
Work on taggingthe LOB Corpus has shown that a tag-set of the order of 133tags is reasonable for English (if we include separate tags fordifferent inflections, since different inflexJons can appear indistinguishable syntactic contexts).
Furthermore, the LOBCorpus has roughly 50,000 word-forms in it (counting, forexample, "man", "men", "roans", "manned", "manning", etcas separate wordfonns).
Working from the 'raw' LOBCorpus, we would have to estimate c18,000 values for a(i,j),and 900,000,000 values for b(i,j,k).
As the process ofestimating each a(i,j) and b(i,j,k) value is in itselfcomputationally expensive, it is impractical to use Baker'sformulae unmodified to automatically extract word-classesfrom the LOB Corpus.Grouping by suffixTo cut down the number of variables, we tried thesimplifying assumption that the last five letters of a worddetermine which grammatical class(es) it belongs to.
Inother words, we assumed words ending in the same suffixshared the same wordclass; a not unreasonable assumption,at least for English.
CLAWS was able to assigngrammatical classes to almost any given word using awordlist of only c7000 words supplemented by a suffixliat,so the assumption seemed intuitively reasonable for mostwords.
To further educe the computation, we used tag-pairprobabilities from the tagged LOB Corpus to initialise a(i,j):by using 'sensible' starting values rather than completelyarbitrary ones, convergence should have been much morerapid.
Unfortunately, there were still far too manyinterdependent variables for computation in a reasonabletime: we estimated that even with a single LOB text insteadof the complete Corpus, the first iteration alone in Baker'sscheme would take c66 hours\[Alternative constraintsAn alternative approach was to abandon Baker'salgorithm and introduce other constraints into the First OrderMarkov model.
Another intuitively acceptable constraintwas to allow each word to belong to only a small number ofpossible word classes (Baker's algorithm allowed words tobelong to many different classes, up to the total number ofclasses in the system).
This allowed us to try entirelydifferent algorithms suggested by (Wolff 76) and (Wolff 78),based on the assumption that the claas(es) a word belongs toare determined by the immediate contexts that word appearsin in the example texts.
Unfortunately, these still involvedprohibitive computing times.
Wolffs second model was themore successful of the two, coming up with putative classessuch as <and at for in of to>, <had was>, <a an it one the>,<at by in not on to with> and <but he i it one there>; yetour implementation took 5 hours CPU time to extract heseclasses from an 11,000 word sample.Heuristic constraintsWe are beginning to investigate alternative strategies; forinstance, Artificial Intelligence techniques such as heuristicsto reduce the 'search space' would seem appropriate.However, any heuristics must not be tied too closely to ourintuitive knowledge of the English language, or else theresultant grammar discovery procedure will effectively havesome of the grammar '"ouilt in" to it.
For example, onemight try constraining the number of tags allowed for eachspecific word (e.g "the", "of", "sexy" can have only one tag;"to", "her", "book" have two possible tags; "cold", "base","about" have three tags; "hack", "bid", "according" have fourtags; "hound", "beat", "round" have five tags; and so on); butthis is clearly against he spirit of a tvaly automatic discoveryprocedure in the Chomskyan sense.
A more 'acceptable'constraint would be a general imit of, say, up to five tagsper word.
A discovery procedure would start by assumingthat the context-set of every word could be partitioned intofive subsets, and then it would attempt a Prolog-style'unification' of pairs of similar context-subsets, u ing beliefrevision techniques from Artificial Intelligence (see, forexample, (Drakos 86)).ApplicationsOverall, we concede that the case for statistical pattern-matching for syntactic lassification is not proven.
However,there have been some promising results, which deservefurther investigation, since there would be useful applicationsfor any successful pattern recognition technique for theacquisition of a grammatical classification system fromUnrestricted English text.Note that variables in formulae mentioned above such as iand j are not tag names (NN, VB, ete), but just integersdenoting positions in a tag-pair matrix.
In a Markov model,59a tag is defined entirely by its couccurrence likelihoods withother tags, and with words: labels like NN, VB will not begenerated by a pattern recognition technique.
However, if weassumed initially that there are 133 tags, e.g.
if we initialiseda(i,j) to a 133"133 matrix, then hopefully there should besome correlation between distributions of tags in the LOBtagset and the automatically generated tagset.
If there ispoor correlation for some tags (e.g.
if the automatically-derived tagset includes some tags whose collocationaldistributions are unlike those of any of the tags used in theLOB Corpus), then this constitutes empirical, objectiveevidence that the LOB tagset could be improved upon.In general, any alternative wordclass system could beempirically assessed in an analogous way.
The LongmanDictionary of Contemporary English (LDOCE; Procter 78)and the Oxford Advanced Learner's Dictionary of CunentEnglish (OALD; Hornby 74) give detailed grammaticalcodes with each entry, but the two classification systems arequite different; if samples of text tagged according to theLDOCE and OALD tag.sets were available, a patternrecognition technique might give us an empirical, objectiveway to compare and assess the classification systems, andsuggest particular areas for improvement in forthcomingrevised editions of L?X~E and OALD.
This would beparticularly useful for Machine Readable versions of suchdictionaries, for use in Natural Language Processing systems(see, for example, (Akkerman et al85), (Alshawi et ai 85),(Atweil forthcoming a)); these could be tailored to a givenapplication domain (semi-)automatically.Even though the experiments mentioned achieved onlylimited success in discovering a complete grammaticalclassification system, a more restricted (and hence moreachievable) aim is to concentrate on specific word classeswhich are traditionally recognised as difficult to define.
Forexample, the techniques were particularly successful atfinding groups of words corresponding to invariant functionword classes, such as particles; Atwell (forthcoming c)explores this further.A bottleneck in commercial exploitation of currentresearch ideas in NIP  is the problem of tailoring systems tospecialised linguistic registers, that is, application-specificvariations in lexicon and grammar.
This research, we hope,points the way to (semi-)automating the solution for a widerange of applications (such as described, for example, byAtwell (86d)).
Particularly appropriate to the approachoutlined in this paper are applications ystems based onstatistical models of grammar, such as (Atwell 86c).
Ifgrammar discovery can be made to work not just for variantregisters of English, but for completely different languagesas wall, then it may be possible to automate (or at leastgreatly simplify) the transfer of systems such as thatdescribed by Atweil (86c) to a wide variety of naturallanguages.ConclusionAutomatic grammar discovery procedures are a tantalisingpossibility, but the techniques we have tried so far are farfrom perfect.
It is worth continuing the search because ofthe enormous potential benefits: a discovery procedure wouldprovide a solution to a major bottleneck in commercialexploitation of NLP technology.
We are keen to findcollaborators and sponsors for further esearch.REFERENCESAkkennan, Erik, Pieter Masereeuw, and Willem Meijs 1985Designing a computerized lexicon for linguistic purposesRudopi, AmsterdamAlshawi, Hiyan, Branimir Boguraev, and Ted Briscoe 1985,"Towards a lexicon support environment for real timeparsing" in Proceedings of the Second Conference of theEuropean Chapter of the Association for ComputationalLinguistics, GenevaAtwell, Eric Steven 1981 LOB Corpus Tagging Project:Manual Pre-edit Handbook.
Departments of ComputerStudies and Linguistics, University of LancasterAtwell, Eric Steven 1982 LOB Corpus Tagging Project:Manual Postedit Handbook (A mini-grammar of LOBCorpus English, examining the types of error commonlymade during automatic (computational) analysis of ordinarywritten English.)
Departments of Computer Studies andLinguistics, University of LancasterAtwell, Eric Steven 1983 "Constituent-Likelihood Grammar'in Newsletter of the International Computer Archive ofModern English (ICAME NEWS) 7: 34-67, NorwegianComputing Centre for the Humanities, Bergen UniversityAtwell, Eric Steven 1986a Extracting a Natural Languagegrammar from raw text Department of Computer StudiesResearch Report no.208, University of LeedsAtwell, Eric Steven 1986b, "A parsing expert system which60learns from corpus analysis" in Willem Meijs (ed) CorpusLinguistics and Beyond."
Proceedings of the SeventhInternational Conference on English Language Research onComputerised Corpora, Amsterdam, Netherlands Rodopi,AmsterdamAtwell, Eric Steven 1986c, "How to detect grammaticalerrors in a text without parsing it" Department of ComputerStudies Research Report no.212, University of Leeds; toappear in Proceedings of the Association for ComputationalLinguistics Third European Chapter Conference,Copenhagen, Denmark (elsewhere in this book).Atwell, Eric Steven 1986d "Beyond the micro: advancedsoftware for research and teaching from computer scienceand artificial intelligence" in Leech, Geoffrey and Candlin,Christopher (eds.)
Computers in English language teachingand research: selected papers from the British CouncilSymposium on computers in English language ducation andresearch, Lancaster, England 167-183, LongmanAtwell, Eric Steven (forthcoming a) "A lexical database forEnglish leamera nd users: the Oxford Advanced Learner'sDictionary" to appear in Proceedings of ICDBHSS87, the1987 International Conference on DataBases in theHumanities and Social Sciences, Montgomery, Alabama,USAAtwell, Eric Steven (forthcoming b) "Transforming a ParsedCorpus into a Corpus Parser", to appear in Proceedings ofthe 1987 ICAME 8th International Conference on EnglishLanguage Research on Computerised Corpora, Heisinki,FinlandAtwell, Eric Steven (forthcoming c) "An Expert System forthe Automatic Discovery of Particles" to appear inProceedings of the 1987 International Conference on theStudy of Particles, Berlin, East GermanyAtwell, Eric Steven, Geoffrey Leech and Roger Garside1984, "Analysis of the LOB Corpus: progress andprospects", in Jan Aarts and Willem Meijs (ed), CorpusLinguistics; Proceedings of the \[CAME Conference on theuse of computer corpora in English Language Research,Nijmegen, Netherlands Rodopi.Baker, J K 1975 "Stochastic modeling for automatic speechunderstanding" in D R Reddy (ed) Speech recognitionAcademic PressBaker, J K 1979 '"I'rainable grammars for speechrecognition" in Klatt, D H and Wolf J J (eds.)
Speechcommunication papers for the 97th meeting of the acousticalsociety of America: 547-550Berwick, R 1985 The acquisition of syntactic knowledgeMIT Press, Cambridge (MA) and LondonChomsky, Noam 1957 Syntactic Structures Mouton, TheHagueDrakos, Nicos Frixou 1986 Electrical circuit analysis usingalgebraic m nip,,' 'ion and belief revision Department ofComputer Studies, Leeds UniversityLeech, Geoffrey, Roger Garside, and Eric Steven Atwell1983a, "Recent developments in the use of computer corporain English language research" in Transactiona of thePhilological Society 1983: 23-40.Leech, Geoffrey, Garside, Roger and Atwell, Eric Steven1983b "The Automatic Grammatical Tagging of the LOBCorpus" in Newsletter of the International Computer Archiveof Modern English (\[CAME NEWS) 7: 13-33, NorwegianComputing Centre for the Humanities, Bergen UniversityGazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag1985 Generalized Phrase Structure Grammar Black'well,OxfordHomby, A S, with Cowie, A P (eds.)
1974 Oxford AdvancedLearner's Dictionary of Current English (third edition)Oxford University PressJohausson, Stig, Geoffrey Leech and Helen Goodluck 1978Manual of information to accompany the Lancaster-OslolBergen Corpus of British English, for use with digitalcomputers Department of English, Oslo UniversityJohansson, Stig, Eric Atwell, Roger Garside, and GeoffreyLeech 1986 The Tagged LOB Corpus Norwcgian Computing61Centre for the Humanities, University of Bergen, Norway.Marcus, M P 1980 A Theory of Syntactic Reco n.ionNatural Language MIT Press, Cambridge, MAProcter, Paul (editor-in-chief) 1978 Longman Dictionary ofContemporary English LongmanSinclair, J, Jones, S, and Daley, R 1970 English lexiealstudies, Report o OSTI on project C/LP/08; Dept of English,Birmingham UniversityWolff, J G 1976 "Frequency, Conceptual Structure andPattern Recognition" in British Journal of Psychology67:377-390Wolff, J G 1978 "The Discovery of Syntagmatic andParadigmatic Classes" in ~ Bulletin 6(1):14162
