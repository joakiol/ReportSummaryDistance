Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1?11, Prague, June 2007. c?2007 Association for Computational LinguisticsModelling Compression with Discourse ConstraintsJames Clarke and Mirella LapataSchool of Informatics, University of Edinburgh2 Bucclecuch Place, Edinburgh EH8 9LW, UKjclarke@ed.ac.uk, mlap@inf.ed.ac.ukAbstractSentence compression holds promise formany applications ranging from summarisa-tion to subtitle generation and subtitle gen-eration.
The task is typically performed onisolated sentences without taking the sur-rounding context into account, even thoughmost applications would operate over entiredocuments.
In this paper we present a dis-course informed model which is capable ofproducing document compressions that arecoherent and informative.
Our model is in-spired by theories of local coherence andformulated within the framework of IntegerLinear Programming.
Experimental resultsshow significant improvements over a state-of-the-art discourse agnostic approach.1 IntroductionThe computational treatment of sentence compres-sion has recently attracted much attention in theliterature.
The task can be viewed as producing asummary of a single sentence that retains the mostimportant information and remains grammaticallycorrect (Jing 2000).
Sentence compression is com-monly expressed as a word deletion problem: givenan input sentence of words W = w1,w2, .
.
.
,wn, theaim is to produce a compression by removing anysubset of these words (Knight and Marcu 2002).Sentence compression can potentially benefitmany applications.
For example, in summarisation,a compression mechanism could improve the con-ciseness of the generated summaries (Jing 2000;Lin 2003).
Sentence compression could be alsoused to automatically generate subtitles for tele-vision programs; the transcripts cannot usually beused verbatim due to the rate of speech being toohigh (Vandeghinste and Pan 2004).
Other applica-tions include compressing text to be displayed onsmall screens (Corston-Oliver 2001) such as mobilephones or PDAs, and producing audio scanning de-vices for the blind (Grefenstette 1998).Most work to date has focused on a rather sim-ple formulation of sentence compression that doesnot allow any rewriting operations, besides word re-moval.
Moreover, compression is performed on iso-lated sentences without taking into account their sur-rounding context.
An advantage of this simple viewis that it renders sentence compression amenable toa variety of learning paradigms ranging from in-stantiations of the noisy-channel model (Galley andMcKeown 2007; Knight and Marcu 2002; Turnerand Charniak 2005) to Integer Linear Programming(Clarke and Lapata 2006a) and large-margin onlinelearning (McDonald 2006).In this paper we take a closer look at one ofthe simplifications associated with the compressiontask, namely that sentence reduction can be realisedin isolation without making use of discourse-levelinformation.
This is clearly not true ?
professionalabstracters often rely on contextual cues while creat-ing summaries (Endres-Niggemeyer 1998).
Further-more, determining what information is important ina sentence is influenced by a variety of contextualfactors such as the discourse topic, whether the sen-tence introduces new entities or events that have notbeen mentioned before, and the reader?s backgroundknowledge.The simplification is also at odds with most appli-cations of sentence compression which aim to cre-ate a shorter document rather than a single sentence.The resulting document must not only be grammat-1ical but also coherent if it is to function as a re-placement for the original.
However, this cannot beguaranteed without knowing how the discourse pro-gresses from sentence to sentence.
To give a simpleexample, a contextually aware compression systemcould drop a word or phrase from the current sen-tence, simply because it is not mentioned anywhereelse in the document and is therefore deemed unim-portant.
Or it could decide to retain it for the sake oftopic continuity.We are interested in creating a compression modelthat is appropriate for documents and sentences.
Tothis end, we assess whether discourse-level informa-tion is helpful.
Our analysis is informed by two pop-ular models of discourse, Centering Theory (Groszet al 1995) and lexical chains (Morris and Hirst1991).
Both approaches model local coherence ?the way adjacent sentences bind together to form alarger discourse.
Our compression model is an ex-tension of the integer programming formulation pro-posed by Clarke and Lapata (2006a).
Their approachis conceptually simple: it consists of a scoring func-tion coupled with a small number of syntactic andsemantic constraints.
Discourse-related informationcan be easily incorporated in the form of additionalconstraints.
We employ our model to perform sen-tence compression throughout a whole document(by compressing sentences sequentially) and evalu-ate whether the resulting text is understandable andinformative using a question-answering task.
Ourmethod yields significant improvements over a dis-course agnostic state-of-the-art compression model(McDonald 2006).2 Related WorkSentence compression has been extensively stud-ied across different modelling paradigms and hasreceived both generative and discriminative formu-lations.
Most generative approaches (Galley andMcKeown 2007; Knight and Marcu 2002; Turnerand Charniak 2005) are instantiations of the noisy-channel model, whereas discriminative formulationsinclude decision-tree learning (Knight and Marcu2002), maximum entropy (Riezler et al 2003),support vector machines (Nguyen et al 2004),and large-margin learning (McDonald 2006).
Thesemodels are trained on a parallel corpus of longsource sentences and their target compressions.
Us-ing a rich feature set derived from parse trees, themodels learn either which constituents to delete orwhich words to place adjacently in the compressionoutput.
Relatively few approaches dispense with theparallel corpus and generate compressions in an un-supervised manner using either a scoring function(Clarke and Lapata 2006a; Hori and Furui 2004) orcompression rules that are approximated from a non-parallel corpus such as the Penn Treebank (Turnerand Charniak 2005).Our work differs from previous approaches in twokey respects.
First, we present a compression modelthat is contextually aware; decisions on whether toremove or retain a word (or phrase) are informed byits discourse properties (e.g., whether it introduces anew topic, whether it is semantically related to theprevious sentence).
Second, we apply our compres-sion model to entire documents rather than isolatedsentences.
This is more in the spirit of real-world ap-plications where the goal is to generate a condensedand coherent text.
Previous work on summarisationhas also utilised discourse information (e.g., Barzi-lay and Elhadad 1997; Daume?
III and Marcu 2002;Marcu 2000; Teufel and Moens 2002).
However, itsapplication to document compression is novel to ourknowledge.3 Discourse RepresentationObtaining an appropriate representation of discourseis the first step towards creating a compressionmodel that exploits contextual information.
In thiswork we focus on the role of local coherence asthis is prerequisite for maintaining global coherence.Ideally, we would like our compressed document tomaintain the discourse flow of the original.
For thisreason, we automatically annotate the source docu-ment with discourse-level information which is sub-sequently used to inform our compression proce-dure.
We first describe our algorithms for obtainingdiscourse annotations and then present our compres-sion model.3.1 Centering TheoryCentering Theory (Grosz et al 1995) is an entity-orientated theory of local coherence and salience.Although an utterance in discourse may contain sev-eral entities, it is assumed that a single entity issalient or ?centered?, thereby representing the cur-rent focus.
One of the main claims underlying cen-tering is that discourse segments in which succes-2sive utterances contain common centers are morecoherent than segments where the center repeatedlychanges.Each utterance Ui in a discourse segment has alist of forward-looking centers, C f (Ui) and a uniquebackward-looking center, Cb(Ui).
C f (Ui) representsa ranking of the entities invoked by Ui accordingto their salience.
The Cb of the current utteranceUi, is the highest-ranked element in C f (Ui?1) that isalso in Ui.
The Cb thus links Ui to the previous dis-course, but it does so locally since Cb(Ui) is chosenfrom Ui?1.Centering Algorithm So far we have presentedcentering without explicitly stating how the con-cepts ?utterance?, ?entities?
and ?ranking?
are in-stantiated.
A great deal of research has been devotedinto fleshing these out and many different instantia-tions have been developed in the literature (see Poe-sio et al 2004 for details).
Since our aim is to iden-tify centers in discourse automatically, our param-eter choice is driven by two considerations, robust-ness and ease of computation.We therefore follow previous work (e.g., Milt-sakaki and Kukich 2000) in assuming that the unit ofan utterance is the sentence (i.e., a main clause withaccompanying subordinate and adjunct clauses).This is in line with our compression task which alsooperates over sentences.
We determine which en-tities are invoked by a sentence using two meth-ods.
First, we perform named entity identificationand coreference resolution on each document usingLingPipe1, a publicly available system.
Named en-tities and all remaining nouns are added to the C flist.
Entity matching between sentences is requiredto determine the Cb of a sentence.
This is done usingthe named entity?s unique identifier (as provided byLingPipe) or by the entity?s surface form in the caseof nouns not classified as named entities.Entities are ranked according to their grammaticalroles; subjects are ranked more highly than objects,which are in turn ranked higher than other grammat-ical roles (Grosz et al 1995); ties are broken usingleft-to-right ordering of the grammatical roles in thesentence (Tetreault 2001).
We identify grammaticalroles with RASP (Briscoe and Carroll 2002).
For-mally, our centering algorithm is as follows (whereUi corresponds to sentence i):1LingPipe can be downloaded from http://www.alias-i.com/lingpipe/.1.
Extract entities from Ui.2.
Create C f (Ui) by ranking the entities inUi according to their grammatical role(subjects > objects > others).3.
Find the highest ranked entity in C f (Ui?1)which occurs in C f (Ui), set the entity tobe Cb(Ui).The above procedure involves several automaticsteps (named entity recognition, coreference reso-lution, identification of grammatical roles) and willunavoidably produce some noisy annotations.
So,there is no guarantee that the right Cb will be iden-tified or that all sentences will be marked with a Cb.The latter situation also occurs in passages that con-tain abrupt changes in topic.
In such cases, none ofthe entities realised in Ui will occur in C f (Ui?1).Rather than accept that discourse information maybe absent in a sentence, we turn to lexical chainsas an alternative means of capturing topical contentwithin a document.3.2 Lexical ChainsLexical cohesion refers to the degree of semantic re-latedness observed among lexical items in a docu-ment.
The term was coined by Halliday and Hasan(1976) who observed that coherent documents tendto have more related terms or phrases than inco-herent ones.
A number of linguistic devices can beused to signal cohesion; these range from repeti-tion, to synonymy, hyponymy and meronymy.
Lexi-cal chains are a representation of lexical cohesion assequences of semantically related words (Morris andHirst 1991) and provide a useful means for describ-ing the topic flow in discourse.
For instance, a docu-ment with many different lexical chains will prob-ably contain several topics.
And main topics willtend to be represented by dense and long chains.Words participating in such chains are important forour compression task ?
they reveal what the docu-ment is about ?
and in all likelihood should not bedeleted.Lexical Chains Algorithm Barzilay and Elhadad(1997) describe a technique for text summarisationbased on lexical chains.
Their algorithm uses Word-Net to build chains of nouns (and noun compounds).These are ranked heuristically by a score based ontheir length and homogeneity.
A summary is thenproduced by extracting sentences corresponding to3strong chains, i.e., chains whose score is two stan-dard deviations above the average score.Like Barzilay and Elhadad (1997), we wish todetermine which lexical chains indicate the mostprevalent discourse topics.
Our assumption is thatterms belonging to these chains are indicative of thedocument?s main focus and should therefore be re-tained in the compressed output.
Barzilay and El-hadad?s scoring function aims to identify sentences(for inclusion in a summary) that have a high con-centration of chain members.
In contrast, we are in-terested in chains that span several sentences.
Wethus score chains according to the number of sen-tences their terms occur in.
For example, the chain{house3, home3, loft3, house5} (where wordi de-notes word occurring in sentence i) would be givena score of two as the terms only occur in two sen-tences.
We assume that a chain signals a prevalentdiscourse topic if it occurs throughout more sen-tences than the average chain.
The scoring algorithmis outlined more formally below:1.
Compute the lexical chains for the document.2.
Score(Chain) = Sentences(Chain).3.
Discard chains if Score(Chain) < Avg(Score).4.
Mark terms from the remaining chains as beingthe focus of the document.We use the method of Galley and McKeown (2003)to compute lexical chains for each document.2 Thisis an improved version of Barzilay and Elhadad?s(1997) original algorithm.Before compression takes place, all documentsare pre-processed using the centering and lexicalchain algorithms described above.
In each sentencewe mark the center Cb(Ui) if one exists.
Words (orphrases) that are present in the current sentence andfunction as the center in the next sentence Cb(Ui+1)are also flagged.
Finally, words are marked if theyare part of a prevalent chain.
An example of our dis-course annotation is given in Figure 1.4 The Compression ModelOur model is an extension of the approach put for-ward in Clarke and Lapata (2006a).
Their work tack-les sentence compression as an optimisation prob-lem.
Given a long sentence, a compression is formedby retaining the words that maximise a scoring func-2The software is available from http://www1.cs.columbia.edu/?galley/.Badweather dashed hopes of attempts to halttheflow1 during what was seen as a lull inthe lava?s momentum.
Experts say that evenif the eruption stoppedtoday2 , the pressure oflava piled up behind for sixmiles3 wouldbring debris cascading down on to the townanyway.
Some estimate the volcano is pouring outone million tons of debris aday2 , at arate1of 15ft3 persecond2 , from a fissure that openedin mid-December.The Italian Armyyesterday2 detonated 400lb ofdynamite 3,500 feet up Mount Etna?s slopes.Figure 1: Excerpt of document from our test set withdiscourse annotations.
Centers are in double boxes;terms occurring in lexical chains are in oval boxes.Words with the same subscript are members of thesame chain (e.g., today, day, second, yesterday)tion.
The latter is essentially a language model cou-pled with a few constraints ensuring that the re-sulting output is grammatical.
The language modeland the constraints are encoded as linear inequal-ities whose solution is found using Integer LinearProgramming (ILP, Vanderbei 2001; Winston andVenkataramanan 2003).We selected this model for several reasons.
Firstit does not require a parallel corpus and thus can beported across domains and text genres, whilst de-livering state-of-the-art results (see Clarke and La-pata 2006a for details).
Second, discourse-level in-formation can be easily incorporated by augment-ing the constraint set.
This is not the case for otherapproaches (e.g., those based on the noisy channelmodel) where compression is modelled by gram-mar rules indicating which constituents to delete in asyntactic context.
Third, the ILP framework deliversa globally optimal solution by searching over the en-tire compression space3 without employing heuris-tics or approximations during decoding.We begin by recapping the formulation of Clarkeand Lapata (2006a).
Let W = w1,w2, .
.
.
,wn denotea sentence for which we wish to generate a com-pression.
A set of binary decision variables repre-sent whether each word wi should be included in the3For a sentence of length n, there are 2n compressions.4compression or not.
Let:yi ={1 if wi is in the compression0 otherwise ?i ?
[1 .
.
.n]A trigram language model forms the backbone ofthe compression model.
The language model is for-mulated as an integer program with the introductionof extra decision variables indicating which wordsequences should be retained or dropped from thecompression.
Let:pi ={1 if wi starts the compression0 otherwise ?i ?
[1 .
.
.n]qi j =??
?1 if sequence wi,w j endsthe compression ?i ?
[1 .
.
.n?1]0 otherwise ?
j ?
[i+ 1 .
.
.n]xi jk =??
?1 if sequence wi,w j,wk ?i ?
[1 .
.
.n?2]is in the compression ?
j ?
[i+ 1 .
.
.n?1]0 otherwise ?k ?
[ j + 1 .
.
.n]The objective function is expressed in Equa-tion (1).
It is the sum of all possible trigrams mul-tiplied by the appropriate decision variable.
The ob-jective function also includes a significance score foreach word multiplied by the decision variable forthat word (see the last summation term in (1)).
Thisscore highlights important content words in a sen-tence and is defined in Section 4.1.maxz =n?i=1pi ?P(wi|start)+n?2?i=1n?1?j=i+1n?k= j+1xi jk ?P(wk|wi,w j)+n?1?i=0n?j=i+1qi j ?P(end|wi,w j)+n?i=1yi ?
I(wi) (1)subject to:yi, pi,qi j,xi jk = 0 or 1 (2)A set of sequential constraints4 are added to theproblem to only allow results which combine validtrigrams.4We have omitted sequential constraints due to space limi-tations.
The full details are given in Clarke and Lapata (2006a).4.1 Significance ScoreThe significance score is an attempt at capturing thegist of a sentence.
It gives more weight to contentwords that appear in the deepest level of embed-ding in the syntactic tree representing the sourcesentence:I(wi) =lN?
fi log FaFi (3)The score is computed over a large corpus where wiis a content word (i.e., a noun or verb), fi and Fi arethe frequencies of wi in the document and corpusrespectively, and Fa is the sum of all content wordsin the corpus.
l is the number of clause constituentsabove wi, and N is the deepest level of embedding.4.2 Sentential ConstraintsThe model also contains a small number ofsentence-level constraints.
Their aim is to preservethe meaning and structure of the original sentenceas much as possible.
The majority of constraintsrevolve around modification and argument struc-ture and are defined over parse trees or gram-matical relations.
For example, the following con-straint template disallows the inclusion of modifiers(e.g., nouns, adjectives) without their head words:yi ?
y j ?
0 (4)?i, j : w j modifies wiOther constraints force the presence of modifierswhen the head is retained in the compression.
Thisway, it is ensured that negation will be preserved inthe compressed output:yi ?
y j = 0 (5)?i, j : w j modifies wi ?
w j = notArgument structure constraints make sure thatthe resulting compression has a canonical argumentstructure.
For instance a constraint ensures that if averb is present in the compression then so are its ar-guments:yi ?
y j = 0 (6)?i, j : w j ?
subject/object of verb wiFinally, Clarke and Lapata (2006a) propose onediscourse constraint which forces the system to pre-serve personal pronouns in the compressed output:yi = 1 (7)?i : wi ?
personal pronouns54.3 Discourse ConstraintsIn addition to the constraints described above, ourmodel includes constraints relating to the centeringand lexical chains representations discussed in Sec-tion 3.
Recall that after some pre-processing, eachsentence is marked with: its own center Cb(Ui), thecenter Cb(Ui+1) of the sentence following it andwords that are members of high scoring chains cor-responding to the document?s focus.
We introducetwo new types of constraints based on these addi-tional knowledge sources.The first constraint is the centering constraintwhich operates over adjacent sentences.
It ensuresthat the Cb identified in the source sentence is re-tained in the target compression.
If present, the en-tity realised as the Cb in the following sentence isalso retained:yi = 1 (8)?i : wi ?
{Cb(Ui),Cb(Ui+1)}Consider for example the discourse in Figure 1.
Theconstraints generated from Equation (8) will requirethe compression to retain lava in the first two sen-tences and debris in sentences two and three.We also add a lexical chain constraint.
This ap-plies only to nouns which are members of prevalentchains:yi = 1 (9)?i : wi ?
document focus lexical chainThis constraint is complementary to the centeringconstraint; the sentences it applies to do not have tobe adjacent and the entities under consideration arenot restricted to a specific syntactic role (e.g., sub-ject or object).
See for instance the words flow andrate in Figure 1 which are members of the samechain (marked with subscript one).
According toconstraint (9) both words must be included in thecompressed document.The constraints just described ensure that thecompressed document will retain the discourse flowof the original and will preserve terms indicativeof important topics.
We argue that these constraintswill additionally benefit sentence-level compres-sion, as words which are not signalled as discourserelevant can be dropped.4.4 Applying the ConstraintsOur compression system is given a (sentence sepa-rated) document as input.
The ILP model just pre-sented is then applied sequentially to all sentencesto generate a compressed version of the original.
Wethus create and solve an ILP for every sentence.5 Inthe formulation of Clarke and Lapata (2006a) a sig-nificance score (see Section 4.1) highlights whichnouns and verbs to include in the compression.
Asfar as nouns are concerned, our discourse constraintsperform a similar task.
Thus, when a sentence con-tains discourse annotations, we are inclined to trustthem more and only calculate the significance scorefor verbs.During development it was observed that apply-ing all discourse constraints simultaneously (seeEquations (7)?
(9)) results in relatively long com-pressions.
To counter this, we employ these con-straints using a back-off strategy that relies on pro-gressively less reliable information.
Our back-offmodel works as follows: if centering information ispresent, we apply the appropriate constraints (Equa-tion (8)).
If no centers are present, we back-off to thelexical chain information using Equation (9), and inthe absence of the latter we back-off to the pronounconstraint (Equation (7)).
Finally, if discourse infor-mation is entirely absent from the sentence, we de-fault to the significance score.
Sentential constraints(see Section 4.2) are applied throughout irrespec-tively of discourse constraints.
In our test data (seeSection 5 for details), the centering constraint wasused in 68.6% of the sentences.
The model backedoff to lexical chains for 13.7% of the test sentences,whereas the pronoun constraint was applied in 8.5%.Finally, the noun and verb significance score wasused on the remaining 9.2%.
An example of our sys-tem?s output for the text in Figure 1 is given in Fig-ure 2.5 Experimental Set-upIn this section we present our experimental set-up.We briefly introduce the model used for compar-ison with our approach and give details regardingour compression corpus and parameter estimation.Finally, we describe our evaluation methodology.5We use the publicly available lp solve solver (http://www.geocities.com/lpsolve/).6Bad weather dashed hopes to halt the flow duringwhat was seen as lull in lava?s momentum.
Ex-perts say that even if eruption stopped, the pres-sure of lava piled would bring debris cascading.Some estimate volcano is pouring million tons ofdebris from fissure opened in mid-December.
TheArmy yesterday detonated 400lb of dynamite.Figure 2: System output on excerpt from Figure 1.Comparison with state-of-the-art An obviousevaluation experiment would involve comparingthe ILP model without any discourse constraintsagainst the discourse informed model presented inthis work.
Unfortunately, the two models obtainmarkedly different compression rates6 which ren-ders the comparison of their outputs problematic.
Toput the comparison on an equal footing, we evalu-ated our approach against a state-of-the-art modelthat achieves a compression rate similar to ourswithout taking discourse-level information into ac-count.
McDonald (2006) formalises sentence com-pression in a discriminative large-margin learningframework as a classification task: pairs of wordsfrom the source sentence are classified as being ad-jacent or not in the target compression.
A largenumber of features are defined over words, partsof speech, phrase structure trees and dependen-cies.
These are gathered over adjacent words in thecompression and the words in-between which weredropped.It is important to note that McDonald (2006) is nota straw-man system.
It achieves highly competitiveperformance compared with Knight and Marcu?s(2002) noisy channel and decision tree models.
Dueto its discriminative nature, the model is able to usea large feature set and to optimise compression ac-curacy directly.
In other words, McDonald?s modelhas a head start against our own model which doesnot utilise a parallel corpus and has only a few con-straints.
The comparison of the two systems allowsus to investigate whether discourse information is re-dundant when using a powerful sentence compres-sion model.Corpus Previous work on sentence compres-sion has used almost exclusively the Ziff-Davis,6The discourse agnostic ILP model has a compression rateof 81.2%; when discourse constraints are include the rate dropsto 65.4%.a compression corpus derived automatically fromdocument-abstract pairs (Knight and Marcu 2002).Unfortunately, this corpus is not suitable for ourpurposes since it consists of isolated sentences.
Wethus created a document-based compression corpusmanually.
Following Clarke and Lapata (2006a), weasked annotators to produce compressions for 82stories (1,629 sentences) from the BNC and the LATimes Washington Post.7 48 documents (962 sen-tences) were used for training, 3 for development (63sentences), and 31 for testing (604 sentences).Parameter Estimation Our parameters for theILP model followed closely Clarke and Lapata(2006a).
We used a language model trained on25 million tokens from the North American Newscorpus.
The significance score was based on 25million tokens from the same corpus.
Our re-implementation of McDonald (2006) used an identi-cal feature set, and a slightly modified loss functionto encourage compression on our data set.8Evaluation Previous studies evaluate how well-formed the automatically derived compressions areout of context.
The target sentences are typi-cally rated by naive subjects on two dimensions,grammaticality and importance (Knight and Marcu2002).
Automatic evaluation measures have alsobeen proposed.
Riezler et al (2003) compare thegrammatical relations found in the system outputagainst those found in a gold standard using F-scorewhich Clarke and Lapata (2006b) show correlatesreliably with human judgements.Following previous work, sentence-based com-pressions were evaluated automatically using F-score computed over grammatical relations whichwe obtained by RASP (Briscoe and Carroll 2002).Besides individual sentences, our goal was to evalu-ate the compressed document as whole.
Our evalu-ation methodology was motivated by two questions:(1) are the documents readable?
and (2) how muchkey information is preserved between the sourcedocument and its target compression?
We assumehere that the compressed document is to function asa replacement for the original.
We can thus measurethe extent to which the compressed version can be7The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data/.8McDonald?s (2006) results are reported on the Ziff-Daviscorpus.7What is posing a threat to the town?
(lava)What hindered attempts to stop the lava flow?
(bad weather)What did the Army do first to stop the lava flow?
(detonate explosives)Figure 3: Example questions with answer key.used to find answers for questions which are derivedfrom the original and represent its core content.We therefore employed a question-answeringevaluation paradigm which has been previously usedfor summarisation evaluation and text comprehen-sion (Mani et al 2002; Morris et al 1992).
Theoverall objective of our Q&A task is to determinehow accurate each document (generated by differ-ent compression systems) is at answering questions.For this we require a methodology for constructingQ&A pairs and for scoring each document.Two annotators were independently instructedto create Q&A pairs for the original documentsin the test set.
Each annotator read the documentand then drafted no more than ten questions andanswers related to its content.
Annotators wereasked to create factual-based questions which re-quired an unambiguous answer; these were typicallywho/what/where/when/how style questions.
Anno-tators then compared and revised their question-answer pairs to create a common agreed upon set.Revisions typically involved merging questions, re-wording and simplifying questions, and in somecases splitting a question into multiple questions.Documents for which too few questions were cre-ated or for which questions or answers were too am-biguous were removed.
This left an evaluation setof six documents with between five to eight con-cise questions per document.
Some example ques-tions corresponding to the document from Figure 1are given in Figure 3; correct answers are shown inparentheses.Compressed documents and their accompanyingquestions were presented to human subjects whowere asked to provide answers as best they could.We elicited answers for six documents in three com-pression conditions: gold standard, using the ILPdiscourse model, and McDonald?s (2006) model.Each participant was also asked to rate the readabil-ity of the compressed document on a seven pointscale.
A Latin Square design prevented participantsModel CompR F-ScoreMcDonald 60.1% 36.0%?Discourse ILP 65.4% 39.6%Gold Standard 70.3% ?
?Table 1: Compression results: compression rate andrelation-based F-score; ?
sig.
diff.
from DiscourseILP (p < 0.05 using the Student t test).Model Readability Q&AMcDonald 2.6?
53.7%?
?Discourse ILP 3.0?
68.3%Gold Standard 5.5?
80.7%Table 2: Human Evaluation Results: average read-ability ratings and average percentage of questionsanswered correctly.
?
: sig.
diff.
from Gold Standard;?
: sig.
diff.
from Discourse ILP.from seeing two different compressions of the samedocument.The study was conducted remotely over the In-ternet.
Participants were presented with a set of in-structions that explained the Q&A task and providedexamples.
Subjects were first asked to read the com-pressed document and rate its readability.
Questionswere then presented one at a time and participantswere allowed to consult the document for the an-swer.
Once a participant had provided an answerthey were not allowed to modify it.
Thirty unpaidvolunteers took part in our Q&A study.
All were selfreported native English speakers.The answers provided by the participants werescored against the answer key.
Answers were con-sidered correct if they were identical to the answerkey or subsumed by it.
For instance, Mount Etnawas considered a right answer to the first questionfrom Figure 3.
A compressed document receives afull score if subjects have answered all questions re-lating to it correctly.6 ResultsAs a sanity check, we first assessed the compres-sions produced by our model and McDonald (2006)on a sentence-by-sentence basis without taking thedocuments into account.
There is no hope for gener-ating shorter documents if the compressed sentencesare either too wordy or too ungrammatical.
Table 1shows the compression rates (CompR) for the two8systems and evaluates the quality of their output us-ing F-score based on grammatical relations.
As canbe seen, the Discourse ILP compressions are slightlylonger than McDonald (65.4% vs. 60.1%) but closerto the human gold standard (70.3%).
This is not sur-prising, the Discourse ILP model takes the entiredocument into account, and compression decisionswill be slightly more conservative.
The DiscourseILP?s output is significantly better than McDonald interms of F-score, indicating that discourse-level in-formation is generally helpful.
Both systems coulduse further improvement as inter-annotator agree-ment on this data yields an F-score of 65.8%.Let us now consider the results of our document-based evaluation.
Table 2 shows the mean readabil-ity ratings obtained for each system and the per-centage of questions answered correctly.
We usedan Analysis of Variance (ANOVA) to examine the ef-fect of compression type (McDonald, Discourse ILP,Gold Standard).
The ANOVA revealed a reliable ef-fect on both readability and Q&A.
Post-hoc Tukeytests showed that McDonald and the Discourse ILPmodel do not differ significantly in terms of read-ability.
However, they are significantly less read-able than the gold standard (?
< 0.05).
For the Q&Atask we observe that our system is significantly bet-ter than McDonald (?
< 0.05) and not significantlyworse than the gold standard.These results indicate that the automatic systemslag behind the human gold standard in terms ofreadability.
When reading entire documents, sub-jects are less tolerant of ungrammatical construc-tions.
We also find out that despite relatively lowreadability, the documents are overall understand-able.
The discourse informed model generates moreinformative documents ?
the number of questionsanswered correctly increases by 15% in comparisonto McDonald.
This is an encouraging result suggest-ing that there may be advantages in developing com-pression models that exploit contextual information.7 Conclusions and Future WorkIn this paper we proposed a novel method for au-tomatic sentence compression.
Central in our ap-proach is the use of discourse-level informationwhich we argue is an important prerequisite for doc-ument (as opposed to sentence) compression.
Ourmodel uses integer programming for inferring glob-ally optimal compressions in the presence of lin-guistically motivated constraints.
Our discourse con-straints aim to capture local coherence and are in-spired by centering theory and lexical chains.
Weshowed that our model can be successfully em-ployed to produce compressed documents that pre-serve most of the original?s core content.Our approach to document compression differsfrom most summarisation work in that our sum-maries are fairly long.
However, we believe this isthe first step into understanding how compressioncan help summarisation.
In the future, we will in-terface our compression model with sentence ex-traction.
The discourse annotations can help guidethe extraction method into selecting topically re-lated sentences which can consequently be com-pressed together.
The compression rate can be tai-lored through additional constraints which act onthe output length to ensure precise word limits areobeyed.We also plan to study the effect of global dis-course structure (Daume?
III and Marcu 2002) on thecompression task.
In general, we will assess the im-pact of discourse information more systematicallyby incorporating it into generative and discrimina-tive modelling paradigms.Acknowledgements We are grateful to Ryan Mc-Donald for his help with the re-implementation ofhis system and our annotators Vasilis Karaiskosand Sarah Luger.
Thanks to Simone Teufel, AlexLascarides, Sebastian Riedel, and Bonnie Web-ber for insightful comments and suggestions.
La-pata acknowledges the support of EPSRC (grantGR/T04540/01).ReferencesBarzilay, R. and M. Elhadad.
1997.
Using lexicalchains for text summarization.
In Proceedings ofthe Intelligent Scalable Text Summarization Work-shop (ISTS), ACL-97.Briscoe, E. J. and J. Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceed-ings of the 3rd International Conference on Lan-guage Resources and Evaluation (LREC?2002).Las Palmas, Gran Canaria, pages 1499?1504.Clarke, James and Mirella Lapata.
2006a.Constraint-based sentence compression: Aninteger programming approach.
In Proceedingsof the COLING/ACL 2006 Main Conference9Poster Sessions.
Sydney, Australia, pages144?151.Clarke, James and Mirella Lapata.
2006b.
Modelsfor sentence compression: A comparison acrossdomains, training requirements and evaluationmeasures.
In Proceedings of the 21st Inter-national Conference on Computational Linguis-tics and 44th Annual Meeting of the Associationfor Computational Linguistics.
Sydney, Australia,pages 377?384.Corston-Oliver, Simon.
2001.
Text Compaction forDisplay on Very Small Screens.
In Proceedings ofthe NAACL Workshop on Automatic Summariza-tion.
Pittsburgh, PA, pages 89?98.Daume?
III, Hal and Daniel Marcu.
2002.
A noisy-channel model for document compression.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL2002).
Philadelphia, PA, pages 449?456.Endres-Niggemeyer, Brigitte.
1998.
SummarisingInformation.
Springer, Berlin.Galley, Michel and Kathleen McKeown.
2003.Improving word sense disambiguation in lexi-cal chaining.
In Proceedings of 18th Interna-tional Joint Conference on Artificial Intelligence(IJCAI?03).
pages 1486?1488.Galley, Michel and Kathleen McKeown.
2007.
Lex-icalized markov grammars for sentence compres-sion.
In In Proceedings of the North Ameri-can Chapter of the Association for ComputationalLinguistics (NAACL-HLT?2007).
Rochester, NY.Grefenstette, Gregory.
1998.
Producing IntelligentTelegraphic Text Reduction to Provide an AudioScanning Service for the Blind.
In Proceedings ofthe AAAI Symposium on Intelligent Text Summa-rization.
Stanford, CA, pages 111?117.Grosz, Barbara J., Scott Weinstein, and Aravind K.Joshi.
1995.
Centering: a framework for modelingthe local coherence of discourse.
ComputationalLinguistics 21(2):203?225.Halliday, M. A. K. and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman, London.Hori, Chiori and Sadaoki Furui.
2004.
Speech sum-marization: an approach through word extractionand a method for evaluation.
IEICE Transactionson Information and Systems E87-D(1):15?25.Jing, Hongyan.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of the6th conference on Applied Natural Language Pro-cessing (ANLP?2000).
Seattle, WA, pages 310?315.Knight, Kevin and Daniel Marcu.
2002.
Summa-rization beyond sentence extraction: a probabilis-tic approach to sentence compression.
ArtificialIntelligence 139(1):91?107.Lin, Chin-Yew.
2003.
Improving summarizationperformance by sentence compression ?
a pilotstudy.
In Proceedings of the 6th InternationalWorkshop on Information Retrieval with AsianLanguages.
Sapporo, Japan, pages 1?8.Mani, Inderjeet, Gary Klein, David House, LynetteHirschman, Therese Firmin, and Beth Sundheim.2002.
SUMMAC: A text summarization evalua-tion.
Natural Language Engineering 8(1):43?68.Marcu, Daniel.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
The MITPress, Cambridge, MA.McDonald, Ryan.
2006.
Discriminative sentencecompression with soft syntactic constraints.
InProceedings of the 11th EACL.
Trento, Italy.Miltsakaki, Eleni and Karen Kukich.
2000.
Therole of centering theory?s rough-shift in the teach-ing and evaluation of writing skills.
In Proceed-ings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics (ACL?2000).pages 408?415.Morris, A., G. Kasper, and D. Adams.
1992.
Theeffects and limitations of automated text condens-ing on reading comprehension performance.
In-formation Systems Research 3(1):17?35.Morris, Jane and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indi-cator of the structure of text.
Computational Lin-guistics 17(1):21?48.Nguyen, Minh Le, Akira Shimazu, SusumuHoriguchi, Tu Bao Ho, and Masaru Fukushi.2004.
Probabilistic sentence reduction usingsupport vector machines.
In Proceedings ofthe 20th COLING.
Geneva, Switzerland, pages743?749.Poesio, Massimo, Rosemary Stevenson, Barbara DiEugenio, and Janet Hitzeman.
2004.
Centering: a10parametric theory and its instantiations.
Compu-tational Linguistics 30(3):309?363.Riezler, Stefan, Tracy H. King, Richard Crouch, andAnnie Zaenen.
2003.
Statistical sentence con-densation using ambiguity packing and stochas-tic disambiguation methods for lexical-functionalgrammar.
In Proceedings of the HLT/NAACL.
Ed-monton, Canada, pages 118?125.Tetreault, Joel R. 2001.
A corpus-based evaluationof centering and pronoun resolution.
Computa-tional Linguistics 27(4):507?520.Teufel, Simone and Marc Moens.
2002.
Summa-rizing scientific articles ?
experiments with rele-vance and rhetorical status.
Computational Lin-guistics 28(4):409?446.Turner, Jenine and Eugene Charniak.
2005.
Su-pervised and unsupervised learning for sentencecompression.
In Proceedings of the 43rd ACL.Ann Arbor, MI, pages 290?297.Vandeghinste, Vincent and Yi Pan.
2004.
Sentencecompression for automated subtitling: A hybridapproach.
In Proceedings of the ACL Workshopon Text Summarization.
Barcelona, Spain, pages89?95.Vanderbei, Robert J.
2001.
Linear Programming:Foundations and Extensions.
Kluwer AcademicPublishers, Boston, 2nd edition.Winston, Wayne L. and Munirpallam Venkatara-manan.
2003.
Introduction to Mathematical Pro-gramming.
Brooks/Cole.11
