Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 252?258,Dublin, Ireland, August 23-24, 2014.ECNU: A Combination Method and Multiple Features for AspectExtraction and Sentiment Polarity ClassificationFangxi Zhang, Zhihua Zhang, Man Lan?Department of Computer Science and TechnologyEast China Normal University51111201041,51131201039@ecnu.cn; mlan@cs.ecnu.edu.cn?AbstractThis paper reports our submissions to thefour subtasks of Aspect Based Sentimen-t Analysis (ABSA) task (i.e., task 4) inSemEval 2014 including aspect term ex-traction and aspect sentiment polarity clas-sification (Aspect-level tasks), aspect cat-egory detection and aspect category sen-timent polarity classification (Category-level tasks).
For aspect term extraction, wepresent three methods, i.e., noun phrase(NP) extraction, Named Entity Recogni-tion (NER) and a combination of NP andNER method.
For aspect sentiment classi-fication, we extracted several features, i.e.,topic features, sentiment lexicon features,and adopted a Maximum Entropy classifi-er.
Our submissions rank above average.1 IntroductionRecently, sentiment analysis has attracted a lot ofattention from researchers.
Most previous workattempted to detect overall sentiment polarity on atext span, such as document, paragraph and sen-tence.
Since sentiments expressed in text alwaysadhere to objects, it is much meaningful to iden-tify the sentiment target and its orientation, whichhelps user gain precise sentiment insights on spe-cific sentiment target.The aspect based sentiment analysis (ABSA)task (Task 4) (Pontiki et al., 2014) in SemEval2014 is to extract aspect terms, determine its se-mantic category, and then to detect the sentimen-t orientation of the extracted aspect terms and itscategory.
Specifically, it consists of 4 subtasks.The aspect term extraction (ATE) aims to extrac-t the aspect terms from the sentences in two giv-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/en domains (laptop and restaurant).
The aspec-t category detection (ACD) is to identify the se-mantic category of aspects in a predefined set ofaspect categories (e.g., food, price).
The aspectterm polarity (ATP) classification is to determinewhether the sentiment polarity of each aspect ispositive, negative, neutral or conflict (i.e., bothpositive and negative).
The aspect category po-larity (ACP) classification is to determine the sen-timent polarity of each aspect category.
We partic-ipated in these four subtasks.Generally, there are three methods to extract as-pect terms: unsupervised learning method basedon word frequency ((Ku et al., 2006), (Long etal., 2010)), supervised machine learning method(Kovelamudi et al., 2011) and semi-supervisedlearning method (Mukherjee and Liu, 2012) whereonly several user interested category seeds aregiven and used to extract more categorize aspectterms.
Since sentiments always adhere to entities,several researchers worked on polarity classifica-tion of entity.
For example, (Godbole et al., 2007)proposed a system that assigned scores represent-ing positive or negative opinion to each distinc-t entity in the corpus.
(Kim et al., 2013) presenteda hierarchical aspect sentiment model to classifythe polarity of aspect terms from unlabeled onlinereviews.
Moreover, some sentiment lexicons, suchas SentiWordNet (Baccianella et al., 2010) and M-PQA Subjectivity Lexicon (Wilson et al., 2009),have been used to generate sentiment score fea-tures (Zhu et al., 2013).The rest of this paper is organized as follows.From Section 2 to Section 5, we describe our ap-proaches to the Aspect Term Extraction task, theAspect Category detection task, the Aspect TermPolarity task and the Aspect Category Polarity taskrespectively.
Section 6 provides the conclusion.2522 Aspect Term Extraction SystemFor aspect terms extraction task, we first adoptedtwo methods: a noun phrase (NP) based methodand a Named Entity Recognition (NER) basedmethod.
In our preliminary experiments, we foundthat the NP-based method generates many noisyterms resulting in high recall and low precision,and the NER-based method performs inverse re-sults.
In order to overcome their drawbacks andmake use of their advantages, we proposed a thirdmethod which combines the two methods by usingthe results of NP-based method as an additionalname list feature to the NER system.2.1 PreprocessingWe used Stanford Parser Tools1for POS taggingand for parsing while the Natural Language Toolk-it2was used for removing stop words and lemma-tization.2.2 NP-based Method(Liu, 2012) showed that the majority of aspec-t terms are noun phrases.
Moreover, through theobservation of the training set, we found that morethan half of the aspects are pure noun phrases ornested noun phrases.
So we considered these twotypes of noun phrases as aspect terms and adopt-ed a rule-based noun phrases extraction system toperform aspect term extraction.
This extractionis performed on parsed sentences.
For example,from parsed sentence:?
(CC but)(S(NP (NN iwork))(VP (VBZ is)(ADJP (JJ cheap))(PP (VBN compared)(PP (TO to)(NP (NN office))))))?iwork and office with NN tag are extracted as as-pect terms.
However, to make a more precise ex-traction, we first removed white lines from parsedsentences.
Then we performed extraction only us-ing three continuous lines.
Since the NPs we ex-tracted contain much noise which only appear inNPs rather than in gold aspect terms list, we builta stopwords list containing these noisy terms espe-cially the numeric expressions.
Table 1 shows theset of manually built rules used for NP extraction.1http://nlp.stanford.edu/software/lex-parser.shtml2http://www.nltk.org/Based on the experimental results on trainingdata, we found the NP-based method achieveshigh recall and low precision as shown in Table2.
This indicates that we extracted plenty of NPswhich consist of a large proportion of aspect termsand much noise such as irrelevant NPs and over-lapping phrases.
Thus the NP-based method alonehas not produced good results.2.3 NER-based MethodWe also cast aspect term extraction task as a tradi-tional NER task (Liu, 2012).
We adopted the com-monly used BIO tag format to represent the aspectterms in the given annotated training data (Toh etal., 2012), where B indicates the beginning of anaspect term, I indicates the inside of an aspect ter-m and O indicates the outside of an aspect term.For example, given ?the battery life is excellent?,where battery life is annotated as aspect term, wetagged the three words the, is and excellent as O,battery as B and life as I.We adopted several widely used features for theNER-based aspect term extraction system.Word features: current word (word 0), previ-ous word (word -1) and next word (word 1) areused as word features.POS feature: the POS tag of current word(POS 0), the POS tags of two words around cur-rent word (POS -2, POS -1, POS 1, POS 2), andthe combinations of contextual POS tags (POS -1/POS 0, POS 0/POS 1, POS -1/POS 0/POS 1)are included as POS features.Word shape: a tag sequence of characters incurrent word is recorded, i.e., the lowercase lettertagged as a, and the uppercase letter tagged as A.Chunk: We extracted this feature from the POStag sequence, which is defined as follows: theshortest phrase based on POS taggers, i.e., ?
(VP(VBD took) (NP (NN way)) (ADVP (RB too) (RBlong))?, took labeled as O, way labeled as B-NP,too labeled as B-ADVP, long labeled as I-ADVP.We implemented a CRF++3based NER systemwith the above feature types.2.4 Combination of NP and NER MethodBased on our preliminary experiments, we con-sidered to combine the above two methods.
Todo so, we adopted the results of the NP systemas additional name lists feature for the NER sys-tem.
Through the observation on the results of the3http://crfpp.googlecode.com/svn/trunk/doc/index.html253if (NP in line 1) then select line 1 as candidateif (NP in line 1 and PP in line 2 and NP in line 3) then select line 1 + line 2 + line 3 as candidateelse if (VB in line 1 and NN in line 2) then select line 1 + line 2 as candidateelse if (NP in line 1 and NP in line 2) then select line 1 + line 2 as candidateelse if (NP in line 1 and CC in line 2 and NN in line 3) then select line 3 as candidateelse if (JJ in line 1 and NN in line 2) then select line 2 as candidateif (current term in candidate existing in stopwords) then remove current termif (CD start candidate) then remove CDif (DT or PRP start candidate) then remove DT or PRPif (JJR in candidate) then remove JJRif (Punctuation in candidate) then remove PunctuationTable 1: The rules in NP-based method.methodLaptop RestaurantPrecision(%) Recall(%) F-score(%) Precision(%) Recall(%) F-score(%)NP-based 44.35 74.43 55.59 45.99 70.50 56.17NER-based 70.46 48.27 57.29 80.87 68.24 74.02Combination 72.79 55.11 62.73 82.31 70.62 76.02Table 2: The F-scores of three methods on training data.NP-based method and the NER-based method, webuilt two types of name lists for our combinationmethod as follows:Gold Namelist: containing the gold aspec-t terms and the intersection between the results ofthe NP-based method and the NER-based method.Stop Namelist: the words in original sentencesbut not in gold aspect terms set or not in NPs setwe extracted before.Table 3 shows the results of feature selectionfor the combination method on training data.
Thebest performance was obtained by using all fea-tures.
Thus, our final submission system adoptedthe combination method with all features.FeatureDatasetLaptop Restaurantword:+word 0 40.35 58.58+word 1 54.78 72.23POS:+POS 0 55.81 71.11+POS 1 57.07 74.02+POS 2 57.18 73.24+POS 0/POS 1 51.85 70.58chunk:+chunk 0 56.74 73.45word shape:+word shape 0 57.29 74.02name list:+Gold Namelist 62.66 75.39+Stop Namelist 62.73 76.02Table 3: The F-scores of combination methodof subtask 1 on training data based on 2 cross-validationTable 2 shows the results of the above threesystems on training data.
Comparing with oth-er two methods, we easily find that the combina-tion method outperforms the other two systems interms of precision, recall and F values on both do-mains.2.5 Result and DiscussionIn constrained run, we submitted the results us-ing the method in combination of NP and NER.Specifically, we adopted all features and the namelists listed in Table 3 and the CRF++ tool for theNER system.
Table 4 lists the results of our fi-nal system and the top two systems officially re-leased by organizers.
On both domains, our sys-tem ranks above the average under constrainedmodel, which proves the effectiveness of the com-bination method by using NP extraction and NER.From Table 2 and Table 4 we find that the re-sults on restaurant data are much better than thoseon laptop data.
Based on our further observationon training data, the possible reason is that thenumber of numeric descriptions in laptop datasetis much larger than those in restaurant dataset andthe aspect terms containing numeric descriptionare quite difficult to be extracted.Dataset DLIREC NRC-Canada Our resultlaptop 70.41 68.57 65.88restaurant 78.34 80.19 78.24Table 4: The F-scores (%) of our system and thetop two systems of subtask 1 on test dataset.2543 Aspect Category Classification SystemAspect category classification task tries to assigneach aspect one or more semantic category labels.Thus, we regarded this task as a multi-class clas-sification problem.
Following (Rennie, 2001), webuilt a binary model for each category, where bag-of-words is used as features.3.1 FeaturesWe adopted the bag-of-words schema to representfeatures as follows.
Since not all training instanceshave annotated aspect terms, we extracted only an-notated aspect terms from sentence if the sentencecontains annotated aspect terms, or extracted allwords from sentence which does not contain anyannotated aspect terms as features, which resultsin 5200 word features in total.3.2 Classification AlgorithmWe adopted the maximum entropy algorithm im-plemented in Mallet toolkit (McCallum, 2002) tobuild a binary classifier for each category.
All pa-rameters are set as defaults.
This subtask only pro-vides restaurant data and there are five predefinedcategories (i.e., food, price, service, ambience andanecdotes/miscellaneous), thus we build five bina-ry classifiers in total.3.3 Results and DiscussionsTable 5 lists the precision, recall and F-score ofour final system along with the top two systemsreleased by the organizers.Precision(%) Recall(%) F-score(%)our system 65.26 69.46 67.30rank 1 system 91.04 86.24 88.58rank 2 system 83.23 81.37 82.29Table 5: The results of our system and the top twosystems of subtask 3 on the test data.From Table 5, we find that there are quite a largeroom to improve our system.
One main reasonis that our system only uses simple features (i.e.,bag-of-words) and these simple features may havepoor discriminating power.
Another possible rea-son may be that in training data there are at leasthalf sentences without annotated aspect terms.
Inthis case, when we used all words in the sentencesas features, it may bring much noise.
In futurework, we consider to generate more effective fea-tures from external resources to indicate the re-lationships between aspects and categories to im-prove our system.4 Aspect Term Sentiment PolarityClassification SystemOnce we extract aspect terms, this task aims atclassifying the sentiment orientation of the anno-tated aspect terms.
To address this task, we firstlyextracted three types of features: sentiment lexi-con based features, topic model based features andother features.
Then two machine learning algo-rithms, i.e., SVM and MaxEnt, were used to con-duct classification models.4.1 Features4.1.1 Sentiment Lexicon (SL) FeaturesWe observed that the sentiment orientation of anaspect term is usually revealed by the surroundingterms.
So in this feature we took four words beforeand four words after the current aspect term andthen calculated their respective positive,negativeand neutral scores.
During the calculation we re-versed the sentiment orientation of the term if anegation occurs before it.
We manually built anegative list: {no, nor, not, neither, none, no-body, nothing, hardly, seldom}.
Eight sentimen-t lexicons are used: Bing Liu opinion lexicon4,General Inquirer lexicon5, IMDB6, MPQA7, Sen-tiWordNet8, NRC emotion lexicon9, NRC Hash-tag Sentiment Lexicon10and NRC Sentiment140Lexicon11.
With regard to the synonym selectionof SentiWordNet, we selected the first term in thesynset as our lexicon.
If the eight words surround-ing the aspect term do not exist in the eight cor-responding sentiment lexicons, we set their threesentiment scores as 0.
Then we got 24 sentimen-t values for each word (3 polarities * 8 lexicons)and summed up the values of eight words for eachsentiment polarity (i.e., positive, negative and neu-ral).
Finally we got 24 sentiment lexicon featuresfor each aspect.4http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html#lexicon5http://www.wjh.harvard.edu/?inquirer/homecat.htm6http://anthology.aclweb.org//S/S13/S13-2.pdf#page=4447http://mpqa.cs.pitt.edu/8http://sentiwordnet.isti.cnr.it/9http://mailman.uib.no/public/corpora/2012-June/015643.html10http://www.umiacs.umd.edu/?saif/WebDocs/NRC-Hashtag-Sentiment-Lexicon-v0.1.zip11http://sentiwordnet.isti.cnr.it/255feature F-pos(%) F-neg(%) F-neu(%) Acc(%)MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVMSL 72.50?
1.91 70.99?
5.91 65.10?
1.99 65.66?
3.48 25.54?
5.68 24.02?
9.28 62.28?
2.59 61.61?
4.68+Other 72.92?
2.12 72.70?
1.44 65.93?
3.89 65.09?
3.67 31.14?
5.77 34.00?
7.31 62.88?
3.22 62.54?
3.17+Topic 73.14?
1.02 72.21?
1.44 65.55?
5.43 65.58?
3.45 34.34?
10.55 12.16?
4.96 63.00?
4.34 61.74?
3.10Table 6: The results of our system in subtask 2 on laptop training data based on 5-fold cross validation.features F-pos(%) F-neg(%) F-neu(%) Acc(%)MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVMSL 79.78?
1.37 79.85?
1.35 49.37?
3.54 47.96?
4.52 26.02?
3.62 31.67?
2.84 65.61?
2.59 65.45?
1.98+Other 80.48?
2.18 79.09?
1.42 53.17?
2.70 50.51?
3.34 29.25?
3.60 33.13?
6.89 66.80?
2.33 65.21?
2.35+Topic 80.71?
1.71 77.94?
1.34 52.61?
2.52 46.65?
3.17 34.51?
3.35 3.40?
2.79 67.18?
2.52 64.72?
1.48Table 7: The results of our system in subtask 2 on restaurant training data based on 5-fold cross valida-tion.4.1.2 Topic FeaturesIn this section we considered to use the bag-of-topics feature to replace the traditional bag-of-words feature since the bag-of-words feature arevery sparse in the data set.
To construct the cluster-s of topics, we used the LDA12based topic modelto estimate the K topics (in our experiment, weset K to 50) from training data.
Then we inferredthe topic distribution from training and test datarespectively as topic features.4.1.3 Other FeaturesBesides, we also proposed the following other fea-tures in order to capture more useful informationfrom the short texts.Aspect distance This feature records the num-ber of words from the current aspect to the nextaspect in the same sentence.
If the current aspectterm is the last term in the sentence, this value iscalculated as the negative number of words fromthe current aspect to the former aspect.
If only oneaspect term exists in a sentence, then the value isset to zero.Number of aspects This feature describes thenumber of aspect terms in the current sentence.Negation flag feature We set this feature as 1if a negation word occurs in the current sentence,otherwise -1.Number of negations This feature is the num-ber of negation words in the current sentence.4.2 Classification AlgorithmsThe maximum entropy and SVMwhich are imple-mented in Mallet toolkit (McCallum, 2002) andLibSVM (Chang and Lin, 2011) respectively are12http://www.cs.princeton.edu/ blei/lda-c/used to construct the classification model fromtraining data.
Due to the limit of time, all parame-ters are set as defaults.4.3 Results and Discussions4.3.1 Results on Training DataTo compare the performance of different featuresand different algorithms, we performed a 5-foldcross validation on training data of two domain-s. Table 6 and Table 7 show the results of twodomains in terms of F-scores and accuracy withmean and standard deviation.
The best results areshown in bold.From above two tables, we found that (1) Max-Ent performed better than SVM on both dataset-s and all feature types, and (2) using all featuresachieved the best results.
Moreover, the F-pos re-sult was the highest in both datasets and the pos-sible reason is that the majority of training in-stances are positive sentiment.
We also found thatin restaurant dataset, F-neg (52.61%) was muchsmaller than F-pos (80.17%).
However, in lap-top dataset, they performed comparable results.The possible reason is that the number of neg-ative instances (805) is much smaller than thenumber of positive instances (2164) in restauran-t dataset, while the distribution is nearly even inlaptop dataset.
So for restaurant data, we also con-ducted another controlled experiment which dou-bled the amount of negative instances of restaurantdataset.
Table 8 shows the preliminary experimen-tal results on the doubled negative training data.
Itillustrates that the F-neg increases a little but theoverall accuracy without any improvement evenslightly decreases after doubling the negative in-stances.
This result is beyond our expectation but256no further deep analysis has been done so far.Strategy F-pos(%) F-neg(%) F-neu(%) Acc(%)Double 80.28 55.11 19.22 65.48No double 80.71 52.61 34.51 67.18Table 8: The results of controlled experiment onrestaurant dataset (MaxEnt).4.3.2 Results on Test DataBased on above results on training data, our finalsystem used all provided training data for both do-mains.
The MaxEnt algorithm is used for our finalsystem.
Table 9 shows our results alone with thetop two systems results released by organizers.Our final results ranked the 12th on the lap-top dataset and the 14th on the restaurant dataset.On one hand, the accuracy in restaurant dataset ishigher than laptop dataset for the possible reasonthat the data size of restaurant dataset is much big-ger than that of laptop dataset.
On the other hand,our results ranked middle in both datasets.
Sincewe utilized eight contextual words around aspectto extract features and it may bring some noise.Dataset laptop restaurantour system 61.16 70.72rank 1 system 70.49 80.95rank 2 system 66.97 80.16Table 9: The Accuracy (%) of our system and thetop two systems on test dataset in subtask 2.5 Aspect Category Sentiment PolaritySystemThe aspect category sentiment polarity classifi-cation task is also only applicable to restauran-t domain.
For this task, we adopted the bag-of-sentiment words representation, extracted sen-timent features and used the supervised machinelearning algorithms to determine the sentimen-t orientation of each category.5.1 FeaturesTo extract features, we firstly used eight sentimentlexicons mentioned in Section 4.1.1 to build a bigsentiment words dictionary.
Then we extracted al-l aspect words and all sentiment words in train-ing set as features.
In the training and test data,we used the sentiment polarity score of sentimentword and the presence or absence of each aspectterm as their feature values.5.2 Classification AlgorithmsTheMaxEnt algorithm implemented inMallet (M-cCallum, 2002) with default parameters is used tobuild a polarity classifier.5.3 Experiment and ResultsWe used all features and the maximum entropy al-gorithm to conduct our final system.
Table 10 list-s the final results of our submitted system alongwith top two systems.As shown in Table 10, the accuracy of our sys-tem is 0.63 while the best result is 0.83.
The mainreason is that the features we used are quite sim-ple.
For the future work, more sufficient featuresare examined to help classification.6 ConclusionIn this work we proposed a combination of NPand NER method and multiple features for aspec-t extraction.
And we also used multiple featuresincluding eight sentiment lexicons for aspect andcategory sentiment classification.
Our final sys-tems rank above average in the four subtasks.
Infuture work, we would expect to improve the re-call of aspect terms extraction by extending namelists using external data and seek other effectivefeatures such as discourse relation, syntactic struc-ture to improve the classification accuracy.Systems our system rank 1 system rank 2 systemAcc(%) 63.41 82.93 78.15Table 10: The accuracy of our system and the toptwo systems of subtask 4 on test datasetAcknowledgementsThe authors would like to thank the organizers andreviewers for this interesting task and their helpfulsuggestions and comments.
This research is sup-ported by grants from National Natural ScienceFoundation of China (No.60903093) and Shang-hai Knowledge Service Platform Project (No.ZF1213).ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In LREC, volume 10, pages 2200?2204.257Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technolo-gy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Namrata Godbole, Manja Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs.
ICWSM, 7.Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, andShixia Liu.
2013.
A hierarchical aspect-sentimentmodel for online reviews.
In Proceedings of AAAI.Sudheer Kovelamudi, Sethu Ramalingam, Arpit Sood,and Vasudeva Varma.
2011.
Domain independen-t model for product attribute extraction from userreviews using wikipedia.
In IJCNLP, pages 1408?1412.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.2006.
Opinion extraction, summarization and track-ing in news and blog corpora.
In AAAI Spring Sym-posium: Computational Approaches to AnalyzingWeblogs, volume 100107.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Chong Long, Jie Zhang, and Xiaoyan Zhut.
2010.
Areview selection approach for accurate feature ratingestimation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 766?774.
Association for Computational Lin-guistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Arjun Mukherjee and Bing Liu.
2012.
Aspect ex-traction through semi-supervised modeling.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Long Papers-Volume 1, pages 339?348.
Association for Compu-tational Linguistics.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Haris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
in proceedings ofthe 8th international workshop on semantic evalua-tion (semeval 2014).
Dublin, Ireland.Jason DM Rennie.
2001.
Improving multi-class textclassification with naive Bayes.
Ph.D. thesis, Mas-sachusetts Institute of Technology.Zhiqiang Toh, Wenting Wang, Man Lan, and Xi-aoli Li.
2012.
An ner-based product identifica-tion and lucene-based product linking approach tocprod1 challenge: Description of submission sys-tem to cprod1 challenge.
In Data Mining Workshop-s (ICDMW), 2012 IEEE 12th International Confer-ence on, pages 869?871.
IEEE.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational linguistics, 35(3):399?433.Tian Tian Zhu, Fang Xi Zhang, and Man Lan.
2013.Ecnucs: A surface information based system de-scription of sentiment analysis in twitter in thesemeval-2013 (task 2).
Atlanta, Georgia, USA, page408.258
