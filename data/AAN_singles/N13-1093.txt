Proceedings of NAACL-HLT 2013, pages 765?771,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsExploiting the Scope of Negations and Heterogeneous Featuresfor Relation Extraction: A Case Study for Drug-Drug Interaction ExtractionMd.
Faisal Mahbub Chowdhury ?
?
and Alberto Lavelli ??
Fondazione Bruno Kessler (FBK-irst), Italy?
University of Trento, Italyfmchowdhury@gmail.com, lavelli@fbk.euAbstractThis paper presents an approach that exploitsthe scope of negation cues for relation extrac-tion (RE) without the need of using any specif-ically annotated dataset for building a separatenegation scope detection classifier.
New fea-tures are proposed which are used in two dif-ferent stages.
These also include non-targetentity specific features.
The proposed RE ap-proach outperforms the previous state of theart for drug-drug interaction (DDI) extraction.1 IntroductionNegation is a linguistic phenomenon where a nega-tion cue (e.g.
not) can alter the meaning of a partic-ular text segment or of a fact.
This text segment (orfact) is said to be inside the scope of that negation(cue).
In the context of RE, there is not much workthat aims to exploit the scope of negations.1 Theonly work on RE that we are aware of is Sanchez-Graillet and Poesio (2007) where they used variousheuristics to extract negative protein interaction.Despite the recent interest on automatically de-tecting the scope of negation2 till now there seemsto be no empirical evidence supporting its exploita-tion for the purpose of RE.
Even if we could man-age to obtain highly accurate automatically detected1In the context of event extraction (a closely related task ofRE), there have been efforts in BioNLP shared tasks of 2009 and2011 for (non-mandatory sub-task of) event negation detection(3 participants in 2009; 2 in 2011) (Kim et al 2009; Kim et al2011).
The participants approached the sub-task using eitherpre-defined patterns or some heuristics.2This task is popularized by various recently held sharedtasks (Farkas et al 2010; Morante and Blanco, 2012).negation scopes, it is not clear how to feed this infor-mation inside the RE approach.
Simply consideringwhether a pair of candidate mentions falls under thescope of a negation cue might not be helpful.In this paper, we propose that the scope of nega-tions can be exploited at two different levels.
Firstly,the system would check whether all the target en-tity3 mentions inside a sentence along with possiblerelation clues (or trigger words), if any, fall (directlyor indirectly) under the scope of a negation cue.
Ifsuch a sentence is found, then it should be discarded(i.e.
candidate mention pairs4 inside that sentencewould not be considered).
Secondly, for each of theremaining pairs of candidate mentions, the systemshould exploit features related to the scope of nega-tion (rather than simply adding a feature for negationcue, approach adopted in various RE systems) thatcan provide indication (if any such evidence exists)that the corresponding relation of interest actuallydoes not hold in that particular context.In the subsequent sections, we describe our ap-proach.
The RE task considered is drug-drug in-teraction (DDI) extraction.
The task has signifi-cant importance for public health safety.5 We used3The target entities, for example, for DDI extraction and forEMP-ORG relation extraction would be {DRUG} and {PER,GPE, ORG} respectively.
Any entity other than the target enti-ties (w.r.t.
the particular RE task) belongs to non-target entities.4Candidate mention pairs for RE are taken from target entitymentions.5After the death of pop star Michael Jackson, allegedly dueto DDI, it was reported that about 2.2 million people in USA,age 57 to 85, were taking potentially dangerous combinations ofdrugs (Landau, 2009).
An earlier report mentioned that deathsfrom accidental drug interactions rose 68 percent between 1999and 2004 (Payne, 2007).765the DDIExtraction-2011 challenge corpus (Segura-Bedmar et al 2011).
The official training and testdata of the corpus contain 4,267 and 1,539 sen-tences, and 2,402 and 755 DDI annotations respec-tively.2 Proposed Approach2.1 Stage 1: Exploiting scope of negation tofilter out sentencesWe propose a two stage RE approach.
In the firststage, our goal is to exploit the scope of negationsto reduce the number of candidate mention pairs bydiscarding sentences.
For this purpose, we proposethe following features to train a binary classifier:?
has2TM: If the sentence has exactly 2 target entitymentions (i.e.
drug mentions for DDI extraction).?
has3OrMoreTM: Whether the sentence has morethan 2 target entity mentions.?
allTMonRight: Whether all target entity mentionsinside the sentence appear after the negation cue.?
neitherAllTMonLeftOrRight: Whether some but notall target entity mentions appear after the negationcue.?
negCue: The negation cue itself.?
immediateGovernor: The word on which the cue isdirectly syntactically dependent.?
nearestVerbGovernor: The nearest verb in the de-pendency graph on which the cue is syntacticallydependent.?
isVerbGovernorRoot: Whether the nearestVerb-Governor is root of the dependency graph of thesentence.?
allTMdependentOnNVG: Whether all target en-tity mentions are syntactically dependent (di-rectly/indirectly) on the nearestVerbGovernor.?
allButOneTMdependentOnNVG: Whether all butone target entity mentions are syntactically depen-dent on the nearestVerbGovernor.?
although*PrecedeCue: Whether the syntacticclause containing the negation cue begins with ?al-though / though / despite / in spite?.?
commaBeforeNextTM: Whether there is a comma inthe text between the negation cue and the next targetentity mention after the cue.?
commaAfterPrevTM: Whether there is a comma inthe text between the previous target entity mentionbefore the negation cue and the cue itself.?
sentHasBut: Whether the sentence contains theword ?but?.The objective of the classifier is to decide whetherall of the target entity mentions (i.e.
drugs) as well asany possible evidence of the relation of interest (forwhich we assume the immediate and the nearest verbgovernors of the negation cue would be good candi-dates) inside the corresponding sentence fall underthe scope of a negation cue in such a way that thesentence is unlikely to contain a DDI.At present, we limit our focus only on the firstoccurrence of the following negation cues: ?no?,?n?t?
or ?not?.6 In the Stage 1, any sentence thatcontains at least one DDI is considered by the clas-sifier as a positive (training/test) instance.
Other sen-tences are considered as negative instances.
We ruleout any sentence (i.e.
we do not consider as train-ing/test instance for the classifier that filters less in-formative sentences) during both training and testingif any of the following conditions holds:?
The sentence contains less than two target entitymentions (such sentence would not contain the re-lation of interest anyway).?
It has any of the following phrases ?
?not recom-mended?, ?should not be?
or ?must not be?.7?
There is no ?no?, ?n?t?
or ?not?
in the sentence.?
No target entity mention appears in the sentence af-ter ?no?, ?n?t?
or ?not?.To assess the effectiveness of the proposed Stage1 classifier, we defined a baseline classifier that fil-ters any sentence that contains ?no?, ?n?t?
or ?not?.2.2 Stage 2Once the sentences which are likely to have no DDIare identified and removed, the next step is to ap-ply a state-of-the-art RE approach on the remainingsentences.
In this section, we propose a new hybridkernel, KHybrid, for this purpose.
It is defined asfollows:KHybrid (R1, R2) = KHF (R1, R2) + KSL(R1, R2) + w * KPET (R1, R2)6These cues usually occur more frequently and generallyhave larger negation scope than other negation cues.7These expressions often provide clues that one of the bio-entity mentions negatively influences the level of activity of theother.766Here, KHF stands for a new feature based kernel(proposed in this paper) that uses a heterogeneousset of features.
KSL stands for the Shallow Linguis-tic (SL) kernel proposed by Giuliano et al(2006).KPET stands for the Path-enclosed Tree (PET) ker-nel (Moschitti, 2004).
w is a multiplicative constantused for the PET kernel.
It allows the hybrid kernelto assign more (or less) weight to the informationobtained using tree structures depending on the cor-pus.The proposed kernel composition is valid accord-ing to the closure properties of kernels.
We ex-ploit the SVM-Light-TK toolkit (Moschitti, 2006;Joachims, 1999) for kernel computation.
In Stage2, each candidate drug mention pair represents aninstance.2.2.1 Proposed KHF kernelAs mentioned earlier, this proposed kernel usesheterogeneous features.
The first version of the het-erogeneous feature set (henceforth, HF v1) com-bines features proposed by two previous RE works.The former is Zhou et al(2005), which uses 51 dif-ferent features.
We select the following 27 of theirfeatures for our feature set:WBNULL, WBFL, WBF, WBL, WBO,BM1F, BM1L, AM2F, AM2L, #MB, #WB,CPHBNULL, CPHBFL, CPHBF, CPHBL,CPHBO, CPHBM1F, CPHBM1L, CPHAM2F,CPHAM2F, CPP, CPPH, ET12SameNP,ET12SamePP, ET12SameVP, PTP, PTPHThe latter is the TPWF kernel (Chowdhury andLavelli, 2012a) from which we use following fea-tures:HasTriggerWord, Trigger-X, DepPattern-i, e-walk, v-walkThe TPWF kernel extracts the HasTriggerWord,Trigger-X and DepPattern-i features from a sub-graph called reduced graph.
We also follow this ap-proach with one minor difference.
Unlike Chowd-hury and Lavelli (2012a), we look for trigger wordsin the whole reduced graph instead of using only theroot of the sub-graph.Due to space limitation we refer the readers tothe corresponding papers for the description of theabove mentioned features and the definition of re-duced graph.In addition, HF v1 also includes surrounding to-kens within the window of {-2,+2} for each candi-date mention.
We are unaware of any available listof trigger words for drug-drug interaction.
So, wecreated such a list.8We extend the heterogeneous feature set byadding features related to the scope of negation(henceforth, HF v2).
We use a list of 13 negationcues9 to search inside the reduced graph of a candi-date pair.
If the reduced graph contains any of thenegation cues or their morphological variants thenwe add the following features:?
negCue: The corresponding negation cue.?
immediateNegatedWord: If the word following thenegation cue is neither a preposition nor a ?be verb?,then that word, otherwise the word after the nextword.10Furthermore, if the corresponding matched nega-tion cue is either ?no?, ?n?t?
or ?not?, then we addadditional features related to negation scope:?
bothEntDependOnImmediateGovernor: Whetherthe immediate governor (if any) of the negation cueis also governor of a dependency sub-tree (of the de-pendency graph of the corresponding sentence) thatincludes both of the candidate mentions.?
immediateGovernorIsVerbGovernor: Whether theimmediate governor of the negation cue is a verb.?
nearestVerbGovernor: The closest verb governor(i.e.
parent or grandparent inside the dependencygraph), if any, of the negation cue.We further extend the heterogeneous feature setby adding features related to relevant non-target en-tities (with respect to the relation of interest; hence-forth, HF v3).
For the purpose of DDI extrac-tion, we deem the presence of DISEASE mentions(which might result as a consequence of a DDI)can provide some clues.
So, we use a publiclyavailable state-of-the-art disease NER system calledBioEnEx (Chowdhury and Lavelli, 2010) to anno-tate the DDIExtraction-2011 challenge corpus.
For8The RE system developed for this work and the cre-ated list of trigger words for DDI can be downloaded fromhttps://github.com/fmchowdhury/HyREX .9No, not, neither, without, lack, fail, unable, abrogate, ab-sence, prevent, unlikely, unchanged, rarely.10For example, ?interested?
from ?...
not interested ...?, and?confused?
from ?...
not to be confused ...?.767each candidate (drug) mention pair, we add the fol-lowing features in HF v3:?
NTEMinsideSentence: Whether the correspondingsentence contains important non-target entity men-tion(s) (e.g.
disease for DDI).?
immediateGovernorIsVerbGovernorOfNTEM: Theimmediate governor (if any) of the non-target entitymention, only if such governor is also governing adependency sub-tree that includes both of the targetcandidate entity mentions.?
nearestVerbGovernorOfNTEM: The closest verbgovernor (if any) of the non-target entity mention,only if it also governs the candidate entity mentions.?
immediateGovernorIsVerbGovernorOfNTEM:Whether the immediate governor is a verb.3 Results and DiscussionWe train a linear SVM classifier in Stage 1 andtune the hyper-parameters (by doing 5-fold cross-validation) for obtaining maximum possible recall.In this way we minimize the number of false neg-atives (i.e.
sentences that contain DDIs but arewrongly identified as not having any).During the cross-validation experiments on thetraining data, 334 sentences (7.83% of the total sen-tences) containing at least 2 drug mentions wereidentified by our proposed classifier (in Section 2.1)as unlikely to have any DDI and hence are candi-dates for discarding.
Only 19 of these sentenceswere incorrectly identified.
When we trained onthe training data and tested on the official test dataof DDIExtraction-2011 challenge corpus, 121 sen-tences (7.86% of the total test sentences) were iden-tified by the classifier as candidates for discarding.Only 5 of them were incorrectly identified.Unlike Stage 1, in Stage 2 where we train the hy-brid kernel based RE classifier and use it for RE (i.e.DDI extraction) from the test data, sentences are notthe RE training/test instances.
Instead, a RE instancecorresponds to a candidate mention pair.All the DDIs (i.e.
positive RE instances) of theincorrectly identified sentences in Stage 1 (i.e.
thesentences which are incorrectly labelled as not hav-ing any DDI and filtered) are automatically consid-ered as false negatives during the calculation of DDIextraction results in Stage 2.To verify whether our proposed hybrid kernelachieves state-of-the-art results without taking ben-efits of the output of Stage 1, we did some experi-ments without discarding any sentence.
These ex-periments are done using Zhou et al(2005), TPWFkernel, SL kernel, different versions of proposedKHF kernel and KHybrid kernel.
Table 1 showsthe results of 5-fold cross-validation experiments(hyper-parameters are tuned for obtaining maximumF-score).
As the results show, there is a gain +0.9points in F-score (mainly due to the boost in re-call) after the addition of features related to negationscope.
There is also some minor improvement dueto the proposed non-target entity specific features.We also performed (5-fold cross validation) ex-periments by combining the Stage 1 classifier witheach of the Zhou et al(2005), TPWF kernel, SLkernel, PET kernel, KHF kernel and KHybrid kernelseparately (only the results of KHybrid are reportedin Table 1 due to space limitation).
In each case,there were improvements in precision, recall and F-score.
The gain in F-score ranged from 1.0 to 1.4points.P / R / F-scoreUsing SL kernel (Giuliano et al 2006) 51.3 / 64.7 / 57.3Using (Zhou et al 2005) 58.7 / 37.1 / 45.5Using PET kernel (Moschitti, 2004) 46.8 / 602 / 52.7TPWF (Chowdhury and Lavelli, 2012a) 43.7 / 60.7 / 50.8Proposed approachesProposed KHF v1 53.4 / 51.5 / 52.4KHF v2 (i.e.
+ neg scope feat.)
53.9 / 52.6 / 53.3 (+0.9)KHF v3 (i.e.
+ non-target entity feat.)
53.6 / 53.5 / 53.6 (+0.3)Proposed KHybrid 56.3 / 68.5 / 61.8Proposed KHybrid with Stage 1 57.3 / 69.4 / 62.8 (+1.0)Table 1: 5-fold cross-validation results on training data.Table 2 reports the results of the previously pub-lished studies that used the same corpus.
Our pro-posed KHybrid kernel obtains an F-score that ishigher than that of the previous state of the art.When the Stage 1 classifier (based on negationscope features) is exploited before using the KHybridkernel, the F-score reaches up to 67.4.
This is+1.0 points higher than without exploiting the Stage1 classifier and +1.7 higher than previous state of768the art.
We did separate experiments (also reportedin Table 2) to assess the performance improvementwhen the output of Stage 1 is used to filter sentencesfrom either training or test data only.
The resultsremain the same when only training sentences arefiltered; while there are some improvements whenonly test sentences are filtered.
Filtering both train-ing and test sentences provides the larger gain whichis statistically significant.Usually, the number of negative instances in acorpus is much higher than that of the positive in-stances.
In a recent work, Chowdhury and Lavelli(2012b) showed that by removing less informative(negative) instances (henceforth, LIIs), not only theskewness in instance distribution could be reducedbut it also leads to a better result.
The proposedStage 1 classifier, presented in this work, also re-duces skewness in instance distribution.
This is be-cause we are only removing those sentences that areunlikely to contain any positive instance.
So, in prin-ciple, the Stage 1 classifier is focused on removingonly negative instances (although the classifier mis-takenly discards few positive instances, too).We wanted to study how the Stage 1 classifierwould contribute if we use it on top of the tech-niques that were proposed in Chowdhury and Lavelli(2012b) to remove LIIs.
As Table 2 shows, by usingthe Stage 1 classifier along with LLI filtering, wecould further improve the results (+3.2 points differ-ence in F-score with the previous state of the art).4 ConclusionA major flexibility in the proposed approach is thatit does not require a separate dataset (which needsto match the genre of the text to be used for RE)annotated with negation scopes.
Instead, the pro-posed Stage 1 classifier uses the RE training data(which do not have negation scope annotations) toself-supervise itself.
Various new features have beenexploited (both in stages 1 and 2) that can providestrong indications of the scope of negation cues withrespect to the relation to be extracted.
The only thingneeded is the list of possible negation cues (Morante(2010) includes such a comprehensive list).Our proposed kernel, which has a component thatexploits a heterogeneous set of features includingnegation scope and presence of non-target entities,already obtains better results than previous studies.P R F-score(Thomas et al 2011) 60.5 71.9 65.7(Chowdhury et al 2011) 58.6 70.5 64.0(Chowdhury and Lavelli, 2011) 58.4 70.1 63.7(Bjorne et al 2011) 58.0 68.9 63.0Proposed KHybrid 60.0 74.3 66.4KHybrid + Stage 1 baseline 61.8 68.9 65.1KHybrid + proposed Stage 1 60.0 74.2 66.4(only training sentences are filtered)KHybrid + proposed Stage 1 61.4 73.8 67.0(only test sentences are filtered)KHybrid + proposed Stage 1 62.1 73.8 67.4 stat.
sig.
(both training and test sentences are filtered)Proposed KHybrid + LII filtering 61.1 75.1 67.4 stat.
sig.Proposed KHybrid + LII filtering 63.5 75.2 68.9 stat.
sig.+ proposed Stage 1Table 2: Results obtained on the official test set of the2011 DDI Extraction challenge.
LII filtering refers to thetechniques proposed in Chowdhury and Lavelli (2012b)for reducing skewness in RE data distribution.
stat.
sig.
in-dicates that the improvement of F-score, due to usage ofStage 1 classifier, is statistically significant (verified usingApproximate Randomization Procedure (Noreen, 1989);number of iterations = 1,000, confidence level = 0.01).The results considerably improve when possible ir-relevant sentences from both training and test dataare filtered by exploiting features related to the scopeof negations.In future, we would like to exploit the scope ofmore negation cues, apart from the three cues thatare used in this study.
We believe our approachwould help to improve RE in other genres of text(such as newspaper) as well.AcknowledgementThis work was carried out in the context of theproject ?eOnco - Pervasive knowledge and datamanagement in cancer care?.ReferencesJ Bjorne, A Airola, T Pahikkala, and T Salakoski.
2011.Drug-drug interaction extraction with RLS and SVMclassifiers.
In Proceedings of the 1st Challenge taskon Drug-Drug Interaction Extraction (DDIExtraction2011), pages 35?42, Huelva, Spain, September.769MFM Chowdhury and A Lavelli.
2010.
Disease mentionrecognition with specific features.
In Proceedings ofthe 2010 Workshop on Biomedical Natural LanguageProcessing, pages 83?90, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.MFM Chowdhury and A Lavelli.
2011.
Drug-drug inter-action extraction using composite kernels.
In Proceed-ings of the 1st Challenge task on Drug-Drug Interac-tion Extraction (DDIExtraction 2011), pages 27?33,Huelva, Spain, September.MFM Chowdhury and A Lavelli.
2012a.
Combining treestructures, flat features and patterns for biomedical re-lation extraction.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL 2012), pages 420?429, Avignon, France, April.
Association for Compu-tational Linguistics.MFM Chowdhury and A Lavelli.
2012b.
Impact of LessSkewed Distributions on Efficiency and Effectivenessof Biomedical Relation Extraction.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (COLING 2012) : Posters, pages 205?216,Mumbai, India, December.MFM Chowdhury, AB Abacha, A Lavelli, andP Zweigenbaum.
2011.
Two different machine learn-ing techniques for drug-drug interaction extraction.
InProceedings of the 1st Challenge task on Drug-DrugInteraction Extraction (DDIExtraction 2011), pages19?26, Huelva, Spain, September.R Farkas, V Vincze, G Mo?ra, J Csirik, and G Szarvas.2010.
The CoNLL-2010 shared task: Learning to de-tect hedges and their scope in natural language text.In Proceedings of the Fourteenth Conference on Com-putational Natural Language Learning, pages 1?12,Uppsala, Sweden, July.
Association for ComputationalLinguistics.C Giuliano, A Lavelli, and L Romano.
2006.
Exploit-ing shallow linguistic information for relation extrac-tion from biomedical literature.
In Proceedings of the11th Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL 2006),pages 401?408.T Joachims.
1999.
Making large-scale support vec-tor machine learning practical.
In Advances in ker-nel methods: support vector learning, pages 169?184.MIT Press, Cambridge, MA, USA.JD Kim, T Ohta, S Pyysalo, Y Kano, and J Tsujii.
2009.Overview of BioNLP?09 shared task on event extrac-tion.
In Proceedings of the BioNLP 2009 WorkshopCompanion Volume for Shared Task, pages 1?9, Boul-der, Colorado, June.
Association for ComputationalLinguistics.JD Kim, Y Wang, T Takagi, and A Yonezawa.
2011.Overview of Genia event task in BioNLP shared task2011.
In Proceedings of BioNLP Shared Task 2011Workshop, pages 7?15, Portland, Oregon, USA, June.Association for Computational Linguistics.E Landau.
2009.
Jackson?s death raises questions aboutdrug interactions [Published in CNN; June 26, 2009].http://articles.cnn.com/2009-06-26/health/jackson.drug.interaction.caution_1_drug-interactions-heart-rhythms-antidepressants?_s=PM:HEALTH.R Morante and E Blanco.
2012.
*SEM 2012 shared task:Resolving the scope and focus of negation.
In *SEM2012: The First Joint Conference on Lexical and Com-putational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 265?274,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.R Morante.
2010.
Descriptive Analysis of NegationCue in Biomedical Texts.
In Proceedings of the 7thInternational Conference on Language Resources andEvaluation (LREC 2010), Malta.A Moschitti.
2004.
A study on convolution kernels forshallow semantic parsing.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics (ACL 2004), Barcelona, Spain.A Moschitti.
2006.
Making Tree Kernels Practicalfor Natural Language Learning.
In Proceedings of11th Conference of the European Chapter of the As-sociation for computational Linguistics (EACL 2006),Trento, Italy.EW Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses : An Introduction.
Wiley-Interscience, April.JW Payne.
2007.
A Dangerous Mix [Publishedin The Washington Post; February 27, 2007].http://www.washingtonpost.com/wp-dyn/content/article/2007/02/23/AR2007022301780.html.O Sanchez-Graillet and M Poesio.
2007.
Negation ofprotein-protein interactions: analysis and extraction.Bioinformatics, 23(13):i424?i432.I Segura-Bedmar, P Mart?
?nez, and CD Pablo-Sa?nchez.2011.
The 1st DDIExtraction-2011 challenge task:Extraction of Drug-Drug Interactions from biomedi-cal texts.
In Proceedings of the 1st Challenge taskon Drug-Drug Interaction Extraction (DDIExtraction2011), pages 1?9, Huelva, Spain, September.P Thomas, M Neves, I Solt, D Tikk, and U Leser.2011.
Relation extraction for drug-drug interactionsusing ensemble learning.
In Proceedings of the 1stChallenge task on Drug-Drug Interaction Extraction770(DDIExtraction 2011), pages 11?18, Huelva, Spain,September.GD Zhou, J Su, J Zhang, and M Zhang.
2005.
Ex-ploring various knowledge in relation extraction.
InProceedings of the 43rd Annual Meeting on Associa-tion for Computational Linguistics (ACL 2005), pages427?434, Ann Arbor, Michigan, USA.771
