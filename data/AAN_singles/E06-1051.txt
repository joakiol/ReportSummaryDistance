Exploiting Shallow Linguistic Information forRelation Extraction from Biomedical LiteratureClaudio Giuliano and Alberto Lavelli and Lorenza RomanoITC-irstVia Sommarive, 1838050, Povo (TN)Italy{giuliano,lavelli,romano}@itc.itAbstractWe propose an approach for extracting re-lations between entities from biomedicalliterature based solely on shallow linguis-tic information.
We use a combination ofkernel functions to integrate two differentinformation sources: (i) the whole sen-tence where the relation appears, and (ii)the local contexts around the interactingentities.
We performed experiments on ex-tracting gene and protein interactions fromtwo different data sets.
The results showthat our approach outperforms most of theprevious methods based on syntactic andsemantic information.1 IntroductionInformation Extraction (IE) is the process of find-ing relevant entities and their relationships withintextual documents.
Applications of IE range fromSemantic Web to Bioinformatics.
For example,there is an increasing interest in automaticallyextracting relevant information from biomedi-cal literature.
Recent evaluation campaigns onbio-entity recognition, such as BioCreAtIvE andJNLPBA 2004 shared task, have shown that sev-eral systems are able to achieve good performance(even if it is a bit worse than that reported on newsarticles).
However, relation identification is moreuseful from an applicative perspective but it is stilla considerable challenge for automatic tools.In this work, we propose a supervised machinelearning approach to relation extraction which isapplicable even when (deep) linguistic process-ing is not available or reliable.
In particular, weexplore a kernel-based approach based solely onshallow linguistic processing, such as tokeniza-tion, sentence splitting, Part-of-Speech (PoS) tag-ging and lemmatization.Kernel methods (Shawe-Taylor and Cristianini,2004) show their full potential when an explicitcomputation of the feature map becomes compu-tationally infeasible, due to the high or even infi-nite dimension of the feature space.
For this rea-son, kernels have been recently used to developinnovative approaches to relation extraction basedon syntactic information, in which the examplespreserve their original representations (i.e.
parsetrees) and are compared by the kernel function(Zelenko et al, 2003; Culotta and Sorensen, 2004;Zhao and Grishman, 2005).Despite the positive results obtained exploitingsyntactic information, we claim that there is stillroom for improvement relying exclusively on shal-low linguistic information for two main reasons.First of all, previous comparative evaluations putmore stress on the deep linguistic approaches anddid not put as much effort on developing effec-tive methods based on shallow linguistic informa-tion.
A second reason concerns the fact that syn-tactic parsing is not always robust enough to dealwith real-world sentences.
This may prevent ap-proaches based on syntactic features from produc-ing any result.
Another related issue concerns thefact that parsers are available only for few lan-guages and may not produce reliable results whenused on domain specific texts (as is the case ofthe biomedical literature).
For example, most ofthe participants at the Learning Language in Logic(LLL) challenge on Genic Interaction Extraction(see Section 4.2) were unable to successfully ex-ploit linguistic information provided by parsers.
Itis still an open issue whether the use of domain-specific treebanks (such as the Genia treebank1)1http://www-tsujii.is.s.u-tokyo.ac.jp/401can be successfully exploited to overcome thisproblem.
Therefore it is essential to better investi-gate the potential of approaches based exclusivelyon simple linguistic features.In our approach we use a combination of ker-nel functions to represent two distinct informa-tion sources: the global context where entities ap-pear and their local contexts.
The whole sentencewhere the entities appear (global context) is usedto discover the presence of a relation between twoentities, similarly to what was done by Bunescuand Mooney (2005b).
Windows of limited sizearound the entities (local contexts) provide use-ful clues to identify the roles of the entities withina relation.
The approach has some resemblancewith what was proposed by Roth and Yih (2002).The main difference is that we perform the extrac-tion task in a single step via a combined kernel,while they used two separate classifiers to identifyentities and relations and their output is later com-bined with a probabilistic global inference.We evaluated our relation extraction algorithmon two biomedical data sets (i.e.
the AImed cor-pus and the LLL challenge data set; see Section4).
The motivations for using these benchmarksderive from the increasing applicative interest intools able to extract relations between relevant en-tities in biomedical texts and, consequently, fromthe growing availability of annotated data sets.The experiments show clearly that our approachconsistently improves previous results.
Surpris-ingly, it outperforms most of the systems based onsyntactic or semantic information, even when thisinformation is manually annotated (i.e.
the LLLchallenge).2 Problem FormalizationThe problem considered here is that of iden-tifying interactions between genes and proteinsfrom biomedical literature.
More specifically, weperformed experiments on two slightly differentbenchmark data sets (see Section 4 for a detaileddescription).
In the former (AImed) gene/proteininteractions are annotated without distinguishingthe type and roles of the two interacting entities.The latter (LLL challenge) is more realistic (andcomplex) because it also aims at identifying theroles played by the interacting entities (agent andtarget).
For example, in Figure 1 three entitiesare mentioned and two of the six ordered pairs ofGENIA/topics/Corpus/GTB.htmlentities actually interact: (sigma(K), cwlH) and(gerE, cwlH).Figure 1: A sentence with two relations, R12 andR32, between three entities, E1, E2 and E3.In our approach we cast relation extraction as aclassification problem, in which examples are gen-erated from sentences as follows.First of all, we describe the complex case,namely the protein/gene interactions (LLL chal-lenge).
For this data set entity recognition is per-formed using a dictionary of protein and genenames in which the type of the entities is unknown.We generate examples for all the sentences con-taining at least two entities.
Thus the number ofexamples generated for each sentence is given bythe combinations of distinct entities (N ) selectedtwo at a time, i.e.
NC2.
For example, as the sen-tence shown in Figure 1 contains three entities, thetotal number of examples generated is 3C2 = 3.
Ineach example we assign the attribute CANDIDATEto each of the candidate interacting entities, whilethe other entities in the example are assigned theattribute OTHER, meaning that they do not partici-pate in the relation.
If a relation holds between thetwo candidate interacting entities the example islabeled 1 or 2 (according to the roles of the inter-acting entities, agent and target, i.e.
to the direc-tion of the relation); 0 otherwise.
Figure 2 showsthe examples generated from the sentence in Fig-ure 1.Figure 2: The three protein-gene examples gener-ated from the sentence in Figure 1.Note that in generating the examples from thesentence in Figure 1 we did not create three neg-402ative examples (there are six potential ordered re-lations between three entities), thereby implicitlyunder-sampling the data set.
This allows us tomake the classification task simpler without loos-ing information.
As a matter of fact, generatingexamples for each ordered pair of entities wouldproduce two subsets of the same size containingsimilar examples (differing only for the attributesCANDIDATE and OTHER), but with different clas-sification labels.
Furthermore, under-sampling al-lows us to halve the data set size and reduce thedata skewness.For the protein-protein interaction task (AImed)we use the correct entities provided by the manualannotation.
As said at the beginning of this sec-tion, this task is simpler than the LLL challengebecause there is no distinction between types (allentities are proteins) and roles (the relation is sym-metric).
As a consequence, the examples are gen-erated as described above with the following dif-ference: an example is labeled 1 if a relation holdsbetween the two candidate interacting entities; 0otherwise.3 Kernel Methods for RelationExtractionThe basic idea behind kernel methods is to embedthe input data into a suitable feature space F viaa mapping function ?
: X ?
F , and then usea linear algorithm for discovering nonlinear pat-terns.
Instead of using the explicit mapping ?, wecan use a kernel function K : X ?
X ?
R, thatcorresponds to the inner product in a feature spacewhich is, in general, different from the input space.Kernel methods allow us to design a modularsystem, in which the kernel function acts as aninterface between the data and the learning algo-rithm.
Thus the kernel function is the only domainspecific module of the system, while the learningalgorithm is a general purpose component.
Po-tentially any kernel function can work with anykernel-based algorithm.
In our approach we useSupport Vector Machines (Vapnik, 1998).In order to implement the approach based onshallow linguistic information we employed alinear combination of kernels.
Different works(Gliozzo et al, 2005; Zhao and Grishman, 2005;Culotta and Sorensen, 2004) empirically demon-strate the effectiveness of combining kernels inthis way, showing that the combined kernel alwaysimproves the performance of the individual ones.In addition, this formulation allows us to evalu-ate the individual contribution of each informa-tion source.
We designed two families of kernels:Global Context kernels and Local Context kernels,in which each single kernel is explicitly calculatedas followsK(x1, x2) =??
(x1), ?(x2)???(x1)???
(x2)?, (1)where ?(?)
is the embedding vector and ?
?
?
is the2-norm.
The kernel is normalized (divided) by theproduct of the norms of embedding vectors.
Thenormalization factor plays an important role in al-lowing us to integrate information from heteroge-neous feature spaces.
Even though the resultingfeature space has high dimensionality, an efficientcomputation of Equation 1 can be carried out ex-plicitly since the input representations defined be-low are extremely sparse.3.1 Global Context KernelIn (Bunescu and Mooney, 2005b), the authors ob-served that a relation between two entities is gen-erally expressed using only words that appear si-multaneously in one of the following three pat-terns:Fore-Between: tokens before and between thetwo candidate interacting entities.
For in-stance: binding of [P1] to [P2], interaction in-volving [P1] and [P2], association of [P1] by[P2].Between: only tokens between the two candidateinteracting entities.
For instance: [P1] asso-ciates with [P2], [P1] binding to [P2], [P1],inhibitor of [P2].Between-After: tokens between and after the twocandidate interacting entities.
For instance:[P1] - [P2] association, [P1] and [P2] interact,[P1] has influence on [P2] binding.Our global context kernels operate on the patternsabove, where each pattern is represented using abag-of-words instead of sparse subsequences ofwords, PoS tags, entity and chunk types, or Word-Net synsets as in (Bunescu and Mooney, 2005b).More formally, given a relation example R, werepresent a pattern P as a row vector?P (R) = (tf(t1, P ), tf(t2, P ), .
.
.
, tf(tl, P )) ?
Rl, (2)where the function tf(ti, P ) records how manytimes a particular token ti is used in P .
Note that,403this approach differs from the standard bag-of-words as punctuation and stop words are includedin ?P , while the entities (with attribute CANDI-DATE and OTHER) are not.
To improve the clas-sification performance, we have further extended?P to embed n-grams of (contiguous) tokens (upto n = 3).
By substituting ?P into Equation 1, weobtain the n-gram kernel Kn, which counts com-mon uni-grams, bi-grams, .
.
.
, n-grams that twopatterns have in common2.
The Global Contextkernel KGC(R1, R2) is then defined asKFB(R1, R2) +KB(R1, R2) +KBA(R1, R2), (3)where KFB , KB and KBA are n-gram kernelsthat operate on the Fore-Between, Between andBetween-After patterns respectively.3.2 Local Context KernelThe type of the candidate interacting entities canprovide useful clues for detecting the agent andtarget of the relation, as well as the presence of therelation itself.
As the type is not known, we usethe information provided by the two local contextsof the candidate interacting entities, called left andright local context respectively.
As typically donein entity recognition, we represent each local con-text by using the following basic features:Token The token itself.Lemma The lemma of the token.PoS The PoS tag of the token.Orthographic This feature maps each token intoequivalence classes that encode attributessuch as capitalization, punctuation, numeralsand so on.Formally, given a relation example R, a local con-text L = t?w, .
.
.
, t?1, t0, t+1, .
.
.
, t+w is repre-sented as a row vector?L(R) = (f1(L), f2(L), .
.
.
, fm(L)) ?
{0, 1}m, (4)where fi is a feature function that returns 1 if it isactive in the specified position of L, 0 otherwise3.The Local Context kernel KLC(R1, R2) is definedasKleft(R1, R2) +Kright(R1, R2), (5)whereKleft andKright are defined by substitutingthe embedding of the left and right local contextinto Equation 1 respectively.2In the literature, it is also called n-spectrum kernel.3In the reported experiments, we used a context windowof ?2 tokens around the candidate entity.Notice that KLC differs substantially fromKGC as it considers the ordering of the tokens andthe feature space is enriched with PoS, lemma andorthographic features.3.3 Shallow Linguistic KernelFinally, the Shallow Linguistic kernelKSL(R1, R2) is defined asKGC(R1, R2) +KLC(R1, R2).
(6)It follows directly from the explicit constructionof the feature space and from closure properties ofkernels that KSL is a valid kernel.4 Data setsThe two data sets used for the experiments concernthe same domain (i.e.
gene/protein interactions).However, they present a crucial difference whichmakes it worthwhile to show the experimental re-sults on both of them.
In one case (AImed) in-teractions are considered symmetric, while in theother (LLL challenge) agents and targets of genicinteractions have to be identified.4.1 AImed corpusThe first data set used in the experiments is theAImed corpus4, previously used for training pro-tein interaction extraction systems in (Bunescu etal., 2005; Bunescu and Mooney, 2005b).
It con-sists of 225 Medline abstracts: 200 are knownto describe interactions between human proteins,while the other 25 do not refer to any interaction.There are 4,084 protein references and around1,000 tagged interactions in this data set.
In thisdata set there is no distinction between genes andproteins and the relations are symmetric.4.2 LLL ChallengeThis data set was used in the Learning Languagein Logic (LLL) challenge on Genic Interactionextraction5 (Nede?llec, 2005).
The objective ofthe challenge was to evaluate the performance ofsystems based on machine learning techniques toidentify gene/protein interactions and their roles,agent or target.
The data set was collected byquerying Medline on Bacillus subtilis transcrip-tion and sporulation.
It is divided in a training set(80 sentences describing 271 interactions) and a4ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/interactions.tar.gz5http://genome.jouy.inra.fr/texte/LLLchallenge/404test set (87 sentences describing 106 interactions).Differently from the training set, the test set con-tains sentences without interactions.
The data setis decomposed in two subsets of increasing diffi-culty.
The first subset does not include corefer-ences, while the second one includes simple casesof coreference, mainly appositions.
Both subsetsare available with different kinds of annotation:basic and enriched.
The former includes word andsentence segmentation.
The latter also includesmanually checked information, such as lemma andsyntactic dependencies.
A dictionary of namedentities (including typographical variants and syn-onyms) is associated to the data set.5 ExperimentsBefore describing the results of the experiments,a note concerning the evaluation methodology.There are different ways of evaluating perfor-mance in extracting information, as noted in(Lavelli et al, 2004) for the extraction of slotfillers in the Seminar Announcement and the JobPosting data sets.
Adapting the proposed classi-fication to relation extraction, the following twocases can be identified:?
One Answer per Occurrence in the Document?
OAOD (each individual occurrence of aprotein interaction has to be extracted fromthe document);?
One Answer per Relation in a given Docu-ment ?
OARD (where two occurrences of thesame protein interaction are considered onecorrect answer).Figure 3 shows a fragment of tagged text drawnfrom the AImed corpus.
It contains three differentinteractions between pairs of proteins, for a totalof seven occurrences of interactions.
For example,there are three occurrences of the interaction be-tween IGF-IR and p52Shc (i.e.
number 1, 3 and7).
If we adopt the OAOD methodology, all theseven occurrences have to be extracted to achievethe maximum score.
On the other hand, if we usethe OARD methodology, only one occurrence foreach interaction has to be extracted to maximizethe score.On the AImed data set both evaluations wereperformed, while on the LLL challenge only theOAOD evaluation methodology was performedbecause this is the only one provided by the eval-uation server of the challenge.Figure 3: Fragment of the AImed corpus with allproteins and their interactions tagged.
The pro-tein names have been highlighted in bold face andtheir same subscript numbers indicate interactionbetween the proteins.5.1 Implementation DetailsAll the experiments were performed using theSVM package LIBSVM6 customized to embed ourown kernel.
For the LLL challenge submission,we optimized the regularization parameter C by10-fold cross validation; while we used its defaultvalue for the AImed experiment.
In both exper-iments, we set the cost-factor Wi to be the ratiobetween the number of negative and positive ex-amples.5.2 Results on AImedKSL performance was first evaluated on theAImed data set (Section 4.1).
We first give anevaluation of the kernel combination and then wecompare our results with the Subsequence Ker-nel for Relation Extraction (ERK) described in(Bunescu and Mooney, 2005b).
All experimentsare conducted using 10-fold cross validation onthe same data splitting used in (Bunescu et al,2005; Bunescu and Mooney, 2005b).Table 1 shows the performance of the three ker-nels defined in Section 3 for protein-protein in-teractions using the two evaluation methodologiesdescribed above.We report in Figure 4 the precision-recall curvesof ERK andKSL using OARD evaluation method-ology (the evaluation performed by Bunescu andMooney (2005b)).
As in (Bunescu et al, 2005;Bunescu andMooney, 2005b), the graph points areobtained by varying the threshold on the classifi-6http://www.csie.ntu.edu.tw/?cjlin/libsvm/405OAODKernel Precision Recall F1KGC 57.7 60.1 58.9KLC 37.3 56.3 44.9KSL 60.9 57.2 59.0OARDKernel Precision Recall F1KGC 58.9 66.2 62.2KLC 44.8 67.8 54.0KSL 64.5 63.2 63.9ERK 65.0 46.4 54.2Table 1: Performance on the AImed data set us-ing the two evaluation methodologies, OAOD andOARD.cation confidence7.
The results clearly show thatKSL outperforms ERK, especially in term of re-call (see Table 1).00.20.40.60.810 0.2 0.4 0.6 0.8 1PrecisionRecallKSL vs. ERKERKKSLFigure 4: Precision-recall curves on the AImeddata set using OARD evaluation methodology.Finally, Figure 5 shows the learning curve of thecombined kernel KSL using the OARD evaluationmethodology.
The curve reaches a plateau witharound 100 Medline abstracts.5.3 Results on LLL challengeThe system was evaluated on the ?basic?
versionof the LLL challenge data set (Section 4.2).Table 2 shows the results of KSL returned bythe scoring service8 for the three subsets of thetraining set (with and without coreferences, andwith their union).
Table 3 shows the best resultsobtained at the official competition performed inApril 2005.
Comparing the results we see thatKSL trained on each subset outperforms the best7For this purpose the probability estimate output of LIB-SVM is used.8http://genome.jouy.inra.fr/texte/LLLchallenge/scoringService.php00.20.40.60.810 50 100 150 200F 1Number of documentsFigure 5: KSL learning curve on the AImed dataset using OARD evaluation methodology.Coref.
Precision Recall F1all 56.0 61.4 58.6with 29.0 31.0 30.0without 54.8 62.9 58.6Table 2: KSL performance on the LLL challengetest set using only the basic linguistic information.systems of the LLL challenge9.
Notice that thebest results at the challenge were obtained by dif-ferent groups and exploiting the linguistic ?en-riched?
version of the data set.
As observed in(Nede?llec, 2005), the scores obtained using thetraining set without coreferences and the wholetraining set are similar.We also report in Table 4 an analysis of the ker-nel combination.
Given that we are interested herein the contribution of each kernel, we evaluatedthe experiments by 10-fold cross-validation on thewhole training set avoiding the submission pro-cess.5.4 Discussion of ResultsThe experimental results show that the combinedkernel KSL outperforms the basic kernels KGCandKLC on both data sets.
In particular, precisionsignificantly increases at the expense of a lower re-call.
High precision is particularly advantageouswhen extracting knowledge from large corpora,because it avoids overloading end users with toomany false positives.Although the basic kernels were designed tomodel complementary aspects of the task (i.e.9After the challenge deadline, Reidel and Klein (2005)achieved a significant improvement, F1 = 68.4% (withoutcoreferences) and F1 = 64.7% (with and without corefer-ences).406Test set Coref.
Precision Recall F1Enriched all 55.6 53.0 54.3with 29.0 31.0 24.4without 60.9 46.2 52.6Basic all n/a n/a n/awith 14.0 82.7 24.0without 50.0 53.8 51.8Table 3: Best performance on basic and enrichedtest sets obtained by participants in the officialcompetition at the LLL challenge.Kernel Precision Recall F1KGC 55.1 66.3 60.2KLC 44.8 60.1 53.8KSL 62.1 61.3 61.7Table 4: Comparison of the performance of kernelcombination on the LLL challenge using 10-foldcross validation.presence of the relation and roles of the interact-ing entities), they perform reasonably well evenwhen considered separately.
In particular, KGCachieved good performance on both data sets.
Thisresult was not expected on the LLL challenge be-cause this task requires not only to recognize thepresence of relationships between entities but alsoto identify their roles.
On the other hand, the out-comes of KLC on the AImed data set show thatsuch kernel helps to identify the presence of rela-tionships as well.At first glance, it may seem strange that KGCoutperforms ERK on AImed, as the latter ap-proach exploits a richer representation: sparsesub-sequences of words, PoS tags, entity andchunk types, or WordNet synsets.
However, anapproach based on n-grams is sufficient to identifythe presence of a relationship.
This result soundsless surprising, if we recall that both approachescast the relation extraction problem as a text cate-gorization task.
Approaches to text categorizationbased on rich linguistic information have obtainedless accuracy than the traditional bag-of-words ap-proach (e.g.
(Koster and Seutter, 2003)).
Shallowlinguistics information seems to be more effectiveto model the local context of the entities.Finally, we obtained worse results performingdimensionality reduction either based on genericlinguistic assumptions (e.g.
by removing wordsfrom stop lists or with certain PoS tags) or usingstatistical methods (e.g.
tf.idf weighting schema).This may be explained by the fact that, in tasks likeentity recognition and relation extraction, usefulclues are also provided by high frequency tokens,such as stop words or punctuation marks, and bythe relative positions in which they appear.6 Related WorkFirst of all, the obvious references for our workare the approaches evaluated on AImed and LLLchallenge data sets.In (Bunescu and Mooney, 2005b), the authorspresent a generalized subsequence kernel thatworks with sparse sequences containing combina-tions of words and PoS tags.The best results on the LLL challenge were ob-tained by the group from the University of Ed-inburgh (Reidel and Klein, 2005), which usedMarkov Logic, a framework that combines log-linear models and First Order Logic, to create aset of weighted clauses which can classify pairs ofgene named entities as genic interactions.
Theseclauses are based on chains of syntactic and se-mantic relations in the parse or Discourse Repre-sentation Structure (DRS) of a sentence, respec-tively.Other relevant approaches include those thatadopt kernel methods to perform relation extrac-tion.
Zelenko et al (2003) describe a relation ex-traction algorithm that uses a tree kernel definedover a shallow parse tree representation of sen-tences.
The approach is vulnerable to unrecover-able parsing errors.
Culotta and Sorensen (2004)describe a slightly generalized version of this ker-nel based on dependency trees, in which a bag-of-words kernel is used to compensate for errors insyntactic analysis.
A further extension is proposedby Zhao and Grishman (2005).
They use compos-ite kernels to integrate information from differentsyntactic sources (tokenization, sentence parsing,and deep dependency analysis) so that process-ing errors occurring at one level may be overcomeby information from other levels.
Bunescu andMooney (2005a) present an alternative approachwhich uses information concentrated in the short-est path in the dependency tree between the twoentities.As mentioned in Section 1, another relevant ap-proach is presented in (Roth and Yih, 2002).
Clas-sifiers that identify entities and relations amongthem are first learned from local information inthe sentence.
This information, along with con-straints induced among entity types and relations,is used to perform global probabilistic inference407that accounts for the mutual dependencies amongthe entities.All the previous approaches have been evalu-ated on different data sets so that it is not possi-ble to have a clear idea of which approach is betterthan the other.7 Conclusions and Future WorkThe good results obtained using only shallow lin-guistic features provide a higher baseline againstwhich it is possible to measure improvements ob-tained using methods based on deep linguistic pro-cessing.
In the near future, we plan to extend ourwork in several ways.First, we would like to evaluate the contribu-tion of syntactic information to relation extractionfrom biomedical literature.
With this aim, we willintegrate the output of a parser (possibly trained ona domain-specific resource such the Genia Tree-bank).
Second, we plan to test the portability ofour model on ACE and MUC data sets.
Third,we would like to use a named entity recognizerinstead of assuming that entities are already ex-tracted or given by a dictionary.
Our long termgoal is to populate databases and ontologies byextracting information from large text collectionssuch as Medline.8 AcknowledgementsWe would like to thank Razvan Bunescu for pro-viding detailed information about the AImed dataset and the settings of the experiments.
Clau-dio Giuliano and Lorenza Romano have been sup-ported by the ONTOTEXT project, funded by theAutonomous Province of Trento under the FUP-2004 research program.ReferencesRazvan Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kernel for relation ex-traction.
In Proceedings of the Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing, Van-couver, B.C, October.Razvan Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
InProceedings of the 19th Conference on Neural In-formation Processing Systems, Vancouver, BritishColumbia.Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-ward M. Marcotte, Raymond J. Mooney, Arun K.Ramani, and Yuk Wah Wong.
2005.
Comparativeexperiments on learning information extractors forproteins and their interactions.
Artificial Intelligencein Medicine, 33(2):139?155.
Special Issue on Sum-marization and Information Extraction from Medi-cal Documents.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Annual Meeting of the Association forComputational Linguistics (ACL 2004), Barcelona,Spain.Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-ava.
2005.
Domain kernels for word sense disam-biguation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005), Ann Arbor, Michigan, June.Cornelis H. A. Koster and Mark Seutter.
2003.
Tamingwild phrases.
In Advances in Information Retrieval,25th European Conference on IR Research (ECIR2003), pages 161?176, Pisa, Italy.Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,Dayne Freitag, Claudio Giuliano, Nicholas Kushm-erick, and Lorenza Romano.
2004.
IE evaluation:Criticisms and recommendations.
In Proceedings ofthe AAAI 2004 Workshop on Adaptive Text Extrac-tion and Mining (ATEM 2004), San Jose, California.Claire Nede?llec.
2005.
Learning language in logic -genic interaction extraction challenge.
In Proceed-ings of the ICML-2005 Workshop on Learning Lan-guage in Logic (LLL05), pages 31?37, Bonn, Ger-many, August.Sebastian Reidel and Ewan Klein.
2005.
Genicinteraction extraction with semantic and syntacticchains.
In Proceedings of the ICML-2005 Workshopon Learning Language in Logic (LLL05), pages 69?74, Bonn, Germany, August.D.
Roth and W. Yih.
2002.
Probabilistic reasoningfor entity & relation recognition.
In Proceedings ofthe 19th International Conference on ComputationalLinguistics (COLING-02), Taipei, Taiwan.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press, New York, NY, USA.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons, New York.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for informationextraction.
Journal of Machine Learning Research,3:1083?1106.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005), Ann Arbor, Michigan, June.408
