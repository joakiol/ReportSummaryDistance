Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 201?204, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsHierarchical Semantic Role LabelingAlessandro Moschitti?moschitti@info.uniroma2.it?
DISP - University of Rome ?Tor Vergata?, Rome, Italy?
ITC-Irst, ?
DIT - University of Trento, Povo, Trento, ItalyAna-Maria Giuglea?ana-maria.giuglea@topex.roBonaventura Coppola?
?coppolab@itc.itRoberto Basili?basili@info.uniroma2.itAbstractWe present a four-step hierarchical SRLstrategy which generalizes the classicaltwo-level approach (boundary detectionand classification).
To achieve this, wehave split the classification step by group-ing together roles which share linguisticproperties (e.g.
Core Roles versus Ad-juncts).
The results show that the non-optimized hierarchical approach is com-putationally more efficient than the tradi-tional systems and it preserves their accu-racy.1 IntroductionFor accomplishing the CoNLL 2005 Shared Taskon Semantic Role Labeling (Carreras and Ma`rquez,2005), we capitalized on our experience on the se-mantic shallow parsing by extending our system,widely experimented on PropBank and FrameNet(Giuglea and Moschitti, 2004) data, with a two-step boundary detection and a hierarchical argumentclassification strategy.Currently, the system can work in both basic andenhanced configuration.
Given the parse tree of aninput sentence, the basic system applies (1) a bound-ary classifier to select the nodes associated with cor-rect arguments and (2) a multi-class labeler to assignthe role type.
For such models, we used some of thelinear (e.g.
(Gildea and Jurasfky, 2002; Pradhan etal., 2005)) and structural (Moschitti, 2004) featuresdeveloped in previous studies.In the enhanced configuration, the boundary an-notation is subdivided in two steps: a first pass inwhich we label argument boundary and a secondpass in which we apply a simple heuristic to elimi-nate the argument overlaps.
We have also tried somestrategies to learn such heuristics automatically.
Inorder to do this we used a tree kernel to classify thesubtrees associated with correct predicate argumentstructures (see (Moschitti et al, 2005)).
The ratio-nale behind such an attempt was to exploit the cor-relation among potential arguments.Also, the role labeler is divided into two steps:(1) we assign to the arguments one out of four possi-ble class labels: Core Roles, Adjuncts, ContinuationArguments and Co-referring Arguments, and (2) ineach of the above class we apply the set of its spe-cific classifiers, e.g.
A0,..,A5 within the Core Roleclass.
As such grouping is relatively new, the tradi-tional features may not be sufficient to characterizeeach class.
Thus, to generate a large set of featuresautomatically, we again applied tree kernels.Since our SRL system exploits the PropBank for-malism for internal data representation, we devel-oped ad-hoc procedures to convert back and forthto the CoNLL Shared Task format.
This conversionstep gave us useful information about the amountand the nature of the parsing errors.
Also, we couldmeasure the frequency of the mismatches betweensyntax and role annotation.In the remainder of this paper, Section 2 describesthe basic system configuration whereas Section 3 il-lustrates its enhanced properties and the hierarchicalstructure.
Section 4 describes the experimental set-ting and the results.
Finally, Section 5 summarizes201our conclusions.2 The Basic Semantic Role LabelerIn the last years, several machine learning ap-proaches have been developed for automatic role la-beling, e.g.
(Gildea and Jurasfky, 2002; Pradhanet al, 2005).
Their common characteristic is theadoption of flat feature representations for predicate-argument structures.
Our basic system is similar tothe one proposed in (Pradhan et al, 2005) and it isdescribed hereafter.We divided the predicate argument labeling in twosubtasks: (a) the detection of the arguments relatedto a target, i.e.
all the compounding words of suchargument, and (b) the classification of the argumenttype, e.g.
A0 or AM.
To learn both tasks we used thefollowing algorithm:1.
Given a sentence from the training-set, generatea full syntactic parse-tree;2.
Let P and A be respectively the set of predicatesand the set of parse-tree nodes (i.e.
the potential ar-guments);3.
For each pair <p, a> ?
P ?A:- extract the feature representation set, Fp,a;- if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T?
(negative examples).We trained the SVM boundary classifier on T+ andT?
sets and the role labeler i on the T+i , i.e.
its pos-itive examples and T?i , i.e.
its negative examples,where T+ = T+i ?
T?i , according to the ONE-vs.-ALL scheme.
To implement the multi-class clas-sifiers we select the argument associated with themaximum among the SVM scores.To represent the Fp,a pairs we used the followingfeatures:- the Phrase Type, Predicate Word, Head Word,Governing Category, Position and Voice defined in(Gildea and Jurasfky, 2002);- the Partial Path, Compressed Path, No DirectionPath, Constituent Tree Distance, Head Word POS,First and Last Word/POS in Constituent, SubCate-gorization and Head Word of Prepositional Phrasesproposed in (Pradhan et al, 2005); and- the Syntactic Frame designed in (Xue and Palmer,2004).Figure 1: Architecture of the Hierarchical Semantic Role La-beler.3 Hierarchical Semantic Role LabelerHaving two phases for argument labeling providestwo main advantages: (1) the efficiency is increasedas the negative boundary examples, which are al-most all parse-tree nodes, are used with one clas-sifier only (i.e.
the boundary classifier), and (2) asarguments share common features that do not occurin the non-arguments, a preliminary classificationbetween arguments and non-arguments advantagesthe boundary detection of roles with fewer trainingexamples (e.g.
A4).
Moreover, it may be simplerto classify the type of roles when the not-argumentnodes are absent.Following this idea, we generalized the above twolevel strategy to a four-step role labeling by group-ing together the arguments sharing similar proper-ties.
Figure 1, shows the hierarchy employed for ar-gument classification:During the first phase, we select the parse treenodes which are likely predicate arguments.
AnSVM with moderately high recall is applied for suchpurpose.In the second phase, a simple heuristic which se-lects non-overlapping nodes from those derived inthe previous step is applied.
Two nodes n1 and n2do not overlap if n1 is not ancestor of n2 and vicev-ersa.
Our heuristic simply eliminates the nodes thatcause the highest number of overlaps.
We have alsostudied how to train an overlap resolver by means oftree kernels; the promising approach and results canbe found in (Moschitti et al, 2005).In the third phase, we classify the detected argu-ments in the following four classes: AX, i.e.
Core202Arguments, AM, i.e.
Adjuncts, CX, i.e.
Continua-tion Arguments and RX, i.e.
the Co-referring Argu-ments.
The above classification relies on linguisticreasons.
For example Core arguments class containsthe arguments specific to the verb frames while Ad-junct Arguments class contains arguments that areshared across all verb frames.In the fourth phase, we classify the memberswithin the classes of the previous level, e.g.
A0 vs.A1, ..., A5.4 The ExperimentsWe experimented our approach with the CoNLL2005 Shared Task standard dataset, i.e.
the Pen-nTree Bank, where sections from 02 to 21 are usedas training set, Section 24 as development set (Dev)and Section 23 as the test set (WSJ).
Additionally,the Brown corpus?
sentences were also used as thetest set (Brown).
As input for our feature extractorwe used only the Charniak?s parses with their POSs.The evaluations were carried out with the SVM-light-TK software (Moschitti, 2004) available athttp://ai-nlp.info.uniroma2.it/moschitti/which encodes the tree kernels in the SVM-lightsoftware (Joachims, 1999).
We used the defaultpolynomial kernel (degree=3) for the linear featurerepresentations and the tree kernels for the structuralfeature processing.As our feature extraction module was designedto work on the PropBank project annotation format(i.e.
the prop.txt index file), we needed to generateit from the CoNLL data.
Each PropBank annota-tion refers to a parse tree node which exactly cov-ers the target argument but when using automaticparses such node may not exist.
For example, onthe CoNLL Charniak?s parses, (sections 02-21 and24), we discovered that this problem affects 10,293out of the 241,121 arguments (4.3%) and 9,741 sen-tences out of 87,257 (11.5%).
We have found outthat most of the errors are due to wrong parsing at-tachments.
This observation suggests that the capa-bility of discriminating between correct and incor-rect parse trees is a key issue in the boundary de-tection phase and it must be properly taken into ac-count.4.1 Basic System EvaluationFor the boundary classifier we used a SVM withthe polynomial kernel of degree 3.
We set the reg-ularization parameter, c, to 1 and the cost factor,j to 7 (to have a slightly higher recall).
To re-duce the learning time, we applied a simple heuristicwhich removes the nodes covering the target predi-cate node.
From the initial 4,683,777 nodes (of sec-tions 02-21), the heuristic removed 1,503,100 nodeswith a loss of 2.6% of the total arguments.
How-ever, as we started the experiments in late, we usedonly the 992,819 nodes from the sections 02-08.
Theclassifier took about two days and half to convergeon a 64 bits machine (2.4 GHz and 4Gb Ram).The multiclassifier was built with 52 binary ar-gument classifiers.
Their training on all argumentsfrom sec 02-21, (i.e.
242,957), required about a halfday on a machine with 8 processors (32 bits, 1.7GHz and overll 4Gb Ram).We run the role multiclassifier on the output of theboundary classifier.
The results on the Dev, WSJ andBrown test data are shown in Table 1.
Note that, theoverlapping nodes cause the generation of overlap-ping constituents in the sentence annotation.
Thisprevents us to use the CoNLL evaluator.
Thus, weused the overlap resolution algorithm also for the ba-sic system.4.2 Hierarchical Role Labeling EvaluationAs the first two phases of the hierarchical labeler areidentical to the basic system, we focused on the lasttwo phases.
We carried out our studies over the GoldStandard boundaries in the presence of argumentsthat do not have a perfect-covering node in the Char-niak trees.To accomplish the third phase, we re-organizedthe flat arguments into the AX, AM, CX and RXclasses and we built a single multi-classifier.
Forthe fourth phase, we built a multi-classifier for eachof the above classes: only the examples related tothe target class were used, e.g.
the AX mutliclas-sifier was designed with the A0,..,A5 ONE-vs-ALLbinary classifiers.In rows 2 and 3, Table 2 shows the numbers oftraining and development set instances.
Row 4 con-tains the F1 of the binary classifiers of the thirdphase whereas Row 5 reports the F1 of the result-ing multi-classifier.
Row 6 presents the F1s of themulti-classifiers of the fourth phase.Row 7 illustrates the F1 measure of the fourthphase classifier applied to the third phase output.
Fi-203Precision Recall F?=1Development 74.95% 73.10% 74.01Test WSJ 76.55% 75.24% 75.89Test Brown 65.92% 61.83% 63.81Test WSJ+Brown 75.19% 73.45% 74.31Test WSJ Precision Recall F?=1Overall 76.55% 75.24% 75.89A0 81.05% 84.37% 82.67A1 77.21% 74.12% 75.63A2 67.02% 68.11% 67.56A3 69.63% 54.34% 61.04A4 74.75% 72.55% 73.63A5 100.00% 40.00% 57.14AM-ADV 55.23% 55.34% 55.28AM-CAU 66.07% 50.68% 57.36AM-DIR 50.62% 48.24% 49.40AM-DIS 77.71% 78.44% 78.07AM-EXT 68.00% 53.12% 59.65AM-LOC 59.02% 63.09% 60.99AM-MNR 67.67% 52.33% 59.02AM-MOD 98.65% 92.56% 95.51AM-NEG 97.37% 96.52% 96.94AM-PNC 42.28% 45.22% 43.70AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 81.90% 74.52% 78.03R-A0 79.50% 84.82% 82.07R-A1 62.23% 75.00% 68.02R-A2 100.00% 31.25% 47.62R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 50.00% 66.67R-AM-EXT 100.00% 100.00% 100.00R-AM-LOC 85.71% 85.71% 85.71R-AM-MNR 22.22% 33.33% 26.67R-AM-TMP 67.69% 84.62% 75.21V 97.34% 97.30% 97.32Table 1: Overall results (top) and detailed results onthe WSJ test (bottom).nally, in Row 8, we report the F1 of the basic systemon the gold boundary nodes.
We note that the basicsystem shows a slightly higher F1 but is less compu-tational efficient than the hierarchical approach.5 Final RemarksIn this paper we analyzed the impact of a hierarchi-cal categorization on the semantic role labeling task.The results show that such approach produces an ac-curacy similar to the flat systems with a higher ef-ficiency.
Moreover, some preliminary experimentsshow that each node of the hierarchy requires differ-ent features to optimize the associated multiclassi-fier.
For example, we found that the SCF tree kernel(Moschitti, 2004) improves the AX multiclassifierAX AM CX RX# train.
examples 172,457 59,473 2,954 7,928# devel.
examples 5,930 2,132 105 284Phase III: binary class.
97.29 97.35 70.86 93.15Phase III 95.99Phase IV 92.50 85.88 91.43 91.55Phase III & IV 88.15Basic System 88.61Table 2: Hierarchical Semantic Role Labeler Resultswhereas the PAF tree kernel seems more suited forthe classification within the other classes, e.g.
AM.Future work on the optimization of each phase isneeded to study the potential accuracy limits of theproposed hierarchical approach.AcknowledgementsWe wish to thank Daniele Pighin for his valuablesupport in the development of the SRL system.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.
In pro-ceedings of CoNLL?05.Daniel Gildea and Daniel Jurasfky.
2002.
Automatic labelingof semantic roles.
Computational Linguistic.Ana-Maria Giuglea and Alessandro Moschitti.
2004.
Knowl-edge Discovering using FrameNet, VerbNet and PropBank.In proceedings of the Workshop on Ontology and KnowledgeDiscovering at ECML?04, Pisa, Italy.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning.Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,and Roberto Basili.
2005.
Engineering of syntactic featuresfor shallow semantic parsing.
In proceedings of the FeatureEngineering Workshop at ACL?05, Ann Arbor, USA.Alessandro Moschitti.
2004.
A study on convolution kernelfor shallow semantic parsing.
In proceedings of ACL-2004,Barcelona, Spain.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Support vectorlearning for semantic argument classification.
to appear inMachine Learning Journal.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP?04,Barcelona, Spain.204
