Prefix Probabil it ies from Stochastic Tree Adjoining Grammars*Mark- Jan  NederhofDFKIStuhlsatzenhausweg 3,D-66123 Saarbriicken,Germanynederhof@dfki, deAnoop SarkarDept.
of Computer and Info.
Sc.Univ of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104 USAanoop?linc,  c i s .
upenn, eduG iorg io  Sat taDip.
di Elettr.
e Inf.Univ.
di Padovavia Gradenigo 6/A,35131 Padova, Italysatta@dei, unipd, itAbst ractLanguage models for speech recognition typ-ically use a probability model of the formPr(an\[al,a2,...,an-i).
Stochastic grammars,on the other hand, are typically used to as-sign structure to utterances, A language modelof the above form is constructed from suchgrammars by computing the prefix probabil-ity ~we~* Pr(al.- .artw), where w representsall possible terminations of the prefix al...an.The main result in this paper is an algorithmto compute such prefix probabilities given astochastic Tree Adjoining Grammar (TAG).The algorithm achieves the required computa-tion in O(n 6) time.
The probability of sub-derivations that do not derive any words in theprefix, but contribute structurally to its deriva-tion, are precomputed to achieve termination.This algorithm enables existing corpus-based s-timation techniques for stochastic TAGs to beused for language modelling.1 In t roduct ionGiven some word sequence al'.
'an-1, speechrecognition language models are used to hy-pothesize the next word an, which could beany word from the vocabulary F~.
Thisis typically done using a probability modelPr(an\[al,...,an-1).
Based on the assumptionthat modelling the hidden structure of nat-* Part of this research was done while the first and thethird authors were visiting the Institute for Researchin Cognitive Science, University of Pennsylvania.
Thefirst author was supported by the German Federal Min-istry of Education, Science, Research and Technology(BMBF) in the framework of the VERBMOBIL Project un-der Grant 01 IV 701 V0, and by the Priority ProgrammeLanguage and Speech Technology, which is sponsored byNWO (Dutch Organization for Scientific Research).
Thesecond and third authors were partially supported byNSF grant SBR8920230 and ARO grant DAAH0404-94-G-0426.
The authors wish to thank Aravind Joshi forhis support in this research.ural language would improve performance ofsuch language models, some researchers tried touse stochastic context-free grammars (CFGs) toproduce language models (Wright and Wrigley,1989; Jelinek and Lafferty, 1991; Stolcke, 1995).The probability model used for a stochas-tic grammar was ~we~* Pr(a l .
.
-anw).
How-ever, language models that are based on tri-gram probability models out-perform stochasticCFGs.
The common wisdom about this failureof CFGs is that trigram models are lexicalizedmodels while CFGs are not.Tree Adjoining Grammars (TAGs) are impor-tant in this respect since they are easily lexical-ized while capturing the constituent structureof language.
More importantly, TAGs allowgreater linguistic expressiveness.
The trees as-sociated with words can be used to encode argu-ment and adjunct relations in various syntacticenvironments.
This paper assumes ome famil-iarity with the TAG formalism.
(Joshi, 1988)and (Joshi and Schabes, 1992) are good intro-ductions to the formalism and its linguistic rele-vance.
TAGs have been shown to have relationswith both phrase-structure grammars and de-pendency grammars (Rambow and Joshi, 1995),which is relevant because recent work on struc-tured language models (Chelba et al, 1997) haveused dependency grammars to exploit their lex-icalization.
We use stochastic TAGs as such astructured language model in contrast with ear-lier work where TAGs have been exploited ina class-based n-gram language model (Srinivas,1996).This paper derives an algorithm to computeprefix probabilities ~we~* Pr (a l .
.
.
anw).
Thealgorithm assumes as input a stochastic TAG Gand a string which is a prefix of some string inL(G), the language generated by G. This algo-rithm enables existing corpus-based estimationtechniques (Schabes, 1992) in stochastic TAGsto be used for language modelling.9532 Notat ionA stochastic Tree Adjoining Grammar (STAG)is represented by a tuple (NT, E,:T, .A, ?)
whereNT is a set of nonterminal symbols, E is a setof terminal symbols, 2: is a set of initial treesand .A is a set of aux i l ia ry  trees.
Trees in :TU.Aare also called e lementary  trees.We refer to the root of an elementary tree t asRt.
Each auxiliary tree has exactly one distin-guished leaf, which is called the foot.
We referto the foot of an auxiliary tree t as Ft. We letV denote the set of all nodes in the elementarytrees.For each leaf N in an elementary tree, exceptwhen it is a foot, we define label(N) to be thelabel of the node, which is either a terminal fromE or the empty string e. For each other nodeN, label(N) is an element from NT.At a node N in a tree such that label(N) ?NT  an operation called ad junct ion  can be ap-plied, which excises the tree at N and insertsan auxiliary tree.Function ?
assigns a probability to each ad-junction.
The probability of adjunction of t ?
Aat node N is denoted by ?
(t, N).
The probabil-ity that at N no adjunction is applied is denotedby ?
(nil, N).
We assume that each STAG Gthat we consider is proper .
That is, for eachN such that label(N) ?
NT,?
(t, N) = 1.tE.AU{nil}For each non-leaAf node N we construct hestring cdn(N) = N1.
.
.
Nm from the (ordered)list of children nodes N1, .
.
.
,Nm by defining,for each d such that 1 < d < m, Nd = label(Nd)in case label(Nd) ?
E U {e}, and N d = Nd oth-erwise.
In other words, children nodes are re-placed by their labels unless the labels are non-terminal symbols.To simplify the exposition, we assume an ad-ditional node for each auxiliary tree t, whichwe denote by 3_.
This is the unique child of theactual foot node Ft. That is, we change the def-inition of cdn such that cdn(Ft) = 2_ for eachauxiliary tree t. We setV ?
= {N e V I label(N) ?
NT} U E U {3_}.We use symbols a ,b ,c , .
.
,  to range over E,symbols v ,w,x , .
.
,  to range over E*, sym-bols N, M, .
.
.
to range over V ?, and symbols~, fl, 7 , .
.
.
to range over (V?)
*.
We use t, t ' , .
.
.to denote trees in 2: U ,4 or subtrees thereof.We define the predicate dft on elements fromV ?
as dft(N) if and only if (i) N E V and Ndominates 3_, or (ii) N = 3_.
We extend dftto strings of the form N1.
.
.Nm E (V?)
* bydefining dft(N1... Nm) if and only if there is ad (1 < d < m) such that dft(Nd).For some logical expression p, we define5(p) = 1 iff p is true, 5(p) = 0 otherwise.3 Overv iewThe approach we adopt in the next section toderive a method for the computation of prefixprobabilities for TAGs is based on transforma-tions of equations.
Here we informally discussthe general ideas underlying equation transfor-mations.Let w = ala2.
.
.an E ~* be a string and letN E V ?.
We use the following representationwhich is standard in tabular methods for TAGparsing.
An i tem is a tuple \[N, i, j, f l ,  f2\] rep-resenting the set of all trees t such that (i) t is asubtree rooted at N of some derived elementarytree; and (ii) t's root spans from position i toposition j in w, t's foot node spans from posi-tion f l  to position f2 in w. In case N does notdominate the foot, we set f l  = f2 = - .
We gen-eralize in the obvious way to items It, i, j, f l ,  f2\],where t is an elementary tree, and \[a, i, j, f l ,  f2\],where cdn (N) = al~ for some N and/3.To introduce our approach, let us start withsome considerations concerning the TAG pars-ing problem.
When parsing w with a TAG G,one usually composes items in order to con-struct new items spanning a larger portion ofthe input string.
Assume there are instances ofauxiliary trees t and t' in G, where the yield oft', apart from its foot, is the empty string.
If?
(t, N) > 0 for some node N on the spine of t',and we have recognized an item \[Rt, i, j, f l ,  f2\],then we may adjoin t at N and hence deducethe existence of an item \[Rt,,i,j, f l ,  f2\] (seeFig.
l(a)).
Similarly, if t can be adjoined ata node N to the left of the spine of t' andf l  = f2, we may deduce the existence of an item\[Rt, i, j, j, j\] (see Fig.
l(b)).
Importantly, oneor more other auxiliary trees with empty yieldcould wrap the tree t' before t adjoins.
Adjunc-tions in this situation are potentially nontermi-hating.One may argue that situations where auxil-iary trees have empty yield do not occur in prac-tice, and are even by definition excluded in the954(a) R t,t t ~(b) R,,Figure 1: Wrapping in auxiliary trees withempty yieldcase of lexicalized TAGs.
However, in the com-putation of the prefix probability we must takeinto account rees with non-empty ield whichbehave like trees with empty yield because theirlexical nodes fall to the right of the right bound-ary of the prefix string.
For example, the twocases previously considered in Fig.
1 now gen-eralize to those in Fig.
2.Rt* Rtle~sp inei f~f2  n i flff/~2 nECFigure 2: Wrapping of auxiliary trees whencomputing the prefix probabilityTo derive a method for the computation ofprefix probabilities, we give some simple recur-sive equations.
Each equation decomposes anitem into other items in all possible ways, inthe sense that it expresses the probability ofthat item as a function of the probabilities ofitems associated with equal or smaller portionsof the input.In specifying the equations, we exploit tech-niques used in the parsing of incomplete in-put (Lang, 1988).
This allows us to computethe prefix probability as a by-product of com-puting the inside probability.In order to avoid the problem of nontermi-nation outlined above, we transform our equa-tions to remove infinite recursion, while preserv-ing the correctness of the probability computa-tion.
The transformation of the equations isexplained as follows.
For an item I, the spanof I, written a(I), is the 4-tuple representingthe 4 input positions in I.
We will define anequivalence r lation on spans that relates to theportion of the input that is covered.
The trans-formations that we apply to our equations pro-duce two new sets of equations.
The first setof equations are concerned with all possible de-compositions of a given item I into set of itemsof which one has a span equivalent to that of Iand the others have an empty span.
Equationsin this set represent endless recursion.
The sys-tem of all such equations can be solved indepen-dently of the actual input w. This is done oncefor a given grammar.The second set of equations have the propertythat, when evaluated, recursion always termi-nates.
The evaluation of these equations com-putes the probability of the input string modulothe computation ofsome parts of the derivationthat do not contribute to the input itself.
Com-bination of the second set of equations with thesolutions obtained from the first set alows theeffective computation of the prefix probability.4 Comput ing  Pre f ix  P robab i l i t iesThis section develops an algorithm for the com-putation of prefix probabilities for stochasticTAGs.4.1 General  equat ionsThe prefix probability is given by:Pr(a l .
.
.anw)  = ~ P(\[t,O,n,-,-\]),wEE* fEZwhere P is a function over items recursively de-fined as follows:P(\[t, i , j ,  f l , f2\]) = P(\[Rt, i , j ,  fl,f2\]); (1)P( \ [ t~N, i , j , - , - \ ] )  = (2)P( \ [a , i , k , - , - \ ] )  .
P ( \ [N ,k , j , - , - \ ] ) ,k(i < k < j)if a ?
e A -~dft(aN);P(\[t~N, i j, f l ,  f2\]) = (3)Z P( \ [a , i , k , - , - \ ] ) -P ( \ [N ,k , j ,  f l ,f2\]),k(i < k < fl)if ~ ?
?
A dft(g);955P(\[aN, i, j, f l ,  f2\]) = (4)P(\[a, i, k, f l ,  f2\]).
P(\[N, k, j, - ,  -\]),k(f2 <_ k <_ j)i f  # c ^P(\[N, i, j, fl,/2\]) = (5)?
(nil,  N).
P(\[cdn(N), i j, fl, f2\]) +P(\[cdn(N), f~, f~, f~, f2\]) .f~,f~(i S f~ S fl A f2 ~_ flo S J)?
(t, N) .
P(\[t, i,j, f\[, f~\]),tEAif N ?
V A dft(N);P( \ [g , i , j , - , - \ ] )  = (6)?
(nil ,  N) .
P(\[cdn(N), i , j , - , - \ ] )  +P(\[cdn(N), f~, f~, - ,  -\]) .y~ ?
(t, N) .
P(\[t, i , j ,f\[,f~\]),tEAif N ?
V A -,dfl(N);P ( \ [a , i , j , - , - \ ] )  = (7)+ 1 = j ^ aj = a) + = j = n);P(\[-l-,i,j, fl,f2\]) = (f(i = f l  A j  = f2); (8)P(\[e, i,j, - ,  - \ ])  = (f(i = j).
(9)Term P(\[t, i, j, f l ,  f2\]) gives the inside probabil-ity of all possible trees derived from elementarytree t, having the indicated span over the input.This is decomposed into the contribution of eachsingle node of t in equations (1) through (6).In equations (5) and (6) the contribution of anode N is determined by the combination ofthe inside probabilities of N's  children and byall possible adjunetions at N. In (7) we rec-ognize some terminal symbol if it occurs in theprefix, or ignore its contribution to the span if itoccurs after the last symbol of the prefix.
Cru-cially, this step allows us to reduce the compu-tation of prefix probabilities to the computationof inside probabilities.4.2 Terminat ing  equat ionsIn general, the recursive equations (1) to (9)are not directly computable.
This is becausethe value of P(\[A, i, j, f ,  if\]) might indirectly de-pend on itself, giving rise to nontermination.We therefore rewrite the equations.We define an equivalence relation over spans,that expresses when two items are associatedwith equivalent portions of the input:(i',j', f~, f~) .~ (i,j, f l ,  f2) if and only if(( i ' , j ' )  = (i, j))A= (fl, f2)v((f~ = f~ = iV f{ = f~ = jV  f{ = f~ = --)A( f l  = :2 = iv f l  = f2 = jvf  = :2 = - ) ) )We introduce two new functions P~ow andP, pm.
When evaluated on some item I, Plow re-cursively calls itself as long as some other itemI' with a given elementary tree as its first com-ponent can be reached, such that a( I )  ~.
a(I').Pto~ returns 0 if the actual branch of recursioncannot eventually reach such an item I', thusremoving the contribution to the prefix proba-bility of that branch.
If item I ' is reached, thenP~ow switches to Psptit.
Complementary to Plow,function P, pm tries to decompose an argumentitem I into items I ~ such that a(I) ~ a(I').
Ifthis is not possible through the actual branchof recursion, P, pm returns 0.
If decomposit ionis indeed possible, then we start again with Pto,oat items produced by the decomposition.
Theeffect of this intermixing of function calls is thesimulation of the original function P,  with Pzo~being called only on potentially nonterminatingparts of the computation, and P, pm being calledon parts that are guaranteed to terminate.Consider some derivation tree spanning someportion of the input string, and the associatedderivation tree 7-.
There must be a unique ele-mentary tree which is represented by a node in7- that is the "lowest" one that entirely spansthe portion of the input of interest.
(This nodemight be the root of T itself.)
Then, for eacht E .A and for each i, j, f l , f2  such that i < jand i < f l  < f2 __< j, we must have:P(\[t, i, j, f l ,  f2\]) = (10)l l ?
.
I l t' E .A, fl,f~((z,3, fl,f~) ,~ (i,j, f1,f2))Similarly, for each t E 27 and for each i, j suchthat i < j, we must have:P(\[t,i, j, - ,  -1) = (11)\[t', L / \ ] ) .t' e {t} u.4, /~ {-,i,j}The reason why P~o~, keeps a record of indicesf{ and f~, i.e., the spanning of the foot nodeof the lowest tree (in the above sense) on whichPlow is called, will become clear later, when weintroduce quations (29) and (30).We define Pzo~:(\[t,i,j, fl,f2\],\[t',f\[,f~\]) andP~o=(\[a,i,j, fl,f2\],\[t',f{,f~\]) for / < j and?
.
!
!
(i,j, f l , f2) ~ (z,3, f l , f~) , as follows.956Pto~o(\[t, i j, f l ,  f2\], \[tt, f{,f~\]) = (12)Pto~o(\[Rt, i, j, f l, f2\], \[tt, f{,f~\]) +6((t, fl, f2) = (it, fl, f2)) "P,,m(\[nt, i, j, fl, f2\]);Pzo~(\[aN, i,j, - ,  -1, \[t, f{, f~\]) = (13)j , - , - \] ,P( \ [N , j , j , - , - \ ] )  +P(\[a, i, i, - ,  -\]) ?P~o~.
(\[N,i,j,-,-\], \[t, f~, f~\]),if a # e A ",dfl(aN);P~o~(\[ag, i,j, ft,f2\], \[t,f{,f~\]) = (14)6(fl ----- j)" Pto~(\[a, i,j,-, -\], \[t, f{, foil) ?P(\[N, j , j ,  fl, f2\]) +P(\[a, i, i, - ,  - \ ] )  ?Pto~,(\[g,i,j, fl,f2\], \[t,f~,f~\]),if a # e A rift(N);P,o~(\[aN, i j, fx,f2\], \[t,f{,f~\]) = (15)P~o~(\[a,i,j,f~,f2\], \[t, f~, f~\]) ?P( \ [N , j , j , - , - \ ] )  +6(i = f2)" P(\[a,  i, i, f l ,  f2\]) "P~o~(\[N,i , j , - , - \] ,  \[t,f~,f~\]),if a # e A dft(a);P~o~,(\[N, i, j, f l ,  f2\], \[t, f{, f~\]) : (16)?
(n i l ,  N )  ?Pzo~ (\[cdn (N), i, j, fl, f2\], \[t, f{, f~\]) +P~o,o(\[cdn(N), i,j, f l ,  f2\], \[t, f l ,  f~\]) ?Et'eA ?
(t', g)  .
P(\[t', i,j, i,j\]) +P(\[cdn(N), f l ,  f2,  f l ,  f2\]) "E ?
(t', N ) .
Pto~ (\[t', i,j, f l ,  f21, \[t, f{, f~\]),t I E .4if N E V A dft (N);Pto~ (\[N, i, j, - ,  - \ ] ,  \[t, f l ,  f~\]) = (17)?
(n i l ,  N )  ?Pzo~,(\[cdn(N),i, j ,-,-\], \[t,f{,f~\]) +P~o~(\[cdn(N), i,j  - , - \ ] ,  \[t, f{, f~\]) ?E t 'eA ?
(t', N) .
P(\[t', i, j, i, j\]) +P(\[ cdn( g) ,  f{', f~, - ,  -\]) "fl',f~'(fl' = S~' = ~vy~' = S~' =~)E ?
(t', N)"P~ow (\[t', i, j, ill', f2'\], \[t, f{, f~\]),t 'EAif N E V A -~dft(N);Pto~(\[a, i,j, - ,  - \ ] ,  \[t, f{, f~\]) = O; (18)Pto~,(\[-L,i,j, fl,f2\], \[t,f{,f?.\]) = 0; (19)i , j ,  - ,  - \ ] ,  \[t, = 0.
(20)The definition of Pto~ parallels the one of Pgiven in ?4.1.
In (12), the second term in theright-hand side accounts for the case in whichthe tree we are visiting is the "lowest" one onwhich Pto,.
should be called.
Note how in theabove equations Pto~ must be called also onnodes that do not dominate the footnode of theelementary tree they belong to (cf.
the definitionof ~).
Since no call to P,p,t is possible throughthe terms in (18), (19) and (20), we must setthe right-hand side of these equations to 0.The specification of P.pm(\[a, i j, f l,f2\]) isgiven below.
Again, the definition parallels theone of P given in ?4.1.P, pm(\[aN, i j, - ,  - \ ] )  = (21)P( \ [a , i , k , - , - \ ] )  .
P ( \ [Y ,k , j , - , - \ ] )  +k(i < k < j)P, pm(\[a, i , j , - , - \ ] )  .
P ( \ [Y , j , j , - , - \ ] )  +P( \ [a , i , i , - , - \ ] )  .
P,p, ,t( \ [Y, i , j , - , - \ ] ) ,if a # e A -,dft(aN);P, pm(\[aY, i j, f l , f2\]) = (22)E P ( \ [a , i , k , - , - \ ] ) .P ( \ [N ,k , j ,  fl,f2\]) +k( i<k< f lAk<3)~(fl = J) " P.p,t(\[a, i , j , - , - \ ] )  ?P( \ [g , j , j ,  fl,f2\]) +P(\[a, i, i, - ,  - \ ] ) .
P,,m(\[N, i, j, f l ,  f2\]),if a # e A dft(N);Pspt,t ( \ [aN,  i, j ,  f l ,  f2 \ ] )  = (23)E P(\[a, i ,k, fl,f2\])" P ( \ [N ,k , j , - , - \ ] )  +k(i <kA f2 <k <j)P.pm(\[a, i j, f l ,  f2\])" P(\[N, j , j ,  - ,  - \ ] )  +5(i = f2)" P(\[ot, i, i, f l ,  f2\])"P, ,m(\ [N, i , j , - , -1) ,if a # e A dfl(a);Pop,,t(\[N, i, j, f l ,  f2\]) = (24)?
(ni l ,  g) .
P~pm(\[cdn(N), i,j, f l ,  f2\]) +y~ P(\[cdn(N),f~,f~,f l ,  2\]) "fl,f~ (i < fl < f~ ^  f2 < f; < j ^(fl,f~)  (i,3) ^  (fl, f2) ?
(fl,f2))?
(t, N) .
P(\[t, i, j, f~, f~\]) +tEAP..,i, (\[cdn (N), i, j, f l ,  f2\]) ??
(t, g)  .
P(\[t, i, j, i, j\]),t fA957if N E V A dft(N);P, , , ,  (\[N, i, j, - ,  - \ ] )  = (25)?
(nil ,  N) .
Psplit (\[cdn (N), i, j, - ,  - \])  +P(\[cdn(N), f~, f~, - ,  -\]) .l I !
I *A  l I fl'f2 (i<--fl <_f~ <--3 (f~,f~)~(i,j)A"~(fl -~f2 =ivf l  = f2 =J))?
( t ,N) .
P(\[t,i, j,f~,f~\]) +tEAPs,u, (\[cdn ( N), i, j, - ,  - \])?
( t ,Y ) .
P(\[t, i , j , i , j \]),tEAif N E Y A --rift(N);P.put(\[a,i,j,--,--\]) ----- (~(i -t- 1 = j A aj = a); (26)P, pm (\[_1_, i  j, f l ,  f2\]) = 0; (27)P,,,,,(\[e, i j, - ,  - \ ])  = 0.
(28)We can now separate those branches of re-cursion that terminate on the given input fromthe cases of endless recursion.
We assume be-low that P,p,,(\[Rt, i,j, f~,f~\]) > 0.
Even if thisis not always valid, for the purpose of derivingthe equations below, this assumption does notlead to invalid results.
We define a new functionPo,..., which accounts for probabilities of sub-derivations that do not derive any words in theprefix, but contribute structurally to its deriva-tion:Po,t~.
(\[t,i,j, fl,f2\], \[t',f~,f~_\]) = (29)Pto=(\[t,i,j, fz,f2\], \[t',f~,f~\]).I " * I I P,,,i, (\[Rt, *, 3, fl, f~\])Po~t,,(\[a,i,j, Yl,:2\], \[t',:~,:~\]) = (30)P~o= (\[a, i, j, f l ,  f2\], \[t', f~, f~\])P,,m (iRe, i, j, f{, fgt\])We can now eliminate the infinite recur-sion that arises in (10) and (11) by rewritingP(\[t, i, j, f l ,  f2\]) in terms of Po.,,,:P(\[t, i,j, fy,/2\]) = (31)Po.,e,(\[t,i,j, fz,f2\], \[t',f~,f~\]).l I i " I t te  A,  f l , f2 ( (  ' J ' f l ' f2 )  ~" ( i , j ,  f l , f2 ) )P,,m(\[nt, , i,j, f~, f~\]);P(\[t, i, j, - ,  - \ ] )  = (32)Po,t,~(\[t, i , j , - ,-\],  \[t ' , f , f \]) .t' e {t} U.A,f E {--,i,j}P, pzit (\[Rt,, i, j, f ,  f\]).Equations for Po~,, will be derived in the nextsubsection.In summary, terminating computat ion of pre-fix probabilities should be based on equa-tions (31) and (32), which replace (1), alongwith equations (2) to (9) and all the equationsfor P, pm.4.3 Off-line EquationsIn this section we derive equations for functionPo~t,r introduced in ?4.2 and deal with all re-maining cases of equations that cause infiniterecursion.In some cases, function P can be computedindependently of the actual input.
For anyi < n we can consistently define the followingquantities, where t E Z U .4  and a E V ?
orcdn(N) = aft for some N and fl:Ht = P(\[t , i , i , f , f \ ]) ;Ha = P(\[c~,i,i,f',f'\]),where f = i if t E .A, f = - otherwise, and ff =i if dft(a), f = - otherwise.
Thus, Ht is theprobability of all derived trees obtained from t,with no lexical node at their yields.
QuantitiesHt and Ha can be computed by means of a sys-tem of equations which can be directly obtainedfrom equations (1) to (9).
Similar quantities asabove must be introduced for the case i = n.For instance, we can set H~ = P(\[t, n, n, f, f\]),f specified as above, which gives the probabil-ity of all derived trees obtained from t (with norestriction at their yields).Function Po~e.
is also independent of theactual input.
Let us focus here on the casef l , f2 ?
; { i , j , -}  (this enforces (fl, f2) = (f~, f~)below).
For any i, j, f l ,  f2 < n, we can consis-tently define the following quantities.Lt,t, = Po~te,(\[t,i,j, fl,f2\], \[t',f~,f~\]);L~,t, = Po.,?.
(\[a,i,j, f l , f2\],  \[t',f~,f~\]).In the case at hand, Lt,t, is the probability of allderived trees obtained from t such that (i) nolexical node is found at their yields; and (ii) atsome 'unfinished' node dominating the foot oft, the probability of the adjunction of t ~ has al-ready been accounted for, but t t itself has notbeen adjoined.It is straightforward to establish a system ofequations for the computat ion of Lt,t, and La,t,,by rewriting equations (12) to (20) accordingto (29) and (30).
For instance, combining (12)and (29) gives (using the above assumptions onf l  and f2):Lt,t '  = LRt , t '  + (~(t = t ') .Also, if a ~ e and dft(N), combining (14)and (30) gives (again, using previous assump-958tions on fl and f2; note that the Ha's are knownterms here):L~N,t' = Ha" LN,t'.For any i, f l , f2  < n and j = n, we also need todefine:L~,t, = Po,, , .
( \[t , i ,n,  f l , f2 \ ] ,  \[t ' , f~,f~\]);L:.t, = Po~,..(\[a,i,n, fx,f2\] ,  \[t',/~,/.~\]).Here L~, t, is the probability of all derived treesobtained from t with a node dominating thefoot node of t, that is an adjunction site for t'and is 'unfinished' in the same sense as above,and with lexical nodes only in the portion ofthe tree to the right of that node.
When wedrop our assumption on fl and f2, we must(pre)compute in addition terms of the formPo~t~r(\[t,i,j,i,i\], \[t',i,i\]) and Po~,~(\[t,i,j,i,i\],\[t',j, j\]) for i < j < n, Po,t~,(\[t,i,n, f l ,n\],\[t',/i,f~\]) for i < 11 < n, Po,, .
.
( \ [ t , i ,n,n,n\] ,\[t', f{, f~\]) for i < n, and similar.
Again, theseare independent of the choice of i, j and f l .
Fulltreatment is omitted due to length restrictions.5 Complex i ty  and  conc lud ingremarksWe have presented a method for the computa-tion of the prefix probability when the underly-ing model is a Tree Adjoining Grammar.
Func-tion P,p,t is the core of the method.
Its equa-tions can be directly translated into an effectivealgorithm, using standard functional memoiza-tion or other tabular techniques.
It is easy tosee that such an algorithm can be made to runin t ime O(n6), where n is the length of the inputprefix.All the quantities introduced in ?4.3 (Ht,Lt,t,, etc.)
are independent of the input andshould be computed off-line, using the system ofequations that can be derived as indicated.
Forquantities Ht we have a non-linear system, sinceequations (2) to (6) contain quadratic terms.Solutions can then be approximated to any de-gree of precision using standard iterative meth-ods, as for instance those exploited in (Stolcke,1995).
Under the hypothesis that the grammaris consistent, hat is Pr(L(G)) = 1, all quanti-ties H~ and H~ evaluate to one.
For quantitiesLt,t, and the like, ?4.3 provides linear systemswhose solutions can easily be obtained usingstandard methods.
Note also that quantitiesLa,t, are only used in the off-line computationof quantities Lt,t,, they do not need to be storedfor the computation ofprefix probabilities (com-pare equations for Lt,t, with (31) and (32)).We can easily develop implementations of ourmethod that can compute prefix probabilitiesincrementally.
That is, after we have computedthe prefix probability for a prefix al ...  an, on in-put an+l we can extend the calculation to prefixal""anan+l without having to recompute allintermediate steps that do not depend on an+l.This step takes time O(n5).In this paper we have assumed that the pa-rameters of the stochastic TAG have been pre-viously estimated.
In practice, smoothing toavoid sparse data problems plays an importantrole.
Smoothing can be handled for prefix prob-ability computation i the following ways.
Dis-counting methods for smoothing simply pro-duce a modified STAG model which is thentreated as input to the prefix probability com-putation.
Smoothing using methods such asdeleted interpolation which combine class-basedmodels with word-based models to avoid sparsedata problems have to be handled by a cognateinterpolation of prefix probability models.Re ferencesC.
Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu-danpur, L. Mangu, H. Printz, E. Ristad, A. Stolcke,R.
Rosenfeld, and D. Wu.
1997.
Structure and per-formance of a dependency language model.
In Proc.of Eurospeech 97, volume 5, pages 2775-2778.F.
Jelinek and J. Lafferty.
1991.
Computation of theprobability of initial substring eneration by stochas-tic context-free grammars.
Computational Linguis-tics, 17(3):315-323.A.
K. Joshi and Y. Schabes.
1992.
Tree-adjoining gram-mars and lexicalized grammars.
In M. Nivat andA.
Podelski, editors, Tree automata nd languages,pages 409-431.
Elsevier Science.A.
K. Joshi.
1988.
An introduction to tree adjoininggrammars.
In A. Manaster-Ramer, ditor, Mathemat-ics of Language.
John Benjamins, Amsterdam.B.
Lang.
1988.
Parsing incomplete sentences.
In Proc.
ofthe 12th International Conference on ComputationalLinguistics, volume 1, pages 365-371, Budapest.O.
Rainbow and A. Joshi.
1995.
A formal look at de-pendency grammars and phrase-structure grammars,with special consideration of word-order phenomena.In Leo Wanner, editor, Current Issues in Meaning-Text Theory.
Pinter, London.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoininggrammars.
In Proc.
of COLING '92, volume 2, pages426--432, Nantes, France.B.
Srinivas.
1996.
"Almost Parsing" technique for lan-guage modeling.
In Proc.
ICSLP '96, volume 3, pages1173-1176, Philadelphia, PA, Oct 3-6.A.
Stolcke.
1995.
An efficient probabilistic ontext-freeparsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):165-201.J.
H. Wright and E. N. Wrigley.
1989.
Probabilistic LRparsing for speech recognition.
In IWPT '89, pages105-114.959
