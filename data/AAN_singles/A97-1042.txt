Identifying Topics by PositionChin -Yew L in  and  Eduard  HovyIn fo rmat ion  Sciences Ins t i tu teof  the  Un ivers i ty  of Southern  Ca l i forn ia4676 Admira l ty  WayMar ina  del Rey,  CA  90292, USA{cyl ,hovy}@isi.
eduAbst rac tThis paper addresses the problem of iden-tifying likely topics of texts by their posi-tion in the text.
It describes the automatedtraining and evaluation of an Optimal Posi-tion Policy, a method of locating the likelypositions of topic-bearing sentences basedon genre-specific regularities of discoursestructure.
This method can be used inapplications uch as information retrieval,routing, and text summarization.1 In t roduct ion :  Top ic  Spot t ing  byPos i t ionIn an increasingly information-laden world, theproblem of automatically finding the major topicsof texts acquires new urgency.
A module that cansuggest likely locations of topics in texts, robustlyand with an acceptable degree of correctness, wouldbe useful for a number of important applications,including information retrieval, gisting, and auto-mated summarization.Several methods have been tried to perform TopicIdentification.
Some involve parsing and semanticanalysis of the text, and are therefore less robustover arbitrary input.
Others, such as the Cue Phraseand Position methods, are more robust, though gen-erally somewhat less accurate.
Of these, the PositionMethod, identified in the late 1950's, remains amongthe best; it can outperform newer methods uch asthose based on word counting (Salton et al, 1994).The Position Method springs from the recognitionthat texts in a genre generally observe a predictablediscourse structure, and that sentences of greatertopic centrality tend to occur in certain specifiablelocations.
The text's title, for example, is a veryinformative position in most genres, as is the Ab-stract paragraph in scientific articles.
Edmundson(Edmundson, 1969) defined the Position Method asfollows:" .
.
.
the machine-readable cues are cer-tain general characteristics of the corpusprovided by the skeletons of documents,i.e.
headings and format.
The Locationmethod is based on the hypothesis that: (1)sentences occurring under certain headingsare positively relevant; and (2) topic sen-tences tend to occur very early or very latein a document and its paragraphs.
"However, since the paradigmatic discourse structurediffers significantly over text genres and subject do-mains, the Position Method cannot be defined asstraightforwardly as Baxendale's (Baxendale, 1958)title plus first and last sentences of each paragraph;it has to be tailored to genre and domain.
Can onedevelop ways of tailoring this method?Furthermore, since the resolution power of thePosition Method is the sentence, while the de-sired output--topics--generally ppear at the wordor phrase level, the most accurate results of thismethod may still include too much spurious mate-rial to be really useful.
How useful is the method ingeneral?
By what measure(s) can one evaluate it?Basic questions about how the Position Methodcan be tailored for optimality over a genre and howit can be evaluated for effectiveness remain unan-swered.
To our knowledge, no systematic study hasyet been performed, though some variant of it hasbeen used in computational studies (see for example(Edmundson, 1969; Luhn, 1958; Baxendale, 1958)),writing-education classes (for example, (Sjostromand Hare, 1984)), and has been the subject of cog-nitive psychological verification (Kieras, 1985).This paper contains an analysis of the PositionMethod.
We first discuss previous work, then in Sec-tion 3 describe the background studies and trainingof an Optimal Position Policy for a genre of texts,and in Section 4 describe its evaluation.2 Re la ted  WorkEdmundson's (Edmundson, 1969) laid the ground-work for the Position Method.
He introduced fourclues for identifying significant words (topics) in atext.
Among them, Title and Location are relatedto the Position Method.
Edmundson assigned pos-283itive weights to sentences according to their ordi-nal position in the text, giving most weight to thefirst sentence in the first paragraph and the last sen-tence in the last paragraph.
He conducted seven-teen experiments to verify the significance of thesemethods.
According to his results, the Title andLocation methods respectively scored around 40%and 53% accuracy, where accuracy was measured asthe coselection rate between sentences elected byEdmundson's program and sentences elected by ahuman.Although Edmundson's work is fundamental, hisexperiments used only 200 documents for trainingand another 200 documents for testing.
Further-more, he did not trying out other possible combina-tions, such as the second and third paragraphs or thesecond-last paragraph.
In order to determine wherethe important words are most likely to be found,Baxendale (Baxendale, 1958) conducted an inves-tigation of a sample of 200 paragraphs.
He foundthat in 85% of paragraphs the topic sentence was inthe first sentence and in 7% the final one.
Donlan(Dolan, 1980) stated that a study of topic sentencesin expository prose showed that only 13% of para-graphs of contemporary professional writers beganwith topic sentences (Braddock, 1974).
Singer andDonlan (Singer and Dolan, 1980) maintain that aparagraph's main idea can appear anywhere in theparagraph, or not be stated at all.Arriving at a negative conclusion, Paijmans (Pai-jmans, 1994) conducted experiments on the relationbetween word position in a paragraph and its signif-icance, and found that "words with a high informa-tion content according to the tf.idf-based weightingschemes do not cluster in the first and the last sen-tences of paragraphs or in paragraphs that consistof a single sentence, at least not to such an extentthat such a feature could be used in the prepara-tion of indices for Information Retrieval purposes.
"In contrast, Kieras (Kieras, 1985) in psychologicalstudies confirmed the importance of the position ofa mention within a text.3 Training the Rules3.1 BackgroundThe purposes of our study are to clarify these contra-dictions, to test the abovementioned intuitions andresults, and to verify the hypothesis that the impor-tance of a sentence in a text is indeed related to itsordinal position.
Furthermore, we wish to discoverempirically which textual positions are in fact therichest ones for topics, and to develop a method bywhich the optimal positions can be determined au-tomatically and their importance valuated.To do all this, one requires a much larger docu-ment collection than that available to Edmundsonand Baxendale.
For the experiments described here,we used the Ziff-Davis texts from the corpus pro-284duced for DARPA's T IPSTER program (Harman,1994).
Volume 1 of the Ziff corpus, on which wetrained the system, consists of 13,000 newspapertexts about new computers and related hardware,computer sales, etc., whose genre can be character-ized as product announcements.
The average textlength is 71 sentences (34.4 paragraphs).
Each textis accompanied by both a set of three to eight topickeywords and an abstract of approx.
6 sentences(both created by a human).In summary, we did the following: To determinethe efficacy of the Position Method, we empiricallydetermined the yield of each sentence position in thecorpus, measuring against the topic keywords.
Wenext ranked the sentence positions by their averageyield to produce the Optimal Position Policy (OPP)for topic positions for the genre.
Finally, now com-paring to the abstracts accompanying the texts, wemeasured the coverage of sentences extracted fromthe texts according to the policy, cumulatively inthe position order specified by the policy.
The highdegree of coverage indicated the effectiveness of theposition method.3.2 Sentence  Pos i t ion  Y ie lds and theOpt imal  Pos i t ion  Po l i cyWe determined the optimal position for topic oc-currence as follows.
Given a text T and a list oftopics keywords t/ of T, we label each sentence ofT with its ordinal paragraph and sentence number(P~,Sn).
We then removed all closed-class wordsfrom the texts.
We did not perform morphologicalrestructuring (such as canonicalization to singularnouns, verb roots, etc.)
or anaphoric resolution (re-placement of pronouns by originals, etc.
), for wantof robust enough methods to do so reliably.
Thismakes the results somewhat weaker than they couldbe.What data is most appropriate for determiningthe optimal position?
We had a choice between thetopic keywords and the abstracts accompanying eachtext in the corpus.
Both keywords and abstractscontain phrases and words which also appear in theoriginal texts; on the assumption that these phrasesor words are more important in the text than otherones, we can assign a higher importance to sentenceswith more such phrases or words (or parts of them))Since a topic keyword has a fixed boundary, using itto rank sentences i easier than using an abstract.For this reason we defined sentence yield as the av-erage number of different opic keywords mentionedin a sentence.
We computed the yield of each sen-tence position in each text essentially by counting1 How many topic keywords would be taken over ver-batim from the texts, as opposed to generated para-phrastically by the human extractor, was a question forempirical determination--the answer provides an upperbound for the power of the Position Method.the number of different opic keywords contained inthe appropriate sentence in each text, and averag-ing over all texts.
Sometimes, however, keywordsconsist of multiple words, such as "spreadsheet soft-ware".
In order to reward a full-phrase mention ina sentence over just a partial overlap with a multi-word keyword/phrase, we used a formula sensitiveto the degree of overlap.
In addition, to take intoaccount word position, we based this formula on theFibonacci function; it monotonically increases withlonger matched substrings, and is normalized to pro-duce a score of 1 for a complete phrase match.
Ourhit function H measures the similarity between topickeyword ti and a window wij that moves across eachsentence (Pm,Sn) of the text.
A window matcheswhen it contains the same words as a topic keywordti.
The length of the window equals the length ofthe topic keyword.
Moving the window from the be-ginning of a sentence to the end, we computed allthe H,  scores and added them together to get thetotal score H,  for the whole sentence.
We acquiredthe H,  scores for all sentences in T and repeatedthe whole process for the each text in the corpus.After obtaining all the H,  scores, we sorted all thesentences according to their paragraph and sentencenumbers.
For each paragraph and sentence numberposition, we computed the average Havg score.These average yields for each position are plottedin Figure 1, which shows the highest-yield sentenceposition to be (P2,$1), followed by (P3,$1), followedby (P4,S1), etc.Finally, we sorted the paragraph and sentence po-sition by decreasing yield Hang scores.
For positionswith equal scores, different policies are possible: onecan prefer sentence positions in different paragraphson the grounds that they are more likely to containsdistinctive topics.
One should also prefer sentencepositions with smaller Sin, since paragraphs are gen-erally short.
Thus the Optimal Position Policy forthe Ziff-Davis corpus is the list\[(T) (P2 ,$1) (P3 ,S~) (P2 ,$2)  {(P4,S~)(P5,S~) (P3,$2)} {(P1,S1) (P6,S1) (P7,S1)(P~ ,$3) (P2,S3)} .
.
.
\]3.3 Add i t iona l  Measures  and ChecksThroughout he above process, we performed addi-tional measures and checks in order to help us pre-vent spurious or wrong rules.
We collected factsabout the training corpus, including the averagenumber of paragraphs per text (PPT),  the averagenumber of sentences per paragraph (SPP), and theaverage number of sentences per human-made sum-mary (SPS).
PPT  and SPP prevent us from forminga rule such as 251h sentence in the lO0lh paragraphwhen PPT  is 15 and SPP is 5.
SPS suggests howmany sentences to extract.
For the ZIFF Vol.
1 cor-pus, PPT  is 34.43, SPP is 2.05, and SPS is 5.76.Most texts have under 30 paragraphs; 97.2% of para-Io.I4,f.I !oI l I I~'-M~I l i l lY !llliTIPSTER ZIFF VOL1 POLICY DETERMINATION MAPk l"l-L \[ J LI I"tTTT?.J, T I ' I I I  itlI"H,, ,I I I I I  Illl.l 'J IAtlI I i t l t  II1"II1"II TI'-ILI ITI't-LIJ.4.1-  I L,kl I t,LIkel% l I I I I I I-l-rl I-f 11- I'/"PARAGRAPH Posn loN IN A TEXTFigure 1: Average yield by paragraph and sentenceposition; lightest shade shows highest yield.graphs have fewer than 5 sentences.
47.7% of para-graphs have only one sentence (thus the first sen-tence is also the last), and 25.2% only two.
Withregard to the abstracts, most have 5 sentences andover 99.5% have fewer than 10.We also counted how many different topic key-words each specific text unit contains, counted onceper keyword.
This different hit measure dhit playedan important role, since the OPP should be tuned tosentence positions that bear as many different opickeywords as possible, instead of positions with veryhigh appearances of just a few topic keywords.
Wecan compute dhit for a sentence, several sentences,or several paragraphs.
Sentenceyield is dhit score ofa sentence.
Figure 2 shows dhit scores for the first 50paragraph positions, and Figure 3 dhit scores for thelast 50 positions (counting backward from the endof each text).
Since PPT=34.43, the first and last50 positions fully cover the majority of texts.
Theformer graph illustrates the immense importance ofthe title sentence (dhit = 1.96), and the importanceof the second (dhit = 0.75) and third (dhit = 0.64)paragraphs relative to the first (dhit = 0.59).
Para-graphs close to the beginning of texts tend to bearmore informative content; this is borne out in Fig-ure 3, which clearly indicates that paragraph posi-tions close to the end of texts do not show particu-larly high values, while the peak occurs at positionP-14 with dhit = 0.42.
This peak occurs preciselywhere most texts have their second or third para-graphs (recall that the average text length is 13 to16 paragraphs).To examine Baxendale's first/last sentence hy-pothesis, we computed the average dhit scores forthe first and the last 10 sentence positions in a para-graph as shown in Figure 4 and Figure 5 respectively.The former indicates that the closer a sentence lies285tJiuI1.4'1.2tI l lTIPSTER ZlFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE TITLESENTENCE AN{) THE RRST SO PARAGRAPH POSITIONSeAoPARAGRAPH POSITIONFigure 2: Vol.
1 dhit distribution for the title sen-tence and the first 50 paragraph positions.TIPSTER ZlFF VOL1 AVERAGE DIgT DISTRIBUTION OF THE FIRST 10 SENTENCEPosmoNs0AS0.40.350.30.250.20.150.10.050S l  S2 S3 S4 S5 S6 S7 S8 S9 S l0SENTENCEPOSmONFigure 4: Vol.
1 dhit distribution of the first 10 sen-tence positions in a paragraph.TIPSTER ~FF VOL1 AVERAGE DHIT DISTRIBUTION OF THE LAST 10 SENTENCEPOSITIONS0AS0.40.350.3"nPSTER ZIFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE LAST SOPARAGRAPH PosmoNsMie4=o.4oJ?tso,,1o.1e~op.1 p.3 p4~ p.7 P.e P-ltp*lL3P-1SI~17P.11P~1P-~3P-25p-2"/P*21p-31P.33P~p*37p-30~1~P~P47P4gp.,2 p.4 p41 p.I P-tOP-t2P.14P-11tP.I|I~:mp-22p~24p~.~tlP*21P-30P.32p-34p~NIP~p-40p42~P~PARAGRAPH POSITIONO.250.20.150.10.050S-1 ,S-2 S-3 S-4 ,S-5 S-6 S-7 S-8 S-9SENTENCE POSITIONS-10Figure 5: Vol.
1 dhit distribution of the last 10 sen-tence positions in a paragraph.to the beginning of a paragraph, the higher its dhitscore is.
This confirms the first sentence hypothe-sis.
On the other hand, the latter figure does notsupport he last sentence hypothesis; it suggests in-stead that the second sentence from the end of aparagraph contains the most information.
This isexplained by the fact that 47.7% of paragraphs inthe corpus contain only one sentence and 25.2% ofthe paragraphs contain two sentences, and the SPPis 2.05: the second-last entence is the first!Figure 3: Vol.
1 dhit distribution for the last 50paragraph positions, counting backward.
4 Eva luat ionThe goal of creating an Optimal Position Policy is toadapt the position hypothesis to various domains orgenres in order to achieve maximal topic coverage.Two checkpoints are required:286TIPSTER ZlFF VOL2 ZF_2S1 TO ZF_300 POLICy DETERMINAllON MAP\ I Ii v Ic; I , - " ,n \ /PARAGRAPH POSITION IN l TEXTFigure 6: Vol.
2 optimal position Policy Determina-tion Map in contour view.1.
applying the procedure of creating an OPP toanother collection in the same domain shouldresult in a similar OPP, and2.
sentences selected according to the OPP shouldindeed carry more information than other sen-tences.Two evaluations were conducted to confirm thesepoints.In both cases, we compared the sentences ex-tracted according to the OPP to the sentences con-tained in the human-generated abstracts.
Thoughwe could have used topic keywords for both trainingand evaluation, we decided that the abstracts wouldprovide a more interesting and practical measure foroutput, since the OPP method extracts from thetext full sentences instead of topic phrases.
Accord-ingly, we used as test corpus another, previously un-seen, set of 2,907 texts from Vol.
2 of the Ziff-Daviscorpus, which contained texts of the same natureand genre as Vol.
1.4.1 Eva luat ion  IThis evaluation established the validity of the Po-sition Hypothesis, namely that the OPP so deter-mined does in fact provide a way of identifying high-yield sentences, and is not just a list of average high-yield positions of the corpus we happened to pick.following the same steps as before, we therefore de-rived a new OPP on the test corpus.The result of the average scores of 300 positions(Pro, Sn) shown in Figure 6, with 1 < m < 30 and1 < n < 10, was a contour map highly similar toFigure 1.Both peak at position (P2, $1) and decrease grad-ually in the X direction and more rapidly in theY direction.
The similarity between the policy de-287termination maps of the training and test sets con-firms two things: First, correspondences xist be-tween topics and sentence positions in texts such asthe ZIFF-Davis collection.
Second, the regularitybetween topics and sentence positions can be usedto identify topic sentences in texts.4.2 Eva luat ion  I IIn the evaluation, we measured the word overlap ofsentences contained in the abstracts with sentence(s)extracted from a text according to the OPP.
For eachmeasure, we recorded scores cumulatively, choosingfirst the most promising sentence according to theOPP, then the two most promising, and so on.We measured word overlap as follows: first, we re-moved all function (closed-class) words from the ab-stract and from the text under consideration.
Then,for the first 500 sentence positions (the top 1, 2,3, .
.
.
,  taken according to the OPP),  we counted thenumber of times a window of text in the extractedsentences matched (i.e., exactly equalled) a windowof text in the abstract.
(Again we performed no mor-phology manipulations or reference resolution, stepswhich would improve the resulting scores.)
We per-formed the counts for window lengths of 1, 2, 3, 4,and 5 words.
If a sentence in an abstract matchedmore than one sentence xtracted by the OP, onlythe first match was tallied.
For each number of sen-tences extracted, and for each window size, we aver-aged the counts over all 2,907 texts.We define some terms and three measures used toassess the quality of the OPP-selected extracts.
Foran extract E and a abstract A:E .
wmi.
a window i of size m in E.wAi: a window i of size m in A.IWEI: total number of windows of size min E.IWmAJ: total number of different windowsof size m in A, i.e., how many A WmiAWrnj  .E i.e., words and word se- hit : Wmi = Wamj,quences in E and a Wrn i Wmj are exactlythe same.P rec i s ion  o f  w indows  size m:# hitsPm--  - -Recal l  o f  w indows  size m:different hitsRm = IWAICoverage  of  w indows  size m:Cm = # sentences in A with at least one hit# sentences in ATIFRTER ZIFF VOL1 POUCY SELECTION PRECISION/RECALL WINDOWS SIZE 1i.I S 4 i !
7 I IOPP SELECTED POSITIONFigure 7: Cumulative precision/recall scores of topten OPP-selected sentence positions of window size1.TIPSTER Z]FF VOL1 OPP SELIECTED PosmoN PRECSION SCOREWITH INI~ICA11ON OF INDIVIDUAL CONTI~RUllON FROMWINDQW ~ OF 1 TO SiItO @t I~Z p3 p4 I~ I~ P7 FIB p9OPP SELECTED POSITIONFigure 9: Precision scores show individual contribu-tion from window size 1 to 5.11P~TER ]~FF VOL1 POUCY SELECTION PRECISION/RECALL WINDOWS SIZE 2LI?LteLI4Ikt2Lt~S~SZ ?
4 ?
?
1 ?
?OPP SELECTED POSITIONFigure 8: Cumulative precision/recall scores of topten OPP-selected sentence positions of window size2.4.2.1 Precision and RecallPrecision, Pro, measures what percentage of win-dows of size m in E can also be found in A (thatis, P,~ indicates what percentage of E is consideredimportant with regard to A).
Recall, Rm, measuresthe diversity of E. A high P,,~ does not guaranteerecovery of all the possible topics in A, but a highRm does ensure that many different topics in A arecovered in E. However, a high Rm alone does notwarrant good performance either.
For example, anOPP that selects all the sentences in the originaltext certainly has a very high Rm, but this extractduplicates the original text and is the last hing wewant as a summary!
Duplicate matches (the sameword(s) in different windows) were counted in P butnot in R.288Figure 7 and Figure 8 show the precision/recallgraphs of window sizes 1 and 2 respectively.
Fig-ure 7 indicates that the precision score decreasesslowly and the recall score increases more rapidlyas we choose more sentences according to the OPP.Selecting 7 sentences (is 10% of the average lengthof a ZIFF text), the precision is 0.38 and the re-call 0.35.
Considering that the matching processrequires exact match and morphological transfor-mation is not used, this result is very encouraging.However, with window size 2, precision and recallscores drop seriously, and more so with even largerwindows.
This suggests using variable-length win-dows, sizing according to maximal match.
So doingwould also avoid counting matches on window size1 into matches of larger window sizes.
The contri-butions of precision, P~, and recall, R~, from eachm-word window alone, can be approximated by:P?
~.
P.,-P,,,+IR~ ~ R, , , -R, .+IFigure 9 and Figure 10 show precision and recallscores with individual contributions from windowsizes 1 to 5.
Precision P~ and recall R~ of variable-length windows can be estimated as follows:lPo_ -ZP  ?rn=lIrn=lThe performance of variable-length windows com-pared with windows of size 1 should have a differ-ence less than the amount shown in the segments ofwindow size > 5.o4us~2us0mTIPSTER ~FF VOL10PP SELECTED POSITION RECALL SCOREWITH INDICATION OF iNOIVtDUAL CONTRIBUTION FROMWINOOW SIZE OF 1 TO SRt R= R3 R4 RS R4 RF RI  R9OFF SELECTED POSITIONFigure 10: Recall scores show individual contribu-tion from window size 1 to 5.TIPSTER ZIFF VOL1 CUMULATIVE AVERAGE COVERAGE SCOREOF THE TOP 10 OPP-SELECTED POSITIONSWiTH WINDOW SIZE OF 1 TO So.== |o., ?~ ; w i. ;  i I -=' m i - w ?u.
I Io, ) _=,T : L L=, L., =',, ; I , .
?17F' I.
[ ;  I i l:l l l l I = = iR, ~ .= ~ ms A= .1 ~ ~ M~eOPP SELECTED POSITION4.2.2 CoverageCoverage, Cm, tests similarity between E and Ain a very loose sense.
It counts the number of sen-tences in A with at least one hit in E (i.e., thereexists at least one pair of windows wmiA and wEmjsuch that wAi = WEj).
Cm estimates the potentialof the OPP procedure.
Figure 11 shows the cumula-tive average coverage scores of the top ten sentencepositions of the training set following the OPP.
Fig-ure 11 indicates that 68% of sentences in A sharedwith the title sentence at least one word, 25% twowords, 10% three words, 4% four words, and 2% fivewords.
The amount of sharing at least one wordgoes up to 88% if we choose the top 5 positions ac-cording to the OPP and 95% if we choose the top 10positions?The contribution of coverage score, C~, solelyfrom m-word match between E and A can be com-puted as follows:C~ = Cm - Cm-1The result is shown in Figure 12.
Notice that thetopmost segment of each column in Figure 12 repre-sents the contribution from matches of at least fivewords long, since we only have Cm up to m = 5.
Theaverage number of sentences per summary (SPS) is5.76.
If we choose the top 5 sentence positions ac-cording to the OPP, Figure 12 tells us that these5-sentences extracts E (the average length of an ab-stract), cover 88% of A in which 42% derives solelyfrom one-word matches, 22% two words, 11% threewords, and 6% four words.
The average numberof sentences per text in the corpus is about 70.
Ifwe produce an extract of about 10% of the averagelength of a text, i.e.
7 sentences, the coverage scoreis 0.91.
This result is extremely promising and con-firms the OPP-selected extract bearing importantcontents.Figure 11: Cumulative coverage scores of top tensentence positions according to the OPP, with win-dow sizes 1 to 5.TIPSTER ZIFF VOL1 CUMULATIVE AVERAGE COVERAGEOF TOP 10 OPP-SELECTED POSITIONSOPP SELECTED POSITIONFigure 12: Cumulative coverage scores of top tensentence positions with contribution marked for eachwindow size.289TIPSTER ZFF  VOL10PP  CUMULA~VE DJtrT PER TOPICOF THE OPP SELECTED TOP 18 POSITIONS?
7 m as7 o4o ~R am ~ ~ ~ ?am I m " ; ;  II - -.
II !
i  u ~4 ??
.
-m i  m : ~s,~  |a4u : :?
?
n a u mO ~ ~ ~1 P~ P4111 Iq~ P3~ P1~$ ~1 PT/Sl P~I  PWS1 Pltt~l P I~  P~ P1~3 P~ P2/S4OPP SELECTED TOP 18 POSITIONSFigure 13: Cumulative dhit per topic for the top 18OPP selected positions.5 Conc lus ionThis study provides empirical validation for the Po-sition Hypothesis.
It also describes a method of de-riving an Optimal Position Policy for a collection oftexts within a genre, as long as a small set of topickeywords is defined with each text.
The Precisionand Recall scores indicate the selective power of thePosition method on individual topics, while the Cov-erage scores indicate a kind of upper bound on topicsand related material as contained in sentences fromhuman-produced abstracts.The results displayed in Figure 13 are especiallypromising.
It is clear that only about 30% of topickeywords are not mentioned in the text directly.This is excellent news: it means that as an upperbound, only about 30% of the humans' abstracts inthis domain derive from some inference processes,which means that in a computational implementa-tion only about the same amount has to be derivedby processes yet to be determined.
Second, the ti-tle contains about 50% of the topic keywords; thetitle plus the two most rewarding sentences provideabout 60%, and the next five or so add another 6%.Thus, a fairly small number of sentences provides2/3 of the keyword topics.It must be remembered that our evaluations treatthe abstract as ideal--they rest on the assumptionthat the central topic(s) of a text are contained inthe abstract made of it.
In many cases, this is agood assumption; it provides what one may call theauthor's perspective of the text.
But this assump-tion does not support goal-oriented topic search, inwhich one wants to know whether a text pertainsto some particular prespecified topics.
For a goal-oriented perspective, one has to develop a differentmethod to derive an OPP; this remains the topic of290future work.Ultimately, the Position Method can only takeone a certain distance.
Because of its limited powerof resolution--the sentence--and its limited methodof identification--ordinal positions in a text- - i t  hasto be augmented by additional, more precise tech-niques.
But the results gained from what is afterall a fairly simple technique are rather astoundingnonetheless.Re ferencesP.
B. Baxendale.
1958.
Machine-made index fortechnical literature - -  an experiment.
IBM Jour-nal, pages 354-361, October.Richard Braddock.
1974.
The frequency and place-ment of topic sentences in expository prose.
InResearch in The Teaching of English, volume 8,pages 287-302.Dan Dolan.
1980.
Locating main ideas in historytextbooks.
In Journal of Reading, pages 135-140.H.
P. Edmundson.
1969.
New methods in automaticextracting.
Journal of the ACM, 16(2):264-285.Donna Harman.
1994.
Data preparation.
In R. Mer-chant, editor, The Proceedings of the TIPSTERText Program Phase I, San Mateo, California.Morgan Kaufmann Publishing Co.D.E.
Kieras, 1985.
Thematic Process in the Com-prehension of Technical Prose, pages 89-108.Lawrence Elrbaum Association, Hillsdale, NewJersey.H.
P. Luhn.
1958.
The automatic reation of lit-erature abstracts.
IBM Journal, pages 159-165,April.J.J.
Paijmans.
1994.
Relative weights of words indocuments.
In L.G.M.
Noordman and W.A.M.de Vroomen, editors, Conference Proceedings ofSTINFON.
StinfoN.Gerard Salton, James Allan, Chris Buckley, andAmit Singhal.
1994.
Automatic analysis,theme generation, and summarization of machine-readable texts.
Science, 264:1421-1426, June.Harry Singer and Dan Dolan.
1980.
Reading AndLearning from Text.
Little Brown, Boston, Mass.Colleen Langdon Sjostrom and Victoria Chou Hare.1984.
Teaching high school students to identifymain ideas in expository text.
Journal of Educa-tional Research, 78(2):114-118.
