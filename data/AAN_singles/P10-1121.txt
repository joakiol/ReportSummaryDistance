Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189?1198,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsComplexity Metrics in an Incremental Right-corner ParserStephen Wu Asaf Bachrach?
Carlos Cardenas?
William Schuler?Department of Computer Science, University of Minnesota?
Unit de Neuroimagerie Cognitive INSERM-CEA?
Department of Brain & Cognitive Sciences, Massachussetts Institute of Technology?
University of Minnesota and The Ohio State Universityswu@cs.umn.edu ?asaf@mit.edu ?cardenas@mit.edu ?schuler@ling.ohio-state.eduAbstractHierarchical HMM (HHMM) parsersmake promising cognitive models: whilethey use a bounded model of workingmemory and pursue incremental hypothe-ses in parallel, they still achieve parsingaccuracies competitive with chart-basedtechniques.
This paper aims to validatethat a right-corner HHMM parser is alsoable to produce complexity metrics, whichquantify a reader?s incremental difficultyin understanding a sentence.
Besidesdefining standard metrics in the HHMMframework, a new metric, embeddingdifference, is also proposed, which teststhe hypothesis that HHMM store elementsrepresents syntactic working memory.Results show that HHMM surprisaloutperforms all other evaluated metricsin predicting reading times, and thatembedding difference makes a significant,independent contribution.1 IntroductionSince the introduction of a parser-based calcula-tion for surprisal by Hale (2001), statistical tech-niques have been become common as models ofreading difficulty and linguistic complexity.
Sur-prisal has received a lot of attention in recent lit-erature due to nice mathematical properties (Levy,2008) and predictive ability on eye-tracking move-ments (Demberg and Keller, 2008; Boston et al,2008a).
Many other complexity metrics havebeen suggested as mutually contributing to readingdifficulty; for example, entropy reduction (Hale,2006), bigram probabilities (McDonald and Shill-cock, 2003), and split-syntactic/lexical versions ofother metrics (Roark et al, 2009).A parser-derived complexity metric such as sur-prisal can only be as good (empirically) as themodel of language from which it derives (Frank,2009).
Ideally, a psychologically-plausible lan-guage model would produce a surprisal that wouldcorrelate better with linguistic complexity.
There-fore, the specification of how to encode a syntac-tic language model is of utmost importance to thequality of the metric.However, it is difficult to quantify linguis-tic complexity and reading difficulty.
The twocommonly-used empirical quantifications of read-ing difficulty are eye-tracking measurements andword-by-word reading times; this paper uses read-ing times to find the predictiveness of severalparser-derived complexity metrics.
Various fac-tors (i.e., from syntax, semantics, discourse) arelikely necessary for a full accounting of linguis-tic complexity, so current computational models(with some exceptions) narrow the scope to syn-tactic or lexical complexity.Three complexity metrics will be calculated ina Hierarchical Hidden Markov Model (HHMM)parser that recognizes trees in right-corner form(the left-right dual of left-corner form).
This typeof parser performs competitively on standard pars-ing tasks (Schuler et al, 2010); also, it reflectsplausible accounts of human language processingas incremental (Tanenhaus et al, 1995; Brants andCrocker, 2000), as considering hypotheses proba-bilistically in parallel (Dahan and Gaskell, 2007),as bounding memory usage to short-term mem-ory limits (Cowan, 2001), and as requiring morememory storage for center-embedding structuresthan for right- or left-branching ones (Chomskyand Miller, 1963; Gibson, 1998).
Also, unlikemost other parsers, this parser preserves the arc-eager/arc-standard ambiguity of Abney and John-1189son (1991).
Typical parsing strategies are arc-standard, keeping all right-descendants open forsubsequent attachment; but since there can be anunbounded number of such open constituents, thisassumption is not compatible with simple mod-els of bounded memory.
A consistently arc-eagerstrategy acknowledges memory bounds, but yieldsdead-end parses.
Both analyses are considered inright-corner HHMM parsing.The purpose of this paper is to determinewhether the language model defined by theHHMM parser can also predict reading times ?it would be strange if a psychologically plausi-ble model did not also produce viable complex-ity metrics.
In the course of showing that theHHMM parser does, in fact, predict reading times,we will define surprisal and entropy reduction inthe HHMM parser, and introduce a third metriccalled embedding difference.Gibson (1998; 2000) hypothesized two typesof syntactic processing costs: integration cost, inwhich incremental input is combined with exist-ing structures; and memory cost, where unfinishedsyntactic constructions may incur some short-termmemory usage.
HHMM surprisal and entropyreduction may be considered forms of integra-tion cost.
Though typical PCFG surprisal hasbeen considered a forward-looking metric (Dem-berg and Keller, 2008), the incremental nature ofthe right-corner transform causes surprisal and en-tropy reduction in the HHMM parser to measurethe likelihood of grammatical structures that werehypothesized before evidence was observed forthem.
Therefore, these HHMM metrics resemblean integration cost encompassing both backward-looking and forward-looking information.On the other hand, embedding difference isdesigned to model the cost of storing center-embedded structures in working memory.
Chen,Gibson, and Wolf (2005) showed that sentencesrequiring more syntactic memory during sen-tence processing increased reading times, and itis widely understood that center-embedding incurssignificant syntactic processing costs (Miller andChomsky, 1963; Gibson, 1998).
Thus, we wouldexpect for the usage of the center-embeddingmemory store in an HHMM parser to correlatewith reading times (and therefore linguistic com-plexity).The HHMM parser processes syntactic con-structs using a bounded number of store states,defined to represent short-term memory elements;additional states are utilized whenever center-embedded syntactic structures are present.
Simi-lar models such as Crocker and Brants (2000) im-plicitly allow an infinite memory size, but Schuleret al (2008; 2010) showed that a right-cornerHHMM parser can parse most sentences in En-glish with 4 or fewer center-embedded-depth lev-els.
This behavior is similar to the hypothesizedsize of a human short-term memory store (Cowan,2001).
A positive result in predicting readingtimes will lend additional validity to the claimthat the HHMM parser?s bounded memory cor-responds to bounded memory in human sentenceprocessing.The rest of this paper is organized as fol-lows: Section 2 defines the language model of theHHMM parser, including definitions of the threecomplexity metrics.
The methodology for evalu-ating the complexity metrics is described in Sec-tion 3, with actual results in Section 4.
Further dis-cussion on results, and comparisons to other work,are in Section 5.2 Parsing ModelThis section describes an incremental parser inwhich surprisal and entropy reduction are sim-ple calculations (Section 2.1).
The parser uses aHierarchical Hidden Markov Model (Section 2.2)and recognizes trees in a right-corner form (Sec-tion 2.3 and 2.4).
The new complexity metric, em-bedding difference (Section 2.5), is a natural con-sequence of this HHMM definition.
The modelis equivalent to previous HHMM parsers (Schuler,2009), but reorganized into 5 cases to clarify theright-corner structure of the parsed sentences.2.1 Surprisal and Entropy in HMMsHidden Markov Models (HMMs) probabilisticallyconnect sequences of observed states ot and hid-den states qt at corresponding time steps t. In pars-ing, observed states are words; hidden states canbe a conglomerate state of linguistic information,here taken to be syntactic.The HMM is an incremental, time-series struc-ture, so one of its by-products is the prefix prob-ability, which will be used to calculate surprisal.This is the probability that that words o1..t havebeen observed at time t, regardless of which syn-tactic states q1..t produced them.
Bayes?
Law andMarkov independence assumptions allow this to1190be calculated from two generative probability dis-tributions.1Pre(o1..t)=?q1..tP(o1..t q1..t) (1)def=?q1..tt??=1P?A(q?
| q??1)?P?B(o?
| q? )
(2)Here, probabilities arise from a TransitionModel (?A) between hidden states and an Ob-servation Model (?B) that generates an observedstate from a hidden state.
These models are sotermed for historical reasons (Rabiner, 1990).Surprisal (Hale, 2001) is then a straightforwardcalculation from the prefix probability.Surprisal(t) = log2Pre(o1..t?1)Pre(o1..t)(3)This framing of prefix probability and surprisal ina time-series model is equivalent to Hale?s (2001;2006), assuming that q1..t ?
Dt, i.e., that the syn-tactic states we are considering form derivationsDt, or partial trees, consistent with the observedwords.
We will see that this is the case for ourparser in Sections 2.2?2.4.Entropy is a measure of uncertainty, defined asH(x) = ?P(x) log2 P(x).
Now, the entropy Htof a t-word string o1..t in an HMM can be written:Ht =?q1..tP(q1..t o1..t) log2 P(q1..t o1..t) (4)and entropy reduction (Hale, 2003; Hale, 2006) atthe tth word is thenER(ot) = max(0, Ht?1 ?
Ht) (5)Both of these metrics fall out naturally from thetime-series representation of the language model.The third complexity metric, embedding differ-ence, will be discussed after additional back-ground in Section 2.5.In the implementation of an HMM, candidatestates at a given time qt are kept in a trel-lis, with step-by-step backpointers to the highest-probability q1..t?1.2 Also, the best qt are often keptin a beam Bt, discarding low-probability states.1Technically, a prior distribution over hidden states,P(q0), is necessary.
This q0 is factored and taken to be a de-terministic constant, and is therefore unimportant as a proba-bility model.2Typical tasks in an HMM include finding the most likelysequence via the Viterbi algorithm, which stores these back-pointers to maximum-probability previous states and canuniquely find the most likely sequence.This mitigates the problems of large state spaces(e.g., that of all possible grammatical derivations).Since beams have been shown to perform well(Brants and Crocker, 2000; Roark, 2001; Bostonet al, 2008b), complexity metrics in this paperare calculated on a beam rather than over all (un-bounded) possible derivations Dt.
The equationsabove, then, will replace the assumption q1..t ?Dtwith qt?Bt.2.2 Hierarchical Hidden Markov ModelsHidden states q can have internal structure; in Hi-erarchical HMMs (Fine et al, 1998; Murphy andPaskin, 2001), this internal structure will be usedto represent syntax trees and looks like severalHMMs stacked on top of each other.
As such, qtis factored into sequences of depth-specific vari-ables ?
one for each of D levels in the HMM hi-erarchy.
In addition, an intermediate variable ft isintroduced to interface between the levels.qtdef= ?q1t .
.
.
qDt ?
(6)ftdef= ?f1t .
.
.
fDt ?
(7)Transition probabilities P?A(qt | qt?1) over com-plex hidden states qt are calculated in two phases:?
Reduce phase.
Yields an intermediatestate ft, in which component HMMs may ter-minate.
This ft tells ?higher?
HMMs to holdover their information if ?lower?
levels are inoperation at any time step t, and tells lowerHMMs to signal when they?re done.?
Shift phase.
Yields a modeled hidden state qt,in which unterminated HMMs transition, andterminated HMMs are re-initialized fromtheir parent HMMs.Each phase is factored according to level-specific reduce and shift models, ?F and ?Q:P?A(qt|qt?1) =?ftP(ft|qt?1)?P(qt|ft qt?1) (8)def=?f1..DtD?d=1P?F(fdt |fd+1t qdt?1qd?1t?1 )?
P?Q(qdt |fd+1t fdt qdt?1qd?1t ) (9)with fD+1t and q0t defined as constants.
Note thatonly qt is present at the end of the probability cal-culation.
In step t, ft?1 will be unused, so themarginalization of Equation 9 does not lose anyinformation.1191.
.
.. .
.. .
.. .
.f3t?1f2t?1f1t?1q1t?1q2t?1q3t?1ot?1f3tf2tf1tq1tq2tq3tot(a) Dependency structure in the HHMMparser.
Conditional probabilities at a node aredependent on incoming arcs.d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8theengineerspulledoff anengineeringtrick?
?
?
?
?
?
??
?
vbdVBD/PRT?
?
?dtNP/NNS/VPS/VPS/NPS/NNS/NN S(b) HHMM parser as a store whose elements at each time step are listedvertically, showing a good hypothesis on a sample sentence out of manykept in parallel.
Variables corresponding to qdt are shown.SNPDTtheNNengineersVPVBDVBDpulledPRToffNPDTanNNNNengineeringNNtrick(c) A sample sentence in CNF.SS/NNS/NNS/NPS/VPNPNP/NNDTtheNNengineersVBDVBD/PRTVBDpulledPRToffDTanNNengineeringNNtrick(d) The right-corner transformed version of (c).Figure 1: Various graphical representations of HHMM parser operation.
(a) shows probabilistic depen-dencies.
(b) considers the qdt store to be incremental syntactic information.
(c)?
(d) demonstrate theright-corner transform, similar to a left-to-right traversal of (c).
In ?NP/NN?
we say that NP is the activeconstituent and NN is the awaited.The Observation Model ?B is comparativelymuch simpler.
It is only dependent on the syntac-tic state at D (or the deepest active HHMM level).P?B(ot | qt)def= P(ot | qDt ) (10)Figure 1(a) gives a schematic of the dependencystructure of Equations 8?10 for D = 3.
Evalua-tions in this paper are done with D = 4, followingthe results of Schuler, et al (2008).2.3 Parsing right-corner treesIn this HHMM formulation, states and dependen-cies are optimized for parsing right-corner trees(Schuler et al, 2008; Schuler et al, 2010).
A sam-ple transformation between CNF and right-cornertrees is in Figures 1(c)?1(d).Figure 1(b) shows the corresponding store-element interpretation3 of the right corner treein 1(d).
These can be used as a case study tosee what kind of operations need to occur in an3This is technically a pushdown automoton (PDA), wherethe store is limited to D elements.
When referring to direc-tions (e.g., up, down), PDAs are typically described oppositeof the one in Figure 1(b); here, we push ?up?
instead of down.HHMM when parsing right-corner trees.
Thereis one unique set of HHMM state values for eachtree, so the operations can be seen on either thetree or the store elements.At each time step t, a certain number of el-ements (maximum D) are kept in memory, i.e.,in the store.
New words are observed input, andthe bottom occupied element (the ?frontier?
of thestore) is the context; together, they determine whatthe store will look like at t+1.
We can characterizethe types of store-element changes by when theyhappen in Figures 1(b) and 1(d):Cross-level Expansion (CLE).
Occupies a newstore element at a given time step.
For exam-ple, at t=1, a new store element is occupiedwhich can interact with the observed word,?the.?
At t = 3, an expansion occupies thesecond store element.In-level Reduction (ILR).
Completes an activeconstituent that is a unary child in the right-corner tree; always accompanied by an in-level expansion.
At t= 2, ?engineers?
com-pletes the active NP constituent; however, the1192level is not yet complete since the NP is alongthe left-branching trunk of the tree.In-level Expansion (ILE).
Starts a new activeconstituent at an already-occupied store ele-ment; always follows an in-level reduction.With the NP complete in t= 2, a new activeconstituent S is produced at t=3.In-level Transition (ILT).
Transitions the storeto a new state in the next time step at the samelevel, where the awaited constituent changesand the active constituent remains the same.This describes each of the steps from t=4 tot=8 at d=1 .Cross-level Reduction (CLR).
Vacates a storeelement on seeing a complete active con-stituent.
This occurs after t = 4; ?off?completes the active (at depth 2) VBD con-stituent, and vacates store element 2.
Thisis accompanied with an in-level transition atdepth 1, producing the store at t=5.
It shouldbe noted that with some probability, complet-ing the active constituent does not vacate thestore element, and the in-level reduction casewould have to be invoked.The in-level/cross-level ambiguity occurs in theexpansion as well as the reduction, similar to Ab-ney and Johnson?s arc-eager/arc-standard compo-sition strategies (1991).
At t=3, another possiblehypothesis would be to remain on store element1 using an ILE instead of a CLE.
The HHMMparser, unlike most other parsers, will preserve thisin-level/cross-level ambiguity by considering bothhypotheses in parallel.2.4 Reduce and Shift ModelsWith the understanding of what operations need tooccur, a formal definition of the language model isin order.
Let us begin with the relevant variables.A shift variable qdt at depth d and time step t isa syntactic state that must represent the active andawaited constituents of right-corner form:qdtdef= ?gAqdt , gWqdt?
(11)e.g., in Figure 1(b), q12=?NP,NN?=NP/NN.
Each g isa constituent from the pre-right-corner grammar,G.Reduce variables f are then enlisted to ensurethat in-level and cross-level operations are correct.fdtdef= ?kfdt , gfdt ?
(12)First, kfdt is a switching variable that differenti-ates between ILT, CLE/CLR, and ILE/ILR.
Thisswitching is the most important aspect of fdt , soregardless of what gfdt is, we will use:?
fdt ?
F0 when kfdt =0, (ILT/no-op)?
fdt ?
F1 when kfdt =1, (CLE/CLR)?
fdt ?
FG when kfdt ?
G. (ILE/ILR)Then, gfdt is used to keep track of a completely-recognized constituent whenever a reduction oc-curs (ILR or CLR).
For example, in Figure 1(b),after time step 2, an NP has been completely rec-ognized and precipitates an ILR.
The NP getsstored in gf13 for use in the ensuing ILE insteadof appearing in the store-elements.This leads us to a specification of the reduce andshift probability models.
The reduce step happensfirst at each time step.
True to its name, the re-duce step handles in-level and cross-level reduc-tions (the second and third case below):P?F(fdt | fd+1t qdt?1qd?1t?1 )def={if fd+1t 6?FG : Jfdt =0Kif fd+1t ?FG, fdt ?
F1 : P?
?F-ILR,d(fdt | qdt?1 qd?1t?1 )if fd+1t ?FG, fdt ?
FG : P?
?F-CLR,d(fdt | qdt?1 qd?1t?1 )(13)with edge cases q0t and fD+1t defined as appropri-ate constants.
The first case is just store-elementmaintenance, in which the variable is not on the?frontier?
and therefore inactive.Examining ?F-ILR,d and ?F-CLR,d, we see thatthe produced fdt variables are also used in the ?if?statement.
These models can be thought of aspicking out a fdt first, finding the matching case,then applying the probability models that matches.These models are actually two parts of the samemodel when learned from trees.Probabilities in the shift step are also split intocases based on the reduce variables.
More main-tenance operations (first case) accompany transi-tions producing new awaited constituents (secondcase below) and expansions producing new activeconstituents (third and fourth case):P?Q(qdt | fd+1t fdt qdt?1qd?1t )def=????
?if fd+1t 6?FG : Jqdt = qdt?1Kif fd+1t ?FG, fdt ?
F0 : P?
?Q-ILT,d(qdt | fd+1t qdt?1 qd?1t )if fd+1t ?FG, fdt ?
F1 : P?
?Q-ILE,d(qdt | fdt qdt?1 qd?1t )if fd+1t ?FG, fdt ?FG : P?
?Q-CLE,d(qdt | qd?1t )(14)1193FACTOR DESCRIPTION EXPECTEDWord order innarrativeFor each story, words were indexed.
Subjects would tend to read faster later in a story.
negativeslopeReciprocallengthLog of the reciprocal of the number of letters in each word.
A decrease in the reciprocal(increase in length) might mean longer reading times.positiveslopeUnigramfrequencyA log-transformed empirical count of word occurrences in the Brown Corpus section ofthe Penn Treebank.
Higher frequency should indicate shorter reading times.negativeslopeBigramprobabilityA log-transformed empirical count of two-successive-word occurrences, with Good-Turing smoothing on words occuring less than 10 times.negativeslopeEmbeddingdifferenceAmount of change in HHMM weighted-average embedding depth.
Hypothesized to in-crease with larger working memory requirements, which predict longer reading times.positiveslopeEntropyreductionAmount of decrease in the HHMM?s uncertainty about the sentence.
Larger reductionsin uncertainty are hypothesized to take longer.positiveslopeSurprisal ?Surprise value?
of a word in the HHMM parser; models were trained on the Wall StreetJournal, sections 02?21.
More surprising words may take longer to read.positiveslopeTable 1: A list of factors hypothesized to contribute to reading times.
All data was mean-centered.A final note: the notation P??(?
| ?)
has been usedto indicate probability models that are empirical,trained directly from frequency counts of right-corner transformed trees in a large corpus.
Alter-natively, a standard PCFG could be trained on acorpus (or hand-specified), and then the grammaritself can be right-corner transformed (Schuler,2009).Taken together, Equations 11?14 define theprobabilistic structure of the HHMM for parsingright-corner trees.2.5 Embedding difference in the HHMMIt should be clear from Figure 1 that at any timestep while parsing depth-bounded right-cornertrees, the candidate hidden state qt will have a?frontier?
depth d(qt).
At time t, the beam ofpossible hidden states qt stores the syntactic state(and a backpointer) along with its probability,P(o1..t q1..t).
The average embedding depth at atime step is then?EMB(o1..t) =?qt?Btd(qt) ?P(o1..t q1..t)?q?t?Bt P(o1..t q?1..t)(15)where we have directly used the beam notation.The embedding difference metric is:EmbDiff(o1..t) = ?EMB(o1..t) ?
?EMB(o1..t?1)(16)There is a strong computational correspondencebetween this definition of embedding differenceand the previous definition of surprisal.
To seethis, we rewrite Equations 1 and 3:Pre(o1..t)=?qt?BtP(o1..t q1..t) (1?
)Surprisal(t) = log2 Pre(o1..t?1) ?
log2 Pre(o1..t)(3?
)Both surprisal and embedding difference includesummations over the elements of the beam, andare calculated as a difference between previousand current beam states.Most differences between these metrics are rel-atively inconsequential.
For example, the dif-ference in order of subtraction only assures thata positive correlation with reading times is ex-pected.
Also, the presence of a logarithm is rel-atively minor.
Embedding difference weighs theprobabilities with center-embedding depths andthen normalizes the values; since the measure isa weighted average of embedding depths ratherthan a probability distribution, ?EMB is not alwaysless than 1 and the correspondence with Kullback-Leibler divergence (Levy, 2008) does not hold, soit does not make sense to take the logs.Therefore, the inclusion of the embeddingdepth, d(qt), is the only significant differencebetween the two metrics.
The result is a met-ric that, despite numerical correspondence to sur-prisal, models the HHMM?s hypotheses aboutmemory cost.3 EvaluationSurprisal, entropy reduction, and embedding dif-ference from the HHMM parser were evaluatedagainst a full array of factors (Table 1) on a cor-pus of word-by-word reading times using a linearmixed-effects model.1194The corpus of reading times for 23 native En-glish speakers was collected on a set of four nar-ratives (Bachrach et al, 2009), each composed ofsentences that were syntactically complex but con-structed to appear relatively natural.
Using Linger2.88, words appeared one-by-one on the screen,and required a button-press in order to advance;they were displayed in lines with 11.5 words onaverage.Following Roark et al?s (2009) work on thesame corpus, reading times above 1500 ms (fordiverted attention) or below 150 ms (for buttonpresses planned before the word appeared) werediscarded.
In addition, the first and last word ofeach line on the screen were removed; this left2926 words out of 3540 words in the corpus.For some tests, a division between open- andclosed-class words was made, with 1450 and 1476words, respectively.
Closed-class words (e.g., de-terminers or auxiliary verbs) usually play somekind of syntactic function in a sentence; our evalu-ations used Roark et al?s list of stop words.
Openclass words (e.g., nouns and other verbs) morecommonly include new words.
Thus, one may ex-pect reading times to differ for these two types ofwords.Linear mixed-effect regression analysis wasused on this data; this entails a set of fixed effectsand another of random effects.
Reading times ywere modeled as a linear combination of factorsx, listed in Table 1 (fixed effects); some randomvariation in the corpus might also be explained bygroupings according to subject i, word j, or sen-tence k (random effects).yijk = ?0 +mX?=1??xijk?
+ bi + bj + bk + ?
(17)This equation is solved for each of m fixed-effect coefficients ?
with a measure of confidence(t-value = ??/SE(??
), where SE is the standard er-ror).
?0 is the standard intercept to be estimatedalong with the rest of the coefficients, to adjust foraffine relationships between the dependent and in-dependent variables.
We report factors as statisti-cally significant contributors to reading time if theabsolute value of the t-value is greater than 2.Two more types of comparisons will be made tosee the significance of factors.
First, a model ofdata with the full list of factors can be comparedto a model with a subset of those factors.
This isdone with a likelihood ratio test, producing (formixed-effects models) a ?21 value and correspond-ing probability that the smaller model could haveproduced the same estimates as the larger model.A lower probability indicates that the additionalfactors in the larger model are significant.Second, models with different fixed effects canbe compared to each other through various infor-mation criteria; these trade off between havinga more explanatory model vs. a simpler model,and can be calculated on any model.
Here, weuse Akaike?s Information Criterion (AIC), wherelower values indicate better models.All these statistics were calculated in R, usingthe lme4 package (Bates et al, 2008).4 ResultsUsing the full list of factors in Table 1, fixed-effectcoefficients were estimated in Table 2.
Fitting thebest model by AIC would actually prune awaysome of the factors as relatively insignificant, butthese smaller models largely accord with the sig-nificance values in the table and are therefore notpresented.The first data column shows the regression onall data; the second and third columns divide thedata into open and closed classes, because an eval-uation (not reported in detail here) showed statis-tically significant interactions between word classand 3 of the predictors.
Additionally, this facil-itates comparison with Roark et al (2009), whomake the same division.Out of the non-parser-based metrics, word orderand bigram probability are statistically significantregardless of the data subset; though reciprocallength and unigram frequency do not reach signif-icance here, likelihood ratio tests (not shown) con-firm that they contribute to the model as a whole.It can be seen that nearly all the slopes have beenestimated with signs as expected, with the excep-tion of reciprocal length (which is not statisticallysignificant).Most notably, HHMM surprisal is seen here tobe a standout predictive measure for reading timesregardless of word class.
If the HHMM parser isa good psycholinguistic model, we would expectit to at least produce a viable surprisal metric, andTable 2 attests that this is indeed the case.
Thoughit seems to be less predictive of open classes, asurprisal-only model has the best AIC (-7804) outof any open-class model.
Considering the AICon the full data, the worst model with surprisal1195FULL DATA OPEN CLASS CLOSED CLASSCoefficient Std.
Err.
t-value Coefficient Std.
Err.
t-value Coefficient Std.
Err.
t-value(Intcpt) -9.340?10?3 5.347?10?2 -0.175 -1.237?10?2 5.217?10?2 -0.237 -6.295?10?2 7.930?10?2 -0.794order -3.746?10?5 7.808?10?6 -4.797?
-3.697?10?5 8.002?10?6 -4.621?
-3.748?10?5 8.854?10?6 -4.232?rlength -2.002?10?2 1.635?10?2 -1.225 9.849?10?3 1.779?10?2 0.554 -2.839?10?2 3.283?10?2 -0.865unigrm -8.090?10?2 3.690?10?1 -0.219 -1.047?10?1 2.681?10?1 -0.391 -3.847?10+0 5.976?10+0 -0.644bigrm -2.074?10+0 8.132?10?1 -2.551?
-2.615?10+0 8.050?10?1 -3.248?
-5.052?10+1 1.910?10+1 -2.645?embdiff 9.390?10?3 3.268?10?3 2.873?
2.432?10?3 4.512?10?3 0.539 1.598?10?2 5.185?10?3 3.082?etrpyrd 2.753?10?2 6.792?10?3 4.052?
6.634?10?4 1.048?10?2 0.063 4.938?10?2 1.017?10?2 4.857?srprsl 3.950?10?3 3.452?10?4 11.442?
2.892?10?3 4.601?10?4 6.285?
5.201?10?3 5.601?10?4 9.286?Table 2: Results of linear mixed-effect modeling.
Significance (indicated by ?)
is reported at p < 0.05.
(Intr) order rlngth ungrm bigrm emdiff entrpyorder .000rlength -.006 -.003unigrm .049 .000 -.479bigrm .001 .005 -.006 -.073emdiff .000 .009 -.049 -.089 .095etrpyrd .000 .003 .016 -.014 .020 -.010srprsl .000 -.008 -.033 -.079 .107 .362 .171Table 3: Correlations in the full model.
(AIC=-10589) outperformed the best model with-out it (AIC=-10478), indicating that the HHMMsurprisal is well worth including in the model re-gardless of the presence of other significant fac-tors.HHMM entropy reduction predicts readingtimes on the full dataset and on closed-classwords.
However, its effect on open-class words isinsignificant; if we compare the model of column2 against one without entropy reduction, a likeli-hood ratio test gives ?21 = 0.0022, p = 0.9623(the smaller model could easily generate the samedata).The HHMM?s average embedding differenceis also significant except in the case of open-class words ?
removing embedding difference onopen-class data yields ?21 = 0.2739, p = 0.6007.But what is remarkable is that there is any signifi-cance for this metric at all.
Embedding differenceand surprisal were relatively correlated comparedto other predictors (see Table 3), which is expectedbecause embedding difference is calculated likea weighted version of surprisal.
Despite this, itmakes an independent contribution to the full-dataand closed-class models.
Thus, we can concludethat the average embedding depth component af-fects reading times ?
i.e., the HHMM?s notion ofworking memory behaves as we would expect hu-man working memory to behave.5 DiscussionAs with previous work on large-scale parser-derived complexity metrics, the linear mixed-effect models suggest that sentence-level factorsare effective predictors for reading difficulty ?
inthese evaluations, better than commonly-used lex-ical and near-neighbor predictors (Pollatsek et al,2006; Engbert et al, 2005).
The fact that HHMMsurprisal outperforms even n-gram metrics pointsto the importance of including a notion of sentencestructure.
This is particularly true when the sen-tence structure is defined in a language model thatis psycholinguistically plausible (here, bounded-memory right-corner form).This accords with an understated result ofBoston et al?s eye-tracking study (2008a): aricher language model predicts eye movementsduring reading better than an oversimplified one.The comparison there is between phrase struc-ture surprisal (based on Hale?s (2001) calculationfrom an Earley parser), and dependency grammarsurprisal (based on Nivre?s (2007) dependencyparser).
Frank (2009) similarly reports improve-ments in the reading-time predictiveness of unlexi-calized surprisal when using a language model thatis more plausible than PCFGs.The difference in predictivity due to word classis difficult to explain.
One theory may be thatclosed-class words are less susceptible to randomeffects because there is a finite set of them forany language, making them overall easier to pre-dict via parser-derived metrics.
Or, we could notethat since closed-class words often serve grammat-ical functions in addition to their lexical content,they contribute more information to parser-derivedmeasures than open-class words.
Previous workwith complexity metrics on this corpus (Roark etal., 2009) suggests that these explanations only ac-count for part of the word-class variation in theperformance of predictors.1196Further comparsion to Roark et al will showother differences, such as the lesser role of wordlength and unigram frequency, lower overall cor-relations between factors, and the greater predic-tivity of their entropy metric.
In addition, theirmetrics are different from ours in that they are de-signed to tease apart lexical and syntactic contri-butions to reading difficulty.
Their notion of en-tropy, in particular, estimates Hale?s definition ofentropy on whole derivations (2006) by isolatingthe predictive entropy; they then proceed to defineseparate lexical and syntactic predictive entropies.Drawing more directly from Hale, our definitionis a whole-derivation metric based on the condi-tional entropy of the words, given the root.
(Theroot constituent, though unwritten in our defini-tions, is always included in the HHMM start state,q0.
)More generally, the parser used in these evalu-ations differs from other reported parsers in thatit is not lexicalized.
One might expect for thisto be a weakness, allowing distributions of prob-abilities at each time step in places not licensedby the observed words, and therefore giving poorprobability-based complexity metrics.
However,we see that this language model performs welldespite its lack of lexicalization.
This indicatesthat lexicalization is not a requisite part of syntac-tic parser performance with respect to predictinglinguistic complexity, corroborating the evidenceof Demberg and Keller?s (2008) ?unlexicalized?
(POS-generating, not word-generating) parser.Another difference is that previous parsers haveproduced useful complexity metrics without main-taining arc-eager/arc-standard ambiguity.
Resultsshow that including this ambiguity in the HHMMat least does not invalidate (and may in fact im-prove) surprisal or entropy reduction as reading-time predictors.6 ConclusionThe task at hand was to determine whether theHHMM could consistently be considered a plau-sible psycholinguistic model, producing viablecomplexity metrics while maintaining other char-acteristics such as bounded memory usage.
Thelinear mixed-effects models on reading times val-idate this claim.
The HHMM can straightfor-wardly produce highly-predictive, standard com-plexity metrics (surprisal and entropy reduction).HHMM surprisal performs very well in predictingreading times regardless of word class.
Our for-mulation of entropy reduction is also significantexcept in open-class words.The new metric, embedding difference, uses theaverage center-embedding depth of the HHMMto model syntactic-processing memory cost.
Thismetric can only be calculated on parsers with anexplicit representation for short-term memory el-ements like the right-corner HHMM parser.
Re-sults show that embedding difference does predictreading times except in open-class words, yieldinga significant contribution independent of surprisaldespite the fact that its definition is similar to thatof surprisal.AcknowledgmentsThanks to Brian Roark for help on the readingtimes corpus, Tim Miller for the formulation ofentropy reduction, Mark Holland for statistical in-sight, and the anonymous reviewers for their input.This research was supported by National ScienceFoundation CAREER/PECASE award 0447685.The views expressed are not necessarily endorsedby the sponsors.ReferencesSteven P. Abney and Mark Johnson.
1991.
Memoryrequirements and local ambiguities of parsing strate-gies.
J. Psycholinguistic Research, 20(3):233?250.Asaf Bachrach, Brian Roark, Alex Marantz, SusanWhitfield-Gabrieli, Carlos Cardenas, and John D.E.Gabrieli.
2009.
Incremental prediction in naturalis-tic language processing: An fMRI study.Douglas Bates, Martin Maechler, and Bin Dai.
2008.lme4: Linear mixed-effects models using S4 classes.R package version 0.999375-31.Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,U.
Patil, and Shravan Vasishth.
2008a.
Parsing costsas predictors of reading difficulty: An evaluation us-ing the Potsdam Sentence Corpus.
Journal of EyeMovement Research, 2(1):1?12.Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,and Shravan Vasishth.
2008b.
Surprising parser ac-tions and reading difficulty.
In Proceedings of ACL-08: HLT, Short Papers, pages 5?8, Columbus, Ohio,June.
Association for Computational Linguistics.Thorsten Brants and Matthew Crocker.
2000.
Prob-abilistic parsing and psychological plausibility.
InProceedings of COLING ?00, pages 111?118.1197Evan Chen, Edward Gibson, and Florian Wolf.
2005.Online syntactic storage costs in sentence com-prehension.
Journal of Memory and Language,52(1):144?169.Noam Chomsky and George A. Miller.
1963.
Intro-duction to the formal analysis of natural languages.In Handbook of Mathematical Psychology, pages269?321.
Wiley.Nelson Cowan.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storagecapacity.
Behavioral and Brain Sciences, 24:87?185.Matthew Crocker and Thorsten Brants.
2000.
Wide-coverage probabilistic sentence processing.
Journalof Psycholinguistic Research, 29(6):647?669.Delphine Dahan and M. Gareth Gaskell.
2007.
Thetemporal dynamics of ambiguity resolution: Evi-dence from spoken-word recognition.
Journal ofMemory and Language, 57(4):483?501.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.Ralf Engbert, Antje Nuthmann, Eike M. Richter, andReinhold Kliegl.
2005.
SWIFT: A dynamical modelof saccade generation during reading.
PsychologicalReview, 112:777?813.Shai Fine, Yoram Singer, and Naftali Tishby.
1998.The hierarchical hidden markov model: Analysisand applications.
Machine Learning, 32(1):41?62.Stefan L. Frank.
2009.
Surprisal-based comparison be-tween a symbolic and a connectionist model of sen-tence processing.
In Proc.
Annual Meeting of theCognitive Science Society, pages 1139?1144.Edward Gibson.
1998.
Linguistic complexity: Local-ity of syntactic dependencies.
Cognition, 68(1):1?76.Edward Gibson.
2000.
The dependency locality the-ory: A distance-based theory of linguistic complex-ity.
In Image, language, brain: Papers from the firstmind articulation project symposium, pages 95?126.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the Sec-ond Meeting of the North American Chapter of theAssociation for Computational Linguistics, pages159?166, Pittsburgh, PA.John Hale.
2003.
Grammar, Uncertainty and SentenceProcessing.
Ph.D. thesis, Cognitive Science, TheJohns Hopkins University.John Hale.
2006.
Uncertainty about the rest of thesentence.
Cognitive Science, 30(4):609?642.Roger Levy.
2008.
Expectation-based syntactic com-prehension.
Cognition, 106(3):1126?1177.Scott A. McDonald and Richard C. Shillcock.
2003.Low-level predictive inference in reading: The influ-ence of transitional probabilities on eye movements.Vision Research, 43(16):1735?1751.George Miller and Noam Chomsky.
1963.
Finitarymodels of language users.
In R. Luce, R. Bush,and E. Galanter, editors, Handbook of MathematicalPsychology, volume 2, pages 419?491.
John Wiley.Kevin P. Murphy and Mark A. Paskin.
2001.
Lin-ear time inference in hierarchical HMMs.
In Proc.NIPS, pages 833?840, Vancouver, BC, Canada.Joakim Nivre.
2007.
Inductive dependency parsing.Computational Linguistics, 33(2).Alexander Pollatsek, Erik D. Reichle, and KeithRayner.
2006.
Tests of the EZ Reader model:Exploring the interface between cognition and eye-movement control.
Cognitive Psychology, 52(1):1?56.Lawrence R. Rabiner.
1990.
A tutorial on hid-den Markov models and selected applications inspeech recognition.
Readings in speech recognition,53(3):267?296.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical andsyntactic expectation-based measures for psycholin-guistic modeling via incremental top-down parsing.Proceedings of the 2009 Conference on EmpiricalMethods in Natural Langauge Processing, pages324?333.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.William Schuler, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2008.
Toward apsycholinguistically-motivated model of language.In Proceedings of COLING, pages 785?792,Manchester, UK, August.William Schuler, Samir AbdelRahman, TimMiller, andLane Schwartz.
2010.
Broad-coverage incremen-tal parsing using human-like memory constraints.Computational Linguistics, 36(1).William Schuler.
2009.
Parsing with a boundedstack using a model-based right-corner transform.In Proceedings of the North American Associationfor Computational Linguistics (NAACL ?09), pages344?352, Boulder, Colorado.Michael K. Tanenhaus, Michael J. Spivey-Knowlton,Kathy M. Eberhard, and Julie E. Sedivy.
1995.
In-tegration of visual and linguistic information in spo-ken language comprehension.
Science, 268:1632?1634.1198
