Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728?736,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPCombining Lexical Semantic Resourceswith Question & Answer Archivesfor Translation-Based Answer FindingDelphine Bernhard and Iryna GurevychUbiquitous Knowledge Processing (UKP) LabComputer Science DepartmentTechnische Universita?t Darmstadt, Hochschulstra?e 10D-64289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.de/AbstractMonolingual translation probabilities haverecently been introduced in retrieval mod-els to solve the lexical gap problem.They can be obtained by training statisti-cal translation models on parallel mono-lingual corpora, such as question-answerpairs, where answers act as the ?source?language and questions as the ?target?language.
In this paper, we proposeto use as a parallel training dataset thedefinitions and glosses provided for thesame term by different lexical semantic re-sources.
We compare monolingual trans-lation models built from lexical semanticresources with two other kinds of datasets:manually-tagged question reformulationsand question-answer pairs.
We also showthat the monolingual translation probabil-ities obtained (i) are comparable to tradi-tional semantic relatedness measures and(ii) significantly improve the results overthe query likelihood and the vector-spacemodel for answer finding.1 IntroductionThe lexical gap (or lexical chasm) often observedbetween queries and documents or questions andanswers is a pervasive problem both in Informa-tion Retrieval (IR) and Question Answering (QA).This problem arises from alternative ways of con-veying the same information, due to synonymyor paraphrasing, and is especially severe for re-trieval over shorter documents, such as sentenceretrieval or question retrieval in Question & An-swer archives.
Several solutions to this problemhave been proposed including query expansion(Riezler et al, 2007; Fang, 2008), query refor-mulation or paraphrasing (Hermjakob et al, 2002;Tomuro, 2003; Zukerman and Raskutti, 2002)and semantic information retrieval (Mu?ller et al,2007).Berger and Lafferty (1999) have formulated afurther solution to the lexical gap problem con-sisting in integrating monolingual statistical trans-lation models in the retrieval process.
Monolin-gual translation models encode statistical word as-sociations which are trained on parallel monolin-gual corpora.
The major drawback of this ap-proach lies in the limited availability of truly par-allel monolingual corpora.
In practice, trainingdata for translation-based retrieval often consist inquestion-answer pairs, usually extracted from theevaluation corpus itself (Riezler et al, 2007; Xueet al, 2008; Lee et al, 2008).
While collection-specific translation models effectively encode sta-tistical word associations for the target documentcollection, it also introduces a bias in the evalua-tion and makes it difficult to assess the quality ofthe translation model per se, independently from aspecific task and document collection.In this paper, we propose new kinds ofdatasets for training domain-independent mono-lingual translation models.
We use the defini-tions and glosses provided for the same termby different lexical semantic resources to auto-matically train the translation models.
This ap-proach has been very recently made possible bythe emergence of new kinds of lexical seman-tic and encyclopedic resources such as Wikipediaand Wiktionary.
These resources are freely avail-able, up-to-date and have a broad coverage andgood quality.
Thanks to the combination of sev-eral resources, it is possible to obtain monolin-gual parallel corpora which are large enough totrain domain-independent translation models.
Inaddition, we collected question-answer pairs andmanually-tagged question reformulations from asocial Q&A site.
We use these datasets to buildfurther translation models.Translation-based retrieval models have been728widely used in practice by the IR and QA commu-nity.
However, the quality of the semantic infor-mation encoded in the translation tables has neverbeen assessed intrinsically.
To do so, we com-pare translation probabilities with concept vectorbased semantic relatedness measures with respectto human relatedness rankings for reference wordpairs.
This study provides empirical evidence forthe high quality of the semantic information en-coded in statistical word translation tables.
Wethen use the translation models in an answer find-ing task based on a new question-answer datasetwhich is totally independent from the resourcesused for training the translation models.
This ex-trinsic evaluation shows that our translation mod-els significantly improve the results over the querylikelihood and the vector-space model.The remainder of the paper is organised as fol-lows.
Section 2 discusses related work on seman-tic relatedness and statistical translation modelsfor retrieval.
Section 3 presents the monolingualparallel datasets we used for obtaining monolin-gual translation probabilities.
Semantic related-ness experiments are detailed in Section 4.
Section5 presents answer finding experiments.
Finally, weconclude in Section 6.2 Related Work2.1 Statistical Translation Models forRetrievalStatistical translation models for retrieval havefirst been introduced by Berger and Lafferty(1999).
These models attempt to address syn-onymy and polysemy problems by encoding sta-tistical word associations trained on monolingualparallel corpora.
This method offers several ad-vantages.
First, it bases upon a sound mathe-matical formulation of the retrieval model.
Sec-ond, it is not as computationally expensive asother semantic retrieval models, since it only re-lies on a word translation table which can easilybe computed before retrieval.
The main draw-back lies in the availability of suitable training datafor the translation probabilities.
Berger and Laf-ferty (1999) initially built synthetic training dataconsisting of queries automatically generated fromdocuments.
Berger et al (2000) proposed to traintranslation models on question-answer pairs takenfrom Usenet FAQs and call-center dialogues, withanswers corresponding to the ?source?
languageand questions to the ?target?
language.Subsequent work in this area often used simi-lar kinds of training data such as question-answerpairs from Yahoo!
Answers (Lee et al, 2008) orfrom the Wondir site (Xue et al, 2008).
Lee etal.
(2008) tried to further improve translation mod-els based on question-answer pairs by selecting themost important terms to build compact translationmodels.Other kinds of training data have also been pro-posed.
Jeon et al (2005) automatically clusteredsemantically similar questions based on their an-swers.
Murdock and Croft (2005) created a firstparallel corpus of synonym pairs extracted fromWordNet, and an additional parallel corpus of En-glish words translating to the same Arabic term ina parallel English-Arabic corpus.Similar work has also been performed in thearea of query expansion using training data con-sisting of FAQ pages (Riezler et al, 2007) orqueries and clicked snippets from query logs (Rie-zler et al, 2008).All in all, translation models have been shownto significantly improve the retrieval resultsover traditional baselines for document retrieval(Berger and Lafferty, 1999), question retrieval inQuestion & Answer archives (Jeon et al, 2005;Lee et al, 2008; Xue et al, 2008) and for sentenceretrieval (Murdock and Croft, 2005).Many of the approaches previously describedhave used parallel data extracted from the retrievalcorpus itself.
The translation models obtained aretherefore domain and collection-specific, whichintroduces a bias in the evaluation and makesit difficult to assess to what extent the transla-tion model may be re-used for other tasks anddocument collections.
We henceforth propose anew approach for building monolingual transla-tion models relying on domain-independent lexi-cal semantic resources.
Moreover, we extensivelycompare the results obtained by these models withmodels obtained from a different type of dataset,namely Question & Answer archives.2.2 Semantic RelatednessThe rationale behind translation-based retrievalmodels is that monolingual translation probabil-ities encode some form of semantic knowledge.The semantic similarity and relatedness of wordshas traditionally been assessed through corpus-based and knowledge-based measures.
Corpus-based measures include Hyperspace Analogue to729Language (HAL) (Lund and Burgess, 1996) andLatent Semantic Analysis (LSA) (Landauer et al,1998).
Knowledge-based measures rely on lexicalsemantic resources such as WordNet and comprisepath length based measures (Rada et al, 1989)and concept vector based measures (Qiu and Frei,1993).
These measures have recently also been ap-plied to new collaboratively constructed resourcessuch as Wikipedia (Zesch et al, 2007) and Wik-tionary (Zesch et al, 2008), with good results.While classical measures of semantic related-ness have been extensively studied and compared,based on comparisons with human relatednessjudgements or word-choice problems, there is nocomparable intrinsic study of the relatedness mea-sures obtained through word translation probabil-ities.
In this study, we use the correlation withhuman rankings for reference word pairs to inves-tigate how word translation probabilities comparewith traditional semantic relatedness measures.
Toour knowledge, this is the first time that word-to-word translation probabilities are used for rankingword-pairs with respect to their semantic related-ness.3 Parallel DatasetsIn order to obtain parallel training data for thetranslation models, we collected three differentdatasets: manually-tagged question reformula-tions and question-answer pairs from the WikiAn-swers social Q&A site (Section 3.1), and glossesfrom WordNet, Wiktionary, Wikipedia and SimpleWikipedia (Section 3.2).3.1 Social Q&A SitesSocial Q&A sites, such as Yahoo!
Answers andAnswerBag, provide portals where users can asktheir own questions as well as answer questionsfrom other users.For our experiments we collected a dataset ofquestions and answers, as well as question refor-mulations, from the WikiAnswers1 (WA) web site.WikiAnswers is a social Q&A site similar to Ya-hoo!
Answers and AnswerBag.
The main orig-inality of WikiAnswers is that users might manu-ally tag question reformulations in order to preventthe duplication of answers to questions asking thesame thing in a different way.
When a user entersa question that is not already part of the questionrepository, the web site displays a list of already1http://wiki.answers.com/existing questions similar to the one just asked bythe user.
The user may then freely select the ques-tion which paraphrases her question, if available.The question reformulations thus labelled by theusers are stored in order to retrieve the same an-swer when a given question reformulation is askedagain.We collected question-answer pairs and ques-tion reformulations from the WikiAnswers site.The resulting dataset contains 480,190 questionswith answers.2 We use this dataset in order to traintwo different translation models:Question-Answer Pairs (WAQA) In this set-ting, question-answer pairs are considered as aparallel corpus.
Two different forms of combi-nations are possible: (Q,A), where questions actas source and answers as target, and (A,Q), whereanswers act as source and questions as target.
Re-cent work by Xue et al (2008) has shown that thebest results are obtained by pooling the question-answer pairs {(q, a)1, ..., (q, a)n} and the answer-question pairs {(a, q)1, ..., (a, q)n} for training,so that we obtain the following parallel corpus:{(q, a)1, ..., (q, a)n}?
{(a, q)1, ..., (a, q)n}.
Over-all, this corpus contains 1,227,362 parallel pairsand will be referred to as WAQA (WikiAnswersQuestion-Answers) in the rest of the paper.Question Reformulations (WAQ) In this set-ting, question and question reformulation pairsare considered as a parallel corpus, e.g.
?Howlong do polar bears live??
and ?What isthe polar bear lifespan??.
For a givenuser question q1, we retrieve its stored re-formulations from the WikiAnswers dataset;q11, q12, ....
The original question and reformu-lations are subsequently combined and pooled toobtain a parallel corpus of question reformula-tion pairs: {(q1, q11), (q1, q12), ..., (qn, qnm)} ?
{(q11, q1), (q12, q1), ..., (qnm, qn)}.
This corpuscontains 4,379,620 parallel pairs and will be re-ferred to as WAQ (WikiAnswers Questions) in therest of the paper.3.2 Lexical Semantic ResourcesGlosses and definitions for the same lexeme in dif-ferent lexical semantic and encyclopedic resourcescan actually be considered as near-paraphrases,since they define the same terms and hence have2A question may have more than one answer.730gem moonWAQ WAQA LSR ALLPool WAQ WAQA LSR ALLPoolgem explorer gem gem moon moon moon moon95 ford diamonds xlt land earth lunar landxlt gem gemstone 95 foot lunar sun earthmodule xlt diamond explorer armstrong apollo earth landedstones demand natural gemstone set landed tides armstrongexpedition lists facets diamonds actually neil moons neilring dash rare natural neil 1969 phase apollogemstone center synthetic diamond landed armstrong crescent setmodual play ruby ford apollo space astronomical footcrystal lights usage ruby walked surface occurs actuallyTable 1: Sample top translations for different training data.
ALL corresponds to WAQ+WAQA+LSR.the same meaning, as shown by the following ex-ample for the lexeme ?moon?:?
Wordnet (sense 1): the natural satellite of theEarth.?
English Wiktionary: The Moon, the satelliteof planet Earth.?
English Wikipedia: The Moon (Latin: Luna)is Earth?s only natural satellite and the fifthlargest natural satellite in the Solar System.We use glosses and definitions contained in thefollowing resources to build a parallel corpus:?
WordNet (Fellbaum, 1998).
We use a freelyavailable API for WordNet (JWNL3) to ac-cess WordNet 3.0.?
English Wiktionary.
We use the Wiktionarydump from January 11, 2009.?
English and Simple English Wikipedia.
Weuse the Wikipedia dump from February6, 2007 and the Simple Wikipedia dumpfrom July 24, 2008.
The Simple EnglishWikipedia is an English Wikipedia targetedat non-native speakers of English which usessimpler words than the English Wikipedia.Wikipedia and Simple Wikipedia articles donot directly correspond to glosses such asthose found in dictionaries, we therefore con-sidered the first paragraph in articles as a sur-rogate for glosses.Given a list of 86,584 seed lexemes extractedfrom WordNet, we collected the glosses for eachlexeme from the four English resources described3http://sourceforge.net/projects/jwordnet/above.
We then built pairs of glosses by consid-ering each possible pair of resource.
Given that alexeme might have different senses, and hence dif-ferent glosses, it is possible to extract several glosspairs for one and the same lexeme and one and thesame pair of resources.
It is therefore necessary toperform word sense alignment.
As we do not needperfect training data, but rather large amounts oftraining data, we used a very simple method con-sisting in eliminating gloss pairs which did not atleast have one lemma in common (excluding stopwords and the seed lexeme itself).The final pooled parallel corpus contains307,136 pairs and is henceforth much smallerthan the previous datasets extracted from WikiAn-swers.
This corpus will be referred to as LSR.3.3 Translation Model TrainingWe used the GIZA++ SMT Toolkit4 (Och andNey, 2003) in order to obtain word-to-wordtranslation probabilities from the parallel datasetsdescribed above.
As is common practice intranslation-based retrieval, we utilised the IBMtranslation model 1.
The only pre-processing stepsperformed for all parallel datasets were tokenisa-tion and stop word removal.53.4 Comparison of Word-to-WordTranslationsTable 1 gives some examples of word-to-wordtranslations obtained for the different parallel cor-pora used (the column ALLPool will be describedin the next section).
As evidenced by this table,4http://code.google.com/p/giza-pp/5For stop word removal we used the list avail-able at: http://truereader.com/manuals/onix/stopwords1.html.731the different kinds of data encode different typesof information, including semantic relatedness andsimilarity, as well as morphological relatedness.As could be expected, the quality of the ?trans-lations?
is variable and heavily dependent on thetraining data: the WAQ and WAQA models revealthe users?
interests, while the LSR model encodeslexicographic and encyclopedic knowledge.
Forinstance, ?gem?
is an acronym for ?generic elec-tronic module?, which is found in Ford vehicles.Since many question-answer pairs in WA are re-lated to cars, this very particular use of ?gem?
ispredominant in the WAQ and WAQA translationtables.3.5 Combination of the DatasetsIn order to investigate the role played by differ-ent kinds of training data, we combined the sev-eral translation models, using the two methods de-scribed by Xue et al (2008).
The first method con-sists in a linear combination of the word-to-wordtranslation probabilities after training:PLin(wi|wj) = ?PWAQ(wi|wj)+ ?PWAQA(wi|wj)+ ?PLSR(wi|wj) (1)where ?
+ ?
+ ?
= 1.
This approach will belabelled with the Lin subscript.The second method consists in pooling thetraining datasets, i.e.
concatenating the parallelcorpora, before training.
This approach will belabelled with the Pool subscript.
Examples forword-to-word translations obtained with this typeof combination can be found in the last column foreach word in Table 1.
The ALLPool setting corre-sponds to the pooling of all three parallel datasets:WAQ+WAQA+LSR.4 Semantic Relatedness ExperimentsThe aim of this first experiment is to perform anintrinsic evaluation of the word translation proba-bilities obtained by comparing them to traditionalsemantic relatedness measures on the task of rank-ing word pairs.
Human judgements of semantic re-latedness can be used to evaluate how well seman-tic relatedness measures reflect human rankings bycorrelating their ranking results with Spearman?srank correlation coefficient.
Several evaluationdatasets are available for English, but we restrictour study to the larger dataset created by Finkel-stein et al (2002) due to the low coverage of manypairs in the word-to-word translation tables.
Thisdataset comprises two subsets, which have beenannotated by different annotators: Fin1?153, con-taining 153 word pairs, and Fin2?200, containing200 word pairs.Word-to-word translation probabilities are com-pared with a concept vector based measure relyingon Explicit Semantic Analysis (Gabrilovich andMarkovitch, 2007), since this approach has beenshown to yield very good results (Zesch et al,2008).
The method consists in representing wordsas a concept vector, where concepts correspond toWordNet synsets, Wikipedia article titles or Wik-tionary entry names.
Concept vectors for eachword are derived from the textual representationavailable for each concept, i.e.
glosses in Word-Net, the full article or the first paragraph of thearticle in Wikipedia or the full contents of a Wik-tionary entry.
We refer the reader to (Gabrilovichand Markovitch, 2007; Zesch et al, 2008) for tech-nical details on how the concept vectors are builtand used to obtain semantic relatedness values.Table 2 lists Spearman?s rank correlation coeffi-cients obtained for concept vector based measuresand translation probabilities.
In order to ensurea fair evaluation, we limit the comparison to theword pairs which are contained in all resourcesand translation tables.Dataset Fin1-153 Fin2-200Word pairs used 46 42Concept vectorsWordNet .26 .46Wikipedia .27 .03WikipediaFirst .30 .38Wiktionary .39 .58Translation probabilitiesWAQ .43 .65WAQA .54 .37LSR .51 .29ALLPool .52 .57Table 2: Spearman?s rank correlation coefficientson the Fin1-153 and Fin2-200 datasets.
Best val-ues for each dataset are in bold format.
ForWikipediaFirst, the concept vectors are based onthe first paragraph of each article.The first observation is that the coverage overthe two evaluation datasets is rather small: only 46pairs have been evaluated for the Fin1-153 datasetand 42 for the Fin2-200 dataset.
This is mainly732due to the natural absence of many word pairs inthe translation tables.
Indeed, translation proba-bilities can only be obtained from observed paral-lel pairs in the training data.
Concept vector basedmeasures are more flexible in that respect since therelatedness value is based on a common represen-tation in a concept vector space.
It is thereforepossible to measure relatedness for a far greaternumber of word pairs, as long as they share someconcept vector dimensions.
The second observa-tion is that, on the restricted subset of word pairsconsidered, the results obtained by word-to-wordtranslation probabilities are most of the time betterthan those of concept vector measures.
However,the differences are not statistically significant.65 Answer Finding Experiments5.1 Retrieval based on Translation ModelsThe second experiment aims at providing an ex-trinsic evaluation of the translation probabilitiesby employing them in an answer finding task.In order to perform retrieval, we use a rank-ing function similar to the one proposed by Xueet al (2008), which builds upon previous workon translation-based retrieval models and tries toovercome some of their flaws:P (q|D) =?w?qP (w|D) (2)P (w|D) = (1?
?
)Pmx(w|D) + ?P (w|C) (3)Pmx(w|D) = (1?
?
)Pml(w|D) +?
?t?DP (w|t)Pml(t|D) (4)where q is the query, D the document, ?
thesmoothing parameter for the document collectionC and P (w|t) is the probability of translating adocument term t to the query term w.The only difference to the original model byXue et al (2008) is that we use Jelinek-Mercersmoothing for equation 3 instead of DirichletSmoothing, as it has been done by Jeon et al(2005).
In all our experiments, ?
was set to 0.8and ?
to 0.5.5.2 The Microsoft Research QA CorpusWe performed an extrinsic evaluation of mono-lingual word translation probabilities by integrat-ing them in the retrieval model previously de-scribed for an answer finding task.
To this aim,6Fisher-Z transformation, two-tailed test with ?=.05.we used the questions and answers contained inthe Microsoft Research Question Answering Cor-pus.7 This corpus comprises approximately 1.4Kquestions collected from 10-13 year old school-children, who were asked ?If you could talk to anencyclopedia, what would you ask it??.
The an-swers to the questions have been manually identi-fied in the full text of Encarta 98 and annotatedwith the following relevance judgements: exactanswer (1), off topic (3), on topic - off target (4),partial answer (5).
In order to use this dataset foran answer finding task, we consider the annotatedanswers as the documents to be retrieved and usethe questions as the set of test queries.This corpus is particularly well suited to con-duct experiments targeted at the lexical gap prob-lem: only 28% of the question-answer pairs corre-spond to a strong match (two or more query termsin the same answer sentence), while about a half(52%) are a weak match (only one query termmatched in the answer sentence) and 16 % are in-direct answers which do not explicitly contain theanswer but provide enough information for deduc-ing it.
Moreover, the Microsoft QA corpus is notlimited to a specific topic and entirely indepen-dent from the datasets used to build our translationmodels.The original corpus contained some inconsis-tencies due to duplicated data and non-labelledentries.
After cleaning, we obtained a corpus of1,364 questions and 9,780 answers.
Table 3 givesone example of a question with different answersand relevance judgements.We report the retrieval performance in termsof Mean Average Precision (MAP) and Mean R-Precision (R-prec), MAP being our primary evalu-ation metric.
We consider the following relevancecategories, corresponding to increasing levels oftolerance for inexact or partial answers:?
MAP1, R-Prec1: exact answer (1)?
MAP1,5, R-Prec1,5: exact answer (1) or par-tial answer (5)?
MAP1,4,5, R-Prec1,4,5: exact answer (1) orpartial answer (5) or on topic - off target (4)Similarly to the training data for translationmodels, the only pre-processing steps performed7http://research.microsoft.com/en-us/downloads/88c0021c-328a-4148-a158-a42d7331c6cf/default.aspx733Question Why is the sun bright?Exact answer Star, large celestial body composed of gravitationally contained hot gasesemitting electromagnetic radiation, especially light, as a result of nuclearreactions inside the star.
The sun is a star.Partial answer Solar Energy, radiant energy produced in the sun as a result of nuclear fu-sion reactions (see Nuclear Energy; Sun).On topic - off target The sun has a magnitude of -26.7, inasmuch as it is about 10 billion timesas bright as Sirius in the earth?s sky.Table 3: Example relevance judgements in the Microsoft QA corpus.Model MAP1 R-Prec1 MAP1,5 R-Prec1,5 MAP1,4,5 R-Prec1,4,5QLM 0.2679 0.1941 0.3179 0.2963 0.3215 0.3057Lucene 0.2705 0.2002 0.3167 0.2956 0.3192 0.3030WAQ 0.3002 0.2149* 0.3557 0.3269 0.3583 0.3375WAQA 0.3000 0.2211 0.3640 0.3328 0.3664 0.3405LSR 0.3046 0.2171* 0.3666 0.3327 0.3723 0.3464WAQ+WAQAPool 0.3062 0.2259 0.3685 0.3339 0.3716 0.3454WAQ+LSRPool 0.3117 0.2224 0.3736 0.3399 0.3766 0.3487WAQA+LSRPool 0.3135 0.2267 0.3818 0.3444 0.3840 0.3515WAQ+WAQA+LSRPool 0.3152 0.2286 0.3832 0.3495 0.3848 0.3569WAQ+WAQA+LSRLin 0.3215 0.2343 0.3921 0.3536 0.3967 0.3673Table 4: Answer retrieval results.
The WAQ+WAQA+LSRLin results have been obtained with ?=0.2?=0.2 and ?=0.6 (the parameter values have been determined empirically based on MAP and R-Prec).The performance gaps between the translation-based models and the baseline models are statisticallysignificant, except for those marked with a ?*?
(two-tailed paired t-test, p < 0.05).for this corpus were tokenisation and stop wordremoval.
Due to the small size of the answercorpus, we built an open vocabulary backgroundcollection model to deal with out of vocabularywords by smoothing the unigram probabilitieswith Good-Turing discounting, using the SRILMtoolkit8 (Stolcke, 2002).5.3 ResultsAs baselines, we consider the query-likelihoodmodel (QLM), corresponding to equation 4 with?
= 0, and Lucene.9The results reported in Table 4 show that modelsincorporating monolingual translation probabili-ties perform consistently better than both baselinesystems especially when they are used in combi-nation.
It is however difficult to provide a rankingof the different types of training data based on theretrieval results: it seems that LSR is slightly moreperformant than WAQ and WAQA, both alone and8http://www.speech.sri.com/projects/srilm/9http://lucene.apache.orgin combination, but the improvement is minor.
Itis worth noticing that while the LSR training dataare comparatively smaller than WAQ and WAQA,they however yield comparable results.
The linearcombination of datasets (WAQ+WAQA+LSRLin)yields statistically significant performance im-provement when compared to the models withoutcombinations (except when compared to WAQAfor R-Prec1, p>0.05), which shows that the differ-ent datasets and resources used are complemen-tary and each contribute to the overall result.Three answer retrieval examples are given inFigure 1.
They provide further evidence forthe results obtained.
The correct answer to thefirst question ?Who invented Halloween??
isretrieved by the WAQ+WAQA+LSRLin model,but not by the QLM.
This is a case of a weakmatch with only ?Halloween?
as matching term.The WAQ+WAQA+LSRLin model is however ableto establish the connection between the ques-tion term ?invented?
and the answer term ?orig-inated?.
Questions 2 and 3 show that transla-tion probabilities can also replace word normali-734QLM top answer WAQ+WAQA+LSRLin top answerQuestion 1: Who invented Halloween?Halloween occurs on October 31 and is observedin the U.S. and other countries with masquerad-ing, bonfires, and games.The observances connected with Halloween arethought to have originated among the ancientDruids, who believed that on that evening,Saman, the lord of the dead, called forth hostsof evil spirits.Question 2: Can mosquito bites spread AIDS?Another species, the Asian tiger mosquito, hascaused health experts concern since it was firstdetected in the United States in 1985.
Proba-bly arriving in shipments of used tire casings,this fierce biter can spread a type of encephalitis,dengue fever, and other diseases.Studies have shown no evidence of HIV trans-mission through insects ?
even in areas wherethere are many cases of AIDS and large popu-lations of insects such as mosquitoes.Question 3: How do the mountains form into a shape?In 1985, scientists vaporized graphite to producea stable form of carbon molecule consisting of60 carbon atoms in a roughly spherical shape,looking like a soccer ball.Geologists believe that most mountains areformed by movements in the earth?s crust.Figure 1: Top answer retrieved by QLM and WAQ+WAQA+LSRLin.
Lexical overlaps between questionand answer are in bold, morphological relations are in italics.sation techniques such as stemming and lemmati-sation, since the answers do not contain the ques-tion terms ?mosquito?
(for question 2) and ?form?
(for question 3), but only their inflected forms?mosquitoes?
and ?formed?.6 Conclusion and Future WorkWe have presented three datasets for training sta-tistical word translation models for use in answerfinding: question-answer pairs, manually-taggedquestion reformulations and glosses for the sameterm extracted from several lexical semantic re-sources.
It is the first time that the two latter typesof datasets have been used for this task.
We havealso provided the first intrinsic evaluation of wordtranslation probabilities with respect to human re-latedness rankings for reference word pairs.
Thisevaluation has shown that, despite the simplicityof the method, monolingual translation models arecomparable to concept vector semantic relatednessmeasures for this task.
Moreover, models based ontranslation probabilities yield significant improve-ment over baseline approaches for answer finding,especially when different types of training data arecombined.
The experiments bear strong evidencethat several datasets encode different and comple-mentary types of knowledge, which are all use-ful for retrieval.
In order to integrate semanticsin retrieval, it is therefore advisable to combineboth knowledge specific to the task at hand, e.g.question-answer pairs, and external knowledge, ascontained in lexical semantic resources.In the future, we would like to further evalu-ate the models presented in this paper for differenttasks, such as question paraphrase retrieval, andlarger datasets.
We also plan to improve ques-tion analysis by automatically identifying questiontopic and question focus.Acknowledgments We thank KonstantinaGaroufi, Nada Mimouni, Christof Mu?ller andTorsten Zesch for contributions to this work.We also thank Mark-Christoph Mu?ller and theanonymous reviewers for insightful comments.We are grateful to Bill Dolan for making usaware of the Microsoft Research QA Corpus.This work has been supported by the GermanResearch Foundation (DFG) under the grant No.GU 798/3-1, and by the Volkswagen Foundationas part of the Lichtenberg-Professorship Programunder the grant No.
I/82806.ReferencesAdam Berger and John Lafferty.
1999.
InformationRetrieval as Statistical Translation.
In Proceedingsof the 22nd Annual International Conference on Re-735search and Development in Information Retrieval(SIGIR ?99), pages 222?229.Adam Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal.
2000.
Bridging the LexicalChasm: Statistical Approaches to Answer-Finding.In Proceedings of the 23rd Annual InternationalConference on Research and Development in Infor-mation Retrieval (SIGIR ?00), pages 192?199.Hui Fang.
2008.
A Re-examination of Query Expan-sion Using Lexical Resources.
In Proceedings ofACL-08: HLT, pages 139?147, Columbus, Ohio.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing Search in Context: theConcept Revisited.
ACM Transactions on Informa-tion Systems (TOIS), 20(1):116?131, January.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence (IJCAI), pages 1606?1611.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural Language Based Reformu-lation Resource and Wide Exploitation for QuestionAnswering.
In Proceedings of the Eleventh Text Re-trieval Conference (TREC 2002).Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.2005.
Finding Similar Questions in Large Questionand Answer Archives.
In Proceedings of the 14thACM International Conference on Information andKnowledge Management (CIKM ?05), pages 84?90.Thomas K. Landauer, Darrell Laham, and Peter Foltz.1998.
Learning Human-like Knowledge by Singu-lar Value Decomposition: A Progress Report.
Ad-vances in Neural Information Processing Systems,10:45?51.Jung-Tae Lee, Sang-Bum Kim, Young-In Song, andHae-Chang Rim.
2008.
Bridging Lexical Gaps be-tween Queries and Questions on Large Online Q&ACollections with Compact Translation Models.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages410?418, Honolulu, Hawaii.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments & Computers, 28(2):203?208.Christof Mu?ller, Iryna Gurevych, and Max Mu?hlha?user.2007.
Integrating Semantic Knowledge into TextSimilarity and Information Retrieval.
In Proceed-ings of the First IEEE International Conference onSemantic Computing (ICSC), pages 257?264.Vanessa Murdock and W. Bruce Croft.
2005.
A Trans-lation Model for Sentence Retrieval.
In Proceedingsof the Conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing (HLT/EMNLP?05), pages 684?691.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Mod-els.
Computational Linguistics, 29(1):19?51.Yonggang Qiu and Hans-Peter Frei.
1993.
ConceptBased Query Expansion.
In Proceedings of the 16thAnnual International Conference on Research andDevelopment in Information Retrieval (SIGIR ?93),pages 160?169.Roy Rada, Hafedh Mili, Ellen Bicknell, and MariaBlettner.
1989.
Development and Application ofa Metric on Semantic Nets.
IEEE Transactions onSystems, Man and Cybernetics, 19(1):17?30.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical Machine Translation for Query Ex-pansion in Answer Retrieval.
In Proceedingsof the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL?
07), pages464?471.Stefan Riezler, Yi Liu, and Alexander Vasserman.2008.
Translating Queries into Snippets for Im-proved Query Expansion.
In Proceedings of the22nd International Conference on ComputationalLinguistics (COLING 2008), pages 737?744.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing (ICSLP), volume 2, pages 901?904.Noriko Tomuro.
2003.
Interrogative ReformulationPatterns and Acquisition of Question Paraphrases.In Proceedings of the International Workshop onParaphrasing, pages 33?40.Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.2008.
Retrieval Models for Question and AnswerArchives.
In Proceedings of the 31st Annual Inter-national Conference on Research and Developmentin Information Retrieval (SIGIR ?08), pages 475?482.Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.2007.
Analyzing and Accessing Wikipedia as a Lex-ical Semantic Resource.
In Data Structures for Lin-guistic Resources and Applications, pages 197?205.Gunter Narr, Tu?bingen.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Using Wiktionary for Computing SemanticRelatedness.
In Proceedings of the Twenty-ThirdAAAI Conference on Artificial Intelligence (AAAI2008), pages 861?867.Ingrid Zukerman and Bhavani Raskutti.
2002.
Lex-ical Query Paraphrasing for Document Retrieval.In Proceedings of the 19th International Confer-ence on Computational linguistics, pages 1177?1183, Taipei, Taiwan.736
