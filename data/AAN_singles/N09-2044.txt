Proceedings of NAACL HLT 2009: Short Papers, pages 173?176,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsClassifying Factored Genres with Part-of-Speech HistogramsS.
Feldman, M. Marin, J. Medero, and M. OstendorfDept.
of Electrical EngineeringUniversity of Washington, Seattle, Washington 98195{sergeyf,amarin,jmedero,ostendor}@u.washington.eduAbstractThis work addresses the problem of genreclassification of text and speech transcripts,with the goal of handling genres not seen intraining.
Two frameworks employing differ-ent statistics on word/POS histograms with aPCA transform are examined: a single modelfor each genre and a factored representationof genre.
The impact of the two frameworkson the classification of training-matched andnew genres is discussed.
Results show that thefactored models allow for a finer-grained rep-resentation of genre and can more accuratelycharacterize genres not seen in training.1 IntroductionWith increasing quantities of text and transcribedspeech available online, the ability to categorizedocuments based on characteristics beyond topic be-comes ever more important.
In particular, the genreof a document ?
whether it is a news report or aneditorial, a speech transcript or a weblog ?
may berelevant for many human tasks.
For example, onemight want to find ?speeches on ethanol?
or ?we-blog entries on Fannie Mae, sorted by most formalfirst.?
Genre classification is also of growing im-portance for human language technologies, such asspeech recognition, parsing, and translation, becauseof the potentially large differences in language as-sociated with genre.
Researchers find that genre-dependent models lead to improved performance onthese tasks, e.g.
(Wang, 2008).
Since text harvestedfrom the web is increasingly used to address prob-lems due to sparse training data, genre classifica-tion can be useful for sampling such text sources toobtain a better match to the target domain for of-fline language model training.
Prior work on genre-dependent web text filtering for language modelingrelied on standard search engine methods, design-ing queries based on frequent n-grams in the do-main, e.g.
(Bulyko et al, 2007).
However, as thevariety of genres online has grown, this method hasbecome less reliable.
This work addresses explicitgenre classification, with the assumption that genrerepresentation in the training data is incomplete.In prior work on genre classification, an impor-tant question has been the definition of ?genre.
?For many studies, genre has been associated withcategories of text, such as research article, novel,news report, editorial, advertisement, etc.
In par-ticular, several studies use classes identified in theBrown corpus or the British National Corpus.
Spo-ken genres, including conversation, interview, de-bate, and planned speech are considered in (Santini,2004).
Examples of spoken and written genres, rep-resented in several corpora available from the Lin-guistics Data Consortium, are explored in (Feldmanet al, 2009).
Yet another study focuses on internet-specific document types, including different types ofhome pages (personal, public, commercial), bulletinboards, and link lists (Lim et al, 2004).
A limita-tion of all of this work is that only a small, fixed setof different genres are explored, with performanceassessed on matched data.
In this paper, we assessclassification results of texts that come from newgenres, as well as those matching the training set.In addressing new genres, we have two maincontributions: new features and factored coding.173The standard features for genre classification mod-els include words, part-of-speech (POS) tags, andpunctuation (Kessler et al, 1997; Stamatatos etal., 2000; Lee and Myaeng, 2002; Biber, 1993),but constituent-based syntactic categories have alsobeen explored (Karlgren and Cutting, 1994).
(Feld-man et al, 2009) used mixed word and POS his-togram mean and variance as features for genre clas-sification.
In this work, we augment those his-togram statistics with higher-order ones, as well asadd new word features aimed at capturing onlinegenres.
Further, we propose a factored genre model,and demonstrate its effect on genre classification ofout-of-domain documents.2 Methods2.1 CorporaTo train our algorithm, we use eight different gen-res: broadcast news (bn, 671 docs), broadcast con-versations (bc, 698 docs), meetings (mt, 493 docs),newswire (nw, 471 docs), conversational telephonespeech (sb, 890 docs), weblogs (wl, 543 docs), Ama-zon reviews of books, videogames and films (aztrain, 218 docs), and chat data (chat, 187 docs).
Totest our algorithm, we add six additional genres:Amazon reviews of appliances (az test, 27 docs),Wikipedia entries (wiki, 254 docs), Wikipedia dis-cussion entries (wiki talk, 1792 docs), European Par-liament transcripts (europarl, 1423 docs), a web col-lection obtained from Google searches for commonconversational n-grams (web, 18540 docs), and tran-scribed McCain and Obama speeches (speeches, 20docs).
With the exception of the chat data, Ama-zon reviews, and a subset of the Europarl transcripts,the training corpora are from standard publisheddatasets.
The reviews, chat, Wikipedia, and webdata were all collected from websites and cleanedlocally.
The documents average 600-1000 words inlength, except for smaller corpora like Amazon re-views, whose documents average about 200 words.For training factored models, we assume that all thedocuments within a corpus share the same class.2.2 Features and ClassifierThe features used in (Feldman et al, 2009) were de-rived from a union of POS tags and a set of hand-picked, informative words.
A similar approach isused here, including a collapsed version of the Tree-bank POS tag set (Marcus et al, 1993), with addi-tions for specific words (e.g.
personal pronouns andfilled pause markers), compound punctuation (e.g.multiple exclamation marks), and a general emoti-con tag, resulting in a total of 41 tags.
Histogramsare computed for a sliding window of length w = 5over the tag sequence, and then statistics of eachhistogram bin are extracted.
In the previous work,mean and standard deviation were extracted from thehistogram bins.
To this, we add skewness and kurto-sis, which we will show are necessary for increaseddifferentiation of unseen genres.
For feature reduc-tion, we used Principal Components Analysis andretained all PC dimensions with variance above 1%of the maximum PC variance.Different approaches have been explored for com-putational modeling, including naive Bayes, lineardiscriminant modeling, and neural networks (San-tini, 2004; Kessler et al, 1997; Stamatatos et al,2000; Lee and Myaeng, 2002).
Since (Feldman etal., 2009) found that quadratic discriminant analysis(QDA) outperforms naive Bayes, we use it here withfull covariance matrices estimated by maximumlikelihood, and trained on the reduced-dimensionPOS histogram features.2.3 FactorsLinguistic research has tended to look at attributesof language rather than defining genre in terms oftask domains.
Since the number of task domains ap-pears to be growing with new uses of the internet, weconjecture that an attribute approach is more practi-cal for web-based text.
We introduce the notion of afactored model for genre.
The genre of each docu-ment can encoded as a vector of factors.
Given datalimits, the set of factors explored so far are:?
number of speakers/authors (1,2,3+),?
level of formality (low, medium, high),?
intended audience (personal, broadcast), and?
intent (inform, persuade).Assuming factor independence, we train four sepa-rate QDA classifiers, one per factor.
Using factorsincreases the richness of the space represented bythe training set, in that it is possible to identify gen-res with factor combinations not seen in training.1743 Experiments and Discussion3.1 Within-Domain ValidationAs a preliminary step, and to ensure that the additionof skewness and kurtosis, as well as extra syntacticfeatures, does not significantly impact the within-domain classification accuracy, we performed ex-periments with both the features in (Feldman et al,2009) and our expanded features.
For this, we splitthe training data 75/25 into training/test sets, and re-peated the random split 50 times.
We ran the experi-ments for both the original genre classification prob-lem and the individual factors.
We found that the ad-dition of new moments and features decreased per-formance by less than 1% on average.
We hypoth-esize that this small deterioration in performance islikely due to overtuning to the original training set.3.2 New Features with Unseen GenresTo assess the use of our new features (added punctu-ation and emoticons) and the higher-order moments,we classified the web data with different processingconfigurations.
In addition to the eight training gen-res, we introduced an ?undetermined?
genre or classfor documents with a uniform posterior probabilityacross all genres, which occurs when there is a largemismatch to all training genres.
The distribution oflabels is shown in Figure 1.
While we do not havehand-labeled categories for this data, we thought ithighly unlikely that the vast majority is bn, as pre-dicted by the models using only mean and variancemoments.To validate our hypothesis that the spread of la-bels was more appropriate for the data, we randomlyselected 100 documents and hand-labeled these us-ing the eight classes plus ?undetermined.?
The unde-termined class was used for new genres (play scripts,lectures, newsgroups, congressional records).
Wefound that it was difficult to annotate the data, sincemany samples had characteristics of more than onegenre; this finding motivates the factor representa-tion.
The main difference between the various fea-ture extraction configurations was in the detectionof the undetermined cases.
For the subset of un-determined documents that we labeled (34), nonewere detected using only 2 moments, but 35-40%were detected with the higher-order moments.
Ofthe false detections, roughly 25-30% were associ-ated with documents with characteristics of multipleclasses.
The effect of adding more detailed punctua-tion and emoticons to the tag set was not significant.It should be noted that the web collection wasbased on queries designed to extract BC-style text,yet only 3 of 100 hand-labeled samples were in thatcategory, none of which were accurately classified.Roughly 16 of the 100 documents are labeled as veryinformal and another 55 include some informal textor are moderately informal.
This finding, combinedwith the observation that many documents reflect amix of genres, suggests that a factored representa-tion of genre (with formality as one ?factor?)
maybe more useful than explicit modeling of genres.2, old 4, old 2, new 4, new00.20.40.60.81number of moments, old/new featuresbc bn mt nw sb wl az chat undeterminedFigure 1: Fraction of web data classified as each genre.3.3 Unseen Genre Factor ResultsWe trained a set of models for each factor and ob-tained posterior estimates for unseen classes.
Figure2 shows the class of out-of-domain documents forthe formality factor, using 3 categories of formal-ity: low (conversational, unprofessional), medium(casual but coherent), high (formal).
We have nothand-labeled individual documents in all of thesesets, but the resulting class proportions match ourintuition for these genres.
The Wikipedia data islabeled as highly formal, and most web data is la-beled as medium.
Examining the 100 hand-labeledweb documents, we find that adding the higher-ordermoments improves classifier accuracy from 23% to55%.
The effect of the added tag set features wasonce again not significant.Figure 3 shows the class of out-of-domain doc-uments for the factor indicating number of speak-ers/authors.
This factor appears difficult to detect.175We hypothesize that there is an unaccounted-for de-pendence on audience.
When there is a listener,speakers may use the term ?you,?
as in conversa-tions and internet chat.
An interesting observationis that the ten Obama speeches all appear to exhibitthis behavior.
McCain speeches, on the other hand,display some variation, and about a third are (cor-rectly) characterized as single speaker.az test wiki wiki talk europarl web speeches00.20.40.60.81low medium high undeterminedFigure 2: Test corpora classification, formality.az test wiki wiki talk europarl web speeches00.20.40.60.811 2 3+ undeterminedFigure 3: Test corpora classification, number of speakers.The audience factor results are very skewed to-wards broadcast data, but this matches our intuition,and the scarcity of data meant for private consump-tion, so they are not included.
However, furtherstudy is needed, since 3-dimensional projections ofthe training data suggest a Gaussian mixture (orother more complex model) may fit better.The intent factor results are also mixed.
Theclassifier labels most of the Wikipedia, europarl,web, and speeches data as ?report,?
and most re-views as ?persuade.?
While the ?report?
category fitsWikipedia, it is not clear that europarl should also beclassified as ?report,?
since parliamentary proceed-ings are notoriously argumentative.
With this factor,the noise inherent in using genre-level labels is sig-nificant.
It is not always clear what is reportage andwhat is persuasion, and we expect some genres (e.g.reviews) to be a mixture of both.4 SummaryWe have introduced new features that are more ro-bust for handling domains unseen in training, andpresented a factored genre framework that allows fora finer-grained representation of genre.
Many openquestions remain, including which other factors canor cannot be captured by our current feature set andclassifier, and whether noisy label learning methodscould address the problem of uncertainty in the la-bels for particular features and genres.ReferencesD.
Biber.
1993.
Using register-diversified corpora forgeneral language studies.
Computational Linguistics,19(2):219?242.I.
Bulyko et al 2007.
Web resources for language model-ing in conversational speech recognition.
ACM Trans.on Speech and Language Processing, 5(1):1?25.S.
Feldman et al 2009.
Part-of-speech histograms forgenre classification of text.
Proc.
ICASSP.W.
Wang.
2008.
Weakly supervised training for parsingmandarin broadcast transcripts.
Proc.
Interspeech.J.
Karlgren and D. Cutting.
1994.
Recognizing text gen-res with simple metrics using discriminant analysis.Proc.
Computational Linguistics, pages 1071?1075.B.
Kessler, G. Numberg, and H. Schu?tze.
1997.
Auto-matic detection of text genre.
ACL-35, pages 32?38.Y.-B.
Lee and S. H. Myaeng.
2002.
Text genre classifi-cation with genre-revealing and subject-revealing fea-tures.
ACM SIGIR, pages 145?150.C.
S. Lim, K. J. Lee, and G. C. Kim.
2004.
Automaticgenre detection of web documents.
IJCNLP.M.
P. Marcus et al 1993.
Building a large annotatedcorpus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.M.
Santini.
2004.
A shallow approach to syntactic fea-ture extraction for genre classification.
CLUK 7: UKspecial-interest group for computational linguistics.E.
Stamatatos, N. Fakotakis, and G. Kokkinakis.
2000.Text genre detection using common word frequencies.Proc.
Computational Linguistics, pages 808?814.176
