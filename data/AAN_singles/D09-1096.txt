Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919?928,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSegmenting Email Message Text into ZonesAndrew Lampert ??
?CSIRO ICT CentrePO Box 76Epping 1710, Australiaandrew.lampert@csiro.auRobert Dale ?rdale@ics.mq.edu.auCe?cile Paris ?
?Centre for Language TechnologyMacquarie UniversityNorth Ryde 2109, Australiacecile.paris@csiro.auAbstractIn the early days of email, widely-usedconventions for indicating quoted replycontent and email signatures made it easyto segment email messages into their func-tional parts.
Today, the explosion of dif-ferent email formats and styles, coupledwith the ad hoc ways in which people varythe structure and layout of their messages,means that simple techniques for identify-ing quoted replies that used to yield 95%accuracy now find less than 10% of suchcontent.
In this paper, we describe Zebra,an SVM-based system for segmenting thebody text of email messages into nine zonetypes based on graphic, orthographic andlexical cues.
Zebra performs this task withan accuracy of 87.01%; when the num-ber of zones is abstracted to two or threezone classes, this increases to 93.60% and91.53% respectively.1 IntroductionEmail message bodies consist of different func-tional parts such as email signatures, quoted re-ply content and advertising content.
We refer tothese as email zones.
Many language process-ing tools stand to benefit from better knowledgeof this message structure, facilitating focus on rel-evant content in specific parts of a message.
Inparticular, access to zone information would al-low email classification, summarisation and anal-ysis tools to separate or filter out ?noise?
and focuson the content in specific zones of a message thatare relevant to the application at hand.
Email con-tact mining tools such as that developed by Culottaet al (2004), for example, might access the emailsignature zone, while tools that attempt to iden-tify tasks or action items in email (e.g., (Bellottiet al, 2003; Corston-Oliver et al, 2004; Bennettand Carbonell, 2007; Lampert et al, 2007)) mightrestrict themselves to the sender-authored and for-warded content.
Despite previous work on thisproblem, there are no available tools that can re-liably extract or identify the different functionalzones of an email message.While there is no agreed standard set of emailzones, there are clearly different functional partswithin the body text of email messages.
For ex-ample, the content of an email disclaimer is func-tionally different from the sender-authored contentand from the quoted reply content automaticallyincluded from previous messages in the thread ofconversation.
Of course, there are different dis-tinctions that can be drawn between zones; inthis paper we explore several different categorisa-tions based on our proposed set of nine underlyingemail zones.Although we focus on content in the body ofemail messages, we recognise the presence of use-ful information in the semi-structured headers, andindeed make use of header information such assender and recipient names in segmenting the un-structured body text.Segmenting email messages into zones is achallenging task.
Accurate segmentation is ham-pered by the lack of standard syntax used by dif-ferent email clients to indicate different messageparts, and by the ad hoc ways in which people varythe structure and layout of their messages.
Whenreplying to a message, for example, it is often use-ful to include all or part of the original messagethat is being replied to.
Different email clients in-dicate quoted material in different ways.
By de-fault, some prefix every line of the quoted messagewith a character such as ?>?
or ?|?, while others in-dent the quoted content or insert the quoted mes-sage unmodified, prefixed by a message header.Sometimes the new content is above the quotedcontent (a style known as ?top-posting?
); in othercases, the new content may appear after the quoted919content (bottom-posting) or interleaved with thequoted content (inline replying).
Confounding theissue further is that users are able to configure theiremail client to suit their individual tastes, and canchange both the syntax of quoting and their quot-ing style (top, bottom or inline replying) on a per-message basis.To address these challenges, in this paper wedescribe Zebra, our email zone classification sys-tem.
First we describe how Zebra builds and im-proves on previous work in Section 2.
Section 3then presents our set of email zones, along withdetails of the email data we use for system train-ing and experiments.
In Section 4 we describe twoapproaches to zone classification, one that is line-based and one that is fragment-based.
The perfor-mance of Zebra across two, three and nine emailzone classification tasks is presented and analysedin Section 5.2 Related WorkSegmenting email messages into zones requiresboth text segmentation and text classification.
Themain focus of most work on text segmentationis topic-based segmentation of news text (e.g.,(Hearst, 1997; Beeferman et al, 1997)), but therehave been some previous attempts at identifyingfunctional zones in email messages.Chen et al (1999) looked at both linguistic andtwo-dimensional layout cues for extracting struc-tured content from email signature zones in emailmessages.
The focus of their work was on extract-ing information from already identified signatureblocks using a combination of two-dimensionalstructural analysis and one-dimensional grammat-ical constraints; the intended application domainwas as a component in a system for email text-to-speech rendering.
The authors claim that theirsystem can be modified to also identify signatureblocks within email messages, but their systemperforms this task with a recall of only 53%.
Noattempt is made to identify functional zones otherthan email signatures.Carvalho and Cohen?s (2004) Jangada systemattempted to identify email signatures within plaintext email messages and to extract email signa-tures and reply lines.
Unfortunately, the 20 News-groups corpus1 they worked with contains 15-year-old Usenet messages which are much morehomogeneous in their syntax than contemporary1http://people.csail.mit.edu/jrennie/20Newsgroups/email, particularly in terms of how quoted textfrom previous messages is indicated.
As a result,using a very simple metric (a line-initial ?>?
char-acter) to identify reply lines achieves more than95% accuracy.
In contrast, this same simple met-ric applied to the Enron email data we annotateddetects less than 10% of actual reply or forwardlines.Usenet messages are also markedly differentfrom contemporary email when it comes to emailsignatures.
Most Usenet clients produced mes-sages which conformed to RFC3676 (Gellens,2004), a standard that formalised a ?long-standingconvention in Usenet news .
.
.
of using two hy-phens -- as the separator line between the bodyand the signature of a message.?
Unfortunately,this convention has long since ceased to be ob-served in email messages.
Carvalho and Cohen?semail signature detection approach also benefitsgreatly from a simplifying assumption that signa-tures are found in the last 10 lines of an email mes-sage.
While this holds true for their Usenet mes-sage data, it is no longer the case for contemporaryemail.In attempting to use Carvalho and Cohen?s sys-tem to identify signature blocks and reply linesin our own work, we identified similar shortcom-ings to those noted by Estival et al (2007).
Inparticular, Jangada did not accurately identify for-warded or reply content in email data from theEnron email corpus.
We believe that the use ofolder Usenet-style messages to train Jangada is asignificant factor in the systematic errors the sys-tem makes in failing to identify quoted reply, for-warded and signature content in messages format-ted in the range of message formats and styles pop-ularised by Microsoft Outlook.
These errors area fundamental problem with Jangada, especiallysince Outlook is the most common client used tocompose messages in our annotated email collec-tion drawn from the Enron corpus.
More gen-erally, we note that Outlook is the most popularemail client in current use, with an estimated 350?400 million users worldwide,2 representing any-where up to 40% of all email users.3More recently, as part of their work on profiling2Xobni Co-founder Adam Smith and former Engi-neering VP Gabor Cselle have both published Outlookuser statistics.
See http://www.xobni.com/asmith/archives/66and http://gaborcselle.com/blog/2008/05/xobnis-journey-to-right-product.html.3http://www.campaignmonitor.com/stats/email-clients/920authors of email messages, Estival et al (2007)classified email bodies into five email zones.
Theirpaper does not provide results for five-zone classi-fication, but they report accuracy of 88.16% usinga CRF classifier to distinguish three zones: reply,author and signature.
We use their classificationscheme as the starting point for our own set ofemail zones.3 Email ZonesAs noted earlier, we refer to the different func-tional components of email messages as emailzones.
The zones we propose refine and extendthe five categories ?
Author Text, Signature, Ad-vertisement (automatically appended advertising),Quoted Text (extended quotations such as songlyrics or poems), and Reply Lines (including for-warded and reply text) ?
identified by Estival etal.
(2007).We consider that each line of text in the bodyof an email message belongs to one of nine morefine-grained email zones.
We intend our nineemail zones to be abstracted and adapted to suitdifferent tasks.
To illustrate, we present thezones below abstracted into three classes: sender-authored content, boilerplate content, and contentquoted from other conversations.
This is the zonepartition we use to generate the three-zone resultsreported in Section 5.
This categorisation is use-ful for problems such as finding action items inemail messages: such detection tools would lookin text from the sender-authored message zonesfor new action item information, and could alsolook in quoted conversation content to link newaction item information (such as reported comple-tions) to previous action item content.Our nine email zones can also be reduced to abinary scheme to distinguish text authored by thesender from text authored by others.
This distinc-tion is useful for problems such as author attribu-tion or profiling tasks.
In this two-class case, thesender-authored zones would be Author, Greeting,Signoff and Signature, while the other-authoredzones would be Reply, Forward, Disclaimer, Ad-vertising and Attachment.
This is the partitionof zones we use in our two-zone experiments re-ported in Section 5.3.1 Sender ZonesSender zones contain text written by the currentemail sender.
The Greeting and Signoff zones aresub-zones of the Author zone, usually appearingas the first and last items respectively in the Authorzone.
Thus, our proposed sender zones are:1.
Author: New content from the current emailsender.
This specifically excludes any textauthored by the sender that is included fromprevious messages.2.
Greeting: Terms of address and recipientnames at the beginning of a message (e.g.,Dear/Hi/Hey Noam).3.
Signoff: The message closing (e.g.,Thanks/Cheers/Regards, John).3.2 Quoted Conversation ZonesQuoted conversation zones include both contentquoted in reply to previous messages in the sameconversation thread and forwarded content fromother conversations.4 Our quoted conversationzones are:4.
Reply: Content quoted from a previous mes-sage in the same conversation thread, includ-ing any embedded signatures, attachments,advertising, disclaimers, author content andforwarded content.
Content in a reply contentzone may include previously sent content au-thored by the current sender.5.
Forward: Content from an email messageoutside the current conversation thread thathas been forwarded by the current emailsender, including any embedded signatures,attachments, advertising, disclaimers, authorcontent and reply content.3.3 Boilerplate ZonesBoilerplate zones contain content that is reusedwithout modification across multiple email mes-sages.
Our proposed boilerplate zones are:6.
Signature: Content containing contact orother information that is automatically in-serted in a message.
In contrast to disclaimeror advertising content, signature content isusually templated content written once bythe email author, and automatically or semi-automatically included in email messages.
A4Although we recognise the need for the Quoted Text zoneproposed by Estival et al (2007), no such data occurs in ourcollection of annotated email messages.
We therefore omitthis zone from our current set.921user may also use a Signature in place of aSignoff; in such cases, we still mark the textas a Signature.7.
Advertising: Advertising material in anemail message.
Such material often appearsat the end of a message (e.g., Do you Ya-hoo!?
), but may also appear prefixed or in-line with the content of the message, (e.g., insponsored mailing lists).8.
Disclaimer: Legal disclaimers and privacystatements, often automatically appended.9.
Attachment: Automated text indicating orreferring to attached documents, such as thatshown in line 16 of Figure 1.
Note that thiszone does not apply to manually authored ref-erence to attachments, nor to the actual con-tent of attachments (which we do not clas-sify).3.4 Email Data and AnnotationThe training data for our zone classifier consists of11881 annotated lines from almost 400 email mes-sages drawn at random from the Enron email cor-pus (Klimt and Yang, 2004).5 We use the databasedump of the corpus released by Andrew Fiore andJeff Heer.6 This version of the corpus has beenprocessed to remove duplicate messages and tonormalise sender and recipient names, resulting injust over 250,000 email messages.
No attachmentsare included.
Following Estival et al (2007), weused only a single annotator since the task revealeditself to be relatively uncontroversial.
Each line inthe body text of selected messages was marked bythe annotator (one of the authors) as belonging toone of the nine zones.
After removing blank lines,which we do not attempt to classify, we are leftwith 7922 annotated lines as training data for Ze-bra.
The frequency of each zone within this anno-tated dataset is shown in Table 3.Figure 1 shows an example of an email mes-sage with each line annotated with the appropriateemail zone.
Two zone annotations are shown foreach line (in separate columns), one using the ninefine-grained zones and the second using the ab-stracted three-zone scheme described in Section 3.Note, however, that not all of the nine fine-grained5This annotated dataset is available fromhttp://zebra.thoughtlets.org/.6http://bailando.sims.berkeley.edu/enron/enron.sql.gzzones, nor all of the three abstracted zones, are ac-tually present in this particular message.4 Zone Segmentation and ClassificationOur email zone classification system is basedaround an SVM classifier using features that cap-ture graphic, orthographic and lexical informationabout the content of an email message.To classify the zones in an email message, weexperimented with two approaches.
The first em-ploys a two-stage approach that segments a mes-sage into zone fragments and then classifies thosefragments.
Our second method simply classifieslines independently, returning a classification foreach non-blank line in an email message.
Our hy-pothesis was that classifying larger text fragmentswould lead to better performance due to the textfragments containing more cues about the zonetype.4.1 Zone Fragment ClassificationZone fragment classification is a two-step process.First it predicts the zone boundaries using a simpleheuristic, then it classifies the resulting zone frag-ments, the sets of content lines that lie betweenthese hypothesised boundaries.In order to determine how well we can detectzone boundaries, we first need to establish the cor-rect zone boundaries in our collection of zone-annotated email messages.4.1.1 Zone BoundariesA zone boundary is defined as a continuous collec-tion of one or more lines that separate two differ-ent email zones.
Lines that separate two zones andare blank, contain only whitespace or contain onlypunctuation characters are called buffer lines.Since classification of blank lines betweenzones is often ambiguous, empty or whitespace-only buffer lines are not included as content in anyzone, and thus are not classified.
Instead, they aretreated as strictly part of the zone boundary.
InFigure 1, these lines are shown without any zoneannotation.
Zone boundary lines that are includedas content in a zone have their zone annotationstyled in bold and underlined.
The important pointhere is that zone boundaries are specific to a zoneclassification scheme.
For nine-zone classifica-tion of the message in Figure 1, there are six zoneboundaries: line 2, lines 10?11, line 12, line 15,lines 17?20, and lines 30?33.
For three-zone clas-922Figure 1: An example email message marked with both nine- and three-zone annotations.sification, the only zone boundary consists of line12, separating the sender and boilerplate zones.Based on these definitions, there are three dif-ferent types of zone boundaries:1.
Blank boundaries contain only empty orwhitespace-only buffer lines.
Lines in thesezone boundaries are strictly separate from thezone content.
An example is Line 12 in Fig-ure 1, for both the three- and nine-zone clas-sification.2.
Separator boundaries contain onlybuffer lines, but must contain at leastone punctuation-character buffer line that isretained as content in one or both zones.
InFigure 1, an example is the zone boundarycontaining lines 17?20 that separates theAttachment and Disclaimer zones for nine-zone classification, since line 20 is retainedas part of the Disclaimer zone content.3.
Adjoining boundaries consist of the lastcontent line of the earlier zone and the firstcontent line of the following zone.
Theseboundaries occur where no buffer lines ex-ist between the two zones.
An example isthe zone boundary containing lines 10 and 11that separates the Author and Signoff zones inFigure 1 for nine-zone classification.9234.1.2 Hypothesising Zone BoundariesTo identify zone boundaries in unannotated emaildata, we employ a very simple heuristic approach.Specifically, we consider every line in the body ofan email message that matches any of the follow-ing criteria to be a zone boundary:1.
A blank line;2.
A line containing only whitespace; or3.
A line beginning with four or more repeatedpunctuation characters, optionally prefixedby whitespace.Our efforts to apply more sophisticatedmachine-learning techniques to identifying zoneboundaries could not match the 90.15% recallachieved by this simple heuristic.
The boundariesmissed by the simple heuristic are all adjoiningboundaries, where two zones are not separatedby any buffer lines.
An example of a boundarythat is not detected by our heuristic is the zoneboundary between the Author and Signoff zonesin Figure 1 formed by lines 10 and 11.Obviously, our simple boundary heuristic de-tects actual boundaries as well as spuriousboundaries that do not actually separate differ-ent email zones.
Unsurprisingly, the number ofspurious boundaries is large.
The precision ofour simple heuristic across our annotated set ofemail messages is 22.5%, meaning that less than1 in 4 hypothesised zone boundaries is an actualboundary.
The underlying email zones averagemore than 12 lines in length, including just over8 lines of non-blank content.
Due to the num-ber of spurious boundaries, fragments contain lessthan half this amount ?
approximately 3 lines ofnon-blank content on average.
One of the mostcommon types of spurious boundaries detected arethe blank lines that frequently separate paragraphswithin a single zone.For three-zone classification, the set of pre-dicted boundaries remains the same, but there areless actual boundaries to find, so recall increases to96.3%.
However, because many boundaries fromthe nine-zone classification are not boundaries forthe three-zone classification, precision decreasesto 14.7%.4.1.3 Classifying Zone FragmentsHaving segmented the email message into candi-date zone fragments, we classify these fragmentsusing the SMO implementation provided by Weka(Witten and Frank, 2005) with the features de-scribed in Section 4.3.Although our boundary detection heuristic hasbetter than 90% recall, the small number of ac-tual boundaries that are not detected result in somezone fragments containing lines from more thanone underlying email zone.
In these cases, we con-sider the mode of all annotation values for linesin the fragment (i.e., the most frequent zone an-notation) to be the gold-standard zone type forthe fragment.
This, of course, may mean that wesomewhat unfairly penalise the accuracy of our au-tomated classification when Zebra detects a zonethat is indeed present in the fragment, but is notthe most frequent zone.4.2 Line ClassificationOur line-based classification approach simply ex-tracts all non-blank lines from an email messageand classifies lines one-by-one, using the samefeatures as for fragment-based classification.
Thisapproach is the same as the signature and replyline classification approach used by Carvalho andCohen (2004).4.3 Classification FeaturesWe use a variety of graphic, orthographic and lex-ical features for classification in Zebra.
The samefeatures are applied in both the line-based and thefragment-based zone classification (to either indi-vidual lines or zone fragments).
In the descriptionof our features, we refer to both single lines andzone fragments (collections of contiguous lines) astext fragments.4.3.1 Graphic FeaturesOur graphic features capture information about thepresentation and layout of text in an email mes-sage, independent of the actual words used.
Thisinformation is a crucial source of information foridentifying zones.
Such information includes howthe text is organised and ordered, as well as the?shape?
of the text.
The specific features we em-ploy are:?
the number of words in the text fragment;?
the number of Unicode code points (i.e.,characters) in the text fragment;?
the start position of the text fragment (equalto one for the first line in the message, two forthe second line and increasing monotonically924through the message; we also normalise theresult for message length);?
the end position of the text fragment (calcu-lated as above and again normalised for mes-sage length);?
the average line length (in characters) withinthe text fragment (equal to the line length forline-based text fragments);?
the length of the text fragment (in characters)relative to the previous fragment;?
the length of the text fragment (in characters)relative to the following fragment;?
the number of blank lines preceding the textfragment; and?
the number of blank lines following the textfragment.4.3.2 Orthographic FeaturesOur orthographic features capture informationabout the use of distinctive characters or charac-ter sequences including punctuation, capital let-ters and numbers.
Like our graphic features, or-thographic features tend to be independent of thewords used in an email message.
The specific or-thographic features we employ include:?
whether all lines start with the same character(e.g., ?>?);?
whether a prior text fragment in the messagecontains a quoted header;?
whether a prior text fragment in the messagecontains repeated punctuation characters;?
whether the text fragment contains a URL;?
whether the text fragment contains an emailaddress;?
whether the text fragment contains a se-quence of four or more digits;?
the number of capitalised words in the textfragment;?
the percentage of capitalised words in the textfragment;?
the number of non-alpha-numeric charactersin the text fragment;?
the percentage of non-alpha-numeric charac-ters in the text fragment;?
the number of numeric characters in the textfragment;?
the percentage of numeric characters in thetext fragment;?
whether the message subject line contains areply syntax marker such as Re: ; and?
whether the message subject line contains aforward syntax marker such as Fw:.4.3.3 Lexical FeaturesFinally, our lexical features capture informationabout the words used in the email text.
We useunigrams to capture information about the vocab-ulary and word bigram features to capture shortrange word order information.
More specifically,the lexical features we apply to each text fragmentinclude:?
each word unigram, calculated with a mini-mum frequency threshold cutoff of three, rep-resented as a separate binary feature;?
each word bigram, calculated with a mini-mum frequency threshold cutoff of three, rep-resented as a separate binary feature;?
whether the text fragment contains thesender?s name;?
whether a prior text fragment in the messagecontains the sender?s name;?
whether the text fragment contains thesender?s initials; and?
whether the text fragment contains a recipi-ent?s name.Features that look for instances of sender or recip-ient names are less likely to be specific to a par-ticular business or email domain.
These featuresuse regular expressions to find name occurrences,based on semi-structured information in the emailmessage headers.
First, we extract and normalisethe names from the email headers to identify therelevant person?s given name and surname.
Ourfeatures then capture whether one or both of thegiven name or surname are present in the currenttext fragment.
Features which detect user initialsmake use of the same name normalisation code toretrieve a canonical form of the user?s name, fromwhich their initials are derived.5 Results and DiscussionTable 1 shows Zebra?s accuracy in classifyingemail zones.
The results are calculated using 10-fold cross-validation.
Accuracy is shown for threetasks ?
nine-, three- and two-zone classification?
using both line and zone-fragment classifica-tion.
Performance is compared against a majorityclass baseline in each case.Zebra?s performance compares favourably withpreviously published results.
While it is difficult to9252 Zones 3 Zones 9 ZonesZebra Baseline Zebra Baseline Zebra BaselineLines 93.60% 61.14% 91.53% 58.55% 87.01% 30.94%Fragments 92.09% 62.18% 91.37% 59.44% 86.45% 30.36%Table 1: Classification accuracy compared against a majority baseline2 Zones 3 Zones 9 ZonesZebra Baseline Zebra Baseline Zebra BaselineLines 90.62% 61.14% 86.56% 58.55% 81.05% 30.94%Fragments 91.14% 62.18% 89.44% 59.44% 82.55% 30.36%Table 2: Classification accuracy, without word n-gram features, compared against a majority baselinedirectly compare, since not all systems are freelyavailable and they are not trained or tested over thesame data, our three-zone classification (identify-ing sender, boilerplate and quoted reply content) isvery similar to the three-zone task for which (Es-tival et al, 2007) report 88.16% accuracy for theirsystem and 64.22% accuracy using Carvalho andCohen?s Jangada system.
Zebra outperforms both,achieving 91.53% accuracy using a line-based ap-proach.
In the two-zone task, where we attemptto identify sender-authored lines, Zebra achieves93.60% accuracy and an F-measure of 0.918, ex-ceeding the 0.907 F-measure reported for Estivalet al?s system tuned for exactly this task.Interestingly, the line-based approach providesslightly better performance than the fragment-based approach for each of the two-zone, three-zone and nine-zone classification tasks.
As notedearlier, our original hypothesis was that zone frag-ments would contain more information about thesequence and text shape of the original message,and that this would lead to better performance forfragment-based classification.When we restrict our feature set to those thatlook only at the text of the line or zone fragment,the fragment-based approach does perform betterthan the line-based one.
Using only word uni-gram features, for example, our fragment classi-fier achieves 78.7% accuracy.
Using the same fea-tures, the line-based classifier achieves only 57.5%accuracy.
When we add further features that cap-ture sequence and shape information from outsidethe text fragment being classified (e.g., the lengthof a text segment compared to the text segmentbefore and after, and whether a segment occursafter another segment containing repeated punc-tuation or the sender?s name), the line-based ap-proach achieves a greater increase in accuracy thanthe fragment-based approach.
This presumably isbecause individual lines intrinsically have less in-formation about the message context, and so ben-efit more from the information added by the newfeatures.We also experimented with removing all wordunigram and bigram features to explore the classi-fier?s portability across different domains.
This re-moved all vocabulary and word order informationfrom our feature set.
In doing so, our feature setwas reduced to less than thirty features, consist-ing of mostly graphic and orthographic informa-tion.
The few remaining lexical features capturedonly the presence of sender and recipient names,which are independent of any particular email do-main.
As expected, performance did drop, but notdramatically.
Table 2 shows that average perfor-mance without n-grams (across two-, three- andnine-zone tasks) for line-based classification dropsby 4.67%.
In contrast, fragment-based classifica-tion accuracy drops by less than half this amount?
an average of 2.26%.
This suggests that, as weoriginally hypothesised, there are additional non-lexical cues in zone fragments that give informa-tion about the zone type.
This makes the zonefragment approach potentially more portable foruse across email data from different enterprise do-mains.Of course, classification accuracy gives only alimited picture of Zebra?s performance.
Table 4shows precision and recall results for each zone inthe nine-zone line-based classification task.
Per-926Total Author Signature Disclaim Advert Greet Signoff Reply Fwd AttachAuthor 2415 2197 56 9 4 14 31 43 53 8Signature 383 93 203 4 0 0 20 28 31 4Disclaim 97 30 4 52 0 0 0 2 9 0Advert 83 47 1 1 20 0 0 7 7 0Greet 85 8 0 0 0 74 2 0 1 0Signoff 195 30 5 0 0 0 147 11 2 0Reply 2451 49 10 3 2 1 10 2222 154 0Fwd 2187 72 13 7 8 1 3 125 1958 0Attach 26 4 0 0 0 0 0 1 1 20Table 3: Confusion Matrix for 9 Zone Line Classificationformance clearly varies significantly across thedifferent zones.
For Author, Greeting, Reply andForward zones, performance is good, with F-measure > 0.8.
This is encouraging, given thatmany email tools, such as action-item detectionand email summarisation would benefit from anability to separate author content from reply con-tent and forwarded content.
The Advertising, Sig-nature and Disclaimer zones show the poorest per-formance, particularly in terms of Recall.
TheAdvertising and Disclaimer zones are almost cer-tainly hindered by a lack of training data; they aretwo of the smallest zones in terms of number oflines of training data.
The relatively poor Signa-ture class performance is more interesting.
Giventhe potential confusion between Signoff contentand Signatures that function as Signoffs, one mightexpect confusion between Signoff and Signaturezones, but Table 3 shows this is not the case.Instead, there is significant confusion betweenSignature and Author content, with almost 25%of Signature lines misclassified as Author lines.When word n-grams are removed from the fea-ture set, the number of these misclassifications in-creases to almost 50%.
These results reinforce ourobservation that the task of email signature extrac-tion is much more difficult that it was in the daysof Usenet messages.6 ConclusionIdentifying functional zones in email messages isa challenging task, due in large part to the diver-sity in syntax used by different email software, andthe dynamic manner in which people employ dif-ferent styles in authoring email messages.
Zebra,our system for segmenting and classifying emailmessage text into functional zones, achieves per-Zone Precision Recall F-MeasureAuthor 0.868 0.910 0.889Signature 0.695 0.530 0.601Disclaimer 0.684 0.536 0.601Advertising 0.588 0.241 0.342Greeting 0.822 0.871 0.846Signoff 0.690 0.754 0.721Reply 0.911 0.907 0.909Forward 0.884 0.895 0.889Attachment 0.625 0.769 0.690Table 4: Precision and recall for nine-zone lineclassificationformance that exceeds comparable systems, andthat is at a level to be practically useful to emailresearchers and system builders.
In addition to re-leasing our annotated email dataset, the Zebra sys-tem will also be available for others to use7.Because we employ a non-sequential learn-ing algorithm, we encode sequence informationinto the feature set.
In future work, we planto determine the effectiveness of using a sequen-tial learning algorithm like Conditional RandomFields (CRF).
We note, however, that Carvalhoand Cohen (2004) demonstrate that using a non-sequential learning algorithm with sequential fea-tures, as we do, has the potential to meet or exceedthe performance of sequential learning algorithms.AcknowledgmentsThe authors are grateful to the anonymous review-ers for their insightful comments and suggestions.7See http://zebra.thoughtlets.org for access to the anno-tated data and Zebra system927ReferencesDouglas Beeferman, Adam Berger, and John Lafferty.1997.
Text segmentation using exponential models.In Proceedings of the 2nd Conference on Empiri-cal Methods in Natural Language Processing, pages35?46, Providence, RI.Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,and Ian Smith.
2003.
Taking email to task: Thedesign and evaluation of a task management centredemail tool.
In Computer Human Interaction Confer-ence, CHI, pages 345?352, Ft Lauderdale, Florida,USA, April 5-10.Paul N Bennett and Jaime G Carbonell.
2007.
Com-bining probability-based rankers for action-item de-tection.
In Proceedings of NAACL HLT 2007, pages324?331, Rochester, NY, April.Vitor R Carvalho and William W Cohen.
2004.
Learn-ing to extract signature reply lines from email.
InProceedings of First Conference on Email and Anti-Spam (CEAS), Mountain View, CA, July 30-31.Hao Chen, Jianying Hu, and Richard W Sproat.
1999.Integrating geometrical and linguistic analysis foremail signature block parsing.
ACM Transactionson Information Systems, 17(4):343?366, October.ISSN: 1046-8188.Simon H. Corston-Oliver, Eric Ringger, Michael Ga-mon, and Richard Campbell.
2004.
Task-focusedsummarization of email.
In ACL-04 Workshop: TextSummarization Branches Out, pages 43?50, July.Aron Culotta, Ron Bekkerman, and Andrew McCal-lum.
2004.
Extracting social networks and contactinformation from email and the web.
In Proceedingsof the Conference on Email and Anti-Spam (CEAS).Dominique Estival, Tanja Gaustad, Son Bao Pham,Will Radford, and Ben Hutchinson.
2007.
Authorprofiling for English emails.
In Proceedings of the10th Conference of the Pacific Association for Com-putational Linguistics, pages 263?272, Melbourne,Australia, Sept 19-21.R.
Gellens.
2004.
RFC3676: The text/plain format anddelsp parameters, February.Marti A. Hearst.
1997.
Texttiling: Segmenting textinto multi-paragraph subtopic passages.
Computa-tional Linguistics, 23(1):33?64.Bryan Klimt and Yiming Yang.
2004.
Introducing theEnron corpus.
In Proceedings of the Conference onEmail and Anti-Spam (CEAS).Andrew Lampert, Ce?cile Paris, and Robert Dale.
2007.Can requests-for-action and commitments-to-act bereliably identified in email messages?
In Proceed-ings of the 12th Australasian Document Comput-ing Symposium, pages 48?55, Melbourne, Australia,December 10.Ian Witten and Eiba Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
Mor-gan Kaufmann, San Francisco, 2nd edition.928
