Assigning Grammatical  Relat ions with a Back-off ModelErika F. de L imaGMD - German National  Research Center for In format ion TechnologyDolivostrasse 1564293 Darmstadt ,  Germanydelima@darmstadt, gmd.
deAbst rac tThis paper presents a corpus-based methodto assign grammatical subject/object re-lations to ambiguous German constructs.It makes use of an unsupervised learningprocedure to collect raining and test data,and the back-off model to make assignmentdecisions.1 In t roduct ionAssigning a parse structure to the German sentence(1) involves addressing the fact that it is syntacti-cally ambiguous:(1) Eine hohe Inflationsrate erwartet die ()konomin.a high inflation rate expects the economist'The economist expects a high inflation rate.
'In this sentence it must be determined which nom-inal phrase is the subject of the verb.
The verb er-war ren  ('to expect') takes, in one reading, a nom-inative NP as its subject and an accusative NP asits object.
The nominal phrases preceding and fol-lowing the verb in (1) are both ambiguous withrespect o case; they may be nominative or ac-cusative.
Further, both NPs agree in number withthe verb, and since in German any major con-stituent may be fronted in a verb-second clause,both NPs may be the subject/object of the verb.In this example, morpho-syntactical information isnot sufficient o determine that the nominal phrase\[NP die C)konomin\] ('the economist') is the subject ofthe verb, and \[NP Eine hohe Inflationsrate\] ('ahighinflation rate') its object.Determining the subject/object of an ambiguousconstruct such as (1) with a knowledge-based ap-proach requires (at least) a lexical representationspecifying the classes of entities which may serve asarguments in the relation(s) denoted by each verbin the vocabulary, as well as membership nforma-tion with respect o these classes for all entities de-noted by nouns in the vocabulary.
One problem withthis approach is that it is usually not available for abroad-coverage system.This paper proposes an approximation, similar tothe empirical approaches toPP attachment decision(Hindle and Rooth, 1993; Ratnaparkhi, Reynar, andRoukos, 1994; Collins and Brooks, 1995).
Thesemake use of unambiguous examples provided by atreebank or a learning procedure in order to train amodel to decide the attachment of ambiguous con-structs.
In the current setting, this approach in-volves learning the classes of nouns occurring unam-biguously as subject/object of a verb in sample text,and using the classes thus obtained to disambiguateambiguous constructs.Unambiguous examples are provided by sentencesin which morpho-syntactical information suffices todetermine the subject and object of the verb.
For in-stance in (2), the nominal phrase \[NP der C)konom\]with a masculine head noun is unambiguously nom-inative, identifying it as the subject of the verb.
In(3), both NPs are ambiguous with respect o case;however, the nominal phrase \[NP Die 0konomen\]with a plural head noun is the only one to agree innumber with the verb, identifying it as its subject.
(2) Eine hohe Inflationsrate erwartet der 0konom.a high inflation rate expects the economist'The economist expects a high inflation rate.
'(3) Die Okonomen erwarten eine hohe Inflationsrate.the economists expect a high inflation rate'The economists expect a high inflation rate.
'This paper describes a procedure to determine thesubject and object in ambiguous German constructsautomatically.
It is based on shallow parsing tech-niques employed to collect training and test datafrom (un)ambiguous examples in a text corpus,90and the back-off model to determine which NP ina morpho-syntactically ambiguous construct is thesubject/object of the verb, based on the evidenceprovided by the collected training data.2 Co l lec t ing  Tra in ing  and  Test  DataShallow parsing techniques are used to collect train-ing and test data from a text corpus.
The corpusis tokenized, morphologically analyzed, lemmatized,and parsed using a standard CFG parser with ahand-written grammar to identify clauses containinga finite verb taking a nominative NP as its subjectand an accusative NP as its object.Constructs covered by the grammar include verb-second and verb-final clauses.
Each clause is seg-mented into phrase-like constituents, including nom-inative (NC), prepositional (PC), and verbal (VC)constituents.
Their definition is non-standard; forinstance, all prepositional phrases, whether comple-ment or not, are left unattached.
As an example,the shallow parse structure for the sentence in (4) isshown in (4') below.
(4) Die Gesellschaft erwartet in diesem Jahrthe society expects in this yearin Siidostasien einen Umsatzin southeast Asia a turnovervon 125 Millionen DM.from 125 million DM'The society expects this year in southeast Asiaa turnover of 125 million DM.
'(4') \[S \[NC3.,.~ .
.
.
.
.
.
~ Die Gesellschaft\]\[vc3.. erwartet\]\[PC in diesem Jahr\]IRe in Sfidostasien\]\[NC3 ...... einen Umsatz\]\[PC yon 125 Millionen DM\]\]Nominal and verbal constituents display person andnumber information; nominal constituents also dis-play case information.
For instance in the structureabove, 3 denotes third person, s denotes ingularnumber, nora and acc denote nominative and ac-cusative case, respectively.
The set {nora, acc} indi-cates that the first nominal constituent in the struc-ture is ambiguous with respect o case; it may benominative or accusative.Test and training tuples are obtained from shallowstructures containing a verbal constituent and twonominative/accusative nominal constituents.
Notethat no subcategorization information is used; it suf-fices for a verb to occur in a clause with two nom-inative/accusative NCs for it to be considered test?ing/training data.Training data consists of tuples (nl,v, n2,x),where v is a verb, nl and n2 are nouns, andx E {1,0} indicates whether nl is the subjectof the verb.
Test data consists of ambiguous tu-ples (nx,v, n2) for which it cannot be establishedwhich noun is the subject/object of the verb basedon morpho-syntacticai information alone.The set of training and test tuples for a given cor-pus is obtained as follows.
For each shallow structures in the corpus containing one verbal and two nomi-native/accusative nominal constituents, let nl, v, n2be such that v is the main verb in s, and nl and n2are the heads of the nominative/accusative NCs ins such that nl precedes n2 in s. In the rules below,i , j  e {1,2},j  ~ i, and g(i) = 1 if i = 1, and 0otherwise.
Note that the last element in a trainingtuple indicates whether the first NC in the structureis the subject of the verb (1 if so, 0 otherwise).Case Nominat ive  Rule.
If ni is masculine, andthe NC headed by ni is unambiguously nominative 1,then (nx, v, n2, g(i)) is a training tuple,Case Accusat ive  Rule.
If ni is masculine, and theNC headed by ni is unambiguously accusative, then(nl, v, n2, g(j)) is a training tuple,Agreement Rule.
If ni but not nj agrees withv in person and number, then (nl,v, n2,g(i)) is atraining tuple,Heur i s t i c  Rule.
If the shallow structure consistsof a verb-second clause with an adverbial in the firstposition, or of a verb-final clause introduced by aconjunction or a complementizer, then (nl, v, n2, 1)is a training tuple (see below for examples),Defau l t  Rule.
(hi, v, n2) is a test triple.For instance, the training tuple (Gesellschaft, er-warren, Umsatz, 1) ('society, expect, turnover') isobtained from the structure (4') above with theCase Accusative Rule, since the NC headed bythe masculine noun Umsatz ('turnover') is unam-biguously accusative and hence the object of theverb.
The training tuple (Inflationsrate, erwarten,Okonom, O) ('inflation rate, expect, economist') and(Okonom, erwarten, Inflationsrate, 1) ('economist,expect, inflation rate') are obtained from sentences(2) and (3) with the Case Nominative and Agree-ment Rules, respectively, and the test tuple (Infla-tionsrate, erwarten, Okonomin) ('inflation rate, ex-pect, economist' ) from the ambiguous entence in(1) by the Default Rule.1Only NCs with a masculine head noun may be un-ambiguous with respect o nominative/accusative casein German.91The Heuristic Rule is based on the observationthat in the constructs tipulated by the rule, al-though the object may potentially precede the sub-ject of the verb, this does not (usually) occur in writ-ten text.
(5) and (6) are sentences to which this ruleapplies.
(5) In diesem Jahr erwartet die Okonominin this year expects the economisteine hohe Inflationsrate.a high inflation rate'This year the economist expects a highinflation rate.
"(6) Weil die Okonomin eine hohe Inflationsratebecause the economist a high inflation rateerwartet , .
.
.expects'Because the economist expects a high inflationra te , .
.
.
'Note that the Heuristic Rule does not apply to verb-final clauses introduced by a relative or interrogativeitem, such as in (7):(7) Die Rate, die die Okonomin erwartet, ...the rate which the economist expects, ...3 Test ingThe testing algorithm makes use of the back-offmodel (Katz, 1987) in order to determine the sub-ject/object in an ambiguous test tuple.
The model,developed within the context of speech recognition,consists of a recursive procedure to estimate n-gramprobabilities from sparse data.
Its generality makesit applicable to other areas; the method has beenused, for instance, to solve prepositional phrase at-tachment in (Collins and Brooks, 1995).3.1 Katz's back-of f  mode lLet w~ denote the n-gram Wl,.. .
,wn, and ff(w~)denote the number of times it occurred in a sampletext.
The back-off estimate computes the probabil-ity of a word given the n - 1 preceding words.
Itis defined recursively as follows.
(In the formulaebelow, O~(W~ -1) is a normalizing factor and dr a dis-count coefficient.
See (Katz, 1987) for a detailedaccount of the model.
)P~ "w 'W n- l "  fP(w. lwF--1), if P(w.lw~ -1) > 0hot nl 1 ) :  ~c~(w~-l)Pbo(Wnlw~-l), otherwise,where P(w.lw7 -1) is defined as follows:~ i f f (w\ [  -1)~0 P(w.lwl '-1) = dI(~7)//(wl-, ~-,),0, otherwise.3.2 The Revised ModelIn the current context, instead of estimating theprobability of a word given the n -1  preceding words,we estimate the probability that the first noun nz ina test triple (nl,v, n2) is the subject of the verb v,i.e., P(S = ilNi = nl, V = v, N2 = n2) where S isan indicator random variable iS = I if the first nounin the triple is the subject of the verb, 0 otherwise).In the estimate Pbo(WnlW~ -I) only one relation--the precedence relation--is relevant to the problem;in the current setting, one would like to make use oftwo implicit relations in the training tuplc subjectand object--in order to produce an estimate forP(l\[nl,v, n2).
The model below is similar to thatin iCollins and Brooks, 1995).Let ?
be the set of lemmata occurring in thetraining triples obtained from a sample text, and letc(nl,v, n2,x) denote the frequency count obtainedfor the training tuple (nl,v,  n2,x) (x E {0, 1}).
Wedefine the count fso(nl,v, n2) : c(nl,v, n2, 1) +c(n2, v, nx, 0) of nl as the subject and n2 as the ob-ject of v. Further, we define the count fs (nl, v) =~n2e?fso(nl,v, n2) of nl as the subject of v withany object, and analogously, the count fo(nx,v) ofnl as the object of v with any subject.
Further,we define the counts fs(v) = ~nl,n2eL c(nl, v, n2, 1)and fo(V) = ~m,n2e?
c(nl, v, n2,0).
The estimatePi(llnl,v, n2 ) C 0 < i < 3) is defined recursively asfollows:Po(llnl,v, n2 ) = 1.0{ ~c'l_nl'.v.
'_n2!, if ti(nl,v, n2) > 0Pi(llnx,v, n2) = ~,~,,, ..... 2,P(i-1) (1\[nl, v, nz), otherwise,where the counts ci(nl,v, n2), and ti(nl,v, n2) aredefined as follows:fso(nx,v, n2), i f i  = 3ci(nl,v, n2) = fs(nl,v) + fo(n2,v), i f i=2fs(v), if i = 1ti i n l ,  V, n2) :fso(nl,v, n2) + fso(n2,v, nl), i f i  = 3fs(nl,v)+fo(nl,v)+fs(n2,v)+foin2,v), i f /=  2fs(v) + foiv), if i = 1The defnition of P3 (l lnl,  v, n2) is analogous to thatof Pbo(Wnlw~-X).
In the case where the counts are92positive, the numerator in the latter is the numberof times the word Wn followed the n-gram w~ -1 intraining data, and in the former, the number of timesnl occurred as the subject with n2 as the object of v.This count is divided, in the latter, by the numberof times the n-gram w~ -1 was seen in training data,and in the former, by the number of times nl wasseen as the subject or object of v with n2 as itsobject/subject respectively.However, the definition of P2(1\]nl, v, n2) is some-what different; it makes use of both the subjectand object relations implicit in the tuple.
InP2(l lnl,  v, n2), one combines the evidence for nl asthe subject of v (with any object) with that of n2 asthe object of v (with any subject).At the P1 level, only the counts obtained for theverb are used in the estimate; although for certainverbs some nouns may have definite preferences forappearing in the subject or object position, this in-formation was deemed on empirical grounds not tobe appropriate for all verbs.When the verb v in a test tuple (nl,v, n2)does not occur in any training tuple, the defaultPo(llnl,v, n2 ) = 1.0 is used; it reflects the fact thatconstructs in which the first noun is the subject ofthe verb are more common.3.3 Decision AlgorithmThe decision algorithm determines for a given testtuple (nl,v, n2), which noun is the subject of theverb v. In case one of the nouns in the tuple isa pronoun, it does not make sense to predict thatit is subject/object of a verb based on how often itoccurred unambiguously assuch in a sample text.
Inthis case, only the information provided by trainingdata for the noun in the test tuple is used.
Further,in case both heads in a test tuple are pronouns, thetuple is not considered.
The algorithm is as follows.If nl and n2 are both nouns, then nl is the subjectof v if P3 (l lnl,  v, n2) > 0.5, else its object.In case n2 (but not nl) is a pronoun, redefine ci andti as follows:" ~ f,(nl,v), i f i=2ci(nl,v, n2) : ~.
fs(v), i f i  = 1f fs(nl,V) -}-fo(nl,V), i f i  = 2 ti(nl,v, n2) I fs(v) + fo(v), if i = 1and calculate P2(llnl,v, n2 ) with these new defini-tions.
If P2(l\[nl,v, n2) > 0.5, then nl is the subjectof the verb v, else its object.
We proceed analogouslyin case nl (but not n2) is a pronoun.3.4 Related WorkIn (Collins and Brooks, 1995) the back-off modelis used to decide PP attachment given a tuple(v, nl,p, n2), where v is a verb, nl and n2 are nouns,and p a preposition such that the PP headed by pmay be attached either to the verb phrase headedby v or to the NP headed by nx, and n: is the headof the NP governed by p.The model presented in section 3.2 is similar tothat in (Collins and Brooks, 1995), however, unlike(Collins and Brooks, 1995), who use examples froma treebank to train their model, the procedure de-scribed in this paper uses training data automati-cally obtained from sample text.
Accordingly, themodel must cope with the fact that training data ismuch more likely to contain errors.
The next sec-tion evaluates the decision algorithm as well as thetraining data obtained by the learning procedure.4 Resu l tsThe method described in the previous section wasapplied to a text corpus consisting of 5 months of thenewspaper Frankfurter Allgemeine Zeitung with ap-proximately 15 million word-like tokens.
The learn-ing procedure produced a total of 24,178 test tuplesand 47,547 training triples.4.1 Learning procedureIn order to evaluate the data used to train the model,1000 training tuples were examined.
Of these tuples,127 were considered to be (partially) incorrect basedon the judgments of a single judge given the originalsentence.
Errors in training and test data may stemfrom the morphology component, from the grammarspecification, from the heuristic rule, or from actualerrors in the text.4.1.1 Subcategorization InformationThe system works without subcategorization in-formation; it suffices for a verb to occur with a possi-bly nominative and a possibly accusative NC for it tobe considered training/test data.
Lack of subcatego-rization leads to errors when verbs occurring with an(ambiguous) dative NC are mistaken for verbs whichsubcategorize for an accusative nominal phrase.
Forinstance in (7) below, the verb gehSren ('to belong')takes, in one reading, a dative NP as its object anda nominative NP as its subject.
Since the nomi-nal constituent \[NC Bill\] is ambiguous with respectto case and possibly accusative, the erroneous tu-pie (Wagen, gehSren, Bill, 1) ('car, belong, Bill') isproduced for this sentence.93(7) Der Wagen gehSrt Bill.the car belongs Bill'The car belongs to Bill.
'Another source of errors is the fact that any ac-cusative NC is considered an object of the verb.For instance in sentence (8), the verb trainieren ('totrain') occurs with two NCs.
Since the NC preced-ing the verb is unambiguously nominative and theone following the verb possibly accusative, the train-ing tuple (Tennisspieler, trainieren, Jahr, 1) ('ten-nis player, train, year') is produced for this sentence,although the second NC is not an object of the verb.
(8) Der Tennisspieler t ainiert das ganze Jahr.the tennis player trains the whole year4.1.2 HomographsIn sentence (9) below, the word morgen ('to-morrow') is an adverb.
However, its capitalizedform may also be a noun, leading in this case tothe erroneous training tuple (Morgen, trainieren,Tennisspieler, O) (since \[NO der Tennisspieler\] is un-ambiguously nominative).
(9) Morgen trainiert der Tennisspieler.tomorrow trains the tennis player'The tennis player will train tomorrow.
'4.1.3 Separable PrefixesIn German, verb prefixes can be separated fromthe verb.
When a finite (separable prefix) main verboccupies the second position in the clause, its prefixtakes the last position in the clause core.
For exam-ple in sentence (10) below, the prefix zur~ick of theverb zuriickweisen ('to reject') follows the object ofthe verb and a subordinate clause with a subjunc-tive main verb.
This construct is not covered by thecurrent version of the grammar.
However, due tothe grammar definition, and since weisen is also averb (without a separable prefix) in German, \[c Erweist die Kritik der Prinzessin\] is still accepted as avalid clause, leading to the erroneous training tuple(er, weisen, Kritik, 1) ('he, point, criticism').
Sucherrors may be avoided with further development ofthe grammar.
(10) Er weist die Kritik der Prinzessin, seinehe rejects the criticism the princess hisOhren seien zu grofl, zurfick.ears are too big PRT'He rejects the princess' criticism that his earsare too big.
"4.1.4 Const i tuent  HeadsThe system is not always able to determine con-stituent heads correctly.
For instance in sentence(11), all words in the name Mexikanische Verband\]iir Menschenrechte are capitalized.
Upon encoun-tering the adjective Mexikanische, the system takesit to be a noun (nouns are capitalized in German),followed by the noun Verband "in apposition".
Sen-tence (11) is the source of the erroneous training tu-ple (Mexikanisch, beschuldigen, BehSrde, 1) ('Mexi-can, blame, public authorities').
(11) Der Mexikanische Verband fiir Menschen-the Mexican Association for Humanrechte beschuldigt die BehSrden.Rights blames the public authorities'The Mexican Association for Human Rightsblames the public authorities.
'4.1.5 Mult i -word lexical unitsThe learning procedure has no access to multi-word lexical units.
For instance in sentence (12), thefirst word in the expression Hand in Hand is consid-ered the object of the verb, leading to the trainingtuple (Architekten, arbeiten, Hand, 1) ('architect,work, hand').
Given the information the system hasaccess to, such errors cannot be avoided.
(12) Alle Architekten sollen Hand in Hand arbeiten.all architects should hand in hand work'All architects hould work hand in hand.
'4.1.6 Source TextNot only spelling errors in the source text are thesource of incorrect uples.
For instance in sentence(13), the verb suchen ('to seek') is erroneously in thethird person plural.
Since Reihe ('series') in Germanis a singular noun, and Kontakte ('contacts') plu-ral, the actual object, but not the subject, agrees innumber with the verb, so the incorrect uple (Reihe,suchen, Kontakt, O) ('series, seek, contact') is ob-tained from this sentence.
(13) *Eine Reihe von Staaten suchen gesch/iftlichea series from states seek businessKontakte zu der Region.contacts to the region'*A series of states seek contacts to the region.
'Finally, a large number of errors, specially in testtuples, stems from the fact that soft constraints areused for words unknown to the morphology.4.2 Decision A lgor i thmIn order to evaluate the accuracy of the decisionalgorithm, 1000 triples were selected from the set oftest triples.
Of these, 285 contained errors, based94P1P0TotalNumber Percent of test tuples Number correct Accuracy2204486237150.2828.5367.973.22100.00219443120647Figure 1: The accuracy of the system at each level100.0095.1088.6886.9690.49on the judgements of a single judge given the origi-nal sentence 2.
The results produced by the systemfor the remaining 715 tuples were compared to thejudgements ofa single judge given the original text.The system performed with an overall accuracy of90.49%.A lower bound for the accuracy of the decision al-gorithm can be defined by considering the first nounin every test tuple to be the subject of the verb (byfar the most common construct), yielding for these715 tuples an accuracy of 87.83%.The above figure shows how many of the 715 eval-uated test tuples were assigned subject/object basedon the values Pn, and the accuracy of the system ateach level.The accuracy for P2 and Ps exceeds 95%.
How-ever, their coverage is relatively low (28.81%).
Sincethe procedure used to collect training data runswithout supervision, increasing the size of the train-ing set depends only on the availability of sampletext and should be further pursued.One reason for the relatively low coverage isthe fact that German compound nouns consider-ably increase the size of the sample space.
For in-stance, the head of the nominal constituent \[NC DerTennisspieler\] ('the tennis player') is considered bythe system to be the compound noun Tennisspieler('tennis player'), instead of its head noun Spieler('player').
Consistently considering the head of pu-tative compound nouns to be the head of nomi-nal constituents may in some cases lead to awk-ward results.
However, reducing the size of the sam-ple space by morphological processing of compoundnouns should be considered in order to increase cov-erage.4.2.1 ExamplesFollowing are examples of test tuples for which adecision was made based on values of P2.
All sen-tences below stem from the corpus.Sentence (14) was the source for the test tuple(Ausstellung, zeigen, Spektrum) ('exhibition, show,2The higher error rate for test uples is due to the softconstraints used for words unknown to the morphology.spectrum').
This tuple was correctly disambiguatedwith P2 = 0.87, with, among others, the trainingtuples (Ausstellung, zeigen, Bild, 1) ('exhibition,show, painting'), (Ausstellung, zeigen, Beispiel, 1)('exhibition, show, example'), and (Ausstellung,zeigen, Querschnitt, 1) ('exhibition, show, cross-section') obtained with the Agreement (sentences(15) and (16)) and Case Rules (sentence (17)), re-spectively.
(14) Die Ausstellung zeigt das Spektrum jfidischerthe exhibition shows the spectrum jewishBuchkunst yon den AnFdngen \[...\]book art from the beginnings'The exhibition shows the spectrum of jewishbook art from the beginnings \[...\].
'(15) die letzte Ausstellung vor der Sommerpausethe last exhibition before the summer pausezeigt Bilder und Zeichnungen von Petrashows paintings und drawings from PetraTrenkel zum Thema "Dorf".Trenkel to the subject village'The last exhibition before the summer pauseshows paintings and drawings by PetraTrenkel on the subject "village".
'(16) Die Ausstellung im Museum fiir Kunst-the exhibition in the museum for arts andhandwerk zeigt Beispiele seiner vielf~iltigencrafts shows examples his manifoldObjekt-Typen \[...\]object types'The exhibition in the museum for arts andcrafts shows examples of his manifoldobject ypes \[...\]'(17) Eine vom franzSsischen Kulturinstituta from the French culture institutemit Unterstiitzung des BSrsenvereinswith support he BSrsenvereinin der Zentralen Kinder- und Jugendbibliothekin the central children and youth library95im Biirgerhaus Bornheimin the community center Bornheimeingerichtete Ausstellung zeigtorganized exhibition showseinen interessanten Querschnitt.an interesting cross-section'A exhibition in the central children's andyouth library in the community center Born-heim, organized by the French cultureinstitute with support of the BSrsenverein,shows an interesting cross-section.
'Sentence (18) below was the source for the test tuple(Altersgrenze, nennen, Gesetz) ('age limit, mention,law').
The system incorrectly considered the nounAltersgrenze to be the subject of the verb.
(18) Eine Altersgrenze nennt das Gesetz nicht.an age limit mentions the law not'The law does not mention an age limit.
'There were no training tuples in which the com-pound noun Altersgrenze occurred as the sub-ject/object of the verb.
However, the noun Gesetzoccurred more frequently as the object of the verbnennen than as its subject, leading to the erroneousdecision.5 ConclusionThis paper describes a procedure to automaticallyassign grammatical subject/object relations to am-biguous German constructs.
It is based on an unsu-pervised learning procedure to collect est and train-ing data and the back-off model to make assignmentdecisions.
The system was implemented and testedon a 15-million word newspaper corpus.The overall accuracy of the decision algorithm wasalmost 3% higher than the baseline of 87.83% es-tablished.
The accuracy of the procedure for tu-ples for which a decision was made based on trainingpairs/triples (P2 and P3) exceeded 95%.In order to increase the coverage for these cases aswell as the overall performance of the procedure, thesample space should be reduced by morphologicallyprocessing German compound nouns, and the size ofthe training set should be increased.
Further, in theexperiment described in this paper, the model wastrained with data obtained by an unsupervised pro-cedure which performs with an accuracy of approxi-mately 87% for training data.
Further developmentof the morphology component and grammar defini-tion should lead to improved results.6 AcknowledgmentsI would like to thank Michael KSnyves-Tdth, whodeveloped the parser engine used in the experimentdescribed in this paper, for his support.
I would alsolike to thank Martin BSttcher and the anonymousreviewers for many helpful comments on an earlierversion of the paper.ReferencesCollins, Michael and James Brooks.
1995.
Prepo-sitional phrase attachment through a backed-offmodel.
In Proceedings of the Third Workshop onVery Large Corpora.Hindle, Donald and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19(1).Katz, S. 1987.
Estimation of probabilities fromsparse data for the language model component ofaspeech recognizer.
IEEE Transactions on Acous-tics, Speech, and Signal Processing, 35(3).Ratnaparkhi, A., J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phraseattachment.
In Proceedings of the ARPA Work-shop on Human Language Technology.96
