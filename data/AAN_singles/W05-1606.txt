Generating Referential Descriptions Under Conditions of UncertaintyHelmut HoracekUniversit?t des SaarlandesF.R.
6.2 InformatikPostfach 151150,  D-66041 Saarbr?cken, Germanyemail: horacek@cs.uni-sb.deAbstractAlgorithms for generating referring expressionstypically assume that an object in a scenary can beidentified through a set of commonly agreedproperties.
This is a strong assumption, since inreality properties of objects may be perceived differ-ently among people, due to a number of factorsincluding vagueness, knowledge discrepancies, andlimited perception capabilities.
Taking these discre-pancies into account, we reinterpret concepts ofalgorithms generating referring expressions in viewof uncertainties about the appearance of objects.
Ourmodel includes two complementary measures oflikelihood in object identification, and adaptedproperty selection and termination criteria.
Theapproach is relevant for situations with potentialperception problems and for scenarios with knowl-edge discrepancies between conversants.1 IntroductionGenerating referring expressions is a traditional, standardtask in natural language generation.
Over the past twodecades, a number of algorithms have been proposed whichdiffer among each other in terms of efficiency and coverage.To the best of our knowledge, all algorithms share theassumption that objects can be identified by a descriptionconsisting of attribute values ascribed to these objects.Moreover, the results are specified in a way that implicitlyassumes complete agreement about these properties,provided they are known to the audience.
We feel that thisassumption may be too strong in reality so that, forinstance, a dialog system in which the reference generationalgorithm is embedded is unlikely to behave adequately whena misunderstanding occurs due to a perception mismatch.In this paper, we address this problem by incorporatingmeasures to deal with uncertainties into a standard algorithmthat generates referring expressions.
In order to representuncertainties, we propose two complementary measuresexpressing the likelihood of object identification.
We definecomputation schemes for combining descriptions withboolean combinations of  attribute values, and we extend theincremental standard reference generation algorithm byadapting property selection and termination criteria.This paper is organized as follows.
First, we motivate ourapproach in more detail.
Then we introduce our method forrepresenting aspects of uncertainty.
We follow by illustratingthe propagation of uncertainty assessments for several attri-bute values, including boolean combinations, and we giveexamples of the effects.
Then we describe extensions to theincremental algorithm, and we discuss their impact.2 MotivationIn the scope of this paper, we adopt the terminology origin-ally formulated in [Dale 1988] and later used by severalothers.
A referential description [Donellan 1966] serves thepurpose of letting the hearer or reader identify a particularobject or set of objects in a given situation.
The referringexpression to be generated is required to be a distinguishingdescription, that is a description of the enitties being referredto, but not to any other object in the context set.
A contextset is defined as the set of the entities the addressee iscurrently assumed to be attending to ?
this is similar to theset of entities in the focus spaces of the discourse focus stackin Grosz' and Sidner's [1986] theory of discourse structure.Moreover, the contrast set  (or the set of potentialdistractors [McDonald 1981]) is defined to entail allelements of the context set except the intended referents.Generating referring expressions is pursued since theeighties [Appelt 1985, Kronfeld 1986, Appelt and Kronfeld1987].
Subsequent years were characterized by a debate aboutcomputational efficiency versus minimality of the elementsappearing in the resulting referring expression [Dale 1988,Reiter 1990, Reiter and Dale 1992].
In the mid-nineties, thisdebate seemed to be settled in favor of the incrementalapproach [Dale and Reiter 1995] ?
motivated by results ofpsychological experiments [Levelt 1989, Pechmann 1989],certain non-minimal expressions are tolerated in favor ofadopting the fast strategy of incrementally selecting ambi-guity-reducing attributes from a domain-dependent preferencelist.
Recently, algorithms have been applied to the identifi-cation of sets of objects rather than individuals [Bateman1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001], andthe repertoire of descriptions has been extended to booleancombinations of attributes, including negations [van Deemter2002].
To avoid the generation of redundant descriptions thatis typical for incremental approaches, Gardent [2002] andHoracek [2003] proposed exhaustive resp.
best-first searches.All these procedures more or less share the design of theunderlying knowledge base.
Objects are conceived in termsof sets of attributes, each with an atomic value as its filler.Some models distinguish specializations of these valuesaccording to a taxonomic hierarchy, so that the most accu-rate value can be replaced by one of its generalizations ifthere are reasons to assume this alternative is preferable ?due to insufficient knowledge attributed to the audience, orto prevent unintended implications.
A few approaches alsodeal with relations to other objects, whose representationdiffers from that of attributes only by the reference to therelated object.
Typically, a user model is assumed to guidethe choice among available descriptors; the user modelexpresses taxonomic knowledge attributed to the user,  indic-ating for a descriptor whether it is known to the user or not.While a knowledge base developed and interpreted in thismanner is adequate for generating referring expressions inmost application-relevant settings, there may be circum-stances in which uncertainties are prominent, so that thesimple boolean attribution of properties to objects becomesproblematic and may prove insufficient.
Uncertainties maymanifest themselves in at least the following three factors:?
Uncertainty about knowledgeThere may not be sufficient evidence to assume thatthe user is or is not acquainted with a specific term.
Infact, most of today's user model components assignsome probability to statements about a user's knowl-edge or capabilities, for example on the basis of infer-ences obtained through a belief network [Pearl 1988].?
Uncertainty about perception capabilitiesThere is an increasing number of applications withnatural language interaction where the objects of thediscourse do not appear on the computer screen (e.g.,ubiquitous tools guiding a user in environments suchas airports and tourist attraction areas, e.g., [Wahlster2004]).
In such situations, perception and recognitionof object properties is much harder to assess; forexample, the visibility of some object or of one of itsparts may not be derivable with complete certainty.?
Uncertainty about conceptual agreementWhile ascribing a value to an attribute is straightfor-ward for certain categories of attributes, problems mayoccur, e.g., in connection with vagueness.
Thisconcept may be relevant for a number of commonlyused properties such as size and shape, and even withcolors, transitions between adjacent color tones maynot be firmly categorized as one of the two candidates.To illustrate these manifestations of uncertainty, let usconsider a scenario with three similar dogs, one of which isa bassett, which is also the intended referent.
In addition,the bassett is brownish and has a long tail.
The other twodogs have shorter tails and their skin is also brown, butwith some white resp.
black portions.
Furthermore, weassume that the audience has little knowledge about dogspecifics, that is, it is not very likely that they may recog-nize the intended referent as a bassett.
We also assume thatthe tails of the dogs cannot be observed easily by theaudience under the given local circumstances.Hence, the three attributes ?category?, ?color?, and ?taillength?
each fall into one of the categories of uncertaintyintroduced above: the categorization of the intended referentas a bassett is associated with uncertainty about knowledge,the limited visibility which may not enable the spectatorsto see the tails of the dogs in each moment constitutes anuncertainty about perception capabilities, and the similarityof the dogs' colors may yield uncertainty about conceptualagreement, that is, it is doubtfull whether the descriptor?brownish?
is attributed only to the intended referent or alsoto some of the other dogs in the given situation.Apparently, these uncertainties have consequences onbuilding human-adequate referring expressions, especially incontexts where most of the descriptors available are asso-ciated with some kind of uncertainty.
Intuitively, we wouldexpect people to produce referring expressions with severalof these descriptors, being redundant in case they are allrecognized, but also hoping that the identification willsucceed if the audience can identify only some part of theoverall description in the given situation.
Moreover, wewould expect people only to use descriptors that have somereasonable chance of being understood.Unfortunately, traditional generation algorithms do notenable us to model such a behavior, since none of theoptions available does justice to the uncertainty involved.
Ifa descriptor is modeled as applying to all entities (e.g., for?brownish?
), it will never be chosen since it yields nodiscrimination.
A similar consequence is obtained when thecapabilities of the audience are interpreted pessimistically.Finally, if a descriptor is assumed to be understood, itmight be chosen without considering any of the other candi-dates associated with uncertainty.
Thus, modeling in theexisting algorithms forces us to make crisp decisions, withstrong impacts on the result of the algorithm.
Redundantexpressions motivated by uncertainties about recognitioncannot be generated under any modeling alternative.There are only a few computational approaches whichaddress the problem of uncertainty about the recognition ofreferring expressions.
For example, [Edmonds 1994] and[Heeman and Hirst 1995] describe both plan-based methods,where a vague and partial description is produced initially,which is narrowed and ultimately confirmed in the subse-quent discourse.
However, the documented examples do onlyemphasize incomplete, but never incorrect interpretations.An approach that fits better to our intentions is the workby Goodman [1987], which emphasizes reference identi-fication and associated failures in task-oriented dialogs[Goodman 1986].
This case study demonstrates variousimpacts of limitations and discrepancies of expertise onreferential identification: subjects exhibit uncertainty inidentification, which manifests itself in tentative actions andchanges of mind, they misinterpret descriptions (e.g.,'outlet' interpreted as 'hole'), and they may find noappropriate referent at all.
In the latter case, subject evenundertake attempts to repair an otherwise uninterpretabledescription by relaxing descriptors.
In the following, weinterpret some of these findings for our model of uncer-tainty, including a model of a repair mechanism.3 Representing UncertaintiesBasically, our model of uncertainty combines the three kindsof uncertainty described in the previous section.
Each ofthem is expressed in terms of a probability, associated witha triple consisting of an object, an attribute applicable tothat object, and the value ascribed to this pair.
The follow-ing probabilities each express the likelihood that the userrecognizes a description correctly from the perspectives of:pK The user is acquainted with the terms mentionedpP The user can perceive the properties utteredpA The user agrees to the applicability of the terms usedIn order to identify an intended referent successfully, allthree factors must be assessed positively, so that theprobability of recognition p becomes the product of thesethree probabilities.
Since the individual properties refer tofactors outside the scope of proper generation, we only dealwith p in the scope of this paper, although it is clear thatthis assessment requires contributions from several sources.The concept of using individual probabilities to representmanifestations of uncertainty is not only simple, it also fitsto knowledge sources where data about these probabilitiescould be found.
For example, user models, the potentialsources for assessing pK, typically assign assessments touser capabilities on the basis of belief networks.
Similarconsiderations hold for representations of vague properties,which fall under the concept of term agreement.
Theseproperties can be modeled by fuzzy logic systems [Zadeh1984, 1996], which allow for an interpretation in terms of asingle probability value, representing the likelihood that aprecise value is perceived as a given vague term.The association of a probability with the applicability ofa descriptor to an object not only expresses the somehowdirect likelihood of success of this task, but the applicationof this likelihood to several candidate objects also gives anindication of the likelihood of success of the overall identi-fication goal.
If a descriptor is assumed to be associated withseveral candidate objects by the audience, with certaindegrees typically different among these objects, several casescan be distinguished: (1) correct identification, where theaudience relates the description only to those objects towhich this descriptor indeed applies, (2) misinterpretation,where none of these objects, but others are associated withthe descriptor by the audience, (3) ambiguity, which is acombination of (1) and (2), and, finally, (4) the case of anuninterpretable description, where the audience does notrelate the descriptor to any of the candidate objects.
In thelast case, people are known to make an attempt to repairtheir unsuccessful interpretation, since they assume that theexpression communicated is indeed intended to refer to someobject or objects in the domain of discourse, according to thework by Goodman.
In order to simulate the effect of thisbehavior, we compute the probability of the occurrence of anuninterpretable description, which we call the repairfactor, and we increase the probability of identification ofthe candidate objects (which we call our repair mechanism),based on the amount of the repair factor and the context ofthese objects in the overall identification task.R F f f( , , , ) ( ( ( , , ), )| ( , , )|)k p p j m n p j m nn iinm jnmjk1101?
===??????=?
??
?f(j,m,n) recursively enumerates all combinations of m out ofn elements (here: natural numbers 1 ?
n) and returns the j-thcombination as a set of numbers M={i1,?,im} with 1?ik?n(k= 1,?,m)F(M,p ) p(1 p )i Mi Miii=????????
?ififFigure 1.
Repair factor for insufficient recognition of k objectsIn concrete terms, if we have n objects for which a descriptoris recognized with probability pi for object i, the probabilitythat none of the objects is recognized by a referringexpression built from that descriptor is ?
(1-pi), (1 ?
i ?
n).Although this number tends to be small if there are severalobjects to which the description matches with some reason-able degree of confidence, the associated need for invoking arepair mechanism becomes increasingly urgent when furtherdescriptors are added to the description built so far, as well aswhen the task is to identify multiple referents rather than asingle one.
For the case of 2 objects, the need for invoking arepair mechanism can be quantified by the repair factor2?i=1,n(1-pi)+?i=1,n(pi?j=1,n(j?i)(1-pj)).
The general case, ifneeded, gets increasingly complex, as illustrated in Figure 1,for k objects to be identified, out of n candidates (k ?
n).Thus, for the likelihood of recognition failure, amechanism is required that simulates identification repairunder these conditions.
Apart from the likelihood of failure,repair should be guided by potential confusability of objectsin view of some given descriptor.
Hence, while we think itis virtually impossible to confuse an animal and a piece ofequipment, at least under any reasonable conditions of visibi-lity, we assume that objects of some degree of appearancesimilarity (size and shape) may potentially be confused witheach other.
Hence, we consider a potentially confusableobject a candidate for being interpreted as an intended referentin case a repair of a reference failure is required.
Confusion inthis sense may be interpreted in two ways: from theperspective of the speaker, those objects are candidates whichthe speaker thinks the hearer could confuse.
From the hearersperspective, those objects are candidates which the hearerthinks the speaker might have confused in producing a badlyinterpretable description.
Since the latter constellationcorresponds to the situation present for repair attempts, wemodel potential candidates quasi ?objectively?
by incorpor-ating annotations in the knowledge base.
The dependency ofuser capabilities as assessed by a user model influences theseassessments indirectly through the probability of recognitionattributed to the user for each descriptor-object pair.Determine probability of identification (D, k, O1, ?, On)O1, ?, Om Objects to which descriptor D appliesOm+1, ?, On Objects to which repair with D is applicablepi, ?, pm Probability that D is recognized for OiObjects ordered along degrees of recognition confidence:?i,j(1 ?
i, j ?
m): (pi > pj) ?
(i > j)Rprop ?
R(k, p1, ?, pm), i ?
11. if (i?m)then Rc ?
Min(Rprop/n, 1-pi), p-idi ?
pi+Rcelse Rc ?
Rprop/n, p-idi ?
Rcendifif (i<n)then i ?
i+1, Rprop ?
Rprop - Rc, goto 1endifFigure 2.
Assessing identification probabilities including repairIn order to keep the repair mechanism simple, we approxi-mate confusability of an object by augmenting its represen-tation with annotations of all property-value combinationsthat do not apply to it, but which could somehow beperceived as holding for this object.
The potentially largeamount of data created this way can be significantly reducedby making use of inheritance.
For example, one can statethat blue and purple (physical) objects can be confused, bymaking annotations about confusability with blue for purpleobjects, and vice-versa.
This annotation is then inherited toall entities that are specializations of (physical) objects.The proper repair is then simulated by collecting allcandidates to which the descriptor in question could arguablyapply, and by assigning these candidates a probability ofidentification through repair , according to the repair factor,as assessed above.
There are two kinds of candidates: (1)those to which the descriptor is recognized with someprobability, and (2) those to which it could apply with somerelaxation, that is, which contains a suitable confusabilityannotation.
The repair factor, which is computed accordingto the schema in Figure 1, is then evenly distributed amongthese two sets of candidates, provided the added probabilitiesof recognition and repair do not get greater than 1 for someobject; this can only be the case if the number of  objects toidentify is close to the number of candidates.
In such a case,the extra amount is distributed recursively among theremaining candidates, always respecting the upper limit of 1.If the number of objects to identify even exceeds the numberof candidates, the effect of the repair mechanism results in amodification of the number of objects to identify, reducing itto the number of available candidates.
The computation ofthe probability of identification through repair is illustratedin Figure 2.
Three examples in Figure 3 illustrate the effectof the repair mechanism in quantitative terms.
They empha-size the relation between expectations about the number ofobjects to be identified and probabilities of identification.For k objects to be identified out of n, judging identifi-cation by descriptor D, which may involve repair measures(D applies to m out of these n with probabilities pi,?,pm)1. k=1, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.12p-id1 = 0.83, p-id2 = 0.43, p-id3 = p-id4 = 0.032. k=2, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.96p-id1 = 1, p-id2 = 0.6533, p-id3 = p-id4 = 0.25333. k=3, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 2.1p-id1 = 1, p-id2 = 1, p-id3 = p-id4 = 0.65Figure 3.
Examples of assessing identification probabilitiesSpecifically, the increasing contributions of the repairfacility are shown, which will be even more pronounced withseveral attributes associated with limited recognition expect-ations.
We will see this effect in context with buildingdescriptor combination in the next section, as well as in thedetailed exposition of an example in Appendix II.4 Identifiability of Descriptor CompositionsSince a single descriptor is rarely sufficient for identifyingone or several objects in scenarios of interesting complexity,boolean compositions of descriptors are generated for thispurpose, conjunctions being required for building identifyingexpressions for single objects.
Their probability of recog-nition is a simple extension of the case of single descriptors.If pi is the probability of recognition of descriptor D i forsome object O , an expression consisting of several D i(i=1,pn) is identified with O through recognition if all Di  areattributed to O.
The probability of this coincidence amountsto the product of all probabilities ?pi (i=1,pn).The probability of identification through repair iscomputed by distributing the repair factor R(k,P1,?,Pm),where each Pj=?pji ( j=1,m;i=1,pn), among all objects quali-fying for the repair measure.
While this distribution is anequal one for the case of a single descriptor, apart from usingthe upper limit of 1 for the total probability, such an evendistribution would not do full justice here.
We propose todistribute the likelihood proportionally to the probabilities ofrecognition for each descriptor, which makes repair morelikely applicable to those objects which are also more likelyto be identified anyway.
In order to perform this operationproperly, ?average?
probabilities (ap) for only reparabledescriptors must be estimated.
Moreover, we want to favorrepairs for objects which require fewer ?average?
probabilitiesfor this computation, by incorporating a "scale-down factor?
(sdf) for each additional repair.
The computation schema isgiven in Figure 4.
For concrete computations, we choose 0.5for both factors ap and sdf ?
see the examples in Figure 5.The first one demonstrates the partitioning of the repairfactor according to the number of attributes which requirerepair.
Specifically, the first three objects get the same shareof the repair factor, while the fourth object gets only half ofit,  since  its identification  is  the  only one  which  requiresCompute identification probability (D1,?,Dnp,k,O1,?,On)O1,?,Om Objects to which all D1,?,np are applicableOm+1,?,On Objects with repair possible for all D1,?,nppi1,?,pinp Probability that D1,?,np is attributed to OiObjects ordered along degrees of identification confidence:?i,j(1?i,j?m): (?l=1,nppil > ?l=1,nppjl) ?
(i > j)for i from 1 to n doPi ?
1, sdfi ?
1/sdffor j from 1 to np doif  pij > 0then Pi ?
Pipijelse Pi ?
Piap, sdfi ?
sdfisdfendifendforendforRprop ?
R(k,Pi,?,Pm), i ?
1, P ?
?i=1,nPi1.
if (i?m)then Rc ?
Min(Rprop(Pi/P),1-Pi), p-idi ?
Pi+Rcelse Rc ?
Rprop(Pisdfi/P), p-idi ?
Rcendifif (i<n)then i ?
i+1, Rprop ?
Rprop - Rc, goto 1endifFigure 4.
Identification probabilities for several descriptorsrepair regarding two descriptors.
The second example featuresthe impact of multiple intended referents on the repair factor,which increases the probabilities of identification substan-tially.
The last example illustrates the compensative effectbetween comparably low probabilities of recognition andhigher ones in connection with the requirement of using therepair facility.
Specifically, this example demonstrates thatthe probability of identification for an object (the secondone) that is only identifiable through the repair mechanismcan even become higher than the probability of identificationfor an object (the second one) that does not require repair forbeing identified.
However, such an effect is only possible inthe context of descriptors applicable with some degree ofconfidence to both candidates, but strongly favoring theobject whose identification relies on the repair mechanismdue to mismatch with another descriptor.
This is the mostcritical effect in choosing descriptors.The incorproation of disjunctions and negations is morelocal, since this extension only generalizes the probabilityof recognition of a single property.
This is because theseoperators appear only in embedded boolean combinations[van Deemter 2002], which are the basis for building largervarieties of expressions [Horacek 2004].
For disjunctions oftwo descriptors with associated probabilities p1 and p2, thejoint probability amounts to p1+p2-p1p2, assuming indepen-dence, which is quite normal for descriptors originating fromFor k objects to be identified out of n, judging identifi-cation by np descriptors D, at least repair possible for all(Dj applies to object i with probability pji, ?i?m: pji > 0)1.  k=1, n=4, m=1, np=2 (p11 =0.5, p21 =0.5, p12 =0.5,p22 =0, p13 =0, p23 =0.5, p14 =0, p24 =0): Rprop = 0.75p-id1 = 0.464, p-id2 = 0.214, p-id3 = 0.214, p-id4 = 0.1072. k=2, n=3, m=1, np=2 (p11 =0.5, p21 =0.6, p12 =0.6,p22 =0.5, p13 =0, p23 =0.55): Rprop = 1.4p-id1 = p-id2 = 0.766, p-id3 = 0.4663. k=1, n=2, m=1, np=3 (p11 =0.5, p21 =0.5, p31 =0.5,p12 =0.9, p22 =0.9, p32 =0): Rprop = 0.875p-id1 = 0.331, p-id2 = 0.668Figure 5.
Examples of assessing identification probabilitiesdistinct properties.
For some properties, prominently thoseassociated with vagueness, building disjunctions ofdescriptors originating from the same property may bebeneficial.
For example, disjunctions of similar colors orshapes may reduce the uncertainty through combining theidentifiability of both.
A simple way to model this constel-lation is by assigning probabilities to the set of applicablevalues so that their sum does not exceed 1, thereby modelingexclusion of the co-occurrence of more than one value.Consequently, the associated probabilities can simply beadded.
Propagation of the ?confusable?
annotation is treatedsimilarly ?
if at least one of the descriptors is marked as?confusable?, this also holds for the disjunction.
For dealingwith negation, the probability of identification is simplyinverted (1-p).
The treatment of the ?confusable?
annotation,however, is a bit problematic.
The invertion operation needsmodification through anticipating the amount of the repairfactor, but this cannot be done locally.
Therefore, this factor,rf, must be estimated in advance.
For concrete computationswe use a value of 0.1, so that ?p for a ?confusable?
pamounts to 0.9.5 An Algorithm Incorporating UncertaintiesIn this section, we describe extensions to the algorithm byDale and Reiter [1995] that take into account the measuresaddressing uncertainty introduced in previous sections.
Thisreference algorithm takes an intended referent r (the generali-zation to several referents is straightforward), the attributes Pthat describe r, and a contrast set C, and incrementally buildsan identifying description L, if possible.
The algorithmassumes an environment with three interface functions:BasicLevelValue, accessing basic level categories of objects[Rosch 1978], MoreSpecificValue for accessing incremen-tally specialized values of an attribute according to a taxo-nomic hierarchy, and UserKnows for judging whether theuser is familiar with the attribute value of an object.The algorithm basically iterates over the attributes P,according to some predetermined ordering which reflectspreferences in the domain of application.
For each attributein P, a value assumed to be known to the user is determined,so that this value describes the intended referent and rules outat least one potential distractor which is still in the contrastset C in the iteration step considered.
If such a value can befound, a pair consisting of the attribute and this value isincluded in the identifying description L .
This step isrepeated until the list P is exhausted or a distinguishingdescription is found, that is, the contrast set C  is empty.Unless the distinguishing description L does not contain adescriptor expressible as a head noun, such a descriptor isadded.
Choosing the value of an attribute is done by anembedded iteration.
It starts with the basic level value  attri-buted to r, after which more specific values also attributed tor and assumed to be known to the user are tested for theirdiscriminatory power.
Finally, the least specific value thatexcludes the largest number of potential distractors and isknown to the user is chosen.
The schema of this procedureis given in Appendix I.
The only modification we have doneto the original version is the result of L as a non-distin-guishing description in case of identification failure.The algorithm by Dale and Reiter contains the principaloperations that also other algorithms for generating referringexpressions apply.
The extension to boolean combinationsof descriptors by van Deemter is essentially realized as aniteration around the Dale and Reiter algorithm, throughbuilding increasingly complex combinations, which othercontrol regimes generate and maintain more effectively.In order to control effects of facilities dealing with uncer-tainty, the extended algorithm has four control parameters:?
pmin, the minimal probability of recognition requiredfor an attribute-value pair applicable to the intendedreferent, to justify its inclusion in the description,?
?p1, the minimal improvement in terms of probabi-lity of identification of the intended referent over apotential distractor obtained through an additionalattribute-value pair,?
?p2, the minimal preference in terms of probabilityof identification of the intended referent over all poten-tial distractors obtained through a description, and?
Complexity-limit, an upper bound on the number ofdescriptors collected in the distinguishing description.In order to incorporate our concepts of representinguncertainty in this algorithm, we have to replace the inter-face functions which access crisp data and we must modifyyes-no decisions.
These enhancements concern:?
the decision about whether a descriptor excludes apotential distractor (in the function RulesOut),?
the choice of a value for an attribute (in the functionFindBestvalue), and?
the termination of the overall procedure (in thefunction MakeReferringExpression)Modifications of the reference algorithm are given indetail in the extended version in Appendix I ?
some lines aremarked by labels [Ni] for references from the text.Expressions of the form pr(r,L) compute the probability ofidentification of referent r  through the description L ,according to the schema described in the previous sections.Under conditions of uncertainty, determining whether adescriptor excludes a potential distractor may become aproper decision rather than a mere computation.
A clear-cutcase is only present if the repair facility is not applicable toone of the members of the contrast set, so that its associatedprobability of identification amounts to 0.
This conditionreplaces the criterion that the user must know that thisdescriptor does not apply to some potential distractor in thefunction RulesOut [N7].
However, it would be a ratherrestrictive strategy to accept only those descriptors whichdefinitely exclude a potential distractor.
In fact, none of thedescriptors that make up the example in Appendix II yieldsuch a crisp discrimination.
In addition to that, a descriptor isalso valuable if it contributes to a better identification of theintended referent by increasing the difference to a potentialdistractor in the associated probabilities of identification by asignificant margin (?p1).
This criterion is added to the crispcriterion described above, encapsulated in the function Domi-nate [N8], which is used for this decision instead of thefunction RulesOut [N2].
The idea is that subsequently chosendescriptors have comparable effects on the identification ofsome of the other potential distractors, so that the intendedreferent ultimately gains over all of them.
The significanceof this margin must be tuned in such a way that the gainover some potential distractors is not outweighted by a lossover some other potential distractors.The suitability of a value for an attribute depends on twofactors associated with uncertainty: the probability of recog-nition associated with that value for the present user, and theeffect of this value on excluding elements from the set ofpotential distractors.
These two factors have adverse effects:while a more specific value has the potential of excluding anincreasing number of potential distractors, its probability ofrecognition when applied to the intended referent may belower than that of a less specific value.
Consequently, it isnot necessarily the case that an improved discriminatorypower leads to a better overall effect.
Hence, the choice of avalue requires a minimal probability of recognition (p min ,[N6]), and calls to Dominate replace calls to RulesOut.
Addi-tional variants of descriptors can be generated by enhancingthe interface function MoreSpecificValue, also buildingdisjunctions of values excluding each other, to cover casesdescribed at the end of Section 4, that is, buildingdisjunctions of descriptors by composing descriptors(possibly vague ones) that cover adjacent value ranges.The third factor, the termination criterion, is adapted touncertainties by enhancing it in two ways: (1) a complexitylimit is applied to the specifications in the description L[N3]; while this cut-off may serve practical considerationsalso without conditions of uncertainty (for a partitioning intosequences of descriptions [Horacek 2004]), it gains on rele-vance in uncertain environments.
(2) a certain degree of beingDominant  in the probability of identification over allpotential distractors is considered sufficient (?p2, [N4]) ratherthan requiring the ultimate exclusion of all potentialdistractors.
Finally, the conditions under which descriptorsare selected, give rise to an optional optimization step.
Theprerequisite for this step is the distinction betweendescriptors which definitively exclude at least one potentialdistractor (Lro in the extended algorithm, [N1]) and otherswhich only affect their associated probabilities of identifi-cation, but do not make them 0.
Then all subsets of thedescription built which contain at least Lro are examined[N5] whether they yield a better preference over all potentialdistractors in terms of their probabilities of identification[N9].
Through this measure, an early chosen descriptor witha probability of identification lower for the intended referentthan for some potential distractors can finally be discarded,provided the discriminating effect on other potentialdistractors is also achieved by later chosen descriptors.
In theexample in Appendix II, all descriptors are categorized asoptional ones, but for the one expressing the head noun ?which is precisely the reason why it is not optional.Altogether, the algorithm selects descriptors which eitherexclude some potential distractors definitively, makes someof them rely on the repair mechanism, or simply increasesthe probability of identification of the intended referentconsiderably in comparison to elements of the contrast set.While this selection process works reasonably in mostcases, it may turn out as problematic when several of thedescriptors chosen are associated with limited probabilitiesof recognition for the intended referent in comparison topotential distractors not completely excluded.
As a conse-quence, these potential distractors may be judged superior interms of the probability of identification even though theyrely on the repair mechanism (see example 3 in Figure 5).This risk can be circumvented by using a relatively highpmin parameter, but this measure may easily lead to theexclusion of an otherwise beneficial descriptor under normalconditions.
An improvement can be obtained by the call tothe procedure Optimize.
If one of the first two descriptorsused in example 3 in Figure 5 does not definitively exclude apotential distractor, the procedure Optimize tests descriptorcombinations without it, and one of those may yield a betterresult ?
see also the example in Appendix II.
A possiblevariations would be to allow just a single violation of thepmin restriction, for a descriptor with very good discrimi-natory power.So far, we have only elaborated changes for incorpo-rating uncertainty concepts to the reference algorithm per se.Handling boolean combinations of descriptors throughapplying the reference algorithm to increasingly complexcombinations also works with uncertainties, since allcomputations required are defined.
More difficulties arisewith ambitious control regimes, which rely on cut-offtechniques, in addition to the complexity cut-off, such asdominance and value cut-offs, as introduced in [Horacek2004].
A complexity cut-off is already included in theextended reference algorithm.
The two other cut-offs can begeneralized, but this is likely to be associated with asignificant loss of efficiency.
In order for a descriptor todominate another one, the dominating one must not onlyexclude all potential distractors that its competitor does, butit must also favor the intended referents over all potentialdistractors in terms of the associated  probabilities of identi-fication ?
this requirement reduces the application frequencyof this cut-off considerably.
A value cut-off, in turn, isapplicable to a partial solution if a solution has already beenfound, and there are no descriptor combinations untested forthe partial solution which may yield a solution with lesscomplex specifications.
This condition can also be met inthe environment associated with uncertainties.
In thisenvironment, however, there is another factor that has animpact on the quality of the solution, that is the probabilityof identification, which cannot be assessed prior to actuallychoosing a descriptor and testing its effects.6 ConclusionIn this paper, we have presented an approach for generatingreferential descriptions under conditions of uncertainty.
Theapproach combines a proper recognition of objects associatedwith some degree of uncertainty, as well as identificationthrough a repair mechanism, motivated by the need toidentify objects even for descriptions that originally appearuninterpretable.
On these lines, we have reinterpretedconcepts of algorithms generating referring expressions inview of uncertainties about the appearance of objects.Incorporating measures of uncertainty in such an algorithmattacks strong assumptions and effects underlying most ofthe existing algorithms:?
They typically require crisp specifications concerningattribution of descriptors to referents and knowledge ofthe audience.
Especially the connection to modern usermodels may require coarse-grained interpretations here.?
A single result is produced even if several reasonablevariants exist, and this choice is implicitly determinedby the preference ordering imposed on the descriptors.?
The interaction with other components of an NL gener-ation system and an embedding dialog system is ratherlimited.
Reference generation is typically conceived asa pure functional service, with no feedback, taking intoaccount syntactic constraints, at best (e.g., [Horacek1997]).
An embedding dialog system has no chance tofind out possible sources for an identification failure.The algorithm incorporating measures to deal with uncer-tainties provides facilities to improve this situation:?
Specifications concerning attribution of descriptors toreferents and knowledge of the audience can be done ina direct fashion, requiring no interpretations.?
There are some parameters to control the choice ofdescriptors, the conciseness and expected effectivenessof the result, including an afterwards optimizationwhich only requires re-calculation of probabilities.?
The probabilities of identification associated with theintended referents and those potential distractors thatfall under the repair facility give an indication aboutthe likelihood of success of the identification task andalso about potential sources for a failure.
Moreover,the situation about probabilities and descriptors maysuggest variants in building surface expressions, suchas putting emphasis on a critical descriptor.References[Appelt 1985] Doug Appelt.
Planning English ReferringExpressions.
Artificial Intelligence 26:1-33, 1985.
[Appelt and Kronfeld 1987] Doug Appelt and AmichaiKronfeld.
A Computational Model of Referring .
In Proc.of the 10th International Joint Conference on ArtificialIntelligence (IJCAI-87), pp.
640-647, Milano, Italy,1987.
[Bateman 1999] John Bateman.
Using Aggregation forSelecting Content when Generating ReferringExpressions.
In Proc.
of the 37th Annual Meeting of theAssociation for Computational Linguistics (ACL-99),pp.
127-134, University of Maryland, 1999.
[Dale 1988] Robert Dale.
Generating Referring Expressionsin a Domain of Objects and Processes .
PhD Thesis,Centre for Cognitive Science, University of Edinburgh,1988.
[Dale and Reiter 1995] Robert Dale and Ehud Reiter.Computational Interpretations of the Gricean Maxims inthe Generation of Referring Expressions.
CognitiveScience 18:233-263, 1995.
[Donellan 1966] K. Donellan.
Reference and DefiniteDescription.
Philosophical Review 75:281-304, 1966.
[Edmonds 1994] Phil Edmonds.
Collaboration on Referenceto Objects that are not Mutually Known .
In Proc.
of the15th International Conference on Computational Lingu-istics (COLING-94), pp.
1118-1122, 1994.
[Gardent 2002] Claire Gardent.
Generating Minimal DefiniteDescriptions.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL-2002), pp.
96-103, Philadelphia, Pennsylvania, 2002.
[Goodman 1986] Bradley Goodman.
Reference Identificationand Reference Identification Failures .
ComputationalLinguistics 12:273-305, 1986.
[Goodman 1987] Bradley Goodman.
Communication andMiscommunication .
Association of ComputationalLinguistics Series of Cambridge University Press,London, England, 1987.
[Grosz and Sidner 1986] Barbara Grosz and Candace Sidner.Attention, Intention, and the Structure of Discourse.Computational Linguistics 12:175-206, 1986.
[Heeman and Hirst 1995] Peter Heeman and Graeme Hirst.Collaborating on Referring Expressions .
ComputationalLinguistics 21:351-382, 1995.
[Horacek 1997] Helmut Horacek.
An Algorithm forGenerating Referential Descriptions with Flexible Inter-faces.
In Proc.
of the 35th Annual Meeting of theAssociation for Computational Linguistics and 8thConference of the European Chapter of the Associationfor Computational Linguistics (ACL-EACL'97), pp.206-213, Madrid, Spain, 1997.
[Horacek 2003] Helmut Horacek.
A Best-First SearchAlgorithm for Generating Referring Expressions.
InProc.
of the 10th Conference of the European Chapterof the Association for Computational Linguistics(EACL-2003), Conference Companion (short paper), pp.103-106, Budapest, Hungary, 2003.
[Horacek 2004] Helmut Horacek.
On Referring to Sets ofObjects Naturally.
In Proc.
of the Third InternationalConference on Natural Language Generation (INLG-2004), pp.
70-79, Brockenhurst, UK, 2004.
[Krahmer, v. Erk and Verleg 2001] Emiel Krahmer, S. v.Erk, Andr?
Verleg.
A Meta-Algorithm for the Generationof Referring Expressions.
In Proc.
of the 8th EuropeanWorkshop on Netural Language Generation (EWNLG-2001), pp.
29-39, Toulouse, France, 2001.
[Kronfeld 1986] Amichai Kronfeld.
Donellan's Distinctionand a Computational Model of Reference.
In Proc.
of  the24th Annual Meeting of the Association for Compu-tational Linguistics (ACL-86), pp.
186-191, New York,NY, 1986.
[Levelt 1989] William Levelt.
Speaking: From Intention toArticulation.
MIT Press, 1989.
[McDonald 1981] David McDonald.
Natural Language Gener-ation as a Process of Decision Making under Constraints.PhD thesis, MIT, 1981.
[Pearl 1988] Judea Pearl.
Probabilistic Reasoning in Intel-ligent Systems: Networks of Plausible Inferences .Morgan Kaufman, San Mateo, California, 1988.
[Pechmann 1989] Thomas Pechmann.
Incremental SpeechProduction and Referential Overspecification .
Linguistics27:89-110, 1989.
[Reiter 1990] Ehud Reiter.
The Computational Complexityof Avoiding Conversational Implicatures.
In Proc.
of the28th Annual Meeting of the Association for Compu-tational Linguistics (ACL-90), pp.
97-104, Pittsburgh,Pennsylvania, 1990.
[Reiter and Dale 1992] Ehud Reiter and Robert Dale.Generating Definite NP Referring Expressions .
In Proc.of the 14th International Conference on ComputationalLingustics (COLING-92), pp.
232-238, Nantes, France,1992.
[Rosch 1978] Eleanor Rosch.
Principles of Categorization.In E. Rosch and B. Llyod  (eds.)
Cognition and Catego-rization, pp.
27-48, Hillsdale, NJ: Lawrence Erlbaum,1978.
[Stone 2000] Matthew Stone.
On Identifying Sets.
In Proc.of the First International Conference on Natural Langu-age Generation (INLG-2000), pp.
116-123, MitzpeRamon, Israel, 2000.
[van Deemter 2002] Kees van Deemter.
Generating ReferringExpressions: Boolean Extensions of the IncrementalAlgorithm.
Computational Linguistics , 28 (1):37-52,2002.
[Wahlster 2004] Wolfgang Wahlster.
REAL: REssourcen-Adaptive Lokalisation .
Project in SFB 378, SaarlandUniversity, 2004.
[Zadeh 1984] Lofti Zadeh.
Making Computers Think likePeople.
IEEE Spektrum, 8:26-32, 1984.
[Zadeh 1996] Lofti Zadeh.
Fuzzy Logic and the Calculi ofFuzzy Rules and Fuzzy Graphs.
International Journal ofMulti-Valued Logic, 8:1-39, 1996.Appendix I: Reference Algorithm ([Dale and Reiter 1995], left) and Extended Algorithm (right)MakeReferringExpression (r,C,P)L ?
{}for each member Ai of list P doV = FindBestValue(r,Ai,BasicLevelValue(r,Ai))if RulesOut(<Ai,V>) ?
nilthen L ?
L ?
{<Ai,V>}C ?
C - RulesOut(<Ai,V>)endifif C = {} thenif <type,X> ?
L for some Xthen return L (an identifying description)else return L ?
{<type,BasicLevelValue(r,type)>}endifendifreturn L (a non-identifying description)FindBestValue (r,A,initial-value)if UserKnows(r,<A,initial-value>) = truethen value ?
initial-valueelse value ?
no-valueendifif (spec-value ?
MoreSpecificValue(r,A,value)) ?
nil ^(new-value ?
FindBestValue(r,A,spec-value)) ?
nil ^(|RulesOut(<A,new-value>)| > |RulesOut(<A,value>)|)then value ?
new-valueendifreturn valueRulesOut (A,V)if V = no-valuethen return nilelse return {x: x ?
C ^ UserKnows(x,<A,V>) = false}endifMakeReferringExpression (r,C,P)L ?
{}, Lro ?
{} [N1]for each member Ai of list P doV = FindBestValue(r,Ai,BasicLevelValue(r,Ai))if Dominate(Ai,V) ?
nil [N2]then L ?
L ?
{<Ai,V>}C ?
C - RulesOut(<Ai,V>)if RulesOut(<Ai,V>) ?
nilthen Lro ?
Lro ?
{<Ai,V>}endif  endifif (C = {}) ?
(|L| > Complexity-limit) ?
[N3](?x ?
C: (pr(r,L) - pr(x,L)) > ?p2) then [N4]L ?
Optimize(L,Lro) (optional) [N5]if <type,X> ?
L for some Xthen return L (a likely identifying description)else return L ?
{<type,BasicLevelValue(r,type)>}endif  endifreturn L (an unlikely identifying description)FindBestValue (r,A,initial-value)if pr(r,{<A,initial-value>}) > pmin [N6]then value ?
initial-valueelse value ?
no-valueendifif (spec-value ?
MoreSpecificValue(r,A,value)) ?
nil ^(new-value ?
FindBestValue(r,A,spec-value)) ?
nil ^(|Dominate(A,new-value)| > |Dominate(A,value)|)then value ?
new-valueendifreturn valueRulesOut (A,V)if V = no-value then return nilelse return {x: x ?
C ^ (pr(x,L ?
{<A,V>}) = 0)} [N7]endifDominate (A,V)if V = no-value then return nilelse return {x: x ?
C ^ ((pr(x,L ?
{<A,V>}) = 0) ?
((pr(r,L ?
{<A,V>}) - pr(x,L ?
{<A,V>})) - [N8](pr(r,L) - pr(x,L))) > ?p1)}endifOptimize (L1,L2)Lopt ?
L1, ppopt ?
Min?x ?
C(pr(r,L1) - pr(x,L1)) [N9]forall L (L1 ?
L ?
L2) dopp ?
Min?x ?
C(pr(r,L) - pr(x,L))if ppopt < pp then Lopt ?
L, ppopt ?
pp endifendforallreturn LoptAppendix II: An Example of the Extended Algorithm at WorkIn this section, we demonstrate the functionality of theextended algorithm by an example that illustrates severalfeatures of this algorithm.
As in the discussion in Section2, the scenario consists of three similar dogs, one of whichis a bassett, which is also the intended referent.
In addition,the bassett is brownish and has a long tail.
The other twodogs have shorter tails and their skin is also brownish, butwith some white resp.
black portions, which makes thedescriptor 'brownish' less appropriate than for the bassett.To make the example suitable for our purposes, theaudience is assumed to have little knowledge about dogspecifics, that is, they may recognize the intended referentas a bassett, but this is not very likely (we assume thecategory identification has a likelihood of 30%).
Inaddition, it is assumed that the tails of the dogs, specific-ally the one of the bassett, cannot be observed easily by theaudience (again, we assume the recognition has a likelihoodof 30%).
Both, dog category and tail length are potentiallyconfusable for all dogs.
These properties and associatedprobabilities of identification as listed below.Objects Attributescategory color tail-lengthdog1 bassett brownish longdog2 dog brown-white shortdog3 dog brown-black shortProbabilities of identification (per attribute-value and object)bassett: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0brownish: p(dog1) = 0.9, p(dog2) = 0.8, p(dog3) = 0.8long-tail: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0Hence, the intended referent r is {dog1}, and the contrast setC is {dog2, dog3}.
For demonstration purposes, we choosethe following parameterizations:?
The attributes are considered according to the orderedpreference list P = (color, category, tail-length), whichin some sense reflects the ease of perception of color?
We choose 5% (0.05) for ?p1, which indicates suffi-cient dominance, and 30% (0.3) for ?pmin, whichindicates sufficient identification potenial (it alwayssucceeds in the example); similarly, we choose 50%(0.5) for ?p2 which indicates sufficient discrimination(it never succeeds here, hence all descriptors are tried)?
We allow a maximum complexity of 3 descriptors, sothat this cut-off criterion does not apply in our simpleexample, and we use the optional optimization step?
In order to compute probabilities of identification, weneed to choose a values for the ?scale-down factor?,which will be 0.5, as mentioned in Section 4.
Sinceno disjunctions and negations of descriptors are neededfor our example, no further parameters are required.We now illustrate the generation process step by step.In the first step, ?brownish?
is chosen as the value of theattribute ?color?
of dog1, and its contribution to discriminatethe intended referent from the elements of the contrast set ischecked.
To start with, its  identification potential, 0.9, is farhigher than pmin (0.25).Since this descriptor is also applic-able to both other dogs, dog2 and dog3, but with lowerprobability, these two objects still remain in the contrast set.Despite this limited discrimination, the attribute is chosenbecause it achieves more than the minimal dominancerequired: 0.9 - 0.8 equals 0.1, which is higher than ?
p 2(0.05).
Thus, the situation after step 1 is as follows (weneglect the repair factor, which is as low as 0.1 x 0.2 x 0.2):pr(dog1) = 0.9, pr(dog2) = pr(dog3) = 0.8?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.1 > ?p1In the next step, ?bassett?
is chosen as the value of theattribute ?category?
of dog1, and again its contribution todiscriminate the intended referent from the elements of thecontrast set is checked.
Its  identification potential, 0.3, issufficient, since it is higher than pmin (0.25).
This descriptoris not applicable to the other dogs, dog2 and dog3.
Never-theless, they still remain in the contrast set since they poten-tially are subject to the repair mechanism.
The probabilitiesof identification are computed according to the schema inFigure 4: the product of the probabilities of "bassett" and"brownish" associated with dog1 yields 0.27.
The repairfactor, the complementing 0.73, is distributed evenly amongall three dogs.
Hence, the degree of dominance of thisdescriptor amounts to 0.27, which is higher than ?p2 (0.05).Thus, the situation after step 2 is as follows:pr(dog1) = 0.513, pr(dog2) = pr(dog3) = 0.243?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.27 > ?p1In the last step, ?long?
is chosen as the value of the attri-bute ?tail-length?.
Again, its  identification potential, 0.3,is sufficient, since it is higher than pmin (0.25).
As in theprevious step, dog2 and dog3 still remain in the contrast setsince they potentially are subject to the repair mechanism.The product of the probabilities for dog1 results from theprevious one, multiplied by 0.3, which yields 0.081.
Therepair factor, the complementing 0.919, is distributed bygiving two parts to dog1 (the scale-down factor applies) andone part to each of the other dogs.
Hence, the degree ofdominance of this descriptor amounts to 0.31075, which ishigher than ?p2 (0.05), which gives the final situation:pr(dog1) = 0.5405, pr(dog2) = pr(dog3) = 0.22975?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.31075 > ?p1Optimization attempts show that ?bassett?
only is slightlyinferior (pr(dog1) = 0.53, pr(dog2) = pr(dog3) = 0.23, ?pr =0.3), while ?bassett?
together with ?long-tailed?
is slightlysuperior (pr(dog1) = 0.545, pr(dog2) = pr(dog3) = 0.225, ?pr= 0.32) to the combination of all descriptors.
Hence, theexample demonstrates benefits and risks are comparable whenonly limited discrimination is possible by each descriptor.
