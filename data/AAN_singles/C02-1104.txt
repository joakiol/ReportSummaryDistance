From Shallow to Deep Parsing Using Constraint SatisfactionJean-Marie BALFOURIER, Philippe BLACHE & Tristan VAN RULLENLaboratoire Parole et Langage29,  Avenue Robert Schuman13621 Aix-en-Provence, France{balfourier, blache, tristan.vanrullen}@lpl.univ-aix.frAbstractWe present in this paper a technique allowing tochoose the parsing granularity within the sameapproach relying on a constraint-based formalism.Its main advantage lies in the fact that the samelinguistic resources are used whatever thegranularity.
Such a method is useful in particularfor systems such as text-to-speech that usually needa simple bracketing, but in some cases requires aprecise syntactic structure.
We illustrate thismethod in comparing the results for three differentgranularity levels and give some figures abouttheir respective performance in parsing a taggedcorpus.IntroductionSome NLP applications make use of shallowparsing techniques (typically the ones treatinglarge data), some others rely on deep analysis(e.g.
machine translation).
The respectivetechniques are quite different: the former usuallyrelies on stochastic methods where the later usessymbolic ones.
However, this can constitute aproblem for applications relying on shallowparsing techniques and needing in someoccasions deep analysis.
This is typically thecase for text-to-speech systems.
Suchapplications usually rely on shallow parsers inorder to calculate intonative groups on the basisof syntactic units (or more precisely on chunks).But in some cases, such a superficial syntacticinformation is not precise enough.
One solutionwould then consist in using a deep analysis forsome constructions.
No system existsimplementing such an approach.
This is inparticular due to the fact that this would requiretwo different treatments, the second one redoingthe entire job.
More precisely, it is difficult toimagine in the generative framework how toimplement a parsing technique capable ofcalculating chunks and, in some cases, phraseswith a possible embedded organization.We present in this paper a formalism relying onconstraints that constitutes a possible answer tothis problem.
This approach allows the use of asame linguistic resource (i.e.
a unique grammar)that can be used fully or partially by the parser.This approach relies on the fact that (1) alllinguistic information is represented by means ofconstraints and (2) the constraints are of regulartypes.
The idea consists then in implementing atechnique that can make use of some constraintsin the case of shallow parsing, and the entire setof them for deep analysis.
In our formalism,constraints are organized into different types.Tuning the granularity of the parse consists thenin selecting the types of constraints to beverified.In the first part of this paper, we present theproperty grammar formalism, its mainadvantages both in terms of representation andimplementation.
In the second part, we describethe parsing technique and the differentapproaches used for shallow and deep parsing.We address in particular in this section somecomplexity aspects illustrating the properties ofthe parsing techniques and we propose anevaluation over a corpus.
In the third part, weillustrate the respective characteristics of thedifferent approaches in describing for the sameexample the consequences of tuning the parsegranularity.
We conclude in presenting someperspectives for such a technique.1 Property GrammarsThe notion of constraints is of deep importancein linguistics, see for example Maruyama(1990), Pollard (1994), Sag (1999).
Recenttheories (from the constraint-based paradigm tothe principle and parameters one) rely on thisnotion.
One of the main interests in usingconstraints comes from the fact that it becomespossible to represent any kind of information(very general as well as local or contextual one)by means of a unique device.
We present in thissection a formalism, called Property Grammars,described in B?s (1999) or Blache (2001), thatmakes it possible to conceive and represent alllinguistic information in terms of constraintsover linguistic objects.
In this approach,constraints are seen as relations between two (ormore) objects: it is then possible to representinformation in a flat manner.
The first step inthis work consists in identifying the relationsusually used in syntax.This can be done empirically and we suggest,adapting a proposal from B?s (1999), the  set offollowing constraints: linearity, dependency,obligation, exclusion, requirement anduniqueness.
In a phrase-structure perspective allthese constraints participate to the description ofa phrase.
The following figure roughly sketchestheir respective roles, illustrated with someexamples for the NP.Constraint DefinitionLinearity (<) Linear precedence constraintsDependency (?)
Dependency relations betweencategoriesObligation (Oblig) Set of compulsory and uniquecategories.
One of these categories(and only one) has to be realized in aphrase.Exclusion () Restriction of cooccurrence betweensets of categoriesRequirement (?)
Mandatory cooccurrence betweensets of categoriesUniqueness (Uniq) Set of categories which cannot berepeated in a phraseIn this approach, describing a phrase  consists inspecifying a set of constraints over somecategories that can constitute it.
A constraint isspecified as follows.
Let R a symbolrepresenting a constraint relation between two(sets of) categories.
A constraint of the form a Rb stipulates that if a and b are realized, then theconstraint a R b must be satisfied.
The set ofconstraints describing a phrase can berepresented as a graph connecting severalcategories.The following example illustrates someconstraints for the NP.Linearity Det <  N;  Det <  AP;AP <  N; N < PPRequirement N[com] ?
DetExclusion N  Pro; N[prop]  DetDependency Det ?
N; AP ?
N; PP ?
NObligation Oblig(NP) = {N, Pro, AP}In this description, one can notice for example arequirement relation between the common nounand the determiner (such a constraintimplements the complementation relation) orsome exclusion that indicate cooccurrencyrestriction between a noun and a pronoun or aproper noun and a determiner.
One can noticethe use of sub-typing: as it is usually the case inlinguistic theories, a category has severalproperties that can be inherited when thedescription of the category is refined (in ourexample, the type noun has two sub-types,proper and common represented in feature basednotation).
All constraints involving a noun alsohold for its sub-types.
Finally, the dependencyrelation, which is a semantic one, indicates thatthe dependent must combine its semanticfeatures with the governor.
In the same way asHPSG does now with the DEPS feature asdescribed in Bouma (2001), this relationconcerns any category, not necessarily thegoverned ones.
In this way, the differencebetween a complement and an adjunct is thatonly the complement is selected by arequirement constraint, both of them beingconstrained with a dependency relation.
Thisalso means that a difference can be donebetween the syntactic head (indicated by theoblig constraint) and the semantic one (thegovernor of the dependency relation), even if inmost of the cases, these categories are the same.Moreover, one can imagine the specification ofdependencies within a phrase between twocategories other than the head.One of the main advantages in this approach isthat constraints form a system and all constraintsare at the same level.
At the difference of otherapproaches as Optimality Theory, presented inPrince (1993), there exists no hierarchy betweenthem and one can choose, according to theneeds, to verify the entire set of constraints or asubpart of it.
In this perspective, using aconstraint satisfaction technique as basis for theparsing strategy makes it possible to implementthe possibility of verifying only a subpart of thisconstraint system.
What is interesting is thatsome constraints like linearity provideindications in terms of boundaries, as describedfor example in Blache (1990).
It follows thatverifying this subset of constraints can constitutea bracketing technique.
The verification of moreconstraints in addition to linearity allows torefine the parse.
In the end, the same parsingtechnique (constraint satisfaction) can be usedboth for shallow and deep parsing.
Moreprecisely, using the same linguistic resources(lexicon and grammar), we propose a techniqueallowing to choose the granularity of the parse.2 Two techniques for parsingProperty GrammarsWe describe in this paper different parsingtechniques, from shallow to deep one, with thisoriginality that they rely on the same formalism,described in the previous section.
In otherwords, in our approach, one can choose thegranularity level of the parse without modifyinglinguistic resources2.1 Shallow parsingIn this technique, we get hierarchical andgrammatical information while preservingrobustness and efficiency of the processing.
Inthis perspective, we make use of a grammarrepresented in the Property Grammar formalismdescribed above.
One of the main interests ofthis formalism is that it doesn't actually makeuse of the grammaticality notion, replacing itwith a more general concept of characterization.A characterization simply consists in the set ofthe constraint system after evaluation (in otherwords the set of properties that are satisfied andthe set of properties that are not satisfied).
Acharacterization only formed with satisfiedproperties specifies a grammatical structure.
Inthis sense, characterization subsumesgrammaticality.
It becomes then possible topropose a description in terms of syntacticproperties for any kind of input (grammatical ornot).
Opening and closing chunks relies here oninformation compiled from the grammar.
Thisinformation consists in the set of left and rightpotential corners, together with the potentialconstituents of chunks.
It is obtained incompiling linear precedence, requirement andexclusion properties described in the previoussections together with, indirectly, that ofconstituency.The result is a compiled grammar which is usedby the parser.
Two stacks, one of openedcategories and a second of closed categories, arecompleted after the parse of each new word: wecan open new categories or close already openedones, following some rules.
This algorithmbeing recursive, the actions opening, continuingand closing are recursive too.
This is the reasonwhy rules must have a strict definition in orderto be sure that the algorithm is deterministic andalways terminates.
This shallow parsingtechnique can be seen as a set ofproduction/reduction/cutting rules.?
Rule 1: Open a phrase p for the currentcategory c if c can be the left corner of p.?
Rule 2: Do not open an already openedcategory if it belongs to the current phrase or isits right corner.
Otherwise, we can reopen it ifthe current word can only be its left corner.?
Rule 3: Close the opened phrases if the morerecently opened phrase can neither continueone of them nor be one of their right corner.?
Rule 4: When closing a phrase, apply rules 1, 2and 3.
This may close or open new phrasestaking into consideration all phrase-levelcategories.2.2 Deep parsingDeep analysis is directly based on propertygrammars.
It consists, for a given sentence, inbuilding all the possible subsets of juxtaposedelements that can describe a syntactic category.A subset is positively characterized if it satisfiesthe constraints of a grammar.
These subsets arecalled edges, they describe a segment of thesentence between two positions.At the first step, each lexical category isconsidered as an edge of level 0.
The next phaseconsists in producing all the possible subsets ofedges at level 0.
The result is a set of edges oflevel 1.
The next steps work in the same wayand produce all the possible subsets of edges,each step corresponding to a level.
Thealgorithm ends when no new edge can be built.An edge is characterized by:?
an initial and a final position in the sentence,?
a syntactic category,?
a set of syntactic features?
a set of constituents: a unique lexicalconstituent at the level 0, and one or severaledges at the other levels.After parsing, a sentence is considered asgrammatical if at least one edge coveringcompletely the sentence and labelled by thecategory S is produce.
But even forungrammatical cases, the set of edges representsall possible interpretations of the sentence: theset of edges contains the set of constraints thatdescribe the input.
By another way, in case ofambiguity, the parser generates several edgescovering the same part and labelled with thesame category.
Such similar edges are distinctby their syntactical features (in the case of anambiguity of features) or by their differentconstituents (typically an ambiguity ofattachment).Several heuristics allow to control the algorithm.For example, an edge at level n must contain atleast an edge at level n-1.
Indeed, if it wouldcontain only edges at levels lower than n-1, itshould have been already produced at the leveln-1.The parse ends in a finite number of steps at thefollowing conditions:?
if the number of syntactic categories of thegrammar is finite,?
if the grammar does not contain a loop ofproduction.
We call loop of production, theeventuality that a category c1 can beconstituted by an unique category c2, itselfconstituted by an unique category c3 and sountil cn and that one of category c2 to cn can beconstituted by the unique category c1.3 Compared complexityOf course, the difference of granularity of thesealgorithms does have a cost which has to beknown when choosing a technique.In order to study their complexity, we parsed afrench corpus of 13,236 sentences (from thenewspaper Le Monde), tagged by linguists (theCLIF project, headed by Talana).3.1 Shallow parsing with Chinks andChunksWith the aim of comparing our techniques, wefirst built a simple robust chunker.
This quickprogram gives an idea of a bottom complexityfor the two techniques based on propertygrammars.
This algorithm relies on theLiberman and Church?s Chink&Chunktechnique (see Liberman & Church (1992)) andon Di Cristo?s chunker (see Di Cristo (1998) andDiCristo & al (2000)).
Its mechanism consists insegmenting the input into chunks, by means of afinite-state automaton making use of functionwords as block borders.
An improvement of thenotion of chunk is implemented, usingconjunctions as neutral elements for chunksbeing built.
This algorithm constitutes aninteresting (and robust) tool for example as basisfor calculating prosodic units in a Text-to-Speech Synthesizer.
Chink/Chunk algorithm is asimple but efficient way to detect syntacticboundaries.
In the average, best and worst cases,for M sentences, each sentence consisting of Nwwords, its complexity has an order ofM*Nw*Constant.
That is to say a linearcomplexity.Instructions / number of wordsfor Chink & Chunk (logarithmic scale)3.2 Shallow parsing with PGWith the shallow parser algorithm, we can detectand label more syntactic and hierarchic data: inthe average, worst and best cases, for Msentences, each sentence consisting of Nwwords; for a set of C precompiled categories, itscomplexity has an order ofM*C*(Nw?+Nw)*Constant.
That is to say apolynomial complexity.1001000100000 20 40 60 80 100 120 140Instructions / number of wordsfor Shallow Parser (logarithmic scale)3.3 Deep parsing with PGFor the evaluation of the deep parser algorithm,we parsed a corpora of 620 sentences of thesame corpus.
Unlike the two previousalgorithms, the dispersal of results is much moreimportant.Million instructions / number of wordsfor Deep Parser (logarithmic scale)In the theory, the algorithm is of exponentialtype but its progress is permanently constrainedby the grammar.
This control being heavilydependent from the grammatical context, thenumber of instructions necessary to parse twosame size sentences can be very different.Nevertheless, in the reality of a corpus, theaverage complexity observed is of polynomialtype.
So, if Nw is the number of words of asentence, the best estimate complexity of itsparse corresponds to a polynomial of order 2.4(Nw2.4*Constant).3.4 Remarks on complexityOur study considers the parser complexity as afunction of two parameters:- the size of the parsed sentence,- the number of grammatical categories.This complexity is relies on the number of?simple instructions?
treated by the programs.Comparing the average complexity of eachparser is then a good way to know which one isfaster.
Ranking techniques can then be extractedfrom the results.
It would have been interestingto compare them in terms of maximalcomplexity, but this is not actually possiblebecause of an important difference between thetwo first parsers which are deterministic, and thelast one which is not:- for the first two techniques, the minimal,average and maximal complexities arepolynomial,- the deep parser has an exponential maximalcomplexity and polynomial minimal andaverage complexities.Moreover, the study of the maximal complexityof the deep parser has to be treated as anotherproblem.
Usually, such a study must have to bedone taking care of the size of the grammar.
Butwith property grammars, other parameters haveto be used: a property grammar is a set ofconstraints (Linearity, Dependency, Obligation,Exclusion, Requirement, Uniqueness) belongingto two different groups.
In a formal terminology,these groups are ?reduction constraints?
and?increasing constraints?.
These groupscharacterize the behavior of the parser, as aformal system would do for a recognitionproblem.
?Increasing constraints?
allow theinstanciation of the search space, where?reduction constraints?
allow pruning this space.Most of the sentences are fastly parsed becauseof their well-formedness: the reductionconstraints are more frequently used thanincreasing ones in such sentences.
Ambiguousand ill-formed sentences require a greater use ofincreasing constraints.Thus, the size of the grammar is less informativeabout the theoretical complexity than the relativeimportance of increasing and reductionconstraints: for instance a greater grammar, with0,11,010,0100,01 000,00 10 20 30 40 50 60 70 80 90 10010001000010000010000000 20 40 60 80 100 120 140more reduction constraints would have a lowertheoretical complexity.
The study of such aproblem does not belong to this study because itwould lead us to a different experimentation.4 Different resultsOur parsers demonstrate the possibility of avariable granularity within a same approach.
Weillustrate in this section the lacks and assets ofthe different techniques with the example below(in French):"Le compositeur et son librettiste ont sucr?er un ?quilibre dramatique astucieux enmariant la com?die espi?gle voire ?grillardeet le drame le plus profond au c?ur desm?mes personnages.
"?The composer and his librettist successfullyintroduced an astute dramatic balance in marryingthe mischievous, ribald comedy with the deepestdrama for the same characters.
?4.1 Chink/chunk approach[(sentence)[(chunk)Le compositeur et son librettisteont su cr?er][(chunk)un ?quilibre dramatique astucieux][(chunk)en mariant][(chunk)la com?die espi?gle][(chunk)voire ?grillarde][(chunk)et le drame][(chunk)le plus profond][(chunk)au coeur des m?mes personnages]]This first example shows a non-hierarchicalrepresentation of the sentence, divided intochunks.
No linguistic information is given.4.2 Shallow parsing approach[(sentence)[(NP)Le compositeur[(AP) et]son librettiste][(VP)ont su cr?er][(NP) un ?quilibre[(AP)dramatique astucieux]][(Compl)en[(VP)mariant]][(NP)la com?die[(AP)espi?gle voire ?grillarde et]le drame[(Sup)le plus profond]][(PP)au c?ur de[(NP)les[(AP)m?mes]personnages]]]This second example gives a hierarchicalrepresentation of the sentence, divided intogrammatically tagged chunks.
Because we useda precompiled version of the grammar(shortened) and because we forced somesyntactic choices in order to keep a deterministand finishing parsing, it appears that some errorshave been made by the shallow parser:Conjunctions are (badly) distinguished asAdverbial Phrases.
In spite of these gaps, cuttingis improved and most of the categories aredetected correctly.4.3 Deep parsing approachThe last example (next figure) presents two ofthe maximum coverages produced by the deepparser.
This figure, which illustrates the PPattachment ambiguity, only presents forreadabiulity reasons the hierarchical structure.However, remind the fact that each labelrepresents in fact a description which the state ofthe constraint system after evaluation.Le compositeur et son librettiste ont su cr?er un ?quilibre dramatique astucieux en mariant la com?die espi?gle voire ?grillarde et le drame le plus profond au_c?ur des m?mes personnagesSNP                          VPNP conj     NP V V V         NPdet N  det N    det N adj adjPP                 PPprep VP                NP             prep     NPV             NP     conj     NP      det adj Ndet N   AP    det N   SupAP conj AP    det adv adjadj  adjPPprep VP                 NPV             NP     conj           NPdet N   AP    det N   Sup       PPAP conj AP    det adv adj prep     NPadj  adj        det adj N5 ConclusionThe experiments presented in this paper showthat it is possible to calculate efficiently thedifferent kind of syntactic structures of asentence using the same linguistic resources.Moreover, the constraint-based frameworkproposed here makes it possible to choose thegranularity, from a rough boundary detection toa deep non-deterministic analysis, via a shallowand deterministic one.
The possibility ofselecting a granularity level according to thedata to be parsed or to the targetted applicationis then very useful.An interesting result for further studies lies inthe perspective of combining or multiplexingdifferent approaches.
It is for exampleinteresting to notice that common boundariesobtained by these algorithms eliminates ill-formed and least remarkable boundaries.
At thesame time, it increases the size of the blockswhile maintaining the linguistic informationavailable (this remains one of the most importantproblems for text-to-speech systems).
Finally, itallows to propose a parameterized granularity inbalancing the relative importance of differentcompeting approaches.ReferencesAbney, S. (1991) "Parsing by chunks"., in Berwick,R., Abney, S., Tenny, C.
(eds.).
Principle-basedParsing, Kluwer Academic Publishers, 257-278.Abney S. (1996) "Partial Parsing via Finite-StateCalculus'', in proceedings of ESSLLI'96 RobustParsing Workshop.Abney, S. (1997) "Part-of-speech tagging and partialparsing", in Young, S., Bloothooft, G. Corpus-Based Methods in Language and SpeechProcessing, Kluwer Academic Publishers, 118-136.Allen, J., Hunnincutt, S., Carlson, R., Granstr?m, B.
(1979) "MITalk-79 : The 1979 MIT text-to-speechsystem", in Wolf and Klatt (eds.
), SpeechCommunications, Papers Presented at the 97thMeeting of the ASA: 507-510.Allen, J., Hunnincutt, S., Klatt, D. (1987) "From textto speech: The MITalk system", CambridgeUniversity Press.B?s G. & P. Blache (1999) "Propri?t?s et analysed'un langage'', in proceedings of  TALN'99.Blache P. & J.-Y.
Morin (1990) "Bottom-upFiltering: a Parsing Strategy for GPSG", inproceedings of COLING'90.Blache P. & J.-M. Balfourier (2001) "PropertyGrammars: a Flexible Constraint-Based Approachto Parsing'', in proceedings of IWPT-2001.Bouma G., R. Malouf & I.
Sag (2001) "SatisfyingConstraints on Extraction and Adjunction'', inNatural Language and Linguistic Theory, 19:1,Kluwer.Chanod J.-P. (2000) "Robust Parsing and Beyond'', inRobustness in Language Technology, Kluwer.Di Cristo P, (1998).
G?n?ration automatique de laprosodie pour la synth?se ?
partir du texte.
Ph.D.thesis, Universit?
de Provence, France.Di Cristo A., Di Cristo P., Campione E, Veronis J,(2000).
A prosodic model for text to speechsynthesis in French.Duchier D. & R. Debusmann (2001) "TopologicalDependency Trees: A Constraint-Based Account ofLinear Precedence'', in proceedings of  ACL.Grinberg D., J. Lafferty & D. Sleator (1995), Arobust parsing algorithm for link grammars, CMU-CS-95-125, Carnegie Mellon University.K?bler S. & E. Hinrichs (2001) "From Chunks toFunction-Argument Structure: A similarity-BasedApproach'', in proceedings of ACL-01.Liberman, M., Church, K. (1992) "Text analysis andword pronunciation in text-to- speech synthesis", inFurui, S., Sondhi, M.M.
(eds), Advances in SpeechSignal Processing, Dekker, 791-831.Maruyama H. (1990), "Structural Disambiguationwith Constraint Propagation'', in proceedings ofACL'90.Pollard C. & I.
Sag (1994), Head-driven PhraseStructure Grammars, CSLI, Chicago UniversityPress.Prince A.
& P. Smolensky (1993) Optimality Theory:Constraint Interaction in Generative Grammars,Technical Report RUCCS TR2, Rutgers Center forCognitive Science.sSag I.
& T. Wasow (1999), Syntactic Theory.
AFormal Introduction, CSLI
