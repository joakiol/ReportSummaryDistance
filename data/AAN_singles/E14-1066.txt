Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 626?635,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Generative Model for User Simulation in a Spatial Navigation DomainAciel Eshky1, Ben Allison2, Subramanian Ramamoorthy1, and Mark Steedman11School of Informatics, University of Edinburgh, UK2Actual Analytics Ltd., Edinburgh, UK{a.eshky,s.ramamoorthy,steedman}@ed.ac.ukballison@actualanalytics.comAbstractWe propose the use of a generative modelto simulate user behaviour in a novel task-oriented dialog domain, where user goalsare spatial routes across artificial land-scapes.
We show how to derive an effi-cient feature-based representation of spa-tial goals, admitting exact inference andgeneralising to new routes.
The use ofa generative model allows us to capturea range of plausible behaviour given thesame underlying goal.
We evaluate intrin-sically using held-out probability and per-plexity, and find a substantial reduction inuncertainty brought by our spatial repre-sentation.
We evaluate extrinsically in ahuman judgement task and find that ourmodel?s behaviour does not differ signif-icantly from the behaviour of real users.1 IntroductionAutomated dialog management is an area of re-search that has undergone rapid advancement inthe last decade.
The driving force of this innova-tion has been the rise of the statistical paradigmfor monitoring dialog state, reasoning about theeffects of possible dialog moves, and planning fu-ture actions (Young et al., 2013).
Statistical di-alog management treats conversations as MarkovDecision Processes, where dialog moves are as-sociated with a utility, estimated online by inter-acting with a simulated user (Levin et al., 1998;Roy et al., 2000; Singh et al., 2002; Williams andYoung, 2007; Henderson and Lemon, 2008).
Slot-filling domains have been the subject of most ofthis research, with the exception of work on trou-bleshooting domains (Williams, 2007) and rela-tional domains (Lison, 2013).Although navigational dialogs have receivedmuch attention in studies of human conversationalbehaviour (Anderson et al., 1991; Thompson etal., 1993; Reitter and Moore, 2007), they have notbeen the subject of statistical dialog managementresearch, and existing systems addressing naviga-tional domains remain largely hand crafted (Ja-narthanam et al., 2013).
Navigational domainspresent an interesting challenge, due to the dispar-ity between the spatial goals and their groundingas utterances.
This disparity renders much of thestatistical management literature inapplicable.
Inthis paper, we address this deficiency.We focus on the task of simulating user be-haviour, both because of the important role sim-ulators plays in the induction of dialog managers,and because it provides a self-contained means ofdeveloping the domain representations which fa-cilitate dialog reasoning.
We show how a genera-tive model of user behaviour can be induced fromdata, alleviating the manual effort typically in-volved in the development of simulators, and pro-viding an elegant mechanism for reproducing thenatural variability observed in human behaviour.1.1 Spatial Goals of UsersUsers in task-oriented domains are goal-directed,with a persistent notion of what they wish to ac-complish from the dialog.
In slot-filling domains,goals are comprised of a group of categorical en-tities, represented as slot-value pairs.
These en-tities can be placed directly into the user?s utter-ance.
For example, in a flight booking domain, ifa user?s goal is to fly to London from New Yorkon the 3rdof November, then the goal takes theform: {origin=?New York?, dest=?London?, de-part date=?03-11-13?
}, and expressing the desti-nation takes the form: Provide dest=?London?.In contrast, consider the task of navigatingsomebody across a landscape.
Figure 1 shows apair of maps taken from a spatial navigation do-main, the Map Task.
Because the Giver aims tocommunicate their route, one can view the route626Natural Language Semantic RepresentationG: you are above the camera shop Instruct POSITION(ABOVE, LM)F : yeah AcknowledgeG: go left jus?
just to the side of the paper, ?
Instruct MOVE(TO, PAGE LEFT) ?then south, Instruct MOVE(TOWARDS, ABSOLUTE SOUTH)under the parked van  Instruct MOVE(UNDER, LM) you have a parked van?
Query-ynF : a parked van no Reply-nG: you go?
you just go west, ?
Clarify MOVE(TOWARDS, ABSOLUTE WEST) ?and down, Clarify MOVE(TOWARDS, ABSOLUTE SOUTH)and then you go along to the?
you go east  Clarify MOVE(TOWARDS, ABSOLUTE EAST) F : south then east CheckG: yeah Reply-yTable 1: A Giver (G) and a Follower (F ) alternating turns in a dialog concerning the maps in Figure 1.The utterances are shown in natural language (left), and the semantic equivalent (right), which is com-posed of Dialog Acts and SEMANTIC UNITS.
Utterances marked ?
demonstrate a plausible variability inexpressing the same part of the route on the Giver?s map, and similarly those marked .
We model theGiver?s behaviour, conditioned on the Follower?s, at the semantic level.as the Giver?s goal for the dialog.
However, unlikegoals in slot-filling domains, it is unclear whetherthe route can be represented categorically in aform that would allow the giver to communicateit by placing it directly into an utterance.
As rawdata, a specific route is represented numerically asa series of pixel coordinates.
Before modelling in-terlocutors in this domain, we must derive a mean-ingful representation for the spatial goals, and thendevise a mechanism that takes us from the spatialgoals to the utterances which express them.1.2 Utterance Variability for the Same GoalIn addition to making sensible utterances, a con-cern for user simulation is providing plausiblevariability in utterances, to provide dialog man-agers with realistic training scenarios.
Considerthe dialog in Table 1, resulting from the maps inFigure 1.
Utterances marked ?
(and similarly thosemarked ) illustrate how the same route can bedescribed in different ways, not only at the natu-ral language level, but also at the semantic level1.A model providing a 1-to-1 mapping from spatialroutes to semantic utterances would fail to capturethis phenomenon.
Instead, we need to be able toaccount for plausible variability in expressing theunderlying spatial route as semantic utterances.1Route descriptor TOWARDS indicates a movement in thedirection of the referent ABS WEST, whereas TO indicates amovement until the referent is reached.Giver FollowerFigure 1: In the Map Task, the instruction Giver?stask is to communicate a route to a Follower,whose map may differ.
The route can be seen asthe Giver?s goal which the Follower tries to infer.A corresponding dialog is shown in Table 1.1.3 Overview of ApproachIn order to perform efficient reasoning, we pro-pose a new feature-based representation of spatialgoals, transforming them from coordinate space toa low-dimensional feature space.
This groups sim-ilar routes together intelligently, permitting exactinference, and generalising to new routes.
To ad-dress the problem of variability of utterances giventhe same underlying route, we learn a distributionover possible utterances given the feature vectorderived from a route, with probability proportionalto the plausibility of the utterance.Because this domain has not been previouslyaddressed in the context of dialog management oruser simulation, there is no directly comparableprior work.
We thus conduct several novel evalu-627ations to validate our model.
We first use intrinsicinformation theoretic measures, which computethe extent of the reduction in uncertainty broughtby our feature-based representation of the spatialgoals.
We then evaluate extrinsically by gener-ating utterances from our model, and comparingthem to held-out utterance of real humans in thetest data.
We also utilise human judgements forthe task, where the judges score the output of thedifferent models and the human utterances basedon their suitability to a particular route.2 Related Work2.1 Related Work on the Map TaskTo our knowledge, there are no attempts to modelinstruction Givers as users in the Map Task do-main.
Two studies model the Follower, in the con-text of understanding natural language instructionsand interpreting them by drawing a route (Levitand Roy, 2007; Vogel and Jurafsky, 2010).
Bothstudies exclude dialog from their modelling.
Al-though their work is not directly comparable toours, they provide a corpus suitable for our task.2.2 Related Work on User SimulationEarly user simulation techniques are based on N-grams (Eckert et al., 1997; Levin and Pieraccini,2000; Georgila et al., 2005; Georgila et al., 2006),ensuring that simulator responses to a machine ut-terance are sensible locally.
However, they do notenforce user consistency throughout the dialog.Deterministic simulators with trainable parame-ters mitigate the lack of consistency using rules inconjunction with explicit goals or agendas (Schef-fler and Young, 2002; Rieser and Lemon, 2006;Pietquin, 2006; Ai and Litman, 2007; Schatzmannand Young, 2009).
However, they require largeamounts of hand crafting and restrict the variabil-ity in user responses, which by extension restrictsthe access of the dialog manager to potentially in-teresting states.
An alternative approach to dealingwith the lack of consistency is to extend N-gramsto explicitly model user goals and condition utter-ances on them (Pietquin, 2004; Cuay?ahuitl et al.,2005; Pietquin and Dutoit, 2006; Rossignol et al.,2010; Rossignol et al., 2011; Eshky et al., 2012).3 The ModelOur task is to model the Giver?s utterances in re-sponse to the Follower?s, at the semantic level.
AGiver?s utterance takes the form:g = Instruct, u = MOVE (UNDER, LM)consisting of a dialog act g and a semantic unit u2.Aligned with u, is an ordered set of waypoints W ,corresponding only to part of the route u describes.Figure 2(a) shows an example of such a sub-route.The point-setW can be seen as the Giver?s currentgoal on which they base their behaviour.
Becausethe routes are drawn on the Giver?s maps, we treatW as observed.To model some of the interaction between theGiver and the Follower, we additionally considerin our model the previous dialog act of the Fol-lower, which could for example be:f = AcknowledgeGiven point-set W and preceding Follower actf , as the giver, we need to determine a procedurefor choosing which dialog act g and semantic unitu to produce.
In other words, we are interested inthe following distribution:p (g, u|f,W ) (1)which says that, as the Giver, we select our utter-ances on the basis of what the Follower says, andon the set of waypoints we next wish to describe.To formalise this idea into a generative model,we assume that the Giver act g depends only onthe Follower act f .
We further assume that the se-mantic unit u depends on the set of waypoints Wwhich it describes, and on the Giver?s choice ofdialog act g. Thus, u and f are conditionally in-dependent given g. This provides a simple way ofincorporating the different sources of informationinto a complete generative model3.
Using Bayes?theorem, we can rewrite Equation (1) as:p (g, u|f,W ) =p(u) p(g|u) p(f |g) p(W |u)?g?u?p(u?)
p(g?|u?)
p(f |g?)
p(W |u?
)(2)requiring four distributions: p(u), p(g|u), p(f |g),and p(W |u).
The first three become the seman-tic component of our model, to which we dedi-cate Section 3.1.
The fourth is the spatio-semanticcomponent, to which we dedicate Sections 3.2?3.4.2We align g and u in a preprocessing step, and store thenames of landmarks which the units abstract away from.3Further advancements to this work would investigate theeffects of relaxing the conditional independence assumption.6283.1 The Semantic ComponentThe semantic component concerns only the cate-gorical variables, f , g, and u, and addresses howthe Giver selects their semantic utterances basedon what the Follower says.
We model the distri-butions u, g|u, and f |g from Equation (2) as cate-gorical distributions with uniform Dirichlet priors:u ?
Cat(?)
?
?
Dir() (3a)g|u ?
Cat(?)
?
?
Dir(?)
(3b)f |g ?
Cat(?)
?
?
Dir(?)
(3c)We use point estimates for ?, ?
and ?, fixing themat their posterior means in the following manner:?
?gu= p(g|u) =Count(g, u) + 1?g?Count(g?, u) + L(4)and similarly for ??
and ??
(L = size of vector ?
).3.2 Spatial Goal AbstractionEach ordered point-set W on some given map canbe seen as the Giver?s current goal, on which theybase their behaviour.
Let W = {wi; 0 ?
i < n},where wi= (xi, yi) is a waypoint, and xi, yiarepixel coordinates on the map, typically obtainedthrough a vision processing step.Given this goal formulation, from Equation (2)we require p(W |u), i.e.
the probability of a setof waypoints given a semantic unit.
However,there are two problems with deriving a generativemodel directly over W .
Firstly, the length of Wvaries from one point-set to the next, making ithard to compare probabilities with different num-bers of observations.
Secondly, deriving a modeldirectly over x, y coordinates introduces sparsityproblems, as we are highly unlikely to encounterthe same set of coordinates multiple times.
Wethus require an abstraction away from the space ofpixel coordinates.Our approach is to extract feature vectors offixed length from the point-sets, and then derive agenerative model over the feature vectors insteadof the point-sets.
Feature extraction allows point-sets with similar characteristics, rather than exactpixel values, to give rise to similar distributionsover units, thus enabling the model to reason givenpreviously unseen point-sets.
The features we ex-tract are detailed in Section 3.4.3.3 The Multivariate Normal DistributionLet M be an unordered point-set describing mapelements, such as landmark locations and mapboundary information.
M = {mj; 0 ?
j < k},where mj= (xj, yj) is a map element with pixelcoordinates xjand yj.
We define a spatial featurefunction ?
: W,M ?
Rnwhich captures, as fea-ture values, the characteristics of the point-set Win relation to elements in M .
Let the spatial fea-ture vector, extracted from the point-setW and themap elements M , be:v = ?
(W,M) (5)Figure 2(b) illustrates the feature extraction pro-cess.
We now define a distribution over the featurevector v given the semantic unit u.
We model v|uas a multivariate normal distribution (recall that vis in Rn):v|u ?
N (?u,?u) (6)where ?
and ?
are the mean vectors and covari-ance matrices respectively.
Subscriptuindicatesthat there is one such parameter for each unit u.Since the alignments between units u and point-sets W are fully observed, parameter estimationis a question of estimating the mean vectors ?u?and the covariance matrices ?u?from the point-sets co-occuring with unit u?.
We use maximumlikelihood estimators.
To avoid issues with de-generate covariance matrices resulting from smallamounts of data, we consider diagonal covariancematrices.
Because v|u is normally distributed, in-ference, both for parameters and conditional distri-butions over units, can be performed exactly, andso the model is exceptionally quick to learn andperform inference.3.4 The Spatial Feature SetsWe derive four feature sets from the ordered point-set W , while considering the map elements in theunordered point-set M :1.
Absolute features capture directions and dis-tances of movement.
We compute the distancebetween the first and last points inW , and com-pute the angle between unit vector <0,-1> andthe line connecting first and last points in W2.
Polynomial features capture shapes of move-ments as straight lines or curves.
We computethe mean residual of a degree one polynomialfit to the points in W (linear), and a degree twopolynomial (quadratic)44These features are computed quickly and efficiently, re-quiring only the solution to a least squares problem.629ugvf?u?u?????
?u = MOVE(UNDER, LM)w6w0w6w0M=W=v =w0w1:wnm0m1:mk(a) sub-route aligned with u (c) the model(b) spatial feature extractionFigure 2: (a) At training time, a Giver?s semantic unit u is aligned with an ordered point-set W , repre-senting a sub-route.
(b) We extract a spatial feature vector v of fixed length, from point-sets W and Mof varying lengths.
(c) We define a generative model of the Giver, over Giver act g and semantic unit u,preceding Follower act f , and spatial feature vector v. Latent parameters and priors are shown.3.
Landmark features capture how close theroute takes the Follower to the nearest land-mark.
We compute the distance between theend-point inW and the nearest landmark inM ,and compute the angle between the route takenin W and the line connecting the start point into the nearest landmark4.
Edge features capture the relationship betweenthe movement and the map edges.
We computethe distance from the start-point in W to thenearest edge and corner inM , and similarly forthe end-point in W3.5 The Complete Generative ModelOur complete generative model of the Giver is adistribution over Giver act g and semantic unit u,given the preceding Follower act f and the spatialfeature vector v. Vector v is the result of apply-ing the feature extraction function ?
over W andM , where W is the ordered point-set describingthe sub-route aligned with u, and M is the point-set describing landmark locations and map edgeinformation.
We rewrite Equation (2) as:p (g, u|f, v) =p(u) p(g|u) p(f |g) p(v|u)?g?u?p(u?)
p(g?|u?)
p(f |g?)
p(v|u?
)(7)We call our model the Spatio-Semantic Model,SSM, and depict it in Figure 2(c).4 Corpus Statistics and State SpaceWe conduct our experiments on the Map Task cor-pus (Anderson et al., 1991), a collection of cooper-ative human-human dialogs arising from the taskexplained in Figure 1 and Table 1.
The originalcorpus was labelled with dialog acts, such as Ac-knowledge and Instruct.
The semantic units canbe obtained through a semantic parse of the nat-ural language utterances, while the spatial infor-mation can be obtained through vision processingof the maps.
We use an existing extension of thecorpus by Levit and Roy (2007), which is seman-tically and spatially annotated.
The spatial anno-tation are x, y pixel coordinates of landmark loca-tions and evenly spaces points on the routes.
All15 maps were annotated.
The semantic units takethe predicates MOVE, TURN, POSITION, or ORI-ENTATION, and two arguments: a route descrip-tor and a referent.
The semantic annotations wererestricted to the Giver?s Instruct, Clarify, and Ex-plain acts.
Out of the original 128 dialogs, 25 weresemantically annotated.For our experiments, we use all 15 pairs ofmaps, and all 25 semantically annotated dialogs.A dialog on average contain 57.5 instances, wherean instance is an occurrence of f , g, u, and W .We find 87 unique semantic units u in our data,however, according to the semantic representation,there can be 456 distinct possible values for u5.As for the rest of the variables, f takes 15 values,g takes 4, and v is a real-valued vector of length10, extracted from the real valued sets W and Mof varying lengths.
We thus reason in a semanticstate space of 87?
15?
4 = 5220, and an infinitespatial state space.5 Intrinsic EvaluationOur first evaluation metric is an information theo-retic one, based on notion that better models findnew instances of data (not used to train them) to bemore predictable.
One such metric is the probabil-ity a model assigns to the data, (higher is better).
A520?2 for TURN and ORIENTATION, + 208?2 forMOVE and POSITION.630second metric is perplexity, which computes howsurprising a model finds the data (lower is better).Both metrics have been used to evaluate user simu-lators in the literature (Georgila et al., 2005; Eshkyet al., 2012; Pietquin and Hastie, 2013).
We com-pute the per-utterance probability of held-out data,instead of the per-dialog probability, since the lat-ter was deemed incompatible across dialogs of dif-ferent lengths by Pietquin and Hastie (2013).
Per-plexity is 2?log2(d)where d is the probability of theinstance in question.
We evaluate using leave-one-out validation, which estimates the model from allbut one dialog, then evaluates the probability ofthat dialog.
We repeat this process until all dialogshave been evaluated as the unseen dialog.Because we evaluate on held-out dialogs, weneed to be able to assign probabilities to pre-viously unseen instances.
We therefore smoothour models (at training time) by learning a back-ground model which we estimate from all thetraining data.
This results in high variance in thedistribution over features and a flat overall dis-tribution.
Where no model can be estimated fora particular semantic unit, we use that semanticunit?s smoothed prior probability combined withthe background model for its likelihood.We first consider the suitability of the differ-ent feature sets for predicting utterances.
Fig-ure 4 shows the mean per-utterance probability ourmodel assigns to held-out data when using differ-ent sets.
The more predictable the model finds thedata, the higher the probability.
Note that the tar-get metric here is not 1, as there is no single cor-rect answer.
It can be seen that the most success-ful features in order of predictiveness are: Abso-lute, then Polynomial, then Landmark, and finallyEdge.
The combination of all buys us further im-provement.
Perplexity is shown in Table 2.Secondly, we consider two baselines inspired bysimilar approaches of comparison in the literature(Eckert et al., 1997; Levin and Pieraccini, 2000;Georgila et al., 2005).
Both are variants of ourmodel that lack the spatial component, i.e.
they arenot goal-based.
Although the baselines are weak,they allow us to measure the reduction in uncer-tainty brought by the introduction of the spatialcomponenet to our model, which is the purposeof this comparison.
Baseline 1 is p(g, u) whileBaseline 2 is (g, u|f).
The first tells us how pre-dictable giver utterances are (in the held-out data),based only on the normalised frequencies.
TheA P L E A.P A.L A.E All0.000.080.160.240.32Figure 3: Mean per-utterance probability, as-signed to held-out data by our model, when de-fined over the four feature sets and their combina-tions, estimated through leave-one-out validation.A=Absolute, P=Polynomial, L=Landmark, andE=Edge.
Error bars are standard deviations.Feature Set PerplexityAbsolute (A) 7.26?
4.08Polynomial (P) 12.86?
8.39Landmark (L) 15.16?
6.27Edge (E) 17.92?
8.47All 4.66?
2.22Table 2: Perplexity scores (and standard devia-tions) of our model, computed over the four fea-ture sets and their combination, estimated throughleave-one-out validation.
(A) outperforms all indi-vidual sets, while the combination performs best.second tells us how predictable they become whenwe condition on the previous follower act.
Detailsof the baselines are similar to Section 3.1.Figure 4 shows the mean per-utterance prob-ability our model assigns to held-out data whencompared to the two baselines.
Baseline 2 slightlyimproves our predictions over Baseline 1, al-though not reliably so, when considering the smallincrease in perplexity in Table 3.
SSM demon-strates a much larger relative improvement acrossboth metrics.
The results demonstrate that ourspatial component enables substantial reduction inuncertainty, brought by the transfer of informationfrom the maps to the utterances.Intrinsic metrics, such as the probability ofheld-out data and perplexity, provide us with an el-egant way of evaluating probabilistic models in asetting where there is no single correct answer, buta range of plausible answers, because they exploitthe model?s inherit ability to assign probability tobehaviour.
However, the metrics can be hard to in-terpret in an absolute sense, providing much better631Baseline1 Baseline2 SSM0.000.080.160.240.32Figure 4: Mean per-utterance probability, assignedto held-out data by our model (SSM), compared totwo baselines which lack the spatial componenet,estimated through leave-one-out validation.
Errorbars are standard deviations.Model PerplexityBaseline1 24.95?
4.05Baseline2 25.06?
12.02SSM 4.66?
2.22Table 3: Perplexity scores of our model (SSM),compared to the two baselines, estimated throughleave-one-out validation.
SSM finds the held-outdata to be least surprising.information about the relative strengths of differ-ent models rather than their absolute utility.
In thenext section, we explore methods for determiningthe utility of the models when applied to tasks.6 Extrinsic EvaluationIn this section, we undertake a task-based evalu-ation of model output.
We train on 22 of the di-alogs, holding out 3 at random for testing.
Thetask is to then generate, for each sub-route in thetest dialogs, the most probable unit to describe it6.Figure 5 shows some examples of sub-routes takenfrom the test dialogs, and shows the the most prob-able unit to describe each under our model, SSM.We first explore a naive notion of accuracy:the percentage of model-generated units matchingReal Giver units observed in the test dialogs.
Wecompute the same for Baseline 1 from Section 5as a lower bound.
A quick glance at the results inTable 4 might suggest that both models have lit-tle utility: SSM is ?correct?
only 33% of the time.However, the extent to which this conclusion fol-lows depends on the suitability of accuracy as a6The models can generate 1 of the 87 units observed inthe training set, but are made to output the most probable inthis experiment.Baseline SSMMatch to Real Giver 7.69% 33.08%Table 4: Percentage of model-generated units thatmatch Real Giver units in the test set.
The modelsoutput the most probable unit to describe a givensub-route.
We argue that this metric is unsuitableas it assumes one correct answer.Mismatch Baseline SSM Real Giver1.45 3.04 5.27 5.11Table 5: Average scores assigned by human judgesto model-generated units on a 7-level Likert scale.Mismatch is judged to be the worst, followed byBaseline.
SSM and Real Giver are scored well,and are judged to be of similar quality.means of evaluating dialog.
In most situations,there is not a single correct description and a hostof incorrect ones, but rather a gradient of descrip-tions from the highly informative and appropriateto the nonsensical and confusing.
Such subtletiesare not captured by an accuracy test (or the closelyrelated recall and precision).
In demonstration ofthis point, we next conduct qualitative evaluationof model output.We ask humans to rate, on a Likert scale of 7,the degree to which a given unit provides a suitabledescription of a given sub-route.
Sub-routes aretaken from the test dialogs, and are marked simi-larly to Figure 5 but on the complete map.
Unitsare generated from SSM, Baseline, Real Giver,and a control condition: a deliberate Mismatchto the sub-route.
The Mismatch is generated au-tomatically by taking the least probable unit underSSM, of the form MOVE(TOWARD, x) where x isone of the four compass directions.
We collect 5judgements for each sub-route-unit combinationson Mechanical Turk, and randomise so that nojudge sees the same order of pairs.
Test dialogscontained 94 distinct sub-routes.We analyse the results with a two-way ANOVA,with the first factor being model, and the secondbeing the sub-route, for a 4?94 design.
The meansof the ?model?
factor are shown in Table 5.
Itcan be seen that Mismatch and Baseline are scoredsensibly poorly, while SSM and Real Giver arescored reasonably well, and are judged to be of asimilar quality.
We thus proceed with a more rig-orous analysis.
The ANOVA summary is shownin Table 6.
A significant effect of the model fac-632(c) (d) (e)(a) (b)Figure 5: Given a sub-route marked with start-point ?
and end-point ?
(in red), SSM generates thefollowing u: (a) MOVE(TOWARDS, ABS NORTHEAST) (b) TURN(ABS WEST) (c) MOVE(FOLLOW-BOUNDARY, LM) (d) MOVE(AROUND, LM) (e) MOVE(TOWARDS, ABS SOUTHWEST)Factor S Sq Df F Pr(>F)Model 4845.3 3 783.93 <0.001Sub-route 1140.0 93 5.95 <0.001M:S 2208.7 279 3.84 <0.001Residuals 3263.5 1584Table 6: Two way ANOVA with factors model (4possibilities), and sub-route (94 possibilities).
Re-sults show a model effect accounting for most ofthe variance.
Meaning that the scores assigned tothe units by human judges are significantly influ-enced by the model used to generate the units.Model Comparison t value Pr(>|t|)Mismatch : Baseline -16.974 <0.001SSM : Baseline 23.882 <0.001Real Giver : Baseline 23.192 <0.001SSM : Mismatch 40.857 <0.001Real Giver : Mismatch 40.507 <0.001SSM : Real Giver 1.171 0.646Table 7: Tukey HSD shows that all models areassigned significantly different scores by judges,apart from SSM and Real Giver.
This asserts that,although only 33% of SSM units match Real Giverunits (as shown in Table 4), the quality of the unitsare not judged to be significantly different.tor is present, meaning that the scores assigned byhuman judges to the units are significantly influ-enced by which model was used to generate theunits.
Additionally, a significant effect for the sub-route factor can be seen, which is due to some sub-routes being harder to describe than others.
An in-teraction effect is also present, which is expectedgiven such a large number of examples.
Note howthe model factor accounts for the largest amountof variance of all the factors.Having confirmed the presence of a model ef-fect, we conduct a post-hoc analysis of the modelfactors.
Table 7 shows a Tukey HSD test, demon-strating that all models are significantly differentfrom one another, except Real Giver and SSM.
Re-sults show that, despite the large number of judge-ments collected, we are unable to separate thequality of our model?s unit from that in the origi-nal data, against which accuracy was being judgedin Table 4.
This demonstrates that when many an-swers are feasible, scoring correctness against theoriginal human units is unsuitable.
It also firmlydemonstrates the suitability of our spatial repre-sentation, and the strength of the generative modelwe have induced for the task.7 Conclusion and DiscussionWe have shown how to represent spatial goals in anavigational domain, and have validated our rep-resentation by inducing (fully from data) a gen-erative model of the Giver?s semantic utterancesconditioned on the spatial goal and the previousFollower act.
Intrinsic and extrinsic evaluationdemonstrate the strength of our model.A direct application of this work is robot guid-ance, by using the Giver?s simulator to induce anoptimal Follower: an MDP-based dialog managerthat interprets and follows navigational instruc-tions.
Another variation would be to learn a gen-erative model of the Follower, by extracting fea-tures from Follower maps (labelled with routesdrawn by real Followers).
Finally, this work hasbroader applications beyond simulation, in partic-ular for systems that describe routes to users (spa-tial goal representation and model dependencieswould hold).
Decisions about which part of theroute to describe next is one extension to that end.AcknowledgementsWe thank Ioannis Konstas, Johanna Moore, RobinHill, S. M. Ali Eslami, and the anonymous review-ers for valuable feedback.
This work is fundedby King Saud University.
Mark Steedman is sup-ported by EC-PF7-270273 Xperience and ERCAdvanced Fellowship 249520 GRAMPLUS.633ReferencesHua Ai and Diane J. Litman.
2007.
Knowledge con-sistent user simulations for dialog systems.
In Inter-Speech 2007, pages 2697?2700.Anne H. Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko, Jan McAllister,Jim Miller, Catherine Sotillo, Henry S. Thompson,and Regina Weinert.
1991.
The hcrc map task cor-pus.
Language and Speech, 34(4):351?366.Heriberto Cuay?ahuitl, Steve Renals, Oliver Lemon, andHiroshi Shimodaira.
2005.
Human-computer dia-logue simulation using hidden markov models.
InASRU 2005, pages 290?295.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue systemevaluation.
In Proceedings of IEEE Workshop onAutomatic Speech Recognition and Understanding.Aciel Eshky, Ben Allison, and Mark Steedman.
2012.Generative goal-driven user simulation for dialogmanagement.
In EMNLP-CoNLL 2012, pages 71?81, Jeju Island, Korea, July.
Association for Compu-tational Linguistics.Kallirroi Georgila, James Henderson, and OliverLemon.
2005.
Learning user simulations for in-formation state update dialogue systems.
In Inter-Speech 2005.Kallirroi Georgila, James Henderson, and OliverLemon.
2006.
User Simulation for Spoken Dia-logue Systems: Learning and Evaluation.
In Inter-Speech 2006.James Henderson and Oliver Lemon.
2008.
Mixturemodel pomdps for efficient handling of uncertaintyin dialogue management.
In ACL, HLT-Short ?08,pages 73?76, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Srinivasan Janarthanam, Oliver Lemon, Phil Bartie,Tiphaine Dalmas, Anna Dickinson, Xingkun Liu,William Mackaness, and Bonnie Webber.
2013.Evaluating a city exploration dialogue system withintegrated question-answering and pedestrian navi-gation.
In ACL.Esther Levin and Roberto Pieraccini.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
In IEEE Transactions on Speechand Audio Processing.Esther Levin, Roberto Pieraccini, and Wieland Eckert.1998.
Using markov decision process for learningdialogue strategies.
In Proc.
ICASSP, pages 201?204.M.
Levit and D. Roy.
2007.
Interpretation of spatiallanguage in a map navigation task.
IEEE Trans-actions on Systems, Man, and Cybernetics, Part A,37(3):667?679.Pierre Lison.
2013.
Model-based bayesian reinforce-ment learning for dialogue management.
In Inter-speech 2013.Olivier Pietquin and Thierry Dutoit.
2006.
A prob-abilistic framework for dialog simulation and opti-mal strategy learning.
Audio, Speech, and LanguageProcessing, IEEE Transactions on, 14(2):589?599,march.Olivier Pietquin and Helen Hastie.
2013.
A surveyon metrics for the evaluation of user simulations.Knowledge Eng.
Review, 28(1):59?73.Olivier Pietquin.
2004.
A Framework for Unsuper-vised Learning of Dialogue Strategies.
Ph.D. thesis,Facult?e Polytechnique de Mons, TCTS Lab (Bel-gique), apr.Olivier Pietquin.
2006.
Consistent goal-directed usermodel for realisitc man-machine task-oriented spo-ken dialogue simulation.
In Multimedia and Expo,2006 IEEE International Conference on, pages 425?428.
IEEE.David Reitter and Johanna D. Moore.
2007.
Predictingsuccess in dialogue.
In ACL.Verena Rieser and Oliver Lemon.
2006.
Cluster-based user simulations for learning dialogue strate-gies.
In INTERSPEECH 2006 - ICSLP, Ninth Inter-national Conference on Spoken Language Process-ing, September.St?ephane Rossignol, Olivier Pietquin, and Michel Ian-otto.
2010.
Simulation of the grounding process inspoken dialog systems with bayesian networks.
InIWSDS, pages 110?121.St?ephane Rossignol, Olivier Pietquin, and Michel Ian-otto.
2011.
Training a bn-based user model for di-alogue simulation with missing data.
In IJCNLP,pages 598?604.Nicholas Roy, Joelle Pineau, and Sebastian Thrun.2000.
Spoken dialogue management using proba-bilistic reasoning.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics, ACL ?00, pages 93?100, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jost Schatzmann and Steve Young.
2009.
The hid-den agenda user simulation model.
Audio, Speech,and Language Processing, IEEE Transactions on,17(4):733?747.Konrad Scheffler and Steve Young.
2002.
Automaticlearning of dialogue strategy using dialogue simula-tion and reinforcement learning.
In Proceedings ofHLT 2002.Satinder Singh, Diane J. Litman, Michael Kearns, andMarilyn A. Walker.
2002.
Optimizing dialoguemanagement with reinforcement learning: Experi-ments with the njfun system.
Journal of ArtificialIntelligence Research, 16:105?133.634Henry S. Thompson, Anne Anderson, Ellen G. Bard,Gwyneth D. Sneddon, Alison Newlands, and CathySotillo.
1993.
The HCRC Map Task corpus: natu-ral dialogue for speech recognition.
In Proceedingsof the workshop on Human Language Technology,HLT ?93, pages 25?30, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Adam Vogel and Daniel Jurafsky.
2010.
Learning tofollow navigational directions.
In ACL 2010, Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, July 11-16,2010, Uppsala, Sweden, pages 806?814.
The Asso-ciation for Computer Linguistics.Jason D. Williams and Steve Young.
2007.
Partiallyobservable markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason D. Williams.
2007.
Applying pomdps to dia-log systems in the troubleshooting domain.
In Pro-ceedings of the Workshop on Bridging the Gap: Aca-demic and Industrial Research in Dialog Technolo-gies, NAACL-HLT ?07, pages 1?8, Morristown, NJ,USA.
Association for Computational Linguistics.Steve Young, Milica Gasic, Blaise Thomson, and Ja-son D. Williams.
2013.
Pomdp-based statisticalspoken dialog systems: A review.
Proceedings ofthe IEEE, 101(5):1160?1179.635
