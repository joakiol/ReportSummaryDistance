INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 150?153,Utica, May 2012. c?2012 Association for Computational LinguisticsShared Task Proposal: Syntactic Paraphrase RankingMichael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210 USAmwhite@ling.ohio-state.eduAbstractWe describe a new shared task on syntac-tic paraphrase ranking that is intended to runin conjunction with the main surface real-ization shared task.
Taking advantage ofthe human judgments collected to evaluatethe surface realizations produced by com-peting systems, the task is to automaticallyrank these realizations?viewed as syntacticparaphrases?in a way that agrees with the hu-man judgments as often as possible.
The taskis designed to appeal to developers of surfacerealization systems as well as machine transla-tion evaluation metrics: for surface realizationsystems, the task sidesteps the thorny issue ofconverting inputs to a common representation;for MT evaluation metrics, the task providesa challenging framework for advancing auto-matic evaluation, as many of the paraphrasesare expected to be of high quality, differingonly in subtle syntactic choices.1 IntroductionFor the first surface realization shared task, the orga-nizers considered running a follow-on task for evalu-ating automatic evaluation metrics?along the linesof similar meta-evaluations carried out for machinetranslation in recent years?though it was deferredfor lack of time.
For the second surface realiza-tion shared task, we propose to generalize this met-rics meta-evaluation task to also usefully encom-pass realization ranking, where the various realiza-tions generated for a given input in the main taskare viewed as syntactic paraphrases of the originalcorpus sentence.
The syntactic paraphrasing sharedtask comprises three tracks, described in the nextsection; in each case, the task is to automatically re-produce the relative preference judgments gatheredduring the human evaluation of the surface realiza-tion main task.
As explained further below, develop-ers of realization systems that can generate and op-tionally rank multiple outputs for a given input willbe encouraged to participate in the task, which willtest the system?s ability to produce acceptable para-phrases and/or to rank competing realizations.The objectives of the shared task are as follows:broaden participation We expect developers ofautomatic quality metrics in the MT commu-nity to be interested in the proposed task, whichis anticipated to be both more focused (withlexical choice largely excluded) and more chal-lenging than in the MT case, given the gener-ally high level of quality in realization results:as realization quality increases, the metrics?task becomes more difficult, since the para-phrases of a given sentence often involve sub-tle differences between acceptable and unac-ceptable variation.
In an earlier study of theutility of automatic metrics with Penn Tree-bank (PTB) surface realization data (Espinosaet al, 2010), we observed moderate correla-tions between the most popular metrics and hu-man judgments, though lower than the levelsseen with MT data.promote reuse of human judgments The task isintended to test the effectiveness of realizationranking models in a way that reuses humanjudgments, making it possible to carry out re-150Track Reference PTB PTBSentence Gold AutoRealization Ranking N Y NHybrid Y Y NMetrics Meta-Eval Y N YTable 1: Additional inputs for the three realization tracksproducible system comparisons.mitigate input conversion issues Realizer evalua-tions have typically focused on single-best out-puts, where the depth and specificity of sys-tem inputs has a large impact on quality, mak-ing comparative evaluation difficult.
While thesurface realization shared task seeks to addressthis issue by developing common ground inputrepresentations, to date it has proved to be dif-ficult to adapt existing systems to work withthese inputs.
By focusing on ranking para-phrases that are distinct from the reference sen-tence, the proposed task may provide a way tomitigate these issues, as discussed below.2 Three Tracks: From RealizationRanking to Metrics Meta-EvaluationWe propose three tracks for the task, going frompure realization ranking to metrics meta-evaluation,with a hybrid case in the middle.
For all three tracks,the input is a set of pairs of syntactic paraphrases(distinct from the reference sentence), and the outputis the preferred member of each pair, where the goalis to match the human judgments of relative prefer-ence.
The tracks differ in the additional inputs thatsystems may use in determining which member ofeach pair is preferred (see Table 1).
In the realiza-tion ranking track, the task is to rank order the para-phrases for a given sentence, without having accessto the reference sentence, using a realization rank-ing model.
To do so, each system is allowed to useits own ?native?
inputs derived from the Penn Tree-bank and PTB-based resources.
To the extent thata system?s statistical ranking model can be used toassign a score to any possible realization, the rank-ing task can be accomplished by simply ranking therealizations by model score.
As such, following thisstrategy, the task is one of analysis by synthesis.For non-statistical realizers, or ones that cannotassign a score to any possible realization, there isan alternative strategy available, namely to auto-matically approximate HTER.
Snover et al (2006)demonstrate that the human-targeted translation editrate (HTER) represents a reliable and easily inter-pretable method of evaluating MT output.
With thismethod, a human annotator produces a targeted ref-erence sentence which is as close as possible to theMT hypothesis while being fully acceptable; fromthe targeted reference, the TER score then repre-sents a normalized post-edit score, which has beenshown to correlate with human ratings at least aswell as more complex competing metrics.
As Mad-nani (2010) points out, generated paraphrases ofthe reference sentence can be used to approximateHTER scoring, as the closest acceptable paraphraseof a reference sentence should correspond to the ver-sion of the MT hypothesis with minimal changes tomake it acceptable.
Indeed, in the limit, it shouldbe possible to use a system that can enumerate alland only the acceptable paraphrases of a referencesentence to fully implement HTER scoring.Naturally, it is possible to combine the analysis-by-synthesis and approximating HTER strategies.One particularly simple way to do so is to (1) usean n-best list of realizations with normalized scores,(2) find the realization with the minimum TER scorefor each paraphrase to rank, then (3) combine the re-alizer?s model score with the TER score, e.g.
just bysubtraction (weights for the combination could alsobe optimized using machine learning).Regarding the issue of whether fair comparisonscan be made when each system is allowed to use itsown PTB-derived ?native?
input, note that it is un-clear whether using shallow, specific inputs is neces-sarily advantageous for ranking a range of possiblerealizations, all distinct from the reference sentence:in the limit, a realizer input that completely speci-fies the reference sentence (and no other variants) isof no help at all, as in this case the approximatingHTER strategy reduces to just doing TER scoringagainst the reference sentence.Turning now to the metrics meta-evaluation track,here the the task is to rank order a set of realizationsfor a given sentence, starting with the reference sen-tence and nothing else.
In principle, it should bepossible to use any MT metric for this task off-the-151shelf.
It should also be possible for realization sys-tems to participate in this track, if they can be pairedwith a parser that produces inputs for the realizer, ora parser whose outputs can be converted to realizerinputs.
To do so, strategies employed in the realiza-tion ranking track can be combined with ones thatmake use of the reference sentence.Finally, between these two tracks is a hybrid track,where one is allowed to substitute automatic parseswith gold parses.
This track can be viewed as pro-viding a way to estimate an upper bound on ap-proaches that pay attention to how well a sentenceexpresses an intended meaning, while also arguablyrepresenting the most sensible way to automaticallyevaluate outputs in a data-to-text setting, where in-tended meanings can be reliably represented.3 Pilot ExperimentsIn this section, we present two pilot experiments in-tended to demonstrate the feasibility of the task.
Theexperiments use the human judgments collected inEspinosa et al?s (2010) study, which consist of ade-quacy and fluency ratings from two judges for a va-riety of realizations for PTB Section 00.
The real-izations in the corpus were generated using severalOpenCCG realization ranking models (White andRajkumar, 2009) and using the XLE symbolic re-alizer with subsequent n-gram ranking (paraphrasesinvolving WordNet substitutions were excluded).For comparison purposes, three well-known met-rics (BLEU, METEOR and TER) were tested, alongwith three OpenCCG ranking models: (I) a gen-erative baseline model, incorporating three n-grammodels as well as Hockenmaier?s (2003) genera-tive model; (II) a model additionally incorporatinga slew of discriminative features, extending White& Rajkumar?s model with dependency ordering fea-tures; and (III) a model adding one additional fea-ture for minimizing dependency length.
Note thatModels II and III are very similar, usually yieldingthe same single-best output, though occasionally dif-fering in important ways; by contrast, both modelsrepresent a substantial refinement of Model I.The two experiments investigate different strate-gies for approaching the hybrid task.
The first exper-iment investigates the approximating-HTER strat-egy (with an analysis-by-synthesis component) us-ing a 20-best list.
For simplicity, edit rate (edit dis-tance normalized by the number of words in the ref-erence sentence) was used to find the realization inthe 20-best list that was closest to the paraphrase tobe ranked.
The score for the paraphrase was thencalculated by normalizing the realizer model scorefor the closest realization (linearly interpolating us-ing the min and max scores across all 20-best lists),subtracting the edit rate, and adding in the met-ric score, for each of BLEU, METEOR and TER.1Since edit rate is less reliable than TER, as it overlypenalizes phrasal shifts, the metric score was usedalone in cases where the edit rate exceeded 0.5.The results of the first experiment appear in Ta-ble 2.
Human judgments were combined by av-eraging the summed adequacy and fluency ratingsfrom each judge.
Excluding exact match realiza-tions, 2838 pairs of realizations with distinct com-bined scores (from approximately 250 sentences)were used to judge ranking accuracy.
Here, BLEUsubstantially outperforms METEOR and TER, andcombining Models I-III with BLEU does not yieldsignificant differences in ranking accuracy.
Note,however, that using TER scores rather than edit rate,and optimizing the way the model scores are com-bined with the TER score and BLEU score, couldperhaps yield significant improvements.
With ME-TEOR and TER, combining the model score, editrate and metric score in the simplest way does yieldhighly significant improvements.
With the ME-TEOR combination, Model II achieves a highly sig-nificant improvement over Model I, though in othercases, only trends are observed across models.The second experiment investigates the analysis-by-synthesis strategy more directly.
Here, the re-alizer?s search was guided to reproduce each para-phrase where possible, with model scores then cal-culated where an exact match could be achieved.The results appear in Table 3 for 474 pairs with dif-fering combined human judgments.
The first col-umn shows the ranking accuracy using the modelscores by themselves; the subsequent columns com-pare the accuracy using BLEU, METEOR and TERagainst using the model score added to the metricscore.
Here we see from the first column that ModelII substantially outperforms Model I, showing the1TER scores were inverted for consistency.152BLEU Model+BLEU METEOR Model+METEOR TER Model+TERModel I 71.2 70.2 58.6 65.4 (***) 59.7 68.7 (***)Model II - 70.8 - 66.7 (***?
?
?)
- 69.4 (***?
)Model III - 71.3 (?)
- 67.1 (***) - 69.9 (***)Table 2: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntacticparaphrases, using n-best realizations from three OpenCCG ranking models and minimum edit rate in combinationwith MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric, usingMcNemar?s test; similarly for number of daggers in comparison to model in previous row)Model BLEU Model+BLEU METEOR Model+METEOR TER Model+TERModel I 62.2 67.7 73.0 (***) 49.2 65.4 (***) 50.6 73.8 (***)Model II 67.1 (?
?
?)
- 72.2 (***) - 68.6 (***??)
- 74.9 (***)Model III 66.2 - 72.6 (***) - 68.8 (***) - 75.1 (***)Table 3: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntacticparaphrases, using exact targeted realizations from three OpenCCG ranking models and minimum edit rate in com-bination with MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric,using McNemar?s test; similarly for number of daggers in comparison to model in previous row)ability of the ranking task to discriminate amongmodels of varying sophistication, though the modeldifferences are largely washed out when the modelscores are combined with metric scores.
In the sub-sequent columns, we see that METEOR and TERare only performing at chance (50%) on these par-ticular ranking cases, while adding the model scoresand metric scores does much better, with Model IIIplus TER performing the best overall, as might havebeen expected.
Even with BLEU, which performsdecently on its own, adding in the model scoresachieves substantial (and highly significant) gains.4 Task OrganizationThe proposed syntactic paraphrase ranking task isintended to be run as a straightforward extension ofthe main surface realization shared task.
For devel-opment and training purposes, the human judgmentscollected for the first surface realization shared taskwill be made available; the data from Espinosa etal.
?s study is already publicly available as well.
Fortest data, the human judgments collected for eval-uation during the second surface realization sharedtask will be used.
Ideally enough systems will enterthe main task to enable many pairwise comparisonsper sentence, and enough judges can be employedto allow majority preferences to be used as the goldstandard.
As baselines for the metrics meta-eval andhybrid tracks, the BLEU, NIST, METEOR and TERmetrics will be run by the organizers.
Time permit-ting, a baseline system that works with n-best real-ization scores will also be made available, so thatany developer of a realization system that can pro-duce n-best outputs can easily participate.AcknowledgmentsThis work was supported in part by NSF grant no.IIS-1143635.
Thanks go to the anonymous review-ers for helpful comments and discussion.ReferencesDominic Espinosa, Rajakrishnan Rajkumar, MichaelWhite, and Shoshana Berleant.
2010.
Further meta-evaluation of broad-coverage surface realization.
InProc.
of EMNLP-10, pages 564?574.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Nitin Madnani.
2010.
The Circle of Meaning: FromTranslation to Paraphrasing and Back.
Ph.D. thesis,University of Maryland.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA-06, pages 223?231.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proc.
ofEMNLP-09, pages 410?419.153
