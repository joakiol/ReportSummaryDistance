TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73?80,Rochester, April 2007 c?2007 Association for Computational LinguisticsDLSITE-2: Semantic Similarity Based on SyntacticDependency Trees Applied to Textual EntailmentDaniel Micol, O?scar Ferra?ndez, Rafael Mun?oz, and Manuel PalomarNatural Language Processing and Information Systems GroupDepartment of Computing Languages and SystemsUniversity of AlicanteSan Vicente del Raspeig, Alicante 03690, Spain{dmicol, ofe, rafael, mpalomar}@dlsi.ua.esAbstractIn this paper we attempt to deduce tex-tual entailment based on syntactic depen-dency trees of a given text-hypothesis pair.The goals of this project are to provide anaccurate and fast system, which we havecalled DLSITE-2, that can be applied insoftware systems that require a near-real-time interaction with the user.
To accom-plish this we use MINIPAR to parse thephrases and construct their correspond-ing trees.
Later on we apply syntactic-based techniques to calculate the seman-tic similarity between text and hypothe-sis.
To measure our method?s precision weused the test text corpus set from SecondPASCAL Recognising Textual EntailmentChallenge (RTE-2), obtaining an accuracyrate of 60.75%.1 IntroductionThere are several methods used to determine tex-tual entailment for a given text-hypothesis pair.
Theone described in this paper uses the informationcontained in the syntactic dependency trees of suchphrases to deduce whether there is entailment ornot.
In addition, semantic knowledge extracted fromWordNet (Miller et al, 1990) has been added toachieve higher accuracy rates.It has been proven in several competitions andother workshops that textual entailment is a complextask.
One of these competitions is PASCAL Recog-nising Textual Entailment Challenge (Bar-Haim etal., 2006), where each participating group develops atextual entailment recognizing system attempting toaccomplish the best accuracy rate of all competitors.Such complexity is the reason why we use a combi-nation of various techniques to deduce whether en-tailment is produced.Currently there are few research projects relatedto the topic discussed in this paper.
Some systemsuse syntactic tree matching as the textual entailmentdecision core module, such as (Katrenko and Adri-aans, 2006).
It is based on maximal embedded syn-tactic subtrees to analyze the semantic relation be-tween text and hypothesis.
Other systems use syn-tactic trees as a collaborative module, not being thecore, such as (Herrera et al, 2006).
The applicationdiscussed in this paper belongs to the first set of sys-tems, since syntactic matching is its main module.The remainder of this paper is structured as fol-lows.
In the second section we will describe themethods implemented in our system.
The third onecontains the experimental results, and the fourth andlast discusses such results and proposes future workbased on our actual research.2 MethodsThe system we have built aims to provide a goodaccuracy rate in a short lapse of time, making itfeasible to be included in applications that requirenear-real-time responses due to their interaction withthe user.
Such a system is composed of few mod-ules that behave collaboratively.
These include treeconstruction, filtering, embedded subtree search andgraph node matching.
A schematic representation ofthe system architecture is shown in Figure 1.73Figure 1: DLSITE-2 system architecture.Each of the steps or modules of DLSITE-2 is de-scribed in the following subsections, that are num-bered sequentially according to their execution or-der.2.1 Tree generationThe first module constructs the corresponding syn-tactic dependency trees.
For this purpose, MINI-PAR (Lin, 1998) output is generated and afterwardsparsed for each text and hypothesis of our corpus.Phrase tokens, along with their grammatical infor-mation, are stored in an on-memory data structurethat represents a tree, which is equivalent to the men-tioned syntactic dependency tree.2.2 Tree filteringOnce the tree has been constructed, we may wantto discard irrelevant data in order to reduce our sys-tem?s response time and noise.
For this purpose wehave generated a database of relevant grammaticalcategories, represented in Table 1, that will allowus to remove from the tree all those tokens whosecategory does not belong to such list.
The result-ing tree will have the same structure as the original,but will not contain any stop words nor irrelevant to-kens, such as determinants or auxiliary verbs.
Thewhole list of ignored grammatical categories is rep-resented in Table 2.We have performed tests taking into account anddiscarding each grammatical category, which has al-lowed us to generate both lists of relevant and ig-nored grammatical categories.Verbs, verbs with one argument, verbs with two ar-guments, verbs taking clause as complement, verbHave, verb BeNounsNumbersAdjectivesAdverbsNoun-noun modifiersTable 1: Relevant grammatical categories.2.3 Graph embedding detectionThe next step of our system consists in determiningwhether the hypothesis?
tree is embedded into thetext?s.
Let us first define the concept of embeddedtree (Katrenko and Adriaans, 2006).Definition 1: Embedded tree A treeT1 = (V1, E1) is embedded into anotherone T2 = (V2, E2) iff1.
V1 ?
V2, and2.
E1 ?
E2where V1 and V2 represent the vertices,and E1 and E2 the edges.In other words, a tree, T1, is embedded into an-other one, T2, if all nodes and branches of T1 arepresent in T2.We believe that it makes sense to reduce the strict-ness of such a definition to allow the appearanceof intermediate nodes in the text?s branches that are74DeterminersPre-determinersPost-determinersClausesInflectional phrasesPreposition and preposition phrasesSpecifiers of preposition phrasesAuxiliary verbsComplementizersTable 2: Ignored grammatical categories.not present in the corresponding hypothesis?
branch,which means that we allow partial matching.
There-fore, a match between two branches will be pro-duced if all nodes of the first one, namely ?1 ?
E1,are present in the second, namely ?2 ?
E2, and theirrespective order is the same, allowing the possibil-ity of appearance of intermediate nodes that are notpresent in both branches.
This is also described in(Katrenko and Adriaans, 2006).To determine whether the hypothesis?
tree is em-bedded into the text?s, we perform a top-downmatching process.
For this purpose we first comparethe roots of both trees.
If they coincide, we then pro-ceed to compare their respective child nodes, whichare the tokens that have some sort of dependencywith their respective root token.In order to add more flexibility to our system,we do not require the pair of tokens to be ex-actly the same, but rather set a threshold that rep-resents the minimum similarity value between them.This is a difference between our approach and theone described in (Katrenko and Adriaans, 2006).Such a similarity is calculated by using the Word-Net::Similarity tool (Pedersen et al, 2004), and,concretely, the Wu-Palmer measure, as defined inEquation 1 (Wu and Palmer, 1994).Sim(C1, C2) =2N3N1 +N2 + 2N3(1)where C1 and C2 are the synsets whose similaritywe want to calculate, C3 is their least common su-perconcept, N1 is the number of nodes on the pathfrom C1 to C3, N2 is the number of nodes on thepath from C2 to C3, and N3 is the number of nodeson the path from C3 to the root.
All these synsetsand distances can be observed in Figure 2.Figure 2: Distance between two synsets.If the similarity rate is greater or equal than theestablished threshold, which we have set empiricallyto 80%, we will consider the corresponding hypoth-esis?
token as suitable to have the same meaningas the text?s token, and will proceed to compare itschild nodes in the hypothesis?
tree.
On the otherhand, if such similarity value is less than the cor-responding threshold, we will proceed to comparethe children of such text?s tree node with the actualhypothesis?
node that was being analyzed.The comparison between the syntactic depen-dency trees of both text and hypothesis will be com-pleted when all nodes of either tree have been pro-cessed.
If we have been able to find a match for allthe tokens within the hypothesis, the correspondingtree will be embedded into the text?s and we will be-lieve that there is entailment.
If not, we will not beable to assure that such an implication is producedand will proceed to execute the next module of oursystem.Next, we will present a text-hypothesis pair sam-ple where the syntactic dependency tree of the hy-pothesis (Figure 3(b)) is embedded into the text?s(Figure 3(a)).
The mentioned text-hypothesis pairis the following:Text: Mossad is one of the world?s mostwell-known intelligence agencies, and isoften viewed in the same regard as the CIAand MI6.Hypothesis: Mossad is an intelligenceagency.75(a) Mossad is one of the world?s most well-known intelligence agencies, and is often viewedin the same regard as the CIA and MI6.
(b) Mossad is an intelligenceagency.Figure 3: Representation of a hypothesis?
syntactic dependency tree that is embedded into the text?s.As one can see in Figure 3, the hypothesis?
syn-tactic dependency tree represented is embedded intothe text?s because all of its nodes are present inthe text in the same order.
There is one exceptionthough, that is the word an.
However, since it is adeterminant, the filtering module will have deletedit before the graph embedding test is performed.Therefore, in this example the entailment would berecognized.2.4 Graph node matchingOnce the embedded subtree comparison has fin-ished, and if its result is negative, we proceed to per-form a graph node matching process, termed align-ment, between both the text and the hypothesis.
Thisoperation consists in finding pairs of tokens in bothtrees whose lemmas are identical, no matter whetherthey are in the same position within the tree.
Wewould like to point out that in this step we do notuse the WordNet::Similarity tool.Some authors have already designed similarmatching techniques, such as the ones described in(MacCartney et al, 2006) and (Snow et al, 2006).However, these include semantic constraints that wehave decided not to consider.
The reason of thisdecision is that we desired to overcome the textualentailment recognition from an exclusively syntacticperspective.
Therefore, we did not want this moduleto include any kind of semantic knowledge.The weight given to a token that has been foundin both trees will depend on the depth in the hypoth-esis?
tree and the token?s grammatical relevance.The first of these factors depends on an empirically-calculated weight that assigns less importance to anode the deeper it is located in the tree.
This weightis defined in Equation 2.
The second factor givesdifferent relevance depending on the grammaticalcategory and relationship.
For instance, a verb willhave the highest weight, while an adverb or an ad-jective will have less relevance.
The values assignedto each grammatical category and relationship arealso empirically-calculated and are shown in Tables3 and 4, respectively.Grammatical category WeightVerbs, verbs with one argument, verbswith two arguments, verbs takingclause as complement1.0Nouns, numbers 0.75Be used as a linking verb 0.7Adjectives, adverbs, noun-noun mod-ifiers0.5Verbs Have and Be 0.3Table 3: Weights assigned to the grammatical cate-gories.76Grammatical relationship WeightSubject of verbs, surface subject, ob-ject of verbs, second object of ditran-sitive verbs1.0The rest 0.5Table 4: Weights assigned to the grammatical rela-tionships.Let ?
and ?
represent the text?s and hypothesis?syntactic dependency trees, respectively.
We as-sume we have found members of a synset, namely ?,present in both ?
and ?.
Now let ?
be the weight as-signed to ?
?s grammatical category (defined in Table3), ?
the weight of ?
?s grammatical relationship (de-fined in Table 4), ?
an empirically-calculated valuethat represents the weight difference between treelevels, and ??
the depth of the node that containsthe synset ?
in ?.
We define the function ?(?)
asrepresented in Equation 2.?(?)
= ?
?
?
?
????
(2)The value obtained by calculating the expressionof Equation 2 would represent the relevance of asynset in our system.
The experiments performedreveal that the optimal value for ?
is 1.1.For a given pair (?
, ?
), we define the set ?
as theone that contains the synsets present in both trees:?
= ?
?
?
??
?
?, ?
?
?
(3)Therefore, the similarity rate between ?
and ?, de-noted by the symbol ?, would be defined as:?
(?, ?)
=?????(?)
(4)One should note that a requirement of our sys-tem?s similarity measure would be to be independentof the hypothesis length.
Thus, we must define thenormalized similarity rate, as shown in Equation 5.?
(?, ?)
=?
(?, ?)?????(?)=?????(?)?????(?
)(5)Once the similarity value, ?
(?, ?
), has been cal-culated, it will be provided to the user together withthe corresponding text-hypothesis pair identifier.
Itwill be his responsibility to choose an appropriatethreshold that will represent the minimum similarityrate to be considered as entailment between text andhypothesis.
All values that are under such a thresh-old will be marked as not entailed.
For this purpose,we suggest using a development corpus in order toobtain the optimal threshold value, as it is done inthe RTE challenges.3 Experimental resultsThe experimental results shown in this paper wereobtained processing a set of text-hypothesis pairsfrom RTE-2.
The organizers of this challenge pro-vide development and test corpora to the partic-ipants, both of them containing 800 pairs manu-ally annotated for logical entailment.
It is com-posed of four subsets, each of them correspond-ing to typical true and false entailments in differenttasks, such as Information Extraction (IE), Informa-tion Retrieval (IR), Question Answering (QA), andMulti-document Summarization (SUM).
For eachtask, the annotators selected the same amount of trueentailments as negative ones (50%-50% split).The organizers have also defined two measures toevaluate the participating systems.
All judgmentsreturned by the systems will be compared to thosemanually assigned by the human annotators.
Thepercentage of matching judgments will provide theaccuracy of the system, i.e.
the percentage of cor-rect responses.
As a second measure, the averageprecision will be computed.
This measure evaluatesthe ability of the systems to rank all the pairs in thecorpus according to their entailment confidence, indecreasing order from the most certain entailment tothe least.
Average precision is a common evaluationmeasure for system rankings that is defined as shownin Equation 6.AP =1Rn?i=1E(i)#correct up to pair ii(6)where n is the amount of the pairs in the test corpus,R is the total number of positive pairs in it, i rangesover the pairs, ordered by their ranking, and E(i) isdefined as follows:77E(i) =??
?1 if the i?
th pair is positive,0 otherwise.
(7)As we previously mentioned, we tested our sys-tem against RTE-2 development corpus, and usedthe test one to evaluate it.First, Table 5 shows the accuracy (ACC) and av-erage precision (AP), both as a percentage, obtainedprocessing the development corpus from RTE-2 fora threshold value of 68.9%, which corresponds tothe highest accuracy that can be obtained using oursystem for the mentioned corpus.
It also providesthe rate of correctly predicted true and false entail-ments.Task ACC AP TRUE FALSEIE 52.00 51.49 54.00 50.00IR 55.50 58.99 32.00 79.00QA 57.50 54.72 53.00 62.00SUM 65.00 81.35 39.00 91.00Overall 57.50 58.96 44.50 70.50Table 5: Results obtained for the development cor-pus.Next, let us show in Table 6 the results obtainedprocessing the test corpus, which is the one usedto compare the different systems that participated inRTE-2, with the same threshold as before.Task ACC AP TRUE FALSEIE 50.50 47.33 75.00 26.00IR 64.50 67.67 59.00 70.00QA 59.50 58.16 80.00 39.00SUM 68.50 75.86 49.00 88.00Overall 60.75 57.91 65.75 55.75Table 6: Results obtained for the test corpus.As one can observe in the previous table, oursystem provides a high accuracy rate by usingmainly syntactical measures.
The number of text-hypothesis pairs that succeeded the graph embed-ding evaluation was three for the development cor-pus and one for the test set, which reflects the strict-ness of such module.
However, we would like topoint out that the amount of pairs affected by thementioned module will depend on the corpus na-ture, so it can vary significantly between differentcorpora.Let us now compare our results with the ones thatwere achieved by the systems that participated inRTE-2.
One should note that the criteria for suchranking is based exclusively on the accuracy, ignor-ing the average precision value.
In addition, eachparticipating group was allowed to submit two dif-ferent systems to RTE-2.
We will consider here thebest result of both systems for each group.
The men-tioned comparison is shown in Table 7, and containsonly the systems that had higher accuracy rates thanour approach.Participant Accuracy(Hickl et al, 2006) 75.38(Tatu et al, 2006) 73.75(Zanzotto et al, 2006) 63.88(Adams, 2006) 62.62(Bos and Markert, 2006) 61.62DLSITE-2 60.75Table 7: Comparison of some of the teams that par-ticipated in RTE-2.As it is reflected in Table 7, our system wouldhave obtained the sixth position out of twenty-fourparticipants, which is an accomplishment consider-ing the limited number of resources that it has built-in.Since one of our system?s modules is based on(Katrenko and Adriaans, 2006), we will comparetheir results with ours to analyze whether the modi-fications we introduced perform correctly.
In RTE-2, they obtained an accuracy rate of 59.00% for thetest corpus.
The reason why we believe we haveachieved better results than their system is due tothe fact that we added semantic knowledge to ourgraph embedding module.
In addition, the syntacticdependency trees to which we have applied such amodule have been previously filtered to ensure thatthey do not contain irrelevant words.
This reducesthe system?s noise and allows us to achieve higheraccuracy rates.In the introduction of this paper we mentionedthat one of the goals of our system was to provide78a high accuracy rate in a short lapse of time.
This isone of the reasons why we chose to construct a lightsystem where one of the aspects to minimize was itsresponse time.
Table 8 shows the execution times1of our system for both development and test text cor-pora from RTE-2.
These include total and average2response times.Development TestTotal 1045 1023Average 1.30625 1.27875Table 8: DLSITE-2 response times (in seconds).As we can see, accurate results can be obtainedusing syntactic dependency trees in a short lapse oftime.
However, there are some limitations that oursystem does not avoid.
For instance, the tree em-bedding test is not applicable when there is no verbentailment.
This is reflected in the following pair:Text: Tony Blair, the British Prime Minis-ter, met Jacques Chirac in London.Hypothesis: Tony Blair is the BritishPrime Minister.The root node of the hypothesis?
tree would bethe one corresponding to the verb is.
Since the en-tailment here is implicit, there is no need for such averb to appear in the text.
However, this is not com-patible with our system, since is would not matchany node of the text?s tree, and thus the hypothesis?tree would not be found embedded into the text?s.The graph matching process would not behavecorrectly either.
This is due to the fact that the mainverb, which has the maximum weight because it isthe root of the hypothesis?
tree and its grammaticalcategory has the maximum relevance, is not presentin the text, so the overall similarity score would havea considerable handicap.The example of limitation of our system that wehave presented is an apposition.
To avoid this spe-cific kind of situations that produce an undesired be-havior in our system, we could add a preprocess-ing module that transforms the phrases that have the1The machine we used to measure the response times had anIntel Core 2 Duo processor at 2GHz.2Average response times are calculated diving the totals bythe number of pairs in the corpus.structureX , Y , Z intoX is Y , andZ.
For the shownexample, the resulting text and hypothesis would beas follows:Text: Tony Blair is the British Prime Min-ister, and met Jacques Chirac in London.Hypothesis: Tony Blair is the BritishPrime Minister.The transformed text would still be syntacticallycorrect, and the entailment would be detected sincethe hypothesis?
syntactic dependency tree is embed-ded into the text?s.4 Conclusions and future workThe experimental results obtained from this researchdemonstrate that it is possible to apply a syntactic-based approach to deduce textual entailment from atext-hypothesis pair.
We can obtain good accuracyrates using the discussed techniques with very shortresponse times, which is very useful for assistingdifferent kinds of tasks that demand near-real-timeresponses to user interaction.The baseline we set for our system was to achievebetter results than the ones we obtained with our lastparticipation in RTE-2.
As it is stated in (Ferra?ndezet al, 2006), the maximum accuracy value obtainedby then was 55.63% for the test corpus.
Therefore,our system is 9.20% more accurate compared to theone that participated in RTE-2, which represents aconsiderable improvement.The authors of this paper believe that if higher ac-curacy rates are desired, a step-based systemmust beconstructed.
This would have several preprocessingunits, such as negation detectors, multi-word associ-ators and so on.
The addition of these units woulddefinitely increase the response time preventing thesystem from being used in real-time tasks.Future work can be related to the cases where noverb entailment is produced.
For this purpose wepropose to extract a higher amount of semantic in-formation that would allow us to construct a charac-terized representation based on the input text, so thatwe can deduce entailment even if there is no appar-ent structure similarity between text and hypothesis.This would mean to create an abstract conceptual-ization of the information contained in the analyzedphrases, allowing us to deduce ideas that are not79explicitly mentioned in the parsed text-hypothesispairs.In addition, the weights and thresholds definedin our system have been established empirically.
Itwould be interesting to calculate those values bymeans of a machine learning algorithm and com-pare them to the ones we have obtained empirically.Some authors have already performed this compari-son, being one example the work described in (Mac-Cartney et al, 2006).AcknowledgmentsThe authors of this paper would like to thank pro-fessors Borja Navarro and Rafael M. Terol for theirhelp and critical comments.This research has been supported by the under-graduate research fellowships financed by the Span-ish Ministry of Education and Science, the projectTIN2006-15265-C06-01 financed by such ministry,and the project ACOM06/90 financed by the Span-ish Generalitat Valenciana.ReferencesRod Adams.
2006.
Textual Entailment Through Ex-tended Lexical Overlap.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment, Venice, Italy.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The Second PASCAL Recognising Textual En-tailment Challenge.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment, Venice, Italy.Johan Bos, and Katja Markert.
2006.
When logical infer-ence helps determining textual entailment (and when itdoesnt).
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognising Textual Entailment,Venice, Italy.O?scar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-cio Mart?
?nez-Barco, and Manuel Palomar.
2006.
Anapproach based on Logic Forms andWordNet relation-ships to Textual Entailment performance.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.Jesu?s Herrera, Anselmo Pen?as, A?lvaro Rodrigo, and Fe-lisa Verdejo.
2006.
UNED at PASCAL RTE-2 Chal-lenge.
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognising Textual Entailment,Venice, Italy.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recog-nizing Textual Entailment with LCC?s GROUNDHOGSystem.
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognising Textual Entailment,Venice, Italy.Sophia Katrenko, and Pieter Adriaans.
2006.
UsingMaximal Embedded Syntactic Subtrees for Textual En-tailment Recognition.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment, Venice, Italy.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems, Granada, Spain.Bill MacCartney, Trond Grenager, Marie-Catherine deMarneffe, Daniel Cer, and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of the North AmericanAssociation of Computational Linguistics (NAACL-06), NewYork City, NewYork, United States of Amer-ica.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.
In-troduction to WordNet: An On-line Lexical Database.International Journal of Lexicography 1990 3(4):235-244.Ted Pedersen, Siddhart Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - Measuring the Re-latedness of Concepts.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-04), Boston, Massachus-sets, United States of America.Rion Snow, Lucy Vanderwende, and Arul Menezes.2006.
Effectively using syntax for recognizing falseentailment.
In Proceedings of the North American As-sociation of Computational Linguistics (NAACL-06),New York City, New York, United States of America.Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,and DanMoldovan.
2006.
COGEX at the Second Rec-ognizing Textual Entailment Challenge.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.Zhibiao Wu, and Martha Palmer.
1994.
Verb Semanticsand Lexical Selection.
In Proceedings of the 32nd An-nual Meeting of the Associations for ComputationalLinguistics, pages 133-138, Las Cruces, New Mexico,United States of America.Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-nacchiotti, and Maria T. Pazienza.
2006.
Learningtextual entailment from examples.
In Proceedings ofthe Second PASCAL Challenges Workshop on Recog-nising Textual Entailment, Venice, Italy.80
