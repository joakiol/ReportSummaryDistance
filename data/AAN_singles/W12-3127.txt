Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222?231,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUsing Categorial Grammar to Label Translation RulesJonathan Weese and Chris Callison-Burch and Adam LopezHuman Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractAdding syntactic labels to synchronouscontext-free translation rules can improveperformance, but labeling with phrase struc-ture constituents, as in GHKM (Galley et al,2004), excludes potentially useful translationrules.
SAMT (Zollmann and Venugopal,2006) introduces heuristics to create newnon-constituent labels, but these heuristicsintroduce many complex labels and tend toadd rarely-applicable rules to the translationgrammar.
We introduce a labeling schemebased on categorial grammar, which allowssyntactic labeling of many rules with a mini-mal, well-motivated label set.
We show thatour labeling scheme performs comparably toSAMT on an Urdu?English translation task,yet the label set is an order of magnitudesmaller, and translation is twice as fast.1 IntroductionThe Hiero model of Chiang (2007) popularizedthe usage of synchronous context-free grammars(SCFGs) for machine translation.
SCFGs modeltranslation as a process of isomorphic syntacticderivation in the source and target language.
But theHiero model is formally, not linguistically syntactic.Its derivation trees use only a single non-terminal la-bel X , carrying no linguistic information.
ConsiderRule 1.X ?
?
maison ; house ?
(1)We can add syntactic information to the SCFGrules by parsing the parallel training data and pro-jecting parse tree labels onto the spans they yield andtheir translations.
For example, if house was parsedas a noun, we could rewrite Rule 1 asN ?
?
maison ; house ?But we quickly run into trouble: how should welabel a rule that translates pour l?e?tablissement deinto for the establishment of?
There is no phrasestructure constituent that corresponds to this Englishfragment.
This raises a model design question: whatlabel do we assign to spans that are natural trans-lations of each other, but have no natural labelingunder a syntactic parse?
One possibility would beto discard such translations from our model as im-plausible.
However, such non-compositional trans-lations are important in translation (Fox, 2002), andthey have been repeatedly shown to improve trans-lation performance (Koehn et al, 2003; DeNeefe etal., 2007).Syntax-Augmented Machine Translation (SAMT;Zollmann and Venugopal, 2006) solves this prob-lem with heuristics that create new labels from thephrase structure parse: it labels for the establish-ment of as IN+NP+IN to show that it is the con-catenation of a noun phrase with a preposition oneither side.
While descriptive, this label is unsatis-fying as a concise description of linguistic function,fitting uneasily alongside more natural labels in thephrase structure formalism.
SAMT introduces manythousands of such labels, most of which are seenvery few times.
While these heuristics are effective(Zollmann et al, 2008), they inflate grammar size,hamper effective parameter estimation due to featuresparsity, and slow translation speed.Our objective is to find a syntactic formalism that222enables us to label most translation rules without re-lying on heuristics.
Ideally, the label should be smallin order to improve feature estimation and reducetranslation time.
Furthering an insight that informsSAMT, we show that combinatory categorial gram-mar (CCG) satisfies these requirements.Under CCG, for the establishment of is labeledwith ((S\NP)\(S\NP))/NP.
This seems complex, butit describes exactly how the fragment should com-bine with other English words to create a completesentence in a linguistically meaningful way.
Weshow that CCG is a viable formalism to add syntaxto SCFG-based translation.?
We introduce two models for labeling SCFGrules.
One uses labels from a 1-best CCG parsetree of training data; the second uses the top la-bels in each cell of a CCG parse chart.?
We show that using 1-best parses performs aswell as a syntactic model using phrase structurederivations.?
We show that using chart cell labels per-forms almost as well than SAMT, but the non-terminal label set is an order of magnitudesmaller and translation is twice as fast.2 Categorial grammarCategorial grammar (CG) (Adjukiewicz, 1935; Bar-Hillel et al, 1964) is a grammar formalism inwhich words are assigned grammatical types, or cat-egories.
Once categories are assigned to each wordof a sentence, a small set of universal combinatoryrules uses them to derive a sentence-spanning syn-tactic structure.Categories may be either atomic, like N, VP, S,and other familiar types, or they may be complexfunction types.
A function type looks like A/B andtakes an argument of type B and returns a type A.The categories A and B may themselves be eitherprimitives or functions.
A lexical item is assigned afunction category when it takes an argument ?
forexample, a verb may be function that needs to becombined with its subject and object, or an a adjec-tive may be a function that takes the noun it modifiesas an argument.Lexical item Categoryand conjcities NPin (NP\NP)/NPown (S\NP)/NPproperties NPthey NPvarious NP/NPvillages NPTable 1: An example lexicon, mapping words to cat-egories.We can combine two categories with function ap-plication.
Formally, we writeX/Y Y ?
X (2)to show that a function type may be combined withits argument type to produce the result type.
Back-ward function application also exists, where the ar-gument occurs to the left of the function.Combinatory categorial grammar (CCG) is an ex-tension of CG that includes more combinators (op-erations that can combine categories).
Steedmanand Baldridge (2011) give an excellent overview ofCCG.As an example, suppose we want to analyze thesentence ?They own properties in various cities andvillages?
using the lexicon shown in Table 1.
We as-sign categories according to the lexicon, then com-bine the categories using function application andother combinators to get an analysis of S for thecomplete sentence.
Figure 1 shows the derivation.As a practical matter, very efficient CCG parsersare available (Clark and Curran, 2007).
As shownby Fowler and Penn (2010), in many cases CCG iscontext-free, making it an ideal fit for our problem.2.1 Labels for phrasesConsider the German?English phrase pair der gro?eMann ?
the tall man.
It is easily labeled as an NPand included in the translation table.
By contrast,der gro?e?
the tall, doesn?t typically correspond toa complete subtree in a phrase structure parse.
Yettranslating the tall is likely to be more useful thantranslating the tall man, since it is more general?itcan be combined with any other noun translation.223They own properties in various cities and villagesNP (S\NP )/NP NP (NP\NP )/NP NP/NP NP conj NP> <?>NP NP\NP<NP>NP\NP<NP>S\NP<SFigure 1: An example CCG derivation for the sentence ?They own properties in various cities and villages?using the lexicon from Table 1. ?
indicates a conjunction operation; > and < are forward and backwardfunction application, respectively.Using CG-style labels with function types, we canassign the type (for example) NP/N to the tall toshow that it can be combined with a noun on its rightto create a complete noun phrase.1 In general, CGcan produce linguistically meaningful labels of mostspans in a sentence simply as a matter of course.2.2 Minimal, well-motivated label setBy allowing slashed categories with CG, we in-crease the number of labels allowed.
Despite the in-crease in the number of labels, CG is advantageousfor two reasons:1.
Our labels are derived from CCG derivations,so phrases with slashed labels represent well-motivated, linguistically-informed derivations,and the categories can be naturally combined.2.
The set of labels is small, relative to SAMT ?it?s restricted to the labels seen in CCG parsesof the training data.In short, using CG labels allows us to keep morelinguistically-informed syntactic rules without mak-ing the set of syntactic labels too big.3 Translation models3.1 Extraction from parallel textTo extract SCFG rules, we start with a heuristic toextract phrases from a word-aligned sentence pair1We could assign NP/N to the determiner the and N/N to theadjective tall, then combine those two categories using functioncomposition to get a category NP/N for the two words together.Formostpeople,Pourlamajorit?desgens,Figure 2: A word-aligned sentence pair fragment,with a box indicating a consistent phrase pair.
(Tillmann, 2003).
Figure 2 shows a such a pair, witha consistent phrase pair inside the box.
A phrasepair (f, e) is said to be consistent with the alignmentif none of the words of f are aligned outside thephrase e, and vice versa ?
that is, there are no align-ment points directly above, below, or to the sides ofthe box defined by f and e.Given a consistent phrase pair, we can immedi-ately extract the ruleX ?
?f, e?
(3)as we would in a phrase-based MT system.
How-ever, whenever we find a consistent phrase pair thatis a sub-phrase of another, we may extract a hierar-chical rule by treating the inner phrase as a gap inthe larger phrase.
For example, we may extract theruleX ?
?
Pour X ; For X ?
(4)from Figure 3.224Formostpeople,Pourlamajorit?desgens,Figure 3: A consistent phrase pair with a sub-phrasethat is also consistent.
We may extract a hierarchicalSCFG rule from this training example.The focus of this paper is how to assign labelsto the left-hand non-terminal X and to the non-terminal gaps on the right-hand side.
We discuss fivemodels below, of which two are novel CG-based la-beling schemes.3.2 Baseline: HieroHiero (Chiang, 2007) uses the simplest labeling pos-sible: there is only one non-terminal symbol, X , forall rules.
Its advantage over phrase-based translationin its ability to model phrases with gaps in them,enabling phrases to reorder subphrases.
However,since there?s only one label, there?s no way to in-clude syntactic information in its translation rules.3.3 Phrase structure parse tree labelingOne first step for adding syntactic information is toget syntactic labels from a phrase structure parsetree.
For each word-aligned sentence pair in ourtraining data, we also include a parse tree of the tar-get side.Then we can assign syntactic labels like this: foreach consistent phrase pair (representing either theleft-hand non-terminal or a gap in the right handside) we see if the target-language phrase is the exactspan of some subtree of the parse tree.If a subtree exactly spans the phrase pair, we canuse the root label of that subtree to label the non-terminal symbol.
If there is no such subtree, wethrow away any rules derived from the phrase pair.As an example, suppose the English side of thephrase pair in Figure 3 is analyzed asPPINForNPJJmostNNpeopleThen we can assign syntactic labels to Rule 4 to pro-ducePP ?
?
Pour NP ; For NP ?
(5)The rules extracted by this scheme are very sim-ilar to those produced by GHKM (Galley et al,2004), in particular resulting in the ?composedrules?
of Galley et al (2006), though we use sim-pler heuristics for handling of unaligned words andscoring in order to bring the model in line with bothHiero and SAMT baselines.
Under this scheme wethrow away a lot of useful translation rules that don?ttranslate exact syntactic constituents.
For example,we can?t labelX ?
?
Pour la majorite?
des ; For most ?
(6)because no single node exactly spans For most: thePP node includes people, and the NP node doesn?tinclude For.We can alleviate this problem by changing theway we get syntactic labels from parse trees.3.4 SAMTThe Syntax-Augmented Machine Translation(SAMT) model (Zollmann and Venugopal, 2006)extracts more rules than the other syntactic modelby allowing different labels for the rules.
In SAMT,we try several different ways to get a label for aspan, stopping the first time we can assign a label:?
As in simple phrase structure labeling, if a sub-tree of the parse tree exactly spans a phrase, weassign that phrase the subtree?s root label.?
If a phrase can be covered by two adjacent sub-trees with labels A and B, we assign their con-catenation A+B.?
If a phrase spans part of a subtree labeled A thatcould be completed with a subtree B to its right,we assign A/B.225?
If a phrase spans part of a subtree A but is miss-ing a B to its left, we assign A\B.?
Finally, if a phrase spans three adjacent sub-trees with labels A, B, and C, we assignA+B+C.Only if all of these assignments fail do we throwaway the potential translation rule.Under SAMT, we can now label Rule 6.
For isspanned by an IN node, and most is spanned by a JJnode, so we concatenate the two and label the ruleasIN+JJ?
?
Pour la majorite?
des ; For most ?
(7)3.5 CCG 1-best derivation labelingOur first CG model is similar to the first phrase struc-ture parse tree model.
We start with a word-alignedsentence pair, but we parse the target sentence usinga CCG parser instead of a phrase structure parser.When we extract a rule, we see if the consistentphrase pair is exactly spanned by a category gener-ated in the 1-best CCG derivation of the target sen-tence.
If there is such a category, we assign that cat-egory label to the non-terminal.
If not, we throwaway the rule.To continue our extended example, suppose theEnglish side of Figure 3 was analyzed by a CCGparser to produceFor most people(S/S)/N N/N N>N>S/SThen just as in the phrase structure model, weproject the syntactic labels down onto the extractablerule yieldingS/S ?
?
Pour N ; For N ?
(8)This does not take advantage of CCG?s ability tolabel almost any fragment of language: the frag-ments with labels in any particular sentence dependon the order that categories were combined in thesentence?s 1-best derivation.
We can?t label Rule 6,because no single category spanned For most in thederivation.
In the next model, we increase the num-ber of spans we can label.S/SS/S N(S/S)/N N/N NFor peoplemostFigure 4: A portion of the parse chart for a sentencestarting with ?For most people .
.
.
.?
Note that thegray chart cell is not included in the 1-best derivationof this fragment in Section 3.5.3.6 CCG parse chart labelingFor this model, we do not use the 1-best CCG deriva-tion.
Instead, when parsing the target sentence, foreach cell in the parse chart, we read the most likelylabel according to the parsing model.
This lets us as-sign a label for almost any span of the sentence justby reading the label from the parse chart.For example, Figure 4 represents part of a CCGparse chart for our example fragment of ?For mostpeople.?
Each cell in the chart shows the most prob-able label for its span.
The white cells of the chartare in fact present in the 1-best derivation, whichmeans we could extract Rule 8 just as in the previousmodel.But the 1-best derivation model cannot label Rule6, and this model can.
The shaded chart cell in Fig-ure 4 holds the most likely category for the span Formost.
So we assign that label to the X:S/S ?
?
Pour la majorite?
des ; For most ?
(9)By including labels from cells that weren?t usedin the 1-best derivation, we can greatly increase thenumber of rules we can label.4 Comparison of resulting grammars4.1 Effect of grammar size and label set onparsing efficiencyThere are sound theoretical reasons for reducing thenumber of non-terminal labels in a grammar.
Trans-lation with a synchronous context-free grammar re-quires first parsing with the source-language projec-tion of the grammar, followed by intersection of the226target-language projection of the resulting grammarwith a language model.
While there are many possi-ble algorithms for these operations, they all dependon the size of the grammar.Consider for example the popular cube pruningalgorithm of Chiang (2007), which is a simple ex-tension of CKY.
It works by first constructing a setof items of the form ?A, i, j?, where each item corre-sponds to (possibly many) partial analyses by whichnonterminal A generates the sequence of words frompositions i through j of the source sentence.
It thenproduces an augmented set of items ?A, i, j, u, v?, inwhich items of the first type are augmented with leftand right language model states u and v. In eachpass, the number of items is linear in the number ofnonterminal symbols of the grammar.
This observa-tion has motivated work in grammar transformationsthat reduce the size of the nonterminal set, often re-sulting in substantial gains in parsing or translationspeed (Song et al, 2008; DeNero et al, 2009; Xiaoet al, 2009).More formally, the upper bound on parsing com-plexity is always at least linear in the size of thegrammar constant G, where G is often loosely de-fined as a grammar constant; Iglesias et al (2011)give a nice analysis of the most common translationalgorithms and their dependence on G. Dunlop etal.
(2010) provide a more fine-grained analysis of G,showing that for a variety of implementation choicesthat it depends on either or both the number of rulesin the grammar and the number of nonterminals inthe grammar.
Though these are worst-case analyses,it should be clear that grammars with fewer rules ornonterminals can generally be processed more effi-ciently.4.2 Number of rules and non-terminalsTable 2 shows the number of rules we can extractunder various labeling schemes.
The rules were ex-tracted from an Urdu?English parallel corpus with202,019 translations, or almost 2 million words ineach language.As we described before, moving from the phrase-structure syntactic model to the extended SAMTmodel vastly increases the number of translationrules ?
from about 7 million to 40 million rules.But the increased rule coverage comes at a cost: thenon-terminal set has increased in size from 70 (theModel Rules NTsHiero 4,171,473 1Syntax 7,034,393 70SAMT 40,744,439 18,368CG derivations 8,042,683 505CG parse chart 28,961,161 517Table 2: Number of translation rules and non-terminal labels in an Urdu?English grammar undervarious models.size of the set of Penn Treebank tags) to over 18,000.Comparing the phrase structure syntax model tothe 1-best CCG derivation model, we see that thenumber of extracted rules increases slightly, and thegrammar uses a set of about 500 non-terminal labels.This does not seem like a good trade-off; since weare extracting from the 1-best CCG derivation therereally aren?t many more rules we can label than witha 1-best phrase structure derivation.But when we move to the full CCG parse chartmodel, we see a significant difference: when read-ing labels off of the entire parse chart, instead ofthe 1-best derivation, we don?t see a significant in-crease in the non-terminal label set.
That is, mostof the labels we see in parse charts of the train-ing data already show up in the top derivations: thecomplete chart doesn?t contain many new labels thathave never been seen before.But by using the chart cells, we are able to as-sign syntactic information to many more translationrules: over 28 million rules, for a grammar about 34the size of SAMT?s.
The parse chart lets us extractmany more rules without significantly increasing thesize of the syntactic label set.4.3 Sparseness of nonterminalsExamining the histograms in Figure 5 gives us adifferent view of the non-terminal label sets in ourmodels.
In each histogram, the horizontal axis mea-sures label frequency in the corpus.
The height ofeach bar shows the number of non-terminals withthat frequency.For the phrase structure syntax model, we seethere are maybe 20 labels out of 70 that show upon rules less than 1000 times.
All the other labelsshow up on very many rules.227More sparse Less sparse1 10 102 103 104 105 106Label Frequency (logscale)110110110110102103NumberofLabels(logscale)Phrase structureCCG 1-bestCCG chartSAMTFigure 5: Histograms of label frequency for each model, illustrating the sparsity of each model.Moving to SAMT, with its heuristically-definedlabels, shows a very different story.
Not only doesthe model have over 18,000 non-terminal labels, butthousands of them show up on fewer than 10 rulesapiece.
If we look at the rare label types, we see thata lot of them are improbable three way concatena-tions A+B+C.The two CCG models have similar sparsenessprofiles.
We do see some rare labels occurring onlya few times in the grammars, but the number ofsingleton labels is an order of magnitude smallerthan SAMT.
Most of the CCG labels show up inthe long tail of very common occurrences.
Interest-ingly, when we move to extracting labels from parsecharts rather than derivations, the number of labelsincreases only slightly.
However, we also obtain agreat deal more evidence for each observed label,making estimates more reliable.5 Experiments5.1 DataWe tested our models on an Urdu?English transla-tion task, in which syntax-based systems have beenquite effective (Baker et al, 2009; Zollmann et al,2008).
The training corpus was the National Insti-tute of Standards and Technology Open MachineTranslation 2009 Evaluation (NIST Open MT09).According to the MT09 Constrained Training Con-ditions Resources list2 this data includes NIST OpenMT08 Urdu Resources3 and the NIST Open MT08Current Test Set Urdu?English4.
This gives us202,019 parallel translations, for approximately 2million words of training data.5.2 Experimental designWe used the scripts included with the Moses MTtoolkit (Koehn et al, 2007) to tokenize and nor-malize the English data.
We used a tokenizer andnormalizer developed at the SCALE 2009 workshop(Baker et al, 2009) to preprocess the Urdu data.
Weused GIZA++ (Och and Ney, 2000) to perform wordalignments.For phrase structure parses of the English data, weused the Berkeley parser (Petrov and Klein, 2007).For CCG parses, and for reading labels out of a parsechart, we used the C&C parser (Clark and Curran,2007).After aligning and parsing the training data, weused the Thrax grammar extractor (Weese et al,2011) to extract all of the translation grammars.We used the same feature set in all the transla-tion grammars.
This includes, for each rule C ?
?f ; e?, relative-frequency estimates of the probabil-2http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf3LDC2009E124LDC2009E11228Model BLEU sec./sent.Hiero 25.67 (0.9781) 0.05Syntax 27.06 (0.9703) 3.04SAMT 28.06 (0.9714) 63.48CCG derivations 27.3 (0.9770) 5.24CCG parse chart 27.64 (0.9673) 33.6Table 3: Results of translation experiments onUrdu?English.
Higher BLEU scores are better.BLEU?s brevity penalty is reported in parentheses.ities p(f |A), p(f |e), p(f |e,A), p(e|A), p(e|f), andp(e|f,A).The feature set alo includes lexical weighting forrules as defined by Koehn et al (2003) and variousbinary features as well as counters for the number ofunaligned words in each rule.To train the feature weights we used the Z-MERTimplementation (Zaidan, 2009) of the MinimumError-Rate Training algorithm (Och, 2003).To decode the test sets, we used the Joshua ma-chine translation decoder (Weese et al, 2011).
Thelanguage model is a 5-gram LM trained on EnglishGigaWord Fourth Edition.55.3 Evaluation criteriaWe measure machine translation performance usingthe BLEU metric (Papineni et al, 2002).
We alsoreport the translation time for the test set in secondsper sentence.
These results are shown in Table 3.All of the syntactic labeling schemes show an im-provement over the Hiero model.
Indeed, they allfall in the range of approximately 27?28 BLEU.
Wecan see that the 1-best derivation CCG model per-forms slightly better than the phrase structure model,and the CCG parse chart model performs a little bet-ter than that.
SAMT has the highest BLEU score.The models with a larger number of rules performbetter; this supports our assertion that we shouldn?tthrow away too many rules.When it comes to translation time, the threesmaller models (Hiero, phrase structure syntax, andCCG 1-best derivations) are significantly faster thanthe two larger ones.
However, even though the CCGparse chart model is almost 34 the size of SAMT interms of number of rules, it doesn?t take 34 of the5LDC2009T13time.
In fact, it takes only half the time of the SAMTmodel, thanks to the smaller rule label set.6 Discussion and Future WorkFinding an appropriate mechanism to inform phrase-based translation models and their hierarchical vari-ants with linguistic syntax is a difficult problemthat has attracted intense interest, with a varietyof promising approaches including unsupervisedclustering (Zollmann and Vogel, 2011), merging(Hanneman et al, 2011), and selection (Mylonakisand Sima?an, 2011) of labels derived from phrase-structure parse trees very much like those used byour baseline systems.
What we find particularlyattractive about CCG is that it naturally assignslinguistically-motivated labels to most spans of asentence using a reasonably concise label set, possi-bility obviating the need for further refinement.
In-deed, the analytical flexibility of CCG has motivatedits increasing use in MT, from applications in lan-guage modeling (Birch et al, 2007; Hassan et al,2007) to more recent proposals to incorporate it intophrase-based (Mehay, 2010) and hierarchical trans-lation systems (Auli, 2009).Our new model builds on these past efforts, rep-resenting a more fully instantiated model of CCG-based translation.
We have shown that the labelscheme allows us to keep many more translationrules than labels based on phrase structure syntax,extracting almost as many rules as the SAMT model,but keeping the label set an order of magnitudesmaller, which leads to more efficient translation.This simply scratches the surface of possible uses ofCCG in translation.
In future work, we plan to movefrom a formally context-free to a formally CCG-based model of translation, implementing combina-torial rules such as application, composition, andtype-raising.AcknowledgementsThank you to Michael Auli for providing code toinspect the full chart from the C&C parser.
Thisresearch was supported in part by the NSF undergrant IIS-0713448 and in part by the EuroMatrix-Plus project funded by the European Commission(7th Framework Programme).
Opinions, interpreta-tions, and conclusions are the authors?
alone.229ReferencesKazimierz Adjukiewicz.
1935.
Die syntaktische kon-nexita?t.
In Storrs McCall, editor, Polish Logic 1920?1939, pages 207?231.
Oxford University Press.Michael Auli.
2009.
CCG-based models for statisti-cal machine translation.
Ph.D. Proposal, Universityof Edinburgh.Kathy Baker, Steven Bethard, Michael Bloodgood, RalfBrown, Chris Callison-Burch, Glen Coppersmith,Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,Mike Kayser, Lori Levin, Justin Marinteau, Jim May-field, Scott Miller, Aaron Phillips, Andrew Philpot,Christine Piatko, Lane Schwartz, and David Zajic.2009.
Semantically informed machine translation: Fi-nal report of the 2009 summer camp for advanced lan-guage exploration (scale).
Technical report, HumanLanguage Technology Center of Excellence.Yehoshua Bar-Hillel, Chaim Gaifman, and EliyahuShamir.
1964.
On categorial and phrase-structuregrammars.
In Yehoshua Bar-Hillel, editor, Languageand Information, pages 99?115.
Addison-Wesley.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proc.
of WMT.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics, 33(4).Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proc.
EMNLP.John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.2009.
Efficient parsing for transducer grammars.
InProc.
NAACL, pages 227?235.Aaron Dunlop, Nathan Bodenstab, and Brian Roark.2010.
Reducing the grammar constant: an analysisof CYK parsing efficiency.
Technical report CSLU-2010-02, OHSU.Timothy A. D. Fowler and Gerald Penn.
2010.
Accu-rate context-free parsing with combinatory categorialgrammar.
In Proc.
ACL, pages 335?344.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proc.
of EMNLP.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.of HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve Deneefe, Wei Wang, and Ignacio Thayer.2006.
Scalable inference and training of context-richsyntactic translation models.
In In ACL, pages 961?968.Greg Hanneman, Michelle Burroughs, and Alon Lavie.2011.
A general-purpose rule extractor for SCFG-based machine translation.
In Proc.
of WMT.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proc.
of ACL.Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`de Gispert, and Michael Riley.
2011.
Hierarchi-cal phrase-based translation representations.
In Proc.EMNLP, pages 1373?1383.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Frederico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.ACL Demonstration Session.Dennis Mehay.
2010.
Linguistically motivated syntaxfor machine translation.
Ph.D. Proposal, Ohio StateUniversity.Markos Mylonakis and Khalil Sima?an.
2011.
Learninghierarchical translation structure with linguistic anno-tations.
In Proc.
of ACL-HLT.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proc.
of ACL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
of ACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
HLT-NAACL.Xinying Song, Shilin Ding, and Chin-Yew Lin.
2008.Better binarization for the CKY parsing.
In Proc.EMNLP, pages 167?176.Mark Steedman and Jason Baldridge.
2011.
Combina-tory categorial grammar.
In Robert Borsley and KerstiBo?rjars, editors, Non-Transformational Syntax.
Wiley-Blackwell.Christoph Tillmann.
2003.
A projection extension algo-rithm for statistical machine translation.
In Proc.
ofEMNLP.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with the thraxgrammar extractor.
In Proc.
of WMT.Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu, andMing Zhou.
2009.
Better synchronous binarizationfor machine translation.
In Proc.
EMNLP, pages 362?370.230Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91(1):79?88.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation.Andreas Zollmann and Stephan Vogel.
2011.
A word-class approach to labeling PSCFG rules for machinetranslation.
In Proc.
of ACL-HLT.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proc.
of COLING.231
