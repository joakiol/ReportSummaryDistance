Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65?70,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsEvaluate with Confidence Estimation: Machine ranking of translationoutputs using grammatical featuresEleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha BurchardtGerman Research Center for Artificial Intelligence (DFKI)Language Technology (LT), Berlin, Germanyname.surname@dfki.deAbstractWe present a pilot study on an evaluationmethod which is able to rank translation out-puts with no reference translation, given onlytheir source sentence.
The system employs astatistical classifier trained upon existing hu-man rankings, using several features derivedfrom analysis of both the source and the tar-get sentences.
Development experiments onone language pair showed that the methodhas considerably good correlation with humanranking when using features obtained from aPCFG parser.1 IntroductionAutomatic evaluation metrics for Machine Transla-tion (MT) have mainly relied on analyzing both theMT output against (one or more) reference transla-tions.
Though, several paradigms in Machine Trans-lation Research pose the need to estimate the qualitythrough many translation outputs, when no referencetranslation is given (n-best rescoring of SMT sys-tems, system combination etc.).
Such metrics havebeen known as Confidence Estimation metrics andquite a few projects have suggested solutions on thisdirection.
With our submission to the Shared Task,we allow such a metric to be systematically com-pared with the state-of-the-art reference-aware MTmetrics.Our approach suggests building a Confidence Es-timation metric using already existing human judg-ments.
This has been motivated by the existenceof human-annotated data containing comparisons ofthe outputs of several systems, as a result of theevaluation tasks run by the Workshops on StatisticalMachine Translation (WMT) (Callison-Burch et al,2008; Callison-Burch et al, 2009; Callison-Burchet al, 2010).
This amount of data, which has beenfreely available for further research, gives an op-portunity for applying machine learning techniquesto model the human annotators?
choices.
MachineLearning methods over previously released evalua-tion data have been already used for tuning com-plex statistical evaluation metrics (e.g.
SVM-Rankin Callison-Burch et al (2010)).
Our propositionis similar, but works without reference translations.We develop a solution of applying machine learningin order to build a statistical classifier that performssimilar to the human ranking: it is trained to rankseveral MT outputs, given analysis of possible qual-itative criteria on both the source and the target sideof every given sentence.
As qualitative criteria, weuse statistical features indicating the quality and thegrammaticality of the output.2 Automatic ranking method2.1 From Confidence Estimation to rankingConfidence estimation has been seen from the Nat-ural Language Processing (NLP) perspective as aproblem of binary classification in order to assessthe correctness of a NLP system output.
Previ-ous work focusing on Machine Translation includesstatistical methods for estimating correctness scoresor correctness probabilities, following a rich searchover the spectrum of possible features (Blatz et al,2004a; Ueffing and Ney, 2005; Specia et al, 2009;Raybaud and Caroline Lavecchia, 2009; Rosti et al,652007).In this work we slightly transform the binary clas-sification practice to fit the standard WMT humanevaluation process.
As human annotators have pro-vided their evaluation in the form of ranking of fivesystem outputs at a sentence level, we build our eval-uation mechanism with similar functionality, aim-ing to training from and evaluating against this data.Evaluation scores and results can be then calculatedbased on comparative analysis of the performance ofeach system.Whereas latest work, such as Specia et al (2010),has focused on learning to assess segment perfor-mance independently for each system output, ourcontribution measures the performance by compar-ing the system outputs with each other and con-sequently ranking them.
The exact method is de-scribed below.2.2 Internal pairwise decompositionWe build one classifier over all input sentences.While the evaluation mechanism is trained and eval-uated on a multi-class (ranking) basis as explainedabove, the classifier is expected to work on a binarylevel: we provide the features from the analysis ofthe two system outputs and the source, and the clas-sifier should decide if the first system output is betterthan the second one or not.In order to accomplish such training, the n sys-tems?
outputs for each sentence are broken down ton ?
(n ?
1) pairs, of all possible comparisons be-tween two system outputs, in both directions (sim-ilar to the calculation of the Spearman correlation).For each pair, the classifier is trained with a classvalue c, for the pairwise comparison of system out-puts ti and tj with respective ranks ri and rj , deter-mined as:c(ri, rj) ={1 ri < rj?1 ri > rjAt testing time, after the classifier has made allthe pairwise decisions, those need to be convertedback to ranks.
System entries are ordered, accordingto how many times each of them won in the pair-wise comparison, leading to rank lists similar to theones provided by human annotators.
Note that thiskind of decomposition allows for ties when there areequal times of winnings.2.3 Acquiring featuresIn order to obtain features indicating the quality ofthe MT output, automatic NLP analysis tools are ap-plied on both the source and the two target (MT-generated) sentences of every pairwise comparison.Features considered can be seen in the following cat-egories, according to their origin:?
Sentence length: Number of words of sourceand target sentences, source-length to target-length ratio.?
Target language model: Language modelsprovide statistics concerning the correctness ofthe words?
sequence on the target language.Such language model features include:?
the smoothed n-gram probability of theentire target sentence for a languagemodel of order 5, along with?
uni-gram, bi-gram, tri-gram probabilitiesand a?
count of unknown words?
Parsing: Processing features acquired fromPCFG parsing (Petrov et al, 2006) for bothsource and target side include:?
parse log likelihood,?
number of n-best trees,?
confidence for the best parse,?
average confidence of all trees.Ratios of the above target features to their re-spective source features were included.?
Shallow grammatical match: The number ofoccurences of particular node tags on both thesource and the target was counted on the PCFGparses.
In particular, NPs, VPs, PPs, NNs andpunctuation occurences were counted.
Thenthe ratio of the occurences of each tag in thetarget sentence by its occurences on the sourcesentence was also calculated.2.4 ClassifiersThe machine learning core of the system was builtsupporting two classification approaches.66?
Na?ve Bayes allows prediction of a binaryclass, given the assumption that the features arestatistically independent.p(C,F1, .
.
.
, Fn) = p(C)i=1?np(Fi|C)p(C) is estimated by relative frequencies ofthe training pairwise examples, while p(Fi|C)for our continuous features are estimated withLOESS (locally weighted linear regressionsimilar to Cleveland (1979))?
k-nearest neighbour (knn) algorithm allowsclassifying based on the closest training exam-ples in the feature space.3 Experiment3.1 Experiment setupA basic experiment was designed in order to deter-mine the exact setup and the feature set of the metricprior to the shared task submission.
The classifiersfor the task were learnt using the German-Englishtestset of the WMT 2008 and 2010 (about 700 sen-tences)1.
For testing, the classifiers were used to per-form ranking on a test set of 184 sentences whichhad been kept apart from the 2010 data, with the cri-terion that they do not contain contradictions amonghuman judgments.In order to allow further comparison with otherevaluation metrics, we performed an extended ex-periment: we trained the classifiers over the WMT2008 and 2009 data and let them perform automaticranking on the full WMT 2010 test set, this timewithout any restriction on human evaluation agree-ment.In both experiments, tokenization was performedwith the PUNKT tokenizer (Kiss et al, 2006; Gar-rette and Klein, 2009), while n-gram features weregenerated with the SRILM toolkit (Stolcke, 2002).The language model was relatively big and had beenbuilt upon all lowercased monolingual training setsfor the WMT 2011 Shared Task, interpolated onthe 2007 test set.
As a PCFG parser, the BerkeleyParser (Petrov and Klein, 2007) was preferred, due1data acquired from http://www.statmt.org/wmt11to the possibility of easily obtaining complex inter-nal statistics, including n-best trees.
Unfortunately,the time required for parsing leads to significant de-lays at the overall processing.
The machine learn-ing algorithms were implemented with the Orangetoolkit (Dem?ar et al, 2004).3.2 Feature selectionAlthough the automatic NLP tools provided a lot offeatures (section 2.3), the classification methods weused (and particularly na?ve Bayes were the develop-ment was focused on) would be expected to performbetter given a smaller group of statistically inde-pendent features.
Since exhaustive training/testingof all possible feature subsets was not possible,we performed feature selection based on the Reli-eff method (Kononenko, 1994; Kira and Rendell,1992).
Automatic ranking was performed based onthe most promising feature subsets.
The results areexamined below.3.3 ResultsThe performance of the classifier is measured afterthe classifier output has been converted back to ranklists, similar to the WMT 2010 evaluation.
We there-fore calculated two types of rank coefficients: aver-aged Kendall?s tau on a segment level, and Spear-man?s rho on a system level, based on the percentagethat the each system?s translations performed betterthan or equal to the translations of any other system.The results for the various combinations of fea-tures and classifiers are depicted on Table 1.
Na?veBayes provides the best score on the test set, with?
= 0.81 on a system level and ?
= 0.26 on asegment level, trained with features including thenumber of the unknown words, the source-lengthby target-length ratio, the VP count ratio and thesource-target ratio of the parsing log-likelihood.
Thenumber of unknown words particularly appears to bea strong indicator for the quality of the sentence.
Onthe first part of the table we can also observe thatlanguage model features do not perform as well asthe features deriving from the processing informa-tion delivered by the parser.
On the second part ofthe table we compare the use of various grammaticalcombinations.
The third part contains the correlationobtained by various similar internal parsing-relatedfeatures.67features na?ve Bayes knnrho tau rho taubasic experimentngram 0.19 0.05 0.13 0.01unk, len 0.67 0.20 0.73 0.24unk, len, bigram 0.61 0.21 0.74 0.21unk, len, ngram 0.63 0.19 0.59 0.21unk, len, trigram 0.67 0.20 0.76 0.21unk, len, logparse 0.75 0.21 0.74 0.25unk, len, nparse, VP 0.67 0.24 0.61 0.20unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24unk, len, nparse, NP, confbestparse 0.78 0.23 0.74 0.23unk, len, nparse, VP, confavg 0.75 0.21 0.78 0.23unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24unk, len, nparse, VP, logparse 0.81 0.26 0.75 0.23extended experimentunk, len, nparse, VP, logparse 0.60 0.23 0.28 0.02Table 1: System-level Spearman?s rho and segment-level Kendall?s tau correlation coefficients achieved on automaticranking (average absolute value)The correlation coefficients of the extended exper-iment, allowing comparison with last year?s sharedtask, are shown on the last line of the table.
Withcoefficients ?
= 0.60 and ?
= 0.23, our metricperforms relatively low compared to the other met-rics of WMT10 (indicatively iBLEU: ?
= 0.95,?
= 0.39 according to Callison-Burch et al (2010).Though, it still has a position in the list, scoring bet-ter than several other reference-aware metrics (e.g.of ?
= 0.47 and ?
= 0.12 respectively) for the par-ticular language pair.4 DiscussionA concern on the use of Confidence Estimation forMT evaluation has to do with the possibility of asystem ?tricking?
such metrics.
This would for ex-ample be the case when a system offers a well-formed candidate translation and gets a good score,despite having no relation to the source sentencein terms of meaning.
We should note that we arenot capable of fully investigating this case basedon the current set of experiments, because all ofthe systems in our data sets have shown acceptablescores (11-25 BLEU and 0.58-0.78 TERp accord-ing to Callison-Burch et al (2010)), when evaluatedagainst reference translations.
Though, we wouldassume that we partially address this problem by us-ing ratios of source to target features (length, syn-tactic constituents), which means that in order for asentence to trick the metric, it would need a com-parable sentence length and a grammatical structurethat would allow it to achieve feature ratios similarto the other systems?
outputs.
Previous work (Blatzet al, 2004b; Ueffing and Ney, 2005) has used fea-tures based on word alignment, such as IBM Mod-els, which would be a meaningful addition from thisaspect.Although k-nearest-neighbour is considered to bea superior classifier, best results are obtained byna?ve Bayes.
This may have been due of the factthat feature selection has led to small sets of uncor-related features, where na?ve Bayes is known to per-form well.
K-nearest-neighbour and other complexclassification methods are expected to prove usefulwhen more complex feature sets are employed.5 Conclusion and Further workThe experiments presented in this article indicatethat confidence metrics trained over human rankingscan be possibly used for several tasks of evaluation,given particular conditions, where e.g.
there is noreference translation given.
Features obtained from68a PCFG parser seem to be leading to better correla-tions, given our basic test set.
Although correlationis not particularly high, compared to other reference-aware metrics in WMT 10, there is clearly a poten-tial for further improvement.Nevertheless this is still a small-scale experiment,given the restricted data size and the single transla-tion direction.
The performance of the system onbroader training and test sets will be evaluated in thefuture.
Feature selection is also subject to changeif other language pairs are introduced, while moresophisticated machine learning algorithms, allowingricher feature sets, may also lead to better results.AcknowledgmentsThis work was done with the support of theTaraXU?
Project2, financed by TSB Technologie-stiftung Berlin?Zukunftsfonds Berlin, co-financedby the European Union?European fund for regionaldevelopment.ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004a.
Confidence estimation formachine translation.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004b.
Confidence estimation formachine translation.
In M. Rollins (Ed.
), Mental Im-agery.
Yale University Press.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 70?106, Columbus, Ohio, June.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2http://taraxu.dfki.de2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.
Revised August 2010.William S. Cleveland.
1979.
Robust locally weightedregression and smoothing scatterplots.
Journal of theAmerican statistical association, 74(368):829?836.Janez Dem?ar, Blaz Zupan, Gregor Leban, and TomazCurk.
2004.
Orange: From experimental machinelearning to interactive data mining.
In Principles ofData Mining and Knowledge Discovery, pages 537?539.Dan Garrette and Ewan Klein.
2009.
An extensi-ble toolkit for computational semantics.
In Proceed-ings of the Eighth International Conference on Com-putational Semantics, IWCS-8 ?09, pages 116?127,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Kenji Kira and Larry A. Rendell.
1992.
The feature se-lection problem: traditional methods and a new algo-rithm.
In Proceedings of the tenth national conferenceon Artificial intelligence, AAAI?92, pages 129?134.AAAI Press.Tibor Kiss, Jan Strunk, Ruhr universit?t Bochum, andRuhr universit?t Bochum.
2006.
Unsupervised mul-tilingual sentence boundary detection.
In Proceedingsof IICS-04, Guadalajara, Mexico and Springer LNCS3473.Igor Kononenko.
1994.
Estimating attributes: analy-sis and extensions of relief.
In Proceedings of theEuropean conference on machine learning on Ma-chine Learning, pages 171?182, Secaucus, NJ, USA.Springer-Verlag New York, Inc.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In In HLT-NAACL ?07.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In In ACL ?06, pages 433?440.Sylvain Raybaud and Kamel Smaili Caroline Lavecchia,David Langlois.
2009.
Word-and sentence-level con-fidence measures for machine translation.
In Euro-pean Association of Machine Translation 2009.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.2007.
Combining outputs from multiple machinetranslation systems.
In Proceedings of the NorthAmerican Chapter of the Association for Compu-tational Linguistics Human Language Technologies,pages 228?235.69Lucia Specia, Marco Turchi, Zhuoran Wang, JohnShawe-Taylor, and Craig Saunders.
2009.
Improv-ing the confidence of machine translation quality es-timates.
In Machine Translation Summit XII, Ottawa,Canada.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.
Ma-chine translation evaluation versus quality estimation.Machine Translation, 24:39?50, March.Andreas Stolcke.
2002.
Srilm?an extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing(ICSLP 2002, pages 901?904.Nicola Ueffing and Hermann Ney.
2005.
Word-levelconfidence estimation for machine translation usingphrase-based translation models.
Computational Lin-guistics, pages 763?770.70
