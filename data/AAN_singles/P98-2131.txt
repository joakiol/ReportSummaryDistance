An Architecture for Dialogue Management, Context Tracking,and Pragmatic Adaptation in Spoken Dialogue SystemsSusann LuperFoy, Dan Loehr, David Duff, Keith Miller, Florence Reeder, Lisa HarperThe MITRE Corporation1820 Dolley Madison Boulevard, McLean VA 22102 USA{luperfoy, loehr, duff, keith, freeder, lisah } @mitre.orgAbstractThis paper details a software architecture fordiscourse processing in spoken dialoguesystems, where the three component tasks ofdiscourse processing are (1) Dialogue Man-agement, (2) Context Tracking, and (3)Pragmatic Adaptation.
We define these threecomponent tasks and describe their roles in acomplex, near-future scenario in whichmultiple humans interact with each otherand with computers in multiple, simulta-neous dialogue exchanges.
This paperreports on the software modules that accom-plish the three component tasks of discourseprocessing, and an architecture for the inter-action among these modules and with othermodules of the spoken dialogue system.
Amotivation of this work is reusable discourseprocessing software for integration withnon-discourse modules in spoken dialoguesystems.
We document the use of this ar-chitecture and its components in severalprototypes, and also discuss its potential ap-plication to spoken dialogue systems definedin the near-future scenario.IntroductionWe present an architecture for spoken dialoguesystems for both human-computer interactionand computer mediation or analysis of humandialogue.
The architecture shares many compo-nents with those of existing spoken dialoguesystems, such as CommandTalk (Moore et al1997), Galaxy (Goddeau et al 1994), TRAINS(Allen et al 1995), Verbmobil (Wahlster 1993),Waxholm (Carlson 1996), and others.
Our ar-chitecture is distinguished from these in itstreatment of discourse-level processing.Most architectures, including ours, contain mod-ules for speech recognition and natural languageinterpretation (such as morphology, syntax, andsentential semantics).
Many include a modulefor interfacing with the back-end application.
Ifthe dialogue is two-way, the architectures alsoinclude modules for natural language generationand speech synthesis.Architectures differ in how they handle dis-course.
Some have a single, separate modulelabeled "discourse processor", "dialogue com-ponent", or perhaps "contextual interpretation".Others, including earlier versions of our system,bury discourse functions inside other modules,such as natural language interpretation or theback-end interface.An innovation of this work is the compartmen-talization of discourse processing into three gen-erically definable components--Dialogue Man-agement, Context Tracking, and Pragmatic Ad-aptation (described in Section 1 below)--and thesoftware control structure for interaction be-tween these and other components of a spokendialogue system (Section 2).In Section 3, we examine the dialogue process-ing requirement in a complex scenario involv-ing multiple users and multiple simultaneousdialogues of diverse types.
We describe howour architecture supports implementations ofsuch a scenario.
Finally, we describe two im-plemented spoken dialogue systems that embodythis architecture (Section 4).1 Component Tasks of DiscourseProcessingWe divide discourse-level processing into threecomponent tasks: Dialogue Management, Con-text Tracking, and Pragmatic Adaptation.1.1 Dialogue ManagementThe Dialogue Manager is an oversight modulewhose purpose is to facilitate the interactionbetween dialogue participants.
In a user-initiatedsystem, the dialogue manager directs the proc-essing of an input utterance from one componentto another through interpretation and back-end794system response.
In the process, it detects andhandles dialogue trouble, invokes the contexttracker when updates are necessary, generatessystem output, and so on.Our conception of Dialogue Manager as con-troller becomes increasingly relevant as thesoftware system moves away from the standard"NL pipeline" in order to deal with dialoguedisfluencies.
Its oversight perspective affords it(and the architecture) certain capabilities, whichare listed in Table 1.1 Supports mixed-initiative system by fielding sponta-neous input from either participant and routing it tothe appropriate components.2 Supports non-linguistic dialogue "events" by accept-ing them and routing them to the Context Tracker(below).53 Increases overall system performance.
For example,awareness of system output allows the DialogueManager to predict user input, boosting speechrecognition accuracy.
Similarly, if the back-end intro-duces a new word into the discourse, the DialogueManager can request he speech recognizer to add itto its vocabulary for later reco\[nition.4 Supports meta-dialogues between the dialogue sys-tem itself and either participant.
An example might bea participant's questions about the status of the dia-lo\[ue s2/stem.Acts as a central point for dialogue troubleshooting,after (Duff et al 1996).
If any component has insuffi-cient input to perform its task, it can alert the Dia-logue Manager, which can then reconsult apreviouslyinvoked component for different output.Table 1.
Dialogue Manager CapabilitiesThe Dialogue Manager is the primary locus ofthe dialogue agent's outward personality as afunction of interaction style; its simple protocolspecifies conditions for interrupting user speechfor permitting interruption by the user, when toinitiate repair dialogues, and how often to back-channel.1.2 Context TrackingThe Context Tracker maintains a record of thediscourse context which it and other componentscan consult in order to (a) resolve dependentforms that occur in input utterances and (b) gen-erate appropriate context-dependent forms forachieving natural output.
Interpretation f defi-nite pronouns, demonstratives (this, those), in-dexicals (you, now, here, tomorrow), definiteNPs (a car...the car), one-anaphora (the earlierone) and ellipsis (how about Seattle) all rely onstored context.The Context Tracker strives to record only thoseentities and events that could become ligible forreference.
Context hus includes linguistic om-municative acts (verbalizations), non-linguisticcommunicative acts (gesture), and non-communicative events that are deemed salient.Since determining salience requires a judge-ment, our implementations rely on heuristicrules to decide which events and objects getentered into the context representation.
For ex-ample, the disappearance of a simulated vehicleoff the edge of a map display might be deemedsalient relative to a particular user model, thediscourse history, or the task structure.1.3 Pragmatic AdaptationThe Pragmatic Adaptation module serves as theboundary between language and action by de-termining what action to take given an inter-preted input utterance or a back-end response.This module's role is to "make sense" of acommunicative act in the current linguistic andnon-linguistic context.The Pragmatic Adapter eceives an interpreta-tion of an input utterance with context-dependent forms resolved.
It then proceeds totranslate that utterance into a valid back-endcommand.
It checks for violations of the Do-main Model, which contains information aboutthe back-end system such as allowable parame-ter values for command arguments.
It alsochecks for commands that are infelicitous giventhe current Back-end State (e.g., the referencedvehicle does not exist at the moment).
ThePragmatic Adapter combines the result of thesesimple tests and a set of if-then heuristics todetermine whether to send through the commandor to intercept the utterance and notify the Dia-logue Manager to initiate a repair dialogue withthe user.The Pragmatic Adapter receives output re-sponses from the back-end and adapts or "trans-lates" them into natural anguage communica-tions which get incorporated by the ContextTracker into the dialogue history.2 An Architecture for SpokenDialogue SystemsHaving introduced our three discourse compo-nents, we now present our overall architecture.It is laid out in Figure 1, and its components aredescribed in Table 2, starting from the user andgoing clockwise.
The discourse components are795left in white, while non-discourse componentshave been shaded gray.- Communication Link ~ = Default Order of Firing (changeable y Dialogue Manager)SpeechRecognitionSpeechSynthesisNatural ~ Context'Language Trackingaterpretation (on Input)NaturalLanguageGenerationDialogueManagerPragmaticAdaptation(on Input)Back-End " ~PragmaticAdaptation 1k (on Output) !Context :~/Tracking ~Figure 1.
An Architecture for Spoken Dialogue Systems, with Discourse Components in White".ml~Oncnt (A,~,cnt) Bri~f l)cscription Pos.~'ihlc Inlml Pos.~ihh" ().timSpeech Reco\[nition::'NL Interp~tation :Context Trackingon InputPragmatic Adaptationon InputConvert waveform to Strin~ 6f words : : :Converi words to meaning representationTrack discourse ntities of input utterance,resolve dependent referencesConvert logical form to back-end commandWaveform ::: Text striri\[ ~ .
"~Logical form (withdependent references)Logical formText string: :~: ,: :Lo~ic~ form, /~,,,Logical form (withdependent referencesreplaced by their referents)Back-end commandPragmatic Adaptationon OutputContext Trackingon OutputDialogue ManagerConvert back-end response to logical formrepresentation of communicative actTrack discourse ntities of output utterance,insert dependent references (ifdesired)High-level control, intelligently routeinformation between all agents and partici-pants (see section 1.1) based on its ownprotocol for interaction.Back-end responseLogical form (w/outdependent references)VariousLogical formLogical form (conditionedby discourse context)Various796Table 2.
Description of the Architecture Components, with Discourse Components in WhiteSeveral items are of note in Figure 1 and Table2.
First, although a default firing order isshown, this order is perturbed any time dialoguetrouble arises.
For example, a Speech Recogni-tion (SR) error, may be detected after NaturalLanguage Interpretation fails to parse the outputof SR. Rather than continuing the flow on to-wards the back-end, the Dialogue Manager canre-consult SR for other hypotheses.
Alterna-tively, the Dialogue Manager can fire NaturalLanguage Generation with an output request forclarification.
That request gets incorporated intothe context representation by Context Tracking,the dialogue state is "pushed" in a repair dia-logue, and a string is ultimately sent to SpeechSynthesis for delivery to the user's ear.
The nextutterance is then interpreted in the context of therepair dialogue.Note also that Context Tracking and PragmaticsAdaptation are called twice each: on "input"(from the user), and on "output" (from the back-end).
The logical Context Tracker may be im-plemented as one or as two related modules,together tracking both sides of that dialogue sothat either user or system can make anaphoricmention of entities introduced earlier.3 A Near-Future Scenario of SpokenDialogue Systems3.1 The ScenarioWe build on images from the popular sciencefiction series Star Trek as a rich source of dia-logue types in complex interrelations.
Theseexample dialogues have more primitive cousinsunder development today.Briefly, our example dialogue types are listed inTable 3.Dialoguewith anApplianceDialoguewith anApplicationDialoguewith anIntelligentRobotComputerMediationof HumanDialogueComputerAnalysisof HumanDialogueDialoguebetween2 charactersFoodReplicatorShip'sComputerAndroid"Data"UniversalTranslatorConver-sationPlaybackHolodeckThe "Food Replicator" on Star Trek acceptsstructured English command language such as"Tea.
Earl Grey.
Hot" and produces results in thephysical world.The ship's computer on Star Trek is an advancedapplication which can understand atural lan-guage queries, and replies either via actions orvia a multimodal interface.
"Data" on Star Trek converses as a human whileproviding information processing ofa computerand is capable of action in the physical world.Star Trek's "Universal Translator" iscapable ofautomatically interpreting between any twohumansThe ship's computer has the ability to retrieve,play back, and analyze previously-recordedconversations.
In this sense, the dialoguebecomes empirical data to be analyzed.Star Trek's "Holodeck" creates imulated hu-mans (or characters) as actors, for the entertain-ment or training of human viewers.HumanHumanHumanHumanHumanCharacterFoodReplicatorShip'sComputerAndroid"Data"HumanHumanCharacterTable 3.
A Scenario f Dialogue TypesUniversalTranslatorShip'sComputer7973.2 Application of the Architecture tothe ScenarioWe now describe the role our architecture, andspecifically our discourse components, play inthese near-future xamples.3.2.1 Dialogue with a Back-End ComputerThe first three examples illustrate dialogues inwhich a human is talking to a computer.
Onedimension distinguishing the three examples isthe agent's intelligent use of context.
In a dia-logue with an "appliance", simple, structured,unambiguous command language utterances areinterpreted one at a time in isolation from priordialogue history.
The Pragmatic Adaptationfacility can follow a simple scheme for mappingeach utterance to one of a very few back-endcommands.
The Context Tracker has no cross-sentence dependent references to contend with,and finally, since the appliance provides no lin-guistic feedback, the Dialogue Manager firesnone of the "output" components (from back-end to human).
In a dialogue with more sophis-ticated application or with a robot, the DialogueManager, Context Tracker, and PragmaticAdapter need greater functionality, to handleboth linguistic and non-linguistic events in bothdirections.3.2.2 Computer-Mediated DialogueThe fourth example, that of the UniversalTranslator, is representative of a general dia-logue type we label Mediator, in which an agentplays a mediation role between humans.
In ad-dition to interpretation, other roles of the me-diator might be (Table 4):lediatorRol~A Genie, which is available for meta-dialogues withthe system itself, instead of with the dialogue partner(much as a human might ask an interpreter torepeatthe partner's last utterance).A Moderator, which, in multi-party dialogues, en-forces an agreed-upon i teraction protocol, such asRobert's Rules of Order or a talk-show format (undercontrol of the host).3 A Bouncer, which decides who may join the dialoguebased on current enrollment (first-come-first-served),clearance l vel, invitation list, etc., as well as permit-ting different types of participation, sothat some mayonly listen while others may fully participate.4 A Stenographer, which records the dialogue, andprepares a "visualization" ofthe dialogue structure.Table 4.
Roles of a Mediator AgentOur architecture is applicable to mediated ia-logues as well.
In fact, it was first developed forbilingual dialogue in a voice-to-voice machinetranslation application.
In this application, theDialogue Manager is available for meta-dialogues with either user (as in Could you re-peat her last utterance?
), and the ContextTracker can use a single discourse representationstructure to track the unfolding context in bothlanguages.3.2.3 Computer-Analyzed DialogueOur fifth example, a post-hoc analysis of a dia-logue, does not require real-time processing.
Itis, nonetheless, a dialogue which can be ana-lyzed using the components of our architecture,exactly as if it were real-time.
The only differ-ence is that no generation will be required, onlyanalysis; thus, the Dialogue Manager need onlyfire the "input" components on each utterance.3.2.4 Character-Character DialogueOur last example concerns a simulated humandialogue between two computer characters, forthe benefit of human viewers.
Such character-character dialogues have been produced by sev-eral researchers, including (Kalra et al 1998).Here, the architecture applies at two levels.First, the architecture can be internal to eachagent, to implement that agent's conversationalability.
Second, the architecture can be usedexternally to analyze the agents' dialogue, asdiscussed in the previous ection.4 Implementations of the ArchitectureWe have implemented two spoken dialoguesystems using the architecture presented.
Thefirst is a telephone-based interface to a simulatedemployee Time Reporting System (TRS), asmight be used at a large corporation.
We thenported the system to a spoken interface to a bat-tlefield simulation (Modular Semi-AutomatedForces, or ModSAF).In our implementation of this architecture, achcomponent is a unique agent which may resideon its own platform and communicate over anetwork.
The middleware our agents use tocommunicate is the Open Agent Architecture(OAA) (Moran et al 1997) from SRI.
TheOAA's flexibility allowed us to easily hook upmodules and experiment with the division oflabor between the three discourse componentswe are studying.
We treat the Dialogue Manageras a special OAA agent that insists on beingcalled frequently so that it can monitor the pro-gress of communicative events through the sys-tem.7984.1 The Time Reporting System (TRS)The architecture components in our TRS systemare listed in Table 5, along with their specificimplementations used.
Each implemented mod-ule included a thin OAA agent layer, allowing itto communicate via the OAA.. ?
~ i I "?
NL !nterpre~ion/Generation S imulated  :i~, '!,,L ~:~i"'Back-End Interface ~:~~:~-::~: "::Simulated ~ '=~ ~.~:Context Trackin\[ (LuperFo~, 1992)Pra\[\[matic Adaptation Currently, SimulatedDialo\[\[ue Manager Current DevelopmentTable 5.
Components ofTRS System, withDiscourse Components in WhiteComponents not in our focus (shaded in gray)are either commercial or simulated software.
ForContext Tracking, we use an algorithm based on(LuperFoy 1992).
For Dialogue Management,we developed a simple agent able to control asystem-initiated dialogue, as well as handle non-linguistic events from the back-end.
The thirddiscourse component, Pragmatic Adaptation,awaits future research, and was simulated forthis system.Figure 2 presents a sample TRS dialogue.System: Welcome.
What is your employee number?User: 12345System: What is your password?User: 54321System: How can I help you?User: What's the first charge number?System: 123GK498JUser: What's the name of that task?System: Project XUser: Charge 6 hours to it today for me.System: 6 hours has been charged to Project X.Figure 2.
Sample TRS DialogueWhen the user logs in, the back-end systembrings up a non-linguistic event--the list oftasks, with associated charge numbers, whichbelong to the user.
The Dialogue Manager e-ceives this and passes it to the Context Tracker.The Context Tracker is then able to resolve thefirst charge number, as well as subsequent de-pendent references such as that task, it, and to-day.4.2 The ModSAF InterfaceWe ported the TRS demo to a simulated battle-field back-end called ModSAF.
We used thesame components with the exception of thespeech recognizer and the back-end interface.The Dialogue Manager was improved over theTRS demo in several ways.
First, we added thecapability of the Dialogue Manager to dynami-cally inform the speech recognizer of what inputto expect, i.e., which language model to use.
TheDialogue Manager could also add words to thespeech recognizer's vocabulary on the fly.
Wechose Nuance (from Nuance Communications)as our speech recognition component specifi-cally because it supports uch run-time updates.Figure 3 presents a sample ModSAF dialogue.Note that only the user speaks.?
Create an M 1 A2 platoon.?
Name it Bravo.?
Give it location 4 9 degrees 3 0 minutes north,1 1 degrees 4 5 minutes east.?
Bravo, advance to checkpoint Charlie.
(At this point, a new platoon appears on the screen,created by another player in the simulation)?
Zoom in on that new platoon.?
Bravo, change location and approach X.
(Where X is the name of the new platoon.
)Figure 3.
Sample ModSAF DialogueWhen the user asks to create an entity, the Dia-logue Manager detects the beginning of a sub-dialogue, and informs the speech recognizer torestrict its expected grammar to that of entitycreation (name and location).
Later, the back-end (ModSAF) sends the Dialogue Manager anon-linguistic event, in which a different platoon(created by another player in the simulation)appears.
This event includes a name for the newplatoon; the Dialogue Manager passes this to thespeech recognizer, so that it may later recognizeit.
In addition, the event is passed to the ContextTracker, so that it may later resolve the referencethat new platoon.To illustrate some advantages of our architec-ture, we briefly mention what we needed tochange to port from TRS to ModSAF.
First, theContext Tracker needed no change atall--operating on linguistic principles, it is do-main-independent.
LuperFoy's framework doesprovide for a layer connected to a knowledgesource, for external context--this would need tobe changed when changing domains.
The Dia-logue Manager also required little change to itscore code, adding only the ability to influence799the speech recognizer.
The Pragmatic Adapta-tion Module, being dependent on the domain ofthe back-end, is where most changes are neededwhen switching domains.ConclusionWe have presented a modular, flexible architec-ture for spoken dialogue systems which sepa-rates discourse processing into three componenttasks with three corresponding software mod-ules: Dialogue Management, Context Tracking,and Pragmatic Adaptation.
We discussed theroles of these components in a complex, near-future scenario portraying a variety of dialoguetypes.
We closed by describing implementationsof these dialogues using the architecture pre-sented, including development and porting of thefirst two discourse components.The architecture itself is derived from a standardblackboard control structure.
This is appropriatefor our current dialogue processing research intwo ways.
First, it does not require a prior fullenumeration of all possible subroutine firingsequences.
Rather, the possibilities emerge fromlocal decisions made by modules that communi-cate with the blackboard, depositing data andconsuming data from the blackboard.
Second,as we learn categories of dialogue segmenttypes, we can move away from the fully decen-tralized control structure, to one in which thecentral Dialogue Manager, as a blackboardmodule with special status, assumes increasingdecision power for processing flow, in cases ofdialogue segment type with which it is familiar.The intended contribution of this work is thus inthe generic definition of standard ialogue func-tions such as dynamic troubleshooting (repair),context updating, anaphora resolution, andtranslation of natural language interpretationsinto functional interface languages of back-endsystems.Future work includes investigation of issuesraised when a human is engaged in more thanone of our scenario dialogues concurrently.
Forexample, how does one speech enabled ialoguesystem among many determine when it is beingaddressed by the user, and how can the systemjudge whether the current utterance is human-computer, i.e., to be fully interpreted and actedupon by the system as opposed to a human-human utterance that is to be simply recorded,transcribed, or translated without interpretation.ReferencesAllen J., Schubert L., Ferguson G., Heeman P.,Hwang C., Kato T., Light M., Martin N., Miller B.,Poesio M., Traum D. (1995) The TRAINS Project:A case study in building a conversational planningagent.
Journal of Experimental nd Theoretical AI,7, pp.
7 48.Carlson R. (1996) The Dialogue Component in theWaxholm System.
Proc.
Twente Workshop on Lan-guage Technology: Dialogue Management i  Natu-ral Language Systems, University of Twente, theNetherlands.Duff D., Gates B., LuperFoy S. (1996) A CentralizedTroubleshooting Mechanism for a Spoken Dia-logue Interface to a Simulation Application.
Proc.International Conference on Spoken LanguageProcessing.Goddeau D., Brill E., Glass J., Pao C., Phillips M.,Polifroni J., Seneff S., Zue V. (1994) GALAXY: AHuman-Language Interface to On-line Travel In-formation.
Proc.
International Conference on Spo-ken Language Processing.Kalra P., Thalmann N., Becheiraz, P., Thalmann D.(1998) Communication Between Synthetic Actors.In "Automated Spoken Dialogue Systems", S. Lu-perFoy, ed.
MIT Press (forthcoming).LuperFoy.
S (1992) The Representation of Multi-modal User-Interface Dialogues Using DiscoursePegs.
Proc.
Annual Meeting of the Association forComputational Linguistics.Moore R., Dowding J., Bratt H., Gawron J., GorfuY., Cheyer, A.
(1997) CommandTalk: A Spoken-Language Interface for Battlefield Simulations.Proc.
Fifth Conference on Applied Natural Lan-guage Processing.Moran D., Cheyer A., Julia L., Martin D. Park S.(1997) Multimodal User Interfaces in the OpenAgent Architecture.
Proc.
International Conferenceon Intelligent User Interfaces.Wahlster W. (1993) Verbmobil: Translation of Face-To-Face Dialogues.
In "Grundlagen und An-wendungen der Ktinstlichen Intelligenz", O. Her-zog, T. Christaller, D. Schiitt, eds., Springer.800RdsumdCet article ddtaille une architecture de logicielpour le traitement de discours dans les syst6mesde dialogue oral, o/l figurent les trois t~chessuivantes: (1) gestion de dialogue, (2) tracementde contexte, et (3) adaptation pragmatique.Nous expliquons ces trois t~ches composantes tddcrivons leurs r61es dans un scdnario complexedu proche avenir dans lequel les humains et lesordinateurs agissent les uns sur les autres, touten faisant pattie de multiples dialoguessimultands.
Cet article rend compte des modulesqui s'occupent des trois taches composantes dutraitement de discours, et d'une architecturefacilite l'interaction de ces modules entre eux etavec d'autres modules du syst6me.
Ce travail apour but de ddvelopper un logiciel pour letraitement de discours qui peut ~tre et intdgrdavec des modules non-discours dans lessyst6mes de dialogue oral.
Nous exposonsl'utilisation de cette architecture dans plusieursprototypes, et nous discutons dgalement lapossibilitd e l'application de l'architecture etdeses composants aux syst6mes de dialogueindiquds dans le scdnario proche-avenir.801
