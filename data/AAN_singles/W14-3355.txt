Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 420?425,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsRED: DCU-CASICT Participation in WMT2014 Metrics TaskXiaofeng Wu?, Hui Yu?, Qun Liu??
?CNGL Centre for Global Intelligent ContentSchool of Computing, Dublin City UniversityDublin 9, Ireland?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of SciencesBeijing, China{xiaofengwu,qliu}@computing.dcu.ie, yuhui@ict.ac.cnAbstractBased on the last year?s DCU-CASISTparticipation on WMT metrics task, wefurther improve our model in the follow-ing ways: 1) parameter tuning 2) supportlanguages other than English.
We tunedour system on all the data of WMT 2010,2012 and 2013.
The tuning results as wellas the WMT 2014 test results are reported.1 IntroductionAutomatic evaluation plays a more and more im-portant role in the evolution of machine transla-tion.
There are roughly two categories can be seen:namely lexical information based and structuralinformation based.1) Lexical information based approaches,among which, BLEU (?
), Translation Error Rate(TER) (?)
and METEOR (?)
are the most popularones and, with simplicity as their merits, cannotadequately reflect the structural level similarity.2) A lot of structural level based methodshave been exploited to overcome the weaknessof the lexical based methods, including SyntacticTree Model(STM)(?
), a constituent tree based ap-proach, and Head Word Chain Model(HWCM)(?
),a dependency tree based approach.
Both ofthe methods compute the similarity between thesub-trees of the hypothesis and the reference.Owczarzak et al (?
; ?
; ?)
presented a methodusing the Lexical-Functional Grammar (LFG) de-pendency tree.
MAXSIM (?)
and the method pro-posed by Zhu et al (?)
also employed the syntac-tic information in association with lexical infor-mation.
As we know that the hypothesis is poten-tially noisy, and these errors are enlarged throughthe parsing process.
Thus the power of syntacticinformation could be considerably weakened.In this paper, based on our attempt on WMTmetrics task 2013 (?
), we propose a metrics namedRED ( REference Dependency based automaticevaluation method).
Our metrics employs only thereference dependency tree which contains both thelexical and syntactic information, leaving the hy-pothesis side unparsed to avoid error propagation.2 Parameter TuningIn RED, we use F -score as our final score.F -score is calculated by Formula (1), where ?
isa value between 0 and 1.F -score =precision ?
recall?
?
precision+ (1?
?)
?
recall(1)The dependency tree of the reference and thestring of the translation are used to calculate theprecision and recall.
In order to calculate preci-sion, the number of the dep-ngrams (the ngramsobtained from dependency tree1) should be given,but there is no dependency tree for the transla-tion in our method.
We know that the numberof dep-ngrams has an approximate linear relation-ship with the length of the sentence, so we use thelength of the translation to replace the number ofthe dep-ngrams in the translation dependency tree.Recall can be calculated directly since we knowthe number of the dep-ngrams in the reference.The precision and recall are computed as follows.precisionn=?d?Dnp(d,hyp)lenhrecalln=?d?Dnp(d,hyp)countn(ref)Dnis the set of dep-ngrams with the length of n.lenhis the length of the translation.
countn(ref)is the number of the dep-ngrams with the lengthof n in the reference.
p(d,hpy)is 0 if there is nomatch and a positive number between 0 and 1 oth-erwise(?
).1We define two types of dep-ngrams: 1) the head wordchain(?
); 2) fix-floating(?
))420The final score of RED is achieved using For-mula (2), in which a weighted sum of the dep-ngrams?
F -score is calculated.
wngram(0 ?wngram?
1) is the weight of dep-ngram with thelength of n. F -scorenis the F -score for the dep-ngrams with the length of n.RED =N?n=1(wngram?
F -scoren) (2)Other parameters to be tuned includes:?
Stem and SynonymStem(?)
and synonym (WordNet2) are intro-duced with the following three steps.
First,we obtain the alignment with METEORAligner (?)
in which not only exact matchbut also stem and synonym are considered.We use stem, synonym and exact match as thethree match modules.
Second, the alignmentis used to match for a dep-ngram.
We thinkthe dep-ngram can match with the transla-tion if the following conditions are satisfied.1) Each of the words in the dep-ngram hasa matched word in the translation accordingto the alignment; 2) The words in dep-ngramand the matched words in translation appearin the same order; 3) The matched wordsin translation must be continuous if the dep-ngram is a fixed-floating ngram.
At last, thematch module score of a dep-ngram is cal-culated according to Formula (3).
Differentmatch modules have different effects, so wegive them different weights.smod=?ni=1wmin, 0 ?
wmi?
1 (3)miis the match module (exact, stem or syn-onym) of the ith word in a dep-ngram.
wmiis the match module weight of the ith word ina dep-ngram.
n is the number of words in adep-ngram.?
ParaphraseWhen introducing paraphrase, we don?t con-sider the dependency tree of the reference,because paraphrases may not be contained inthe head word chain and fixed-floating struc-tures.
Therefore we first obtain the align-2http://wordnet.princeton.edu/ment with METEOR Aligner, only consid-ering paraphrase; Then, the matched para-phrases are extracted from the alignment anddefined as paraphrase-ngram.
The score ofa paraphrase is 1 ?
wpar, where wparis theweight of paraphrase-ngram.?
Function wordWe introduce a parameterwfun(0 ?
wfun?1) to distinguish function words and contentwords.
wfunis the weight of function words.The function word score of a dep-ngram orparaphrase-ngram is computed according toFormula (4).sfun=Cfun?
wfun+ Ccon?
(1?
wfun)Cfun+ Ccon(4)Cfunis the number of function words in thedep-ngram or paraphrase-ngram.
Cconis thenumber of content words in the dep-ngram orparaphrase-ngram.REDp =N?n=1(wngram?
F -scorepn) (5)F -scorep=precisionp?
recallp?
?
precisionp+ (1?
?)
?
recallp(6)precisionpand recallPin Formula (6) are cal-culated as follows.precisionp=scoreparn+ scoredepnlenhrecallp=scoreparn+ scoredepncountn(ref) + countn(par)lenhis the length of the translation.
countn(ref)is the number of the dep-ngrams with the lengthof n in the reference.
countn(par) is the num-ber of paraphrases with length of n in refer-ence.
scoreparnis the match score of paraphrase-ngrams with the length of n. scoredepnis thematch score of dep-ngrams with the length of n.scoreparnand scoredepnare calculated as follows.scoreparn=?par?Pn(1?
wpar?
sfum)scoredepn=?d?Dn(p(d,hyp)?
smod?
sfun)421Metrics BLEU TER HWCM METEOR RED RED-sent RED-syssentWMT 2010cs-en 0.255 0.253 0.245 0.319 0.328 0.342 0.342de-en 0.275 0.291 0.267 0.348 0.361 0.371 0.375es-en 0.280 0.263 0.259 0.326 0.333 0.344 0.347fr-en 0.220 0.211 0.244 0.275 0.283 0.329 0.328ave 0.257 0.254 0.253 0.317 0.326 0.346 0.348WMT 2012cs-en 0.157 - 0.158 0.212 0.165 0.218 0.212de-en 0.191 - 0.207 0.275 0.218 0.283 0.279es-en 0.189 - 0.203 0.249 0.203 0.255 0.256fr-en 0.210 - 0.204 0.251 0.221 0.250 0.253ave 0.186 - 0.193 0.246 0.201 0.251 0.250WMT 2013cs-en 0.199 - 0.153 0.265 0.228 0.260 0.256de-en 0.220 - 0.182 0.293 0.267 0.298 0.297es-en 0.259 - 0.220 0.324 0.312 0.330 0.326fr-en 0.224 - 0.194 0.264 0.257 0.267 0.266ru-en 0.162 - 0.136 0.239 0.200 0.262 0.225ave 0.212 - 0.177 0.277 0.252 0.283 0.274WMT 2014hi-en - - - 0.420 - 0.383 0.386de-en - - - 0.334 - 0.336 0.338cs-en - - - 0.282 - 0.283 0.283fr-en - - - 0.406 - 0.403 0.404ru-en - - - 0.337 - 0.328 0.329ave - - - 0.355 - 0.347 0.348Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014.
Thevalue in bold is the best result in each raw.
ave stands for the average result of the language pairs on eachyear.
RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1Pnis the set of paraphrase-ngrams with thelength of n. Dnis the set of dep-ngrams with thelength of n.There are totally nine parameters in RED.
Wetried two parameter tuning strategies: Geneticsearch algorithm (?)
and a Grid search over twosubsets of parameters.
The results of Grid searchis more stable, therefore our final submission isbased upon Grid search.
We separate the 9 pa-rameters into two subsets.
When searching Sub-set 1, the parameters in Subset 2 are fixed, andvice versa.
Several iterations are executed to fin-ish the parameter tuning process.
For systemlevel coefficient score, we set two optimizationgoals: G.sys.1) to maximize the sum of Spear-man?s ?
rank correlation coefficient on systemlevel and Kendall?s ?
correlation coefficient onsentence level or G.sys.2) only the former; Forsentence level coefficient score, we also set twooptimization goals: G.sent.1) the same as G.sys.1,G.sent.2) only the latter part of G.sys.1.3 ExperimentsIn this section we report the experimental resultsof RED on the tuning set, which is the combi-nation of WMT2010, WMT2012 and WMT2013data set, as well as the test results on theWMT2014.
Both the sentence level evaluation andthe system level evaluation are conducted to assessthe performance of our automatic metrics.
At thesentence level evaluation, Kendall?s rank correla-tion coefficient ?
is used.
At the system level eval-uation, the Spearman?s rank correlation coefficient?
is used.3.1 DataThere are four language pairs in WMT2010 andWMT2012 including German-English, Czech-English, French-English and Spanish-English.
ForWMT2013, except these 4 language pairs, there isalso Russian-English.
As the test set, WMT 2014has also five language pairs, but the organizer re-moved Spanish-English and replace it with Hindi-English.
For into-English tasks, we parsed the En-422Metrics BLEU TER HWCM METEOR RED RED-sys RED-syssentWMT 2010cs-en 0.840 0.783 0.881 0.839 0.839 0.937 0.881de-en 0.881 0.892 0.905 0.927 0.933 0.95 0.948es-en 0.868 0.903 0.824 0.952 0.969 0.965 0.969fr-en 0.839 0.833 0.815 0.865 0.873 0.875 0.905ave 0.857 0.852 0.856 0.895 0.903 0.931 0.925WMT 2012cs-en 0.886 0.886 0.943 0.657 1 1 1de-en 0.671 0.624 0.762 0.885 0.759 0.935 0.956es-en 0.874 0.916 0.937 0.951 0.951 0.965 0.958fr-en 0.811 0.821 0.818 0.843 0.818 0.871 0.853ave 0.810 0.811 0.865 0.834 0.882 0.942 0.941WMT 2013cs-en 0.936 0.800 0.818 0.964 0.964 0.982 0.972de-en 0.895 0.833 0.816 0.961 0.951 0.958 0.978es-en 0.888 0.825 0.755 0.979 0.930 0.979 0.965fr-en 0.989 0.951 0.940 0.984 0.989 0.995 0.984ru-en 0.670 0.581 0.360 0.789 0.725 0.847 0.821ave 0.875 0.798 0.737 0.834 0.935 0.952 0.944WMT 2014hi-en 0.956 0.618 - 0.457 - 0.676 0.644de-en 0.831 0.774 - 0.926 - 0.897 0.909cs-en 0.908 0.977 - 0.980 - 0.989 0.993fr-en 0.952 0.952 - 0.975 - 0.981 0.980ru-en 0.774 0.796 - 0.792 - 0.803 0.797ave 0.826 0.740 - 0.784 - 0.784 0.770Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014.
The value inbold is the best result in each raw.
ave stands for the average result of the language pairs on each year.RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1Metrics BLEU TER METEOR RED RED-sent RED-syssentWMT 2010en-fr 0.33 0.31 0.369 0.338 0.390 0.369en-de 0.15 0.08 0.166 0.141 0.214 0.185WMT 2012en-fr - - 0.26 0.171 0.273 0.266en-de - - 0.180 0.129 0.200 0.196WMT 2013en-fr - - 0.236 0.220 0.237 0.239en-de - - 0.203 0.185 0.215 0.219WMT 2014en-fr - - 0.278 - 0.297 0.293en-de - - 0.233 - 0.236 0.229Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014.
Thevalue in bold is the best result in each raw.
RED stands for our untuned system, RED-sent is G.sent.2,RED-syssent is G.sent.1glish reference into constituent tree by Berkeleyparser and then converted the constituent tree intodependency tree by Penn2Malt3.
We also con-ducted English-to-French and English-to-Germanexperiments.
The German and French dependencyparser we used is Mate-Tool4.3http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html4https://code.google.com/p/mate-tools/In the experiments, we compare the perfor-mance of our metric with the widely used lexi-cal based metrics BLEU, TER, METEOR and adependency based metrics HWCM.
The results ofRED are provided with exactly the same externalresources like METEOR.
The results of BLEU,TER and METOER are obtained from official re-port of WMT 2010, 2012, 2013 and 2014 (if they423Metrics BLEU TER METEOR RED RED-sys RED-syssentWMT 2010en-fr 0.89 0.89 0.912 0.881 0.932 0.928en-de 0.66 0.65 0.688 0.657 0.734 0.734WMT 2012en-fr 0.80 0.69 0.82 0.639 0.914 0.914en-de 0.22 0.41 0.180 0.143 0.243 0.243WMT 2013en-fr 0.897 0.912 0.924 0.914 0.931 0.936en-de 0.786 0.854 0.879 0.85 0.8 0.8WMT 2014en-fr 0.934 0.953 0.940 - 0.942 0.943en-de 0.065 0.163 0.128 - 0.047 0.047Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and2013; tested on WMT 2014.
The value in bold is the best result in each raw.
RED stands for our untunedsystem, RED-sys is G.sys.2, RED-syssent is G.sys.1are available).
The experiments of HWCM is per-formed by us.3.2 Sentence-level EvaluationKendall?s rank correlation coefficient ?
is em-ployed to evaluate the correlation of all the MTevaluation metrics and human judgements at thesentence level.
A higher value of ?
means a bet-ter ranking similarity with the human judges.
Thecorrelation scores of are shown in Table 1.
Ourmethod performs best when maximum length ofdep-n-gram is set to 3, so we present only theresults when the maximum length of dep-n-gramequals 3.
From Table 1, we can see that: firstly, pa-rameter tuning improve performance significantlyon all the three tuning sets; secondly, althoughthe best scores in the column RED-sent are muchmore than RED-syssent, but the outperform isvery small, so by setting these two optimizationgoals, RED can achieve comparable performance;thirdly, without parameter tuning, RED does notperform well on WMT 2012 and 2013, and evenwith parameter tuning, RED does not outperformMETEOR as much as WMT 2010; lastly, on thetest set, RED does not outperform METEOR.3.3 System-level EvaluationWe also evaluated the RED scores with the humanrankings at the system level to further investigatethe effectiveness of our metrics.
The matching ofthe words in RED is correlated with the positionof the words, so the traditional way of computingsystem level score, like what BLEU does, is notfeasible for RED.
Therefore, we resort to the wayof adding the sentence level scores together to ob-tain the system level score.
At system level evalu-ation, we employ Spearman?s rank correlation co-efficient ?.
The correlations and the average scoresare shown in Table 2.From Table 2, we can see similar trends as inTable 1 with the following difference: firstly, with-out parameter tuning, RED perform comparablywith METEOR on all the three tuning sets; sec-ondly, on test set, RED also perform comparablywith METEOR.
thirdly, RED perform very bad onHindi-English, which is a newly introduced taskthis year.3.4 Evaluation of English to OtherLanguagesWe evaluate both sentence level and system levelscore of RED on English to French and German.The reason we only conduct these two languagesare that the dependency parsers are more reliablein these two languages.
The results are shown inTable 3 and 4.From Table 3 and 4 we can see that the tunedversion of RED still perform slightly better thanMETEOR with the only exception of system levelen-de.4 ConclusionIn this paper, based on the last year?s DCU-CASICT submission, we further improved ourmethod, namely RED.
The experiments are car-ried out at both sentence-level and system-levelusing to-English and from-English corpus.
Theexperiment results indicate that although REDachieves better correlation than BLEU, HWCM,TER and comparably performance with METEORat both sentence level and system level, the per-formance is not stable on all language pairs, suchas the sentence level correlation score of Hindi to424English and the system level score of English toGerman.
To further study the sudden diving of theperformance is our future work.AcknowledgmentsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part ofthe CNGL Centre for Global Intelligent Content(www.cngl.ie) at Dublin City University and Na-tional Natural Science Foundation of China (Grant61379086).ReferencesErgun Bic?ici and Deniz Yuret.
2011.
Instance selec-tion for machine translation using feature decay al-gorithms.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 272?283.
As-sociation for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2008.
Maxsim: Amaximum similarity metric for machine translationevaluation.
In Proceedings of ACL-08: HLT, pages55?62.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 85?91, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: anautomatic metric for mt evaluation with high levelsof correlation with human judgments.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, StatMT ?07, pages 228?231, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In Proceed-ings of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization, pages 25?32.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007a.
Dependency-based automatic eval-uation for machine translation.
In Proceedings ofthe NAACL-HLT 2007/AMTA Workshop on Syntaxand Structure in Statistical Translation, SSST ?07,pages 80?87, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007b.
Evaluating machine translation withlfg dependencies.
Machine Translation, 21(2):95?119, June.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007c.
Labelled dependencies in machinetranslation evaluation.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,StatMT ?07, pages 104?111, Stroudsburg, PA, USA.Association for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th annualmeeting on association for computational linguis-tics, pages 311?318.
Association for ComputationalLinguistics.Martin F Porter.
2001.
Snowball: A language for stem-ming algorithms.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-dependency statistical machine transla-tion.
Computational Linguistics, 36(4):649?671.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas, pages 223?231.Xiaofeng Wu, Hui Yu, and Qun Liu.
2013.
Dcu partic-ipation in wmt2013 metrics task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation.
Association for Computational Linguistics.H.
Yu, X. Wu, Q. Liu, and S. Lin.
2014.
RED: AReference Dependency Based MT Evaluation Met-ric.
In To be published.Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, andTiejun Zhao.
2010.
All in strings: a powerful string-based automatic mt evaluation metric with multi-ple granularities.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, COLING ?10, pages 1533?1540, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.425
