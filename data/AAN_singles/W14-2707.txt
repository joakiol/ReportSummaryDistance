Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 50?57,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsDetecting and Evaluating Local Text Reuse in Social NetworksShaobin Xu*, David A. Smith*, Abigail Mullen?, and Ryan Cordell?NULab for Texts, Maps, and NetworksCollege of Computer and Information Science*, Department of History?, Department of English?Northeastern University, Boston, MAAbstractTexts propagate among participants inmany social networks and provide evi-dence for network structure.
We describeintrinsic and extrinsic evaluations for algo-rithms that detect clusters of reused pas-sages embedded within longer documentsin large collections.
We explore applica-tions of these approaches to two case stud-ies: the culture of free reprinting in thenineteenth-century United States and theuse of similar language in the public state-ments of U.S. members of Congress.1 IntroductionWhile many studies of social networks use surveysand direct observation to catalogue actors (nodes)and their interactions (edges), we often cannot di-rectly observe network links.
Instead, we mightobserve behavior by network participants that pro-vides indirect evidence for social ties.One revealing form of shared behavior is thereuse of text by different social actors.
Meth-ods to uncover invisible links among sources oftext methods would have broad applicability be-cause of the very general nature of the problem?sources of text include websites, newspapers, in-dividuals, corporations, political parties, and soon.
Further, discerning those hidden links be-tween sources would provide more effective waysof identifying the provenance and diverse sourcesof information, and to build predictive models ofthe diffusion of information.There are substantial challenges, however, inbuilding a methodology to study text reuse, includ-ing: scalable detection of reused passages; iden-tification of appropriate statistical models of textmutation; inference methods for characterizingmissing nodes that originate or mediate text trans-mission; link inference conditioned on textualtopics; and the development of testbeds throughwhich predictions of the resulting models mightbe validated against some broader understandingof the processes of transmission.In this paper, we sketch relevant features of ourtwo testbed collections (?2) and then describe ini-tial progress on developing algorithms for detect-ing reused passages embedded within the largertext output of social network nodes (?3).
We thendescribe an intrinsic evaluation of the efficiency ofthese techniques for scaling up text reuse detec-tion (?4).
Finally, we perform an extrinsic evalua-tion of the network links inferred from text reuseby correlating them with side information aboutthe underlying social networks (?5).
A prelimi-nary version of the text reuse detection system waspresented for a single, smaller corpus in (Anony-mous, 2013), but without the extrinsic or much ofthe intrinsic evaluation and without data on the un-derlying networks.2 Case Studies in Text ReuseThe case studies in this paper, which form thebasis for our experimental evaluations below, in-volve two fairly divergent domains: the infor-mational and literary ecology of the nineteenth-century United States and of twenty-first centuryU.S.
legislators.2.1 Tracking Viral Texts in 19c NewspapersIn American Literature and the Culture of Reprint-ing, McGill (2003) argues that American literaryculture in the nineteenth century was shaped by thewidespread practice of reprinting stories and po-ems, usually without authorial permission or even50knowledge, in newspapers, magazines, and books.Without substantial copyright enforcement, textscirculated promiscuously through the print marketand were often revised by editors during the pro-cess.
These ?viral?
texts?be they news stories,short fiction, or poetry?are much more than his-torical curiosities.
The texts that editors chose topass on are useful barometers of what was excit-ing or important to readers during the period, andthus offer significant insight into the priorities andconcerns of the culture.Nineteenth-century U.S. newspapers were usu-ally associated with a particular political party, re-ligious denomination, or social cause (e.g., tem-perance or abolition).
Mapping the specific lo-cations and venues in which varied texts circu-lated would therefore allow us to answer ques-tions about how reprinting and the public sphere ingeneral were affected by geography, communica-tion and transportation networks, and social, polit-ical, and religious affinities.
These effects shouldbe particularly observable in the period before theCivil War and the rise of wire services that broad-cast content at industrial scales (Figure 1).To study the reprint culture of this period, wecrawled the online newspaper archives of the Li-brary of Congress?s Chronicling America project(chroniclingamerica.loc.gov).
Sincethe Chronicling America project aggregates state-level digitization efforts, there are some significantgaps: e.g., there are no newspapers from Mas-sachusetts, which played a not insubstantial rolein the literary culture of the period.
While we con-tinue to collect data from other sources in order toimprove our network analysis, the current datasetremains a useful, and open, testbed for text reusedetection and analysis of overall trends.
For thepre-Civil War period, this corpus contains 1.6 bil-lion words from 41,829 issues of 132 newspapers.Another difficulty with this collection is that itconsists of the OCR?d text of newspaper issueswithout any marking of article breaks, headlines,or other structure.
The local alignment methodsdescribed in ?3 are designed not only to mitigatethis problem, but also to deal with partial reprint-ing.
One newspaper issue, for instance, mightreprint chapters 4 and 5 of a Thackeray novelwhile another issue prints only chapter 5.Since our goal is to detect texts that spread fromone venue to another, we are not interested in textsthat were reprinted frequently in the same newspa-Figure 1: Newspaper issues mentioning ?associ-ated press?
by year, from the Chronicling Americacorpus.
The black regression line fits the raw num-ber of issues; the red line fits counts corrected forthe number of times the Associated Press is men-tioned in each issue.per, or series, to use the cataloguing term.
This in-cludes material such as mastheads and manifestosand also the large number of advertisements thatrecur week after week in the same newspaper.2.2 Statements by Members of CongressMembers of the U.S. Congress are of course evenmore responsive to political debates and incentivesthan nineteenth-century newspapers.
Representa-tives and senators are also a very well-studied so-cial network.
Following Margolin et al.
(2013),we analyzed a dataset of more than 400,000 pub-lic statements made by members of the 112th Sen-ate and House between January 2011 and August2012.
The statements were downloaded from theVote Smart Project website (votesmart.com).According to Vote Smart, the Members?
pub-lic statements include any press releases, state-ments, newspaper articles, interviews, blog en-tries, newsletters, legislative committee websites,campaign websites and cable news show websites(Meet the Press, This Week, etc.)
that containdirect quotes from the Member.
Since we areprimarily interested in the connections betweenMembers, we will, as we see below, want to fil-ter out reuse among different statements by thesame member.
That information could be interest-ing for other reasons?for instance, tracking slight51changes in the phrasing of talking points or sub-stantive positions.We supplemented these texts with categoricaldata chambers and parties and with continuousrepresentations of ideology using the first dimen-sion of the DW-NOMINATE scores (Carroll et al.,2009).3 Text Reuse DetectionAs noted above, we are interested in detecting pas-sages of text reuse (poems or stories; political talk-ing points) that comprise a small fraction of thecontaining documents (newspaper issues; politicalspeeches).
Using the terminology of biological se-quence alignment, we are interested in local align-ments between documents.
In text reuse detectionresearch, two primary methods are n-gram shin-gling and locality-sensitive hashing (LSH) (Hen-zinger, 2006).
The need for local alignmentsmakes LSH less practical without performing alarge number of sliding-window matches.In contrast to work on near-duplicate documentdetection and to work on ?meme tracking?
thattakes text between quotation marks as the unit ofreuse (Leskovec et al., 2009; Suen et al., 2013),here the boundaries of the reused passages are notknown.
Also in contrast to work on the contempo-rary news cycle and blogosphere, we are interestedboth in texts that are reprinted within a few daysand after many years.
We thus cannot excludepotentially matching documents for being far re-moved in time.
Text reuse that occurs only amongdocuments from the same ?source?
(run of news-papers; Member of Congress) should be excluded.Similarly, Henzinger (2006) notes that many of theerrors in near-duplicate webpage detection arosefrom false matches among documents from thesame website that shared boilerplate navigationalelements.3.1 Efficient N-gram IndexingThe first step is to build for each n-gram feature aninverted index of the documents where it appears.As in other duplicate detection and text reuse ap-plications, we are only interested in the n-gramsshared by two or more documents.
The index,therefore, does not need to contain entries for then-grams that occur only once.
We use the two-pass space-efficient algorithm described in Hustonet al.
(2011), which, empirically, is very efficienton large collections.
In a first pass, n-grams arehashed into a fixed number of bins.
On the sec-ond pass, n-grams that hash to bins with one oc-cupant can be discarded; other postings are passedthrough.
Due to hash collisions, there may stillbe a small number of singleton n-grams that reachthis stage.
These singletons are filtered out as theindex is written.In building an index of n-grams, an index of(n-1)-grams can also provide a useful filter.
No5-gram, for example, can occur twice unless itsconstituent 4-grams occur at least twice.
We donot use this optimization in our experiments; inpractice, n-gram indexing is less expensive thanthe later steps.3.2 Extracting and Ranking Candidate PairsOnce we have an inverted index of the documentsthat contain each (skip) n-gram, we use it to gen-erate and rank document pairs that are candidatesfor containing reprinted texts.
Each entry, or post-ing list, in the index may be viewed as a set of pairs(di, pi) that record the document identifier and po-sition in that document of that n-gram.Once we have a posting list of documents con-taining each distinct n-gram, we output all pairs ofdocuments in each list.
We suppress repeated n-grams that appear in different issues of the samenewspaper.
These repetitions often occur in edito-rial boilerplate or advertisements, which, while in-teresting, are outside the scope of this project.
Wealso suppress n-grams that generate more than(u2)pairs, where u is a parameter.1These frequent n-grams are likely to be common fixed phrases.
Fil-tering terms with high document frequency has ledto significant speed increases with small loss in ac-curacy in other document similarity work (Elsayedet al., 2008).
We then sort the list of repeated n-grams by document pair, which allows us to assigna score to each pair based on the number of over-lapping n-grams and the distinctiveness of thosen-grams.
Table 1 shows the parameters for tradingoff recall and precision at this stage.3.3 Computing Local AlignmentsThe initial pass returns a large ranked list of can-didate document pairs, but it ignores the orderof the n-grams as they occur in each document.We therefore employ local alignment techniquesto find compact passages with the highest proba-bility of matching.
The goal of this alignment is1The filter is parameterized this way because it is appliedafter removing document pairs in the same series.52n n-gram orderw maximum width of skip n-gramsg minimum gap of skip n-gramsu maximum distinct series in the posting listTable 1: Parameters for text reuse detectionto increase the precision of the detected documentpairs while maintaining high recall.
Due to thehigh rate of OCR errors, many n-grams in match-ing articles will contain slight differences.Unlike some partial duplicate detection tech-niques based on global alignment (Yalniz et al.,2011), we cannot expect all or even most of thearticles in two newspaper issues, or the text in twobooks with a shared quotation, to align.
Rather,as in some work on biological subsequence align-ment (Gusfield, 1997), we are looking for re-gions of high overlap embedded within sequencesthat are otherwise unrelated.
We therefore em-ploy the Smith-Waterman dynamic programmingalgorithm with an affine gap penalty.
This useof model-based alignment distinguishes this ap-proach for other work, for detecting shorter quota-tions, that greedily expands areas of n-gram over-lap (Kolak and Schilit, 2008; Horton et al., 2010).We do, however, prune the dynamic programmingsearch by forcing the alignment to go through po-sition pairs that contain a matching n-gram fromthe previous step, as long as the two n-grams areunique in their respective texts.
Even the exactSmith-Waterman algorithm, however, is an ap-proximation to the problem we aim to solve.
If,for instance, two separate articles from one news-paper issue were reprinted in another newspaperissue in the opposite order?or separated by a longspan of unrelated matter?the local alignment al-gorithm would simply output the better-aligned ar-ticle pair and ignore the other.
Anecdotally, weonly observed this phenomenon once in the news-paper collection, where two different parodies ofthe same poem were reprinted in the same issue.In any case, our approach can easily align differ-ent passages in the same document to passages intwo other documents.The dynamic program proceeds as follows.
Inthis paper, two documents would be treated as se-quences of text X and Y whose individual charac-ters are indexed as Xiand Yj.
Let W (Xi, Yj) bethe score of aligning character Xito character Yj.Higher scores are better.
We use a scoring functionwhere only exact character matches get a positivescore and any other pair gets a negative score.
Wealso account for additional text appearing on eitherX or Y .
Let Wgbe the score, which is negative,of starting a ?gap?, where one sequence includestext not in the other.
Let Wcbe the cost for con-tinuing a gap for one more character.
This ?affinegap?
model assigns a lower cost to continuing agap than to starting one, which has the effect ofmaking the gaps more contiguous.
We use an as-signment of weights fairly standard in genetic se-quences where matching characters score 2, mis-matched characters score -1, beginning a gap costs-5, and continuing a gap costs -0.5.
We leave forfuture work the optimization of these weights forthe task of capturing shared policy ideas.As with other dynamic programming algo-rithms such as Levenshtein distance, the Smith-Waterman algorithm operates by filling in a?chart?
of partial results.
The chart in this caseis a set of cells indexed by the characters in X andY , and we initialize it as follows:H(0, 0) = 0H(i, 0) = E(i, 0) = Wg+ i ?WcH(0, j) = F (0, j) = Wg+ j ?WcThe algorithm is then defined by the following re-currence relations:H(i, j) = max??????
?0E(i, j)F (i, j)H(i?
1, j ?
1) +W (Xi, Yj)E(i, j) = max{E(i, j ?
1) +WcH(i, j ?
1) +Wg+WcF (i, j) = max{F (i?
1, j) +WcH(i?
1, j) +Wg+WcThe main entry in each cell H(i, j) represents thescore of the best alignment that terminates at po-sition i and j in each sequence.
The intermediatequantities E and F are used for evaluating gaps.Due to taking a max with 0, H(i, j) cannot be neg-ative.
This is what allows Smith-Waterman to ig-nore text before and after the locally aligned sub-strings of each input.After completing the chart, we then find the op-timum alignment by tracing back from the cellwith the highest cumulative value H(i, j) until a53cell with a value of 0 is reached.
These two cellsrepresent the bounds of the sequence, and the over-all SW alignment score reflects the extent to whichthe characters in the sequences align and the over-all length of the sequence.In our implementation, we include one furtherspeedup: since in a previous step we identified n-grams that are shared between the two documents,we assume that any alignment of those documentsmust include those n-grams as matches.
In somecases, this anchoring of the alignment might leadto suboptimal SW alignment scores.4 Intrinsic EvaluationTo evaluate the precision and recall of text reusedetection, we create a pseudo-relevant set of doc-ument pairs by pooling the results of several runswith different parameter settings.
For each doc-ument pair found in the union of these runs, weobserve the length, in matching characters, of thelongest local alignment.
(Using matching charac-ter length allows us to abstract somewhat from theprecise cost matrix.)
We can then observe howmany aligned passages each method retrieves thatare at least 50,000 character matches in length, atleast 20,000 character matches in length, and soon.
The candidate pairs are sorted by the numberof overlapping n-grams; we measure the pseudo-recall at several length cutoffs.
For each positionin a ranked list of document pairs, we then mea-sure the precision: what proportion of documentsretrieved are in fact 50k, 20k, etc., in length?
Sincewe wish to rank documents by the length of thealigned passages they contain, this is a reason-able metric.
One summary of these various valuesis the average precision: the mean of the preci-sion at every rank position that contains an actu-ally relevant document pair.
One of the few earlierevaluations of local text reuse, by Seo and Croft(2008), compared fingerprinting methods to a tri-gram baseline.
Since their corpus contained shortindividual news articles, the extent of the reusedpassages was evaluated qualitatively rather than byalignment.Figure 2 shows the average precision of differ-ent parameter settings on the newspaper collec-tion, ranked by the number of pairs each returns.If the pairwise document step returns a large num-ber of pairs, we will have to perform a large num-ber of more costly Smith-Waterman alignments.On this collection, a good tradeoff between space5e+06 1e+07 2e+07 5e+070.00.10.20.30.40.5Pairs examinedAverage precisionn2.u100.w105.g95n2.u100.w25.g15n2.u100.w55.g45n4.u100n5.u100n5.u50n7.u100n7.u5010k10k10k 10k10k10k10k10k5k5k5k5k5k5k5k5k2k2k2k2k2k2k2k2k1k1k1k1k1k1k1k1kFigure 2: Average precision for aligned passagesof different minimum length in characters.
Verti-cal red lines indicate the performance of differentparameter settings (see Table 1).and speed is achieved by skip bigram features.
Inthe best case, we look at bigrams where there is agap of at least 95, and not more than 105, wordsbetween the first and second terms (n=2 u=100w=105 g=95).While average precision is a good summary ofthe quality of the ranked list at any one point,many applications will simply be concerned withthe total recall after some fixed amount of pro-cessing.
Figure 3 also summarizes these recall re-sults by the absolute number of document pairsexamined.
From these results, it is clear theseveral good settings perform well at retrievingall reprinted passages of at least 5000 charac-ters.
Even using the pseudo-recall metric, how-ever, even the best operating points fail in the endto retrieve about 10% of the reprints detected bysome other setting for all documents of at least1000 characters.5 Extrinsic EvaluationWhile political scientists, historians, and literaryscholars will, we hope, find these techniques use-ful and perform close reading and manual analysison texts of interest, we would like to validate ourresults without a costly annotation campaign.
Inthis paper, we explore the correlation of patterns oftext reuse with what is already known from other541e+01 1e+03 1e+05 1e+070.00.20.40.60.81.0Pairs examinedRecall100020005000100002000050000Figure 3: (Pseudo-)Recall for aligned passages ofdifferent minimum lengths in characters.sources about the connections among Members ofCongress, newspaper editors, and so on.
This ideawas inspired by Margolin et al.
(2013), who usedthese techniques to test rhetorical theories of ?se-mantic organizing processes?
on the congressionalstatements corpus.The approach is quite simple: measure the cor-relation between some metric of text reuse be-tween actors in a social network and other featuresof the network links between those actors.
Themetric of text reuse might be simply the number ofexact n-grams shared by the language of two au-thors (Margolin et al., 2013); alternately, it mightbe the absolute or relative length of all the alignedpassages shared by two authors or the tree distancebetween them in a phylogenetic reconstruction.
Tomeasure the correlation of a text reuse metric witha single network, we can simply use Pearson?s cor-relation; for more networks, we can use multivari-ate regression.
Due to, for instance, autocorrela-tion among edges arising from a particular node,we cannot proceed as if the weight of each edge inthe text reuse network can be compared indepen-dently to the weight of the corresponding edges inother networks.
We therefore use nonparametricpermutation tests using the quadratic assignmentprocedure (QAP) to resample several networkswith the same structure but different labels andweights.
The QAP achieves this by reordering therows and columns of one network?s adjacency ma-trix according to the same permutation.
The per-muted network then has the same structure?e.g.,degree distribution?but should no longer exhibitthe same correlations with the other network(s).We can run QAP to generate confidence intervalsfor both single (Krackhardt, 1987) and multiplecorrelations (Dekker et al., 2007).5.1 Congressional StatementsWe model the connection between the log magni-tude of reused text and the strength of ties amongMembers according to whether they are in thesame chamber and how similar they are on thefirst dimension of the DW-nominate ideologicalscale (Carroll et al., 2009).
On the left side of Ta-ble 2 are shown the results for correlating reusedpassages of certain minimum lengths (10, 16, 32words) with these underlying features.
On theright are shown the similar results of (Margolinet al., 2013) that simply used the exact size ofthe n-gram overlap between Members?
statementsfor increasing values of n. The alignment anal-ysis proposed in this paper achieves similar re-sults when passages and n-grams are short.
Ouranalysis, however, achieves higher single and mul-tiple correlations among networks are the pas-sages grow longer.
This is unsurprising since theprobability of an exact 32-gram match is muchsmaller than that of a 32-word-long alignment thatmight contain a few differences.
In particular,the much higher coefficients for DW-nominate atlonger aligned lengths suggests that ideological in-fluence still dominates over similarities induced bythe procedural environment of each congressionalchamber.5.2 Network Connections of 19c ReprintsFor the antebellum newspaper corpus, we are alsointerested in how political affinity correlates withreprinting similar texts.
We have also addedvariables for social causes such as temperance,women?s rights, and abolition that?while cer-tainly not orthogonal to political commitments?might sometimes operate independently.
In addi-tion, we also added a ?shared state?
variable to ac-count for shared political and social environmentsof more limited scope.
Figure 4 shows a partic-ularly strong example of a geographic effect: thestatement of the radical abolitionist John Brownafter being condemned to death for attacking afederal arsenal and attempting to raise a slave re-bellion was very unlikely to be published in the55aligned passages of ?
n words n-grams of length10 16 32 8 16 32First-order Pearson correlationsDW-nominate 0.26*** 0.25*** 0.23*** 0.26*** 0.22*** 0.16***same chamber 0.05* 0.08** 0.13*** -0.05*** 0.21*** 0.10***Regression coefficientsDW-nominate 0.72*** 0.75*** 0.74*** 1.31*** 2.67*** 0.36same chamber 0.15** 0.27*** 0.42*** 0.20 3.14*** 0.81***R-squared .069 .070 .073 .068 .073 .010Table 2: Correlations between log length of aligned text and other author networks in public statementsby Members of Congress.
?p < .05, ?
?
p < .01, ?
?
?p < .001South.Using information from the Chronicling Amer-ica cataloguing and from other newspaper histo-ries, we coded each of the 132 newspapers in thecorpus with these political and social affinities.We then counted the number of reprinted passagesshared by each pair of newspapers.
There is nota deterministic relationship between the numberof pairs of newspapers sharing an affinity and thenumber of reprints shared by those papers.
Whileour admittedly partial corpus only contains a sin-gle pair of avowedly abolitionist papers?a radicalposition at the time?those two papers shared ar-ticles 306 times, compared for instance to the 71stories shared among the 6 pairs of ?nativist?
pa-pers.Table 3 shows that geographic proximity had byfar the strongest correlation with (log) reprintingcounts.
Interestingly, the only political affinity toshow as strong a correlation was the Republicanparty, which in this period had just been organizedand, one might suppose, was trying to controlits ?message?.
The Republicans were more ge-ographically concentrated in any case, comparedto the sectionally more diffuse Democrats.
An-other counterexample is the Whigs, the party fromwhich the new Republican party drew many of itsmembers, which also has a slight negative effecton reprinting.
The only other large coefficientsare in the complete model for smaller movementssuch as nativism and abolition.
It is interesting tospeculate about whether the speed or faithfulnessof reprinting?as opposed to the volume?mightbe correlated with more of these variables.6 ConclusionsWe have presented techniques for detecting reusedpassages embedded within the larger discoursesFigure 4: Reprints of John Brown?s 1859 speechat his sentencing.
Counties are shaded with histor-ical population data, where available.
Even takingpopulation differences into account, few newspa-pers in the South printed the abolitionist?s state-ment.produced by actors in social networks.
Some ofthis shared content is as brief as partisan talkingpoints or lines of poetry; other reprints can en-compass extensive legislative boilerplate or chap-ters of novels.
The longer passages are easier todetect, with prefect pseudo-recall without exhaus-tive scanning of the corpus.
Precision-recall trade-offs will vary with the density of text reuse andthe noise introduced by optical character recog-nition and other features of data collection.
Wethen showed the feasibility of using network re-gression to measure the correlations between con-nections inferred from text reuse and networks de-rived from outside information.ReferencesRoyce Carroll, Jeff Lewis, James Lo, Nolan McCarty,Keith Poole, and Howard Rosenthal.
2009.
Measur-56newspaper pairs of regression w/pairsaffinity papers reprints ?
1 ?
10 ?
100Republican 1176 134,302 0.74*** 0.73* 0.72***Whig 1176 91,139 -0.35 -0.34 -0.35Democrat 1081 62,609 -0.08 -0.09 -0.07same state 672 103,057 1.12*** 1.11*** 1.13***anti-secession 435 22,009 -0.58* -0.58 -0.60anti-slavery 231 12,742 -0.65 -0.64 -0.60pro-slavery 120 11,040 -0.35 -0.35 -0.27Free-State 15 1,194 0.80 0.80Constitutional Union 15 1,070 -0.21 -0.21pro-secession 15 529 0.11 0.11Free Soil 10 1,936 -0.42 -0.42Copperhead 10 797 1.53 1.54temperance 6 560 0.65independent 6 186 -0.22nativist 6 71 -1.93*women?s rights 3 721 1.91abolitionist 1 306 3.49**Know-Nothing 1 25 1.33Mormon 1 3 -1.13R-squared ?
?
.065 .063 .062Table 3: Correlations between shared reprints between 19c newspapers and political and other affinities.While many Whig papers became Republican, they do not completely overlap in our dataset; the identicalnumber of pairs is coincidental.ing bias and uncertainty in DW-NOMINATE idealpoint estimates via the parametric bootstrap.
Politi-cal Analysis, 17(3).David Dekker, David Krackhardt, and Tom Snijders.2007.
Sensitivity of MRQAP tests to collinear-ity and autocorrelation conditions.
Psychometrika,72(4):563?581.Tamer Elsayed, Jimmy Lin, and Douglas W. Oard.2008.
Pairwise document similarity in large collec-tions with MapReduce.
In ACL Short Papers.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.Monika Henzinger.
2006.
Finding near-duplicate webpages: A large-scale evaluation of algorithms.
InSIGIR.Russell Horton, Mark Olsen, and Glenn Roe.
2010.Something borrowed: Sequence alignment and theidentification of similar passages in large text collec-tions.
Digital Studies / Le champ num?erique, 2(1).Samuel Huston, Alistair Moffat, and W. Bruce Croft.2011.
Efficient indexing of repeated n-grams.
InWSDM.Okan Kolak and Bill N. Schilit.
2008.
Generating linksby mining quotations.
In Hypertext.David Krackhardt.
1987.
QAP partialling as a test ofspuriousness.
Social Networks, 9(2):171?186.Jure Leskovec, Lars Backstrom, and Jon Kleinberg.2009.
Meme-tracking and the dynamics of the newscycle.
In KDD.Drew Margolin, Yu-Ru Lin, and David Lazer.
2013.Why so similar?
: Identifying semantic organizingprocesses in large textual corpora.
SSRN.Meredith L. McGill.
2003.
American Literature andthe Culture of Reprinting, 1834?1853.
U. Penn.Press.Jangwon Seo and W. Bruce Croft.
2008.
Local textreuse detection.
In SIGIR.Caroline Suen, Sandy Huang, Chantat Eksombatchai,Rok Sosi?c, and Jure Leskovec.
2013.
NIFTY: Asystem for large scale information flow tracking andclustering.
In WWW.Ismet Zeki Yalniz, Ethem F. Can, and R. Manmatha.2011.
Partial duplicate detection for large book col-lections.
In CIKM.57
