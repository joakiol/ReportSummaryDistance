Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650?658,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsComparable Entity Mining from Comparative QuestionsShasha Li1?Chin-Yew Lin2?Young-In Song2?Zhoujun Li31National University of Defense Technology, Changsha, China2Microsoft Research Asia, Beijing, China3Beihang University, Beijing, Chinashashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2,lizj@buaa.edu.cn3AbstractComparing one thing with another is a typicalpart of human decision making process.
How-ever, it is not always easy to know what tocompare and what are the alternatives.
To ad-dress this difficulty, we present a novel way toautomatically mine comparable entities fromcomparative questions that users posted on-line.
To ensure high precision and high recall,we develop a weakly-supervised bootstrappingmethod for comparative question identificationand comparable entity extraction by leveraginga large online question archive.
The experi-mental results show our method achieves F1-measure of 82.5% in comparative questionidentification and 83.3% in comparable entityextraction.
Both significantly outperform anexisting state-of-the-art method.1 IntroductionComparing alternative options is one essentialstep in decision-making that we carry out everyday.
For example, if someone is interested in cer-tain products such as digital cameras, he or shewould want to know what the alternatives areand compare different cameras before making apurchase.
This type of comparison activity isvery common in our daily life but requires highknowledge skill.
Magazines such as ConsumerReports and PC Magazine and online media suchas CNet.com strive in providing editorial com-parison content and surveys to satisfy this need.In the World Wide Web era, a comparison ac-tivity typically involves: search for relevant webpages containing information about the targetedproducts, find competing products, read reviews,and identify pros and cons.
In this paper, we fo-cus on finding a set of comparable entities givena user?s input entity.
For example, given an enti-ty, Nokia N95 (a cellphone), we want to findcomparable entities such as Nokia N82, iPhoneand so on.In general, it is difficult to decide if two enti-ties are comparable or not since people do com-pare apples and oranges for various reasons.
Forexample, ?Ford?
and ?BMW?
might be compa-rable as ?car manufacturers?
or as ?market seg-ments that their products are targeting?, but werarely see people comparing ?Ford Focus?
(carmodel) and ?BMW 328i?.
Things also get morecomplicated when an entity has several functio-nalities.
For example, one might compare?iPhone?
and ?PSP?
as ?portable game player?while compare ?iPhone?
and ?Nokia N95?
as?mobile phone?.
Fortunately, plenty of compara-tive questions are posted online, which provideevidences for what people want to compare, e.g.
?Which to buy, iPod or iPhone??.
We call ?iPod?and ?iPhone?
in this example as comparators.
Inthis paper, we define comparative questions andcomparators as:?
Comparative question: A question that in-tends to compare two or more entities and ithas to mention these entities explicitly in thequestion.?
Comparator: An entity which is a target ofcomparison in a comparative question.According to these definitions, Q1 and Q2 be-low are not comparative questions while Q3 is.
?iPod Touch?
and ?Zune HD?
are comparators.Q1: ?Which one is better?
?Q2: ?Is Lumix GH-1 the best camera?
?Q3: ?What?s the difference between iPodTouch and Zune HD?
?The goal of this work is mining comparatorsfrom comparative questions.
The results wouldbe very useful in helping users?
exploration of650alternative choices by suggesting comparableentities based on other users?
prior requests.To mine comparators from comparative ques-tions, we first have to detect whether a questionis comparative or not.
According to our defini-tion, a comparative question has to be a questionwith intent to compare at least two entities.Please note that a question containing at leasttwo entities is not a comparative question if itdoes not have comparison intent.
However, weobserve that a question is very likely to be acomparative question if it contains at least twoentities.
We leverage this insight and develop aweakly supervised bootstrapping method to iden-tify comparative questions and extract compara-tors simultaneously.To our best knowledge, this is the first attemptto specially address the problem on finding goodcomparators to support users?
comparison activi-ty.
We are also the first to propose using com-parative questions posted online that reflect whatusers truly care about as the medium from whichwe mine comparable entities.
Our weakly super-vised method achieves 82.5% F1-measure incomparative question identification, 83.3% incomparator extraction, and 76.8% in end-to-endcomparative question identification and compa-rator extraction which outperform the most rele-vant state-of-the-art method by Jindal & Liu(2006b) significantly.The rest of this paper is organized as follows.The next section discusses previous works.
Sec-tion 3 presents our weakly-supervised method forcomparator mining.
Section 4 reports the evalua-tions of our techniques, and we conclude the pa-per and discuss future work in Section 5.2 Related Work2.1 OverviewIn terms of discovering related items for an enti-ty, our work is similar to the research on recom-mender systems, which recommend items to auser.
Recommender systems mainly rely on simi-larities between items and/or their statistical cor-relations in user log data (Linden et al, 2003).For example, Amazon recommends products toits customers based on their own purchase histo-ries, similar customers?
purchase histories, andsimilarity between products.
However, recom-mending an item is not equivalent to finding acomparable item.
In the case of Amazon, thepurpose of recommendation is to entice their cus-tomers to add more items to their shopping cartsby suggesting similar or related items.
While inthe case of comparison, we would like to helpusers explore alternatives, i.e.
helping them makea decision among comparable items.For example, it is reasonable to recommend?iPod speaker?
or ?iPod batteries?
if a user isinterested in ?iPod?, but we would not comparethem with ?iPod?.
However, items that are com-parable with ?iPod?
such as ?iPhone?
or ?PSP?which were found in comparative questions post-ed by users are difficult to be predicted simplybased on item similarity between them.
Althoughthey are all music players, ?iPhone?
is mainly amobile phone, and ?PSP?
is mainly a portablegame device.
They are similar but also differenttherefore beg comparison with each other.
It isclear that comparator mining and item recom-mendation are related but not the same.Our work on comparator mining is related tothe research on entity and relation extraction ininformation extraction (Cardie, 1997; Califf andMooney, 1999; Soderland, 1999; Radev et al,2002; Carreras et al, 2003).
Specifically, themost relevant work is by Jindal and Liu (2006aand 2006b) on mining comparative sentences andrelations.
Their methods applied class sequentialrules (CSR) (Chapter 2, Liu 2006) and label se-quential rules (LSR) (Chapter 2, Liu 2006)learned from annotated corpora to identify com-parative sentences and extract comparative rela-tions respectively in the news and review do-mains.
The same techniques can be applied tocomparative question identification and compa-rator mining from questions.
However, their me-thods typically can achieve high precision butsuffer from low recall (Jindal and Liu, 2006b)(J&L).
However, ensuring high recall is crucialin our intended application scenario where userscan issue arbitrary queries.
To address this prob-lem, we develop a weakly-supervised bootstrap-ping pattern learning method by effectively leve-raging unlabeled questions.Bootstrapping methods have been shown to bevery effective in previous information extractionresearch (Riloff, 1996; Riloff and Jones, 1999;Ravichandran and Hovy, 2002; Mooney and Bu-nescu, 2005; Kozareva et al, 2008).
Our work issimilar to them in terms of methodology usingbootstrapping technique to extract entities with aspecific relation.
However, our task is differentfrom theirs in that it requires not only extractingentities (comparator extraction) but also ensuringthat the entities are extracted from comparativequestions (comparative question identification),which is generally not required in IE task.6512.2 Jindal & Liu 2006In this subsection, we provide a brief summaryof the comparative mining method proposed byJindal and Liu (2006a and 2006b), which is usedas baseline for comparison and represents thestate-of-the-art in this area.
We first introducethe definition of CSR and LSR rule used in theirapproach, and then describe their comparativemining method.
Readers should refer to J&L?soriginal papers for more details.CSR and LSRCSR is a classification rule.
It maps a sequencepattern S(?1?2???)
to a class C.  In our problem,C is either comparative or non-comparative.Given a collection of sequences with class in-formation, every CSR is associated to two para-meters: support and confidence.
Support is theproportion of sequences in the collection contain-ing S as a subsequence.
Confidence is the propor-tion of sequences labeled as C in the sequencescontaining the S. These parameters are importantto evaluate whether a CSR is reliable or not.LSR is a labeling rule.
It maps an input se-quence pattern ?(?1?2???
???)
to a labeledsequence ??(?1?2?
??
???)
by replacing one to-ken (??)
in the input sequence with a designatedlabel (??
).
This token is referred as the anchor.The anchor in the input sequence could be ex-tracted if its corresponding label in the labeledsequence is what we want (in our case, a compa-rator).
LSRs are also mined from an annotatedcorpus, therefore each LSR also have two para-meters: support and confidence.
They are simi-larly defined as in CSR.Supervised Comparative Mining MethodJ&L treated comparative sentence identificationas a classification problem and comparative rela-tion extraction as an information extraction prob-lem.
They first manually created a set of 83 key-words such as beat, exceed, and outperform thatare likely indicators of comparative sentences.These keywords were then used as pivots tocreate part-of-speech (POS) sequence data.
Amanually annotated corpus with class informa-tion, i.e.
comparative or non-comparative, wasused to create sequences and CSRs were mined.A Na?ve Bayes classifier was trained using theCSRs as features.
The classifier was then used toidentify comparative sentences.Given a set of comparative sentences, J&Lmanually annotated two comparators with labels$ES1 and $ES2 and the feature compared withlabel $FT for each sentence.
J&L?s method wasonly applied to noun and pronoun.
To differen-tiate noun and pronoun that are not comparatorsor features, they added the fourth label $NEF, i.e.non-entity-feature.
These labels were used aspivots together with special tokens li & rj1 (tokenposition), #start (beginning of a sentence), and#end (end of a sentence) to generate sequencedata, sequences with single label only and mini-mum support greater than 1% are retained, andthen LSRs were created.
When applying thelearned LSRs for extraction, LSRs with higherconfidence were applied first.J&L?s method have been proved effective intheir experimental setups.
However, it has thefollowing weaknesses:?
The performance of J&L?s method reliesheavily on a set of comparative sentence in-dicative keywords.
These keywords weremanually created and they offered no guide-lines to select keywords for inclusion.
It isalso difficult to ensure the completeness ofthe keyword list.?
Users can express comparative sentences orquestions in many different ways.
To havehigh recall, a large annotated training corpusis necessary.
This is an expensive process.?
Example CSRs and LSRs given in Jindal &Liu (2006b) are mostly a combination ofPOS tags and keywords.
It is a surprise thattheir rules achieved high precision but lowrecall.
They attributed most errors to POStagging errors.
However, we suspect thattheir rules might be too specific and overfittheir small training set (about 2,600 sen-tences).
We would like to increase recall,avoid overfitting, and allow rules to includediscriminative lexical tokens to retain preci-sion.In the next section, we introduce our method toaddress these shortcomings.3 Weakly Supervised Method for Com-parator MiningOur weakly supervised method is a pattern-basedapproach similar to J&L?s method, but it is dif-ferent in many aspects: Instead of using separateCSRs and LSRs, our method aims to learn se-1 li marks a token is at the ithposition to the left of the pivotand rj marks a token is at jth position to the right of thepivot where i and j are between 1 and 4 in J&L (2006b).652quential patterns which can be used to identifycomparative question and extract comparatorssimultaneously.In our approach, a sequential pattern is definedas a sequence S(s1s2?
si ?
sn) where si can be aword, a POS tag, or a symbol denoting either acomparator ($C), or the beginning (#start) or theend of a question (#end).
A sequential pattern iscalled an indicative extraction pattern (IEP) if itcan be used to identify comparative questionsand extract comparators in them with high relia-bility.
We will formally define the reliabilityscore of a pattern in the next section.Once a question matches an IEP, it is classifiedas a comparative question and the token se-quences corresponding to the comparator slots inthe IEP are extracted as comparators.
When aquestion can match multiple IEPs, the longestIEP is used 2 .
Therefore, instead of manuallycreating a list of indicative keywords, we create aset of IEPs.
We will show how to acquire IEPsautomatically using a bootstrapping procedurewith minimum supervision by taking advantageof a large unlabeled question collection in thefollowing subsections.
The evaluations shown insection 4 confirm that our weakly supervisedmethod can achieve high recall while retain highprecision.This pattern definition is inspired by the workof Ravichandran and Hovy (2002).
Table 1shows some examples of such sequential pat-terns.
We also allow POS constraint on compara-tors as shown in the pattern ?<, $C/NN or $C/NN?
#end>?.
It means that a valid comparator musthave a NN POS tag.3.1 Mining Indicative Extraction PatternsOur weakly supervised IEP mining approach isbased on two key assumptions:2 It is because the longest IEP is likely to be the most specif-ic and relevant pattern for the given question.Figure 1: Overview of the bootstrapping alogorithm?
If a sequential pattern can be used to extractmany reliable comparator pairs, it is very likelyto be an IEP.?
If a comparator pair can be extracted by anIEP, the pair is reliable.Based on these two assumptions, we designour bootstrapping algorithm as shown in Figure 1.The bootstrapping process starts with a singleIEP.
From it, we extract a set of initial seed com-parator pairs.
For each comparator pair, all ques-tions containing the pair are retrieved from aquestion collection and regarded as comparativequestions.
From the comparative questions andcomparator pairs, all possible sequential patternsare generated and evaluated by measuring theirreliability score defined later in the Pattern Eval-uation section.
Patterns evaluated as reliable onesare IEPs and are added into an IEP repository.Then, new comparator pairs are extracted fromthe question collection using the latest IEPs.
Thenew comparators are added to a reliable compa-rator repository and used as new seeds for patternlearning in the next iteration.
All questions fromwhich reliable comparators are extracted are re-moved from the collection to allow finding newpatterns efficiently in later iterations.
Theprocess iterates until no more new patterns canbe found from the question collection.There are two key steps in our method: (1)pattern generation and (2) pattern evaluation.
Inthe following subsections, we will explain themin details.Pattern GenerationTo generate sequential patterns, we adapt thesurface text pattern mining method introduced in(Ravichandran and Hovy, 2002).
For any givencomparative question and its comparator pairs,comparators in the question are replaced withsymbol $Cs.
Two symbols, #start and #end, areattached to the beginning and the end of a sen-Sequential Patterns<#start which city is better, $C or $C ?
#end><, $C or $C ?
#end><#start $C/NN or $C/NN ?
#end><which NN is better, $C or $C ?><which city is JJR, $C or $C ?><which NN is JJR, $C or $C ?>...Table 1: Candidate indicative extraction pattern (IEP)examples of the question ?which city is better, NYC orParis?
?653tence in the question.
Then, the following threekinds of sequential patterns are generated fromsequences of questions:?
Lexical patterns: Lexical patterns indicatesequential patterns consisting of only wordsand symbols ($C, #start, and #end).
They aregenerated by suffix tree algorithm (Gusfield,1997) with two constraints: A pattern shouldcontain more than one $C, and its frequencyin collection should be more than an empiri-cally determined number ?.?
Generalized patterns: A lexical pattern canbe too specific.
Thus, we generalize lexicalpatterns by replacing one or more words withtheir POS tags.
2?
?
1 generalized patternscan be produced from a lexical pattern con-taining N words excluding $Cs.?
Specialized patterns: In some cases, a pat-tern can be too general.
For example, al-though a question ?ipod or zune??
is com-parative, the pattern ?<$C or $C>?
is toogeneral, and there can be many non-comparative questions matching the pattern,for instance, ?true or false??.
For this reason,we perform pattern specialization by addingPOS tags to all comparator slots.
For exam-ple, from the lexical pattern ?<$C or $C>?and the question ?ipod or zune?
?, ?<$C/NNor $C/NN?>?
will be produced as a specia-lized pattern.Note that generalized patterns are generated fromlexical patterns and the specialized patterns aregenerated from the combined set of generalizedpatterns and lexical patterns.
The final set ofcandidate patterns is a mixture of lexical patterns,generalized patterns and specialized patterns.Pattern EvaluationAccording to our first assumption, a reliabilityscore ??(??)
for a candidate pattern ??
at itera-tion k can be defined as follows:??
??
=??
(?????
?
)???
??????1??
(????
)(1), where ??
can extract known reliable comparatorpairs ???
.
???
?1 indicates the reliable compara-tor pair repository accumulated until the(?
?
1)??
iteration.
??(?)
means the number ofquestions satisfying a condition x.
The condition??
?
???
denotes that ???
can be extracted froma question by applying pattern ??
while the con-dition ??
??
denotes any question containingpattern ??
.However, Equation (1) can suffer from in-complete knowledge about reliable comparatorpairs.
For example, very few reliable pairs aregenerally discovered in early stage of bootstrap-ping.
In this case, the value of Equation (1)might be underestimated which could affect theeffectiveness of equation (1) on distinguishingIEPs from non-reliable patterns.
We mitigate thisproblem by a lookahead procedure.
Let us denotethe set of candidate patterns at the iteration k by?
?
.
We define the support ?
for comparator pair??
?
which can be extracted by ??
and does notexist in the current reliable set:?
??
?
= ??
( ???
??
?)
(2)where ?
?
?
??
?
means that one of the patterns in?
?
can extract ???
in certain questions.
Intuitive-ly, if  ??
?
can be extracted by many candidatepatterns in ?
?
, it is likely to be extracted as areliable one in the next iteration.
Based on thisintuition, a pair ???
whose support S is more thana threshold ?
is regarded as a likely-reliable pair.Using likely-reliable pairs, lookahead reliabilityscore ?
??
is defined:?
?
??
=??
(?????
i )???
????
??????
(????
)(3), where ??
????
indicates a set of likely-reliablepairs based on ?
?
.By interpolating Equation (1) and (3), the finalreliability score ?(??)??????
for a pattern is de-fined as follows:?(??)??????
= ?
?
??
??
+ (1?
?)
?
??(??)
(4)Using Equation (4), we evaluate all candidatepatterns and select patterns whose score is morethan threshold ?
as IEPs.
All necessary parame-ter values are empirically determined.
We willexplain how to determine our parameters in sec-tion 4.4 Experiments4.1 Experiment SetupSource DataAll experiments were conducted on about 60Mquestions mined from Yahoo!
Answers?
questiontitle field.
The reason that we used only a title654field is that they clearly express a main intentionof an asker with a form of simple questions ingeneral.Evaluation DataTwo separate data sets were created for evalua-tion.
First, we collected 5,200 questions by sam-pling 200 questions from each Yahoo!
Answerscategory3.
Two annotators were asked to labeleach question manually as comparative, non-comparative, or unknown.
Among them, 139(2.67%) questions were classified as comparative,4,934 (94.88%) as non-comparative, and 127(2.44%) as unknown questions which are diffi-cult to assess.
We call this set SET-A.Because there are only 139 comparative ques-tions in SET-A, we created another set whichcontains more comparative questions.
We ma-nually constructed a keyword set consisting of 53words such as ?or?
and ?prefer?, which are goodindicators of comparative questions.
In SET-A,97.4% of comparative questions contains one ormore keywords from the keyword set.
We thenrandomly selected another 100 questions fromeach Yahoo!
Answers category with one extracondition that all questions have to contain atleast one keyword.
These questions were labeledin the same way as SET-A except that their com-parators were also annotated.
This second set ofquestions is referred as SET-B.
It contains 853comparative questions and 1,747 non-comparative questions.
For comparative questionidentification experiments, we used all labeledquestions in SET-A and SET-B.
For comparatorextraction experiments, we used only SET-B.
Allthe remaining unlabeled questions (called asSET-R) were used for training our weakly super-vised method.As a baseline method, we carefully imple-mented J&L?s method.
Specifically, CSRs forcomparative question identification were learnedfrom the labeled questions, and then a statisticalclassifier was built by using CSR rules as fea-tures.
We examined both SVM and Na?ve Bayes(NB) models as reported in their experiments.For the comparator extraction, LSRs werelearned from SET-B and applied for comparatorextraction.To start the bootstrapping procedure, we ap-plied the IEP ?<#start nn/$c vs/cc nn/$c ?/.#end>?
to all the questions in SET-R and ga-thered 12,194 comparator pairs as the initialseeds.
For our weakly supervised method, there3 There are 26 top level categories in Yahoo!
Answers.are four parameters, i.e.
?, ?, ?, and ?, need to bedetermined empirically.
We first mined all poss-ible candidate patterns from the suffix tree usingthe initial seeds.
From these candidate patterns,we applied them to SET-R and got a new set of59,410 candidate comparator pairs.
Among thesenew candidate comparator pairs, we randomlyselected 100 comparator pairs and manually clas-sified them into reliable or non-reliable compara-tors.
Then we found ?
that maximized precisionwithout hurting recall by investigating frequen-cies of pairs in the labeled set.
By this method, ?was set to 3 in our experiments.
Similarly, thethreshold parameters ?
and ?
for pattern evalua-tion were set to 10 and 0.8 respectively.
For theinterpolation parameter ?
in Equation (3), wesimply set the value to 0.5 by assuming that tworeliability scores are equally important.As evaluation measures for comparative ques-tion identification and comparator extraction, weused precision, recall, and F1-measure.
All re-sults were obtained from 5-fold cross validation.Note that J&L?s method needs a training data butours use the unlabeled data (SET-R) with weaklysupervised method to find parameter setting.This 5-fold evaluation data is not in the unla-beled data.
Both methods were tested on thesame test split in the 5-fold cross validation.
Allevaluation scores are averaged across all 5 folds.For question processing, we used our own sta-tistical POS tagger developed in-house4.4.2 Experiment ResultsComparative Question Identification andComparator ExtractionTable 2 shows our experimental results.
In thetable, ?Identification only?
indicates the perfor-mances in comparative question identification,?Extraction only?
denotes the performances ofcomparator extraction when only comparativequestions are used as input, and ?All?
indicatesthe end-to-end performances when questionidentification results were used in comparatorextraction.
Note that the results of J&L?s methodon our collections are very comparable to what isreported in their paper.In terms of precision, the J&L?s method iscompetitive to our method in comparative ques-4  We used NLC-PosTagger which is developed by NLCgroup of Microsoft Research Asia.
It uses the modifiedPenn Treebank POS set for its output; for example, NNS(plural nouns), NN (nouns), NP (noun phrases), NPS (pluralnoun phrases), VBZ (verb, present tense, 3rd person singu-lar), JJ (adjective), RB(adverb), and so on.655tion identification.
However, the recall is signifi-cantly lower than ours.
In terms of recall, ourmethod outperforms J&L?s method by 35% and22% in comparative question identification andcomparator extraction respectively.
In our analy-sis, the low recall of J&L?s method is mainlycaused by low coverage of learned CSR patternsover the test set.In the end-to-end experiments, our weakly su-pervised method performs significantly betterthan J&L?s method.
Our method is about 55%better in F1-measure.
This result also highlightsanother advantage of our method that identifiescomparative questions and extracts comparatorssimultaneously using one single pattern.
J&L?smethod uses two kinds of pattern rules, i.e.
CSRsand LSRs.
Its performance drops significantlydue to error propagations.
F1-measure of J&L?smethod in ?All?
is about 30% and 32% worsethan the scores of ?Identification only?
and ?Ex-traction?
only respectively, our method onlyshows small amount of performance decrease(approximately 7-8%).We also analyzed the effect of pattern genera-lization and specialization.
Table 3 shows theresults.
Despite of the simplicity of our methods,they significantly contribute to performance im-provements.
This result shows the importance oflearning patterns flexibly to capture variouscomparative question expressions.
Among the6,127 learned IEPs in our database, 5,930 pat-terns are generalized ones, 171 are specializedones, and only 26 patterns are non-generalizedand specialized ones.To investigate the robustness of our bootstrap-ping algorithm for different seed configurations,we compare the performances between two dif-ferent seed IEPs.
The results are shown in Table4.
As shown in the table, the performance of ourbootstrapping algorithm is stable regardless ofsignificantly different number of seed pairs gen-erated by the two IEPs.
This result implies thatour bootstrapping algorithm is not sensitive tothe choice of IEP.Table 5 also shows the robustness of our boot-strapping algorithm.
In Table 5, ?All?
indicatesthe performances that all comparator pairs from asingle seed IEP is used for the bootstrapping, and?Partial?
indicate the performances using only1,000 randomly sampled pairs from ?All?.
Asshown in the table, there is no significant per-formance difference.In addition, we conducted error analysis forthe cases where our method fails to extract cor-rect comparator pairs:?
23.75% of errors on comparator extractionare due to wrong pattern selection by oursimple maximum IEP length strategy.?
The remaining 67.63% of errors come fromcomparative questions which cannot be cov-ered by the learned IEPs.Recall Precision F-scoreOriginal Patterns 0.689  0.
449 0.544+ Specialized 0.731  0.602 0.665+ Generalized 0.760  0.776 0.768Table 3: Effect of pattern specialization and Generali-zation in the end-to-end experiments.Seed patterns # of resultedseed pairsF-score<#start nn/$c vs/cc nn/$c?/.
#end>12,194 0.768<#start which/wdt is/vbbetter/jjr , nn/$c or/ccnn/$c ?/.
#end>1,478 0.760Table 4: Performance variation over different initialseed IEPs in the end-to-end experimentsSet  (# of seed pairs) Recall Precision F-scoreAll (12,194) 0.760 0.774 0.768Partial (1,000) 0.724 0.763 0.743Table 5: Performance variation over different sizes ofseed pairs generated from a single initial seed IEP?<#start nn/$c vs/cc nn/$c ?/.
#end>?.Identification only(SET-A+SET-B)Extraction only(SET-B)All(SET-B)J&L (CSR) OurMethodJ&L(LSR)OurMethodJ&L OurMethod SVM NB SVM NBRecall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760*Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776*F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768*Table 2: Performance comparison between our method and Jindal and Bing?s Method (denoted as J&L).The values with * indicate statistically significant improvements over J&L (CSR) SVM or J&L (LSR)according to t-test  at p < 0.01 level.656Examples of Comparator ExtractionBy applying our bootstrapping method to theentire source data (60M questions), 328,364unique comparator pairs were extracted from679,909 automatically identified comparativequestions.Table 6 lists top 10 frequently compared enti-ties for a target item, such as Chanel, Gap, in ourquestion archive.
As shown in the table, ourcomparator mining method successfully discov-ers realistic comparators.
For example, for ?Cha-nel?, most results are high-end fashion brandssuch as ?Dior?
or ?Louis Vuitton?, while the rank-ing results for ?Gap?
usually contains similar ap-parel brands for young people, such as ?Old Navy?or ?Banana Republic?.
For the basketball player?Kobe?, most of the top ranked comparators arealso famous basketball players.
Some interestingcomparators are shown for ?Canon?
(the compa-ny name).
It is famous for different kinds of itsproducts, for example, digital cameras and prin-ters, so it can be compared to different kinds ofcompanies.
For example, it is compared to ?HP?,?Lexmark?, or ?Xerox?, the printer manufacturers,and also compared to ?Nikon?, ?Sony?, or ?Kodak?,the digital camera manufactures.
Besides gener-al entities such as a brand or company name, ourmethod also found an interesting comparableentity for a specific item in the experiments.
Forexample, our method recommends ?Nikon d40i?,?Canon rebel xti?, ?Canon rebel xt?, ?Nikond3000?, ?Pentax k100d?, ?Canon eos 1000d?
ascomparators for the specific camera product ?Ni-kon 40d?.Table 7 can show the difference between ourcomparator mining and query/item recommenda-tion.
As shown in the table, ?Google relatedsearches?
generally suggests a mixed set of twokinds of related queries for a target entity: (1)queries specified with subtopics for an originalquery (e.g., ?Chanel handbag?
for ?Chanel?)
and(2) its comparable entities (e.g., ?Dior?
for ?Cha-nel?).
It confirms one of our claims that compara-tor mining and query/item recommendation arerelated but not the same.5 ConclusionIn this paper, we present a novel weakly super-vised method to identify comparative questionsand extract comparator pairs simultaneously.
Werely on the key insight that a good comparativequestion identification pattern should extractgood comparators, and a good comparator pairshould occur in good comparative questions tobootstrap the extraction and identificationprocess.
By leveraging large amount of unla-beled data and the bootstrapping process withslight supervision to determine four parameters,we found 328,364 unique comparator pairs and6,869 extraction patterns without the need ofcreating a set of comparative question indicatorkeywords.The experimental results show that our me-thod is effective in both comparative questionidentification and comparator extraction.
It sig-Chanel Gap iPod Kobe Canon1 Dior Old Navy Zune Lebron Nikon2 Louis Vuitton American Eagle mp3 player Jordan Sony3 Coach Banana Republic PSP MJ Kodak4 Gucci Guess by Marciano cell phone Shaq Panasonic5 Prada ACP Ammunition iPhone Wade Casio6 Lancome Old Navy brand Creative Zen T-mac Olympus7 Versace Hollister Zen Lebron James Hp8 LV Aeropostal iPod nano Nash Lexmark9 Mac American Eagle outfitters iPod touch KG Pentax10 Dooney Guess iRiver Bonds XeroxTable 6: Examples of comparators for different entitiesChanel Gap iPod Kobe CanonChanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2iChanel sunglass Gap outlet iPod touch Lakers Kobe Canon printersChanel earrings Gap card iPod best buy Kobe espn Canon printer driversChanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloadsChanel shoes Gap casting call Apple Kobe NBA Canon copiersChanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scannerChanel clothing Old navy iPod support Kobe san Antonio Canon lensesDior Banana republic iPod classic Kobe Bryant 24 NikonTable 7: Related queries returned by Google related searches for the same target entities in Table 6.
The boldones indicate overlapped queries to the comparators in Table 6.657nificantly improves recall in both tasks whilemaintains high precision.
Our examples showthat these comparator pairs reflect what users arereally interested in comparing.Our comparator mining results can be used fora commerce search or product recommendationsystem.
For example, automatic suggestion ofcomparable entities can assist users in their com-parison activities before making their purchasedecisions.
Also, our results can provide usefulinformation to companies which want to identifytheir competitors.In the future, we would like to improve extrac-tion pattern application and mine rare extractionpatterns.
How to identify comparator aliases suchas ?LV?
and ?Louis Vuitton?
and how to separateambiguous entities such ?Paris vs. London?
aslocation and ?Paris vs. Nicole?
as celebrity areall interesting research topics.
We also plan todevelop methods to summarize answers pooledby a given comparator pair.6 AcknowledgementThis work was done when the first authorworked as an intern at Microsoft Research Asia.ReferencesMary Elaine Califf and Raymond J. Mooney.
1999.Relational learning of pattern-match rules for in-formation extraction.
In Proceedings of AAAI?99/IAAI?99.Claire Cardie.
1997.
Empirical methods in informa-tion extraction.
AI magazine, 18:65?79.Dan Gusfield.
1997.
Algorithms on strings, trees, andsequences: computer science and computationalbiology.
Cambridge University Press, New York,NY, USATaher H. Haveliwala.
2002.
Topic-sensitive pagerank.In Proceedings of WWW ?02, pages 517?526.Glen Jeh and Jennifer Widom.
2003.
Scaling persona-lized web search.
In Proceedings of WWW ?03,pages 271?279.Nitin Jindal and Bing Liu.
2006a.
Identifying compar-ative sentences in text documents.
In Proceedingsof SIGIR ?06, pages 244?251.Nitin Jindal and Bing Liu.
2006b.
Mining compara-tive sentences and relations.
In Proceedings ofAAAI ?06.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofACL-08: HLT, pages 1048?1056.Greg Linden, Brent Smith and Jeremy York.
2003.Amazon.com Recommendations: Item-to-ItemCollaborative Filtering.
IEEE Internet Computing,pages 76-80.Raymond J. Mooney and Razvan Bunescu.
2005.Mining knowledge from text using information ex-traction.
ACM SIGKDD Exploration Newsletter,7(1):3?10.Dragomir Radev, Weiguo Fan, Hong Qi, and HarrisWu and Amardeep Grewal.
2002.
Probabilisticquestion answering on the web.
Journal of theAmerican Society for Information Science andTechnology, pages 408?419.Deepak Ravichandran and Eduard Hovy.
2002.Learning surface text patterns for a question ans-wering system.
In Proceedings of ACL ?02, pages41?47.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-levelbootstrapping.
In Proceedings of AAAI ?99/IAAI ?99, pages 474?479.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedings ofthe 13th National Conference on Artificial Intelli-gence, pages 1044?1049.Stephen Soderland.
1999.
Learning information ex-traction rules for semi-structured and free text.
Ma-chine Learning, 34(1-3):233?272.658
