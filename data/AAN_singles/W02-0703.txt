Spoken Language Parsing Using Phrase-Level Grammars and TrainableClassifiersChad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay PetersonLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, USA{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.eduAbstractIn this paper, we describe a novelapproach to spoken language analysisfor translation, which uses a combinationof grammar-based phrase-level parsingand automatic classification.
The job ofthe analyzer is to produce a shallowsemantic interlingua representation forspoken task-oriented utterances.
Thegoal of our hybrid approach is to provideaccurate real-time analyses whileimproving robustness and portability tonew domains and languages.1 IntroductionInterlingua-based approaches to MachineTranslation (MT) are highly attractive in systemsthat support a large number of languages.
For eachsource language, an analyzer that converts thesource language into the interlingua is required.For each target language, a generator that convertsthe interlingua into the target language is needed.Given analyzers and generators for all supportedlanguages, the system simply connects the sourcelanguage analyzer with the target languagegenerator to perform translation.Robust and accurate analysis is critical ininterlingua-based translation systems.
In speech-to-speech translation systems, the analyzer must berobust to speech recognition errors, spontaneousspeech, and ungrammatical inputs as described byLavie (1996).
Furthermore, the analyzer should runin (near) real time.In addition to accuracy, speed, and robustness,the portability of the analyzer with respect to newdomains and new languages is an importantconsideration.
Despite continuing improvements inspeech recognition and translation technologies,restricted domains of coverage are still necessaryin order to achieve reasonably accurate machinetranslation.
Porting translation systems to newdomains or even expanding the coverage in anexisting domain can be very difficult and time-consuming.
This creates significant challenges insituations where translation is needed for a newdomain within relatively short notice.
Likewise,demand can be high for translation systems thatcan be rapidly expanded to include new languagesthat were not previously considered important.Thus, it is important that the analysis approachused in a translation system be portable to newdomains and languages.One approach to analysis in restricted domainsis to use semantic grammars, which focus onparsing semantic concepts rather than syntacticstructure.
Semantic grammars can be especiallyuseful for parsing spoken language because theyare less susceptible to syntactic deviations causedby spontaneous speech effects.
However, the focuson meaning rather than syntactic structuregenerally makes porting to a new domain quitedifficult.
Since semantic grammars do not exploitsyntactic similarities across domains, completelynew grammars must usually be developed.While grammar-based parsing can provide veryaccurate analyses on development data, it isdifficult for a grammar to completely cover adomain, a problem that is exacerbated by spokeninput.
Furthermore, it generally takes a great dealof effort by human experts to develop a high-coverage grammar.
On the other hand, machinelearning approaches can generalize beyond trainingdata and tend to degrade gracefully in the face ofnoisy input.
Machine learning methods may,however, be less accurate on clearly in-domaininput than grammars and may require a largeamount of training data.We describe a prototype version of an analyzerthat combines phrase-level parsing and machineAssociation for Computational Linguistics.Algorithms and Systems, Philadelphia, July 2002, pp.
15-22.Proceedings of the Workshop on Speech-to-Speech Translation:learning techniques to take advantage of thebenefits of each.
Phrase-level semantic grammarsand a robust parser are used to extract low-levelinterlingua arguments from an utterance.
Then,automatic classifiers assign high-level domainactions to semantic segments in the utterance.2 MT System OverviewThe analyzer we describe is used for English andGerman in several multilingual human-to-humanspeech-to-speech translation systems, including theNESPOLE!
system (Lavie et al, 2002).
The goalof NESPOLE!
is to provide translation forcommon users within real-world e-commerceapplications.
The system currently providestranslation in the travel and tourism domainbetween English, French, German and Italian.NESPOLE!
employs an interlingua-basedtranslation approach that uses four basic steps toperform translation.
First, an automatic speechrecognizer processes spoken input.
The best-ranked hypothesis from speech recognition is thenpassed through the analyzer to produce interlingua.Target language text is then generated from theinterlingua.
Finally, the target language text issynthesized into speech.This interlingua-based translation approachallows for distributed development of thecomponents for each language.
The componentsfor each language are assembled into a translationserver that accepts speech, text, or interlingua asinput and produces interlingua, text, andsynthesized speech.
In addition to the analyzerdescribed here, the English translation server usesthe JANUS Recognition Toolkit for speechrecognition, the GenKit system (Tomita & Nyberg,1988) for generation, and the Festival system(Black et al, 1999) for synthesis.NESPOLE!
uses a client-server architecture(Lavie et al, 2001) to enable users who arebrowsing the web pages of a service provider (e.g.a tourism bureau) to seamlessly connect to ahuman agent who speaks a different language.Using commercially available software such asMicrosoft NetMeeting?, a user is connected to theNESPOLE!
Mediator, which establishesconnections with the agent and with translationservers for the appropriate languages.
During adialogue, the Mediator transmits spoken input fromthe users to the translation servers and synthesizedtranslations from the servers to the users.3 The InterlinguaThe interlingua used in the NESPOLE!
system iscalled Interchange Format (IF) (Levin et al, 1998;Levin et al, 2000).
The IF defines a shallowsemantic representation for task-orientedutterances that abstracts away from language-specific syntax and idiosyncrasies while capturingthe meaning of the input.
Each utterance is dividedinto semantic segments called semantic dialogunits (SDUs), and an IF is assigned to each SDU.An IF representation consists of four parts: aspeaker tag, a speech act, an optional sequence ofconcepts, and an optional set of arguments.
Therepresentation takes the following form:speaker : speech act +concept* (argument*)The speaker tag indicates the role of the speakerin the dialogue.
The speech act captures thespeaker?s intention.
The concept sequence, whichmay contain zero or more concepts, captures thefocus of an SDU.
The speech act and conceptsequence are collectively referred to as the domainaction (DA).
The arguments use a feature-valuerepresentation to encode specific information fromthe utterance.
Argument values can be atomic orcomplex.
The IF specification defines all of thecomponents and describes how they can be legallycombined.
Several examples of utterances withcorresponding IFs are shown below.Thank you very much.a:thankHello.c:greeting (greeting=hello)How far in advance do I need to book a room for the Al-Cervo Hotel?c:request-suggestion+reservation+room (suggest-strength=strong,time=(time-relation=before,time-distance=question),who=i,room-spec=(room, identifiability=no,location=(object-name=cervo_hotel)))4 The Hybrid Analysis ApproachOur hybrid analysis approach uses a combinationof grammar-based parsing and machine learningtechniques to transform spoken utterances into theIF representation described above.
The speaker tagis assumed to be given.
Thus, the goal of theanalyzer is to identify the DA and arguments.The hybrid analyzer operates in three stages.First, semantic grammars are used to parse anutterance into a sequence of arguments.
Next, theutterance is segmented into SDUs.
Finally, the DAis identified using automatic classifiers.4.1 Argument ParsingThe first stage in analysis is parsing an utterancefor arguments.
During this stage, utterances areparsed with phrase-level semantic grammars usingthe robust SOUP parser (Gavald?, 2000).4.1.1 The ParserThe SOUP parser is a stochastic, chart-based, top-down parser that is designed to provide real-timeanalysis of spoken language using context-freesemantic grammars.
One important featureprovided by SOUP is word skipping.
The amountof skipping allowed is configurable and a list ofunskippable words can be defined.
Another featurethat is critical for phrase-level argument parsing isthe ability to produce analyses consisting ofmultiple parse trees.
SOUP also supports modulargrammar development (Woszczyna et al, 1998).Subgrammars designed for different domains orpurposes can be developed independently andapplied in parallel during parsing.
Parse tree nodesare then marked with a subgrammar label.
Whenan input can be parsed in multiple ways, SOUP canprovide a ranked list of interpretations.In the prototype analyzer, word skipping is onlyallowed between parse trees.
Only the best-rankedargument parse is used for further processing.4.1.2 The GrammarsFour grammars are defined for argument parsing:an argument grammar, a pseudo-argumentgrammar, a cross-domain grammar, and a sharedgrammar.
The argument grammar contains phrase-level rules for parsing arguments defined in the IF.Top-level argument grammar nonterminalscorrespond to top-level arguments in the IF.The pseudo-argument grammar contains top-level nonterminals that do not correspond tointerlingua concepts.
These rules are used forparsing common phrases that can be grouped intoclasses to capture more useful information for theclassifiers.
For example, all booked up, full, andsold out might be grouped into a class of phrasesthat indicate unavailability.
In addition, rules in thepseudo-argument grammar can be used forcontextual anchoring of ambiguous arguments.
Forexample, the arguments [who=] and [to-whom=]have the same values.
To parse these argumentsproperly in a sentence like ?Can you send me thebrochure?
?, we use a pseudo-argument grammarrule, which refers to the arguments [who=] and [to-whom=] within the appropriate context.The cross-domain grammar contains rules forparsing whole DAs that are domain-independent.For example, this grammar contains rules forgreetings (Hello, Good bye, Nice to meet you, etc.
).Cross-domain grammar rules do not cover allpossible domain-independent DAs.
Instead, therules focus on DAs with simple or no argumentlists.
Domain-independent DAs with complexargument lists are left to the classifiers.
Cross-domain rules play an important role in theprediction of SDU boundaries.Finally, the shared grammar contains commongrammar rules that can be used by all othersubgrammars.
These include definitions for mostof the arguments, since many can also appear assub-arguments.
RHSs in the argument grammarcontain mostly references to rules in the sharedgrammar.
This method eliminates redundant rulesin the argument and shared grammars and allowsfor more accurate grammar maintenance.4.2 SegmentationThe second stage of processing in the hybridanalysis approach is segmentation of the input intoSDUs.
The IF representation assigns DAs at theSDU level.
However, since dialogue utterancesoften consist of multiple SDUs, utterances must besegmented into SDUs before DAs can be assigned.Figure 1 shows an example utterance containingfour arguments segmented into two SDUs.SDU1  SDU2greeting= disposition= visit-spec= location=hello i would like to take a vacation in val di fiemmeFigure 1.
Segmentation of an utterance into SDUs.The argument parse may contain trees for cross-domain DAs, which by definition cover a completeSDU.
Thus, there must be an SDU boundary onboth sides of a cross-domain tree.
Additionally, noSDU boundaries are allowed within parse trees.The prototype analyzer drops words skippedbetween parse trees, leaving only a sequence oftrees.
The parse trees on each side of a potentialboundary are examined, and if either tree wasconstructed by the cross-domain grammar, an SDUboundary is inserted.
Otherwise, a simple statisticalmodel similar to the one described by Lavie et al(1997) estimates the likelihood of a boundary.The statistical model is based only on the rootlabels of the parse trees immediately preceding andfollowing the potential boundary position.
Supposethe position under consideration looks like[A1?A2], where there may be a boundary betweenarguments A1 and A2.
The likelihood of an SDUboundary is estimated using the following formula:])C([A  ])C([A])AC([  ])C([A])AF([A212121+?+??
?The counts C([A1?
]), C([?A2]), C([A1]), C([A2])are computed from the training data.
An evaluationof this baseline model is presented in section 6.4.3 DA ClassificationThe third stage of analysis is the identification ofthe DA for each SDU using automatic classifiers.After segmentation, a cross-domain parse tree maycover an SDU.
In this case, analysis is completesince the parse tree contains the DA.
Otherwise,automatic classifiers are used to assign the DA.
Inthe prototype analyzer, the DA classification taskis split into separate subtasks of classifying thespeech act and concept sequence.
This reduces thecomplexity of each subtask and allows for theapplication of specialized techniques to identifyeach component.One classifier is used to identify the speech act,and a second classifier identifies the conceptsequence.
Both classifiers are implemented usingTiMBL (Daelemans et al, 2000), a memory-basedlearner.
Speech act classification is performed first.Input to the speech act classifier is a set of binaryfeatures that indicate whether each of the possibleargument and pseudo-argument labels is present inthe argument parse for the SDU.
No other featuresare currently used.
Concept sequence classificationis performed after speech act classification.
Theconcept sequence classifier uses the same featureset as the speech act classifier with one additionalfeature: the speech act assigned by the speech actclassifier.
We present an evaluation of this baselineDA classification scheme in section 6.4.4 Using the IF SpecificationThe IF specification imposes constraints on howelements of the IF representation can legallycombine.
DA classification can be augmented withknowledge of constraints from the IF specification,providing two advantages over otherwise na?veclassification.
First, the analyzer must producevalid IF representations in order to be useful in atranslation system.
Second, using knowledge fromthe IF specification can improve the quality of theIF produced, and thus the translation.Two elements of the IF specification areespecially relevant to DA classification.
First, thespecification defines constraints on thecomposition of DAs.
There are constraints on howconcepts are allowed to pair with speech acts aswell as ordering constraints on how concepts areallowed to combine to form a valid conceptsequence.
These constraints can be used toeliminate illegal DAs during classification.
Thesecond important element of the IF specification isthe definition of how arguments are licensed byspeech acts and concepts.
In order for an IF to bevalid, at least one speech act or concept in the DAmust license each argument.The prototype analyzer uses the IF specificationto aid classification and guarantee that a valid IFrepresentation is produced.
The speech act andconcept sequence classifiers each provide a rankedlist of possible classifications.
When the bestspeech act and concept sequence combine to forman illegal DA or form a legal DA that does notlicense all of the arguments, the analyzer attemptsto find the next best legal DA that licenses themost arguments.
Each of the alternative conceptsequences (in ranked order) is combined with eachof the alternative speech acts (in ranked order).
Foreach possible legal DA, the analyzer checks if allof the arguments found during parsing are licensed.If a legal DA is found that licenses all of thearguments, then the process stops.
If not, oneadditional fallback strategy is used.
The analyzerthen tries to combine the best classified speech actwith each of the concept sequences that occurred inthe training data, sorted by their frequency ofoccurrence.
Again, the analyzer checks if eachlegal DA licenses all of the arguments and stops ifsuch a DA is found.
If this step fails to produce alegal DA that licenses all of the arguments, thebest-ranked DA that licenses the most arguments isreturned.
In this case, any arguments that are notlicensed by the selected DA are removed.
Thisapproach is used because it is generally better toselect an alternative DA and retain more argumentsthan to keep the best DA and lose the informationrepresented by the arguments.
An evaluation ofthis strategy is presented in the section 6.5 Grammar Development andClassifier TrainingDuring grammar development, it is generallyuseful to see how changes to the grammar affectthe IF representations produced by the analyzer.
Ina purely grammar-based analysis approach, fullinterlingua representations are produced as theresult of parsing, so testing new grammars simplyrequires loading them into the parser.
Because thegrammars used in our hybrid approach parse at theargument level, testing grammar modifications atthe complete IF level requires retraining thesegmentation model and the DA classifiers.When new grammars are ready for testing,utterance-IF pairs for the appropriate language areextracted from the training database.
Eachutterance-IF pair in the training data consists of asingle SDU with a manually annotated IF.
Usingthe new grammars, the argument parser is appliedto each utterance to produce an argument parse.The counts used by the segmentation model arethen recomputed based on the new argumentparses.
Since each utterance contains a singleSDU, the counts C([?A2]) and C([A1?])
can becomputed directly from the first and last argumentsin the parse respectively.Next, the training examples for the DAclassifiers are constructed.
Each training examplefor the speech act classifier consists of the speechact from the annotated IF and a vector of binaryfeatures with a positive value set for each argumentor pseudo-argument label that occurs in theargument parse.
The training examples for theconcept sequence classifiers are similar with theaddition of the annotated speech act to the featurevector.
After the training examples are constructed,new classifiers are trained.Two tools are available to support easy testingduring grammar development.
First, the entiretraining process can be run using a single script.Retraining for a new grammar simply requiresrunning the script with pointers to the newgrammars.
Then, a special development mode ofthe translation servers allows the grammar writersto load development grammars and theircorresponding segmentation model and DAclassifiers.
The translation server supports input inthe form of individual utterances or files andallows the grammar developers to look at theresults of each stage of the analysis process.6 EvaluationWe present the results from recent experiments tomeasure the performance of the analyzercomponents and of end-to-end translation using theanalyzer.
We also report the results of an ablationexperiment that used earlier versions of theanalyzer and IF specification.6.1 Translation ExperimentAcceptable PerfectSR Hypotheses 66% 56%Translation fromTranscribed Text 58% 43%Translation fromSR Hypotheses 45% 32%Table 1.
English-to-English end-to-end translationAcceptable PerfectTranslation fromTranscribed Text 55% 38%Translation fromSR Hypotheses 43% 27%Table 2.
English-to-Italian end-to-end translationTables 1 and 2 show end-to-end translationresults of the NESPOLE!
system.
In thisexperiment, the input was a set of Englishutterances.
The utterances were paraphrased backinto English via the interlingua (Table 1) andtranslated into Italian (Table 2).
The data used totrain the DA classifiers consisted of 3350 SDUsannotated with IF representations.
The test setcontained 151 utterances consisting of 332 SDUsfrom 4 unseen dialogues.
Translations werecompared to human transcriptions and graded asdescribed in (Levin et al, 2000).
A grade ofperfect, ok, or bad was assigned to eachtranslation by human graders.
A grade of perfector ok is considered acceptable.
The table shows theaverage of grades assigned by three graders.The row in Table 1 labeled SR Hypothesesshows the grades when the speech recognizeroutput is compared directly to human transcripts.As these grades show, recognition errors can be amajor source of unacceptable translations.
Thesegrades provide a rough bound on the translationperformance that can be expected when using inputfrom the speech recognizer since meaning lost dueto recognition errors cannot be recovered.
Therows labeled Translation from Transcribed Textshow the results when human transcripts are usedas input.
These grades reflect the combinedperformance of the analyzer and generator.
Therows labeled Translation from SR Hypothesesshow the results when the speech recognizerproduces the input utterances.
As expected,translation performance was worse with theintroduction of recognition errors.Precision Recall70% 54%Table 3.
SDU boundary detection performanceTable 3 shows the performance of thesegmentation model on the test set.
The SDUboundary positions assigned automatically werecompared with manually annotated positions.Classifier AccuracySpeech Act 65%Concept Sequence 54%Domain Action 43%Table 4.
Classifier accuracy on transcriptionFrequencySpeech Act 33%Concept Sequence 40%Domain Action 14%Table 5.
Frequency of most common DA elementsTable 4 shows the performance of the DAclassifiers, and Table 5 shows the frequency of themost common DA, speech act, and conceptsequence in the test set.
Transcribed utteranceswere used as input and were segmented into SDUsbefore analysis.
This experiment is based on only293 SDUs.
For the remaining SDUs in the test set,it was not possible to assign a valid representationbased on the current IF specification.These results demonstrate that it is not alwaysnecessary to find the canonical DA to produce anacceptable translation.
This can be seen bycomparing the Domain Action accuracy from Table4 with the Transcribed grades from Table 1.Although the DA classifiers produced thecanonical DA only 43% of the time, 58% of thetranslations were graded as acceptable.ChangedSpeech Act 5%Concept Sequence 26%Domain Action 29%Table 6.
DA elements changed by IF specificationIn order to examine the effects of using IFspecification constraints, we looked at the 182SDUs which were not parsed by the cross-domaingrammar and thus required DA classification.Table 6 shows how many DAs, speech acts, andconcept sequences were changed as a result ofusing the constraints.
DAs were changed eitherbecause the DA was illegal or because the DA didnot license some of the arguments.
Without the IFspecification, 4% of the SDUs would have beenassigned an illegal DA, and 29% of the SDUs(those with a changed DA) would have beenassigned an illegal IF.
Furthermore, without the IFspecification, 0.38 arguments per SDU would haveto be dropped while only 0.07 arguments per SDUwere dropped when using the fallback strategy.The mean number of arguments per SDU was 1.47.6.2 Ablation ExperimentClassification Accuracy (16-fold CrossValidation)00.20.40.60.8500 1000 2000 3000 4000 5000 6009Training Set SizeMeanAccuracySpeech ActConceptSequenceDomain ActionFigure 2: DA classifier accuracy with varyingamounts of dataFigure 2 shows the results of an ablationexperiment that examined the effect of varying thetraining set size on DA classification accuracy.Each point represents the average accuracy using a16-fold cross validation setup.The training data contained 6409 SDU-interlingua pairs.
The data were randomly dividedinto 16 test sets containing 400 examples each.
Ineach fold, the remaining data were used to createtraining sets containing 500, 1000, 2000, 3000,4000, 5000, and 6009 examples.The performance of the classifiers appears tobegin leveling off around 4000 training examples.These results seem promising with regard to theportability of the DA classifiers since a data set ofthis size could be constructed in a few weeks.7 Related WorkLavie et al (1997) developed a method foridentifying SDU boundaries in a speech-to-speechtranslation system.
Identifying SDU boundaries isalso similar to sentence boundary detection.Stevenson and Gaizauskas (2000) use TiMBL(Daelemans et al, 2000) to identify sentenceboundaries in speech recognizer output, and Gotohand Renals (2000) use a statistical approach toidentify sentence boundaries in automatic speechrecognition transcripts of broadcast speech.Munk (1999) attempted to combine grammarsand machine learning for DA classification.
InMunk?s SALT system, a two-layer HMM was usedto segment and label arguments and speech acts.
Aneural network identified the concept sequences.Finally, semantic grammars were used to parseeach argument segment.
One problem with SALTwas that the segmentation was often inaccurate andresulted in bad parses.
Also, SALT did not use across-domain grammar or interlingua specification.Cattoni et al (2001) apply statistical languagemodels to DA classification.
A word bigram modelis trained for each DA in the training data.
To labelan utterance, the most likely DA is assigned.Arguments are identified using recursive transitionnetworks.
IF specification constraints are used tofind the most likely valid DA and arguments.8 Discussion and Future WorkOne of the primary motivations for developing thehybrid analysis approach described here is toimprove the portability of the analyzer to newdomains and languages.
We expect that movingfrom a purely grammar-based parsing approach tothis hybrid approach will help attain this goal.The SOUP parser supports portability to newdomains by allowing separate grammar modulesfor each domain and a grammar of rules sharedacross domains (Woszczyna et al, 1998).
Thismodular grammar design provides an effectivemethod for adding new domains to existinggrammars.
Nevertheless, developing a fullsemantic grammar for a new domain requiressignificant effort by expert grammar writers.The hybrid approach reduces the manual laborrequired to port to new domains by incorporatingmachine learning.
The most labor-intensive part ofdeveloping full semantic grammars for producingIF is writing DA-level rules.
This is exactly thework eliminated by using automatic DA classifiers.Furthermore, the phrase-level argument grammarsused in the analyzer contain fewer rules than a fullsemantic grammar.
The argument-level grammarsare also less domain-dependent than the fullgrammars and thus more reusable.
The DAclassifiers should also be more tolerant than fullgrammars of deviations from the domain.We analyzed the grammars from a previousversion of the translation system, which producedcomplete IFs using strictly grammar-based parsing,to estimate what portion of the grammar wasdevoted to the identification of domain actions.Approximately 2200 rules were used to cover 400DAs.
Nonlexical rules made up about half of thegrammar, and the DA rules accounted for about20% of the nonlexical rules.
Using these figures,we can project the number of DA rules that wouldhave to be added to the current system, which usesour hybrid analysis approach.
The database for thenew system contains approximately 600 DAs.Assuming the average number of rules per DA isthe same as before, roughly 3300 DA-level ruleswould have to be added to the current grammar,which has about 17500 nonlexical rules, to coverthe DAs in the database.Our hybrid approach should also improve theportability of the analyzer to new languages.
Sincegrammars are language specific, adding a newlanguage still requires writing new argumentgrammars.
Then the DA classifiers simply need tobe retrained on data for the new language.
Iftraining data for the new language were notavailable, DA classifiers using only language-independent features, from the IF for example,could be trained on data for existing languages andused for the new language.
Such classifiers couldbe used as a starting point until training data wasavailable in the new language.The experimental results indicate the promiseof the analysis approach we have described.
Thelevel of performance reported here was achievedusing a simple segmentation model and simple DAclassifiers with limited feature sets.
We expect thatperformance will substantially improve with amore informed design of the segmentation modeland DA classifiers.
We plan to examine variousdesign options, including richer feature sets andalternative classification techniques.
We are alsoplanning experiments to evaluate robustness andportability when the coverage of the NESPOLE!system is expanded to the medical domain laterthis year.
In these experiments, we will measurethe effort needed to write new argument grammars,the extent to which existing argument grammarsare reusable, and the effort required to expand theargument grammar to include DA-level rules.9 AcknowledgementsThe research work reported here was supported bythe National Science Foundation under Grantnumber 9982227.
Special thanks to Alex Waibeland everyone in the NESPOLE!
group for theirsupport on this work.ReferencesBlack, A., P. Taylor, and R. Caley.
1999.
TheFestival Speech Synthesis System: SystemDocumentation.
Human Computer ResearchCentre, University of Edinburgh, Scotland.http://www.cstr.ed.ac.uk/projects/festival/manualCattoni, R., M. Federico, and A. Lavie.
2001.Robust Analysis of Spoken Input CombiningStatistical and Knowledge-Based InformationSources.
In Proceedings of the IEEE AutomaticSpeech Recognition and UnderstandingWorkshop, Trento, Italy.Daelemans, W., J. Zavrel, K. van der Sloot, and A.van den Bosch.
2000.
TiMBL: Tilburg MemoryBased Learner, version 3.0, Reference Guide.ILK Technical Report 00-01.http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gzGavald?, M. 2000.
SOUP: A Parser for Real-World Spontaneous Speech.
In Proceedings ofthe IWPT-2000, Trento, Italy.Gotoh, Y. and S. Renals.
Sentence BoundaryDetection in Broadcast Speech Transcripts.
2000.In Proceedings on the International SpeechCommunication Association Workshop:Automatic Speech Recognition: Challenges forthe New Millennium, Paris.Lavie, A., F. Metze, F. Pianesi, et al 2002.Enhancing the Usability and Performance ofNESPOLE!
?
a Real-World Speech-to-SpeechTranslation System.
In Proceedings of HLT-2002, San Diego, CA.Lavie, A., C. Langley, A. Waibel, et al 2001.Architecture and Design Considerations inNESPOLE!
: a Speech Translation System for E-commerce Applications.
In Proceedings of HLT-2001, San Diego, CA.Lavie, A., D. Gates, N. Coccaro, and L. Levin.1997.
Input Segmentation of Spontaneous Speechin JANUS: a Speech-to-speech TranslationSystem.
In Dialogue Processing in SpokenLanguage Systems: Revised Papers from ECAI-96 Workshop, E. Maier, M. Mast, and S.Luperfoy (eds.
), LNCS series, Springer Verlag.Lavie, A.
1996.
GLR*: A Robust Grammar-Focused Parser for Spontaneously SpokenLanguage.
PhD dissertation, Technical ReportCMU-CS-96-126, Carnegie Mellon University,Pittsburgh, PA.Levin, L., D. Gates, A. Lavie, et al 2000.Evaluation of a Practical Interlingua for Task-Oriented Dialogue.
In Workshop on AppliedInterlinguas: Practical Applications ofInterlingual Approaches to NLP, Seattle.Levin, L., D. Gates, A. Lavie, and A. Waibel.1998.
An Interlingua Based on Domain Actionsfor Machine Translation of Task-OrientedDialogues.
In Proceedings of ICSLP-98, Vol.
4,pp.
1155-1158, Sydney, Australia.Munk, M. 1999.
Shallow Statistical Parsing forMachine Translation.
Diploma Thesis, KarlsruheUniversity.Stevenson, M. and R. Gaizauskas.
Experiments onSentence Boundary Detection.
2000.
InProceedings of ANLP and NAACL-2000, Seattle.Tomita, M. and E. H. Nyberg.
1988.
GenerationKit and Transformation Kit, Version 3.2: User?sManual.
Technical Report CMU-CMT-88-MEMO, Carnegie Mellon University, Pittsburgh,PA.Woszczyna, M., M. Broadhead, D. Gates, et al1998.
A Modular Approach to Spoken LanguageTranslation for Large Domains.
In Proceedingsof AMTA-98, Langhorne, PA.
