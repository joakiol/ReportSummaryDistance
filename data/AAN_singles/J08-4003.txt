Algorithms for Deterministic IncrementalDependency ParsingJoakim Nivre?,??Va?xjo?
University, Uppsala UniversityParsing algorithms that process the input from left to right and construct a single derivationhave often been considered inadequate for natural language parsing because of the massiveambiguity typically found in natural language grammars.
Nevertheless, it has been shownthat such algorithms, combined with treebank-induced classifiers, can be used to build highlyaccurate disambiguating parsers, in particular for dependency-based syntactic representations.In this article, we first present a general framework for describing and analyzing algorithmsfor deterministic incremental dependency parsing, formalized as transition systems.
We thendescribe and analyze two families of such algorithms: stack-based and list-based algorithms.In the former family, which is restricted to projective dependency structures, we describe anarc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant.
For each of the four algorithms, we give proofs of correctness and complexity.In addition, we perform an experimental evaluation of all algorithms in combination withSVM classifiers for predicting the next parsing action, using data from thirteen languages.
Weshow that all four algorithms give competitive accuracy, although the non-projective list-basedalgorithm generally outperforms the projective algorithms for languages with a non-negligibleproportion of non-projective constructions.
However, the projective algorithms often producecomparable results when combined with the technique known as pseudo-projective parsing.
Thelinear time complexity of the stack-based algorithms gives them an advantage with respect toefficiency both in learning and in parsing, but the projective list-based algorithm turns out tobe equally efficient in practice.
Moreover, when the projective algorithms are used to implementpseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning)than the non-projective list-based algorithm.
Although most of the algorithms have been partiallydescribed in the literature before, this is the first comprehensive analysis and evaluation of thealgorithms within a unified framework.1.
IntroductionBecause parsers for natural language have to cope with a high degree of ambigu-ity and nondeterminism, they are typically based on different techniques than theones used for parsing well-defined formal languages?for example, in compilers for?
School of Mathematics and Systems Engineering, Va?xjo?
University, 35195 Va?xjo?, Sweden.E-mail: joakim.nivre@vxu.se.??
Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.Submission received: 29 May 2007; revised submission received 22 September 2007; accepted for publication:3 November 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 4programming languages.
Thus, the mainstream approach to natural language parsinguses algorithms that efficiently derive a potentially very large set of analyses in parallel,typically making use of dynamic programming and well-formed substring tables orcharts.
When disambiguation is required, this approach can be coupled with a statisticalmodel for parse selection that ranks competing analyses with respect to plausibility.Although it is often necessary, for efficiency reasons, to prune the search space priorto the ranking of complete analyses, this type of parser always has to handle multipleanalyses.By contrast, parsers for formal languages are usually based on deterministic parsingtechniques, which are maximally efficient in that they only derive one analysis.
Thisis possible because the formal language can be defined by a non-ambiguous formalgrammar that assigns a single canonical derivation to each string in the language, aproperty that cannot be maintained for any realistically sized natural language gram-mar.
Consequently, these deterministic parsing techniques have been much less popularfor natural language parsing, except as a way of modeling human sentence process-ing, which appears to be at least partly deterministic in nature (Marcus 1980; Shieber1983).More recently, however, it has been shown that accurate syntactic disambiguationfor natural language can be achieved using a pseudo-deterministic approach, wheretreebank-induced classifiers are used to predict the optimal next derivation step whenfaced with a nondeterministic choice between several possible actions.
Compared tothe more traditional methods for natural language parsing, this can be seen as a severeform of pruning, where parse selection is performed incrementally so that only a singleanalysis is derived by the parser.
This has the advantage of making the parsing processvery simple and efficient but the potential disadvantage that overall accuracy suffersbecause of the early commitment enforced by the greedy search strategy.
Somewhatsurprisingly, though, research has shown that, with the right choice of parsing algorithmand classifier, this type of parser can achieve state-of-the-art accuracy, especially whenused with dependency-based syntactic representations.Classifier-based dependency parsing was pioneered by Kudo and Matsumoto(2002) for unlabeled dependency parsing of Japanese with head-final dependenciesonly.
The algorithm was generalized to allow both head-final and head-initial depen-dencies by Yamada and Matsumoto (2003), who reported very good parsing accuracyfor English, using dependency structures extracted from the Penn Treebank for trainingand testing.
The approach was extended to labeled dependency parsing by Nivre, Hall,and Nilsson (2004) (for Swedish) and Nivre and Scholz (2004) (for English), using adifferent parsing algorithm first presented in Nivre (2003).
At a recent evaluation ofdata-driven systems for dependency parsing with data from 13 different languages(Buchholz and Marsi 2006), the deterministic classifier-based parser of Nivre et al (2006)reached top performance together with the system of McDonald, Lerman, and Pereira(2006), which is based on a global discriminative model with online learning.
Theseresults indicate that, at least for dependency parsing, deterministic parsing is possiblewithout a drastic loss in accuracy.
The deterministic classifier-based approach has alsobeen applied to phrase structure parsing (Kalt 2004; Sagae and Lavie 2005), although theaccuracy for this type of representation remains a bit below the state of the art.
In thissetting, more competitive results have been achieved using probabilistic classifiers andbeam search, rather than strictly deterministic search, as in the work by Ratnaparkhi(1997, 1999) and Sagae and Lavie (2006).A deterministic classifier-based parser consists of three essential components: aparsing algorithm, which defines the derivation of a syntactic analysis as a sequence514Nivre Deterministic Incremental Dependency Parsingof elementary parsing actions; a feature model, which defines a feature vector represen-tation of the parser state at any given time; and a classifier, which maps parser states,as represented by the feature model, to parsing actions.
Although different types ofparsing algorithms, feature models, and classifiers have been used for deterministicdependency parsing, there are very few studies that compare the impact of differentcomponents.
The notable exceptions are Cheng, Asahara, and Matsumoto (2005), whocompare two different algorithms and two types of classifier for parsing Chinese, andHall, Nivre, and Nilsson (2006), who compare two types of classifiers and several typesof feature models for parsing Chinese, English, and Swedish.In this article, we focus on parsing algorithms.
More precisely, we describe twofamilies of algorithms that can be used for deterministic dependency parsing, supportedby classifiers for predicting the next parsing action.
The first family uses a stack to storepartially processed tokens and is restricted to the derivation of projective dependencystructures.
The algorithms of Kudo and Matsumoto (2002), Yamada and Matsumoto(2003), and Nivre (2003, 2006b) all belong to this family.
The second family, representedby the algorithms described by Covington (2001) and recently explored for classifier-based parsing in Nivre (2007), instead uses open lists for partially processed tokens,which allows arbitrary dependency structures to be processed (in particular, structureswith non-projective dependencies).
We provide a detailed analysis of four differentalgorithms, two from each family, and give proofs of correctness and complexity foreach algorithm.
In addition, we perform an experimental evaluation of accuracy andefficiency for the four algorithms, combined with state-of-the-art classifiers, using datafrom 13 different languages.
Although variants of these algorithms have been partiallydescribed in the literature before, this is the first comprehensive analysis and evaluationof the algorithms within a unified framework.The remainder of the article is structured as follows.
Section 2 defines the task ofdependency parsing and Section 3 presents a formal framework for the characterizationof deterministic incremental parsing algorithms.
Sections 4 and 5 contain the formalanalysis of four different algorithms, defined within the formal framework, with proofsof correctness and complexity.
Section 6 presents the experimental evaluation; Section 7reports on related work; and Section 8 contains our main conclusions.2.
Dependency ParsingDependency-based syntactic theories are based on the idea that syntactic structure canbe analyzed in terms of binary, asymmetric dependency relations holding between thewords of a sentence.
This basic conception of syntactic structure underlies a variety ofdifferent linguistic theories, such as Structural Syntax (Tesnie`re 1959), Functional Gener-ative Description (Sgall, Hajic?ova?, and Panevova?
1986), Meaning-Text Theory (Mel?c?uk1988), and Word Grammar (Hudson 1990).
In computational linguistics, dependency-based syntactic representations have in recent years been used primarily in data-drivenmodels, which learn to produce dependency structures for sentences solely from anannotated corpus, as in the work of Eisner (1996), Yamada and Matsumoto (2003), Nivre,Hall, and Nilsson (2004), and McDonald, Crammer, and Pereira (2005), among others.One potential advantage of such models is that they are easily ported to any domain orlanguage in which annotated resources exist.In this kind of framework the syntactic structure of a sentence is modeled by a depen-dency graph, which represents each word and its syntactic dependents through labeleddirected arcs.
This is exemplified in Figure 1, for a Czech sentence taken from the Prague515Computational Linguistics Volume 34, Number 4(?Only one of them concerns quality.?
)ROOT0 Z1(Out-of?
?AuxPnich2them?
?Atrje3is?
?Predjen4only?
?AuxZjedna5one-FEM-SG?
?Sbna6to?
?AuxPkvalitu7quality??
Adv.8.)?
?AuxKFigure 1Dependency graph for a Czech sentence from the Prague Dependency Treebank.ROOT0 Economic1?
?NMODnews2?
?SBJhad3?
?ROOTlittle4?
?NMODeffect5?
?OBJon6?
?NMODfinancial7?
?NMODmarkets8?
?PMOD.9??
PFigure 2Dependency graph for an English sentence from the Penn Treebank.Dependency Treebank (Hajic?
et al 2001; Bo?hmova?
et al 2003), and in Figure 2, for anEnglish sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz1993; Marcus et al 1994).1 An artificial word ROOT has been inserted at the beginningof each sentence, serving as the unique root of the graph.
This is a standard device thatsimplifies both theoretical definitions and computational implementations.Definition 1Given a set L = {l1, .
.
.
, l|L|} of dependency labels, a dependency graph for a sentencex = (w0,w1, .
.
.
,wn) is a labeled directed graph G = (V,A), where1.
V = {0, 1, .
.
.
,n} is a set of nodes,2.
A ?
V ?
L?
V is a set of labeled directed arcs.The set V of nodes (or vertices) is the set of non-negative integers up to and includingn, each corresponding to the linear position of a word in the sentence (including ROOT).The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and jare nodes and l is a dependency label.
Because arcs are used to represent dependencyrelations, we will say that i is the head and l is the dependency type of j. Conversely,we say that j is a dependent of i.1 In the latter case, the dependency graph has been derived automatically from the constituency-basedannotation in the treebank, using the Penn2Malt program, available at http://w3.msi.vxu.se/users/nivre/research/Penn2Malt.html.516Nivre Deterministic Incremental Dependency ParsingDefinition 2A dependency graph G = (V,A) is well-formed if and only if:1.
The node 0 is a root, that is, there is no node i and label l such that(i, l, 0) ?
A.2.
Every node has at most one head and one label, that is, if (i, l, j) ?
A thenthere is no node i?
and label l?
such that (i?, l?, j) ?
A and i = i?
or l = l?.3.
The graph G is acyclic, that is, there is no (non-empty) subset of arcs{(i0, l1, i1), (i1, l2, i2), .
.
.
, (ik?1, lk, ik)} ?
A such that i0 = ik.We will refer to conditions 1?3 as ROOT, SINGLE-HEAD, and ACYCLICITY, respectively.Any dependency graph satisfying these conditions is a dependency forest; if it is alsoconnected, it is a dependency tree, that is, a directed tree rooted at the node 0.
It is worthnoting that any dependency forest can be turned into a dependency tree by adding arcsfrom the node 0 to all other roots.Definition 3A dependency graph G = (V,A) is projective if and only if, for every arc (i, l, j) ?A and node k ?
V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1),(i1, l2, i2), .
.
.
(ik?1, lk, ik)} ?
A such that ik = k.In a projective dependency graph, every node has a continuous projection, where theprojection of a node i is the set of nodes reachable from i in the reflexive and transitiveclosure of the arc relation.
This corresponds to the ban on discontinuous constituentsin orthodox phrase structure representations.
We call this condition PROJECTIVITY.When discussing PROJECTIVITY, we will often use the notation i??
j to mean that jis reachable from i in the reflexive and transitive closure of the arc relation.Example 1For the graphs depicted in Figures 1 and 2, we have:Figure 1: G1 = (V1,A1)V1 = {0, 1, 2, 3, 4, 5, 6, 7, 8}A1 = {(0, Pred, 3), (0,AuxK, 8), (1,Atr, 2), (3, Sb, 5), (3, AuxP, 6),(5,AuxP, 1), (5,AuxZ, 4), (6,Adv, 7)}Figure 2: G2 = (V2,A2)V2 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}A2 = {(0, ROOT, 3), (2, NMOD, 1), (3, SBJ, 2), (3, OBJ, 5), (3, P, 9),(5, NMOD, 4), (5, NMOD, 6), (6, PMOD, 8), (8, NMOD, 7)}Both G1 and G2 are well-formed dependency forests (dependency trees, to be specific),but only G2 is projective.
In G1, the arc (5,AuxP, 1) spans node 3, which is not reachablefrom node 5 by following dependency arcs.3.
Deterministic Incremental Dependency ParsingIn this section, we introduce a formal framework for the specification of deterministicdependency parsing algorithms in terms of two components: a transition system, which517Computational Linguistics Volume 34, Number 4is nondeterministic in the general case, and an oracle, which always picks a singletransition out of every parser configuration.
The use of transition systems to studycomputation is a standard technique in theoretical computer science, which is herecombined with the notion of oracles in order to characterize parsing algorithms withdeterministic search.
In data-driven dependency parsing, oracles normally take theform of classifiers, trained on treebank data, but they can also be defined in terms ofgrammars and heuristic disambiguation rules (Nivre 2003).The main reason for introducing this framework is to allow us to characterizealgorithms that have previously been described in different traditions and to comparetheir formal properties within a single unified framework.
In particular, whereas thistype of framework has previously been used to characterize algorithms in the stack-based family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the list-based algorithms first discussed by Covington (2001).Definition 4A transition system for dependency parsing is a quadruple S = (C,T, cs,Ct), where1.
C is a set of configurations, each of which contains a buffer ?
of(remaining) nodes and a set A of dependency arcs,2.
T is a set of transitions, each of which is a (partial) function t : C?
C,3.
cs is an initialization function, mapping a sentence x = (w0,w1, .
.
.
,wn) to aconfiguration with ?
= [1, .
.
.
,n],4.
Ct ?
C is a set of terminal configurations.A configuration is required to contain at least a buffer ?, initially containing the nodes[1, .
.
.
,n] corresponding to the real words of a sentence x = (w0,w1, .
.
.
,wn), and a setA of dependency arcs, defined on the nodes in V = {0, 1, .
.
.
,n}, given some set ofdependency labels L. The specific transition systems defined in Sections 4 and 5 willextend this basic notion of configuration with different data structures, such as stacksand lists.
We use the notation ?c and Ac to refer to the value of ?
and A, respectively, ina configuration c; we also use |?| to refer to the length of ?
(i.e., the number of nodes inthe buffer) and we use [ ] to denote an empty buffer.Definition 5Let S = (C,T, cs,Ct) be a transition system.
A transition sequence for a sentence x =(w0,w1, .
.
.
,wn) in S is a sequence C0,m = (c0, c1, .
.
.
, cm) of configurations, such that1.
c0 = cs(x),2. cm ?
Ct,3.
for every i (1 ?
i ?
m), ci = t(ci?1) for some t ?
T.The parse assigned to x by C0,m is the dependency graph Gcm = ({0, 1, .
.
.
,n},Acm ),where Acm is the set of dependency arcs in cm.Starting from the initial configuration for the sentence to be parsed, transitions willmanipulate ?
and A (and other available data structures) until a terminal configurationis reached.
Because the node set V is given by the input sentence itself, the set Acm ofdependency arcs in the terminal configuration will determine the output dependencygraph Gcm = (V,Acm ).518Nivre Deterministic Incremental Dependency ParsingDefinition 6A transition system S = (C,T, cs,Ct) is incremental if and only if, for every configurationc ?
C and transition t ?
T, it holds that:1. if ?c = [ ] then c ?
Ct,2.
|?c| ?
|?t(c)|,3.
if a ?
Ac then a ?
At(c).The first two conditions state that the buffer ?
never grows in size and that parsingterminates as soon as it becomes empty; the third condition states that arcs addedto A can never be removed.
Note that this is only one of several possible notions ofincrementality in parsing.
A weaker notion would be to only require that the set of arcsis built monotonically (the third condition); a stronger notion would be to require alsothat nodes in ?
are processed strictly left to right.Definition 7Let S = (C,T, cs,Ct) be a transition system for dependency parsing.1.
S is sound for a class G of dependency graphs if and only if, for everysentence x and every transition sequence C0,m for x in S, the parse Gcm ?
G.2.
S is complete for a class G of dependency graphs if and only if, for everysentence x and every dependency graph Gx for x in G, there is a transitionsequence C0,m for x in S such that Gcm = Gx.3.
S is correct for a class G of dependency graphs if and only if it is soundand complete for G.The notions of soundness and completeness, as defined here, can be seen as correspond-ing to the notions of soundness and completeness for grammar parsing algorithms,according to which an algorithm is sound if it only derives parses licensed by thegrammar and complete if it derives all such parses (Shieber, Schabes, and Pereira 1995).Depending on the nature of a transition system S, there may not be a transitionsequence for every sentence, or there may be more than one such sequence.
Thesystems defined in Sections 4 and 5 will all be such that, for any input sentencex = (w0,w1, .
.
.
,wn), there is always at least one transition sequence for x (and usuallymore than one).Definition 8An oracle for a transition system S = (C,T, cs,Ct) is a function o : C?
T.Given a transition system S = (C,T, cs,Ct) and an oracle o, deterministic parsing can beachieved by the following simple algorithm:PARSE(x = (w0,w1, .
.
.
,wn))1 c?
cs(x)2 while c ?
Ct3 c?
[o(c)](c)4 return Gc519Computational Linguistics Volume 34, Number 4It is easy to see that, provided that there is at least one transition sequence in S forevery sentence, such a parser constructs exactly one transition sequence C0,m for asentence x and returns the parse defined by the terminal configuration cm, that is,Gcm = ({0, 1, .
.
.
,n},Acm ).
The reason for separating the oracle o, which maps a configu-ration c to a transition t, from the transition t itself, which maps a configuration c to anew configuration c?, is to have a clear separation between the abstract machine definedby the transition system, which determines formal properties such as correctness andcomplexity, and the search mechanism used when executing the machine.In the experimental evaluation in Section 6, we will use the standard techniqueof approximating oracles with classifiers trained on treebank data.
However, in theformal characterization of different parsing algorithms in Sections 4 and 5, we willconcentrate on properties of the underlying transition systems.
In particular, assumingthat both o(c) and t(c) can be performed in constant time (for every o, t and c), whichis reasonable in most cases, the worst-case time complexity of a deterministic parserbased on a transition system S is given by an upper bound on the length of transitionsequences in S. And the space complexity is given by an upper bound on the size ofa configuration c ?
C, because only one configuration needs to be stored at any giventime in a deterministic parser.4.
Stack-Based AlgorithmsThe stack-based algorithms make use of a stack to store partially processed tokens, thatis, tokens that have been removed from the input buffer but which are still consideredas potential candidates for dependency links, either as heads or as dependents.
A parserconfiguration is therefore defined as a triple, consisting of a stack, an input buffer, anda set of dependency arcs.Definition 9A stack-based configuration for a sentence x = (w0,w1, .
.
.
,wn) is a triple c = (?,?,A),where1.
?
is a stack of tokens i ?
k (for some k ?
n),2. ?
is a buffer of tokens j > k,3.
A is a set of dependency arcs such that G = ({0, 1, .
.
.
,n},A) is adependency graph for x.Both the stack and the buffer will be represented as lists, although the stack will have itshead (or top) to the right for reasons of perspicuity.
Thus, ?|i represents a stack with topi and tail ?, and j|?
represents a buffer with head j and tail ?.2 We use square bracketsfor enumerated lists, for example, [1, 2, .
.
.
,n], with [ ] for the empty list as a special case.Definition 10A stack-based transition system is a quadruple S = (C,T, cs,Ct), where1.
C is the set of all stack-based configurations,2.
cs(x = (w0,w1, .
.
.wn)) = ([0], [1, .
.
.
,n], ?
),2 The operator | is taken to be left-associative for the stack and right-associative for the buffer.520Nivre Deterministic Incremental Dependency ParsingTransitionsLEFT-ARCl (?|i, j|?,A) ?
(?, j|?,A?
{(j, l, i)})RIGHT-ARCsl (?|i, j|?,A) ?
(?, i|?,A?
{(i, l, j)})SHIFT (?, i|?,A) ?
(?|i,?,A)PreconditionsLEFT-ARCl ?
[i = 0]??k?l?
[(k, l?, i) ?
A]RIGHT-ARCsl ??k?l?
[(k, l?, j) ?
A]Figure 3Transitions for the arc-standard, stack-based parsing algorithm.3.
T is a set of transitions, each of which is a function t : C?
C,4.
Ct = {c ?
C|c = (?, [ ],A)}.A stack-based parse of a sentence x = (w0,w1, .
.
.
,wn) starts with the artificial root node0 on the stack ?, all the nodes corresponding to real words in the buffer ?, and anempty set A of dependency arcs; it ends as soon as the buffer ?
is empty.
The transitionsused by stack-based parsers are essentially composed of two types of actions: adding(labeled) arcs to A and manipulating the stack ?
and input buffer ?.
By combining suchactions in different ways, we can construct transition systems that implement differentparsing strategies.
We will now define two such systems, which we call arc-standardand arc-eager, respectively, adopting the terminology of Abney and Johnson (1991).4.1 Arc-Standard ParsingThe transition set T for the arc-standard, stack-based parser is defined in Figure 3 andcontains three types of transitions:1.
Transitions LEFT-ARCl (for any dependency label l) add a dependency arc(j, l, i) to A, where i is the node on top of the stack ?
and j is the first node inthe buffer ?.
In addition, they pop the stack ?.
They have as a preconditionthat the token i is not the artificial root node 0 and does not already havea head.2.
Transitions RIGHT-ARCsl (for any dependency label l) add a dependencyarc (i, l, j) to A, where i is the node on top of the stack ?
and j is the firstnode in the buffer ?.
In addition, they pop the stack ?
and replace j by iat the head of ?.
They have as a precondition that the token j does notalready have a head.33.
The transition SHIFT removes the first node i in the buffer ?
and pushesit on top of the stack ?.3 The superscript s is used to distinguish these transitions from the non-equivalent RIGHT-ARCel transitionsin the arc-eager system.521Computational Linguistics Volume 34, Number 4Transition Configuration( [0], [1, .
.
.
, 9], ?
)SHIFT =?
( [0, 1], [2, .
.
.
, 9], ?
)LEFT-ARCNMOD =?
( [0], [2, .
.
.
, 9], A1 = {(2, NMOD, 1)} )SHIFT =?
( [0, 2], [3, .
.
.
, 9], A1 )LEFT-ARCSBJ =?
( [0], [3, .
.
.
, 9], A2 = A1?
{(3, SBJ, 2)} )SHIFT =?
( [0, 3], [4, .
.
.
, 9], A2 )SHIFT =?
( [0, 3, 4], [5, .
.
.
, 9], A2 )LEFT-ARCNMOD =?
( [0, 3], [5, .
.
.
, 9], A3 = A2?
{(5, NMOD, 4)} )SHIFT =?
( [0, 3, 5], [6, .
.
.
, 9], A3 )SHIFT =?
( [0, .
.
.
6], [7, 8, 9], A3 )SHIFT =?
( [0, .
.
.
, 7], [8, 9], A3 )LEFT-ARCNMOD =?
( [0, .
.
.
6], [8, 9], A4 = A3?
{(8, NMOD, 7)} )RIGHT-ARCsPMOD =?
( [0, 3, 5], [6, 9], A5 = A4?
{(6, PMOD, 8)} )RIGHT-ARCsNMOD =?
( [0, 3], [5, 9], A6 = A5?
{(5, NMOD, 6)} )RIGHT-ARCsOBJ =?
( [0], [3, 9], A7 = A6?
{(3, OBJ, 5)} )SHIFT =?
( [0, 3], [9], A7 )RIGHT-ARCsP =?
( [0], [3], A8 = A7?
{(3, P, 9)} )RIGHT-ARCsROOT =?
( [ ], [0], A9 = A8?
{(0, ROOT, 3)} )SHIFT =?
( [0], [ ], A9 )Figure 4Arc-standard transition sequence for the English sentence in Figure 2.The arc-standard parser is the closest correspondent to the familiar shift-reduce parserfor context-free grammars (Aho, Sethi, and Ullman 1986).
The LEFT-ARCl and RIGHT-ARCsl transitions correspond to reduce actions, replacing a head-dependent structurewith its head, whereas the SHIFT transition is exactly the same as the shift action.
Onepeculiarity of the transitions, as defined here, is that the ?reduce?
transitions apply toone node on the stack and one node in the buffer, rather than two nodes on the stack.The reason for this formulation is to facilitate comparison with the arc-eager parserdescribed in the next section and to simplify the definition of terminal configurations.By way of example, Figure 4 shows the transition sequence needed to parse the Englishsentence in Figure 2.Theorem 1The arc-standard, stack-based algorithm is correct for the class of projective dependencyforests.Proof 1To show the soundness of the algorithm, we show that the dependency graph definedby the initial configuration, Gcs(x) = (Vx, ?
), is a projective dependency forest, and thatevery transition preserves this property.
We consider each of the relevant conditions inturn, keeping in mind that the only transitions that modify the graph are LEFT-ARCland RIGHT-ARCsl .1.
ROOT: The node 0 is a root in Gcs(x), and adding an arc of the form (i, l, 0) isprevented by an explicit precondition of LEFT-ARCl.522Nivre Deterministic Incremental Dependency Parsing2.
SINGLE-HEAD: Every node i ?
Vx has in-degree 0 in Gcs(x), and bothLEFT-ARCl and RIGHT-ARCsl have as a precondition that the dependentof the new arc has in-degree 0.3.
ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycleonly if there is a directed path from j to i.
But this would require that aprevious transition had added an arc of the form (k, l?, i) (for some k, l?
),in which case i would no longer be in ?
or ?.4.
PROJECTIVITY: Gcs(x) is projective, and adding an arc (i, l, j) will make thegraph non-projective only if there is a node k such that i < k < j or j < k < iand neither i??
k nor j??
k. Let C0,m be a configuration sequence forx = (w0,w1, .
.
.
,wn) and let ?
(p, i, j) (for 0 < p < m, 0 ?
i < j ?
n) be theclaim that, for every k such that i < k < j, i??
k or j??
k in Gcp .
To provethat no arc can be non-projective, we need to prove that, if cp ?
C0,m andcp = (?|i, j|?,Acp ), then ?
(p, i, j).
(If cp = (?|i, j|?,Acp ) and ?
(p, i, j),then ?
(p?, i, j) for all p?
such that p < p?, since in cp every node k such thati < k < jmust already have a head.)
We prove this by induction over thenumber ?
(p) of transitions leading to cp from the first configurationcp??
(p) ?
C0,m such that cp??
(p) = (?|i,?,Acp??
(p) ) (i.e., the firstconfiguration where i is on the top of the stack).Basis: If ?
(p) = 0, then i and j are adjacent and ?
(p, i, j) holdsvacuously.Inductive step: Assume that ?
(p, i, j) holds if ?
(p) ?
q (for someq > 0) and that ?
(p) = q+ 1.
Now consider the transition tp thatresults in configuration cp.
There are three cases:Case 1: If tp = RIGHT-ARCsl (for some l), then there is a node ksuch that j < k, ( j, l, k) ?
Acp , and cp?1 = (?|i|j, k|?,Acp?
{( j, l, k)}).
This entails that there is an earlier configurationcp?r (2 ?
r ?
?
(p)) such that cp?r = (?|i, j|?,Acp?r ).
Because?(p?
r) = ?(p)?
r ?
q, we can use the inductive hypothesisto infer ?(p?
r, i, j) and hence ?
(p, i, j).Case 2: If tp = LEFT-ARCl (for some l), then there is a node ksuch that i < k < j, ( j, l, k) ?
Acp , and cp?1 = (?|i|k, j|?,Acp?
{( j, l, k)}).
Because ?(p?
1) ?
q, we can use the inductivehypothesis to infer ?(p?
1, k, j) and, from this, ?
(p, k, j).Moreover, because there has to be an earlier configurationcp?r (r < ?
(p)) such that cp?r = (?|i, k|?,Acp?r ) and?(p?
r) ?
q, we can use the inductive hypothesis againto infer ?(p?
r, i, k) and ?
(p, i, k).
?
(p, i, k), ?
(p, k, j) and( j, l, k) ?
Acp together entail ?
(p, i, j).Case 3: If the transition tp = SHIFT, then it must have beenpreceded by a RIGHT-ARCsl transition (for some l), becauseotherwise i and jwould be adjacent.
This means that thereis a node k such that i < k < j, (i, l, k) ?
Acp , and cp?2 =(?|i, k|j|?,Acp ?
{(i, l, k)}).
Because ?(p?
2) ?
q, we canagain use the inductive hypothesis to infer ?(p?
2, i, k)and ?
(p, i, k).
Furthermore, it must be the case that either kand j are adjacent or there is an earlier configuration cp?r523Computational Linguistics Volume 34, Number 4(r < ?
(p)) such that cp?r = (?|k, j|?,Acp?r ); in both cases itfollows that ?
(p, k, j) (in the latter through the inductivehypothesis via ?(p?
r, k, j)).
As before, ?
(p, i, k), ?
(p, k, j)and (i, l, k) ?
Acp together entail ?
(p, i, j).For completeness, we need to show that for any sentence x and projective dependencyforest Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx.
Weprove this by induction on the length |x| of x = (w0,w1, .
.
.
,wn).Basis: If |x| = 1, then the only projective dependency forest for x isG = ({0}, ?)
and Gcm = Gx for C0,m = (cs(x)).Inductive step: Assume that the claim holds if |x| ?
p (for some p > 1) andassume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, .
.
.
, p}).
Considerthe subgraph Gx?
= (Vx ?
{p},A?p), where A?p = Ax ?
{(i, l, j)|i = p ?
j =p}, that is, the graph Gx?
is exactly like Gx except that the node p and allthe arcs going into or out of this node are missing.
It is obvious that,if Gx is a projective dependency forest for the sentence x = (w0,w1, .
.
.
,wp),then Gx?
is a projective dependency forest for the sentence x?
= (w0,w1, .
.
.
,wp?1), and that, because |x?| = p, there is a transition sequence C0,q suchthat Gcq = Gx?
(in virtue of the inductive hypothesis).
The terminalconfiguration of G0,q must have the form cq = (?cq , [ ],A?p), where i ?
?cqif and only if i is a root in Gx?
(else iwould have been removed in aLEFT-ARCl or RIGHT-ARCsl transition).
It follows that, in Gx, i is either aroot or a dependent of p. In the latter case, any j such that j ?
?cq and i < jmust also be a dependent of p (else Gx would not be projective, given thati and j are both roots in Gx?
).
Moreover, if p has a head k in Gx, then kmust be the topmost node in ?cq that is not a dependent of p (anything elsewould again be inconsistent with the assumption that Gx is projective).Therefore, we can construct a transition sequence C0,m such that Gcm =Gx, by starting in c0 = cs(x) and applying exactly the same q transitionsas in C0,q, followed by as many LEFT-ARCl transitions as there are leftdependents of p in Gx, followed by a RIGHT-ARCsl transition if and onlyif p has a head in Gx, followed by a SHIFT transition (moving the head ofp back to the stack and emptying the buffer).
Theorem 2The worst-case time complexity of the arc-standard, stack-based algorithm is O(n),where n is the length of the input sentence.Proof 2Assuming that the oracle and transition functions can be computed in some constanttime, the worst-case running time is bounded by the maximum number of transitionsin a transition sequence C0,m for a sentence x = (w0,w1, .
.
.
,wn).
Since a SHIFT transitiondecreases the length of the buffer ?
by 1, no other transition increases the length of ?,and any configuration where ?
= [ ] is terminal, the number of SHIFT transitions in C0,mis bounded by n. Moreover, since both LEFT-ARCl and RIGHT-ARCsl decrease the heightof the stack by 1, only SHIFT increases the height of the stack by 1, and the initial heightof the stack is 1, the combined number of instances of LEFT-ARCl and RIGHT-ARCsl inC0,m is also bounded by n. Hence, the worst case time complexity is O(n).
524Nivre Deterministic Incremental Dependency ParsingRemark 1The assumption that the oracle function can be computed in constant time will be dis-cussed at the end of Section 6.1, where we approximate oracles with treebank-inducedclassifiers in order to experimentally evaluate the different algorithms.
The assumptionthat every transition can be performed in constant time can be justified by noting thatthe only operations involved are those of adding an arc to the graph, removing the firstelement from the buffer, and pushing or popping the stack.Theorem 3The worst-case space complexity of the arc-standard, stack-based algorithm is O(n),where n is the length of the input sentence.Proof 3Given the deterministic parsing algorithm, only one configuration c = (?,?,A) needs tobe stored at any given time.
Assuming that a single node can be stored in some constantspace, the space needed to store ?
and ?, respectively, is bounded by the number ofnodes.
The same holds for A, given that a single arc can be stored in constant space,because the number of arcs in a dependency forest is bounded by the number of nodes.Hence, the worst-case space complexity is O(n).
4.2 Arc-Eager ParsingThe transition set T for the arc-eager, stack-based parser is defined in Figure 5 andcontains four types of transitions:1.
Transitions LEFT-ARCl (for any dependency label l) add a dependency arc( j, l, i) to A, where i is the node on top of the stack ?
and j is the first node inthe buffer ?.
In addition, they pop the stack ?.
They have as a preconditionthat the token i is not the artificial root node 0 and does not already havea head.TransitionsLEFT-ARCl (?|i, j|?,A) ?
(?, j|?,A?
{( j, l, i)})RIGHT-ARCel (?|i, j|?,A) ?
(?|i|j,?,A?
{(i, l, j)})REDUCE (?|i,?,A) ?
(?,?,A)SHIFT (?, i|?,A) ?
(?|i,?,A)PreconditionsLEFT-ARCl ?
[i = 0]??k?l?
[(k, l?, i) ?
A]RIGHT-ARCel ??k?l?
[(k, l?, j) ?
A]REDUCE ?k?l[(k, l, i) ?
A]Figure 5Transitions for the arc-eager, stack-based parsing algorithm.525Computational Linguistics Volume 34, Number 42.
Transitions RIGHT-ARCel (for any dependency label l) add a dependencyarc (i, l, j) to A, where i is the node on top of the stack ?
and j is the firstnode in the buffer ?.
In addition, they remove the first node j in the buffer?
and push it on top of the stack ?.
They have as a precondition that thetoken j does not already have a head.3.
The transition REDUCE pops the stack ?
and is subject to the preconditionthat the top token has a head.4.
The transition SHIFT removes the first node i in the buffer ?
and pushes iton top of the stack ?.The arc-eager parser differs from the arc-standard one by attaching right dependents(using RIGHT-ARCel transitions) as soon as possible, that is, before the right dependenthas found all its right dependents.
As a consequence, the RIGHT-ARCel transitionscannot replace the head-dependent structure with the head, as in the arc-standardsystem, but must store both the head and the dependent on the stack for furtherprocessing.
The dependent can be popped from the stack at a later time through theREDUCE transition, which completes the reduction of this structure.
The arc-eagersystem is illustrated in Figure 6, which shows the transition sequence needed to parsethe English sentence in Figure 2 with the same output as the arc-standard sequence inFigure 4.Theorem 4The arc-eager, stack-based algorithm is correct for the class of projective dependencyforests.Transition Configuration( [0], [1, .
.
.
, 9], ?
)SHIFT =?
( [0, 1], [2, .
.
.
, 9], ?
)LEFT-ARCNMOD =?
( [0], [2, .
.
.
, 9], A1 = {(2, NMOD, 1)} )SHIFT =?
( [0, 2], [3, .
.
.
, 9], A1 )LEFT-ARCSBJ =?
( [0], [3, .
.
.
, 9], A2 = A1?
{(3, SBJ, 2)} )RIGHT-ARCeROOT =?
( [0, 3], [4, .
.
.
, 9], A3 = A2?
{(0, ROOT, 3)} )SHIFT =?
( [0, 3, 4], [5, .
.
.
, 9], A3 )LEFT-ARCNMOD =?
( [0, 3], [5, .
.
.
, 9], A4 = A3?
{(5, NMOD, 4)} )RIGHT-ARCeOBJ =?
( [0, 3, 5], [6, .
.
.
, 9], A5 = A4?
{(3, OBJ, 5)} )RIGHT-ARCeNMOD =?
( [0, .
.
.
, 6], [7, 8, 9], A6 = A5?
{(5, NMOD, 6)} )SHIFT =?
( [0, .
.
.
, 7], [8, 9], A6 )LEFT-ARCNMOD =?
( [0, .
.
.
6], [8, 9], A7 = A6?
{(8, NMOD, 7)} )RIGHT-ARCePMOD =?
( [0, .
.
.
, 8], [9], A8 = A7?
{(6, PMOD, 8)} )REDUCE =?
( [0, .
.
.
, 6], [9], A8 )REDUCE =?
( [0, 3, 5], [9], A8 )REDUCE =?
( [0, 3], [9], A8 )RIGHT-ARCeP =?
( [0, 3, 9], [ ], A9 = A8?
{(3, P, 9)} )Figure 6Arc-eager transition sequence for the English sentence in Figure 2.526Nivre Deterministic Incremental Dependency ParsingProof 4To show the soundness of the algorithm, we show that the dependency graph definedby the initial configuration, Gc0(x) = (Vx, ?
), is a projective dependency forest, and thatevery transition preserves this property.
We consider each of the relevant conditions inturn, keeping in mind that the only transitions that modify the graph are LEFT-ARCland RIGHT-ARCel .1.
ROOT: Same as Proof 1.2.
SINGLE-HEAD: Same as Proof 1 (substitute RIGHT-ARCel for RIGHT-ARCsl ).3.
ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycleonly if there is a directed path from j to i.
In the case of LEFT-ARCl, thiswould require the existence of a configuration cp = (?|j, i|?,Acp ) suchthat (k, l?, i) ?
Acp (for some k and l?
), which is impossible because anytransition adding an arc (k, l?, i) has as a consequence that i is no longer inthe buffer.
In the case of RIGHT-ARCel , this would require a configurationcp = (?|i, j|?,Acp ) such that, given the arcs in Acp , there is a directed pathfrom j to i.
Such a path would have to involve at least one arc (k, l?, k?)
suchthat k?
?
i < k, which would entail that i is no longer in ?.
(If k?
= i, then iwould be popped in the LEFT-ARCl?
transition adding the arc; if k?
< i,then iwould have to be popped before the arc could be added.)4.
PROJECTIVITY: To prove that, if cp ?
C0,m and cp = (?|i, j|?,Acp ), then?
(p, i, j), we use essentially the same technique as in Proof 1, only withdifferent cases in the inductive step because of the different transitions.As before, we let ?
(p) be the number of transitions that it takes to reachcp from the first configuration that has i on top of the stack.Basis: If ?
(p) = 0, then i and j are adjacent, which entails ?
(p, i, j).Inductive step: We assume that ?
(p, i, j) holds if ?
(p) ?
q (for someq > 0) and that ?
(p) = q+ 1, and we concentrate on the transitiontp that results in configuration cp.
For the arc-eager algorithm, thereare only two cases to consider, because if tp = RIGHT-ARCel (forsome l) or tp = SHIFT then ?
(p) = 0, which contradicts ourassumption that ?
(p) > q > 0.
(This follows because the arc-eageralgorithm, unlike its arc-standard counterpart, does not allownodes to be moved back from the stack to the buffer.
)Case 1: If tp = LEFT-ARCl (for some l), then there is a node ksuch that i < k < j, ( j, l, k) ?
Acp , and cp?1 = (?|i|k, j|?,Acp?
{( j, l, k)}).
Because ?(p?
1) ?
q, we can use the inductivehypothesis to infer ?(p?
1, k, j) and, from this, ?
(p, k, j).Moreover, because there has to be an earlier configurationcp?r (r < ?
(p)) such that cp?r = (?|i, k|?,Acp?r ) and?(p?
r) ?
q, we can use the inductive hypothesis againto infer ?(p?
r, i, k) and ?
(p, i, k).
?
(p, i, k), ?
(p, k, j) and( j, l, k) ?
Acp together entail ?
(p, i, j).Case 2: If the transition tp = REDUCE, then there is a node ksuch that i < k < j, (i, l, k) ?
Acp , and cp?1 = (?|i|k, j|?,Acp ).Because ?(p?
1) ?
q, we can again use the inductivehypothesis to infer ?(p?
1, k, j) and ?
(p, k, j).
Moreover,527Computational Linguistics Volume 34, Number 4there must be an earlier configuration cp?r (r < ?
(p)) suchthat cp?r = (?|i, k|?,Acp?r ) and ?(p?
r) ?
q, which entails?(p?
r, i, k) and ?
(p, i, k).
As before, ?
(p, i, k), ?
(p, k, j) and(i, l, k) ?
Acp together entail ?
(p, i, j).For completeness, we need to show that for any sentence x and projective depen-dency forest Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx.Using the same idea as in Proof 1, we prove this by induction on the length |x| ofx = (w0,w1, .
.
.
,wn).Basis: If |x| = 1, then the only projective dependency forest for x isG = ({0}, ?)
and Gcm = Gx for C0,m = (cs(x)).Inductive step: Assume that the claim holds if |x| ?
p (for some p > 1)and assume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, .
.
.
, p}).
As inProof 1, we may now assume that there exists a transition sequence C0,qfor the sentence x?
= (w0,w1,wp?1) and subgraph Gx?
= (Vx ?
{p},A?p),where the terminal configuration has the form cq = (?cq , [ ],A?p).
For thearc-eager algorithm, if i is a root in Gx?
then i ?
?cq ; but if i ?
?cq then i iseither a root or has a head j such that j < i in Gx?
.
(This is because imayhave been pushed onto the stack in a RIGHT-ARCel transition and mayor may not have been popped in a later REDUCE transition.)
Apart from thepossibility of unreduced right dependents, we can use the same reasoningas in Proof 1 to show that, for any i ?
?cq that is a root in Gx?
, if i is adependent of p in Gx then any j such that j ?
?cq , i < j and j is a root inGx?
must also be a dependent of p in Gx (or else Gx would fail to beprojective).
Moreover, if p has a head k in Gx, then kmust be in ?cq andany j such that j ?
?cq and k < jmust either be a dependent of p in Gx ormust have a head to the left in both Gx?
and Gx (anything else would againbe inconsistent with the assumption that Gx is projective).
Therefore, wecan construct a transition sequence C0,m such that Gcm = Gx, by startingin c0 = cs(x) and applying exactly the same q transitions as in C0,q,followed by as many LEFT-ARCl transitions as there are left dependents ofp in Gx, interleaving REDUCE transitions whenever the node on top of thestack already has a head, followed by a RIGHT-ARCel transition if p has ahead in Gx and a SHIFT transition otherwise (in both cases moving p to thestack and emptying the buffer).
Theorem 5The worst-case time complexity of the arc-eager, stack-based algorithm is O(n), wheren is the length of the input sentence.Proof 5The proof is essentially the same as Proof 2, except that both SHIFT and RIGHT-ARCeldecrease the length of ?
and increase the height of ?, while both REDUCE and LEFT-ARCl decrease the height of ?.
Hence, the combined number of SHIFT and RIGHT-ARCeltransitions, as well as the combined number of REDUCE and LEFT-ARCl transitions, arebounded by n. 528Nivre Deterministic Incremental Dependency ParsingTheorem 6The worst-case space complexity of the arc-eager, stack-based algorithm is O(n), wheren is the length of the input sentence.Proof 6Same as Proof 3.
5.
List-Based AlgorithmsThe list-based algorithms make use of two lists to store partially processed tokens, thatis, tokens that have been removed from the input buffer but which are still consideredas potential candidates for dependency links, either as heads or as dependents.
A parserconfiguration is therefore defined as a quadruple, consisting of two lists, an input buffer,and a set of dependency arcs.Definition 11A list-based configuration for a sentence x = (w0,w1, .
.
.
,wn) is a quadruple c =(?1, ?2,?,A), where1.
?1 is a list of tokens i1 ?
k1 (for some k1 ?
n),2.
?2 is a list of tokens i2 ?
k2 (for some k2, k1 < k2 ?
n),3. ?
is a buffer of tokens j > k,4.
A is a set of dependency arcs such that G = ({0, 1, .
.
.
,n},A) is adependency graph for x.The list ?1 has its head to the right and stores nodes in descending order, and the list ?2has its head to the left and stores nodes in ascending order.
Thus, ?1|i represents a listwith head i and tail ?1, whereas j|?2 represents a list with head j and tail ?2.4 We usesquare brackets for enumerated lists as before, and we write ?1.
?2 for the concatenationof ?1 and ?2, so that, for example, [0, 1].
[2, 3, 4] = [0, 1, 2, 3, 4].
The notational conven-tions for the buffer ?
and the set A of dependency arcs are the same as before.Definition 12A list-based transition system is a quadruple S = (C,T, cs,Ct), where1.
C is the set of all list-based configurations,2.
cs(x = (w0,w1, .
.
.wn)) = ([0], [ ], [1, .
.
.
,n], ?),3.
T is a set of transitions, each of which is a function t : C?
C,4.
Ct = {c ?
C|c = (?1, ?2, [ ],A)}.A list-based parse of a sentence x = (w0,w1, .
.
.
,wn) starts with the artificial root node 0as the sole element of ?1, an empty list ?2, all the nodes corresponding to real words inthe buffer ?, and an empty set A of dependency arcs; it ends as soon as the buffer ?
isempty.
Thus, the only difference compared to the stack-based systems is that we havetwo lists instead of a single stack.
Otherwise, both initialization and termination are4 The operator | is taken to be left-associative for ?1 and right-associative for ?2.529Computational Linguistics Volume 34, Number 4TransitionsLEFT-ARCnl (?1|i, ?2, j|?,A) ?
(?1, i|?2, j|?,A?
{( j, l, i)})RIGHT-ARCnl (?1|i, ?2, j|?,A) ?
(?1, i|?2, j|?,A?
{(i, l, j)})NO-ARCn (?1|i, ?2,?,A) ?
(?1, i|?2,?,A)SHIFT?
(?1, ?2, i|?,A) ?
(?1.
?2|i, [ ],?,A)PreconditionsLEFT-ARCnl ?
[i = 0]??k?l?
[(k, l?, i) ?
A]?[i??
j]ARIGHT-ARCnl ??k?l?
[(k, l?, j) ?
A]?[j??
i]AFigure 7Transitions for the non-projective, list-based parsing algorithm.essentially the same.
The transitions used by list-based parsers are again composed oftwo types of actions: adding (labeled) arcs toA and manipulating the lists ?1 and ?2, andthe input buffer ?.
By combining such actions in different ways, we can construct transi-tion systems with different properties.
We will now define two such systems, which wecall non-projective and projective, respectively, after the classes of dependency graphsthat they can handle.A clarification may be in order concerning the use of lists instead of stacks for thisfamily of algorithms.
In fact, most of the transitions to be defined subsequently makeno essential use of this added flexibility and could equally well have been formalizedusing two stacks instead.
However, we will sometimes need to append two lists intoone, and this would not be a constant-time operation using standard stack operations.We therefore prefer to define these structures as lists, even though they will mostly beused as stacks.5.1 Non-Projective ParsingThe transition set T for the non-projective, list-based parser is defined in Figure 7 andcontains four types of transitions:1.
Transitions LEFT-ARCnl (for any dependency label l) add a dependency arc( j, l, i) to A, where i is the head of the list ?1 and j is the first node in thebuffer ?.
In addition, they move i from the list ?1 to the list ?2.
They haveas a precondition that the token i is not the artificial root node and doesnot already have a head.
In addition, there must not be a path from i to j inthe graph G = ({0, 1, .
.
.
,n},A).55 We use the notation [i ???
j]A to signify that there is a path connecting i and j in G = ({0, 1, .
.
.
,n},A).530Nivre Deterministic Incremental Dependency Parsing2.
Transitions RIGHT-ARCnl (for any dependency label l) add a dependencyarc (i, l, j) to A, where i is the head of the list ?1 and j is the first node in thebuffer ?.
In addition, they move i from the list ?1 to the list ?2.
They haveas a precondition that the token j does not already have a head and thatthere is no path from j to i in G = ({0, 1, .
.
.
,n},A).3.
The transition NO-ARC removes the head i of the list ?1 and inserts it atthe head of the list ?2.4.
The transition SHIFT removes the first node i in the buffer ?
and inserts itat the head of a list obtained by concatenating ?1 and ?2.
This list becomesthe new ?1, whereas ?2 is empty in the resulting configuration.The non-projective, list-based parser essentially builds a dependency graph by consid-ering every pair of nodes (i, j) (i < j) and deciding whether to add a dependency arcbetween them (in either direction), although the SHIFT transition allows it to skip certainpairs.
More precisely, if i is the head of ?1 and j is the first node in the buffer ?
whena SHIFT transition is performed, then all pairs (k, j) such that k < i are ignored.
The factthat both the head and the dependent are kept in either ?2 or ?
makes it possible toconstruct non-projective dependency graphs, because the NO-ARCn transition allowsa node to be passed from ?1 to ?2 even if it does not (yet) have a head.
However, anarc can only be added between two nodes i and j if the dependent end of the arc isnot the artificial root 0 and does not already have a head, which would violate ROOTand SINGLE-HEAD, respectively, and if there is no path connecting the dependent to thehead, which would cause a violation of ACYCLICITY.
As an illustration, Figure 8 showsthe transition sequence needed to parse the Czech sentence in Figure 1, which has anon-projective dependency graph.Theorem 7The non-projective, list-based algorithm is correct for the class of dependency forests.Proof 7To show the soundness of the algorithm, we simply observe that the dependency graphdefined by the initial configuration, Gc0(x) = ({0, 1, .
.
.
,n}, ?
), satisfies ROOT, SINGLE-HEAD, and ACYCLICITY, and that none of the four transitions may lead to a violationof these constraints.
(The transitions SHIFT?
and NO-ARCn do not modify the graph atall, and LEFT-ARCnl and RIGHT-ARCnl have explicit preconditions to prevent this.
)For completeness, we need to show that for any sentence x and dependency for-est Gx = (Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx.
Usingthe same idea as in Proof 1, we prove this by induction on the length |x| of x =(w0,w1, .
.
.
,wn).Basis: If |x| = 1, then the only dependency forest for x is G = ({0}, ?)
andGcm = Gx for C0,m = (cs(x)).Inductive step: Assume that the claim holds if |x| ?
p (for some p > 1)and assume that |x| = p+ 1 and Gx = (Vx,Ax) (Vx = {0, 1, .
.
.
, p}).
As inProof 1, we may now assume that there exists a transition sequence C0,qfor the sentence x?
= (w0,w1,wp?1) and subgraph Gx?
= (Vx ?
{p},A?p),but the terminal configuration now has the form cq = (?cq , [ ], [ ],A?p),where ?cq = [0, 1, .
.
.
, p?
1].
In order to construct a transition sequenceC0,m such that Gcm = Gx we instead start from the configuration531Computational Linguistics Volume 34, Number 4Transition Configuration( [0], [ ], [1, .
.
.
, 8], ?
)SHIFT?
=?
( [0, 1], [ ], [2, .
.
.
, 8], ?
)RIGHT-ARCnAtr =?
( [0], [1], [2, .
.
.
, 8], A1 = {(1,Atr, 2)} )SHIFT?
=?
( [0, 1, 2], [ ], [3, .
.
.
, 8], A1 )NO-ARCn =?
( [0, 1], [2], [3, .
.
.
, 8], A1 )NO-ARCn =?
( [0], [1, 2], [3, .
.
.
, 8], A1 )RIGHT-ARCnPred =?
( [ ], [0, 1, 2], [3, .
.
.
, 8], A2 = A1?
{(0, Pred, 3)} )SHIFT?
=?
( [0, .
.
.
, 3], [ ], [4, .
.
.
, 8], A2 )SHIFT?
=?
( [0, .
.
.
, 4], [ ], [5, .
.
.
, 8], A2 )LEFT-ARCnAuxZ =?
( [0, .
.
.
, 3], [4], [5, .
.
.
, 8], A3 = A2?
{(5,AuxZ, 4)} )RIGHT-ARCnSb =?
( [0, 1, 2], [3, 4], [5, .
.
.
, 8], A4 = A3?
{(3, Sb, 5)} )NO-ARCn =?
( [0, 1], [2, 3, 4], [5, .
.
.
, 8], A4 )LEFT-ARCnAuxP =?
( [0], [1, .
.
.
, 4], [5, .
.
.
, 8], A5 = A4?
{(5,AuxP, 1)} )SHIFT?
=?
( [0, .
.
.
, 5], [ ], [6, 7, 8], A5 )NO-ARCn =?
( [0, .
.
.
, 4], [5], [6, 7, 8], A5 )NO-ARCn =?
( [0, .
.
.
, 3], [4, 5], [6, 7, 8], A5 )RIGHT-ARCnAuxP =?
( [0, 1, 2], [3, 4, 5], [6, 7, 8], A6 = A5?
{(3,AuxP, 6)} )SHIFT?
=?
( [0, .
.
.
, 6], [ ], [7, 8], A6 )RIGHT-ARCnAdv =?
( [0, .
.
.
, 5], [6], [7, 8], A7 = A6?
{(6,Adv, 7)} )SHIFT?
=?
( [0, .
.
.
, 7], [ ], [8], A7 )NO-ARCn =?
( [0, .
.
.
, 6], [7], [8], A7 )NO-ARCn =?
( [0, .
.
.
, 5], [6, 7], [8], A7 )NO-ARCn =?
( [0, .
.
.
, 4], [5, 6, 7], [8], A7 )NO-ARCn =?
( [0, .
.
.
, 3], [4, .
.
.
, 7], [8], A7 )NO-ARCn =?
( [0, 1, 2], [3, .
.
.
, 7], [8], A7 )NO-ARCn =?
( [0, 1], [2, .
.
.
, 7], [8], A7 )NO-ARCn =?
( [0], [1, .
.
.
, 7], [8], A7 )RIGHT-ARCnAuxK =?
( [ ], [0, .
.
.
, 7], [8], A8 = A7?
{(0,AuxK, 8)} )SHIFT?
=?
( [0, .
.
.
, 8], [ ], [ ], A8 )Figure 8Non-projective transition sequence for the Czech sentence in Figure 1.c0 = cs(x) and apply exactly the same q transitions, reaching theconfiguration cq = (?cq , [ ], [p],A?p).
We then perform exactly p transitions,in each case choosing LEFT-ARCnl if the token i at the head of ?1 is adependent of p in Gx (with label l), RIGHT-ARCnl?
if i is the head of p (withlabel l?)
and NO-ARCn otherwise.
One final SHIFT?
transition takes us tothe terminal configuration cm = (?cq |p, [ ], [ ],Ax).
Theorem 8The worst-case time complexity of the non-projective, list-based algorithm is O(n2),where n is the length of the input sentence.Proof 8Assuming that the oracle and transition functions can be performed in some constanttime, the worst-case running time is bounded by the maximum number of transitions532Nivre Deterministic Incremental Dependency Parsingin a transition sequence C0,m for a sentence x = (w0,w1, .
.
.
,wn).
As for the stack-basedalgorithms, there can be at most n SHIFT?
transitions in C0,m.
Moreover, because each ofthe three other transitions presupposes that ?1 is non-empty and decreases its length by1, there can be at most i such transitions between the i?1th and the ith SHIFT transition.It follows that the total number of transitions in C0,m is bounded by?ni=1 i+1, which isO(n2).
Remark 2The assumption that transitions can be performed in constant time can be justified bythe same kind of considerations as for the stack-based algorithms (cf.
Remark 1).
Theonly complication is the SHIFT?
transition, which involves appending the two lists ?1and ?2, but this can be handled with an appropriate choice of data structures.
A moreserious complication is the need to check the preconditions of LEFT-ARCnl and RIGHT-ARCnl , but if we assume that it is the responsibility of the oracle to ensure that thepreconditions of any predicted transition are satisfied, we can postpone the discussionof this problem until the end of Section 6.1.Theorem 9The worst-case space complexity of the non-projective, list-based algorithm is O(n),where n is the length of the input sentence.Proof 9Given the deterministic parsing algorithm, only one configuration c = (?1, ?2,?,A)needs to be stored at any given time.
Assuming that a single node can be stored insome constant space, the space needed to store ?1, ?2, and ?, respectively, is boundedby the number of nodes.
The same holds for A, given that a single arc can be stored inconstant space, because the number of arcs in a dependency forest is bounded by thenumber of nodes.
Hence, the worst-case space complexity is O(n).
5.2 Projective ParsingThe transition set T for the projective, list-based parser is defined in Figure 9 andcontains four types of transitions:1.
Transitions LEFT-ARCpl(for any dependency label l) add a dependency arc( j, l, i) to A, where i is the head of the list ?1 and j is the first node in thebuffer ?.
In addition, they remove i from the list ?1 and empty ?2.
Theyhave as a precondition that the token i is not the artificial root node anddoes not already have a head.2.
Transitions RIGHT-ARCpl(for any dependency label l) add a dependencyarc (i, l, j) to A, where i is the head of the list ?1 and j is the first node in thebuffer ?.
In addition, they move j from the buffer ?
and empty the list ?2.They have as a precondition that the token j does not already have a head.3.
The transition NO-ARCp removes the head i of the list ?1 and inserts it atthe head of the list ?2.
It has as a precondition that the node i already hasa head.4.
The transition SHIFT?
removes the first node i in the buffer ?
and inserts itat the head of a list obtained by concatenating ?1 and ?2.
This list becomesthe new ?1, while ?2 is empty in the resulting configuration.533Computational Linguistics Volume 34, Number 4TransitionsLEFT-ARCpl(?1|i, ?2, j|?,A) ?
(?1, [ ], j|?,A?
{( j, l, i)})RIGHT-ARCpl(?1|i, ?2, j|?,A) ?
(?1|i|j, [ ],?,A?
{(i, l, j)})NO-ARCp (?1|i, ?2,?,A) ?
(?1, i|?2,?,A)SHIFT?
(?1, ?2, i|?,A) ?
(?1.
?2|i, [ ],?,A)PreconditionsLEFT-ARCpl?
[i = 0]??k?l?
[(k, l?, i) ?
A]RIGHT-ARCpl??k?l?
[(k, l?, j) ?
A]NO-ARCp ?k?l[(k, l, i) ?
A]Figure 9Transitions for the projective, list-based parsing algorithm.The projective, list-based parser uses the same basic strategy as its non-projective coun-terpart, but skips any pair (i, j) that could give rise to a non-projective dependency arc.The essential differences are the following:1.
While LEFT-ARCnl stores the dependent i in the list ?2, allowing it to havedependents to the right of j, LEFT-ARCpldeletes it and in addition empties?2 because any dependency arc linking i, or any node between i and j, to anode succeeding j would violate PROJECTIVITY.2.
While RIGHT-ARCnl allows the dependent j to seek dependents to theleft of i, by simply moving i from ?1 to ?2, RIGHT-ARCplessentiallyincorporates a SHIFT?
transition by moving j to ?1|i, because anydependency arc linking j to a node preceding iwould violatePROJECTIVITY.
In addition, it does not move any nodes from ?2 to ?1,since these nodes can no longer be linked to any node succeeding jwithout violating PROJECTIVITY.3.
While NO-ARCn is permissible as long as ?1 is not empty, NO-ARCprequires that the node i already has a head because any dependency arcspanning a root node would violate PROJECTIVITY (regardless of whicharcs are added later).The fact that the projective algorithm skips many node pairs that are considered by thenon-projective algorithm makes it more efficient in practice, although the worst-casetime complexity remains the same.
Figure 10 shows the transition sequence neededto parse the English sentence in Figure 2 with the same output as the stack-basedsequences in Figures 4 and 6.Theorem 10The projective, list-based algorithm is correct for the class of projective dependencyforests.534Nivre Deterministic Incremental Dependency ParsingTransition Configuration( [0], [ ], [1, .
.
.
, 9], ?
)SHIFT?
=?
( [0, 1], [ ], [2, .
.
.
, 9], ?
)LEFT-ARCpNMOD =?
( [0], [ ], [2, .
.
.
, 9], A1 = {(2, NMOD, 1)} )SHIFT?
=?
( [0, 2], [ ], [3, .
.
.
, 9], A1 )LEFT-ARCpSBJ =?
( [0], [ ], [3, .
.
.
, 9], A2 = A1?
{(3, SBJ, 2)} )RIGHT-ARCpROOT =?
( [0, 3], [ ], [4, .
.
.
, 9], A3 = A2?
{(0, ROOT, 3)} )SHIFT?
=?
( [0, 3, 4], [ ], [5, .
.
.
, 9], A3 )LEFT-ARCpNMOD =?
( [0, 3], [ ], [5, .
.
.
, 9], A4 = A3?
{(5, NMOD, 4)} )RIGHT-ARCpOBJ =?
( [0, 3, 5], [ ], [6, .
.
.
, 9], A5 = A4?
{(3, OBJ, 5)} )RIGHT-ARCpNMOD =?
( [0, .
.
.
, 6], [ ], [7, 8, 9], A6 = A5?
{(5, NMOD, 6)} )SHIFT?
=?
( [0, .
.
.
, 7], [ ], [8, 9], A6 )LEFT-ARCNMOD =?
( [0, .
.
.
6], [ ], [8, 9], A7 = A6?
{(8, NMOD, 7)} )RIGHT-ARCpPMOD =?
( [0, .
.
.
, 8], [ ], [9], A8 = A7?
{(6, PMOD, 8)} )NO-ARCp =?
( [0, .
.
.
, 6], [8], [9], A8 )NO-ARCp =?
( [0, 3, 5], [6, 8], [9], A8 )NO-ARCp =?
( [0, 3], [5, 6, 8], [9], A8 )RIGHT-ARCpP =?
( [0, 3, 9], [ ], [ ], A9 = A8?
{(3, P, 9)} )Figure 10Projective transition sequence for the English sentence in Figure 2.Proof 10To show the soundness of the algorithm, we show that the dependency graph definedby the initial configuration, Gc0(x) = (V, ?
), is a projective dependency forest, and thatevery transition preserves this property.
We consider each of the relevant conditions inturn, keeping in mind that the only transitions that modify the graph are LEFT-ARCpland RIGHT-ARCpl.1.
ROOT: Same as Proof 1 (substitute LEFT-ARCplfor LEFT-ARCl).2.
SINGLE-HEAD: Same as Proof 1 (substitute LEFT-ARCpland RIGHT-ARCplfor LEFT-ARCl and RIGHT-ARCsl , respectively).3.
ACYCLICITY: Gcs(x) is acyclic, and adding an arc (i, l, j) will create a cycleonly if there is a directed path from j to i.
In the case of LEFT-ARCpl, thiswould require the existence of a configuration cp = (?1|j, ?2, i|?,Acp ) suchthat (k, l?, i) ?
Acp (for some k < i and l?
), which is impossible because anytransition adding an arc (k, l?, i) has as a consequence that i is no longer inthe buffer.
In the case of RIGHT-ARCpl, this would require a configurationcp = (?1|i, ?2, j|?,Acp ) such that, given the arcs in Acp , there is a directedpath from j to i.
Such a path would have to involve at least one arc (k, l?, k?
)such that k?
?
i < k, which would entail that i is no longer in ?1 or ?2.
(If k?
= i, then iwould be removed from ?1?and not added to ?2?in theLEFT-ARCpl?transition adding the arc; if k?
< i, then i would have to bemoved to ?2 before the arc can be added and removed as this list isemptied in the LEFT-ARCpl?transition.
)535Computational Linguistics Volume 34, Number 44.
PROJECTIVITY: Gcs(x) is projective, and adding an arc (i, l, j) will make thegraph non-projective only if there is a node k such that i < k < j or j < k < iand neither i??
k nor j??
k. Let C0,m be a configuration sequence forx = (w0,w1, .
.
.
,wn) and let ?
(p, i, j) (for 0 < p < m, 0 ?
i < j ?
n) be theclaim that, for every k such that i < k < j, i??
k or j??
k in Gcp .
To provethat no arc can be non-projective, we need to prove that, if cp ?
C0,m andcp = (?1|i, ?2, j|?,Acp ), then ?
(p, i, j).
(If cp = (?1|i, ?2, j|?,Acp ) and ?
(p, i, j),then ?
(p?, i, j) for all p?
such that p < p?, because in cp every node k suchthat i < k < jmust already have a head.)
We prove this by induction overthe number ?
(p) of transitions leading to cp from the first configurationcp??
(p) ?
C0,m such that cp??
(p) = (?1, ?2, j|?,Acp??
(p) ) (i.e., the firstconfiguration where j is the first node in the buffer).Basis: If ?
(p) = 0, then i and j are adjacent and ?
(p, i, j) holdsvacuously.Inductive step: Assume that ?
(p, i, j) holds if ?
(p) ?
q (for someq > 0) and that ?
(p) = q+ 1.
Now consider the transition tp thatresults in configuration cp.
For the projective, list-based algorithm,there are only two cases to consider, because if tp = RIGHT-ARCpl(for some l) or tp = SHIFT then ?
(p) = 0, which contradicts ourassumption that ?
(p) > q > 0.
(This follows because there is notransition that moves a node back to the buffer.
)Case 1: If tp = LEFT-ARCpl(for some l), then there is a node ksuch that i < k < j, ( j, l, k) ?
Acp , cp?1 = (?1|i|k, ?2, j|?,Acp?
{( j, l, k)}), and cp = (?1|i, [ ], j|?,Acp ).
Because ?(p?
1) ?
q,we can use the inductive hypothesis to infer ?(p?
1, k, j)and, from this, ?
(p, k, j).
Moreover, because there has tobe an earlier configuration cp?r (r < ?
(p)) such that cp?r =(?1|i, ?2?
, k|?,Acp?r ) and ?(p?
r) ?
q, we can use theinductive hypothesis again to infer ?(p?
r, i, k) and ?
(p, i, k).?
(p, i, k), ?
(p, k, j), and ( j, l, k) ?
Acp together entail ?
(p, i, j).Case 2: If the transition tp = NO-ARCp, then there is a node ksuch that i < k < j, (i, l, k) ?
Acp , cp?1 = (?1|i|k, ?2, j|?,Acp ),and cp = (?1|i, k|?2, j|?,Acp ).
Because ?(p?
1) ?
q, we canagain use the inductive hypothesis to infer ?(p?
1, k, j) and?
(p, k, j).
Moreover, there must be an earlier configurationcp?r (r < ?
(p)) such that cp?r = (?1|i, ?2?
, k|?,Acp?r ) and?(p?
r) ?
q, which entails ?(p?
r, i, k) and ?
(p, i, k).
Asbefore, ?
(p, i, k), ?
(p, k, j), and (i, l, k) ?
Acp together entail?
(p, i, j).For completeness, we need to show that for any sentence x and dependency forest Gx =(Vx,Ax) for x, there is a transition sequence C0,m such that Gcm = Gx.
The proof is byinduction on the length |x| and is essentially the same as Proof 7 up to the point wherewe assume the existence of a transition sequence C0,q for the sentence x?
= (w0,w1,wp?1)and subgraphGx?
= (Vx ?
{p},A?p), where the terminal configuration still has the formcq = (?cq , [ ], [ ],A?p), but where it can no longer be assumed that ?cq = [0, 1, .
.
.
, p?
1].If i is a root in Gx?
then i ?
?cq ; but if i ?
?cq then i is either a root or has a head j such536Nivre Deterministic Incremental Dependency Parsingthat j < i in Gx?
.
(This is because a RIGHT-ARCpltransition leaves the dependent in ?1while a LEFT-ARCplremoves it.)
Moreover, for any i ?
?cq that is a root in Gx?
, if i is adependent of p in Gx then any j such that j ?
?cq , i < j and j is a root in Gx?
must also bea dependent of p in Gx (else Gx would fail to be projective).
Finally, if p has a head k inGx, then kmust be in ?cq and any j such that j ?
?cq and k < jmust either be a dependentof p in Gx or must have a head to the left in both Gx?
and Gx (anything else would againbe inconsistent with the assumption that Gx is projective).
Therefore, we can constructa transition sequence C0,m such that Gcm = Gx, by starting in c0 = cs(x) and applyingexactly the same q transitions as in C0,q, followed by as many LEFT-ARCpltransitionsas there are left dependents of p in Gx, interleaving NO-ARCp transitions whenever thenode at the head of ?1 already has a head, followed by a RIGHT-ARCpltransition if phas a head in Gx.
One final SHIFTn transition takes us to the terminal configurationcm = (?cm , [ ], [ ],Ax).
Theorem 11The worst-case time complexity of the projective, list-based algorithm is O(n2), where nis the length of the input sentence.Proof 11Same as Proof 8.
Theorem 12The worst-case space complexity of the projective, list-based algorithm is O(n), where nis the length of the input sentence.Proof 12Same as Proof 9.
6.
Experimental EvaluationWe have defined four different transition systems for incremental dependency parsing,proven their correctness for different classes of dependency graphs, and analyzed theirtime and space complexity under the assumption that there exists a constant-time oraclefor predicting the next transition.
In this section, we present an experimental evaluationof the accuracy and efficiency that can be achieved with these systems in deterministicdata-driven parsing, that is, when the oracle is approximated by a classifier trainedon treebank data.
The purpose of the evaluation is to compare the performance of thefour algorithms under realistic conditions, thereby complementing the purely formalanalysis presented so far.
The purpose is not to produce state-of-the-art results for allalgorithms on the data sets used, which would require extensive experimentation andoptimization going well beyond the limits of this study.6.1 Experimental SetupThe data sets used are taken from the CoNLL-X shared task on multilingual dependencyparsing (Buchholz and Marsi 2006).
We have used all the available data sets, taken537Computational Linguistics Volume 34, Number 4Table 1Data sets.
Tok = number of tokens (?1000); Sen = number of sentences (?1000); T/S = tokensper sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grainedpart-of-speech tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number ofmorphosyntactic features (split into atoms); Dep = number of dependency types; NPT =proportion of non-projective dependencies/tokens (%); NPS = proportion of non-projectivedependency graphs/sentences (%).Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPSArabic 54 1.5 37.2 yes 14 19 19 27 0.4 11.2Bulgarian 190 14.4 14.8 no 11 53 50 18 0.4 5.4Chinese 337 57.0 5.9 no 22 303 0 82 0.0 0.0Czech 1,249 72.7 17.2 yes 12 63 61 78 1.9 23.2Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4German 700 39.2 17.8 no 52 52 0 46 2.3 27.8Japanese 151 17.0 8.9 no 20 77 0 7 1.1 5.3Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2Spanish 89 3.3 27.0 yes 15 38 33 21 0.1 1.7Swedish 191 11.0 17.3 no 37 37 0 56 1.0 9.8Turkish 58 5.0 11.5 yes 14 30 82 25 1.5 11.6from treebanks of thirteen different languages with considerable typological variation.Table 1 gives an overview of the training data available for each language.For data sets that include a non-negligible proportion of non-projective dependencygraphs, it can be expected that the non-projective list-based algorithm will achievehigher accuracy than the strictly projective algorithms.
In order to make the comparisonmore fair, we therefore also evaluate pseudo-projective versions of the latter algorithms,making use of graph transformations in pre- and post-processing to recover non-projective dependency arcs, following Nivre and Nilsson (2005).
For each language,seven different parsers were therefore trained as follows:1.
For the non-projective list-based algorithm, one parser was trainedwithout preprocessing the training data.2.
For the three projective algorithms, two parsers were trained afterpreprocessing the training data as follows:(a) For the strictly projective parser, non-projective dependency graphsin the training data were transformed by lifting non-projectivearcs to the nearest permissible ancestor of the real head.
Thiscorresponds to the Baseline condition in Nivre and Nilsson (2005).
(b) For the pseudo-projective parser, non-projective dependencygraphs in the training data were transformed by liftingnon-projective arcs to the nearest permissible ancestor of thereal head, and augmenting the arc label with the label of the realhead.
The output of this parser was post-processed by loweringdependency arcs with augmented labels using a top-down,left-to-right, breadth-first search for the first descendant of thehead that matches the augmented arc label.
This corresponds tothe Head condition in Nivre and Nilsson (2005).538Nivre Deterministic Incremental Dependency ParsingTable 2Feature models.
Rows represent tokens defined relative to the current configuration (L[i] = ithelement of list/stack L of length n; hd(x) = head of x; ld(x) = leftmost dependent of x; rd(x) =rightmost dependent of x).
Columns represent attributes of tokens (Form = word form; Lem =lemma; CPoS = coarse part-of-speech; FPoS = fine part-of-speech; Feats = morphosyntacticfeatures; Dep = dependency label).
Filled cells represent features used by one or morealgorithms (All = all algorithms; S = arc-standard, stack-based; E = arc-eager, stack-based;N = non-projective, list-based; P = projective, list-based).AttributesTokens Form Lem CPoS FPoS Feats Dep?
[0] All All All All All N?
[1] All All?
[2] All?
[3] Allld(?
[0]) Allrd(?
[0]) S?
[0] SE SE SE SE SE E?
[1] SEhd(?
[0]) Eld(?
[0]) SErd(?
[0]) SE?1[0] NP NP NP NP NP NP?1[1] NPhd(?1[0]) NPld(?1[0]) NPrd(?1[0]) NP?2[0] N?2[n] NAll parsers were trained using the freely available MaltParser system,6 which providesimplementations of all the algorithms described in Sections 4 and 5.
MaltParser alsoincorporates the LIBSVM library for support vector machines (Chang and Lin 2001),which was used to train classifiers for predicting the next transition.
Training data forthe classifiers were generated by parsing each sentence in the training set using the gold-standard dependency graph as an oracle.
For each transition t(c) in the oracle parse, atraining instance (?
(c), t) was created, where ?
(c) is a feature vector representation ofthe parser configuration c. Because the purpose of the experiments was not to optimizeparsing accuracy as such, no work was done on feature selection for the differentalgorithms and languages.
Instead, all parsers use a variant of the simple feature modelused for parsing English and Swedish in Nivre (2006b), with minor modifications to suitthe different algorithms.Table 2 shows the feature sets used for different parsing algorithms.7 Each rowrepresents a node defined relative to the current parser configuration, where nodes6 Available at http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.7 For each of the three projective algorithms, the strictly projective and the pseudo-projective variantsused exactly the same set of features, although the set of values for the dependency label featureswere different because of the augmented label set introduced by the pseudo-projective technique.539Computational Linguistics Volume 34, Number 4defined relative to the stack ?
are only relevant for stack-based algorithms, whereasnodes defined relative to the lists ?1 and ?2 are only relevant for list-based algorithms.We use the notation L[i], for arbitrary lists or stacks, to denote the ith element of L, withL[0] for the first element (top element of a stack) and L[n] for the last element.
Nodesdefined relative to the partially-built dependency graph make use of the operators hd, ld,and rd, which return, respectively, the head, the leftmost dependent, and the rightmostdependent of a node in the dependency graph Gc defined by the current configurationc, if such a node exists, and a null value otherwise.
The columns in Table 2 representattributes of nodes (tokens) in the input (word form, lemma, coarse part-of-speech, finepart-of-speech, morphosyntactic features) or in the partially-built dependency graph(dependency label), which can be used to define features.
Each cell in the table thusrepresents a feature fij = aj(ni), defined by selecting the attribute aj in the jth columnfrom the node ni characterized in the ith row.
For example, the feature f11 is the wordform of the first input node (token) in the buffer ?.
The symbols occurring in filledcells indicate for which parsing algorithms the feature is active, where S stands for arc-standard stack-based, E for arc-eager stack-based, N for non-projective list-based, andP for projective list-based.
Features that are used for some but not all algorithms aretypically not meaningful for all algorithms.
For example, a right dependent of the firstnode in the buffer ?
can only exist (at decision time) when using the arc-standard stack-based algorithm.
Hence, this feature is inactive for all other algorithms.The SVM classifiers were trained with a quadratic kernel K(xi, xj) = (?xTi xj + r)2and LIBSVM?s built-in one-versus-one strategy for multi-class classification, convert-ing symbolic features to numerical ones using the standard technique of binarization.The parameter settings were ?
= 0.2 and r = 0 for the kernel parameters, C = 0.5 forthe penalty parameter, and  = 1.0 for the termination criterion.
These settings wereextrapolated from many previous experiments under similar conditions, using cross-validation or held-out subsets of the training data for tuning, but in these experimentsthey were kept fixed for all parsers and languages.
In order to reduce training times, theset of training instances derived from a given training set was split into smaller sets, forwhich separate multi-class classifiers were trained, using FPoS(?
[0]), that is, the (fine-grained) part of speech of the first node in the buffer, as the defining feature for thesplit.The seven different parsers for each language were evaluated by running them onthe dedicated test set from the CoNLL-X shared task, which consists of approximately5,000 tokens for all languages.
Because the dependency graphs in the gold standardare always trees, each output graph was converted, if necessary, from a forest to a treeby attaching every root node i (i > 0) to the special root node 0 with a default labelROOT.
Parsing accuracy was measured by the labeled attachment score (LAS), that is,the percentage of tokens that are assigned the correct head and dependency label, aswell as the unlabeled attachment score (UAS), that is, the percentage of tokens withthe correct head, and the label accuracy (LA), that is, the percentage of tokens with thecorrect dependency label.
All scores were computed with the scoring software fromthe CoNLL-X shared task, eval.pl, with default settings.
This means that punctuationtokens are excluded in all scores.
In addition to parsing accuracy, we evaluatedefficiency by measuring the learning time and parsing time in seconds for each data set.Before turning to the results of the evaluation, we need to fulfill the promise fromRemarks 1 and 2 to discuss the way in which treebank-induced classifiers approximateoracles and to what extent they satisfy the condition of constant-time operation thatwas assumed in all the results on time complexity in Sections 4 and 5.
When pre-dicting the next transition at run-time, there are two different computations that take540Nivre Deterministic Incremental Dependency Parsingplace: the first is the classifier returning a transition t as the output class for an inputfeature vector?
(c), and the second is a check whether the preconditions of t are satisfiedin c. If the preconditions are satisfied, the transition t is performed; otherwise a defaulttransition (with no preconditions) is performed instead.8 (The default transition is SHIFTfor the stack-based algorithms and NO-ARC for the list-based algorithms.)
The timerequired to compute the classification t of ?
(c) depends on properties of the classifier,such as the number of support vectors and the number of classes for a multi-class SVMclassifier, but is independent of the length of the input and can therefore be regardedas a constant as far as the time complexity of the parsing algorithm is concerned.9The check of preconditions is a trivial constant-time operation in all cases except one,namely the need to check whether there is a path between two nodes for the LEFT-ARCnland RIGHT-ARCnl transitions of the non-projective list-based algorithm.
Maintaining theinformation needed for this check and updating it with each addition of a new arcto the graph is equivalent to the union-find operations for disjoint set data structures.Using the techniques of path compression and union by rank, the amortized time peroperation is O(?
(n)) per operation, where n is the number of elements (nodes in thiscase) and ?
(n) is the inverse of the Ackermann function, which means that ?
(n) isless than 5 for all remotely practical values of n and is effectively a small constant(Cormen, Leiserson, and Rivest 1990).
With this proviso, all the complexity results fromSections 4 and 5 can be regarded as valid also for the classifier-based implementation ofdeterministic, incremental dependency parsing.6.2 Parsing AccuracyTable 3 shows the parsing accuracy obtained for each of the 7 parsers on each of the13 languages, as well as the average over all languages, with the top score in eachrow set in boldface.
For comparison, we also include the results of the two top scoringsystems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006)and Nivre et al (2006).
Starting with the LAS, we see that the multilingual averageis very similar across the seven parsers, with a difference of only 0.58 percentagepoints between the best and the worst result, obtained with the non-projective andthe strictly projective version of the list-based parser, respectively.
However, given thelarge amount of data, some of the differences are nevertheless statistically significant(according to McNemar?s test, ?
= .05).
Broadly speaking, the group consisting of thenon-projective, list-based parser and the three pseudo-projective parsers significantlyoutperforms the group consisting of the three projective parsers, whereas there areno significant differences within the two groups.10 This shows that the capacity tocapture non-projective dependencies does make a significant difference, even thoughsuch dependencies are infrequent in most languages.The best result is about one percentage point below the top scores from the originalCoNLL-X shared task, but it must be remembered that the results in this article have8 A more sophisticated strategy would be to back off to the second best choice of the oracle, assuming thatthe oracle provides a ranking of all the possible transitions.
On the whole, however, classifiers very rarelypredict transitions that are not legal in the current configuration.9 The role of this constant in determining the overall running time is similar to that of a grammar constantin grammar-based parsing.10 The only exception to this generalization is the pseudo-projective, list-based parser, which is significantlyworse than the non-projective, list-based parser, but not significantly better than the projective,arc-standard, stack-based parser.541Computational Linguistics Volume 34, Number 4Table 3Parsing accuracy for 7 parsers on 13 languages, measured by labeled attachment score (LAS),unlabeled attachment score (UAS) and label accuracy (LA).
NP-L = non-projective list-based;P-L = projective list-based; PP-L = pseudo-projective list-based; P-E = projective arc-eagerstack-based; PP-E = pseudo-projective arc-eager stack-based; P-S = projective arc-standardstack-based; PP-S = pseudo-projective arc-standard stack-based; McD = McDonald, Lermanand Pereira (2006); Niv = Nivre et al (2006).Labeled Attachment Score (LAS)Language NP-L P-L PP-L P-E PP-E P-S PP-S McD NivArabic 63.25 63.19 63.13 64.93 64.95 65.79 66.05 66.91 66.71Bulgarian 87.79 87.75 87.39 87.75 87.41 86.42 86.71 87.57 87.41Chinese 85.77 85.96 85.96 85.96 85.96 86.00 86.00 85.90 86.92Czech 78.12 76.24 78.04 76.34 77.46 78.18 80.12 80.18 78.42Danish 84.59 84.15 84.35 84.25 84.45 84.17 84.15 84.79 84.77Dutch 77.41 74.71 76.95 74.79 76.89 73.27 74.79 79.19 78.59German 84.42 84.21 84.38 84.23 84.46 84.58 84.58 87.34 85.82Japanese 90.97 90.57 90.53 90.83 90.89 90.59 90.63 90.71 91.65Portuguese 86.70 85.91 86.20 85.83 86.12 85.39 86.09 86.82 87.60Slovene 70.06 69.88 70.12 69.50 70.22 72.00 71.88 73.44 70.30Spanish 80.18 79.80 79.60 79.84 79.60 78.70 78.42 82.25 81.29Swedish 83.03 82.63 82.41 82.63 82.41 82.12 81.54 82.55 84.58Turkish 64.69 64.49 64.37 64.37 64.43 64.67 64.73 63.19 65.68Average 79.77 79.19 79.49 79.33 79.63 79.38 79.67 80.83 80.75Unlabeled Attachment Score (UAS)Language NP-L P-L PP-L P-E PP-E P-S PP-S McD NivArabic 76.43 76.19 76.49 76.31 76.25 77.76 77.98 79.34 77.52Bulgarian 91.68 91.92 91.62 91.92 91.64 90.80 91.10 92.04 91.72Chinese 89.42 90.24 90.24 90.24 90.24 90.42 90.42 91.07 90.54Czech 84.88 82.82 84.58 82.58 83.66 84.50 86.44 87.30 84.80Danish 89.76 89.40 89.42 89.52 89.52 89.26 89.38 90.58 89.80Dutch 80.25 77.29 79.59 77.25 79.47 75.95 78.03 83.57 81.35German 87.80 87.10 87.38 87.14 87.42 87.46 87.44 90.38 88.76Japanese 92.64 92.60 92.56 92.80 92.72 92.50 92.54 92.84 93.10Portuguese 90.56 89.82 90.14 89.74 90.08 89.00 89.64 91.36 91.22Slovene 79.06 78.78 79.06 78.42 78.98 80.72 80.76 83.17 78.72Spanish 83.39 83.75 83.65 83.79 83.65 82.35 82.13 86.05 84.67Swedish 89.54 89.30 89.03 89.30 89.03 88.81 88.39 88.93 89.50Turkish 75.12 75.24 75.02 75.32 74.81 75.94 75.76 74.67 75.82Average 85.43 84.96 85.29 84.95 85.19 85.04 85.39 87.02 85.96Label Accuracy (LA)Language NP-L P-L PP-L P-E PP-E P-S PP-S McD NivArabic 75.93 76.15 76.09 78.46 78.04 79.30 79.10 79.50 80.34Bulgarian 90.68 90.68 90.40 90.68 90.42 89.53 89.81 90.70 90.44Chinese 88.01 87.95 87.95 87.95 87.95 88.33 88.33 88.23 89.01Czech 84.54 84.54 84.42 84.80 84.66 86.00 86.58 86.72 85.40Danish 88.90 88.60 88.76 88.62 88.82 89.00 88.98 89.22 89.16Dutch 82.43 82.59 82.13 82.57 82.15 80.71 80.13 83.89 83.69German 89.74 90.08 89.92 90.06 89.94 90.79 90.36 92.11 91.03Japanese 93.54 93.14 93.14 93.54 93.56 93.12 93.18 93.74 93.34Portuguese 90.56 90.54 90.34 90.46 90.26 90.42 90.26 90.46 91.54Slovene 78.88 79.70 79.40 79.32 79.48 81.61 81.37 82.51 80.54Spanish 89.46 89.04 88.66 89.06 88.66 88.30 88.12 90.40 90.06Swedish 85.50 85.40 84.92 85.40 84.92 84.66 84.01 85.58 87.39Turkish 77.79 77.32 77.02 77.00 77.39 77.02 77.20 77.45 78.49Average 85.84 85.83 85.63 85.99 85.87 86.06 85.96 86.96 87.03542Nivre Deterministic Incremental Dependency Parsingbeen obtained without optimization of feature representations or learning algorithmparameters.
The net effect of this can be seen in the result for the pseudo-projectiveversion of the arc-eager, stack-based parser, which is identical to the system used byNivre et al (2006), except for the lack of optimization, and which suffers a loss of 1.12percentage points overall.The results for UAS show basically the same pattern as the LAS results, but witheven less variation between the parsers.
Nevertheless, there is still a statistically sig-nificant margin between the non-projective, list-based parser and the three pseudo-projective parsers, on the one hand, and the strictly projective parsers, on the other.11For label accuracy (LA), finally, the most noteworthy result is that the strictly projec-tive parsers consistently outperform their pseudo-projective counterparts, although thedifference is statistically significant only for the projective, list-based parser.
This canbe explained by the fact that the pseudo-projective parsing technique increases thenumber of distinct dependency labels, using labels to distinguish not only betweendifferent syntactic functions but also between ?lifted?
and ?unlifted?
arcs.
It is there-fore understandable that the pseudo-projective parsers suffer a drop in pure labelingaccuracy.Despite the very similar performance of all parsers on average over all languages,there are interesting differences for individual languages and groups of languages.These differences concern the impact of non-projective, pseudo-projective, and strictlyprojective parsing, on the one hand, and the effect of adopting an arc-eager or an arc-standard parsing strategy for the stack-based parsers, on the other.
Before we turnto the evaluation of efficiency, we will try to analyze some of these differences in alittle more detail, starting with the different techniques for capturing non-projectivedependencies.First of all, we may observe that the non-projective, list-based parser outperforms itsstrictly projective counterpart for all languages except Chinese.
The result for Chineseis expected, given that it is the only data set that does not contain any non-projectivedependencies, but the difference in accuracy is very slight (0.19 percentage points).Thus, it seems that the non-projective parser can also be used without loss in accuracyfor languages with very few non-projective structures.
The relative improvement inaccuracy for the non-projective parser appears to be roughly linear in the percent-age of non-projective dependencies found in the data set, with a highly significantcorrelation (Pearson?s r = 0.815, p = 0.0007).
The only language that clearly divergesfrom this trend is German, where the relative improvement is much smaller thanexpected.If we compare the non-projective, list-based parser to the strictly projective stack-based parsers, we see essentially the same pattern but with a little more variation.
Forthe arc-eager, stack-based parser, the only anomaly is the result for Arabic, which issignificantly higher than the result for the non-projective parser, but this seems to bedue to a particularly bad performance of the list-based parsers as a group for thislanguage.12 For the arc-standard, stack-based parser, the data is considerably morenoisy, which is related to the fact that the arc-standard parser in itself has a higher11 The exception this time is the pseudo-projective, arc-eager parser, which has a statistically significantdifference up to the non-projective parser but a non-significant difference down to the projective,arc-standard parser.12 A possible explanation for this result is the extremely high average sentence length for Arabic, whichleads to a greater increase in the number of potential arcs considered for the list-based parsers than forthe stack-based parsers.543Computational Linguistics Volume 34, Number 4variance than the other parsers, an observation that we will return to later on.
Still, thecorrelation between relative improvement in accuracy and percentage of non-projectivedependencies is significant for both the arc-eager parser (r = 0.766, p = 0.001) and thearc-standard parser (r = 0.571, p = 0.02), although clearly not as strong as for the list-based parser.
It therefore seems reasonable to conclude that the non-projective parser ingeneral can be expected to outperform a strictly projective parser with a margin that isdirectly related to the proportion of non-projective dependencies in the data.Having compared the non-projective, list-based parser to the strictly projectiveparsers, we will now scrutinize the results obtained when coupling the projectiveparsers with the pseudo-projective parsing technique, as an alternative method forcapturing non-projective dependencies.
The overall pattern is that pseudo-projectiveparsing improves the accuracy of a projective parser for languages with more than 1%of non-projective dependencies, as seen from the results for Czech, Dutch, German, andPortuguese.
For these languages, the pseudo-projective parser is never outperformedby its strictly projective counterpart, and usually does considerably better, although theimprovements for German are again smaller than expected.
For Slovene and Turkish,we find improvement only for two out of three parsers, despite a relatively high shareof non-projective dependencies (1.9% for Slovene, 1.5% for Turkish).
Given that Sloveneand Turkish have the smallest training data sets of all languages, this is consistent withprevious studies showing that pseudo-projective parsing is sensitive to data sparseness(Nilsson, Nivre, and Hall 2007).
For languages with a lower percentage of non-projectivedependencies, the pseudo-projective technique seems to hurt performance more oftenthan not, possibly as a result of decreasing the labeling accuracy, as noted previously.It is worth noting that Chinese is a special case in this respect.
Because there are nonon-projective dependencies in this data set, the projectivized training data set will beidentical to the original one, which means that the pseudo-projective parser will behaveexactly as the projective one.Comparing non-projective parsing to pseudo-projective parsing, it seems clear thatboth can improve parsing accuracy in the presence of significant amounts of non-projective dependencies, but the former appears to be more stable in that it seldomor never hurts performance, whereas the latter can be expected to have a negative effecton accuracy when the amount of training data or non-projective dependencies (or both)is not high enough.
Moreover, the non-projective parser tends to outperform the bestpseudo-projective parsers, both on average and for individual languages.
In fact, thepseudo-projective technique outperforms the non-projective parser only in combinationwith the arc-standard, stack-based parsing algorithm, and this seems to be due more tothe arc-standard parsing strategy than to the pseudo-projective technique as such.
Therelevant question here is therefore why arc-standard parsing seems to work particularlywell for some languages, with or without pseudo-projective parsing.Going through the results for individual languages, it is clear that the arc-standardalgorithm has a higher variance than the other algorithms.
For Bulgarian, Dutch, andSpanish, the accuracy is considerably lower than for the other algorithms, in mostcases by more than one percentage point.
But for Arabic, Czech, and Slovene, we findexactly the opposite pattern, with the arc-standard parsers sometimes outperformingthe other parsers by more than two percentage points.
For the remaining languages,the arc-standard algorithm performs on a par with the other algorithms.13 In order to13 The arc-standard algorithm achieves the highest score also for Chinese, German, and Turkish, but inthese cases only by a small margin.544Nivre Deterministic Incremental Dependency Parsingexplain this pattern we need to consider the way in which properties of the algorithmsinteract with properties of different languages and the way they have been annotatedsyntactically.First of all, it is important to note that the two list-based algorithms and the arc-eager variant of the stack-based algorithm are all arc-eager in the sense that an arc (i, l, j)is always added at the earliest possible moment, that is, in the first configuration wherei and j are the target tokens.
For the arc-standard stack-based parser, this is still true forleft dependents (i.e., arcs (i, l, j) such that j < i) but not for right dependents, where anarc (i, l, j) (i < j) should be added only at the point where all arcs of the form ( j, l?, k) havealready been added (i.e., when the dependent j has already found all its dependents).This explains why the results for the two list-based parsers and the arc-eager stack-based parser are so well correlated, but it does not explain why the arc-standard strategyworks better for some languages but not for others.The arc-eager strategy has an advantage in that a right dependent j can be attachedto its head i at any time without having to decide whether j itself should have a rightdependent.
By contrast, with the arc-standard strategy it is necessary to decide not onlywhether j is a right dependent of i but also whether it should be added now or later,which means that two types of errors are possible even when the decision to attach jto i is correct.
Attaching too early means that right dependents can never be attachedto j; postponing the attachment too long means that j will never be added to i.
None ofthese errors can occur with the arc-eager strategy, which therefore can be expected towork better for data sets where this kind of ?ambiguity?
is commonly found.
In orderfor this to be the case, there must first of all be a significant proportion of left-headedstructures in the data.
Thus, we find that in all the data sets for which the arc-standardparsers do badly, the percentage of left-headed dependencies is in the 50?75% range.However, it must also be pointed out that the highest percentage of all is found inArabic (82.9%), which means that a high proportion of left-headed structures may bea necessary but not sufficient condition for the arc-eager strategy to work better thanthe arc-standard strategy.
We conjecture that an additional necessary condition is anannotation style that favors more deeply embedded structures, giving rise to chainsof left-headed structures where each node is dependent on the preceding one, whichincreases the number of points at which an incorrect decision can be made by an arc-standard parser.
However, we have not yet fully verified the extent to which this condi-tion holds for all the data sets where the arc-eager parsers outperform their arc-standardcounterparts.Although the arc-eager strategy has an advantage in that the decisions involved inattaching a right dependent are simpler, it has the disadvantage that it has to commitearly.
This may either lead the parser to add an arc (i, l, j) (i < j) when it is not correctto do so, or fail to add the same arc in a situation where it should have been added, inboth cases because the information available at an early point makes the wrong decisionlook probable.
In the first case, the arc-standard parser may still get the analysis right,if it also seems probable that j should have a right dependent (in which case it willpostpone the attachment); in the second case, it may get a second chance to add the arcif it in fact adds a right dependent to j at a later point.
It is not so easy to predict whattype of structures and annotation will favor the arc-standard parser in this way, but itis likely that having many right dependents attached to (or near) the root could causeproblems for the arc-eager algorithms, since these dependencies determine the globalstructure and often span long distances, which makes it harder to make correct decisionsearly in the parsing process.
This is consistent with earlier studies showing that parsersusing the arc-eager, stack-based algorithm tend to predict dependents of the root with545Computational Linguistics Volume 34, Number 4lower precision than other algorithms.14 Interestingly, the three languages for whichthe arc-standard parser has the highest improvement (Arabic, Czech, Slovene) have avery similar annotation, based on the Prague school tradition of dependency grammar,which not only allows multiple dependents of the root but also uses several differentlabels for these dependents, which means that they will be analyzed correctly only if aRIGHT-ARC transition is performed with the right label at exactly the right point in time.This is in contrast to annotation schemes that use a default label ROOT, for dependentsof the root, where such dependents can often be correctly recovered in post-processingby attaching all remaining roots to the special root node with the default label.
Wecan see the effect of this by comparing the two stack-based parsers (in their pseudo-projective versions) with respect to precision and recall for the dependency type PRED(predicate), which is the most important label for dependents of the root in the data setsfor Arabic, Czech, and Slovene.
While the arc-standard parser has 78.02% precision and70.22% recall, averaged over the three languages, the corresponding figures for the arc-eager parser are as low as 68.93% and 65.93%, respectively, which represents a drop ofalmost ten percentage points in precision and almost five percentage points in recall.Summarizing the results of the accuracy evaluation, we have seen that all four algo-rithms can be used for deterministic, classifier-based parsing with competitive accuracy.The results presented are close to the state of the art without any optimization of featurerepresentations and learning algorithm parameters.
Comparing different algorithms,we have seen that the capacity to capture non-projective dependencies makes a signif-icant difference in general, but with language-specific effects that depend primarily onthe frequency of non-projective constructions.
We have also seen that the non-projectivelist-based algorithm is more stable and predictable in this respect, compared to the useof pseudo-projective parsing in combination with an essentially projective parsing algo-rithm.
Finally, we have observed quite strong language-specific effects for the differencebetween arc-standard and arc-eager parsing for the stack-based algorithms, effects thatcan be tied to differences in linguistic structure and annotation style between differentdata sets, although a much more detailed error analysis is needed before we can drawprecise conclusions about the relative merits of different parsing algorithms for differentlanguages and syntactic representations.6.3 EfficiencyBefore we consider the evaluation of efficiency in both learning and parsing, it isworth pointing out that the results will be heavily dependent on the choice of supportvector machines for classification, and cannot be directly generalized to the use ofdeterministic incremental parsing algorithms together with other kinds of classifiers.However, because support vector machines constitute the state of the art in classifier-based parsing, it is still worth examining how learning and parsing times vary with theparsing algorithm while parameters of learning and classification are kept constant.Table 4 gives the results of the efficiency evaluation.
Looking first at learning times,it is obvious that learning time depends primarily on the number of training instances,which is why we can observe a difference of several orders of magnitude in learningtime between the biggest training set (Czech) and the smallest training set (Slovene)14 This is shown by Nivre and Scholz (2004) in comparison to the iterative, arc-standard algorithm ofYamada and Matsumoto (2003) and by McDonald and Nivre (2007) in comparison to the spanningtree algorithm of McDonald, Lerman, and Pereira (2006).546Nivre Deterministic Incremental Dependency ParsingTable 4Learning and parsing time for seven parsers on six languages, measured in seconds.NP-L = non-projective list-based; P-L = projective list-based; PP-L = pseudo-projective list-based;P-E = projective arc-eager stack-based; PP-E = pseudo-projective arc-eager stack-based; P-S =projective arc-standard stack-based; PP-S = pseudo-projective arc-standard stack-based.Learning TimeLanguage NP-L P-L PP-L P-E PP-E P-S PP-SArabic 1,814 614 603 650 647 1,639 1,636Bulgarian 6,796 2,918 2,926 2,919 2,939 3,321 3,391Chinese 17,034 13,019 13,019 13,029 13,029 13,705 13,705Czech 546,880 250,560 248,511 279,586 280,069 407,673 406,857Danish 2,964 1,248 1,260 1,246 1,262 643 647Dutch 7,701 3,039 2,966 3,055 2,965 7,000 6,812German 48,699 16,874 17,600 16,899 17,601 24,402 24,705Japanese 211 191 188 203 208 199 199Portuguese 25,621 8,433 8,336 8,436 8,335 7,724 7,731Slovene 167 78 90 93 99 86 90Spanish 1,999 562 566 565 565 960 959Swedish 2,410 942 1,020 945 1,022 1,350 1,402Turkish 720 498 519 504 516 515 527Average 105,713 46,849 46,616 51,695 51,876 74,798 74,691Parsing TimeLanguage NP-L P-L PP-L P-E PP-E P-S PP-SArabic 213 103 131 108 135 196 243Bulgarian 139 93 102 93 103 135 147Chinese 1,008 855 855 855 855 803 803Czech 5,244 3,043 5,889 3,460 6,701 3,874 7,437Danish 109 66 83 66 83 82 106Dutch 349 209 362 211 363 253 405German 781 456 947 455 945 494 1,004Japanese 10 8 8 9 10 7 7Portuguese 670 298 494 298 493 437 717Slovene 69 44 62 47 65 43 64Spanish 133 67 75 67 75 80 91Swedish 286 202 391 201 391 242 456Turkish 218 162 398 162 403 153 380Average 1,240 712 1,361 782 1,496 897 1,688for a given parsing algorithm.
Broadly speaking, for any given parsing algorithm, theranking of languages with respect to learning time follows the ranking with respectto training set size, with a few noticeable exceptions.
Thus, learning times are shorterthan expected, relative to other languages, for Swedish and Japanese, but longer thanexpected for Arabic and (except in the case of the arc-standard parsers) for Danish.However, the number of training instances for the SVM learner depends not onlyon the number of tokens in the training set, but also on the number of transitionsrequired to parse a sentence of length n. This explains why the non-projective list-basedalgorithm, with its quadratic complexity, consistently has longer learning times thanthe linear stack-based algorithms.
However, it can also be noted that the projective, list-based algorithm, despite having the same worst-case complexity as the non-projective547Computational Linguistics Volume 34, Number 4algorithm, in practice behaves much more like the arc-eager stack-based algorithm andin fact has a slightly lower learning time than the latter on average.
The arc-standardstack-based algorithm, finally, again shows much more variation than the other algo-rithms.
On average, it is slower to train than the arc-eager algorithm, and sometimesvery substantially so, but for a few languages (Danish, Japanese, Portuguese, Slovene)it is actually faster (and considerably so for Danish).
This again shows that learning timedepends on other properties of the training sets than sheer size, and that some data setsmay be more easily separable for the SVM learner with one parsing algorithm than withanother.It is noteworthy that there are no consistent differences in learning time betweenthe strictly projective parsers and their pseudo-projective counterparts, despite the factthat the pseudo-projective technique increases the number of distinct classes (because ofits augmented arc labels), which in turn increases the number of binary classifiers thatneed to be trained in order to perform multi-class classification with the one-versus-onemethod.
The number of classifiers ism(m?1)2 , where m is the number of classes, and thepseudo-projective technique with the encoding scheme used here can theoretically leadto a quadratic increase in the number of classes.
The fact that this has no noticeable effecton efficiency indicates that learning time is dominated by other factors, in particular thenumber of training instances.Turning to parsing efficiency, we may first note that parsing time is also dependenton the size of the training set, through a dependence on the number of support vectors,which tend to grow with the size of the training set.
Thus, for any given algorithm, thereis a strong tendency that parsing times for different languages follow the same order astraining set sizes.
The notable exceptions are Arabic, Turkish, and Chinese, which havehigher parsing times than expected (relative to other languages), and Japanese, whereparsing is surprisingly fast.
Because these deviations are the same for all algorithms, itseems likely that they are related to specific properties of these data sets.
It is also worthnoting that for Arabic and Japanese the deviations are consistent across learning andparsing (slower than expected for Arabic, faster than expected for Japanese), whereasfor Chinese there is no consistent trend (faster than expected in learning, slower thanexpected in parsing).Comparing algorithms, we see that the non-projective list-based algorithm is slowerthan the strictly projective stack-based algorithms, which can be expected from thedifference in time complexity.
But we also see that the projective list-based algorithm,despite having the same worst-case complexity as the non-projective algorithm, inpractice behaves like the linear-time algorithms and is in fact slightly faster on averagethan the arc-eager stack-based algorithm, which in turn outperforms the arc-standardstack-based algorithm.
This is consistent with the results from oracle parsing reported inNivre (2006a), which show that, with the constraint of projectivity, the relation betweensentence length and number of transitions for the list-based parser can be regardedas linear in practice.
Comparing the arc-eager and the arc-standard variants of thestack-based algorithm, we find the same kind of pattern as for learning time in thatthe arc-eager parser is faster for all except a small set of languages: Chinese, Japanese,Slovene, and Turkish.
Only two of these, Japanese and Slovene, are languages for whichlearning is also faster with the stack-based algorithm, which again shows that there isno straightforward correspondence between learning time and parsing time.Perhaps the most interesting result of all, as far as efficiency is concerned, is to befound in the often dramatic differences in parsing time between the strictly projectiveparsers and their pseudo-projective counterparts.
Although we did not see any cleareffect of the increased number of classes, hence classifiers, on learning time earlier, it is548Nivre Deterministic Incremental Dependency Parsingquite clear that there is a noticeable effect on parsing time, with the pseudo-projectiveparsers always being substantially slower.
In fact, in some cases the pseudo-projectiveparsers are also slower than the non-projective list-based parser, despite the differencein time complexity that exists at least for the stack-based parsers.
This result holds onaverage over all languages and for five out of thirteen of the individual languages andshows that the advantage of linear-time parsing complexity (for the stack-based parsers)can be outweighed by the disadvantage of a more complex classification problem inpseudo-projective parsing.
In other words, the larger constant associated with a largercohort of SVM classifiers for the pseudo-projective parser can be more important thanthe better asymptotic complexity of the linear-time algorithm in the range of sentencelengths typically found in natural language.
Looking more closely at the variation insentence length across languages, we find that the pseudo-projective parsers are fasterthan the non-projective parser for all data sets with an average sentence length above18.
For data sets with shorter sentences, the non-projective parser is more efficient in allexcept three cases: Bulgarian, Chinese, and Japanese.
For Chinese this is easily explainedby the absence of non-projective dependencies, making the performance of the pseudo-projective parsers identical to their strictly projective counterparts.
For the other twolanguages, the low number of distinct dependency labels for Japanese and the low per-centage of non-projective dependencies for Bulgarian are factors that mitigate the effectof enlarging the set of dependency labels in pseudo-projective parsing.
We concludethat the relative efficiency of non-projective and pseudo-projective parsing dependson several factors, of which sentence length appears to be the most important, butwhere the number of distinct dependency labels and the percentage of non-projectivedependencies also play a role.7.
Related WorkData-driven dependency parsing using supervised machine learning was pioneered byEisner (1996), who showed how traditional chart parsing techniques could be adaptedfor dependency parsing to give efficient parsing with exact inference over a probabilisticmodel where the score of a dependency tree is the sum of the scores of individual arcs.This approach has been further developed in particular by Ryan McDonald and hiscolleagues (McDonald, Crammer, and Pereira 2005; McDonald et al 2005; McDonaldand Pereira 2006) and is now known as spanning tree parsing, because the problemof finding the most probable tree under this type of model is equivalent to findingan optimum spanning tree in a dense graph containing all possible dependency arcs.If we assume that the score of an individual arc is independent of all other arcs, thisproblem can be solved efficiently for arbitrary non-projective dependency trees usingthe Chu-Liu-Edmonds algorithm, as shown by McDonald et al (2005).
Spanning treealgorithms have so far primarily been combined with online learning methods such asMIRA (McDonald, Crammer, and Pereira 2005).The approach of deterministic classifier-based parsing was first proposed forJapanese by Kudo and Matsumoto (2002) and for English by Yamada and Matsumoto(2003).
In contrast to spanning tree parsing, this can be characterized as a greedyinference strategy, trying to construct a globally optimal dependency graph by makinga sequence of locally optimal decisions.
The first strictly incremental parser of this kindwas described in Nivre (2003) and used for classifier-based parsing of Swedish by Nivre,Hall, and Nilsson (2004) and English by Nivre and Scholz (2004).
Altogether it has nowbeen applied to 19 different languages (Nivre et al 2006, 2007; Hall et al 2007).
Mostalgorithms in this tradition are restricted to projective dependency graphs, but it is549Computational Linguistics Volume 34, Number 4possible to recover non-projective dependencies using pseudo-projective parsing(Nivre and Nilsson 2005).
More recently, algorithms for non-projective classifier-basedparsing have been proposed by Attardi (2006) and Nivre (2006a).
The strictly deter-ministic parsing strategy has been relaxed in favor of n-best parsing by Johanssonand Nugues (2006), among others.
The dominant learning method in this tradition issupport vector machines (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003;Nivre et al 2006) but memory-based learning has also been used (Nivre, Hall, andNilsson 2004; Nivre and Scholz 2004; Attardi 2006).Of the algorithms described in this article, the arc-eager stack-based algorithm isessentially the algorithm proposed for unlabeled dependency parsing in Nivre (2003),extended to labeled dependency parsing in Nivre, Hall, and Nilsson (2004), and mostfully described in Nivre (2006b).
The major difference is that the parser is now initializedwith the special root node on the stack, whereas earlier formulations had an emptystack at initialization.15 The arc-standard stack-based algorithm is briefly described inNivre (2004) but can also be seen as an incremental version of the algorithm of Yamadaand Matsumoto (2003), where incrementality is achieved by only allowing one left-to-right pass over the input, whereas Yamada and Matsumoto perform several iterationsin order to construct the dependency graph bottom-up, breadth-first as it were.
Thelist-based algorithms are both inspired by the work of Covington (2001), although theformulations are not equivalent.
They have previously been explored for deterministicclassifier-based parsing in Nivre (2006a, 2007).
A more orthodox implementation ofCovington?s algorithms for data-driven dependency parsing is found in Marinov (2007).8.
ConclusionIn this article, we have introduced a formal framework for deterministic incrementaldependency parsing, where parsing algorithms can be defined in terms of transitionsystems that are deterministic only together with an oracle for predicting the nexttransition.
We have used this framework to analyze four different algorithms, provingthe correctness of each algorithm relative to a relevant class of dependency graphs, andgiving complexity results for each algorithm.To complement the formal analysis, we have performed an experimental evaluationof accuracy and efficiency, using SVM classifiers to approximate oracles, and using datafrom 13 languages.
The comparison shows that although strictly projective dependencyparsing is most efficient both in learning and in parsing, the capacity to produce non-projective dependency graphs leads to better accuracy unless it can be assumed thatall structures are strictly projective.
The evaluation also shows that using the non-projective, list-based parsing algorithm gives a more stable improvement in this respectthan applying the pseudo-projective parsing technique to a strictly projective parsingalgorithm.
Moreover, despite its quadratic time complexity, the non-projective parser isoften as efficient as the pseudo-projective parsers in practice, because the extended setof dependency labels used in pseudo-projective parsing slows down classification.
Thisdemonstrates the importance of complementing the theoretical analysis of complexitywith practical running time experiments.Although the non-projective, list-based algorithm can be said to give the best trade-off between accuracy and efficiency when results are averaged over all languages in thesample, we have also observed important language-specific effects.
In particular, the15 The current version was first used in the CoNLL-X shared task (Nivre et al 2006).550Nivre Deterministic Incremental Dependency Parsingarc-eager strategy inherent not only in the arc-eager, stack-based algorithm but also inboth versions of the list-based algorithm appears to be suboptimal for some languagesand syntactic representations.
In such cases, using the arc-standard parsing strategy,with or without pseudo-projective parsing, may lead to significantly higher accuracy.More research is needed to determine exactly which properties of linguistic structuresand their syntactic analysis give rise to these effects.On the whole, however, the four algorithms investigated in this article give verysimilar performance both in terms of accuracy and efficiency, and several previousstudies have shown that both the stack-based and the list-based algorithms can achievestate-of-the-art accuracy together with properly trained classifiers (Nivre et al 2006;Nivre 2007; Hall et al 2007).AcknowledgmentsI want to thank my students Johan Hall andJens Nilsson for fruitful collaboration and fortheir contributions to the MaltParser system,which was used for all experiments.
I alsowant to thank Sabine Buchholz, MatthiasBuch-Kromann, Walter Daelemans, Gu?ls?enEryig?it, Jason Eisner, Jan Hajic?, SandraKu?bler, Marco Kuhlmann, Yuji Matsumoto,Ryan McDonald, Kemal Oflazer, Kenji Sagae,Noah A. Smith, and Deniz Yuret for usefuldiscussions on topics relevant to this article.I am grateful to three anonymous reviewersfor many helpful suggestions that helpedimprove the final version of the article.
Thework has been partially supported by theSwedish Research Council.ReferencesAbney, Steven and Mark Johnson.
1991.Memory requirements and localambiguities of parsing strategies.
Journalof Psycholinguistic Research, 20:233?250.Aho, Alfred V., Ravi Sethi, and Jeffrey D.Ullman.
1986.
Compilers: Principles,Techniques, and Tools.
Addison-Wesley,Reading, MA.Attardi, Giuseppe.
2006.
Experimentswith a multilanguage non-projectivedependency parser.
In Proceedings ofthe Tenth Conference on ComputationalNatural Language Learning (CoNLL-X),pages 166?170, New York.Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,and Barbora Hladka?.
2003.
The PragueDependency Treebank: A three-levelannotation scenario.
In Anne Abeille?,editor, Treebanks: Building and UsingParsed Corpora.
Kluwer AcademicPublishers, Dordrecht, pages 103?127.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings ofthe Tenth Conference on ComputationalNatural Language Learning, pages 149?164,New York.Chang, Chih-Chung and Chih-Jen Lin,2001.
LIBSVM: A Library for SupportVector Machines.
Software availableat http://www.csie.ntu.edu.tw/?cjlin/libsvm.Cheng, Yuchang, Masayuki Asahara,and Yuji Matsumoto.
2005.
Machinelearning-based dependency analyzer forChinese.
In Proceedings of InternationalConference on Chinese Computing (ICCC),pages 66?73, Singapore.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introduction toAlgorithms.
MIT Press, Cambridge, MA.Covington, Michael A.
2001.
A fundamentalalgorithm for dependency parsing.
InProceedings of the 39th Annual ACMSoutheast Conference, pages 95?102,Athens, GA.Eisner, Jason M. 1996.
Three newprobabilistic models for dependencyparsing: An exploration.
In Proceedingsof the 16th International Conference onComputational Linguistics (COLING),pages 340?345, Copenhagen.Hajic?, Jan, Barbora Vidova Hladka, JarmilaPanevova?, Eva Hajic?ova?, Petr Sgall, andPetr Pajas.
2001.
Prague DependencyTreebank 1.0.
LDC, 2001T10.Hall, J., J. Nilsson, J. Nivre, G. Eryig?it,B.
Megyesi, M. Nilsson, and M. Saers.2007.
Single malt or blended?
A studyin multilingual parser optimization.
InProceedings of the CoNLL shared task ofEMNLP-CoNLL 2007, pages 933?939,Prague.Hall, Johan, Joakim Nivre, and Jens Nilsson.2006.
Discriminative classifiers fordeterministic dependency parsing.
InProceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 316?323,Sydney.551Computational Linguistics Volume 34, Number 4Hudson, Richard A.
1990.
English WordGrammar.
Blackwell, Oxford.Johansson, Richard and Pierre Nugues.2006.
Investigating multilingualdependency parsing.
In Proceedingsof the Tenth Conference on ComputationalNatural Language Learning (CoNLL-X),pages 206?210, New York.Kalt, Tom.
2004.
Induction of greedycontrollers for deterministic treebankparsers.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP), pages 17?24,Barcelona.Kudo, Taku and Yuji Matsumoto.
2002.Japanese dependency analysis usingcascaded chunking.
In Proceedings of theSixth Workshop on Computational LanguageLearning (CoNLL), pages 63?69, Taipei.Marcus, Mitchell P. 1980.
A Theory of SyntacticRecognition for Natural Language.
MITPress, Cambridge, MA.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Marcus, Mitchell P., Beatrice Santorini,Mary Ann Marcinkiewicz, RobertMacIntyre, Ann Bies, Mark Ferguson,Karen Katz, and Britta Schasberger.1994.
The Penn Treebank: Annotatingpredicate-argument structure.
InProceedings of the ARPA Human LanguageTechnology Workshop, pages 114?119,Plainsboro, NJ.Marinov, S. 2007.
Covington variations.
InProceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 1144?1148,Prague.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Onlinelarge-margin training of dependencyparsers.
In Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics (ACL),pages 91?98, Ann Arbor, MI.McDonald, Ryan, Kevin Lerman, andFernando Pereira.
2006.
Multilingualdependency analysis with a two-stagediscriminative parser.
In Proceedingsof the Tenth Conference on ComputationalNatural Language Learning (CoNLL),pages 216?220.McDonald, Ryan and Joakim Nivre.
2007.Characterizing the errors of data-drivendependency parsing models.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 122?131, Prague.McDonald, Ryan and Fernando Pereira.2006.
Online learning of approximatedependency parsing algorithms.
InProceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL),pages 81?88, Trento.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic?.
2005.Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings ofthe Human Language Technology Conferenceand the Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP),pages 523?530, Vancouver.Mel?c?uk, Igor.
1988.
Dependency Syntax:Theory and Practice.
State University ofNew York Press, New York.Nilsson, Jens, Joakim Nivre, and Johan Hall.2007.
Generalizing tree transformationsfor inductive dependency parsing.
InProceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 968?975, Prague.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT),pages 149?160, Nancy.Nivre, Joakim.
2004.
Incrementality indeterministic dependency parsing.
InProceedings of the Workshop on IncrementalParsing: Bringing Engineering and CognitionTogether (ACL), pages 50?57, Barcelona.Nivre, Joakim.
2006a.
Constraints onnon-projective dependency graphs.In Proceedings of the 11th Conferenceof the European Chapter of the Associationfor Computational Linguistics (EACL),pages 73?80, Trento.Nivre, Joakim.
2006b.
Inductive DependencyParsing.
Springer, Dordrecht.Nivre, Joakim.
2007.
Incrementalnon-projective dependency parsing.
InProceedings of Human Language Technologies:The Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL HLT), pages 396?403,Rochester, NY.Nivre, Joakim, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.In Proceedings of the 8th Conference onComputational Natural Language Learning(CoNLL), pages 49?56, Boston, MA.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?ls?en Eryig?it, SandraKu?bler, Svetoslav Marinov, and552Nivre Deterministic Incremental Dependency ParsingErwin Marsi.
2007.
MaltParser: Alanguage-independent system fordata-driven dependency parsing.Natural Language Engineering, 13:95?135.Nivre, Joakim, Johan Hall, Jens Nilsson,Gu?lsen Eryig?it, and Svetoslav Marinov.2006.
Labeled pseudo-projectivedependency parsing with supportvector machines.
In Proceedings ofthe Tenth Conference on ComputationalNatural Language Learning (CoNLL),pages 221?225, New York, NY.Nivre, Joakim and Jens Nilsson.
2005.Pseudo-projective dependency parsing.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 99?106,Ann Arbor, MI.Nivre, Joakim and Mario Scholz.
2004.Deterministic dependency parsingof English text.
In Proceedings of the20th International Conference onComputational Linguistics (COLING),pages 64?70, Geneva.Ratnaparkhi, Adwait.
1997.
A linearobserved time statistical parser basedon maximum entropy models.
InProceedings of the Second Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 1?10,Providence, RI.Ratnaparkhi, Adwait.
1999.
Learning toparse natural language with maximumentropy models.Machine Learning,34:151?175.Sagae, Kenji and Alon Lavie.
2005.
Aclassifier-based parser with linearrun-time complexity.
In Proceedings of the9th International Workshop on ParsingTechnologies (IWPT), pages 125?132,Vancouver.Sagae, Kenji and Alon Lavie.
2006.
Abest-first probabilistic shift-reduceparser.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions,pages 691?698, Sydney.Sgall, Petr, Eva Hajic?ova?, and JarmilaPanevova?.
1986.
The Meaning of the Sentencein Its Pragmatic Aspects.
Reidel, Dordrecht.Shieber, Stuart M. 1983.
Sentencedisambiguation by a shift-reduce parsingtechnique.
In Proceedings of the 21stConference on Association for ComputationalLinguistics (ACL), pages 113?118,Cambridge, MA.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation of deductive parsing.Journal of Logic Programming, 24:3?36.Tesnie`re, Lucien.
1959.
E?le?ments de syntaxestructurale.
Editions Klincksieck, Paris.Yamada, Hiroyasu and Yuji Matsumoto.2003.
Statistical dependency analysis withsupport vector machines.
In Proceedings ofthe 8th International Workshop on ParsingTechnologies (IWPT), pages 195?206, Nancy.553
