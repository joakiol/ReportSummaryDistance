Proceedings of the NAACL HLT 2010: Demonstration Session, pages 45?48,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn Overview of Microsoft Web N-gram Corpus and ApplicationsKuansan Wang        Christopher Thrasher       Evelyne ViegasXiaolong Li        Bo-june (Paul) HsuMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052, USAwebngram@microsoft.comAbstractThis document describes the properties andsome applications of the Microsoft Web N-gram corpus.
The corpus is designed to havethe following characteristics.
First, in contrastto static data distribution of previous corpusreleases, this N-gram corpus is made publiclyavailable as an XML Web Service so that itcan be updated as deemed necessary by theuser community to include new words andphrases constantly being added to the Web.Secondly, the corpus makes available varioussections of a Web document, specifically, thebody, title, and anchor text, as separates mod-els as text contents in these sections are foundto possess significantly different statisticalproperties and therefore are treated as distinctlanguages from the language modeling pointof view.
The usages of the corpus are demon-strated here in two NLP tasks: phrase segmen-tation and word breaking.1 IntroductionSince Banko and Brill?s pioneering work almost adecade ago (Banko and Brill 2001), it has beenwidely observed that the effectiveness of statisticalnatural language processing (NLP) techniques ishighly susceptible to the data size used to developthem.
As empirical studies have repeatedly shownthat simple algorithms can often outperform theirmore complicated counterparts in wide varieties ofNLP applications with large datasets, many havecome to believe that it is the size of data, not thesophistication of the algorithms that ultimatelyplay the central role in modern NLP (Norvig,2008).
Towards this end, there have been consider-able efforts in the NLP community to gather everlarger datasets, culminating the release of the Eng-lish Giga-word corpus (Graff and Cieri, 2003) andthe 1 Tera-word Google N-gram (Thorsten andFranz, 2006) created from arguably the largest textsource available, the World Wide Web.Recent research, however, suggests that studieson the document body alone may no longer be suf-ficient in understanding the language usages in ourdaily lives.
A document, for example, is typicallyassociated with multiple text streams.
In additionto the document body that contains the bulk of thecontents, there are also the title and the file-name/URL the authors choose to name the docu-ment.
On the web, a document is often linked withanchor text or short messages from social networkapplications that other authors use to summarizethe document, and from the search logs we learnthe text queries formulated by the general public tospecify the document.
A large scale studies revealthat these text streams have significantly differentproperties and lead to varying degrees of perfor-mance in many NLP applications (Wang et al2010, Huang et al 2010).
Consequently from thestatistical modeling point of view, these streamsare better regarded as composed in distinctive lan-guages and treated as such.This observation motivates the creation of Mi-crosoft Web N-gram corpus in which the materialsfrom the body, title and anchor text are madeavailable separately.
Another notable feature of thecorpus is that Microsoft Web N-gram is availableas a cross-platform XML Web service1 that can befreely and readily accessible by users through theInternet anytime and anywhere.
The service archi-tecture also makes it straightforward to perform on1 Please visit http://research.microsoft.com/web-ngram formore information.45demand updates of the corpus with the new con-tents that can facilitate the research on the dynam-ics of the Web.22 General Model InformationLike the Google N-gram, Microsoft Web N-gramcorpus is based on the web documents indexed bya commercial web search engine in the EN-USmarket, which, in this case, is the Bing servicefrom Microsoft.
The URLs in this market visitedby Bing are at the order of hundreds of billion,though the spam and other low quality web pagesare actively excluded using Bing?s proprietary al-gorithms.
The various streams of the web docu-ments are then downloaded, parsed and tokenizedby Bing, in which process the text is lowercasedwith the punctuation marks removed.
However, nostemming, spelling corrections or inflections areperformed.Unlike the Google N-gram release which con-tains raw N-gram counts, Microsoft Web N-gramprovides open-vocabulary, smoothed back-off N-gram models for the three text streams using theCALM algorithm (Wang and Li, 2009) that dy-namically adapts the N-gram models as web doc-uments are crawled.
The design of CALM ensuresthat new N-grams are incorporated into the modelsas soon as they are encountered in the crawling andbecome statistically significant.
The models aretherefore kept up-to-date with the web contents.CALM is also designed to make sure that dupli-cated contents will not have outsized impacts inbiasing the N-gram statistics.
This property is use-ful as Bing?s crawler visits URLs in parallel and onthe web many URLs are pointing to the same con-tents.
Currently, the maximum order of the N-gramavailable is 5, and the numbers of N-grams areshown in Table 1.Table 1: Numbers of N-grams for various streamsBody Title Anchor1-gram 1.2B 60M 150M2-gram 11.7B 464M 1.1B3-gram 60.1B 1.4B 3.2B4-gram 148.5B 2.3B 5.1B5-gram 237B 3.8B 8.9B2 The WSDL for the web service is located at http://web-ngram.research.microsoft.com/Lookup.svc/mex?wsdl.CALM algorithm adapts the model from a seedmodel based on the June 30, 2009 snapshot of theWeb with the algorithm described and imple-mented in the MSRLM toolkit (Nguyen et al2007).
The numbers of tokens in the body, title,and anchor text in the snapshot are of the order of1.4 trillion, 12.5 billion, and 357 billion, respec-tively.3 Search Query SegmentationIn this demonstration, we implement a straightfor-ward algorithm that generates hypotheses of thesegment boundaries at all possible placements in aquery and rank their likelihoods using the N-gramservice.
In other words, a query of T terms willhave 2T-1 segmentation hypotheses.
Using the fam-ous query ?mike siwek lawyer mi?
described in(Levy, 2010) as an example, the likelihoods andthe segmented queries for the top 5 hypotheses areshown in Figure 1.Body:Title:Anchor:Figure 1: Top 5 segmentation hypotheses underbody, title, and anchor language models.As can be seen, the distinctive styles of the lan-guages used to compose the body, title, and theanchor text contribute to their respective modelsproducing different outcomes on the segmentation46task, many of which research issues have been ex-plored in (Huang et al 2010).
It is hopeful that therelease of Microsoft Web N-gram service can ena-ble the community in general to accelerate the re-search on this and related areas.4 Word Breaking DemonstrationWord breaking is a challenging NLP task, yet theeffectiveness of employing large amount of data totackle word breaking problems has been demon-strated in (Norvig, 2008).
To demonstrate the ap-plicability of the web N-gram service for the workbreaking problem, we implement the rudimentaryalgorithm described in (Norvig, 2008) and extendit to use body N-gram for ranking the hypotheses.In essence, the word breaking task can be regardedas a segmentation task at the character level wherethe segment boundaries are delimitated by whitespaces.
By using a larger N-gram model, the democan successfully tackle the challenging wordbreaking examples as mentioned in (Norvig, 2008).Figure 2 shows the top 5 hypotheses of the simplealgorithm.
We note that the word breaking algo-rithm can fail to insert desired spaces into stringsthat are URL fragments and occurred in the docu-ment body frequently enough.Figure 2: Norvig's word breaking examples (Norvig,2008) re-examined with Microsoft Web N-gram47Two surprising side effects of creating the N-gram models from the web in general are worthnoting.
First, as more and more documents containmulti-lingual contents, the Microsoft Web N-gramcorpus inevitably include languages other than EN-US, the intended language.
Figure 3 shows exam-ples in German, French and Chinese (Romanized)each.Figure 3: Word breaking examples for foreign lan-guages: German (top), French and Romanized Chi-neseSecondly, since the web documents contain manyabbreviations that are popular in short messaging,the consequent N-gram model lends the simpleword breaking algorithm to cope with the commonshort hands surprisingly well.
An example that de-codes the short hand for ?wait for you?
is shown inFigure 4.Figure 4: A word breaking example on SMS-stylemessage.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.Michel Banko and Eric Brill.
2001.
Mitigating the pauc-ity-of-data problem: exploring the effect of trainingcorpus size on classifier performance for natural lan-guage processing.
Proc.
1st Internal Conference onhuman language technology research, 1-5, San Di-ego, CA.David Graff and Christopher Cieri.
2003.
English Gi-gaword.
Linguistic Data Consortium, ISBN: 1-58563-260-0, Philadelphia.Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,Kuansan Wang, and Fritz Behr.
2010.
Exploring webscale language models for search query processing.In Proc.
19th International World Wide Web Confe-rence (WWW-2010), Raleigh, NC.Steven Levy, 2010.
How Google?s algorithm rules theweb.
Wired Magazine, February.Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.2007.
MSRLM: a scalable language modeling tool-kit.
Microsoft Research Technical Report MSR-TR-2007-144.Peter Norvig.
2008.
Statistical learning as the ultimateagile development tool.
ACM 17th Conference on In-formation and Knowledge Management IndustryEvent (CIKM-2008), Napa Valley, CA.Kuansan Wang, Jianfeng Gao, and Xiaolong Li.
2010.The multi-style language usages on the Web andtheir implications on information retrieval.
In sub-mission.Kuansan Wang, Xiaolong Li and Jianfeng Gao, 2010.Multi-style language model for web scale informa-tion retrieval.
In Proc.
ACM 33rd Conference on Re-search and Development in Information Retrieval(SIGIR-2010), Geneva, Switzerland.Kuansan Wang and Xiaolong Li, 2009.
Efficacy of aconstantly adaptive language modeling technique forweb scale application.
In Proc.
IEEE InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP-2009), Taipei, Taiwan.48
