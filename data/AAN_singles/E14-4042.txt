Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215?220,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsOne Sense per Tweeter ... and Other Lexical Semantic Tales of TwitterSpandana Gella, Paul Cook and Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbournesgella@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.netAbstractIn recent years, microblogs such as Twit-ter have emerged as a new communicationchannel.
Twitter in particular has becomethe target of a myriad of content-basedapplications including trend analysis andevent detection, but there has been littlefundamental work on the analysis of wordusage patterns in this text type.
In thispaper ?
inspired by the one-sense-per-discourse heuristic of Gale et al.
(1992)?
we investigate user-level sense distri-butions, and detect strong support for ?onesense per tweeter?.
As part of this, we con-struct a novel sense-tagged lexical sampledataset based on Twitter and a web corpus.1 IntroductionSocial media applications such as Twitter enableusers from all over the world to create and shareweb content spontaneously.
The resulting user-generated content has been identified as havingpotential in a myriad of applications includingreal-time event detection (Petrovi?c et al., 2010),trend analysis (Lau et al., 2012) and natural dis-aster response co-ordination (Earle et al., 2010).However, the dynamism and conversational na-ture of the text contained in social media cancause problems for traditional NLP approachessuch as parsing (Baldwin et al., 2013), mean-ing that most content-based approaches use sim-ple keyword search or a bag-of-words representa-tion of the text.
This paper is a first step towardsfull lexical semantic analysis of social media text,in investigating the sense distribution of a rangeof polysemous words in Twitter and a general-purpose web corpus.The primary finding of this paper is that thereare strong user-level lexical semantic priors inTwitter, equivalent in strength to document-levellexical semantic priors, popularly termed the ?onesense per discourse?
heuristic (Gale et al., 1992).This has potential implications for future applica-tions over Twitter which attempt to move beyond asimple string-based meaning representation to ex-plicit lexical semantic analysis.2 Related WorkThe traditional approach to the analysis of word-level lexical semantics is via word sense dis-ambiguation (WSD), where usages of a givenword are mapped onto discrete ?senses?
in a pre-existing sense inventory (Navigli, 2009).
The mostpopular sense inventory used in WSD research hasbeen WordNet (Fellbaum, 1998), although its fine-grained sense distinctions have proven to be diffi-cult to make for human annotators and WSD sys-tems alike.
This has resulted in a move towardsmore coarse-grained sense inventories (Palmer etal., 2004; Hovy et al., 2006; Navigli et al., 2007),or alternatively away from pre-existing sense in-ventories altogether, towards joint word sense in-duction (WSI) and disambiguation (Navigli andVannella, 2013; Jurgens and Klapaftis, 2013).Two heuristics that have proven highly powerfulin WSD and WSI research are: (1) first sense tag-ging, and (2) one sense per discourse.
First sensetagging is based on the observation that sense dis-tributions tend to be Zipfian, such that if the pre-dominant or ?first?
sense can be identified, simplytagging all occurrences of a given word with thissense can achieve high WSD accuracy (McCarthyet al., 2007).
Unsurprisingly, there are significantdifferences in sense distributions across domains(cf.
cloud in the COMPUTING and METEOROLOG-ICAL domains), motivating the need for unsuper-vised first sense learning over domain-specific cor-pora (Koeling et al., 2005).One sense per discourse is the observation thata given word will often occur with a single senseacross multiple usages in a single document (Gale215et al., 1992).
Gale et al.
established the heuristicon the basis of 9 ambiguous words using a coarse-grained sense inventory, finding that the probabil-ity of a given pair of usages of a word taken from agiven document having the same sense was 94%.However, Krovetz (1998) found that for a fine-grained sense inventory, only 67% of words exhib-ited the single-sense-per-discourse property for alldocuments in a corpus.A radically different view on WSD is word us-age similarity, whereby two usages of a givenword are rated on a continuous scale for similar-ity, in isolation of any sense inventory (Erk et al.,2009).
Gella et al.
(2013) constructed a word us-age similarity dataset for Twitter messages, anddeveloped a topic modelling approach to the task,building on the work of Lui et al.
(2012).
To thebest of our knowledge, this has been the only at-tempt to carry out explicit word-level lexical se-mantic analysis of Twitter text.3 Dataset ConstructionIn order to study sense distributions of words inTwitter, we need a sense inventory to annotateagainst, and also a set of Twitter messages to an-notate.
Further, as a point of comparison for thesense distributions in Twitter, we require a secondcorpus; here we use the ukWaC (Ferraresi et al.,2008), a corpus built from web documents.For the sense inventory, we chose the Macmil-lan English Dictionary Online1(MACMILLAN,hereafter), on the basis of: (1) its coarse-grainedgeneral-purpose sense distinctions, and (2) its reg-ular update cycle (i.e.
it contains many recently-emerged senses).
These criteria are importantin terms of inter-annotator agreement (especiallyas we crowdsourced the sense annotation, as de-scribed below) and also sense coverage.
Theother obvious candidate sense inventory which po-tentially satisfied these criteria was ONTONOTES(Hovy et al., 2006), but a preliminary sense-tagging exercise indicated that MACMILLAN bet-ter captured Twitter-specific usages.Rather than annotating all words, we opted fora lexical sample of 20 polysemous nouns, as listedin Table 1.
Our target nouns were selected to spanthe high- to mid-frequency range in both Twitterand the web corpus, and have at least 3 MACMIL-LAN senses.
The average sense ambiguity is 5.5.1http://www.macmillandictionary.comband bar case charge dealdegree field form function issuejob light match panel paperposition post rule sign trackTable 1: The 20 target nouns used in this research3.1 Data SamplingWe sampled tweets from a crawl made using theTwitter Streaming API from January 3, 2012 toFebruary 29, 2012.
The web corpus was built fromukWaC (Ferraresi et al., 2008), which was basedon a crawl of the .uk domain from 2007.
In con-trast to ukWaC, the tweets are not restricted to doc-uments from any particular country.For both corpora, we first selected only theEnglish documents using langid.py, an off-the-shelf language identification tool (Lui and Bald-win, 2012).
We next identified documents whichcontained nominal usages of the target words,based on the POS tags supplied with the corpusin the case of ukWaC, and the output of the CMUARK Twitter POS tagger v2.0 (Owoputi et al.,2012) in the case of Twitter.For Twitter, we are interested in not just theoverall lexical distribution of each target noun,but also per-user lexical distributions.
As such,we construct two Twitter-based datasets: (1)TWITTERRAND, a random sample of 100 usages ofeach target noun; and (2) TWITTERUSER, 5 usagesof each target noun from each member of a ran-dom sample of 20 Twitter users.
Naively select-ing users for TWITTERUSERwithout filtering re-sulted in a preponderance of messages from ac-counts that were clearly bots, e.g.
from commer-cial sites with a single post per item advertised forsale, with artificially-skewed sense distributions.In order to obtain a more natural set of messagesfrom ?real?
people, we introduced a number ofuser-level filters, including removing users whoposted the same message with different user men-tions or hashtags, and users who used the targetnouns more than 50 times over a 2-week period.From the remaining users, we randomly selected20 users per target noun, resulting in 20 nouns ?20 users ?
5 messages = 2000 messages.For ukWaC, we similarly constructed twodatasets: (1) UKWACRAND, a random sampleof 100 usages of each target noun; and (2)UKWACDOC, 5 usages of each target noun from 20documents which contained that noun in at least216Figure 1: Screenshot of a sense annotation HIT for position5 sentences.
5 such sentences were selected forannotation, resulting in a total of 20 nouns ?
20documents ?
5 sentences = 2000 sentences.3.2 Annotation SettingsWe sense-tagged each of the four datasets usingAmazon Mechanical Turk (AMT).
Each HumanIntelligence Task (HIT) comprised 5 occurrencesof a given target noun, with the target noun high-lighted in each.
Sense definitions and an exam-ple sentence (where available) were provided fromMACMILLAN.
Turkers were free to select multi-ple sense labels where applicable, in line with bestpractice in sense labelling (Mihalcea et al., 2004).We also provided an ?Other?
sense option, in caseswhere none of the MACMILLAN senses were ap-plicable to the current usage of the target noun.
Ascreenshot of the annotation interface for a singleusage is provided in Figure 1.Of the five sentences in each HIT, one was aheldout example sentence for one of the senses ofthe target noun, taken from MACMILLAN.
Thisgold-standard example was used exclusively forquality assurance purposes, and used to filter theannotations as follows:1.
Accept all HITs from Turkers whose gold-standard tagging accuracy was ?
80%;2.
Reject all HITs from Turkers whose gold-standard tagging accuracy was ?
20%;3.
Otherwise, accept single HITs with correctgold-standard sense tags, or at least 2/4 (non-gold-standard) annotations in common withTurkers who correctly annotated the gold-standard usage; reject any other HITs.This style of quality assurance has been shownto be successful for sense tagging tasks on AMT(Bentivogli et al., 2011; Vuurens et al., 2011), andresulted in us accepting around 95% of HITs.In total, the annotation was made up of 500HITs (= 2000/4 usages per HIT) for each of thefour datasets, each of which was annotated by5 Turkers.
Our analysis of sense distribution isbased on only those HITs which were accepted inaccordance with the above methodology, exclud-ing the gold-standard items.
We arrive at a singlesense label per usage by unweighted voting acrossthe annotations, allowing multiple votes from asingle Turker in the case of multiple sense annota-tions.
In this, the ?Other?
sense label is consideredas a discrete sense label.Relative to the majority sense, inter-annotatoragreement post-filtering was respectably high interms of Fleiss?
kappa at ?
= 0.64 for bothUKWACRANDand UKWACDOC.
For TWITTERUSER,the agreement was actually higher at ?
= 0.71, butfor TWITTERRANDit was much weaker, ?
= 0.47.All four datasets have been released for pub-lic use: http://www.csse.unimelb.edu.au/~tim/etc/twitter_sense.tgz.4 AnalysisIn TWITTERUSER, the proportion of users who useda target noun with one sense across all 5 usagesranged from 7/20 for form to 20/20 for degree, atan average of 65%.
That is, for 65% of users, agiven noun (with average polysemy = 5.5 senses)is used with the same sense across 5 separate mes-sages.
For UKWACDOCthe proportion of docu-ments with a single sense of a given target noun217Partition Agreement (%)Gale et al.
(1992) document 94.4TWITTERUSERuser 95.4TWITTERUSER?
62.9TWITTERRAND?
55.1UKWACDOCdocument 94.2UKWACDOC?
65.9UKWACRAND?
60.2Table 2: Pairwise agreement for each dataset,based on different partitions of the data (???
indi-cates no partitioning, and exhaustive comparison)across all usages ranged from 1/20 for case to20/20 for band, at an average of 63%.
As such,the one sense per tweeter heuristic is at least asstrong as the one sense per discourse heuristic inUKWACDOC.Looking back to the original work of Gale etal.
(1992), it is important to realise that their re-ported agreement of 94% was calculated pairwisebetween usages in a given document.
When werecalculate the agreement in TWITTERUSERandUKWACDOCusing this methodology, as detailedin Table 2 (calculating pairwise agreement withinpartitions of the data based on ?user?
and ?docu-ment?, respectively), we see that the numbers forour datasets are very close to those of Gale et al.on the basis of more than twice as many nouns,and many more instances per noun.
Moreover, theone sense per tweeter trend again appears to beslightly stronger than the one sense per discourseheuristic in UKWACDOC.One possible interpretation of these results isthat they are due to a single predominant sense,common to all users/documents rather than user-specific predominant senses.
To test this hy-pothesis, we calculate the pairwise agreement forTWITTERUSERand UKWACDOCacross all anno-tations (without partitioning on user/document),and also for TWITTERRANDand UKWACRAND.The results are, once again, presented in Ta-ble 2 (with partition indicated as ???
for therespective datasets), and are substantially lowerin all cases (< 66%).
This indicates that thefirst sense preference varies considerably betweenusers/documents.
Note that the agreement isslightly lower for TWITTERRANDand UKWACRANDsimply because of the absence of the biasing effectfor users/documents.Comparing TWITTERRANDand UKWACRAND,there were marked differences in first sense pref-erences, with 8/20 of the target nouns having adifferent first sense across the two corpora.
Onesurprising observation was that the sense distri-butions in UKWACRANDwere in general moreskewed than in TWITTERRAND, with the entropy ofthe sense distribution being lower (= more biased)in UKWACRANDfor 15/20 of the target nouns.All datasets included instances of ?Other?senses (i.e.
usages which didn?t conform to anyof the MACMILLAN senses), with the highest rel-ative such occurrence being in TWITTERRANDat12.3%, as compared to 6.6% for UKWACRAND.Interestingly, the number of such usages inthe user/document-biased datasets was aroundhalf these numbers, at 7.4% and 3.6% forTWITTERUSERand UKWACDOC, respectively.5 DiscussionIt is worthwhile speculating why Twitter userswould have such a strong tendency to use a givenword with only one sense.
This could arise inpart due to patterns of user behaviour, in a givenTwitter account being used predominantly to com-ment on a favourite sports team or political events,and as such is domain-driven.
Alternatively, it canperhaps be explained by the ?reactive?
nature ofTwitter, in that posts are often emotive responsesto happenings in a user?s life, and while differentthings excite different individuals, a given individ-ual will tend to be excited by events of similarkinds.
Clearly more research is required to testthese hypotheses.One highly promising direction for this researchwould be to overlay analysis of sense distributionswith analysis of user profiles (e.g.
Bergsma et al.
(2013)), and test the impact of geospatial and soci-olinguistic factors on sense preferences.
We wouldalso like to consider the impact of time on the onesense per tweeter heuristic, and consider whether?one sense per Twitter conversation?
also holds.To summarise, we have investigated sense dis-tributions in Twitter and a general web corpus,over both a random sample of usages and a sampleof usages from a single user/document.
We foundstrong evidence for Twitter users to use a givenword with a single sense, and also that individualfirst sense preferences differ between users, sug-gesting that methods for determining first senseson a per user basis could be valuable for lexical se-mantic analysis of tweets.
Furthermore, we foundthat sense distributions in Twitter are overall lessskewed than in a web corpus.218ReferencesTimothy Baldwin, Paul Cook, Marco Lui, AndrewMacKinlay, and Li Wang.
2013.
How noisy so-cial media text, how diffrnt social media sources?In Proceedings of the 6th International Joint Con-ference on Natural Language Processing (IJCNLP2013), pages 356?364, Nagoya, Japan.Luisa Bentivogli, Marcello Federico, GiovanniMoretti, and Michael Paul.
2011.
Getting expertquality from the crowd for machine translationevaluation.
Proceedings of the MT Summmit,13:521?528.Shane Bergsma, Mark Dredze, Benjamin Van Durme,Theresa Wilson, and David Yarowsky.
2013.Broadly improving user classification viacommunication-based name and location clus-tering on Twitter.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (NAACL HLT 2013), pages1010?1019, Atlanta, USA.Paul Earle, Michelle Guy, Richard Buckmaster, ChrisOstrum, Scott Horvath, and Amy Vaughan.
2010.OMG earthquake!
can Twitter improve earth-quake response?
Seismological Research Letters,81(2):246?251.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word us-ages.
In Proceedings of the Joint conference of the47th Annual Meeting of the Association for Compu-tational Linguistics and the 4th International JointConference on Natural Language Processing of theAsian Federation of Natural Language Processing(ACL-IJCNLP 2009), pages 10?18, Singapore.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,USA.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.In Proceedings of the 4th Web as Corpus Workshop:Can we beat Google, pages 47?54, Marrakech, Mo-rocco.William A Gale, Kenneth W Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the workshop on Speech and NaturalLanguage, pages 233?237.Spandana Gella, Paul Cook, and Bo Han.
2013.
Unsu-pervised word usage similarity in social media texts.In Proceedings of the Second Joint Conference onLexical and Computational Semantics (*SEM 2013),pages 248?253, Atlanta, USA.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% solution.
In Proceedings ofthe Main Conference on Human Language Technol-ogy Conference of the North American Chapter ofthe Association of Computational Linguistics, pages57?60, New York City, USA.David Jurgens and Ioannis Klapaftis.
2013.
Semeval-2013 task 13: Word sense induction for graded andnon-graded senses.
In Proceedings of the 7th In-ternational Workshop on Semantic Evaluation (Se-mEval 2013), pages 290?299, Atlanta, USA.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings of the2005 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2005), pages 419?426, Vancouver, Canada.Robert Krovetz.
1998.
More than one sense per dis-course.
NEC Princeton NJ Labs., Research Memo-randum.Jey Han Lau, Nigel Collier, and Timothy Baldwin.2012.
On-line trend analysis with topic models:#twitter trends detection topic model online.
In Pro-ceedings of the 24th International Conference onComputational Linguistics (COLING 2012), pages1519?1534, Mumbai, India.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2012)Demo Session, pages 25?30, Jeju, Republic of Ko-rea.Marco Lui, Timothy Baldwin, and Diana McCarthy.2012.
Unsupervised estimation of word usage simi-larity.
In Proceedings of the Australasian LanguageTechnology Workshop 2012 (ALTW 2012), pages33?41, Dunedin, New Zealand.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,4(33):553?590.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Proceedings of Senseval-3: Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 25?28, Barcelona,Spain.Roberto Navigli and Daniele Vannella.
2013.SemEval-2013 task 11: Word sense induction anddisambiguation within an end-user application.
InProceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), pages 193?201, Atlanta, USA.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
SemEval-2007 task 07: Coarse-grained English all-words task.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations, pages 30?35, Prague, Czech Republic.219Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys (CSUR), 41(2):10.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, and Nathan Schneider.
2012.
Part-of-speech tagging for Twitter: Word clusters andother advances.
Technical Report CMU-ML-12-107, Machine Learning Department, Carnegie Mel-lon University.Martha Palmer, Olga Babko-Malaya, and Hoa TrangDang.
2004.
Different sense granularities for differ-ent applications.
In Proceedings of the HLT-NAACL2004 Workshop: 2nd Workshop on Scalable Natu-ral Language Understanding, pages 49?56, Boston,USA.Sasa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with appli-cation to twitter.
In Proceedings of Human Lan-guage Technologies: The 11th Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL HLT 2010),pages 181?189, Los Angeles, USA.Jeroen Vuurens, Arjen P de Vries, and Carsten Eick-hoff.
2011.
How much spam can you take?
an anal-ysis of crowdsourcing results to increase accuracy.In Proc.
ACM SIGIR Workshop on Crowdsourcingfor Information Retrieval (CIR 2011), pages 21?26.220
