Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012?1022,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLatent Vector Weighting for Word Meaning in ContextTim Van de CruysRCEALUniversity of Cambridgetv234@cam.ac.ukThierry PoibeauLaTTiCe, UMR8094CNRS & ENSthierry.poibeau@ens.frAnna KorhonenComputer Laboratory & RCEALUniversity of Cambridgealk23@cam.ac.ukAbstractThis paper presents a novel method for the com-putation of word meaning in context.
We makeuse of a factorization model in which words, to-gether with their window-based context wordsand their dependency relations, are linked tolatent dimensions.
The factorization model al-lows us to determine which dimensions are im-portant for a particular context, and adapt thedependency-based feature vector of the wordaccordingly.
The evaluation on a lexical substi-tution task ?
carried out for both English andFrench ?
indicates that our approach is able toreach better results than state-of-the-art meth-ods in lexical substitution, while at the sametime providing more accurate meaning repre-sentations.1 IntroductionAccording to the distributional hypothesis of meaning(Harris, 1954), words that occur in similar contextstend to be semantically similar.
In the spirit of this bynow well-known adage, numerous algorithms havesprouted up that try to capture the semantics of wordsby looking at their distribution in texts, and compar-ing those distributions in a vector space model.Up till now, the majority of computational ap-proaches to semantic similarity represent the mean-ing of a word as the aggregate of the word?s contexts,and hence do not differentiate between the differentsenses of a word.
The meaning of a word, however, islargely dependent on the particular context in whichit appears.
Take for example the word work in sen-tences (1) and (2).
(1) The painter?s recent work is a classic exampleof art brut.
(2) Equal pay for equal work!The meaning of work is quite different in both sen-tences.
In sentence (1), work refers to the product of acreative act, viz.
a painting.
In sentence (2), it refersto labour carried out as a source of income.
TheNLP community?s standard answer to the ambiguityproblem has always been some flavour of word sensedisambiguation (WSD), which in its standard formboils down to choosing the best-possible fit from apre-defined sense inventory.
In recent years, it hasbecome clear that this is in fact a very hard task tosolve for computers and humans alike (Ide and Wilks,2006; Erk et al, 2009; Erk, 2010).With these findings in mind, researchers havestarted looking at different methods to tackle lan-guage?s ambiguity, ranging from coarser-grainedsense inventories (Hovy et al, 2006) and gradedsense assignment (Erk and McCarthy, 2009), overword sense induction (Schu?tze, 1998; Pantel and Lin,2002; Agirre et al, 2006), to the computation of indi-vidual word meaning in context (Erk and Pado?, 2008;Thater et al, 2010; Dinu and Lapata, 2010).
Thisresearch inscribes itself in the same line of thought,in which the meaning disambiguation of a word isnot just the assignment of a pre-defined sense; in-stead, the original meaning representation of a wordis adapted ?on the fly?, according to ?
and specifi-cally tailored for ?
the particular context in whichit appears.
To be able to do so, we build a factor-ization model in which words, together with theirwindow-based context words and their dependency1012relations, are linked to latent dimensions.
The factor-ization model allows us to determine which dimen-sions are important for a particular context, and adaptthe dependency-based feature vector of the word ac-cordingly.
The evaluation on a lexical substitutiontask ?
carried out for both English and French ?
indi-cates that our method is able to reach better resultsthan state-of-the-art methods in lexical substitution,while at the same time providing more accurate mean-ing representations.The remainder of this paper is organized as follows.In section 2, we present some earlier work that isrelated to the research presented here.
Section 3describes the methodology of our method, focusingon the factorization model, and the computation ofmeaning in context.
Section 4 presents a thoroughevaluation on a lexical substitution task, both forEnglish and French.
The last section then drawsconclusions, and presents a number of topics thatdeserve further exploration.2 Related workOne of the best known computational models of se-mantic similarity is latent semantic analysis ?
LSA(Landauer and Dumais, 1997; Landauer et al, 1998).In LSA, a term-document matrix is created, that con-tains the frequency of each word in a particular doc-ument.
This matrix is then decomposed into threeother matrices with a mathematical factorization tech-nique called singular value decomposition (SVD).The most important dimensions that come out of theSVD are said to represent latent semantic dimensions,according to which nouns and documents can be rep-resented more efficiently.
Our model also appliesa factorization technique (albeit a different one) inorder to find a reduced semantic space.The nature of a word?s context is a determiningfactor in the kind of the semantic similarity that is in-duced.
A broad context window (e.g.
a paragraph ordocument) yields broad, topical similarity, whereasa small context window yields tight, synonym-likesimilarity.
This has lead a number of researchers(e.g.
Lin (1998)) to use the dependency relations thata particular word takes part in as context features.An overview of dependency-based semantic spacemodels is given in Pado?
and Lapata (2007).A number of researchers have exploited the no-tion of context to differentiate between the differentsenses of a word in an unsupervised way (a task la-beled word sense induction or WSI).
Schu?tze (1998)proposed a context-clustering approach, in whichcontext vectors are created for the different instancesof a particular word, and those contexts are groupedinto a number of clusters, representing the differentsenses of the word.
The context vectors are rep-resented as second-order co-occurrences (i.e.
thecontexts of the target word are similar if the wordsthey in turn co-occur with are similar).
Van de Cruys(2008) proposed a model for sense induction basedon latent semantic dimensions.
Using a factorizationtechnique based on non-negative matrix factorization,the model induces a latent semantic space accordingto which both dependency features and broad con-textual features are classified.
Using the latent space,the model is able to discriminate between differentword senses.
Our approach makes use of a simi-lar factorization model, but we extend the approachwith a probabilistic framework that is able to adaptthe original vector according to the context of theinstance.Recently, a number of models emerged that aimto model the individual meaning of words in context.Erk and Pado?
(2008, 2009) make use of selectionalpreferences to express the meaning of a word in con-text; the meaning of a word in the presence of anargument is computed by multiplying the word?s vec-tor with a vector that captures the inverse selectionalpreferences of the argument.
Thater et al (2009) andThater et al (2010) extend the approach based on se-lectional preferences by incorporating second-orderco-occurrences in their model; their model allowsfirst-order co-occurrences to act as a filter upon thesecond-order vector space, which allows for the com-putation of meaning in context.Erk and Pado?
(2010) propose an exemplar-basedapproach, in which the meaning of a word in contextis represented by the activated exemplars that aremost similar to it.
And Mitchell and Lapata (2008)propose a model for vector composition, focusing onthe different functions that might be used to combinethe constituent vectors.
Their results indicate thata model based on pointwise multiplication achievesbetter results than models based on vector addition.Finally, Dinu and Lapata (2010) propose a proba-bilistic framework that models the meaning of words1013as a probability distribution over latent dimensions(?senses?).
Contextualized meaning is then mod-eled as a change in the original sense distribution.The model presented in this paper bears some resem-blances to their approach; however, while their ap-proach computes the contextualized meaning directlywithin the latent space, our model exploits the latentspace to determine the features that are importantfor a particular context, and adapt the original (out-of-context) dependency-based feature vector of thetarget word accordingly.
This allows for a more pre-cise and more distinct computation of word meaningin context.
Secondly, Dinu and Lapata use window-based context features to build their latent model,while our approach combines both window-basedand dependency-based features.3 Methodology3.1 Non-negative Matrix FactorizationOur model uses non-negative matrix factorization(Lee and Seung, 2000) in order to find latent dimen-sions.
There are a number of reasons to prefer NMFover the better known singular value decompositionused in LSA.
First of all, NMF allows us to mini-mize the Kullback-Leibler divergence as an objec-tive function, whereas SVD minimizes the Euclideandistance.
The Kullback-Leibler divergence is bettersuited for language phenomena.
Minimizing the Eu-clidean distance requires normally distributed data,and language phenomena are typically not normallydistributed.
Secondly, the non-negative nature ofthe factorization ensures that only additive and nosubtractive relations are allowed.
This proves partic-ularly useful for the extraction of semantic dimen-sions, so that the NMF model is able to extract muchmore clear-cut dimensions than an SVD model.
Andthirdly, the non-negative property allows the resultingmodel to be interpreted probabilistically, which is notstraightforward with an SVD factorization.The key idea is that a non-negative matrix A isfactorized into two other non-negative matrices, Wand HAi?j ?Wi?kHk?j (1)where k is much smaller than i, j so that both in-stances and features are expressed in terms of a fewcomponents.
Non-negative matrix factorization en-forces the constraint that all three matrices must benon-negative, so all elements must be greater than orequal to zero.Using the minimization of the Kullback-Leibler di-vergence as an objective function, we want to find thematrices W and H for which the Kullback-Leiblerdivergence between A and WH (the multiplicationof W and H) is the smallest.
This factorization iscarried out through the iterative application of updaterules.
Matrices W and H are randomly initialized,and the rules in 2 and 3 are iteratively applied ?
alter-nating between them.
In each iteration, each vector isadequately normalized, so that all dimension valuessum to 1.Ha?
?
Ha??iWiaAi?(WH)i?
?kWka(2)Wia ?Wia??Ha?Ai?(WH)i?
?vHav(3)3.2 Combining syntax and context wordsUsing an extension of non-negative matrix factor-ization (Van de Cruys, 2008), it is possible tojointly induce latent factors for three different modes:words, their window-based context words, and theirdependency-based context features.
As input tothe algorithm, three matrices are constructed thatcapture the pairwise co-occurrence frequencies forthe different modes.
The first matrix contains co-occurrence frequencies of words cross-classifiedby dependency-based features, the second matrixcontains co-occurrence frequencies of words cross-classified by words that appear in the word?s contextwindow, and the third matrix contains co-occurrencefrequencies of dependency-based features cross-classified by co-occurring context words.
NMF isthen applied to the three matrices, and the separatefactorizations are interleaved (i.e.
the results of theformer factorization are used to initialize the factor-ization of the next matrix).
A graphical represen-tation of the interleaved factorization algorithm isgiven in figure 1.When the factorization is finished, the three dif-ferent modes (words, window-based context wordsand dependency-based features) are all representedaccording to a limited number of latent factors.1014= xW H= xV G= xU FjiskijkAwords xdependency relationsBwords xcontext wordsCcontext words xdependency relationsk kkkijisjs sFigure 1: A graphical representation of the interleavedNMFThe factorization that comes out of the NMF modelcan be interpreted probabilistically (Gaussier andGoutte, 2005; Ding et al, 2008).
More specifically,we can transform the factorization into a standardlatent variable model of the formp(wi, dj) =K?z=1p(z)p(wi|z)p(dj |z) (4)by introducing two K ?K diagonal scaling matricesX and Y, such that Xkk = ?iWik and Ykk =?jHkj .
The factorization WH can then be rewrittenasWH = (WX?1X)(YY?1H)= (WX?1)(XY)(Y?1H)(5)such that WX?1 represents p(wi|z), (Y?1H)T rep-resents p(dj |z), and XY represents p(z).
UsingBayes?
theorem, it is now straightforward to deter-mine p(z|wi) and p(z|dj).p(z|wi) =p(wi|z)p(z)p(wi)(6)p(z|dj) =p(dj |z)p(z)p(dj)(7)3.3 Meaning in Context3.3.1 OverviewUsing the results of the factorization model de-scribed above, we can now adapt a word?s feature vec-tor according to the context in which it appears.
Intu-itively, the contextual features of the word (i.e.
thewindow-based context words or dependency-basedcontext features) pinpoint the important semantic di-mensions of the particular instance, creating a proba-bility distribution over latent factors.
For a number ofcontext words of a particular instance, we determinethe probability distribution over latent factors giventhe context, p(z|C), as the average of the contextwords?
probability distributions over latent factors(equation 8).p(z|C) =?wi?C p(z|wi)|C| (8)The probability distribution over latent factorsgiven a number of dependency-based context featurescan be computed in a similar fashion, replacing wiwith dj .
Additionally, this step allows us to combineboth windows-based context words and dependency-based context features in order to determine the latentprobability distribution (e.g.
by taking a linear com-bination).The resulting probability distribution over latentfactors can be interpreted as a semantic fingerprint ofthe passage in which the target word appears.
Usingthis fingerprint, we can now determine a new prob-ability distribution over dependency features giventhe context.p(d|C) = p(z|C)p(d|z) (9)The last step is to weight the original probabilityvector of the word according to the probability vectorof the dependency features given the word?s context,by taking the pointwise multiplication of probabilityvectors p(d|wi) and p(d|C).p(d|wi, C) = p(d|wi) ?
p(d|C) (10)Note that this final step is a crucial one in our ap-proach.
We do not just build a model based on latentfactors, but we use the latent factors to determinewhich of the features in the original word vector arethe salient ones given a particular context.
This al-lows us to compute an accurate adaptation of theoriginal word vector in context.Also note the resemblance to Mitchell and Lap-ata?s best scoring vector composition model which,likewise, uses pointwise multiplication.
However,1015the model presented here has two advantages.
Firstof all, it allows to take multiple context features intoaccount, each of which contributes to the probabilitydistribution over latent factors.
Secondly, the targetword and its features do not need to live in the samevector space (i.e.
they do not need to be defined ac-cording to the same features), as the connections andthe appropriate weightings are determined throughthe latent model.3.3.2 ExampleLet us exemplify the procedure with an example.Say we want to compute the distributionally similarwords to the noun record in the context of examplesentences (3) and (4).
(3) Jack is listening to a record.
(4) Jill updated the record.First, we extract the context features for both in-stances, in this case C1 = {listen?1prep(to)} for sen-tence (3), and C2 = {update?1obj} for sentence (4).1Next, we compute p(z|C1) and p(z|C2) ?
the proba-bility distributions over latent factors given the con-text ?
by averaging over the latent probability dis-tributions of the individual context features.2 Usingthese probability distributions over latent factors, wecan now determine the probability of each depen-dency feature given the different contexts ?
p(d|C1)and p(d|C2).The former step yields a general probability dis-tribution over dependency features that tells us howlikely a particular dependency feature is given thecontext that our target word appears in.
Our last stepis now to weight the original probability vector ofthe target word (the aggregate of dependency-basedcontext features over all contexts of the target word)according to the new distribution given the contextin which the target word appears.
For the first sen-tence, features associated with the music sense ofrecord (or more specifically, the dependency featuresassociated with latent factors that are related to thefeature {listen?1prep(to)}) will be emphasized, while1In this example we use dependency features, but the compu-tations are similar for window-based context words.2In this case, the sets of context features contain only oneitem, so the average probability distribution of the sets is just thelatent probability distribution of their respective item.features associated with unrelated latent factors areleveled out.
For the second sentence, features thatare associated with the administrative sense of record(dependency features associated with latent factorsthat are related to the feature {update?1obj}) are em-phasized, while unrelated features are played down.We can now return to our original matrix A andcompute the top similar words for the two adaptedvectors of record given the different contexts, whichyields the results presented below.1.
recordN , C1: album, song, recording, track, cd2.
recordN , C2: file, datum, document, database,list4 EvaluationIn this section, we present a thorough evaluation ofthe method described above, and compare it withrelated methods for meaning computation in context.In order to test the applicability of the method tomultiple languages, we present evaluation results forboth English and French.4.1 DatasetsFor English, we make use of the SEMEVAL 2007 En-glish Lexical Substitution task (McCarthy and Nav-igli, 2007; McCarthy and Navigli, 2009).
The task?sgoal is to find suitable substitutes for a target word ina particular context.
The complete data set contains200 target words (about 50 for each part of speech,viz.
nouns, verbs, adjectives, and adverbs).
Eachtarget word occurs in 10 different sentences, whichyields a total of 2000 sentences.
Five annotators pro-vided suitable substitutes for each target word in thedifferent contexts.For French, we developed a small-scale lexical sub-stitution task ourselves, closely following the guide-lines of the original English task.
We manually se-lected 10 ambiguous French nouns, and for each nounwe selected 10 different sentences from the FRWaCcorpus (Baroni et al, 2009).
Four different nativeFrench speakers were then asked to provide suitablesubstitutes for the nouns in context.33The task is provided as supplementary material to this paper;it is also available from the first author?s website.10164.2 Implementational detailsThe model for English has been trained on part of theUKWaC corpus (Baroni et al, 2009), covering about500M words.
The corpus has been part of speechtagged and lemmatized with Stanford Part-Of-SpeechTagger (Toutanova and Manning, 2000; Toutanovaet al, 2003), and parsed with MaltParser (Nivre etal., 2006) trained on sections 2-21 of the Wall StreetJournal section of the Penn Treebank extended withabout 4000 questions from the QuestionBank4, sothat dependency triples could be extracted.
The sen-tences of the English lexical substitution task havebeen tagged, lemmatized and parsed in the same way.The model for French has been trained on the Frenchversion of Wikipedia (?
100M words), parsed withthe FRMG parser (Villemonte de La Clergerie, 2010)for French.For English, we built different models for eachpart of speech (nouns, verbs, adjectives and adverbs),which yields four models in total.
For each model, thematrices needed for our interleaved NMF factoriza-tion are extracted from the corpus.
The noun model,for example, was built using 5K nouns, 80K depen-dency relations, and 2K context words5 (excludingstop words) with highest frequency in the trainingset, which yields matrices of 5K nouns ?
80K de-pendency relations, 5K nouns ?
2K context words,and 80K dependency relations ?
2K context words.The models for the three other parts of speech wereconstructed in a similar vein.
For French, we onlyconstructed a model for nouns, as our lexical substi-tution task for French is limited to this part of speech.The interleaved NMF model was carried out usingK = 600 (the number of factorized dimensions inthe model), and applying 100 iterations.6 The inter-leaved NMF algorithm was implemented in Matlab;the preprocessing scripts and scripts for vector com-putation in context were written in Python.
Cosinewas used as a similarity measure.4http://maltparser.org/mco/english_parser/engmalt.html5We used a fairly large, paragraph-like window of four sen-tences.6We experimented with different values (in the range 300?1500) for K, but the models did not seem to improve muchbeyond K = 600; hence, we stuck with 600 factors, due tospeed and memory advantages of a lower number of factors.4.3 MeasuresUp till now, most researchers have interpreted thelexical substitution task as a ranking problem, inwhich the possible substitutes are given beforehandand the goal is to rank the substitutes according totheir suitability in a particular context, so that soundsubstitutes are given a higher rank than their non-suitable counterparts.
This means that all possiblesubstitutes for a given target word (extracted from thegold standard) are lumped together, and the systemthen has to produce a ranking for the complete set ofsubstitutes.We also adopt this approach in our evaluationframework, but we complement it with the originalevaluation measures of the lexical substitution task,in which the system is not given a list of possible sub-stitutes beforehand, but has to come up with the suit-able candidates itself.
This is a much harder task, butwe believe that such an approach is more compellingin assessing the system?s ability to induce a propermeaning representation for word usage in context.We coin the former approach paraphrase ranking,and the latter one paraphrase induction.
In the nextparagraphs, we will describe the actual evaluationmeasures that have been used for both approaches.Paraphrase ranking Following Dinu and Lapata(2010), we compare the ranking produced by ourmodel with the gold standard ranking using Kendall?s?b (which is adjusted for ties).
For reasons of com-parison, we also compute general average precision(GAP, Kishida (2005)), which was used by Erk andPado?
(2010) and Thater et al (2010) to evaluate theirrankings.
Differences between models are tested forsignificance using stratified shuffling (Yeh, 2000),using a standard number of 10000 iterations.We compare the results for paraphrase ranking totwo different baselines.
The first baseline is a ran-dom one, in which the gold standard is comparedto an arbitrary ranking.
The second baseline is adependency-based vector space model that does nottake the context of the particular instance into ac-count (and thus returns the same ranking for eachinstance of the target word).
This is a fairly competi-tive baseline, as noted by other researchers (Erk andPado?, 2008; Thater et al, 2009; Dinu and Lapata,2010).1017Paraphrase induction To evaluate the system?sability to come up with suitable substitutes fromscratch, we use the measures designed to evaluatesystems that took part in the original English lexicalsubstitution task (McCarthy and Navigli, 2007).
Twodifferent measures were used, which were coinedbest and out-of-ten (oot).
The strict best measureallows the system to give as many candidate substi-tutes as it considers appropriate, but the credit foreach correct substitute is divided by the total numberof guesses.
Recall is then calculated as the averageannotator response frequency of substitutes found bythe system over all items T.Rbest =?s?M?G f(s)|M | ?
?s?G f(s)(11)whereM is the system?s candidate list7,G is the gold-standard data, and f(s) is the annotator responsefrequency of the candidate.The out-of-ten measure is more liberal; it allowsthe system to give up to ten substitutes, and the creditfor each correct substitute is not divided by the totalnumber of guesses.
The more liberal measure wasintroduced to account for the fact that the lexicalsubstitution task?s gold standard is susceptible to aconsiderate amount of variation, and there is only alimited number of annotators.P10 =?s?M?G f(s)?s?G f(s)(12)where M is the system?s list of 10 candidates, andG and f(s) are the same as above.
Because we onlyuse the best guess with Rbest, the two measures areexactly the same except for the number of candidatesM .4.4 Results4.4.1 EnglishTable 1 presents the paraphrase ranking results ofour approach, comparing them to the two baselinesand to a number of previous approaches to meaningcomputation in context.The first two models represent our baselines.
Thefirst baseline is the random baseline, where the can-didate substitutes are ranked randomly (?b close to7In our evaluations, we calculate best using the system?s bestguess only, so the candidate list contains only one item.model ?b GAPrandom -0.61 29.98vectordep 16.57 45.08EP09 ?
32.2 HEP10 ?
39.9 HTFP ?
45.94HDL 16.56 41.68NMFcontext 20.64??
47.60?
?NMFdep 22.49??
48.97?
?NMFc+d 22.59??
49.02?
?Table 1: Kendall?s ?b and GAP paraphrase ranking scoresfor the English lexical substitution task.
Scores markedwith ?H?
are copied from the authors?
respective papers.Scores marked with ????
are statistically significant withp < 0.01 compared to the second baseline.zero indicates that there is no correlation).
The sec-ond baseline is a standard dependency-based vectorspace model, which yields the same ranking for allinstances of a target word.
Note that the second base-line is a rather competitive one.The next four models represent previous ap-proaches to meaning computation in context.
EP09is Erk and Pado?s (2009) selectional preference ap-proach; EP10 is Erk and Pado?s (2010) exemplar-based approach; TFP stands for Thater et al?s (2010)approach; and DL is Dinu and Lapata?s (2010) latentmodeling approach.
The results are reproduced fromtheir respective papers, except for Dinu and Lapata?sapproach, which we reimplemented ourselves.8 Notethat the reproduced results (EP09, EP10 and TFP) arenot entirely comparable, because the authors only usea subset of the lexical substitution task.The last three models are instantiations of our ap-proach: NMFcontext is a model that uses window-based context features, NMFdep is a model that usesdependency-based context features, and NMFc+d isa model that uses a linear combination of window-based and dependency-based context features, givingequal weight to both.The three instantiations of our approach reach bet-ter results than all previous approaches.
Moreover,our approach is the only one able to significantly8The original paper reports a slightly lower ?b of 16.01 fortheir best scoring model.1018beat our second (competitive) baseline of a stan-dard dependency-based vector model.
Comparingour three instantiations, the model that combineswindow-based context and dependency-based con-text scores best, closely followed by the dependency-based model.
The model that only uses window-based context gets the lowest score of the three, butis still fairly competitive compared to the previousapproaches.
The differences between the models arestatistically significant (p < 0.01), except for thedifference between NMFdep and NMFc+d.model n v a rvectordep 15.85 11.68 16.71 25.29NMFcontext 20.58 16.24 21.00 27.22NMFdep 21.96 17.33 24.57 28.16NMFc+d 22.68 17.47 23.84 28.66Table 2: Kendall?s ?b paraphrase ranking scores for theEnglish lexical substitution task across different parts ofspeechTable 2 shows the performance of the three modelinstantiations on paraphrase ranking across differentparts of speech.
The results largely confirm tenden-cies reported by other researchers (cfr.
Dinu andLapata (2010)), viz.
that verbs are the most difficult,followed by nouns and adjectives.
These parts ofspeech also benefit the most from the use of a contex-tualized model.
Adverbs are easier, but there is lessto be gained from using contextualized models.model Rbest P10vectordep 8.78 30.21DL 1.06 7.59KU 20.65 46.15IRST2 20.33 68.90NMFcontext 8.81 30.49NMFdep 7.73 26.92NMFc+d 8.96 29.26Table 3: Rbest and P10 paraphrase induction scores forthe English lexical substitution taskTable 3 shows the performance of the differentmodels on the paraphrase induction task.
Noteonce again that our baseline vectordep ?
a simpledependency-based vector space model ?
is a highlycompetitive one.
NMFcontext and NMFc+d are able toreach marginally better results, but the differences arenot statistically significant.
However, all of our mod-els are able to reach much better results than Dinuand Lapata?s approach.
The results indicate that ourapproach, after vector adaptation in context, is stillable to provide accurate similarity calculations acrossthe complete word space.
While other algorithms areable to rank candidate substitutes at the expense ofaccurate similarity calculations, our approach is ableto do both.
This is one of the important advantagesof our approach.For reasons of comparison, we also included thescores of the best performing models that partici-pated in the SEMEVAL 2007 lexical substitution task(KU (Yuret, 2007) and IRST2 (Giuliano et al, 2007),which got the best scores for Rbest and P10, respec-tively).
These models reach better scores comparedto our models.
Note, however, that all participantsof the SEMEVAL 2007 lexical substitution task reliedon a predefined sense inventory (i.e.
WordNet, ora machine readable thesaurus).
Our system, on theother hand, induces paraphrases in a fully unsuper-vised way.
To our knowledge, this is the first time afully unsupervised system is tested on the paraphraseinduction task.model n v a rvectordep 31.66 23.53 29.91 38.43NMFcontext 33.73??
25.21?
28.58 36.45NMFdep 31.40 25.97??
20.56 31.48NMFc+d 33.37?
25.99??
24.20 35.81Table 4: P10 paraphrase induction scores for the Englishlexical substitution task across different parts of speech.Scores marked with ????
and ???
are statistically significantwith respectively p < 0.01 and p < 0.05 compared to thebaseline.Table 4 presents the results for paraphrase induc-tion (oot) across the different parts of speech.
Theresults indicate that paraphrase induction works bestfor nouns and verbs, with statistically significant im-provements over the baseline.
The differences amongthe models themselves are not significant.
Adjectivesand adverbs yield lower scores, indicating that their1019contextualization yields less precise vectors for mean-ing computation.
Note, however, that the NMFcontextmodel is still quite apt for meaning computation,yielding results that are only slightly lower than thedependency-based vector space model.4.4.2 FrenchThis section presents the results on the French lex-ical substitution task.
Table 5 presents the results forparaphrase ranking, while table 6 shows the models?performance on the paraphrase induction task.model Kendall?s ?b GAPvectordep 7.79 36.46DL 17.99 41.73NMFcontext 18.63 44.96NMFdep 17.15 44.66NMFc+d 18.40 43.14Table 5: Kendall?s ?b and GAP paraphrase ranking scoresfor the French lexical substitution taskThe results for paraphrase ranking in French (ta-ble 5) show similar tendencies as the results for En-glish: all of our models are able to improve signifi-cantly over the dependency-based vector space base-line.
Note, however, thar our models generally scorea bit lower compared to the English results.
This dropin performance is not present for Dinu and Lapata?smodel.
The difference might be due to the differ-ence in corpora size: for the method to operate at fullpower, we need to make a good estimate of the co-occurrences of three modes (words, window-basedcontext words and dependency-based features), andthus our methods requires a significant amount ofdata.
Nevertheless, our approach still yields the bestresults, with NMFcontext as the best scoring model.Finally, the results for paraphrase induction inFrench (table 6) interestingly show a significant andlarge improvement over the baseline.
The improve-ments indicate once again that the models are ableto carry out precise similarity computations over thewhole word space, while at the same time providingan adequately adapted contextualized meaning vector.Dinu and Lapata?s model, which performs similaritycalculations in the latent space, is not able to provideaccurate word vectors, and thus perform worse at theparaphrase induction task.model Rbest P10vectordep 6.38 24.43DL 0.50 5.34NMFcontext 10.71 31.42NMFdep 9.65 28.52NMFc+d 10.64 35.32Table 6: Rbest and P10 paraphrase induction scores forthe French lexical substitution task5 ConclusionIn this paper, we presented a novel method for themodeling of word meaning in context.
We make useof a factorization model based on non-negative ma-trix factorization, in which words, together with theirwindow-based context words and their dependencyrelations, are linked to latent dimensions.
The factor-ization model allows us to determine which particulardimensions are important for a target word in a partic-ular context.
A key feature of the algorithm is that weadapt the original dependency-based feature vectorof the target word through the latent semantic space.By doing so, our model is able to make accurate simi-larity calculations for word meaning in context acrossthe whole word space.
Our evaluation shows that theapproach presented here is able to improve upon thestate-of-the art performance on paraphrase ranking.Moreover, our approach scores well for both para-phrase ranking and paraphrase induction, whereasprevious approaches only seem capable of improvingperformance on the former task at the expense of thelatter.During our research, a number of topics surfacedthat we consider worth exploring in the future.
Firstof all, we would like to further investigate the opti-mal configuration for combining window-based anddependency-based contexts.
At the moment, the per-formance of the combined model does not yield auniform picture.
The results might improve furtherif window-based context and dependency-based con-text are combined in an optimal way.
Secondly, wewould like to subject our approach to further evalu-ation, in particular on a number of different evalua-tion tasks, such as semantic compositionality.
Andthirdly, we would like to transfer the general ideaof the approach presented in this paper to a tensor-1020based framework (which is able to capture the multi-way co-occurrences of words, together with theirwindow-based and dependency-based context fea-tures, in a natural way) and investigate whether sucha framework proves beneficial for the modeling ofword meaning in context.AcknowledgementsThe work reported in this paper was funded bythe Isaac Newton Trust (Cambridge, UK), theEU FP7 project ?PANACEA?, the EPSRC grantEP/G051070/1 and the Royal Society (UK).ReferencesEneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle, andAitor Soroa.
2006.
Two graph-based algorithms forstate-of-the-art wsd.
In Proceedings of the EmpiricalMethods in Natural Language Processing (EMNLP)Conference, pages 585?593, Sydney, Australia.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evaluation,43(3):209?226.Chris Ding, Tao Li, and Wei Peng.
2008.
On the equiv-alence between non-negative matrix factorization andprobabilistic latent semantic indexing.
ComputationalStatistics & Data Analysis, 52(8):3913?3927.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA, October.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 440?449, Suntec, Singapore.Katrin Erk and Sebastian Pado?.
2008.
A structured vectorspace model for word meaning in context.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 897?906, Waikiki,Hawaii, USA.Katrin Erk and Sebastian Pado?.
2009.
Paraphrase assess-ment in structured vector space: Exploring parametersand datasets.
In Proceedings of the Workshop on Geo-metrical Models of Natural Language Semantics, pages57?65, Athens, Greece.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedings ofthe ACL 2010 Conference Short Papers, pages 92?97,Uppsala, Sweden.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009.Investigations on word senses and word usages.
InProceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 10?18.Katrin Erk.
2010.
What is word meaning, really?
(andhow can distributional models help us describe it?).
InProceedings of the 2010 Workshop on GEometricalModels of Natural Language Semantics, pages 17?26.Eric Gaussier and Cyril Goutte.
2005.
Relation betweenPLSA and NMF and implications.
In Proceedings ofthe 28th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 601?602, Salvador, Brazil.Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.2007.
Fbk-irst: Lexical substitution task exploitingdomain and syntagmatic coherence.
In Proceedings ofthe Fourth International Workshop on Semantic Evalu-ations, pages 145?148.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, pages57?60, New York, New York, USA.Nancy Ide and Yorick Wilks.
2006.
Making Sense AboutSense.
In Word Sense Disambiguation: AlgorithmsAnd Applications, chapter 3.
Springer, Dordrecht.Kazuaki Kishida.
2005.
Property of average precision andits generalization: An examination of evaluation indi-cator for information retrieval experiments.
Technicalreport, National Institute of Informatics.Thomas Landauer and Susan Dumais.
1997.
A solutionto Plato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representation ofknowledge.
Psychology Review, 104:211?240.Thomas Landauer, Peter Foltz, and Darrell Laham.
1998.An Introduction to Latent Semantic Analysis.
Dis-course Processes, 25:295?284.Daniel D. Lee and H. Sebastian Seung.
2000.
Algorithmsfor non-negative matrix factorization.
In Advances inNeural Information Processing Systems 13, pages 556?562.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand 17th International Conference on ComputationalLinguistics (COLING-ACL98), Volume 2, pages 768?774, Montreal, Quebec, Canada.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 task 10: English lexical substitution task.
In1021Proceedings of the 4th International Workshop on Se-mantic Evaluations (SemEval-2007), pages 48?53.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language resources andevaluation, 43(2):139?159.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings of ACL-08: HLT, pages 236?244.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC-2006, pages 2216?2219.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 613?619, Edmonton, Alberta, Canada.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009.Ranking paraphrases in context.
In Proceedings of the2009 Workshop on Applied Textual Inference, pages44?47, Suntec, Singapore.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedings ofthe 48th Annual Meeting of the Association for Compu-tational Linguistics, pages 948?957, Uppsala, Sweden.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora(EMNLP/VLC-2000), pages 63?70.Kristina Toutanova, Dan Klein, Christopher Manning, andYoram Singer.
2003.
Feature-rich part-of-speech tag-ging with a cyclic dependency network.
In Proceedingsof HLT-NAACL 2003, pages 252?259.Tim Van de Cruys.
2008.
Using three way data for wordsense discrimination.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 929?936, Manchester.Eric Villemonte de La Clergerie.
2010.
Building factor-ized TAGs with meta-grammars.
In Proceedings ofthe 10th International Conference on Tree AdjoiningGrammars and Related Formalisms (TAG+10), pages111?118, New Haven, Connecticut, USA.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics,pages 947?953, Saarbru?cken, Germany.Deniz Yuret.
2007.
Ku: Word sense disambiguation bysubstitution.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations, pages 207?213.1022
