Coping With Ambiguity in a Large-Scale Machine Translation SystemKathryn  L. Baker ,  A lexander  M.  F ranz ,  Pamela  W. Jo rdan ,Teruko  Mi tamura ,  E r i c  H. Nyberg ,  3 rdCenter  for Mach ine  Trans lat ionCarneg ie  Mel lon  Un ivers i tyP i t tsburgh,  PA 15213Topical Paper: machine translation, parsingAbstractIn an interlingual knowledge-based machine trans-lation system, ambiguity arises when the source 1.qn-guage analyzer produces more than one interlinguaexpression for a source sentence.
This can have anegative impact on translation quality, since a tar-get sentence may be produced from an unintendedmeaning.
In this paper we describe the ,nethodsnsed in the KANT machine translation system toreduce or eliminate ambiguity in a large-scale ap-plication domain.
We also test these methods on alarge corpus of test sentences, in order to illustratehow the different disambiguation methods redtucethe average number of parses per sentence,1 In t roduct ionThe KANT system \[Mitamura etal., 1991\] is a system forKnowledge-basexl, Accurate Natural-language Translation.The system is used in focused technical domains for multi-lingual translation of controlled source language documents.KANT is an interlingua-based system: the sonrce languageanalyzer produces an interlingua expression for each sourcesentence, and this interlingua is processed to produce thecorresponding target sentence.
The problen3 el' ambiguityarises when the system produces more that~ ()tie interlinguarepresentation for a single input sentence.
If the goal is toautomate translation and produce output hat does not requirepost-editing, then the presence of ambiguity has a negativeimpact on translation quality, since a target sentence may heproduced from an unintended meaning.
When it is possibleto limit tile interpretations of a sentence to just those that arecoherent in the translation domain, then the accuracy of theMT system is enhanced.Ambiguity can occnr at different levels of processing insource analysis.
In this paper, we describe how we copewith ambiguity in the KANT controlled lexicon, grammar,and semantic domain model, and how these :ire designed toreduce or eliminate ambiguity in a given translation domain.2 Const ra in ing  the  Source  TextThe KANT domain lexicon and grammar are a constrainedsubset of the general source language lexicon and gra,nmar.The strategy of constraining the source text has three mainIIl-:igurc 1: The KANT Systemgoals.
First, it encourages clear and direct writing, whichis beneficial to both the reader of tile source text and to thetranslation process.
Second, it facilitates consistent writingamong tile many authors who use the system and across alldocument types.
And third, the selection of unambiguouswords :111(I constructions tobe used during authoring reducesthe necessity for ambiguity resolution during the auto,naticstages of processing.
It is important to reduce the processingoverhead associaled wilh amhiguity resolution in order tokeeptile system fast enough for on-line use.2.1 The l)omain LexicmlThe domain lexicon is built using corpt, s analysis.
Listsof terms, arranged by part of speech, are automatically ex-tracted from the corpus \[Mitamura etal., 1993\].
"File lexiconconsists of closed-class general words, open-class generalwords, idioms, and nomenclature phrases.
Closed-class gen-eral words (e.g.
the, with.
should) are taken from generalEnglish.
Open-class general words (e.g.
drain, run, hot) arelimited in the lexicon to one sense per part of speech withsome exceptions ~.
Idioms (e.g.
on and off) and nomencl>tnre phrases (e.g.
summing valve) are domain-specilic andare limited to those phrases identilied in the domain corpus.Phrases, too, are delined with a single sense.
Special vncab-t Far example, in the heavy-equipment lexicon, there are a fewhundred terms out of 60,000 which have more than one sense perpart of speech.90ulary items, including symbols, abbreviations, and the like,,are restricted in use and are chosen for the lexicon in collab-oration with domain experts.
Senses for prepositions, whichare highly ambiguous and context-dependent, are determined(luring processing using the semantic domain model (of.
Sec-tion 4).Nominal compounds in the domain may be several wordslong.
Because of the potential ambiguity associated wit h com-positional parsing of nominal compounds, non-productivenominal compounds are listed explicitly ill tile lexicon asidioms or nomenclature phrases.2.2 Controlled GrammarSome constructions in the general source l,'nlgtmge that arc in-herently ambiguous are excluded from the restricted grammar,since they may l~td to multiple analyses during processing:?
Conjunction of VPs, ADJs, or ADVs e.g.
*Extend andretract the cylinder.?
Pronominal reference, .g.
*Start the engine and keel) itrunning.?
Ellipsis, e.g.
reduced relative clauses: *the tools !~tfor the procedure?
Long-distance dependencies, snch as interrogatives andobject-gap relative clauses, e.g.
The parts which theservice representative ordered.?
Nominal compounding which is not explicitly coded inthe phrasal lexicon.On the other h,'md, tim grammar inchules the following con-structions:?
Active, passive and imperative sentences, e.g.
Start theengine.?
Conjunction of NPs, PPs or Ss.
Sentences may be con-joined using coordinate or subordinate con jr, notions, e.g.If you are on the last parameter, ~zen lhe program pro-ceeds to the lop.?
Subject-gap relative clauses, e.g.
The service represen-tative can determine the parts which are faulty.Tile recommendations i  tile controlled grammar includeguidelines for authoring, such as how to rewrile a text fromgeneral English into the domain language.
Authors are ad-'vised, for example, to choose the most concise terms availablein the lexicon and to rewrite long, conjoined sentences intoshort, simple ones.
The recommendations are useful bothfor rewriting old text and creating new text (set l:igure 2 forexamples).Example 1: Rewrite Anaphnric Use (1t' NumeralsProblematic Text:Suggested Rewrite:Loosen tile smaller (me first.Loosen the smaller bolt ftrst.Example 2: Use Concise VocabularyProblematic Text: The parts must beput ba_ck toget h!
'L.Suggested Rewrite: The parts must be (easse~tbl#d.Figure 2: Grammar  Recommendatiml Examl)les2.3 SGML Text Markupq'he grammar makes use of Standard Generalized MarkupLanguage (SGML) text markl,p tags.
The set of markup tagsfor our applicatiou were developed in conjunction with do-,nain experts.
A set of domain-specific tags is used not onlyto demarcate tile text but also to identify tile content of poten-tially ambiguous expressions, and to help during vocabl,larychecking.
For example, at the lexical level, number tagsidentify numerals as diagram callouts, part munbers, productmodel numbers, or parts of measurement exl)ressions.
Atthe syntactic level, rules for tag combinations restrict howphrases rnay be constructed, aswith tagged part rmmbers an(lpart names (see Figure 3 for an example).
'\['tie <p~l/"t;no> 4S152-1  </parLno> <parLr~amo>Hose  A~{sornt ) \ ]y  </parLname> <Cd\ ] IouL> l</ca  l lout :> el  t.he <pa l t r lo> 5'\['65-'\]q< / \[)El #'L NO> < f)/l \[ \[ lID, Ill(}-" \[~i~ El k (.
~ COl"tLr o \[ G lTOklp<~/~)~I/~LEIglmQZ > IIILI~;L nOW k)() connOcLed  Lo  Lho<parLno> 4K2986 </par t :no> <partzname>Arl(:!lOi TOO </D~lrt.r/,lm~?
:, .t.
'igure 3: Sample S(;ML Text Mark-Up3 Granun 'w Des ign  I ssuesThe parser in KANT is based on the "Universal Parser"\[Tonfita nd Carbonell, 19871.
"File gramnmr consists ofcontext-free rules that define tile input's constitt,ent s ruc-ture (c-structure) and these rules are annotated with con-straint equations that define the input's functional structure(f-structure).
'l'omita's parser compiles the gratnnmr into anl.R-table, and the constraint equations into Lisp code.
Al-though this compilation results in f.'lst run-time parsing, theneed to minimize ambiguity still exists.One source of ambiguity is the attachment site for a prepo-sitional phrase, llowever, many of the PP attachments areencoded irectly.in the gramma," because tile syntactic on-text indicates an unanfl)iguous attaclunent site.
For example:+ A partitive where the PP attaches to the noun: a gallonof antifreeze.)
) )  ?
A pre-sentential I P where tile I \[ attaches to the sentence:For this test, ensttre thor a signal line is connected fromlhe pump outpul to the pump compensator.?
A PI' attaches to the verb be when there is no predicaleadjective: The trm:'k is in the shop.?
A ditransitive verb where the PP attaches to the verb:Give your suggestions to tile dealer.,, A stand-alone PP inside ;m SGML tag such as QUAL-II"IER where tile PP attaches to tile MDLDESC tagcontents: Inspect <mdldesc> all track-type trac-tors <qualifier> with hydraulic heads </qualifier></mdldesc>.3.1 l'assive vs. Cnpt,l'lr with Participial?There are many adjectives in English that have tile same form.
as ,'m -ed participle, l:or example:7"tie radius is poorly formed.
(adjective)The calibration mode is enabled by moving therocker switch.
(participle)i"R} distinguish the qdjectival from the participial form wehave added two heuristics to tile constraint rules of the gram-mar.
The litst is to use verb class mapping information, If the91verb is classified as being more active than stative, then tilepassive reading is preferred.
So, for example, an intransitiveverb would indicate an adjectival reading:The display is faded.
(adjective)The second heuristic uses the notion of "quasi-agents".There are several prepositions that can introduce "quasi-agents" \[Quirk et al, 197211, such as: about, at, over, to,with.
If the domain model indicates that the -ed verb is apossible attachment site for a prepositional phrase occurringin the sentence, then the passive reading is preferred.These two heuristics are incorporated into tile constraintsof rules involving predicate adjectives.
If the -ed feral isclassified as active, or if there is at PP in the sentence thatcan attach to the -ed verb form, tfien tile adjectival readingis ruled out.
In the constraints of rules for the passive, timpassive reading is ruled out if the -ed form is classified assmtive.3.2 Adverb or Adjective?For tile most part, eacfi word in the system is limited to onemeaning per part of speeclt.
So while we have nearly elimi-nated one source of lexical ambiguity, there is still the l)roblemof ambiguity between the various parts of speech for a par-ticuhlr word.
While ambiguity between, lbr example, a nounand a verb is usually resolved by the syntactic context, parts ofspeech that participate in similar contexts are still a problem.For example, the content of the SGML tag, POSITION, canbe an adjective or adverb phrase and "as \[ <adj >l<ad v>\] as"can contain either an adjective or an adverb.
This means thatan input such as "as fast as" would have two analyses.
WeImve found witll our domain that tile COtTeCt hing to do isto prefer the adverb reading.
We put this preference directlyinto the constraints of rules involvingadjectives forwhich thesame context allows an adverb.
If the word is also an ad-verb then tim adjective rule will fail.
This allows tile adverbreading to be preferred.4 Semantic Donmin ModelWe have implemented a practical method for integrating se-mantic rules intoan LR parser.
The resulting system combinesthe merits of a semantic domain nlodel with the generality andwide coverage of syntactic parsing, and is fast and efficientenough to remain practical.4.1 Interleaved vs. Sentence-final ConstraintsSome previous knowledge-l)ased natural hmglmge analysissystems have constructed tile semantic represent'ilion for thesentence in tandem with syntactic parsing, lit this schenlesemantic onstr;fints from tile domain model filter out se-mantically ill-lormed representations and kill tile associatedpro'sing path.
Examples include AIISITY tHirst, 19861 andKBMT-89 \[Goodman anti Nireuburg, 1991\].
Other inevioussystems have delayed semantic interpretation a d al)plicalionof semantic well-formedness constraints until after tile syn-tactic parse.Both of these schemes entail performance problems.
Thesolution to this probleln lies ill importing the right type andright amount of semantic information into syntactic lmrsing.Iu KANT, the relevant knowledge sonrces are reorganized intodata structures that arc optimized for ambiguity resolutionduring parsing.4.2 Example of Attachment AmbiguityTile knowledge-based disambiguation scheme covers Prepo-sitional Phrase attachment, Noun-Notre conlponnding, andAdjective-Noun attachment.
The remainder of this sectiondiscusses examples involving PP-attacl/m~nt.
The syntacticgrammar contains two rules that allow these attachments:VP ,---- VP PPNP ,--- NP PPConsider tim sentence Measure the voltage with the voltmeter.Syntactically, the PP with the voltmeter can modify either tileverb measure, or tile noun voltage.4.3 Slructure and Content of tile I)omain ModelWe use knowledge abol,t the domain to resolve ambiguitieslike PP-attachment.
Tile domain model cent;fins all of thesemantic oncepts in the domain.
Leaf concepts, such as*O-VOLTMETER, correspond closely to linguistic expres-sions.
The concepls are.
arranged in an inheritance hierarchy,and other concepts, uch as *O-MEASURING-DEVICE, rep-resent abstract concepts.
The domain model is implementedas a hierarchy of concepts.
Constraints on possible attributesof concepls, along with semantic onstraints on the fillers, areinherited through this hierarchy.
Figure 4 shows an example.?
( "a- Pl A 61~JSl I('-,~C'll O~ "N)(INglRtlMFNr ~'}- ME& ~ URDdE brr.
\[&~'l(l:') &B.AFigure 4: Excerp!
t'rnm l)unutiu Model4.4 Using Semantics in tile SyntaxIn order tO keel) parsing traclable, the domain model is con-suited at the earliest possible stage during parsing.
Everygrannnar ule that involves an attachment decision that issubject o knowledge-based disamhigv'ltion calls a functionthat consults the domain model, and allows the gramn/ar ruleto succeed only if the attachmcut is sclnantically licensed.The grammar formalism allows procedural calls to be madedirectly fron/tim gramnmr ules.
The function that performsor deuies attachment based on the domaiu model is calledsere-attach.The inpttts to the sem-at t  ach  function are the functiomllstructures (f-strttcturcs) lk)r the potential attachment site, tilestructure to be attached, and the type of attachment (e.g., PP= t:'repositional Phrase).
sere -a t tach  consults inlbrm,'ltionfrom the domain model to decide whether the attachment issemantically licensed.
This process is described in the nextsubsection.4.5 Steps in Sem,'mtle Disaml)iguation IThere are three main steps in selnanlic disambiguation ofpossib\]e syntactic attachments: (1) mapping from syntax to92semantic concepts using tile lexical mappiug rules; (2) check-ing inform,'ttkm from the domain model; and (3) determiningsemantic roles using tile semantic interpretation rules.
?7   v :iiiiFigure 5: Lexical Mapping RulesLexical Mapping Rules.
The first step is to real I fromsyntactic structures to semantic oncepts.
The lexical map-ping rnles associate syntactic lexicon entries with conceplsfrom the Domain Model (Figure 5).Domain Model.
"File second step consists in looking upthe appropriate concepts in the Domain Model (Figure 4).Semantic Interpretation Rules.
The third step consistsof consulting the semantic interpretation rules to determinewhether the concepts from tile sentence can lo,'m approl~ri-ate modilication relationships.
Semantic interpretation rulesdescribe the mapping from the syntactic representation to thefrmne-based semantic representation, An interpretation rubconsisls of a syntactic path (an index into tile f-structure), asemantic Imth (an index into tile senmntic frame), and an op.tional syntactic onstraint on the mapping rule.
For exmnple,below is mt interpretation rule for the INSTRUMENT role:( : :~yn-pat .h  IPP OBJ): sem~path  ZN.qTRUNENT: sys-consttraint((pp ((root (*OR* "wlth .... by")) })) )Eflicient Run-time Use.
In order to make this process asefticient as possible, aml to minimize delays during parsing,the knowledge described in this section is reorganized off-line I)cfore parsing.
The result of this reorganization aredata strtletnres known as S&*;'lalllic" restrictors.
The SelllLIIlliL'restrictors have three main properties:1.
They are indexed by head concept, and provide a list ofall approl)riate modiiiers.2.
All inheritance in the Domain Model is performed off-line, so that the restrictors contain all necessary informa..tion.3.
The semantic restrictors are stored in a Slmce-efficientstructure+shared manne, +.S Author  l ) i sambiguat io t !Once KANT has analyzed a source sentence and all l)OS-s ine disambiguations h;tve been performed, there may stillbe more than one interlingua representation for tim sentence.This occurs when the sentence is truly ambiguous, i.e., it hnsmore than one acceptable domain interpretation.
I  this case,KANT makes use of disambiguation by the author - -  tileambiguity is described to the author and the author is thenpmml)ted to select the desired interpretation.
The choice is"remembered" byplacing extra in fomuttion into tile input textat the point of alnbiguity.
There are two types of ambiguitycnrrently addressed by author disambit~uation:?
Lexical Ambiguity.
When more than one interlingua isproduced because a certain word or phrase ean be interpreted in more than one way (iv.
as two differentconcepts), then the author is prompted to select he de-sired meaning.
* Structural kmbigt.tity.
When more than one attachmentsite is possible for a phrase like a prel)o~ilional phrase,the different attachments are glossed for tile :luther, whois then prompted to select ile desired inteq)retation.Since author disambiguation is utilized only when the sen-tence cannot I)c disambiguated by other nteans, it will notoccur very frequently once tile system is complete.
On tileother hand, having such a mech,'mism available during systemdevelopment is very helpful, since it helps to point out wherethere is residu-d ambiguity left to be addressed by knowledgeSt} tlrce ieli neltlent.6 Test ing  l ) i saml f igua l ion  MethodsWhen disambiguation methods are int,oduced, the numberof parses per sentence can be reduced dramatically.
If weuse a general lexicon and grammar to parse texts lro\[n ~.1 spe-cialized dolnain corpus (rather than a general corpus),  thenmore lmrses will be assigned than those thai are desired inthe dOlnain.
Figure 6 illustrates how the successive introduc-tion of disambiguation ntethods reduces the set of l)ossiblcparses to just those desired in tile domain.
The smallest set ofinterpretations is that remaining after tile controlled lexicon,gla\[nllrar, seln\[illlic restrictions, and author disambigtmtionhave \[)cell applied; in practice this sel ,,viii contain just oneinterpretation, since the author will select only the intendedinterpretation.# Inlerprelalion~Ouin~l GeneralI N Inle llWUt aliot~ U ~+ing Dotu,~irl h l le lpte lu l lo l l l\[}i~ambi~ualiolLFollowia~ A~Jlllo?l:igure 6: Reducing the Set of Possible lnterl)retatinnsWc have experimented with tile KANT analyzer in order todetermine the effects of the different disambiguation strate-gies mentioned above.
We used a test suite containing 891sentences which is used fo, regression testing during systemdevelopment.
The sentences in the test suite range in lenglhfronl t word to over 25 words.General lexicon entries were derived automatically from theonline version of Wcbster's 7th dictionary.
Webster's includes55,000 reels that are in at least one open class category (verh,ram,, adjective, adverb).
One diclionary entry was createdfor each sense of one of these dalegories, This resulted in117,000 lexicon entries.
The constrained lexicon consists of10,000 words and 50,000 phrases talk)red to the application9.3domain.
For the results listed below, the "general lexicon"consists of the constrained lexicon plus the general entriesfrom Webster's.The constrained grammar has been tailored to the restrictedsource language for the domain (of.
Section 2).
In ,'tddition, itincludes a number of constraint annotations and parse prefer-ences that limit the number of ambiguous parses (cf.
Section3).
A general grammar was derived from the constrainedgrammar by removing most restrictions and constraints onspecific rules, leaving only the most general constraints suchas subject-verb agreement.When noun-noun compounding is allowed, sequences ofnouns may form NPs even if they ~u'e not listed as nomencla-ture phrases in the lexicon.
Each such sequence isonly parsedone way; the parser does not build different smmtures for thesequence of nouns, but just reads them into a list.In order to reduce the exponential complexity of some ofthe longer sentences, all test results were produced using the"shared packed forest" method of ambiguity packing for am-biguity internal to a sentence \[Tomita, 1986\].
The resultsfor "parses per sentence" is simply the average for all thesentences.Test LEX GRA N-N DM P1 GEN GEN YES NO 27.02 GEN GEN NO NO 10.23 GEN CON YES NO 8.44 CON GEN YES NO 1.75 CON GEN NO NO 1.66 CON CON NO YES 1.5LEX: Lexicon GEN: GeneralGILA: Grammar CON: ConstrainedN-N: Noun-Noun CompoundingDM: Semantic Restriction with l)omain ModelFigure 7: Testing Disambiguation Methods (12/17/93)The results of this testing ~u'e shown in Figure 7.
Test 1 isthe baseline result for parsing with a general lexicon, generalgrammar, noun-noun compounding and no semantic restric-tions.
As expected, the average number of parses per sentenceis quite high (27.0).
Limiting noun-noun compounding ('lest2) cuts this number by more than hal f, yielding 1(/.2 parses persentence.
Note that a similar effect is achieved if we run thetest with a controlled grammar and noun-noun compounding(Test 3, 8.4 parses per sentence).Constraining the lexicon seems to achieve the largest re-duction in the average number of parses per sentence (Tests 4,5, 6), with elimination of noun-notre compounding yieldingonly slight improvements when the lexicon has already beenrestricted.
As expected, the best results are achieved when thesystem is run with constrained lexicon and grammar, no noun-noun compounding, and semantic restriction with a domainmodel (Test 6).We expect that tile primary reason wily tile addition ofsemantic restrictions from a domain model does not have agreater impact is due to tile incomplete natttre of the domainmodel we used in the experiment.
The domain model used inthe experiment captures the domain relationships associatedwith prepositional phrase attachment to VP and object N1; lintthere are several areas of the domain model still under devel-opment.
When complete, these will further educe ambiguityby placing additional limitations on the following:?
The semantic lassification of words inside particularSGML tags;,, Attachment of prepositional phrases to suhject NP;?
Attachment of inlinitive clauses;,, Attachment of relative clauses.This testing has proved extremely useful ill prioritizing thelevel of effort expended oil different disambiguation methodsdnring system development.
As is often the case, theoreti-cally interesting or difticult issues (such as noun-noun com-ponnding) are reduced in effect when other domain-relatedrestrictions are put in place (such as a controlled lexicon).On the other hand, this type of testing can also identify Iheareas of the system (such as the semantic domain model)which are not reducing ambiguity as much as expected.
Inour ongoing work, we will complete the domain model forthe KANT he,'tvy-eqtfipment application in those areas men-tioned above; in the process, we expect o rednce the averagenumber of parses per sentence in the most constrained ease.7 Ac l (nowledgementsWe would like to thank Jaime Carbonell, Radha Rao, andTodd Kaufinann, 'rod all of ottr colleagttes on the KANTproject, inchtding James Altucher, Nicholas Brownlow, Mil-dred Galarza, Sue Hohn, Kathi lannamico, Kevin 1(eck, Mar-ion Kee, Sarah Law, John Leavitt, Daniela Lonsdale, DeryleLonsdale, Jeanne Mier, Venkatesh Narayan, Amalio Nieto,and Will Walker, our sponsors at Caterpillar Inc., and onrcolleagues at Carnegie Group.References\[Chevalier tal., 1978\] Chevalier, M., Danscre:m, J., andPoulin, G. (1978).
TatmHnetco: description dn systOne.Technical report, Groupe (le recherches pour la traductionautomatique, Universit6 de MontrS,ql.\[Goodman and Nirenburg, 19911 Goodman, K. and Niren-burg, S. (1991).
The KBMT Project: A Case Study inKnowledge-Based Machine Translation.
Morgan Kauf-ulan\[1, S:.ln Mated, CA.\[Hirst, 1986\] Hirst, G. (1986).
Semantic Inlerpretation andtile Resolution of Ambiguity.
Cambridge University Press,Cambridge.\[Mitamura etal., 1991\] Mitamura, T., Nybcrg, E., and Car-bonell, J.
(1991).
An efficient interlingua translation sys-tem for multi-lingtmldocument prodttction.
InProceedingsof Machine Translation Summit I11, Washington, DC.\[Mitamnra etal., 19931 Mitamura, T., Nyberg, E., anti Car-bonell, J.
(1993).
Automated corl)uS analysis ,'rod the acqui-sition of large, multi-lingual knowledge hases for MT.
In5th International Cotlference on 771eoretical and Method-ological Issues in Machine Translation, Kyoto, Japan.\[Quirk et al, 1972\] Quirk, R., Grccnbaum, S.. Leech, G., andSvartvik, J.
(1972).
A Grammar of Contemporary English.l_ongman Group UK Limited, Essex Engl,'md.\[Suzuki, 19921 Suzuki, M. (1992).
A method of utilizingdomain and langnage-sl)ecitic conslraints i l l  dialog trans-lation.
In Coling-92.\[Tonfita, 1986 \] Tom ira, M. (1986).
Efficient Parsing for Nat-ural Language.
Kluwer Academic Publishers, Boston,MA.\[qk)mita nd Carbonell, 19871 "ll)mita, M. and Carbonell, J.(1987).
"i'he Universal Parser architecurc Rlr Km)wledge-based Machine Translation.
Technical Report CMU-CMT-87-101, Center for Machine Translation, Carnegie MellonUniversity.94
