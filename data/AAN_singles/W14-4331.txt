Proceedings of the SIGDIAL 2014 Conference, pages 238?242,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsDetecting Inappropriate Clarification Requests in Spoken DialogueSystemsAlex Liu1, Rose Sloan2, Mei-Vern Then1, Svetlana Stoyanchev3,Julia Hirschberg1, Elizabeth Shriberg4Columbia University1, Yale University2, AT&T Labs Research3, SRI International4{al3037@columbia.edu, rose.sloan@yale.edu,mt2837@columbia.edu, sveta@research.att.com,julia@cs.columbia.edu, elizabeth.shriberg@sri.com}AbstractSpoken Dialogue Systems ask for clarifi-cation when they think they have misun-derstood users.
Such requests may dif-fer depending on the information the sys-tem believes it needs to clarify.
However,when the error type or location is misiden-tified, clarification requests appear confus-ing or inappropriate.
We describe a clas-sifier that identifies inappropriate requests,trained on features extracted from user re-sponses in laboratory studies.
This classi-fier achieves 88.5% accuracy and .885 F-measure in detecting such requests.1 IntroductionWhen Spoken Dialogue Systems (SDS) believethey have not understood a user, they generate re-quests for clarification.
For example, in the fol-lowing exchange, the System believes it has mis-understood the word Washington in the user?s ut-terance and asks a clarification question, prompt-ing the user to repeat the misrecognized word.User: I?d like a ticket to Washington.System: A ticket to where?User: Washington.Clarification requests may be generic or specificto the type and location of the information the sys-tem believes it has not recognized.
Targeted clar-ifications focus on a specific part of an utterance,as in the system?s question above.
They use under-stood portions of an utterance (?I?d like a ticketto?)
to query a misunderstood portion (?Wash-ington?).
Targeted clarification is a type of task-related request, which has been shown to be moreeffective and prevalent in human-human dialoguesthan more general clarification requests (Skantze,2005).
Such generic clarifications signal mis-understanding without identifying the type or lo-cation of the misunderstanding.
They often takethe form of a request to repeat or rephrase, e.g.
?please repeat?, ?please rephrase?, ?what didyou say?
?.Questions that address a particular type of mis-recognition come in several varieties.
Systemsmay ask reprise clarification questions, by repeat-ing a recognized portion of an utterance (Ginzburgand Cooper, 2004; Purver, 2004).
Systems mayalso request that users spell a word if they be-lieve the misrecognized word is a proper name,especially one that is not in its vocabulary (OOV).They may ask the user to provide a synonym forOOV terms that are not proper names.
Systemsmay also ask users to disambiguate homophones(e.g.
?Did you mean ?right?
as in correct or ?rite?
asin a ritual??).
They may request confirmation ex-plicitly (e.g.
?I heard you say Washington.
Is thatcorrect??
), or implicitly, by repeating the recog-nized information while asking a follow-up query(e.g.
?When do you want to go to Washington??
).Each request type may be appropriate in differentcircumstances.
However, when systems make in-appropriate requests to users, such as to rephrasea proper name or to confirm a statement that con-tains a misrecognized word, dialogues often goawry.
Therefore, it is extremely important for sys-tems to know when a request is inappropriate, sothat they can provide a different clarification re-quest or fall back to a more generic strategy.In this work, we develop a data-driven methodfor detecting inappropriate clarification requests.We have defined a list of inappropriate requesttypes and have collected a corpus of speaker re-sponses to both appropriate and inappropriate re-quests under laboratory conditions.
We use thiscorpus to train an inappropriate clarification clas-sifier to be used by a system after a user respondsto a system request, in order to determine whetherthe question was appropriate or not.
In Section 2,we describe previous research on error handling indialogue.
We describe our data set in Section 3 and238our approach in Section 4.
We present our evalua-tion results in Section 5.
We conclude in Section 6and discuss future directions.2 Related WorkToday?s SDS use generic approaches to clarifica-tion, asking the user to repeat or rephrase an en-tire utterance when the system believes it has notbeen understood correctly.
They use confidencescores on the ASR hypothesis to decide whetherto accept, reject, or ask for clarification (Bohusand Rudnicky, 2005).
Hypotheses with low scoresmay be confirmed and those with lower scores willtrigger a generic request for repetition or rephras-ing.
Researchers have found that the formulationof system prompts has a significant effect on thesuccess of SDS interaction.
Goldberg et al.
(2003)find that form of a clarification question affectsuser frustration and the consequent success of clar-ification subdialogue.
In previous work, we ex-plored the use of targeted reprise clarifications toimprove naturalness (Stoyanchev et al., 2014).Lendvai et al.
(2002) apply machine learningmethods to detect errors in human-machine di-alogue, focusing on predicting when a user ut-terance causes a misunderstanding.
Litman etal.
(2006) identify user corrections of the system?srecognition errors from speech prosody, ASR con-fidence scores, and the dialogue history.
In con-trast, we focus here on detecting when a systemclarification request is the cause of dialogue prob-lems.
We employ only lexical features here, aswell as the type of system request, to investigateuser responses to a wide variety of system re-quests, and to identify system errors in request for-mulation from user reactions.
In future work wewill include acoustic and prosodic features as well.3 DataOur data consists of spoken answers to clarifica-tion requests collected at Columbia University us-ing a simulated dialogue system in order to controlrecognition results and type of system response.The system displays a sentence and asks the userto read it.
The system then issues a pre-preparedclarification request, which may be appropriate orinappropriate, to which the user responds.
For ex-ample, in the following exchange, the system sim-ulates a misunderstanding of the word furor byasking a targeted reprise clarification question.User: We hope this won?t create a furor.System: Create a what?User: A furor, an uproar.The system issued six different types of clari-fication requests: confirmation; rephrase, spell, ordisambiguate part of the utterance; targeted repriseclarification; and a targeted-reprise-rephrase com-bination.
These request types were chosen basedon the types of requests made by the SRI Thunder-BOLT speech-to-speech translation system (Ayanand others, 2013).
Confirmation questions sim-ply ask the user to confirm an ASR hypothesis.Rephrase-part requests ask users to rephrase a spe-cific part of an utterance which is played backto the user.
Spell questions ask users to spell aword or phrase using the NATO alphabet.
Disam-biguate questions clarify ambiguous terms.
Tar-geted reprise clarification questions make use ofthe recognized portion of an utterance to query thepart that has been misrecognized based on the sys-tem?s assessment.
Targeted-reprise-rephrase re-quests are similar, with the additional request forthe user to rephrase a portion of the utterancebelieved to have been misrecognized, which isplayed to the user.Inappropriate requests in this study were de-fined as those that resulted from the Thunder-BOLT system?s incorrect identification of an er-ror segment or an error type.
For example, theclarification request ?Please say a different wordfor Afdhal?
is inappropriate since it asks for arephrasal of a proper name.
A request to spella very long phrase is also identified as inappro-priate since users have found this difficult, espe-cially when using the NATO alphabet.
Requeststo disambiguate in the system provide two possi-ble senses of the ambiguous word and are inap-propriate when the correct sense is not one of thetwo provided.
Targeted reprise clarification ques-tions are inappropriate when the error segment isnot correctly recognized and an errorful segmentis included in the question (e.g.
?The okay I zoowould like what??).
An appropriate question cor-rectly identifies the error segment or ambiguousterm and the error type.
For example, the ques-tion ?I think ?Afdhal?
is a name.
Please spell it?,would be appropriate when ?Afdhal?
is OOV be-cause it correctly targets the error and its type.For each clarification request type, except forconfirmation questions, which are always appro-priate, we created one or more types of inappro-priate requests for each of the conditions we ob-239served in dialogues collected with the Thunder-BOLT system.
For example, when the systemasks the user to rephrase a part of their utter-ance which the system believes to be a misrecog-nized non-proper-name, the question is appropri-ate when indeed that non-proper-name has beenmisrecognized.
However, the request will be in-appropriate when the hypothesized error segmentplayed back to the user is a partial word, a propername, an extended segment including a name, ora function word.
We created instances of eachof these conditions for our users to respond to inour experiment.
A full list of the system questiontypes and their appropriate and inappropriate con-ditions is provided in Table 3, in the Appendix.We prepared 228 clarification requests (84 appro-priate and 144 inappropriate), 12 for each of the 19categories listed in Table 3 in the Appendix, basedon data in the TRANSTAC dataset (Akbacak andothers, 2009).
Our subjects were 17 native Ameri-can English speakers, each of whom answered 114requests.
We recorded speakers?
answers to 714appropriate and 1224 inappropriate requests.
Asmost request types have more than one inappro-priate version, 63% of the requests in the data setare inappropriate.4 ExperimentWe used the Weka machine learning library (Wit-ten and Eibe, 2005) to train classifiers to predictwhether a clarification request was appropriate orinappropriate.
Our features were extracted fromtranscripts of user utterances, and included lexical,syntactic, numeric, and features from the output ofLinguistic Inquiry and Word Count (LIWC) (Pen-nebaker et al., 2007) as described in Table 1.We included unigram and bigram features, ex-cluding unigrams that appeared fewer than 3 timesin the dataset (11% of the unigrams), and bi-grams that appeared fewer than 2 times (25%),with thresholds set empirically.
LIWC featureswere extracted using the LIWC 2007 software,which includes lexical categories, such as articlesand negations, and psychological constructs, suchas affect and cognition.
In one version of thecorpus, we replaced sequences of user spellingswith the tag ?SPELL?
and disfluencies with thesymbol ?DISF?.
We used the Stanford POS tag-ger (Toutanova and others, 2003) to tag boththe original corpus as well as the modified ver-sion.
In the latter, we replaced the ?SPELL?
andFeature Descriptionword unigrams(Lexical)Count of unigramsword bigrams(Lexical)Count of bigramspos bigrams(Syntactic)Bigrams of POS assigned by Stanfordtaggerliwc LIWC Outputfunc ratio Proportion of function words in re-sponselen spell Total length of spelling sequences inresponserequest type Type of request preceding responseTable 1: Features used in Classification.?DISF?
tags with the symbols themselves.
Wealso mapped nine of the most frequent unigramsto their own POS classes, such as ?no?, ?not?,and ?neither?
to ?NO?
and ?word?
to ?WORD?.We then used counts of POS bigrams as a syn-tactic feature.
Additionally, as we observed thatresponses to inappropriate requests contained ahigher proportion of function words, we added thisas a numeric feature.
We also observed that aver-age length of responses to inappropriate requestswas greater than responses to appropriate ones,and we hypothesized this was in part due to in-appropriate requests to spell long phrases.
There-fore, we also used the length of the total spellingsequences, or the count of letters spelled out, as anumeric feature.
We also added type of clarifica-tion request as a feature since some requests areless likely to be inappropriate than others.
For ex-ample, we consider confirmation questions (?Didyou say .
.
.
??)
to always be appropriate.5 ResultsWe report classification results using Weka?s J48decision tree classifier with 10-fold cross valida-tion in Table 2, which outperformed JRip andLibSVM in our experiments.
Compared to themajority baseline of 63.2% accuracy and .489 F-measure, our classifier which uses all of the fea-tures in Table 1 achieves a significant improve-ment, with an accuracy of 88.5% and an F-measure of .885.
A baseline method that usesonly system request type feature (Req.
type base-line) achieves accuracy of 73.7% and F-measureof .686, which is significantly below the perfor-mance of the trained classifier.
To identify themost important features in predicting inappropri-ate requests, we iteratively removed a single fea-ture from the full feature set and re-evaluated pre-diction accuracy.
Table 2 shows absolute decrease240Features Acc (%) P/R/F-MeasureMajority baseline 63.2 * 0.399/0.632/0.489Req.
type baseline 73.7 * 0.814/0.737/0.686All Features 88.5 0.885/0.885/0.885less request type ?7.6 * ?0.076less liwc ?2.3 ?0.023less pos bigrams ?2.0 ?0.020less word unigrams ?0.4 ?0.004less func ratio ?0.1 ?0.001less len spell ?0.05 ?0.0005less word bigrams +0.05 +0.0007Table 2: Classifying Inappropriate Requests: AllFeatures vs. Baseline vs. Leave-One-Out Classi-fiers, where * indicates statistically significant dif-ference from All Features (p < 0.01)in percentage points and in F-measure when eachfeature is removed in turn compared to the clas-sifier trained on the full features set.
We foundthat system request type was the most importantfeature, as performance decreased by 7.6 percent-age points without it.
This makes sense in light ofthe fact that the ratio of inappropriate to appropri-ate requests varied for the different request typesrepresented in our dataset.
The next most usefulfeatures were the output of LIWC and the POSbigrams.
We had hypothesized that, since LIWCcaptures the presence of negations and assents, itcould capture negative user responses to the sys-tem such as yes or no.
As for the POS bigrams, wemodified the POS tags to mark common words andincluded start and end markers in the bigrams be-cause we hypothesized that the first words and lastwords in the responses might be particularly infor-mative.
Looking at the decision tree created withall our features, we find that the first five branchesinvolve decisions regarding the unigrams ?name?and ?SPELL?
(a collapsed spelling sequence), the?START, ?neither??
bigram, the LIWC ingestion-word feature, and the type of request, in that order.Not only do these findings confirm our hypothe-ses, they also confirm that the unigrams ?name?,?SPELL?, and ?neither?
which we had mapped tospecial POS classes are particularly useful.After training our model, we used it to classifyour entire dataset to see which responses it per-formed well on and which it tended to misclassify.Responses to targeted reprise and targeted-reprise-rephrase questions together accounted for aroundhalf of the misclassified instances.
Many easilyidentifiable responses to inappropriate requests in-volved the user correcting the system, as in the fol-lowing example:User: You are going to need to dole outpunishment.System: I think this is a name: ?dole outpunishment?.
Please spell that name.User: It is not a name, it is a phrase, doleout punishment.However, when the users did not correct the sys-tem after an inappropriate request, their responsesappeared no different from answers to appropri-ate requests.
In the following example, the systemmisrecognizes ?hyperbaric?
and interprets it as theword ?hyper?
followed by an unknown phrase, butthe user simply ignores the request and repeats.User: We are going to put you in ahyperbaric chamber.System: Put you in a high what?
Pleasegive me another word or phrasefor ?perbaric?.User: Hyperbaric chamber.Many cases in which appropriate requests weremisclassified as inappropriate involved users re-sponding correctly to targeted or targeted-rephrasequestions.
We hypothesize that these are also dueprimarily to users ignoring the inappropriate sys-tem request and providing the information the sys-tem should have asked for.
As a result, those casesmake it difficult to distinguish between responsesto appropriate and inappropriate targeted ques-tions.
Of course, users may be giving prosodiccues to indicate confusion or uncertainty or hyper-articulating in their responses.
We will address theuse of prosodic features in predicting inappropri-ate requests in future work.6 ConclusionsIn this work, we have addressed a novel task ofidentifying inappropriate clarification requests us-ing features extracted from user responses.
Wecollected responses to inappropriate clarificationrequests based on six request types in a simulatedSDS environment.
The classifier trained on thisdataset detects inappropriate requests with accu-racy of 88.5%, which is 25.3 percentage pointsabove the majority baseline, and an F-measure of.885, which is .396 points above the majority F-measure.
In future work, we will include acousticand prosodic features as well as lexical featuresand we will evaluate the use of an inappropriateclarification request component in our speech-to-speech translation system.241ReferencesM.
Akbacak et al.
2009.
Recent advances in SRI?sIraqCommtmIraqi Arabic-English speech-to-speechtranslation system.
In ICASSP, pages 4809?4812.N.
F. Ayan et al.
2013.
?Can you give me another wordfor hyperbaric??
: Improving speech translation usingtargeted clarification questions.
In Acoustics, Speechand Signal Processing (ICASSP), 2013 IEEE Interna-tional Conference on, pages 8391?8395.
IEEE.D.
Bohus and A. I. Rudnicky.
2005.
A principled ap-proach for rejection threshold optimization in spokendialog systems.
In INTERSPEECH, pages 2781?2784.J.
Ginzburg and R. Cooper.
2004.
Clarification, ellip-sism and the nature of contextual updates.
Linguisticsand Philosophy, 27(3).J.
Goldberg, M. Ostendorf, and K. Kirchhoff.
2003.The impact of response wording in error correctionsubdialogs.
In ISCA Tutorial and Research Workshopon Error Handling in Spoken Dialogue Systems.P.
Lendvai, A. van den Bosch, E. Krahmer, andM.
Swerts.
2002.
Improving machine-learned de-tection of miscommunications in human-machine di-alogues through informed data splitting.
In Proceed-ings of the ESSLLI Workshop on Machine Learning Ap-proaches in Computational Linguistics, pages 1?15.D.
Litman, J. Hirschberg, and M. Swerts.
2006.
Char-acterizing and predicting corrections in spoken dia-logue systems.
Computational linguistics, 32(3):417?438.J.
W. Pennebaker, C. K. Chung, M. Ireland, A. Gon-zales, and R. J. Booth, 2007.
The development andpsychometric properties of LIWC2007.
Austin, TX.M.
Purver.
2004.
The Theory and Use of ClarificationRequests in Dialogue.
Ph.D. thesis, King?s College,University of London.G.
Skantze.
2005.
Exploring human error recoverystrategies: Implications for spoken dialogue systems.Speech Communication, 45(2-3):325?341.S.
Stoyanchev, A. Liu, and J. Hirschberg.
2014.
To-wards natural clarification questions in dialogue sys-tems.
In Proceedings of AISB2014.K.
Toutanova et al.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1.Association for Computational Linguistics.I.
Witten and F. Eibe.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kauf-mann, San Francisco, 2nd edition.AppendixID Simulation Appro.
Example1.
Confirmation1 Correctly recognized utterance yes Did you say ?place this on the pane?
?2 Misrecognized utterance yes Did you say ?these are in um searches will cause the insur-gents to priest buyer??2.
Rephrase-part1 Full non-name word or phrase yes Please say a different word for ?surmise?.2 Partial word no Please say a different word for ?nouncing?.3 Name no Please say a different word for ?Afdhal?.4 Extended segment including name no Please say a different word for ?checkpoint at Betirma?.5 Function word no Please say a different word for ?off over?.3.
Disambiguate1 One choice is correct yes Did you mean fliers as in handouts or fliers as in pilots?2 Neither choice is correct no Did you mean plane as in aircraft or plain as in simple?3 Word being disambiguated was not said no Did you mean sight as in vision or site as in location?4.
Spell1 Name yes Please spell ?Hadi Al Hemdani?.2 Non-name no I think this is a name: ?eluding?.
Please spell that name.3 Extended segment no Please spell ?staff are stealing themselves?.5.
Reprise1 Error segment correctly recognized andno other errorsyes We will search some of the what?2 Recognition error right before ?what?wordno Supplies of I see them what?3 Recognition error which is not the lastword before ?what?no Ask if they are for eating for what?6.
Reprise rephrase1 No errors outside of the error segment yes Use a what?
Please say another word for ?bristled?.2 Error segment is a partial word no Are there any my what?
Please say another word for ?nors?.3 Error outside the targeted segment no Be a right is what?
Please say another word for ?rain?.Table 3: Clarification Requests and Contexts in which they are Appropriate and Inappropriate.242
