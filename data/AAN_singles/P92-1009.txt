CONVERSATIONAL IMPL ICATURES IN  INDIRECT REPL IESNancy GreenSandra CarberryDepar tment  of Computer  and Informat ion SciencesUniversity of DelawareNewark, Delaware 19716, USAemail: green@cis.udel.edu, carberry~cis.udel.eduAbst rac t  IIn this paper we present algorithms for theinterpretation and generation of a kind of particu-larized conversational implicature occurring in cer-tain indirect replies.
Our algorithms make use ofdiscourse xpectations, discourse plans, and dis-course relations.
The algorithms calculate implica-tures of discourse units of one or more sentences.Our approach has several advantages.
First, bytaking discourse relations into account, it can cap-ture a variety of implicatures not handled before.Second, by treating implicatures of discourse unitswhich may consist of more than one sentence, itavoids the limitations of a sentence-at-a-time ap-proach.
Third, by making use of properties of dis-course which have been used in models of other dis-course phenomena, our approach can be integratedwith those models.
Also, our model permits thesame information to be used both in interpretationand generation.1 In t roduct ionIn this paper we present algorithms for theinterpretation and generation of a certain kind ofconversational implicature occurring in the follow-ing type of conversational exchange.
One partici-pant (Q) makes an illocutionary-level r quest 2 tobe informed if p; the addressee (A), whose replymay consist of more than one sentence, conversa-tionally implicates one of these replies: p, "-p, thatthere is support for p, or that there is support for"-p. For example, in (1), assuming Q's utterancehas been interpreted as a request o be informed ifA went shopping, and given certain mutual beliefse.g., that A's car breaking down would normallye sufficient to prevent A from going shopping, andi We wish to thank Kathy McCoy for her comments onthis paper.~i.e., using Austin's (Austin, 1962) distinction betweenlocutionary and il\]ocutionary force, Q's utterance is intendedto function as a request (although it need not have the gram-matical form of a question)that A's reply is coherent and cooperative), A's re-ply is intended to convey, in part, a 'no'.
(1) Q: Did you go shopping?A: a.
My car~s not running.b.
The timing belt broke.Such indirect replies satisfy the conditionsproposed by Grice and others (Grice, 1975;Hirschberg, 1985; Sadock, 1978) for being classi-fied as particularized conversational implicatures.First, A's reply does not entail (in virtue of itsconventional meaning) that A did not go shopping.Second, the putative implicature can be cancelled;for example, it can be denied without the resultsounding inconsistent, as can be seen by consider-ing the addition of (2) to the end of A's reply in(1.
)(2) A: So I took the bus to the mall.Third, it is reinforceable; A's reply in (1) could havebeen preceded by an explicit "no" without destroy-ing coherency or sounding redundant.
Fourth, theputative implicature is nondetachable; the same re-ply would have been conveyed by an alternative r -alization of (la) and (lb) (assuming that the al-ternative did not convey a Manner-based implica-ture).
Fifth, Q and A must mutually believe that,given the assumption that A's reply is cooperative,and given certain shared background information,Q can and will infer that by A's reply, A meant 'no'.This paper presents algorithms for calculating suchan inference from an indirect response and for gen-erating an indirect response intended to carry suchan inference.2 So lu t ion2.1 Overv iewOur algorithms are based upon three notionsfrom discourse research: discourse expectations,discourse plans, and implicit relational propositionsin discourse.B4At certain points in a coherent conversa-tion, the participants hare certain expectations(Reichman, 1984; Carberry, 1990) about what kindof utterance is appropriate.
In the type of exchangewe are studying, at the point after Q's contribu-tion, the participants hare the beliefs that Q hasrequested to be informed if p and that the requestwas appropriate; hence, they share the discourseexpectation that for A to be cooperative, he mustnow say as much as he can truthfully say in regardto the truth of p. (For convenience, we shall referto this expectation as Answer-YNQ(p).
)A discourse plan operator 3 (Lambert & Car-berry, 1991) is a representation f a normal or con-ventional way of accomplishing certain communica-tive goals.
Alternatively, a discourse plan operatorcould be considered as a defeasihle rule expressingthe typical (intended) effect(s) of a sequence of illo-cutionary acts in a context in which certain appli-cability conditions hold.
These discourse plan op-erators are mutually known by the conversationalparticipants, and can be used by a speaker to con-struct a plan for achieving his communicative goals.We provide a set of discourse plan operators whichcan be used by A as part of a plan for fulfillingAnswer-YNQ(p).Mann and Thompson (Mann ~z Thompson,1983; Mann & Thompson, 1987) have describedhow the structure of a written text can be analyzedin terms of certain implicit relational propositionsthat may plausibly be attributed to the writer topreserve the assumption of textual coherency.
4 Therole of discourse relations in our approach is moti-vated by the observation that direct replies mayoccur as part of a discourse unit conveying a rela-tional proposition.
For example, in (3), (b) is pro-vided as the (most salient) obstacle to the action(going shopping) denied by (a);(3) Q: Did you go shopping?A:a. No.b .
my car~s  not  runn ing .in (4), as an elaboration of the action (going shop-ping) conveyed by (a);(4) Q: Did you go shopping?A:a. Yes,b.
I bought some shoes.and in (5), as a concession for failing to do theaction (washing the dishes) denied by (a).
(S) Q: Did you wash the dishes?A:a. No,b.
(but) I scraped them.3in Pollack's terminology, a recipe-for-action (Pollack,1988; Grosz & Sidner, 1988)~4Although they did not study dialogue, they suggestedthat it can be analyzed similarly.
Also note that the rela-tional predicates which we define are similar but not neces-sarily identical to theirs.Note that given appropriate context, the (b) repliesin (3) through (5)would be sufficient o conversa-tionally implicate the corresponding direct replies.This, we claim, is by virtue of the recognition ofthe relational proposition that would be conveyedby use of the direct reply and the (b) sentences.Our strategy, then, is to generate/interpretA's contribution using a set of discourse plan oper-ators having the following properties: (1) if the ap-plicability conditions hold, then executing the bodywould generate a sequence of utterances intended toimplicitly convey a relational proposition R(p, q);(2) the applicability conditions include the condi-tion that R(p, q) is plausible in the discourse con-text; (3) one of the goals is that Q believe that p,where p is the content of the direct reply; and (4)the step of the body which realizes the direct re-ply can be omitted under certain conditions.
Thus,whenever the direct reply is omitted, it is neverthe-less implicated as long as the intended relationalproposition can be recognized.
Note that prop-erty (2) requires a judgment hat some relationalproposition is plausible.
Such judgments will be de-scribed using defeasible inference rules.
The nextsection describes our discourse relation inferencerules and discourse plan operators.2.2 Discourse Plan Opera-tors and Discourse Relation In-ference RulesA typical reason for the failure of an agent'sattempt o achieve a domain goal is that the agent'sdomain plan encountered an obstacle.
Thus, wegive the rule in (6) for inferring a plausible discourserelation of Obstacle.
s(8)If (i) coherently-relatedCA,B), and(ii) A is a proposition that an agentfailed to perform an action ofact  type  T,  and(iii) B is a proposition thata) a normal applicability conditionof T did not hold, orb) a normal precondition of Tfailed, orc) a normal step of T failed, ord) the agent did not want toachieve a normal goal of T,then plausible(Obstacle(B,A)).In (6) and in the rules to follow, 'coherently-related(A,B)' means that the propositions A and Bare assumed to be coherently related in the dis-course.
The terminology in clause (iii) is that ofthe extended STRIPS planning formalism (Fikes5For simplicity of exposition, (6) and the discourse rela-tion inference rules to follow are stated in terms of the past;we plan to extend their coverage of times.65& Nilsson, 1971; Allen, 1979; Carberry, 1990; Lit-man & Allen, 1987).Examples of A and B satisfying each of theconditions in (6.iii) are given in (7a) - (7d), respec-tively.
(7) \ [A \ ] I  d idn ' t  go shopping.a.
\[B\] The s tores  were closed.b.
\[B\] My car  wasn ' t  run- ing .c.
\[B\] My car  broke doen on the way.d.
\[B\] I d idn ' t  want to  buy anyth ing.The discourse plan operator given in (8) de-scribes a standard way of performing a denial (ex-emplified in (3)) that uses the discourse relation ofObstacle given in (6).
In (8), as in (6), A is a propo-sition that an action of type T was not performed.
(8) Deny (with Obstacle)Applicabi l i ty conditions:1) S BMB plausible(Obstacle(B,A))Bo~ (unordered)  :(opt iona l )  S inform H that  A2) Tel I(S,H,B)Goals:1) H believe that  A2) H believe that Obstacle(B,A)In (8) (and in the discourse plan operatorsto follow) the" formalism described above is used;'S' and 'H' denote speaker and hearer, respectively;'BMB' is the one-sided mutual belief s operator(Clark & Marshall, 1981); 'inform' denotes an il-locutionary act of informing; 'believe' is Hintikka's(Hintikka, 1962) belief operator; 'TelI(S,H,B)' is asubgoal that can be achieved in a number of ways(to be discussed shortly), including just by S in-forming H that B; and steps of the body are notordered.
(Note that to use these operators for gen-eration of direct replies, we must provide a methodto determine a suitable ordering of the steps.
Also,although it is sufficient for interpretation to spec-ify that step 1 is optional, for generation, more in-formation is required to decide whether it can orshould be omitted; e.g., it should not be omitted ifS believes that H might believe that some relationbesides Obstacle is plausible in the context.
7 Theseare areas which we are currently investigating; forrelated research, see section 3.
)Next, consider that a speaker may wish toinform the hearer of an aspect of the plan by whichshe accomplished a goal, if she believes that H maynot be aware of that aspect.
Thus, we give the rulein (9) for inferring a plausible discourse relation ofElaboration.e'S BMB p' is to be read as 'S believes that it is mutuallybelieved between S and H that p'.ZA related question, which has been studied by oth-ers (Joshi, Webber ~ Weischedel, 1984a; Joshi, Webber& Weischedel, 1984b), is in what situations is a speaker re-quired to supply step 2 to avoid misleading the hearer?
(9)If ( i )( i i )coherently-related(A,B), andA is a proposit ion that an agentperformed some action of act typeT, and(iii) B is a proposit ion that describesinformation believed to be new toH abouta) the satisfaction of a normalapplicabil ity condition of T suchthat its satisfaction is notbelieved likely by H, orb) the satisfaction of a normalprecondit ion of T such that itssatisfaction is not believedlikely by H, orc) the success of a normal step of T,ord) the achievement of a normal goalof T,then plausible(Elaboration(B,A)).Examples of A and B satisfying each of theconditions in (9.iii) are given in (10a) - (10d), re-spectively.
(I0) \ [A \ ] I  went shopping today.a.
\[B\] I found a store that was open.b.
\[B\] I got my car fixed yesterday.c.
\[B\] I went to Macy's.d.
\[B\] I got running shoes.The discourse plan operator given in (11) de-scribes a standard way of performing an affirmation(exemplified in (4)) that uses the discourse relationof Elaboration.
(11) Affirm (with Elaboration)Applicabil ity conditions:1) S BMB plausible(Elaboration(B,A))Body (unordered):1) (optional) S inform H that A2) TelI(S,H,B)Goals:1) H believe that A2) H believe that Elaboration(B,A)Finally, note that a speaker may concede afailure to achieve a certain goal while seeking creditfor the partial success of a plan to achieve that goal.For example, the \[B\] utterances in (10) can be usedfollowing (12) (or aIone, in the right context) toconcede failure.
(12) \ [A \ ] I  didn't go shopping today, butThus, the rule we give in (13)for inferring aplausible discourse relation of Concession is similar(but not identical) to (9).
(13)I f  (i)(ii)coherently-related(A,B), andA is a proposition that an agentfailed to do an action of acttype T, and(iii) B is a proposition that describesa) the satisfaction of a normalapplicability condition of T, orb) the satisfaction of a normalprecondition of T, orc) the success of a normal step of T,ord) the achievement of a normal goalof T, and(iv) the achievement of the plan'scomponent in B may bring creditto the agent,then plausible(Concession(B,A)).A discourse plan operator, Deny (with Con-cession), can be given to describe another standardway of  performing a denial (exemplified in (5)).This operator is similar to the one given in (8),except with Concession i  the place of Obstacle.An interesting implication of the discourseplan operators for Affirm (with Elaboration) andDeny (with Concession) is that, in cases where thespeaker chooses not to perform the optional step(i.e., chooses to omit the direct reply), it requiresthat the intended iscourse relation be inferred inorder to correctly interpret the indirect reply, sinceeither an affirmation or denial could be realizedwith the same utterance.
(Although (9) and (13)contain some features that differentiate Elaborationand Concession, other factors, such as intonation,will be considered in future research.
)The next two discourse relations (describedin (14) and (16)) may be part of plan operators forconveying a 'yes' similar to Affirm (with Elabora-tion).
(14)If (i) coherently-related(A,B), and(ii) A is a proposition that an agentperformed an action X, and(iii) B is a proposition that normallyimplies that the agent has a goalG, and(iv) X is a type of action occurringas a normal part of a plan toachieve G,then plausible(Motivate-Volitional-Action(B,A)).15) shows tile use of Motivate-Volitional-ActionMVA) in an indirect (affirmative) reply.
(15) Q: Did you close the window?A: I was cold.
(16)If (i) coherently-related(A,B), and(ii) A is a proposition that an event Eoccurred, and(iii) B is a proposition that an event Foccurred, and(iv) it is not believed that F followedE, and(v)  F - type events  normal ly  causeE- type events ,then plausible(Cause-Non-Vol it ional(B,A))./17) shows the use of Cause-Non-Volitional (CNV)m an indirect (affirmative) reply.
(17) Q: Did you wake up very early?A: The neighbor's dog was barking.The discourse relation described in (18) maybe part of a plan operator similar to Deny (withObstacle) for conveying a 'no'.
(18)If (i) coherently-related(A,B), and(ii) A is a proposition that an event Edid not occur, and(iii) B is a proposition that an action Fwas performed, and(iv) F-type actions are normallyperformed as a way of preventingE-type events,then plausible(Prevent(B,A)).
(19) showsthe use of Preventin an indirect denial.
(19) Q: Did you catch the flu?A: I got a flu shot.The discourse relation described in (20) canbe part of a plan operator similar to the others de-scribed above except hat one of the speaker's goalsis, rather than affirming or denying p, to providesupport for the belief that p.(20)If (i) coherently-related(A,B), and(ii) B is a proposition that describesa typical result of the situationdescribed in proposition A,then plausible(Evidence(B,A)).67Assuming an appropriate context, (21) is an.example of use of this relation to convey support,Le., to convey that it is likely that someone ishome.
(21) Q: Is anyone home?A: The upstairs lights are on.A similar rule could be defined for a relation usedto convey support against a belief.2 .3  Imp l i ca tures  o f  D iscourse  Un i tsConsider the similar dialogues in (22) and(23).
(22) Q: Did you go shopping?A:a. I had to take the bus.b.
(because) My car's not running.c.
(You see,) The timing belt broke.
(23) Q: Did you go shopping?A:a.
My car's not running.b.
The timing belt broke.c.
(So) I had to take the bus.First, note that although the order of the sentencesrealizing A's reply varies in (22) and (23), A's over-all discourse purpose in both is to convey a 'yes'.Second, note that it is necessary to have a rule sothat if A's reply consists olely of (22a) (=23c), animplicated 'yes' is derived; and if It consists olelyof (22b) (=23a), an implicated 'no'.In existing sentence-at-a-time odels of cal-culating implicatures (Gazdar, 1979; Hirschberg,1985), processing (22a) would result in an impli-cated 'yes' being added to the context, which wouldsuccessfully block the addition of an implicated 'no'on processing (22b).
However, processing (23a)would result in a putatively implicated 'no" be-in S added to the context (incorrectly attributinga fleeting intention of A to convey a 'no'); then, onprocessing (23c) the conflicting but intended 'yes'would be blocked by context, giving an incorrectresult.
Thus, a sentence-at-a-time odel must pre-dict when (23c) should override (23a).
Also, in thatmodel, processing (23) requires "extra effort", anonmonotonic revision of belief not needed to han-dle (22); yet (23) seems more like (22) than a casein which a speaker actually changes her mind.In our model, since implicatures correspondto goals of inferred or constructed hierarchicalplans, we avoid this problem.
(22A) and (23A) bothcorrespond to step 2 of Affirm (with Elaboration),TelI(S,H,B); several different discourse plan opera-tors can be used to construct a plan for this Tellaction.
For example, one operator for Tell(S,H,B)is given below in (24); the operator epresents thatin telling H that B, where B describes an agent'svolitional action, a speaker may provide motivationfor the agent's action.
(24) Tell(S,H,p)Applicability Conditions:1) S BMB plausible(Motivate-Volitional-Action(q,p))Body (unordered):1) Tel l (S,H,q)2) S inform H that pGoals:I) H believe that p2) H believe thatMotivate-Volitional-Action(q,p)(We are currently investigating, in generation,when to use an operator such as (24).
For ex-ample, a speaker might want to use (24) in casehe thinks that the hearer might doubt the truthof B unless he knows of the motivation.)
Thus,(22a)/(23c) corresponds to step 2 of (24); (22b) -(22c), as well as (23a) - (23b), correspond to step1.
Another operator for Tell(S,H,p) could representthat in telling H that p, a speaker may provide thecause of an event; i.e., the operator would be like(24) but with Cause-Non-Volitional asthe discourserelation.
This operator could be used to decom-pose (22b)- (22c)/(23a)- (23b).
The structure pro-posed for (22A)/(23A) is illustrated in Figure 1. sLinear precedence in the tree does not necessarilyrepresent narrative order; one way of ordering thetwo nodes directly dominated by TeII(MVA) gives(22A), another gives (23A).
(Narrative order in thegeneration of indirect replies is an area we are cur-rently investigating also; for related research, seesection 3.
)Note that Deny (with Obstacle) can not beused to generate/interpret (22A) or (23A) sinceits body can not be expanded to account forb22a)/(23c).
Thus, the correct implicatures cane derived without attributing spurious intentionsto A, and without requiring cancellation of spuriousimplicatures.8To use the terminology of (Moore & Paris, 1989; Moore& Paris, 1988), the labelled arcs represent satellites, and theunlabelled arcs nucleii.
However, note that in their model, anucleus can not be optional.
This differs from our approach,in that we have shown that direct replies are optional incontexts uch as those described by plan operators uch asAffirm (with Elaboration).9Determiuing this requires that the end of the relevantdiscourse unit be marked/recognlzed by cue phrases, into-nation, or shift of focus; we plan to investigate his problem.68Affirm (with Elaboration)II went shopping(Motivate-Volitional-Action)JMy car's not runningITell (CNV)I(Elaboration)Tell (MVA)III bad to take the bus(Cause-Non-Volitional)The timing belt brokeFigure 1.
A Sample Discourse Structure2.4  A lgor i thmsGeneration and interpretation algorithmsare given in (25) and (26), respectively.
Theypresuppose that the plausible discourse relation isavailable.
1?
The generation algorithm assumes asgiven an illocutionary-level representation of A'scommunicative goals.ll(25) Generation of indirect reply:I.
Select discourse plan operator: Selectfrom the Ans,er-YHQ(p) plan operatorsall those for ,hicha) the applicability conditions hold,andb) the goals include S's goals.2.
If more than one operator was selectedin step I, then choose one.
Also,determine step ordering and whether itis necessary to include optional steps.
(We are currently investigating howthese choices are determined.)3.
Construct a plan from the chosenoperator and execute it.1?We plan to implement an inference mechanism for thediscourse relation inference rules.11 Note that A's goals depend, in part, on the illocutionary-level representation f Q's request.
We assume that ananalysis, such as provided in (Perrault & Allen, 1980), isavailable.
(26) Interpretation of indirect reply:I. Infer discourse plan: Select from theAnsver-YNQ(p) plan operators all thosefor ,hicha) the second step of the body matchesS's contribution, andb) the applicability conditions hold,and?)
it is mutually believed that thegoals are consistent with S's goals.2.
If more than one operator was selectedin step I, then choose one.
(We arecurrently investigatin E what factorsare involved in this choice.
Of course,the utterance may be ambiguous.)3.
Ascribe to S the goal(s) of the chosenplan operator.3 Compar i son  to Past  Re-searchMost previous work in computational or for-mal linguistics on particularized conversational im-plicature (Green, 1990; Horacek, 1991; Joshi,Webber & Weischedel, 1984a; .\]oshi, WebberWeischedel, 1984b; Reiter, 1990; Whiner & Maida,1991) has treated other kinds of implicature thanwe consider here.
ttirschberg (Hirschberg, 1985)provided licensing rules making use of mutual be-liefs about salient partial orderings of entities in69the discourse context to calculate the scalar im-plicatures of an utterance.
Our model is similarto Hirschberg's in that both rely on the represen-tation of aspects of context to generate implica-tures, and our discourse plan operators are roughlyanalogous in function to her licensing rules.
How-ever, her model makes no use of discourse relations.Therefore, it does not handle several kinds of indi-rect replies which we treat.
For example, althoughA in (27) could be analyzed as scalar implicatinga 'no' in some contexts, Hirschberg's model couldnot account for the use of A in other contexts as anelaboration (of how A managed to read chapter 1)intended to convey a 'yes'.
12(27) Q: Did you read the first chapter?A: I took it to the beach with me.Furthermore, Hirschberg provided no computa-tional method for determining the salient partiallyordered set in a context.
Also, in her model, impli-catures are calculated one sentence at a time, whichhas the potential problems described above.Lascarides, Asher, and Oberlander(Lascarides & Asher, 1991; Lascarides & Oberlan-der, 1992) described the interpretation and gen-eration of temporal implicatures.
Although thattype of implicature (being Manner-based) is some-what different from what we are studying, we haveadopted their technique of providing defeasible in-ference rules for inferring discourse relations.In philosophy, Thomason (Thomason, 1990)suggested that discourse xpectations play a role insome implicatures.
McCafferty (McCafferty, 1987)argued that interpreting certain implicated repliesrequires domain plan reconstruction.
However, hedid not provide a computational method for inter-preting implicatures.
Also, his proposed techniquecan not handle many types of indirect replies.
Forexample, it can not account for the implicated nega-tive replies in (1) and (5), since their interpretationinvolves reconstructing domain plans that were notexecuted successfully; it can not account for the im-plicated affirmative reply in (17), in which no rea-soning about domain plans is involved; and it cannot account for implicated replies conveying sup-port for or against a belief, as in (21).
Lastly, hisapproach cannot handle implicatures conveyed bydiscourse units containing more than one sentence.Finally, note that our approach of includingrhetorical goals in discourse plans is modelled onthe work of Hovy (Hovy, 1988) and Moore andParis (Moore & Paris, 1989; Moore & Paris, 1988),who used rhetorical plans to generate coherent text.12 The two intended interpretations are marked by differentintonations.4 ConclusionsWe have provided algorithms for the inter-pretation/generation f a type of reply involvinga highly context-dependent conversational implica-ture.
Our algorithms make use of discourse ex-pectations, discourse plans, and discourse relations.The algorithms calculate implicatures of discourseunits of one or more sentences.
Our approach asseveral advantages.
First, by taking discourse rela-tions into account, it can capture a variety of im-plicatures not handled before.
Second, by treatingimplicatures of discourse units which may consist ofmore than one sentence, it avoids the limitations ofa sentence-at-a-time approach.
Third, by makinguse of properties of discourse which have been usedin models of other discourse phenomena, our ap-proach can be integrated with those models.
Also,our model permits the same information to be usedboth in interpretation and in generation.Our current and anticipated research in-cludes: refining and implementing our algorithms(including developing an inference mechanism forthe discourse relation rules); extending our modelto other types of implicatures; and investigating theintegration of our model into general interpretationand generation frameworks.ReferencesAllen, James F. (1979).
A Plan-Based Approach toSpeech Act Recognition.
PhD thesis, Universityof Toronto, Toronto, Ontario, Canada.Austin, J. L. (1962).
How To Do Things WithWords.
Cambridge, Massachusetts: HarvardUniversity Press.Carberry, Sandra (1990).
Plan Recognition in Nat-ural Language Dialogue.
Cambridge, Mas-sachusetts: MIT Press.Clark, H. & Marshall, C. (1981).
Definite refer-ence and mutual knowledge.
In A. K. Joshi,B.
Webber, & I.
Sag (Eds.
), Elements of dis-course understanding.
Cambridge: CambridgeUniversity Press.Fikes, R. E. & Nilsson, N. J.
(1971).
Strips: A newapproach to the application of theorem provingto problem solving.
Artificial Intelligence, 2,189-208.Gazdar, G. (1979).
Pragmatics: lmplicature, Pre-supposition, and Logical Form.
New York:Academic Press.Green, Nancy L. (1990).
Normal state impli-cature.
In Proceedings of the 28th AnnualMeeting, Pittsburgh.
Association for Compu-tational Linguistics.Grice, H. Paul (1975).
Logic and conversation.
InCole, P. & Morgan, J. L.
(Eds.
), Syntax and70Semantics III: Speech Acts, (pp.
41-58)., NewYork.
Academic Press.Grosz, Barbara & Sidner, Candace (1988).
Plansfor discourse.
In P. Cohen, J. Morgan,M.
Pollack (Eds.
), Intentions in Communica-tion.
MIT Press.Hintikka, J.
(1962).
Knowledge and Belief.
Ithaca:Cornell University Press.Hirschberg, Julia B.
(1985).
A Theory of ScalarImplicature.
PhD thesis, University of Penn-sylvania.ttoracek, Helmut (1991).
Exploiting conversa-tional implicature for generating concise xpla-nations.
In Proceedings.
European Associationfor Computational Linguistics.Hovy, Eduard H. (1988).
Planning coherent multi-sentential text.
In Proceedings of the 26th An-nual Meeting, (pp.
163-169).
Association forComputational Linguistics.Joshi, Aravind, Webber, Bonnie, & Weischedel,Ralph (1984a).
Living up to expectations:Computing expert responses.
In Proceedingsof the Fourth National Conference on ArtificialIntelligence, (pp.
169-175)., Austin, Texas.Joshi, Aravind, Webber, Bonnie, & Weischedel,Ralph (1984b).
Preventing false inferences.In Proceedings of Coling84, (pp.
134-138),Stanford University, California.
Association forComputational Linguistics.Lambert, Lynn & Carberry, Sandra (1991).
A tri-partite plan-based model of dialogue.
In Pro-ceedings of the 29th Annual Meeting, (pp.
47-54).
Association for Computational Linguis-tics.Lascarides, Alex & Asher, Nicholas (1991).
Dis-course relations and defensible knowledge.
InProceedings of the 2gth Annual Meeting, (pp.55-62).
Association for Computational Lin-guistics.Lascarides, Alex & Oberlander, Jon (1992).
Tem-poral coherence and defensible knowledge.Theoretical Linguistics, 18.Litman, Diane & Allen, James (1987).
A planrecognition model for subdialogues in conver-sation.
Cognitive Science, 11, 163-200.Mann, William C. & Thompson, Sandra A.
(1983).Relational propositions in discourse.
TechnicalReport ISI/RR-83-115, ISI/USC.Mann, William C. & Thompson, Sandra A.
(1987).Rhetorical structure theory: Toward a func-tional theory of text organization.
Text, 8(3),167-182.McCafferty, Andrew S. (1987).
Reasoning about Im-plicature: a Plan-Based Approach.
PhD thesis,University of Pittsburgh.Moore, Johanna D. & Paris, Cecile (1989).
Plan-ning text for advisory dialogues.
In Proceed-ings of the 27th Annual Meeting, University ofBritish Columbia, Vancouver.
Association ofComputational Linguistics.Moore, Johanna D. ~z Paris, Cecile L. (1988).
Con-structing coherent ext using rhetorical rela-tions.
In Proc.
lOth Annual Conference.
Cog-nitive Science Society.Perrault, Raymond & Allen, James (1980).
Aplan-based analysis of indirect speech acts.American Journal of Computational Linguis-tics, 6(3-4), 167-182.Pollack, Martha (1988).
Plans as complex men-tal attitudes.
In P. Cohen, J. Morgan, &M. Pollack (Eds.
), Intentions in Communica-tion.
MIT Press.Reichman, Rachel (1984).
Extended person-machine interface.
Artificial Intelligence, 22,157-218.Reiter, Ehud (1990).
The computational complex-ity of avoiding conversational implicatures.
InProceedings of the 28th Annual Meeting, (pp.97-104)., Pittsburgh.
Association for Compu-tational Linguistics.Sadock, Jerrold M. (1978).
On testing for conversa-tional implicature.
In Cole, P. & Morgan, J.
L.(Eds.
), Syntax and Semantics, (pp.
281-297).,N.Y.
Academic Press.Thomason, Richmond H. (1990).
Accommoda-tion, meaning, and implicature: Interdisci-plinary foundations for pragmatics.
In P. Co-hen, J. Morgan, &; M. Pollack (Eds.
), In-tentions in Communication.
Cambridge, Mas-sachusetts: MIT Press.Wainer, Jacques & Maida, Anthony (1991).
Goodand bad news in formalizing eneralized impli-catures.
In Proceedings of the Sixteenth An-nual Meeting of the Berkeley Linguistics Soci-ety, (pp.
66-71)., Berkeley, California.71
