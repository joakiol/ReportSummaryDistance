Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061?1069,Honolulu, October 2008. c?2008 Association for Computational LinguisticsInformation Retrieval Oriented Word Segmentation based on CharacterAssociative Strength RankingYixuan Liu, Bin Wang, Fan Ding, Sheng XuInformation Retrieval GroupCenter for Advanced Computing ResearchInstitute of Computing TechnologyChinese Academy of SciencesBeijing, 100190, P.R.China{liuyixuan, wangbin, dingfan, xusheng}@ict.ac.cnAbstractThis paper presents a novel, ranking-styleword segmentation approach, called RSVM-Seg, which is well tailored to Chinese informa-tion retrieval(CIR).
This strategy makes seg-mentation decision based on the ranking of theinternal associative strength between each pairof adjacent characters of the sentence.
On thetraining corpus composed of query items, aranking model is learned by a widely-used toolRanking SVM, with some useful statisticalfeatures, such as mutual information, differ-ence of t-test, frequency and dictionary infor-mation.
Experimental results show that, thismethod is able to eliminate overlapping am-biguity much more effectively, compared tothe current word segmentation methods.
Fur-thermore, as this strategy naturally generatessegmentation results with different granular-ity, the performance of CIR systems is im-proved and achieves the state of the art.1 IntroductionTo improve information retrieval systems?
perfor-mance, it is important to comprehend both queriesand corpus precisely.
Unlike English and otherwestern languages, Chinese does not delimit wordsby white-space.
Word segmentation is therefore akey preprocessor for Chinese information retrievalto comprehend sentences.Due to the characteristics of Chinese, two mainproblems remain unresolved in word segmentation:segmentation ambiguity and unknown words, whichare also demonstrated to affect the performance ofChinese information retrieval (Foo and Li, 2004).Overlapping ambiguity and combinatory ambiguityare two forms of segmentation ambiguity.
The firstone refers to that ABC can be segmented into ABC or A BC.
The second one refers to that stringAB can be a word, or A can be a word and B canbe a word.
In CIR, the combinatory ambiguity isalso called segmentation granularity problem (Fanet al, 2007).
There are many researches on therelationship between word segmentation and Chi-nese information retrieval (Foo and Li, 2004; Penget al, 2002a; Peng et al, 2002b; Jin and Wong,2002).
Their studies show that the segmentationaccuracy does not monotonically influence subse-quent retrieval performance.
Especially the overlap-ping ambiguity, as shown in experiments of (Wang,2006), will cause more performance decrement ofCIR.
Thus a CIR system with a word segmenter bet-ter solving the overlapping ambiguity, may achievebetter performance.
Besides, it also showed that theprecision of new word identification was more im-portant than the recall.There are some researches show that when com-pound words are split into smaller constituents, bet-ter retrieval results can be achieved (Peng et al,2002a).
On the other hand, it is reasonable that thelonger the word which co-exists in query and cor-pus, the more similarity they may have.
A hypothe-sis, therefore, comes to our mind, that different seg-mentation granularity can be incorporated to obtainbetter CIR performance.In this paper we present a novel word segmenta-tion approach for CIR, which can not only obviouslyreduce the overlapping ambiguity, but also introducedifferent segmentation granularity for the first time.1061In our method, we first predict the ranking result ofall internal association strength (IAS) between eachpair of adjacent characters in a sentence using Rank-ing SVM model, and then, we segment the sentenceinto sub-sentences with smaller and smaller granu-larity by cutting adjacent character pairs accordingto this rank.
Other machine-learning based segmen-tation algorithms (Zhang et al, 2003; Lafferty et al,2001; Ng and Low, 2004) treat segmentation prob-lem as a character sequence tagging problem basedon classification.
However, these methods cannot di-rectly obtain different segmentation granularity.
Ex-periments show that our method can actually im-prove information retrieval performance.This paper is structured as follows.
It starts witha brief introduction of the related work on the wordsegmentation approaches.
Then in Section 3, we in-troduce our segmentation method.
Section 4 evalu-ates the method based on experimental results.
Fi-nally, Section 5 makes summary of this whole paperand proposes the future research orientation.2 Related WorkVarious methods have been proposed to addressthe word segmentation problem in previous studies.They fall into two main categories, rule-based ap-proaches that make use of linguistic knowledge andstatistical approaches that train on corpus with ma-chine learning methods.
In rule-based approaches,algorithms of string matching based on dictionaryare the most commonly used, such as maximummatching.
They firstly segment sentences accord-ing to a dictionary and then resort to some rulesto resolve ambiguities (Liu, 2002; Luo and Song,2001).
These rule-based methods are fast, how-ever, their performances depend on the dictionarywhich cannot include all words, and also on the ruleswhich cost a lot of time to make and must be up-dated frequently.
Recent years statistical approachesbecame more popular.
These methods take advan-tage of various probability information gained fromlarge corpus to segment sentences.
Among them,Wang?s work (Wang, 2006) is the most similar toour method, since both of us apply statistics infor-mation of each gap in the sentence to eliminate over-lapping ambiguity in methods.
However, when com-bining different statistics, Wang decided the weightby a heuristic way which was too simply to be suit-able for all sentences.
In our method, we employ amachine-learning method to train features?
weights.Many machine-learning methods, such asHMM (Zhang et al, 2003), CRF (Lafferty et al,2001), Maximum Entropy (Ng and Low, 2004),have been exploited in segmentation task.
To ourknowledge, machine-learning methods used in seg-mentation treated word segmentation as a charactertagging problem.
According to the model trainedfrom training corpus and features extracted from thecontext in the sentence, these methods assign eachcharacter a positional tag, indicating its relative po-sition in the word.
These methods are difficult to getdifferent granularity segmentation results directly.Our method has two main differences with them.Firstly, we tag the gap between characters ratherthan characters themselves.
Secondly, our methodis based on ranking rather than classification.Then, we will present our ranking-based segmen-tation method, RSVM-Seg.3 Ranking based SegmentationTraditional segmentation methods always take thesegmentation problem as classification problem andgive a definite segmentation result.
In our approach,we try to solve word segmentation problem from theview of ranking.
For easy understanding, let?s rep-resent a Chinese sentence S as a character sequence:C1:n = C1C2 .
.
.
CnWe also explicitly show the gap Gi(i = 1 .
.
.
n?
1)between every two adjacent characters Ci and Ci+1:C1:n|G1:n?1 = C1G1C2G2 .
.
.
Gn?1CnIASi(i = 1 .
.
.
n) is corresponding to Gi(i =1 .
.
.
n), reflecting the internal association strengthbetween Ci and Ci+1.
The higher the IAS value is,the stronger the associative between the two charac-ters is.
If the association between two characters isweak, then they can be segmented.
Otherwise, theyshould be unsegmented.
That is to say we couldmake segmentation based on the ranking of IASvalue.
In our ranking-style segmentation method,Ranking SVM is exploited to predict IAS ranking.In next subsections, we will introduce how totake advantage of Ranking SVM model to solve our1062problem.
Then, we will describe features used fortraining the Ranking SVM model.
Finally, we willgive a scheme how to get segmentation result frompredicted ranking result of Ranking SVM.3.1 Segmentation based on Ranking SVMRanking SVM is a classical algorithm for ranking,which formalizes learning to rank as learning forclassification on pairs of instances and tackles theclassification issue by using SVM (Joachims, 2002).Suppose that X?Rd is the feature space, where d isthe number of features, and Y = r1, r2, .
.
.
, rK isthe set of labels representing ranks.
And there existsa total order between ranks r1 > r2 > .
.
.
> rK ,where > denotes the order relationship.
The actualtask of learning is formalized as a Quadratic Pro-gramming problem as shown below:min?,???12??
?2 + C????s.t.?
?, x?
?
x??
> 1?
???,?x?
?
x?, ???
?
0(1)where ???
denotes l2 norm measuring the marginof the hyperplane and ?ij denotes a slack variable.xi ?
xj means the rank class of xi has an orderprior to that of xj , i.e.
Y (xi) > Y (xj).
Supposethat the solution to (1) is ?
?, then we can make theranking function as f(x) = ??
?, x?.When applying Ranking SVM model to our prob-lems, an instance (feature vector x) is created fromall bigrams (namely CiCi+1, i = 1 .
.
.
n ?
1) ofa sentence in the training corpus.
Each featureis defined as a function of bigrams (we will de-scribe features in detail in next subsection).
Theinstances from all sentences are then combined fortraining.
And Y refers to the class label of theIAS degree.
As we mentioned above, segmenta-tion decision is based on IAS value.
Therefore,the number of IAS degree?s class label is also cor-respondent to the number of segmentation class la-bel.
In traditional segmentation algorithms, they al-ways label segmentation as two classes, segmentedand unsegmented.
However, for some phrases, it isa dilemma to make a segmentation decision basedon this two-class scheme.
For example, Chinesephrase ??????(Notepad)?
can be segmentedas ????(Note)?
and ???(computer)?
or canbe viewed as one word.
We cannot easily classifythe gap between ???
and ???
as segmented or un-segmented.
Therefore, beside these two class la-bels, we define another class label, semisegmented,which means that the gap between two characterscould be segmented or unsegmented, either will beright.
Correspondingly, IAS degree is also dividedinto three classes, definitely inseparable (marked as3), partially inseparable (marked as 2), and sepa-rable (marked as 1).
?Separable?
corresponds tobe segmented?
; ?partially inseparable?
correspondsto semisegmented; ?definitely inseparable?
corre-sponds to be unsegmented.
Obviously, there existsorders between these labels?
IAS values, namelyIAS(1) < IAS(2) < IAS(3), IAS(?)
representsthe IAS value of different labels.
Next, we willdescribe the features used to train Ranking SVMmodel.3.2 Features for IAS computationMutual Information: Mutual information, mea-suring the relationship between two variables, hasbeen extensively used in computational language re-search.
Given a Chinese character string ?xy?
(asmentioned above, in our method, ?xy?
refers to bi-gram in a sentence), mutual information betweencharacters x and y is defined as follows:mi(x, y) = log2 p(x, y)p(x)p(y) (2)where p(x, y) is the co-occurrence probability of xand y, namely the probability that bigram ?xy?
oc-curs in the training corpus, and p(x), p(y) are theindependent probabilities of x and y respectively.From (2), we conclude that mi(x, y) ?
0 meansthat IAS is strong; mi(x, y) ?
0 means that itis indefinite for IAS between characters x and y;mi(x, y) ?
0 means that there is no associationbeen characters x and y.
However, mutual infor-mation has no consideration of context, so it can-not solve the overlapping ambiguity effectively (SiliWang 2006).
To remedy this defect, we introduceanother statistics measure, difference of t-test.Difference of t-score (DTS): Difference of t-score is proposed on the basis of t-score.
Givena Chinese character string ?xyz?, the t-score of thecharacter y relevant to character x and z is defined1063as:tx,z(y) = p(z|y)?
p(y|x)?
?2(p(z|y)) + ?2(p(y|x))(3)where p(y|x) is the conditional probability of ygiven x, and p(z|y), of z given y, and ?2(p(y|x)),?2(p(z|y)) are variances of p(y|x) and of p(z|y) re-spectively.
Sun et al gave the derivation formula of?2(p(y|x)), ?2(p(z|y)) (Sun et al, 1997) as?2(p(z|y)) ?
r(y, z)r2(y) ?2(p(y|x)) ?r(x, y)r2(x) (4)where r(x, y), r(y, z), r(y), r(z) are the frequencyof string xy, yz, y, and z respectively.
Thus formula(3) is deducted astx,z(y) =r(y,z)r(y) ?r(x,y)r(x)?r(y,z)r2(y) +r(x,y)r2(x)(5)tx,z(y) indicates the binding tendency of y in thecontext of x and z: if tx,z(y) > 0 then y tends tobe bound with z rather than with x; if tx,z(y) < 0,they y tends to be bound with x rather than with z.To measure the binding tendency between two ad-jacent characters ?xy?
(also, it refers to bigram in asentence in our method), we use difference of t-score(DTS) (Sun et al, 1998) which is defined asdts(x, y) = tv,y(x)?
tx,w(y) (6)Higher dts(x, y) indicates stronger IAS betweenadjacent characters x and y.Dictionary Information: Both statistics mea-sures mentioned above cannot avoid sparse dataproblem.
Then Dictionary Information is used tocompensate for the shortage of statistics informa-tion.
The dictionary we used includes 75784 terms.We use binary value to denote the dictionary feature.If a bigram is in the dictionary or a part of dictionaryterm, we label it as ?1?, otherwise, we label is as ?0?.Frequency: An important characteristic of newword is its repeatability.
Thus, we also use fre-quency as another feature to train Ranking SVMmodel.
Here, the frequency is referred to the numberof times that a bigram occurs in the training corpus.We give a training sentence for a better under-standing of features mentioned above.
The sentenceAlgorithm 1 : Generate various granularity terms1: Input: A Chinese sentence S = C1 : CnIAS = IAS1:n?1 LB = 1;RB = n2: Iterative(S, IAS):3: while length(S) ?
3 do4: MB = FindMinIAS(IAS)5: SL = CLB:MB6: SR = CMB+1:RB7: IASL = IASLB:MB8: IASR = IASMB+1:RB9: Iterative(SL, IASL)10: Iterative(SR, IASR)11: end whileis ????????
(China Construction Bank net-work)?
We extract all bigrams in this sentence, com-pute the four above features and give the IAS a la-bel for each bigram.
The feature vectors of all thesebigrams for training are shown in Table 1.3.3 Segmentation schemeIn order to compare with other segmentation meth-ods, which give a segmentation result based on twoclass labels, segmented and unsegmented, it is nec-essary to convert real numbers result given by Rank-ing SVM to these two labels.
Here, we make aheuristic scheme to segment the sentence based onIAS ranking result predicted by Ranking SVM.
Thescheme is described in Algorithm 1.
In each itera-tion we cut the sentence at the gap with minimumIAS value.
Nie et.al.
pointed out that the averagelength of words in usage is 1.59 (Nie et al, 2000).Therefore, we stop the segmentation iterative whenthe length of sub sentence is 2 or less than 2.
Bythis method, we could represent the segmentation re-sult as a binary tree.
Figure 1 shows an example ofthis tree.
With this tree, we can obtain various gran-ularity segmentations easily, which could be usedin CIR.
This segmentation scheme may cause somecombinatory ambiguity.
However, Nie et.al.
(Nieet al, 2000) also pointed out that there is no accu-rate word definition, thus whether combinatory am-biguity occurs is uncertain.
What?s more, comparedto overlapping ambiguity, combinatory ambiguity isnot the fatal factor for information retrieval perfor-mance as mentioned in introduction.
Therefore, thisscheme is reasonable for Chinese information re-1064Bigram MI DTS Dictionary Frequency IAS??
(China) 6.67 1985.26 1 1064561 3??
2.59 -1447.6 0 14325 1??
(Construction) 8.67 822.64 1 200129 3??
5.94 -844.05 0 16098 2??
(Bank) 9.22 931.25 1 236976 3??
2.29 -471.24 0 15282 1Table 1: Example of feature vector??????
?(Traffic map of JiangXi Province)???
????
(JiangXi Province)     (Traffic map)???
????
(JiangXi) (Province) (Traffic)    (Map)Figure 1: Example 1trieval.4 Experiments and analysis4.1 DataSince the label scheme and evaluation measure (de-scribed in next subsection) of our segmentationmethod are both different from the traditional seg-mentation methods, we did not carry out experi-ments on SIGHAN.
Instead, we used two query logs(QueryLog1 and QueryLog2) as our experiment cor-pus, which are from two Chinese search engine com-panies.
900 queries randomly from QueryLog1 werechosen as training corpus.
110 Chinese queries fromPKU Tianwang1 , randomly selected 150 queriesfrom QueryLog1 and 100 queries from QueryLog2were used as test corpus.
The train and test cor-pus have been tagged by three people.
They weregiven written information need statements, and wereasked to judge the IAS of every two adjacent char-acters in a sentence on a three level scale as men-tioned above, separable, partially inseparable, anddefinitely inseparable.
The assessors agreed in 84%of the sentences, the other sentences were checked1Title field of SEWM2006 and SEWM2007 web retrievalTD task topics.
See http://www.cwirf.org/by all assessors, and a more plausible alternative wasselected.
We exploited SVM light2 as the toolkit toimplement Ranking SVM model.4.2 Evaluation MeasureSince our approach is based on the ranking of IASvalues, it is inappropriate to evaluate our method bythe traditional method used in other segmentationalgorithms.
Here, we proposed an evaluation mea-sure RankPrecision based on Kendall?s ?
(Joachims,2002), which compared the similarity between thepredicted ranking of IAS values and the rankingsof these tags as descending order.
RankPrecisionformula is as follows:RankPrecision =1?
?ni=1InverseCount(si)?ni=1CompInverseCount(si)(7)where si represents the ith sentence (unsegmentedstring), InverseCount(si) represents the numberof discordant pairs inversions in the ranking of thepredicted IAS value compared to the correct labeledranking.
CompInverseCount(si) represents thenumber of discordant pairs inversions when the la-bels totally inverse.4.3 Experiments ResultsContributions of the Features: We investi-gated the contribution of each feature by gen-erating many versions of Ranking SVM model.RankPrecision as described above was used forevaluations in these and following experiments.We used Mutual Information(MI); Differenceof T-Score(DTS); Frequency(F); mutual informa-tion and difference of t-score(MI+DTS); mu-2http://svmlight.joachims.org/1065Feature CorpusTrain Query Query TianLog1 Log2 WangMI 0.882 0.8719 0.8891 0.9444DTS 0.9054 0.8954 0.9086 0.9444F 0.8499 0.8416 0.8563 0.9583MI+DTS 0.9077 0.9117 0.923 0.9769MI+DTS+F 0.8896 0.8857 0.9209 0.9815MI+DTS+D 0.933 0.916 0.9384 0.9954MI+DTS+F+D 0.932 0.93 0.9374 0.9954Table 2: The segmentation performance with differentfeaturesFeaturesMI DTS F MI+DTS MI+DTS+F MI+DTS+D MI+DTS+F+DRankPrecision.82.84.86.88.90.92.94.96.981.001.02TrainCorpusQueryLog1QueryLog2TianWangFigure 2: Effects of featurestual information, difference of t-score and Fre-quency(MI+DTS+F); mutual information, differ-ence of t-score and dictionary(MI+DTS+D); mutualinformation, difference of t-score, frequency andDictionary(MI+DTS+F+D) as features respectively.The results are shown in Table 2 and Figure 2.From the results, we can see that:?
Using all described features together, the Rank-ing SVM achieved a good performance.
Andwhen we added MI, DTS, frequency, dictio-nary as features one by one, the RankPrecisionimproved step by step.
It demonstrates that thefeatures we selected are useful for segmenta-tion.Size of CorpusTrain Train Query Query TianCorpus Log1 Log2 Wang100 0.9149 0.9070 0.9209 0.9630200 0.9325 0.9304 0.9446 0.9907400 0.9169 0.9057 0.9230 0.9630500 0.9320 0.9300 0.9374 0.9954600 0.9106 0.9050 0.9312 0.9907700 0.9330 0.9284 0.9353 0.9954900 0.9217 0.9104 0.9240 0.9907Table 3: The segmentation performance with differentsize training corpusNumber of Train Query0 200 400 600 800 1000RankPrecision.80.85.90.951.001.051.10TrainCorpusQueryLog1QueryLog2TianWangFigure 3: Effects of Corpus Size?
The lowest RankPrecision is above 85%, whichsuggests that the predicted rank result by ourapproach is very close to the right rank.
It isshown that our method is effective.?
When we used each feature alone, differenceof t-score achieved highest RankPrecise, fre-quency was worst on most of test corpus (ex-cept TianWang).
It is induced that differenceof t-test is the most effective feature for seg-mentation.
It is explained that because dts iscombined with the context information, whicheliminates overlapping ambiguity errors.?
It is surprising that when mutual informationand difference of t-score was combined with1066frequency, the RankPrecision was hurt on threetest corpus, even worse than dts feature.
Thereason is supposed that some non-meaning butcommon strings, such as ????
would be tookfor a word with high IAS values.
To correctthis error, we could build a stop word list, andwhen we meet a character in this list, we treatthem as a white-space.Effects of corpus size:We trained different Rank-ing SVM models with different corpus size to in-vestigate the effects of training corpus size to ourmethod performance.
The results are shown in Ta-ble 3 and Figure 3.
From the results, we can see thatthe effect of corpus size to the performance of ourapproach is minors.
Our segmentation approach canachieve good performance even with small trainingcorpus, which indicates that Ranking SVM has gen-eralization ability.
Therefore we can use a relativesmall corpus to train Ranking SVM, saving labelingeffort.Effects on Finding Boundary: In algorithm1, we could get different granularity segmentationwords when we chose different length as stopcondition.
Figure 4 shows the ?boundary precision?at each stop condition.
Here, ?boundary precision?is defined asNo.of right cut boundariesNo.of all cut boundaries (8)From the result shown in figure 4, we can seethat as the segmentation granularity gets smaller, theboundary precision gets lower.
The reason is obvi-ous, that we may segment a whole word into smallerparts.
However, as we analyzed in introduction, inCIR, we should judge words boundaries correctly toavoid overlapping ambiguity.
As for combinatoryambiguity, through setting different stop length con-dition, we can obtain different granularity segmen-tation result.Effects on Overlapping Ambiguity: Due to theinconsistency of train and test corpus, it is difficult tokeep fair for Chinese word segmentation evaluation.Since ICTCLAS is considered as the best Chineseword segmentation systems.
We chose ICTCLASas the comparison object.
Moreover, we choseMaximum Match segmentation algorithm, which isrule-based segmentation method, as the baseline.Stop length2~3 4~5 6~7PrecisionofBoundary.88.89.90.91.92.93.94.95.96Figure 4: Precision of boundary with different stop wordlength conditionsCorpus NOA NOA NOA(RSVM Seg) (ICTCLAS) (MM)QueryLog1 7 10 21QueryLog2 2 6 16TianWang 0 0 1Table 4: Number of Overlapping AmbiguityWe compared the number of overlapping ambigu-ity(NOA) among these three approaches on test cor-pus QueryLog1, QueryLog2 and TianWang.
The re-sult is shown in Table 4.
On these three test cor-pus, the NOA of our approach is smallest, whichindicates our method resolve overlapping ambiguitymore effectively.
For example, the sentence ?????
(basic notes)?, the segmentation result of ICT-CLAS is ????
(basic class)/?
(article)?, the word???(notes)?
is segmented, overlapping ambiguityoccurring.
However, with our method, the predictedIAS value rank of positions between every two ad-jacent characters in this sentence is ??3?1?2?
?,which indicates that the character ???
has strongerinternal associative strength with the character ??
?than with the character ??
?, eliminating overlap-ping ambiguity according to this ISA rank results.Effects on Recognition Boundaries of newword: According to the rank result of all IAS values1067??????
(Hainan High School?s Entry Recruitme)??
????
(Hainan) (High School?s Entry Recruitment)??
??
(High School?s Entry)(Recruitment)Figure 5: Example of New Word boundaryin a sentence, our method can recognize the bound-aries of new words precisely, avoiding the overlap-ping ambiguity caused by new words.
For example,the phrase ???????
(Hainan High School?sEntry Recruitment)?, the ICTCLAS segmentationresult is ???/?/??/?
?, because the new word????
cannot be recognized accurately, thus thecharacter ???
is combined with its latter charac-ter ??
?, causing overlapping ambiguity.
By ourmethod, the segmentation result is shown as figure5, in which no overlapping ambiguity occurs.Performance of Chinese Information Re-trieval: To evaluate the effectiveness of RSVM-Segmethod on CIR, we compared it with the FMM seg-mentation.
Our retrieval system combines differ-ent query representations obtained by our segmen-tation method, RSVM-Seg.
In previous TREC Tere-byte Track, Markov Random Field(MRF) (Metzlerand Croft, 2005) model has displayed better perfor-mance than other information retrieval models, andit can much more easily include dependence fea-tures.
There are three variants of MRF model, fullindependence(FI), sequential dependence(SD), andfull dependence(FD).
We chose SD as our retrievalmodel, since Chinese words are composed by char-acters and the adjacent characters have strong de-pendence relationship.
We evaluated the CIR per-formance on the Chinese Web Corpora CWT200gprovided by Tianwang 3, which, as we know, isthe largest publicly available Chinese web corpustill now.
It consists of 37, 482, 913 web pageswith total size of 197GB.
We used the topic set3http://www.cwirf.org/SegmentationMethod MAP R-P GMAPFMM 0.0548 0.0656 0.0095RSVM-Seg 0.0623 0.0681 0.0196Table 5: Evaluation of CIR performancefor SEWM2007 and SEWM2006 Topic Distillation(TD) task which contains 121 topics.
MAP, R-Precision and GMAP (Robertson, 2006) were asmain evaluation metrics.
GMAP is the geometricmean of AP(Average Precision) through differentqueries, which was introduced to concentrate on dif-ficult queries.
The result is shown in 5.
From thetable, we can see that our segmentation method im-prove the CIR performance compared to FMM.5 Conclusion and Future workFrom what we have discussed above, we can safelydraw the conclusion that our work includes severalmain contributions.
Firstly, to our best known, thisis the first time to take the Chinese word segmenta-tion problem as ranking problem, which provides anew view for Chinese word segmentation.
This ap-proach has been proved to be able to eliminate over-lapping ambiguity and also be able to obtain varioussegmentation granularities.
Furthermore, our seg-mentation method can improve Chinese informationretrieval performance to some extent.As future work, we would search another moreencouraging method to make a segmentation deci-sion from the ranking result.
Moreover, we will tryto relabel SIGHAN corpus on our three labels, anddo experiments on them, which will be more con-venient to compare with other segmentation meth-ods.
Besides, we will carry out more experiments tosearch the effectiveness of our segmentation methodto CIR.AcknowledgmentsThis paper is supported by China Natural ScienceFounding under No.
60603094 and China National863 key project under No.
2006AA010105.
We ap-preciate Wenbin Jiang?s precious modification ad-vices.
Finally, we would like to thank the threeanonymous EMNLP reviewers for their helpful andconstructive comments.1068ReferencesD.
Fan, W. Bin, and W. Sili.
2007.
A Heuristic Approachfor Segmentation Granularity Problem in Chinese In-formation Retrieval.
Advanced Language Processingand Web Information Technology, 2007.
ALPIT 2007.Sixth International Conference on, pages 87?91.S.
Foo and H. Li.
2004.
Chinese word segmentation andits effect on information retrieval.
volume 40, pages161?190.
Elsevier.H.
Jin and K.F.
Wong.
2002.
A Chinese dictionaryconstruction algorithm for information retrieval.
ACMTransactions on Asian Language Information Process-ing (TALIP), 1(4):281?296.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
Proceedings of the eighth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 133?142.J.D.
Lafferty, A. McCallum, and F.C.N.
Pereira.
2001.Conditional Random Fields: Probabilistic Models forSegmenting and Labeling Sequence Data.
Proceed-ings of the Eighteenth International Conference onMachine Learning table of contents, pages 282?289.Q.
Liu.
2002. Review of Chinese lexical and syntactictechnology.Z.Y.
Luo and R. Song.
2001.
Proper noun recognition inChinese word segmentation research.
Conference ofinternational Chinese computer, 328:2001?323.D.
Metzler and W.B.
Croft.
2005.
A Markov randomfield model for term dependencies.
Proceedings ofthe 28th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 472?479.H.T.
Ng and J.K. Low.
2004.
Chinese part-of-speechtagging: one-at-a-time or all-at-once?
word-based orcharacter-based.
Proc of EMNLP.J.Y.
Nie, J. Gao, J. Zhang, and M. Zhou.
2000.
Onthe use of words and n-grams for Chinese informa-tion retrieval.
Proceedings of the fifth internationalworkshop on on Information retrieval with Asian lan-guages, pages 141?148.F.
Peng, X. Huang, D. Schuurmans, and N. Cercone.2002a.
Investigating the relationship between wordsegmentation performance and retrieval performancein Chinese IR.
Proceedings of the 19th internationalconference on Computational linguistics-Volume 1,pages 1?7.F.
Peng, X. Huang, D. Schuurmans, N. Cercone, and S.E.Robertson.
2002b.
Using self-supervised word seg-mentation in Chinese information retrieval.
Proceed-ings of the 25th annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 349?350.S.
Robertson.
2006.
On GMAP: and other transforma-tions.
Proceedings of the 15th ACM international con-ference on Information and knowledge management,pages 78?83.Sili Wang.
2006.
Research on chinese word segmenta-tion for large scale information retrieval.H.P.
Zhang, H.K.
Yu, D.Y.
Xiong, and Q. Liu.
2003.HHMM-based Chinese Lexical Analyzer ICTCLAS.Proceedings of Second SIGHAN Workshop on ChineseLanguage Processing, pages 184?187.1069
