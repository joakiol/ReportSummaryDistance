Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 486?493,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Iterative Reinforcement Approach for Fine-GrainedOpinion MiningWeifu DuHaerbin Institute of TechnologyHaerbin, Chinaduweifu@software.ict.ac.cnSongbo TanInstitute of Computing TechnologyBeijing, Chinatansongbo@software.ict.ac.cnAbstractWith the in-depth study of sentiment analysisresearch, finer-grained opinion mining, whichaims to detect opinions on different review fea-tures as opposed to the whole review level, hasbeen receiving more and more attention in thesentiment analysis research community re-cently.
Most of existing approaches rely mainlyon the template extraction to identify the ex-plicit relatedness between product feature andopinion terms, which is insufficient to detectthe implicit review features and mine the hid-den sentiment association in reviews, whichsatisfies (1) the review features are not appearexplicit in the review sentences; (2) it can bededuced by the opinion words in its context.From an information theoretic point of view,this paper proposed an iterative reinforcementframework based on the improved informationbottleneck algorithm to address such problem.More specifically, the approach clusters prod-uct features and opinion words simultaneouslyand iteratively by fusing both their semantic in-formation and co-occurrence information.
Theexperimental results demonstrate that our ap-proach outperforms the template extractionbased approaches.1 IntroductionIn the Web2.0 era, the Internet turns from a staticinformation media into a platform for dynamicinformation exchanging, on which people can ex-press their views and show their selfhood.
Moreand more people are willing to record their feel-ings (blog), give voice to public affairs (news re-view), express their likes and dislikes on products(product review), and so on.
In the face of the vol-ume of sentimental information available on theInternet continues to increase, there is growinginterest in helping people better find, filter, andmanage these resources.Automatic opinion mining (Turney et al, 2003;Ku et al, 2006; Devitt et al, 2007) can play animportant role in a wide variety of more flexibleand dynamic information management tasks.
Forexample, with the help of sentiment analysis sys-tem, in the field of public administration, the ad-ministrators can receive the feedbacks on one pol-icy in a timelier manner; in the field of business,manufacturers can perform more targeted updateson products to improve the consumer experience.The research of opinion mining began in 1997,the early research results mainly focused on thepolarity of opinion words (Hatzivassiloglou et al,1997) and treated the text-level opinion mining asa classification of either positive or negative on thenumber of positive or negative opinion words inone text (Turney et al, 2003; Pang et al, 2002;Zagibalov et al, 2008;).
With the in-depth study ofopinion mining, researchers committed their ef-forts for more accurate results: the research of sen-timent summarization (Philip et al, 2004; Hu et al,KDD 2004), domain transfer problem of the sen-timent analysis (Kanayama et al, 2006; Tan et al,2007; Blitzer et al, 2007; Tan et al, 2008; An-dreevskaia et al, 2008; Tan et al, 2009) and fine-grained opinion mining (Hatzivassiloglou et al,2000; Takamura et al, 2007; Bloom et al, 2007;Wang et al, 2008; Titov et al, 2008) are the mainbranches of the research of opinion mining.
In thispaper, we focus on the fine-grained (feature-level)opinion mining.For many applications (e.g.
the task of publicaffairs review analysis and the products reviewanalysis), simply judging the sentiment orientationof a review unit is not sufficient.
Researchers (Ku-shal, 2003; Hu et al, KDD 2004; Hu et al, AAAI2004; Popescu et al, 2005) began to work onfiner-grained opinion mining which predicts thesentiment orientation related to different reviewfeatures.
The task is known as feature-level opin-ion mining.486In feature-level opinion mining, most of the ex-isting researches associate product features andopinion words by their explicit co-occurrence.Template extraction based method (Popescu et al,2005) and association rule mining based method(Hu et al, AAAI 2004) are the representative ones.These approaches did good jobs for identifyingthe review features that appear explicitly in re-views, however, real reviews from customers areusually complicated.
In some cases, the reviewfeatures are implicit in the review sentences, butcan be deduced by the opinion words in its context.The detection of such hidden sentiment associationis a big challenge in feature-level opinion miningon Chinese reviews due to the nature of Chineselanguage (Qi et al, 2008).
Obviously, neither thetemplate extraction based method nor the associa-tion rule mining based method is effective for suchcases.
Moreover, in some cases, even if the reviewfeatures appear explicitly in the review sentences,the co-occurrence information between reviewfeatures and opinion words is too quantitativelysparse to be utilized.
So we consider whether it is amore sensible way to construct or cluster reviewfeature groups and opinion words groups to minethe implicit or hidden sentiment association in thereviews.The general approach will cluster the two typesof objects separately, which neglects the highlyinterrelationship.
To address this problem, in thispaper, we propose an iterative reinforcementframework, under which we cluster product fea-tures and opinion words simultaneously and itera-tively by fusing both their semantic informationand sentiment link information.
We take improvedinformation bottleneck algorithm (Tishby, 1999)as the kernel of the proposed framework.The information bottleneck approach was pre-sented by Tishby (1999).
The basic idea of the ap-proach is that it treats the clustering problems fromthe information compressing point of view, andtakes this problem as a case of much more funda-mental problem: what are the features of the vari-able X that are relevant for the prediction of an-other, relevance, variable Y?
Based on the infor-mation theory, the problem can be formulated as:find a compressed representation of the variable X,denoted C, such that the mutual information be-tween C and Y is as high as possible, under a con-straint on the mutual information between X and C.For our case, take the hotel reviews as example, Xis one type of objects of review features (e.g.
fa-cilities, service, surrounding environment, etc) oropinion words (e.g.
perfect, circumspect, quiet,etc), and Y is another one.
Given some review fea-tures (or opinion words) gained from review cor-pus, we want to assemble them into categories,conserving the information about opinion words(or review features) as high as possible.The information bottleneck algorithm has somebenefits, mainly including (1) it treats the trade-offof precision versus complexity of clustering modelthrough the rate distortion theory, which is a sub-field of information theory; (2) it defines the ?dis-tance?
or ?similarity?
in a well-defined way basedon the mutual information.
The efficiency of in-formation bottleneck algorithm (Slonim andTishby, 2000) motivates us to take it as the kernelof our framework.
As far as we know, this ap-proach has not been employed in opinion miningyet.In traditional information bottleneck approach,the distance between two data objects is measuredby the Jensen-Shannon divergence (Lin, 1991),which aims to measure the divergence betweentwo probability distributions.
We alter this meas-ure to integrate more semantic information, whichwill be illustrated in detail in the following sec-tions, and the experimental result shows theeffectiveness of the alteration.It would be worthwhile to highlight several as-pects of our work here:z We propose an iterative reinforcementframework, and under this framework, reviewfeature words and opinion words are organizedinto categories in a simultaneous and iterativemanner.z In the process of clustering, the semantic in-formation and the co-occurrence informationare integrated.z The experimental results on real Chineseweb reviews demonstrate that proposedmethod outperforms the template extractionbased algorithm.2 Proposed Algorithm2.1 The ProblemIn product reviews, opinion words are used to ex-press opinion, sentiment or attitude of reviewers.Although some review units may express generalopinions toward a product, most review units are487regarding to specific features of the product.A product is always reviewed under a certainfeature set F. Suppose we have got a lexical list Owhich includes all the opinion expressions andtheir sentiment polarities.
For the feature-levelopinion mining, identifying the sentiment associa-tion between F and O is essential.
The key pointsin the whole process are as follows:z get opinion word set O (with polarity labels)z get product feature set Fz identify relationships between F and OThe focus of the paper is on the latter two steps,especially for the case of hidden sentiment asso-ciation that the review features are implicit in thereview sentences, but can be deduced by the opin-ion words in its context.
In contrast to existing ex-plicit adjacency approaches, the proposedapproach detects the sentiment associationbetween F and O based on review featurecategories and opinion word groups gained fromthe review corpus.To this end, we first consider two sets of asso-ciation objects: the set of product feature words F= {f1,f2,?,fm} and the set of opinion words O ={o1,o2,?on}.
A weighted bipartite graph from Fand O can be built, denoted by G = {F, O, R}.Here R = [rij] is the m*n link weight matrix con-taining all the pair-wise weights between set F andO.
The weight can be calculated with differentweighting schemes, in this paper, we set rij by theco-appearance frequency of fi and oj in clause level.We take F and O as two random variables, andthe question of constructing or clustering the ob-ject groups can be defined as finding compressedrepresentation of each variable that reserves theinformation about another variable as high as pos-sible.
Take F as an example, we want to find itscompression, denoted as C, such that the mutualinformation between C and O is as high as possi-ble, under a constraint on the mutual informationbetween F and C.We propose an iterative reinforcement frame-work to deal with the tasks.
An improved informa-tion bottleneck algorithm is employed in thisframework, which will be illustrated in detail inthe following sections.2.2 Information Bottleneck AlgorithmThe information bottleneck method (IB) was pre-sented by Tishby et al (1999).
According to Shan-non?s information theory (Cover and Thomas,1991), for the two random variables X, Y, the mu-tual information I(X;Y) between the random vari-ables X, Y is given by the symmetric functional:,( | )( ; ) ( ) ( | ) log( )x X y Yp y xI X Y p x p y xp y?
?= ?
(1)and the mutual information between them meas-ures the relative entropy between their joint distri-bution p(x, y) and the product of respective mar-ginal distributions p(x)p(y), which is the only con-sistent statistical measure of the information thatvariable X contains about variable Y (and viceversa).
Roughly speaking, some of the mutual in-formation will be lost in the process of compres-sion, e.g.
( , ) ( , )I C Y I X Y?
(C is a compressed rep-resentation of X).This representation is defined through a (possi-bly stochastic) mapping between each valuex X?
to each representative value c C?
.
Formally,this mapping can be characterized by a conditionaldistribution p(c|x), inducing a soft partitioning of Xvalues, Specifically, each value of X _is associatedwith all the codebook elements (C values), withsome normalized probability._The IB method is based on the following simpleidea.
Given the empirical joint distribution of twovariables, one variable is compressed so that themutual information about the other variable is pre-served as much as possible.
The method can beconsidered as finding a minimal sufficient partitionor efficient relevant coding of one variable withrespect to the other one.
This problem can besolved by introducing a Lagrange multiplier ?
,and then minimizing the functional:[ ( | )] ( , ) ( , )L p c x I C X I C Y?= ?
(2)This solution is given in terms of the three dis-tributions that characterize every cluster c C?
, theprior probability for this cluster, p(c), its member-ship probabilities p(c|x), and its distribution overthe relevance variable p(y|c).
In general, the mem-bership probabilities, p(c|x) is ?soft?, i.e.
everyx X?
can be assigned to every c C?
in some(normalized) probability.
The information bottle-neck principle determines the distortion measurebetween the points x and c to bethe [ ] ( | )( | ) || ( | ) ( | )log( | )KL yp y xD p y x p y c p y xp y c=?
, theKullback-Leibler divergence (Cover and Thomas,1991) between the conditional distributions p(y|x)488and p(y|c).
Specifically, the formal optimal solu-tion is given by the following equations whichmust be solved together.
( )( | ) exp( [ ( | ) || ( | )])( , )1( | ) ( | ) ( ) ( | )( )( ) ( | ) ( )KLxxp cp c x D p y x p y cZ xp y c p c x p x p y xp cp c p c x p x???
= ????
=???
=????
(3)Where ( , )Z x?
is a normalization factor, and thesingle positive (Lagrange) parameter ?
determinesthe ?softness?
of the classification.
Intuitively, inthis procedure the information contained in Xabout Y ?squeezed?
through a compact ?bottleneck?of clusters C, that is forced to represent the ?rele-vant?
part in X w.r.t to Y.An important special case is the ?hard?
cluster-ing case where C is a deterministic function of X.That is, p(c|x) can only take values of zero or one,This restriction, which corresponds to thelimit ?
??
in Eqs 3 meaning every x X?
is as-signed to exactly one cluster c C?
with a prob-ability of one and to all the others with a probabil-ity of zero.
This yields a natural distance measurebetween distributions which can be easily imple-mented in an agglomerative hierarchical clusteringprocedure (Slonim and Tishby, 1999).1,( | )0,1( | ) ( , )( )( ) ( )x cx cif x cp c xotherwisep y c p x yp cp c p x???
?
?= ??
????
=???
=?????
(4)The algorithm starts with a trivial partitioninginto |X| singleton clusters, where each cluster con-tains exactly one element of X.
At each step wemerge two components of the current partition intoa single new component in a way that locallyminimizes the loss of mutual information about thecategories.
Every merger, *( , )i jc c c?
, is formallydefined by the following equation:*** **1,( | )0,( )( )( | ) ( | ) ( | )( ) ( )( ) ( ) ( )i jjii ji jx c or x cp c xotherwisep cp cp y c p y c p y cp c p cp c p c p c?
?
???=?
??????
= +???
= +???
(5)The decrease in the mutual information I(C, Y) dueto this merger is defined by( , ) ( , ) ( , )i j before afterI c c I C Y I C Y?
?
?
(6)When ( , )beforeI C Y  and ( , )afterI C Y are the informa-tion values before and after the merger, respec-tively.
After a little algebra, one can see( )( , ) ( ) ( ) ( | ), ( | )i j i j JS i jI c c p c p c D p y c p y c?
?
??
+ ?
?
?
(7)Where the functional DJS  is the Jensen-Shannondivergence (Lin, 1991), defined as^ ^, || ||JS i j i KL i j KL jD p p D p p D p p?
??
?
?
??
?
= +?
?
?
?
?
??
?
?
?
(8)where in our case{ } { }{ }* *^, ( | ), ( | )( )( ), ,( ) ( )( | ) ( | )i j i jjii ji i j jp p p y c p y cp cp cp c p cp p y c p y c?
??
??
???
?
??
??
?
??
???
= +??
(9)By introducing the information optimization cri-terion the resulting similarity measure directlyemerges from the analysis.
The algorithm is nowvery simple.
At each step we perform ?the bestpossible merger?, i.e.
merge the clusters { , }i jc cwhich minimize ( , )i jI c c?
.2.3 Improved Information Bottleneck Algo-rithm for Semantic InformationIn traditional information bottleneck approach, thedistance between two data objects is measured bythe difference of information values before andafter the merger, which is measured by the Jensen-Shannon divergence.
This divergence aims tomeasure the divergence between two probabilitydistributions.
For our case, the divergence is basedon the co-occurrence information between the twovariables F and O.While the co-occurrence in corpus is usuallyquantitatively sparse; additionally, Statistics based489on word-occurrence loses semantic related infor-mation.
To avoid such reversed effects, in the pro-posed framework we combine the co-occurrenceinformation and semantic information as the finaldistance between the two types of objects.
( , ) ( , )(1 ) ( , ){ } { }i j semantic i ji ji j i jD X X D X XI X Xwhere X F X F X O X O??
?=+ ??
?
?
?
?
?
?
(10)In equation 10, the distance between two dataobjects Xi and Xj is denoted as a linear combinationof semantic distance and information value differ-ence.
The parameter ?
reflects the contribution ofdifferent distances to the final distance.Input: Joint probability distribution p(f,o)Output: A partition of F into m clusters, ?m?
{1,?,|F|}, and a partition of O into n clusters ?n?{1,?,|O|}1.
t?02.
Repeata.
Construct CFt?Ftb.
?i, j=1,?,|CFt|, i<j, calculate( , ) (1 ) ( , )t t t t tij semantic i j i jd D cf cf I cf cf?
?
??
+ ?c.
for m?|CFt|-1 to 11) find the indices {i, j}, for which dijt isminimized2) merge {cfit, cfjt}into cf*t3) update CFt?
{CFt -{cfit, cfjt}}U {cf*t}4) update dijt costs w.r.t cf*td.
Construct COt?Ote.
?i, j=1,?,|COt|, i<j,calculate( , ) (1 ) ( , )t t t t tij semantic i j i jd D co co I co co?
?
??
+ ?f.
for n?|COt|-1 to 11) find the indices {i, j}, for which dijt isminimized2) merge {coit,cojt}into co*t3) update COt ?
{COt -{coit,cojt}}U {co*t}4) update dijt costs w.r.t co*tg.
t?t+13.
until (CFt = CFt-1 and COt =COt-1)Figure 1: Pseudo-code of semantic information bot-tleneck in iterative reinforcement frameworkThe semantic distance can be got by the usageof lexicon, such as WordNet (Budanitsky and Hirst,2006).
In this paper, we use the Chinese lexiconHowNet1.The basic idea of the iterative reinforcementprinciple is to propagate the clustered results be-tween different type data objects by updating theirinter-relationship spaces.
The clustering processcan begin from an arbitrary type of data object.The clustering results of one data object type up-date the interrelationship thus reinforce the dataobject categorization of another type.
The processis iterative until clustering results of both objecttypes converge.
Suppose we begin the clusteringprocess from data objects in set F, and then thesteps can be expressed as Figure 1.
After the itera-tion, we can get the strongest n links betweenproduct feature categories and opinion wordgroups.
That constitutes our set of sentiment asso-ciation.3 Experimental SetupIn this section we describe our experiments and thedata used in these experiments.3.1 DataOur experiments take hotel reviews (in Chinese)as example.
The corpus used in the experiments iscomposed by 4000 editor reviews on hotel, includ-ing 857,692 Chinese characters.
They are extractedfrom www.ctrip.com.
Each review contains auser?s rating represented by ?stars?, the number ofthe star denotes the user?s satisfaction.
The de-tailed information is illustrated in Table 1,Table 1: The detail information of corpusUser?s rating Number1 star 5552 star 13753 star 704 star 2000Then we use ICTCLAS2, a Chinese word seg-mentation software to extract candidate reviewfeatures and opinion words.Usually, adjectives are normally used to expressopinions in reviews.
Therefore, most of the exist-ing researches take adjectives as opinion words.
Inthe research of Hu et al (2004), they proposed that1 http://www.keenage.com/2 www.searchforum.org.cn490other components of a sentence are unlikely to beproduct features except for nouns and nounphrases.
Some researchers (Fujii and Ishikawa,2006) targeted nouns, noun phrases and verbphrases.
The adding of verb phrases caused theidentification of more possible product features,while brought lots of noises.
So in this paper, wefollow the points of Hu?s, extracting nouns andnoun phrases as candidate product feature words.Take the whole set of nouns and noun phrasesas candidate features will bring some noise.
In or-der to reduce such adverse effects, we use thefunction of Named Entity Recognition (NER) inICTCLAS to filter out named entities, including:person, location, organization.
Since the NEs havesmall probability of being product features, weprune the candidate nouns or noun phrases whichhave the above NE taggers.Table 2: The number of candidate review featuresand opinion words in our corpusExtracted In-stance TotalNon-RepeatedCandidate re-view feature 86,623 15,249Opinion word 26,721 1,231By pruning candidate product feature words, weget the set of product feature words F. And the setof opinion words O is composed by all the adjec-tives in reviews.
The number of candidate productfeature words and opinion words extracted fromthe corpus are shown as Table 2:3.2 Experimental ProcedureWe evaluate our approach from two perspectives:1) Effectiveness of product feature categoryconstruction by mutual reinforcement based clus-tering;2) Precision of sentiment association betweenproduct feature categories and opinion wordgroups;4 Experimental Results and Discussion4.1 Evaluation of Review Feature CategoryConstructionTo calculate agreement between the review featurecategory construction results and the correct labels,we make use of the Rand index (Rand, 1971).
Thisallows for a measure of agreement between twopartitions, P1 and P2, of the same data set D. Eachpartition is viewed as a collection of n*(n-1)/2 pairwise decisions, where n is the size of D. For eachpair of points di and dj in D, Pi either assigns themto the same cluster or to different clusters.
Let a bethe number of decisions where di is in the samecluster as dj in P1 and in P2.
Let b be the number ofdecisions where the two instances are placed indifferent clusters in both partitions.
Total agree-ment can then be calculated using1 2( , ) ( 1) / 2a bRand P Pn n+= ?
(11)In our case, the parts of product feature words inthe pre-constructed evaluation set are used to rep-resent the data set D; a and b represent the parti-tion agreements between the pairs of any twowords in the parts and in the clustering results re-spectively.In equation 10, the parameter ?
reflects the re-spective contribution of semantic information andco-occurrence information to the final distance.When 0?
= or 1?
= , the co-occurrence informa-tion or the semantic information will be utilizedalone.In order to get the optimal combination of thetwo type of distance, we adjust the parameter?
from 0 to 1(stepped by 0.2), and the accuracy offeature category construction with different ?
areshown in Figure 2:Figure 2: The accuracy of review feature categoryconstruction with the variation of the parameter ?From this figure, we can find that the semanticinformation (?
=1) contributes much more to theaccuracy of review feature category constructionthan the co-occurrence information ( ?
=0), andwhen ?
=0, the approach is equivalent to the tradi-tional information bottleneck approach.
We con-sider this is due partly to the sparseness of the cor-491pus, by enlarging the scale of the corpus or usingthe search engine (e.g.
google etc), we can getmore accurate results.Additionally, by a sensible adjust on the pa-rameter ?
(in this experiment, we set ?
as 0.6),we can get higher accuracy than the two baselines( ?
=0 and ?
=1), which indicates the necessityand effectiveness of the integration of semanticinformation and co-occurrence information in theproposed approach.4.2 Evaluation of Sentiment AssociationWe use precision to evaluate the performance ofsentiment association.
An evaluation set is con-structed manually first, in which there are not onlythe categories that every review feature word be-long to, but also the relationship between eachcategory and opinion word.
Then we define preci-sion as:number of correctly associated pairsPrecisionnumber of detected pairs=(12)A comparative result is got by the means oftemplate-extraction based approach on the sametest set.
By the usage of regular expression, thenouns (phrase) and gerund (phrase) are extractedas the review features, and the nearest adjectivesare extracted as the related opinion words.
Becausethe modifiers of adjectives in reviews also containrich sentiment information and express the view ofcustoms, we extract adjectives and their modifierssimultaneously, and take them as the opinionwords.Table 3: Performance comparison in sentiment asso-ciationApproach Pairs PrecisionTemplate extraction 27,683 65.89%Proposed approach 141,826 78.90%Table 3 shows the advantage of our approachover the extraction by explicit adjacency.
Usingthe same product feature categorization, our sen-timent association approach get a more accuratepair set than the direct extraction based on explicitadjacency.
The precision we obtained by the itera-tive reinforcement approach is 78.90%, almost 13points higher than the adjacency approach.
Thisindicates that there are a large number of hiddensentiment associations in the real custom reviews,which underlines the importance and value of ourwork.5 Conclusions and Further WorkIn this paper, we propose a novel iterative rein-forcement framework based on improved informa-tion bottleneck approach to deal with the feature-level product opinion-mining problem.
We altertraditional information bottleneck method by inte-gration with semantic information, and the ex-perimental result demonstrates the effectiveness ofthe alteration.
The main contribution of our workmainly including:z We propose an iterative reinforcement in-formation bottleneck framework, and in thisframework, review feature words and opinionwords are organized into categories in a simul-taneous and iterative manner.z In the process of clustering, the semantic in-formation and the co-occurrence informationare integrated.z The experimental results based on real Chi-nese web reviews demonstrate that our methodoutperforms the template extraction based al-gorithm.Although our methods for candidate productfeature extraction and filtering (see in 3.1) canpartly identify real product features, it may losesome data and remain some noises.
We?ll conductdeeper research in this area in future work.
Addi-tionally, we plan to exploit more information, suchas background knowledge, to improve the per-formance.6 AcknowledgmentsThis work was mainly supported by two funds, i.e.,0704021000 and 60803085, and one another pro-ject, i.e., 2004CB318109.ReferencesA.
Andreevskaia, S. Bergler.
When Specialists andGeneralists Work Together: Overcoming DomainDependence in Sentiment Tagging.
ACL 2008.A.
Budanitsky and G. Hirst.
Evaluating wordnetbasedmeasures of lexical semantic relatedness.
Computa-tional Linguistics, 32(1):13?47, 2006.492A.
Devitt, K. Ahmad.
Sentiment Polarity Identificationin Financial News: A Cohesion-based Approach.ACL 2007.A.
Fujii and T. Ishikawa.
A system for summarizingand visualizing arguments in subjective documents:Toward supporting decision making.
The Workshopon Sentiment and Subjectivity in Text ACL2006.2006.A.
Popescu and O. Etzioni.
Extracting product featuresand opinions from reviews.
HLT-EMNLP 2005.B.
Liu, M. Hu, and J. Cheng.
Opinion observer: analyz-ing and comparing opinions on the web.
WWW 2005.B.
Pang and L. Lee.
Seeing stars: Exploiting class rela-tionships for sentiment categorization with respect torating scales.
ACL 2005.B.
Pang, L. Lee, and S. Vaithyanathan.
Thumbs up?Sentiment classification using machine learningtechniques.
EMNLP 2002.B.
Philip, T. Hastie, C. Manning, and S. Vaithyanathan.Exploring sentiment summarization.
In AAAI SpringSymposium on Exploring Attitude and Affect in Text:Theories and Applications (AAAI tech report SS-04-07).
2004.B.
Wang, H. Wang.
Bootstrapping Both Product Fea-tures and Opinion Words from Chinese CustomerReviews with Cross-Inducing.
IJCNLP 2008.D.
Kushal, S. Lawrence, and D. Pennock.
Mining thepeanut gallery: Opinion extraction and semantic clas-sification of product reviews.
WWW 2003.H.
Kanayama, T. Nasukawa.
Fully Automatic LexiconExpansion for Domain-oriented Sentiment Analysis.EMNLP 2006H.
Takamura, T. Inui.
Extracting Semantic Orientationsof Phrases from Dictionary.
NAACL-HLT 2007.I.
Titov, R. McDonald.
Modeling online reviews withmulti-grain topic models.
WWW 2008.L.
Ku, Y. Liang and H. Chen.
Opinion Extraction,Summarization and Tracking in News and Blog Cor-pora.
AAAI-CAAW 2006.J.
Blitzer, M. Dredze, F. Pereira.
Biographies, Bolly-wood, Boom-boxes and Blenders: Domain Adapta-tion for Sentiment Classification.
ACL 2007.J.
Lin.
Divergence Measures Based on the ShannonEntropy.
IEEE Transactions on Information theory,37(1):145?151, 1991.K.
Bloom and N. Garg and S. Argamon.
ExtractingAppraisal Expressions.
NAACL-HLT 2007.M.
Hu and B. Liu.
Mining and summarizing customerreviews.
KDD 2004.M.
Hu and B. Liu.
Mining opinion features in customerreviews.
AAAI 2004.N.
Slonim, N. Tishby.
Agglomerative information bot-tleneck.
NIPS 1999.N.
Slonim and N. Tishby.
Document Clustering Usingword Clusters via the Information BottleneckMethod.
SIGIR 2000.N.
Slonim and N. Tishby.
The power of word clustersfor text classification.
ECIR 2001.N.
Tishby, F. Pereira, W. Bialek.
The information bot-tleneck method.
1999, arXiv: physics/0004057v1P.
Turney.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of re-views.
ACL 2002.P.
Turney and M. Littman.
Measuring Praise and Criti-cism: Inference of Semantic Orientation from Asso-ciation.
ACM Transactions on Information Systems,2003,21(4): 315-346.Q.
Su, X. Xu, H. Guo, Z. Guo, X. Wu, X. Zhang, B.Swen and Z. Su.
Hidden sentiment association inChinese web opinion mining.
WWW 2008.S.
Tan, G. Wu, H. Tang and X. Cheng.
A novel schemefor domain-transfer problem in the context of senti-ment analysis.
CIKM 2007.S.
Tan, Y. Wang, G. Wu and X. Cheng.
Using unla-beled data to handle domain-transfer problem ofsemantic detection.
SAC 2008.S.
Tan, X. Cheng, Y. Wang and H. Xu.
Adapting NaiveBayes to Domain Adaptation for Sentiment Analysis.ECIR 2009.T.
Cover and J. Thomas.
Elements of Information The-ory.
John Wiley & Sons, New York, 1991.T.
Zagibalov, J. Carroll.
Automatic Seed Word Selec-tion for Unsupervised Sentiment Classification ofChinese Text.
Coling 2008.V.
Hatzivassiloglou and K. McKeown.
Predicting thesemantic orientation of adjectives.
ACL 1997.V.
Hatzivassiloglou and J. Wiebe.
Effects of adjectiveorientation and gradability on sentence subjectivity.Coling 2000.W.
Rand.
Objective criteria for the evaluation of clus-tering methods.
Journal of the American StatisticalAssociation, 66, 846-850.
1971493
