Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
208?217, Prague, June 2007. c?2007 Association for Computational LinguisticsJoint Morphological and Syntactic Disambiguation?Shay B. Cohen and Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213 USA{scohen,nasmith}@cs.cmu.eduAbstractIn morphologically rich languages, should morphological andsyntactic disambiguation be treated sequentially or as a sin-gle problem?
We describe several efficient, probabilistically-interpretable ways to apply joint inference to morphologicaland syntactic disambiguation using lattice parsing.
Joint infer-ence is shown to compare favorably to pipeline parsing methodsacross a variety of component models.
State-of-the-art perfor-mance on Hebrew Treebank parsing is demonstrated using thenew method.
The benefits of joint inference are modest withthe current component models, but appear to increase as com-ponents themselves improve.1 IntroductionAs the field of statistical NLP expands to handlemore languages and domains, models appropriatefor standard benchmark tasks do not always workwell in new situations.
Take, for example, pars-ing the Wall Street Journal Penn Treebank, a long-standing task for which highly accurate context-freemodels stabilized by the year 2000 (Collins, 1999;Charniak, 2000).
On this task, the Collins modelachieves 90% F1-accuracy.
Extended for new lan-guages by Bikel (2004), it achieves only 75% onArabic and 72% on Hebrew.1It should come as no surprise that Semitic parsinglags behind English.
The Collins model was care-fully designed and tuned for WSJ English.
Many ofthe features in the model depend on English syntaxor Penn Treebank annotation conventions.
Inherentin its crafting is the assumption that a million wordsof training text are available.
Finally, for English, itneed not handle morphological ambiguity.Indeed, the figures cited above for Arabic andHebrew are achieved using gold-standard morpho-logical disambiguation and part-of-speech tagging.
?The authors acknowledge helpful feedback from theanonymous reviewers, Sharon Goldwater, Rebecca Hwa, AlonLavie, and Shuly Wintner.1Compared to the Penn Treebank, the Arabic Treebank(Maamouri et al, 2004) has 60% as many word tokens, andthe Hebrew Treebank (Sima?an et al, 2001) has 6%.Given only surface words, Arabic performancedrops by 1.5 F1 points.
The Hebrew Treebank (un-like Arabic) is built over morphemes, a conventionwe view as sensible, though it complicates parsing.This paper considers parsing for morphologicallyrich languages, with Hebrew as a test case.
Mor-phology and syntax are two levels of linguistic de-scription that interact.
This interaction, we argue,can affect disambiguation, so we explore here thematter of joint disambiguation.
This involves thecomparison of a pipeline (where morphology is in-ferred first and syntactic parsing follows) with jointinference.
We present a generalization of the two,and show new ways to do joint inference for this taskthat does not involve a computational blow-up.The paper is organized as follows.
?2 describesthe state of the art in NLP for Hebrew and somephenomena it exhibits that motivate joint inferencefor morphology and syntax.
?3 describes our ap-proach to joint inference using lattice parsing, andgives three variants of weighted lattice parsing withtheir probabilistic interpretations.
The different fac-tor models and their stand-alone performance aregiven in ?4.
?5 presents experiments on Hebrewparsing and explores the benefits of joint inference.2 BackgroundIn this section we discuss prior work on statisticalmorphological and syntactic processing of Hebrewand motivate the joint approach.2.1 NLP for Modern HebrewWintner (2004) reviews work in Hebrew NLP, em-phasizing that the challenges stem from the writ-ing system, rich morphology, unique word forma-tion process of roots and patterns, and relative lackof annotated corpora.We know of no publicly available statistical parserdesigned specifically for Hebrew.
Sima?an et al208h.Figure 1: (a.)
A sentence in Hebrew (to be read right to left), with (b.)
one morphological analysis, (c.) English glosses, and (d.)natural translation; and (e.) a different morphological analysis, (f.) English glosses, and (g.) less natural translation.
(h.) shows amorphological ?sausage?
lattice that encodes the morpheme-sequence analyses L(~x) possible for a shortened sentence (unmodified?meadow?).
Shaded states are word boundaries, white states are intra-word morpheme boundaries; in practice we add POS tags tothe arcs, to permit disambiguation.
According to both native speakers we polled, both interpretations are grammatical?note thelong-distance agreement required for grammaticality.
(2001) built a Hebrew Treebank of 88,747 words(4,783 sentences) and parsed it using a probabilis-tic model.
However, they assumed that the input tothe parser was already (perfectly) morphologicallydisambiguated.
This assumption is very common inmultilingual parsing (see, for example, Cowan et al,2005, and Buchholz et al, 2006).Until recently, the NLP literature on morpho-logical processing was dominated by the largelynon-probabilistic application of finite-state trans-ducers (Kaplan and Kay, 1981; Koskenniemi, 1983;Beesley and Karttunen, 2003) and the largely unsu-pervised discovery of morphological patterns in text(Goldsmith, 2001; Wicentowski, 2002); Hebrewmorphology receives special attention in Levingeret al (1995), Daya et al (2004), and Adler and El-hadad (2006).
Lately a few supervised disambigua-tion methods have come about, including hiddenMarkov models (Hakkani-Tu?r et al, 2000; Hajic?
etal., 2001), conditional random fields (Kudo et al,2004; Smith et al, 2005b), and local support vectormachines (Habash and Rambow, 2005).
There arealso morphological disambiguators designed specif-ically for Hebrew (Segal, 2000; Bar-Haim et al,2005).2.2 Why Joint Inference?In NLP, the separation of syntax and morphology isunderstandable when the latter is impoverished, asin English.
When both involve high levels of am-biguity, this separation becomes harder to justify,as argued by Tsarfaty (2006).
To our knowledge,that is the only study to move toward joint inferenceof syntax and morphology, presenting joint modelsand testing approximation of these models with twoparsers: one a pipeline (segmentation ?
tagging ?parsing), the other involved joint inference of seg-mentation and tagging, with the result piped to theparser.
The latter was slightly more accurate.
Tsar-faty discussed but did not carry out joint inference.In a morphologically rich language, the differentmorphemes that make up a word can play a varietyof different syntactic roles.
A reasonable linguisticanalysis might not make such morphemes immedi-ate sisters in the tree.
Indeed, the convention of theHebrew Treebank is to placemorphemes (rather thanwords) at the leaves of the parse tree, allowing mor-phemes of a word to attach to different nonterminalparents.2Generating parse trees over morphemes requiresthe availability of morphological information whenparsing.
Because this analysis is not in general re-ducible to sequence labeling (tagging), the problemis different from POS tagging.
Figure 1 gives an2The Arabic Treebank, by contrast, annotates words mor-phologically but keeps the morphemes together as a single nodetagged with a POS sequence.
In Bikel?s Arabic parser, complexPOS tags are projected to a small atomic set; it is unclear howmuch information is lost.209example from Hebrew that illustrates the interactionbetween morphology and syntax.
In this example,we show two interpretations of the surface text, withthe first being a more common natural analysis forthe sentence.
The first and third-to-last words?
anal-yses depend on each other if the resulting analysisis to be the more natural one: for this analysis thefirst seven words have to be a noun phrase, while forthe less common analysis (?lying there nicely?)
onlythe first six words compose a noun phrase, with thelast two words composing a verb phrase.
Consis-tency depends on a long-distance dependency that afinite-state morphology model cannot capture, but amodel that involves syntactic information can.
Dis-ambiguating the syntax aids in disambiguating themorphology, suggesting that a joint model will per-form both more accurately.In sum, joint inference of morphology and syntaxis expected to allow decisions of both kinds to influ-ence each other, enforce adherence to constraints atboth levels, and to diminish the propagation of errorsinherent in pipelines.3 Joint Inference of Morphology andSyntaxWe now formalize the problem and supply the nec-essary framework for performing joint morphologi-cal disambiguation and syntactic parsing.3.1 Notation and Morphological SausagesLet X be the language?s word vocabulary and M beits morpheme inventory.
The set of valid analysesfor a surface word is defined using a morphologi-cal lexicon L, which defines L(x) ?
M+.
L(~x) ?
(M+)+ (sequence of sequences) is the set of whole-sentence analyses for sentence ~x = ?x1, x2, ..., xn?,produced by concatenating elements of L(xi) in or-der.
L(~x) can be represented as an acyclic latticewith a ?sausage?
shape familiar from speech recog-nition (Mangu et al, 1999) and machine translation(Lavie et al, 2004).
Fig.
1h shows a sausage lat-tice for a sentence in Hebrew.
We use ~m to denotean element of L(~x) and ~mi to denote an element ofL(xi); in general, ~m = ?~m1, ~m2, ..., ~mn?.We are interested in a function f :X+ ?
(M+)+ ?
T, where T is the set of syntactic treesfor the language.
f can be viewed as a structuredclassifier.
We use DG(~m) ?
T to denote the set ofvalid trees under a grammar G (here, a PCFG withterminal alphabetM) for morpheme sequence ~m.
Tobe precise, f(~x) selects a mutually consistent mor-phological and syntactic analysis fromGEN(~x) = {?~m, ??
| ~m ?
L(~x), ?
?
DG(~m)}3.2 Product of ExpertsOur mapping f(~x) is based on a joint probabilitymodel p(?, ~m | ~x) which combines two probabil-ity models pG(?, ~m) (a PCFG built on the gram-mar G) and pL(~m | ~x) (a morphological disam-biguation model built on the lexicon L).
Factoringthe joint model into sub-models simplifies training,since we can train each model separately, and in-ference (parsing), as we will see later in this sec-tion.
Factored estimation has been quite popular inNLP of late (Klein and Manning, 2003b; Smith andSmith, 2004; Smith et al, 2005a, inter alia).The most obvious joint parser uses pG as a condi-tional model over trees given morphemes and maxi-mizes the joint likelihood:flik(~x)= argmax?~m,???GEN(~x)pG(?
| ~m) ?
pL(~m | ~x) (1)= argmax?~m,??
?GEN(~x)pG(?, ~m)??
?pG(?
?, ~m)?pL(~m, ~x)?~m?pL(~m?, ~x)This is not straightforward, because it involves sum-ming up the trees for each ~m to compute pG(~m),which calls for the O(|~m|3)-Inside algorithm tobe called on each ~m.
Instead, we use the joint,pG(?, ~m), which, strictly speaking, makes the modeldeficient (?leaky?
), but permits a dynamic program-ming solution.Our models will be parametrized using either un-normalized weights (a log-linear model) or multino-mial distributions.
Either way, both models definescores over parts of analyses, and it may be advanta-geous to give one model relatively greater strength,especially since we often ignore constant normal-izing factors.
This is known as a product of ex-perts (Hinton, 1999), where a new combined distri-bution over events is defined by multiplying compo-nent distributions together and renormalizing.
In the210present setting, for some value ?
?
0,fpoe,?
(~x) = argmax?~m,??
?GEN(~x)pG(?, ~m) ?
pL(~m | ~x)?Z(~x, ?
)(2)where Z(~x, ?)
need not be computed (since it is aconstant in ~m and ?
).
?
tunes the relative weightof the morphology model with respect to the pars-ing model.
The higher ?
is, the more we trust themorphology model over the parser to correctly dis-ambiguate the sentence.
We might trust one modelmore than the other for a variety of reasons: it couldbe more robustly or discriminatively estimated, or itcould be known to come from a more appropriatefamily.This formulation also generalizes two more na?
?veparsing methods.
If ?
= 0, the morphology is mod-eled only through the PCFG and pL is ignored ex-cept as a constraint on which analyses L(~x) are al-lowed (i.e., on the definition of the set GEN(~x)).
Atthe other extreme, as ?
?
+?, pL becomes moreimportant.
Because pL does not predict trees, pGstill ?gets to choose?
the syntax tree, but in the limitit must find a tree for argmax~m?L(~x) pL(~m | ~x).This is effectively the morphology-first pipeline.33.3 Parsing AlgorithmsTo parse, we apply a dynamic programming algo-rithm in the ?max, +?
semiring to solve the fpoe,?problem shown in Eq.
4.
If pL is a unigram-factoredmodel, such that for some single-word morphologi-cal model ?
we havepL(~m | ~x) =?ni=1 ?
(~mi | xi) (3)then we can implement morpho-syntactic parsing byweighting the sausage lattice.
Let the weight of eacharc that starts an analysis ~mi ?
L(xi) be equal tolog ?
(~mi | xi), and let other arcs have weight 0.In the parsing algorithm, the weight on an arc issummed in when the arc is first used to build a con-stituent.In general, we would like to define a joint modelthat assigns (unnormalized) probabilities to ele-ments of GEN(~x).
If pG is a PCFG and pL can3There is a slight difference.
If no parse tree exists for thepL-best morphological analysis, then a less probable ~m may bechosen.
So as ?
?
+?, we can view flik,?
as finding the bestgrammatical ~m and its best tree?not exactly a pipeline.be described as a weighted finite-state transducer,then this joint model is their weighted composition,which is a weighted CFG; call the composed gram-mar I and its (unnormalized) distribution pI .
Com-pared to G, I will have many more nonterminals ifpL has a Markov order greater than 0 (unigram, asabove).
Because parsing runtime depends heavily onthe grammar constant (at best, quadratic in the num-ber of nonterminals), parsing with pI is not compu-tationally attractive.4 fpoe,?
is not, then, a scalablesolution when we wish to use a morphology modelpL that can make interdependent decisions about dif-ferent words in ~x in context.
We propose two new,efficient dynamic programming solutions for jointparsing.In the first, we approximate the distributionpL( ~M | ~x) using a unigram-factored model of theform in Eq.
3:p?L(~m | ~x) =?ni=1 pL( ~Mi = ~mi | ~x)?
??
?posterior, depends on all of ~x(7)Similar methods were applied by Matsuzaki et al(2005) and Petrov and Klein (2007) for parsing un-der a PCFG with nonterminals with latent anno-tations.
Their approach was variational, approxi-mating the true posterior over coarse parses usinga sentence-specific PCFG on the coarse nontermi-nals, created directly out of the true fine-grainedPCFG.
In our case, we approximate the full distri-bution over morphological analyses for the sentenceby a simpler, sentence-specific unigram model thatassumes each word?s analysis is to be chosen inde-pendently of the others.
Note that our model (pL)does not make such an assumption, only the ap-proximate model p?L does, and the approximation isper-sentence.
The idea resembles a mean-field vari-ational approximation for graphical models.
Turn-ing to implementation, we can solve for pL(~mi | ~x)exactly using the forward-backward algorithm.
Wewill call this method fvari,?
(see Eq.
5).A closely related method, applied by Goodman(1996) is called minimum-risk decoding.
Good-man called it ?maximum expected recall?
when ap-plying it to parsing.
In the HMM community it4In prior work involving factored syntax models?lexicalized (Klein and Manning, 2003b) and bilingual (Smithand Smith, 2004)?fpoe,1 was applied, and the asymptotic run-time went to O(n5) and O(n7).211fpoe,?
(~x) = argmax?~m,??
?GEN(~x)log pG(?, ~m) + ?
log pL(~m | ~x) (4)fvari,?
(~x) = argmax?~m,??
?GEN(~x)log pG(?, ~m) + ?
?ni=1 log pL(~mi | ~x) (5)frisk,?
(~x) = argmax?~m,??
?GEN(~x)log pG(?, ~m) + ?
?ni=1 pL(~mi | ~x) (6)is sometimes called ?posterior decoding.?
Mini-mum risk decoding is attributable to Goel and Byrne(2000).
Applied to a single model, it factors theparsing decision by penalizable errors, and choosesthe solution that minimizes the risk (expected num-ber of errors under the model).
This factors into asum of expectations, one per potential mistake.
Thismethod is expensive for parsing models (since it re-quires the Inside algorithm to compute expected re-call mistakes), but entirely reasonable for sequencelabeling models.
The idea is to score each word-analysis ~mi in the morphological lattice by the ex-pected value (under pL) that ~mi is present in the fi-nal analysis ~m.
This is, of course pL( ~Mi = ~mi | ~x),the same quantity computed for fvari,?, except thescore of a path in the lattice is now a sum of pos-teriors rather than a product.
Our second approxi-mate joint parser tries to maximize the probabilityof the parse (as before) and at the same time to min-imize the risk of the morphological analysis.
Seefrisk,?
in Eq.
6; the only difference between frisk,?and fvari,?
is whether posteriors are added (frisk,?
)or multiplied (fvari,?
).To summarize this section, fvari,?
and frisk,?are two approximations to the expensive-in-generalfpoe,?
that boil down to parsing over weighted lat-tices.
The only difference between them is howthe lattice is weighted: using ?
log pL(~mi | ~x) forfvari,?
or using ?pL(~mi | ~x) for frisk,?.5 In case ofa unigram pL, fpoe,?
is equivalent to fvari,?
; other-wise fpoe,?
is likely to be too expensive.3.4 Lattice ParsingTo parse the weighted lattices using fvari,?
andfrisk,?
in the previous section, we use lattice parsing.Lattice parsing is a straightforward generalization of5Until now, we have talked about weighting word analyses,which may cover several arcs, rather than arcs.
In practice weapply the weight to the first arc of a word analysis, and weightthe remaining arcs of that analysis with 0 (no cost or benefit),giving the desired effect.string parsing that indexes constituents by states inthe lattice rather than word interstices.
At parsingtime, a ?max, +?
lattice parser finds the best com-bined parse tree and path through the lattice.
Im-portantly, the data structures that are used in chartparsing need not change in order to accommodatelattices.
The generalization over classic Earley orCKY parsing is simple: keep in the parsing chartconstituents created over a pair of start state and endstate (instead of start position and end position), and(if desired) factor in weights on lattice arcs; see Hall(2005).4 Factored ModelsA fair comparison of joint and pipeline parsing mustmake some attempt to control for the componentmodels.
We describe here two PCFGs we used forpG(?, ~m) and two finite-state morphological modelswe used for pL(~m | ~x).
We show how these mod-els perform in stand-alone evaluations.
For all ex-periments, we used the Hebrew Treebank (Sima?anet al, 2001).
After removing traces and removingfunctional information from the nonterminals, wehad 3,770 sentences in the training set, 371 sen-tences in the development set (used primarily to se-lect the value of ?)
and 370 sentences in the test set.4.1 Syntax ModelOur first syntax model is an unbinarized PCFGtrained using relative frequencies.
Preterminal (POStag ?
morpheme) rules are smoothed using back-off to a model that predicts the morpheme lengthand letter sequence.
The PCFG is not binarized.This grammar is remarkably good, given the lim-ited effort that went into it.
The rules in the train-ing set had high coverage with respect to the de-velopment set: an oracle experiment in which wemaximized the number of recovered gold-standardconstituents (on the development set) gave F1 ac-curacy of 93.7%.
In fact, its accuracy supersedes212more complex, lexicalized, models: given gold-standard morphology, it achieves 81.2% (comparedto 72.0% by Bikel?s parser, with head rules specifiedby a native speaker).
This is probably attributableto the dataset?s size, which makes training withhighly-parameterized lexicalized models precariousand prone to overfitting.
With first-order verticalmarkovization (i.e., annotating each nonterminalwith its parent as in Johnson, 1998), accuracy is alsoat 81.2%.
Tuning the horizontal markovization ofthe grammar rules (Klein and Manning, 2003a) hada small, adverse effect on this dataset.Since the PCFG model was relatively successfulcompared to lexicalized models, and is faster to run,we decided to use a vanilla PCFG, denoted Gvan,and a parent-annotated version of that PCFG (John-son, 1998), denoted Gv=2.4.2 Morphology ModelBoth of our morphology models use the same mor-phological lexicon L, which we describe first.4.2.1 Morphological LexiconIn this work, a morphological analysis of a wordis a sequence of morphemes, possibly with a tag foreach morpheme.
There are several available analyz-ers for Hebrew, including Yona and Wintner (2005)and Segal (2000).
We use instead an empirically-constructed generative lexicon that has the advan-tage of matching the Treebank data and conventions.If the Treebank is enriched, this would then directlybenefit the lexicon and our models.Starting with the training data from the HebrewTreebank, we first create a set of prefixesMp ?
M;this set includes any morpheme seen in a non-finalposition within any word.
We also create a set ofstems Ms ?
M that includes any morpheme seenin a final position in a word.
This effectively cap-tures the morphological analysis convention in theHebrew Treebank, where a stem is prefixed by a rel-atively dominant low-entropy sequence of 0?5 prefixmorphemes.
For example,MHKLB (?from the dog?
)is analyzed as M+H+KLB with prefixes M (?from?
)and H (?the?)
and KLB (?dog?)
is the stem.
In prac-tice, |Mp| = 124 (including some conventions fornumerals) and |Ms| = 13,588.
The morphologicallexicon is then defined as any analysis givenMp andMs:L(x) = {mk1 ?
M?p ?Ms | concat(mk1) = x)}?
{mk1 | count(mk1, x) ?
1} (9)where mk1 denotes ?m1, ...,mk?
and count(mk1, x)denotes the number of occurrences of x disam-biguated as mk1 in the training set.
Note that L(x)also includes any analysis of x observed in the train-ing data.
This permits the memorization of anyobserved analysis that is more involved than sim-ple segmentation (4% of word tokens in the train-ing set; e.g., LXDR (?to the room?)
is analyzed asL+H+XDR).
This will have an effect on evaluation(see ?5.1).
On the development data, L has 98.6%coverage.4.2.2 Unigram BaselineThe baseline morphology model, puniL , first de-fines a joint distribution following Eq.
8.
The wordmodel factors out when we conditionalize to formpuniL (?m1, ...,mk?
| x).
The prefix sequence modelis multinomial estimated by MLE.
The stem model(conditioned on the prefix sequence) is smoothed topermit any stem that is a sequence of Hebrew char-acters.
On the development data, puniL is 88.8% ac-curate (by word).4.2.3 Conditional Random FieldThe second morphology model, pcrfL , which isbased on the same morphological lexicon L, usesa second-order conditional random field (Lafferty etal., 2001) to disambiguate the full sentence by mod-eling local contexts (Kudo et al, 2004; Smith et al,2005b).
Space does not permit a full description; themodel uses all the features of Smith et al (2005b)except the ?lemma?
portion of the model, since theHebrew Treebank does not provide lemmas.
Theweights are trained to maximize the probability ofthe correct path through the morphological lattice,conditioned on the lattice.
This is therefore a dis-criminative model that defines pL(~m | ~x) directly,though we ignore the normalization factor in pars-ing.Until now we have described pL as a model ofmorphemes, but this CRF is trained to predict POStags as well?we can either use the tags (i.e., labelthe morphological lattice with tag/morpheme pairs,213puniL (?m1,m2, ...,mk?, x) = p(x | ?m1,m2, ...,mk?)?
??
?word?
p(mk | ?m1, ...,mk?1?)?
??
?stem?
p(?m1, ...,mk?1?)?
??
?prefix sequence(8)so that the lattice parser finds a parse that is con-sistent under both models), or sum the tags out andlet the parser do the tagging.
One subtlety is thetagging of words not seen in the training data; forsuch words an unsegmented hypothesis with tag UN-KNOWN is included in the lattice and may thereforebe selected by the CRF.
On the development data,pcrfL is 89.8% accurate on morphology, with 74.9%fine-grained POS-tagging F1-accuracy (see ?5.1).Note on generative and discriminative models.The reader may be skeptical of our choice to com-bine a generative PCFG with a discrimative CRF.We point out that both are used to define conditionaldistributions over desired ?output?
structures given?input?
sequences.
Notwithstanding the fact that thefactors can be estimated in very different ways, ourcombination in an exact or approximate product-of-experts is a reasonable and principled approach.5 ExperimentsIn this section we evaluate parsing performance, butan evaluation issue is resolved first.5.1 Evaluation MeasuresThe ?Parseval?
measures (Black et al, 1991) areused to evaluate a parser?s phrase-structure treesagainst a gold standard.
They compute precision andrecall of constituents, each indexed by a label andtwo endpoints.
As pointed out by Tsarfaty (2006),joint parsing of morphology and syntax renders thisindexing inappropriate, since it assumes the yieldsof the trees are identical?that assumption is vio-lated if there are any errors in the hypothesized ~m.Tsarfaty (2006) instead indexed by non-whitespacecharacter positions, to deal with segmentation mis-matches.
In general (and in this work) that is stillinsufficient, since L(~x) may include ~m that are notsimply segmentations of ~x (see ?4.2.1).Roark et al (2006) propose an evaluation met-ric for comparing a parse tree over a sentence gen-erated by a speech recognizer to a gold-standardparse.
As in our case, the hypothesized tree couldhave a different yield than the original gold-standardparse tree, because of errors made by the speechrecognizer.
The metric is based on an alignmentbetween the hypothesized sentence and the gold-standard sentence.
We used a similar evaluationmetric, which takes into account the informationabout parallel word boundaries as well, a piece ofinformation that does not appear naturally in speechrecognition.
Given the correct ~m?
and the hypothe-sis ~?m, we use dynamic programming to find an op-timal many-to-many monotonic alignment betweenthe atomic morphemes in the two sequences.
Thealgorithm penalizes each violation (by a morpheme)of a one-to-one correspondence,6 and each characteredit required to transform one side of a correspon-dence into the other (without whitespace).
Wordboundaries are (here) known and included as indexpositions.
In the case where ~?m = ~m?
(or equal up towhitespace) the method is identical to Parseval (andalso to Tsarfaty, 2006).
POS tag accuracy is evalu-ated the same way, for the same reasons; we reportF1-accuracy for tagging and parsing.5.2 Experimental ComparisonIn our experiment we vary four settings:?
Decoding algorithm: fpoe,?, frisk,?, or fvari,?(?3.3).?
Syntax model: Gvan or Gv=2 (?4.1).?
Morphology model: puniL or pcrfL (?4.2).
In the lat-ter case, we can use the scores over morphemesequences only (summing out tags before latticeparsing; denoted m.-pcrfL ) or the full model overmorphemes and tags, denoted t.-pcrfL .7?
?, the relative strength given to the morphol-ogy model (see ?3).
We tested values of ?
in{0,+?}
?
{10q | q ?
{0, 1, ..., 16}}.
Recallthat ?
= 0 ignores the morphology model prob-abilities altogether (using an unweighted lattice),6That is, in a correspondence of a morphemes in one stringwith b in the other, the penalty is a+ b?2, since the morphemeon each side is not in violation.7One subtlety is that any arc with the UNKNOWN POStag can be relabeled?to any other tag?by the syntax model,whose preterminal rules are smoothed.
This was crucial for?
= +?
(pipeline) parsing with t.-pcrfL as the morphologymodel, since the parser does not recognize UNKNOWN as a tag.214tuned ?
pipeline (?
?
+?)parsermorph.
modelsyntaxmodelseg.accfinePOSF 1coarsePOSF 1parseF 1seg.accfinePOSF 1coarsePOSF 1parseF 1puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9m.-pcrfL pGvan ?
?
?
?
90.9 75.6 80.2 63.7f poe,?pGv=2 ?
?
?
?
90.9 75.3 80.2 64.2t.-pcrfL pGvan ?
?
?
?
90.9 77.2?81.5 63.0pGv=2 ?
?
?
?
90.9 77.2?81.5 64.0puniL pGvan 87.9 70.9 75.3 58.9 88.5 71.5 76.1 59.8pGv=2 87.8 70.9 75.6 59.5 88.6 70.8 75.6 59.9m.-pcrfL pGvan 89.8 74.5 78.9 62.5 89.8 74.5 78.9 62.4f risk,?pGv=2 89.8 74.3 79.1 63.0 89.8 74.3 79.1 63.0t.-pcrfL pGvan 90.2 76.6 80.5 62.4 89.9 76.4 80.4 61.6pGv=2 90.2 76.6 80.5 63.1 89.9 76.4 80.4 62.2puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9m.-pcrfL pGvan?91.1 75.6 80.4 64.0 90.9 74.8 79.3 62.9pGv=2 90.9 75.4 80.5?64.4 90.1 74.6 79.5 63.2f vari,?t.-pcrfL pGvan?91.3 ?77.7 ?81.7 63.0 90.9 77.0 ?81.3 62.6pGv=2?91.3 ?77.6 ?81.6 63.6 90.9 77.0 ?81.3 63.6Table 1: Results of experi-ments on Hebrew (test data,max.
length 40).
This tableshows the performance ofjoint parsing (finite ?
; left)and a pipeline (?
?
+?;right).
Joint parsing with a non-unigram morphology modelis too expensive (marked ?
).Morphological analysis accu-racy (by word), fine-grained(full tags) and coarse-grained(only parts of speech) POStagging accuracy (F1), and gen-eralized constituent accuracy(F1) are reported; ?
was tunedfor each of these separately.Boldface denotes that figureswere significantly better thantheir counterparts in the samerow, under a binomial sign test(p < 0.05).
?
marks the bestoverall accuracy and figuresthat are not significantly worse(binomial sign test, p < 0.05).and as ?
?
+?
a morphology-first pipeline isapproached.We measured four outcome values: segmentationaccuracy (fraction of word tokens segmented cor-rectly), fine- and coarse-grained tagging accuracy,8and parsing accuracy.
For tagging and parsing, F1-measures are given, according to the generalizedevaluation measure described in ?5.1.5.3 ResultsTab.
1 compares parsing with tuned ?
values to thepipeline.The best results were achieved using fvari,?, us-ing the CRF and joint disambiguation.
Without theCRF (using puniL ), the difference between the decod-ing algorithms is less apparent, suggesting an inter-action between the sophistication of the componentsand the best way to decode with them.
These re-sults suggest that fvari,?, which permits pL to ?veto?any structure involving a morphological analysis forany word that is a posteriori unlikely (note that8Although the Hebrew Treebank is small, the size of its POStagset is large (four times larger than the Penn Treebank), be-cause the tags encode morphological features (gender, person,and number).
These features have either been ignored in priorwork or encoded differently.
In order for our POS-tagging fig-ures to be reasonably comparable to previous work, we includeaccuracy for coarse-grained tags (only the core part of speech)tags as well as the detailed Hebrew Treebank tags.log pL(~mi | ~x) can be an arbitrarily large negativenumber), is beneficial as a ?filter?
on parses.9 frisk,?,on the other hand, is only allowed to give ?bonuses?of up to ?
to each morphological analysis that pLbelieves in; its influence is therefore weaker.
Thisresult is consistent with the findings of Petrov et al(2007) for another approximate parsing task.The advantage of the parent-annotated PCFG isalso more apparent when the CRF is used for mor-phology, and when ?
is tuned.
All other thingsequal, then, pcrfL led to higher accuracy all around.Letting the CRF help predict the POS tags helpedtagging accuracy but not parsing accuracy.While the gains over the pipeline are modest,the segmentation, fine POS, and parsing accuracyscores achieved by joint disambiguation with fvari,?with the CRF are significantly better than any of thepipeline conditions.Interestingly, if we had not tested with the CRF,we might have reached a very different conclusionabout the usefulness of tuning ?
as opposed to apipeline.
With the unigram morphology model,joint parsing frequently underperforms the pipeline,sometimes even signficantly.
The explanation, we9Another way to describe this combination is to call it aproduct of |~x|+1 experts: one for the morphological analysis ofeach word, plus the grammar.
The morphology experts (softly)veto any analysis that is dubious based on surface criteria, andthe grammar (softly) vetoes less-grammatical parses.215parsermorph.
modelsyntaxmodelseg.accfinePOSF 1coarsePOSF 1parseF 1puniL pGvan 90.7 73.4 78.5 64.3pGv=2 90.2 73.0 78.5 64.9m.-pcrfL pGvan 90.7 75.4 80.0 65.2f risk,?pGv=2 90.8 75.1 80.2 65.4t.-pcrfL pGvan 91.2 78.1 82.4 65.7pGv=2 91.1 78.0 82.2 66.2puniL pGvan 90.6 73.2 78.3 63.5pGv=2 90.2 72.8 78.4 64.4m.-pcrfL pGvan 92.0 76.6 81.5 66.9pGv=2 91.9 76.2 81.6 66.9f vari,?t.-pcrfL pGvan 91.8 79.1 83.2 66.5pGv=2 91.7 78.7 83.0 67.4Table 2: Oracle results of experiments on Hebrew (test data,max.
length 40).
This table shows the performance of mor-phological segmentation, part-of-speech tagging, coarse part-of-speech tagging and parsing when using an oracle to selectthe best ?
for each sentence.
The notation and interpretation ofthe numbers are the same as in Tab.
1.believe, has to do with the ability of the unigrammodel to estimate a good distribution over analy-ses.
While the unigram model is nearly as goodas the CRF at picking the right segmentation for aword, joint parsing demands much more.
In casethe best segmentation does not lead to a grammat-ical morpheme sequence (under the syntax model),the morphology model needs to be able to give rela-tive strengths to the alternatives.
The unigrammodelis less able to do this, because it ignores the contextof the word, and so the benefit of joint parsing is lost.Most commonly the tuned value of ?
is around10 (not shown, to preserve clarity).
Because of ig-nored normalization constants, this does not meanthat morphology is ?10?
more important than syn-tax,?
but it does mean that, for a particular pL andpG, tuning their relative importance in decoding canimprove accuracy.
In Tab.
2 we show how perfor-mance would improve if the oracle value of ?
wasselected for each test-set sentence; this further high-lights the potential impact of perfecting the tradeoffbetween models.
Of course, selecting ?
automati-cally at test-time, per sentence, is an open problem.To our knowledge, the parsers we have describedrepresent the state-of-the-art in Modern Hebrewparsing.
The closest result is Tsarfaty (2006), whichwe have not directly replicated.
Tsarfaty?s model isessentially a pipeline application of fpoe,?
with agrammar like pGvan .
Her work focused more on theinterplay between the segmentation and POS tag-ging models and the amount of information passedto the parser.
Some key differences preclude directcomparison: we modeled fine-grained tags (thoughwe report both kinds of tagging accurcy), we em-ployed a richer morphological lexicon (permittinganalyses that are not just segmentation), and a dif-ferent training/test split and length filter (we usedlonger sentences).
Nonetheless, our conclusionssupport the argument in Tsarfaty (2006) for more in-tegrated parsing methods.We conclude that tuning the relative importanceof the two models?rather than pipelining to giveone infinitely more importance?can provide an im-provement on segmentation, tagging, and parsingaccuracy.
This suggests that future parsing effortsfor languages with rich morphology might con-tinue to assume separately-trained (and separately-improved) morphology and syntax components,which would stand to gain from joint decoding.
Inour experiments, better morphological disambigua-tion was crucial to getting any benefit from jointdecoding.
Our result also suggests that exploringnew, fully-integrated models (and training methodsfor them) may be advantageous.6 ConclusionWe showed that joint morpho-syntactic parsing canimprove the accuracy of both kinds of disambigua-tion.
Several efficient parsing methods were pre-sented, using factored state-of-the-art morphologyand syntax models for the language under considera-tion.
We demonstrated state-of-the-art performanceon and consistent improvements across many set-tings for Modern Hebrew, a morphologically-richlanguage with a relatively small treebank.ReferencesM.
Adler and M. Elhadad.
2006.
An unsupervisedmorpheme-based HMM for Hebrew morphologicaldisambiguation.
In Proc.
of COLING-ACL.R.
Bar-Haim, K. Sima?an, and Y.
Winter.
2005.
Choos-ing an optimal architecture for segmentation and POS-tagging of Modern Hebrew.
In Proc.
of ACLWorkshopon Computational Approaches to Semitic Languages.K.
R. Beesley and L. Karttunen.
2003.
Finite State Mor-phology.
CSLI.216D.
Bikel.
2004.
Multilingual statistical pars-ing engine.
http://www.cis.upenn.edu/?dbikel/software.html#stat-parser.E.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In Proc.
of DARPA Workshop onSpeech and Natural Language.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proc.
of NAACL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, U. Penn.B.
Cowan and M. Collins.
2005.
Morphology andreranking for the statistical parsing of Spanish.
InProc.
of HLT-EMNLP.E.
Daya, D. Roth, and S. Wintner.
2004.
LearningHebrew roots: Machine learning with linguistic con-straints.
In Proc.
of EMNLP.V.
Goel and W. Byrne.
2000.
Minimum Bayes risk auto-matic speech recognition.
Computer Speech and Lan-guage, 14(2):115?135.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of natural language.
Comp.
Ling., 27(2):153?198.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProc.
of ACL.N.
Habash and O. Rambow.
2005.
Arabic tokeniza-tion, part-of-speech tagging, and morphological dis-ambiguation in one fell swoop.
In Proc.
of ACL.J.
Hajic?, P. Krbec, P.
Kve?ton?, K. Oliva, and V. Petkevic?.2001.
Serial combination of rules and statistics: Acase study in Czech tagging.
In Proc.
of ACL.D.
Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r.
2000.
Statis-tical morphological disambiguation for agglutinativelanguages.
In Proc.
of COLING.K.
Hall.
2005.
Best-first Word-lattice Parsing: Tech-niques for Integrated Syntactic Language Modeling.Ph.D.
thesis, Brown University.G.
E. Hinton.
1999.
Products of experts.
In Proc.
ofICANN.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Comp.
Ling., 24(4):613?632.R.
M. Kaplan and M. Kay.
1981.
Phonological rules andfinite-state transducers.
Presented at LSA.D.
Klein and C. D. Manning.
2003a.
Accurate unlexical-ized parsing.
In Proc.
of ACL, pages 423?430.D.
Klein and C. D. Manning.
2003b.
Fast exact inferencewith a factored model for natural language parsing.
InAdvances in NIPS 15.K.
Koskenniemi.
1983.
A general computational modelof word-form recognition and production.
TechnicalReport 11, University of Helsinki.T.
Kudo, K. Yamamoto, and Y. Matsumoto.
2004.
Ap-plying conditional random fields to Japanese morpho-logical analysis.
In Proc.
of EMNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.A.
Lavie, S. Wintner, Y. Eytani, E. Peterson, andK.
Probst.
2004.
Rapid prototyping of a transfer-based Hebrew-to-English machine translation system.In Proc.
of TMI.M.
Levinger, U. Ornan, and A. Itai.
1995.
Learning mor-pholexical probabilities from an untagged corpus withan application to Hebrew.
Comp.
Ling., 21:383?404.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus.
In Proc.
of NEMLAR.L.
Mangu, E. Brill, and A. Stolcke.
1999.
Finding con-sensus among words: Lattice-based word error mini-mization.
In Proc.
of ECSCT.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In Proc.
of ACL.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of HLT-NAACL.B.
Roark, M. Harper, E. Charniak, B. Dorr, M. Johnson,J.
Kahn, Y. Liu, M. Ostendorf, J. Hale, A. Krasnyan-skaya, M. Lease, I. Shafran, M. Snover, R. Stewart,and Lisa Yung.
2006.
Sparseval: Evaluation metricsfor parsing speech.
In Proc.
of LREC.E.
Segal.
2000.
A probabilistic morphological analyzerfor Hebrew undotted texts.
Master?s thesis, Technion.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Na-tiv.
2001.
Building a treebank of modern Hebrew text.Journal Traitement Automatique des Langues.
Avail-able at http://mila.cs.technion.ac.il.D.
A. Smith and N. A. Smith.
2004.
Bilingual parsingwith factored estimation: Using English to parse Ko-rean.
In Proc.
of EMNLP, pages 49?56.A.
Smith, T. Cohn, and M. Osborne.
2005a.
Logarithmicopinion pools for conditional random fields.
In Proc.of ACL.N.
A. Smith, D. A. Smith, and R. W. Tromble.2005b.
Context-based morphological disambiguationwith random fields.
In Proc.
of HLT-EMNLP.R.
Tsarfaty.
2006.
Integrated morphological and syn-tactic disambiguation for Modern Hebrew.
In Proc.
ofCOLING-ACL Student Research Workshop.R.
Wicentowski.
2002.
Modeling and Learning Mul-tilingual Inflectional Morphology in a Minimally Su-pervised Framework.
Ph.D. thesis, Johns Hopkins U.S. Wintner.
2004.
Hebrew computational linguistics:Past and future.
Art.
Int.
Rev., 21(2):113?138.S.
Yona and S. Wintner.
2005.
A finite-state morpholog-ical grammar of Hebrew.
In Proc.
of ACL Workshopon Computational Approaches to Semitic Languages.217
