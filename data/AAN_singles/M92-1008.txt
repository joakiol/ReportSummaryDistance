GE NLTOOLSET :MUC-4 TEST RESULTS AND ANALYSI SLisa Rau, George Krupka, and Paul JacobsArtificial Intelligence Laborator yGE Research and DevelopmentSchenectady, NY 12301 USAE-mail : rauCkrd .ge .comPhone: (518;) 387 - 5059andIra Sider and Lois ChildsMilitary and Data Systems OperationGE AerospaceAbstractThis paper reports on the GE NLTooLSET customization effort for MUC-4, and analyzes th eresults of the TST3 and TST4 runs .INTRODUCTIO NWe report on the GE results from the MUC-4 conference and provide an analysis of system performance .In general, MUC-4 was a very successful effort for GE.
The NLTooLSET, a suite of natural language tex tprocessing tools designed for easy application in new domains, proved its mettle, as we were quickly able t ointegrate the changes from the MUC-3 to the MUC-4 task .On the positive side, MUC-4 provided a thorough, fair test of system capabilities, and allowed us t oimplement and test new strategies within the context of a task-driven system .
Once again, the methodologyof testing on a real task, along with the benefit of a common corpus, has produced advances in the field a swell as highlighting certain new aspects of text interpretation .
One surprise was that we continued to makeimprovements in sentence-level parsing and interpretation, while at the end of MUC-3 we had suspected tha timprovements in parsing would not yield substantial improvements to our overall performance .On the negative side, major and significant improvements were not easy to make .
Although improvingthe accuracy and coverage of the core language parsing mechanism accounted for some percentage of ou rimprovements, the remainder of the gain in score is attributable to increases in the accuracy of the templat epost-filtering and to many small, incremental enhancements and modifications to the existing system .
Thes e"diminishing returns" continue to stand in the way of vastly improved system performance .
Although ther eare some major problems (such as world knowledge, event-based reasoning, and reference resolution) tha tcan be said to account for much of the remaining error in MUC, it is not clear that MUC is really measurin gprogress toward solving these major problems so much as progress on the many minor problems that ar emore easily solved .RESULTSOur overall results on both TST3 and TST4 were very good in relation to other systems .
Figure 1 summarizesour results on these tests .In addition to these core results, Figure 2 summarizes our performance on the adjunct test .Finally, to put these runs in the context of our other results, Figure 3 illustrates how our system improve dover time, and puts the TST3 and TST4 scores in perspective .94Matched OnlyAll TemplatesMUC-3 CompHigh Prec MOHigh Prec ATTST3TST4REC PRE OVG F70 70 17 71 70 1 958 54 36 55.9 62 53 39 57.1 550 43 46 46.2 50 43 46 46.260 75 8 64 77 941 62 23 49.4 46 61 28 52.5Figure 1 : GE MUC-4 TST3 and TST4 Results(orig.)(orig.
)Matched OnlyREC PRE OVGAll TemplatesREC PRE OVG FTST31MT1ST2MTNSTHigh-prec70 70 17 58 54 36 55.
977 69 18 70 53 38 60.378 64 22 68 36 56 47.
161 70 10 46 49 37 47.485 78 13 43 78 13 55.460 75 8 41 62 23 49.4Figure 2 : Performance on Adjunct TestingTST3 - TST4 SCORE COMPARISO NWe were very pleased that our performance on the new time slice (TST4) was virtually indistinguishabl efrom (even higher than) our performance on the test sample from the same time as the training set .
Weattribute this to the fact that our system was developed and tested for general portability across subjec tareas, application areas, and different types of language and text .
We think this is clear evidence that ourapproach to text interpretation is not in any way geared or slanted toward the particulars of the trainin gset .
Most of the work in the system is still done from core knowledge and basic linguistic principles .These comparison numbers also indicate our general reluctance to encode any knowledge or write an ycode that was domain-specific and would interfere with general processing .OPTIONAL HIGH-PRECISION RU NIn addition to the required testing, we performed one optional test, indicated in the HIGH-PREC rows of Figure1 .
Our system could not produce significantly higher recall without effectively guessing, so we decided onl yto reconfigure the system to produce a high-precision result .
First, we noticed that most of our errors werebeing introduced through the incorrect application of template merging decisions .
There are fewer of thesedecisions when all the information about an event appears in a sentence .
Also, our single sentence leve l9 57060 -30 -510152025Figure 3 : Improvementprocessing was more accurate than our multi-sentence processing, so this strategy was likely to produce hig hprecision .For the high-precision configuration, we set the system to use only one sentence to fill the content o feach template, using the single sentence for each template that contained the most fills .
This strategy isvery crude, and more clever methods are likely to improve upon this .
For example.
we could performselective merging of information from multiple sentences when there is a high degree of overlap between th etwo.
However, even this crude method produced significantly higher precision (15% higher on TST3 and 22 %higher on TST4) .EFFORTWe spent overall about 10 1/2 person-months on MUC-4, as compared with about 15 person-months o nMUC-3 .
This time was divided as follows :2 mo: Knowledge Additions : New or altered patterns, grammar rules, activators, domain expectations ,names, and places.
Complete addition of all possible target fills .
Addition of primary and supporttemplates, lexicon, phrases, patterns and hierarchy .1 mo: New Place and Time Mechanism: A new and clean location and time handler was integrate dinto the Toolset .1 mo: Answer Key Mechanism : Design and implementation of mechanism to use information extractedfrom a canonical, conceptual version of the entire answer key .2 mo: Parser Improvements : Parser recovery and improving attachment .1 mo: TRUMPET upgrade : Revision of domain expectation mechanism to use structure sharing .1/2 mo: MUC-4 Upgrade : Upgrade of MUC-specific mechanisms to be compatible with new MUC- 4format .20096Type Missing SpuriousPost-processing filter ; relevancy 18% 57%Bugs, glitches 23% 17%Code-level 29% 20 %Knowledge-level 30% 6%Figure 4 : Attribution of Error in TST 31 mo: Misc.
Bug Fixing: Hundreds of small bugs were found and fixed .2 mo: Scoring, Reporting : Meetings, reporting, incremental and final scoring, analysis and other over -head .We performed an in-depth study to attribute all errors in the TST3 run to components of the system.For TST3, we had a total of 24 missing templates and 33 spurious templates.
For TST4, we had 13 missin gtemplates and 33 spurious templates .
Moreover, 76% of the spurious points came from whole spuriou stemplates, whereas 41% of the missing points came from missing templates.
This indicates the the larges tsingle source of immediate improvement in our score should come from increasing the accuracy of ou rtemplate filtering stage .
Template filtering is the process when we determine after a template has been fille dout if it is spurious due to relevancy conditions .The Figure 4 summarizes the source of error in terms of the percentage of points between our score an da perfect score .Most of the code errors were due to inaccuracies in, the determination of event boundaries ; part of th e"discourse module" .
In fact, 25% of our missing points come from inaccurate reference resolution, an devent splitting and merging problems.
We hope to address these problems in the next improvements to th eNLTOOLSET .TRAININ GOur method of training was to run our system over the messages in the development and TST1 corpus.
Wekept the TST2 set of messages and answers separate as a safeguard to over-training.
We used the results ofthese runs to detect problems and determine where we needed additional effort .We experimented with using the answer key as an aid in the automatic acquisition of domain-specifi cknowledge .
In particular, the entire development answer key was canonicalized by transforming all naturallanguage strings present as fillers of slots in the key to their conceptual heads.
Second, generalizations wereextracted to reflect reliable information on the habitual roles certain concepts play in the database domai nof the texts .
This process was found to be useful in four places :Detecting Gaps in Knowledge: By sending all the strings present in the answer key through our naturallanguage system, we can detect errors, gaps in knowledge and other problems within the domain o fthe answer key .
This process is a prerequisite to using the answer key, as it produces a canonical ,conceptual version .Determining Valid Generalizations : Certain combinations of fills always occur together .
These general-izations are automatically detected and used to prevent incorrect slot filling .
For example, the terroris torganization SHINING PATH always carries out its terrorist activities in PERU .Determining Hard Constraints : Certain fillers make particularly good fillers for certain slots .
For ex-ample, someone described in a text as a VICTIM is a much better filler for the TARGET slot than th ePERPETRATOR .
These constraints serve to prevent inappropriate fillers from appearing .Encoding Specific, Recurring Events : In certain domains, and with certain types of texts, frequentlyrecurring events can be encoded more specifically to aid in the accuracy of their interpretation .Anomaly Detection : Finally, a canonical answer key, when compiled into lists of unique fillers for eachslot, allows for the easy detection of incorrect answers .9 7Our initial test runs on the MUC-4 TST3 and TST4 showed a small increase (1 point each) in bothrecall and precision on TST3 and a negligible effect on TST4 from the answer key training .
In particular ,the combined (recall and precision) measure was 53 .93 without these data, and 54 .90 with the data for th eTST3 (same time period as the answer key) test .
Recall went from 56 to 57 and precision went from 5 2to 53 .
Although these increments may seem small, our experience has been that any noticeable increase i nperformance is significant at these levels of accuracy .
That is, as systems make fewer and fewer mistakes i ninterpreting texts, it becomes more and more difficult to find areas for any improvements .For the TST4 time slice, as we anticipated, there was no noticeable effect of answer key training .
Asidefrom the use of the knowledge to fill gaps in the system's knowledge base, the information in a novel set ofmessages does not intersect with the information extracted from an old set of messages .
Thus, the answerkey training neither helps nor hurts novel messages .In addition to this experiment, which used training based on an answer key to resolve template-leve ldecisions, we tried several methods for corpus-based training to help with sentence-level interpretation .These experiments included corpus-based part-of-speech tagging, statistically-based information to help wit hattachment, and a "last ditch" method for guessing a parse where the parser produced a suspicious result .In some cases, these methods showed a marginal improvement in early tests .
However, as we neared thefinal MUC test, none of them showed any positive effect .
We treat this as evidence that it is difficult to us eautomated training to guide sentence-level interpretation when sentence-level accuracy is already very high .RETROSPECTIVE ON THE TASK AND RESULT SIn retrospect, we probably could have made additional improvements to performance if we had made signif-icant changes to the mechanism that splits stories into events, and the mechanism that resolves reference s(including definite anaphora and multiple descriptions of the same object or event) .
Aside from these areas ,all the other portions of the NLToolset are working quite well .The speed of our system, around 500/words per minute on this task, understates its real speed due t oa non-optimized configuration .
Nonetheless, this speed is achieved on conventional hardware and is alreadyway ahead of human performance .
This suggests that this technology will be able to process large volume sof text .
We were able to process TST3 in 1 hour and 28 minutes, and TST4 in 1 hour and 5 minutes .We were similarly pleased that the sentence-level performance of the NLTooLSET was as good as it was .While we fixed minor problems with the lexicon, grammar, parser, and semantic interpreter, robustness o flinguistic processing did not seem to be a major problem .
For MUC-3, we believed this was because becausethe domain was still quite narrow .
It was much broader than MUCK-II, and the linguistic complexity is achallenge, but knowledge base and control issues are relatively minor because there are simply not that man ydifferent ways that bombings, murders, and kidnappings occur .
However, given the improvements we mad eto the sentence-level interpretation in MUC-4, we feel that the correct interpretation of individual sentencescan have a significant effect on the overall accuracy of interpretation .
This is partly due to our observatio nthat the effects of parsing "fan out" to affect the accuracy of other components of the system .
Also, w echanged other components to take advantage of the increased accuracy of the interpretations .REUSABILITYWe estimate that about 70% of the knowledge encoded for this effort is reusable, whereas over 80% of th ecode is reusable .
This includes most of the improvements to the parser recovery (or 20% of the total effort) .LESSONS LEARNEDThe GE Syste mThis task has proven our system's transportability, robustness and accuracy .
The major sources of improve dperformance were increasing the accuracy of the template filtering, the coverage and robustness of th eparser, and error recovery handler .
The other improvements have come from the additive effect of man ylittle enhancements and fixes throughout the system (See the GE system summary paper in this volume) .98There is a bit of a puzzle in the observation that our improvement seems to have come from doingbetter at things we already could do well, while the error that remains seems to come predominately fro mproblems we don ' t really have solutions for at all, like general reference resolution and reasoning abou tbackground information .
One conclusion that we have drawn from this analysis is that our progress on thes emajor problems will come not from new modules, but from adding new sources of knowledge to our existin gmodules .
This was not the approach we had taken in MUC-3 .Our experience with MUC-4 has pointed out the need for much closer integration of the discourse (even tand reference resolution) components of the system with the control and core language understanding com-ponents.
It is clear to us that increased performance will not come easily from separable modules .These will continue to be the areas where we hope our system will improve over time .Evaluation and the MUC TaskWe continue to believe that MUC is an interesting, realistic task and that it advances the state of the art i nnatural language processing as well as providing a good test of system capabilities .The move from MUC-3 to MUC-4 produced some clear improvements in the test as well as the systems .For example, the emphasis on ALL TEMPLATES was clearly a better way of evaluating systems, whic hnone of the sites seemed to recognize prior to MUC-3 .
The minimal matching constraints prevented someof the rewards for overgeneration that were a problem in MUC-3 .
The combined F-measure, while hidin gmany of the interesting aspects of performance, at least gives an explicit basis for comparison .It would be nice if the price tag for these tests could be reduced, by reducing the time and effor trequired to run through the mechanics of executing and evaluating each test, and perhaps making forma levaluations less frequent .
In addition, a new domain with a smaller amount of time for development migh tbe more rewarding than repeatedly testing on the same task and domain .
New tasks or domains with shor tpreparation times: (1) minimize the work that each site can do that is task-specific, (2) allow new systems t oparticipate on equal footing with others, and (3) test transportability and general techniques by preventin gtoo much specific development or knowledge coding .
The strategy of coming up with new domains coul dmake it harder to show the overall progress of the field, but it is actually at least as hard to attribute progres sin the MUC-3 - MUC-4 sequence as it was in the MUCK-II - MUC-3 sequence, which represented a muc hmore drastic shift in task and domain .One of the most promising new areas that has emerged from these evaluations is the ability, within agiven system, to test new modules, strategies and configurations, as a controlled way of testing the impact o fa particular algorithm or strategy .
We have learned not only from our own experiments of this sort, such asthe comparison between the GE and CMU parsers (see the GE-CMU site report and system summary in thi svolume), the high-precision run, and the answer key experiment, but also from the controlled experiment sthat other teams have done (such as, in previous MUC's, NYU's analysis of recovery strategies and SRI' sdecomposition of errors) .
This whole style of experimentation should be embraced as one of the mos trewarding, non-competitive aspects of the evaluations .SUMMARYThe GE system performed well, as we had hoped, on both TST3 and TST4 .
The tests proved some of th epositive advances we had achieved, as well as surprising us somewhat by showing progress in areas wher eour system was already strong .
Some of the experiments and analyses we did as part of the test were a srewarding as the comparative results .
The whole MUC experiment has thus opened up a new methodologythat we are beginning to explore in using comparative and controlled testing to guide algorithmic research .99
