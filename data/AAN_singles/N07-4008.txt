NAACL HLT Demonstration Program, pages 15?16,Rochester, New York, USA, April 2007. c?2007 Association for Computational LinguisticsRavenCalendar: A Multimodal Dialog System for Managing aPersonal Calendar?Svetlana Stenchikova and Basia Mucha and Sarah Hoffman and Amanda StentDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400sveta,basia,shoffman,stent@cs.sunysb.edu1 IntroductionDialog applications for managing calendars havebeen developed for every generation of dialogsystems research (Heidorn, 1978; Yankelovich,1994; Constantinides and others, 1998; Horvitzand Paek, 2000; Vo and Wood, 1996; Huangand others, 2001).
Today, Web-based calendarapplications are widely used.
A spoken dialoginterface to a Web-based calendar applicationpermits convenient use of the system on a hand-held device or over the telephone.In this demo, we present RavenCalendar,a multimodal dialog system built around theGoogle Calendar and Google Maps Web appli-cations.
RavenCalendar allows the user to cre-ate, modify and remove calendar events, queryfor events, and hear descriptions of events.
Inour demonstration we will focus on two aspectsof RavenCalendar: its flexible approach to lan-guage understanding and dialog management,and its multimodal interface.Flexible dialog management In RavenCal-endar, during event management or event query-ing a user can provide event information eitherin a single utterance (e.g.
?Add a meeting fortomorrow from two to three in the computer sci-ence lounge?
or by engaging in a subdialog withthe system (Table 1).
This functionality is madepossible because we use the Ravenclaw dialogmanager (Bohus and Rudnicky, 2003).
?We would like to thank Dan Bohus, Antoine Raux,and Thomas Harris for providing Olympus, and for an-swering many questions about Ravenclaw.
This researchis based on work supported by the National Science Foun-dation under grant no.
0325188.S: Welcome to RavenCalendar,the automated dialog calendar system.S: What can I do for you today?U: add a meeting for MondayS: For what time do you want to add the meeting?U: from ten until elevenS: The event was added type meetingfor tomorrow starting at ten a.m.U: List all events for the next weekTable 1: Sample dialog with RavenCalendarMultimodality A RavenCalendar user mayinteract with the calendar directly using theGoogle Calendar interface, or may interactthrough RavenCalendar using text, speech, mapgestures or a combination of these media.
A usermay use the Google Maps interface to specifythe location of an event; the system uses GoogleMaps to display the locations of events.2 System DescriptionRavenCalendar, whose architecture is shown inFigure 1, is developed using Ravenclaw andOlympus (Bohus and others, 2007).
Olympusis a dialog system shell; Ravenclaw is the Olym-pus dialog manager.
In developing RavenCal-endar, we chose to use an existing dialog shellto save time on system development.
(We aregradually replacing the Olympus componentsfor speech recognition, generation and TTS.
)RavenCalendar is one of the first dialog systemsbased on Olympus to be developed outside ofCMU.
Other Olympus-based systems developedat CMU include the Let?s Go (Raux and oth-ers, 2005), Room Line, and LARRI (Bohus andRudnicky, 2002) systems.Flexible dialog management The Raven-claw dialog manager (Bohus and Rudnicky,2003) allows ?object-oriented?
specification of a15Figure 1: RavenCalendar Designdialog structure.
In RavenCalendar, we definethe dialog as a graph.
Each node in the graphis a minimal dialog component that performs aspecific action and has pre- and post-conditions.The dialog flow is determined by edges betweennodes.
With this structure, we maximize thereuse of minimal dialog components.
Ravenclawgives a natural way to define a dialog, but fine-tuning the dialog manager was the most chal-lenging part of system development.Multimodality In RavenCalendar, a back-end server integrates with Google Calendar forstoring event data.
Also, a maps front end serverintegrates with Google Maps.
In addition to thelocations recognized by Google Maps, an XMLfile with pre-selected location-name mappingshelps the user specify locations.3 Current and Future WorkWe are currently modifying RavenCalendarto use grammar-based speech recognition fortighter integration of speech recognition andparsing, to automatically modify its parsinggrammar to accommodate the words in theuser?s calendar, to permit trainable, adaptableresponse generation, and to connect to addi-tional Web services and Web-based data re-sources.
This last topic is particularly inter-esting to us.
RavenCalendar already uses sev-eral Web-based applications, but there are manyother Web services of potential utility to mo-bile users.
We are now building a componentfor RavenClaw that searches a list of URLs forevent types of interest to the user (e.g.
sportsevents, music events), and automatically notifiesthe user of events of interest.
In the future, weplan to incorporate additional Web-based func-tionality, with the ultimate goal of creating ageneral-purpose dialog interface to Web appli-cations and services.ReferencesD.
Bohus et al 2007.
Olympus: an open-sourceframework for conversational spoken language in-terface research.
In Proceedings of the Workshop?Bridging the Gap?
at HLT/NAACL 2007.D.
Bohus and A. Rudnicky.
2002.
LARRI: Alanguage-based maintenance and repair assistant.In Proceedings of IDS.D.
Bohus and A. Rudnicky.
2003.
Ravenclaw: Dia-log management using hierarchical task decompo-sition and an expectation agenda.
In Proceedingsof Eurospeech.P.
Constantinides et al 1998.
A schema based ap-proach to dialog control.
In Proceedings of ICSLP.G.
Heidorn.
1978.
Natural language dialogue formanaging an on-line calendar.
In Proceedings ofACM/CSCER.E.
Horvitz and T. Paek.
2000.
DeepListener: Har-nessing expected utility to guide clarification dia-log in spoken language systems.
In Proceedings ofICSLP.X.
Huang et al 2001.
MIPAD: A next generationPDA prototype.
In Proceedings of ICSLP.A.
Raux et al 2005.
Let?s go public!
Taking a spo-ken dialog system to the real world.
In Proceedingsof Interspeech.M.
Tue Vo and C. Wood.
1996.
Building an appli-cation framework for speech and pen input inte-gration in multimodal learning interfaces.
In Pro-ceedings of ICASSP.N.
Yankelovich.
1994.
Talking vs taking: Speech ac-cess to remote computers.
In Proceedings of theConference on Human Factors in Computing Sys-tems.16
