Confidence Estimation for Translation PredictionSimona GandraburRALI, Universite?
de Montre?algandrabu@iro.umontreal.caGeorge FosterRALI, Universite?
de Montre?alfoster@iro.umontreal.caAbstractThe purpose of this work is to investigate theuse of machine learning approaches for confi-dence estimation within a statistical machinetranslation application.
Specifically, we at-tempt to learn probabilities of correctness forvarious model predictions, based on the nativeprobabilites (i.e.
the probabilites given by theoriginal model) and on features of the currentcontext.
Our experiments were conducted us-ing three original translation models and twotypes of neural nets (single-layer and multi-layer perceptrons) for the confidence estima-tion task.1 IntroductionMost statistical models used in natural language appli-cations are capable in principle of generating probabilityestimates for their outputs.
However, in practice, theseestimates are often quite poor and are usually interpretedsimply as scores that are monotonic with probabilities.There are many contexts where good estimates of trueprobabilities are desirable:?
in a decision-theoretic setting, posterior probabili-ties are required in order to choose the lowest-costoutput for a given input.?
when a collection of different models is available forsome problem, output probabilities provide a princi-pled and convenient way of combining them; and?
when multiplying conditional probabilities to com-pute joint distributions, the accuracy of the resultis crucially dependent on the stability of the con-ditional estimates across different contexts?this isimportant for applications like speech recognitionand machine translation that perform searches overa large space of output sentences, represented as se-quences of words.Given a statistical model that produces a probabilisticscore, a straightforward way of obtaining a true probabil-ity is to use the score as input to another model whoseoutput is interpreted as the desired probability.
The ideais that the second model can learn how to transform thebase model?s score by observing its performance on newtext, possibly in conjunction with other features.
This ap-proach, which is known as confidence estimation (CE), iswidely used in speech recognition (Guillevic et al, 2002;Moreno et al, 2001; Sanchis et al, 2003; Stolcke et al,1997) but is virtually unknown in other areas of naturallanguage progessing (NLP).
1The alternatives to confidence estimation are tradi-tional smoothing techniques such as backing off to sim-pler models and cross validation, along with carefulmarginalization and scaling where applicable to obtainthe desired posterior probabilities.
There is some evi-dence (Wessel et al, 2001) that this approach can giveresults that are at least as good as those obtainable withan external CE model.
However, CE as we present it hereis not incompatible with traditional techniques, and hasseveral practical advantages.
First, it can easily incorpo-rate specialized features that are highly indicative of howwell the base model will perform on a given input, butthat may be of little use for the task of choosing the out-put.
Since such features may be inconvenient to includein the base model, CE represents a kind of modulariza-tion, particularly as it may be possible to reuse some fea-tures for many different problems.
Another advantage isthat a CE layer is usually much smaller and easier to trainthan the baseline model; this means that it can be usedto rapidly adapt a system?s performance to new domains.Finally, CE typically concentrates on only the top few hy-1A recent exception is Manmatha and Sever (2002), who de-scribe a form of confidence estimation for combining the resultsof different query engines in information retrieval.potheses output by the baseline model, which is an easiertask than estimating a complete distribution.
This is es-pecially true when the hypotheses of interest are drawnfrom a joint distribution that may be impossible in prac-tice to enumerate.In this paper we describe an application of confidenceestimation to an interactive target-text prediction task ina translation setting, using two different types of neuralnets: single-layer perceptron (SLPs) and multi-layer per-ceptrons (MLPs) with 20 hidden units.The main issues that we investigate here are:?
the benefit that can be gained by using confidenceestimates, in discrimination power and/or over-allapplication quality as computed by a simulation thatestimates the benefit to the user;?
the use of different machine learning (ML) tech-niques for CE;?
the relevance of various confidence features; and?
model combinations: we experiment with variousmodel combination schemes based on the CE layerin order to improve the over-all prediction accuracyof the application.Among the more interesting results we will present arethe comparisons between the discrimination capacity ofthe native probabilities and the probabilities of correct-ness produced by the CE layer.
Depending on the un-derlying SMT model, we obtained a relative improve-ment in correct rejection rate (CR) ranging from 3.90%to 33.09% at a fixed 0.80 correct acceptance rate (CA)for prediction lengths of up to four words.
We also mea-sured relative improvements of approximately 10% in es-timated benefit to the user with our application.In the following section we briefly describe the textprediction application we are aiming to improve.
Next weoutline the CE approach and the evaluation methods weapplied.
Finally, we report the results obtained in our ex-periments and conclude with suggestions for future work.2 Text Prediction for TranslatorsThe application we are concerned with in this paper is aninteractive text prediction tool for translators.
The sys-tem observes a translator in the process of typing a targettext and, after every character typed, has the opportunityto display a suggestion about what will come next, basedon the source sentence under translation and the prefix ofits translation that has already been typed.
The transla-tor may incorporate suggestions into the text if they arehelpful, or simply ignore them and keep typing.Suggestions may range in length from 0 characters tothe end of the target sentence; it is up to the system todecide how much text to predict in a given context, bal-ancing the greater potential benefit of longer predictionsagainst a greater likelihood of being wrong, and a highercost to the user (in terms of distraction and editing) if theyare wrong or only partially right.Our solution to the problem of how much text to pre-dict is based on a decision-theoretic framework in whichwe attempt to find the prediction that maximizes the ex-pected benefit to the translator in the current context (Fos-ter et al, 2002b).
Formally, we seek:x?
= argmaxxB(x|h, s), (1)where x is a prediction about what will follow h inthe translation of a source sentence s, and B(x|h, s)is the expected benefit in terms of typing time saved.As described in (Foster et al, 2002b), B(x?m|h, s) =?lk=0p(k|x, h, s)B(x|h, s, k) depends on two mainquantities: the probability p(k|x, h, s) that exactly kcharacters from the beginning of x are correct, andthe benefit B(x|h, s, k) to the translator if this is thecase.
B(x|h, s, k) is estimated from a model of userbehaviour?based on data collected in user trials of thetool?that captures the cost of reading a prediction andperforming any necessary editing, as well as the some-what random nature of people?s decisions to accept.
Pre-diction probabilities p(k|x, h, s) are derived from a statis-tical translation model for p(w|h, s), the probability thatsome word w will follow the target text h in the transla-tion of a source sentence s.Because optimizing (1) directly is expensive, we usea heuristic search procedure to approximate x?.
For eachlength m from 1 to a fixed maximum of M (4 in thispaper), we perform a Viterbi-like beam search with thetranslation model to find the sequence of words w?m=w1, .
.
.
, wmmost likely to follow h. For each such se-quence, we form a corresponding character sequence x?mand evaluate its benefit B(x?m, h, s).
The final output isthe prediction x?mwith maximum benefit, or nothing ifall benefit estimates are negative.To evaluate the system, we simulate a translator?s ac-tions on a given source text, using an existing transla-tion as the text the translator wishes to type, and the usermodel to determine his or her responses to predictionsand to estimate the resulting benefit.
Further details aregiven in (Foster et al, 2002b).2.1 Translation ModelsWe experimented with three different translation modelsfor p(w|h, s).
All have the property of being fast enoughto support real-time searches for predictions of up to 5words.The first model, referred to as Maxent1 below, is a log-linear combination of a trigram language model with amaximum entropy translation component that is an ana-log of the IBM translation model 2 (Brown et al, 1993).This model is described in (Foster, 2000).
Its major weak-ness is that it does not keep track of which words in thecurrent source sentence have already been translated, andhence it is prone to repeating previous suggestions.
Thesecond model, called Maxent2 below, is similar to Max-ent1 but with the addition of extra parameters to limit thisbehaviour (Foster et al, 2002a).The final model, called Bayes below, is also describedin (Foster et al, 2002a).
It is a noisy-channel combinationof a trigram language model and an IBM model 2 for thesource text given target text.
This model has roughly thesame theoretical predictive capability as Maxent2, but un-like the Maxent models it is not discriminatively trained,and hence its native probability estimates tend to be muchworse than theirs.2.2 Computing Smoothed Conditional ProbabilitiesIn order to calculate the character-based probabili-ties p(k|x, h, s) required for estimating expected ben-efit, we need to know the conditional probabilitiesp(w|w1, .
.
.
, wi?1, h, s) that some word w will followw1, .
.
.
, wi?1in the context (h, s).
These are derivedfrom correctness estimates obtained from our confidence-estimation layer as follows.
As explained below, es-timates from the CE layer are in the form p(C =1|w?m, h, s), where w?mis the most probable predictionof length m according to the base translation model.Define a smoothed joint distribution over predictions oflength m as:ps(wm|h, s) ={p(C = 1|w?m, h, s), wm= w?mp(wm|h, s)/zm, else(2)where p(wm|h, s) =?mi=1p(wi|w1, .
.
.
, wi?1, h, s) iscalculated from the conditional probabilities given by thebase model; andzm=1 ?
p(w?m|h, s)1 ?
p(C = 1|w?m, h, s)is a normalization factor.
Then the required smoothedconditional probabilities are estimated from the smoothedjoint distributions in a straightforward way:ps(w|w1, .
.
.
, wi?1, h, s) =ps(w1, .
.
.
, wi?1, w|h, s)ps(w1, .
.
.
, wi?1|h, s),where p(w1, .
.
.
, wi?1|h, s) ?
1 when i = 1.3 Confidence Estimation with Neural NetsOur approach for CE consists in training neural nets to es-timate the conditional probability of correctness p(C =1|w?m, h, s, {w1m, .
.
.
, wnm}), where w?m= w1mis themost probable prediction of length m from a n-best setof alternative predictions according to the base model.
Inour experiments the prediction length m varies between1 and 4 and n is at most 5.
As the n-best predictions{w1m, .
.
.
, wnm} are themselves a function of the context,we will simply note the conditional probability of cor-rectness by p(C = 1|w?m, h, s).We experimented with two types of neural nets: single-layer perceptrons (SLPs) and multi-layer perceptrons(MLPs) with 20 hidden units.
For both, we used asoftmax activation function and gradient descent train-ing with a negative log-likelihood error function.
Givensuitably-behaved class-conditional feature distributions,this setup is guaranteed to yield estimates of the true pos-terior probabilities p(C = 1|w?m, h, s) (Bishop, 1995).3.1 Single Layer Neural Nets and MaximumEntropy ModelsIt is interesting to note the relation between the SLP andmaximum entropy models.
For the problem of estimatingp(y|x) for a set of classes y over a space of input vectorsx, a single-layer neural net with ?softmax?
outputs takesthe form:p(y|x) = exp(~?y?
x + b)/Z(x)where ~?yis a vector of weights for class y, b is a biasterm, and Z(x) is a normalization factor, the sum overall classes of the numerator.
A maximum entropy modelis a generalization of this in which an arbitrary featurefunction fy(x) is used to transform the input space as afunction of y:p(y|x) = exp(~?
?
fy(x))/Z(x).Both models are trained by maximum likelihood meth-ods.
Given C classes, the maximum entropy model cansimulate a SLP by dividing its weight vector into Cblocks, each the size of x, then using fy(x) to pick outthe yth block:fy(x) = (01, .
.
.
, 0y?1, x, 0y+1, .
.
.
, 0C, 1),where each 0iis a vector of 0?s and the final 1 yields abias term.The advantage of maximum-entropy models is thattheir features can depend on the target class.
For natural-language applications where target classes correspond towords, this produces an economical and powerful repre-sentation.
However, for CE, where the output is binary(correct or incorrect), this capacity is less interesting.
Infact, there is no a priori reason to use a different set offeatures for correct outputs or incorrect ones, so the nat-ural form of a maxent model for this problem is identicalto a SLP (modulo a bias term).
Therefore the experimentswe describe below can be seen as a comparison betweenmaxent models and neural nets with a hidden layer.3.2 Confidence FeaturesThe features we use can be divided into three families:ones designed to capture the intrinsic difficulty of thesource sentence s (for any NLP task); ones intended toreflect how hard s is to translate in general, and ones in-tended to reflect how hard s is for the current model totranslate.
For the first two families, we used two sets ofvalues: static ones that depend on s; and dynamic onesthat depend on only those words in s that are deemedto be still untranslated, as determined by an IBM2 wordalignment between s and h. The features are:?
family 1: trigram perplexity, minimum trigram wordprobability, average word frequency, average wordlength, and number of words;?
family 2: average number of translations per sourceword (according to an independent IBM1), averageIBM1 source word entropy, number of source tokensstill to be translated, number of unknown source to-kens, ratio of linked to unlinked source words withinthe aligned region of the source sentence, and lengthof the current target-text prefix; and?
family 3: average number of search hypothesespruned (ie outside the beam) per time step, finalsearch lattice size, active vocabulary size (numberof target words considered in the search), number ofnbest hypotheses, rank of current hypothesis, prob-ability ratio of best hypothesis to sum of top 5 hy-potheses, and base model probability of current pre-diction.4 EvaluationEvaluation is performed using test sets of translation pre-dictions, each tagged as correct or incorrect.
A translationprediction wmis tagged as correct if and only if an iden-tical word sequence is found in the reference translation,properly aligned.
This reflects our application, where weattempt to match what a particular translator has in mind,not simply produce any correct translation.
We use twotypes of evaluation methods: ROC curves and a user sim-ulation as described above.4.1 ROC curvesConsider a set of tokens ti?
D from given domain D.Each token tiis labelled with a tag C(ti) = 1 if it isconsidered correct or C(ti) = 0 if it is false.
Consider afunction s : D ?
[a, b] that associates a confidence scores(t) ?
[a, b] to any token ti?
D. s is not necessarily aprobability, it can range over any real interval [a, b].Given a rejection threshold ?
?
[a, b], any token ti?D is rejected if s(ti) < ?
and it is accepted otherwise.The correct acceptence rate CA(?)
of a threshold ?
overD is the rate of correct tokens ti?
D with s(ti) ?
?.That is:CA(?)
=|{ti?
D | C(ti) = 1 ?
s(ti) ?
?}||{ti?
D | C(ti) = 1}|.
(3)Similarly, the correct rejection rate CR(?)
is the rateof false tokens tisuch that s(ti) < ?:CR(?)
=|{ti?
D | C(ti) = 0 ?
s(ti) < ?}||{ti?
D | C(ti) = 0}|.
(4)As ?
ranges over [a, b], the value pairs(CA(?
), CR(?))
?
[0, 1] ?
[0, 1] define a curve,called the ROC curve of s over D. The discriminationcapacity of s is given by its capacity to distinguishcorrect from false tokens.
Consequently, a perfect ROCcurve would describe the square (0, 1), (1, 1), (1, 0).This is the case whenever there exists a threshold?
?
[a, b] that separates all correct tokens in D fromall the false ones, meaning that the score ranges ofcorrect, respectively false, tokens don?t overlap.
Theworst case scenario, describing a scoring function thatis completely irrelevent for correct/false discrimination,corresponds to the diagonal (0, 1), (1, 0).
Note that theinverse of the ideal ROC curve, the plot overlapping theaxes (1, 0), (0, 0), (1, 0) is equivalent to its inverse froma discrimination capacity point of view: it suffices toinvert the rejection algorithm by accepting all tokens thathave a score inferior to the rejection threshold.In our setting, the tokens are the w?mtranslation predic-tions and the score function is the conditional probabilityp(C = 1|w?m, h, s).In order to easily compare the discrimination capacityof various scoring functions we use a raw measure, theintegral of the ROC curve, or IROC.
A perfect ROC curvewill have an IROC = 1.0 (respectively 0.0 in the inversecase).
The worst case scenario corresponds to an IROCof 0.5.
We also compare various scoring functions byfixing an operational point at CA = 0.80 and observingthe corresponding CR values.5 Experimental Set-upThe data for our experiments originates from the HansardEnglish-French parallel corpus.
In order to generate thetrain and test sets, we use 1.3 million (900000 for train-ing and 400000 for testing purposes) translation predic-tions for each fixed prediction length of one, two, threeand four words, summing to a total of 5.2 million pre-diction examples.
Each original SMT model experimentwas combined with two different CE model architectures:MLPs with one hidden layer containing 20 hidden unitsand SLP (sometimes also referred to as MLPs with 0 hid-den units).
Moreover, for each (native model, CE modelarchitecture)-pair, we train five separate CE models: oneBayes: m = 1, .
.
.
, 4, CA = 0.80Model IROC CRnative probability 0.8019 0.6604SLP 0.8357 0.7211MLP 0.8679 0.7728Table 1: Comparison of discrimination capacity betweenthe Bayes prediction model probability and the CE of thecorresponding SLP and MLP on predictions of up to fourwordsfor each fixed prediction length of one, two, three or fourwords, and an additional model for variable predictionlengths of up to four words.26 ROC EvaluationsIn this section we report the ROC evaluation results.
Theuser-model evaluation results are presented in the follow-ing section.6.1 CE and Native SMT ProbabilitesThe first question we wish to address is whether we canimprove the correct/false discrimination capacity by us-ing the propability of correctness estimated by the CEmodel instead of the native probabilites.For each SMT model we compare the ROC plots,IROC and CA/CR values obtained by the native proba-bility and the estimated probability of correctness outputby the corresponding SLPs (also noted as mlp-0-hu) andthe 20 hidden units MLPs on the one-to-four word pre-diction task.Results obtained for various length predictions of upto four words using the Bayes models are summarized infigure (1)and in table 1 below, and are encouraging.
At afixed CA of 0.80 we obtain CR increases from 0.6604 forthe native probability to 0.7211 for the SLP and 0.7728for the MLP.
The over-all gain is also evident from thethe relative improvements in IROC obtained by the SLPand MLP models over the native probability, that are re-spectively 17.06% and 33.31%.
These results are quitesignificant.Note that the improvements obtained in the fixed-length 4-word-prediction tasks with the Bayes model (fig-ure (2) and table 2) model are even larger: the relativeimprovements on IROC are 32.36% and 50.07% for theSLP and the MLP, respectively.However, the results obtained in the Maxent modelsare much less positive: the SLP CR actually drops, whilethe MLP CR only increases slightly to a 4.80% relative2Training and testing of the neural nets was done us-ing the open-source Torch toolkit ((Collobert et al, 2002),http://www.torch.ch/), which provides efficient C++ implemen-tations of many ML algorithms.Figure 1: Bayes: m = 1, .
.
.
, 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91CACRmlp?20?humlp?0?huorig.
prob.Bayes: m = 4, CA = 0.80Model IROC CRnative probability 0.7281 0.4998SLP 0.8161 0.6602MLP 0.8560 0.7503Table 2: Comparison of discrimination capacity betweenthe Bayes prediction model probability and the CE of thecorresponding SLP and MLP on fixed-length predictionsof four wordsFigure 2: Bayes: m = 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91CACRmlp?20?humlp?0?huorig.
prob.Maxent1: m = 1, .
.
.
, 4, CA = 0.80Model IROC CRnative probability 0.8581 0.7467SLP 0.8401 0.7142MLP 0.8636 0.7561Table 3: Comparison of discrimination capacity betweenthe Maxent1 prediction model probability and the CE ofthe corresponding SLP and MLP on predictions of up tofour wordsMaxent2: m = 1, .
.
.
, 4, CA = 0.80Model IROC CRnative probability 0.8595 0.7479SLP 0.8352 0.6973MLP 0.8638 0.7599Table 4: Comparison of discrimination capacity betweenthe Maxent2 prediction model probability and the CE ofthe corresponding SLP and MLP on predictions of up tofour wordsimprovement in the CR rate for the Maxent1 model ( ta-ble 3) and only 3.9% for the Maxent2 model ( table 4).The results obtained with the two Maxent models are verysimilar.
We therefore only draw the ROC curve for theMaxent2 model (figure (3).It is interesting to note that the native model predic-tion accuracy didn?t affect the discrimination capacity ofthe corresponding probability of correctness of the CEmodels.
This result is illustrated in table below, where%C = 1 is the percentage of correct predictions.
Eventhough the Bayes?
model accuracy and IROC is signifi-cantly lower then the Maxent model?s, the CE IROC val-ues are almost identical.6.2 Relevance of Confidence FeaturesWe investigated the relevance of different confidence fea-tures by using the IROC values of single-feature modelsfor the 1?4 word prediction task, with both Maxent1 andBayes base models.The group of features that performs best over bothmodels are the model- and search-dependent features de-scribed above, followed by the features that capture theintrinsic difficulty of the source sentence and the target-prefix.
Least valuable are the remaining features thatcapture translation difficulty.
The single most significantfeature is native probability, followed by the probabilityratio of the best hypothesis, and the prediction length.Somewhat unsurprisingly, the weaker Bayes models aremuch more sensitive to longer translations than the Max-ent models.Figure 3: Maxent2: m = 1, .
.
.
, 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91CACRmlp?20?humlp?0?huorig.
prob.Discrimination vs. prediction length: Maxent2Prediction %C=1 IROC native IROC CElength m probability MLPm = 1 44.78 0.7926 0.7986m = 2 23.30 0.8074 0.8121m = 3 13.12 0.8261 0.8245m = 4 7.74 0.8517 0.8567m = 1, ..., 4 22.23 0.8595 0.8638Table 5: Impact of prediction length on discriminationcapacity and accuracy for the Maxent2 prediction model6.3 Dealing with predictions of various lengthsWe compared different approaches for dealing with vari-ous length predictions: we trained four separate MLPs forfixed length predictions of one through four words; and asingle MLP over predictions of varying lengths.
Resultsare given in table 5 and figure (4)7 Model CombinationIn this section we describe how various model combi-nations schemes affect prediction accuracy.
We use theBayes and the Maxent2 prediction models: we try to ex-ploit the fact that these two models, being fundamentallydifferent, tend to be complementary in some of their re-sponses.
The CE models we use are the correspondingMLPs, as they clearly outperform the SLPs.
The resultspresented in table 6 are reported on the variable-lengthprediction task for up to four words.The combination schemes are the following: we runthe two prediction models in parallel and choose one ofthe proposed prediction hypotheses according to the fol-lowing voting criteria:?
Maximum CE vote: choose the prediction with thehighest CE;Figure 4: Maxent2: m = 1, m = 2, m = 3, m = 4, m =1, .
.
.
, 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91CACRm = 1m = 2m = 3m = 4m = 1, ?, 4Model Combination Prediction AccuracyPrediction model combination AccuracyBayes alone 8.77Maxent alone 22.23Max native probability vote combination 17.49Max CE vote combination 23.86Optimal combination 27.79Table 6: Prediction accuracy of the Bayes and Maxent2model compared with combined model accuracy?
Maximum native probability vote: choose the pre-diction with the highest native probability.As a baseline comparison, we use the accuracy of theindividual native prediction models.
Then we computethe maximum gain we can expect with an optimal modelcombination strategy, obtained by running an ?oracle?that always picks the right answer.The results are very positive: the maximum CE votingscheme obtains a 29.31% of the maximum possible ac-curacy gain over the better of the two indiviual models(Maxent2).
Moreover, if we choose the maximum nativeprobability vote, the overall accuracy actually drops sig-nificantly.
These results are a strong motivation for ourpost-prediction confidence estimation approach: by train-ing an additional CE layer using the same confidence fea-tures and training data for different underlying predictionmodels we obtain more uniform estimates of the proba-bility of correctness.8 User-Model EvaluationsAs described in section 2, we evaluated the predictionsystem as a whole by simulating the actions of a trans-lator on a given source text and measuring the gainmodel base mults SLP MLP bestBayes 3.2 6.5 6.4 6.4 11.8ME1 16.6 16.5 18.1 18.3 23.5ME2 17.4 17.4 19.0 19.3 24.3Table 7: Percentage of typing time saved for various CEconfigurations.with a user model.
In order to abstract away from ap-proximations made in deriving character-based proba-bilities p(k|x, h, s) used in the benefit calculation fromword-based probabilities, we employed a specialized usermodel.
In contrast to the realistic model described in(Foster et al, 2002b), this assumes that users accept pre-dictions only at the beginnings of words, and only whenthey are correct in their entirety.
To reduce variation fur-ther, it also assumes that the user always accepts a correctprediction as soon as it is suggested.
Thus the model?sestimates of benefit to the user may be slightly over-optimistic: the limited opportunities for accepting andediting must be balanced against the user?s inhumanlyperfect decision-making.
However, its main purpose isnot realism but simply to allow for a fair comparison be-tween the base and the CE models.Simulations with all three translation models were per-formed using a 500-sentence test text.
At each predictionpoint, the benefits associated with best predictions of 1?4words in length were compared to decide which (if any)to propose.
The results, in terms of percentages of typingtime saved, are shown in table 8: base corresponds to thebase model; mults to length-specific probability multipli-ers tuned to optimize benefit on a held-out corpus; SLPand MLP to CE estimates; and best to using an oracle topick the length that maximizes benefit.Although the CE layer provides no gain over the muchsimpler probability-multiplier approach for the Bayesmodel, the gain for both maxent models is substantial,around 10% in relative terms and 25% of the theoreticalmaximum gain (over the base model) with the MLP andslightly lower with the SLP.9 ConclusionThe results obtained in this paper can be summarized inthe following set of questions and answers:?
Can the probabilities of correctness estimated bythe CE layer exceed the native probablities in dis-crimination capacity?
Depending on the underlyingSMT model, we obtained a relative improvement incorrect rejection rate (CR) ranging from 3.90% to33.09% at a fixed 0.80 (CA) correct acceptance ratefor prediction lengths varying between 1 and 4.?
Can we improve the overall performance of the un-derlying SMT application using confidence estima-tion?
In simulated results, we found a significantgain (10% relative) in benefit to a translator due tothe use of a CE layer in two of three translation mod-els tested.?
Can prediction accuracy of the application be im-proved using prediction model combinations?
Amaximum CE voting scheme yields a 29.31% ac-curacy improvement of the maximum possible ac-curacy gain.
A similar voting scheme using nativeprobabilies significantly decreases the accuracy ofthe model combination.?
How does the prediction accuracy of the native mod-els influence the CE accuracy?
Prediction accuracydidn?t prove to be a significant factor in determiningthe discrimination capacity of the confidence esti-mate.?
How does CE accuracy change with various ML ap-proches?
A multi-layer perceptron (MLP) with 20hidden units significantly outperformed one with 0hidden units (equivalent to a maxent model for thisapplication).?
Confidence feature selection: which confidence fea-tures are more useful and how does their discrimi-nation capacity vary with different contexts and dif-ferent native SMT models?
Confidence featuresbased on the original model and the n-best predic-tion turned out to be the most relevant group of fea-tured, folowed by features that capture the intrinsicdifficulty of the source text and finally translation-difficulty-specific features.
We also observed inter-esting variations in relevance as the original modelschanged.Future work will include the search for more relevantconfidence features, such as features based on consenusover word-lattices ((Mangu et al, 2000)), past perfor-mance, the use of more appropriate correct/false taggingmethods and experiments with different machine learningtechniques.
Finally, we would like to investigate whetherconfidence estimation can be used to improve the modelprediction accuray, either by using re-scoring techniquesor using the confidence estimates during search (decod-ing).ReferencesChristopher M. Bishop.
1995.
Neural Networks for Pat-tern Recognition.
Oxford.Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer.
1993.
The mathematicsof Machine Translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?312, June.R.
Collobert, S. Bengio, and J. Marie?thoz.
2002.
Torch:a modular machine learning software library.
Techni-cal Report IDIAP-RR 02-46, IDIAP.George Foster, Philippe Langlais, and Guy Lapalme.2002a.
Text prediction with fuzzy alignments.
InStephen D. Richardson, editor, Proceedings of the 5thConference of the Association for Machine Transla-tion in the Americas, Tiburon, California, October.Springer-Verlag.George Foster, Philippe Langlais, and Guy Lapalme.2002b.
User-friendly text prediction for translators.In Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Philadelphia, PA.George Foster.
2000.
Incorporating position infor-mation into a Maximum Entropy / Minimum Di-vergence translation model.
In Proceedings of the4th Computational Natural Language Learning Work-shop (CoNLL), Lisbon, Portugal, September.
ACLSigNLL.Didier Guillevic, Simona Gandrabur, and Yves Nor-mandin.
2002.
Robust semantic confidence scoring.In Proceedings of the 7th International Conference onSpoken Language Processing (ICSLP) 2002, Denver,Colorado, September.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding con-sensus in speech recognition: word error minimizationand other applications of confusion networks.
Com-puter Speech and Language, 14(4):373?400.R.
Manmatha and H. Sever.
2002.
A formal approachto score normalization for meta-search.
In M. Mar-cus, editor, Proceedings of HLT 2002, Second Inter-national Conference on Human Language TechnologyResearch, pages 98?103, San Francisco.
Morgan Kauf-mann.P.
Moreno, B. Logan, and B. Raj.
2001.
A boostingapproach for confidence scoring.
In Eurospeech.A.
Sanchis, A. Juan, and E. Vidal.
2003.
A simple hy-brid aligner for generating lexical correspondences inparallel texts.
In ICASSP 2003, pages 29?35.A.
Stolcke, Y. Koenig, and M. Weintraub.
1997.
Explicitword error minimization in n-best list rescoring.
InProc.
5th Eur.
Conf.
Speech Communication and Tech-nology, volume 1, pages 163?166.Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and Her-mann Ney.
2001.
Confidence measures for large vo-cabulary continuous speech recognition.
IEEE Trans-actions on Speech and Audio Processsing, 9(3):288?298.
