Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1586?1596,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproving Arabic Dependency Parsing with Form-based and FunctionalMorphological FeaturesYuval MartonT.J.
Watson Research CenterIBMyymarton@us.ibm.comNizar Habash and Owen RambowCenter for Computational Learning SystemsColumbia University{habash,rambow}@ccls.columbia.eduAbstractWe explore the contribution of morphologi-cal features ?
both lexical and inflectional ?to dependency parsing of Arabic, a morpho-logically rich language.
Using controlled ex-periments, we find that definiteness, person,number, gender, and the undiacritzed lemmaare most helpful for parsing on automaticallytagged input.
We further contrast the contri-bution of form-based and functional features,and show that functional gender and number(e.g., ?broken plurals?)
and the related ratio-nality feature improve over form-based fea-tures.
It is the first time functional morpho-logical features are used for Arabic NLP.1 IntroductionParsers need to learn the syntax of the modeled lan-guage in order to project structure on newly seensentences.
Parsing model design aims to come upwith features that best help parsers to learn the syn-tax and choose among different parses.
One as-pect of syntax, which is often not explicitly mod-eled in parsing, involves morphological constraintson syntactic structure, such as agreement, which of-ten plays an important role in morphologically richlanguages.
In this paper, we explore the role ofmorphological features in parsing Modern StandardArabic (MSA).
For MSA, the space of possible mor-phological features is fairly large.
We determinewhich morphological features help and why.
Wealso explore going beyond the easily detectable, reg-ular form-based (?surface?)
features, by represent-ing functional values for some morphological fea-tures.
We expect that representing lexical abstrac-tions and inflectional features participating in agree-ment relations would help parsing quality, but otherinflectional features would not help.
We further ex-pect functional features to be superior to surface-only features.The paper is structured as follows.
We firstpresent the corpus we use (Section 2), then rele-vant Arabic linguistic facts (Section 3); we surveyrelated work (Section 4), describe our experiments(Section 5), and conclude with an analysis of pars-ing error types (Section 6).2 CorpusWe use the Columbia Arabic Treebank (CATiB)(Habash and Roth, 2009).
Specifically, we usethe portion converted automatically from part 3 ofthe Penn Arabic Treebank (PATB) (Maamouri etal., 2004) to the CATiB format, which enriches theCATiB dependency trees with full PATB morpho-logical information.
CATiB?s dependency represen-tation is based on traditional Arabic grammar andemphasizes syntactic case relations.
It has a re-duced POS tagset (with six tags only ?
henceforthCATIB6), but a standard set of eight dependency re-lations: SBJ and OBJ for subject and (direct or indi-rect) object, respectively, (whether they appear pre-or post-verbally); IDF for the idafa (possessive) re-lation; MOD for most other modifications; and otherless common relations that we will not discuss here.For more information, see Habash et al (2009).
TheCATiB treebank uses the word segmentation of thePATB: it splits off several categories of orthographicclitics, but not the definite article ?
@ Al.
In all of theexperiments reported in this paper, we use the gold1586VRB??
?K t?ml?work?MODPRT??
fy?in?OBJNOM?P@Y??
@ AlmdArs?the-schools?MODNOM?J???m?
'@ AlHkwmy~?the-governmental?SBJNOMH@YJ?k HfydAt?granddaughters?MODNOMHAJ?Y?
@ Al?kyAt?smart?IDFNOMI.KA??
@ AlkAtb?the-writer?Figure 1: CATiB Annotation example (tree display from rightto left).?J???m?
'@ ?P@Y??
@ ??
HAJ?Y?
@ I.KA??
@ H@YJ?k ??
?Kt?ml HfydAt AlkAtb Al?kyAt fy AlmdArs AlHkwmy~ ?Thewriter?s smart granddaughters work for public schools.?segmentation.
An example CATiB dependency treeis shown in Figure 1.3 Relevant Linguistic ConceptsIn this section, we present the linguistic concepts rel-evant to our discussion of Arabic parsing.Orthography The Arabic script uses optional di-acritics to represent short vowels, consonantal dou-bling and the indefininteness morpheme (nunation).For example, the word I.J?
kataba ?he wrote?
is of-ten written as I.J?ktb, which can be ambiguous withother words such as I.J?
kutubu?
?books?.
In newstext, only around 1.6% of all words have any dia-critic (Habash, 2010).
As expected, the lack of dia-critics contributes heavily to Arabic?s morphologicalambiguity.
In this work, we only use undiacritizedtext; however, some of our parsing features whichare derived through morphological disambiguationinclude diacritics (specifically, lemmas, see below).Morphemes Words can be described in terms oftheir morphemes; in Arabic, in addition to concate-native prefixes and suffixes, there are templatic mor-phemes called root and pattern.
For example, theword 	?
?J.KA?Kyu+kAtib+uwn ?they correspond?
hasone prefix and one suffix, in addition to a stem com-posed of the root H.H?
k-t-b ?writing related?
andthe pattern 1A2i3.1Lexeme and features Alternatively, Arabicwords can be described in terms of lexemes andinflectional features.
The set of word forms thatonly vary inflectionally among each other is calledthe lexeme.
A lemma is a specific word form cho-sen to represent the lexeme word set; for example,Arabic verb lemmas are third person masculine sin-gular perfective.
We explore using both the dia-critized lemma and the undiacritized lemma (here-after LMM).
Just as the lemma abstracts over in-flectional morphology, the root abstracts over bothinflectional and derivational morphology and thusprovides a deeper level of lexical abstraction, indi-cating the ?core?
meaning of the word.
The pat-tern is a generally complementary abstraction some-times indicating sematic notions such causation andreflexiveness.
We use the pattern of the lemma, notof the word form.
We group the ROOT, PATTERN,LEMMA and LMM in our discussion as lexical fea-tures.
Nominal lexemes can also be classified intotwo groups: rational (i.e., human) or irrational (i.e.
?non-human).2 The rationality feature interacts withsyntactic agreement and other inflectional features(discussed next); as such, we group it with those fea-tures in this paper?s experiments.The inflectional features define the the space ofvariations of the word forms associated with a lex-eme.
PATB-tokenized words vary along nine di-mensions: GENDER and NUMBER (for nominals andverbs); PERSON, ASPECT, VOICE and MOOD (forverbs); and CASE, STATE, and the attached defi-nite article proclitic DET (for nominals).
Inflectionalfeatures abstract away from the specifics of mor-pheme forms.
Some inflectional features affect morethan one morpheme in the same word.
For exam-ple, changing the value of the ASPECT feature inthe example above from imperfective to perfectiveyields the word form @?J.KA?
kAtab+uwA ?they corre-sponded?, which differs in terms of prefix, suffix andpattern.1The digits in the pattern correspond to the positions rootradicals are inserted.2Note that rationality (?human-ness?
???A?
Q?/??A??)
is nar-rower than animacy; its expression is wide-spead in Arabic, butless so English, where it mainly shows in pronouns (he/she vs.it) and relativizers (the student who... vs. the desk/bird which...).1587Surface vs. functional features Additionally,some inflectional features, specifically gender andnumber, are expressed using different morphemesin different words (even within the same part-of-speech).
There are four sound gender-number suf-fixes in Arabic:3 +?
(null morpheme) for masculinesingular,?+ +~ for feminine singular, 	?
?+ +wn formasculine plural and H@+ +At for feminine plural.Plurality can be expressed using sound plural suf-fixes or using a pattern change together with singu-lar suffixes.
A sound plural example is the word pairH@YJ?k/?YJ?k Hafiyd+a~/Hafiyd+At ?granddaugh-ter/granddaughters?.
On the other hand, the plural ofthe inflectionally and morphemically feminine sin-gular word??PY?
madras+a~ ?school?
is the word?P@Y?
madAris+?
?schools?, which is feminine andplural inflectionally, but has a masculine singularsuffix.
This irregular inflection, known as brokenplural, is similar to the English mouse/mice, but ismuch more common in Arabic (over 50% of pluralsin our training data).
A similar inconsistency ap-pears in feminine nouns that are not inflected usingsound gender suffixes, e.g., the feminine form of themasculine singular adjective?P 	P@ ?zraq+?
?blue?
isZAP?
P zarqA?+?
not?P?
P@* *?zraq+a~.
To addressthis inconsistency in the correspondence between in-flectional features and morphemes, and inspired by(Smr?, 2007), we distinguish between two types ofinflectional features: surface (or form-based)4 fea-tures and functional features.Most available Arabic NLP tools and resourcesmodel morphology using surface inflectional fea-tures and do not mark rationality; this includes thePATB (Maamouri et al, 2004), the Buckwalter mor-phological analyzer (BAMA) (Buckwalter, 2004)and tools using them such as the MorphologicalAnalysis and Disambiguation for Arabic (MADA)system (Habash and Rambow, 2005).
The Elixir-FM analyzer (Smr?, 2007) readily provides the func-tional inflectional number feature, but not full func-tional gender (only for adjectives and verbs but notfor nouns), nor rationality.
Most recently, Alkuhlaniand Habash (2011) present a version of the PATB(part 3) that is annotated for functional gender, num-3We ignore duals, which are regular in Arabic, and case/statevariations in this discussion for simplicity.4Smr?
(2007) uses the term illusory for surface features.ber and rationality features for Arabic.
We use thisresource in modeling these features in Section 5.5.Morpho-syntactic interactions Inflectional fea-tures and rationality interact with syntax in twoways.
In agreement relations, two words in a spe-cific syntactic configuration have coordinated valuesfor specific sets of features.
MSA has standard (i.e.,matching value) agreement for subject-verb pairs onPERSON, GENDER, and NUMBER, and for noun-adjective pairs on NUMBER, GENDER, CASE, andDET.
There are three very common cases of excep-tional agreement: verbs preceding subjects are al-ways singular, adjectives of irrational plural nounsare always feminine singular, and verbs whose sub-jects are irrational plural are also always femininesingular.
See the example in Figure 1: the adjective,HAJ?Y?
@ Al?kyAt ?smart?, of the feminine plural (andrational) H@YJ?k HafiydAt ?granddaughters?
is fem-inine plural; but the adjective,?J???m?
'@ AlHkwmy~?the-governmental?, of the feminine plural (and irra-tional) ?P@Y?
madAris ?schools?
is feminine singu-lar.
These agreement rules always refer to functionalmorphology categories; they are orthogonal to themorpheme-feature inconsistency discussed above.MSA exhibits marking relations in CASE andSTATE marking.
Different types of dependentshave different CASE, e.g., verbal subjects are al-ways marked NOMINATIVE.
CASE and STATE arerarely explicitly manifested in undiacritized MSA.The DET feature plays an important role in distin-guishing between the N-N idafa (possessive) con-struction, in which only the last noun may bear thedefinite article, and the N-A modifier construction,in which both elements generally exhibit agreementin definiteness.Lexical features do not constrain syntactic struc-ture as inflectional features do.
Instead, bilexicaldependencies are used to model semantic relationswhich often are the only way to disambiguate amongdifferent possible syntactic structures.
Lexical ab-straction also reduces data sparseness.The core POS tagsets Words also have associ-ated part-of-speech (POS) tags, e.g., ?verb?, whichfurther abstract over morphologically and syntac-tically similar lexemes.
Traditional Arabic gram-mars often describe a very general three-way dis-tinction into verbs, nominals and particles.
In com-1588parison, the tagset of the Buckwalter Morphologi-cal Analyzer (Buckwalter, 2004) used in the PATBhas a core POS set of 44 tags (before morphologi-cal extension).
Cross-linguistically, a core set con-taining around 12 tags is often assumed, including:noun, proper noun, verb, adjective, adverb, preposi-tion, particles, connectives, and punctuation.
Hence-forth, we reduce CORE44 to such a tagset, and dubit CORE12.
The CATIB6 tagset can be viewed asa further reduction, with the exception that CATIB6contains a passive voice tag; however, this tag con-stitutes only 0.5% of the tags in the training.Extended POS tagsets The notion of ?POStagset?
in natural language processing usually doesnot refer to a core set.
Instead, the Penn EnglishTreebank (PTB) uses a set of 46 tags, includingnot only the core POS, but also the complete setof morphological features (this tagset is still fairlysmall since English is morphologically impover-ished).
In PATB-tokenized MSA, the correspondingtype of tagset (core POS extended with a completedescription of morphology) would contain upwardsof 2,000 tags, many of which are extremely rare (inour training corpus of about 300,000 words, we en-counter only 430 of such POS tags with completemorphology).
Therefore, researchers have proposedtagsets for MSA whose size is similar to that of theEnglish PTB tagset, as this has proven to be a usefulsize computationally.
These tagsets are hybrids inthe sense that they are neither simply the core POS,nor the complete morphologically enriched tagset,but instead they selectively enrich the core POStagset with only certain morphological features.
Afull dicussion of how these tagsets affect parsing ispresented in Marton et al (2010); we summarize themain points here.The following are the various tagsets we use inthis paper: (a) the core POS tagset CORE12; (b)the CATiB treebank tagset CATIBEX, a newly in-troduced extension of CATIB6 (Habash and Roth,2009) by simple regular expressions of the wordform, indicating particular morphemes such as theprefix ?
@ Al+ or the suffix 	??
+wn; this tagsetis the best-performing tagset for Arabic on pre-dicted values.
(c) the PATB full tagset (BW), size?2000+ (Buckwalter, 2004); We only discuss herethe best performing tagsets (on predicted values),and BW for comparison.4 Related WorkMuch work has been done on the use of morpholog-ical features for parsing of morphologically rich lan-guages.
Collins et al (1999) report that an optimaltagset for parsing Czech consists of a basic POS tagplus a CASE feature (when applicable).
This tagset(size 58) outperforms the basic Czech POS tagset(size 13) and the complete tagset (size ?3000+).They also report that the use of gender, number andperson features did not yield any improvements.
Wegot similar results for CASE in the gold experimen-tal setting (Marton et al, 2010) but not when usingpredicted POS tags (POS tagger output).
This maybe a result of CASE tagging having a lower error ratein Czech (5.0%) (Hajic?
and Vidov?-Hladk?, 1998)compared to Arabic (?14.0%, see Table 2).
Simi-larly, Cowan and Collins (2005) report that the useof a subset of Spanish morphological features (num-ber for adjectives, determiners, nouns, pronouns,and verbs; and mode for verbs) outperforms othercombinations.
Our approach is comparable to theirwork in terms of its systematic exploration of thespace of morphological features.
We also find thatthe number feature helps for Arabic.
Looking at He-brew, a Semitic language related to Arabic, Tsarfatyand Sima?an (2007) report that extending POS andphrase structure tags with definiteness informationhelps unlexicalized PCFG parsing.As for work on Arabic, results have been reportedon PATB (Kulick et al, 2006; Diab, 2007; Greenand Manning, 2010), the Prague Dependency Tree-bank (PADT) (Buchholz and Marsi, 2006; Nivre,2008) and the Columbia Arabic Treebank (CATiB)(Habash and Roth, 2009).
Recently, Green andManning (2010) analyzed the PATB for annotationconsistency, and introduced an enhanced split-stateconstituency grammar, including labels for shortIdafa constructions and verbal or equational clauses.Nivre (2008) reports experiments on Arabic pars-ing using his MaltParser (Nivre et al, 2007), trainedon the PADT.
His results are not directly compara-ble to ours because of the different treebanks?
repre-sentations, even though all the experiments reportedhere were performed using MaltParser.
Our resultsagree with previous work on Arabic and Hebrew inthat marking the definite article is helpful for pars-ing.
However, we go beyond previous work in that1589we also extend this morphologically enhanced fea-ture set to include additional lexical and inflectionalfeatures.
Previous work with MaltParser in Russian,Turkish and Hindi showed gains with case but notwith agreement features (Nivre et al, 2008; Eryigitet al, 2008; Nivre, 2009).
Our work is the first usingMaltParser to show gains using agreement-orientedfeatures (Marton et al, 2010), and the first to usefunctional features for this task (this paper).5 ExperimentsThroughout this section, we only report results us-ing predicted input feature values (e.g., generatedautomatically by a POS tagger).
After presentingthe parser we use (Section 5.1), we examine a largespace of settings in the following order: the contri-bution of numerous inflectional features in a con-trolled fashion (Section 5.2);5 the contribution ofthe lexical features in a similar fashion, as well asthe combination of lexical and inflectional features(Section 5.3); an extension of the DET feature (Sec-tion 5.4); using functional NUMBER and GENDERfeature values, as well as the RATIONALITY feature(Section 5.5); finally, putting best feature combina-tions to test with the best-performing POS tagset,and on an unseen test set (Section 5.6).
All resultsare reported mainly in terms of labeled attachmentaccuracy score (parent word and the dependency re-lation to it, a.k.a.
LAS).
Unlabeled attachment ac-curacy score (UAS) is also given.
We use McNe-mar?s statistical significance test as implemented byNilsson and Nivre (2008), and denote p < 0.05 andp < 0.01 with + and ++, respectively.5.1 ParserFor all experiments reported here we used the syn-tactic dependency parser MaltParser v1.3 (Nivre,2003; Nivre, 2008; K?bler et al, 2009) ?
atransition-based parser with an input buffer and astack, using SVM classifiers to predict the next statein the parse derivation.
All experiments were doneusing the Nivre "eager" algorithm.6 For training, de-5In this paper, we do not examine the contribution of differ-ent POS tagsets, see Marton et al (2010) for details.6Nivre (2008) reports that non-projective and pseudo-projective algorithms outperform the "eager" projective algo-rithm in MaltParser, but our training data did not contain anynon-projective dependencies.
The Nivre "standard" algorithmvelopment and testing, we follow the splits used byRoth et al (2008) for PATB part 3 (Maamouri et al,2004).
We kept the test unseen during training.There are five default attributes, in the MaltParserterminology, for each token in the text: word ID (or-dinal position in the sentence), word form, POS tag,head (parent word ID), and deprel (the dependencyrelation between the current word and its parent).There are default MaltParser features (in the ma-chine learning sense),7 which are the values of func-tions over these attributes, serving as input to theMaltParser internal classifiers.
The most commonlyused feature functions are the top of the input buffer(next word to process, denoted buf[0]), or top of thestack (denoted stk[0]); following items on buffer orstack are also accessible (buf[1], buf[2], stk[1], etc.
).Hence MaltParser features are defined as POS tagat stk[0], word form at buf[0], etc.
K?bler et al(2009) describe a ?typical?
MaltParser model con-figuration of attributes and features.8 Starting withit, in a series of initial controlled experiments, wesettled on using buf[0-1] + stk[0-1] for wordforms,and buf[0-3] + stk[0-2] for POS tags.
For features ofnew MaltParser-attributes (discussed later), we usedbuf[0] + stk[0].
We did not change the features fordeprel.
This new MaltParser configuration resultedin gains of 0.3-1.1% in labeled attachment accuracy(depending on the POS tagset) over the default Malt-Parser configuration.9 All experiments reported be-low were conducted using this new configuration.5.2 Inflectional featuresIn order to explore the contribution of inflectionaland lexical information in a controlled manner, wefocused on the best performing core (?morphology-free?)
POS tagset, CORE12, as baseline; using threeis also reported to do better on Arabic, but in a preliminary ex-perimentation, it did similarly or slightly worse than the "eager?one, perhaps due to high percentage of right branching (leftheaded structures) in our Arabic training set ?
an observationalready noted in Nivre (2008).7The terms ?feature?
and ?attribute?
are overloaded in theliterature.
We use them in the linguistic sense, unless specifi-cally noted otherwise, e.g., ?MaltParser feature(s)?.8It is slightly different from the default configuration.9We also experimented with normalizing word forms (AlifMaqsura conversion to Ya, and hamza removal from Alif forms)as is common in parsing and statistical machine translation lit-erature ?
but it resulted in a similar or slightly decreased perfor-mance, so we settled on using non-normalized word forms.1590setup LAS LASdiff UASAll CORE12 78.68 ?
82.48+ all inflectional features 77.91 -0.77 82.14Sep+DET 79.82++ 1.14 83.18+STATE 79.34++ 0.66 82.85+GENDER 78.75 0.07 82.35+PERSON 78.74 0.06 82.45+NUMBER 78.66 -0.02 82.39+VOICE 78.64 -0.04 82.41+ASPECT 78.60 -0.08 82.39+MOOD 78.54 -0.14 82.35+CASE 75.81 -2.87 80.24Greedy+DET+STATE 79.42++ 0.74 82.84+DET+GENDER 79.90++ 1.22 83.20+DET+GENDER+PERSON 79.94++ 1.26 83.21+DET+PNG 80.11++ 1.43 83.29+DET+PNG+VOICE 79.96++ 1.28 83.18+DET+PNG+ASPECT 80.01++ 1.33 83.20+DET+PNG+MOOD 80.03++ 1.35 83.21Table 1: CORE12 with inflectional features, predicted input.Top: Adding all nine features to CORE12.
Second part: Addingeach feature separately, comparing difference from CORE12.Third part: Greedily adding best features from second part.different setups, we added nine morphological fea-tures with values predicted by MADA: DET (pres-ence of the definite determiner), PERSON, ASPECT,VOICE, MOOD, GENDER, NUMBER, STATE (mor-phological marking as head of an idafa construc-tion), and CASE.
In setup All , we augmented thebaseline model with all nine MADA features (asnine additional MaltParser attributes); in setup Sep ,we augmented the baseline model with the MADAfeatures, one at a time; and in setup Greedy , wecombined them in a greedy heuristic (since the entirefeature space is too vast to exhaust): starting with themost gainful feature from Sep, adding the next mostgainful feature, keeping it if it helped, or discardingit otherwise, and continuing through the least gainfulfeature.
See Table 1.Somewhat surprisingly, setup All hurts perfor-mance.
This can be explained if one examines theprediction accuracy of each feature (top of Table 2).Features which are not predicted with very high ac-curacy, such as CASE (86.3%), can dominate thenegative contribution, even though they are top con-tributors when provided as gold input (Marton et al,2010); when all features are provided as gold in-put, All actually does better than individual features,which puts to rest a concern that its decrease herefeature acc set sizeDET 99.6 3*PERSON 99.1 4*ASPECT 99.1 5*VOICE 98.9 4*MOOD 98.6 5*GENDER 99.3 3*NUMBER 99.5 4*STATE 95.6 4*CASE 86.3 5*ROOT 98.4 9646PATTERN 97.0 338LEMMA (diacritized) 96.7 16837LMM (undiacritized lemma) 98.3 15305normalized word form (A,Y) 99.3 29737non-normalized word form 98.9 29980Table 2: Feature prediction accuracy and set sizes.
* = The setincludes a "N/A" value.setup LAS LASdiff UASAll CORE12 (repeated) 78.68 ?
82.48+ all lexical features 78.85 0.17 82.46Sep+LMM 78.96+ 0.28 82.54+ROOT 78.94+ 0.26 82.64+LEMMA 78.80 0.12 82.42+PATTERN 78.59 -0.09 82.39Greedy +LMM+ROOT 79.04++ 0.36 82.63+LMM+ROOT+LEMMA 79.05++ 0.37 82.63+LMM+ROOT+PATTERN 78.93 0.25 82.58Table 3: Lexical features.
Top part: Adding each featureseparately; difference from CORE12 (predicted).
Bottom part:Greedily adding best features from previous part.is due to data sparseness.
Here, when features arepredicted, the DET feature (determiner), followed bythe STATE (construct state, idafa) feature, are top in-dividual contributors in setup Sep.
Adding DET andthe so-called ?-features (PERSON, NUMBER, GEN-DER, also shorthanded PNG) in the Greedy setup,yields 1.43% gain over the CORE12 baseline.5.3 Lexical featuresNext, we experimented with adding the lexical fea-tures, which involve semantic abstraction to somedegree: LEMMA, LMM (the undiacritized lemma),and ROOT.
We experimented with the same setupsas above: All, Sep, and Greedy.
Adding all fourfeatures yielded a minor gain in setup All.
LMMwas the best single contributor, closely followed byROOT in Sep. CORE12+LMM+ROOT (with or with-1591CORE12 + .
.
.
LAS LASdiff UAS+DET+PNG (repeated) 80.11++ 1.43 83.29+DET+PNG+LMM 80.23++ 1.55 83.34+DET+PNG+LMM +ROOT 80.10++ 1.42 83.25+DET+PNG+LMM +PATTERN 80.03++ 1.35 83.15Table 4: Inflectional+lexical features together.CORE12 + .
.
.
LAS LASdiff UAS+DET (repeated) 79.82++ ?
83.18+DET2 80.13++ 0.31 83.49+DET+PNG+LMM (repeated) 80.23++ ?
83.34+DET2+PNG+LMM 80.21++ -0.02 83.39Table 5: Extended inflectional features.out LEMMA) was the best greedy combination insetup Greedy.
See Table 3.
All lexical features arepredicted with high accuracy (bottom of Table 2).Following the same greedy heuristic, weaugmented the best inflection-based modelCORE12+DET+PNG with lexical features, andfound that only the undiacritized lemma (LMM)alone improved performance (80.23%).
See Table 4.5.4 Inflectional feature engineeringSo far we experimented with morphological fea-ture values as predicted by MADA.
However, it islikely that from a machine-learning perspective, rep-resenting similar categories with the same tag maybe useful for learning.
Therefore, we next exper-imented with modifying inflectional features thatproved most useful.As DET may help distinguish the N-N idafa con-struction from the N-A modifier construction, weattempted modeling also the DET values of pre-vious and next elements (as MaltParser?s stk[1] +buf[1], in addition to stk[0] + buf[0]).
This vari-ant, denoted DET2, indeed helps: when added tothe CORE12, DET2 improves non-gold parsing qual-ity by more than 0.3%, compared to DET (Ta-ble 5).
This improvement unfortunately does notcarry over to our best feature combination to date,CORE12+DET+PNG+LMM.
However, in subsequentfeature combinations, we see that DET2 helps again,or at least, doesn?t hurt: LAS goes up by 0.06% inconjunction with features LMM+PERSON +FN*NGRin Table 6.CORE12 + .
.
.
LAS LASdiff UASCORE12 (repeated) 78.68 ?
82.48+PERSON (repeated) 78.74 0.06 82.45+GENDER (repeated) 78.75 0.07 82.35+NUMBER (repeated) 78.66 -0.02 82.39+FN*GENDER 78.96++ 0.28 82.53+FN*NUMBER 78.88+ 0.20 82.53+FN*NUMDGTBIN 78.87 0.19 82.53+FN*RATIONALITY 78.91+ 0.23 82.60+FN*GNR 79.32++ 0.64 82.78+PERSON+FN*GNR 79.34++ 0.66 82.82+DET+LMM+PERSON+FN*NGR 80.47++ 1.79 83.57+DET2+LMM+PERSON+FN*NGR 80.53++ 1.85 83.66+DET2+LMM+PERSON+FN*NG 80.43++ 1.75 83.56+DET2+LMM+PNG+FN*NGR 80.51++ 1.83 83.66CATIBEX 79.74 ?
83.30+DET2+LMM +PERSON+FN*NGR 80.83++ 1.09 84.02BW 72.64 ?
77.91+DET2+LMM +PERSON+FN*NGR 74.40++ 1.76 79.40Table 6: Functional features: gender, number, rationality.We also experimented with PERSON.
We changedthe values of proper names from ?N/A?
to ?3?
(thirdperson), but it resulted in a similar or slightly de-creased performance, so it was abandoned.5.5 Functional feature valuesThe NUMBER and GENDER features we have usedso far only reflect surface (as opposed to functional)values, e.g., broken plurals are marked as singular.This might have a negative effect on learning gen-eralizations over the complex agreement patterns inMSA (see Section 3), beyond memorization of wordpairs seen together in training.Predicting functional features To predict func-tional GENDER, functional NUMBER and RATIO-NALITY, we build a simple maximum likelihood es-timate (MLE) model using these annotations in thecorpus created by Alkuhlani and Habash (2011).
Wetrain using the same training data we use through-out this paper.
For all three features, we select themost seen value in training associated with the tripleword-CATIBEX-lemma; we back off to CATIBEX-lemma and then to lemma.
For gender and num-ber, we further back off to the surface values; forrationality, we back off to the most common value(irrational).
On our predicted dev set, the over-all accuracy baseline of predicting correct functionalgender-number-rationality using surface features is159285.1% (for all POS tags).
Our MLE model reducesthe error by two thirds reaching an overall accuracyof 95.5%.
The high accuracy may be a result of thelow percentage of words in the dev set that do notappear in training (around 4.6%).Digit tokens (e.g., ?4?)
are also marked singu-lar by default.
They don?t show surface agreement,even though the corresponding number-word token(??K.
P@ Arb?~ ?four.fem.sing?)
would.
We further ob-serve that MSA displays complex agreement pat-terns with numbers (Dada, 2007).
Therefore, wealternatively experimented with binning the digit to-kens?
NUMBER value accordingly:?
the number 0 and numbers ending with 00?
the number 1 and numbers ending with 01?
the number 2 and numbers ending with 02?
the numbers 3-10 and those ending with 03-10?
the numbers, and numbers ending with, 11-99?
all other number tokens (e.g., 0.35 or 7/16)and denoted these experiments with NUMDGTBIN.Almost 1.5% of the tokens are digit tokens in thetraining set, and 1.2% in the dev set.10Results using these new features are shown in Ta-ble 6.
The first part repeats the CORE12 baseline.The second part repeats previous experiments withsurface morphological features.
The third part usesthe new functional morphological features instead.The performance using NUMBER and GENDER in-creases by 0.21% and 0.22%, respectively, as we re-place surface features with functional features.
(Re-call that there is no functional PERSON.)
We then seethat the change in the representation of digits doesnot help; in the large space of experiments we haveperformed, we saw some improvement through theuse of this alternative representation, but not in anyof the feature combinations that performed best andthat we report on in this paper.
We then use just theRATIONALITY feature, which results in an increaseover the baseline.
The combination of all three func-tional features (NUMBER, GENDER, RATIONALITY)provides for a nice cumulative effect.
Adding PER-SON hardly improves further.In the fourth part of the table, we include the otherfeatures which we found previously to be helpful,10We didn?t mark the number-words since in our training datathere were less than 30 lemmas of less than 2000 such tokens, sopresumably their agreement patterns can be more easily learned.namely DET and LMM.
Here, using DET2 instead ofDET (see Section 5.4) gives us a slight improvement,providing our best result using the CORE12 POStagset: 80.53%.
This is a 1.85% improvement overusing only the CORE12 POS tags (an 8.7% error re-duction); of this improvement, 0.3% absolute (35%relative) is due to the use of functional features.
Wethen use the best configuration, but without the RA-TIONALITY feature; we see that this feature on itsown contributes 0.1% absolute, confirming its placein Arabic syntax.
In gold experiments which wedo not report here, the contribution was even higher(0.6-0.7%).
The last row in the fourth part of Table 6shows that using both surface and functional variantsof NUMBER and GENDER does not help (hurts, infact); the functional morphology features carry suf-ficient information for syntactic disambiguation.The last part of the table revalidates the gainsachieved of the best feature combination using thetwo other POS tagsets mentioned in Section 3: CAT-IBEX (the best performing tagset with predicted val-ues), and BW (the best POS tagset with gold val-ues in Marton et al (2010), but results shown hereare with predicted values).
The CATIBEX result of80.83% is our overall best result.
The result usingBW reconfirms that BW is not the best tagset to usefor parsing Arabic with current prediction ability.5.6 Validating results on unseen test setOnce experiments on the development set weredone, we ran the best performing models on the pre-viously unseen test set (Section 5.1).
Table 7 showsthat the same trends hold on this set as well.Model LAS LASdiff UASCATIBEX 78.46 ?
81.81+DET2+LMM+PER+FN*NGR 79.45++ 0.99 82.56Table 7: Results on unseen test set for models which performedbest on dev set ?
predicted input.6 Error AnalysisWe analyze the attachment accuracy by attachmenttype.
We show the accuracy for selected attach-ment types in Table 8.
Using just CORE12, we seethat some attachments (subject, modifications) areharder than others (objects, idafa).
We see that by1593Features SBJ OBJ MN MP IDF Tot.CORE12 67.9 90.4 72.0 70.3 94.5 78.7CORE12 + LMM 68.8 90.4 72.6 70.9 94.6 79.0CORE12 + DET2+LMM+PNG 71.7 91.0 74.9 72.4 95.5 80.2CORE12 + DET2+LMM+PERS+FN*NGR 72.3 91.0 76.0 73.3 95.4 80.5Table 8: Error analysis: Accuracy by attachment type (se-lected): subject, object, modification by a noun, modification(of a verb or a noun) by a preposition, idafa, and overall results(which match previously shown results)adding LMM, all attachment types improve a littlebit; this is as expected, since this feature providesa slight lexical abstraction.
We then add featuresdesigned to improve idafa and those relations sub-ject to agreement, subject and nominal modification(DET2, PERSON, NUMBER, GENDER).
We see thatas expected, subject, nominal modification (MN),and idafa reduce error by substantial margins (errorreduction over CORE12+LMM greater than 8%, inthe case of idafa the error reduction is 16.7%), whileobject and prepositional attachment (MP) improveto a lesser degree (error reduction of 6.2% or less).We assume that the relations not subject to agree-ment (object and prepositional attachment) improvebecause of the overall improvement in the parse dueto the improvements in the other relations.When we move to the functional features, weagain see a reduction in the attachments which aresubject to agreement, namely subject and nomi-nal modification (error reductions over surface fea-tures of 2.1% and 4.4%, respectively).
Idafa de-creases slightly (since this relation is not affectedby the functional features), while object stays thesame.
Surprisingly, prepositional attachment alsoimproves, with an error reduction of 3.3%.
Again,we can only explain this by proposing that the im-provement in nominal modification attachment hasthe indirect effect of ruling out some bad preposi-tional attachments as well.In summary, we see that not only do morphologi-cal features ?
and functional morphology features inparticular ?
improve parsing, but they improve pars-ing in the way that we expect: those relations subjectto agreement improve more than those that are not.Last, we point out that MaltParser does not modelgeneralized feature checking or matching directly,i.e., it has not learned that certain syntactic relationsrequire identical (functional) morphological featurevalues.
The gains in parsing quality reflect that theMaltParser SVM classifier has learned that the pair-ing of specific morphological feature values ?
e.g.,fem.sing.
for both the verb and its subject ?
is use-ful, with no generalization from each specific valueto other values, or to general pair-wise value match-ing.7 Conclusions and Future WorkWe explored the contribution of different morpho-logical (inflectional and lexical) features to depen-dency parsing of Arabic.
We find that definiteness(DET), ?-features (PERSON, NUMBER, GENDER),and undiacritized lemma (LMM) are most helpful forArabic dependency parsing on predicted input.
Wefurther find that functional morphology features andrationality improve over surface morphological fea-tures, as predicted by the complex agreement rulesof Arabic.
To our knowledge, this is the first resultin Arabic NLP that uses functional morphology fea-tures, and that shows an improvement over surfacefeatures.In future work, we intend to improve the predic-tion of functional morphological features in order toimprove parsing accuracy.
We also intend to investi-gate how these features can be integrated into otherparsing frameworks; we expect them to help inde-pendently of the framework.
We plan to make ourparser available to other researchers.
Please contactthe authors if interested.AcknowledgmentsThis work was supported by the DARPA GALEprogram, contract HR0011-08-C-0110.
We thankJoakim Nivre for his useful remarks, Otakar Smr?for his help with Elixir-FM, Ryan Roth and SarahAlkuhlani for their help with data, and three anony-mous reviewers for useful comments.
Part ofthe work was done while the first author was atColumbia University.1594ReferencesSarah Alkuhlani and Nizar Habash.
2011.
A corpus formodeling morpho-syntactic agreement in Arabic: gen-der, number and rationality.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics (ACL), Portland, Oregon, USA.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of Computational Natural LanguageLearning (CoNLL), pages 149?164.Timothy A. Buckwalter.
2004.
Buckwalter Arabic Mor-phological Analyzer Version 2.0.
Linguistic DataConsortium, University of Pennsylvania, 2002.
LDCCat alog No.
: LDC2004L02, ISBN 1-58563-324-0.Michael Collins, Jan Hajic, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics (ACL),College Park, Maryland, USA, June.Brooke Cowan and Michael Collins.
2005.
Morphologyand reranking for the statistical parsing of spanish.
InProceedings of Human Language Technology (HLT)and the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 795?802.Ali Dada.
2007.
Implementation of Arabic numerals andtheir syntax in GF.
In Proceedings of the Workshopon Computational Approaches to Semitic Languages,pages 9?16, Prague, Czech Republic.Mona Diab.
2007.
Towards an optimal pos tag set formodern standard arabic processing.
In Proceedingsof Recent Advances in Natural Language Processing(RANLP), Borovets, Bulgaria.G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer.
2008.Dependency parsing of turkish.
Computational Lin-guistics, 34(3):357?389.Spence Green and Christopher D. Manning.
2010.
BetterArabic parsing: Baselines, evaluations, and analysis.In Proceedings of the 23rd International Conferenceon Computational Linguistics (COLING), pages 394?402, Beijing, China.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 573?580, Ann Ar-bor, Michigan.Nizar Habash and Ryan Roth.
2009.
Catib: Thecolumbia arabic treebank.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August.Nizar Habash, Reem Faraj, and Ryan Roth.
2009.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Jan Hajic?
and Barbora Vidov?-Hladk?.
1998.
Tag-ging Inflective Languages: Prediction of Morpholog-ical Categories for a Rich, Structured Tagset.
In Pro-ceedings of the International Conference on Com-putational Linguistics (COLING)- the Association forComputational Linguistics (ACL), pages 483?490.Sandra K?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan and ClaypoolPublishers.Seth Kulick, Ryan Gabbard, and Mitch Marcus.
2006.Parsing the Arabic Treebank: Analysis and improve-ments.
In Proceedings of the Treebanks and Linguis-tic Theories Conference, pages 31?42, Prague, CzechRepublic.Mohamed Maamouri, Ann Bies, Timothy A. Buckwalter,and Wigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InProceedings of the NEMLAR Conference on ArabicLanguage Resources and Tools, pages 102?109, Cairo,Egypt.Yuval Marton, Nizar Habash, and Owen Rambow.
2010.Improving Arabic dependency parsing with inflec-tional and lexical morphological features.
In Proceed-ings of Workshop on Statistical Parsing of Morpho-logically Rich Languages (SPMRL) at the 11th Meet-ing of the North American Chapter of the Associationfor Computational Linguistics (NAACL) - Human Lan-guage Technology (HLT), Los Angeles, USA.Jens Nilsson and Joakim Nivre.
2008.
MaltEval: Anevaluation and visualization tool for dependency pars-ing.
In Proceedings of the sixth International Confer-ence on Language Resources and Evaluation (LREC),Marrakech, Morocco.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Joakim Nivre, Igor M. Boguslavsky, and Leonid K.Iomdin.
2008.
Parsing the SynTagRus Treebank ofRussian.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (COLING),pages 641?648.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Conference on Parsing Technologies(IWPT), pages 149?160, Nancy, France.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4).1595Joakim Nivre.
2009.
Parsing Indian languages withMaltParser.
In Proceedings of the ICON09 NLP ToolsContest: Indian Language Dependency Parsing, pages12?18.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic morphological tag-ging, diacritization, and lemmatization using lexememodels and feature ranking.
In Proceedings of ACL-08: HLT, Short Papers, pages 117?120, Columbus,Ohio, June.
Association for Computational Linguis-tics.Otakar Smr?.
2007.
Functional Arabic Morphology.
For-mal System and Implementation.
Ph.D. thesis, CharlesUniversity, Prague.Reut Tsarfaty and Khalil Sima?an.
2007.
Three-dimensional parametrization for parsing morphologi-cally rich languages.
In Proceedings of the 10th Inter-national Conference on Parsing Technologies (IWPT),pages 156?167, Morristown, NJ, USA.1596
