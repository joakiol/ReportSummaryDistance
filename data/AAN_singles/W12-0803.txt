Proc.
EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 21?30,Avignon, France, April 24 2012. c?2012 Association for Computational LinguisticsTTT: A tree transduction language for syntactic and semantic processingAdam PurteeUniversity of RochesterDepartment of Computer Scienceapurtee@cs.rochester.eduLenhart SchubertUniversity of RochesterDepartment of Computer Scienceschubert@cs.rochester.eduAbstractIn this paper we present the tree to treetransduction language, TTT.
We moti-vate the overall ?template-to-template?
ap-proach to the design of the language, andoutline its constructs, also providing someexamples.
We then show that TTT al-lows transparent formalization of rules forparse tree refinement and correction, log-ical form refinement and predicate disam-biguation, inference, and verbalization oflogical forms.1 IntroductionPattern matching and pattern-driven transforma-tions of list-structured symbolic expressions ortrees are fundamental tools in AI.
They facilitatemany symbol manipulation tasks, including oper-ations on parse trees and logical forms, and eveninference and aspects of dialogue and translation.The TTT system allows concise and transpar-ent specification of rules for such tasks, in par-ticular (as we will show), parse tree refinementand correction, predicate disambiguation, logicalform refinement, inference, and verbalization intoEnglish.In parse tree refinement, our particular focushas been on repair of malformed parses of imagecaptions, as obtained by the Charniak-Johnsonparser (Charniak and Johnson, 2005).
This hasencompassed such tasks as distinguishing pas-sive participles from past participles and temporalnominals from non-temporal ones, among othertasks which will be discussed later.
For exam-ple, standard treebank parses tag both past par-ticiples (as in ?has written?)
and passive partici-ples (as in ?was written?)
as VBN.
This is undesir-able for subsequent compositional interpretation,as the meanings of past and passive participles aredistinct.
We can easily relabel the past partici-ples as VBEN by looking for parse tree subex-pressions where a VBN is preceded by a form of?have?, either immediately or with an interven-ing adverb or adverbial, and replacing VBN byVBEN in such subexpressions.
Of course this canbe accomplished in a standard symbol manipula-tion language like Lisp, but the requisite multi-ple lines of code obscure the simple nature of thetask.
We have also been able to repair system-atic PP (prepositional phrase) misattachments, atleast in the limited domain of image captions.
Forexample, a common error is attachment of a PPto the last conjunct of a conjunction, where in-stead the entire conjunction should be modified bythe PP.
Thus when a statistically obtained parse ofthe sentence ?
Tanya and Grandma Lillian at herhighschool graduation party?
brackets as ?Tanyaand (Grandma Lillian (at her highschool gradu-ation party.
))?, we want to lift the PP so that ?ather highschool graduation party?
modifies ?Tanyaand Grandma Lillian?.Another systematic error is faulty classificationof relative pronouns/determiners as wh-questionpronouns/determiners, e.g., ?the student whosemother contacted you?
vs. ?I know whose mothercontacted you?
?
an important distinction in com-positional semantics.
(Note that only the first oc-currence, i.e., the relative determiner, can be para-phrased as with the property that his, and only thesecond occurrence, in which whose forms a wh-nominal, can be paraphrased as the person withthe property that his.)
An important point here isthat detecting the relative-determiner status of awh-word like whose may require taking account21of an arbitrarily deep context.
For example, inthe phrase ?the student in front of whose par-ents you are standing?, whose lies two levels ofphrasal structure below the nominal it is seman-tically bound to.
Such phenomena motivate thedevices in TTT for detecting ?vertical patterns?of arbitrary depth.
Furthermore, we need to beable to make local changes ?on the fly?
in match-ing vertical patterns, because the full set of treefragments flanking a vertical match cannot in gen-eral be saved using match variables.
In the caseof a wh-word that is to be re-tagged as a relativeword, we need to rewrite it at the point wherethe vertical pattern matches it, rather than in aseparate tree-(re)construction phase following thetree-matching phase.An example of a discourse phenomenon thatrequires vertical matching is anaphoric referentdetermination.
In particular, consider the well-known rule that a viable referent for an anaphoricpronoun is an NP that C-commands it, i.e., that isa (usually left) sibling of an ancestor of the pro-noun.
For example, in the sentence ?John showsLillian the snowman that he built?, the NP forJohn C-commands the pronominal NP for he, andthus is a viable referent for it (modulo gender andnumber agreement).
We will later show a sim-ple TTT rule that tags such an anaphoric pronounwith the indices of its C-commanding NP nodes,thus setting the stage for semantic interpretation.We have also been able to perform Skolemiza-tion, conjunct separation, simple inference, andlogical form verbalization with TTT and suspectits utility to logic tasks will increase as develop-ment continues.The rest of the paper is organized as follows:we discuss related work in section 2, discuss theTTT language (including pattern matching andtransduction syntax, and some theoretical proper-ties) in section 3, and go though several detailedexample applications in section 4.A beta version of the system can be found athttp://www.cs.rochester.edu/research/ttt/.2 Related WorkThere are several pattern matching and transduc-tion facilities available; however, none provedsufficiently general and perspicuous to serve ourvarious purposes.
The Tiburon tool is a com-prehensive system for manipulating regular treegrammars, tree automata, and tree transducers,including weighted variants (May and Knight,2008).
It supports many useful algorithms, suchas intersection, determinization, recognition, top-k generation, and maximum likelihood training.However, variables that appear in both a rule?s lhsand rhs must occur at a depth less than two on theleft, and Tiburon cannot easily simulate our verti-cal path or sequence operators.Timbuk is a system for deciding reachabilitywith term rewriting systems and tree automata(Genet, 2003), and it also performs intersec-tion, union, and determinization of tree automata.Though variables can appear at arbitrary locationsin terms, they always match exactly one term froma fixed set, and therefore do not match sequencesor vertical paths.The three related tools Tgrep, Tregex, andTsurgeon provide powerful tree matching and re-structuring capabilities (Levy and Andrew, 2006).However, Tgrep and Tregex provide no transduc-tion mechanism, and Tsurgeon?s modificationsare limited to local transformations on trees.
Also,it presupposes list structures that begin with anatom (as in Treebank trees, but not in parse treeswith explicit phrasal features), and its patterns arefundamentally tree traversal patterns rather thantree templates, and can be quite hard to read.Xpath and XSLT are languages for manipula-tion of XML trees (World Wide Web Consortium,1999; World Wide Web Consortium, 1999).
As itsname indicates, Xpath expressions describe pathsin trees to the relevant nodes, rather than patternsrepresenting the trees to be matched, as in theTTT approach.
It is useful for extracting struc-tured but unordered information from trees, andsupports numerous functions and predicates overmatched nodes, but does not match ordered se-quences.
XSLT is also more procedurally orientedthan TTT, and is useful for constructing XML rep-resentations of transformations of data extractedby Xpath.
The primary advantages of TTT overXpath and XSLT are a more concise syntax, or-dered sequence matching, compositional patternsand templates, and in-place modification of trees.Peter Norvig?s pattern matching language,?pat-match?, from (Norvig, 1991) provides a nicepattern matching facility within the Lisp environ-ment, allowing for explicit templates with vari-ables (that can bind subexpressions or sequencesof them), and including ways to apply arbitrarytests to expressions and to match boolean combi-22nations of patterns.
However, there is no provi-sion for ?vertical?
pattern matching or subexpres-sion replacement ?on the fly?, which are featuresof TTT we have found useful.
Also the notationfor alternatives, along with exclusions, is moreconcise than in Norvig?s matcher, for instance notrequiring explicit ORs.
Like TTT, pat-match sup-ports matching multi-level structures, but unlikeTTT, the pattern operators are not composable.Mathematica also allows for sophisticated pat-tern matching, including matching of sequencesand trees.
It also includes a term rewriting sys-tem that is also capable of rewriting ordered se-quences.
It provides functions to apply patterns toarbitrary subtrees of a tree until all matches havebeen found or some threshold count is reached,and it can return all possible ways of applying aset of rules to an expression.
However, as in thecase of Norvig?s matcher there is no provision forvertical patterns or on-the-fly transduction (Wol-fram Research, Inc, 2010).3 TTTPattern MatchingPatterns in TTT are hierarchically composed ofsub-patterns.
The simplest kind of pattern is anarbitrary, explicit list structure (tree) containingno match operators, and this will match only anidentical list structure.
Slightly more flexible pat-terns are enabled by the ?underscore operators?
!, +, ?, *.
These match any single tree, anynon-empty sequence of trees, the empty sequenceor a sequence of one tree, and any (empty or non-empty) sequence of trees respectively.
These op-erators (as well as all others) can also be thoughtof as match variables, as they pick up the tree orsequence of trees they match as their binding.The bindings are ?non-sticky?, i.e., an operatorsuch as !
will match any tree, causing replace-ment of any prior binding (within the same pat-tern) by that tree.
However, bindings can be pre-served in two ways: by use of new variable names,or by use of sticky variables.
New variable namesare obtained by appending additional characters?
conventionally, digits ?
to the basic ones, e.g.,!1, !2, etc.
Sticky variables are written with adot, i.e., !., +., ?., *., where again thesesymbols may be followed by additional digits orother characters.
The important point concern-ing sticky variables is that multiple occurrences ofsuch a variable in a pattern can only be bound bythe same unique value.
Transductions are spec-ified by a special pattern operator / and will bedescribed in the next section.More flexible operators, allowing for alter-natives, negation, and vertical patterns amongother constructs, are written as a list headed byan operator without an underscore, followed byone or more arguments.
For example, (!
A(B C)) will match either the symbol A or thelist (B C), i.e., the two arguments provide al-ternatives.
As an example involving negation,(+ A (B !)
?
(B B)) will match anynonempty sequence whose elements are As ortwo-element lists headed by B, but disallowing el-ements of type (B B).
Successful matches causethe matched expression or sequence of expres-sions to become the value of the operator.
Again,sticky versions of match operators use a dot, andthe operators may be extended by appending dig-its or other characters.The ten basic argument-taking pattern opera-tors are:!
Match exactly one sub-pattern argument.+ Match a sequence of one or more arguments.?
Match the empty sequence or one argument.
* Match the empty sequence or one or morearguments.
{} Match any permutation of the arguments.<> Match the sequence of arguments directly(without the parenthesis enclosing the <>operator)?
Match a tree that has a child matching one ofthe arguments.?
* Match a tree that has a descendant matchingone of the arguments.?
@ Match a vertical path./ Attempt a transduction.
(Explained later.
)Various examples will be provided below.
Any ofthe arguments to a pattern operator may be com-posed of arbitrary patterns.Negation: The operators !, +, ?, *, and ?
sup-port negation (pattern exclusion); i.e., the argu-ments of these operators may include not only al-ternatives, but also a negation sign ?
(after the23alternatives) that is immediately followed by oneor more precluded patterns.
If no alternativesare provided, only precluded patterns, this is in-terpreted as ?anything goes?, except for the pre-cluded patterns.
For example, (+ ?
(A A) B)will match any nonempty sequence of expressionsthat contains no elements of type (A A) or B.Note that the negation operator does not appearby itself; one must instead specify it in conjunc-tion with one of the other operators.
The pattern(!
?
P) matches any single tree which does notmatch pattern P.Conjunction: We have so far found no com-pelling need for an explicit conjunction operator.If necessary, a way to say that a tree must matcheach of two or more patterns is to use doublenegation.
For example, suppose we want to saythat an expression must begin with an A or B butmust contain an A (at the top level); this could beexpressed as(!
?
(!
?
((!
A B) *) ( * A *))).However, this would be more perspicuously ex-pressed in terms of alternatives, i.e.,(!
(A *) (B * A *)).We also note that the allowance for computablepredicates (discussed below) enables introductionof a simple construct like(!
(and?
P1 P2)) , where P1 and P2 are ar-bitrary TTT patterns, and and?
is an executablepredicate that applies the TTT matcher to its argu-ments and returns a non-nil value if both succeedand nil otherwise.
In the former case, the bindingof the outer !
will become the matched tree.Bounded Iteration: The operators !, +, ?,*, and ?
also support bounded iteration, usingsquare brackets.
This enables one to write pat-terns that match exactly n, at least n, at most n,or from n to m times, where n and m are integers.Eg.
(!
[3] A) would match the sequence A AA.
The vertical operator ?
[n] matches trees witha depth-n descendant that matches one of the op-erator?s arguments.Vertical Paths: The operators ?
* and ?
@ en-able matching of vertical paths of arbitrary depth.The first, as indicated, requires the existence ofa descendant of the specified type, while the sec-ond, with arguments such as (?
@ P1 P2 ...Pn) matches a tree whose root matches P1, andhas a child matching P2, which in turn has a childmatching P3, and so on.
Note that this basic formis indifferent to the point of attachment of eachsuccessive offspring to its parent; but we can alsospecify a point of attachment in any of the P1, P2,etc., by writing @ for one of its children.
Becausethis operator (@) does not appear outside the ver-tical path context, it was not listed with the otheroperators above.
Note as well that the argumentsequence P1 P2 ... can itself be specified as apattern (e.g., via (+ ...)), and in this case thereis no advance commitment to the depth of the treebeing matched.Computable Predicates: Arbitrary predicatescan be used during the pattern matching pro-cess (and consequently the transduction process).Symbols with names ending in a question mark,and with associated function definitions, are in-terpreted as predicates.
When a predicate is en-countered during pattern matching, it is calledwith the current subtree as input.
The result isnil or non-nil, and when nil is returned the currentmatch fails, otherwise it succeeds (but the non-nil value is not used further).
Additionally, sup-porting user-defined predicates enables the use ofnamed patterns.Some Example Patterns: Here are examplesof particular patterns, with verbal explanations.Also see Table 1, at the top of the next page, foradditional patterns with example bindings.?
(!
(+ A) (+ B))Matches a non-empty sequence of A?s or anon-empty sequence of B?s, but not a se-quence containing both.?
(* (<> A A))Matches an even number of A?s.?
(B (* (<> B B)))Matches an odd number of B?s.?
(({} A B C))Matches (A B C), (A C B), (B A C),(B C A), (C A B) and (C B A) andnothing else.?
((<> A B C))Matches (A B C) and nothing else.?
(?
* X)Matches any tree that has descendant X.?
(?
@ (+ (@ *)) X)Matches any tree with leftmost leaf X.24Pattern Tree Bindings!
(A B C) ( !
(A B C)( * F) (A B (C D E) F) ( * A B (C D E))(A B ?
F) (A B (C D E) F) ( ?
(C D E))(A B ?
(C D E) F) (A B (C D E) F) ( ?)(?
@ !
(C *) E) (A B (C D E) F) (?
@ (A B (C D E) F)) ( * D E)(A B (<> (C D E)) F) (A B (C D E) F) (<> (C D E))(A B (<> C D E) F) (A B (C D E) F) nilTable 1: Binding ExamplesTransductionsTransductions are specified with the transductionoperator, /, which takes two arguments.
The leftargument may be any tree pattern and the rightargument may be constructed of literals, variablesfrom the lhs pattern, and function calls.Transductions may be applied to the roots oftrees or arbitrary subtrees, and they may be re-stricted to apply at most once, or until conver-gence.
When applying transductions to arbitrarysubtrees, trees are searched top-down, left to right.When a match to the transduction lhs pattern oc-curs, the resulting bindings and transduction rhsare used to create a new tree, which then replacesthe tree (or subtree) that matched the lhs.Here are a few examples of simple template totemplate transductions:?
(/ X Y)Replaces the symbol X with the symbol Y.?
(/ (!
X Y Z) (A))Replaces any X, Y, or Z with A.?
(/ (!
X) (!
!
))Duplicates an X.?
(/ (X * Y) (X Y))Remove all subtrees between X and Y.?
(/ ( !
* !1) ( !1 * !
))Swaps the subtrees on the boundaries.A transduction operator may appear nested withina composite pattern.
The enclosing pattern ef-fectively restricts the context in which the trans-duction will be applied, because only a match tothe entire pattern will trigger a transduction.
Inthis case, the transduction is applied at the lo-cation in the tree where it matches.
The rhs ofsuch a transduction is allowed to reference thebindings of variables that appear in the enclos-ing pattern.
We call these local transductions, asdistinct from replacement of entire trees.
Localtransductions are especially advantageous whenperforming vertical path operations, allowing forvery concise specifications of local changes.
Forexample, the transduction(?
@ (* ((!
S SBAR) +))(/ (WH !
)(REL-WH (WH !
))))wraps (REL-WH ...) around a (WH ...)constituent occurring as a descendant of a ver-tical succession of clausal (S or SBAR) con-stituents.
Applied to the tree (S (SBAR (WHX) B) A), this yields the new tree (S (SBAR(REL-WH (WH X)) B) A).
Additional ex-amples appear later (especially in the parse treerefinement section).TTT also supports constructive functions, withbound variables as arguments, in the rhs tem-plates, such as join-with-dash!, which con-catenates all the bound symbols with interven-ing dashes, and subst-new!, which will bediscussed later.
One can imagine additionalfunctions, such as reverse!, l-shift!,r-shift!, or any other function of a list ofterms that may be useful to the application athand.
Symbols with names ending in the excla-mation mark are assumed to be associated withfunction definitions, and when appearing as thefirst element of a list are executed during out-put template construction.
To avoid writing manynear-redundant functions, we use the simple func-tion apply!
to apply arbitrary Lisp functionsduring template construction.Theoretical PropertiesA thorough treatment of the formal propertiesof tree transducers is (Comon, 2007).
A good25overview of the dimensions of variability amongformal tree transducers is given in (Knight, 2007).The main properties are restrictions on the heightof the tree fragments allowed in rules, linearity,and whether the rules can delete arbitrary sub-trees.
Among the more popular and recent ones,synchronous tree substitution grammars (STSG),synchronous tree sequence substitution grammars(STSSG), and multi bottom-up tree transduc-ers (MBOT) constrain their rules to be linearand non-deleting, which is important for efficientrule learning and transduction execution (Chiang,2004; Galley et.
al, 2004; Yamada and Knight,2001; Zhang et.
al, 2008; Maletti, 2010).The language TTT does not have any suchrestrictions, as it is intended as a general pro-gramming aid, with a concise syntax for po-tentially radical transformations, rather than amodel of particular classes of linguistic opera-tions.
Thus, for example, the 5-element pat-tern (!
((* A) B) ((* A) C) ((* A) D)((* A) E) ((* A))) applied to the expres-sion (A A A A A) rescans the latter 5 times, im-plying quadratic complexity.
(Our current imple-mentation does not attempt regular expression re-duction for efficient recognition.)
With the addi-tion of the permutation operator {}, we can forceall permutations of certain patterns to be tried inan unsuccessful match (e.g., (({} (!
A B C)(!
A B C) (!
A B C))) applied to (C BE)), leading to exponential complexity.
(Again,our current implementation does not attempt tooptimize.)
Also, allowance for repeated applica-tion of a set of rules to a tree, until no furtherapplications are possible, leads to Turing equiv-alence.
This of course is true even if only the 4underscore-operators are allowed: We can simu-late the successive transformations of the config-urations of a Turing machine with string rewritingrules, which are easily expressed in terms of thoseoperators and /.
Additionally, pattern predicatesand function application in the right-hand sides ofrules are features present in TTT that are not in-cluded in the above formal models.
In themselves(even without iterative rule application), these un-restricted predicates and functions lead to Turingequivalence.The set of pattern matching operators was cho-sen so that a number of disparate pattern match-ing programs could all be replaced with conciseTTT rules.
It does subsume regular tree expres-sions and can therefore be used to match any reg-ular tree language.
Specifically, alternation canbe expressed with !
and (vertical) iteration with?
@ and *.
The example expression from (Comon,2007) can be specified as (?
@ (* (cons 0@)) nil), which matches Lisp expressions cor-responding to lists of zero or more zeros.
TTTalso differs from standard tree automata by lackof an explicit state.Nondeterminism and noncommutativity: Ingeneral, given a set of transductions (or even a sin-gle transduction) and an input tree there may beseveral ways to apply the transductions, resultingin different trees.
This phenomenon comes fromthree sources:?
Rule application order - transductions are notin general commutative.?
Bindings - a pattern may have many sets ofconsistent bindings to a tree (e.g., pattern( * *1) can be bound to the tree (X YZ) in four distinct ways).?
Subtree search order - a single transductionmay be applicable to a tree in multiple lo-cations (e.g., (/ !
X) could replace anynode of a tree, including the root, with a sin-gle symbol).Therefore some trees may have many reducedforms with respect to a set of transductions (whereby reduced we mean a tree to which no trans-ductions are applicable) and even more reachableforms.Our current implementation does not attempt toenumerate possible transductions.
Rather, for agiven tree and a list of transductions, each trans-duction (in the order given) is applied in top-downfashion at each feasible location (matching thelhs), always using the first binding that resultsfrom this depth-first, left-to-right (i.e., pre-order)search.
Our assumption is that the typical user hasa clear sense of the order in which transformationsare to be performed, and is working with rules thatdo not interact in unexpected ways.
For exam-ple, consider the cases of PP misattachment men-tioned earlier.
In most cases, such misattachmentsare disjoint (e.g., consider a caption reading ?Johnand Mary in front and David and Sue in the back?,where both PPs may well have been attached tothe proper noun immediately to the left, instead26of to the appropriate conjunction).
It is also pos-sible for one rule application to change the contextof another, but this is not necessarily problematic.For instance, suppose that in the sentence ?Johndrove the speaker to the airport in a hurry?
the PP?to the airport?
has been misattached to the NPfor ?the speaker?
and that the PP ?in a hurry?
hasbeen misattached to the NP for ?the airport?.
Sup-pose further that we have a repair rule that carriesa PP attached to an NP upward in the parse treeuntil it reaches a VP node, reattaching the PP as achild of that VP.
(The repair rule might incorpo-rate a computable predicate that detects a poor fitbetween an NP and a PP that modifies it.)
Thenthe result will be the same regardless of the orderin which the two repairs are carried out.
The dif-ference is just that with a preorder discipline, thesecond PP (?in a hurry?)
will move upward by onestep less than if the order is reversed, because thefirst rule application will have shortened the pathto the dominating VP by one step.In the future it may be worthwhile to implementexhaustive exploration of all possible matches andexpression rewrites, as has been done in Mathe-matica.
In general this would call for lazy compu-tation, since the set of rewrites may be an infiniteset.4 Some linguistic examplesParse Tree Refinement: First, here is a sim-ple transduction to delete empty brackets, whichsometimes occur in the Brown corpus: (/ ( *() *1) ( * *1)).To distinguish between past and passive partici-ples, we want to search for the verb have, andchange the participle token correspondingly, asdiscussed earlier.
The next two transductions areequivalent ?
the first is global and the second is anexample of a local or on-the-fly transduction.
Forsimplicity we consider only the has form of have.Observe the more concise form, and simpler vari-able specifications of the second transduction.
(/ (VP _* (VBZ HAS) _*1 (VBN _!)
_*2)(VP _* (VBZ HAS) _*1 (VBEN _!)
_*2))(VP _* (VBZ HAS) _* ((/ VBN VBEN) _!)
_*)To distinguish temporal and non-temporalnominals, we use a computable predicate to de-tect temporal nouns, and then annotate the NP tagaccordingly.
(One more time, we show global andlocal variants.
)(/ (NP * nn-temporal?
)(NP-TIME * nn-temporal?
))((/ NP NP-TIME) * nn-temporal?
)Assimilation of verb particles into single con-stituents is useful to semantic interpretation, andis accomplished with the transduction:(/ (VP (VB \_!1)(\{\} (PRT (RP _!2)) (NP _*1)))(VP (VB _!1 _!2) (NP _*1)))We often particularize PPs to show thepreposition involved, e.g., PP-OF, PP-FROM,etc.
Note that this transduction uses thejoin-with-dash!
function, which enables usto avoid writing a separate transduction for eachpreposition:(/ (PP (IN !)
*1)((join-with-dash!
PP !
)(IN !)
*1))Such a rule transforms subtrees such as (PP (INFROM)) by rewriting the PP tag as (PP-FROM(IN FROM) .As a final syntactic processing example (tran-sitioning to discourse phenomena and semantics),we illustrate the use of TTT in establishing poten-tial coreferents licensed by C-command relations,for the sentence mentioned earlier.
We assumethat for reference purposes, NP nodes are deco-rated with a SEM-INDEX feature (with an integervalue), and pronominal NPs are in addition deco-rated with a CANDIDATE-COREF feature, whosevalue is a list of indices (initially empty).
Thus wehave the following parse structure for the sentenceat issue (where for understandabilty of the rela-tively complex parse tree we depart from Tree-bank conventions not only in the use of some ex-plicit features but also in using linguistically moreconventional phrasal and part-of-speech categorynames; R stands for relative clause):(S ((NP SEM-INDEX 1) (NAME John))(VP (V shows)((NP SEM-INDEX 2) (NAME Lillian))((NP SEM-INDEX 3) (DET the)(N (N snowman)(R (RELPRON that)((S GAP NP)((NP SEM-INDEX 4CANDIDATE-COREF ())(PRON he))((VP GAP NP) (V built)((NP SEM-INDEX 4)(PRON *trace*)))))))))Here is a TTT rule that adjoins the index ofa C-commanding NP node to the CANDIDATE-COREF list of a C-commanded pronominal NP:(_* ((NP _* SEM-INDEX _!.
_*) _+) _*27(?
* ((NP _* CANDIDATE-COREF(/ _!(adjoin!
_!.
_!))
_*) (PRON _!)))
_*)The NP on the first line is the C-commandingNP, and note that we are using a sticky vari-able ?
!.?
for its index, since we need to use itlater.
(None of the other match variables needto be sticky, and we reuse ?
*?
and ?
!?
multi-ple times.)
The key to understanding the rule isthe constituent headed by ??
*?, which triggers asearch for a (right) sibling or descendant of a sib-ling of the NP node that reaches an NP consistingof a pronoun, and thus bearing the CANDIDATE-COREF feature.
This feature is replaced ?on thefly?
by adjoining the index of the C-commandingnode (the value of ?
!.?)
to it.
For the sampletree, the result is the following (note the value?(1)?
of the CANDIDATE-COREF list):(S ((NP SEM-INDEX 1) (NAME John))(VP (V shows)((NP SEM-INDEX 2) (NAME Lillian))((NP SEM-INDEX 3) (DET the)(N (N snowman)(R (RELPRON that)((S GAP NP)((NP SEM-INDEX 4CANDIDATE-COREF (1))(PRON he))((VP GAP NP) (V built)((NP SEM-INDEX 4)(PRON *trace*)))))))))Of course, this does not yet incorporate numberand gender checks, but while these could be in-cluded, it is preferable to gather candidates andheuristically pare them down later.
Thus repeatedapplication of the rule would also add the index 2(for Lillian) to CANDIDATE-COREF.Working with Logical FormsSkolemization: Skolemization of an existentialformula of type (some x R S), where x isa variable, R is a restrictor formula and S is thenuclear scope, is performed via the transduction(/ (some !
!1 !2)(subst-new!
!
( !1 and.cc !2))).The function subst-new!
replaces all oc-currences of a free variable symbol in anexpression with a new one.
(We assume thatno variable occurs both bound and free in thesame expression.)
It uses a TTT transductionto accomplish this.
For example, (some x(x politician.n) (x honest.a)) be-comes ((C1.skol politician.n) and.cc(C1.skol honest.a)).Inference: We can use the following rule to ac-complish simple default inferences such as that ifmost things with property P have property Q, andmost things with property Q have property R,then (in the absence of knowledge to the contrary)many things with property P also have propertyR.
(Our logical forms use infix syntax for predica-tion, i.e., the predicate follows the ?subject?
argu-ment.
Predicates can be lambda abstracts, and thecomputable boolean function pred?
checks forarbitrary predicative constructs.
)(/(_* (most _!.1(_!.1 (!.p pred?
))(_!.1 (!.q pred?
)))_* (most _!.2(_!.2 !.q)(_!.2 (!.r pred?)))
_*)(many _!.1 (_!.1 !.p) (_!.1 !.r)))For example, ((most x (x dog.n) (x pet.n))(most y (y pet.n) (x friendly.a))) yieldsthe default inference (many (x dog.n) (xfriendly.a)).The assumption here is that the two most-formulas are embedded in a list of formulas (se-lected by the inference algorithm), and the threeoccurrences of * allow for miscellaneous sur-rounding formulas.
(To allow for arbitrary or-dering of formulas in the working set, we alsoprovide a variant with the two most-formulas inreverse order.)
Inference with tree transductionrules has also been performed by (Koller and Ste-fan, 2010).Predicate Disambiguation: The followingrules are applicable to patterns of predica-tion such as ((det dog.n have.v (dettail.n)), ((det bird.n have.v (detnest.n)), and ((det man.n) have.v(det accident.n)).
(Think of det as anunspecified, unscoped quantifier.)
The rulessimultaneously introduce plausible patterns ofquantification and plausible disambiguations ofthe various senses of have.v (e.g., have as part,possess, eat, experience):(/ ((det (!
animal?))
have.v(det (!1 animal-part?
)))(all-or-most x (x !
)(some e ((pair x e) enduring)(some y (y !1)((x have-as-part.v y) ** e)))))(/ ((det (!
agent?))
have.v(det (!1 possession?
)))(many x (x !
)(some e(some y (y !1)(x possess.v y) ** e))))28(/ ((det (!
animal?))
have.v(det (!1 food?
)))(many x (x !
)(occasional e(some y (y !1)(x eat.v y) ** e))))(/ ((det (!
person?))
have.v(det (!1 event?
)))(many x (x !
)(occasional e(some y (y !1)((x experience.v y) ** e)))))Computable predicates such as animal?
andevent?
are evaluated with the help of WordNetand other resources.
Details of the logical formneed not concern us, but it should be noted thatthe ?**?
connects sentences to events they charac-terize much as in various other theories of eventsand situations.Thus, for example, ((det dog.n have.v(det tail.n)) is mapped to:(all-or-most x (x dog.n(some e ((pair x e) enduring)(some y (y tail.n)((x have-as-part.v y) ** e)))))This expresses that for all or most dogs, the doghas an enduring attribute (formalized as an agent-event pair) of having a tail as a part.Logical Interpretation: The following trans-ductions directly map some simple parse trees tological forms.
The rules, applied as often as possi-ble to a parse tree, replace all syntactic constructs,recognizable from (Treebank-style) phrase head-ers like (JJ ...), (NP ...), (VP ...), (S...), etc., by corresponding semantic constructs.For example, ?The dog bit John Doe?, parsed as(S (NP (DT the) (NN dog))(VP (VBD bit)(NP (NNP John) (NNP Doe))))yields (the x (x dog.n) (x bit.vJohn Doe.name)).Type-extensions such as ?.a?, ?.n?, and ?.v?indicate adjectival, nominal, and verbal predi-cates, and the extension ?.name?
indicates an in-dividual constant (name); these are added by thefunctions make-adj!, make-noun!, and soon.
The fourth rule below combines two succes-sive proper nouns (NNPs) into one.
We omit eventvariables, tense and other refinements.
(/ (JJ !)
(make-adj!
!
))(/ (NN !)
(make-noun!
!
))(/ (VBD !)
(make-verb!
!
))(/ ( *.a (NNP !.1) (NNP !.2) *.b)( *.a (NNP !.1 !.2) *.b))(/ (NNP +) (make-name!
( +)))(/ (NP !)
!
)(/ (S (NP (DT the) !)
(VP +))(the x (x !)
(x +))These rules are illustrative only, and are notfully compositional, as they interpret an NP witha determiner only in the context of a senten-tial subject, and a VP only in the context of asentential predicate.
Also, by scoping the vari-able of quantification, they do too much work atonce.
A more general approach would use com-positional rules such as (/ (S (!1 NP?)
(!2VP?))
((sem!
!1) (sem!
!2))), where thesem!
function again makes use of TTT, re-cursively unwinding the semantics, with ruleslike the first five above providing lexical-levelsem!-values.We have also experimented with rendering log-ical forms back into English, which is rather eas-ier, mainly requiring dropping of variables andbrackets and some reshuffling of constituents.5 ConclusionThe TTT language is well-suited to the applica-tions it was aimed at, and is already proving use-ful in current syntactic/semantic applications.
Itprovides a very concise, transparent way of speci-fying transformations that previously required ex-tensive symbolic processing.
Some remaining is-sues are efficient access to, and deployment of,rules that are locally relevant to a transduction;and heuristics for executing matches and trans-ductions more efficiently (e.g., recognizing vari-ous cases where a complex rule cannot possiblymatch a given tree, because the tree lacks someconstituents called for by the rule; or use of ef-ficient methods for matching regular-expressionsubpatterns).The language also holds promise for rule-learning, thanks to its simple template-to-template basic syntax.
The kinds of learning en-visioned are learning parse-tree repair rules, andperhaps also LF repair rules and LF-to-Englishrules.AcknowledgmentsThe work was supported by ONR-STTR awardN00014-11-10417, and NSF grants IIS-1016735,NSF IIS-0916599, and NSF IIS-0910611.29ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
ACL 2005, 173?180.
Association forComputational Linguistics, Ann Arbor, MI, USA.David Chiang.
2004.
Evaluation of Grammar For-malisms for Applications to Natural Language Pro-cessing and Biological Sequence Analysis.
Phd.Thesis.
University of Pennsylvania.H.
Comon and M. Dauchet and R. Gilleron andC.
Lo?ding and F. Jacquemard and D. Lugiezand S. Tison and M. Tommasi 2007.
Tree Au-tomata Techniques and Applications Available on:http://www.grappa.univ-lille3.fr/tataMichel Galley and Mark Hopkins and Kevin Knightand Daniel Marcu 2004.
What?s in a Transla-tion Rule?.
NAACL 2004, 273?280.
Boston, MA,USA.Thomas Genet and Valerie View Triem Tong2003.
Timbuk: A Tree Automata Libraryhttp://www.irisa.fr/celtique/genet/timbuk/Ralph Griswold 1971.
The SNOBOL4 ProgrammingLanguage.
Prentice-Hall, Inc. Upper Saddle River,NJ, USA.Paul Hudak, John Peterson, and Joseph Fasel.2000.
A Gentle Introduction To Haskell: Ver-sion 98.
Los Alamos National Laboratory.http://www.haskell.org/tutorial/patterns.html.Alexander Koller and Stefan Thater.
2010.
Comput-ing weakest readings.
ACL 2010.
30?39.
Strouds-burg, PA, USA.Kevin Knight.
2007.
Capturing practical natural lan-guage transformations.
Machine Translation, Vol21, Issue 2, 121?133.
Kluwer Academic Publish-ers.
Hingham, MA, USA.Roger Levy and Galen Andrew 2006.
Tregex andTsurgeon: tools for querying and manipulating treedata structures.
Language Resources EvaluationConference (LREC ?06).Andreas Maletti 2010.
Why synchronous tree sub-stitution grammars?.
HLT 2010.
Association forComputational Linguistics, Stroudsburg, PA, USA.Jonathan May and Kevin Knight 2008 A Primer onTree Automata Software for Natural Language Pro-cessing.
http://www.isi.edu/licensed-sw/tiburon/Peter Norvig 1991.
Paradigms of Artificial Intelli-gence Programming Morgan Kaufmann.
Waltham,MA, USA.Don Rozenberg 2002.
SnoPy - SnobolPattern Matching Extension for Python.http://snopy.sourceforge.net/user-guide.html.Wolfram Research, Inc. 2010.
Wolfram Mathe-matica 8 Documentation.
Champagne, IL, USA.http://reference.wolfram.com/mathematica/guide/RulesAndPatterns.html.World Wide Web Consortium.
1999.
XML Path Lan-guage (XPath) http://www.w3.org/TR/xpath/1999.
XSL Transformations (XSLT)http://www.w3.org/TR/xsltKenji Yamada and Kevin Knight 2001.
A Syntax-Based Statistical Translation Model.
ACL 2001,523?530.
Stroudsburg, PA, USA.Min Zhang and Hongfei Jiang and Aiti Aw andHaizhou Li and Chew Lim Tan and Sheng Li 2008.A tree sequence alignment-based tree-to-tree trans-lation model.
ACL 2008.30
