Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 925?931,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSort Story: Sorting Jumbled Images and Captions into StoriesHarsh Agrawal?,1 Arjun Chandrasekaran?,1,?Dhruv Batra3,1 Devi Parikh3,1 Mohit Bansal4,21Virginia Tech 2TTI-Chicago 3Georgia Institute of Technology 4UNC Chapel Hill{harsh92, carjun, dbatra, parikh}@vt.edu, mbansal@cs.unc.eduAbstractTemporal common sense has applications inAI tasks such as QA, multi-document summa-rization, and human-AI communication.
Wepropose the task of sequencing ?
given a jum-bled set of aligned image-caption pairs thatbelong to a story, the task is to sort themsuch that the output sequence forms a coher-ent story.
We present multiple approaches,via unary (position) and pairwise (order) pre-dictions, and their ensemble-based combina-tions, achieving strong results on this task.
Weuse both text-based and image-based features,which depict complementary improvements.Using qualitative examples, we demonstratethat our models have learnt interesting aspectsof temporal common sense.1 IntroductionSequencing is a task for children that is aimed at im-proving understanding of the temporal occurrence ofa sequence of events.
The task is, given a jumbledset of images (and maybe captions) that belong toa single story, sort them into the correct order sothat they form a coherent story.
Our motivation inthis work is to enable AI systems to better under-stand and predict the temporal nature of events inthe world.
To this end, we train machine learningmodels to perform the task of ?sequencing?.Temporal reasoning has a number of applicationssuch as multi-document summarization of multiplesources of, say, news information where the relativeorder of events can be useful to accurately mergeinformation in a temporally consistent manner.
Inquestion answering tasks (Richardson et al, 2013;?Denotes equal contribution.
?Part of this work was done during an internship at TTIC.Figure 1: (a) The input is a jumbled set of alignedimage-caption pairs.
(b) Actual output of the system?
an ordered sequence of image-caption pairs thatform a coherent story.Fader et al, 2014; Weston et al, 2015; Ren et al,2015), answering questions related to when an eventoccurs, or what events occurred prior to a particularevent require temporal reasoning.
A good temporalmodel of events in everyday life, i.e., a ?temporalcommon sense?, could also improve the quality ofcommunication between AI systems and humans.Stories are a form of narrative sequences that havean inherent temporal common sense structure.
Wepropose the use of visual stories depicting personalevents to learn temporal common sense.
We usestories from the Sequential Image Narrative Dataset(SIND) (Ting-Hao Huang, 2016) in which a set of5 aligned image-caption pairs together form a co-herent story.
Given an input story that is jumbled(Fig.
1(a)), we train machine learning models to sortthem into a coherent story (Fig.
1(b)).11Note that ?jumbled?
here refers to the loss of temporal or-dering; image-caption pairs are still aligned.925Our contributions are as follows:?
We propose the task of visual story sequencing.?
We implement two approaches to solve the task:one based on individual story elements to predictposition, and the other based on pairwise story ele-ments to predict relative order of story elements.
Wealso combine these approaches in a voting schemethat outperforms the individual methods.?
As features, we represent a story element as bothtext-based features from the caption and image-based features, and show that they provide comple-mentary improvements.
For text-based features, weuse both sentence context and relative order baseddistributed representations.?
We show qualitative examples of our models learn-ing temporal common sense.2 Related WorkTemporal ordering has a rich history in NLP re-search.
Scripts (Schank and Abelson, 2013), andmore recently, narrative chains (Chambers and Ju-rafsky, 2008) contain information about the partic-ipants and causal relationships between events thatenable the understanding of stories.
A number ofworks (Mani and Schiffman, 2005; Mani et al,2006; Boguraev and Ando, 2005) learn temporal re-lations and properties of news events from the dense,expert-annotated TimeBank corpus (Pustejovsky etal., 2003).
In our work, however, we use multi-modal story data that has no temporal annotations.A number of works also reason about temporalordering by using manually defined linguistic cues(Webber, 1988; Passonneau, 1988; Lapata and Las-carides, 2006; Hitzeman et al, 1995; Kehler, 2000).Our approach uses neural networks to avoid featuredesign for learning temporal ordering.Recent works (Modi and Titov, 2014; Modi,2016) learn distributed representations for predi-cates in a sentence for the tasks of event ordering andcloze evaluation.
Unlike their work, our approachmakes use of multi-modal data with free-form nat-ural language text to learn event embeddings.
Fur-ther, our models are trained end-to-end while theirpipelined approach involves parsing and extractingverb frames from each sentence, where errors maypropagate from one module to the next (as discussedin Section 4.3).Chen et al (2009) use a generalized Mallowsmodel for modeling sequences for coherence withinsingle documents.
Their approach may also be ap-plicable to our task.
Recently, Mostafazadeh et al(2016) presented the ?ROCStories?
dataset of 5-sentence stories with stereotypical causal and tem-poral relations between events.
In our work though,we make use of a multi-modal story-dataset that con-tains both images and associated story-like captions.Some works in vision (Pickup et al, 2014; Bashaet al, 2012) also temporally order images; typicallyby finding correspondences between multiple im-ages of the same scene using geometry-based ap-proaches.
Similarly, Choi et al (2016) compose astory out of multiple short video clips.
They definemetrics based on scene dynamics and coherence,and use dense optical flow and patch-matching.
Incontrast, our work deals with stories containing po-tentially visually dissimilar but semantically coher-ent set of images and captions.A few other recent works (Kim et al, 2015; Kimet al, 2014; Kim and Xing, 2014; Sigurdsson et al,2016; Bosselut et al, 2016; Wang et al, 2016) sum-marize hundreds of individual streams of informa-tion (images, text, videos) from the web that dealwith a single concept or event, to learn a commontheme or storyline or for timeline summarization.Our task, however, is to predict the correct sortingof a given story, which is different from summa-rization or retrieval.
Ramanathan et al (2015) at-tempt to learn temporal embeddings of video framesin complex events.
While their motivation is similarto ours, they deal with sampled frames from a videowhile we attempt to learn temporal common sensefrom multi-modal stories consisting of a sequenceof aligned image-caption pairs.3 ApproachIn this section, we first describe the two componentsin our approach: unary scores that do not use con-text, and pairwise scores that encode relative order-ings of elements.
Next, we describe how we com-bine these scores through a voting scheme.3.1 Unary ModelsLet ?
?
?n denote a permutation of n elements(image-caption pairs).
We use ?i to denote the posi-tion of element i in the permutation ?.
A unary score926Su(?)
captures the appropriateness of each story el-ement i in position ?i:Su(?)
=n?i=1P (?i|i) (1)where P (?i|i) denotes the probability of the ele-ment i being present in position ?i, which is theoutput from an n-way softmax layer in a deepneural network.
We experiment with 2 networks ?
(1) A language-alone unary model (Skip-Thought+MLP) that uses a Gated RecurrentUnit (GRU) proposed by Cho et al (2014) to embeda caption into a vector space.
We use the Skip-Thought (Kiros et al, 2015) GRU, which is trainedon the BookCorpus (Zhu et al, 2015) to predict thecontext (preceding and following sentences) of agiven sentence.
These embeddings are fed as inputinto a Multi-Layer Perceptron (MLP).
(2) A language+vision unary model (Skip-Thought+CNN+MLP) that embeds the captionas above and embeds the image via a ConvolutionalNeural Network (CNN).
We use the activationsfrom the penultimate layer of the 19-layer VGG-net (Simonyan and Zisserman, 2014), which havebeen shown to generalize well.
Both embeddingsare concatenated and fed as input to an MLP.In both cases, the best ordering ofthe story elements (optimal permutation)??
= arg max??
?n Su(?)
can be found effi-ciently in O(n3) time with the Hungarian algo-rithm (Munkres, 1957).
Since these unary scoresare not influenced by other elements in the story,they capture the semantics and linguistic structuresassociated with specific positions of stories e.g., thebeginning, the middle, and the end.3.2 Pairwise ModelsSimilar to learning to rank approaches (Hang, 2011),we develop pairwise scoring models that given a pairof elements (i, j), learn to assign a score:S([[?i < ?j ]] | i, j) indicating whether element ishould be placed before element j in the permutation?.
Here, [[?]]
indicates the Iverson bracket (which is1 if the input argument is true and 0 otherwise).
Wedevelop and experiment with the following 3 pair-wise models:(1) A language-alone pairwise model (Skip-Thought+MLP) that takes as input a pair of Skip-Thought embeddings and trains an MLP (withhinge-loss) that outputs S([[?i < ?j ]] | i, j), thescore for placing i before j.
(2) A language+vision pairwise model (Skip-Thought+CNN+MLP) that concatenates the Skip-Thought and CNN embeddings for i and j and trainsa similar MLP as above.
(3) A language-alone neural position embedding(NPE) model.
Instead of using frozen Skip-Thoughtembeddings, we learn a task-aware ordered dis-tributed embedding for sentences.
Specifically,each sentence in the story is embedded X =(x1, .
.
.
,xn), xi ?
Rd+, via an LSTM (Hochreiterand Schmidhuber, 1997) with ReLU non-linearities.Similar to the max-margin loss that is applied to neg-ative examples by Vendrov et al (2016), we use anasymmetric penalty that encourages sentences ap-pearing early in the story to be placed closer to theorigin than sentences appearing later in the story.Lij =??
?max(0, ??
(xj ?
xi))??
?2Loss =?1<=i<j=nLij(2)At train time, the parameters of the LSTM arelearned end-to-end to minimize this asymmetric or-dered loss (as measured over the gold-standard se-quences).
At test time, we use S([[?i < ?j ]] | i, j) =Lij .
Thus, as we move away from the origin in theembedding space, we traverse through the sentencesin a story.
Each of these three pairwise approachesassigns a score S(?i, ?j |i, j) to an ordered pair ofelements (i,j), which is used to construct a pairwisescoring model:Sp(?)
=?1<=i<j<=n{S([[?i < ?j ]])?
S([[?j < ?i]])},(3)by summing over the scores for all possible orderedpairs in the permutation.
This pairwise score cap-tures local contextual information in stories.
Find-ing the best permutation ??
= arg max??
?n Sp(?
)under this pairwise model is NP-hard so approxi-mations will be required.
In our experiments, westudy short sequences (n = 5), where the space ofpermutations is easily enumerable (5!
= 120).
Forlonger sequences, we can utilize integer program-ming methods or well-studied spectral relaxationsfor this problem.9273.3 Voting-based EnsembleTo combine the complementary information cap-tured by the unary (Su) and pairwise models (Sp),we use a voting-based ensemble.
For each methodin the ensemble, we find the top three permuta-tions.
Each of these permutations (?k) then votefor a particular element to be placed at a particu-lar position.
Let V be a vote matrix such that Vijstores the number of votes for ith element to oc-cur at jth position, i.e.
Vij = ?k[[?ki == j]]).We use the Hungarian algorithm to find the optimalpermutation that maximizes the votes assigned, i.e.?
?vote = arg max??
?n?ni=1?nj=1 Vij ?
[[?i == j]].We experimented with a number of model votingcombinations and found the combination of pairwiseSkip-Thought+CNN+MLP and neural position em-beddings to work best (based on a validation set).4 Experiments4.1 DataWe train and evaluate our model on personal multi-modal stories from the SIND (Sequential ImageNarrative Dataset) (Ting-Hao Huang, 2016), whereeach story is a sequence of 5 images and correspond-ing story-like captions.
The narrative captions in thisdataset, e.g., ?friends having a good time?
(as op-posed to ?people sitting next to each other?)
capturea sequential, conversational language, which is char-acteristic of stories.
We use 40,155 stories for train-ing, 4990 for validation and 5055 stories for testing.4.2 MetricsWe evaluate the performance of our model at cor-rectly ordering a jumbled set of story elements usingthe following 3 metrics:Spearman?s rank correlation (Sp.)
(Spearman,1904) measures if the ranking of story elements inthe predicted and ground truth orders are monotoni-cally related (higher is better).Pairwise accuracy (Pairw.)
measures the fractionof pairs of elements whose predicted relative order-ing is the same as the ground truth order (higher isbetter).Average Distance (Dist.)
measures the averagechange in position of all elements in the predictedMethod Features Sp.
Pairw.
Dist.Random Order 0.000 0.500 1.601Unary SkipThought 0.508 0.718 1.373SkipThought + Image 0.532 0.729 1.352Pairwise SkipThought 0.546 0.732 0.923SkipThought + Image 0.565 0.740 0.897Pairwise Order NPE 0.480 0.704 1.010Voting SkipThought + Image 0.675 0.799 0.724(Pairwise) + NPETable 1: Performance of our different models andfeatures at the sequencing task.story from their respective positions in the groundtruth story (lower is better).4.3 ResultsPairwise Models vs Unary Models As shown inTable 1, the pairwise models based on Skip-Thoughtfeatures outperform the unary models in our task.However, the Pairwise Order Model performs worsethan the unary Skip-Thought model, suggesting thatthe Skip-Thought features, which encode context ofa sentence, also provide a crucial signal for temporalordering of story sentences.Contribution of Image Features Augmenting thetext features with image features results in a visibleperformance improvement of both the model trainedwith unary features and the model trained with pair-wise features.
While image features by themselvesresult in poor performance on this task, they seem tocapture temporal information that is complementaryto the text features.Ensemble Voting To exploit the fact that unaryand pairwise models, as well as text and image fea-tures, capture different aspects of the story, we com-bine them using a voting ensemble.
Based on thevalidation set, we found that combining the Pair-wise Order model and the Pairwise model with bothSkip-Thought and Image (CNN) features performsthe best.
This voting based method achieves thebest performance on all three metrics.
This showsthat our different approaches indeed capture comple-mentary information regarding feasible orderings ofcaption-image pairs to form a coherent story.For comparison to existing related work, we tried928(a) First Position (b) Second Position (c) Third Position(d) Fourth Position (e) Fifth PositionFigure 2: Word cloud corresponding to most discriminative words for each position.to duplicate the pipelined approach of Modi andTitov (2014).
For this, we first parse our storysentences to extract SVO (subject, verb, object) tu-ples (using the Stanford Parser (Chen and Manning,2014)).
However, this step succeeds for only 60%of our test data.
Now even if we consider a perfectdownstream algorithm that always makes the cor-rect position prediction given SVO tuples, the over-all performance is still a Spearman correlation ofjust 0.473, i.e., the upper bound performance of thispipelined approach is lower than the performanceof our text-only end-to-end model (correlation of0.546) in Table 1.4.4 Qualitative AnalysisVisualizations of position predictions from ourmodel demonstrate that it has learnt the three actstructure (Trottier, 1998) in stories ?
the setup, themiddle and the climax.
We also present success andfailure examples of our sorting model?s predictions.See the supplementary for more details and figures.We visualize our model?s temporal commonsense, in Fig.
2.
The word clouds show discrim-inative words ?
the words that the model believesare indicative of sentence positions in a story.
Thesize of a word is proportional to the ratio of its fre-quency of occurring in that position to other po-sitions.
Some words like ?party?, ?wedding?, etc.,probably because our model believes that the startthe story describes the setup ?
the occasion or event.People often tend to describe meeting friends orfamily members which probably results in the dis-criminative words such as ?people?, ?friend?, ?every-one?
in the second and the third sentences.
More-over, the model believes that people tend to concludethe stories using words like ?finally?, ?afterwards?,tend to talk about ?great day?, group ?pictures?
witheveryone, etc.5 ConclusionWe propose the task of ?sequencing?
in a set ofimage-caption pairs, with the motivation of learn-ing temporal common sense.
We implement multi-ple neural network models based on individual andpairwise element-based predictions (and their en-semble), and utilize both image and text features,to achieve strong performance on the task.
Ourbest system, on average, predicts the ordering ofsentences to within a distance error of 0.8 (out of5) positions.
We also analyze our predictions andshow qualitative examples that demonstrate tempo-ral common sense.AcknowledgementsWe thank Ramakrishna Vedantam and the anony-mous reviewers for their helpful suggestions.
Thiswork was supported by: NSF CAREER awards toDB and DP, ARO YIP awards to DB and DP, IC-TAS Junior Faculty awards to DB and DP, GoogleFaculty Research award to DP and DB, ARL grantW911NF-15-2-0080 to DP and DB, ONR grantN00014-14-1-0679 to DB and N00014-16-1-2713 toDP, ONR YIP award to DP, Paul G. Allen FamilyFoundation Allen Distinguished Investigator awardto DP, Alfred P. Sloan Fellowship to DP, AWS inEducation Research grant to DB, NVIDIA GPU do-nations to DB and MB, an IBM Faculty Awardand Bloomberg Data Science Research Grant toMB.929References[Basha et al2012] Tali Basha, Yael Moses, and Shai Avi-dan.
2012.
Photo sequencing.
In ECCV.
2[Boguraev and Ando2005] Branimir Boguraev andRie Kubota Ando.
2005.
Timeml-compliant textanalysis for temporal reasoning.
In IJCAI.
2[Bosselut et al2016] Antoine Bosselut, Jianfu Chen,David Warren, Hannaneh Hajishirzi, and Yejin Choi.2016.
Learning prototypical event structure fromphoto albums.
In ACL.
2[Chambers and Jurafsky2008] Nathanael Chambers andDaniel Jurafsky.
2008.
Unsupervised learning of nar-rative event chains.
In ACL.
Citeseer.
2[Chen and Manning2014] Danqi Chen and Christopher DManning.
2014.
A fast and accurate dependencyparser using neural networks.
In EMNLP.
5[Chen et al2009] Harr Chen, SRK Branavan, ReginaBarzilay, David R Karger, et al 2009.
Content mod-eling using latent permutations.
Journal of ArtificialIntelligence Research.
[Cho et al2014] Kyunghyun Cho, Bart Van Merrie?nboer,Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder-decoder forstatistical machine translation.
In EMNLP.
[Choi et al2016] Jinsoo Choi, Tae-Hyun Oh, andIn So Kweon.
2016.
Video-story composition via plotanalysis.
In CVPR.
[Fader et al2014] Anthony Fader, Luke Zettlemoyer, andOren Etzioni.
2014.
Open question answering overcurated and extracted knowledge bases.
In ACMSIGKDD.
1[Hang2011] LI Hang.
2011.
A short introduction tolearning to rank.
IEICE TRANSACTIONS on Infor-mation and Systems.
3[Hitzeman et al1995] Janet Hitzeman, Marc Moens, andClaire Grover.
1995.
Algorithms for analysing thetemporal structure of discourse.
In EACL.
2[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural computation.
3[Kehler2000] Andrew Kehler.
2000.
Coherence and theresolution of ellipsis.
Linguistics and Philosophy.
2[Kim and Xing2014] Gunhee Kim and Eric Xing.
2014.Reconstructing storyline graphs for image recommen-dation from web community photos.
In CVPR.
2[Kim et al2014] Gunhee Kim, Leonid Sigal, and EricXing.
2014.
Joint summarization of large-scale col-lections of web images and videos for storyline recon-struction.
In CVPR.
2[Kim et al2015] Gunhee Kim, Seungwhan Moon, andLeonid Sigal.
2015.
Joint photo stream and blog postsummarization and exploration.
In CVPR.
2[Kiros et al2015] Ryan Kiros, Yukun Zhu, Ruslan RSalakhutdinov, Richard Zemel, Raquel Urtasun, An-tonio Torralba, and Sanja Fidler.
2015.
Skip-thoughtvectors.
In NIPS.
3[Lapata and Lascarides2006] Mirella Lapata and AlexLascarides.
2006.
Learning sentence-internal tem-poral relations.
Journal of Artificial Intelligence Re-search.
2[Mani and Schiffman2005] Inderjeet Mani and BarrySchiffman.
2005.
Temporally anchoring and order-ing events in news.
Time and Event Recognition inNatural Language.
John Benjamins.
2[Mani et al2006] Inderjeet Mani, Marc Verhagen, BenWellner, Chong Min Lee, and James Pustejovsky.2006.
Machine learning of temporal relations.
InCOLING-ACL.
2[Modi and Titov2014] Ashutosh Modi and Ivan Titov.2014.
Inducing neural models of script knowledge.
InCoNLL.
2[Modi2016] Ashutosh Modi.
2016.
Event embeddingsfor semantic script modeling.
In CoNLL.
2[Mostafazadeh et al2016] Nasrin Mostafazadeh,Nathanael Chambers, Xiaodong He, Devi Parikh,Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli,and James Allen.
2016.
A corpus and cloze evaluationfor deeper understanding of commonsense stories.
InNAACL.
[Munkres1957] James Munkres.
1957.
Algorithms forthe assignment and transportation problems.
Journalof the Society for Industrial and Applied Mathematics.3[Passonneau1988] Rebecca J Passonneau.
1988.
A com-putational model of the semantics of tense and aspect.Computational Linguistics.
2[Pickup et al2014] Lyndsey Pickup, Zheng Pan, DonglaiWei, YiChang Shih, Changshui Zhang, Andrew Zis-serman, Bernhard Scholkopf, and William Freeman.2014.
Seeing the arrow of time.
In CVPR.
2[Pustejovsky et al2003] James Pustejovsky, PatrickHanks, Roser Sauri, Andrew See, Robert Gaizauskas,Andrea Setzer, Dragomir Radev, Beth Sundheim,David Day, Lisa Ferro, et al 2003.
The timebankcorpus.
In Corpus linguistics.
2[Ramanathan et al2015] Vignesh Ramanathan, KevinTang, Greg Mori, and Li Fei-Fei.
2015.
Learningtemporal embeddings for complex video analysis.
InCVPR.
[Ren et al2015] Mengye Ren, Ryan Kiros, and RichardZemel.
2015.
Exploring models and data for imagequestion answering.
In NIPS.
1[Richardson et al2013] Matthew Richardson, Christo-pher JC Burges, and Erin Renshaw.
2013.
Mctest: Achallenge dataset for the open-domain machine com-prehension of text.
In EMNLP.
1930[Schank and Abelson2013] Roger C Schank and Robert PAbelson.
2013.
Scripts, plans, goals, and under-standing: An inquiry into human knowledge struc-tures.
Psychology Press.
2[Sigurdsson et al2016] Gunnar A Sigurdsson, XinleiChen, and Abhinav Gupta.
2016.
Learning visualstorylines with skipping recurrent neural networks.
InECCV.
2[Simonyan and Zisserman2014] Karen Simonyan andAndrew Zisserman.
2014.
Very deep convolutionalnetworks for large-scale image recognition.
arXivpreprint arXiv:1409.1556.
3[Spearman1904] Charles Spearman.
1904.
The proof andmeasurement of association between two things.
TheAmerican journal of psychology.
4[Ting-Hao Huang2016] Nasrin Mostafazadeh IshanMisra Aishwarya Agrawal Jacob Devlin Ross Gir-shick Xiaodong He Pushmeet Kohli Dhruv Batra C.Lawrence Zitnick Devi Parikh Lucy VanderwendeMichel Galley Margaret Mitchell Ting-Hao Huang,Francis Ferraro.
2016.
Visual storytelling.
In NAACL.1, 4[Trottier1998] David Trottier.
1998.
The screenwriter?sbible: A complete guide to writing, formatting, andselling your script.
Silman-James Press.
5[Vendrov et al2016] Ivan Vendrov, Ryan Kiros, Sanja Fi-dler, and Raquel Urtasun.
2016.
Order-embeddings ofimages and language.
In ICLR.
[Wang et al2016] William Yang Wang, Yashar Mehdad,Dragomir R Radev, and Amanda Stent.
2016.
A low-rank approximation approach to learning joint embed-dings of news stories and images for timeline summa-rization.
In NAACL.
2[Webber1988] Bonnie Lynn Webber.
1988.
Tense as dis-course anaphor.
Computational Linguistics.
2[Weston et al2015] Jason Weston, Antoine Bordes, SumitChopra, and Tomas Mikolov.
2015.
Towards AI-complete question answering: A set of prerequisite toytasks.
arXiv preprint arXiv:1502.05698.
1[Zhu et al2015] Yukun Zhu, Ryan Kiros, Rich Zemel,Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-ralba, and Sanja Fidler.
2015.
Aligning books andmovies: Towards story-like visual explanations bywatching movies and reading books.
In CVPR.
3931
