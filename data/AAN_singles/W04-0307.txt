A Statistical Constraint Dependency Grammar (CDG) ParserWen WangSpeech Technology and Research LabSRI InternationalMenlo Park, CA 94025,U.S.A.,wwang@speech.sri.comMary P. HarperElectrical and Computer EngineeringPurdue UniversityWest Lafayette, IN 47907-1285,U.S.A.,harper@ecn.purdue.eduAbstractCDG represents a sentence?s grammatical structureas assignments of dependency relations to func-tional variables associated with each word in thesentence.
In this paper, we describe a statisticalCDG (SCDG) parser that performs parsing incre-mentally and evaluate it on the Wall Street Jour-nal Penn Treebank.
Using a tight integration ofmultiple knowledge sources, together with distancemodeling and synergistic dependencies, this parserachieves a parsing accuracy comparable to severalstate-of-the-art context-free grammar (CFG) basedstatistical parsers using a dependency-based eval-uation metric.
Factors contributing to the SCDGparser?s performance are analyzed.1 IntroductionStatistical parsing has been an important focus ofrecent research (Magerman, 1995; Eisner, 1996;Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999;Charniak, 2000).
Several of these parsers gen-erate constituents by conditioning probabilities onnon-terminal labels, part-of-speech (POS) tags, andsome headword information (Collins, 1999; Rat-naparkhi, 1999; Charniak, 2000).
They utilizenon-terminals that go beyond the level of a sin-gle word and do not explicitly use lexical fea-tures.
Collins?
Model 2 parser (1999) learns thedistinction between complements and adjuncts byusing heuristics during training, distinguishes com-plement and adjunct non-terminals, and includesa probabilistic choice of left and right subcate-gorization frames, while his Model 3 parser usesgap features to model wh-movement.
Charniak(Charniak, 2000) developed a state-of-the-art sta-tistical CFG parser and then built an effective lan-guage model based on it (Charniak, 2001).
Buthis parser and language model were originally de-signed to analyze complete sentences.
Among thestatistical dependency grammar parsers, Eisner?s(1996) best probabilistic dependency model usedunlabeled links between words and their heads, aswell as between words and their complements andadjuncts.
However, the parser does not distinguishbetween complements and adjuncts or model wh-movement.
Collins?
bilexical dependency grammarparser (1999) used head-modifier relations betweenpairs of words much as in a dependency grammar,but they are limited to relationships between wordsin reduced sentences with base NPs.Our research interest focuses on building a highquality statistical parser for language modeling.
Wechose CDG as the underlying grammar for severalreasons.
Since CDGs can be lexicalized at the word-level, a CDG parser-based language model is animportant alternative to CFG parser-based models,which must model both non-terminals and termi-nals.
Furthermore, the lexicalization of CDG parserules is able to include not only lexical category in-formation, but also a rich set of lexical features tomodel subcategorization and wh-movement.
By us-ing CDG, our statistical model is able to distinguishbetween adjuncts and complements.
Additionally,CDG is more powerful than CFG and is able tomodel languages with crossing dependencies andfree word ordering.In this paper, we describe and evaluate a statisti-cal CDG parser for which the probabilities of parseprefix hypotheses are incrementally updated whenthe next input word is available, i.e., it parses in-crementally.
Section 2 describes how CDG repre-sents a sentence?s parse and then defines a Super-ARV, which is a lexicalization of CDG parse rulesused in our parsing model.
Section 3 presents theparsing model, while Section 4 motivates the eval-uation metric used to evaluate our parser.
Section 5presents and discusses the experimental results.2 CDG ParsingCDG (Harper and Helzerman, 1995) represents syn-tactic structures using labeled dependencies be-tween words.
Consider an example CDG parse forthe sentence What did you learn depicted in thewhite box of Figure 1.
Each word in the parse has alexical category, a set of feature values, and a set ofroles that are assigned role values, each comprisedof a label indicating the grammatical role of theword and its modifiee (i.e., the position of the wordit is modifying when it takes on that role).
Considerthe role value assigned to the governor role (denotedG) of you, np-2.
The label np indicates the gram-matical function of you when it is governed by itshead in position 2.
Every word in a sentence musthave a governor role with an assigned role value.Need roles are used to ensure that the grammaticalrequirements of a word are met (e.g., subcategoriza-tion).pronouncase=commonbehavior=nominaltype=interrogativeagr=3sG=np-4verbsubcat=baseverbtype=pastvoice=activeinverted=yestype=nonegapp=yesmood=whquestionagr=allG=vp-1Need1=S-3Need2=S-4Need3=S-2pronouncase=commonbehavior=nominaltype=personalagr=2sG=np-21what2did3youThe SuperARV of the word "did":Category: Verb4learnverbsubcat=objvtype=infinitivevoice=activeinverted=notype=nonegapp=yesmood=whquestionagr=noneG=vp-2Need1=S-4Need2=S-1Need3=S-4Features: {verbtype=past, voice=active, inverted=yes,gapp=yes,mood=whquestion,agr=all}Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)Dependent Positional Constraints:MX[G] < PX = MX[Need3] < MX[Need1]< MX[Need2] MC}needroleconstraints}}}CF }}(R,L,UC,MC)+DCFigure 1: An example of a CDG parse and the Super-ARV of the word did in the sentence what did you learn.PX and MX([R]) represent the position of a word and itsmodifiee (for role R), respectively.Note that CDG parse information can be easilylexicalized at the word level.
This lexicalization isable to include not only lexical category and syn-tactic constraints, but also a rich set of lexical fea-tures to model subcategorization and wh-movementwithout a combinatorial explosion of the parametricspace (Wang and Harper, 2002).
CDG can distin-guish between adjuncts and complements due to theuse of need roles (Harper and Helzerman, 1995),is more powerful than CFG, and has the ability tomodel languages with crossing dependencies andfree word ordering (hence, this research could beapplicable to a wide variety of languages).An almost-parsing LM based on CDG has beendeveloped in (Wang and Harper, 2002).
The un-derlying hidden event of this LM is a SuperARV.A SuperARV is formally defined as a four-tuple fora word, ?C, F , (R, L, UC, MC)+, DC?, where Cis the lexical category of the word, F = {Fname1= Fvalue1, .
.
.
, FNamef = FV aluef} is a fea-ture vector (where Fnamei is the name of a featureand Fvaluei is its corresponding value), DC repre-sents the relative ordering of the positions of a wordand all of its modifiees, (R, L, UC, MC)+ is a listof one or more four-tuples, each representing an ab-straction of a role value assignment, where R is arole variable, L is a functionality label, UC repre-sents the relative position relation of a word and itsdependent, and MC encodes some modifiee con-straints, namely, the lexical category of the modifieefor this dependency relation.
The gray box of Figure1 presents an example of a SuperARV for the worddid.
From this example, it is easy to see that a Su-perARV is a join on the role value assignments of aword, with explicit position information replaced bya relation that expresses whether the modifiee pointsto the current word, a previous word, or a subse-quent word.
The SuperARV structure provides anexplicit way to organize information concerning oneconsistent set of dependency links for a word thatcan be directly derived from a CDG parse.
Super-ARVs encode lexical information as well as syntac-tic and semantic constraints in a uniform represen-tation that is much more fine-grained than part-of-speech (POS).
A sentence tagged with SuperARVsis an almost-parse since all that remains is to spec-ify the precise position of each modifiee.
SuperARVLMs have been effective at reducing word error rate(WER) on wide variety of continuous speech recog-nition (CSR) tasks, including Wall Street Journal(Wang and Harper, 2002), Broadcast News (Wanget al, 2003), and Switchboard tasks (Wang et al,2004).3 SCDG Parser3.1 The Basic Parsing AlgorithmOur SCDG parser is a probabilistic generativemodel.
It can be viewed as consisting of two com-ponents: SuperARV tagging and modifiee determi-nation.
These two steps can be either loosely ortightly integrated.
To simplify discussion, we de-scribe the loosely integrated version, but we imple-ment and evaluate both strategies.
The basic parsingalgorithm for the loosely integrated case is summa-rized in Figure 2, with the algorithm?s symbols de-fined in Table 1.
In the first step, the top N-bestSuperARV assignments are generated for an inputsentence w1, .
.
.
, wn using token-passing (Younget al, 1997) on a Hidden Markov Model with tri-gram probabilistic estimations for both transitionand emission probabilities.
Each SuperARV se-quence for the sentence is represented as a sequenceof tuples: ?w1, s1?, .
.
.
, ?wn, sn?, where ?wk, sk?represents the word wk and its SuperARV assign-ment sk.
These assignments are stored in a stackranked in non-increasing order by tag assignmentprobability.During the second step, the modifiees are statis-tically specified in a left-to-right manner.
Note thatthe algorithm utilizes modifiee lexical category con-straints to filter out candidates with mismatched lex-ical categories.
When processing the word wk, k =1, .
.
.
, n, the algorithm attempts to determine theleft dependents of wk from the closest to the far-thest.
The dependency assignment probability whenchoosing the (c + 1)th left dependent (with its posi-tion denoted dep(k,?
(c + 1))) is defined as:Pr(link(sdep(k,?
(c+1)), sk,?
(c + 1)|syn,H))where H = ?w, s?k, ?w, s?dep(k,?
(c+1)), ?w, s?dep(k,?c)dep(k,?1).The dependency assignment probability is con-ditioned on the word identity and SuperARVassignment of wk and wdep(k,?
(c+1)) as well asall of the c previously chosen left dependents?w, s?dep(k,?c)dep(k,?1) for wk.
A Boolean random variablesyn is used to model the synergistic relationshipbetween certain role pairs.
This mechanism allowsus to elevate, for example, the probability that thesubject of a sentence wi is governed by a tensedverb wj when the need role value of wj points towi as its subject.
The syn value for a dependencyrelation is determined heuristically based on thelexical category, role name, and label informationof the two dependent words.
After the algorithmstatistically specifies the left dependents for wk,it must also determine whether wk could be the(d+1)th right dependent of a previously seen wordwp, p = 1, .
.
.
, k ?
1 (where d denotes the numberof already assigned right dependents of wp), asshown in Figure 2.After processing word wk in each partial parse onthe stack, the partial parses are re-ranked accordingto their updated probabilities.
This procedure is it-erated until the top parse in the stack covers the en-tire sentence.
For the tightly coupled parser, the Su-perARV assignment to a word and specification ofits modifiees are integrated into a single step.
Theparsing procedure, which is completely incremen-tal, is implemented as a simple best-first stack-basedsearch.
To control time and memory complexity, weused two pruning thresholds: maximum stack depthand maximum difference between the log proba-bilities of the top and bottom partial parses in thestack.
These pruning thresholds are tuned based onthe tradeoff of time/memory complexity and pars-ing accuracy on a heldout set, and they both havehard limits.Note the maximum likelihood estimation of de-pendency assignment probabilities in the basicloosely coupled parsing algorithm presented in Fig-ure 2 is likely to suffer from data sparsity, and theestimates for the tightly coupled algorithm are likelyto suffer even more so.
Hence, we smooth the prob-abilities using Jelinek-Mercer smoothing (Jelinek,1997), as described in (Wang and Harper, 2003;Wang, 2003).3.2 Additions to the Basic ModelSome additional features are added to the basicmodel because of their potential to improve SCDGparsing accuracy.
Their efficacy is evaluated in Sec-tion 5.Modeling crossing dependencies: The basic pars-ing algorithm was implemented to preclude cross-ing dependencies; however, it is important to allowthem in order to model wh-movement in some cases(e.g., wh-PPs).Distance and barriers between dependents: Be-cause distance between two dependent words isan important factor in determining the modifieesof a word, we evaluate an alternative model thatadds distance, ?dep(k,?
(c+1)),k to H in Figure 2.Note that ?dep(k,?
(c+1)),k represents the distancebetween position dep(k,?
(c + 1)) and k. To avoiddata sparsity problems, distance is bucketed and adiscrete random variable is used to model it.
Wealso model punctuation and verbs based on priorwork.
Like (Collins, 1999), we also found thatverbs appear to act as barriers that impact modifieelinks.
Hence, a Boolean random variable that rep-resents whether there is a verb between the depen-dencies is added to condition the probability esti-mations.
Punctuation is treated similarly to coordi-nation constructions with punctuation governed bythe headword of the following phrase, and heuris-tic questions on punctuation were used to provideadditional constraints on dependency assignments(Wang, 2003).Modifiee lexical features: The SuperARV struc-ture employed in the SuperARV LM (Wang andHarper, 2002) uses only lexical categories of mod-ifiees as modifiee constraints.
In previous work(Harper et al, 2001), modifiee lexical features werecentral to increasing the selectivity of a CDG.Hence, we have developed methods to add ad-ditional relevant lexical features to modifiee con-straints of a SuperARV structure (Wang, 2003).4 Parsing Evaluation MetricTo evaluate our parser, which generates CDG anal-yses rather than CFG constituent bracketing, weTable 1: Definitions of symbols used in the basic parsing algorithm.Term DenotesL(sk), R(sk) all dependents of sk to the left and right of wk, respectivelyN(L(sk)), N(R(sk)) the number of left and right dependents of sk, respectivelydep(k,?c), dep(k, c) cth left dependent and right dependent of sk, respectivelydep(k,?1), dep(k, 1) the position of the closest left dependent and right dependent of sk, respectivelydep(k,?N(L(sk))), dep(k, N(L(sk))) the position of the farthest left dependent and right dependent of sk, respectivelyCat(sk) the lexical category of skModCat(sk,?c), ModCat(sk, c) the lexical category of sk?s cth left and right dependent (encoded in the SuperARVstructure), respectivelylink(si, sj , k) the dependency relation between SuperARV si and sj with wi assigned as the kthdependent of sj , e.g., link(sdep(k,?
(c+1)), sk,?
(c + 1)) indicates thatwdep(k,?
(c+1)) is the (c + 1)th left dependent of sk.D(L(sk)), D(R(sk))) the number of left and right dependents of sk already assigned, respectively?w, s?dep(k,?c)dep(k,?1) words and SuperARVs of sk?s closest left dependent up to its cth left dependent?w, s?dep(k,c)dep(k,1) words and SuperARVs of sk?s closest right dependent up to its cth right dependentsyn a random variable denoting the synergistic relation between some dependentscan either convert the CDG parses to CFG brack-eting and then use PARSEVAL, or convert the CFGbracketing generated from the gold standard CFGparses to CDG parses and then use a metric based ondependency links.
Since our parser is trained usinga CFG-to-CDG transformer (Wang, 2003), whichmaps a CFG parse tree to a unique CDG parse,it is sensible to evaluate our parser?s accuracy us-ing gold standard CDG parse relations.
Further-more, in the 1998 Johns Hopkins Summer work-shop final report (Hajic et al, 1998), Collins et alpointed out that in general the mapping from de-pendencies to tree structures is one-to-many: thereare many possible trees that can be generated fora given dependency structure since, although gen-erally trees in the Penn Treebank corpus are quiteflat, they are not consistently ?flat.?
This variabilityadds a non-deterministic aspect to the mapping fromCDG dependencies to CFG parse trees that couldcause spurious PARSEVAL scoring errors.
Addi-tionally, when there are crossing dependencies, thenno tree can be generated for that set of dependen-cies.
Consequently, we have opted to use a trans-former to convert CFG trees to CDG parses and de-fine a new dependency-based metric adapted from(Eisner, 1996).
We define role value labeled pre-cision (RLP) and role value labeled recall (RLR)on dependency links as follows:RLP = correct modifiee assignmentsnumber of modifiees our parser foundRLR = correct modifiee assignmentsnumber of modifiess in the gold test set parseswhere a correct modifiee assignment for a wordwi in a sentence means that a three-tuple?role id, role label, modifiee word position?
(i.e.,a role value) for wi is the same as the three-tuplerole value for the corresponding role id of wi in thegold test parse.
This differs from Eisner?s (1996)precision and recall metrics which use no label in-formation and score only parent (governor) assign-ments, as in traditional dependency grammars.
Wewill evaluate role value labeled precision and recallon all roles of the parse, as well as the governor-only portion of a parse.
Eisner (Eisner, 1996) andLin (Lin, 1995) argued that dependency link eval-uation metrics are valuable for comparing parserssince they are less sensitive than PARSEVAL to sin-gle misattachment errors that may cause significanterror propagation to other constituents.
This, to-gether with the fact that we must train our parserusing CDG parses generated in a lossy manner froma CFG treebank, we chose to use RLP and RLR tocompare our parsing accuracy with several state-of-the-art parsers.5 Evaluation and DiscussionAll of the evaluations were performed on the WallStreet Journal Penn Treebank task.
Following thetraditional data setup, sections 02-21 are used fortraining our parser, section 23 is used for testing,and section 24 is used as the development set for pa-rameter tuning and debugging.
As in (Ratnaparkhi,1999; Charniak, 2000; Collins, 1999), we evaluateon all sentences with length ?
40 words (2,245 sen-tences) and length ?
100 words (2,416 sentences).For training our probabilistic CDG parser on thistask, the CFG bracketing of the training set is trans-BASIC PARSING ALGORITHM1.
Using SuperARV tagging on word sequence w1, .
.
.
, wn, obtain a set of N-best SuperARV sequences with eachelement consisting of n (word, SuperARV) tuples, denoted ?w1, s1?, .
.
.
, ?wn, sn?, which we will call an assignment.2.
For each SuperARV assignment, initialize the stack of parse prefixes with this assignment:/?
From left-to-right, process each ?word, tag?
of the assignment and generate parse prefixes ?/for k : = 1, n do/?
Step a: ?//* decide left dependents of ?wk, sk?
from the nearest to the farthest */for c from 0 to N(L(sk)) ?
1 do/?
Choose a position for the (c + 1)th left dependent of ?wk, sk?
from the set of possible positionsC = {1, .
.
.
, dep(k,?c) ?
1}.
The position choice is denoted dep(k,?
(c + 1)) ?
//?
In the following equations, different left dependent assignments will generatedifferent parse prefixes, each of which is stored in the stack ?
/for each dep(k,?
(c + 1)) from positions C = {1, .
.
.
, dep(k,?c) ?
1}/?
Check whether the lexical category of the choice matches the modifiee lexicalcategory of the (c + 1)th left dependent of ?wk, sk?
?
/if Cat(sdep(k,?
(c+1))) == ModCat(sk,?
(c + 1)) thenPr(T ) : = Pr(T ) ?
Pr(link(sdep(k,?
(c+1)), sk,?
(c + 1)|syn,H))where H = ?w, s?k, ?w, s?dep(k,?
(c+1)), ?w, s?dep(k,?c)dep(k,?1)/?
End of choosing left dependents of ?wk, sk?
for this parse prefix ?//?
Step b: ?//?
For the word/tag pair ?wk, sk?, check whether it could be a right dependent of any previouslyseen word within a parse prefix of ?w1, s1?, .
.
.
, ?wk?1, sk?1?
?/for p : = 1, k ?
1 do/?
If ?wp, sp?
still has right dependents left unspecified, then try out?wk, sk?
as a right dependent */if D(R(sp)) 6= N(R(sp)) thend : = D(R(sp))/?
If the lexical category of ?wk, sk?
matches the modifiee lexical category of the(d + 1)th rightdependent of ?wp, sp?, then sk might be ?wp, sp?
?s (d + 1)th right dependent ?
/if Cat(sk) == ModCat(sp, d + 1) thenPr(T ) : = Pr(T ) ?
Pr(link(sk, sp, d + 1)|syn,H), where H = ?w, s?p, ?w, s?k, ?w, s?dep(p,d)dep(p,1)Sort the parse prefixes in the stack according to logPr(T ) and apply pruning using the thresholds.3.
After processing w1, .
.
.
, wn, pick the parse with the highest logPr(T ) in the stack as the parse for that sentence.Figure 2: The basic loosely coupled parsing algorithm.
Note the algorithm updates the probabilities of parseprefix hypotheses incrementally when processing each input word.formed into CDG annotations using a CFG-to-CDGtransformer (Wang, 2003).
Note that the sound-ness of the CFG-to-CDG transformer was evaluatedby examining the CDG parses generated from thetransformer on the Penn Treebank development setto ensure that they were correct given our grammardefinition.5.1 Contribution of Model FactorsFirst, we investigate the contribution of the modeladditions described in Section 3 to parse accuracy.Since these factors are independent of the couplingbetween the SuperARV tagger and modifiee spec-ification, we investigate their impact on a looselyintegrated SCDG parser by comparing four models:(1) the basic loosely integrated model; (2) the ba-sic model with crossing dependencies; (3) model 2with distance and barrier information; (4) model 3with SuperARVs augmented with additional modi-fiee lexical feature constraints.
Each model uses atrigram SuperARV tagger to generate 40-best Su-perARV sequences prior to modifiee specification.Table 2 shows the results for each of the four modelsincluding SuperARV tagging accuracy (%) and rolevalue labeled precision and recall (%).
Allowingcrossing dependencies improves the overall parsingaccuracy, but using distance information with verbbarrier and punctuation heuristics produces an evengreater improvement especially on the longer sen-tences.
The accuracy is further improved by the ad-ditional modifiee lexical feature constraints added tothe SuperARVs.
Note that RLR is lower than RLPin these investigations possibly due to SuperARVtagging errors and the use of a tight stack pruningthreshold.Next, we evaluate the impact of increasing thecontext of the SuperARV tagger to a 4-gram whileincreasing the size of the N-best list passed fromthe tagger to the modifiee specification step of theparser.
For this evaluation, we use model (4)Table 2: Results on Section 23 of the WSJ Penn Tree-bank for four loosely-coupled model variations.
Theevaluation metrics, RLR and RLP, are our dependency-based role value labeled precision and recall.
Note:Model (1) denotes the basic model, Model (2) de-notes (1)+crossing dependencies, Model (3) denotes(2)+distance (punctuation) model, and Model (4) denotes(3)+modifiee lexical features.Models ?
40 words (2,245 sentences)Tagging governor only all rolesAcc.
RLP RLR RLP RLR(1) 94.7 90.6 90.3 86.8 86.2(2) 95.0 90.7 90.5 87.0 86.5(3) 95.7 91.1 90.9 87.4 87.0(4) 96.2 91.5 91.2 88.0 87.4Models ?
100 words (2,416 sentences)Tagging governor only all rolesAcc.
RLP RLR RLP RLR(1) 94.0 89.7 89.3 86.0 85.5(2) 94.2 89.9 89.6 86.2 85.8(3) 94.7 90.4 90.2 86.8 86.3(4) 95.4 90.9 90.5 87.5 86.8from Table 2, the most accurate model so far.
Wealso evaluate whether a tight integration of left-to-right SuperARV tagging and modifiee specifica-tion produces a greater parsing accuracy than thebest loosely coupled counterpart.
Table 3 showsthe SuperARV tagging accuracy (%) and role valuelabeled precision and recall (%) for each model.Consistent with our intuition, a stronger SuperARVtagger and a larger search space of SuperARV se-quences produces greater parse accuracy.
However,tightly integrating SuperARV prediction with mod-ifiee specification achieves the greatest overall ac-curacy.
Note that SuperARV tagging accuracy andparse accuracy improve in tandem, as can be seenin Tables 2 and 3.
These results are consistentwith the observations of (Collins, 1999) and (Eis-ner, 1996).
It is important to note that each of thefactors contributing to improved parse accuracy inthese two experiments also improved the word pre-diction capability of the corresponding parser-basedLM (Wang and Harper, 2003).5.2 Comparing to Other ParsersCharniak?s state-of-the-art PCFG parser (Charniak,2000) has achieved the highest PARSEVAL LP/LRwhen compared to Collins?
Model 2 and Model3 (Collins, 1999), Roark?s (Roark, 2001), Ratna-parkhi?s (Ratnaparkhi, 1999), and Xu & Chelba?s(Xu et al, 2002) parsers.
Hence, we will com-pare our best loosely integrated and tightly inte-grated SCDG parsers to Charniak?s parser.
Ad-ditionally, we will compare with Collins?
ModelTable 3: Results on Section 23 of the WSJ Penn Tree-bank comparing models that utilize different SuperARVtaggers and N-best sizes with the tightly coupled imple-mentation.
Note L denotes Loose coupling and T de-notes Tight coupling.
Also (a) denotes trigram, 40-best;(b) denotes trigram, 100-best; (c) denotes 4-gram, 40-best; (d) denotes 4-gram, 100-best.Models ?
40 words (2,245 sentences)Tagging governor only all rolesAcc.
RLP RLR RLP RLRL (a) 96.2 91.5 91.2 88.0 87.4(b) 96.7 91.9 91.5 88.3 87.7(c) 96.9 92.2 91.7 88.6 88.1(d) 97.2 92.4 92.3 89.1 88.6T 97.4 93.2 92.9 89.8 89.2Models ?
100 words (2,416 sentences)Tagging governor only all rolesAcc.
RLP RLR RLP RLRL (a) 95.4 90.9 90.5 87.5 86.8(b) 95.8 91.3 90.8 87.7 87.0(c) 96.0 91.7 91.2 88.0 87.4(d) 96.3 91.8 91.5 88.5 87.8T 96.6 92.6 92.2 89.1 88.52 since it makes the complement/adjunct distinc-tion and Model 3 since it handles wh-movement(Collins, 1999).
Charniak?s parser does not explic-itly model these phenomena.Among the statistical CFG parsers to be com-pared, only Collins?
Model 3 produces trees withinformation about wh-movement.
Since the trans-former uses empty node information to transformthe CFG parse trees to CDG parses, the accuracyof Charniak?s parser and Collins?
Model 2 may beslightly reduced for sentences with empty nodes.Hence, we compare results on two test sets: one thatomits all sentences with traces and one that does not.As can be seen in Table 4, our tightly coupled parserconsistently produces an accuracy that equals or ex-ceeds the accuracies of the other parsers, with oneexception (Collins?
Model 3), regardless of whetherthe test set contains sentences with traces.Using our evaluation metrics, Collins?
Model 3achieves a better precision/recall than Model 2 andCharniak?s parser.
Since trace information is usedby the CFG-to-CDG transformer to generate cer-tain lexical features (Wang, 2003), the output fromModel 3 is likely to be mapped to more accu-rate CDG parses.
Although Charniak?s maximum-entropy inspired parser achieved the highest PAR-SEVAL results, Collins?
Model 3 is more accu-rate using our dependency metric, possibly be-cause it makes the complement/adjunct distinctionand models wh-movement.
Since the statisticalTable 4: Evaluation of five models on Section 23 sentences with and without traces: L denotes the best looselycoupled CDG parser and T the tightly coupled CDG parser.Models ?
40 words (2,245 sentences)Without TRACE All(1,903 sentences) (2,245 sentences)governor only all roles governor only all rolesRLP RLR RLP RLR RLP RLR RLP RLRL 92.4 92.4 89.5 88.7 92.4 92.3 89.1 88.6T 93.2 92.9 89.9 89.3 93.2 92.9 89.8 89.2Charniak (Charniak, 2000) 92.6 92.5 89.4 88.9 92.5 92.3 88.9 88.7Collins, Model 2 (Collins, 1999) 92.5 92.3 89.1 88.5 92.2 92.1 89.0 88.5Collins, Model 3 (Collins, 1999) 92.8 92.7 89.9 89.4 92.7 92.4 89.3 89.1Models ?
100 words (2,416 sentences)Without TRACE All(1,979 sentences) (2,416 sentences)governor only all roles governor only all rolesRLP RLR RLP RLR RLP RLR RLP RLRL 91.9 91.6 88.8 88.1 91.8 91.5 88.5 87.8T 92.7 92.3 89.4 88.7 92.6 92.2 89.1 88.5Charniak (Charniak, 2000) 92.0 91.8 88.8 88.2 91.9 91.6 88.4 87.9Collins, Model 2 (Collins, 1999) 91.8 91.6 88.6 88.0 91.7 91.5 88.2 87.9Collins, Model 3 (Collins, 1999) 92.2 92.1 89.4 88.8 92.1 91.9 88.8 88.5CFG parsers may loose accuracy from the CFG-to-CDG transformation, similarly to Collins?
experi-ment reported in (Hajic et al, 1998), we also trans-formed our CDG parses to Penn Treebank styleCFG parse trees and scored them using PARSE-VAL.
On the WSJ PTB test set, Charniak?s parserachieved 89.6% LR and 89.5% LP, Collins?
Model 2and 3 obtained 88.1% LR and 88.3% LP and 88.0%LR and 88.3% LP, while the tightly coupled CDGparser obtains 85.8% LR and 86.4% LP.
It is im-portant to remember that this score is impacted bytwo lossy conversions, one for training and one fortesting.We have conducted a non-parametric MonteCarlo test to determine the significance of the differ-ences between the parsing accuracy results in Table3 and Table 4.
We found that the difference betweenthe tightly and loosely coupled SCDG parsers is sta-tistically significant, as well as the difference be-tween the SCDG parser and Charniak?s parser andCollins?
Model 2.
Although the difference betweenour parser and Collins?
Model 3 is not statisticallysignificant, our parser represents a first attempt tobuild a high quality SCDG parser, and there is stillroom for improvement, e.g., better handling of bar-riers (including punctuation) and employing moresophisticated search and pruning strategies.This paper has presented a statistical implemen-tation of a CDG parser, which is both genera-tive and highly lexicalized.
With a frameworkof tightly integrated, multiple knowledge sources,model distance, and synergistic dependencies, wehave achieved a parsing accuracy comparable to thestate-of-the-art statistical parsers trained on the WallStreet Journal Penn Treebank corpus.
However,more work must be done to build a parser modelcapable of coping with speech disfluencies presentin spontaneous speech.
We also intend to investi-gate a hybrid parser that combines the generality ofa CFG with the specificity of a CDG.ReferencesE.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings ofthe Fourteenth National Conference on Artificial In-telligence.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of the First Annual Meetingof the North American Association for ComputationalLinguistics.E.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In Proceedings of ACL?2001.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.J.
M. Eisner.
1996.
An empirical comparison of prob-ability models for dependency grammar.
Technicalreport, University of Pennsylvania, CIS Department,Philadelphia PA 19104-6389.J.
Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,C.
Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, andD.
Zeman.
1998.
Core natural language processingtechnology applicable to multiple languages ?
Work-shop ?98.
Technical report, Johns Hopkins Univ.M.
P. Harper and R. A. Helzerman.
1995.
Extensionsto constraint dependency parsing for spoken languageprocessing.
Computer Speech and Language.M.
P. Harper, W. Wang, and C. M. White.
2001.
Ap-proaches for learning constraint dependency grammarfrom corpora.
In Proceedings of the Grammar andNatural Language Processing Conference, Montreal,Canada.F.
Jelinek.
1997.
Statistical Methods For Speech Recog-nition.
The MIT Press.D.
Lin.
1995.
A dependency-based method for evaluat-ing broad-coverage parsers.
In Proceedings of the In-ternational Joint Conference on Artificial Intelligence,pages 1420?1427.D.
M. Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In Proceedings of the 33rd Annual Meet-ing of the Association for Computational Linguistics,pages 276?283.A.
Ratnaparkhi.
1999.
Learning to parse natural lan-guage with maximum entropy models.
MachineLearning, 34:151?175.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.W.
Wang and M. P. Harper.
2002.
The SuperARV lan-guage model: Investigating the effectiveness of tightlyintegrating multiple knowledge sources.
In Proceed-ings of Conference of Empirical Methods in NaturalLanguage Processing.W.
Wang and M. P. Harper.
2003.
Language model-ing using a statistical dependency grammar parser.
InProceedings of International Workshop on AutomaticSpeech Recognition and Understanding.W.
Wang, M. P. Harper, and A. Stolcke.
2003.
The ro-bustness of an almost-parsing language model givenerrorful training data.
In ICASSP 2003.W.
Wang, A. Stolcke, and M. P. Harper.
2004.
The useof a linguistically motivated language model in con-versational speech recognition.
In ICASSP 2004.W.
Wang.
2003.
Statistical Parsing and Language Mod-eling based on Constraint Dependency Grammar.Ph.D.
thesis, Purdue University.P.
Xu, C. Chelba, and F. Jelinek.
2002.
A study on richersyntactic dependencies for structured language mod-eling.
In Proceedings of ACL 2002.S.
J.
Young, J. Odell, D. Ollason, V. Valtchev, and P. C.Woodland, 1997.
The HTK Book.
Entropic Cam-bridge Research Laboratory, Ltd.
