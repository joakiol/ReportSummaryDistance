Proceedings of the Workshop on BioNLP: Shared Task, pages 77?85,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsBiomedical Event Annotation with CRFs and Precision GrammarsAndrew MacKinlay, David Martinez and Timothy BaldwinNICTA Victoria Research LaboratoriesUniversity of Melbourne, VIC 3010, Australia{amack,davidm,tim}@csse.unimelb.edu.auAbstractThis work describes a system for the tasksof identifying events in biomedical text andmarking those that are speculative or negated.The architecture of the system relies onboth Machine Learning (ML) approaches andhand-coded precision grammars.
We submit-ted the output of our approach to the event ex-traction shared task at BioNLP 2009, whereour methods suffered from low recall, al-though we were one of the few teams to pro-vide answers for task 3.1 IntroductionWe present in this paper our techniques for the tasks1 and 3 of the event extraction shared task at BioNLP2009.
We make use of both Machine Learning (ML)approaches and hand-coded precision grammars inan architecture that combines multiple dedicatedmodules.
In the third task on negation/speculation,we extract extract rich linguistic features resultingfrom our HPSG high-precision grammar to train anML classifier.2 Methodology2.1 Task 1: Shallow Features and CRFsOur system consists of two main modules, the firstof which is devoted to the detection of event triggerwords, and the second to event?theme analysis.2.1.1 Trigger-word detectionWe developed two separate systems to performtrigger word detection, and also a hybrid systemwhich combines their outputs.
The first system isa simple dictionary-based look-up tagger; the sec-ond system learns a structured model from the train-ing data using conditional random fields (CRFs).For pre-processing, we relied on the domain-specifictoken and sentence splitter from the JULIE Lab(Tomanek et al, 2007) and the GENIA tagger forlemmatisation, POS tagging, chunking, and proteindetection (Tsuruoka et al, 2005).The look-up tagger operates by counting the oc-currences in the training data of different event tagsfor a given term.
Over the development and test data,each occurrence of a given term is assigned the eventclass with the highest prior in the training data.
Weexperimented with a frequency cut-off that allows usto explore the precision/recall trade-off.Our second system relies on CRFs, as imple-mented in the CRF++ toolkit (Lafferty et al, 2001).CRFs provide a discriminative framework for build-ing structured models to segment and label sequencedata.
CRFs have the well-known advantage that theyboth model sequential effects and support the use oflarge numbers of features.
In our experiments weused the following feature types: word-forms, lem-mas, POS, chunk tags, protein annotation, and gram-matical dependencies.
For dependency annotation,we used the Bikel parser and GDep as provided bythe organisers.
This information was provided as afeature that expresses the grammatical function ofthe token.
We explored window sizes of?3 and?4.Finally, we tested combining the outputs of thelook-up tagger and CRF, by selecting all triggerwords from both outputs.772.1.2 Event-theme constructionWe constructed the output for task 1 by differenti-ating among three types of events, according to theirexpected themes: basic events, binding events, andregulation events.
We applied a simple strategy, as-signing the closest events or proteins within a givensentence as themes.For the basic events, we simply assigned the clos-est protein, an approach that we found to performwell over the training and development data.
Forbinding events, we estimated the maximum dis-tance away from the event word(s) for themes, andthe maximum number of themes.
For regulationevents, we had to choose between proteins or eventsas themes, and the CAUSE field was also required.Again, we relied on a maximum distance threshold,and gave priority to events over proteins as themes.We removed regulation events as theme candidates,since our basic approach could not indicate the di-rection of the regulation.
We also tested predictingthe CAUSE by relying on the protein closest to theregulation event.2.2 Task 3: Deep Parsing and MaximumEntropy classificationFor task 3 we ran a syntactic parser over the abstractsand used the outputs to construct feature vectors fora machine learning algorithm.
We built two classi-fiers (possibly with overlapping sets of feature vec-tors) for each training run: one to identify specula-tion and one for negation.
We deliberately built aseparate binary classifier for each task instead of asingle four-class classifier, since the problem natu-rally decomposes this way.
Speculation and nega-tion are independent of one another (informally, notstatistically) and it enables us to focus on feature en-gineering for each subtask.2.2.1 Deep Parsing with the ERGIt seemed likely that syntactico-semantic analysiswould be useful for task 3.
To identify negation orspeculation with relatively high precision, it is prob-able that knowledge of the relationships of possiblydistant elements (such as the negation particle not)to a particular target word would provide valuableinformation for classification.Further to this, it was our intention to evaluatethe utility of deep parsing in such an approach,rather than a shallower annotation such as the out-put of a dependency parser.
With this in mind,we selected the English Resource Grammar1 (ERG:Copestake and Flickinger (2000)), an open-source,broad-coverage high-precision grammar of Englishin the HPSG framework.While the ERG is relatively robust across dif-ferent domains, it is a general-purpose resource,and there are some aspects of the language used inthe biomedical abstracts that cause difficulties; un-known word handling is especially important giventhe nature of terms in the domain.
Fortunately wecan make some optimisations to mitigate this.
TheGENIA tagger mentioned in Section 2.1.1 providesboth POS and named entity annotations, which weused to constrain the input to the ERG in two ways:?
Biological named entities identified by the GE-NIA tagger are flagged as such, and the parserdoes not attempt to decompose them.?
POS tags are appended to each input token toconstrain the token to an appropriate categoryif it is absent from the ERG lexicon.With these modifications to the parser, as well aspreprocessing to handle differences in the tokenisa-tion expected by the ERG to the output of the tagger,we were able to obtain a spanning parse for 72% ofthe training sentences.
This still leaves 28% of thesentences inaccessible ?
the need for a fallback strat-egy is discussed further in Section 4.2.2.2.2 Feature Extraction from RMRSsRather than outputting syntactic parse trees, theERG can also produce output in particular semanticformalisms: Minimal Recursion Semantics (MRS:Copestake et al (2005)) and the closely related Ro-bust Minimal Recursion Semantics (RMRS: Copes-take (2004)).
For our feature generation here wemake use of the latter.Figure 1 shows an example RMRS obtained fromone of the training documents.
While there is in-sufficient space to give a complete treatment here,we highlight several aspects for expository purposes.1Specifically the July 2008 version, downloadable fromhttp://lingo.stanford.edu/ftp/test/78l1,{ l3: thus a 1?62:67?
(e5, ARG1: h4),l16: generic unk nom rel?68:78?
(x11, CARG: ?nf- kappa b?
),l6: udef q rel?68:89?
(x9, RSTR: h8, BODY: h7),l10: compound rel?68:89?
(e12, ARG1: x9, ARG2: x11),l13: udef q rel?68:89?
(x11, RSTR: h15, BODY: h14),l101: activation n 1?79:89?
(x9),l17: neg rel?94:97?
(e19, ARG1: h18),l20: require v 1?98:106?
(e2, ARG1: u21, ARG2: x9),l102: parg d rel?98:106?
(e22, ARG1: e2, ARG2: x9),l103: for p?107:110?
(e24, ARG1: e2, ARG2: x23),l34: generic unk nom rel?111:129?
(x29,CARG: ?neuroblastoma cell?
),l25: udef q rel?111:146?
(x23, RSTR: h27, BODY: h26),l28: compound rel?111:146?
(e30, ARG1: x23, ARG2: x29),l31: udef q rel?111:146?
(x29, RSTR: h33, BODY: h32),l104: differentiation n of?130:146?
(x23, ARG1: u35) },{ h4 qeq l17, h8 qeq l10, h15 qeq l16, h18 qeq l20, h27 qeq l28,h33 qeq l34 },{ l10 in-g l101, l20 in-g l102, l20 in-g l103, l28 in-g l104 }Figure 1: RMRS representation of the sentence Thus NF-kappa B activation requires neuroblastoma cell differ-entiation showing, in order, elementary predicates, qeq-constraints, and in-g constraintsThe primary component of an RMRS is bag of ele-mentary predicates, or EPs.
Each EP shown has: (a)a label, such as ?l104?
; (b) a predicate name, such as?
differentiation n 1?
(where ?n?
indicates the part-of-speech); (c) character indices to the source sen-tence; and (d) a set of arguments.
The first argu-ment is always ARG0 and is afforded special sta-tus, generally referring to the variable introduced bythe predicate.
Subsequent arguments are labelled ac-cording to the relation of the argument to the pred-icate.
Arguments can be variables such as ?e30?
or?x23?
(where the first letter indicates the nature ofthe variable ?
?e?
referring to events and ?x?
to enti-ties), or handles such as ?h33?.These handles are generally used in the qeq con-straints, which relate a handle to a label, indicatinga particular kind of outscoping relationship betweenthe handle and the label ?
either that the handle andlabel are equal or that the handle is equal to the labelexcept that one or more quantifiers occur betweenthe two (the name is derived from ?equality mod-ule quantifiers?).
Finally there are in-g constraintswhich indicate that labels can be treated as equal.For our purposes this simply affects which qeq con-straints they participate in ?
for example from thein-g constraint ?l28 in-g l104?
and the qeq constraint?h27 qeq l28?, we can also infer that ?h27 qeq l104?.In constructing features, we make use of:?
The outscopes relationship (specifically qeq-outscopes) ?
if EP A has a handle argumentwhich qeq-outscopes the label of EP B, A issaid to immediately outscope B ; outscopes isthe transitive closure of this.?
The shared-argument relationship, where EPsC and D refer to the same variable in oneor more of their argument positions.
We alsoin some cases make further restrictions on thetypes of arguments (ARG0 , RSTR , etc) thatmay be shared on either end of the relationship.2.2.3 Feature Sets and ClassificationFeature vectors for a given event are constructedon the basis of the trigger word for the particularevent, which we assume has already been identified;a natural consequence is that all events with the sametrigger words have identical feature vectors.
We usethe term trigger EPs to describe the EP(s) which cor-respond to that trigger word ?
i.e.
those whose char-acter span encompasses the trigger word.
We havea potentially large set of related EPs (with the kindsof relationships described above), which we filter tocreate the various feature sets, as outlined below.We have several feature sets targeted at identify-ing negation:?
NEGOUTSCOPE2: If any EPs in the RMRShave predicate names in { no q, but+not c,nor c, only a, never a, not+as+yet a,not+as+yet a, unable a, neg rel}, and thatEP outscopes a trigger EP, set a general featureas well as a specific one for the particle.?
NEGCONJINDEX: If any EPs in the RMRShave predicate names in { not c, but+not c,nor c}, the R-INDEX (RHS of a conjunction)of that EP is the ARG0 a trigger EP, set a gen-eral feature as well as a specific one for the par-ticle ?
capturing the notion that these conjunc-tions are semantically negative for the particleon the right.
This also had a corresponding fea-ture for the L-INDEX of nor c, correspondingto the LHS of the neither...nor construction.79?
ARG0NEGOUTSCOPEESA: For any EPswhich have an argument that matches theARG0 of a trigger EP, if they are outscopedby an EP whose predicate name is inthe list { only a, never a, not+as+yet a,not+as+yet a, unable a, neg rel}, set a gen-eral feature to true, as well as features for thename of the outscoping and outscoped EPs.This is designed to catch trigger EP which arenouns, where the verb of which they are subjector object (or indeed an adjective/preposition towhich they are linked) is semantically negated.And several targeted at identifying speculation:?
SPECVOBJ2: if a verb is a member ofthe set { investigate, study, examine, test,evaluate, observe} and its ARG2 (which cor-responds to the verb object) is the ARG0 of atrigger EP.
This has a general feature for if anyof the verbs match, and a feature which is spe-cific to each verb in the target list.?
SPECVOBJ2+WN: as above, but augment thelist of seed verbs with a list of WordNet sisters(i.e.
any lemmas from any synsets for the verb),and add a feature which is set for the seed verbswhich gave rise to other sister verbs.?
MODALOUTSCOPE: modal verbs (can, should,etc) may be strong indicators of specula-tion; this sets a value when the trigger EP isoutscoped by any predicate corresponding to amodal, both as a general feature and a specificfeature for the particular modal.?
ANALYSISSA: the ARG0 of the trigger EP isalso an argument of an EP with the predicatename analysis n. Such constructions involv-ing the word analysis are relatively frequent inspeculative events in the data.And some general features, aiming to see if thelearning algorithm could pick up other patterns wehad missed:?
TRIGPREDPROPS: Set a feature value for thepredicate name of each trigger EP, as well asthe POS of each trigger EP.?
TRIGOUTSCOPES: Set a feature value for thepredicate name and POS of each EP that isoutscoped by the trigger EP.?
MODADJ: Set a feature value for any EPswhich have an ARG1 which matches the ARG0of the trigger EP if their POS is marked as ad-jective or adverb.?
+CONJ: This is actually a variant on the featureextraction method, which attempts to abstractaway the effect of conjunctions.
If the triggerEP is a member of a conjunction (i.e.
sharesan ARG0 with the L-INDEX or R-INDEX of aconjunction), also treat the EPs which are con-junction parents (and their conjunctive parentsif they exist) as trigger EPs in the feature con-struction.2.2.4 ImplementationTo produce training data to feed into a classifier,we parsed as many sentences as possible using theERG, and used the output RMRSs to create train-ing data using various combinations of the featuresets described above.
The construction of features,however, presupposes annotations for the events andtrigger words.
For producing training data, we usedthe provided trigger annotations.
For the test phase,we simply use the outputs of the classifier we builtin phase 1, selecting the combination with the bestperformance over the development set.
This pipelinearchitecture places limits on annotation performance?
in particular, the recall in task 1 is an upper boundon task 3 recall.
We used a maximum entropy clas-sification algorithm for the ML component here ?
ithas a low level of parameterization and is a solid per-former in NLP tasks.
The implementation we usedwas Zhang Le?s Maxent Toolkit.23 Development experiments3.1 Task 1We devised a set of experiments over the trial, train-ing, and development data in order to estimate theparameters for our final submission.
Using the trialdata, we performed manual error analysis on therules used to construct events.
With the training2http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html80data, we performed our own evaluation based oncross-validation to detect trigger words and con-struct events.
For the experiments over the devel-opment data, we relied on the evaluation interfaceprovided by the organisation.
We focused on testingthe following modules: look-up tagger, CRF, com-bined system, and event construction.First, we tuned the parameters of our look-up tag-ger over the training data.
We used a threshold onthe minimum number of term occurrences requiredto use the class information for that term from thetraining data.
We evaluated thresholding on raw fre-quencies, and also on the percentage of occurrencesof the term that were linked to the majority event.
Incross-validation over the training data, we found thatthe raw-frequency threshold worked best, achiev-ing a maximum F-score of 38.86%, as compared to30.81% for the percentage approach (the results areshown in the bottom part of Table 1).
We also es-timated the frequency threshold as ?
25, and ob-served that most of the terms identified consisted ofa single word, due to data sparseness in the trainingset.Our next experiments are devoted to the CRF sys-tem, focusing on feature engineering.
The resultsover the training data for: (a) the full feature set, and(b) removing one feature type at a time, are shownin Table 1, for windows of size ?3 and ?4.
We cansee that the best F-score is achieved by the?3 word-window system when removing the syntactic depen-dencies from Bikel?s parser.
These results improvedover the look-up system.As a final experiment on feature combinationsand window size, we used the development evalu-ation interface.
We submitted the best combinationsshown in the above experiment, and also syntacticdependencies extracted with GDep.
We observedthe same behaviour as in training data, with the ?3word window obtaining the best F-score, and syn-tactic dependencies harming performance.
These re-sults are shown in the upper part of Table 2.
Ourfinal CRF system used this configuration (?3 wordwindow and all feature types except syntactic depen-dencies).Our next step was to test the integration of thelook-up tagger and CRF into a single system.
Weobserved that by combining the outputs directly weW.
size Feats.
Rec.
Prec.
FSc.
?3 All 30.28 64.44 41.20?3 ?synt.
dep.
30.20 65.01 41.24?3 ?protein NER 28.04 65.73 39.31?3 ?chunking 30.13 65.16 41.20?3 ?POS 29.68 65.25 40.80?3 ?lemma 27.96 62.60 38.66?3 ?word form 29.98 63.81 40.79?4 All 28.86 66.15 40.19?4 ?synt.
dep.
29.75 67.06 41.22?4 ?protein NER 28.11 66.73 39.56?4 ?chunking 28.56 66.61 39.98?4 ?POS 28.19 66.67 39.62?4 ?lemma 26.55 65.20 37.73?4 ?word form 28.19 65.28 39.38Look-up (freq.)
52.14 30.97 38.86Look-up (perc.)
38.20 25.82 30.81Table 1: Trigger-word detection performance over train-ing data.
Results for the look-up tagger and CRFs withthe full feature set and when removing one feature typeat a time, for 3 and 4 word windows.
The best results percolumn are shown in bold.W.
size Feats.
Rec.
Prec.
FSc.
?3 All - synt.
17.55 56.17 26.75?4 All - synt.
17.38 56.75 26.62?3 All (GDep) 15.48 58.69 24.50Combined (All) 26.94 27.83 27.38Combined (Best) 21.24 39.92 27.73Table 2: Performance of selected feature and window-size combinations over development data.
Best resultsper column are given in bold.could improve over the recall of CRF, and achievehigher F-score.
This approach is referred to as?Combined (All)?
in Table 2.
We also tested theresults when choosing either the look-up tagger orCRF depending on their performance over eachevent in the training data.
The results of this sys-tem (?Combined (Best)?)
show a slight improve-ment over the basic combination.Finally, we analysed the results of the event con-struction step.
We used the gold-standard trigger an-notation over the trial data and analysed the errors ofour rules.
We found out that there were three maintypes of error: (1) incorrect assignation of regulationthemes; (2) trigger words having multiple themes;and (3) themes crossing sentence boundaries.
Weplan to address these problems in future work.
Wealso observed that predicting CAUSE for the regula-tory events caused the F-score to drop, resulting inus removing this functionality from the system.81N1: NEGOUTSCOPE2+CONJ, NEGCONJINDEXN2: N1, TRIGPREDPROPSN3: N1, ARG0NEGOUTSCOPEESAN4: N3, TRIGPREDPROPS, NEGVOUTSCOPEN5: N3, NEGVOUTSCOPES1: SPECVOBJ2+WN+CONJ, ANALYSISSAS2: S1, TRIGPREDPROPSS3: S1, MODADJ, MODALOUTSCOPES4: S3, TRIGOUTSCOPESS5: SPECVOBJ2+WN+CONJ, MODADJ,MODALOUTSCOPE,TRIGOUTSCOPESB+y?x: Context window of lemmatized tokens: x preceding and yfollowing.Table 3: Task 3 feature sets3.2 Task 3We evaluated the classification performance of vari-ous feature sets (including some not described here)using 10-fold cross-validation over the training datain the initial stages.
We ran various combinations ofthe most promising features over the developmentdata and evaluated their relative performance in anattempt to avoid overfitting.To evaluate the performance boost we got in task3 relative to more naive methods, we also experi-mented with feature sets based on a bag-of-wordsapproach with a sliding context window of lemma-tised tokens on either side.
We evaluated all com-binations of preceding and following context win-dow sizes from 0 to 3.
There are features for tokensthat precede the trigger, follow the trigger, or lie any-where within the context window, as well as for thetrigger itself.
A ?token?
here may also be a namedbiological entity (protein etc) produced by GENIAtagger in our preprocessing phase, which would notbe lemmatised.
For comparability we only evaluatethese features for sentences which we were able toparse.
For the best performing baseline and RMRS-based feature sets, we also tested them in combina-tion to see whether the features produced were com-plementary.In Table 4 we present the results over the develop-ment data, using the provided gold-standard annota-tions of trigger words, as well as some selected re-sults for our other task 1 outputs.
The gold-standardfigures are unrealistically high compared to whatwe would expect to achieve against the test data,but they are indicative at least of what we couldachieve with a perfect event classifier.
Similar toTask 1 Mod Feats.
Rec.
Prec.
FSc.Gold Spec B+2?2 23.2 40.0 29.3Gold Spec B+3?3 22.1 47.7 30.2Gold Spec S2 15.8 83.3 26.5Gold Spec S3 18.9 78.3 30.5Gold Spec S3,B+2?2 21.1 58.8 31.0Gold Spec S3,B+3?3 23.2 57.9 33.1Comb(best) Spec S3 4.2 21.0 7.0Gold Spec S4 17.9 94.4 30.1Gold Spec S5 17.9 100.0 30.4Gold Neg B+0?2 14.0 33.3 19.7Gold Neg B+1?3 15.0 30.2 20.0Gold Neg N2 19.6 61.8 29.8Comb(best) Neg N2 0.9 7.7 1.7Gold Neg N3 15.9 68.0 25.8Gold Neg N4 19.6 67.7 30.4Gold Neg N4,B+1?3 22.4 52.2 31.4Gold Neg N4,B+0?2 24.3 68.4 35.9Gold Neg N5 16.8 69.2 30.1Table 4: Results (exact match) over development datafor task 3 using gold-standard event/trigger annotationsand selected other annotations for task 1.
Feature setsdescribed in Table 3task 1, our system shows reasonable precision butsuffers badly in recall.
The substantially poorer per-formance when using our own annotations for the in-put events is discussed in more detail in Section 4.2One area where we could improve is to go afterthe 30% of sentences for which we do not have aspanning parse and resultant RMRS.
To reuse ex-isting infrastructure, we could produce RMRS out-put from an alternative processing component withbroader coverage but less precision.
Several meth-ods exist to do this ?
e.g.
producing RMRS out-put from RASP (Briscoe et al, 2006) is described inFrank (2004).
However there is clearly room for im-provement in the remaining 70% of sentences whichwe can parse ?
our results in Table 4 are still wellbelow the limit of roughly 70% recall.3Additional lexical resources beyond WordNet,particularly domain-specific ones, are likely to beuseful in boosting performance since they will helpmaximally utilise the training data.
Additionally,we have not yet made use of other event annota-tions apart from the trigger words ?
features basedon characteristics such as the event class or proper-ties of the event arguments could also be useful.3We have not performed any analysis to verify whether thenumber of events per sentence differs between parseable andunparseable sentences.82System Rec.
Prec.
FSc.Combined (Best) 17.44 39.99 24.29Combined (All) 24.36 30.87 27.23CRF 12.23 62.24 20.44CRF (+ synt feats) 12.01 61.91 20.11Look-Up 22.88 29.67 25.84Look-Up (freq >= 20) 23.26 26.74 24.88Look-Up (freq >= 30) 21.37 30.50 25.13Table 5: Task 1 results with approximate span matching,recursive evaluation (our final submission is in bold)4 Results4.1 Task 1Our experiments on the training and development setshowed that our CRF++ was biased towards preci-sion at the cost of recall, and for the look-up systemthe best F-score was obtained when aiming for highrecall at the cost of lower precision.
The best resultswere obtained when combining both approaches,and this was the composition of the system we sub-mitted.For our final submission, the CRF++ approachhad a ?3 word window, and all the features ex-cept for syntactic dependencies, which were foundto harm performance.
Our final look-up system re-lied on raw frequencies to choose candidate terms,and those above 24 occurrences in training data wereincluded in the dictionary.
For the combination, weobserved that for most events the look-up systemperformed better (although the overall F-score waslower), and we decided to use the CRF++ outputonly for the events that showed better performancethan the look-up system (TRANSCRIPTION, GENEEXPRESSION, and POSITIVE REGULATION).The results over the test data for our final submis-sion and the main variants we explored are shown inTable 5.
We can see that the CRF performed poorly,with very low recall over the test set, in contrast withthe development results, where the higher recall re-sulted in a higher F-score than the look-up approach.The best of our systems was the full combination ofCRF and the look-up tagger, with a 27.23% F-score.The results for each event separately are given inTable 6.
The system performs much worse on regu-lation events, due to the difficulty of having to cor-Event Class Rec.
Prec.
FSc.Localization 25.86 65.22 37.04Binding 17.00 28.92 21.42Gene-expression 45.71 69.18 55.05Transcription 34.31 26.26 29.75Protein-catabolism 42.86 85.71 57.14Phosphorylation 45.19 64.21 53.04EVT-TOTAL 35.84 53.15 42.81Regulation 15.46 13.24 14.26Positive-regulation 13.84 14.82 14.31Negative-regulation 12.14 20.44 15.23REG-TOTAL 13.73 15.31 14.48ALL-TOTAL 24.36 30.87 27.23Table 6: Results for the different events from our com-bined system.
Averaged scores for single events, regula-tions, and all.rectly identify other events in the near context.4.2 Task 3For testing, we repurposed all of the developmentdata as training data and retrained our classifiers.The results in Table 7 were somewhat disappointing,but a drop in recall versus the equivalent run overthe development data using oracle task 1 annotationswas unsurprising and the ratio of this drop is withinthe bounds of what we would expect.
The substan-tial drop in precision can similarly be explained byflow-on effects from our task 1 classification, a nat-ural consequence of our pipeline architecture.
It isquite possible for our system to identify false pos-itive events as being modified; in the online eval-uation system, these classifications of non-existentevents reduce our precision in task 3.In the feature engineering stage, we primarilyused the oracle data for task 1 to maximise theamount of training data available.
We felt that if wewere to use our task 1 classifications for events andtrigger words, the effectively lower number of train-ing instances would only hurt performance.
How-ever this possibly led to bias towards features whichwere more useful for classifying events that wecouldn?t successfully classify in task 1.
The devel-opment set shows similar performance drops underthese conditions in Table 4.It is also possible that our features work reason-ably but that our classification engine trained overthe oracle data simply learnt the wrong parameters83Task 1 Mod Fts.
Rec.
Prec.
FSc.Comb(Best) Spc B+3?3 2.88 12.24 4.67Comb(Best) Spc S2 4.33 37.50 7.76Comb(Best) Spc S3 4.81 30.30 8.30Comb(All) Spc S3 5.29 26.19 8.80Comb(Best) Spc S3,B+3?3 4.81 14.08 7.17Comb(Best) Spc S4 3.85 27.59 6.75Comb(Best) Spe S5 3.85 27.59 6.75Comb(Best) Neg B+3?1 3.96 25.00 6.84Comb(Best) Neg N2 5.29 34.48 9.17Comb(All) Neg N2 5.73 30.00 9.62Comb(Best) Neg N3 5.29 27.78 8.88Comb(Best) Neg N4 5.29 34.48 9.17Comb(Best) Neg N4,B+0?2 4.85 28.12 8.27Comb(All) Neg N4 5.73 27.27 9.47Comb(Best) Neg N5 5.29 29.41 8.96Table 7: Results over test data for task 3 using gold-standard event annotations (approx recursive matching),showing which set of trigger word classifications fromtask 1 was used as input (submitted results in bold).
Fea-ture sets described in table 3for the events we had identified correctly in task 1.We could check this by training a classifier usingour task 1 event classifications combined with thegold-standard trigger annotations.
However com-bining the gold-standard annotations for task 3 withthe classifier outputs of task 1 is non-trivial and wasnot attempted due to time constraints.
It also wouldhave been instructive to calculate a ceiling on ourtask 3 performance given our performance in task 1?
i.e.
how many modifications we could have cor-rectly identified with a perfect task 3 classifier, butwe were not able to show this for similar reasons.5 ConclusionsOur analysis of task 1 seemed to indicate that thescarcity of training instances was the main reasonfor the low recall of CRFs.
The look-up system con-tributed to increase the recall, but at the cost of lowerprecision.
In order to improve this module we planto find ways to extend the training data automaticallyin a bootstrapping process.Another limitation of our system is the event-construction module, which follows simple rulesand performs poorly on regulation events.
For thissubtask we plan to extend the rule set and apply op-timisation techniques, following the lessons learnedin error analysis.In task 3 we investigated the application of a pre-cise, general-purpose grammar over this domain,and were relatively successful.
However, while theparse coverage for task 3 is very respectable for aprecision grammar on comparatively difficult mate-rial, it is clearly unwise to throw away 30% of sen-tences, so a method to extract features from these isdesirable.
Further sources of data would also be use-ful, such as data from the event annotations them-selves, and additional lexical resources tailored tothe biomedical domain.We have also shown the syntactico-semantic out-put of a deep parser, in the form of an RMRS, canbe beneficial in such a task compared with a morenaive approach based on bags of words within asliding context window.
From Table 4, for nega-tion, the syntactic features provided substantial per-formance gains over the best set of baseline param-eters we could find.
For speculation the evidencehere is less compelling, with similar scores fromboth approaches.
Over test data in Table 7, the thedeep methods showed superior performance, albeitover a smaller number of instances.
Regardless, theRMRS still has some advantages, giving (unsurpris-ingly) higher precision than the baseline methods.Combining naive and deep features does tend to giveslightly higher performance than either of the inputsover the development data (although not over the testdata, perhaps due to the poorer performance of naivemethods), suggesting that the two approaches iden-tify slightly different kinds of modification.Our system suffered from the pipeline approach ?there was no way to recover from an incorrect classi-fication in task 1, resulting in greatly reduced preci-sion and recall in task 3.
It is possible that a carefullyconstructed integrated system could annotate eventsfor trigger words and argument at the same time asmodification, with features shared between the two,which may avoid some of these issues.AcknowledgementsWe wish to thank Rebecca Dridan, Dan Flickingerand Lawrence Cavedon for their advice.
NICTAis funded by the Australian Government as repre-sented by the Department of Broadband, Communi-cations and the Digital Economy and the AustralianResearch Council through the ICT Centre of Excel-lence program.84ReferencesEdward Briscoe, John Carroll, and Rebecca Watson.2006.
The second release of the RASP system.
In Pro-ceedings of the COLING/ACL 2006 Interactive PosterSystem, pages 77?80, Sydney, Australia.Ann Copestake and Dan Flickinger.
2000.
An opensource grammar development environment and broad-coverage English grammar using HPSG.
In Interna-tional Conference on Language Resources and Evalu-ation.Ann Copestake, Dan Flickinger, Ivan A.
Sag, and CarlPollard.
2005.
Minimal recursion semantics: An in-troduction.
Research on Language and Computation,pages 281?332.Ann Copestake.
2004.
Report on the design of RMRS.Technical Report D1.1a, University of Cambridge,Cambridge, UK.Anette Frank.
2004.
Constraint-based RMRS construc-tion from shallow grammars.
In COLING ?04: Pro-ceedings of the 20th international conference on Com-putational Linguistics, page 1269, Morristown, NJ,USA.
Association for Computational Linguistics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on MachineLearning, pages 282?289.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
Sentence and token splitting based on condi-tional random fields.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 49?57, Melbourne, Australia.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics - 10th Panhellenic Conference on Infor-matics, pages 382?392.85
