Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Two Level Model for Context Sensitive Inference RulesOren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor??
Computer Science Department, Bar-Ilan University?
Computer Science Department, Stanford University?
Faculty of Engineering, Bar-Ilan University?
Yahoo!
Research Israel{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.iljoberant@stanford.eduidan@yahoo-inc.comAbstractAutomatic acquisition of inference rulesfor predicates has been commonly ad-dressed by computing distributional simi-larity between vectors of argument words,operating at the word space level.
A re-cent line of work, which addresses contextsensitivity of rules, represented contexts ina latent topic space and computed similar-ity over topic vectors.
We propose a noveltwo-level model, which computes simi-larities between word-level vectors thatare biased by topic-level context repre-sentations.
Evaluations on a naturally-distributed dataset show that our modelsignificantly outperforms prior word-leveland topic-level models.
We also release afirst context-sensitive inference rule set.1 IntroductionInference rules for predicates have been identi-fied as an important component in semantic ap-plications, such as Question Answering (QA)(Ravichandran and Hovy, 2002) and InformationExtraction (IE) (Shinyama and Sekine, 2006).
Forexample, the inference rule ?X treat Y ?
X relieveY?
can be useful to extract pairs of drugs and theillnesses which they relieve, or to answer a ques-tion like ?Which drugs relieve headache??.
Alongthis vein, such inference rules constitute a crucialcomponent in generic modeling of textual infer-ence, under the Textual Entailment paradigm (Da-gan et al, 2006; Dinu and Wang, 2009).Motivated by these needs, substantial researchwas devoted to automatic learning of inferencerules from corpora, mostly in an unsupervised dis-tributional setting.
This research line was mainlyinitiated by the highly-cited DIRT algorithm (Linand Pantel, 2001), which learns inference for bi-nary predicates with two argument slots (like therule in the example above).
DIRT represents apredicate by two vectors, one for each of the ar-gument slots, where the vector entries correspondto the argument words that occurred with the pred-icate in the corpus.
Inference rules between pairsof predicates are then identified by measuring thesimilarity between their corresponding argumentvectors.
This general scheme was further en-hanced in several directions, e.g.
directional sim-ilarity (Bhagat et al, 2007; Szpektor and Dagan,2008) and meta-classification over similarity val-ues (Berant et al, 2011).
Consequently, severalknowledge resources of inference rules were re-leased, containing the top scoring rules for eachpredicate (Schoenmackers et al, 2010; Berant etal., 2011; Nakashole et al, 2012).The above mentioned methods provide a sin-gle confidence score for each rule, which is basedon the obtained degree of argument-vector sim-ilarities.
Thus, a system that applies an infer-ence rule to a text may estimate the validity ofthe rule application based on the pre-specified rulescore.
However, the validity of an inference rulemay depend on the context in which it is applied,such as the context specified by the given predi-cate?s arguments.
For example, ?AT&T acquire T-Mobile ?
AT&T purchase T-Mobile?, is a validapplication of the rule ?X acquire Y ?
X pur-chase Y?, while ?Children acquire skills ?
Chil-dren purchase skills?
is not.
To address this issue, aline of works emerged which computes a context-sensitive reliability score for each rule application,based on the given context.The major trend in context-sensitive inferencemodels utilizes latent or class-based methods forcontext modeling (Pantel et al, 2007; Szpektor etal., 2008; Ritter et al, 2010; Dinu and Lapata,2010b).
In particular, the more recent methods(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-eled predicates in context as a probability distribu-tion over topics learned by a Latent Dirichlet Allo-1331cation (LDA) model.
Then, similarity is measuredbetween the two topic distribution vectors corre-sponding to the two sides of the rule in the givencontext, yielding a context-sensitive score for eachparticular rule application.We notice at this point that while context-insensitive methods represent predicates by ar-gument vectors in the original fine-grained wordspace, context-sensitive methods represent themas vectors at the level of latent topics.
This raisesthe question of whether such coarse-grained topicvectors might be less informative in determiningthe semantic similarity between the two predi-cates.To address this hypothesized caveat of priorcontext-sensitive rule scoring methods, we pro-pose a novel generic scheme that integrates word-level and topic-level representations.
Our schemecan be applied on top of any context-insensitive?base?
similarity measure for rule learning, whichoperates at the word level, such as Cosine orLin (Lin, 1998).
Rather than computing a singlecontext-insensitive rule score, we compute a dis-tinct word-level similarity score for each topic inan LDA model.
Then, when applying a rule in agiven context, these different scores are weighedtogether based on the specific topic distributionunder the given context.
This way, we calculatesimilarity over vectors in the original word space,while biasing them towards the given context viaa topic model.In order to promote replicability and equal-termcomparison with our results, we based our experi-ments on publicly available datasets, both for un-supervised learning of the evaluated models andfor testing them over a random sample of rule ap-plications.
We apply our two-level scheme overthree state-of-the-art context-insensitive similar-ity measures.
The evaluation compares perfor-mances both with the original context-insensitivemeasures and with recent LDA-based context-sensitive methods, showing consistent and robustadvantages of our scheme.
Finally, we releasea context-sensitive rule resource comprising over2,000 frequent verbs and one million rules.2 Background and Model SettingThis section presents components of prior workwhich are included in our model and experiments,setting the technical preliminaries for the rest ofthe paper.
We first present context-insensitive rulelearning, based on distributional similarity at theword level, and then context-sensitive scoring forrule applications, based on topic-level similarity.Some further discussion of related work appearsin Section 6.2.1 Context-insensitive Rule LearningA predicate inference rule ?LHS ?
RHS?, suchas ?X acquire Y ?
X purchase Y?, specifies adirectional inference relation between two predi-cates.
Each rule side consists of a lexical pred-icate and (two) variable slots for its arguments.1Different representations have been used to spec-ify predicates and their argument slots, such asword lemma sequences, regular expressions anddependency parse fragments.
A rule can be ap-plied when its LHS matches a predicate with apair of arguments in a text, allowing us to infer itsRHS, with the corresponding instantiations for theargument variables.
For example, given the text?AT&T acquires T-Mobile?, the above rule infers?AT&T purchases T-Mobile?.The DIRT algorithm (Lin and Pantel, 2001)follows the distributional similarity paradigm tolearn predicate inference rules.
For each predi-cate, DIRT represents each of its argument slotsby an argument vector.
We denote the two vectorsof the X and Y slots of a predicate pred by vxpredand vypred, respectively.
Each entry of a vector vcorresponds to a particular word (or term) w thatinstantiated the argument slot in a learning corpus,with a value v(w) = PMI(pred, w) (with PMIstanding for point-wise mutual information).To learn inference rules, DIRT considers (inprinciple) each pair of binary predicates thatoccurred in the corpus for a candidate rule,?LHS ?
RHS?.
Then, DIRT computes a reliabil-ity score for the rule by combining the measuredsimilarities between the corresponding argumentvectors of the two rule sides.
Concretely, denot-ing by l and r the predicates appearing in the tworule sides, DIRT?s reliability score is defined asfollows:(1)scoreDIRT(LHS ?
RHS)=?sim(vxl , vxr ) ?
sim(vyl , vyr )where sim(v, v?)
is a vector similarity measure.Specifically, DIRT employs the Lin similarity1We follow most of the inference-rule learning literature,which focused on binary predicates.
However, our context-sensitive scheme can be applied to any arity.1332measure from (Lin, 1998), defined as follows:(2)Lin(v, v?)
=?w?v?v?
[v(w) + v?(w)]?w?v?v?
[v(w) + v?
(w)]We note that the general DIRT scheme may beused while employing other ?base?
vector similar-ity measures.
For example, the Lin measure issymmetric, and thus using it would yield the samereliability score when swapping the two sides ofa rule.
This issue has been addressed in a sepa-rate line of research which introduced directionalsimilarity measures suitable for inference rela-tions (Bhagat et al, 2007; Szpektor and Dagan,2008; Kotlerman et al, 2010).
In our experimentswe apply our proposed context-sensitive similarityscheme over three different base similarity mea-sures.DIRT and similar context-insensitive inferencemethods provide a single reliability score for alearned inference rule, which aims to predict thevalidity of the rule?s applications.
However, asexemplified in the Introduction, an inference rulemay be valid in some contexts but invalid in oth-ers (e.g.
acquiring entails purchasing for goods,but not for skills).
Since vector similarity in DIRTis computed over the single aggregate argumentvector, the obtained reliability score tends to bebiased towards the dominant contexts of the in-volved predicates.
For example, we may expecta higher score for ?acquire ?
purchase?
than for?acquire ?
learn?, since the former matches amore frequent sense of acquire in a typical corpus.Following this observation, it is desired to obtaina context-sensitive reliability score for each ruleapplication in a given context, as described next.2.2 Context-sensitive Rule ApplicationsTo assess the reliability of applying an inferencerule in a given context we need some model forcontext representation, that should affect the rulereliability score.
A major trend in past work isto represent contexts in a reduced-dimensionalitylatent or class-based model.
A couple of earlierworks utilized a cluster-based model (Pantel et al,2007) and an LSA-based model (Szpektor et al,2008), in a selectional-preferences style approach.Several more recent works utilize a Latent Dirich-let Allocation (LDA) (Blei et al, 2003) frame-work.
We now present an underlying unified viewof the topic-level models in (Ritter et al, 2010;Dinu and Lapata, 2010b), which we follow in ourown model and in comparative model evaluations.We note that a similar LDA model constructionwas employed also in (Se?aghdha, 2010), for esti-mating predicate-argument likelihood.First, an LDA model is constructed, as follows.Similar to the construction of argument vectorsin the distributional model (described above insubsection 2.1), all arguments instantiating eachpredicate slot are extracted from a large learningcorpus.
Then, for each slot of each predicate, apseudo-document is constructed containing the setof all argument words that instantiated this slot inthe corpus.
We denote the two documents con-structed for the X and Y slots of a predicate predby dxpred and dypred, respectively.
In comparison tothe distributional model, these two documents cor-respond to the analogous argument vectors vxpredand vypred, both containing exactly the same set ofwords.Next, an LDA model is learned from the setof all pseudo-documents, extracted for all predi-cates.2 The learning process results in the con-struction of K latent topics, where each topic tspecifies a distribution over all words, denoted byp(w|t), and a topic distribution for each pseudo-document d, denoted by p(t|d).Within the LDA model we can derive thea-posteriori topic distribution conditioned on aparticular word within a document, denoted byp(t|d,w) ?
p(w|t) ?
p(t|d).
In the topic-levelmodel, d corresponds to a predicate slot and w toa particular argument word instantiating this slot.Hence, p(t|d,w) is viewed as specifying the rele-vance (or likelihood) of the topic t for the predi-cate slot in the context of the given argument in-stantiation.
For example, for the predicate slot ?ac-quire Y?
in the context of the argument ?IBM?, weexpect high relevance for a topic about companies,while in the context of the argument ?knowledge?we expect high relevance for a topic about abstractconcepts.
Accordingly, the distribution p(t|d,w)over all topics provides a topic-level representa-tion for a predicate slot in the context of a particu-lar argument w. This representation is used by thetopic-level model to compute a context-sensitivescore for inference rule applications, as follows.2We note that there are variants in the type of LDA modeland the way the pseudo-documents are constructed in thereferenced prior work.
In order to focus on the inferencemethods rather than on the underlying LDA model, we usethe LDA framework described in this paper for all comparedmethods.1333Consider the application of an inference rule?LHS ?
RHS?
in the context of a particular pairof arguments for the X and Y slots, denoted bywx and wy, respectively.
Denoting by l and r thepredicates appearing in the two rule sides, the reli-ability score of the topic-level model is defined asfollows (we present a geometric mean formulationfor consistency with DIRT):(3)scoreTopic(LHS ?
RHS, wx, wy)=?sim(dxl , dxr , wx) ?
sim(dyl , dyr , wy)where sim(d, d?, w) is a topic-distribution similar-ity measure conditioned on a given context word.Specifically, Ritter et al (2010) utilized the dotproduct form for their similarity measure:(4)simDC(d, d?, w) = ?t[p(t|d,w) ?
p(t|d?, w)](the subscript DC stands for double-conditioning,as both distributions are conditioned on the argu-ment word, unlike the measure below).Dinu and Lapata (2010b) presented a slightlydifferent similarity measure for topic distributionsthat performed better in their setting as well as in arelated later paper on context-sensitive scoring oflexical similarity (Dinu and Lapata, 2010a).
In thismeasure, the topic distribution for the right handside of the rule is not conditioned on w:(5)simSC(d, d?, w) = ?t[p(t|d,w) ?
p(t|d?
)](the subscript SC stands for single-conditioning,as only the left distribution is conditioned on theargument word).
They also experimented with afew variants for the structure of the similarity mea-sure and assessed that best results are obtainedwith the dot product form.
In our experiments,we employ these two similarity measures for topicdistributions as baselines representing topic-levelmodels.Comparing the context-insensitive and context-sensitive models, we see that both of them mea-sure similarity between vector representations ofcorresponding predicate slots.
However, whileDIRT computes sim(v, v?)
over vectors in theoriginal word-level space, topic-level models com-pute sim(d, d?, w) by measuring similarity of vec-tors in a reduced-dimensionality latent space.
Asconjectured in the introduction, such coarse-grainrepresentation might lead to loss of information.Hence, in the next section we propose a com-bined two-level model, which represents predicateslots in the original word-level space while biasingthe similarity measure through topic-level contextmodels.3 Two-level Context-sensitive InferenceOur model follows the general DIRT schemewhile extending it to handle context-sensitive scor-ing of rule applications, addressing the scenariodealt by the context-sensitive topic models.
Inparticular, we define the context-sensitive scorescoreWT, where WT stands for the combinationof the Word/Topic levels:(6)scoreWT(LHS ?
RHS, wx, wy)=?sim(vxl , vxr , wx) ?
sim(vyl , vyr , wy)Thus, our model computes similarity over word-level (rather than topic-level) argument vectors,while biasing it according to the specific argu-ment words in the given rule application con-text.
The core of our contribution is thus definingthe context-sensitive word-level vector similaritymeasure sim(v, v?, w), as described in the remain-der of this section.Following the methods in Section 2, for eachpredicate pred we construct, from the learningcorpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred anddypred.
For convenience, when referring to an ar-gument vector v, we will denote the correspond-ing pseudo-document by dv.
Based on all pseudo-documents we learn an LDA model and obtain itsassociated probability distributions.The calculation of sim(v, v?, w) is composed oftwo steps.
At learning time, we compute for eachcandidate rule a separate, topic-biased, similarityscore per each of the topics in the LDA model.Then, at rule application time, we compute anoverall reliability score for the rule by combiningthe per-topic similarity scores, while biasing thescore combination according to the given contextof w. These two steps are described in the follow-ing two subsections.3.1 Topic-biased Word-vector SimilaritiesGiven a pair of word vectors v and v?, andany desired ?base?
vector similarity measure sim(e.g.
simLin), we compute a topic-biased sim-ilarity score for each LDA topic t, denoted bysimt(v, v?).
simt(v, v?)
is computed by applying1334the original similarity measure over topic-biasedversions of v and v?, denoted by vt and v?t:simt(v, v?)
= sim(vt, v?t)wherevt(w) = v(w) ?
p(t|dv, w)That is, each value in the biased vector, vt(w),is obtained by weighing the original value v(w)by the relevance of the topic t to the argumentword w within dv.
This way, rather than replac-ing altogether the word-level values v(w) by thetopic probabilities p(t|dv, w), as done in the topic-level models, we use the latter to only bias the for-mer while preserving fine-grained word-level rep-resentations.
The notation Lint denotes the simtmeasure when applied using Lin as the base simi-larity measure sim.This learning process results in K differenttopic-biased similarity scores for each candidaterule, where K is the number of LDA topics.
Ta-ble 1 illustrates topic-biased similarities for the Yslot of two rules involving the predicate ?acquire?.As can be seen, the topic-biased score Lint for ?ac-quire?
learn?
for t2 is higher than the Lin score,since this topic is characterized by arguments thatcommonly appear with both predicates of the rule.Consequently, the two predicates are found to bedistributionally similar when biased for this topic.On the other hand, the topic-biased similarity fort1 is substantially lower, since prominent wordsin this topic are likely to occur with ?acquire?
butnot with ?learn?, yielding low distributional simi-larity.
Opposite behavior is exhibited for the rule?acquire?
purchase?.3.2 Context-sensitive SimilarityWhen applying an inference rule, we computefor each slot its context-sensitive similarity scoresimWT(v, v?, w), where v and v?
are the slot?s ar-gument vectors for the two rule sides and w is theword instantiating the slot in the given rule appli-cation.
This score is computed as a weighted aver-age of the rule?s K topic-biased similarity scoressimt.
In this average, each topic is weighed byits ?relevance?
for the context in which the rule isapplied, which consists of the left-hand-side pred-icate v and the argument w. This relevance is cap-Topic t1 t2Top 5wordscalbiochem rightscorel syndromenetworks majorityviacom knowledgefinancially skillacquire ?
learnLint(v, v?)
0.040 0.334Lin(v, v?)
0.165acquire ?
purchaseLint(v, v?)
0.427 0.241Lin(v, v?)
0.267Table 1: Two characteristic topics for the Y slot of?acquire?, along with their topic-biased Lin sim-ilarities scores Lint, compared with the originalLin similarity, for two rules.
The relevance of eachtopic to different arguments of ?acquire?
is illus-trated by showing the top 5 words in the argumentvector vyacquire for which the illustrated topic is themost likely one.tured by p(t|dv, w):simWT(v, v?, w) =?t[p(t|dv, w) ?
simt(v, v?
)](7)This way, a rule application would obtain a highscore only if the current context fits those topicsfor which the rule is indeed likely to be valid, ascaptured by a high topic-biased similarity.
The no-tation LinWT denotes the simWT measure, whenusing Lint as the topic-biased similarity measure.Table 2 illustrates the calculation of context-sensitive similarity scores in four rule applica-tions, involving the Y slot of the predicate ?ac-quire?.
We observe that relative to the fixedcontext-insensitive Lin score, the score of ?ac-quire ?
learn?
is substantially promoted forthe argument ?skill?
while being demoted for?Skype?.
The opposite behavior is observed for?acquire ?
purchase?, altogether demonstratinghow our model successfully biases the similarityscore according to rule validity in context.4 Experimental SettingsTo evaluate our model, we compare it both tocontext-insensitive similarity measures as well asto prior context-sensitive methods.
Furthermore,to better understand its applicability in typicalNLP tasks, we focus on an evaluation setting thatcorresponds to a natural distribution of examplesfrom a large corpus.1335Topic t1 t2Top 5wordscalbiochem rightscorel syndromenetworks majorityviacom knowledgefinancially skill?acquire Skype ?
learn Skype?p(t|dv, w) 0.974 0.000Lint(v, v?)
0.040 0.334LinWT(v, v?, w) 0.039Lin(v, v?)
0.165?acquire Skype ?
purchase Skype?p(t|dv, w) 0.974 0.000Lint(v, v?)
0.427 0.241LinWT(v, v?, w) 0.417Lin(v, v?)
0.267?acquire skill ?
learn skill?p(t|dv, w) 0.000 0.380Lint(v, v?)
0.040 0.334LinWT(v, v?, w) 0.251Lin(v, v?)
0.165?acquire skill ?
purchase skill?p(t|dv, w) 0.000 0.380Lint(v, v?)
0.427 0.241LinWT(v, v?, w) 0.181Lin(v, v?)
0.267Table 2: Context-sensitive similarity scores (inbold) for the Y slots of four rule applications.
Thecomponents of the score calculation are shown forthe topics of Table 1.
For each rule application,the table shows a couple of the topic-biased scoresLint of the rule (as in Table 1), along with the topicrelevance for the given context p(t|dv, w), whichweighs the topic-biased scores in the LinWT cal-culation.
The context-insensitive Lin score isshown for comparison.4.1 Evaluated Rule Application MethodsWe evaluated the following rule application meth-ods: the original context-insensitive word model,following DIRT (Lin and Pantel, 2001), as de-scribed in Equation 1, denoted by CI; our owntopic-word context-sensitive model, as describedin Equation 6, denoted by WT.
In addition, weevaluated two variants of the topic-level context-sensitive model, denoted DC and SC.
DC followsthe double conditioned contextualized similaritymeasure according to Equation 4, as implementedby (Ritter et al, 2010), while SC follows the sin-gle conditioned one at Equation 5, as implementedby (Dinu and Lapata, 2010b; Dinu and Lapata,2010a).Since our model can contextualize various dis-tributional similarity measures, we evaluated theperformance of all the above methods on severalbase similarity measures and their learned rule-sets, namely Lin (Lin, 1998), BInc (Szpektor andDagan, 2008) and vector Cosine similarity.
TheLin similarity measure is described in Equation 2.Binc (Szpektor and Dagan, 2008) is a directionalsimilarity measure between word vectors, whichoutperformed Lin for predicate inference (Szpek-tor and Dagan, 2008).To build the rule-sets and models for the testedapproaches we utilized the ReVerb corpus (Faderet al, 2011), a large scale publicly available web-based open extractions data set, containing about15 million unique template extractions.3 ReVerbtemplate extractions/instantiations are in the formof a tuple (x, pred, y), containing pred, a verbpredicate, x, the argument instantiation of the tem-plate?s slot X , and y, the instantiation of the tem-plate?s slot Y .ReVerb includes over 600,000 different tem-plates that comprise a verb but may also includeother words, for example ?X can accommodate upto Y?.
Yet, many of these templates share a similarmeaning, e.g.
?X accommodate up to Y?, ?X canaccommodate up to Y?, ?X will accommodate upto Y?, etc.
Following Sekine (2005), we clusteredtemplates that share their main verb predicate inorder to scale down the number of different pred-icates in the corpus and collect richer word co-occurrence statistics per predicate.Next, we applied some clean-up preprocessingto the ReVerb extractions.
This includes discard-ing stop words, rare words and non-alphabeticalwords instantiating either the X or the Y argu-ments.
In addition, we discarded all predicatesthat co-occur with less than 100 unique argumentwords in each slot.
The remaining corpus consistsof 7 million unique extractions and 2,155 verbpredicates.Finally, we trained an LDA model, as describedin Section 2, using Mallet (McCallum, 2002).Then, for each original context-insensitive simi-larity measure, we learned from ReVerb a rule-setcomprised of the top 500 rules for every identi-fied predicate.
To complete the learning, we cal-culated the topic-biased similarity score for eachlearned rule under each LDA topic, as specifiedin our context-sensitive model.
We release a ruleset comprising the top 500 context-sensitive rulesthat we learned for each of the verb predicates inour learning corpus, along with our trained LDA3ReVerb is available at http://reverb.cs.washington.edu/1336Method Lin BInc CosineValid 266 254 272Invalid 545 523 539Total 811 777 811Table 3: Sizes of rule application test set for eachlearned rule-set.model.44.2 Evaluation TaskTo evaluate the performance of the different meth-ods we chose the dataset constructed by Zeich-ner et al (2012).
5 This publicly available datasetcontains about 6,500 manually annotated predi-cate template rule applications, each one labeledas correct or incorrect.
For example, ?Jack agreewith Jill 9 Jack feel sorry for Jill?
is a rule ap-plication in this dataset, labeled as incorrect, and?Registration open this month?
Registration be-gin this month?
is another rule application, labeledas correct.
Rule applications were generated byrandomly sampling extractions from ReVerb, suchas (?Jack?,?agree with?,?Jill?)
and then samplingpossible rules for each, such as ?agree with?
feelsorry for?.
Hence, this dataset provides naturallydistributed rule inferences with respect to ReVerb.Whenever we evaluated a distributional similar-ity measure (namely Lin, BInc, or Cosine), wediscarded instances from Zeichner et al?s datasetin which the assessed rule is not in the context-insensitive rule-set learned for this measure or theargument instantiation of the rule is not in the LDAlexicon.
We refer to the remaining instances as thetest set per measure, e.g.
Lin?s test set.
Table 3details the size of each such test set in our experi-ment.Finally, the task under which we assessed thetested models is to rank all rule applications ineach test set, aiming to rank the valid rule appli-cations above the invalid ones.5 ResultsWe evaluated the performance of each testedmethod by measuring Mean Average Precision(MAP) (Manning et al, 2008) of the rule appli-cation ranking computed by this method.
In order4Our resource is available at: http://www.cs.biu.ac.il/?
nlp/downloads/wt-rules.html5The dataset is available at: http://www.cs.biu.ac.il/?nlp/downloads/annotation-rule-application.htmMethod Lin BInc CosineCI 0.503 0.513 0.513DC 0.451 (1200) 0.455 (1200) 0.455 (1200)SC 0.443 (1200) 0.458 (1200) 0.452 (1200)WT 0.562 (100) 0.584 (50) 0.565 (25)Table 4: MAP values on corresponding test set ob-tained by each method.
Figures in parentheses in-dicate optimal number of LDA topics.to compute MAP values and corresponding statis-tical significance, we randomly split each test setinto 30 subsets.
For each method we computedAverage Precision on every subset and then tookthe average over all subsets as the MAP value.Since all tested context-sensitive approaches arebased on LDA topics, we varied for each methodthe number of LDA topics K that optimizes itsperformance, ranging from 25 to 1600 topics.
Weused LDA hyperparameters ?
= 0.01 and ?
= 0.1for K < 600 and ?
= 50K for K >= 600.Table 4 presents the optimal MAP performanceof each tested measure.
Our main result is thatour model outperforms all other methods, bothcontext-insensitive and context-sensitive, by a rel-ative increase of more than 10% for all three sim-ilarity measures that we tested.
This improvementis statistically significant at p < 0.01 for BInc andLin, and p < 0.015 for Cosine, using paired t-test.
This shows that our model indeed success-fully leverages contextual information beyond thebasic context-agnostic rule scores and is robustacross measures.Surprisingly, both baseline topic-level context-sensitive methods, namely DC and SC, underper-formed compared to their context-insensitive base-lines.
While Dinu and Lapata (Dinu and Lap-ata, 2010b) did show improvement over context-insensitive DIRT, this result was obtained on theverbs of the Lexical Substitution Task in SemEval(McCarthy and Navigli, 2007), which was manu-ally created with a bias for context-sensitive sub-stitutions.
However, our result suggests that topic-level models might not be robust enough when ap-plied to a random sample of inferences.An interesting indication of the differences be-tween our word-topic model, WT, and topic-onlymodels, DC and SC, lies in the optimal number ofLDA topics required for each method.
The num-ber of topics in the range 25-100 performed almostequally well under the WT model for all base mea-sures, with a moderate decline for higher numbers.1337The need for this rather small number of topics isdue to the nature of utilization of topics in WT.Specifically, topics are leveraged for high-leveldomain disambiguation, while fine grained word-level distributional similarity is computed for eachrule under each such domain.
This works best fora relatively low number of topics.
However, inhigher numbers, topics relate to narrower domainsand then topic biased word level similarity maybecome less effective due to potential sparseness.On the other hand, DC and SC rely on topics asa surrogate to predicate-argument co-occurrencefeatures, and thus require a relatively large num-ber of them to be effective.Delving deeper into our test-set, Zeichner et alprovided a more detailed annotation for each in-valid rule application.
Specifically, they annotatedwhether the context under which the rule is ap-plied is valid.
For example, in ?John bought mycar 9 John sold my car?
the inference is invaliddue to an inherently incorrect rule, but the con-text is valid.
On the other hand in ?my boss raisedmy salary 9 my boss constructed my salary?
thecontext {?my boss?, ?my salary?}
for applying?raise?
construct?
is invalid.
Following, we splitthe test-set for the base Lin measure into two test-sets: (a) test-setvc, which includes all correct ruleapplications and incorrect ones only under validcontexts, and (b) test-setivc, which includes againall correct rule applications but incorrect ones onlyunder invalid contexts.Table 5 presents the performance of each com-pared method on the two test sets.
On test-setivc, where context mismatches are abundant,our model outperformed all other baselines (sta-tistically significant at p < 0.01).
In addition,this time DC slightly outperformed CI.
This re-sult more explicitly shows the advantages of in-tegrating word-level and context-sensitive topic-level similarities for differentiating valid and in-valid contexts for rule applications.
Yet, many in-valid rule applications occur under valid contextsdue to inherently incorrect rules, and we want tomake sure that also in this scenario our modeldoes not fall behind the context-insensitive mea-sure.
Indeed, on test-setvc, in which context mis-matches are rare, our algorithm is still better thanthe original measure, indicating that WT can besafely applied to distributional similarity measureswithout concerns of reduced performance in dif-ferent context scenarios.test-setivc test-setvcSize(valid:invalid)432(266:166)645(266:379)CI 0.780 0.587DC 0.796 0.498SC 0.779 0.512WT 0.854 0.621Table 5: MAP results for the two split Lin test-sets.6 Discussion and Future WorkThis paper addressed the problem of computingcontext-sensitive reliability scores for predicate in-ference rules.
In particular, we proposed a novelscheme that applies over any base distributionalsimilarity measure which operates at the wordlevel, and computes a single context-insensitivescore for a rule.
Based on such a measure, ourscheme constructs a context-sensitive similaritymeasure that computes a reliability score for pred-icate inference rules applications in the context ofgiven arguments.The contextualization of the base similarityscore was obtained using a topic-level LDAmodel, which was used in a novel way.
First,it provides a topic bias for learning separate per-topic word-level similarity scores between predi-cates.
Then, given a specific candidate rule ap-plication, the LDA model is used to infer thetopic distribution relevant to the context speci-fied by the given arguments.
Finally, the context-sensitive rule application score is computed as aweighted average of the per-topic word-level sim-ilarity scores, which are weighed according to theinferred topic distribution.While most works on context-insensitive pred-icate inference rules, such as DIRT (Lin and Pan-tel, 2001), are based on word-level similarity mea-sures, almost all prior models addressing context-sensitive predicate inference rules are based ontopic models (except for (Pantel et al, 2007),which was outperformed by later models).
Wetherefore focused on comparing the performanceof our two-level scheme with state-of-the-art priortopic-level and word-level models of distributionalsimilarity, over a random sample of inference ruleapplications.
Under this natural setting, the two-level scheme consistently outperformed both typesof models when tested with three different basesimilarity measures.
Notably, our model showsstable performance over a large subset of the data1338where context sensitivity is rare, while topic-levelmodels tend to underperform in such cases com-pared to the base context-insensitive methods.Our work is closely related to another researchline that addresses lexical similarity and substi-tution scenarios in context.
While we focus onlexical-syntactic predicate templates and instanti-ations of their argument slots as context, lexicalsimilarity methods consider various lexical unitsthat are not necessarily predicates, with their con-text typically being the collection of words in awindow around them.Various approaches have been proposed to ad-dress lexical similarity.
A number of works arebased on a compositional semantics approach,where a prior representation of a target lexical unitis composed with the representations of words inits given context (Mitchell and Lapata, 2008; Erkand Pado?, 2008; Thater et al, 2010).
Other works(Erk and Pado?, 2010; Reisinger and Mooney,2010) use a rather large word window around tar-get words and compute similarities between clus-ters comprising instances of word windows.
In ad-dition, (Dinu and Lapata, 2010a) adapted the pred-icate inference topic model from (Dinu and Lap-ata, 2010b) to compute lexical similarity in con-text.A natural extension of our work would be to ex-tend our two level model to accommodate context-sensitive lexical similarity.
For this purpose wewill need to redefine the scope of context in ourmodel, and adapt our method to compute context-biased lexical similarities accordingly.
Then wewill also be able to evaluate our model on theLexical Substitution Task (McCarthy and Navigli,2007), which has been commonly used in recentyears as a benchmark for context-sensitive lexicalsimilarity models.In a different NLP task, Eidelman et al (2012)utilize a similar approach to ours for improvingthe performance of statistical machine translation(SMT).
They learn an LDA model on the sourcelanguage side of the training corpus with the pur-pose of identifying implicit sub-domains.
Thenthey utilize the distribution over topics inferred foreach document in their corpus to compute sepa-rate per-topic translation probability tables.
Fi-nally, they train a classifier to translate a giventarget word based on these tables and the inferredtopic distribution of the given document in whichthe target word appears.
A notable difference be-tween our approach and theirs is that we use predi-cate pseudo-documents consisting of argument in-stantiations to learn our LDA model, while Eidel-man et al use the real documents in a corpus.We believe that combining these two approachesmay improve performance for both textual infer-ence and SMT and plan to experiment with thisdirection in future work.AcknowledgmentsThis work was partially supported by the IsraeliMinistry of Science and Technology grant 3-8705,the Israel Science Foundation grant 880/12, andthe European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).ReferencesJonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InACL.Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-rina Rey.
2007.
Ledir: An unsupervised algorithmfor learning directionality of inference rules.
In Pro-ceedings of EMNLP-CoNLL.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Lecture Notes in Computer Science,volume 3944, pages 177?190.Georgiana Dinu and Mirella Lapata.
2010a.
Measur-ing distributional similarity in context.
In Proceed-ings of EMNLP.Georgiana Dinu and Mirella Lapata.
2010b.
Topicmodels for meaning similarity in context.
In Pro-ceedings of COLING: Posters.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entail-ment.
In Proceedings EACL.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic transla-tion model adaptation.
In Proceedings of the ACLconference short papers.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL conference short papers.1339Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
discov-ery of inference rules from text.
In Proceedings ofACM SIGKDDConference on Knowledge Discoveryand Data Mining 2001.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL.Christopher D Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of SemEval.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: A taxonomy of relationalpatterns with semantic types.
EMNLP12.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Hu-man Language Technologies 2007: The Conferenceof the North American Chapter of the Associationfor Computational Linguistics.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answeringsystem.
In Proceedings of ACL.Joseph Reisinger and Raymond J Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chapterof the Association for Computational Linguistics.Alan Ritter, Oren Etzioni, et al 2010.
A latent dirich-let alocation method for selectional preferences.
InProceedings of ACL.Stefan Schoenmackers, Jesse Davis, Oren Etzioni, andDaniel Weld.
2010.
Learning first-order hornclauses from web text.
In Proceedings of EMNLP.Diarmuid O Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of ACL.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of the Third International Workshop onParaphrasing (IWP2005).Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, MainConference.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings ofCOLING.Idan Szpektor, Ido Dagan, Roy Bar-Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Pro-ceedings of ACL-08: HLT.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of ACL.Naomi Zeichner, Jonathan Berant, and Ido Dagan.2012.
Crowdsourcing inference-rule evaluation.
InProceedings of ACL (short papers).1340
