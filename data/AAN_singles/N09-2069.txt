Proceedings of NAACL HLT 2009: Short Papers, pages 273?276,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomatic Chinese Abbreviation Generation Using Conditional RandomFieldDong Yang, Yi-cheng Pan, and Sadaoki FuruiDepartment of Computer ScienceTokyo Institute of TechnologyTokyo 152-8552 Japan{raymond,thomas,furui}@furui.cs.titech.ac.jpAbstractThis paper presents a new method for au-tomatically generating abbreviations for Chi-nese organization names.
Abbreviations arecommonly used in spoken Chinese, especiallyfor organization names.
The generation ofChinese abbreviation is much more complexthan English abbreviations, most of which areacronyms and truncations.
The abbreviationgeneration process is formulated as a charactertagging problem and the conditional randomfield (CRF) is used as the tagging model.
Acarefully selected group of features is used inthe CRF model.
After generating a list of ab-breviation candidates using the CRF, a lengthmodel is incorporated to re-rank the candi-dates.
Finally the full-name and abbreviationco-occurrence information from a web searchengine is utilized to further improve the per-formance.
We achieved top-10 coverage of88.3% by the proposed method.1 IntroductionLong named entities are frequently abbreviated inoral Chinese language for efficiency and simplic-ity.
Therefore, abbreviation modeling is an impor-tant building component for many systems that ac-cept spoken input, such as directory assistance andvoice search systems.While English abbreviations are usually formedas acronyms, Chinese abbreviations are much morecomplex, as shown in Figure 1.
Most of the Chi-nese abbreviations are formed by selecting severalcharacters from full-names, which are not necessar-ily the first character of each word.
Usually the orig-inal character order in the full-name is preserved in????
??
T s i n g h u a U n i v e r s i t y???????
??
C h i n a c e n t r a l t e l e v i s i o nF u l l ?
n a m e  a b b r e v i a t i o n  E n g l i s h  e x p l a n a t i o n??????
??
????
P e k i n g  U n i v e r s i t y  N o .
3  h o s p i t a lFigure 1: Chinese abbreviation examplesthe abbreviation.
However, re-ordering of charac-ters as shown in the third example in Figure 1 wherecharacters ?n?
and ???
are swapped in the abbre-viation, also happens.There has been a considerable amount of researchon extracting full-name and abbreviation pairs inthe same document for obtaining abbreviations (Liand Yarowsky, 2008; Sun et al, 2006; Fu et al,2006).
However, generation of abbreviations givena full-name is still a non-trivial problem.
Changand Lai (Chang and Lai, 2004) have proposed usinga hidden Markov model to generate abbreviationsfrom full-names.
However, their method assumesthat there is no word-to-null mapping, which meansthat every word in the full-name has to contribute atleast one character to the abbreviation.
This assump-tion does not hold for organizations?
names whichhave many word skips in the abbreviation genera-tion.The CRF was first introduced to natural languageprocessing (NLP) by (Lafferty et al, 2001) and hasbeen widely used in word segmentation, part-of-speech (POS) tagging, and some other NLP tasks.In this paper, we convert the Chinese abbreviationgeneration process to a CRF tagging problem.
Thekey problem here is how to find a group of discrim-273inant and robust features.
After using the CRF, weget a list of abbreviation candidates with associateprobability scores.
We also use the prior condi-tional probability of the length of the abbreviationsgiven the length of the full-names to complement theCRF probability scores.
Such global information ishard to include in the CRF model.
In addition, weapply the full-name and abbreviation candidate co-occurrence statistics obtained on the web to increasethe correctness of the abbreviation candidates.2 Chinese Abbreviation IntroductionChinese abbreviations are generated by three meth-ods (Lee, 2005): reduction, elimination, and gener-alization.Both in the reduction and elimination methods,characters are selected from the full-name, and theorder of the characters is sometime changed.
Notethat this paper does not cover the case when the or-der is changed.
The elimination means that one ormore words in the full-name are ignored completely,while the reduction requires that at least one char-acter is selected from each word.
All the three ex-amples in Figure 1 are produced by the elimination,where at least one word is skipped.Generalization, which is used to abbreviate a listof similar terms, is usually composed of the numberof terms and a shared character across the terms.
Aexample is ?n?
(three forces) for ???????
(land force, sea force, air force).
This is themost difficult scenario for the abbreviations and isnot considered in this paper.3 CRF Model for Abbreviation Modeling3.1 CRF modelA CRF is an undirected graphical model and assignsthe following probability to a label sequence L =l1l2 .
.
.
lT , given an input sequence C = c1c2 .
.
.
cT ,P (L|C) = 1Z(C)exp(T?t=1?k?kfk(lt, lt?1, C, t))(1)Here, fk is the feature function for the k-th fea-ture, ?k is the parameter which controls the weightof the k-th feature in the model, and Z(C) is the nor-malization term that makes the summation of theprobability of all label sequences to 1.
CRF trainingis usually performed through the typical L-BFGS al-gorithm (Wallach, 2002) and decoding is performedby Viterbi algorithm (Viterbi, 1967).
In this paper,we use an open source toolkit ?crf++?.3.2 Abbreviation modeling as a taggingproblemIn order to use the CRF method in abbreviation gen-eration, the abbreviation generation problem wasconverted to a tagging problem.
The character isused as a tagging unit and each character in a full-name is tagged by a binary variable with the valuesof either Y or N: Y stands for a character used in theabbreviation and N means not.
An example is givenin Figure 2.???????
??
?/ N ?/ N ?/ N ?/ Y ?/ N ?/ Y ?/ NFigure 2: Abbreviation in the CRF tagging format3.3 Feature selection for the CRFIn the CRF method, feature function describesa co-occurrence relation, and it is defined asfk(lt, lt?1, C, t) (Eq.
1).
fk is usually a binary func-tion, and takes the value 1 when both observation ctand transition lt?1 ?
lt are observed.
In our ab-breviation generation model, we use the followingfeatures:1.
Current character The character itself is themost important feature for abbreviation as it will beeither retained or discarded.
For example, ???
(bu-reau) and ???
(institue), indicating a governmentdepartment, are very common characters used in ab-breviations.
When they appear in full-names, theyare likely to be kept in abbreviations.2.
Current word In the full name of ??I?????
(China Agricultural university), the word ??I?
(China) is usually ignored in the abbreviation,but the word ????
(agriculture) is usually kept.The length (the number of characters) is also an im-portant feature of the current word.3.
Position of the current character in the cur-rent word Previous work (Chang and Lai, 2004)showed that the first character of a word has highpossibility to form part of the abbreviation and thisis also true for the last character of a three-characterword.4.
Combination of feature 2. and 3. aboveCombination of the features 2 and 3 is expected toimprove the performance, since the position infor-274mation affects the abbreviation along with the cur-rent word.
For example, ending character in ????
(university) and that in ????
(research institute)have very different possibilities to be selected for ab-breviations.Besides the features above, we have examinedcontext information (previous word, previous char-acter, next character, etc.)
and other local featureslike the length of the word, but these features didnot improve the performance.
The reason may bedue to the sparseness of the training data.4 Improvement by a Length Model and aWeb Search Engine4.1 Length modelThere is a strong correlation between the length oforganizations?
full-names and their abbreviations.We use the length modeling based on discrete prob-ability of P (M |L), in which the variables M andL are lengths of abbreviations and full-names, re-spectively.
Since it is difficult to incorporate lengthinformation into the CRF model explicitly, we useP (M |L) to rescore the output of the CRF.In order to use the length information, we modelthe abbreviation process with two steps:?
1st step: evaluate the length in abbreviation ac-cording to the length model P (M |L);?
2nd step: choose the abbreviation, given thelength and full-name.We assume the following approximation:P (A|F ) ?
P (M |L) ?
P (A|M,F ) (2)in which variable A is the abbreviation and F is thefull-name; P (M |L) is the length model, and the sec-ond probability can be calculated according to theBayesian rule:P (A|M,F ) = P (A,M |F )P (M |F )= P (A,M |F )?length(A?
)=M P (A?,M |F )(3)It is obvious that P (A,M |F ) = P (A|F ) (as Acontains the information M implicitly) and P (A|F )can be obtained from the output of the CRF.4.2 Web search engineCo-occurrence of a full-name and an abbreviationcandidate can be a clue of the correctness of the ab-breviation.
We use the ?abbreviation candidate?+?full-name?
as queries and input them to the mostpopular Chinese search engine (www.baidu.com),and then we use the number of hits as the metricto perform re-ranking.
The hits is theoretically re-lated to the number of pages which contain both thefull-name and abbreviation.
The bigger the value ofhits, the higher probability that the abbreviation iscorrect.We then simply multiply the previous probabilityscore, obtained from Eq.
2, by the number of hitsand re-rank the top-30 candidates accordingly.There are some other ways to use information re-trieval methods (Mandala et al, 2000).
Our methodhas an advantage that the access load to the websearch engine is relatively small.5 Experiment5.1 Data introductionThe corpus we use in this paper comes from twosources: one is the book ?modern Chinese abbre-viation dictionary?
(Yuan and Ruan, 2002) and theother is the wikipedia.
Altogether we collected 1945pairs of organization full-names and their abbrevia-tions.The data is randomly divided into two parts, atraining set with 1298 pairs and a test set with 647pairs.
Table 1 shows the length mapping statisticsof the training set.
It can be seen that the averagelength of full-names is about 7.29.
We know that fora full-name with length N, the number of abbrevia-tion candidates is about 2N ?
2?N (exclude lengthof 0, 1, and N) and we can conclude that the averagenumber of candidates for organization names in thiscorpus is more than 100.5.2 ResultsThe abbreviation method described is part of aproject to develop a voice-based search application.For our name abbreviation system we plan to add 10abbreviation candidates for each organization nameinto the vocabulary of our voice search application,hence here we consider top-10 coverage.275length of length of abbreviationfull-name 2 3 4 5 >5 sum4 107 1 0 0 0 1085 89 140 0 0 0 2296 96 45 46 0 0 1877 60 189 49 16 0 3148 48 29 60 3 6 1469 10 47 35 12 2 10610 18 11 29 8 6 73others 21 43 38 17 14 133average length of the full-name 7.27average length of the abbreviation 3.01Table 1: Length statistics on the training set7RSFRYHUDJH)HDWXUH)HDWXUH)HDWXUH)HDWXUHFigure 3: Contribution of features in CRFFigure 3 shows the result for various combina-tions of features introduced in Section 3.3.Figure 4 displays the coverage results obtainedusing the CRF method and the improvements gainedfrom the inclusion of the length feature and the websearch hits.
As we can see the CRF gives a coverage79.9%.
Both length model and web search engineshow significant improvement over the CRF base-line and the coverage increases to 88.3%.7RSFRYHUDJH&5)&5)/HQJWK&5)/HQJWK:HEFigure 4: Results of different methods6 Conclusions and Future workThe CRF works well in generating abbreviations fororganization names, while both length model andweb search engine further improve the performance.We are going to perform word clustering or char-acter clustering to alleviate the data sparseness prob-lem.
Also we notice that multiple abbreviations forsingle full-name is very common, such as ??I?>??
(China central television) with abbrevi-ations ???
and ???.
We plan to collectmultiple abbreviations for reference.
After that weare going to combine the abbreviation modeling inthe voice search system to alleviate the weakness ofspeech recognition for unknown abbreviation words,which are unlikely to be correctly recognized due tothe out of vocabulary problem.ReferencesJing-shin Chang and Yu-Tso Lai 2004.
A PreliminaryStudy on Probabilistic Models for Chinese Abbrevia-tions.
Proceedings of ACL SIGHAN Workshop 2004,pages 9-16.Guohong Fu, Kang-Kwong Luke, GuoDong Zhou andRuifeng Xu 2006.
Automatic Expansion of Abbre-viations in Chinese News Text.
Lecture Notes in Com-puter Science, Washington, DC.John Lafferty, Andrew McCallum, and Fernando Pereira2001.
Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Sequence Data.,In Proceedings of International Conference on Ma-chine Learning 2001, pages 282-289Hiu Wing Doris Lee 2005.
A Study of Automatic Ex-pansion of Chinese Abbreviations.
MA Thesis, TheUniversity of Hong Kong.Zhifei Li and David Yarowsky.
2008.
UnsupervisedTranslation Induction for Chinese Abbreviations us-ing Monolingual Corpora.
Proceedings of ACL 2008,pages 425-433.Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka2000.
Query expansion using heterogeneous thesauri.,In Information Processing and Management Volume36 , Issue 3 2000,Pages 361 - 378Xu Sun, Houfeng Wang and Yu Zhang 2006.
Chi-nese Abbreviation-Definition Identification: A SVMApproach Using Context Information.
Lecture Notesin Computer Science,Volume 4182/2006, pages 530-536.Andrew J. Viterbi 1967.
Error Bounds for ConvolutionalCodes and an Asymptotically Optimum Decoding Al-gorithm.
in IEEE Transactions on Information Theory,Volume IT-13, in April, 1967,pages 260-269,Hanna Wallach 2002.
Efficient Training of ConditionalRandom Fields.
M. Thesis, University of Edinburgh,2002.Hui Yuan and Xianzhong Ruan 2002.
Modern Chineseabbreviation dictionary.
Yuwen press, Beijing, China.276
