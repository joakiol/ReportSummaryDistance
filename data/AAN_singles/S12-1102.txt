First Joint Conference on Lexical and Computational Semantics (*SEM), pages 684?688,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSoft Cardinality + ML: Learning Adaptive Similarity Functionsfor Cross-lingual Textual EntailmentSergio JimenezUniversidad Nacionalde Colombia, Bogota,Ciudad Universitariaedificio 453, oficina 220sgjimenezv@unal.edu.coClaudia BecerraUniversidad Nacionalde Colombia, Bogotacjbecerrac@unal.edu.coAlexander GelbukhCIC-IPNAv.
Juan Dios B?tiz,Av.
Mendiz?bal, Col.Nueva Industrial Vallejo,CP 07738, DF, M?xicogelbukh@gelbukh.comAbstractThis paper presents a novel approach for buildingadaptive similarity functions based on cardinality us-ing machine learning.
Unlike current approachesthat build feature sets using similarity scores, wehave developed these feature sets with the cardinal-ities of the commonalities and differences betweenpairs of objects being compared.
This approach al-lows the machine-learning algorithm to obtain anasymmetric similarity function suitable for direc-tional judgments.
Besides using the classic set cardi-nality, we used soft cardinality to allow flexibility inthe comparison between words.
Our approach usedonly the information from the surface of the text,a stop-word remover and a stemmer to address thecross-lingual textual entailment task 8 at SEMEVAL2012.
We have the third best result among the 29systems submitted by 10 teams.
Additionally, thispaper presents better results compared with the bestofficial score.1 IntroductionAdaptive similarity functions are those functions that, be-yond using the information of two objects being com-pared, use information from a broader set of objects(Bilenko and Mooney, 2003).
Therefore, the same sim-ilarity function may return different results for the samepair of objects, depending on the context of where theobjects are.
Adaptability is intended to improve the per-formance of the similarity function in relation to the taskin question associated with the entire set of objects.
Forexample, adaptiveness improves relevance of documentsretrieved for a query in an information retrieval task for aparticular document collection.In text applications there are mainly three methodsto provide adaptiveness to similarity functions: termweighting, adjustment or learning the parameters of thesimilarity function, and machine learning.
Term weight-ing is a common practice that assigns a degree of im-portance to each occurrence of a term in a text collec-tion (Salton and Buckley, 1988; Lan et al, 2005).
Sec-ondly, if a similarity function has parameters, these canbe adjusted or learned to adapt to a particular data set.Depending on the size of the search space defined bythese parameters, they can be adjusted either manuallyor using a technique of AI.
For instance, Jimenez etal.
manually adjusted a single parameter in the gener-alized measure of Monge-Elkan (1996) (Jimenez et al,2009) and Ristrad and Yanilios (1998) learned the costsof editing operations between particular characters forthe Levenshtein distance (1966) using HMMs.
Thirdly,the machine-learning approach aims to learn a similar-ity function based on a vector representation of texts us-ing a subset of texts for training and a learning func-tion (Bilenko and Mooney, 2003).
The three methodsof adaptability can also be used in a variety of combina-tions, e.g.
term weighting in combination with machinelearning (Debole and Sebastiani, 2003; Lan et al, 2005).Finally, to achieve adaptability, other approaches use datasets considerably larger, such as large corpora or the Web,e.g.
distributional similarity (Lee, 1999).In the machine-learning approach, a vector representa-tion of texts is used in conjunction with an algorithm ofclassification or regression (Alpaydin, 2004).
Each vec-tor of features ?f1, f2, .
.
.
, fm?
is associated to each pair?Ti, Tj?
of texts.
Thus, Bilenko et al (2003) proposed aset of features indexed by the data set vocabulary, simi-lar to Zanzotto et al, (2009) who used fragments of parsetrees.
However, a more common approach is to select asfeatures the scores of different similarity functions.
Usingthese features, the machine-learning algorithm discoversthe relative importance of each feature and a combina-tion mechanism that maximizes the alignment of the finalresult with a gold standard for the particular task.In this paper, we propose a novel approach to extractfeature sets for a machine-learning algorithm using car-684dinalities rather than scores of similarity functions.
Forinstance, instead of using as a feature the score obtainedby the Dice?s coefficient (i.e.
2?|Ti?Tj |/|Ti|+|Tj |), we use|Ti|, |Tj | and |Ti ?
Tj | as features.
The rationale behindthis idea is that despite the similarity scores being suitablefor learning a combined function of similarity, they hidethe information imbalance between the original pair oftexts.
Our hypothesis is that the information coded in thisimbalance could provide the machine-learning algorithmwith better information to generate a combined similar-ity score.
For instance, consider these pairs of texts: ?
?The beach house is white.
?, ?The house was completelyempty.?
?
and ?
?The house?, ?The beach house was com-pletely empty and isolated?
?.
Both pairs have the samesimilarity score using the Dice coefficient, but it is evi-dent that the latter has an imbalance of information lost inthat single score.
This imbalance of information is evenmore important if the task requires to identify directionalsimilarities, such as ?T1 is more similar to T2, than T2 isto T1?.However, unlike the similarity functions, which arenumerous, there is only one set cardinality.
This issuecan be addressed using the soft cardinality proposed byJimenez et al (2010), which uses an auxiliary function ofsimilarity between elements to make a soft count of theelements in a set.
For instance, the classic cardinality ofthe set A = { ?Sunday?, ?Saturday? }
is |A| = 2; and thesoft cardinality of the same set, using a normalized edit-distance as auxiliary similarity function, is |A|?sim = 1.23because of the commonalities between both words.
Fur-thermore, soft cardinality allows weighting of elementsgiving it additional capacity to adapt.We used the proposed approach to participate in thecross-lingual textual-entailment task 8 at SEMEVAL2012.
The task was to recognize bidirectional, forward,backward or lack of entailment in pairs of texts writtenin five languages.
We built a system based on the pro-posed method and the use of surface information of thetext, a stop-word remover and a stemmer.
Our systemachieved the third best result in official classification and,after some debugging, we are reporting better results thanthe best official scores.This paper is structured as follows.
Section 2 brieflydescribes soft cardinality and other cardinalities for textapplications.
Section 3 presents the proposed method.Experimental validation is presented in Section 4.
A briefdiscussion is presented in Section 5.
Finally, conclusionsare drawn in Section 6.2 Cardinalities for textCardinality is a measure of counting the number of el-ements in a set.
The cardinality of classical set theoryrepresents the number of non-repeated elements in a set.However, this cardinality is rigid because it counts in thesame manner very similar or highly differentiated ele-ments.
In text applications, text can be modeled as aset of words and a desirable cardinality function shouldtake into account the similarities between words.
In thissection, we present some methods to soften the classicalconcept of cardinality.2.1 Lemmatizer CardinalityThe simplest approach is to use a stemmer that collapseswords with common roots in a single lemma.
Considerthe sentence: ?I loved, I am loving and I will love you?.The plain word counting of this sentence is 10 words.
Theclassical cardinality collapses the three occurrences of thepronoun ?I?
giving a count of 8.
However, a lemmatizersuch as Porter?s stemmer (1980) also collapses the words?loved?, ?loving?
and ?love?
in a single lemma ?love?
fora count of 6.
Thus, when a text is lemmatized, it inducesa relaxation of the classical cardinality of a text.
In ad-dition, to provide corpus adaptability, a weighted versionof this cardinality can add weights associated with eachword occurrence instead of adding 1 for each word (e.g.tf-idf).2.2 LCS cardinalityLongest common subsequence (LCS) length is a measureof the commonalities between two texts, unlike set in-tersection, taking into account the order.
Therefore, acardinality function of a pair of texts A and B couldbe |A ?
B| = len(LCS(A,B)), |A| = len(A) and|B| = len(B).
Functions len(?)
and LCS(?, ?)
calcu-late length and LCS respectively, either in character orword granularity.2.3 Soft CardinalitySoft cardinality is a function that uses an auxiliary simi-larity function to make a soft count of the elements (i.e.words) in a set (i.e.
text) (Jimenez et al, 2010).
The aux-iliary similarity function can be any measure or metricthat returns scores in the interval [0, 1], with 0 being thelowest degree of similarity and 1 the highest (i.e.
identi-cal words).
Clearly, if the auxiliary similarity function isa rigid comparator that returns 1 for identical words and0 otherwise, the soft cardinality becomes the classic setcardinality.The soft cardinality of a set A = {a1, a2, .
.
.
, a|A|}can be calculated by the following expression: |A|?sim '?|A|i wai(?|A|j sim(ai, aj)p)?1.
Where sim(?, ?)
isthe auxiliary similarity function for approximate wordcomparison, wai are weights associated with each wordai, and p is a tuning parameter that controls the degreeof smoothness of the cardinality, i.e.
if 0 ?
p all ele-ments in a set are considered identical and if p??
softcardinality becomes classic cardinality.6852.4 Dot-product VSM ?Cardinality?Resemblance coefficients are cardinality-based simi-larity functions.
For instance, the Dice coefficientis the ratio between the cardinality of the intersec-tion divided by the arithmetic mean of individualcardinalities:2?|A?B|/|A|+|B|.
The cosine coefficient issimilar but instead of using the arithmetic mean it usesthe geometric mean: |A?B|/?|A|??|B|.
Furthermore, thecosine similarity is a well known metric used in the vec-tor space model (VSM) proposed by Salton et al (1975)cosine(A,B) =?wai?wbi??w2ai???w2bi.
Clearly, this expres-sion can be compared with the cosine coefficient inter-preting the dot-product operation in the cosine similar-ity as a cardinality.
Thus, the obtained cardinalities are:|A ?
B|vsm =?wpai ?
wpbi, |A|vsm =?w2pai and|B|vsm =?w2pbi .
The exponent p controls the effectof weighting providing no effect if 0?
p or emphasisingthe weights if p > 0.
In a similar application, Gonza-lez and Caicedo (2011) used p = 0.5 and normalizationjustified by the quantum information retrieval theory.3 Learning Similarity Functions fromCardinalitiesDifferent similarity measures use different knowledge,identify different types of commonalities, and compareobjects with different granularity.
In many of the auto-matic text-processing applications, the qualities of sev-eral similarity functions may be required to achieve thefinal task.
The combination of similarity scores with amachine-learning algorithm to obtain a unified effect fora particular task is a common practice (Bilenko et al,2003; Malakasiotis and Androutsopoulos, 2007; Malaka-siotis, 2009).
For each pair of texts for comparison, thereis provided a vector representation based on multiple sim-ilarity scores as a set of features.
In addition, a class at-tribute is associated with each vector which contains theobjective of the task or the gold standard to be learned bythe machine-learning algorithm.However, the similarity scores conceal important in-formation when the task requires dealing with directionalproblems, i.e.
whenever the order of comparing each pairof texts is related with the class attribute.
For instance,textual entailment is a directional task since it is neces-sary to recognize whether the first text entails the secondtext or vice versa.
This problem can be addressed us-ing asymmetric similarity functions and including scoresfor sim(A,B) and sim(B,A) in the resulting vector foreach pair ?A,B?.
Nevertheless, the similarity measuresthat are more commonly used are symmetric, e.g.
edit-distance (Levenshtein, 1966), LCS (Hirschberg, 1977),cosine similarity, and many of the current semantic re-latedness measures (Pedersen et al, 2004).
Although,there are asymmetric measures such as the Monge-Elkanmeasure (1996) and the measure proposed by Corley andMihalcea (Corley and Mihalcea, 2005), they are outnum-bered by the symmetric measures.
Clearly, this situationrestricts the use of the machine learning as a method ofcombination for directional problems.Alternatively, we propose the construction of a vectorfor each pair of texts using cardinalities instead of sim-ilarity scores.
Moreover, using cardinalities rather thansimilarity scores allows the machine-learning algorithmto discover patterns to cope with directional tasks.Basically, we propose to use a set with six features foreach cardinality function: |A|, |B|, |A ?
B|, |A ?
B|,|A?B| and |B ?A|.4 Experimental Setup4.1 Cross-lingual Textual Entailment (CLTE) TaskThis task consist of recognizing in a pair of topically re-lated text fragments T1 and T2 in different languages, oneof the following possible entailment relations: i) bidi-rectional T1 ?
T2 ?
T1 ?
T2, i.e.
semantic equiv-alence; ii) forward T1 ?
T2 ?
T1 : T2; iii) back-ward T1 ; T2 ?
T1 ?
T2; and iv) no entailmentT1 ; T2 ?
T1 : T2.
Besides, both T1 and T2 are as-sumed to be true statements; hence contradictory pairsare not allowed.Data sets consist of a collection of 1,000 text pairs(500 for training and 500 for testing) each one labeledwith one of the possible entailment types.
Four balanceddata sets were provided using the following languagepairs: German-English (deu-eng), French-English (fra-eng), Italian-English (ita-eng) and Spanish-English (spa-eng).
The evaluation measure for experiments was accu-racy, i.e.
the ratio of correctly predicted pairs by the totalnumber of predictions.
For a comprehensive descriptionof the task see (Negri et al, 2012).4.2 ExperimentsGiven that each pair of texts ?T1, T2?
are in different lan-guages, a pair of translations ?T t1 , Tt2?
were provided us-ing Google Translate service.
Thus, each one of the textpairs ?T1, T t2?
and ?Tt1 , T2?
were in the same language.Then, all produced pairs were pre-processed by remov-ing stop-words in their respective languages.
Finally, alltexts were lemmatized using Porter?s stemmer (1980) forEnglish and Snowball stemmers for other languages us-ing an implementation provided by the NLTK (Loper andBird, 2002).Then, different set of features were generated usingsimilarity scores or cardinalities.
While each symmet-ric similarity function generates 2 features i)sim(T1, T t2)and ii)sim(T t1 , T2), asymmetric functions generate twoadditional features iii)sim(T t2 , T1) and iv)sim(T2, Tt1).686On the other hand, each cardinality function generates12 features: i) |T1|, ii) |T t2 |, iii) |T1 ?
Tt2 |, iv) |T1 ?
Tt2 |,v) |T1 ?
T t2 |, vi) |Tt2 ?
T1|, vii) |Tt1 |, viii) |T2|, ix)|T t1 ?
T2|, x) |Tt1 ?
T2|, xi) |Tt1 ?
T2|, and xii) |T2 ?
Tt1 |.Various combinations of cardinalities, symmetric andasymmetric functions were used to generate the follow-ing feature sets:Sym.simScores: scores of the following symmetricsimilarity functions: Jaccard, Dice, and cosine coef-ficients using classical cardinality and soft cardinality(edit-distance as auxiliar sim.
function).
In addition, co-sine similarity, softTFIDF (Cohen et al, 2003) and edit-distance (total 18 features).Asym.LCS.sim: scores of the following asymmetricsimilarity functions: sim(T1, T2) = lcs(T1,T2)/len(T1)and sim(T1, T2) = lcs(T1,T2)/len(T2) at character level (4features).Classic.card: cardinalities using classical set cardinal-ity (12 features).Dot.card.w: dot-product cardinality using idf weightsas described in Section 2.4, using p = 1 (12 features).LCS.card: LCS cardinality at word-level using idfweights as described in Section 2.1 (12 features).SimScores: combined features sets fromSym.SimScores, Asym.LCS.sim and the general-ized Monge-Elkan measure (Jimenez et al, 2009) usingp = 1, 2, 3 (30 features).Dot.card.w.0.5: same as Dot.card.w using p = 0.5.Classic.card.w: classical cardinality using idf weights(12 features).Soft.card.w: soft cardinality using idf weights as de-scribed in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).The machine-learning classification algorithm for allfeature sets was SVM (Cortes and Vapnik, 1995) with thecomplexity parameter C = 1.5 and a linear polynomialkernel.
All experiments were conducted using WEKA(Hall et al, 2009).4.3 ResultsIn Semeval 2012 exercise, participants were given a par-ticular subdivision into training and test subsets for eachdata set.
For official results, participants received only thegold-standard labels for the subset of training, and accu-racies of each system in the test subset was measured bythe organizers.
In Table 1, the results for that particulardivision are shown.
At the bottom of that table, the of-ficial results for the first three systems are shown.
Oursystem, ?3rd.Softcard?
was configured using soft cardi-nality with edit-distance as auxiliary similarity functionand p = 2.
Erroneously, at the time of the submission,all texts in the 5 languages were lemmatized using an En-glish stemmer and stop-words in all languages were ag-gregated into a single set before the withdrawal.
In spiteof these bugs, our system was the third best score.FEATURES SPA ITA FRA DEU avg.Sym.simScores 0.404 0.410 0.410 0.410 0.409Asym.LCS.sim 0.490 0.492 0.482 0.474 0.485Classic.card 0.560 0.534 0.570 0.542 0.552Dot.card.w 0.562 0.568 0.550 0.548 0.557LCS.card 0.606 0.566 0.568 0.558 0.575SimScores 0.600 0.562 0.568 0.572 0.576Dot.card.w.0.5 0.584 0.574 0.586 0.572 0.579Classic.card.w 0.584 0.576 0.588 0.590 0.585Soft.card.w 0.598 0.602 0.624 0.604 0.607SEMEVAL 2012 OFFICIAL RESULTS1st.HDU.run2 0.632 0.562 0.570 0.552 0.5792nd.HDU.run1 0.630 0.554 0.564 0.558 0.5773rd.Softcard 0.552 0.566 0.570 0.550 0.560Table 1: Accuracy results for Semeval2012 task 8Soft.card.w 60.174(1.917)% imprv.
Sign.Sym.simScore 39.802(1.783)% 51.2% <0.001Asym.LCS.sim 48.669(1.820)% 23.6% <0.001Classic.card 55.278(2.422)% 8.9% 0.010Dot.card.w 54.906(2.024)% 9.6% 0.004LCS.card 55.131(2.471) % 9.1% 0.015SimScores 56.889(2.412) % 5.8% 0.124Dot.card.w.0.5 57.114(2.141)% 5.4% 0.059Classic.card.w 56.708(2.008)% 6.1% 0.017Table 2: Average accuracy comparison vs. Soft.card.w in 100runsTo compare our approach of using feature sets basedon soft cardinality versus other approaches, we gener-ated 100 random training-test subdivisions (50%-50%) ofeach data set.
The average results were compared andtested statistically with the paired T-tested corrected test.Results, deviations, the percentage of improvement, andits significance in comparison with the Soft.card.w sys-tem are shown in Table2.5 DiscusionResults in Table 2 show that our hypothesis that fea-ture sets obtained from cardinalities should outperformfeatures sets obtained from similarity scores was de-mostrated when compared versus similarity functions al-ternatively symmetrical or asymetrical.
However, whenour approach is compared with a feature set obtained bycombining symmetric and asymmetric functions, we ob-tained an improvement of 5.8% but only with a signif-icance of 0.124.
Regarding soft cardinality comparedto alternative cardinalities, soft cardinality outperformedothers in all cases with significance <0.059.6876 ConclusionsWe have proposed a new method to compose feature setsusing cardinalities rather than similarity scores.
Our ap-proach proved to be effective for directional text compar-ison tasks such as textual entailment.
Furthermore, thesoft cardinality function proved to be the best for obtain-ing such sets of features.AcknowledgmentsThis research was funded by the Systems and IndustrialEngineering Department, the Office of Student Welfareof the National University of Colombia, Bogot?, andthrought a grant from the Colombian Department forScience, Technology and Innovation Colciencias, proj.110152128465.
The second author recognizes the sup-port from Mexican Government (SNI, COFAA-IPN, SIP20113295, CONACYT 50206-H) and CONACYT?DSTIndia (proj.
?Answer Validation through Textual Entail-ment?
).ReferencesEthem Alpaydin.
2004.
Introduction to Machine Learning.MIT press.Mikhail Bilenko and Raymond J. Mooney.
2003.
Adaptive du-plicate detection using learnable string similarity measures.In Proc.
of the ninth ACM SIGKDD international conferenceon Knowledge discovery and data mining, Washington, D.C.Mikhail Bilenko, Raymond Mooney, William Cohen, PradeepRavikumar, and Stephen Fienberg.
2003.
Adaptive namematching in information integration.
IEEE Intelligent Sys-tems, 18(5):16?23.William W Cohen, Pradeep Ravikumar, and Stephen E Fien-berg.
2003.
A comparison of string distance metrics forName-Matching tasks.
In Proc.
of the IJCAI2003 Workshopon Information Integration on the Web II Web03.Courtney Corley and Rada Mihalcea.
2005.
Measuring the se-mantic similarity of texts.
In Proceedings of the ACL Work-shop on Empirical Modeling of Semantic Equivalence andEntailment, Stroudsburg, PA.Corinna Cortes and Vladimir N. Vapnik.
1995.
Support-Vectornetworks.
Machine Learning, 20(3):273?297.Franca Debole and Fabrizio Sebastiani.
2003.
Supervised termweighting for automated text categorization.
In Proc.
of the2003 ACM symposium on applied computing, New York,NY.Fabio A. Gonzalez and Juan C. Caicedo.
2011.
Quantum la-tent semantic analysis.
In Proc.
of the Third internationalconference on Advances in information retrieval theory.Mark Hall, Frank Eibe, Geoffrey Holmes, and BernhardPfahringer.
2009.
The WEKA data mining software: Anupdate.
SIGKDD Explorations, 11(1):10?18.Daniel S. Hirschberg.
1977.
Algorithms for the longest com-mon subsequence problem.
J. ACM, 24(4):664?675.Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, andFabio Gonzalez.
2009.
Generalized Monge-Elkan methodfor approximate text string comparison.
In ComputationalLinguistics and Intelligent Text Processing, volume 5449 ofLNCS, pages 559?570.Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.2010.
Text comparison using soft cardinality.
In String Pro-cessing and Information Retrieval, volume 6393 of LNCS,pages 297?302.Man Lan, Chew-Lim Tan, Hwee-Boon Low, and Sam-YuanSung.
2005.
A comprehensive comparative study on termweighting schemes for text categorization with support vec-tor machines.
In Special interest tracks and posters of the14th international conference on World Wide Web, NewYork, NY.Lillian Lee.
1999.
Measures of distributional similarity.
InProc.
of the 37th annual meeting of the Association for Com-putational Linguistics on Computational Linguistics, Col-lege Park, Maryland.Vladimir I. Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
Soviet PhysicsDoklady, 10(8):707?710.Edward Loper and Steven Bird.
2002.
NLTK: the natural lan-guage toolkit.
In In Proceedings of the ACL Workshop onEffective Tools andMethodologies for Teaching Natural Lan-guage Processing and Computational Linguistics, Philadel-phia, PA.Prodromos Malakasiotis and Ion Androutsopoulos.
2007.Learning textual entailment using SVMs and string similaritymeasures.
In Proc.
of the ACL-PASCALWorkshop on TextualEntailment and Paraphrasing, Stroudsburg, PA.Prodromos Malakasiotis.
2009.
Paraphrase recognition usingmachine learning to combine similarity measures.
In Proc.
ofthe ACL-IJCNLP 2009 Student Research Workshop, Strouds-burg, PA.Alvaro E. Monge and Charles Elkan.
1996.
The field matchingproblem: Algorithms and applications.
In Proc.
KDD-96,Portland, OR.Matteo Negri, Alessandro Marchetti, Yashar Mehdad, LuisaBentivogli, and Danilo Giampiccolo.
2012.
2012. semeval-2012 task 8: Cross-lingual textual entailment for content syn-chronization.
In In Proc.
of the 6th International Workshopon Semantic Evaluation (SemEval 2012), Montreal, Canada.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
WordNet::Similarity: measuring the relatedness ofconcepts.
In Proc.
HLT-NAACL?Demonstration Papers,Stroudsburg, PA.Martin Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 3(14):130?137.Eric S. Ristad and Peter N. Yianilos.
1998.
Learning stringedit distance.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 20(5):522?532.Gerard Salton and Christopher Buckley.
1988.
Term-weightingapproaches in automatic text retrieval.
Information Process-ing & Management, 24(5):513?523.Gerard Salton, Andrew K. C. Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.
Com-mun.
ACM, 18(11):613?620.Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-dro Moschitti.
2009.
A machine learning approach to tex-tual entailment recognition.
Natural Language Engineering,15(Special Issue 04):551?582.688
