Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 152?161,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsDetecting Levels of Interest from Spoken Dialog with MultistreamPrediction Feedback and Similarity Based Hierarchical Fusion LearningWilliam Yang WangDepartment of Computer ScienceColumbia UniversityNew York, NY 10027yw2347@columbia.eduJulia HirschbergDepartment of Computer ScienceColumbia UniversityNew York, NY 10027julia@cs.columbia.eduAbstractDetecting levels of interest from speakersis a new problem in Spoken Dialog Under-standing with significant impact on real worldbusiness applications.
Previous work has fo-cused on the analysis of traditional acous-tic signals and shallow lexical features.
Inthis paper, we present a novel hierarchical fu-sion learning model that takes feedback fromprevious multistream predictions of promi-nent seed samples into account and uses amean cosine similarity measure to learn rulesthat improve reclassification.
Our method isdomain-independent and can be adapted toother speech and language processing areaswhere domain adaptation is expensive to per-form.
Incorporating Discriminative Term Fre-quency and Inverse Document Frequency (D-TFIDF), lexical affect scoring, and low andhigh level prosodic and acoustic features, ourexperiments outperform the published resultsof all systems participating in the 2010 Inter-speech Paralinguistic Affect Subchallenge.1 IntroductionIn recent years, there has been growing interest inidentifying speakers?
emotional state from speech(Devillers and Vidrascu, 2006; Ai et al, 2006; Lis-combe et al, 2005).
For Spoken Dialog Systems(SDS), the motivation has been to provide users withimproved over-the-phone services by recognizingemotions such as anger and frustration and direct-ing users to a human attendant.
Other forms of par-alinguistic information which researchers have at-tempted to detect automatically include other classicemotions, charismatic speech (Biadsy et al, 2008),and deceptive speech (Hirschberg et al, 2005).More recently, the 2010 Interspeech ParalinguisicAffect Subchallenge sparked interest in detecting aspeaker?s level of interest (LOI), including both thespeaker?s interest in the topic and his/her willingnessto participating in the dialog (Schuller et al, 2010).Sensing users?
LOI in SDS should be useful in salesdomains, political polling, or service subscription.In this paper, we present a similarity-based hi-erarchical regression approach to predicting speak-ers?
LOI.
The system has been developed based onthe hierarchical fusion learning of lexical and acous-tic cues from speech.
We investigate the contri-bution of a novel source of information, Discrimi-native TFIDF; lexical affect scoring; and prosodicevent features.
Inspired by the successful use ofPseudo Relevance Feedback (Tao and Zhai, 2006)techniques in Information Retrieval and the cosinesimilarity measure (Salton, 1989) in Data Mining,we design a novel learning model which takes themultistream prediction feedback that is initially re-turned from seed samples 1 and uses a mean cosinesimilarity measure to calculate the distance betweenthe new instance and prominent seed data points inthe Euclidean Space.
We then add this similaritymeasure as a new feature to perform a reclassifi-cation.
Our main contributions in this paper are:(1) the novel Discriminative TFIDF approach forlexical modeling and keywords spotting; (2) usinglexical affect scoring and language modeling tech-niques to augment lexical modeling; (3) combin-1Seed samples are from a random small subset in the testset.152ing (1) and (2) with additional low-level prosodicfeatures together with voice quality and high-levelprosodic event features; and (4) introducing a mul-tistream prediction feedback and mean cosine simi-larity based fusion learning approach.We outline related work in Section 2.
The corpus,system features, and machine learning approachesare described in Section 3.
We describe our experi-mental results in Section 4 and conclude in Section5.2 Related WorkSchuller et al (2006) were among the first to studyLOI from conversational speech.
They framed thistask as either a three-way or binary classification,extracting standard acoustic features and building abag-of-words vector space model for lexical anal-ysis.
By linearly combining lexical features withacoustic features, they achieved high F-measureswhen using Support Vector Machine (SVM).
Sincea bag-of-words model is a naive model, there maybe more valuable lexical information that it cannotcapture.
Moreover, as lexical and acoustic featuresare extracted from different domains, a single layerlinear combination may not yield the optimal results.In 2010, Interspeech launched a ParalinguisticChallenge (Schuller et al, 2010) that included thetask of detecting LOI from speech as a subchallenge.Competitors were given conversational speech cor-pora with annotated LOI, baseline acoustic features,and two baseline results.
The evaluation metric usedfor the challenge was primarily the cross correlation2 (CC) measure (Grimm et al, 2008), with meanlinear error 3 (MLE) also taken into consideration.The baseline was built only on acoustic features, andthe CC and MLE for Training vs. Development setswere 0.604 and 0.118.
For the test data, CC andMLE scores of 0.421 and 0.146 were observed.Gajsek et al (2010) participated in this challengeand proposed the use of Gaussian Mixture Modelsas Universal Background Model (GMM-UBM) withrelevance MAP estimation for the acoustic data.This is based on the success of GMM-UBM mod-2Pearson product-moment correlation coefficient is a mea-sure of the linear dependence that is widely used in regressionsettings.3MLE is a regression performance measure for the mean ab-solute error between an estimator and the true value.eling in the speaker identification tasks (Reynolds etal., 2000).
They achieved CC and MLE of 0.630 and0.123 in the training vs. development condition, butCC and MLE of only 0.390 and 0.143 in the testingcondition.
This performance may have been due tothe fact that different subsets of the corpus includedifferent speakers: acoustic features alone may notbe robust enough to capture the speaker variation.Jeon et al (2010) approach won the 2010 Sub-challenge for this task.
In addition to the baselineacoustic features provided, they used term frequencyand a subjectivity dictionary to mine the lexical in-formation.
In addition to a linear combination ofall lexical and acoustic features, they designed a hi-erarchical regression framework with multiple levelof combinations.
Its first two combiners tackle theprediction problems from different acoustic classi-fiers and then uses a final stage SVM classifier tocombine the overall acoustic predictions with lexi-cal predictions to form the final output.
They reporta result of 0.622 for CC and 0.115 for MLE.
On thetest set, they report CC and MLE of 0.428 and 0.146respectively.3 Our SystemUnlike previous approaches, we emphasize lexicalmodeling, to counter problems of speaker variationin acoustic features (Jeon et al, 2010).
We proposean improved version of standard TFIDF (Spa?rckJones, 1972) ?
Discriminative TFIDF ?
whichcomputes the IDF score of the target word by dis-criminating its different mean LOI score tags duringtraining to produce more informative keyword spot-ting in testing.In addition to Discriminative TFIDF, we uti-lize the Dictionary of Affect in Language (DAL)(Whissell, 1989) to detect lexical affect and com-pute an utterance-level affect score.
To maximizethe coverage of lexical cues, we also train trigramlanguage models on the training data to capture con-textual information and use the test output log like-lihoods and perplexities as features.
Besides theselexical features and the 1582 baseline acoustic fea-tures from the Interspeech Paralinguistic Challenge,we extract 32 additional prosodic and voice qualityfeatures using Praat (Boersma, 2001).
In order tomodel sentence-level prosodic events, we use Au-153ToBI (Rosenberg, 2010) to extract pitch accent andphrase-based features.
These features are describedin detail in Section 3.2.The simplest approach to classification is to in-clude all features in a single classifier.
However,different features streams include different numberof features, extracted and represented in differentdomains.
The Sum Rule approach (Kittler et al,1998) is an early solution to this classifier combi-nation problem.
Instead, we train 1st-tier classi-fiers for each of the feature streams and then traina 2nd-tier classifier to weight the posterior predic-tions of the 1st-tier classifiers.
We further improvethis method by integrating a novel model which con-siders the 1st-tier multistream prediction feedbackfrom the seed samples and uses a mean cosine simi-larity method to measure the distance between a newinstance and prominent seed samples.
We use thissimilarity measure to improve classification.3.1 CorpusThe corpus we use in our experiments is the 2010Paralinguistic Challenge Affect Subchallenge cor-pus Technische Universta?t Munche?n Audiovisual In-terest Corpus (TUM AVIC), provided by Schuller(2010).
The corpus consists of 10 hours of audio-visual recordings of interviews in which an inter-viewer provides commercial presentations of vari-ous products to a subject.
The subject and inter-viewer discuss the product, and the subject com-ments on his/her interest in it.
Subjects were in-structed to relax and not to worry about politenessin the conversation.
21 subjects participated (11male, 10 female), including three Asians and the restof European background.
All interviews were con-ducted in English; while none of the subjects werenative speakers, all were fluent.
11 subjects wereyounger than 30; 7 were between 30-40; and 3 wereover 40.
The subject portions of the recordings weresegmented into speaker turns (continuous speech byone speaker with backchannels by the interviewerignored).
These were further segmented into sub-speaker turns at grammatical phrase boundaries suchthat each segment is shorter than 2sec.These smaller segments were annotated by fourmale undergraduate psychology students for subjectLOI, using a 5-point scale as follows: (-2) Disin-terest (subject is totally tired of discussing this topicand totally passive); (-1) Indifference (subject is pas-sive and does not want to give feedback); (0) Neu-trality (subject follows and participates in the dialog,but it is not recognized if she/he is interested in thetopic); (1) Interest (subject wants to talk about thetopic, follows the interviewer and asks questions);(2) Curiosity (subject is strongly interest in the topicand wants to learn more.)
A normalized mean LOIis then derived from mean LOI/2, to map the scoresinto [-1, +1].
(Note that no negative scores occurfor this corpus.)
In our experiments, we considerthe normalized mean LOI score as the label for eachsub-speaker turn segment; we refer to this as ?meanLOI?
below.
The corpus was divided for the Sub-challenge into training, development, and test cor-pora; we use these divisions in our experimens.3.2 Feature SetsTable 1 provides an overview of the feature sets inour system.Discriminative TFIDFIn the standard vector space model, each wordis associated with its Term Frequency (TF) in theutterance.
The Inverse Document Frequency (IDF)provides information on how rare the word is overall utterances.
The standard TFIDF vector of a termt in an utterance u is represented as V(t,u):V (t, u) = TF ?
IDF =C(t, u)C(v, u)?
log|U |?u(t)TF is calculated by dividing the number of occur-rences of term t in the utterance u by the total num-ber of tokens v in the utterance u. IDF is the log ofthe total number of utterances U in the training set,divided by the number of utterances in the trainingset in which the term t appears.
u(t) can be viewedas a simple function: if t appears in utterance u, thenit returns 1, otherwise 0.In Discriminative TFIDF we add additional infor-mation to the TFIDF metrics.
When calculating IDF,we weight each word by the distribution of its labelsin the training set.
This helps us to weight words bythe LOI of the utterances they are uttered in.
An in-tuitive example is this: Although the words ?chaos?and ?Audi?
both appear once in the corpus, the oc-currence of ?Audi?
is in an utterance with a MeanLOI score of 0.9, while ?chaos?
appears in an utter-ance with a label of 0.1.
A standard TFIDF approach154Feature Sets FeaturesDiscriminative TFIDF Sum of word-level Discriminative TFIDF scoresLexical Affect Scoring Sum of word-level lexical affect scoresLanguage Modeling Trigram language model log-likelihood and perplexityAcoustic Features 1582 acoustic features.
Detail see Schuller et.
al, (2010)Pulses # Pulses, # Periods, Mean Periods, SDev PeriodVoicing Fraction, # Voice Breaks, Degree, Voiced2total FramesJitter Local, Local (absolute), RAP, PPQ5Shimmer Local, Local (dB), APQ3, APQ5, APQ11Harmonicity Mean Autocorrelation, Mean NHR, Mean NHR (dB)Duration SecondsFundamental Frequency Min, Max, Mean, Median, SDev, MASEnergy Min, Max, Mean, SDevProsodic Events Pitch accents, intermediate phrase, and intonational boundaries.Table 1: Feature Sets.
RAP: Relative Average Perturbation.
PPQ5: five-point Period Perturbation Quotient.
APQn:n-point Amplitude Perturbation Quotient.
NHR: Noise-to-Harmonics Ratio.
MAS: Mean Absolute Slope.will give these two terms the same score.
To differ-entiate the importance of these two words, we defineour Discriminative TFIDF measure as follow:V (t, u) =C(t, u)C(v, u)?log|U |?u(t) ?
(1?
|MeanLOI|)Here, the Mean LOI score ranging from (0,1) isthe label of each utterance.
Instead of summingthe u(t) scores directly, we now assign a weight toeach utterance.
The weight is (1?
|MeanLOI|) inour task.
The overall IDF score of words importantto identifying the LOI of an utterance will thus beboosted, as the denominator of the IDF metric de-creases compared to the standard TFIDF.
Discrimi-native TFIDF can be viewed as a generalized versionof Delta TFIDF (Martineau and Finin, 2009) that canbe used in various regression settings.Wang and McKeown (2010) show that addingPart-of-Speech (POS) information to text can behelpful in similar classification tasks.
So we havealso used the Stanford POS tagger (Toutanova andManning, 2000) to tag these transcripts before cal-culating the Discriminative TFIDF score.Lexical Affect ScoringWhissell?s Dictionary of Affect in Language(DAL) (Whissell, 1989) attempts to quantify emo-tional language by asking raters to judge 8742 wordscollected from various sources including college es-says, interviews, and teenagers descriptions of theirown emotional state.
Its pleasantness (EE) score in-dicates the negative or positive valence of a word,rated on a scale from 1 to 3.
For example, ?aban-don?
scores 1.0, implying a fairly low level of pleas-antness.
A previous study (Agarwal et al, 2009)notes that one of the advantages of this dictionaryis that it has different scores for various forms of aroot word.
For example, the words ?affect?
and ?af-fection?
have very different meanings; if they weregiven the same score, the lexical affect quantifica-tion might not be discriminative.
To calculate anutterance?s lexical affect score, we first remove thestopwords and then sum up 4 the EE score of eachword in the utterance.Statistical Language ModelingIn order to capture the contextual information andmaximize the use of lexical information, we alsotrain a statistical language model to augment theDiscriminative TFIDF and lexical affect scores.
Wetrain trigram language models on the training setusing the SRI Language Modeling Tookit (Stolcke,2002).
In the testing stage, the log likelihood andperplexity scores are used as language modeling fea-tures.
Due to the data sparsity issue, we are not ableto train language models on subsets of training datathat correspond to different LOI scores.4We have experimented with Min, Max and Mean scores,but the results were poor.155Acoustic, Prosodic and Voice Quality FeaturesAs noted above, the TUM AVIC corpus includesacoustic features (Schuller et al, 2010) for all of thedata sets.
These include: PCM loudness, MFCC[0-14], log Mel Frequency Band[0-7], Line SpectralPairs Frequency [0-7], F0 by Sub-Harmonic Sum.,F0 Envelope, Voicing Probability, Jitter Local, Jit-ter Difference of Difference of Periods, and Shim-mer local.
We have extracted an additional 32 stan-dard prosodic and voice quality features to aug-ment these, including Glottal Pulses, Voicing, Jitter,Shimmer, Harmonicity, Duration, Fundamental Fre-quency, and Energy (See Table 1).Prosodic Event FeaturesTo examine the contribution of higher-levelprosodic events, we have also experimented withAuToBI (Rosenberg, 2010) to automatically de-tect pitch accents, word boundaries, intermedi-ate phrase boundaries, and intonational bound-aries in utterances.
AuToBI requires annotatedword boundary information; since we do not havehand-annotated boundaries, we use the Penn Pho-netics Lab Forced Aligner (Yuan and Liberman,2008) to align each utterance with its transcription.We use AuToBI?s models, which were trained onthe spontaneous speech Boston Directions Corpus(BDC) (Hirschberg and Nakatani, 1996), to identifyprosodic events in our corpus.3.3 Fusion Learning ApproachesAssuming that our various lexical, acoustic andprosodic feature streams are informative to some ex-tent when tested separately, we want to combine in-formation from the streams in different domains toimprove prediction.
We experimented with severalapproaches, including Bag-of-Features, Sum Rulecombination, Hierarchical Fusion, and a new ap-proach.
We present here results of each on our LOIprediction task.
In the Bag-of-Features approach,a simple classification method includes all featuresin a single classifier.
A potential problem with thismethod is that, when combining 1582 acoustic fea-tures with 10 lexical features, the classifier will treatthem equally, so potentially more useful lexical fea-tures will not be evaluated properly.
A second prob-lem is that our features are extracted from differ-ent domains using different methods, and normal-ization across domains is not possible in a bag-of-features classification/regression approach.
Anotherpossible approach is the Sum Rule Combiner, whichuses product or sum rules to combine the predictionsfrom 1st-tier classifiers.
Kittler et al (1998) showthat the Sum Rule approach outperforms the productrule, max rule and mean rule approaches when com-bining classifiers.
Their sensitivity analysis showsthat this approach is most resilient to estimation er-rors.A third method of combining features is the Hier-archical Fusion approach of fusing multistream in-formation, which involves multiple classifiers andperforms classification/regression in multiple stages.This can be implemented by first training 1st-tierclassifiers for each single stream of features, collect-ing the predictions from these classifiers, and train-ing a 2nd-tier supervector classifier to weight the im-portance of predictions from the different streamsand make a final prediction.
The rationale behindthis approach is to solve the cross-domain issue byletting the 2nd-tier classifier weight the streams, asthe predictions from 1st-tier classifiers will be in aunified/normalized form (e.g.
0 to 1 in this task).The Multistream Prediction Feedback and MeanCosine Similarity based Hierarchical FusionOur Multistream Prediction Feedback and MeanCosine Similarity based Hierarchical Fusion ap-proach combines a similarity based two-stage ap-proach with a multistream feedback approach.
Fig-ure 1 shows the architecture of this system.
It isbased on the intuition that, if we can identify theprominent samples (e.g.
the samples that all 1st-tierclassifiers assign high average prediction scores),then we can measure the average distance betweena new sample and all these prominent samples in theEuclidean Space.
Furthermore, we can use this av-erage distance (average similarity) as a new featureto improve the 2nd-tier classifier?s final prediction.To implement this process, we first train five1st-tier Additive Logistic Regression (Friedman etal., 2000) classifiers and a Random Subspace metalearning (Ho, 1998) 1st-tier classifier (for the acous-tic stream), using six different feature streams in ourtraining data.
In the testing stage, we use a randomsubset of the test set as seed samples.
Next, we runthe seed samples for each of these 1st-tier classifiers156SeedSamplesDiscriminativeTFIDF2nd-TierClassifier:RBF Kernel SVM1st-TierAddictive Regression andRandom SubspaceClassifiers:NewSamplesLexical AffectScoringLanguageModelingProsodic andVoice QualityAcousticProsodic Events1st-Tier Predictions (seed)S1: 0.8, 0.9, 0.6, 0.5, 0.7, 0.8S2: 0.3, 0.5, 0.4, 0.3, 0.2, 0.4S3: 0.4, 0.1, 0.3, 0.3, 0.1, 0.5????
?Maxn (Mean(Si))Top-NProminent SamplesAvg.
Cosine Similarly1st-Tier Predictions (new)S4: 0.7, 0.8, 0.6, 0.5, 0.8, 0.8, 99%S5: 0.6, 0.5, 0.4, 0.3, 0.7, 0.4, 72%????
?Final PredictionFigure 1: The Overview of Multistream Prediction Feedback and Mean Cosine Similarity based HierarchicalFusion Learningto obtain prediction scores ranging from 0 to 1.
Now,we take the mean of these predicted scores for eachsample, and use the following method to select thetop n samples from the seed samples S as ?promi-nent samples?
:Prominent(S, n) = Maxn(Mean(S))Recall that the cosine similarity (Salton, 1989) oftwo utterances Ui, Uj in the vector-space model is:cos(Ui, Uj) =Ui ?
Uj||Ui||2 ?
||Uj ||2where ???
indicates ?dot product?.
Now, given ourhypothesized prominent samples, for each of thesesamples and new samples, we choose the originalDiscriminative TFIDF, Lexical Affect Scoring, Lan-guage Modeling, Prosodic and Voice Quality, andProsodic Event features as k vectors to represent allthe samples in Euclidean Space.
The reason we dropthe acoustic features from the vector space model isbecause of the dimensionality issue ?
1582 acous-tic features.
We substitute our 32 standard prosodicfeatures instead.
Now we use the mean cosine simi-larity score to represent how far a new sample Un isfrom the prominent samples US in the space:Sim(Un, US) = Mean??
?ki=1 Vn ?
Vs?
?ki=1 V2n ??
?ki=1 V2s?
?In the next step, we add this mean cosine sim-ilarity measure as a new feature and include it inthe 2nd-tier classifier for reclassification.
Now, inthe reclassification stage, all 1st-tier feature streampredictions will be re-weighted by the new 2nd-tierclassifier that incorporated with Multistream Feed-back information.The reason why the Multistream Prediction Feed-back is useful in this task is that, like many spokenlanguage understanding tasks, in LOI detection, ifwe have a different set of speakers with differentgenders, ages, and speaker styles, the overall featuredistribution for lexical, prosodic, and acoustic cuesin the test set can be very different from the trainingset.
Traditional speaker adaptation techniques typi-157cally focus only on the acoustic stream and may bevery expensive to perform.
So, by extracting moreknowledge about the lexical, prosodic, and acousticfeatures distributions in test set using our novel ap-proach, we will have a better understanding aboutthe skewed distributions in the test set.
In addition,our approach is inexpensive and does not require ex-tra unlabeled data.4 Experiments and ResultsWe conduct our experiments in three parts.
First, weexamine how well the Discriminative TFIDF featureperforms, compared with standard TFIDF feature.Secondly, we look at how different feature sets influ-ence our results.
For the first two parts, we evaluateour features using the Subchallenge training vs. de-velopment sets only.
Finally, we compare our sim-ilarity based multistream fusion feedback approachto other feature-combining approaches.
We exam-ine our final system first comparing training vs. de-velopment performance, and then combined trainingand development sets vs. the test set.
WEKA (Wit-ten and Frank, 2005) and LIBSVM (Chang and Lin,2001) are used for regression.4.1 TFIDF v.s.
Discriminative TFIDFMethod CC MLETFIDF 0.296 0.142D-TFIDF 0.368 0.140S-D-TFIDF 0.381 0.136Table 2: Single TFIDF Feature Stream Single Re-gression Results (Train vs.
Develop, Additive Logis-tic Regression).
D-TFIDF: Discriminative TFIDF.
S-D-TFIDF: the POS tagged version of D-TFIDF.
CC: CrossCorrelation.
MLE: Mean Linear Error.When working with the training and develop-ment sets, we are able to access the label and tran-scriptions of each set to calculate the Discrimina-tive TFIDF scores.
For the testing scenario dis-cussed in in Section 4.3, we do not have these anno-tations.
So, we redefine the task as a keyword spot-ting task, where we can use the identified keywordsin the training and development sets as keyword fea-tures in testing.
We also sum up the word-levelTFIDF scores and use the sentence-level TFIDF asa single feature for the classification experiment.The regression algorithm we use is Additive Logis-tic Regression with 50 iterations.
Table 2 showshow different approaches perform in the experiment.We see that the Syntactic Discriminative TFIDF ap-proach is much more informative than the standardTFIDF approach.
Note that, after calculating theglobal IDF score, the standard TFIDF approach se-lects 732 terms as top-1 level keywords.
In contrast,our Discriminative TFIDF has stronger discrimina-tive power and picks a total number of 59 truly rareterms as top-1 level keywords.4.2 Regression with Different Feature StreamsTable 3 shows performance using different featurestreams in our system.
We see that the acousticFeature Streams CC MLES-D-TFIDF 0.394 0.132Language Modeling 0.404 0.141Prosodic Events 0.458 0.133Lexical Affect Scoring 0.459 0.132Standard Prosody + VQ 0.591 0.122Acoustic 0.607 0.118Multistream Feedback (n=3) 0.234 0.150Multistream Feedback (n=10) 0.262 0.149Multistream Feedback (n=20) 0.290 0.146Table 3: Comparing Contributions of Different Fea-ture Streams in the 2nd-tier Classifier (Training vs. De-velopmen, Random Subspace for the 1st-tier classifier ofAcoustic Stream, and Additive Logistic Regression forother 1st-tier classifiers.
Radial Basis Function (RBF)Kernel SVM as 2nd-tier Classifier.)
S-D-TFIDF: the POStagged version of D-TFIDF.
VQ: Voice Quality.
n: Top-nFeedback.
CC: Cross Correlation.
MLE: Mean LinearError.and prosodic features are the dominating features inthis task.
The Prosodic Events feature stream alsoemerges as a new informative high-level prosodicfeature in this task.When testing the multistream feedback informa-tion as a single feature stream, we see in the bottomhalf of Table 3 that CC and MLE are improved whenwe increase the number of prominent samples.
Dis-criminative TFIDF and Language Modeling are also158important, as seen from these results, but the Lexi-cal Affect Scoring feature performs best among thelexical features in this task.
We suspect that the rea-son may be a data sparsity issue, as we do not have alarge amount of data for training robust global Dis-criminative IDF scores, language models, and thefeedback stream.
In contrast, the DAL is trained onmuch larger amounts of data.4.3 Comparing with State-of-the-Art SystemsTable 4 compares our approach to alternative learn-ing approaches.
The first half of this table reportsresults on training vs. development sets, and the sec-ond half compares combined training and develope-men vs. test set result.Method CC MLEShuller et al,(2010) 0.604 0.118Jeon et al, (2010) 0.622 0.115Gajsek et al (2010) 0.630 0.123Bag-of-features Fusion 0.602 0.118Sum Rule Combination 0.617 0.117SVM Hierarchical Fusion 0.628 0.115Feedback + Hierarchical Fusion 0.640 0.113Gajsek et al (2010) 0.390 0.143Shuller et al,(2010) 0.421 0.146Jeon et al, (2010) 0.428 0.146Bag-of-features Fusion 0.420 0.145Sum Rule Combination 0.422 0.138SVM Hierarchical Fusion 0.450 0.131Feedback + Hierarchical Fusion 0.480 0.131Table 4: Comparing Different Systems.
Above: Train-ing vs. Development.
Bottom: Combined Training+ De-velopment vs. Test.
CC: Cross Correlation.
MLE: MeanLinear Error.Note that, in order to transcribe the test data, wehave trained a 20 Gaussian per state 39 MFCC Hid-den Markov Model speech recognizer with HTK, us-ing the training and development sets together withTIMIT (Fisher et al, 1986), the Boston DirectionsCorpus (BDC) (Hirschberg and Nakatani, 1996),and the Columbia Game Corpus (Hirschberg et al,2005).
The word error rate (WER) is 29% on thedevelopment set.Note that a Bag-of-Features approach combin-ing all features results in poorer performance thanthe use of acoustic features alone.
The Sum Ruleapproach improves over this method by achievingCC score of 0.422.
Although the improvement ofCC seems small, it is extremely statistically signifi-cant (Paired t-test with two-tailed P-value less than0.0001), comparing to the Bag-of-features model.However, when using the SVM as the 2nd-tier su-pervector classifier to weight different predictionstreams, we achieve 0.628 CC and 0.115 MLE intraining vs. development data, and 0.450 CC and0.131 MLE on the test set; this result is significantlydifferent from the Bag-of-features baseline (pairedt-test, p < 0.0001), but it is not significantly differ-ent from the Sum Rule Combination approach.Augmenting the SVM hierarchical fusion learn-ing approach with multistream feedback, we observea significant improvement over all other systems andmethods.
We obtain a final CC of 0.480 and MLE of0.131 in the test mode, which is sigificantly differ-ent from the Bag-of-features approach (paired t-testp < 0.0001), but does not differ significantly fromthe SVM hierarchical fusion approach.5 ConclusionDetecting levels of interest from speakers is an im-portant problem for Spoken Dialog Understanding.While earlier work, done in the 2010 InterspeechParalinguistic Affect Subchallenge, employing tra-ditional acoustic features and shallow lexical fea-tures, achieved good results, our new features ?Discriminative TFIDF, lexical affect scoring, lan-guage modeling, prosodic event ?
when used withstandard prosodic features and our new MultistreamPrediction Feedback and Mean Cosine Similarityheuristic-based Hierarchical Learning method im-proves over all published results on the LOI cor-pus.
Our method is domain-independent and canbe adapted to other speech and language process-ing areas where domain adaptation is expensive toperform.
In the future, we would like to experimentwith different distributional similarity measures andbootstrapping strategies.159AcknowledgmentsThe first author was funded by Kathleen McKeownwhile conducting the research.
We would also liketo thank Andrew Rosenberg and three anonymousreviewers for their useful comments.ReferencesAgarwal, Apoorv and Biadsy, Fadi and Mckeown, Kath-leen R. 2009.
Contextual Phrase-Level Polarity Anal-ysis Using Lexical Affect Scoring And Syntactic N-Grams.
in EACL 2009.Ai, Hua and Litman, Diane J. and Forbes-Riley, Kate andRotaru, Mihai and Tetreault, Joel and Purandare, Am-ruta 2006.
Using System and User Performance Fea-tures to Improve Emotion Detection in Spoken Tutor-ing Dialogs.
in INTERSPEECH 2006.Biadsy, Fadi and Rosenberg, Andrew and Carlson, Rolfand Hirschberg, Julia and Strangert, Eva.
2008.
ACross-Cultural Comparison of American, Palestinian,and Swedish Perception of Charismatic Speech.
inProceedings of the Speech Prosody 2008.Boersma, Paul.
2001.
Praat, a system for doing phonet-ics by computer.
in Glot International.Chang,Chih-Chung and Lin, Chih-Jen.
2001.
LIBSVM:a library for support vector machines.
Software avail-able at www.csie.ntu.edu.tw/?
cjlin/libsvm.Devillers, Laurence and Vidrascu, Laurence 2006.
Real-life Emotions Detection with Lexical and Paralinguis-tic Cues on Human-Human Call Center Dialogs.
inINTERSPEECH 2006.Fisher, William M. and Doddington, George R. andGoudie-Marshall, Kathleen M. 1986.
The DARPASpeech Recognition Research Database: Specifica-tions and Status.
in DARPA Workshop on SpeechRecognition.Friedman, Jerome and Hastie, Trevor and Tibshirani,Robert.
2000.
Additive logistic regression: a statis-tical view of boosting.
in Ann.
Statist..Gajs?ek, Rok and Z?ibert, Janez and Justin, Tadej andS?truc, Vitomir and Vesnicer, Bos?tjan and Mihelic?,France.
2010.
Gender and Affect Recognition Basedon GMM and GMMUBM modeling with relevanceMAP estimation.
in INTERSPEECH 2010.Grimm, Michael and Kroschel, Kristian and Narayana,Shrikanth.
2008.
The Vera am Mittag German Audio-Visual Emotional Speech Database.
in IEEE ICME.Hirschberg, Julia and Nakatani, Christine H. 1996.
Aprosodic analysis of discourse segments in direction-giving monologues.
in ACL 1996.Hirschberg, Julia and Benus, Stefan and Brenier, JasonM.
and Enos, Frank and Friedman, Sarah and Gilman,Sarah and Gir, Cynthia and Graciarena, Martin andKathol, Andreas and Michaelis, Laura.
2005.
Distin-guishing Deceptive from Non-Deceptive Speech.
inINTERSPEECH 2005.Ho, Tin Kam.
1998.
The Random Subspace Method forConstructing Decision Forests.
IEEE Transactions onPAMI.Jeon, Je Hun and Xia, Rui and Liu, Yang.
2010.
Level ofInterest Sensing in Spoken Dialog Using Multi-levelFusion of acoustic and Lexical Evidence.
in INTER-SPEECH 2010.Kittler, Josef and Hatef, Mohamad and Duin, Robert P.W.
and Matas, Jiri.
1998.
On combining classifiers.IEEE Transactions on PAMI.Laskowski, Kornel and Burger, Susanne.
2007.
Analysisof the Occurrence of Laughter in Meetings.
in INTER-SPEECH 2007.Liscombe, Jackson and Hirschberg, Julia and Venditti,Jennifer J.. 2005.
Detecting Certainness in SpokenTutorial Dialogues.
in Eurospeech.Martineau, Justin and Finin, Tim.
2009.
Delta TFIDF:An Improved Feature Space for Sentiment Analysis.in ICWSM.Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn,Robert B.
2000.
Speaker verication using adaptedgaussian mixture models.
in Digital Signal Process-ing.Rosenberg, Andrew.
2010.
AuToBI - A Tool for Auto-matic ToBI Annotation.
in INTERSPEECH 2010.Salto, Gerard 1989.
Automatic Text Processing: TheTransformation, Analysis, and Retrieval of Informa-tion by Computer.Schuller, Bjo?ern, and Ko?hler, Niels and Mu?eller, Ronaldand Rigoll, Gerhard.
2006.
Recognition of Interestin Human Conversational Speech.
in INTERSPEECH2006.Schuller, Bjo?ern, and Steidl, Stefan and Batliner, An-ton and Burkhardt, Felix and Devillers, Laurence andMu?eller, Christian and Narayanan, Shrikanth.
2010.The INTERSPEECH 2010 Paralinguistic Challenge.in INTERSPEECH 2010.Spa?rck Jones, Karen.
1972.
A statistical interpretationof term specificity and its application in retrieval.
inJournal of Documentation.Stolcke, Andreas.
2002.
SRILM-an extensible languagemodeling toolkit.
in ICSLP 2002.Toutanova, Kristina and Manning, Christopher D..2000.
Enriching the Knowledge Sources Used ina Maximum Entropy Part-of-Speech Tagger.
inEMNLP/VLC-2000.Tao, Tao and Zhai, ChengXiang 2006.
Regularized esti-mation of mixture models for robust pseudo-relevancefeedback.
in SIGIR 2006.160Wang, William Yang and McKeown, Kathleen.
2010.?Got You!?
: Automatic Vandalism Detection inWikipedia with Web-based Shallow Syntactic-Semantic Modeling.
in COLING 2010.Wang, Chingning and Zhang, Ping and Choi, Risook andDEredita, Michael.
2002.
Understanding consumersattitude toward advertising.
in Eighth Americas conf.on Information System.Witten, Ian H. and Frank, Eibe 2005.
Data mining:Practical machine learning tools and techniques, 2ndEdition.
San Francisco: Morgan Kaufmann.Whissell, Cynthia.
1989.
The Dictionary of Affect inLanguage.
in R. Plutchik and H. Kellerman, Editors,Emotion: Theory Research and Experience.Yuan, Jiahong and Liberman, Mark.
2008.
Speaker iden-tification on the SCOTUS corpus.
in Proceedings ofacoustics ?08.161
