Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183?1193,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsNouns are vectors, adjectives are matrices:Representing adjective-noun constructions in semantic spaceMarco Baroni and Roberto ZamparelliCenter for Mind/Brain Sciences, University of TrentoRovereto (TN), Italy{marco.baroni,roberto.zamparelli}@unitn.itAbstractWe propose an approach to adjective-nouncomposition (AN) for corpus-based distribu-tional semantics that, building on insightsfrom theoretical linguistics, represents nounsas vectors and adjectives as data-induced (lin-ear) functions (encoded as matrices) overnominal vectors.
Our model significantly out-performs the rivals on the task of reconstruct-ing AN vectors not seen in training.
A smallpost-hoc analysis further suggests that, whenthe model-generated AN vector is not simi-lar to the corpus-observed AN vector, this isdue to anomalies in the latter.
We show more-over that our approach provides two novelways to represent adjective meanings, alter-native to its representation via corpus-basedco-occurrence vectors, both outperforming thelatter in an adjective clustering task.1 IntroductionAn influential approach for representing the mean-ing of a word in NLP is to treat it as a vectorthat codes the pattern of co-occurrence of that wordwith other expressions in a large corpus of language(Sahlgren, 2006; Turney and Pantel, 2010).
Thisapproach to semantics (sometimes called distribu-tional semantics) naturally captures word cluster-ing, scales well to large lexicons and doesn?t re-quire words to be manually disambiguated (Schu?tze,1997).
However, until recently it has been limited tothe level of content words (nouns, adjectives, verbs),and it hasn?t tackled in a general way compositional-ity (Frege, 1892; Partee, 2004), that crucial propertyof natural language which allows speakers to de-rive the meaning of a complex linguistic constituentfrom the meaning of its immediate syntactic subcon-stituents.Formal semantics (FS), the research programstemming from Montague (1970b; 1973), has oppo-site strengths and weaknesses.
Its core semantic no-tion is the sentence, not the word; at the lexical level,it focuses on the meaning of function words; oneof its main goals is to formulate recursive composi-tional rules that derive the quantificational propertiesof complex sentences and their antecedent-pronoundependencies.Given its focus on quantification, FS treats themeanings of nouns and verbs as pure extensions:nouns and (intransitive) verbs are properties, andthus denote sets of individuals.
Adjectives are alsooften assumed to denote properties: in this viewredadj would be the set of ?entities which are red?,plasticadj , the set of ?objects made of plastic?, andso forth.
In the simplest case, the meaning of an at-tributive adjective-noun (AN) constituent can be ob-tained as the intersection of the adjective and nounextensions A?N:[ red car ] = {.
.
.
red objects.
.
. }
?
{.
.
.
cars.
.
.
}However, the intersective method of combinationis well-known to fail in many cases (Kamp, 1975;Montague, 1970a; Siegel, 1976): for instance, afake gun is not a gun.
Even for red, the manner inwhich the color combines with a noun will be dif-ferent in red Ferrari (the outside), red watermelon(the inside), red traffic light (the signal).
These prob-lems have prompted a more flexible FS representa-tion for attributive adjectives ?
functions from themeaning of a noun onto the meaning of a modifiednoun (Montague, 1970a).
This mapping could nowbe sensitive to the particular noun the adjective re-ceives, and it does not need to return a subset of the1183original noun denotation (as in the case of fake N).However, FS has nothing to say on how these func-tions should be constructed.In the last few years there have been attempts tobuild compositional models that use distributionalsemantic representations as inputs (see Section 2 be-low), most of them focusing on the combination of averb and its arguments.
This paper addresses insteadthe combination of nouns and attributive adjectives.This case was chosen as an interesting testbed be-cause it has the property of recursivity (it applies inblack dog, but also in large black dog, etc.
), and be-cause very frequent adjectives such as different areat the border between content and function words.Following the insight of FS, we treat attributive ad-jectives as functions over noun meanings; however,noun meanings are vectors, not sets, and the func-tions are learnt from corpus-based noun-AN vectorpairs.Original contribution We propose and evaluate anew method to derive distributional representationsfor ANs, where an adjective is a linear function froma vector (the noun representation) to another vector(the AN representation).
The linear map for a spe-cific adjective is learnt, using linear regression, frompairs of noun and AN vectors extracted from a cor-pus.Outline Distributional approaches to composi-tionality are shortly reviewed in Section 2.
In Sec-tion 3, we introduce our proposal.
The experimen-tal setting is described in Section 4.
Section 5 pro-vides some empirical justification for using corpus-harvested AN vectors as the target of our functionlearning and evaluation benchmark.
In Section 6, weshow that our model outperforms other approachesat the task of approximating such vectors for unseenANs.
In Section 7, we discuss how adjectival mean-ing can be represented in our model and evaluate thisrepresentation in an adjective clustering task.
Sec-tion 8 concludes by sketching directions for furtherwork.2 Related workThe literature on compositionality in vector-basedsemantics encompasses various related topics, someof them not of direct interest here, such as how toencode word order information in context vectors(Jones and Mewhort, 2007; Sahlgren et al, 2008)or sophisticated composition methods based on ten-sor products, quantum logic, etc., that have not yetbeen empirically tested on large-scale corpus-basedsemantic space tasks (Clark and Pulman, 2007;Rudolph and Giesbrecht, 2010; Smolensky, 1990;Widdows, 2008).
Closer to our current purposes isthe general framework for vector composition pro-posed by Mitchell and Lapata (2008), subsumingvarious earlier proposals.
Given two vectors u andv, they identify two general classes of compositionmodels, (linear) additive models:p = Au+Bv (1)where A and B are weight matrices, and multiplica-tive models:p = Cuvwhere C is a weight tensor projecting the uv tensorproduct onto the space of p. Mitchell and Lapata de-rive two simplified models from these general forms.Their simplified additive model p = ?u+?v was acommon approach to composition in the earlier liter-ature, typically with the scalar weights set to 1 or tonormalizing constants (Foltz et al, 1998; Kintsch,2001; Landauer and Dumais, 1997).
Mitchell andLapata also consider a constrained version of themultiplicative approach that reduces to component-wise multiplication, where the i-th component ofthe composed vector is given by: pi = uivi.
Thesimplified additive model produces a sort of (sta-tistical) union of features, whereas component-wisemultiplication has an intersective effect.
They alsoevaluate a weighted combination of the simplifiedadditive and multiplicative functions.
The best re-sults on the task of paraphrasing noun-verb combi-nations with ambiguous verbs (sales slump is morelike declining than slouching) are obtained using themultiplicative approach, and by weighted combina-tion of addition and multiplication (we do not testmodel combinations in our current experiments).The multiplicative approach also performs best (butonly by a small margin) in a later application to lan-guage modeling (Mitchell and Lapata, 2009).
Erkand Pado?
(2008; 2009) adopt the same formalismbut focus on the nature of input vectors, suggest-ing that when a verb is composed with a noun, thenoun component is given by an average of verbs thatthe noun is typically object of (along similar lines,1184Kintsch (2001) also focused on composite input vec-tors, within an additive framework).
Again, the mul-tiplicative model works best in Erk and Pado?
?s ex-periments.The above-mentioned researchers do not exploitcorpus evidence about the p vectors that result fromcomposition, despite the fact that it is straightfor-ward (at least for short constructions) to extractdirect distributional evidence about the compositeitems from the corpus (just collect co-occurrenceinformation for the composite item from windowsaround the contexts in which it occurs).
Themain innovation of Guevara (2010), who focuses onadjective-noun combinations (AN), is to use the co-occurrence vectors of observed ANs to train a su-pervised composition model (we became aware ofGuevara?s approach after we had developed our ownmodel, that also exploits observed ANs for training).Guevara adopts the full additive composition formfrom Equation (1) and he estimates the A and Bweights using partial least squares regression.
Thetraining data are pairs of adjective-noun vector con-catenations, as input, and corpus-derived AN vec-tors, as output.
Guevara compares his model tothe simplified additive and multiplicative models ofMitchell and Lapata.
Observed ANs are nearer, inthe space of observed and predicted test set ANs, tothe ANs generated by his model than to those fromthe alternative approaches.
The additive model, onthe other hand, is best in terms of shared neighborcount between observed and predicted ANs.In our empirical tests, we compare our approachto the simplified additive and multiplicative modelsof Mitchell and Lapata (the former with normaliza-tion constants as scalar weights) as well as to Gue-vara?s approach.3 Adjectives as linear mapsAs discussed in the introduction, we will take ad-jectives in attributive position to be functions fromone noun meaning to another.
To start simple, weassume here that adjectives in the attributive posi-tion (AN) are linear functions from n-dimensional(noun) vectors onto n-dimensional vectors, an oper-ation that can be expressed as multiplication of theinput noun column vector by a n ?
n matrix, thatis our representation for the adjective (in the lan-guage of linear algebra, an adjective is an endomor-phic linear map in noun space).
In the framework ofMitchell and Lapata, our approach derives from theadditive form in Equation (1) with the matrix multi-plying the adjective vector (say, A) set to 0:p = Bvwhere p is the observed AN vector, B the weightmatrix representing the adjective at hand, and v anoun vector.
In our approach, the weight matrix B isspecific to a single adjective ?
as we will see in Sec-tion 7 below, it is our representation of the meaningof the adjective.Like Guevara, we estimate the values in theweight matrix by partial least squares regression.In our case, the independent variables for the re-gression equations are the dimensions of the corpus-based vectors of the component nouns, whereas theAN vectors provide the dependent variables.
UnlikeGuevara, (i) we train separate models for each adjec-tive (we learn adjective-specific functions, whereasGuevara learns a generic ?AN-slot?
function) and,consequently, (ii) corpus-harvested adjective vectorsplay no role for us (their values would be constantacross the training input vectors).A few considerations are in order.
First, althoughwe use a supervised learning method (least squaresregression), we do not need hand-annotated data,since the target AN vectors are automatically col-lected from the corpus just like vectors for singlewords are.
Thus, there is no extra ?external knowl-edge?
cost with respect to unsupervised approaches.Second, our approach rests on the assumption thatthe corpus-derived AN vectors are interesting ob-jects that should constitute the target of what a com-position process tries to approximate.
We providepreliminary empirical support for this assumption inSection 5 below.
Third, we have some reasonablehope that our functions can capture to a certain ex-tent the polysemous nature of adjectives: we couldlearn, for example, a green matrix with large posi-tive weights mapping from noun features that per-tain to concrete objects to color dimensions of theoutput vector (green chair), as well as large positiveweights from features characterizing certain classesof abstract concepts to political/social dimensions inthe output (green initiative).
Somewhat optimisti-cally, we hope that chair will have near-0 values1185on the relevant abstract dimensions, like initiativeon the concrete features, and thus the weights willnot interfere.
We do not evaluate this claim specif-ically, but our quantitative evaluation in Section 6shows that our approach does best with high fre-quency, highly ambiguous adjectives.
Fourth, theapproach is naturally syntax-sensitive, since we trainit on observed data for a specific syntactic position:we would train separate linear models for, say, thesame adjective in attributive (AN) and predicative(N is A) position.
As a matter of fact, the currentmodel is too syntax-sensitive and does not capturesimilarities across different constructions.
Finally,although adjective representations are not directlyharvested from corpora, we can still meaningfullycompare adjectives to each other or other words byusing their estimated matrix, or an average vector forthe ANs that contain them: both options are testedin Section 7 below.4 Experimental setup4.1 CorpusWe built a large corpus by concatenating theWeb-derived ukWaC corpus (http://wacky.sslmit.unibo.it/), a mid-2009 dump of theEnglish Wikipedia (http://en.wikipedia.org) and the British National Corpus (http://www.natcorp.ox.ac.uk/).
This concate-nated corpus, tokenized, POS-tagged and lemma-tized with the TreeTagger (Schmid, 1995), containsabout 2.83 billion tokens (excluding punctuation,digits, etc.).
The ukWaC and Wikipedia sections canbe freely downloaded, with full annotation, from theukWaC site.We performed some of the list extraction andchecking operations we are about to describe on amore manageable data-set obtained by selecting thefirst 100M tokens of ukWaC; we refer to this subsetas the sample corpus below.4.2 VocabularyWe could in principle limit ourselves to collectingvectors for the ANs to be analyzed (the AN test set)and their components.
However, to make the anal-ysis more challenging and interesting, we populatethe semantic space where we will look at the be-haviour of the ANs with a large number of adjectivesand nouns, as well as further ANs not in the test set.We refer to the overall list of items we build seman-tic vectors for as the extended vocabulary.
We usea subset of the extended vocabulary containing onlynouns and adjectives (the core vocabulary) for fea-ture selection and dimensionality reduction, so thatwe do not implicitly bias the structure of the seman-tic space by our choice of ANs.To construct the AN test set, we first selected 36adjectives across various classes: size (big, great,huge, large, major, small, little), denominal (Amer-ican, European, national, mental, historical, elec-tronic), colors (white, black, red, green) positiveevaluation (nice, excellent, important, appropriate),temporal (old, recent, new, young, current), modal(necessary, possible), plus some common abstractantonymous pairs (difficult, easy, good, bad, spe-cial, general, different, common).
We were care-ful to include intersective cases such as electronicas well as non-intersective adjectives that are almostfunction words (the modals, different, etc.).
We ex-tracted all nouns that occurred at least 300 timesin post-adjectival position in the sample corpus, ex-cluding some extremely frequent temporal and mea-sure expressions such as time and range, for a to-tal of 1,420 distinct nouns.
By crossing the selectedadjectives and nouns, we constructed a test set con-taining 26,440 ANs, all attested in the sample cor-pus (734 ANs per adjective on average, ranging from1,337 for new to 202 for mental).The core vocabulary contains the top 8K mostfrequent noun lemmas and top 4K adjective lemmasfrom the concatenated corpus (excluding the top 50most frequent nouns and adjectives).
The extendedvocabulary contains this core plus (i) the 26,440test ANs, (ii) the 16 adjectives and 43 nouns thatare components of these ANs and that are not in thecore set, and (iii) 2,500 more ANs randomly sam-pled from those that are attested in the sample cor-pus, have a noun from the same list used for the testset ANs, and an adjective that occurred at least 5Ktimes in the sample corpus.
In total, the extendedvocabulary contains 40,999 entries: 8,043 nouns,4,016 adjectives and 28,940 ANs.4.3 Semantic space constructionFull co-occurrence matrix The 10K lemmas(nouns, adjectives or verbs) that co-occur with1186the largest number of items in the core vocabu-lary constitute the dimensions (columns) of our co-occurrence matrix.
Using the concatenated corpus,we extract sentence-internal co-occurrence counts ofall the items in the extended vocabulary with the10K dimension words.
We then transform the rawcounts into Local Mutual Information (LMI) scores(LMI is an association measure that closely approx-imates the Log-Likelihood Ratio, see Evert (2005)).Dimensionality reduction Since, for each test setadjective, we need to estimate a regression modelfor each dimension, we want a compact space withrelatively few, dense dimensions.
A natural way todo this is to apply the Singular Value Decomposi-tion (SVD) to the co-occurrence matrix, and repre-sent the items of interest with their coordinates inthe space spanned by the first n right singular vec-tors.
Applying SVD is independently justified be-cause, besides mitigating the dimensionality prob-lem, it often improves the quality of the semanticspace (Landauer and Dumais, 1997; Rapp, 2003;Schu?tze, 1997).
To avoid bias in favour of dimen-sions that capture variance in the test set ANs, weapplied SVD to the core vocabulary subset of theco-occurrence matrix (containing only adjective andnoun rows).
The core 12K?10K matrix was re-duced using SVD to a 12K?300 matrix.
The otherrow vectors of the full co-occurrence matrix (in-cluding the ANs) were projected onto the same re-duced space by multiplying them by a matrix con-taining the first n right singular vectors as columns.Merging the items used to compute the SVD andthose projected onto the resulting space, we obtain a40,999?300 matrix representing 8,043 nouns, 4,016adjectives and 28,940 ANs.
This reduced matrixconstitutes a realistically sized semantic space, thatalso contains many items that are not part of our testset, but will be potential neighbors of the observedand predicted test ANs in the experiments to follow.The quality of the SVD reduction itself was indepen-dently validated on a standard similarity judgmentdata-set (Rubenstein and Goodenough, 1965), ob-taining similar (and state-of-the-art-range) Pearsoncorrelations of vector cosines and human judgmentsin both the original (r = .70) and reduced (r = .72)spaces.There are several parameters involved in con-structing a semantic space (choice of full and re-duced dimensions, co-occurrence span, weightingmethod).
Since our current focus is on alterna-tive composition methods evaluated on a shared se-mantic space, exploring parameters pertaining to theconstruction of the semantic space is not one of ourpriorities, although we cannot of course exclude thatthe nature of the underlying semantic space affectsdifferent composition methods differently.4.4 Composition methodsIn the proposed adjective-specific linear map (alm)method, an AN is generated by multiplying an adjec-tive weight matrix with a noun (column) vector.
Thej weights in the i-th row of the matrix are the coeffi-cients of a linear regression predicting the values ofthe i-th dimension of the AN vector as a linear com-bination of the j dimensions of the component noun.The linear regression coefficients are estimated sep-arately for each of the 36 tested adjectives fromthe corpus-observed noun-AN pairs containing thatadjective (observed adjective vectors are not used).Since we are working in the 300-dimensional rightsingular vector space, for each adjective we have300 regression problems with 300 independent vari-ables, and the training data (the noun-AN pairs avail-able for each test set adjective) range from about200 to more than 1K items.
We estimate the coef-ficients using (multivariate) partial least squares re-gression (PLSR) as implemented in the R pls pack-age (Mevik and Wehrens, 2007).
With respect tostandard least squares estimation, this technique ismore robust against over-training by effectively us-ing a smaller number of orthogonal ?latent?
vari-ables as predictors (Hastie et al, 2009, Section 3.4),and it exploits the multivariate nature of the prob-lem (different regressions for each AN vector di-mension to be predicted) when determining the la-tent dimensions.
The number of latent variables tobe used in the core regression are a free parameter ofPLSR.
For efficiency reasons, we did not optimize it.We picked instead 50 latent variables, by the rule-of-thumb reasoning that for any adjective we canuse at least 200 noun-AN pairs for training, and theindependent-variable-to-training-item ratio will thusnever be above 1/4.
We adopt a leave-one-out train-ing regime, so that each target AN is generated byan adjective matrix that was estimated from all the1187other ANs with the same adjective, minus the target.We use PLSR with 50 latent variables also forour re-implementation of Guevara?s (2010) singlelinear map (slm) approach, in which a single re-gression matrix is estimated for all ANs across ad-jectives.
The training data in this case are givenby the concatenation of the observed adjective andnoun vectors (600 independent variables) coupledwith the corresponding AN vectors (300 dependentvariables).
For each target AN, we randomly sam-ple 2,000 other adjective-noun-AN tuples for train-ing (with larger training sets we run into memoryproblems), and use the resulting coefficient matrix togenerate the AN vector from the concatenated targetadjective and noun vectors.Additive AN vectors (add method) are obtainedby summing the corresponding adjective and nounvectors after normalizing them (non-normalized ad-dition was also tried, but it did not work nearly aswell as the normalized variant).
Multiplicative vec-tors (mult method) were obtained by component-wise multiplication of the adjective and noun vec-tors (normalization does not matter here since itamounts to multiplying the composite vector by ascalar, and the cosine similarity measure we use isscale-invariant).
Finally, the adj and noun baselinesuse the adjective and noun vectors, respectively, assurrogates of the AN vector.For the add, mult, adj and noun methods, we ranthe tests of Section 6 not only in the SVD-reducedspace, but also in the original 10K-dimensional co-occurrence space.
Only the mult method achievedbetter performance in the original space.
We con-jecture that this is because the SVD dimensions canhave negative values, leading to counter-intuitive re-sults with component-wise multiplication (multiply-ing large opposite-sign values results in large nega-tive values).
We tried to alleviate this problem by as-signing a 0 to composite dimensions where the twoinput vectors had different signs.
The resulting per-formance was better but still below that of mult inoriginal space.
Thus, in Section 6 we report multresults from the full co-occurrence matrix; reducedspace results for all other methods.5 Study 1: ANs in semantic spaceThe actual distribution of ANs in the corpus, asrecorded by their co-occurrence vectors, is funda-mental to what we are doing.
Our method relies onthe hypothesis that the semantics of AN composi-tion does not depend on the independent distribu-tion of adjectives themselves, but on how adjectivestransform the distribution of nouns, as evidenced byobserved pairs of noun-AN vectors.
Moreover, co-herently with this view, our evaluation below will bebased on how closely the models approximate theobserved vectors of unseen ANs.That our goal in modeling composition should beto approximate the vectors of observed ANs is ina sense almost trivial.
Whether we synthesize anAN for generation or decoding purposes, we wouldwant the synthetic AN to look as much as possiblelike a real AN in its natural usage contexts, and co-occurrence vectors of observed ANs are a summaryof their usage in actual linguistic contexts.
However,it might be the case that the specific resources weused for our vector construction procedure are notappropriate, so that the specific observed AN vectorswe extract are not reliable (e.g., they are so sparse inthe original space as to be uninformative, or they arestrictly tied to the domains of the input corpora).
Weprovide here some preliminary qualitative evidencethat this is in general not the case, by tapping intoour own intuitions on where ANs should be locatedin semantic space, and thus on how sensible theirneighbors are.First, we computed centroids from normalizedSVD space vectors of all the ANs that share the sameadjective (e.g., the normalized vectors of Americanadult, American menu, etc., summed to constructthe American N centroid).
We looked at the near-est neighbors of these centroids in semantic spaceamong the 41K items (adjectives, nouns and ANs)in our extended vocabulary (here and in all experi-ments below, similarity is quantified by the cosine ofthe angle between two vectors).
As illustrated for arandom sample of 9 centroids in Table 1 (but apply-ing to the remaining 27 adjectives as well), centroidsare positioned in intuitively reasonable areas of thespace, typically near the adjective itself or the corre-sponding noun (the noun green near green N), proto-typical ANs for that adjective (black face), elements1188related to the definition of the adjective (mental ac-tivity, historical event, green colour, quick and littlecost for easy N), and so on.American N black N easy NAm.
representative black face easy startAm.
territory black hand quickAm.
source black (n) little costgreen N historical N mental Ngreen (n) historical mental activityred road hist.
event mental experiencegreen colour hist.
content mental energynecessary N nice N young Nnecessary nice youthfulnecessary degree good bit young doctorsufficient nice break young staffTable 1: Nearest 3 neighbors of centroids of ANs thatshare the same adjective.How about the neighbors of specific ANs?
Ta-ble 2 reports the nearest 3 neighbors of 9 randomlyselected ANs involving different adjectives (we in-spected a larger random set, coming to similar con-clusions to the ones emerging from this table).bad electronic historicalluck communication mapbad elec.
storage topographicalbad weekend elec.
transmission atlasgood spirit purpose hist.
materialimportant route nice girl little warimportant transport good girl great warimportant road big girl major warmajor road guy small warred cover special collection young husbandblack cover general collection small sonhardback small collection small daughterred label archives mistressTable 2: Nearest 3 neighbors of specific ANs.The nearest neighbors of the corpus-based ANvectors in Table 2 make in general intuitive sense.Importantly, the neighbors pick up the compositemeaning rather than that of the adjective or nounalone.
For example, cover is an ambiguous word,but the hardback neighbor relates to its ?front of abook?
meaning that is the most natural one in com-bination with red.
Similarly, it makes more sensethat a young husband (rather than an old one) wouldhave small sons and daughters (not to mention themistress!
).We realize that the evidence presented here isof a very preliminary and intuitive nature.
Indeed,we will argue in the next section that there arecases in which the corpus-derived AN vector mightnot be a good approximation to our semantic in-tuitions about the AN, and a model-composed ANvector is a better semantic surrogate.
One of themost important avenues for further work will be tocome to a better characterization of the behaviour ofcorpus-observed ANs, where they work and wherethe don?t.
Still, the neighbors of average and AN-specific vectors of Tables 1 and 2 suggest that, forthe bulk of ANs, such corpus-based co-occurrencevectors are semantically reasonable.6 Study 2: Predicting AN vectorsHaving tentatively established that the sort of vec-tors we can harvest for ANs by directly collectingtheir corpus co-occurrences are reasonable represen-tations of their composite meaning, we move on tothe core question of whether it is possible to recon-struct the vector for an unobserved AN from infor-mation about its components.
We use nearness tothe corpus-observed vectors of held-out ANs as avery direct way to evaluate the quality of model-generated ANs, since we just saw that the observedANs look reasonable (but see the caveats at the endof this section).
We leave it to further work to as-sess the quality of the generated ANs in an appliedsetting, for example adapting Mitchell and Lapata?sparaphrasing task to ANs.
Since the observed vec-tors look like plausible representations of compos-ite meaning, we expect that the closer the model-generated vectors are to the observed ones, the betterthey should also perform in any task that requires ac-cess to the composite meaning, and thus that the re-sults of the current evaluation should correlate withapplied performance.More in detail, we evaluate here the compositionmethods (and the adjective and noun baselines) bycomputing, for each of them, the cosine of the testset AN vectors they generate (the ?predicted?
ANs)with the 41K vectors representing our extended vo-cabulary in semantic space, and looking at the posi-tion of the corresponding observed ANs (that werenot used for training, in the supervised approaches)1189in the cosine-ranked lists.
The lower the rank, thebetter the approximation.
For efficiency reasons, weflatten out the ranks after the top 1,000 neighbors.The results are summarized in Table 3 by the me-dian and the other quartiles, calculated across all26,440 ANs in the test set.
These measures (unlikemean and variance) are not affected by the cut-offafter 1K neighbors.
To put the reported results intoperspective, a model with a first quartile rank of 999does very significantly better than chance (the bino-mial probability of 1/4 or more of 26,440 trials be-ing successful with pi = 0.024 is virtually 0, wherethe latter quantity is the probability of an observedAN being at rank 999 or lower according to a geo-metric distribution with pi=1/40999).method 25% median 75%alm 17 170 ?1Kadd 27 257 ?1Knoun 72 448 ?1Kmult 279 ?1K ?1Kslm 629 ?1K ?1Kadj ?1K ?1K ?1KTable 3: Quartile ranks of observed ANs in cosine-rankedlists of predicted AN neighbors.Our proposed method, alm, emerges as the bestapproach.
The difference with the second bestmodel, add (the only other model that does betterthan the non-trivial baseline of using the compo-nent noun vector as a surrogate for AN), is highlystatistically significant (Wilcoxon signed rank test,p< 0.00001).
If we randomly downsample the ANset to keep an equal number of ANs per adjective(200), the difference is still significant with p belowthe same threshold, indicating that the general resultis not due to a better performance of alm on a fewcommon adjectives.1Among the alternative models, the fact that theperformance of add is decidedly better than that ofmult is remarkable, since earlier studies found that1The semantic space in which we rank the observed ANswith respect to their predicted counterparts also contain the ob-served vectors of nouns and ANs that were used to train alm.We do not see how this should affect performance, but we nev-ertheless repeated the evaluation leaving out, for each AN, theobserved items used in training, and we obtained the same re-sults reported in the main text (same ordering of method perfor-mance, and very significant difference between alm and add).multiplicative models are, in general, better than ad-ditive ones in compositionality tasks (see Section 2above).
This might depend on the nature of ANcomposition, but there are also more technical is-sues at hand: (i) we are not sure that previous stud-ies normalized before summing like we did, and(ii) the multiplicative model, as discussed in Section4, does not benefit from SVD reduction.
The sin-gle linear mapping model (slm) proposed by Gue-vara (2010) is doing even worse than the multiplica-tive method, suggesting that a single set of weightsdoes not provide enough flexibility to model a vari-ety of adjective transformations successfully.
Thisis at odds with Guevara?s experiment in which slmoutperformed mult and add on the task of rankingpredicted ANs with respect to a target observed AN.Besides various differences in task definition andmodel implementation, Guevara trained his modelon ANs that include a wide variety of adjectives,whereas our training data were limited to ANs con-taining one of our 36 test set adjectives.
Future workshould re-evalute the performance of Guevara?s ap-proach in our task, but under his training regime.Looking now at the alm results in more detail, thebest median ranks are obtained for very frequent ad-jectives.
The top ones are new (median rank: 34),great (79), American (82), large (82) and different(97).
There is a high inverse correlation betweenmedian rank and adjective frequency (Spearman?s?
=?0.56).
Although from a statistical perspec-tive it is expected that we get better results wherewe have more data, from a linguistic point of view itis interesting that alm works best with extremely fre-quent, highly polysemous adjectives like new, largeand different, that border on function words ?
a do-main where distributional semantics has generallynot been tested.Although, in relative terms and considering thedifficulty of the task, alm performs well, it is still farfrom perfect ?
for 27% alm-predicted ANs, the ob-served vector is not even in the top 1K neighbor set!A qualitative look at some of the most problematicexamples indicates however that a good proportionof them might actually not be instances where ourmodel got the AN vector wrong, but cases of anoma-lous observed ANs.
The left side of Table 4 com-pares the nearest neighbors (excluding each other)of the observed and alm-predicted vectors in 10 ran-1190SIMILAR DISSIMILARadj N obs.
neighbor pred.
neighbor adj N obs.
neighbor pred.
neighborcommon understanding common approach common vision American affair Am.
development Am.
policydifferent authority diff.
objective diff.
description current dimension left (a) current elementdifferent partner diff.
organisation diff.
department good complaint current complaint good beginninggeneral question general issue same great field excellent field gr.
distributionhistorical introduction hist.
background same historical thing different today hist.
realitynecessary qualification nec.
experience same important summer summer big holidaynew actor new cast same large pass historical region large dimensionrecent request recent enquiry same special something little animal special thingsmall drop droplet drop white profile chrome (n) white showyoung engineer young designer y. engineering young photo important song young imageTable 4: Left: nearest neighbors of observed and alm-predicted ANs (excluding each other) for a random set of ANswhere rank of observed w.r.t.
predicted is 1.
Right: nearest neighbors of predicted and observed ANs for random setwhere rank of observed w.r.t.
predicted is ?
1K.domly selected cases where the observed AN is thenearest neighbor of the predicted one.
Here, theANs themselves make sense, and the (often shared)neighbors are also sensible (recent enquiry for re-cent request, common approach and common visionfor common understanding, etc.).
Moving to theright, we see 10 random examples of ANs where theobserved AN was at least 999 neighbors apart fromthe alm prediction.
First, we notice some ANs thatare difficult to interpret out-of-context (importantsummer, white profile, young photo, large pass, .
.
.
).Second, at least subjectively, we find that in manycases the nearest neighbor of predicted AN is actu-ally more sensible than that of observed AN: cur-rent element (vs. left) for current dimension, histori-cal reality (vs. different today) for historical thing,special thing (vs. little animal) for special some-thing, young image (vs. important song) for youngphoto.
In the other cases, the predicted AN neighboris at least not obviously worse than the observed ANneighbor.There is a high inverse correlation between thefrequency of occurrence of an AN and the rank ofthe observed AN with respect to the predicted one(?
=?0.48), suggesting that our model is worse atapproximating the observed vectors of rare forms,that might, in turn, be those for which the corpus-based representation is less reliable.
In these cases,dissimilarities between observed and expected vec-tors, rather than signaling problems with the model,might indicate that the predicted vector, based on acomposition function learned from many examples,is better than the one directly extracted from the cor-pus.
The examples in the right panel of Table 4 bringsome preliminary support to this hypothesis, to besystematically explored in future work.7 Study 3: Comparing adjectivesIf adjectives are functions, and not corpus-derivedvectors, is it still possible to compare them mean-ingfully?
We explore two ways to accomplish thisin our framework: one is to represent adjectives bythe average of the AN vectors that contain them(the centroid vectors whose neighbors are illustratedin Table 1 above), and the other to compare thembased on the 300?300 weight matrices we esti-mate from noun-AN pairs (we unfold these matri-ces into 90K-dimensional vectors).
We compare thequality of these representations to that of the stan-dard approach in distributional semantics, i.e., rep-resenting the adjectives directly with their corpusco-occurrence profile vectors (in our case, projectedonto the SVD-reduced space).We evaluate performance on the task of cluster-ing those 19 adjectives in our set that can be rel-atively straightforwardly categorized into generalclasses comprising a minimum of 4 items.
Thetest set built according to these criteria contains 4classes: color (white, black, red, green), positiveevaluation (nice, excellent, important, major, ap-propriate), time (recent, new, current, old, young),and size (big, huge, little, small, large).
We clus-ter with the CLUTO toolkit (Karypis, 2003), us-ing the repeated bisections with global optimization1191method, accepting all of CLUTO?s default valuesfor this choice.
Cluster quality is evaluated by per-centage purity (Zhao and Karypis, 2003).
If nir isthe number of items from the i-th true (gold stan-dard) class assigned to the r-th cluster, n is the to-tal number of items and k the number of clusters,then: Purity = 1n?kr=1 maxi(nir).
We calculateempirical 95% confidence intervals around purity bya heuristic bootstrap procedure based on 10K resam-plings of the data set (Efron and Tibshirani, 1994).The random baseline distribution is obtained by 10Krandom assignments of adjectives to the clusters, un-der the constraint that no cluster is empty.Table 5 shows that all methods are significantlybetter than chance.
Our two ?indirect?
represen-tations achieve similar performance, and they are(slightly) better than the traditional method based onadjective co-occurrence vectors.
We conclude that,although our approach does not provide a direct en-coding of adjective meaning in terms of such inde-pendently collected vectors, it does have meaningfulways to represent their semantic properties.input puritymatrix 73.7 (68.4-94.7)centroid 73.7 (63.2-94.7)vector 68.4 (63.2-89.5)random 45.9 (36.8-57.9)Table 5: Percentage purity in adjective clustering withbootstrapped 95% confidence intervals.8 ConclusionThe work we reported constitutes an encouragingstart for our approach to modeling (AN) composi-tion.
We suggested, along the way, various direc-tions for further studies.
We consider the followingissues to be the most pressing ones.We currently train each adjective-specific modelseparately: We should explore hierarchical model-ing approaches that exploit similarities across adjec-tives (and possibly syntactic constructions) to esti-mate better models.Evaluation-wise, the differences between ob-served and predicted ANs must be analyzed moreextensively, to support the claim that, when theirvectors differ, model-based prediction improves onthe observed vector.
Evaluation in a more appliedtask should also be pursued ?
in particular, we willdesign a paraphrasing task similar to the one pro-posed by Mitchell and Lapata to evaluate noun-verbconstructions.Since we do not collect vectors for the ?functor?component of a composition process (for AN con-structions, the adjective), our approach naturally ex-tends to processes that involve bound morphemes,such as affixation, where we would not need to col-lect independent co-occurrence information for theaffixes.
For example, to account for re- prefixationwe do not need to collect a re- vector (required by allother approaches to composition), but simply vec-tors for a set of V/reV pairs, where both members ofthe pairs are words (e.g., consider/reconsider).Our approach can also deal, out-of-the-box, withrecursive constructions (sad little red hat), and canbe easily extended to more abstract constructions,such as determiner N (mapping dog to the/a/onedog).
Still, we need to design a good testing scenarioto evaluate the quality of such model-generated con-structions.Ultimately, we want to compose larger and largerconstituents, up to full sentences.
It remains to beseen if the approach we proposed will scale up tosuch challenges.AcknowledgmentsWe thank Gemma Boleda, Emilano Guevara,Alessandro Lenci, Louise McNally and the anony-mous reviewers for useful information, advice andcomments.ReferencesS.
Clark and S. Pulman.
2007.
Combining symbolic anddistributional models of meaning.
In Proceedings ofthe First Symposium on Quantum Interaction, pages52?55.B.
Efron and R. Tibshirani.
1994.
An Introduction to theBootstrap.
Chapman and Hall, Boca Raton, FL.K.
Erk and S. Pado?.
2008.
A structured vector spacemodel for word meaning in context.
In Proceedings ofEMNLP, pages 897?906.K.
Erk and S. Pado?.
2009.
Paraphrase assessmentin structured vector space: Exploring parameters anddatasets.
In Proceedings of the EACL GEMS Work-shop, pages 57?65.1192S.
Evert.
2005.
The Statistics of Word Cooccurrences.Dissertation, Stuttgart University.P.
Foltz, W. Kintsch, and Th.
Landauer.
1998.
The mea-surement of textual coherence with Latent SemanticAnalysis.
Discourse Processes, 25:285?307.G.
Frege.
1892.
U?ber sinn und bedeutung.
Zeitschriftfuer Philosophie un philosophische Kritik, 100.E.
Guevara.
2010.
A regression model of adjective-nouncompositionality in distributional semantics.
In Pro-ceedings of the ACL GEMS Workshop, pages 33?37.T.
Hastie, R. Tibshirani, and J. Friedman.
2009.
The El-ements of Statistical Learning, 2nd ed.
Springer, NewYork.M.
Jones and D. Mewhort.
2007.
Representing wordmeaning and order information in a composite holo-graphic lexicon.
Psychological Review, 114:1?37.H.
Kamp.
1975.
Two theories about adjectives.
InE.
Keenan, editor, Formal Semantics of Natural Lan-guage, pages 123?155.
Cambridge University Press.G.
Karypis.
2003.
CLUTO: A clustering toolkit.
Tech-nical Report 02-017, University of Minnesota Depart-ment of Computer Science.W.
Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.Th.
Landauer and S. Dumais.
1997.
A solution to Plato?sproblem: The Latent Semantic Analysis theory of ac-quisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240.B.
Mevik and R. Wehrens.
2007.
The pls package: Prin-cipal component and partial least squares regression inR.
Journal of Statistical Software, 18(2).J.
Mitchell and M. Lapata.
2008.
Vector-based models ofsemantic composition.
In Proceedings of ACL, pages236?244.J.
Mitchell and M. Lapata.
2009.
Language modelsbased on semantic composition.
In Proceedings ofEMNLP, pages 430?439.R.
Montague.
1970a.
English as a formal language.
InB.
Visentini, editor, Linguaggi nella Societa` e nellaTecnica, pages 189?224.
Edizioni di Comunita`, Milan.Reprinted in Thomason (1974).R.
Montague.
1970b.
Universal grammar.
Theoria,36:373?398.
Reprinted in Thomason (1974).R.
Montague.
1973.
The proper treatment of quantifica-tion in English.
In K.J.J.
Hintikka, editor, Approachesto Natural Language, pages 221?242.
Reidel, Dor-drecht.
Reprinted in Thomason (1974).B.
Partee.
2004.
Compositionality.
In Compositionalityin Formal Semantics: Selected Papers by Barbara H.Partee.
Blackwell, Oxford.R.
Rapp.
2003.
Word sense discovery based on sense de-scriptor dissimilarity.
In Proceedings of the MT Sum-mit, pages 315?322.H.
Rubenstein and J. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8(10):627?633.S.
Rudolph and E. Giesbrecht.
2010.
Compositionalmatrix-space models of language.
In Proceedings ofACL.M.
Sahlgren, A. Holst, and P. Kanerva.
2008.
Permu-tations as a means to encode order in word space.
InProceedings of CogSci, pages 1300?1305.M.
Sahlgren.
2006.
The Word-Space Model.
Disserta-tion, Stockholm University.H.
Schmid.
1995.
Improvements in part-of-speech tag-ging with an application to German.
In Proceedings ofthe EACL-SIGDAT Workshop.H.
Schu?tze.
1997.
Ambiguity Resolution in Natural Lan-guage Learning.
CSLI, Stanford, CA.M.
Siegel.
1976.
Capturing the Adjective.
Ph.D. thesis,University of Massachusetts at Amherst.P.
Smolensky.
1990.
Tensor product variable binding andthe representation of symbolic structures in connec-tionist networks.
Artificial Intelligence, 46:159?216.R.
H. Thomason, editor.
1974.
Formal Philosophy: Se-lected Papers of Richard Montague.
Yale UniversityPress, New York.P.
Turney and P. Pantel.
2010.
From frequency to mean-ing: Vector space models of semantics.
Journal of Ar-tificial Intelligence Research, 37:141?188.D.
Widdows.
2008.
Semantic vector products: Some ini-tial investigations.
In Proceedings of the Second Sym-posium on Quantum Interaction, Oxford.Y.
Zhao and G. Karypis.
2003.
Criterion functions fordocument clustering: Experiments and analysis.
Tech-nical Report 01-40, University of Minnesota Depart-ment of Computer Science.1193
