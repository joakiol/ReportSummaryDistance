Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 96?103, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsProbabilistic Head-Driven Parsing for Discourse StructureJason Baldridge and Alex LascaridesSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotland, UK{jbaldrid,alex}@inf.ed.ac.ukAbstractWe describe a data-driven approach tobuilding interpretable discourse structuresfor appointment scheduling dialogues.
Werepresent discourse structures as headedtrees and model them with probabilis-tic head-driven parsing techniques.
Weshow that dialogue-based features regard-ing turn-taking and domain specific goalshave a large positive impact on perfor-mance.
Our best model achieves an f -score of 43.2% for labelled discourse rela-tions and 67.9% for unlabelled ones, sig-nificantly beating a right-branching base-line that uses the most frequent relations.1 IntroductionAchieving a model of discourse interpretation that isboth robust and deep is a major challenge.
Considerthe dialogue in Figure 1 (the sentence numbers arefrom the Redwoods treebank (Oepen et al, 2002)).A robust and deep interpretation of it should resolvethe anaphoric temporal description in utterance 154to the twenty sixth of July in the afternoon.
It shouldidentify that time and before 3pm on the twenty-seventh as potential times to meet, while ruling outJuly thirtieth to August third.
It should gracefullyhandle incomplete or ungrammatical utterances like152 and recognise that utterances 151 and 152 haveno overall effect on the time and place to meet.According to Hobbs et al (1993) and Asher andLascarides (2003), a discourse structure consistingof hierarchical rhetorical connections between utter-ances is vital for providing a unied model of a wide149 PAM: maybe we can get together, and, discuss, theplanning, say, two hours, in the next, couple weeks,150 PAM: let me know what your schedule is like.151 CAE: okay, let me see.152 CAE: twenty,153 CAE: actually, July twenty sixth and twenty seventh looksgood,154 CAE: the twenty sixth afternoon,155 CAE: or the twenty seventh, before three p.m., geez.156 CAE: I am out of town the thirtieth through the,157 CAE: the third, I am in San Francisco.Figure 1: A dialogue extract from Redwoods.range of anaphoric and intentional discourse phe-nomena, contributing to the interpretations of pro-nouns, temporal expressions, presuppositions andellipses (among others), as well as influencing com-municative goals.
This suggests that a robust modelof discourse structure could complement current ro-bust interpretation systems, which tend to focus ononly one aspect of the semantically ambiguous ma-terial, such as pronouns (e.g., Stru?be and Mu?ller(2003)), definite descriptions (e.g., Vieira and Poe-sio (2000)), or temporal expressions (e.g., Wiebeet al (1998)).
This specialization makes it hard toassess how they would perform in the context of amore comprehensive set of interpretation tasks.To date, most methods for constructing discoursestructures are not robust.
They typically rely ongrammatical input and use symbolic methods whichinevitably lack coverage.
One exception is Marcu?swork (Marcu, 1997, 1999) (see also Soricut andMarcu (2003) for constructing discourse structuresfor individual sentences).
Marcu (1999) uses adecision-tree learner and shallow syntactic features96to create classifiers for discourse segmentation andfor identifying rhetorical relations.
Together, theseamount to a model of discourse parsing.
However,the results are trees of Rhetorical Structure Theory(RST) (Mann and Thompson, 1986), and the clas-sifiers rely on well-formedness constraints on RSTtrees which are too restrictive (Moore and Pollack,1992).
Furthermore, RST does not offer an accountof how compositional semantics gets augmented,nor does it model anaphora.
It is also designed formonologue rather than dialogue, so it does not of-fer a precise semantics of questions or non-sententialutterances which convey propositional content (e.g.,154 and 155 in Figure 1).
Another main approach torobust dialogue processing has been statistical mod-els for identifying dialogue acts (e.g., Stolcke et al(2000)).
However, dialogue acts are properties ofutterances rather than hierarchically arranged rela-tions between them, so they do not provide a basisfor resolving semantic underspecification generatedby the grammar (Asher and Lascarides, 2003).Here, we present the first probabilistic approachto parsing the discourse structure of dialogue.We use dialogues from Redwoods?
appointmentscheduling domain and adapt head-driven genera-tive parsing strategies from sentential parsing (e.g.,Collins (2003)) for discourse parsing.
The discoursestructures we build conform to Segmented Dis-course Representation Theory (SDRT) (Asher andLascarides, 2003).
SDRT provides a precise dynamicsemantic interpretation for its discourse structureswhich augments the conventional semantic repre-sentations that are built by most grammars.
We thusview the task of learning a model of SDRT-style dis-course structures as one step towards achieving thegoal of robust and precise semantic interpretations.We describe SDRT in the context of our domainin Section 2.
Section 3 discusses how we encodeand annotate discourse structures as headed treesfor our domain.
Section 4 provides background onprobabilistic head-driven parsing models, and Sec-tion 5 describes how we adapt the approach for dis-course and gives four models for discourse parsing.We report results in Section 6, which show the im-portance of dialogue-based features on performance.Our best model performs far better than a baselinethat uses the most frequent rhetorical relations andright-branching segmentation.h0 : Request-Elab(149, 150)?Plan-Elab(150, h1)h1 : Elaboration(153, h2)?Continuation(153, 156)?Continuation(156, 157)h2 : Alternation(154, 155)Figure 2: The SDRS for the dialogue in Figure 1.2 Segmented Discourse RepresentationTheorySDRT extends prior work in dynamic semantics (e.g.,van Eijk and Kamp (1997)) via logical forms thatfeature rhetorical relations.
The logical forms con-sist of speech act discourse referents which la-bel content (either of a clause or of text seg-ments).
Rhetorical relations such as Explanationrelate these referents.
The resulting structures arecalled segmented discourse representation struc-tures or SDRSs.
An SDRS for the dialogue in Fig-ure 1 is given in Figure 2; we have used the numbersof the elementary utterances from Redwoods as thespeech act discourse referents but have omitted theirlabelled logical forms.
Note that utterances 151 and152, which do not contribute to the truth conditionsof the dialogue, are absent ?
we return to this shortly.There are several things to note about this SDRS.First, SDRT?s dynamic semantics of rhetorical rela-tions imposes constraints on the contents of its argu-ments.
For example, Plan-Elab(150, h1) (standingfor Plan-Elaboration) means that h1 provides infor-mation from which the speaker of 150 can elaboratea plan to achieve their communicative goal (to meetfor two hours in the next couple of weeks).
Therelation Plan-Elab contrasts with Plan-Correction,which would relate the utterances in dialogue (1):(1) a.
A: Can we meet at the weekend?b.
B: I?m afraid I?m busy then.Plan-Correction holds when the content of the sec-ond utterance in the relation indicates that its com-municative goals conflict with those of the first one.In this case, A indicates he wants to meet next week-end, and B indicates that he does not (note that thenresolves to the weekend).
Utterances (1ab) wouldalso be related with IQAP (Indirect Question Answer97Pair): this means that (1b) provides sufficient infor-mation for the questioner A to infer a direct answerto his question (Asher and Lascarides, 2003).The relation Elaboration(153, h2) in Figure 2means that the segment 154 to 155 resolves to aproposition which elaborates part of the content ofthe proposition 153.
Therefore the twenty sixth in154 resolves to the twenty sixth of July?any otherinterpretation contradicts the truth conditional con-sequences of Elaboration.
Alternation(154, 155)has truth conditions similar to (dynamic) disjunc-tion.
Continuation(156, 157) means that 156 and157 have a common topic (here, this amounts to aproposition about when CAE is unavailable to meet).The second thing to note about Figure 2 is howone rhetorical relation can outscope another: thiscreates a hierarchical segmentation of the discourse.For example, the second argument to the Elabo-ration relation is the label h2 of the Alternation-segment relating 154 to 155.
Due to the semanticsof Elaboration and Alternation, this ensures that thedialogue entails that one of 154 or 155 is true, but itdoes not entail 154, nor 155.Finally, observe that SDRT allows for a situ-ation where an utterance connects to more thanone subsequent utterance, as shown here withElaboration(153, h2) ?
Continuation(153, 156).
Infact, SDRT also allows two utterances to be relatedby multiple relations (see (1)) and it allows an utter-ance to rhetorically connect to multiple utterances inthe context.
These three features of SDRT capture thefact that an utterance can make more than one illo-cutionary contribution to the discourse.
An exampleof the latter kind of structure is given in (2):(2) a.
A: Shall we meet on Wednesday?b.
A: How about one pm?c.
B: Would one thirty be OK with you?The SDRS for this dialogue would feature the re-lations Plan-Correction(2b, 2c), IQAP(2b, 2c) andQ-Elab(2a, 2c).
Q-Elab, or Question-Elaboration,always takes a question as its second argument;any answers to the question must elaborate a planto achieve the communicative goal underlying thefirst argument to the relation.
From a logical per-spective, recognising Plan-Correction(2b, 2c) andQ-Elab(2a, 2c) are co-dependent.Segment/h0ind149Request-Elabimp150Plan-ElabSegment/h1Passpause151irr152ind153Elaboration/h2ind154Alternationind155Continuationind156Continuationind157Figure 3: The discourse structure for the dialoguefrom Figure 1 in tree form.3 Augmenting the Redwoods treebankwith discourse structuresOur starting point is to create training material forprobabilistic discourse parsers.
For this, we haveaugmented dialogues from the Redwoods Treebank(Oepen et al, 2002) with their analyses within afragment of SDRT (Baldridge and Lascarides, 2005).This is a very different effort from that being pur-sued for the Penn Discourse Treebank (Miltsakakiet al, 2004), which uses discourse connectivesrather than abstract rhetorical relations like those inSDRT in order to provide theory neutral annotations.Our goal is instead to leverage the power of the se-mantics provided by SDRT?s relations, and in partic-ular to do so for dialogue as opposed to monologue.Because the SDRS-representation scheme, asshown in Figure 2, uses graph structures that do notconform to tree constraints, it cannot be combineddirectly with statistical techniques from sententialparsing.
We have therefore designed a headed treeencoding of SDRSs, which can be straightforwardlymodeled with standard parsing techniques and fromwhich SDRSs can be recovered.For instance, the tree for the dialogue in Figure 1is given in Figure 3.
The SDRS in Figure 2 is recov-ered automatically from it.
In this tree, utterancesare leaves which are immediately dominated by theirtag, indicating either the sentence mood (indicative,interrogative or imperative) or that it is irrelevant, apause or a pleasantry (e.g., hello), annotated as pls.Each non-terminal node has a unique head daugh-ter: this is either a Segment node, Pass node, or a98leaf utterance tagged with its sentence mood.
Non-terminal nodes may in addition have any number ofdaughter irr, pause and pls nodes, and an additionaldaughter labelled with a rhetorical relation.The notion of headedness has no status in the se-mantics of SDRSs themselves.
The heads of thesediscourse trees are not like verbal heads with sub-categorization requirements in syntax; here, they arenothing more than the left argument of a rhetor-ical relation, like 154 in Alternation(154, 155).Nonetheless, defining one of the arguments ofrhetorical relations as a head serves two main pur-poses.
First, it enables a fully deterministic algo-rithm for recovering SDRSs from these trees.
Sec-ond, it is also crucial for creating probabilistic head-driven parsing models for discourse structure.Segment and Pass are non-rhetorical node types.The former explicitly groups multiple utterances.The latter allows its head daughter to enter into re-lations with segments higher in the tree.
This allowsus to represent situations where an utterance attachesto more than one subsequent utterance, such as 153in dialogue (1).
Annotators manually annotate therhetorical relation, Segment and Pass nodes and de-termine their daughters.
They also tag the individualutterances with one of the three sentence moods orirr, pause or pls.
The labels for segments (e.g., h0and h1 in Figure 3) are added automatically.
Non-veridical relations such as Alternation also introducesegment labels on their parents; e.g., h2 in Figure 3.The SDRS is automatically recovered from thistree representation as follows.
First, each rela-tion node generates a rhetorical connection in theSDRS: its first argument is the discourse referentof its parent?s head daughter, and the second is thediscourse referent of the node itself (which unlessstated otherwise is its head daughter?s discourse ref-erent).
For example, the structure in Figure 3 yieldsRequest-Elab(149, 150), Alternation(154, 155) andElaboration(153, h2).
The labels for the relationsin the SDRS?which determine segmentation?mustalso be recovered.
This is easily done: any nodewhich has a segment label introduces an outscopesrelation between that and the discourse referentsof the node?s daughters.
This produces, for ex-ample, outscopes(h0, 149), outscopes(h1, 153) andoutscopes(h2, 154).
It is straightforward to deter-mine the labels of all the rhetorical relations fromthese conditions.
Utterances such as 151 and 152,which are attached with pause and irr to indicate thatthey have no overall truth conditional effect on thedialogue, are ignored when constructing the SDRS,so SDRT does not assign these terms any semantics.Overall, this algorithm generates the SDRS in Fig-ure 2 from the tree in Figure 3.Thus far, 70 dialogues have been annotated andreviewed to create our gold-standard corpus.
On av-erage, these dialogues have 237.5 words, 31.5 ut-terances, and 8.9 speaker turns.
In all, there are 30different rhetorical relations in the inventory for thisannotation task, and 6 types of tags for the utterancesthemselves: ind, int, imp, pause, irr and pls.Finally, we annotated all 6,000 utterances in theVerbmobil portion of Redwoods with the following:whether the time mentioned (if there is one) is agood time to meet (e.g., I?m free then or Shall wemeet at 2pm?)
or a bad time to meet (e.g., I?m busythen or Let?s avoid meeting at the weekend).
Theseare used as features in our model of discourse struc-ture (see Section 5).
We use these so as to minimiseusing directly detailed features from the utterancesthemselves (e.g.
the fact that the utterance containsthe word free or busy, or that it contains a negation),which would lead to sparse data problems given thesize of our training corpus.
We ultimately aim tolearn good-time and bad-time from sentence-levelfeatures extracted from the 6,000 Redwoods analy-ses, but we leave this to future work.4 Generative parsing modelsThere is a significant body of work on probabilisticparsing, especially that dealing with the English sen-tences found in the annotated Penn Treebank.
Oneof the most important developments in this work isthat of Collins (2003).
Collins created several lex-icalised head-driven generative parsing models thatincorporate varying levels of structural information,such as distance features, the complement/adjunctdistinction, subcategorization and gaps.
These mod-els are attractive for constructing our discourse trees,which contain heads that establish non-local depen-dencies in a manner similar to that in syntactic pars-ing.
Also, the co-dependent tasks of determiningsegmentation and choosing the rhetorical connec-tions are both heavily influenced by the content of99the utterances/segments which are being considered,and lexicalisation allows the model to probabilisti-cally relate such utterances/segments very directly.Probabilistic Context Free Grammars (PCFGs)determine the conditional probability of a right-hand side of a rule given the left-hand side,P(RHS|LHS).
Collins instead decomposes thecalculation of such probabilities by first generating ahead and then generating its left and right modifiersindependently.
In a supervised setting, doing thisgathers a much larger set of rules from a set of la-belled data than a standard PCFG, which learns onlyrules that are directly observed.1The decomposition of a rule begins by noting thatrules in a lexicalised PCFG have the form:P (h) ?
Ln(ln) .
.
.
L1(l1)H(h)R1(r1) .
.
.
Rm(rm)where h is the head word, H(h) is the label of thehead constituent, P (h) is its parent, and Li(li) andRi(ri) are the n left and m right modifiers, respec-tively.
It is also necessary to include STOP sym-bols Ln+1 and Rm+1 on either side to allow theMarkov process to properly model the sequences ofmodifiers.
By assuming these modifiers are gener-ated independently of each other but are dependenton the head and its parent, the probability of suchexpansions can be calculated as follows (where Ph,Pl and Pr are the probabilities for the head, left-modifiers and right-modifiers respectively):P(Ln(ln) .
.
.
L1(l1)H(h)R1(r1) .
.
.
Rm(rm)|P (h)) =Ph(H|P (h))?Yi=1...n+1Pl(Li(li)|P (h), H)?Yi=1...m+1Pr(Ri(ri)|P (h), H)This provides the simplest of models.
More con-ditioning information can of course be added fromany structure which has already been generated.
Forexample, Collins?
model 1 adds a distance featurethat indicates whether the head and modifier it isgenerating are adjacent and whether a verb is in thestring between the head and the modifier.1A similar effect can be achieved by converting n-ary treesto binary form.5 Discourse parsing modelsIn Section 3, we outlined how SDRSs can be repre-sented as headed trees.
This allows us to create pars-ing models for discourse that are directly inspired bythose described in the previous section.
These mod-els are well suited for our discourse parsing task.They are lexicalised, so there is a clear place in thediscourse model for incorporating features from ut-terances: simply replace lexical heads with wholeutterances, and exploit features from those utter-ances in discourse parsing in the same manner aslexical features are used in sentential parsing.Discourse trees contain a much wider variety ofkinds of information than syntactic trees.
The leavesof these trees are sentences with full syntactic andsemantic analyses, rather than words.
Furthermore,each dialogue has two speakers, and speaker stylecan change dramatically from dialogue to dialogue.Nonetheless, the task is also more constrained inthat there are fewer overall constituent labels, thereare only a few labels which can act as heads, andtrees are essentially binary branching apart fromconstituents containing ignorable utterances.The basic features we use are very similar to thosefor the syntactic parsing model given in the previoussection.
The feature P is the parent label that is thestarting point for generating the head and its modi-fiers.
H is the label of the head constituent.
The tagt is also used, except that rather than being a part-of-speech, it is either a sentence mood label (ind, int, orimp) or an ignorable label (irr, pls, or pause).
Theword feature w in our model is the first discourse cuephrase present in the utterance.2 In the absence of acue phrase, w is the empty string.
The distance fea-ture ?
is true if the modifier being generated is adja-cent to the head and false otherwise.
To incorporatea larger context into the conditioning information,we also utilize a feature HCR, which encodes thechild relation of a node?s head.We have two features that are particular to dia-logue.
The first ST , indicates whether the head ut-terance of a segment starts a turn or not.
The other,TC, encodes the number of turn changes within asegment with one of the values 0, 1, or ?
2.Finally, we use the good/bad-time annotationsdiscussed in Section 3 for a feature TM indicating2We obtained our list of cue phrases from Oates (2001).100Head features Modifier featuresP t w HCR ST TC TM P t w H ?
HCR ST TC TMModel 1 X X X X X X X XModel 2 X X X X X X X X X X X XModel 3 X X X X X X X X X X X X X XModel 4 X X X X X X X X X X X X X X X XFigure 4: The features active for determining the head and modifier probabilities in each of the four models.one of the following values for the head utterance ofa segment: good time, bad time, neither, or both.With these features, we create the four modelsgiven in Figure 4.
As example feature values, con-sider the Segment node labelled h1 in Figure 3.
Here,the features have as values: P=Segment, H=Pass,t=ind (the tag of utterance 153), w=Actually (see153 in Figure 1), HCR=Elaboration, ST=false,TC=0, and TM=good time.As is standard, linear interpolation with back-offlevels of decreasing specificity is used for smooth-ing.
Weights for the levels are determined as inCollins (2003).6 ResultsFor our experiments, we use a standard chart parsingalgorithm with beam search that allows a maximumof 500 edges per cell.
The figure of merit for thecut-off combines the probability of an edge with theprior probability of its label, head and head tag.
Hy-pothesized trees that do not conform to some simplediscourse tree constraints are also pruned.3The parser is given the elementary discourse unitsas defined in the corpus.
These units correspond di-rectly to the utterances already defined in Redwoodsand we can thus easily access their complete syntac-tic analyses directly from the treebank.The parser is also given the correct utterancemoods to start with.
This is akin to getting the cor-rect part-of-speech tags in syntactic parsing.
Wedo this since we are using the parser for semi-automated annotation.
Tagging moods for a newdiscourse is a very quick and reliable task for thehuman.
With them the parser can produce the morecomplex hierarchical structure more accurately thanif it had to guess them ?
with the potential to dra-matically reduce the time to annotate the discourse3E.g., nodes can have at most one child with a relation label.structures of further dialogues.
Later, we will createa sentence mood tagger that presents an n-best listfor the parser to start with, from the tag set ind, int,imp, irr, pause, and pls.Models are evaluated by using a leave-one-outstrategy, in which each dialogue is parsed after train-ing on all the others.
We measure labelled and un-labelled performance with both the standard PAR-SEVAL metric for comparing spans in trees and arelation-based metric that compares the SDRS?s pro-duced by the trees.
The latter gives a more direct in-dication of the accuracy of the actual discourse log-ical form, but we include the former to show perfor-mance using a more standard measure.
Scores areglobally determined rather than averaged over all in-dividual dialogues.For the relations metric, the relations from thederived discourse tree for the test dialogue are ex-tracted; then, the overlap with relations from thecorresponding gold standard tree is measured.
Forlabelled performance, the model is awarded a pointfor a span or relation which has the correct discourserelation label and both arguments are correct.
Forunlabelled, only the arguments need to be correct.4Figure 5 provides the f -scores5 of the variousmodels and compares them against those of a base-line model and annotators.
All differences betweenmodels are significant, using a pair-wise t-test at99.5% confidence, except that between the baselineand Model 2 for unlabelled relations.The baseline model is based on the most frequentway of attaching the current utterance to its dia-4This is a much stricter measure than one which measuresrelations between a head and its dependents in syntax becauseit requires two segments rather than two heads to be related cor-rectly.
For example, Model 4?s labelled and unlabelled relationf-scores using segments are 43.2% and 67.9%, respectively; ona head-to-head basis, they rise to 50.4% and 81.8%.5The f -score is calculated as 2?precision?recallprecision+recall .101PARSEVAL RelationsModel Lab.
Unlab.
Lab.
Unlab.Baseline 14.7 33.8 7.4 53.3Model 1 22.7 42.2 23.1 47.0Model 2 30.1 51.1 31.0 54.3Model 3 39.4 62.8 39.4 64.4Model 4 46.3 69.2 43.2 67.9Inter-annotator 53.7 76.5 50.3 73.0Annotator-gold 75.9 88.0 75.3 84.0Figure 5: Model performance.logue context.
The baseline is informed by the gold-standard utterance moods.
For this corpus, this re-sults in a baseline which is a right-branching struc-ture, where the relation Plan-Elaboration is used ifthe utterance is indicative, Question-Elaboration ifit is interrogative, and Request-Elaboration if it isimperative.
The baseline also appropriately handlesignorable utterances (i.e, those with the mood labelsirrelevant, pause, or pleasantry).The baseline performs poorly on labelled rela-tions (7.4%), but is more competitive on unlabelledones (53.3%).
The main reason for this is thatit takes no segmentation risks.
It simply relatesevery non-ignorable utterance to the previous one,which is indeed a typical configuration with com-mon content-level relations like Continuation.
Thegenerative models take risks that allow them to cor-rectly identify more complex segments ?
at the costof missing some of these easier cases.Considering instead the PARSEVAL scores for thebaseline, the labelled performance is much higher(14.7%) and the unlabelled is much lower (33.8%)than for relations.
The difference in labelled per-formance is due to the fact that the intentional-levelrelations used in the baseline often have argumentsthat are multi-utterance segments in the gold stan-dard.
These are penalized in the relations compar-ison, but the spans used in PARSEVAL are blind tothem.
On the other hand, the unlabelled score dropsconsiderably ?
this is due to poor performance ondialogues whose gold standard analyses do not havea primarily right-branching structure.Model 1 performs most poorly of all the models.It is significantly better than the baseline on labelledrelations, but significantly worse on unlabelled rela-tions.
All its features are derived from the structureof the trees, so it gets no clues from speaker turns orthe semantic content of utterances.Model 2 brings turns and larger context via theST and HCR features, respectively.
This improvessegmentation over Model 1 considerably, so that themodel matches the baseline on unlabelled relationsand beats it significantly on labelled relations.The inclusion of the TC feature in Model 3 bringslarge (and significant) improvements over Model 2.Essentially, this feature has the effect of penalizinghypothesized content-level segments that span sev-eral turns.
This leads to better overall segmentation.Finally, Model 4 incorporates the domain-basedTM feature that summarizes some of the semanticcontent of utterances.
This extra information im-proves the determination of labelled relations.
Forexample, it is especially useful in distinguishing aPlan-Correction from a Plan-Elaboration.The overall trend of differences between PARSE-VAL and relations scoring show that PARSEVAL istougher on overall segmentation and relations scor-ing is tougher on whether a model got the right ar-guments for each labelled relation.
It is the latterthat ultimately matters for the discourse structuresproduced by the parser to be useful; nonetheless, thePARSEVAL scores do show that each model progres-sively improves on capturing the trees themselves,and that even Model 1 ?
as a syntactic model ?
isfar superior to the baseline for capturing the overallform of the trees.We also compare our best model against two up-perbounds: (1) inter-annotator agreement on tendialogues that were annotated independently and(2) the best annotator against the gold standardagreed upon after the independent annotation phase.For the first, the labelled/unlabelled relations f -scores are 50.3%/73.0% and for the latter, they are75.3%/84.0%?this is similar to the performance onother discourse annotation projects, e.g., Carlsonet al (2001).
On the same ten dialogues, Model 4achieves 42.3%/64.9%.It is hard to compare these models with Marcu?s(1999) rhetorical parsing model.
Unlike Marcu, wedid not use a variety of corpora, have a smaller train-ing corpus, are analysing dialogues as opposed tomonologues, have a larger class of rhetorical re-lations, and obtain the elementary discourse units102from the Redwoods annotations rather than estimat-ing them.
Even so, it is interesting that the scoresreported in Marcu (1999) for labelled and unlabelledrelations are similar to our scores for Model 4.7 ConclusionIn this paper, we have shown how the complex taskof creating structures for SDRT can be adapted to astandard probabilistic parsing task.
This is achievedvia a headed tree representation from which SDRSscan be recovered.
This enables us to directly ap-ply well-known probabilistic parsing algorithms anduse features inspired by them.
Our results showthat using dialogue-based features are a major factorin improving the performance of the models, bothin terms of determining segmentation appropriatelyand choosing the right relations to connect them.There is clearly a great deal of room for improve-ment, even with our best model.
Even so, thatmodel performed sufficiently well for use in semi-automated annotation: when correcting the model?soutput on ten dialogues, one annotator took 30 sec-onds per utterance, compared to 39 for another an-notator working on the same dialogues with no aid.In future work, we intend to exploit an exist-ing implementation of SDRT?s semantics (Schlangenand Lascarides, 2002), which adopts theorem prov-ing to infer resolutions of temporal anaphora andcommunicative goals from SDRSs for scheduling di-alogues.
This additional semantic content can inturn be added (semi-automatically) to a training cor-pus.
This will provide further features for learn-ing discourse structure and opportunities for learn-ing anaphora and goal information directly.AcknowledgmentsThis work was supported by Edinburgh-StanfordLink R36763, ROSIE project.
Thanks to Mirella La-pata and Miles Osborne for comments.ReferencesN.
Asher and A. Lascarides.
Logics of Conversation.
Cam-bridge University Press, 2003.J.
Baldridge and A. Lascarides.
Annotating discourse struc-tures for robust semantic interpretation.
In Proceedings ofthe 6th International Workshop on Computational Seman-tics, Tilburg, The Netherlands, 2005.L.
Carlson, D. Marcu, and M. Okurowski.
Building a discourse-tagged corpus in the framework of rhetorical structure the-ory.
In Proceedings of the 2nd SIGDIAL Workshop on Dis-course and Dialogue, Eurospeech, 2001.M.
Collins.
Head-driven statistical models for natural languageparsing.
Computational Linguistics, 29(4):589?638, 2003.J.
R. Hobbs, M. Stickel, D. Appelt, and P. Martin.
Interpretationas abduction.
Artificial Intelligence, 63(1?2):69?142, 1993.W.
C. Mann and S. A. Thompson.
Rhetorical structure theory:Description and construction of text structures.
In G. Kem-pen, editor, Natural Language Generation: New Results inArtificial Intelligence, pages 279?300.
1986.D.
Marcu.
The rhetorical parsing of unrestricted natural lan-guage texts.
In Proceedings of ACL/EACL, pages 96?103,Somerset, New Jersey, 1997.D.
Marcu.
A decision-based approach to rhetorical parsing.In Proceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics (ACL99), pages 365?372,Maryland, 1999.E.
Miltsakaki, R. Prasad, A. Joshi, and B. Webber.
The PennDiscourse TreeBank.
In Proceedings of the Language Re-sources and Evaluation Conference, Lisbon, Portugal, 2004.J.
D. Moore and M. E. Pollack.
A problem for RST: The needfor multi-level discourse analysis.
Computational Linguis-tics, 18(4):537?544, 1992.S.
Oates.
Generating multiple discourse markers in text.
Mas-ter?s thesis, ITRI, University of Brighton, 2001.S.
Oepen, E. Callahan, C. Manning, and K. Toutanova.
LinGORedwoods?a rich and dynamic treebank for HPSG.
In Pro-ceedings of the LREC parsing workshop: Beyond PARSEVAL,towards improved evaluation measures for parsing systems,pages 17?22, Las Palmas, 2002.D.
Schlangen and A. Lascarides.
Resolving fragments usingdiscourse information.
In Proceedings of the 6th Interna-tional Workshop on the Semantics and Pragmatics of Dia-logue (Edilog), Edinburgh, 2002.R.
Soricut and D. Marcu.
Sentence level discourse parsing usingsyntactic and lexical information.
In Proceedings of HumanLanguage Technology and North American Association forComputational Linguistics, Edmonton, Canada, 2003.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, D. JurafskyR.
Bates, P. Taylor, R. Martin, C. van Ess-Dykema, andM.
Meteer.
Dialogue act modeling for automatic tagging andrecognition of conversational speech.
Computational Lin-guistics, 26(3):339?374, 2000.M.
Stru?be and C. Mu?ller.
A machine learning approach to pro-noun resolution in spoken dialogue.
In Proceedings of the41st Annual Meeting of the Association for ComputationalLinguistics (ACL2003), pages 168?175, 2003.J.
van Eijk and H. Kamp.
Representing discourse in context.In J. van Benthem and A. ter Meulen, editors, Handbook ofLogic and Linguistics, pages 179?237.
Elsevier, 1997.R.
Vieira and M. Poesio.
Processing definite descriptions incorpora.
In Corpus-based and computational approaches toanaphora.
UCL Press, 2000.J.
M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and K. J. Mc-Keever.
An empirical approach to temporal reference resolu-tion.
Journal of Artificial Intelligence Research, 9:247?293,1998.103
