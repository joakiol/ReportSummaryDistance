JDomain-Specific Semantic Class Disambiguation Using WordNetLi Sh iuan  PehDSO National Laboratories20 Science Park DriveSingapore 118230pl ishiua~dso, org.
sgHwee Tou  NgDSO National Laboratories20 Science Park DriveSingapore 118230nhweetou@dso, org.
sgAbst ractThis paper presents an approach which ex-ploits general-purpose algori.t~m~ and re-sources for domain-specific semantic classdis~mhiguation, thus facilitating the gen-eralization of semautic patterns fTomword-based to class-based representations.Through the mapping of the donza?u-specific semantic hierarchy onto Word-Net and the application of general-purposeword sense disambiguation and semanticdistance metrics, the approach proposes aportable, wide-coverage method for disam-biguating semantic classes.
Unlike exist-ing methods, the approach does not requireannotated corpora.
When tested on theMUC-4  terrorism domain, the approachis shown to outperform the most frequentheuristic substan~lly and achieve compa-rable accuracy with human judges.
Itsp~fo?~ance also compares favourably withtwo supervised learning algorithm.q.1 In t roduct ionThe semantic classi~cation f words refers to theabstraction of ambiguous (surface) words to un-ambiguous concepts.
These concepts may be ex-plicitly expressed in a pre-defmed taxonomy ofclasses, or implicitly derived through the clusteringof sen~-ticany-related words.
Semantic classifica-tion has proved useful in a range of application ar-eas, such as information extraction (Soderland et at.,1995), acquim'tion of domain knowledge (Mikheevand Finch, 1995) and improvement of parsing accu-racy through the speci~cation of selectional restric-tions (Grishman and Sterling, 1994; Gri~h,n~n audSterling, 1992).In this paper, we address the problem of s~manticclass disambiguation, with a view towards applyingit to information extraction.
The disambiguation ofthe semantic class of words in a particular contextfacilitates the generalization of semantic extractionpatterns used in information extraction from word-based to class-based forms.
This abstraction is effec-tively taFped by CRYSTAL  (Soderland et aL, 1995),one of the first few approaches to the automatic in-.duction of extraction patterns.Many existing information extraction systems(MUC-6, lg96) rely on tedious knowledge engineer-ing approaches to hard-code semantic classes ofwords in a semantic lexicon, thus hampering theportability of their systems to di~erent domaln~.A notable exception is the approach taken by theUniversi~ of Massachusetts.
Its knowledge acquisi-tion framework, Kenmore, uses a case-based learn-ing mech--;am to learn domain knowledge automat-icaUy (Cardie, 1993).
Kenmore, being a supervisedalgorithm, relies on an annotated corpus of domain-specific classes.
Grishman et aL (1992) too venturedtowards automatic semantic acquisition for informa-tion extraction.
However, they expressed reserva-tions regmrding the use of WordNet to augment theirsemantic hierarchy automatically, citing examples ofunintemded senses of words resulting in erroneous se-mantic l~L~Sz~ation.To circumvent the ~notation bottleneck faced byKenmore, our approach exploits general a~orithmsand resources for the disambiguation of do,~i--specific semantic classes.
Unlike Grishmau et al'sapproach, our application of general word sense dis-ambiguation algorithms and semantic distance met-rics allows for an effective use of the Rue sense gran-ularity of WordNet.
Experiments carried out on theMUC-4 (1992) terrorism domain saw our approachouttperform~g supervised algorithms and matchingb,~n judgements.56iIIIiFigure I : Semantic Class Dis~mhiguation.2 Our  ApproachAs opposed to proponents of "domain-specific in-formation for domain-specific applications", our ap-proach veztures towards the application of general-purpose algor\]t~,~ and resources to our dom~i,-specific s~rn~tic class disaznbiguation problem.Our information source is the extensive seman-tic hierarchy WordNet (Miller, 1990) which was de-signed to capture the semantics of general nuancesand uses of the English language.
Our approach rec-onciles the domain-specific hierarchy with this ~astnetwork and exploits WordNet o uncover semanticci~s~es, without he need of an ~otated  corpus.Firstly, the domain-specific hierarchy is mappedonto the semantic network of WordNet, by manu-ally as.~zni~g corresponding WordNet node(s) to theclasses in the do~,~-speci~c hierarchy.
To disem-biguate a word, the sentence context of the wordis first streamed through a general word sense dis-ambiguation module which assigns the appropriatesense of the word.
The word sense disambiguationmodule hence effectively pinpoints a partic~l~r nodein WordNet that corresponds to the current senseof the word.
Thereafter, this chosen concept nodeis piped througJa a semantic distance module whichdetermines the s~m~c distances between this con-cept node and all the s~m~,~tic class nodes in thedomain-speci~c hierarchy.
If the distance betweenthe concept node and a semantic class node is be-low some threshold, the semantic class node becomesa candidate class node.
The nearest candidate eJ~ssnode is then chosen as the semautic class of the word.If no such candidates exist, the word does not belongto any of the semantic classes in the hierarchy, andis usually labelled as the "entity" class.
The flow ofour approach is illustrated in Figure 1.A walkthrough ofthe approach with a simple ex-ample w~l better illustrate it.
Consider a domain-specit~c hierarchy with just 3 classes :- VEHICLE,AIRCRAFT and CAR, as shown in Figure 2(a).Mapping this domainospeci~c hierarchy to Word-Net simply involves finding the specific sense(s) of57m r motor_vehic le :  1Figure 2 : (a) A simple domain-specific hier-archy (b) The classes of the domain-specifichierarchy as mapped onto WordNet, togetherwith the word to be dis~mhigtmted, "plane' .the classes.
In this case, all three classes correspondto their first sense in WordNet.Then, given a sentence, say, "The plane win betaking off in 5 minutes time.
~, to dis~m~iguate thsemantic class of the word "plane", the sentence isfed to the word sense disambiguation module.
Themodule win determine the sense of this wor& Inthis example, the correct sense of "plane" is sense 1,i.e.
the sense of an aeroplane.
Having identified theparticular concept node in Word.Net that "plaue"corresponds to, the distances between this conceptnode and the three semantic class nodes are then cal-culated by the semantic distance module.
Based onWordNet, the module will conclude that the conceptnode "plane:l" is nearer to the semantic lass node"aircraft:l" and should hence be cl~Lssified as AIR-CRAFT.
Figure 2(b) shows the relative positions ofthe concept node ~plane:l ~and the three semanticcl~q nodes in Word_Net.2.1  Word Sense Dis~mhlguationWord sense disambiguation s an active research areain natural language processing, with a great numberof novel methods proposed.
Methods can typicallybe delineated along two dimensions, corpns-basedvs.
dictionary-based approaches.Corpus-based word sense disambignation algo-rjthm~ such as (Ng and Lee, 1996; Bruce and Wiebe,1994; Yarowsky, 1994) relied on supervised learn-ing fzom annotated corpora.
The main drawbackof these approaches i  their requirement ofa sizablesense-tagged corpus.
Attempts to alleviate this tag-bottleneck i~lude tmotst r~ias  (Te~ ot ill,,1996; Hearst, 1991) and unsupervised algorith~(Yarowsky, 199s)Dictionary-based approaches rely on linguisticknowledge sources uch as ma~l~i,~e-readable dictio-naries (Luk, 1995; Veronis and Ide, 1990) and Word-Net (Agirre and Rigau, 1996; Resnik, 1995) and e0(-ploit these for word sense disaznbiguation.Thus far, two notable sense-tagged corpora, thesemantic oncordance of WordN'et 1.5 (Miller et al,1994) and the DSO corpus of 192,800 sense-taggedocctuTences of191 words used by (Ng and Lee, 1996)are still insu~cient in scale for supervised algorithmsto perform well on a wide range of texts.Unsupervised algorit~m~ such as (Yarowsky,1995) have reported good accuracy that rivals thatof supervised algorithms.
However, the algorithmwas only tested on coarse-level senses and not onthe refined sense distinctioas of WordNet, which isthe required sense granularity of our approach.We hence turn to dictionary-based approaches, fo-cusing on WordNet-based algorithms Since they fitin snugly with our WordNet-based semantic lassdisambiguation task.In format ion  ContentResnik (1995) proposed a word sense disambigua-tion algorithm which determ~ the senses of wordsin noun groupings.
The sense of a word is disam-biguated by choosing the sense which is most highlysupported by the other nouus of the noun group.The extent of support depends on the informationcontent of the subsumers of the nouns in Word.Net,whereby information content is defined as negativelog 1;1~1.~hood -togp(c), and p(c) is the probabilityof encountering an instance of concept c.As mentioned in his paper, although his approachwas only reported on the disambiguation f words inrelated noun groupings, it can potentially be appliedto word sense disambiguation of nouns in r-~-;~gtext.In our implementation f his approach, we appliedthe method to general word sense disambiguation.We used the surrounding nouns of a word in freevmn~g text as the "norm grouping" and followedhis algorit~r~ without modifications ~.Conceptua l  Dens i tyAgirre and Rigau:s (1996) approach as a ~imilarmotivation as Kesnik's.
Both approaches hinge onthe belief that surrounding noun.~ provide strongclues as to the sense of a word.The main difference lies in how they determine theextent of support offered by the surrounding nouns.Agirre and Rigau uses the conceptual density of theancestors of the nouns in WordNet as their metric.Our implementation foliow$ the pseudo-code pre-ZThe pseudo-code of his algorithm is detailed in(Res~ik, x995).=Surrounding ouns in the o~na\ ]  ResnJk's approachrefers to the other nouns in the noun grouping.umted in (Agirre and Rigan, 1996) s. For wordswhich the algorithm failed to disambiguate (whenno senses or more than one sense is returned), werelied on the most frequent heuristic.2.2 Semant ic  DistanceThe task of the semantic distance module is to re-flect accurately the notion of "closeness" betweenthe chosen concept node of the word and the seman-tic class nodes.
It thus requires a metric which caneffectively represent the semantic distance bet~veentwo nodes in a taxonomy such as Word.Net.Conceptua l  DistanceRada et.
al (1989) proposed such a metric termedas conceptual distance.
Conceptual distance be-tween two nodes is defined as the m~.ir-mn um-ber of edges separating the two nodes.
Take theexample in Figure 2(b), the conceptual distance be-tween "plane:l" and "aircraft:I" is 1, that between=plane:l" and "vehicle:l" is 2, and that between=plane:l" and "car:l" is 44.L ink Probab i l i tyThe 11~1~ probability metric is our variant of theconceptual distance metric.
Instead of consideringall edges as equi-distance, the probability of the 1.1n\]?
(or edge) is used to bias its distance.
This metric ismotivated by Resnik's use of the probability of in-stance occurrences of concepts, p(c) (Resnik, 1995).Link probability is defined as the difference betweenthe probability of instance occurrences of the parentand child of the \]i.k~ Formally,Lin&l'P(e, b) = p(a )  - p (b) ,SWe clarified with the authoz~ certain parts of thealgorithm which we find unclear.
These axe the poin~worth noting :-(1) corrtpu%e.concephtaLdens/b 9 of Step 2 only computesthe conceptual density of concepts which are not ~-rkedinva~d;(2) ex/%Ioop of Step 3 occurs whsu all senses subsumed byconce~ were already pzeviously disambiguatecl orwhenone or more senses of the word to be disambiguated aresubsumed by con~elm~(3) ~z~rLd~=r'n.5~r~zte&ser~ of Step 4 marks sensessubsumed by concept as disambiguated, marks conceptand its clfddren as invalid, and discards other senses ofthe wor~ wi~ sere(s) disambiguated by ?on~;(4) disambiguated se~es of 'words which form the con-text are not brought forward to the next window.41.n Word.Net, these are 25 unique beginners of thetaxonomy, instead of a co~on root.
Hence, in our hn-plementation, we ~.ign a large conceptual distance of999 to the virtual edges between two unique beginners.IIiiIi III!IIIIiIIIi58 !~b.ere ~?)
ffi~h,e.re wm,ds(?)
i~ 'the ?e?
o,f ~,0~.~  the~orlm,~ w~h ~re a~ba~med b~ the ?~.
.e~ ?,~d 2V/# the ~o~Z ~mbee o f  ~to~ffi~o?~rr~/n  ~e ?orp~,  (~.e~/k ,  1998)a m ~r~t~ o f  the linJ~,b m ?dt//.d o.f the link.The intuition behind this mewic is that the dis-tance between the parent and the child should be"closer if the probability of the parent is close tothat of the child, since that implies that wheneveran instance of the parent occurs in the corpus, it isusually an instance of the child.Descendant  CoverageIn the same spirit, the descendant coverage met-tic attempts to tweak the constant edge distanceassumption of the conceptual distance metric.
In-stead of relying on corpus statistics, static inforn~.-tion from Word.Net is exploited.
Descendant cov-erage of a l~nlc is defined as the difference in thepercentage of descendants subsumed by the parentand that subsumed by the child :-Des~ee(a ,  b) ---- ~(a) --  d(b),Tota l  ~umber  o f  de~cet~d~,~ i WordNe?
"b ~ ~ o~ ~he iinJ#.The same intuition underlies this metric; that thedistance between the parent and the child should be"nearer" if the percentage ofdescendants subsumedby the parent is close to that of the child, since itindic~es that most descendants of the pare~ arealso descendants of the child.Taxonomic  Link (IS-A)All the metrics detailed above were designed tocapture semantic similarity or closeness.
The seman-tic class d i sambi~ion  problem, however, is essen-tially to identify membership of the chosen conceptnode in the semantic lass nodes.A simple implementation f the s~n~n~c distancemodule can thus be just a waversal of the taxonomicl~b~ (IS-A) of Word.Net.
If the chosen concept nodeis s descendant of a s~n~=~ic class node, it shouldbe classified as that s~a~t ic  lass.3 Eva luat ionThe domain we worked on is the MUC-4 (1992) ter-rorism domaln.
Nouns are extracted from the first 1859passages (dev-muc4-0001 to dev-muc4-0018) of thecorpus of news wire art.ides to form our test corpus.The nouns extracted are the head nouns within nounphrases which are recognised by WordNet, includingproper nouns such as "United States".
These 1023nouns are hand-tagged with their sense and seman-tic class in the particular context to form the answerkeys for subsequent experiments.3.1 Mapp ing  dom~,;~-specifi?
h ierarchyonto Word.NetThe domain-specific hierarchy used in our work isthat crafWd by researchers fzom the University ofMassachusetts for their information extraction sys-tem, which was one of the participants at MUC-4(Riloff, 1994).Mapping from the dom~,~-specific hierarchy toWordNer ~3rpically requires only the assignment ofsenses to the classes.
For instance, the semanticclass "human" is mapped onto its sense I node inWordNet, the uhuman:l" concept node.
Classes canalso be mapped onto more than one concept node inWordNet.
The semantic lass "attack", for e~ample,is mapped onZo both senses I and 5.There are cases where the exact wording of a se-mantic class in the domain-specific hierarchy is notpre~mt in WordNet.
Take for instance the seman-tic class ~goveroment..ot~.cia/" in the domain-specifichiermx:hy.
Since the collocation is not in Word-Net, we mapped it to the concept node ~govern-ment.agent:l" which we felt is closest in meaning.The set of mapped semantic l~-~Ses in WordNetis shown in Figure 3 s.3.2 Word Sense Dis~mhigzmtlonWe ran our two/mplementstions of word sense dis-ambiguation algorithms, the information content al-gorithm and the conceptual density method, on ourdomain-specific rest set.
For the information contentalgorithm, a window size of 10, i.e.
5 nouns to thelefz and right, was found to yield the best results;w~1~t for the conceptual density algorithm, the op-t imum window size was found to be 30.
For bothalgorithm% only the nouns Of the same passage areincorporated into the context window.
If the nounto be disambiguated is the first noun of the passage,the window will include the subsequent .Nnouns ofthe same passage.The probability statistics required for Resuik'stneematton ?~um?
algoctchm were eonecmdsAs this hie~zchy is adopted, and not created by us,occasionally, "we can only furnish guesses as to the exactmeaning of the semantic classes.II?~po?
lw i~: l.~,~.
,~:  1 ~ 1L_,.. ~ Jm~F igure  3 : MUC-4  semantic lass h ierarchy as mapped onto WordNet .777,857 noun occurrences of the entire Brown cor-pus and Wall Street Jottrnal corpus.The results are shown in Table I.
The most fre-quent baseline is obtained by following the stxategyof always picking sense 1 of WordNet, since Word-Net orders its senses such that sense I is the mostlikely sense.As both algofithm.q performed below the most fre-quent baseline, it prompted us to evaluate the in-dicativeness of surrounding nouns for word sensedisambiguation.
We hence provided 2 h ,m~ judgeswith a randomly selected sample of 80 ex~wples fromthe 734 polysemic nouns of our test corpus of 1023e~'~ples.
The human judges are provided with the10 nouns surrounding the word to be disambiguated.Based only on these clues, they have to select a sin-gle sense of the word in the particular sentence con-text.
Their responses are then tallied with the seuse-tagged test corpus.Table 2 shows the accuracies attained by the hu-man judges.
Both judges are able ?o perform sub--scantially better than the most frequent heuristicbaseline, despite the seeming)y impoverished knowl-edge source.
Feedback from the)udges reveal possi-ble leverage for future improvements.
Firstly, judgesreflect hat frequently, just one indicative surround-ing noun is enough to provide clear evidence forsense disambig~tion.
The other nouns will just beglossed over and do not contribute to the decision.ALso, indicative nouns may not just hold is-a rela-tionships, which are the only relationships exploitedby both algorithms.
Rather, they are simply relatedin some m~-ner to the noun to be disambiguated.For instance, a surzounding context including theword "church ~will indicate a strong support for the"pastor" sense of ~m;~i~ter ~ as opposed 1;o its otherse~.ses.
These reflections of the human judges seemto point towards the need for an effective methodfor selecting only particular nouns in the surround-ing context as evidence.
Use of other relatioushipsbesides is-a may also help in d isambi~t ion,  as isalready expounded by (Sussna, lg93).3.3 Semantic  Distance Metr icsTo evaluate the semantic distance metrics, we feedthe se~t ic  distance mod~e with the correct sensesof the entire test corpus and observe the resultantsemantic c!~ss disambiguation accuracy.The conceptual distance, link probability and de-SCend~mt coverage metrics all require trAversal of11~1~ from one node to another.
However, all of themetrics are commutative, i.e.
distance from concepta to b is the same as chat from b to ~ In seman-tic class d isambi~t ion,  a distinction is necessarysince the taxonomic links indicate membership re-lationships which are not commutative ("aircraft:l"is a "vehicle:l ~but "vehicle:l ~need not be an "air-craft:l ').
We hence associate different weights tothe upwards and downwards traversal of links, withthe 25 unique be~ers  of Word.Net being the top-most nodes.
Upward traversal of links towards theunique beginners are weighted consistently at 0.3whilst downward traversal of links towards the leavesIi!Ii!iI|/iiiiIi60 iIIiIII |++IIII~.er~at i~ co=ent (po.t~ena?
)Conceptual deasity (polysemic) ,"C,o.cez~t-~a~ denS" +.,,Most fzeque~.t heuristic (po\]ysemlc)Most frequent heuristic (polysemlc)Information content (overall) ....Conceptual density (overall)Conceptroa.l density +Most frequent headstic (overall)Most, fre~ent heuz-~'tic (overaU)~Examples #l~m~iguated I ~Correct I Accu~tcy734 734 292 39.78 %734 27~ 68 24.73 %734 734 I 385 I 52.4,5 %734 ' 734 464 63.22%\[o~ II I ,I?
:H, --_.~gllv~1023107,3102,31023674 ~3 65.88 % 73.61%Table 1: Word  sense disnmhiguat ion results.#Bxamples #Correct AccuracyHuman A 80 57 71.25 ~0Human B 80 59 73.75 %Most frequent heuristic 80 45 56.25 %' l~ble 2: Word sense dls~mhiguation using surrounding nouns.are weighted at 1.7 s..Also, different thresholds axe used for different lev-els of the domain-specific hierarchy.
Since higherlevel classes, such as the level 0 "human" class,encompasses at wider range of words, it is evidentthat the thresholds for higher level classes-r~n-otbe stricter than that of lower level classes.
For faircomparison of each metric, the best thresholds arearrived through exhaustive searching of a reasonablespace 7.
The results are detailed in Table 3.Accuracy on specific se~,mtic classes refers to anexact match of the pcogram's response with the cor-pus answer.
The general ~n~t ; i c  lass disambigua-tion accuracy, on the other hand, considers a re-spouse correct as long as the response class is in thesub-hierarchy which originated, fz'om the same level0 class as the answer.
For example, if the program'sreeponse is class =po l i t i~" ,  whilst the answer isclass =lawyer", since both e\]~qses originated from thesame level 0 class =b-m~ ~, this response is consid-ered correct when calculating the general semanticclass accuracy.
The specific se~t ic  class disam-biguation accuracy is hence the stricter measure.It may seem puzzling that semantic class disam-biguation does not achieve 100% accuracy even whensupplied with the correct senses, i.e.
even when theword sense d;~mhiguation module is able m attain100~0 accuracy, the overall semantic class disam-biguation accuracy still lags behind the ideal.
SinceSThese weights are found to be optimum for all threeznetric$.~Integral thresholds are searched for the conceptualdistance meetri~ whilst the thresholds of the other met-tics are searched in steps of 0.01.61the taxonomic 1~nlc~ in Word.Net are designed to cap-ture membership of words in classes, it may sennodd that the correct identification of the word sensecoupled with the IS-A taxonomic 1~ still do notguarantee correct semantic lass disambiguation.The reason for this paradox is perceptive di~er-ences; that between the designers of the MUC-4domain-specific hierarchy we adopted and the Word-Net hierarchy, and that between the an-orator of theanswer corpus and the WordNet designers.Take for example the monosemic word "kidnap-ping".
Its correct semantic class is =a~ack:5 s'.However, it is not a descendant of =attack:Y inWord.Net.
The hypemyms of "kidnapping" axe \[cap-ture ~ felony --~ crime --> evil-doing -+ wrong-doing--> activity .-+ act\] and thatt of =attack:5" are \[bakery--~ crime ~ evil.doing ~ wrong-doing ~ activityact\].
Both perceptions of =kidnatpping" are correct.
"kidnapping" can be viewed as a form of =attack:Yand ~m+\]~dy, it can be viewed as a form of =cap-t~re  ~ .An effective semantic distance metric is henceneeded here.
The semantic distance module shouldinfer the close distance between the two conceptnodes "kidnapping" aud "attack:5" and thus colrectly classify "lddz~ppin~.3.4 Semant ic  Class Dis~mTdguationAfter evaluation of the separate phases, we corn-blued the best algorithms of the two phases andevaluated the performance ofour semantic lass dis-ambiguattion approach.
Hence, the most ftequentS=attack:5" refers to an assault on someone whilst'%track:l" refers to the be~n~g of an o~m~rve.Disambiguation Accuracy Thresholds ~Specific C\]a.sses General CCla~esConceptual Distance .... 81.52 % .
87~10 % (3,2,2,1)Link Probability 80.16 % 85.24 % ~ 10,1,0.01,0.01,0.01)Co,  e % I 83.87 % i (}.02,0.01,0.01,0.01)Taxonomic L 79.67 % !
85.14 % L Not applicableTable 3: Effect o f  dif ferent semantic distance metr ics  on semaut ic  class dls=mblguation.
(Ass -m;~g per fect  word sense dls=n'~higuation)~Format :- (t~o, t~z, t~, t~s), where tz~ is the threshold that is applied to the ith level of the hierarchy.sense heuristic is used for the word sense disambigua-tion module and the conceptual distance metric isadopted for the semantic distance moduleIt should be emphasized, however, that our al>-proach to s~m~-tic class disambiguation need not becoupled with any specific word sense disambiguationalgorithm.
The most frequent Word.Net sense is cho-sen simply because current word sense disambigua-tion algofithm~ still cannot beat the most frequentbaseline consistently for all words.
Our approach,in effect, allows domain-specific s~-~ic  class dis-~mBiguation tO latch onto the improvements in theactive research area of word sense disambiguation.As a baseline, we again sought the most frequentheuristic, which is the occurrence probability of themost frequent senantic lass "entity".
9We compared our approach with supervised meth-ods ?o contrast their reliance on annotated corporawith our r~nce  on WordNet.
One of the fore-most semantic e.l~?,S disambiguation system whichemploys machine learning is the Kenwore framework(Cardie, 1993).
Huwever, as we are unable to reportcomparative t sts with K~ore  z?, we adapted cwoother supervised algorithm% both successfully ap-plied to general word sense di~mhiguation, to thetask of semantic class disambiguation.The first is the LBXAS algorithm which uses anexemplar-based learning framework s;mi l~-  to thecase-based reasoning foundation of Kenmore (Ng,1997; Ng and Lee, 1996).
L ~  was shown toachieve high accuracy as compared to other wordsense disambiguation algorithms.We also applied Teo et als Bayesian word sensedisambiguation algorithm to the task (Teo et al,1996).
The approach compares favourably withother methods in word sense disambiguation whentested on a common data set of the word "interest".9This baseline is also used to evaluate the perfor-mance of K~ore  (Cardie, 1993).Z?As work on one of the important input sources, theconceptu~ parser, is underway, per~.___ce results ofKenm~e on S~m~t~ic class dL~higuation cannot yetbe reportecLThe features used for both supervised algorithmsare the local collocations of the surrounding 4words zz.
Local collocation was shown to be the mostindicative knowledge source for LBxA8 and these7 features are the common features used in bothLF~X.AS and Teo et als Bayesian algorithm.
Bothalgorithmg are used for learning the specific sema--tic class of words.For both algorithmg, the 1023-sentence test set israndomly partitioned into a 90% training set anda 10% testing set, in proportion with the overallclass distribution.
The algorithms are trained onthe t r~;ng  set and then used to d is~tdguate thedistinct esting set.
This was averaged over 10 runs.As with K~more,  the t r~-~g set contains featuresof all the words in the training sentences, and thealgorithms are to pick one s~- t i c  class for eachword in the testing set.
A word in the testing setneed not have occurred in the training set.
Thisis --fflce word sense disambiguation, whereby thetraining set cont~-~ features of one word, and thealgorithm picks one sense for each occurence of thisword in the testing set.To obtain a g~uge of human performance on thistask, we sourced two independent human judge-ments.
Two human judges are presented with a setof 80 sentences randomly selected from the 1023-example test corpus, each with a noun to be disam-biguated.
Based on their understanding of the sen-tence, each noun is assigned aspecific semantic la.~of the dom~n-specific hierarchy.
Their responses arethen compared ag~t  the tagged answers of the testcorpus.The s ,~ ic  class disambiguation results arecompiled and tabulated in Table 4.
The definitionsof general and specif ic semantic lass disambigttationaccuracy are detailed in Section 3.3.As is evident, our approach outperforms the mostfrequent heuristic substantially.
Also, the perfor-zZGiven a word win the following sentence segment :-12 12 w rz ~'=, the 7features used are 12-h, lz..rl, rl..r2,12,l~, r2 and ~'2, whereby the first 3 features are concatena-tions off the words.62IIIIIIiIIiiIIIIIiIIOur Approach (1.023 exm:~i'~les)Most fzequent heuristic (10233 examples)f S~ervised :LZXAS) ST.a0 %Supervised :Bayes)...
s.7.
:s %Our Approac~h (80 examples) .... 71.15 %HI~ C (80 examples) W.50%Hruman D (S,O,.examples) 70.00 %Most fzequent heur~c (80 example_ ) ~1.~ %Table 4: Sem~mtic lass dlsambiguation results.Disambiguation AccaracySpedific Classes General Classes7s.9o % 8o.1646.92 % 46.92 %57.30%58.88%75.00 %82.50 %75.00 %51,25 %mance of both supervised algorithms lag b-hl-d thatof our approach.
Comparable performance with thetwo human judges is also achieved.It should be noted, though, that the amount oftraining data available to the supervised algorithmsmay not be sufficient.
Ng and Lee (1996) found thattrain/rig sets of 1000-1500 e~mples per word arenecessary for sense dJ-~mhiguation ofone highly am-biguous word.
The amount of Er~ining data neededfor a supervised learning algorithm to achieve goodperformance on semantic class disambiguation maybe larger than what we have used.
Cardie (1993),for instance, used a larger 2056-instance case base inthe evaluation of K~ore .4 Conc lus ionWe have presented a portable, wide-coverage ap-proach to domain-specific semantic d~ disam-biguation which performs comparably with humanjudges.
Our approach arnesses WordNet eHectivelyto outperform supervised methods which rely on an-nots~ed corpora.
Unlike existing methods which re-quire h~d-cra.fting oflexicon or ~-ua l  annotation,the only human etfort involved in our approach isthe mapping of the domain-specific semantic lassesonto WordNeer.
Through the use of general wordsense disaznbiguation algorithms and semantic dis-tance metrics, our approach correlates the perfor-mance of semantic lass disambiguation with the im-provemen~ in these actively researched fields.ReferencesEneko Agirre and Germ~- Rigau.
1996.
WordSense Disambiguation Using Conceptual Density.In Proceedings ofCOLING-96.Rebecca Bruce and Janyce Wiebe.
1994.
WordSense Disambiguation using Decomposable Mod-eels.
In Proceedings of A CL-94.Claire Cardie.
1993.
A Case-Based Approach toKnowledge Acquisition for Domain-Specific Sen-tence .Analysis.
In Proceedings of AAAI-9?Ralph Grishman, John Sterling and CatherineMadeod.
1992.
New York Univexs/ty ProteusSystem: MUC-4 Test Results and Analysis.
InProceeding8 of MUG-J.Ralph Grishman and John Sterling.
1992.
Acqui-sition of Selecrional Pstterns.
In Proceedings ofCOLING-9~.Ralph Grishman and John Sterling.
1994.
Gener-alizing Automatically Generated Selectional Pat-terns.
In Proceedings ofCOLING-#4.Marti A. Hearst.
1991.
Noun Homograph Disam-biguation Using Local Context in Large Text Cor-pora.
In Using Corpora, Univers/ty of Waterloo,Waterloo, Ontario.Alpha K. Luk.
1995.
Statistical Sense Disambigua-tion with Relatively Small Corpora Using Dictio-vary Definitions.
In Proceedings of A CL-#5.Andrei Mi~heev and Steven Finch.
1995.
Towards aWordbench for Acquis/tion of Domain Knowledgefrom Natural Language.
In Proceedings of EAUL-g5.George A.
Miner.
1990.
An On-Line LexicalDatabase.
In International Journal of Lezicography, 3(4):235-312, 1990.George A. Miller, Martin Chodorow, Shad Landes,Claudia Leacock, and Robert G. Thom~?.
1994.Using a Semantic Concordance for Sense Identifi-cation.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.MUC-4 Proceedings.
1992.
Proceedings ofthe Fourth Message Understanding Conference(MUU-~), San Msteo, CA: Morgan Ka,,Cm~,m.MUC-6 Proceedings.
1996.
Proceedings of theSizth Message Understanding Conference (MUG-6), San Marco, CA: Morgan Kaufr-a-n.63Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing Multiple Knowledge Sources to DisambigusteWord Sense: An Exemplar-Based Approach.
InProceedings of ACL-96.Hwee Tou Ng.
1997.
Exemplar-Based Word SenseDissmbiguation: Some Recent Improyements.
InProceedings of the Second Conference on Empiri.cal Methods in Natural Language Proc~sing.Roy Rad~ Hafedh Mill, Enen Bi&nell and MariaBleumer.
1989.
Development and Application of?
a Metric on S~m~tic Nets.
In IEEE Transactionson Systems, Man, and Cybernetics, Vol.
19, No.1, Jan/Feb.Philip I:tesnik.
1995.
Disarnbigua,ting Norm Group-ings with Respect to Word.Net Senses.
In.
Praceed-ings of the Third Workshop on Very Large Cot-.pore.Ellen M. Riloff.
1994.
Informstion Extraction Asa Basis for Portable Text Classification Systems.PhD the~, University of Massachmetts, Septem-ber 1994.Stephen Soderland, David Fisher, Jonathan Aseltineand Wendy Lehnert.
1995.
CRYSTAL: Inducing aConceptual Dictionary= In Proceedings ofIJCAI-95.Michael Sussna.
1993.
Word Sense Disambiguationfor Free-TexZ lude~:ing using a massive ,Se~ticNet~rork.
In Proceedings of the Seoond Interna-tional Co~ferenoe nInformation and KnowledgeManagement (CIKM-93).Edward Teo, Chzistopher Ting, Li-Shiuan Peh andHian-Beng Lee.
1996.
Probabilistic Word-SenseDisambiguation a d Bootstrapping with Unsuper-vised Gradient Descent.
manuscript.Jean Veronis and Nancy Ide.
1990.
Word SenseDisambigustion with Very Large Neural Networksextrazted from Machine Readable Dictionaries.
InProcee.d: g6 of COLING-gO.David Yarowsky.
1994.
Decision Lists for Lexi-cal Ambiguity Resolution: Application to AccentRestoration i Spanish and French.
In Proceed/rigsoi ACL-g~.David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation RivMi~g Supervised Methods.
InP~ocecedmgs of A ~ 95.64 I, ,65
