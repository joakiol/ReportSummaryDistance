Attribute Selection for Referring Expression Generation:New Algorithms and Evaluation MethodsAlbert GattDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKa.gatt@abdn.ac.ukAnja BelzNatural Language Technology GroupUniversity of BrightonBrighton BN2 4GJ, UKa.s.belz@brighton.ac.ukAbstractReferring expression generation has recentlybeen the subject of the first Shared Task Chal-lenge in NLG.
In this paper, we analyse thesystems that participated in the Challenge interms of their algorithmic properties, compar-ing new techniques to classic ones, based onresults from a new human task-performanceexperiment and from the intrinsic measuresthat were used in the Challenge.
We also con-sider the relationship between different eval-uation methods, showing that extrinsic task-performance experiments and intrinsic evalu-ation methods yield results that are not signif-icantly correlated.
We argue that this high-lights the importance of including extrinsicevaluation methods in comparative NLG eval-uations.1 Introduction and BackgroundThe Generation of Referring Expressions (GRE) isone of the most intensively studied sub-tasks ofNatural Language Generation (NLG).
Much re-search has focused on content determination in GRE,which is typically framed as an attribute selectiontask in which properties are selected in order todescribe an intended referent.
Since such proper-ties are typically defined as attribute-value pairs,in what follows, we will refer to this as the AS-GRE (Attribute Selection for Generating ReferringExpressions) task.1.1 Approaches to ASGRESince early work on ASGRE, which focused on prag-matic motivations behind different types of refer-ence (Appelt, 1985; Appelt and Kronfeld, 1987), thefocus has increasingly been on definite descriptionsand identification, where the set of attributes se-lected should uniquely distinguish the intended ref-erent from other entities (its ?distractors?).
UniqueReference in this sense is a dominant criterion forselecting attribute sets in classic ASGRE algorithms.Following Dale (1989), and especially Dale and Re-iter (1995), several contributions have extended theremit of ASGRE algorithms to handle relations (Daleand Haddock, 1991; Kelleher and Kruijff, 2006) andgradable attributes (van Deemter, 2006); and also toguarantee logical completeness of algorithms (vanDeemter, 2002; Gardent, 2002; Horacek, 2004; Gattand van Deemter, 2007).Much of this work has incorporated the prin-ciple of brevity.
Based on the Gricean Quantitymaxim (Grice, 1975), and originally discussed byAppelt (1985), and further by Dale (1989), Reiter(1990) and Gardent (2002), this principle holds thatdescriptions should contain no more informationthan is necessary to distinguish an intended refer-ent.
In ASGRE, this has been translated into a crite-rion which determines the adequacy of an attributeset, implemented in its most straightforward form inFull Brevity algorithms which select the smallest at-tribute set that uniquely refers to the intended refer-ent (Dale, 1989).Another frequent property of ASGRE algorithms isIncrementality which involves selection of attributesone at a time (rather than exhaustive search for adistinguishing set), and was initially motivated byalgorithmic complexity considerations.
The Incre-mental Algorithm of Dale and Reiter (1995) wasalso justified with reference to psycholinguistic find-ings that (a) humans overspecify their references(i.e.
they are not brief when they produce a refer-50ence); and (b) this tends to occur with propertieswhich seem to be very salient or in some sense pre-ferred (Pechmann, 1989; Belke and Meyer, 2002;Arts, 2004).
In the Incremental Algorithm (Daleand Reiter, 1995), incrementality took the form ofhillclimbing along an attribute order which reflectsthe preference that humans manifest for certain at-tributes (such as COLOUR).
This Modelling of Hu-man Preferences in the Incremental Algorithm hasproven influential.
Another feature proposed byDale and Reiter is to hardwire the inclusion of thetype of an entity in a description, reflecting the hu-man tendency to always include the category of anobject (e.g.
chair or man), even if it has no dis-criminatory value.
A compromise may be reachedbetween incrementality and brevity; for example,Dale?s (1989) Greedy Algorithm selects attributesone at a time, and computes the DiscriminatoryPower of attributes, that is, it bases selection on theextent to which an attribute helps distinguish an en-tity from its distractors.In the remainder of this paper, we uniformly usethe term ?algorithmic property?
for the selection cri-teria and other properties of ASGRE algorithms de-scribed above, and refer to them by the followingshort forms: Full Brevity, Uniqueness, Discrimina-tory Power, Hardwired Type Selection, Human Pref-erence Modelling and Incrementality.11.2 ASGRE and EvaluationThough ASGRE evaluations have been carried out(Gupta and Stent, 2005; Viethen and Dale, 2006;Gatt et al, 2007), these have focused on ?classic?
al-gorithms, and have been corpus-based.
The absenceof task-performance evaluations is surprising, con-sidering the well-defined nature of the ASGRE task,and the predominance of task-performance studieselsewhere in the NLG evaluation literature (Reiter etal., 2003; Karasimos and Isard, 2004).Given the widespread agreement on task defini-tion and input/output specifications, ASGRE was anideal candidate for the first NLG shared task evalu-ation challenge.
The challenge was first discussed1While terms like ?Unique Reference?
and ?Brevity?
arecharacteristics of outputs of ASGRE algorithms, we use them asshorthand here for those properties of algorithms which guar-antee that they will achieve these characteristics in the attributesets generated.during a workshop held at Arlington, Va. (Dale andWhite, 2007), and eventually organised as part ofthe UCNLG+MT Workshop in September 2007 (Belzand Gatt, 2007).The ASGRE Shared Task provided an opportunityto (a) assess the extent to which the field has diver-sified since its inception; (b) carry out a compar-ative evaluation involving both automatic methodsand human task-performance methods.1.3 OverviewIn the ASGRE Challenge report (Belz and Gatt,2007) we presented the results of the ASGRE Chal-lenge evaluations objectively and with little inter-pretation.
In this paper, we present the results of anew task-performance evaluation and a new intrinsicmeasure involving the same 15 systems and comparethe new results with the earlier ones.
We also exam-ine all results in the light of the algorithmic proper-ties of the participating systems.
We thus focus ontwo issues in ASGRE and its evaluation.
We exam-ine the similarities and differences among the sys-tems submitted to the ASGRE Challenge and com-pare them to classic approaches (Section 2.1).
Welook at the results of intrinsic and extrinsic evalu-ations (Section 4) and examine how these relate toalgorithmic properties (Section 4.1 and 4.2).
Finallywe look at how intrinsic and extrinsic evaluationscorrelate with each other (Section 4.3).2 The ASGRE ChallengeThe ASGRE Challenge used the TUNA Corpus (Gattet al, 2007), a set of human-produced referring ex-pressions (REs) for entities in visual domains ofpictures of furniture or people.
The corpus wascollected during an online elicitation experiment inwhich subjects typed descriptions of a target referentin a DOMAIN in which there were also 6 other enti-ties (?distractors?).
Each RE in the corpus is pairedwith a domain representation consisting of the tar-get referent and the set of distractors, each with theirpossible attributes and values; Figure 1(a) shows anexample of an entity representation.In each human-produced RE, substrings are an-notated with the attribute(s) they express (?realise?
).For the Challenge, the attributes in each RE wereextracted to produce a DESCRIPTION, i.e.
a set of51<ENTITY ID="121" TYPE="target"><ATTRIBUTE NAME="colour" VALUE="blue" /><ATTRIBUTE NAME="orientation" VALUE="left" /><ATTRIBUTE NAME="type" VALUE="fan" /><ATTRIBUTE NAME="size" VALUE="small" /><ATTRIBUTE NAME="x-dimension" VALUE="1" /><ATTRIBUTE NAME="y-dimension" VALUE="3" /></ENTITY>(a) Entity representation<DESCRIPTION><ATTRIBUTE NAME="colour" VALUE="blue" /><ATTRIBUTE NAME="orientation" VALUE="left" /><ATTRIBUTE NAME="type" VALUE="fan" /></DESCRIPTION>(b) Description: the blue fan facing leftFigure 1: Example of the input and output data in the ASGRE Challengeattribute-value pairs to describe the target referent.An example, corresponding to a human-producedRE for Figure 1(a), is shown in Figure 1(b).For the ASGRE Challenge, the 780 singular de-scriptions in the corpus were used, and dividedinto 60% training data, 20% development data and20% test data.
Participants were given both input(DOMAIN) and output (DESCRIPTION) parts in thetraining and development data, but just inputs in thetest data.
They were asked to submit the correspond-ing outputs for test data inputs.2.1 SystemsThe evaluations reported below included 15 ofthe 22 submitted ASGRE systems.
Table 1 is anoverview of these systems in terms of five of theclassic algorithmic properties described in Section 1,and one property that has emerged in more recentASGRE algorithms: Trainability, or automatic adapt-ability of systems from data.
This classification ofsystems is based on the reports submitted by theirdevelopers to the ASGRE Challenge.
Properties areindicated in the first column (abbreviations are ex-plained in the table caption).The version of the IS-FBS system that was origi-nally submitted to ASGRE contained a bug and didnot actually output minimal attribute sets (but addedan arbitrary attribute to each set).
Unlike the ASGREChallenge task-performance evaluation, the analysispresented in this paper uses the corrected version ofthis system.3 Evaluation methodsEvaluation methods can be characterised as eitherintrinsic or extrinsic.
While intrinsic methods eval-uate the outputs of algorithms in their own right, ei-ther relative to a corpus or based on absolute evalu-ation metrics, extrinsic methods assess the effect ofan algorithm on something external to it, such as itseffect on human performance on some external task.3.1 Intrinsic measuresIn the evaluation results reported below, we use twoof the intrinsic methods used in the ASGRE Chal-lenge.
The first of these is Minimality, defined asthe proportion of descriptions produced by a systemthat are maximally brief, as per the original defini-tion in Dale (1989).
The second is the Dice coeffi-cient, used to compare the description produced bya system (DS) to the human-produced description(DH ) on the same input domain.
Dice is estimatedas follows:2?
|DS ?DH ||DS |+ |DH |(1)For this paper, we also computed MASI, a ver-sion of the Jaccard similarity coefficient proposedby Passonneau (2006) which multiplies the simi-larity value by a monotonicity coefficient, biasingthe measure towards those cases where DS andDH have an empty set difference.
Intuitively, thismeans that those system-produced descriptions arepreferred which do not include attributes that areomitted by a human.
Thus, two of our intrinsic mea-sures assess Humanlikeness (Dice and MASI), whileMinimality reflects the extent to which an algorithmconforms to brevity, one of the principles that hasemerged from the ASGRE literature.3.2 Extrinsic measuresIn the ASGRE Challenge, the extrinsic evaluationwas performed via an experiment in which partici-52CAM GRAPH IS TITCHT TU B BU DIT-DS FP SC FBS FBN IAC NIL RS RS+ AS AS+Incr Y Y Y Y Y n n n n Y Y Y Y Y YDP Y Y Y Y n n n n n n n Y Y Y YTrain Y Y n n Y Y Y n Y Y Y/n Y Y Y YType Y Y Y Y Y Y Y n n n n Y Y Y YHum Y Y Y Y n Y n n n n n n n n nFB n n n n n n n Y n n n n n n nTable 1: Overview of properties of systems submitted to the ASGRE Challenge.
All systems produce outputs withunique reference.
The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality;DP = Discriminatory Power; Train = Trainability; Type = Hardwired Type Selection; Hum = Human PreferenceModelling; FB = Full Brevity.pants were shown visual representations of the do-mains that were used as inputs in the test data,coupled with a system-generated description on thesame screen.
Their task was to identify the intendedreferent in the domain by means of a mouse click.The rationale behind the experiment was to assessthe degree to which an algorithm achieved its statedpurpose of generating a description that facilitatesidentification of the intended referent.Since system outputs were sets of attributes (seeFigure 1(b)), they were first mapped to NL stringsusing a deterministic, template-based method whichalways maps each attribute to word(s) in the sameway and in the same position regardless of context2.The present analysis is based on results froma new evaluation experiment which replicated theoriginal methodology with a slight difference.While participants in the ASGRE experiment wereshown visual domains and descriptions at the sametime, this experiment sought to distinguish readingand identification time.
Thus, for each system out-put, participants first saw the description, using themouse to call up the visual domain once they hadread it.
This yields three dependent measures: (a)reading time (RT); (b) identification time (IT); (c) er-ror rate (ER), defined as the proportion of trials persystem for which participants identified the wrongreferent.One of the possible complications with the RTmeasure is that, while it depends on the semanticsof a description (the attributes that an algorithm se-lects), the syntactic complexity of the descriptionwill also impact the time it takes to read it.
In our2The template-based string-mapping algorithm was createdby Irene Langkilde-Geary at the University of Brighton.setup, the adoption of a deterministic, one-to-onemapping between an attribute and its linguistic rep-resentation controls for this to some extent.
Sinceevery attribute in any semantic representation willinvariably map to the same surface form, there is novariation between systems in terms of how a partic-ular attribute is realised.Design: As in the original experiment, we used aRepeated Latin Squares design in which each com-bination of peer system and test set item is allocatedone trial.
Because there were 148 items in the testset, but 15 peer systems, 2 test set items were ran-domly selected and duplicated to give a test set sizeof 150, and 10 Latin Squares.
The 150 test set itemswere divided into two sets of 75; 15 of the 30 partici-pants did the first 75 items (the first 5 Latin Squares),while the other 15 did the rest.
This resulted in 2250trials (each corresponding to a combination of testset item and system) in all.
For the purposes of thispaper, the duplicate items were treated as fillers, thatis, every item was included only once in the analysis.Participants and procedure: The experimentwas carried out by 30 participants recruited fromamong the faculty and administrative staff of theUniversity of Brighton.
Participants carried out theexperiment under supervision in a quiet room on alaptop.
Stimulus presentation was carried out us-ing DMDX, a Win-32 software package for psy-cholinguistic experiments involving time measure-ments (Forster and Forster, 2003).
Participants initi-ated each trial, which consisted of an initial warningbell and a fixation point flashed on the screen for1000ms.
They then read the description and calledup the visual domain to identify the referent.
Identi-53fication was carried out by clicking on the image thata participant thought was the target referent of thedescription they had read.
RT was measured fromthe point at which the description was presented, tothe point at which a participant called up the nextscreen via mouse click.
IT was measured from thepoint at which pictures (the visual domain) were pre-sented on the screen to the point where a participantidentified a referent by clicking on it.
In additionto the time taken to identify a referent, the programrecorded the location of a participant?s mouse click,in order to assess whether the object identified wasin fact the correct one.
Trials timed out after 15 sec-onds; this only occurred in 2 trials overall (out of2250).4 Results and discussionTable 2 displays aggregate (mean or percentage)scores for each of the intrinsic and extrinsic mea-sures.
As an initial test for differences betweenthe 15 systems separate univariate ANOVAs wereconducted using SYSTEM as the independent vari-able (15 levels), testing its effect on the extrinsictask-performance measures (RT and IT).
For er-ror rate (ER), we used a Kruskall-Wallis ranks testto compare identification accuracy rates across sys-tems3.
The main effect of SYSTEM was significanton RT (F (14, 2219) = 2.58, p < .01), and IT(F (14, 2219) = 1.90, p < .05).
No significant maineffect was found on ER (?2 = 13.88, p > .4).4Systems also differed significantly on Dice(F (14, 2219) = 18.66, p < .001) and MASI scores(F (14, 2219) = 15.94, p < .001).In the remainder of this section, we first considerthe differences between systems based on the algo-rithmic properties listed in Table 1 (and describedin Section 1).
For this part of the analysis, we con-sider intrinsic and extrinsic evaluation measures sep-arately (Section 4.1 and 4.2).
Subsequently, we ex-plicitly compare the intrinsic and extrinsic methods(Section 4.3).3The large number of zero values in ER proportions, and ahigh dependency of variance on the mean, meant that a non-parametric test was more appropriate.4We left out the duplicated items in this analysis.4.1 Differences between system typesAs Table 1 indicates, there are some similaritiesbetween the new systems submitted for the AS-GRE tasks, and the classic approaches discussed inSection 1.
Eleven of the systems are incremen-tal, although only a minority (5 systems) explic-itly hardwire human preferences.
Furthermore, 8systems (all variations on two basic systems, CAMand TITCH) compute the discriminatory value of theproperties selected.
Just one system adopts the FullBrevity approach of Dale (1989), while the major-ity (11 systems) adopt the Dale and Reiter conven-tion of always adding TYPE.
Perhaps the greatestpoint of divergence between the ASGRE Challengesystems and the classic algorithms discussed in Sec-tion 1 is the predominance of data-driven approaches(12 systems are trainable), a characteristic that mir-rors trends elsewhere in HLT.It is worth asking how the particular algorithmicproperties impact on performance, as measured withthe evaluation methods used.
Table 3 displays meanscomputed with all the extrinsic and intrinsic mea-sures, for all system properties except full brevityand discriminatory power.
Since the latter are re-lated to Minimality, which was a separate evaluationmeasure in this study, they are treated separately be-low (Section 4.2).
Note that the aggregate scores inTable 3 are based on unequal-sized samples, sincethis is a post-hoc analysis which the evaluation stud-ies described above did not directly target.Table 3 indicates that systems that were directlytrained on data display higher agreement (Dice andMASI) scores with the human data.
This is rela-tively unsurprising; what is less straightforwardlypredictable is that trainable systems have better task-performance scores (IT and RT).
Systems which al-ways include TYPE and those which are incremen-tal apparently score better overall.
Somewhat sur-prisingly, incorporating human attribute preferencesresults in smaller improvements than incremental-ity and TYPE inclusion.
This is likely due to thefact that the attributes preferred by humans are notstraightforwardly determinable in the people subdo-main (van der Sluis et al, 2007).
In contrast, thosein the furniture domain, which include COLOUR,SIZE and ORIENTATION, are fairly predictable as faras human preference goes, based on previous psy-54Minimality Dice MASI RT IT ERIS-FBN 1.35 0.770 0.601 1837.546 2188.923 6DIT-DS 0 0.750 0.595 1304.119 1859.246 2IS-IAC 0 0.746 0.597 1356.146 1973.193 6CAM-T 0 0.725 0.560 1475.313 1978.237 5.33CAM-TU 0 0.721 0.557 1297.372 1809.044 4GRAPH-FP 4.73 0.689 0.479 1382.039 2053.326 3.33GRAPH-SC 4.73 0.671 0.466 1349.047 1899.585 2TITCH-RS+ 0 0.669 0.459 1278.008 1814.933 1.33TITCH-AS+ 0 0.660 0.452 1321.204 1817.303 4.67TITCH-RS 0 0.655 0.432 1255.278 1866.935 4.67TITCH-AS 0 0.645 0.422 1229.417 1766.350 4.67CAM-BU 10.14 0.630 0.420 1251.316 1877.948 4NIL 20.27 0.625 0.477 1482.665 1960.314 5.33CAM-B 8.11 0.620 0.403 1309.070 1952.394 5.33IS-FBS 100 0.368 0.182 1461.453 2181.883 7.33Table 2: Results for systems and evaluation measures (in order of Dice).trainable includes type incremental human preferencesno yes no yes no yes no yesDice 0.561 0.700 0.627 0.676 0.625 0.677 0.656 0.677MASI 0.370 0.511 0.464 0.477 0.432 0.488 0.468 0.484RT 1376.13 1371.41 1534.45 1313.83 1507.52 1323.63 1387.49 1343.02IT 1993.13 1911.55 2076.08 1881.39 2080.93 1879.63 1932.87 1934.19ER 5.2% 4.2% 6.3% 3.8% 4.7% 4.4% 4.5% 4.5%Table 3: Means on intrinsic and extrinsic measures, by system type.Dice MASI IT RTTrainable 171.67?
145.63?
.002 4.36Incremental 47.90?
45.61?
7.43 1.64Includes type 18.42?
29.38?
6.89 11.27Human pref.
3.27 5.12 2.11 .83Incr.
?
Train.
40.82?
30.99?
.001 7.08Table 4: Multivariate tests examining impact of systemtype on evaluation measures.
Cells indicate F?valueswith 4 numerator and 2201 error degrees of freedom.
?
:p ?
.05 after Bonferroni correction.cholinguistic results (Pechmann, 1989; Arts, 2004).As a further test of these differences, a multivari-ate ANOVA was conducted, comparing scores on theintrinsic and extrinsic measures, as a function of thepresence or absence of the 4 algorithmic propertiesbeing considered here.
The results are shown inTable 4 which displays figures for all main effectsand for the one interaction that was significant (in-crementality ?
trainability, bottom row).
Since thisis a multiple-test post-hoc analysis on unequal-sizedsamples, all significance values are based on a Bon-ferroni correction5 The table does not include ER, as5The Bonferroni correction adjusts for the increased like-no main effect of SYSTEM was found on this mea-sure.The results show that trainability, followed by in-crementality and inclusion of TYPE had the strongestimpact on system quality, as far as this is measuredby Dice and MASI.The incrementality ?
trainability interaction ismostly due to the huge effect that trainability hason how non-incremental systems perform.
In thecase of incremental systems, trainability gives rise tomarginally better performance: the mean Dice scorefor non-trainable incremental systems was .625, ascompared to .696 for trainable incremental systems.Similar patterns are observable with MASI (.433vs .439).
However, the most significant featureof the interaction is the large benefit that trainingon data confers on non-incremental systems.
Un-trained non-incremental systems obtained a meanDice score of .368 (MASI = .182), while the meanDice score on trained non-incremental systems was.710, with a mean MASI score of .515.
Another,lihood of finding results statistically significant when multipletests are applied by lowering the alpha value (the significancethreshold) by dividing it by the number of tests involved.55Intrinsic measures Extrinsic measuresDice MASI ER RT IT-.904??
-.789??
.505 .183 .560?Table 5: Correlations between Minimality and other eval-uation measures.
Legend: ??
: p ?
.01, ?
: p ?
.05more tentative, conclusion that could be made bylooking at these figures is that trainable systems per-form better if they are non-incremental; however,the difference (Dice of .710 vs .696) seems rathermarginal and would require further testing.
Whatis also striking in this table is the absence of anysignificant impact of these properties on the task-performance measures of reading and identificationtime, despite initial impressions based on means (Ta-ble 3).In summary, at least one of the principles that hasbeen widely adopted in ASGRE ?
incrementality ?seems to be validated by the evaluation results forthese new algorithms.
However, results show thatimprovement on an automatically computed human-likeness metric such as MASI or Dice does not neces-sarily imply an improvement on a task-performancemeasure.
This is a theme to which we return in Sec-tion 4.3.4.2 The role of minimalityApart from incrementality, the other dominant prin-ciple that has emerged from nearly three decades ofASGRE research is brevity.
While psycholinguisticresearch has shown (Pechmann, 1989; Belke andMeyer, 2002; Arts, 2004) that the strict interpre-tation of the Gricean Quantity Maxim, adopted forexample by Dale (1989), is not observed by speak-ers, brevity has remained a central concern in recentapproaches (Gardent, 2002).
Only one algorithmadopted a full-brevity approach in the ASGRE Chal-lenge; however, several algorithms emphasised dis-criminatory power, and some of these, as a compar-ison between Tables 1 and 2 shows, tended to havehigher Minimality scores overall.Since Minimality was part of the overall designof the evaluation, it is possible to see the extentto which it covaries significantly with other perfor-mance measures.
The relevant correlations are dis-played in Table 5.
There are highly significant neg-ative correlations between Minimality and the twointrinsic humanlikeness measures (Dice and MASI).The significant positive correlation with IT impliesthat participants in our evaluation experiment tendedto take longer to identify referents in the case of sys-tems which produce more minimal descriptions.The negative correlation with the humanlikenessmeasures corroborates previous findings that peo-ple do not observe a strict interpretation of theGricean Quantity Maxim (Pechmann, 1989; Belkeand Meyer, 2002; Arts, 2004).
On the other hand,the direction of the correlation with IT is somewhatmore surprising.
One possible explanation is thatminimal descriptions often do not include a TYPEattribute, since this seldom has any discriminatorypower in the TUNA Corpus domains.The role of Minimality remains a topic of somedebate in the psycholinguistic literature.
A recentstudy (Engelhardt et al, 2006) showed that identifi-cation of objects in a visual domain could be delayedif a description of the target referent was overspeci-fied, suggesting a beneficial effect from Minimality.However, Engelhardt et al also found, in a separatestudy, that overspecified descriptions tended to berated as no worse than brief, or minimal ones.
In-deed, one of the points that emerges from this studyis that intrinsic and extrinsic assessments can yieldcontrasting results.
It is to this issue that we nowturn.4.3 Intrinsic vs. extrinsic methodsSince humanlikeness (intrinsic) and task perfor-mance (extrinsic) are different perspectives on theoutputs of an ASGRE algorithm, it is worth ask-ing to what extent they agree.
This is an impor-tant question for comparative HLT evaluation moregenerally, which has become dominated by corpus-based humanlikeness metrics.
To obtain an indi-cation of agreement, we computed correlations be-tween the two humanlikeness measures and the task-performance measures; these are displayed in Table6.The two time measures (RT and IT) are stronglycorrelated, implying that when subjects took longerto read descriptions, they also took longer to iden-tify a referent.
This may seem surprising in viewof the positive correlation between Minimality andIT (shorter descriptions imply longer IT) which wasreported in Section 4.2, where Minimality was also56RT IT ER Dice MASIRT 1 .8** 0.46 0.12 .23IT .8** 1 .59* -0.28 -.17ER 0.46 .59* 1 -0.39 -.29Dice 0.12 -0.28 -0.39 1 .97**MASI 0.23 -0.17 -0.29 .97** 1Table 6: Pairwise correlations between humanlikenessand task-performance measures (?
: p ?
.05; ??
: p ?
.01)shown not to correlate with RT (shorter descriptionsdo not imply longer or shorter RT).
What this resultindicates is that rather than the number of attributesin a description, as measured by Minimality, it is thecontent that influences identification latency.
Part ofthe effect may be due to the lack of TYPE in minimaldescriptions noted earlier.Table 6 also shows that error rate is significantlycorrelated with IT (but not with RT), i.e.
where sub-jects took longer to identify referents, the identifi-cation was more likely to be wrong.
This points toa common underlying cause for slower reading andidentification, possibly arising from the use of at-tributes (such as SIZE) which impose a greater cog-nitive load on the reader.The very strong correlation (0.97) between Diceand MASI is to be expected, given the similarity inthe way they are defined.Another unambiguous result emerges: none of thesimilarity-based metrics covary significantly withany of the task-performance measures.
An extendedanalysis involving a larger range of intrinsic met-rics confirmed this lack of significant covariationfor string-based similarity metrics as well as set-similarity metrics across two task-performance ex-periments (Belz and Gatt, 2008).
This indicates thatat least for some areas of HLT, task-performanceevaluation is vital: without the external reality checkprovided by extrinsic evaluations, intrinsic evalua-tions may end up being too self-contained and dis-connected from notions of usefulness to provide ameaningful assessment of systems?
quality.5 ConclusionComparative evaluation can be of great benefit, es-pecially in an area as mature and diverse as GRE.A shared-task evaluation like the ASGRE Challengecan help identify the strengths and weaknesses of al-ternative approaches and techniques, as measured bydifferent evaluation criteria.
Perhaps even more im-portantly, such a challenge helps evaluate the evalu-ation methods themselves and reveals which (com-binations of) evaluation methods are appropriate fora given evaluation purpose.As far as the first issue is concerned, this paperhas shown that trainability, incrementality and notaiming for minimality are the algorithmic proper-ties most helpful in achieving high humanlikenessscores.
This result is in line with psycholinguisticresearch on attribute selection in reference by hu-mans.
Less clear-cut is the relationship betweenthese properties and task-performance measures thatassess how efficiently a referent can be identifiedbased on a description.The second part of the analysis presented hereshowed that intrinsic and extrinsic perspectives onthe quality of system outputs quality can yield un-correlated sets of results, making it difficult to pre-dict quality as measured by one based on qualityas measured by the other.
Furthermore, while in-tuitively it might be expected that higher human-likeness entailed better task performance, our resultssuggest that this is not necessarily the case.Our main conclusions for ASGRE evaluation arethat (a) while humanlikeness evaluation may providea measure of one aspect of systems but we need to becautious in relying on humanlikeness as a criterionof overall quality (standard in evaluation of MT andsummarisation); and (b) that we must not leave theNLG tradition of task-performance evaluations be-hind as we move towards more comparative formsof evaluation.AcknowledgementsWe gratefully acknowledge the contribution made tothe evaluations by the faculty and staff at BrightonUniversity who participated in the identification ex-periments.
The biggest contribution was, of course,made by the participants in the ASGRE Challengewho created the systems involved in the evalua-tions: Bernd Bohnet, Ann Copestake, Pablo Gerva?s,Raquel Herva?s, John Kelleher, Emiel Krahmer,Takahiro Kurosawa, Advaith Siddharthan, PhilippSpanger, Takenobu Tokunaga, Mariet Theune, Pas-cal Touset and Jette Viethen.57ReferencesD.
Appelt and A. Kronfeld.
1987.
A computa-tional model of referring.
In Proc.
10th InternationalJoint Conference on Artificial Intelligence (IJCAI-87),pages 640?647.D.
Appelt.
1985.
Planning English referring expressions.Artificial Intelligence, 26(1):1?33.A.
Arts.
2004.
Overspecification in Instructive Texts.Ph.D.
thesis, University of Tilburg.E.
Belke and A. Meyer.
2002.
Tracking the time courseof multidimensional stimulus discrimination: Analy-sis of viewing patterns and processing times duringsame-different decisions.
European Journal of Cog-nitive Psychology, 14(2):237?266.A.
Belz and A. Gatt.
2007.
The Attribute Selection forGRE Challenge: Overview and evaluation results.
InProc.
2nd UCNLG Workshop: Language Generationand Machine Translation (UCNLG+MT), pages 75?83.A.
Belz and A. Gatt.
2008.
Intrinsic vs. extrinsic evalu-ation measures for referring expression generation.
InProc.
46th Annual Meeting of the Association for Com-putational Linguistics (ACL-08).
To appear.R.
Dale and N. Haddock.
1991.
Generating refer-ring expressions containing relations.
In Proc.
5thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL-91)., pages161?166.R.
Dale and E. Reiter.
1995.
Computational interpreta-tion of the Gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 19(8):233?263.R.
Dale and M. White, editors.
2007.
Shared tasks andcomparative evaluation in Natural Language Genera-tion: Workshop Report.Robert Dale.
1989.
Cooking up referring expressions.In Proc.
27th Annual Meeting of the Association forComputational Linguistics (ACL-89)., pages 68?75.P.
E. Engelhardt, K.G.D Bailey, and F. Ferreira.
2006.Do speakers and listeners observe the Gricean Maximof Quantity?
Journal of Memory and Language,54:554?573.K.
I. Forster and J. C. Forster.
2003.
DMDX: A win-dows display program with millisecond accuracy.
Be-havior Research Methods, Instruments, & Computers,35(1):116?124.C.
Gardent.
2002.
Generating minimal definite descrip-tions.
In Proc.
40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-02)., pages96?103.A.
Gatt and K. van Deemter.
2007.
Incremental genera-tion of plural descriptions: Similarity and partitioning.In Proc.
2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP/CONLL-07), pages 102?111.A.
Gatt, I. van der Sluis, and K. van Deemter.
2007.Evaluating algorithms for the generation of referringexpressions using a balanced corpus.
In Proc.
11thEuropean Workshop on Natural Language Generation(ENLG-07)., pages 49?56.H.
P. Grice.
1975.
Logic and conversation.
In P. Coleand J. L. Morgan, editors, Syntax and Semantics:Speech Acts., volume III.
Academic Press.S.
Gupta and A. J. Stent.
2005.
Automatic evalua-tion of referring expression generation using corpora.In Proc.
1st Workshop on Using Corpora in NLG(UCNLG-05)., pages 1?6.H.
Horacek.
2004.
On referring to sets of objects natu-rally.
In Proc.
3rd International Conference on Natu-ral Language Generation (INLG-04)., pages 70?79.A.
Karasimos and A. Isard.
2004.
Multilingual evalua-tion of a natural language generation system.
In Proc.4th International Conference on Language Resourcesand Evaluation (LREC-04).J.
D. Kelleher and G-J Kruijff.
2006.
Incremental gen-eration of spatial referring expressions in situated di-alog.
In Proc.
joint 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(ACL/COLING-06)., pages 1041?1048.R.
Passonneau.
2006.
Measuring agreement on set-valued items (MASI) for semantic and pragmatic anno-tation.
In Proc.
5th International Conference on Lan-guage Resources and Evaluation (LREC-06)., pages831?836.Thomas Pechmann.
1989.
Incremental speech pro-duction and referential overspecification.
Linguistics,27:89?110.E.
Reiter, R. Robertson, and L. Osman.
2003.
Lessonsfrom a failure: Generating tailored smoking cessationletters.
Artificial Intelligence, 144:41?58.E.
Reiter.
1990.
The computational complexity of avoid-ing conversational implicatures.
In Proc.
28th AnnualMeeting of the Association for Computational Linguis-tics (ACL-90)., pages 97?104.K.
van Deemter.
2002.
Generating referring expres-sions: Boolean extensions of the incremental algo-rithm.
Computational Linguistics, 28(1):37?52.K.
van Deemter.
2006.
Generating referring expressionsthat involve gradable properties.
Computational Lin-guistics, 32(2):195?222.I.
van der Sluis, A. Gatt, and K. van Deemter.
2007.Evaluating algorithms for the generation of referringexpressions: Going beyond toy domains.
In Proc.
In-ternational Conference on Recent Advances in NaturalLanguage Processing (RANLP-07).J.
Viethen and R. Dale.
2006.
Algorithms for generat-ing referring expressions: Do they do what people do?In Proc.
4th International Conference on Natural Lan-guage Generation (INLG-06)., pages 63?72.58
