Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsReview of Hypothesis Alignment Algorithms for MT System Combinationvia Confusion Network DecodingAntti-Veikko I.
Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing ZhangfaApple Inc., Cupertino, CA 95014arosti@apple.combMicrosoft Research, Redmond, WA 98052xiaohe@microsoft.comcJohns Hopkins University, Baltimore, MD 21218{damianos,yuan.cao,jrsmith}@jhu.edudSAIC, Monheimsallee 22, D-52062 Aachen, Germanygregor.leusch@saic.comeRWTH Aachen University, D-52056 Aachen, Germany{freitag,ney}@cs.rwth-aachen.defRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138{smatsouk,bzhang}@bbn.comAbstractConfusion network decoding has proven tobe one of the most successful approachesto machine translation system combination.The hypothesis alignment algorithm is a cru-cial part of building the confusion networksand many alternatives have been proposed inthe literature.
This paper describes a sys-tematic comparison of five well known hy-pothesis alignment algorithms for MT sys-tem combination via confusion network de-coding.
Controlled experiments using identi-cal pre-processing, decoding, and weight tun-ing methods on standard system combina-tion evaluation sets are presented.
Transla-tion quality is assessed using case insensitiveBLEU scores and bootstrapping is used to es-tablish statistical significance of the score dif-ferences.
All aligners yield significant BLEUscore gains over the best individual system in-cluded in the combination.
Incremental indi-rect hidden Markov model and a novel incre-mental inversion transduction grammar withflexible matching consistently yield the besttranslation quality, though keeping all thingsequal, the differences between aligners are rel-atively small.
?The work reported in this paper was carried out while theauthors were at Raytheon BBN Technologies and?RWTH Aachen University.1 IntroductionCurrent machine translation (MT) systems are basedon different paradigms, such as rule-based, phrase-based, hierarchical, and syntax-based.
Due to thecomplexity of the problem, systems make variousassumptions at different levels of processing andmodeling.
Many of these assumptions may besuboptimal and complementary.
The complemen-tary information in the outputs from multiple MTsystems may be exploited by system combination.Availability of multiple system outputs within theDARPA GALE program as well as NIST Open MTand Workshop on Statistical Machine Translationevaluations has led to extensive research in combin-ing the strengths of diverse MT systems, resulting insignificant gains in translation quality.System combination methods proposed in the lit-erature can be roughly divided into three categories:(i) hypothesis selection (Rosti et al, 2007b; Hilde-brand and Vogel, 2008), (ii) re-decoding (Frederkingand Nirenburg, 1994; Jayaraman and Lavie, 2005;Rosti et al, 2007b; He and Toutanova, 2009; De-vlin et al, 2011), and (iii) confusion network de-coding.
Confusion network decoding has proven tobe the most popular as it does not require deep N -best lists1 and operates on the surface strings.
It has1N -best lists of around N = 10 have been used in confu-sion network decoding yielding small gains over using 1-best191also been shown to be very successful in combiningspeech recognition outputs (Fiscus, 1997; Mangu etal., 2000).
The first application of confusion net-work decoding in MT system combination appearedin (Bangalore et al, 2001) where a multiple stringalignment (MSA), made popular in biological se-quence analysis, was applied to the MT system out-puts.
Matusov et al (2006) proposed an alignmentbased on GIZA++ Toolkit which introduced wordreordering not present in MSA, and Sim et al (2007)used the alignments produced by the translation editrate (TER) (Snover et al, 2006) scoring.
Extensionsof the last two are included in this study togetherwith alignments based on hidden Markov model(HMM) (Vogel et al, 1996) and inversion transduc-tion grammars (ITG) (Wu, 1997).System combinations produced via confusion net-work decoding using different hypothesis alignmentalgorithms have been entered into open evalua-tions, most recently in 2011 Workshop on StatisticalMachine Translation (Callison-Burch et al, 2011).However, there has not been a comparison of themost popular hypothesis alignment algorithms us-ing the same sets of MT system outputs and other-wise identical combination pipelines.
This paper at-tempts to systematically compare the quality of fivehypothesis alignment algorithms.
Alignments wereproduced for the same system outputs from threecommon test sets used in the 2009 NIST Open MTEvaluation and the 2011 Workshop on StatisticalMachine Translation.
Identical pre-processing, de-coding, and weight tuning algorithms were used toquantitatively evaluate the alignment quality.
Caseinsensitive BLEU score (Papineni et al, 2002) wasused as the translation quality metric.2 Confusion Network DecodingA confusion network is a linear graph where allpaths visit all nodes.
Two consecutive nodes may beconnected by one or more arcs.
Given the arcs repre-sent words in hypotheses, multiple arcs connectingtwo consecutive nodes can be viewed as alternativewords in that position of a set of hypotheses encodedby the network.
A special NULL token representsa skipped word and will not appear in the systemcombination output.
For example, three hypothesesoutputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?may be aligned as follows:twelve big blue carstwelve NULL NULL carsdozen NULL blue carsThis alignment may be represented compactly as theconfusion network in Figure 1 which encodes a totalof eight unique hypotheses.40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)Figure 1: Confusion network from three strings ?twelvebig blue cars?, ?twelve cars?, and ?dozen blue cars?
us-ing the first as the skeleton.
The numbers in parenthesesrepresent counts of words aligned to the correspondingarc.Building confusion networks from multiple ma-chine translation system outputs has two main prob-lems.
First, one output has to be chosen as the skele-ton hypothesis which defines the final word order ofthe system combination output.
Second, MT systemoutputs may have very different word orders whichcomplicates the alignment process.
For skeleton se-lection, Sim et al (2007) proposed choosing the out-put closest to all other hypotheses when using eachas the reference string in TER.
Alternatively, Ma-tusov et al (2006) proposed leaving the decision todecoding time by connecting networks built usingeach output as a skeleton into a large lattice.
Thesubnetworks in the latter approach may be weightedby prior probabilities estimated from the alignmentstatistics (Rosti et al, 2007a).
Since different align-ment algorithm produce different statistics and thegain from the weights is relatively small (Rosti et al,2011), weights for the subnetworks were not usedin this work.
The hypothesis alignment algorithmsused in this work are briefly described in the follow-ing section.The confusion networks in this work were repre-sented in a text lattice format shown in Figure 2.Each line corresponds to an arc, where J is the arcindex, S is the start node index, E is the end node in-dex, SC is the score vector, and W is the word label.The score vector has as many elements as there areinput systems.
The elements correspond to each sys-tem and indicate whether a word from a particular192J=0 S=0 E=1 SC=(1,1,0) W=twelveJ=1 S=0 E=1 SC=(0,0,1) W=dozenJ=2 S=1 E=2 SC=(1,0,0) W=bigJ=3 S=1 E=2 SC=(0,1,1) W=NULLJ=4 S=2 E=3 SC=(1,0,1) W=blueJ=5 S=2 E=3 SC=(0,1,0) W=NULLJ=6 S=3 E=4 SC=(1,1,1) W=carsFigure 2: A lattice in text format representing the con-fusion network in Figure 1.
J is the arc index, S and Eare the start and end node indexes, SC is a vector of arcscores, and W is the word label.system was aligned to a given link2.
These may beviewed as system specific word confidences, whichare binary when aligning 1-best system outputs.
Ifno word from a hypothesis is aligned to a given link,a NULL word token is generated provided one doesnot already exist, and the corresponding element inthe NULL word token is set to one.
The systemspecific word scores are kept separate in order toexploit system weights in decoding.
Given systemweights wn, which sum to one, and system specificword scores snj for each arc j (the SC elements), theweighted word scores are defined as:sj =Ns?n=1wnsnj (1)where Ns is the number of input systems.
The hy-pothesis score is defined as the sum of the log-word-scores along the path, which is linearly interpolatedwith a logarithm of the language model (LM) scoreand a non-NULL word count:S(E|F ) =?j?J (E)log sj + ?SLM (E) + ?Nw(E)(2)where J (E) is the sequence of arcs generating thehypothesis E for the source sentence F , SLM (E)is the LM score, and Nw(E) is the number ofnon-NULL words.
The set of weights ?
={w1, .
.
.
, wNs , ?, ?}
can be tuned so as to optimizean evaluation metric on a development set.Decoding with an n-gram language model re-quires expanding the lattice to distinguish paths with2A link is used as a synonym to the set of arcs between twoconsecutive nodes.
The name refers to the confusion networkstructure?s resemblance to a sausage.unique n-gram contexts before LM scores can be as-signed the arcs.
Using long n-gram context may re-quire pruning to reduce memory usage.
Given uni-form initial system weights, pruning may removedesirable paths.
In this work, the lattices were ex-panded to bi-gram context and no pruning was per-formed.
A set of bi-gram decoding weights weretuned directly on the expanded lattices using a dis-tributed optimizer (Rosti et al, 2010).
Since thescore in Equation 2 is not a simple log-linear inter-polation, the standard minimum error rate training(Och, 2003) with exact line search cannot be used.Instead, downhill simplex (Press et al, 2007) wasused in the optimizer client.
After bi-gram decod-ing weight optimization, another set of 5-gram re-scoring weights were tuned on 300-best lists gener-ated from the bi-gram expanded lattices.3 Hypothesis Alignment AlgorithmsTwo different methods have been proposed forbuilding confusion networks: pairwise and incre-mental alignment.
In pairwise alignment, eachhypothesis corresponding to a source sentence isaligned independently with the skeleton hypothe-sis.
This set of alignments is consolidated using theskeleton words as anchors to form the confusion net-work (Matusov et al, 2006; Sim et al, 2007).
Thesame word in two hypotheses may be aligned with adifferent word in the skeleton resulting in repetitionin the network.
A two-pass alignment algorithm toimprove pairwise TER alignments was introduced in(Ayan et al, 2008).
In incremental alignment (Rostiet al, 2008), the confusion network is initialized byforming a simple graph with one word per link fromthe skeleton hypothesis.
Each remaining hypothesisis aligned with the partial confusion network, whichallows words from all previous hypotheses be con-sidered as matches.
The order in which the hypothe-ses are aligned may influence the alignment qual-ity.
Rosti et al (2009) proposed a sentence specificalignment order by choosing the unaligned hypoth-esis closest to the partial confusion network accord-ing to TER.
The following five alignment algorithmswere used in this study.1933.1 Pairwise GIZA++ Enhanced HypothesisAlignmentMatusov et al (2006) proposed using the GIZA++Toolkit (Och and Ney, 2003) to align a set of tar-get language translations.
A parallel corpus whereeach system output acting as a skeleton appears asa translation of all system outputs corresponding tothe same source sentence.
The IBM Model 1 (Brownet al, 1993) and hidden Markov model (HMM) (Vo-gel et al, 1996) are used to estimate the alignment.Alignments from both ?translation?
directions areused to obtain symmetrized alignments by interpo-lating the HMM occupation statistics (Matusov etal., 2004).
The algorithm may benefit from the factthat it considers the entire test set when estimatingthe alignment model parameters; i.e., word align-ment links from all output sentences influence theestimation, whereas other alignment algorithms onlyconsider words within a pair of sentences (pairwisealignment) or all outputs corresponding to a singlesource sentence (incremental alignment).
However,it does not naturally extend to incremental align-ment.
The monotone one-to-one alignments are thentransformed into a confusion network.
This aligneris referred to as GIZA later in this paper.3.2 Incremental Indirect Hidden MarkovModel AlignmentHe et al (2008) proposed using an indirect hiddenMarkov model (IHMM) for pairwise alignment ofsystem outputs.
The parameters of the IHMM areestimated indirectly from a variety of sources in-cluding semantic word similarity, surface word sim-ilarity, and a distance-based distortion penalty.
Thealignment between two target language outputs aretreated as the hidden states.
A standard Viterbi al-gorithm is used to infer the alignment.
The pair-wise IHMM was extended to operate incrementallyin (Li et al, 2009).
Sentence specific alignment or-der is not used by this aligner, which is referred toas iIHMM later in this paper.3.3 Incremental Inversion TransductionGrammar Alignment with FlexibleMatchingKarakos et al (2008) proposed using inversion trans-duction grammars (ITG) (Wu, 1997) for pairwisealignment of system outputs.
ITGs form an editdistance, invWER (Leusch et al, 2003), that per-mits properly nested block movements of substrings.For well-formed sentences, this may be more nat-ural than allowing arbitrary shifts.
The ITG algo-rithm is very expensive due to its O(n6) complexity.The search algorithm for the best ITG alignment, abest-first chart parsing (Charniak et al, 1998), wasaugmented with an A?
search heuristic of quadraticcomplexity (Klein and Manning, 2003), resulting insignificant reduction in computational complexity.The finite state-machine heuristic computes a lowerbound to the alignment cost of two strings by allow-ing arbitrary word re-orderings.
The ITG hypothesisalignment algorithm was extended to operate incre-mentally in (Karakos et al, 2010) and a novel ver-sion where the cost function is computed based onthe stem/synonym similarity of (Snover et al, 2009)was used in this work.
Also, a sentence specificalignment order was used.
This aligner is referredto as iITGp later in this paper.3.4 Incremental Translation Edit RateAlignment with Flexible MatchingSim et al (2007) proposed using translation edit ratescorer3 to obtain pairwise alignment of system out-puts.
The TER scorer tries to find shifts of blocksof words that minimize the edit distance betweenthe shifted reference and a hypothesis.
Due to thecomputational complexity, a set of heuristics is usedto reduce the run time (Snover et al, 2006).
Thepairwise TER hypothesis alignment algorithm wasextended to operate incrementally in (Rosti et al,2008) and also extended to consider synonym andstem matches in (Rosti et al, 2009).
The shiftheuristics were relaxed for flexible matching to al-low shifts of blocks of words as long as the edit dis-tance is decreased even if there is no exact match inthe new position.
A sentence specific alignment or-der was used by this aligner, which is referred to asiTER later in this paper.3.5 Incremental Translation Edit Rate PlusAlignmentSnover et al (2009) extended TER scoring to con-sider synonyms and paraphrase matches, called3http://www.cs.umd.edu/?snover/tercom/194TER-plus (TERp).
The shift heuristics in TERpwere also relaxed relative to TER.
Shifts are allowedif the words being shifted are: (i) exactly the same,(ii) synonyms, stems or paraphrases of the corre-sponding reference words, or (iii) any such combina-tion.
Xu et al (2011) proposed using an incrementalversion of TERp for building consensus networks.
Asentence specific alignment order was used by thisaligner, which is referred to as iTERp later in thispaper.4 Experimental EvaluationCombination experiments were performed on (i)Arabic-English, from the informal system combi-nation track of the 2009 NIST Open MT Evalua-tion4; (ii) German-English from the system com-bination evaluation of the 2011 Workshop on Sta-tistical Machine Translation (Callison-Burch et al,2011) (WMT11) and (iii) Spanish-English, againfrom WMT11.
Eight top-performing systems (asevaluated using case-insensitive BLEU) were usedin each language pair.
Case insensitive BLEU scoresfor the individual system outputs on the tuning andtest sets are shown in Table 1.
About 300 and800 sentences with four reference translations wereavailable for Arabic-English tune and test sets, re-spectively, and about 500 and 2500 sentences with asingle reference translation were available for bothGerman-English and Spanish-English tune and testsets.
The system outputs were lower-cased and to-kenized before building confusion networks usingthe five hypothesis alignment algorithms describedabove.
Unpruned English bi-gram and 5-gram lan-guage models were trained with about 6 billionwords available for these evaluations.
Multiple com-ponent language models were trained after dividingthe monolingual corpora by source.
Separate setsof interpolation weights were tuned for the NISTand WMT experiments to minimize perplexity onthe English reference translations of the previousevaluations, NIST MT08 and WMT10.
The sys-tem combination weights, both bi-gram lattice de-coding and 5-gram 300-best list re-scoring weights,were tuned separately for lattices build with each hy-pothesis alignment algorithm.
The final re-scoring4http://www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease/indexISC.htmloutputs were detokenized before computing case in-sensitive BLEU scores.
Statistical significance wascomputed for each pairwise comparison using boot-strapping (Koehn, 2004).Decode OracleAligner tune test tune testGIZA 60.06 57.95 75.06 74.47iTER 59.74 58.63?
73.84 73.20iTERp 60.18 59.05?
76.43 75.58iIHMM 60.51 59.27??
76.50 76.17iITGp 60.65 59.37??
76.53 76.05Table 2: Case insensitive BLEU scores for NIST MT09Arabic-English system combination outputs.
Note, fourreference translations were available.
Decode corre-sponds to results after weight tuning and Oracle corre-sponds to graph TER oracle.
Dagger (?)
denotes statisti-cally significant difference compared to GIZA and doubledagger (?)
compared to iTERp and the aligners above it.The BLEU scores for Arabic-English systemcombination outputs are shown in Table 2.
The firstcolumn (Decode) shows the scores on tune and testsets for the decoding outputs.
The second column(Oracle) shows the scores for oracle hypotheses ob-tained by aligning the reference translations with theconfusion networks and choosing the path with low-est graph TER (Rosti et al, 2008).
The rows rep-resenting different aligners are sorted according tothe test set decoding scores.
The order of the BLEUscores for the oracle translations do not always fol-low the order for the decoding outputs.
This may bedue to differences in the compactness of the confu-sion networks.
A more compact network has fewerpaths and is therefore less likely to contain signif-icant parts of the reference translation, whereas areference translation may be generated from a lesscompact network.
On Arabic-English, all incremen-tal alignment algorithms are significantly better thanthe pairwise GIZA, incremental IHMM and ITGwith flexible matching are significantly better thanall other algorithms, but not significantly differentfrom each other.
The incremental TER and TERpwere statistically indistinguishable.
Without flexi-ble matching, iITG yields a BLEU score of 58.85 ontest.
The absolute BLEU gain over the best individ-ual system was between 6.2 and 7.6 points on thetest set.195Arabic German SpanishSystem tune test tune test tune testA 48.84 48.54 21.96 21.41 27.71 27.13B 49.15 48.97 22.61 21.80 28.42 27.90C 49.30 49.50 22.77 21.99 28.57 28.23D 49.38 49.59 22.90 22.41 29.00 28.41E 49.42 49.75 22.90 22.65 29.15 28.50F 50.28 50.69 22.98 22.65 29.53 28.61G 51.49 50.81 23.41 23.06 29.89 29.82H 51.72 51.74 24.28 24.16 30.55 30.14Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three sourcelanguages.Decode OracleAligner tune test tune testGIZA 25.93 26.02 37.32 38.22iTERp 26.46 26.10 38.16 38.76iTER 26.27 26.39?
37.00 37.66iIHMM 26.34 26.40?
37.87 38.48iITGp 26.47 26.50?
37.99 38.60Table 3: Case insensitive BLEU scores for WMT11German-English system combination outputs.
Note, onlya single reference translation per segment was available.Decode corresponds to results after weight tuning andOracle corresponds to graph TER oracle.
Dagger (?
)denotes statistically significant difference compared toiTERp and GIZA.The BLEU scores for German-English systemcombination outputs are shown in Table 3.
Again,the graph TER oracle scores do not follow the sameorder as the decoding scores.
The scores for GIZAand iTERp are statistically indistinguishable, andiTER, iIHMM, and iITGp are significantly betterthan the first two.
However, they are not statisticallydifferent from each other.
Without flexible match-ing, iITG yields a BLEU score of 26.47 on test.
Theabsolute BLEU gain over the best individual systemwas between 1.9 and 2.3 points on the test set.The BLEU scores for Spanish-English systemcombination outputs are shown in Table 4.
All align-ers but iIHMM are statistically indistinguishable andiIHMM is significantly better than all other align-ers.
Without flexible matching, iITG yields a BLEUscore of 33.62 on test.
The absolute BLEU gain overthe best individual system was between 3.5 and 3.9Decode OracleAligner tune test tune testiTERp 34.20 33.61 50.45 51.28GIZA 34.02 33.62 50.23 51.20iTER 34.44 33.79 50.39 50.39iITGp 34.41 33.85 50.55 51.33iIHMM 34.61 34.05?
50.48 51.27Table 4: Case insensitive BLEU scores for WMT11Spanish-English system combination outputs.
Note, onlya single reference translation per segment was available.Decode corresponds to results after weight tuning andOracle corresponds to graph TER oracle.
Dagger (?
)denotes statistically significant difference compared toaligners above iIHMM.points on the test set.5 Error AnalysisError analysis was performed to better understandthe gains from system combination.
Specifically, (i)how the different types of translation errors are af-fected by system combination was investigated; and(ii) an attempt to quantify the correlation betweenthe word agreement that results from the differentaligners and the translation error, as measured byTER (Snover et al, 2006), was made.5.1 Influence on Error TypesFor each one of the individual systems, and for eachone of the three language pairs, the per-sentence er-rors that resulted from that system, as well as fromeach one of the the different aligners studied in thispaper, were computed.
The errors were broken196down into insertions/deletions/substitutions/shiftsbased on the TER scorer.The error counts at the document level were ag-gregated.
For each document in each collection, thenumber of errors of each type that resulted from eachindividual system as well as each system combina-tion were measured, and their difference was com-puted.
If the differences are mostly positive, thenit can be said (with some confidence) that systemcombination has a significant impact in reducing theerror of that type.
A paired Wilcoxon test was per-formed and the p-value that quantifies the probabil-ity that the measured error reduction was achievedunder the null hypothesis that the system combina-tion performs as well as the best system was com-puted.Table 5 shows all conditions under consideration.All cases where the p-value is below 10?2 are con-sidered statistically significant.
Two observationsare in order: (i) all alignment schemes significantlyreduce the number of substitution/shift errors; (ii)in the case of insertions/deletions, there is no cleartrend; there are cases where the system combinationincreases the number of insertions/deletions, com-pared to the individual systems.5.2 Relationship between Word Agreementand Translation ErrorThis set of experiments aimed to quantify the rela-tionship between the translation error rate and theamount of agreement that resulted from each align-ment scheme.
The amount of system agreement ata level x is measured by the number of cases (con-fusion network arcs) where x system outputs con-tribute the same word in a confusion network bin.For example, the agreement at level 2 is equal to 2in Figure 1 because there are exactly 2 arcs (withwords ?twelve?
and ?blue?)
that resulted from theagreement of 2 systems.
Similarly, the agreement atlevel 3 is 1, because there is only 1 arc (with word?cars?)
that resulted from the agreement of 3 sys-tems.
It is hypothesized that a sufficiently high levelof agreement should be indicative of the correctnessof a word (and thus indicative of lower TER).
Theagreement statistics were grouped into two values:the ?weak?
agreement statistic, where at most halfof the combined systems contribute a word, and the?strong?
agreement statistic, where more than halfnon-NULL words NULL wordsweak strong weak strongArabic 0.087 -0.068 0.192 0.094German 0.117 -0.067 0.206 0.147Spanish 0.085 -0.134 0.323 0.102Table 6: Regression coefficients of the ?strong?
and?weak?
agreement features, as computed with a gener-alized linear model, using TER as the target variable.of the combined systems contribute a word.
To sig-nify the fact that real words and ?NULL?
tokenshave different roles and should be treated separately,two sets of agreement statistics were computed.A regression with a generalized linear model(glm) that computed the coefficients of the agree-ment quantities (as explained above) for each align-ment scheme, using TER as the target variable, wasperformed.
Table 6 shows the regression coeffi-cients; they are all significant at p-value < 0.001.As is clear from this table, the negative coefficient ofthe ?strong?
agreement quantity for the non-NULLwords points to the fact that good aligners tend toresult in reductions in translation error.
Further-more, increasing agreements on NULL tokens doesnot seem to reduce TER.6 ConclusionsThis paper presented a systematic comparison offive different hypothesis alignment algorithms forMT system combination via confusion network de-coding.
Pre-processing, decoding, and weight tun-ing were controlled and only the alignment algo-rithm was varied.
Translation quality was comparedqualitatively using case insensitive BLEU scores.The results showed that confusion network decod-ing yields a significant gain over the best individ-ual system irrespective of the alignment algorithm.Differences between the combination output usingdifferent alignment algorithms were relatively small,but incremental alignment consistently yielded bet-ter translation quality compared to pairwise align-ment based on these experiments and previouslypublished literature.
Incremental IHMM and a novelincremental ITG with flexible matching consistentlyyield highest quality combination outputs.
Further-more, an error analysis shows that most of the per-197Language Aligner ins del sub shftGIZA 2.2e-16 0.9999 2.2e-16 2.2e-16iHMM 2.2e-16 0.433 2.2e-16 2.2e-16Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16iTERp 2.2e-16 1 2.2e-16 2.2e-16GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16German iITGp 0.8551 0.2848 2.2e-16 2.2e-16iTER 0.2491 1.233e-07 2.2e-16 2.2e-16iTERp 0.9997 0.007489 2.2e-16 2.2e-16GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16iHMM 2.2e-16 1 2.2e-16 2.2e-16Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16iTER 2.2e-16 1 2.2e-16 2.2e-16iTERp 3.335e-16 1 2.2e-16 2.2e-16Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.formance gains from system combination can be at-tributed to reductions in substitution errors and wordre-ordering errors.
Finally, better alignments of sys-tem outputs, which tend to cause higher agreementrates on words, correlate with reductions in transla-tion error.ReferencesNecip Fazil Ayan, Jing Zheng, and Wen Wang.
2008.Improving alignments for better confusion networksfor combining machine translation systems.
In Proc.Coling, pages 33?40.Srinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In Proc.
ASRU,pages 351?354.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar F. Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Proc.WMT, pages 22?64.Eugene Charniak, Sharon Goldwater, and Mark Johnson.1998.
Edge-based best-first chart parsing.
In Proc.Sixth Workshop on Very Large Corpora, pages 127?133.
Morgan Kaufmann.Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-ishnan, and Spyros Matsoukas.
2011.
System combi-nation using discriminative cross-adaptation.
In Proc.IJCNLP, pages 667?675.Jonathan G. Fiscus.
1997.
A post-processing system toyield reduced word error rates: Recognizer output vot-ing error reduction (ROVER).
In Proc.
ASRU, pages347?354.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proc.
ANLP, pages 95?100.Xiaodong He and Kristina Toutanova.
2009.
Joint opti-mization for machine translation system combination.In Proc.
EMNLP, pages 1202?1211.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proc.
EMNLP, pages 98?107.Almut S. Hildebrand and Stephan Vogel.
2008.
Combi-nation of machine translation systems via hypothesisselection from combined n-best lists.
In AMTA, pages254?261.Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-engine machine translation guided by explicit wordmatching.
In Proc.
EAMT.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translation sys-tem combination using ITG-based alignments.
InProc.
ACL, pages 81?84.Damianos Karakos, Jason R. Smith, and Sanjeev Khu-danpur.
2010.
Hypothesis ranking and two-pass ap-proaches for machine translation system combination.In Proc.
ICASSP.198Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact Viterbi parse selection.
In Proc.NAACL, pages 40?47.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP,pages 388?395.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003.A novel string-to-string distance measure with appli-cations to machine translation evaluation.
In Proc.
MTSummit 2003, pages 240?247, September.Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.2009.
Incremental hmm alignment for mt system com-bination.
In Proc.
ACL/IJCNLP, pages 949?957.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: Word errorminimization and other applications of confusion net-works.
Computer Speech and Language, 14(4):373?400.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric word alignments for statistical ma-chine translation.
In Proc.
COLING, pages 219?225.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Proc.
EACL, pages 33?40.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
ACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
ACL, pages311?318.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2007.
Numerical recipes:the art of scientific computing.
Cambridge UniversityPress, 3rd edition.Antti-Veikko I. Rosti, Spyros Matsoukas, and RirchardSchwartz.
2007a.
Improved word-level system com-bination for machine translation.
In Proc.
ACL, pages312?319.Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,Richard Schwartz, Necip Fazil Ayan, and Bonnie J.Dorr.
2007b.
Combining outputs from multiplemachine translation systems.
In Proc.
NAACL-HLT,pages 228?235.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothesisalignment for building confusion networks with appli-cation to machine translation system combination.
InProceedings of the Third Workshop on Statistical Ma-chine Translation, pages 183?186.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2009.
Incremental hypothesisalignment with flexible matching for building confu-sion networks: BBN system description for WMT09system combination task.
In Proc.
WMT, pages 61?65.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
BBN system descrip-tion for WMT10 system combination task.
In Proc.WMT, pages 321?326.Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,Necip Fazil Ayan, Jason Eisner, Damianos Karakos,Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-ros Matsoukas, Hermann Ney, Richard Schwartz, BingZhang, and Jing Zheng.
2011.
Confusion network de-coding for MT system combination.
In Joseph Olive,Caitlin Christianson, and John McCary, editors, Hand-book of Natural Language Processing and MachineTranslation: DARPA Global Autonomous LanguageExploitation, pages 333?361.
Springer.Khe Chai Sim, William J. Byrne, Mark J.F.
Gales,Hichem Sahbi, and Phil C. Woodland.
2007.
Con-sensus network decoding for statistical machine trans-lation system combination.
In Proc.
ICASSP.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciula, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
AMTA, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy orHTER?
exploring different human judgments with atunable MT metric.
In Proc.
WMT, pages 259?268.Stephan Vogel, Hermann Ney, and Christoph Tillman.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
ICCL, pages 836?841.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403, Septem-ber.Daguang Xu, Yuan Cao, and Damianos Karakos.
2011.Description of the JHU system combination schemefor WMT 2011.
In Proc.
WMT, pages 171?176.199
