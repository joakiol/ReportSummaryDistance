Feedback Cleaning of Machine Translation RulesUsing Automatic EvaluationKenji Imamura, Eiichiro SumitaATR Spoken Language TranslationResearch LaboratoriesSeika-cho, Soraku-gun, Kyoto, Japan{kenji.imamura,eiichiro.sumita}@atr.co.jpYuji MatsumotoNara Institute ofScience and TechnologyIkoma-shi, Nara, Japanmatsu@is.aist-nara.ac.jpAbstractWhen rules of transfer-based machinetranslation (MT) are automatically ac-quired from bilingual corpora, incor-rect/redundant rules are generated due toacquisition errors or translation variety inthe corpora.
As a new countermeasureto this problem, we propose a feedbackcleaning method using automatic evalua-tion of MT quality, which removes incor-rect/redundant rules as a way to increasethe evaluation score.
BLEU is utilizedfor the automatic evaluation.
The hill-climbing algorithm, which involves fea-tures of this task, is applied to searchingfor the optimal combination of rules.
Ourexperiments show that the MT quality im-proves by 10% in test sentences accordingto a subjective evaluation.
This is consid-erable improvement over previous meth-ods.1 IntroductionAlong with the efforts made in accumulating bilin-gual corpora for many language pairs, quite a fewmachine translation (MT) systems that automati-cally acquire their knowledge from corpora havebeen proposed.
However, knowledge for transfer-based MT acquired from corpora contains many in-correct/redundant rules due to acquisition errors ortranslation variety in the corpora.
Such rules con-flict with other existing rules and cause implausibleMT results or increase ambiguity.
If incorrect rulescould be avoided, MT quality would necessarily im-prove.There are two approaches to overcoming incor-rect/redundant rules:?
Selecting appropriate rules in a disambiguationprocess during the translation (on-line process-ing, (Meyers et al, 2000)).?
Cleaning incorrect/redundant rules afterautomatic acquisition (off-line processing,(Menezes and Richardson, 2001; Imamura,2002)).We employ the second approach in this paper.The cutoff by frequency (Menezes and Richardson,2001) and the hypothesis test (Imamura, 2002) havebeen applied to clean the rules.
The cutoff by fre-quency can slightly improve MT quality, but the im-provement is still insufficient from the viewpoint ofthe large number of redundant rules.
The hypothesistest requires very large corpora in order to obtain asufficient number of rules that are statistically confi-dent.Another current topic of machine translation isautomatic evaluation of MT quality (Papineni et al,2002; Yasuda et al, 2001; Akiba et al, 2001).
Thesemethods aim to replace subjective evaluation in or-der to speed up the development cycle of MT sys-tems.
However, they can be utilized not only as de-velopers?
aids but also for automatic tuning of MTsystems (Su et al, 1992).We propose feedback cleaning that utilizesan automatic evaluation for removing incor-rect/redundant translation rules as a tuning methodTrainingCorpusAutomaticAcquisitionTranslationRulesEvaluationCorpusMTEngineAutomaticEvaluationMT ResultsRuleSelection/DeletionFeedback CleaningFigure 1: Structure of Feedback Cleaning(Figure 1).
Our method evaluates the contributionof each rule to the MT results and removes inap-propriate rules as a way to increase the evaluationscores.
Since the automatic evaluation correlateswith a subjective evaluation, MT quality will im-prove after cleaning.Our method only evaluates MT results and doesnot consider various conditions of the MT engine,such as parameters, interference in dictionaries, dis-ambiguation methods, and so on.
Even if an MTengine avoids incorrect/redundant rules by on-lineprocessing, errors inevitably remain.
Our methodcleans the rules in advance by only focusing on theremaining errors.
Thus, our method complementson-line processing and adapts translation rules to thegiven conditions of the MT engine.2 MT System and Problems of AutomaticAcquisition2.1 MT EngineWe use the Hierarchical Phrase Alignment-basedTranslator (HPAT) (Imamura, 2002) as a transfer-basedMT system.
The most important knowledge inHPAT is transfer rules, which define the correspon-dences between source and target language expres-sions.
An example of English-to-Japanese transferrules is shown in Figure 2.
The transfer rules areregarded as a synchronized context-free grammar.When the system translates an input sentence, thesentence is first parsed by using source patterns ofthe transfer rules.
Next, a tree structure of the tar-get language is generated by mapping the sourcepatterns to the corresponding target patterns.
Whennon-terminal symbols remain in the target tree, tar-get words are inserted by referring to a translationdictionary.Ambiguities, which occur during parsing or map-ping, are resolved by selecting the rules that mini-mize the semantic distance between the input wordsand source examples (real examples in the trainingcorpus) of the transfer rules (Furuse and Iida, 1994).For instance, when the input phrase ?leave at 11a.m.?
is translated into Japanese, Rule 2 in Figure2 is selected because the semantic distance from thesource example (arrive, p.m.) is the shortest to thehead words of the input phrase (leave, a.m.).2.2 Problems of Automatic AcquisitionHPAT automatically acquires its transfer rules fromparallel corpora by using Hierarchical Phrase Align-ment (Imamura, 2001).
However, the rule set con-tains many incorrect/redundant rules.
The reasonsfor this problem are roughly classified as follows.?
Errors in automatic rule acquisition?
Translation variety in corpora?
The acquisition process cannot generalizethe rules because bilingual sentences de-pend on the context or the situation.?
Corpora contain multiple (paraphrasable)translations of the same source expres-sion.In the experiment of Imamura (2002), about92,000 transfer rules were acquired from about120,000 bilingual sentences 1.
Most of these rulesare low-frequency.
They reported that MT qualityslightly improved, even though the low-frequencyrules were removed to a level of about 1/9 the pre-vious number.
However, since some of them, suchas idiomatic rules, are necessary for translation, MTquality cannot be dramatically improved by only re-moving low-frequency rules.3 Automatic Evaluation of MT QualityWe utilize BLEU (Papineni et al, 2002) for the au-tomatic evaluation of MT quality in this paper.BLEU measures the similarity between MT re-sults and translation results made by humans (called1In this paper, the number of rules denotes the number ofunique pairs of source patterns and target patterns.Rule No.
Syn.
Cat.
Source Pattern Target Pattern Source Example1 VP XVPat YNP?
Y?
de X?
((present, conference) ...)2 VP XVPat YNP?
Y?
ni X?
((stay, hotel), (arrive, p.m) ...)3 VP XVPat YNP?
Y?
wo X?
((look, it) ...)4 NP XNPat YNP?
Y?
no X?
((man, front desk) ...)Figure 2: Example of HPAT Transfer Rulesreferences).
This similarity is measured by N-gramprecision scores.
Several kinds of N-grams can beused in BLEU.
We use from 1-gram to 4-gram inthis paper, where a 1-gram precision score indicatesthe adequacy of word translation and longer N-gram(e.g., 4-gram) precision scores indicate fluency ofsentence translation.
The BLEU score is calculatedfrom the product of N-gram precision scores, so thismeasure combines adequacy and fluency.Note that a sizeable set of MT results is necessaryin order to calculate an accurate BLEU score.
Al-though it is possible to calculate the BLEU score of asingle MT result, it contains errors from the subjec-tive evaluation.
BLEU cancels out individual errorsby summing the similarities of MT results.
There-fore, we need all of the MT results from the evalua-tion corpus in order to calculate an accurate BLEUscore.One feature of BLEU is its use of multiple ref-erences for a single source sentence.
However, onereference per sentence is used in this paper becausean already existing bilingual corpus is applied to thecleaning.4 Feedback CleaningIn this section, we introduce the proposed method,called feedback cleaning.
This method is carried outby selecting or removing translation rules to increasethe BLEU score of the evaluation corpus (Figure 1).Thus, this task is regarded as a combinatorial op-timization problem of translation rules.
The hill-climbing algorithm, which involves the features ofthis task, is applied to the optimization.
The fol-lowing sections describe the reasons for using thismethod and its procedure.
The hill-climbing al-gorithm often falls into locally optimal solutions.However, we believe that a locally optimal solutionis more effective in improving MT quality than theprevious methods.4.1 Costs of Combinatorial OptimizationMost combinatorial optimization methods iteratechanges in the combination and the evaluation.
Inthe machine translation task, the evaluation processrequires the longest time.
For example, in order tocalculate the BLEU score of a combination (solu-tion), we have to translate C times, where C denotesthe size of the evaluation corpus.
Furthermore, inorder to find the nearest neighbor solution, we haveto calculate all BLEU scores of the neighborhood.If the number of rules is R and the neighborhoodis regarded as consisting of combinations made bychanging only one rule, we have to translate C ?
Rtimes to find the nearest neighbor solution.
Assumethat C = 10, 000 and R = 100, 000, the numberof sentence translations (sentences to be translated)becomes one billion.
It is infeasible to search forthe optimal solution without reducing the number ofsentence translations.A feature of this task is that removing rules is eas-ier than adding rules.
The rules used for translatinga sentence can be identified during the translation.Conversely, the source sentence set S[r], where arule r is used for the translation, is determined oncethe evaluation corpus is translated.
When r is re-moved, only the MT results of S[r] will change,so we do not need to re-translate other sentences.Assuming that five rules on average are applied totranslate a sentence, the number of sentence trans-lations becomes 5 ?
C + C = 60, 000 for testingall rules.
On the contrary, to add a rule, the entirecorpus must be re-translated because it is unknownwhich MT results will change by adding a rule.4.2 Cleaning ProcedureBased on the above discussion, we utilize the hill-climbing algorithm, in which the initial solutioncontains all rules (called the base rule set) and thesearch for a combination is done by only removingstatic: Ceval, an evaluation corpusRbase, a rule set acquired from the entire training corpus (the base rule set)R, a current rule set, a subset of the base rule setS[r], a source sentence set where the rule r is used for the translationDociter, an MT result set of the evaluation corpus translated with the current rule setprocedure CLEAN-RULESET ()R ?
RbaserepeatRiter?
RRremove?
?scoreiter?
SET-TRANSLATION()for each r in Riterdoif S[r] = ?
thenR ?
Riter?
{r}translate all sentences in S[r], and obtain the MT results T [r]Doc[r] ?
the MT result set that T [r] is replaced from Dociterthe rule contribution contrib[r] ?
scoreiter?
BLEU-SCORE(Doc[r])if contrib[r] < 0 then add r to RremoveendR ?
Riter?Rremoveuntil Rremove= ?function SET-TRANSLATION () returns a BLEU score of the evaluation corpus translated with RDociter?
?for each r in Rbasedo S[r] ?
?
endfor each s in Cevaldotranslate s and obtain the MT result tobtain the rule set R[s] that is used for translating sfor each r in R[s] do add s to S[r] endadd t to Dociterendreturn BLEU-SCORE(Dociter)Figure 3: Feedback Cleaning Algorithmrules.
The algorithm is shown in Figure 3.
This al-gorithm can be summarized as follows.?
Translate the evaluation corpus first and thenobtain the rules used for the translation and theBLEU score before removing rules.?
For each rule one-by-one, calculate the BLEUscore after removing the rule and obtain the dif-ference between this score and the score beforethe rule was removed.
This difference is calledthe rule contribution.?
If the rule contribution is negative (i.e., theBLUE score increases after removing the rule),remove the rule.In order to achieve faster convergence, this algo-rithm removes all rules whose rule contribution isnegative in one iteration.
This assumes that the re-moved rules are independent from one another.5 N-fold Cross-cleaningIn general, most evaluation corpora are smaller thantraining corpora.
Therefore, omissions of cleaningTrainingCorpusTrainingEvaluationTrainingEvaluationTrainingEvaluationTrainingBaseRule SetRuleSubset 1RuleSubset 2RuleSubset 3FeedbackCleaningFeedbackCleaningFeedbackCleaningRuleDeletion Rule ContributionsCleanedRule SetDivideFigure 4: Structure of Cross-cleaning(In the case of three-fold cross-cleaning)will remain because not all rules can be tested by theevaluation corpus.
In order to avoid this problem, wepropose an advanced method called cross-cleaning(Figure 4), which is similar to cross-validation.The procedure of cross-cleaning is as follows.1.
First, create the base rule set from the entiretraining corpus.2.
Next, divide the training corpus into N piecesuniformly.3.
Leave one piece for the evaluation, acquirerules from the rest (N ?
1) of the pieces, andrepeat them N times.
Thus, we obtain N pairsof rule set and evaluation sub-corpus.
Each ruleset is a subset of the base rule set.4.
Apply the feedback cleaning algorithm to eachof the N pairs and record the rule contributionseven if the rules are removed.
The purpose ofthis step is to obtain the rule contributions.5.
For each rule in the base rule set, sum up therule contributions obtained from the rule sub-sets.
If the sum is negative, remove the rulefrom the base rule set.The major difference of this method from cross-validation is Step 5.
In the case of cross-cleaning,Set Name Feature English JapaneseTraining # of Sentences 149,882Corpus # of Words 868,087 984,197Evaluation # of Sentences 10,145Corpus # of Words 59,533 67,554Test # of Sentences 10,150Corpus # of Words 59,232 67,193Table 1: Corpus Sizethe rule subsets cannot be directly merged becausesome rules have already been removed in Step 4.Therefore, we only obtain the rule contributionsfrom the rule subsets and sum them up.
The summedcontribution is an approximate value of the rulecontribution to the entire training corpus.
Cross-cleaning removes the rules from the base rule setbased on this approximate contribution.Cross-cleaning uses all sentences in the trainingcorpus, so it is nearly equivalent to applying a largeevaluation corpus to feedback cleaning, even thoughit does not require specific evaluation corpora.6 EvaluationIn this section, the effects of feedback cleaning areevaluated by using English-to-Japanese translation.6.1 Experimental SettingsBilingual Corpora The corpus used in the fol-lowing experiments is the Basic Travel ExpressionCorpus (Takezawa et al, 2002).
This is a collec-tion of Japanese sentences and their English trans-lations based on expressions that are usually foundin phrasebooks for foreign tourists.
We divided itinto sub-corpora for training, evaluation, and test asshown in Table 1.
The number of rules acquiredfrom the training corpus (the base rule set size) was105,588.Evaluation Methods of MT Quality We used thefollowing two methods to evaluate MT quality.1.
Test Corpus BLEU ScoreThe BLUE score was calculated with the testcorpus.
The number of references was one foreach sentence, in the same way used for thefeedback cleaning.0.220.240.260.280.30.320 1 2 3 4 5 6 7 8 980k90k100k110k120kBLEUScoreNumber of RulesNumber of IterationsTest Corpus BLEU ScoreEvaluation Corpus BLEU ScoreNumber of RulesFigure 5: Relationship between Number of Itera-tions and BLEU Scores/Number of Rules2.
Subjective QualityA total of 510 sentences from the test corpuswere evaluated by paired comparison.
Specif-ically, the source sentences were translated us-ing the base rule set, and the same sources weretranslated using the rules after the cleaning.One-by-one, a Japanese native speaker judgedwhich MT result was better or that they wereof the same quality.
Subjective quality is repre-sented by the following equation, where I de-notes the number of improved sentences and Ddenotes the number of degraded sentences.Subj.
Quality = I ?D# of test sentences(1)6.2 Feedback Cleaning Using EvaluationCorpusIn order to observe the characteristics of feedbackcleaning, cleaning of the base rule set was carriedout by using the evaluation corpus.
The results areshown in Figure 5.
This graph shows changes inthe test corpus BLEU score, the evaluation corpusBLEU score, and the number of rules along with thenumber of iterations.Consequently, the removed rules converged atnine iterations, and 6,220 rules were removed.
Theevaluation corpus BLEU score was improved by in-creasing the number of iterations, demonstrating thatthe combinatorial optimization by the hill-climbingalgorithm worked effectively.
The test corpus BLEUscore reached a peak score of 0.245 at the seconditeration and slightly decreased after the third itera-tion due to overfitting.
However, the final score was0.244, which is almost the same as the peak score.The test corpus BLEU score was lower thanthe evaluation corpus BLEU score because therules used in the test corpus were not exhaustivelychecked by the evaluation corpus.
If the evaluationcorpus size could be expanded, the test corpus scorewould improve.About 37,000 sentences were translated on aver-age in each iteration.
This means that the time foran iteration is estimated at about ten hours if trans-lation speed is one second per sentence.
This is ashort enough time for us because our method doesnot require real-time processing.
26.3 MT Quality vs.
Cleaning MethodsNext, in order to compare the proposed methodswith the previous methods, the MT quality achievedby each of the following five methods was measured.1.
BaselineThe MT results using the base rule set.2.
Cutoff by FrequencyLow-frequency rules that appeared in the train-ing corpus less often than twice were removedfrom the base rule set.
This threshold wasexperimentally determined by the test corpusBLEU score.3.
?2 TestThe ?2 test was performed in the same manneras in Imamura (2002)?s experiment.
We intro-duced rules with more than 95 percent confi-dence (?2 ?
3.841).4.
Simple Feedback CleaningFeedback cleaning was carried out using theevaluation corpus in Table 1.5.
Cross-cleaningN-fold cross-cleaning was carried out.
We ap-plied five-fold cross-cleaning in this experi-ment.The results are shown in Table 2.
This table showsthat the test corpus BLEU score and the subjective2In this experiment, it took about 80 hours until convergenceusing a Pentium 4 2-GHz computer.Previous Methods Proposed MethodsBaseline Cutoff by Freq.
?2 Test Simple FC Cross-cleaning# of Rules 105,588 26,053 1,499 99,368 82,462Test Corpus BLEU Score 0.232 0.234 0.157 0.244 0.277Subjective Quality +1.77% -6.67% +6.67% +10.0%# of Improved Sentences 83 115 83 100# of Same Quality 353 246 378 361(Same Results) (257) (114) (266) (234)# of Degraded Sentences 74 149 49 49Table 2: MT Quality vs.
Cleaning Methodsquality of the proposed methods (simple feedbackcleaning and cross-cleaning) are considerably im-proved over those of the previous methods.Focusing on the subjective quality of the proposedmethods, some MT results were degraded from thebaseline due to the removal of rules.
However, thesubjective quality levels were relatively improvedbecause our methods aim to increase the portion ofcorrect MT results.Focusing on the number of the rules, the ruleset of the simple feedback cleaning is clearly a lo-cally optimal solution, since the number of rulesis more than that of cross-cleaning, although theBLEU score is lower.
In comparing the number ofrules in cross-cleaning with that in the cutoff by fre-quency, the former is three times higher than the lat-ter.
We assume that the solution of cross-cleaningis also the locally optimal solution.
If we could findthe globally optimal solution, the MT quality wouldcertainly improve further.7 Discussion7.1 Other Automatic Evaluation MethodsThe idea of feedback cleaning is independent ofBLEU.
Some automatic evaluation methods of MTquality other than BLEU have been proposed.
Forexample, Su et al (1992), Yasuda et al (2001), andAkiba et al (2001) measure similarity between MTresults and the references by DP matching (edit dis-tances) and then output the evaluation scores.
Theseautomatic evaluation methods that output scores areapplicable to feedback cleaning.The characteristics common to these methods, in-cluding BLEU, is that the similarity to referencesare measured for each sentence, and the evaluationscore of an MT system is calculated by aggregatingthe similarities.
Therefore, MT results of the eval-uation corpus are necessary to evaluate the system,and reducing the number of sentence translations isan important technique for all of these methods.The effects of feedback cleaning depend on thecharacteristics of objective measures.
DP-basedmeasures and BLEU have different characteristics(Yasuda et al, 2003).
The exploration of severalmeasures for feedback cleaning remains an interest-ing future work.7.2 Domain AdaptationWhen applying corpus-based machine translation toa different domain, bilingual corpora of the new do-main are necessary.
However, the sizes of the newcorpora are generally smaller than that of the orig-inal corpus because the collection of bilingual sen-tences requires a high cost.The feedback cleaning proposed in this paper canbe interpreted as adapting the translation rules sothat the MT results become similar to the evaluationcorpus.
Therefore, if we regard the bilingual corpusof the new domain as the evaluation corpus and carryout feedback cleaning, the rule set will be adapted tothe new domain.
In other words, our method can beapplied to adaptation of an MT system by using asmaller corpus of the new domain.8 ConclusionsIn this paper, we proposed a feedback cleaningmethod that utilizes automatic evaluation to removeincorrect/redundant translation rules.
BLEU wasutilized for the automatic evaluation of MT qual-ity, and the hill-climbing algorithm was applied tosearching for the combinatorial optimization.
Uti-lizing features of this task, incorrect/redundant ruleswere removed from the initial solution, which con-tains all rules acquired from the training corpus.
Inaddition, we proposed N-fold cross-cleaning to re-duce the influence of the evaluation corpus size.
Ourexperiments show that the MT quality was improvedby 10% in paired comparison and by 0.045 in theBLEU score.
This is considerable improvement overthe previous methods.AcknowledgmentThe research reported here is supported in part bya contract with the Telecommunications Advance-ment Organization of Japan entitled, ?A study ofspeech dialogue translation technology based on alarge corpus.
?ReferencesYasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.2001.
Using multiple edit distances to automaticallyrank machine translation output.
In Proceedings ofMachine Translation Summit VIII, pages 15?20.Osamu Furuse and Hitoshi Iida.
1994.
Constituentboundary parsing for example-based machine transla-tion.
In Proceedings of COLING-94, pages 105?111.Kenji Imamura.
2001.
Hierarchical phrase alignmentharmonized with parsing.
In Proceedings of the 6thNatural Language Processing Pacific Rim Symposium(NLPRS 2001), pages 377?384.Kenji Imamura.
2002.
Application of translation knowl-edge acquired by hierarchical phrase alignment forpattern-based MT.
In Proceedings of the 9th Confer-ence on Theoretical and Methodological Issues in Ma-chine Translation (TMI-2002), pages 74?84.Arul Menezes and Stephen D. Richardson.
2001.
Abest first alignment algorithm for automatic extrac-tion of transfer mappings from bilingual corpora.
InProceedings of the ?Workshop on Example-Based Ma-chine Translation?
in MT Summit VIII, pages 35?42.Adam Meyers, Michiko Kosaka, and Ralph Grishman.2000.
Chart-based translation rule application in ma-chine translation.
In Proceedings of COLING-2000,pages 537?543.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 311?318.Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang.
1992.A new quantitative quality measure for machine trans-lation systems.
In Proceedings of COLING-92, pages433?439.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a broad-coverage bilingual corpus for speechtranslation of travel conversations in the real world.In Proceedings of the Third International Conferenceon Language Resources and Evaluation (LREC 2002),pages 147?152.Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-ichi Yamamoto, and Masuzo Yanagida.
2001.
An au-tomatic evaluation method of translation quality usingtranslation answer candidates queried from a parallelcorpus.
In Proceedings of Machine Translation Sum-mit VIII, pages 373?378.Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-ichi Yamamoto, and Masuzo Yanagida.
2003.
Appli-cations of automatic evaluation methods to measuringa capability of speech translation system.
In Proceed-ings of the 10th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL 2003), pages 371?378.
