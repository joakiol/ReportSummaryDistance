Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193?196,New York, June 2006. c?2006 Association for Computational LinguisticsSubword-based Tagging by Conditional Random Fields for Chinese WordSegmentationRuiqiang Zhang1,2 and Genichiro Kikui?
and Eiichiro Sumita1,21National Institute of Information and Communications Technology2ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan{ruiqiang.zhang,eiichiro.sumita}@atr.jpAbstractWe proposed two approaches to improve Chi-nese word segmentation: a subword-based tag-ging and a confidence measure approach.
Wefound the former achieved better performancethan the existing character-based tagging, andthe latter improved segmentation further bycombining the former with a dictionary-basedsegmentation.
In addition, the latter can beused to balance out-of-vocabulary rates andin-vocabulary rates.
By these techniques weachieved higher F-scores in CITYU, PKU andMSR corpora than the best results from SighanBakeoff 2005.1 IntroductionThe character-based ?IOB?
tagging approach has beenwidely used in Chinese word segmentation recently (Xueand Shen, 2003; Peng and McCallum, 2004; Tsenget al, 2005).
Under the scheme, each character of aword is labeled as ?B?
if it is the first character of amultiple-character word, or ?O?
if the character func-tions as an independent word, or ?I?
otherwise.?
For ex-ample, ?
(whole) (Beijing city)?
is labeled as?
(whole)/O (north)/B (capital)/I (city)/I?.We found that so far all the existing implementationswere using character-based IOB tagging.
In this workwe propose a subword-based IOB tagging, which as-signs tags to a pre-defined lexicon subset consisting ofthe most frequent multiple-character words in addition tosingle Chinese characters.
If only Chinese characters areused, the subword-based IOB tagging is downgraded intoa character-based one.
Taking the same example men-tioned above, ?
(whole) (Beijing city)?
is la-beled as ?
(whole)/O (Beijing)/B (city)/I?
in thesubword-based tagging, where ?
(Beijing)/B?
is la-beled as one unit.
We will give a detailed description ofthis approach in Section 2.?
Now the second author is affiliated with NTT.In addition, we found a clear weakness with the IOBtagging approach: It yields a very low in-vocabulary (IV)rate (R-iv) in return for a higher out-of-vocabulary (OOV)rate (R-oov).
In the results of the closed test in Bakeoff2005 (Emerson, 2005), the work of (Tseng et al, 2005),using conditional random fields (CRF) for the IOB tag-ging, yielded very high R-oovs in all of the four corporaused, but the R-iv rates were lower.
While OOV recog-nition is very important in word segmentation, a higherIV rate is also desired.
In this work we propose a confi-dence measure approach to lessen the weakness.
By thisapproach we can change R-oovs and R-ivs and find anoptimal tradeoff.
This approach will be described in Sec-tion 2.2.In the followings, we illustrate our word segmentationprocess in Section 2, where the subword-based tagging isimplemented by the CRFs method.
Section 3 presents ourexperimental results.
Section 4 describes current state-of-the-art methods for Chinese word segmentation, withwhich our results were compared.
Section 5 provides theconcluding remarks.2 Our Chinese word segmentation processOur word segmentation process is illustrated in Fig.
1.
Itis composed of three parts: a dictionary-based N-gramword segmentation for segmenting IV words, a subword-based tagging by the CRF for recognizing OOVs, and aconfidence-dependent word segmentation used for merg-ing the results of both the dictionary-based and the IOBtagging.
An example exhibiting each step?s results is alsogiven in the figure.Since the dictionary-based approach is a well-knownmethod, we skip its technical descriptions.
However,keep in mind that the dictionary-based approach can pro-duce a higher R-iv rate.
We will use this advantage in theconfidence measure approach.2.1 Subword-based IOB tagging using CRFsThere are several steps to train a subword-based IOB tag-ger.
First, we extracted a word list from the training datasorted in decreasing order by their counts in the training193????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\input???????
?+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\Dictionary-based word segmentation?%?,?,?2?2?
?%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,Subword-based IOB tagging?%?,?,?2?2?
?%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,Confidence-based segmentation???????
?+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\outputFigure 1: Outline of word segmentation processdata.
We chose all the single characters and the top multi-character words as a lexicon subset for the IOB tagging.If the subset consists of Chinese characters only, it is acharacter-based IOB tagger.
We regard the words in thesubset as the subwords for the IOB tagging.Second, we re-segmented the words in the trainingdata into subwords belonging to the subset, and assignedIOB tags to them.
For a character-based IOB tagger,there is only one possibility of re-segmentation.
How-ever, there are multiple choices for a subword-basedIOB tagger.
For example, ?
(Beijing-city)?
canbe segmented as ?
(Beijing-city)/O,?
or ?
(Beijing)/B (city)/I,?
or ?
(north)/B (capital)/I(city)/I.?
In this work we used forward maximal match(FMM) for disambiguation.
Of course, backward max-imal match (BMM) or other approaches are also appli-cable.
We did not conduct comparative experiments be-cause trivial differences of these approaches may not re-sult in significant consequences to the subword-based ap-proach.In the third step, we used the CRFs approach to trainthe IOB tagger (Lafferty et al, 2001) on the training data.We downloaded and used the package ?CRF++?
from thesite ?http://www.chasen.org/t?aku/software.?
According tothe CRFs, the probability of an IOB tag sequence, T =t0t1 ?
?
?
tM , given the word sequence, W = w0w1 ?
?
?wM , isdefined byp(T |W) =exp???????M?i=1???????
?k?k fk(ti?1, ti,W) +?k?kgk(ti,W)?????????????
?/Z,Z =?T=t0t1??
?tMp(T |W)(1)where we call fk(ti?1, ti,W) bigram feature functions be-cause the features trigger the previous observation ti?1and current observation ti simultaneously; gk(ti,W), theunigram feature functions because they trigger only cur-rent observation ti.
?k and ?k are the model parameterscorresponding to feature functions fk and gk respectively.The model parameters were trained by maximizing thelog-likelihood of the training data using L-BFGS gradi-ent descent optimization method.
In order to overcomeoverfitting, a gaussian prior was imposed in the training.The types of unigram features used in our experimentsincluded the following types:w0,w?1,w1,w?2,w2,w0w?1,w0w1,w?1w1,w?2w?1,w2w0where w stands for word.
The subscripts are position in-dicators.
0 means the current word; ?1, ?2, the first orsecond word to the left; 1, 2, the first or second word tothe right.For the bigram features, we only used the previous andthe current observations, t?1t0.As to feature selection, we simply used absolute countsfor each feature in the training data.
We defined a cutoffvalue for each feature type and selected the features withoccurrence counts over the cutoff.A forward-backward algorithm was used in the train-ing and viterbi algorithm was used in the decoding.2.2 Confidence-dependent word segmentationBefore moving to this step in Figure 1, we produced twosegmentation results: the one by the dictionary-based ap-proach and the one by the IOB tagging.
However, nei-ther was perfect.
The dictionary-based segmentation pro-duced results with higher R-ivs but lower R-oovs whilethe IOB tagging yielded the contrary results.
In this sec-tion we introduce a confidence measure approach to com-bine the two results.
We define a confidence measure,CM(tiob|w), to measure the confidence of the results pro-duced by the IOB tagging by using the results from thedictionary-based segmentation.
The confidence measurecomes from two sources: IOB tagging and dictionary-based word segmentation.
Its calculation is defined as:CM(tiob|w) = ?CMiob(tiob|w) + (1 ?
?)?
(tw, tiob)ng (2)where tiob is the word w?s IOB tag assigned by the IOBtagging; tw, a prior IOB tag determined by the results ofthe dictionary-based segmentation.
After the dictionary-based word segmentation, the words are re-segmentedinto subwords by FMM before being fed to IOB tagging.Each subword is given a prior IOB tag, tw.
CMiob(t|w), aconfidence probability derived in the process of IOB tag-ging, is defined asCMiob(t|wi) =?T=t0t1??
?tM ,ti=t P(T |W,wi)?T=t0t1??
?tM P(T |W)where the numerator is a sum of all the observation se-quences with word wi labeled as t.194?
(tw, tiob)ng denotes the contribution of the dictionary-based segmentation.
It is a Kronecker delta function de-fined as?
(tw, tiob)ng = {1 if tw = tiob0 otherwiseIn Eq.
2, ?
is a weighting between the IOB taggingand the dictionary-based word segmentation.
We foundthe value 0.7 for ?, empirically.By Eq.
2 the results of IOB tagging were re-evaluated.A confidence measure threshold, t, was defined for mak-ing a decision based on the value.
If the value was lowerthan t, the IOB tag was rejected and the dictionary-basedsegmentation was used; otherwise, the IOB tagging seg-mentation was used.
A new OOV was thus created.
Forthe two extreme cases, t = 0 is the case of the IOB tag-ging while t = 1 is that of the dictionary-based approach.In a real application, a satisfactory tradeoff between R-ivs and R-oovs could find through tuning the confidencethreshold.
In Section 3.2 we will present the experimentalsegmentation results of the confidence measure approach.3 ExperimentsWe used the data provided by Sighan Bakeoff 2005 totest our approaches described in the previous sections.The data contain four corpora from different sources:Academia Sinica (AS), City University of Hong Kong(CITYU), Peking University (PKU) and Microsoft Re-search in Beijing (MSR).
Since this work was to evaluatethe proposed subword-based IOB tagging, we carried outthe closed test only.
Five metrics were used to evaluatesegmentation results: recall(R), precision(P), F-score(F),OOV rate(R-oov) and IV rate(R-iv).
For detailed info.
ofthe corpora and these scores, refer to (Emerson, 2005).For the dictionary-based approach, we extracted aword list from the training data as the vocabulary.
Tri-gram LMs were generated using the SRI LM toolkit fordisambiguation.
Table 1 shows the performance of thedictionary-based segmentation.
Since there were somesingle-character words present in the test data but not inthe training data, the R-oov rates were not zero in thisexperiment.
In fact, there were no OOV recognition.Hence, this approach produced lower F-scores.
However,the R-ivs were very high.3.1 Effects of the Character-based and thesubword-based taggerThe main difference between the character-based and theword-based is the contents of the lexicon subset usedfor re-segmentation.
For the character-based tagging, weused all the Chinese characters.
For the subword-basedtagging, we added another 2000 most frequent multiple-character words to the lexicons for tagging.
The segmen-tation results of the dictionary-based were re-segmentedR P F R-oov R-ivAS 0.941 0.881 0.910 0.038 0.982CITYU 0.928 0.851 0.888 0.164 0.989PKU 0.948 0.912 0.930 0.408 0.981MSR 0.968 0.927 0.947 0.048 0.993Table 1: Our segmentation results by the dictionary-based approach for the closed test of Bakeoff 2005, verylow R-oov rates due to no OOV recognition applied.R P F R-oov R-ivAS 0.951 0.942 0.947 0.678 0.9640.953 0.940 0.947 0.647 0.967CITYU 0.939 0.943 0.941 0.700 0.9580.950 0.942 0.946 0.736 0.967PKU 0.940 0.950 0.945 0.783 0.9490.943 0.946 0.945 0.754 0.955MSR 0.957 0.960 0.959 0.710 0.9640.965 0.963 0.964 0.716 0.972Table 2: Segmentation results by a pure subword-basedIOB tagging.
The upper numbers are of the character-based and the lower ones, the subword-based.using the FMM, and then labeled with ?IOB?
tags by theCRFs.
The segmentation results using CRF tagging areshown in Table 2, where the upper numbers of each slotwere produced by the character-based approach while thelower numbers were of the subword-based.
We foundthat the proposed subword-based approaches were effec-tive in CITYU and MSR corpora, raising the F-scoresfrom 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 forMSR corpus.
There were no F-score changes for AS andPKU corpora, but the recall rates were improved.
Com-paring Table 1 and 2, we found the CRF-modeled IOBtagging yielded better segmentation than the dictionary-based approach.
However, the R-iv rates were gettingworse in return for higher R-oov rates.
We will tacklethis problem by the confidence measure approach.3.2 Effect of the confidence measureIn section 2.2, we proposed a confidence measure ap-proach to re-evaluate the results of IOB tagging by com-binations of the results of the dictionary-based segmen-tation.
The effect of the confidence measure is shown inTable 3, where we used ?
= 0.7 and confidence thresholdt = 0.8.
In each slot, the numbers on the top were of thecharacter-based approach while the numbers on the bot-tom were the subword-based.
We found the results in Ta-ble 3 were better than those in Table 2 and Table 1, whichprove that using confidence measure approach achievedthe best performance over the dictionary-based segmen-tation and the IOB tagging approach.
The act of con-fidence measure made a tradeoff between R-ivs and R-oovs, yielding higher R-oovs than Table 1 and higher R-195R P F R-oov R-ivAS 0.953 0.944 0.948 0.607 0.9690.956 0.947 0.951 0.649 0.969CITYU 0.943 0.948 0.946 0.682 0.9640.952 0.949 0.951 0.741 0.969PKU 0.942 0.957 0.949 0.775 0.9520.947 0.955 0.951 0.748 0.959MSR 0.960 0.966 0.963 0.674 0.9670.972 0.969 0.971 0.712 0.976Table 3: Effects of combination using the confidencemeasure.
The upper numbers and the lower numbers areof the character-based and the subword-based, respec-tivelyAS CITYU MSR PKUBakeoff-best 0.952 0.943 0.964 0.950Ours 0.951 0.951 0.971 0.951Table 4: Comparison our results with the best ones fromSighan Bakeoff 2005 in terms of F-scoreivs than Table 2.Even with the use of confidence measure, the word-based IOB tagging still outperformed the character-basedIOB tagging.
It proves the proposed word-based IOB tag-ging was very effective.4 Discussion and Related worksThe IOB tagging approach adopted in this work is not anew idea.
It was first used in Chinese word segmentationby (Xue and Shen, 2003), where maximum entropy meth-ods were used.
Later, this approach was implementedby the CRF-based method (Peng and McCallum, 2004),which was proved to achieve better results than the maxi-mum entropy approach because it can solve the label biasproblem (Lafferty et al, 2001).Our main contribution is to extend the IOB tagging ap-proach from being a character-based to a subword-based.We proved the new approach enhanced the word segmen-tation significantly.
Our results are listed together withthe best results from Bakeoff 2005 in Table 4 in termsof F-scores.
We achieved the highest F-scores in CITYU,PKU and MSR corpora.
We think our proposed subword-based tagging played an important role for the good re-sults.
Since it was a closed test, some information suchas Arabic and Chinese number and alphabetical letterscannot be used.
We could yield a better results than thoseshown in Table 4 using such information.
For example,inconsistent errors of foreign names can be fixed if al-phabetical characters are known.
For AS corpus, ?AdamSmith?
are two words in the training but become a one-word in the test, ?AdamSmith?.
Our approaches pro-duced wrong segmentations for labeling inconsistency.Another advantage of the word-based IOB taggingover the character-based is its speed.
The subword-basedapproach is faster because fewer words than characterswere labeled.
We found a speed up both in training andtest.The idea of using the confidence measure has appearedin (Peng and McCallum, 2004), where it was used to rec-ognize the OOVs.
In this work we used it more delicately.By way of the confidence measure we combined resultsfrom the dictionary-based and the IOB-tagging-based andas a result, we could achieve the optimal performance.5 ConclusionsIn this work, we proposed a subword-based IOB taggingmethod for Chinese word segmentation.
Using the CRFsapproaches, we prove that it outperformed the character-based method using the CRF approaches.
We also suc-cessfully employed the confidence measure to make aconfidence-dependent word segmentation.
This approachis effective for performing desired segmentation based onusers?
requirements to R-oov and R-iv.AcknowledgementsThe authors appreciate the reviewers?
effort and good ad-vice for improving the paper.ReferencesThomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, Jeju, Korea.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
ofICML-2001, pages 591?598.Fuchun Peng and Andrew McCallum.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
In Proc.
of Coling-2004, pages562?568, Geneva, Switzerland.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for Sighan bakeoff2005.
In Proceedings of the Fourth SIGHANWorkshopon Chinese Language Processing, Jeju, Korea.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as LMR tagging.
In Proceedings of theSecond SIGHAN Workshop on Chinese Language Pro-cessing.196
