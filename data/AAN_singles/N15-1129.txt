Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1221?1226,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMulti-Task Word Alignment Triangulation for Low-Resource LanguagesTomer Levinboim and David ChiangDepartment of Computer Science and EngineeringUniversity of Notre Dame{levinboim.1,dchiang}@nd.eduAbstractWe present a multi-task learning approachthat jointly trains three word alignment mod-els over disjoint bitexts of three languages:source, target and pivot.
Our approach buildsupon model triangulation, following Wang etal., which approximates a source-target modelby combining source-pivot and pivot-targetmodels.
We develop a MAP-EM algorithmthat uses triangulation as a prior, and showhow to extend it to a multi-task setting.
Ona low-resource Czech-English corpus, usingFrench as the pivot, our multi-task learning ap-proach more than doubles the gains in both F-and Bleu scores compared to the interpolationapproach of Wang et al Further experimentsreveal that the choice of pivot language doesnot significantly affect performance.1 IntroductionWord alignment (Brown et al, 1993; Vogel et al,1996) is a fundamental task in the machine transla-tion (MT) pipeline.
To train good word alignmentmodels, we require access to a large parallel corpus.However, collection of parallel corpora has mostlyfocused on a small number of widely-spoken lan-guages.
As such, resources for almost any other pairare either limited or non-existent.To improve word alignment and MT in a low-resource setting, we design a multitask learningapproach that utilizes parallel data of a third lan-guage, called the pivot language (?3).
Specifi-cally, we derive an efficient and easy-to-implementMAP-EM-like algorithm that jointly trains source-target, source-pivot and pivot-target algnment mod-els, each on its own bitext, such that each model ben-efits from observations made by the other two.Our method subsumes the model interpolation ap-proach of Wang et al (2006), who independentlytrain these three models and then interpolate thesource-target model with an approximate source-target model, constructed by combining the source-pivot and pivot-target models.Pretending that Czech-English is low-resource,we conduct word alignment and MT experi-ments (?4).
With French as the pivot, our approachsignificantly outperforms the interpolation methodof Wang et al (2006) on both alignment F- and Bleuscores.
Somewhat surprisingly, we find that our ap-proach is insensitive to the choice of pivot language.2 Triangulation and InterpolationWang et al (2006) focus on learning a word align-ment model without a source-target corpus.
To doso, they assume access to both source-pivot andpivot-target bitexts on which they independentlytrain a source-pivot word alignment model ?spand apivot-target model ?pt.
They then combine the twomodels by marginalizing over the pivot language, re-sulting in an approximate source-target model ?
?st.This combination process is referred to as triangu-lation (see ?5).In particular, they construct the triangulatedsource-target t-table t?stfrom the source-pivot andpivot-target t-tables tsp, tptusing the following ap-proximation:t?st(t | s) =?pt(t | p, s) ?
t(p | s)?
?ptpt(t | p) ?
tsp(p | s) (1)Subsequently, if a source-target corpus is available,they train a standard source-target model ?st, andtune the interpolation?tst= ?interptst+ (1 ?
?interp)t?stwith respect to ?interpto reduce alignment error rate(Koehn, 2005) over a hand-aligned development set.1221Wang et al (2006) propose triangulation heuris-tics for other model parameters; however, in this pa-per, we consider only t-table triangulation.3 Our MethodWe now discuss two approaches that better exploitmodel triangulation.
In the first, we use the triangu-lated t-table to construct a prior on the source-targett-table.
In the second, we place a prior on each ofthe three models and train them jointly.3.1 Triangulation as a Fixed PriorWe first propose to better utilize the triangulated t-table t?st(Eq.
1) by using it to construct an informa-tive prior for the source-target t-table tst?
?st.Specifically, we modify the word alignment gen-erative story by placing Dirichlet priors on each ofthe multinomial t-table distributions tst(?
| s):tst(?
| s) ?
Dirichlet(?s) for all s. (2)Here, each ?s= (.
.
.
, ?st, .
.
.)
denotes a hyperparam-eter vector which will be defined shortly.Fixing this prior, we optimize the model posteriorlikelihood P(?st| bitextst) to find a maximum-a-posteriori (MAP) estimate.
This is done accordingthe MAP-EM framework (Dempster et al, 1977),which differs slightly from standard EM.
The E-step remains as is: fixing the model ?st, we collectexpected counts E[c(s, t)] for each decision in thegenerative story.
The M-step is modified to max-imize the regularized expected complete-data log-likelihood with respect to the model parameters ?st,where the regularizer corresponds to the prior.Due to the conjugacy of the Dirichlet priors withthe multinomial t-table distributions, the sole modi-fication to the regular EM implementation is in theM-step update rule of the t-table parameters:tst(t | s) =E[c(s, t)] + ?st?
1?t(E[c(s, t)] + ?st?
1)(3)where E[c(s, t)] is the expected number of timessource word s aligns with target word t in the source-target bitext.
Moreover, through Eq.
3, we can view?st?
1 as a pseudo-count for such an alignment.To define the hyperparameter vector ?swe de-compose it as follows:?s= Cs?
ms+ 1 (4)where Cs> 0 is a scalar parameter, msis a proba-bility vector, encoding the mode of the Dirichlet and1 denotes an all-one vector.
Roughly, when Csishigh, samples drawn from the Dirichlet are likely toconcentrate near the mode ms.
Using this decompo-sition, we set for all s:ms= t?st(?
| s) (5)Cs= ?
?
c(s)???s?
c(s?)?s?
c(s?)?
(6)where c(s) is the count of source word s in thesource-target bitext, and the scalar hyperparameters?, ?
> 0 are to be tuned (We experimented with com-pletely eliminating the hyperparameters ?, ?
by di-rectly learning the parameters Cs.
To do so, we im-plemented the algorithm of Minka (2000) for learn-ing the Dirichlet prior, but only learned the parame-ters Cswhile keeping the means msfixed to the trian-gulation.
However, preliminary experiments showedperformance degradation compared to simple hyper-parameter tuning).
Thus, the distribution tst(?
| s)arises from a Dirichlet with mode t?st(?
| s) and willtend to concentrate around this mode as a functionof the frequency of s.The hyperparameter ?
linearly controls thestrength of all priors.
The last term in Eq.
6 keepsthe sum of Csinsensitive to ?, such that?sCs=??sc(s).
In all our experiments we fixed ?
= 0.5.Setting ?
< 1 down-weights the parameter Csof fre-quent words s compared to rare ones.
This makesthe Dirichlet prior relatively weaker for frequentwords, where we can let the data speak for itself,and relatively stronger for rare ones, where a goodprior is needed.Finally, note that this EM procedure reduces toan interpolation method similar to that of Wang etal.
by applying Eq.
3 only at the very last M-step,with ?s, msas above and Cs= ?
?tE[c(s, t)].3.2 Joint TrainingNext, we further exploit the triangulation idea in de-signing a multi-task learning approach that jointlytrains the three word alignment models ?st, ?sp,and ?pt.To do so, we view each model?s t-table as orig-inating from Dirichlet distributions defined by thetriangulation of the other two t-tables.
We then train1222Algorithm 1 Joint training of ?st,?sp,?ptParameters: ?, ?
> 0?
Initialize{?(0)st,?(0)sp,?(0)pt}?
Initialize {Cs}, {Cp}, {Ct} as in Eq.
6?
For each EM iteration i:Estimate hyperparameters ?:1.
Compute t(i)?stfrom t(i?1)spand t(i?1)pt(Eq.
1)2.
Set ?
(i)st:= Cs?
t(i)?st(t | s) + 1E: collect expected counts E[c(?
)](i)from ?
(i?1)stM: Update ?
(i)stusing E[c(?
)](i)and ?(i)st(Eq.
3)Repeat for ?(i)sp,?
(i)ptusing Eq.
7 as requiredthe models in a MAP-EM like manner, updatingboth the model parameters and their prior hyperpa-rameters at each iteration.
Roughly, this approachaims at maximizing the posterior likelihood of thethree models with respect to both model parametersand their hyperparameters (see Appendix).Procedurally, the idea is simple: In the E-step, ex-pected counts E[c(?)]
are collected from each modelas usual.
In the M-step, each t-table is updated ac-cording to Eq.
3 using the current expected countsE[c(?)]
and an estimate of ?
from the triangulationof the most recent version of the other two models.See Algorithm 1.Note, however, that we cannot obtain the triangu-lated t-tables t?sp, t?ptby simply applying the trian-gulation equation (Eq.
1).
For example, to constructt?spwe need both source-to-target and target-to-pivotdistributions.
While we have the former in tst, wedo not have ttp.
To resolve this issue, we simplyapproximate ttpfrom the reverse t-table tpt?
?ptas follows:ttp(p | t) :=c(p)tpt(t | p)?pc(p)tpt(t | p)(7)where c(p) denotes the unigram frequency of theword p. A similar transformation is done on tsptoobtain tps, which is then used in computing t?pt.3.3 Adjustment of the t-tableNote that a t-table resulting from the triangulationequation (Eq.
1) is both noisy and dense.
To seewhy, consider that t?st(t | s) is non-zero wheneverthere is a pivot word p that co-occurs with both sand t. This is very likely to occur, for example, if pis a function word.To adjust for both density and noise, we pro-pose a simple product-of-experts re-estimation thatrelies on the available source-target parallel data.The two experts are the triangulated t-table as de-fined by Eq.
1 and the exponentiated pointwise mu-tual information (PMI), derived from simple tokenco-occurrence statistics of the source-target bitext.That is, we adjust:t?st(t | s) := t?st(t | s) ?p(s, t)p(s)p(t)and normalize the result to form valid conditionaldistributions.Note that the sparsity pattern of the adjusted t-table matches that of a co-occurrence t-table.
Weapplied this adjustment in all of our experiments.4 Experimental ResultsPretending that Czech-English is a low-resourcepair, we conduct two experiments.
In the first, we setFrench as the pivot language and compare our fixed-prior (Sec.
?3.1) and joint training (Sec.
?3.2) ap-proaches against the interpolation method of Wanget al and a baseline HMM word alignment model(Vogel et al, 1996).In the second, we examine the effect of the pivotlanguage identity on our joint training approach,varying the pivot language over French, German,Greek, Hungarian, Lithuanian and Slovak.4.1 DataFor word alignment, we use the Czech-EnglishNews Commentary corpus, along with a develop-ment set of 460 hand aligned sentence pairs.
Forthe MT experiments, we use the WMT10 tuningset (2051 parallel sentences), and both WMT09/10shared task test sets.
See Table 1.For each of the 6 pivot languages, we createdCzech-pivot and pivot-English bitexts of roughly thesame size (ranging from 196k sentences for English-Greek to 223k sentences for Czech-Lithuanian).Each bitext was created by forming a Czech-pivot-English tritext, consisting of about 500k sentences1223from the Europarl corpus (Koehn, 2005) which wasthen split into two disjoint Czech-pivot and pivot-English bitexts of equal size.
Sentences of lengthgreater than 40 were filtered out from all trainingcorpora.4.2 Experiment 1: Method ComparisonWe trained word alignment models in both source-to-target and target-to-source directions.
We used5 iterations of IBM Model 1 followed by 5 itera-tions of HMM.
We tuned hyperparameters to max-imize alignment F-score of the hand-aligned devel-opment set.
Both interpolation parameters ?interpand?
were tuned over the range [0, 1].
For our methods,we fixed ?
= 0.5, which we found effective duringpreliminary experiments.
Alignment F-scores usinggrow-diag-final-and (gdfa) symmetrization (Koehn,2010) are reported in Table 2, column 2.We conducted MT experiments using the Mosestranslation system (Koehn, 2005).
We used a 5-gramLM trained on the Xinhua portion of English Giga-word (LDC2007T07).
To tune the decoder, we usedthe WMT10 tune set.
MT Bleu scores are reportedin Table 2, columns 3?4.Both our methods outperform the baseline and theinterpolation approach.
In particular, the joint train-ing approach more than doubles the gains obtainedby the interpolation approach, on both F- and Bleu.We also evaluated the Czech-French and French-English alignments produced as a by-product of ourjoint method.
While our French-to-English MT ex-periments showed no improvement in Bleu, we sawa +0.6 (25.6 to 26.2) gain in Bleuon the Czech-to-French translation task.
This shows that joint train-ing may lead to some improvements even on high-resource bitexts.4.3 Other Pivot LanguagesWe examined how the choice of pivot language af-fects the joint training approach by varying it over6 languages (French, German, Greek, Hungarian,train dev WMT09 WMT10sentences 85k 460 2525 2489cz tokens 1.63M 9.7k 55k 53ken tokens 1.78M 10k 66k 62kTable 1: Czech-English sentence and token statistics.F Bleumethod/dataset dev WMT09 WMT10baseline 63.8 16.2 16.6interpolation (Wang) 66.2 16.6 17.1fixed-prior (?3.1) 67.3 16.9 17.3joint (?3.2) 70.1 17.2 17.7Table 2: F- and Bleu scores for Czech-English viaFrench.
The joint training method outperforms all othermethods tested.fr fr, sk fr, el fr, sk, el all 6Tune 16.1 16.4 16.4 16.4 16.4WMT09 17.2 17.2 17.2 17.3 17.4WMT10 17.7 17.8 17.8 17.8 17.8Table 3: Czech-English Bleu scores over pivot languagecombinations.
Key: fr=French, sk=Slovak, el=Greek.Lithuanian and Slovak), while keeping the size ofthe pivot language resources roughly the same.Somewhat surprisingly, all models achieved anF-score of about 70%, which resulted in Bleuscores comparable to those reported with French(Table 2).
Subsequently, we combined all pivot lan-guages by simply concatenating the aligned paral-lel texts across pairs, triples and all pivot languages.Combining all pivots yielded modest Bleu score im-provements of +0.2 and +0.1 on the test datasets(Table 3).Considering the low variance in F- and Bleuscores across pivot languages, we computed thepairwise F-scores between the predicted alignments:All scores ranged around 97?98%, indicating thatthe choice of pivot language had little effect on thejoint training procedure.To further verify, we repeated this experimentover Greek-English and Lithuanian-English as thesource-target task (85k parallel sentences), using thesame pivot languages as above, and with comparableamounts of parallel data (?200k sentences).
We ob-tained similar results: In all cases, pairwise F-scoreswere above 97%.5 Related WorkThe term ?triangulation?
comes from the phrase-table triangulation literature (Cohn and Lapata,2007; Razmara and Sarkar, 2013; Dholakia and1224Sarkar, 2014), in which source-pivot and pivot-targetphrase tables are triangulated according to Eq.
1(with words replaced by phrases).
The resulting tri-angulated phrase table can then be combined with anexisting source-target phrase table, and is especiallyuseful in increasing the source language vocabularycoverage, reducing OOVs.
In our case, since wordalignment is a closed vocabulary task, OOVs arenever an issue.In word alignment, Kumar et al (2007) usesmultilingual parallel data to compute better source-target algnment posteriors.
Filali and Bilmes (2005)tag each source token and target token with theirmost likely translation in a pivot language, and thenproceed to align (source word, source tag) tuple se-quences to (target word, target tag) tuple sequences.In contrast, our word alignment method can be ap-plied without multilingual parallel data, and does notcommit to hard decisions.6 Conclusion and Future WorkWe presented a simple multi-task learning algorithmthat jointly trains three word alignment models overdisjoint bitexts.
Our approach is a natural extensionof a mathematically sound MAP-EM algorithm weoriginally developed to better utilize the model tri-angulation idea.
Both algorithms are easy to imple-ment (with closed-form solutions for each step) andrequire minimal effort to integrate into an EM-basedword alignment system.We evaluated our methods on a low-resourceCzech-English word alignment task using additionalCzech-French and French-English corpora.
Ourmulti-task learning approach significantly improvesF- and Bleu scores compared to both baseline andthe interpolation method of Wang et al (2006).
Fur-ther experiments showed our approach is insensitiveto the choice of pivot language, producing roughlythe same alignments over six different pivot lan-guage choices.For future work, we plan to improve word align-ment and translation quality in a more data restrictedcase where there are very weak source-pivot re-sources: for example, word alignment of Malagasy-English via French, using only a Malagasy-Frenchdictionary, or Pashto-English via Persian.AcknowledgementsThe authors would like to thank Kevin Knight,Daniel Marcu and Ashish Vaswani for their com-ments and insights as well as the anonymous re-viewers for their valuable feedback.
This workwas partially supported by DARPA grants DOI/NBCD12AP00225 and HR0011-12-C-0014 and a GoogleFaculty Research Award to Chiang.Appendix: Joint Training Generative StoryWe argue that our joint training procedure can beseen as optimizing the posterior likelihood of thethree models.
Specifically, suppose we place Dirich-let priors on each of the t-tables tst, tsp, tptas be-fore, but define the prior parameterization using asingle hyperparameter ?
= {?spt} and its marginalssuch that:tst(?
| s) ?
D(.
.
.
, ?s?t, .
.
.)
?s?t=?p?spttsp(?
| s) ?
D(.
.
.
, ?sp?, .
.
.)
?sp?=?t?spttpt(?
| p) ?
D(.
.
.
, ?
?pt, .
.
.)
?
?pt=?s?sptIntuitively, ?sptrepresents the number of times asource-pivot-target triplet (s, p, t) was observed.With this prior, we can maximize the posteriorlikelihood of the three models given the three bitexts(denoted data = {bitextst, bitextsp, bitextpt})with respect to all parameters and hyperparameters:arg max?,?P(?
| ?, data) =arg max?,??d?
{st,sp,pt}P(bitextd| ?d)P(?d| ?
)Under the generative story, we need only observe themarginals ?s?t, ?sp?, ?
?ptof ?.
Therefore, instead ofexplicitly optimizing over ?, we can optimize overthe marginals while keeping them consistent (viaconstraints such as?t?s?t=?p?sp?for all s).In our joint training algorithm (Algorithm 1)we abandon these consistency constraints in fa-vor of closed form estimates of the marginals?s?t, ?sp?, ?
?pt.ReferencesPeter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.1225Trevor Cohn and Mirella Lapata.
2007.
Machine trans-lation by triangulation: Making effective use of multi-parallel corpora.
In Proc.
ACL 2007.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.Rohit Dholakia and Anoop Sarkar.
2014.
Pivot-basedtriangulation for low-resource languages.
In Proc.AMTA.Karim Filali and Jeff Bilmes.
2005.
Leveraging multiplelanguages to improve statistical MT word alignments.In Proc.
IEEE Automatic Speech Recognition and Un-derstanding Workshop (ASRU).P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In Proc.
Machine TranslationSummit X, pages 79?86.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA, 1stedition.Shankar Kumar, Franz Och, and Wolfgang Macherey.2007.
Improving word alignment with bridge lan-guages.
In Proc.
EMNLP-CoNLL.Thomas P. Minka.
2000.
Estimating a Dirichlet distribu-tion.
Technical report, MIT.Majid Razmara and Anoop Sarkar.
2013.
Ensemble tri-angulation for statistical machine translation.
In Proc.IJCNLP, pages 252?260.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
COLING, pages 836?841.Haifeng Wang, Hua Wu, and Zhanyi Liu.
2006.
Wordalignment for languages with scarce resources usingbilingual corpora of other language pairs.
In Proc.COLING/ACL, pages 874?881.1226
