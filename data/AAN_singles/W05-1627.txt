Spatial descriptions as referring expressions in the MapTask domainSebastian VargesInformation Technology Research InstituteUniversity of BrightonSebastian.Varges@itri.brighton.ac.ukAbstractWe discuss work-in-progress on a hybrid ap-proach to the generation of spatial descriptions,using the maps of the Map Task dialogue cor-pus as domain models.
We treat spatial de-scriptions as referring expressions that distin-guish particular points on the maps from allother points (potential ?distractors?).
Our ap-proach is based on rule-based overgenerationof spatial descriptions combined with rankingwhich currently is based on explicit goodnesscriteria but will ultimately be corpus-based.Ranking for content determination tasks suchas referring expression generation raises a num-ber of deep and vexing questions about the roleof corpora in NLG, the kind of knowledge theycan provide and how it is used.1 Introduction, or: The lack of domainmodel annotation in corpora used forranking in NLGIn recent years, ranking approaches to Natural LanguageGeneration (NLG) have become increasingly popular.They abandon the idea of generation as a determinis-tic decision-making process in favour of approaches thatcombine overgeneration with ranking at some stage inprocessing.
A major motivation is the potential reduc-tion of manual development costs and increased adapt-ability and robustness.Several approaches to sentence realization use rank-ing models trained on corpora of human-authored textsto judge the fluency of the candidates produced by thegeneration system.
The work of [Langkilde and Knight,1998; Langkilde, 2002] describes a sentence realizer thatuses word ngram models trained on a corpus of 250 mil-lion words to rank candidates.
[Varges and Mellish, 2001]present an approach to sentence realization that employsan instance-based ranker trained on a semantically an-notated subset of the Penn treebank II (?Who?s News?texts).
[Ratnaparkhi, 2000] describes a sentence realizerthat had been trained on a domain-specific corpus (in theair travel domain) augmented with semantic attribute-value pairs.
[Bangalore and Rambow, 2000] describe arealizer that uses a word ngram model combined witha tree-based stochastic model trained on a version ofthe Penn treebank annotated in XTAG grammar format.
[Karamanis et al, 2004] discuss centering-based metricsof coherence that could be used for choosing among com-peting text structures.
The metrics are derived from theGnome corpus [Poesio et al, 2004].In sum, these approaches use corpora with varioustypes of annotation: syntactic trees, semantic roles, textstructure, or no annotation at all (for word-based ngrammodels).
However, what they all have in common, evenwhen dealing with higher-level text structures, is the ab-sence of any domain model annotation, i.e.
informationabout the available knowledge pool from which the con-tent was chosen.
This seems to be unproblematic forsurface realization where the semantic input has beendetermined beforehand.This paper asks what the lack of domain informationmeans for ranking in the context of content determina-tion, focusing on the generation of referring expressions(GRE).
A particularly intriguing aspect of GRE is therole of distractors in choosing the content (types, at-tributes, relations) used to describe the target object(s).For example, we may describe a target as ?the red car?if there is also a blue one, but we may just describe itas ?the car?
if there are no other cars in the domain (butpossibly objects of other types).
[Stone, 2003] proposesto use this observation to reason backwards from a givenreferring expression to the state of the knowledge basethat motivated it.
We may call this the ?presupposi-tional?
or ?abductive?
view of GRE.
The approach is in-tended to address the knowledge acquisition bottleneckin NLG by means of example specifications constructedfor the purpose of knowledge acquisition.
It seems to usthat, if the approach were to be applied to actual textcorpora, one needed to address the fact that people of-ten include ?redundant?
attributes that do not eliminateany distractors.
Thus, ?the red car?
does not necessar-ily presuppose the existence of another car of differentcolour.
Furthermore, there are likely to be a large num-ber of domain models/knowledge bases that could havemotivated the production of a referring expression.
[Siddharthan and Copestake, 2004] take a corpus-based perspective and essentially regard a text as aknowledge base from which descriptions of domain ob-jects can be extracted.
Some NPs are descriptions ofthe same object (for example if they have the same headnoun and share attributes and relations in certain ways),others are deemed distractors.
It seems that, in contrastto [Stone, 2003], this approach cannot recover those do-main objects or properties that are never mentioned be-cause it only extracts what is explicitly stated in thetext.Both the work reported in [Stone, 2003] and in [Sid-dharthan and Copestake, 2004] can be seen as attemptsto deal with the lack of domain model information insituations where only the surface forms of referring ex-pressions are given.
Obtaining such a domain modelis highly desirable in order to establish which part ofa larger knowledge pool is actually selected for realiza-tion.
This could be used to automatically learn modelsof content selection, for example.
However, as we ob-served above, most corpora do not provide this kind ofknowledge for obvious practical reasons.
For example,how can we know what knowledge a Wall Street Journalauthor had available at the time of writing?In this paper, we describe work-in-progress on exploit-ing a corpus that provides not only surface forms butalso domain model information: the MapTask dialoguecorpus [Anderson et al, 1991].2 Spatial descriptions as referringexpressionsIn the Map Task dialogues, a subject gives route direc-tions to another subject, involving the production of de-scriptions such as ?at the left-hand side of the bananatree?
and ?about three quarters up on the page, to theextreme left?.
16 Maps and 32 subjects (8 groups of 4speakers) were used to produce 128 dialogues.
The sub-jects were not able to see each other?s maps and thushad to resort to verbal descriptions of the map contents.There are some (intentional) mismatches the subject?smaps such as additional landmarks, changed names etc.This is a good source of spatial descriptions, for exam-ple: ?have you got gorillas?
... well, they?re in the bottomleft-hand corner.
?Figure 1 shows one of the ?giver?
maps, which, incontrast to the corresponding ?follower?
map, shows theroute the follower is supposed to take.
A main charac-teristic of both giver and follower maps is the display ofnamed landmarks.
These names typically refer to thetype of the landmark.
With a few exceptions, for exam-ple the ?great viewpoint?
in figure 1, most of the land-mark names only occur once.
This seems to make it dif-ficult to use the MapTask dialogues from the perspectiveof GRE: the names/types rule out most distractors andthere is not much left to do for a GRE algorithm.
How-ever, as can be seen in figure 1, the routes do not leadthrough the landmarks but rather around them alongfeature-less points.
The subjects of the MapTask exper-iments therefore often refer to points on the route, forFigure 1: A ?giver?
map of the MapTask experimentsexample to those where the next turn had to be taken.These observations can be used to frame the genera-tion of spatial descriptions as a GRE task in which targetpoints on the map are distinguished from all other points(the distractors).
Since most points are feature-less, twoof the properties commonly used in GRE, types and at-tributes, cannot be used in many cases.
This leaves thethird property, relations, which can be used by a GREalgorithm to relate the target position to surroundinglandmarks on the maps.
This is also what the subjectsin the MapTask experiments are doing.Our current, on-going work addresses the generation ofdescriptions referring to individual points on the maps.Ultimately, we hope to move on to the generation of de-scriptions of (straight) paths encompassing start and endpoints, and a description of how to travel between these.Looking at the MapTask corpus from the perspective ofGRE, we make the following observations:?
The MapTask corpus consists of transcriptions ofspoken language and contains many disfluencies.
Aspatial description can even span more than oneturn, for example: ?okay ... fine move ... upwards... an?
... so that you?re to the left of the brokengate .?
TURN ?and just level to the gatepost .?
Weexpect more polished, written language as output ofour generator.?
GRE only deals with a subset of the corpus.
Weneed to find ways of making use of the appropriateparts of the data while ignoring the other ones.?
Spatial descriptions containing some form of vague-ness seem to be frequent:1 ?quite close to the moun-tain on its right-hand side?, ?just before the middleof the page?, ?a bit up from the fallen pillars on theleft?, ?about two inches southwest?.
There even seemto be rare cases of vague types: ?a sort of curve?.?
Discourse context is of crucial importance, i.e.
manyspatial descriptions do not mention a particularpoint for the first time.
Furthermore, already es-tablished information is not always given explicitly,for example ?two inches to the left [from where I amnow].??
Perspective is a related issue: it seems that the sub-jects zoom in on parts of the map, ignoring for ex-ample a second lake at the other end of the map.?
The subjects switch between referring to objectsfrom an inside-the-map perspective (?beneath thetree?)
to using the physical maps as a perspective(?a couple of centimeters ... from the bottom of thepage?).
The latter may be caused by the lack of agrid indicating the distances in miles or kilometers.In sum, the MapTask corpus contains a wealth of datacombined with a domain model (the maps).
The chal-lenge is to make best use of these resources.
In the fol-lowing sections, we report on work-in-progress on hybridrule-based/instance-based generation of spatial referringexpressions.3 Overgenerating spatial descriptionsFollowing the inferential approach to GRE [Varges, 2004;Varges and van Deemter, 2005], we are implementing asystem that finds all combinations of properties that aretrue of a given target referent and distinguish it fromits distractors.
The approach pairs the logical forms de-rived from a domain representation with the correspond-ing ?extensions?, the set of objects that the logical formdenotes.
We represent spatial domains as grids and do-main objects (landmarks) as sets of coordinates of thosegrids.
For example, the telephone kiosk in figure 1 isrepresented as the set {(2, 18), (2, 19)}.
The grid resolu-tion has implications for the definition of targets, whichare, in fact, target areas, i.e.
they often consist of morethan one coordinate.We implemented a number of content determinationrules that recursively build up descriptions, starting from(NPs realizing) the landmarks of the domain model:1. spatial functions: Prep NP ?
PP: ?above the westlake?.
The spatial functions used so far are: ?above?,?below?, ?to the left of?, ?to the right of?.2.
intersection: PP and PP ?
PP: ?above the westlake and to the left of the great viewpoint?.3.
union: NP or NP ?
NP: ?the stile or the ruinedmonastery?.1The following examples do not always refer to the mapshown in figure 1.+------------------+| | grid resolution: 17 x 24| x x | (last two rows omitted from grid)| x xxxxxx || x xxxxx || xxxxx || x x || x || || xx | This map is the ?extension?
of| xx x | ?the telephone kiosk or the great| x | viewpoint or the ruined monastry| xxxx | or the stone circle or the dead| xxxxxx | tree ...?| xxxxx || xxxxx || xxxx || x || || #x | #: target area of example| #x xx | (see section 4)| xx xxxxx || xxx xxxxx || |+------------------+Figure 2: Grid representation of map in fig.
1The descriptions generated by these rules are all associ-ated with an extension.
For example ?above X?
denotesall all the points above X (this may be changed to thosepoints that are also ?close?
to X).
The grid in figure 2is a graphical depiction of an extension associated withthe disjoined NPs shown on the right.
All generateddescriptions can be visualized in this way.The three content determination rules listed above arenot sufficient for singling out all areas of the maps.
Forexample, the corners of the maps are typically not ?reach-able?.
Therefore, we define a further rule:4. recursive spatial functions: Prep PP ?
PP: ?abovethe points to the left of the farmer?s gate?.We intend to generate a wide variety of realization can-didates.
For example, rule 4 could also produce the morefluent (and realistic) ?above the farmer?s gate slightly tothe left?, which will also require us to make vaguenessmore precise.
An alternative is to use non-recursive PPslike ?southeast of X?.4 Ranking spatial descriptionsAs candidates for ranking we use those NPs and PPs thatcontain at least one target coordinate, a loose definitionthat increases the number of candidates.
We always gen-erate NP ?the points?
as a candidate which refers to allcoordinates.
However, it should be ranked low becauseit does not rule out any distractors.
We currently usethe ratio of extension size to described target coordi-nates as our first (non-empirical) ranking criterion (?e/t?below).
The second (non-empirical) criterion is brevity,measured by the number of characters (?chars?).
Hereare some ranked output candidates for the target arealabeled ?#?
in figure 2 (which contains two points):e/t | chars | realization | extension size--------------------------------------------------2 34 to the left of the telephone kiosk 42 71 to the left of the farmer?s gateand to the left of the telephone kiosk 22 88 above the points to the left of thefarmer?s gate and to the left of thetelephone kiosk 210 32 to the left of the farmer?s gate 10225 10 the points 450The first candidate is preferred because it has the samee/t ratio as some of its competitors but in addition is alsoshorter than these.
In fact, this is how one of the subjectsrefers to the starting point in one of the dialogues.
Thethird candidate requires the use of appropriate bracket-ing to yield the desired reading.
For example, the gen-erator could introduce a comma after ?gate?.5 Toward using empirical data forrankingThe generation rules sketched above produce non-redundant spatial descriptions, i.e.
the generator is ?eco-nomical?
[Stone, 2003] and follows the ?local brevity?
in-terpretation of the Gricean Maxims [Dale and Reiter,1995].
The candidate of least ?complexity?
is the ?fullbrevity?
solution.
A word similarity-based ranker couldalign the generation output (i.e.
the highest-ranked can-didate) with previous utterances in the discourse con-text.
To increase choice, we intend to also generate ad-ditional candidates that include a limited amount of re-dundant information.
One could furthermore generatecandidates that, by themselves, do not rule out all dis-tractors.
In contrast to the inclusion of redundant in-formation, these candidates would only be safe to usein combination with, for example, a reliable model ofdiscourse salience that reduces the set of possible dis-tractors.It is possible (but not without difficulty) to annotateparts of the corpus with map coordinates.
For exam-ple, we can annotate the turn ?on the right side of thetree?
with coordinates (15,9), (15,10) in figure 2.
Furthermarkup could be applied to ?redundant?
information (inthe GRE sense) or highlight available discourse context.However, for obvious reasons it is preferable to use cor-pus data without any additional annotation for ranking.The maps enable us to determine how much we gain fromthe availability of a domain model.6 AcknowledgmentsOur thanks go to Kees van Deemter, Richard Power andAlbert Gatt for very helpful discussions on the topics ofthis paper.
The presented research has been conductedas part of the TUNA2 project funded by EPSRC (grantnumber GR/S13330/01).2Towards a UNified Algorithm for the Generation of Re-ferring Expressions.References[Anderson et al, 1991] A. Anderson, M. Bader,E.
Bard, E. Boyle, G. M. Doherty, S. Garrod, S. Isard,J.
Kowtko, J. McAllister, J. Miller, C. Sotillo, H. S.Thompson, and R. Weinert.
The HCRC Map TaskCorpus.
Language and Speech, 34:351?366, 1991.
[Bangalore and Rambow, 2000] Srinivas Bangalore andOwen Rambow.
Corpus-based Lexical Choice in Nat-ural Language Generation.
In Proc.
of ACL-00, 2000.
[Dale and Reiter, 1995] Robert Dale and Ehud Reiter.Computational Interpretations of the Gricean Maximsin the Generation of Referring Expressions.
CognitiveScience, 19:233?263, 1995.
[Karamanis et al, 2004] Nikiforos Karamanis, MassimoPoesio, Chris Mellish, and Jon Oberlander.
Evaluat-ing centering-based metrics of coherence for text struc-turing using a reliably annotated corpus.
In Proc.
ofACL-04, 2004.
[Langkilde and Knight, 1998] IreneLangkilde and Kevin Knight.
Generation that Ex-ploits Corpus-based Statistical Knowledge.
In Proc.of COLING/ACL-98, 1998.
[Langkilde, 2002] Irene Langkilde.
An Empirical Veri-fication of Coverage and Correctness for a General-Purpose Sentence Generator.
In Proc.
of INLG-02,2002.
[Poesio et al, 2004] Massimo Poesio, Rosemary Steven-son, Barbara di Eugenio, and Janet Hitzeman.
Center-ing: A parametric theory and its instantiations.
Com-putational Linguistics, 30(3), 2004.
[Ratnaparkhi, 2000] Adwait Ratnaparkhi.
TrainableMethods for Surface Natural Language Generation.In Proc.
of NAACL-00, 2000.
[Siddharthan and Copestake, 2004]Advaith Siddharthan and Ann Copestake.
Generat-ing referring expressions in open domains.
In Proc.
ofACL-04, 2004.
[Stone, 2003] Matthew Stone.
Specifying Generation ofReferring Expressions by Example.
In Proc.
of theAAAI Spring Symposium on Natural Language Gen-eration in Spoken and Written Dialogue, 2003.
[Varges and Mellish, 2001] Sebastian Varges and ChrisMellish.
Instance-based Natural Language Genera-tion.
In Proc.
NAACL-01, 2001.
[Varges and van Deemter, 2005] Sebastian Varges andKees van Deemter.
Generating referring expressionscontaining quantifiers.
In Proceedings of IWCS-6,2005.
[Varges, 2004] Sebastian Varges.
Overgenerating refer-ring expressions involving relations.
In Proc.
of INLG-04, 2004.
