Proceedings of NAACL-HLT 2013, pages 12?21,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsBeyond Left-to-Right: Multiple Decomposition Structures for SMTHui Zhang?USC/ISILos Angeles, CA 90089hzhang@isi.eduKristina ToutanovaMicrosoft ResearchRedmond, WA 98502kristout@microsoft.comChris QuirkMicrosoft ResearchRedmond, WA 98502chrisq@microsoft.comJianfeng GaoMicrosoft ResearchRedmond, WA 98502jfgao@microsoft.comAbstractStandard phrase-based translation models donot explicitly model context dependence be-tween translation units.
As a result, they relyon large phrase pairs and target language mod-els to recover contextual effects in translation.In this work, we explore n-gram models overMinimal Translation Units (MTUs) to explic-itly capture contextual dependencies acrossphrase boundaries in the channel model.
Asthere is no single best direction in which con-textual information should flow, we exploremultiple decomposition structures as well asdynamic bidirectional decomposition.
Theresulting models are evaluated in an intrin-sic task of lexical selection for MT as wellas a full MT system, through n-best rerank-ing.
These experiments demonstrate that ad-ditional contextual modeling does indeed ben-efit a phrase-based system and that the direc-tion of conditioning is important.
Integratingmultiple conditioning orders provides consis-tent benefit, and the most important directionsdiffer by language pair.1 IntroductionThe translation procedure of a classical phrase-based translation model (Koehn et al 2003) first di-vides the input sentence into a sequence of phrases,translates each phrase, explores reorderings of thesetranslations, and then scores the resulting candi-dates with a linear combination of models.
Conven-tional models include phrase-based channel modelsthat effectively model each phrase as a large uni-gram, reordering models, and target language mod-els.
Of these models, only the target language model?This research was conducted during the author?s internshipat Microsoft Research(and, to some weak extent, the lexicalized reorderingmodel) captures some lexical dependencies that spanphrase boundaries, though it is not able to model in-formation from the source side.
Larger phrases cap-ture more contextual dependencies within a phrase,but individual phrases are still translated almost in-dependently.To address this limitation, several researchershave proposed bilingual n-gram Markov models(Marino et al 2006) to capture contextual depen-dencies between phrase pairs.
Much of their workis limited by the requirement ?that the source andtarget side of a tuple of words are synchronized, i.e.that they occur in the same order in their respectivelanguages?
(Crego and Yvon, 2010).For language pairs with significant typological di-vergences, such as Chinese-English, it is quite dif-ficult to extract a synchronized sequence of units;in the limit, the smallest synchronized unit may bethe whole sentence.
Other approaches explore incor-poration into syntax-based MT systems or replacingthe phrasal translation system altogether.We investigate the addition of MTUs to a phrasaltranslation system to improve modeling of con-text and to provide more robust estimation of longphrases.
However, in a phrase-based system thereis no single synchronized traversal order; instead,we may consider the translation units in many pos-sible orders: left-to-right or right-to-left accordingto either the source or the target are natural choices.Alternatively we consider translating a particularlyunambiguous unit in the middle of the sentenceand building outwards from there.
We investigateboth consistent and dynamic decomposition ordersin several language pairs, looking at distinct ordersin isolation and combination.122 Related workMarino et al(2006) proposed a translation modelusing a Markov model of bilingual n-grams, demon-strating state-of-the-art performance compared toconventional phrase-based models.
Crego andYvon (2010) further explored factorized n-gram ap-proaches, though both models considered ratherlarge n-grams; this paper focuses on small units withasynchronous orders in source and target.
Durraniet al(2011) developed a joint model that capturestranslation of contiguous and gapped units as well asreordering.
Two prior approaches explored similarmodels in syntax based systems.
MTUs have beenused in dependency translation models (Quirk andMenezes, 2006) to augment syntax directed trans-lation systems.
Likewise in target language syntaxsystems, one can consider Markov models over min-imal rules, where the translation probability of eachrule is adjusted to include context information fromparent rules (Vaswani et al 2011).Most prior work tends to replace the existingprobabilities rather than augmenting them.
We be-lieve that Markov rules provide an additional sig-nal but are not a replacement.
Their distributionsshould be more informative than the so-called ?lex-ical weighting?
models, and less sparse than rela-tive frequency estimates, though potentially not aseffective for truly non-compositional units.
There-fore, we explore the inclusion of all such informa-tion.
Also, unlike prior work, we explore combina-tions of multiple decomposition orders, as well asdynamic decompositions.
The most useful contextfor translation differs by language pair, an importantfinding when working with many language pairs.We build upon a standard phrase-based approach(Koehn et al 2003).
This acts as a proposal dis-tribution for translations; the MTU Markov modelsprovide additional signal as to which translations arecorrect.3 MTU n-gram Markov modelsWe begin by defining Minimal Translation Units(MTUs) and describing how to identify them inword-aligned text.
Next we define n-gram Markovmodels over MTUs, which requires us to definetraversal orders over MTUs.
?Yu ?
?ZuoTian ?
?JuXingheld the meeting?
?HuiTanyesterdaynullnullM1 M2 M3 M5M4M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting?Yu ?
?ZuoTian ??JuXing?
?
???HuiTan?
?nullFigure 1: Word alignment and minimum translation units.3.1 Definition of an MTUInformally, the notion of a minimal translation unitis simple: it is a translation rule that cannot bebroken down any further without violating the con-straints of the rules.
We restrict ourselves to contigu-ous MTUs.
They are similar to small phrase pairs,though unlike phrase pairs we allow MTUs to haveeither an empty source or empty target side, therebyallowing insertion and deletion phrases.
Conven-tional phrase pairs may be viewed as compositionsof these MTUs up to a given size limit.Consider a word-aligned sentence pair consistingof a sequence of source words s = s1 .
.
.
sm, a se-quence of target words t = t1 .
.
.
tn, and a word align-ment relation between the source and target words?
?
{1..m} ?
{1..n}.
A translation unit is a sequenceof source words si..s j and a sequence of target wordstk..tl (one of which may be empty) such that for allaligned pairs i?
?
k?, we have i ?
i?
?
j if and onlyif k ?
k?
?
l. This definition, nearly identical tothat of a phrase pair (Koehn et al 2003), relaxes theconstraint that one aligned word must be present.A set of translation units is a partition of the sen-tence pair if each source and target word is coveredexactly once.
Minimal translation units is the par-tition with the smallest average unit size, or, equiv-alently, the largest number of units.
For example,Figure 1 shows a word-aligned sentence pair and itscorresponding set of MTUs.
We extract these min-imal translation units with an algorithm similar tothat of phrase extraction.We train n-gram Markov models only over min-13imal rules for two reasons.
First, the segmentationof the sentence pair is not unique under composedrules, which makes probability estimation compli-cated.
Second, some phrase pairs are very large,which results in sparse data issues and compromisesthe model quality.
Therefore, training an n-grammodel over minimal translation units turns out tobe a simple and clean choice: the resulting segmen-tation is unique, and the distribution is smooth.
Ifwe want to capture more context, we can simply in-crease the order of the Markov model.Such Markov models address issues in largephrase-based translation approaches.
Where stan-dard phrase-based models rely upon large unigramsto capture contextual information, n-grams of mini-mal translation units allow a robust contextual modelthat is less constrained by segmentation.3.2 MTU enumeration ordersWhen defining a joint probability distribution overMTUs of an aligned sentence pair, it is necessaryto define a decomposition, or generation order forthe sentence pair.
For a single sequence in lan-guage modeling or synchronized sequences in chan-nel modeling, the default enumeration order hasbeen left-to-right.Different decomposition orders have been usedin part-of-speech tagging and named entity recog-nition (Tsuruoka and Tsujii, 2005).
Intuitively, in-formation from the left or right could be more use-ful for particular disambiguation choices.
Our re-search on different decomposition orders was moti-vated by this work.
When applying such ideas tomachine translation, there are additional challengesand opportunities.
The task exhibits much more am-biguity ?
the number of possible MTUs is in themillions.
An opportunity arises from the reorderingphenomenon in machine translation: while in POStagging the natural decomposition orders to studyare only left-to-right and right-to-left, in machinetranslation we can further distinguish source and tar-get sentence orders.We first define the source left-to-right and the tar-get left-to-right orders of the aligned sets of MTUs.The definition is straightforward when there are noinserted or deleted word.
To place the nulls corre-sponding to such word we use the following defi-nition: the source position of the null for a targetinserted word is just after the position of the lastsource word aligned to the closest preceding non-null aligned target word.
The target position for anull corresponding to a source deleted MTU is de-fined analogously.
In Figure 1 we define the posi-tion of M4 to be right after M3 (because ?the?
isafter ?held?
in left-to-right order on the target side).The complete MTU sequence in source left-to-right order is M1-M2-M3-M4-M5.
The sequencein target left-to-right order is M3-M4-M5-M1-M2.This illustrates that decomposition structure maydiffer significantly depending on which language isused to define the enumeration order.Once a sentence pair is represented as a sequenceof MTUs, we can define the probability of thesentence pair using a conventional n-gram Markovmodel (MM) over MTUs.
For example, the 3-gramMM probability of the sentence pair in Figure 1under the source left-to-right order is as follows:P(M1)?P(M2|M1)?P(M3|M1,M2)?P(M4|M2,M3)?P(M5|M3,M4).Different decomposition orders use different con-text for disambiguation and it is not clear aprioriwhich would perform best.
We compare all fourdecomposition orders (source order left-to-right andright-to-left, and target order left-to-right and right-to-left).
Although the independence assumptions ofleft-to-right and right-to-left are the same, the result-ing models may be different due to smoothing.In addition to studying these four basic decompo-sition orders, we report performance of two cyclicorders: cyclic in source or target sentence order.These models are inspired by the cyclic depen-dency network model proposed for POS tagging(Toutanova et al 2003) and also used as a baselinein previous work on dynamic decomposition orders(Tsuruoka and Tsujii, 2005).
1The probability according to the cyclic orders isdefined by conditioning each MTU on both its leftand right neighbor MTUs.
For example, the prob-ability of the sentence pair in Figure 1 under thesource cyclic order, using a 3-gram model is definedas: P(M1|M2) ?
P(M2|M1,M3) ?
P(M3|M2,M4) ?P(M4|M3,M5) ?
P(M5|M4).All n-gram Markov models over MTUs are esti-1The correct application of such models requires samplingto find the highest scoring sequence, but we apply the max prod-uct approximation as done in previous work.14mated using Kneser-Ney smoothing.
Each MTU istreated as an atomic unit in the vocabulary of then-gram model.
Counts of all n-grams are obtainedfrom the parallel MT training data, using differentMTU enumeration orders.Note that if we use a target-order decomposition,the model provides a distribution over target sen-tences and the corresponding source sides of MTUs,albeit unordered.
Likewise source order based mod-els provide distributions over source sentences andunordered target sides of MTUs.
We attempted tointroduce reordering models to predict an order overthe resulting MTU sequences using approaches sim-ilar to reordering models for phrases.
Althoughthese models produced gains in some language pairswhen used without translation MTU MMs, therewere no additional gains over a model using mul-tiple translation MTU MMs.4 Lexical selectionWe perform an empirical evaluation of differentMTU decomposition orders on a simplified machinetranslation task: lexical selection.
In this task weassume that the source sentence segmentation intominimal translation units is given and that the or-der of the corresponding target sides of the minimaltranslation units is also given.
The problem is topredict the target sides of the MTUs, called targetMTUs for brevity (see Figure 2).
The lexical selec-tion task is thus similar to sequence tagging taskslike part-of-speech tagging, though much more dif-ficult: the predicted variables are sequences of targetlanguage words with millions of possible outcomes.
?Yu ?
?ZuoTian ?
?JuXingheld the meet ng?
?HuiTanyesterdaynullnullM1 M2 M3 M5M4M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting?Yu ?
?ZuoTian ??JuXing?
?
???HuiTan?
?nullFigure 2: Lexical selection.We use this constrained MT setting to evaluate theperformance of models using different MTU decom-position orders and models using combinations ofdecomposition orders.
The simplified setting allowscontrolled experimentation while lessening the im-pact of complicating factors in a full machine trans-lation setting (search error, reordering limits, phrasetable pruning, interaction with other models).To perform the tagging task, we use trigram MTUmodels.
The four basic decomposition orders forMTU Markov models we use are left-to-right in tar-get sentence order, right-to-left in target sentence or-der, left-to-right in source sentence order, and right-to-left in source sentence order.
We also considercyclic orders in source and target.Regardless of the decomposition order used, weperform decoding using a beam search decoder, sim-ilar to ones used in phrase-based machine transla-tion.
The decoder builds target hypotheses in left-to-right target sentence order.
At each step, it fills inthe translation of the next source MTU, in the con-text of the already predicted MTUs to its left.
Thetop scoring complete hypotheses covering the first mMTUs are maintained in a beam.
When scoring witha target left-to-right MTU Markov model (L2RT),we can score each partial hypothesis exactly at eachstep.
When scoring using a R2LT model or a sourceorder model, we use lower-order approximations tothe trigram MTU Markov model scores as futurescores, since not all needed context is available for ahypothesis at the time of construction.
As additionalcontext becomes available, the exact score can becomputed.
24.1 Basic decomposition order combinationsWe first introduce two methods of combining differ-ent decomposition orders: product and system com-bination.The product method arises naturally in the ma-chine translation setting, where probabilities fromdifferent models are multiplied together and furtherweighted to form the log-linear model for machinetranslation (Och and Ney, 2002).
We define a similarscoring function using a set of MTU Markov modelsMM1, ...,MMk for a hypothesis h as follows:Score(h) = ?1logPMM1(h) + ... + ?klogPMMk (h)2We apply hypothesis recombination, which can merge hy-potheses that are indistinguishable with respect to future contin-uations.
This is similar to recombination in a standard-phrasebased decoder with the difference that it is not always the lasttwo target MTUs that define the context needed by future ex-tensions.15The weights ?
of different models are trained on adevelopment set using MER training to maximizethe BLEU score of the resulting model.
Note thatthis method of model combination was not consid-ered in any of the previous works comparing differ-ent decompositions.The system combination method is motivatedby prior work in machine translation which com-bined left-to-right and right-to-left machine trans-lation systems (Finch and Sumita, 2009).
Simi-larly, we perform sentence-level system combina-tion between systems using different MTU Markovmodels to come up with most likely translations.If we have k systems guessing hypotheses basedon MM1, .
.
.
,MMk respectively, we generate 1000-best lists from each system, resulting in a pool ofup to 1000k possible distinct translations.
Each ofthe candidate hypotheses from MMi is scored withits Markov model log-probability logPMMi(h).
Wecompute normalized probabilities for each system?sn-best by exponentiating and normalizing: Pi(h) ?PMMi(h).
If a hypothesis h is not in system i?s n-best list, we assume its probability is zero accordingto that system.
The final scoring function for eachhypothesis in the combined list of candidates is:Score(h) = ?1P1(h) + ... + ?kPk(h)The weights ?
for the combination are tuned usingMERT as for the product model.4.2 Dynamic decomposition ordersA more complex combination method chooses thebest possible decomposition order for each transla-tion dynamically, using a set of constraints to de-fine the possible decomposition orders, and a set offeatures to score the candidate decompositions.
Weterm this method dynamic combination.
The scoreof each translation is defined as its score accordingto the highest-scoring decomposition order for thattranslation.This method is very similar to the bidirectionaltagging approach of Tsuruoka and Tsujii (2005).For this approach we only explored combinations oftarget language orders (L2RT, CycT, and R2LT).
Ifsource language orders were included, the complex-ity of decoding would increase substantially.Figure 3 shows two possible decompositions fora short MTU sequence.
The structures displayed are1 121  2| 132  3| 2,	 142  4| 3,	 2 1  2| 1,	 3 1  3| 4  4Figure 3: Different decompositions.directed graphical models.
They define the set ofparents (context) used to predict each target MTU.The decomposition structures we consider are lim-ited to acyclic graphs where each node can have oneof the following parent configurations: no parents(C = 0 in the Figure), one left parent (C = 1L),one right parent (C = 1R), one left and one rightparent (C = LR), two left parents (C = 2L), andtwo right parents (C = 2R).
If all nodes have twoleft parents, we recover the left-to-right decomposi-tion order, and if all nodes have two right parents,the right-to-left decomposition order.
A mixture ofparent configurations defines a mixed, dynamic de-composition order.
The decomposition order chosenvaries from translation to translation.A directed graphical model defines the probabilityof an assignment of MTUs to the variable nodes as aproduct of local probabilities of MTUs given theirparents.
Here we extend this definition to scoresof assignments by using a linear model with con-figuration features and log-probability features.
Theconfiguration features are indicators of which par-ent configuration is active at a node and the settingsof these features for the decompositions in Figure3 are shown as assignments to the C variables.
Thelog-probability feature values are obtained by query-ing the appropriate n-gram model: L2RT, CycT, orR2LT.
For a node with one or two left parents, thelog-probability is computed according to the L2RTmodel.
For a node with one or two right parents, theR2LT model is queried.
The CycT model is used fornodes with one left and one right parent.To find the best translation of a sentence themodel now searches over hidden decomposition or-16ders in addition to assignments to target MTUs.
Thefinal score of a translation and decomposition is alinear combination of the two types of feature values?
model log-probabilities and configuration types.There is one feature weight for each parent con-figuration (six configuration weights) and one fea-ture weight for each component model (three modelweights).
The final score of the second decomposi-tion and assignment in Figure 3 is:Score(h)= 2 ?
wC0 + wCLR + wC1R+ wL2RlogPLR(m1) + wCyclogPCyc(m2|m1,m3)+ wR2LlogPRL(m3|m4) + wL2RlogPLR(m4)There are two main differences between our ap-proach and that of Tsuruoka and Tsujii (2005): weperform beam search with hypothesis recombinationinstead of exact decoding (due to the larger size ofthe hypothesis set), and we use parameters to beable to globally weight the probabilities from dif-ferent models and to develop preferences for usingcertain types of decompositions.
For example, themodel can learn to prefer right-to-left decomposi-tions for one language pair, and left-to-right decom-positions for another.
An additional difference fromprior work is the definition of the possible decompo-sition orders that are searched over.Compared to the structures allowed in (Tsuruokaand Tsujii, 2005) for a trigram baseline model, ourallowed structures are a subset; in (Tsuruoka andTsujii, 2005) there are sixteen possible parent con-figurations (up to two left and two right parents),whereas we allow only six.
We train and use onlythree n-gram Markov models to assign probabilities:L2RT, R2LT, and CycT, whereas the prior work usedsixteen models.
One could potentially see additionalgains from considering a larger space of structuresbut the training time and runtime memory require-ments might become prohibitive for the machinetranslation task.Because of the maximization over decompositionstructures, the score of a translation is not a simplelinear function of the features, but rather a maximumover linear functions.
The score of a translation fora fixed decomposition is a linear function of the fea-tures, but the score of a translation is a maximum oflinear functions (over decompositions).
Therefore,if we define hypotheses as just containing transla-tions, MERT training does not work directly for op-timizing the weights of the dynamic combinationmethod.
3 We used a combination of approaches;we did MERT training followed by local simplex-method search starting from three starting points:the MERT solution, a weight vector that stronglyprefers left-to-right decompositions, and a weight-vector that strongly prefers right-to-left decomposi-tions.
In the Experiments section, we report resultsfor the weights that achieved the best developmentset performance.5 N-best rerankingTo evaluate the impact of these models in a full MTsystem, we investigate n-best reranking.
We use aphrase-based MT system to output 1000-best can-didate translations.
For each candidate translation,we have access to the phrase pairs it used as well asthe alignments inside each phrase pair.
Thus, eachsource sentence and its candidate translation form aword-aligned parallel sentence pair.
We can extractMTU sequences from this sentence pair and com-pute its probability according to MTU Markov mod-els.
These MTU MM log-probabilities are appendedto the original MT features and used to rerank the1000-best list.
The weight vectors for systems usingthe original features along with one or more MTUMarkov model log-probabilities are trained on thedevelopment set using MERT.6 ExperimentsWe report experimental results on the lexical selec-tion task and the reranking task on three languagepairs.
The datasets used for the different languagesare described in detail in Section 6.2.6.1 Lexical selection experimentsThe data used for the lexical selection experimentsconsists of the training portion of the datasets usedfor MT.
These training sets are split into three sec-tions: lex-train, for training MTU Markov modelsand extracting possible translations for each source3If we include the decompositions in the hypotheses wecould use MERT but then the n-best lists used for training mightnot contain much variety in terms of translation options.
This isan interesting direction for future research.17Model Chs-En Deu-En En-BgrDev Test Dev Test Dev TestBaseline 06.45 06.30 11.60 10.98 15.09 14.40Oracle 69.79 70.78 72.28 75.39 85.15 84.32L2RT 24.02 25.09 28.69 28.70 49.86 46.45R2LT 23.79 24.91 30.14 30.14* 49.22 46.58CycT 18.59 20.33 25.91 26.83 41.30 38.85L2RS 25.81 27.89* 25.52 25.10 45.69 43.98R2LS 26.48 27.96* 26.03 26.30 47.36 43.91CycS 21.62 23.38 22.68 23.58 39.11 36.44Table 1: Lexical selection results for individual MTUMarkov models.MTU, lex-dev for tuning combination weights forsystems using several MTU MMs, and lex-test, forfinal evaluation results.
The possible translations foreach source MTU are defined as the most frequent100 translations seen in lex-train.
The lex-dev setscontain 200 sentence pairs each and the lex-test setscontains 1000 sentence pairs each.
These develop-ment and test sets consist of equally spaced sen-tences taken from the full MT training sets.We start by reporting BLEU scores of the six in-dividual MTU MMs on the three language pairs inTable 1.
The baseline predicts the most frequent tar-get MTU for each source MTU (unigram MM notusing context).
The oracle looks at the correct trans-lation and always chooses the correct target MTU ifit is in the vocabulary of available MTUs.We can see that there is a large difference betweenthe baseline and oracle performance, underscoringthe importance of modeling context for accurate pre-diction.
The best decomposition order varies fromlanguage to language: right-to-left in source order isbest for Chinese-English, right-to-left in target orderis best for German-English and left-to-right or right-to-left in target order are best in English-Bulgarian.We computed statistical significance tests, testingthe difference between the L2RT model (the stan-dard in prior work) and models achieving higher testset performance.
The models that are significantlybetter at significance ?
< 0.01 are marked with astar in the table.
We used a paired bootstrap test with10,000 trials (Koehn, 2004).Next we evaluate the methods for combining de-composition orders introduced in Sections 4.1 and4.2.
The results are reported in Table 2.
The up-per part of the table focuses on combining differentModel Chs-En Deu-En En-BgrDev Test Dev Test Dev TestBaseline-1 24.04 25.09 30.14 30.14 49.86 46.45TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53Table 2: Lexical selection results for combinations ofMTU Markov models.target-order decompositions.
The lower part of thetable looks at combining all six decomposition or-ders.
The baseline for the target order combinations,Baseline-1, is the best single target MTU Markovmodel from Table 1.
Baseline-2 in the lower partof the table is the best individual model out of allsix.
We can see that the product models TgtProduct(a product of the three target-order MTU MMs) andAllProduct (a product of all six MTU MMs) are con-sistently best.
The dynamic decomposition modelsTgtDynamic achieve slight but not significant gainsover the baseline.
The combination models that arestatistically significantly better than correspondingbaselines (?
< 0.01) are marked with a star.Our takeaway from these experiments is that mul-tiple decomposition orders are good, and thus takinga product (which encourages agreement among themodels) is a good choice for this task.
The dynamicdecomposition method shows some promise, but itdoes not outperform the simpler product approach.Perhaps a lager space of decompositions wouldachieve better results, especially given a larger pa-rameter set to trade off decompositions and bettertuning for those parameters.6.2 Datasets and reranking settingsFor Chinese-English, the training corpus consistsof 1 million sentence pairs from the FBIS andHongKong portions of the LDC data for the NISTMT evaluation.
We used the union of the NIST2002 and 2003 test sets as the development set andthe NIST 2005 test set as our test set.
The baselinephrasal system uses a 5-gram language model withmodified Kneser-Ney smoothing (Kenser and Ney,1995), trained on the Xinhua portion of the EnglishGigaword corpus (238M English words).For German-English we used the dataset from18Language Training Dev TestChs-En 1 Mln NIST02+03 NIST05Deu-En 751 K WMT06dev WMT06testEn-Bgr 4 Mln 1,497 2,498Table 3: Data sets for different language pairs.the WMT 2006 shared task on machine translation(Koehn and Monz, 2006).
The parallel training setcontains approximately 751K sentences.
We alsoused the English monolingual data of around 1 mil-lion sentences for language model training.
The de-velopment set contains 2000 sentences.
The finaltest set (the in-domain test set for the shared task)also contains 2000 sentences.
Two Kneser-Ney lan-guage models were used as separate features: a 4-gram LM trained on the parallel portion of the data,and a 5-gram LM trained on the monolingual corpus.For English-Bulgarian we used a dataset con-taining sentences from several data sources: JRC-Acquis (Steinberger et al 2006), TAUS4, and web-scraped data.
The development set consists of 1,497sentences, the English side from WMT 2009 newstest data, and the Bulgarian side a human translationthereof.
The test set comes from the same mixture ofsources as the training set.
For this system we useda single four-gram target language model trained onthe target side of the parallel corpus.All systems used phrase tables with a maximumlength of seven words on either side and lexicalizedreordering models.
For the Chinese-English sys-tem we used GIZA++ alignments, and for the othertwo we used alignments by an HMM model aug-mented with word-based distortion (He, 2007).
Thealignments were symmetrized and then combinedwith the heuristics ?grow-diag-final-and?.
5 We tuneparameters using MERT (Och, 2003) with randomrestarts (Moore and Quirk, 2008) on the develop-ment set.
Case-insensitive BLEU-4 is our evaluationmetric (Papineni et al 2002).3-gram models 5-gram modelsModel Dev Test Dev TestBaseline 32.58 31.78 32.58 31.78L2RT 33.05 32.78* 33.16 32.88*R2LT 33.05 32.96* 33.16 32.81*L2RS 32.90 33.00* 32.98 32.98*R2LS 32.94 32.98* 33.09 32.96*4 MMs 33.22 33.07* 33.37 33.00*4 MMs phrs 32.58 31.78 32.58 31.78Table 4: Reranking with 3-gram and 5-gram MTU trans-lation models on Chinese-English.
Starred results on thetest set indicate significantly better performance than thebaseline.6.3 MT reranking experimentsWe first report detailed experiments on Chinese-English, and then verify our main conclusions on theother language pairs.
Table 4 looks at the impact ofindividual 3-gram and 5-gram MTU Markov modelsand their combination.
Amongst the decompositionorders tested (L2RT, R2LT, L2RS, and R2LS), eachof the individual MTU MMs was able to achievesignificant improvement over the baseline, around 1BLEU point.6 The results achieved by the individ-ual models differ, and the combination of four direc-tions is better than the best individual direction, butthe difference is not statistically significant.We ran an additional experiment to test whetherMTU MMs make effective use of context acrossphrase boundaries, or whether they simply pro-vide better smoothed estimates of phrasal transla-tion probabilities.
The last row of the table reportsthe results achieved by a combination of MTU MMsthat do not use context across the phrasal bound-aries.
Since an MTU MM limited to look only insidephrases can provide improved smoothing comparedto whole phrase relative frequency counts, it is con-ceivable it could provide a large improvement.
How-ever, there is no improvement in practice for this lan-guage pair; the additional improvements from MTUMMs stem from modeling cross-phrase context.4www.tausdata.org5The combination heuristic was further refined to disallowcrossing one-to-many alignments, which would result in the ex-traction of larger minimum translation units.
We found that thisfurther refinement on the combination heuristic consistently im-proved the BLEU scores by between 0.3 and 0.7.6Here again we call a difference significant if the pairedbootstrap p-value is less than 0.01.19Table 5 shows the test set results of individ-ual 3-gram MTU Markov models and the com-bination of 3-gram and 5-gram models on theEnglish-Bulgarian and German-English datasets.For English-Bulgarian all individual 3-gram Markovmodels achieve significant improvements of close toone point; their combination is better than the bestindividual model (but not significantly).
The indi-vidual 5-gram models and their combination bringmuch larger improvement, for a total increase of2.82 points over the baseline.
We believe the 5-gram models were more effective in this setting be-cause the larger training set alwed for successfultraining of models of larger capacity.
Also the in-creased context size helps to resolve ambiguity inthe forms of morphologically-rich Bulgarian words.For German-English we see a similar pattern, withthe combination of models outperforming the in-dividual ones, and the 5-gram models being betterthan the 3-gram.
Here the individual 3-gram modelsare better than the baseline at significance level 0.02and their combination is better than the baseline atour earlier defined threshold of 0.01.
The within-phrase MTU MMs (results shown in the last tworows) improve upon the baseline slightly, but hereagain the improvements mostly stem from the use ofcontext across phrase boundaries.
Our final resultson German-English are better than the best result of27.30 from the shared task (Koehn and Monz, 2006).Thanks to the reviewers for referring us to re-cent work by (Clark et al 2011) that pointed outproblems with significance tests for machine trans-lation, where the randomness and local optima in theMERT weight tuning method lead to a large vari-ance in development and test set performance acrossdifferent runs of optimization (using a different ran-dom seed or starting point).
(Clark et al 2011) pro-posed a stratified approximate randomization statis-tical significance test, which controls for optimizerinstability.
Using this test, for the English-Bulgariansystem, we confirmed that the combination of four3-gram MMs and the combination of 5-gram MMsis better than the baseline (p = .0001 for both, usingfive runs of parameter tuning).
We have not run thetest for the other language pairs.Model En-Bgr Deu-EnBaseline 45.75 27.92L2RT 3-gram 47.07* 28.15R2LT 3-gram 47.06* 28.19L2RS 3-gram 46.44* 28.15R2LS 3-gram 47.04* 28.184 3-gram 47.17* 28.37*4 5-gram 48.57* 28.47*4 3-gram phrs 46.08 27.924 5-gram phrs 46.17* 27.93Table 5: English-Bulgarian and German-English test setresults: reranking with MTU translation models.7 ConclusionsWe introduced models of Minimal Translation Unitsfor phrasal systems, and showed that they make asubstantial and statistically significant improvementon three distinct language-pairs.
Additionally westudied the importance of decomposition order whendefining the probability of MTU sequences.
In asimplified lexical selection task, we saw that therewere large differences in performance among thedifferent decompositions, with the best decomposi-tions differing by language.
We investigated multi-ple methods to combine decompositions and foundthat a simple product approach was most effective.Results in the lexical selection task were consistentwith those obtained in a full MT system, althoughthe differences among decompositions were smaller.In future work, perhaps we would see larger gainsby including additional decomposition orders (e.g.,top-down in a dependency tree), and taking this ideadeeper into the machine translation model, down tothe word-alignment and language-modeling levels.We were surprised to find n-best reranking so ef-fective.
We are incorporating the models into firstpass decoding, in hopes of even greater gains.ReferencesJonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proc.
ACL-11.JM Crego and F Yvon.
2010.
Factored bilingual n-gram language models for statistical machine transla-tion.
Machine Translation, Special Issue: Pushing thefrontiers of SMT, 24(2):159?175.20Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with inte-grated reordering.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.
Association forComputational Linguistics.Andrew Finch and Eiichiro Sumita.
2009.
Bidirectionalphrase-based machine translation.
In In proceedingsof EMNLP.Xiaodong He.
2007.
Using word-dependent transitionmodels in hmm based word alignment for statisticalmachine translation.
In WMT workshop.Reinhard Kenser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Proc.ICASSP 1995, pages 181?184.Philipp Koehn and Christof Monz.
2006.
Manual and au-tomatic evaluation of machine translation between eu-ropean languages.
In Proceedings on the Workshop onStatistical Machine Translation, pages 102?121, June.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003, pages 127?133.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In In Proceedings ofEMNLP.JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-bert, JA Fonollosa, and MR Costa-Jussa.
2006.
N-gram-based machine translation.
Computational Lin-guistics, 32(4):527?549.Robert C. Moore and Chris Quirk.
2008.
Randomrestarts in minimum error training for statistical ma-chine translation.
In Proc.
Coling-08.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In In Proceedings of ACL,pages 295?302.Franz Joseph Och.
2003.
Minimum error training in sta-tistical machine translation.
In Proc.
ACL-03.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
40th AnnualMeeting of the ACL, pages 311?318.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
challenging the conventional wisdom in sta-tistical machine translation.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,Main Conference, pages 9?16, New York City, USA,June.
Association for Computational Linguistics.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma Erjavec, Dan Tufis, and DnielVarga.
2006.
The JRC-Acquis: A multilingualaligned parallel corpus with 20+ languages.
In LREC,Genoa, Italy.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In In Pro-ceedings of HLT-NAACL.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategyfor tagging sequence data.
In In proceedings ofHLT/EMNLP.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule markov models for fast tree-to-string translation.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 856?864,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.21
