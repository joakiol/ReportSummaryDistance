The CIPS-SIGHAN CLP 2010Chinese Word Segmentation BakeoffHongmei Zhao and Qun LiuKey Laboratory of Intelligent Information Processing,Institute of Computing Technology, Chinese Academy of Sciences{zhaohongmei,liuqun}@ict.ac.cnAbstractThe CIPS-SIGHAN CLP 2010 ChineseWord Segmentation Bakeoff was heldin the summer of 2010 to evaluate thecurrent state of the art in wordsegmentation.
It focused on the cross-domain performance of Chinese wordsegmentation algorithms.
Eighteengroups submitted 128 results over twotracks (open training and closedtraining), four domains (literature,computer science, medicine and finance)and two subtasks (simplified Chineseand traditional Chinese).
We found thatcompared with the previous Chineseword segmentation bakeoffs, theperformance of cross-domain Chineseword segmentation is not much lower,and the out-of-vocabulary recall isimproved.1 IntroductionChinese is written without inter-word spaces, sofinding word-boundaries is an essential firststep in many natural language processing tasksranging from part of speech tagging to parsing,reference resolution and machine translation.SIGHAN, the Special Interest Group forChinese Language Processing of theAssociation for Computational Linguistics,successfully conducted four prior wordsegmentation bakeoffs, in 2003 (Sproat andEmerson, 2003), 2005 (Emerson, 2005), 2006(Levow, 2006) and  2007 (Jin and Chen, 2007),and the bakeoff 2007 was jointly organized withthe Chinese Information Processing Society ofChina (CIPS).
These evaluations establishedbenchmarks for word segmentation with whichresearchers evaluate their segmentation system.After years of intensive researches, Chineseword segmentation has achieved a quite highprecision, though the out-of-vocabularyproblem is still a continuing challenge.However, the performance of segmentation isnot so satisfying for out-of-domain text.The CIPS-SIGHAN CLP 2010 ChineseWord Segmentation Bakeoff continues theongoing series of the SIGHAN Chinese WordSegmentation Bakeoff.
It was organized byInstitute of Computing Technology, ChineseAcademy of Sciences (abbreviated as ICTbelow).
It focused on the cross-domainperformance of Chinese word segmentationalgorithms.
And the bakeoff results will bereported in conjunction with the First CIPS-SIGHAN joint conference on ChineseLanguage Processing, Beijing, China.2 Details of the Evaluation2.1 CorporaThere are two kinds of corpora in the evaluation,with one using the simplified Chinesecharacters and another using the traditionalChinese characters.
For the simplified Chinesecorpora, the test corpora, reference corpora, andthe unlabeled training corpora were provided byICT, and the labeled training corpus (1 monthdata of The People's Daily in 1998) wasprovided by Peking University.
For thetraditional Chinese corpora, all the training, testand reference corpora were provided by theHongkong City University.There are four domains in this evaluation.Before the releasing of the test data, two ofthem (literature and computer science, weabbreviate ?computer science?
to ?computer?below) are known to the participants (weprovided the corresponding unlabeled trainingcorpora Characters Tokens Word Types TTR OOV RateSimplifiedChineseTestLiterature 50,637 35,736 6,364 0.18 0.069Computer 53,382 35,319 4,150 0.12 0.152Medicine 50,969 31,490 5,076 0.16 0.11Finance 53,253 33,028 4,918 0.15 0.087TrainingLabeled 1,820,456 1,109,947 55,303 0.05Unlabeled-L 100,352Unlabeled-C 103,764TraditionalChineseTestLiterature 54,357 36,378 8,141 0.22 0.094Computer 67,321 43,499 6,197 0.14 0.094Medicine 68,090 43,458 6,510 0.15 0.075Finance 74,461 47,144 6,652 0.14 0.068TrainingLabeled 1,863,298 1,146,988 63,588 0.06Unlabeled-L 105,653Unlabeled-C 109,303Table1.
Overall corpus statisticsSite ID Site Name Contact Simplified ChineseTraditionalChineseS1 College of Computer and Information Engineering, Anyang Normal University, Henan province, China Jiangde Yu ??
?
?S2 Institute of Intelligent Information Processing, Beijing Information Science & Technology University Wenjie Su ?S3 Beijing Institute of Technology Huaping Zhang ?S4 Center for Language Information Processing Institute?Beijing Language and Culture UniversityZhiyongLuo ?S5 Beijing University of Posts and Telecommunications Caixia Yuan ?
?S6 Dalian University of Technology Huiwei Zhou ?
?S7 Fudan University Xipeng Qiu ?
?S8 Shenzhen Graduate School Harbin Institute of TechnologyJianpingShen ?
?S9 Language Technologies Institute, Carnegie Mellon University Qin Gao ?
?S10 National Central University, Taiwan Yu-Chieh Wu ?
?S11 Natural Language Processing Lab, Northeastern University, ChinaHuizhenWang ?S12 National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science Kun Wang ?S13 Institute of Computer Science and Technology, Peking UniversityLiangZong ?S14 Institute of Computational Linguistics, Peking University Mairgup ?S15 Queensland University of Technology Eric Tang ??
?
?S16 Institute of Information Science, Academia Sinica, TaiwanCheng-Lung Sung ?
?S17 Natural Language Processing Lab, Suzhou University Junhui Li ?S18 Anhui Speech and language Technology Engineering Research CenterZhigangChen ?
?Table 2.
Participating groups (?=closed track, ?=open track, there are four domains on every track)corpora for each during the training phrase),and another two domains (medicine and finance)are unknown to the participants (without any in-domain training corpora).
All corpora are UTF-8 encoded.
Details on each corpus are providedin Table 1.
We introduce a type-token ratio(TTR) to indicate the vocabulary diversity ineach corpus.During the process of building the referencecorpora for the simplified Chinese wordsegmentation subtask, we manually check theautomatically segmented results of the test dataagainst the standard provided in ?TheSpecification for the Basic Processing ofContemporary Chinese Corpus from PekingUniversity?.
In this process, we refer to thelabeled training data frequently with a view tokeep the annotation consistency between thesetwo kinds of corpora.
Furthermore, we made acomparison test which compared thesegmentation of the same character stringspresent in both corpora automatically, andcorrected the inconsistent cases.
However, inthe labeled training corpus, there are minorincorrect segmentation cases against thestandard from Peking University, such as ???
?
(yin2 jia1, with the meaning of ?thewinner?, this word should be regarded as aword according to the above-mentionedstandard), and there are also a few interiorinconsistent cases in this corpus, such as ????
and ??
??
(huan4 you3, with themeaning of ?suffer from?).
Whenever thesegmentation of the reference corpora wasdifferent from the above-mentioned incorrect orinconsistent segmentation in the training corpus,we followed the standard from PekingUniversity.
All the evaluation corpora can beaccessible from the Chinese Linguistic DataConsortium at:  http://www.chineseldc.org.2.2 Rules and ProceduresThis bakeoff followed a strict set of guidelinesand a rigid timetable.
The detailed instructionsfor the bakeoff can be found athttp://www.cipsc.org.cn/clp2010/cfpa.htm.
Thetraining material of simplified Chinese wordsegmentation was available starting April 1, thetraining material of traditional Chinese wordsegmentation was available April 23, testingmaterial was available June 9, and the resultshad to be returned to the organizer by email byJune 11 no later than 18:00 Beijing time.The participating groups (?sites?)
of CIPS-SIGHAN CLP 2010 Bakeoff registered byemail.
There are two subtasks in this evaluation:word segmentation for simplified Chinese textand word segmentation for traditional Chinesetext.
The participating sites were required todeclare which subtask they would participate in.The open and closed tracks were defined asfollows:z For the closed training evaluation,participants can only use data provided bythe organizer to train their systems.Specifically, the following data resourcesand software tools are not permitted to beused in the training:1.
Unspecified corpus;2.
Unspecified dictionary, word list orcharacter list: include the dictionaries ofnamed entity, character lists for specifictype of Chinese named entities, idiomdictionaries, semantic lexicons, etc.;3.
Human-encoded rule bases;4.
Unspecified software tools, includeword segmenters, part-of-speechtaggers, or parsers which are trainedusing unspecified data resources.The character type information todistinguish the following four charactertypes can be used in training: Chinesecharacters, English letters, digits andpunctuations.z In the Open training evaluation,participants can use any languageresources, including the training dataprovided by the organizer.Participants were asked to submit their datausing specific naming conventions, and fromthe result file name we can see in which trackthe result was run, as well as other necessaryinformation.
Of course, the results on bothtracks are welcomed.Scoring was done automatically using acombination of Perl and shell scripts.
Thescripts (Sproat and Emerson, 2003, 2005) usedfor scoring can be downloaded fromhttp://www.sighan.org/bakeoff2005/.
Thebakeoff organizer provided an on-line scoringsystem to all the participants who had submittedtheir bakeoff results for their follow-upexperiments.2.3 Participating sitesEighteen sites submitted results and a technicalreport.
Mainland China had the greatest numberwith 14, followed by Taiwan (2), the UnitedStates (1) and Australia (1).
A summary ofparticipating groups and the tracks for whichthey submitted results can be found in Table 2on the preceding page.
There are more siteswho had registered for the bakeoff.
However,several of them withdrew due to technicaldifficulties or other problems.
Altogether 128runs were submitted for scoring.3 Results3.1 Baseline and topline experimentsFollowing previous bakeoffs, to provide abasis for comparison, we computed baselineand topline scores for each of the corpora.When computing a baseline, we compiled adictionary of all the words in the labeledtraining corpus, and then we used thisdictionary with a simple left-to-right maximalmatch algorithm to segment the test corpus.
Theresults of this experiment are shown in Table 3.We expect systems to do at least as well as thebaseline.
The topline employed the sameprocedure, but instead used the dictionary of allthe words in the test corpus.
These results arepresented in Table 4.
We expect systems togenerally underperform this topline, because noone could exactly know the set of words thatoccur in the test corpus.In these and subsequent tables, we list theword count for the test corpus, test recall (R),test precision (P), balanced F score (where F =2PR/(P+R)), the out-of-vocabulary (OOV) rateon the test corpus, the recall on OOV words(Roov), and the recall on in-vocabulary words(Riv).3.2 Raw scoresAll the results are presented in Tables 5-20.Column headings are as above, except for ?Cr?and ?Cp?
for which see Section 3.3.
All tablesare sorted by F score.3.3 Statistical significance of the resultsFollowing previous bakeoffs, let us assume thatthe recall rates represent the probability p that aword will be successfully identified, and let usfurther assume the binomial distribution isappropriate for this experiment.
Given theCentral Limit Theorem for Bernouli trials ?e.g.
(Grinstead and Snell, 1997), then the 95%confidence interval is given as 2 (1 ) /p p n?
?
,where n is the number of trials (words).
Therecall-based confidences ( 2 (1 ) /p p n?
? )
aregiven as ?Cr?
in Tables 5-20.
Similarly, we canassume the precision rates represent theprobability that a character string that has beenidentified as a word is really a word.
And theprecision-based confidences are given as ?Cp?in the tables.
They can be interpreted as follows:To decide whether two systems are significantlydifferent (at the 95% confidential level), onejust has to compute whether their confidenceintervals overlap.
If at least one of the ?Cr?
and?Cp?
are different, we can treat these twosystems as significantly different (at the 95%confidential level).
Using this criterion allsystems in this bakeoff are significantlydifferent from each other.4 Discussion4.1 Comparison between open and closedtracksIn this bakeoff, there are 8 systems that ran onboth closed and open tracks, which result in 32pairs of scores for F measure and OOV recallrespectively.
Table 21 shows the results of thesesystems.
We can see that their scores of Fmeasures on open track don?t have advantageover their counterparts on the closed track: only14 scores (in 32 scores) on open track arehigher than their counterparts on the closedtrack.
This is different from the previousbakeoffs.
But for OOV recall, the case isdifferent.
There are 23 scores (in 32 scores) onopen track are higher than their counterparts onthe closed track.4.2 Improved OOV recall over the priorbakeoffsFrom all the results, we can see that the widestvariation among systems lies in the OOV recallrate.
And dealing with unknown words is stillthe most difficult problem of Chinese wordsegmentation.However, while comparing the top OOVrecall rates of this bakeoff with those of theprior four bakeoffs, we found the OOV recallrates of this bakeoff achieved an obviousimprovement.
Table 22 shows the comparisons.We managed to find four pairs of test corporawith similar OOV rates for comparisons.
In thecomparisons, most top OOV recall rates ofbakeoff 2010 are much higher than theircounterparts of prior bakeoffs.
An exceptioncomes from the open track of medicine domainfor traditional CWS subtask, and because only 3systems submitted results, this comparisonseems less meaningful.4.3 Performance under different domainsWe listed the top performance by F measure onevery track, domain and subtask on Table 23.Generally we think that cross-domain wordsegmentation will lead to a lower performancethan in-domain word segmentation.
In thisbakeoff, it seems that the best performance ofcross-domain word segmentation is at almostthe same level of that of the prior bakeoffs.
Weknow that the performance of different test setis incomparable.
However, the performance inthe out-of-domain text is somewhat surprisingto us.
We guess one reason may be the usage ofdomain adaptive technology, another reasonmay be the new technologies used by theparticipants.
We hope to see the exact reasonsin the technological reports of participants inthe coming conference.We provided unlabeled data to two domains.However, we did not see significant differenceon the performance of closed test between thesedomains and other domains.
Some participantspointed out that it is because the size of theunlabeled data is rather small.And we found that among four domains,the performance (by the value of F measure andOOV recall, with scores in bold in the table) onfinance is always the best or very close to thebest.
Perhaps this is because the OOV rate onfinance test corpus is rather low.Corpus  Word Count R P F OOV Roov RivSimplifedChineseL 35736 0.917 0.862 0.889 0.069 0.156 0.973C 35319 0.856 0.632 0.727 0.152 0.163 0.98M 31490 0.886 0.774 0.826 0.11 0.123 0.981F 33028 0.914 0.803 0.855 0.087 0.233 0.979TraditionalChineseL 36378 0.863 0.788 0.824 0.094 0.041 0.948C 43499 0.873 0.701 0.778 0.094 0.01 0.963M 43458 0.886 0.81 0.846 0.075 0.027 0.955F 47144 0.888 0.826 0.855 0.068 0.006 0.952Table 3.
Baseline scores: Results for maximum match with training vocabulary (L=literature, C=computer,M=medicine, F=finance)Corpus  Word Count R P F OOV Roov RivSimplifedChineseL 35736 0.986 0.99 0.988 0.069 0.996 0.985C 35319 0.991 0.993 0.992 0.152 0.99 0.991M 31490 0.989 0.991 0.99 0.11 0.98 0.99F 33028 0.994 0.995 0.994 0.087 0.995 0.994TraditionalChineseL 36378 0.981 0.988 0.985 0.094 0.998 0.979C 43499 0.988 0.991 0.99 0.094 0.996 0.987M 43458 0.984 0.989 0.986 0.075 0.992 0.983F 47144 0.981 0.986 0.984 0.068 0.997 0.98Table 4.
Topline scores: Results for maximum match with testing vocabulary (L=literature, C=computer,M=medicine, F=finance)Site ID Word Count R Cr P Cp F OOV Roov RivS5 35736 0.945 ?0.00241 0.946 ?0.00239 0.946 0.069 0.816 0.954S6 35736 0.94 ?0.00251 0.942 ?0.00247 0.941 0.069 0.649 0.961S12 35736 0.937 ?0.00257 0.937 ?0.00257 0.937 0.069 0.652 0.958S10 35736 0.936 ?0.00259 0.932 ?0.00266 0.934 0.069 0.564 0.964S11 35736 0.931 ?0.00268 0.936 ?0.00259 0.934 0.069 0.648 0.952S18 35736 0.932 ?0.00266 0.935 ?0.00261 0.933 0.069 0.654 0.953S14 35736 0.925 ?0.00279 0.931 ?0.00268 0.928 0.069 0.667 0.944S9 35736 0.92 ?0.00287 0.925 ?0.00279 0.923 0.069 0.625 0.942S7 35736 0.915 ?0.00295 0.925 ?0.00279 0.92 0.069 0.577 0.94S13 35736 0.916 ?0.00293 0.922 ?0.00284 0.919 0.069 0.613 0.939S16 35736 0.917 ?0.00292 0.921 ?0.00285 0.919 0.069 0.699 0.933S1 35736 0.908 ?0.00306 0.918 ?0.00290 0.913 0.069 0.556 0.935S17 35736 0.909 ?0.00304 0.903 ?0.00313 0.906 0.069 0.707 0.924S15 35736 0.907 ?0.00307 0.862 ?0.00365 0.884 0.069 0.206 0.959S2 35736 0.695 ?0.00487 0.744 ?0.00462 0.719 0.069 0.381 0.719Table 5.
Simplified Chinese: Literature -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS6 35736 0.958 ?0.00212 0.953 ?0.00224 0.955 0.069 0.655 0.981S3 35736 0.965 ?0.00194 0.94 ?0.00251 0.952 0.069 0.814 0.976S18 35736 0.942 ?0.00247 0.943 ?0.00245 0.942 0.069 0.702 0.959S9 35736 0.939 ?0.00253 0.943 ?0.00245 0.941 0.069 0.699 0.957S1 35736 0.908 ?0.00306 0.916 ?0.00293 0.912 0.069 0.535 0.936S5 35736 0.893 ?0.00327 0.918 ?0.00290 0.905 0.069 0.803 0.899S4 35736 0.897 ?0.00322 0.907 ?0.00307 0.902 0.069 0.688 0.913S15 35736 0.869 ?0.00357 0.873 ?0.00352 0.871 0.069 0.657 0.885S8 35736 0.836 ?0.00392 0.841 ?0.00387 0.838 0.069 0.609 0.853Table 6.
Simplified Chinese: Literature --Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS6 35319 0.953 ?0.00225 0.95 ?0.00232 0.951 0.152 0.827 0.975S11 35319 0.948 ?0.00236 0.945 ?0.00243 0.947 0.152 0.853 0.965S12 35319 0.941 ?0.00251 0.94 ?0.00253 0.94 0.152 0.757 0.974S9 35319 0.938 ?0.00257 0.936 ?0.00260 0.937 0.152 0.805 0.962S13 35319 0.939 ?0.00255 0.934 ?0.00264 0.937 0.152 0.81 0.962S18 35319 0.935 ?0.00262 0.934 ?0.00264 0.935 0.152 0.792 0.961S5 35319 0.946 ?0.00241 0.914 ?0.00298 0.93 0.152 0.808 0.971S14 35319 0.941 ?0.00251 0.916 ?0.00295 0.928 0.152 0.796 0.967S7 35319 0.934 ?0.00264 0.919 ?0.00290 0.926 0.152 0.739 0.969S10 35319 0.915 ?0.00297 0.915 ?0.00297 0.915 0.152 0.594 0.972S17 35319 0.921 ?0.00287 0.9 ?0.00319 0.91 0.152 0.748 0.952S1 35319 0.89 ?0.00333 0.908 ?0.00308 0.899 0.152 0.592 0.943S15 35319 0.876 ?0.00351 0.844 ?0.00386 0.86 0.152 0.457 0.951S16 35319 0.876 ?0.00351 0.799 ?0.00426 0.836 0.152 0.456 0.952S2 35319 0.713 ?0.00481 0.641 ?0.00511 0.675 0.152 0.257 0.795Table 7.
Simplified Chinese: Computer -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS9 35319 0.95 ?0.00232 0.95 ?0.00232 0.95 0.152 0.82 0.973S18 35319 0.948 ?0.00236 0.946 ?0.00241 0.947 0.152 0.812 0.973S6 35319 0.948 ?0.00236 0.929 ?0.00273 0.939 0.152 0.735 0.986S3 35319 0.951 ?0.00230 0.926 ?0.00279 0.938 0.152 0.775 0.982S8 35319 0.951 ?0.00230 0.915 ?0.00297 0.932 0.152 0.77 0.983S5 35319 0.918 ?0.00292 0.896 ?0.00325 0.907 0.152 0.771 0.945S1 35319 0.893 ?0.00329 0.908 ?0.00308 0.9 0.152 0.607 0.944S4 35319 0.892 ?0.00330 0.88 ?0.00346 0.886 0.152 0.791 0.91S15 35319 0.859 ?0.00370 0.878 ?0.00348 0.868 0.152 0.668 0.893Table 8.
Simplified Chinese: Computer -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS6 31490 0.942 ?0.00263 0.936 ?0.00276 0.939 0.11 0.75 0.965S18 31490 0.937 ?0.00274 0.934 ?0.00280 0.936 0.11 0.761 0.959S5 31490 0.94 ?0.00268 0.928 ?0.00291 0.934 0.11 0.761 0.962S7 31490 0.927 ?0.00293 0.924 ?0.00299 0.925 0.11 0.714 0.953S10 31490 0.933 ?0.00282 0.915 ?0.00314 0.924 0.11 0.642 0.969S11 31490 0.924 ?0.00299 0.922 ?0.00302 0.923 0.11 0.756 0.944S12 31490 0.93 ?0.00288 0.917 ?0.00311 0.923 0.11 0.674 0.961S14 31490 0.928 ?0.00291 0.918 ?0.00309 0.923 0.11 0.73 0.953S9 31490 0.923 ?0.00300 0.917 ?0.00311 0.92 0.11 0.729 0.947S13 31490 0.917 ?0.00311 0.911 ?0.00321 0.914 0.11 0.699 0.944S1 31490 0.902 ?0.00335 0.907 ?0.00327 0.904 0.11 0.633 0.935S16 31490 0.9 ?0.00338 0.896 ?0.00344 0.898 0.11 0.596 0.937S17 31490 0.894 ?0.00347 0.873 ?0.00375 0.884 0.11 0.647 0.925S15 31490 0.885 ?0.00360 0.804 ?0.00447 0.842 0.11 0.218 0.967S2 31490 0.735 ?0.00497 0.74 ?0.00494 0.738 0.11 0.378 0.779Table 9.
Simplified Chinese: Medicine -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS9 31490 0.94 ?0.00268 0.936 ?0.00276 0.938 0.11 0.768 0.962S18 31490 0.941 ?0.00266 0.935 ?0.00278 0.938 0.11 0.787 0.96S6 31490 0.951 ?0.00243 0.92 ?0.00306 0.935 0.11 0.67 0.986S3 31490 0.953 ?0.00239 0.913 ?0.00318 0.933 0.11 0.704 0.984S5 31490 0.917 ?0.00311 0.907 ?0.00327 0.912 0.11 0.704 0.943S4 31490 0.91 ?0.00323 0.901 ?0.00337 0.906 0.11 0.725 0.933S1 31490 0.904 ?0.00332 0.906 ?0.00329 0.905 0.11 0.635 0.937S15 31490 0.865 ?0.00385 0.846 ?0.00407 0.855 0.11 0.559 0.903S8 31490 0.839 ?0.00414 0.832 ?0.00421 0.836 0.11 0.618 0.866Table 10.
Simplified Chinese: Medicine -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS6 33028 0.959 ?0.00218 0.96 ?0.00216 0.959 0.087 0.827 0.972S12 33028 0.957 ?0.00223 0.956 ?0.00226 0.957 0.087 0.813 0.971S9 33028 0.956 ?0.00226 0.955 ?0.00228 0.956 0.087 0.857 0.965S11 33028 0.953 ?0.00233 0.956 ?0.00226 0.955 0.087 0.871 0.961S18 33028 0.955 ?0.00228 0.956 ?0.00226 0.955 0.087 0.848 0.965S5 33028 0.956 ?0.00226 0.952 ?0.00235 0.954 0.087 0.849 0.966S10 33028 0.945 ?0.00251 0.941 ?0.00259 0.943 0.087 0.666 0.972S7 33028 0.94 ?0.00261 0.942 ?0.00257 0.941 0.087 0.719 0.961S13 33028 0.943 ?0.00255 0.94 ?0.00261 0.941 0.087 0.773 0.959S14 33028 0.948 ?0.00244 0.928 ?0.00284 0.937 0.087 0.761 0.965S1 33028 0.925 ?0.00290 0.938 ?0.00265 0.931 0.087 0.664 0.95S17 33028 0.935 ?0.00271 0.915 ?0.00307 0.925 0.087 0.736 0.954S16 33028 0.91 ?0.00315 0.906 ?0.00321 0.908 0.087 0.562 0.943S15 33028 0.904 ?0.00324 0.865 ?0.00376 0.884 0.087 0.321 0.96S2 33028 0.736 ?0.00485 0.752 ?0.00475 0.744 0.087 0.23 0.784Table 11.
Simplified Chinese: Finance -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS9 33028 0.96 ?0.00216 0.96 ?0.00216 0.96 0.087 0.847 0.971S6 33028 0.964 ?0.00205 0.95 ?0.00240 0.957 0.087 0.763 0.983S18 33028 0.948 ?0.00244 0.955 ?0.00228 0.951 0.087 0.853 0.957S3 33028 0.963 ?0.00208 0.938 ?0.00265 0.95 0.087 0.758 0.982S1 33028 0.925 ?0.00290 0.937 ?0.00267 0.931 0.087 0.669 0.95S5 33028 0.928 ?0.00284 0.934 ?0.00273 0.931 0.087 0.808 0.939S8 33028 0.893 ?0.00340 0.896 ?0.00336 0.894 0.087 0.796 0.902S4 33028 0.885 ?0.00351 0.893 ?0.00340 0.889 0.087 0.757 0.897S15 33028 0.853 ?0.00390 0.85 ?0.00393 0.851 0.087 0.438 0.893Table 12.
Simplified Chinese: Finance -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS10 36378 0.942 ?0.00245 0.942 ?0.00245 0.942 0.094 0.788 0.958S1 36378 0.888 ?0.00331 0.905 ?0.00307 0.896 0.094 0.728 0.904S7 36378 0.869 ?0.00354 0.91 ?0.00300 0.889 0.094 0.698 0.887S16 36378 0.871 ?0.00351 0.891 ?0.00327 0.881 0.094 0.67 0.891S15 36378 0.864 ?0.00359 0.789 ?0.00428 0.825 0.094 0.105 0.943Table 13.
Traditional Chinese: Literature -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS1 36378 0.905 ?0.00307 0.9 ?0.00315 0.902 0.094 0.775 0.918S8 36378 0.868 ?0.00355 0.802 ?0.00418 0.834 0.094 0.503 0.905S15 36378 0.804 ?0.00416 0.722 ?0.00470 0.761 0.094 0.234 0.863Table 14.
Traditional Chinese: Literature -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS10 43499 0.948 ?0.00213 0.957 ?0.00195 0.952 0.094 0.666 0.977S7 43499 0.933 ?0.00240 0.949 ?0.00211 0.941 0.094 0.791 0.948S1 43499 0.908 ?0.00277 0.931 ?0.00243 0.919 0.094 0.684 0.931S16 43499 0.913 ?0.00270 0.917 ?0.00265 0.915 0.094 0.663 0.939S15 43499 0.868 ?0.00325 0.85 ?0.00342 0.859 0.094 0.316 0.926Table 15.
Traditional Chinese: Computer -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS1 43499 0.911 ?0.00273 0.924 ?0.00254 0.918 0.094 0.698 0.933S8 43499 0.875 ?0.00317 0.829 ?0.00361 0.851 0.094 0.594 0.904S15 43499 0.789 ?0.00391 0.736 ?0.00423 0.761 0.094 0.35 0.834Table 16.
Traditional Chinese: Computer -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS10 43458 0.953 ?0.00203 0.957 ?0.00195 0.955 0.075 0.798 0.966S7 43458 0.908 ?0.00277 0.932 ?0.00242 0.92 0.075 0.771 0.919S1 43458 0.905 ?0.00281 0.924 ?0.00254 0.914 0.075 0.725 0.919S16 43458 0.9 ?0.00288 0.915 ?0.00268 0.908 0.075 0.668 0.919S15 43458 0.871 ?0.00322 0.815 ?0.00373 0.842 0.075 0.115 0.932Table 17.
Traditional Chinese: Medicine -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS1 43458 0.903 ?0.00284 0.903 ?0.00284 0.903 0.075 0.729 0.917S8 43458 0.879 ?0.00313 0.814 ?0.00373 0.846 0.075 0.48 0.912S15 43458 0.811 ?0.00376 0.74 ?0.00421 0.774 0.075 0.254 0.856Table 18.
Traditional Chinese: Medicine -- Open (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS10 47144 0.964 ?0.00172 0.962 ?0.00176 0.963 0.068 0.812 0.975S7 47144 0.925 ?0.00243 0.939 ?0.00220 0.932 0.068 0.793 0.935S16 47144 0.922 ?0.00247 0.929 ?0.00237 0.925 0.068 0.732 0.935S1 47144 0.891 ?0.00287 0.912 ?0.00261 0.901 0.068 0.676 0.907S15 47144 0.875 ?0.00305 0.834 ?0.00343 0.854 0.068 0.169 0.926Table 19.
Traditional Chinese: Finance -- Closed (italics indicate performance below baseline)Site ID Word Count R Cr P Cp F OOV Roov RivS1 47144 0.903 ?0.00273 0.916 ?0.00256 0.91 0.068 0.721 0.916S8 47144 0.832 ?0.00344 0.76 ?0.00393 0.794 0.068 0.356 0.866S15 47144 0.811 ?0.00361 0.753 ?0.00397 0.781 0.068 0.235 0.853Table 20.
Traditional Chinese: Finance -- Open (italics indicate performance below baseline)Subtask Site ID TrackLiterature Computer Medicine FinanceF Roov F Roov F Roov F RoovSimplifiedChineseS1?
0.913 0.556 0.899 0.592 0.904 0.633 0.931 0.664?
0.912 0.535 0.9 0.607 0.905 0.635 0.931 0.669S5?
0.946 0.816 0.93 0.808 0.934 0.761 0.954 0.849?
0.905 0.803 0.907 0.771 0.912 0.704 0.931 0.808S6?
0.941 0.649 0.951 0.827 0.939 0.75 0.959 0.827?
0.955 0.655 0.939 0.735 0.935 0.67 0.957 0.763S9?
0.923 0.625 0.937 0.805 0.92 0.729 0.956 0.857?
0.941 0.699 0.95 0.82 0.938 0.768 0.96 0.847S15?
0.884 0.206 0.86 0.457 0.842 0.218 0.884 0.321?
0.871 0.657 0.868 0.668 0.855 0.559 0.851 0.438S18?
0.933 0.654 0.935 0.792 0.936 0.761 0.955 0.848?
0.942 0.702 0.947 0.812 0.938 0.787 0.951 0.853TraditionalChineseS1?
0.896 0.728 0.919 0.684 0.914 0.725 0.901 0.676?
0.902 0.775 0.918 0.698 0.903 0.729 0.91 0.721S15?
0.825 0.105 0.859 0.316 0.842 0.115 0.854 0.169?
0.761 0.234 0.761 0.35 0.774 0.254 0.781 0.235Table 21.
Comparison: closed track vs. open track (?=closed track, ?=open track)bakeoff corpus characters OOV ratewordcountclosed track open trackRoov F Roov F2007 CKIP traditionalChinese0.074 90678 0.740 0.947 0.780 0.9562010 medicine 0.075 43458 0.798 0.955 0.729 0.9032006 UPUC simplifiedChinese0.088 155K 0.707 0.933 0.768 0.9442010 finance 0.087 33028 0.871 0.955 0.853 0.9512005 CityU traditionalChinese0.074 41K 0.736 0.941 0.806 0.9622010 medicine 0.075 43458 0.798 0.955 0.729 0.9032003 PK simplifiedChinese0.069 17K 0.763 0.940 0.799 0.9592010 literature 0.069 35736 0.816 0.946 0.814 0.952Table 22.
Comparisons of top OOV recall rates of different bakeoffs on the test corpora with similar OOVrates (2003, 2005, 2006 and 2007 represent the SIGHAN bakeoff 2003, 2005, 2006 and 2007 respectively, and2010 represents the CIPS-SIGHAN CLP 2010 bakeoff)Closed Track Open TrackOOV ID R P F Roov Riv ID R P F Roov RivSL 0.069 S5 0.945 0.946 0.946 0.816 0.954 S6 0.958 0.953 0.955 0.655 0.981C 0.152 S6 0.953 0.95 0.951 0.827 0.975 S9 0.95 0.95 0.95 0.82 0.973M 0.11 S6 0.942 0.936 0.939 0.75 0.965 S9 0.94 0.936 0.938 0.768 0.962S18 0.941 0.935 0.938 0.787 0.96F 0.087 S6 0.959 0.96 0.959 0.827 0.972 S9 0.96 0.96 0.96 0.847 0.971TL 0.094 S10 0.942 0.942 0.942 0.788 0.958 S1 0.905 0.9 0.902 0.775 0.918C 0.094 S10 0.948 0.957 0.952 0.666 0.977 S1 0.911 0.924 0.918 0.698 0.933M 0.075 S10 0.953 0.957 0.955 0.798 0.966 S1 0.903 0.903 0.903 0.729 0.917F 0.068 S10 0.964 0.962 0.963 0.812 0.975 S1 0.903 0.916 0.91 0.721 0.916Table 23.
Top performance on every subtask, domain, and track (S=simplified Chinese test, T=traditionalChinese test, L=literature, C=computer, M=medicine, F=finance)5 Conclusions & Future DirectionsThe CIPS-SIGHAN CLP 2010 Chinese WordSegmentation Bakeoff successfully broughttogether a collection of 18 strong researchgroups to assess the progress of thisfundamental research in Chinese languageprocessing.There is clearly no single best system.
Andthe participating sites S1, S10, S9, S6, S5 andS18 have all achieved respectable scores ondifferent track runs of this bakeoff.
Animprovement on the OOV recall over the priorbakeoffs has been observed.It is the first time to apply wordsegmentation bakeoff on four domains.
It?s alsothe first time to use unlabeled training corporain the bakeoff to test the unsupervised or semi-supervised learning ability of the segmentationsystem.
Unsupervised or Semi-supervisedlearning needs to incorporate large amounts ofunlabeled data.
We design the evaluation withtwo unknown domains without any in-domaintraining corpora, compared with two knowndomains each with an in-domain unlabeledtraining corpus.
Although no significantdifference has been found, it?s still worth it.
Thesize of our unlabeled training corpora was toosmall in this bakeoff, and we hope to improvethis in next evaluation.The word segmentation is a necessary pre-processing phase for the downstream processingtasks.
In future evaluations, we hope to see theintegration of word segmentation task with ahigher level task such as machine translation,with a view to exactly evaluate the impact ofimprovements in word segmentation on broaderdownstream applications.AcknowledgementThis work is supported by the National NaturalScience Foundation of China (Grant No.90920004).
We gratefully acknowledge thegenerous assistance of the organizations listedbelow who provided the data and the Chineseword segmentation standard for this bakeoff;without their support, it could not have takenplace:z City University of Hong Kongz Institute for Computational Linguistics,Beijing University, Beijing, ChinaWe thank Le Sun for his organization of theFirst CIPS-SIGHAN Joint Conference onChinese Language Processing of which thisbakeoff is part.
We thank Lili Zhao for hersegmentation-inconsistency-checking programand other preparation works for this bakeoff,and thank Guanghui Luo and Siyang Cao forthe online scoring system they set up andmaintained.
We thank Yajuan L?
for her helpfulsuggestions on this paper.
Finally we thank allthe participants for their interest and hard workin making this bakeoff a success.ReferencesCharles Grinstead and J. Laurie Snell.
1997.Introduction to Probability.
AmericanMathematical Society, Providence, RI.Gina-Anne Levow.
2006.
The thirdinternational Chinese language processingbakeoff: Word segmentation and namedentity recognition.
In Proceedings of the FifthSIGHAN Workshop on Chinese LanguageProcessing, pages 108?117, Sydney,Australia, July.
Association forComputational Linguistics.Guangjin Jin and Xiao Chen.
2007.
The FourthInternational Chinese Language ProcessingBakeoff: Chinese Word Segmentation,Named Entity Recognition and Chinese POSTagging.
In the Sixth SIGHAN Workshopon Chinese Language Processing, pages 69-81, Hyderabad, India.Richard Sproat and Thomas Emerson.
2003.The first international Chinese wordsegmentation bakeoff.
In The SecondSIGHAN Workshop on Chinese LanguageProcessing, pages 133?143, Sapporo, Japan.Thomas Emerson.
2005.
The secondinternational Chinese word segmentationbakeoff.
In Proceedings of the FourthSIGHAN Workshop on Chinese LanguageProcessing, pages 123?133, Jeju Island,Korea.
