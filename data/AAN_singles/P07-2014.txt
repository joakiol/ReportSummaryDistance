Proceedings of the ACL 2007 Demo and Poster Sessions, pages 53?56,Prague, June 2007. c?2007 Association for Computational LinguisticsDeriving an Ambiguous Word?s Part-of-Speech Distributionfrom Unannotated TextReinhard RappUniversitat Rovira i VirgiliPl.
Imperial Tarraco, 1E-43005 Tarragona, Spainreinhard.rapp@urv.catAbstractA distributional method for part-of-speechinduction is presented which, in contrastto most previous work, determines thepart-of-speech distribution of syntacti-cally ambiguous words without explicitlytagging the underlying text corpus.
This isachieved by assuming that the word pairconsisting of the left and right neighbor ofa particular token is characteristic of thepart of speech at this position, and byclustering the neighbor pairs on the basisof their middle words as observed in alarge corpus.
The results obtained in thisway are evaluated by comparing them tothe part-of-speech distributions as foundin the manually tagged Brown corpus.1 IntroductionThe purpose of this study is to automatically in-duce a system of word classes that is in agreementwith human intuition, and then to assign all possi-ble parts of speech to a given ambiguous or unam-biguous word.
Two of the pioneering studies con-cerning this as yet not satisfactorily solved prob-lem are Finch (1993) and Sch?tze (1993) who clas-sify words according to their context vectors as de-rived from a corpus.
More recent studies try tosolve the problem of POS induction by combiningdistributional and morphological information (Clark,2003; Freitag, 2004), or by clustering words andprojecting them to POS vectors (Rapp, 2005).Whereas all these studies are based on globalco-occurrence vectors who reflect the overall be-havior of a word in a corpus, i.e.
who in the case ofsyntactically ambiguous words are based on POS-mixtures, in this paper we raise the question if it isreally necessary to use an approach based on mix-tures or if there is some way to avoid the mixingbeforehand.
For this purpose, we suggest to look atlocal contexts instead of global co-occurrence vec-tors.
As can be seen from human performance, inalmost all cases the local context of a syntacticallyambiguous word is sufficient to disambiguate itspart of speech.The core assumption underlying our approach,which in the context of cognition and child lan-guage has been proposed by Mintz (2003), is thatwords of a particular part of speech often have thesame left and right neighbors, i.e.
a pair of suchneighbors can be considered to be characteristic ofa part of speech.
For example, a noun may be sur-rounded by the pair ?the ...
is?, a verb by the pair?he ...
the?, and an adjective by the pair ?the ...thing?.
For ease of reference, in the remainder ofthis paper we call these local contexts neighborpairs.
The idea is now to cluster the neighbor pairson the basis of the middle words they occur with.This way neighbor pairs typical of the same part ofspeech are grouped together.
For classification, aword is assigned to the cluster where its neighborpairs are found.
If its neighbor pairs are spreadover several clusters, the word can be assumed tobe ambiguous.
This way ambiguity detection fol-lows naturally from the methodology.2 ApproachLet us illustrate our approach by looking at Table 1.The rows in the table are the neighbor pairs that wewant to consider, and the columns are suitablemiddle words as we find them in a corpus.
Mostwords in our example are syntactically unambigu-ous.
Only link can be either a noun or a verb andtherefore shows the co-occurrence patterns of both.Apart from the particular choice of features, whatdistinguishes our approach from most others is thatwe do not cluster the words (columns) whichwould be the more straightforward thing to do.
In-stead we cluster the neighbor pairs (rows).
Clus-tering the columns would be fine for unambiguouswords, but has the drawback that ambiguous words53tend to be assigned only to the cluster relating totheir dominant part of speech.
This means that noambiguity detection takes place at this stage.In contrast, the problem of demixing can be av-oided by clustering the rows which leads to thecondensed representation as shown in Table 2.
Theneighbor pairs have been grouped in such a waythat the resulting clusters correspond to classes thatcan be linguistically interpreted as nouns, adjec-tives, and verbs.
As desired, all unambiguous wordshave been assigned to only a single cluster, and theambiguous word link has been assigned to the twoappropriate clusters.Although it is not obvious from our example,there is a drawback of this approach.
The disad-vantage is that by avoiding the ambiguity problemfor words we introduce it for the neighbor pairs,i.e.
ambiguities concerning neighbor pairs are notresolved.
Consider, for example, the neighbor pair?then ...
comes?, where the middle word can eitherbe a personal pronoun like he or a proper noun likeJohn.
However, we believe that this is a problemthat for several reasons is of less importance:Firstly, we are not explicitly interested in the am-biguities of neighbor pairs.
Secondly, the ambigui-ties of neighbor pairs seem less frequent and lesssystematic than those of words (an example is theomnipresent noun/verb ambiguity in English), andtherefore the risk of misclusterings is lower.Thirdly, this problem can be reduced by consider-ing longer contexts which tend to be less ambigu-ous.
That is, by choosing an appropriate contextwidth a reasonable tradeoff between data sparse-ness and ambiguity reduction can be chosen.car cup discuss link quick seek tall thina ... has    a ... is    a ... man        a ... woman        the ... has    the ... is    the ... man        the ... woman        to ... a      to ... the      you ... a      you ... the      Table 1: Matrix of neighbor pairs and their corresponding middle words.car cup discuss link quick seek tall thina ... has, a ... is,the ... has, the ... is    a ... man, a ... woman,the ... man, the ... woman        to ... a, to ... the, you ... a,you ... the      Table 2: Clusters of neighbor pairs.3 ImplementationOur computations are based on the 100 millionword British National Corpus.
As the number ofword types and neighbor pairs is prohibitivelyhigh in a corpus of this size, we considered only aselected vocabulary, as described in section 4.From all neighbor pairs we chose the top 2000which had the highest co-occurrence frequencywith the union of all words in the vocabulary anddid not contain punctuation marks.By searching through the full corpus, we constructeda matrix as exemplified in Table 1.
However, as a largecorpus may contain errors and idiosyncrasies, the ma-trix cells were not filled with binary yes/no decisions,but with the frequency of a word type occurring as themiddle word of the respective neighbor pair.
Note thatwe used raw co-occurrence frequencies and did notapply any association measure.
However, to accountfor the large variation in word frequency and to give anequal chance to each word in the subsequent com-putations, the matrix columns were normalized.54As our method for grouping the rows we usedK-means clustering with the cosine coefficient asour similarity measure.
The clustering algorithmwas started using random initialization.
In order tobe able to easily compare the clustering resultswith expectation, the number of clusters was spe-cified to correspond to the number of expectedword classes.After the clustering has been completed, to ob-tain their centroids, in analogy to Table 2 the col-umn vectors for each cluster are summed up.
Thecentroid values for each word can now be inter-preted as evidence of this word belonging to theclass described by the respective cluster.
For ex-ample, if we obtained three clusters correspondingto nouns, verbs, and adjectives, and if the corre-sponding centroid values for e.g.
the word linkwould be 0.7, 0.3, and 0.0, this could be inter-preted such that in 70% of its corpus occurrenceslink has the function of a noun, in 30% of thecases it appears as a verb, and that it never occursas an adjective.
Note that the centroid values for aparticular word will always add up to 1 since, asmentioned above, the column vectors have beennormalized beforehand.As elaborated in Rapp (2007), another usefulapplication of the centroid vectors is that they al-low us to judge the quality of the neighbor pairswith respect to their selectivity regarding a parti-cular word class.
If the row vector of a neighborpair is very similar to the centroid of its cluster,then it can be assumed that this neighbor pair onlyaccepts middle words of the correct class, whereasneighbor pairs with lower similarity to the cen-troid are probably less selective, i.e.
they occa-sionally allow for words from other clusters.4 ResultsAs our test vocabulary we chose a sample of 50words taken from a previous study (Rapp, 2005).The list of words is included in Table 3 (columns1 and 8).
Columns 2 to 4 and 9 to 11 of Table 3show the centroid values corresponding to eachword after the procedure described in the previoussection has been conducted, that is, the 2000 mostfrequent neighbor pairs of the 50 words were clus-tered into three groups.
For clarity, all values weremultiplied by 1000 and rounded.To facilitate reference, instead of naming eachcluster by a number or by specifying the corre-sponding list of neighbor pairs (as done in Table 2), wemanually selected linguistically motivated names, namelynoun, verb, and adjective.If we look at Table 3, we find that some words, suchas encourage, imagine, and option, have one valueclose to 1000, with the other two values in the onedigit range.
This is a typical pattern for unambiguouswords that belong to only one word class.
However,perhaps unexpectedly, the majority of words has val-ues in the upper two digit or three digit range in two oreven three columns.
This means that according to oursystem most words seem to be ambiguous in one oranother way.
For example, the word brief, although inthe majority of cases clearly an adjective in the senseof short, can occasionally also occur as a noun (in thesense of document) or a verb (in the sense of to instructsomebody).
In other cases, the occurrences of differentparts of speech are more balanced.
An example is theverb to strike versus the noun the strike.According to our judgment, the results for all wordsseem roughly plausible.
Only the values for rain as anoun versus a verb seemed on first glance counterintui-tive, but can be explained by the fact that for semanticreasons the verb rain usually only occurs in third per-son singular, i.e.
in its inflected form rains.To provide a more objective measure for the qualityof the results, columns 5 to 7 and 12 to 14 of Table 3show the occurrence frequencies of the 50 words asnouns, verbs, and adjectives in the manually POS-tagged Brown corpus, which is probably almost errorfree (Ku?era, & Francis, 1967).
The respective tags inthe Brown-tagset are NN, VB, and JJ.Generally, the POS-distributions of the Brown cor-pus show a similar pattern as the automatically gener-ated ones.
For example, for drop the ratios of theautomatically generated numbers 334 / 643 / 24 aresimilar to those of the pattern from the Brown corpuswhich is 24 / 34 / 1.
Overall, for 48 of the 50 words theoutcome with regard to the most likely POS is identi-cal, with the two exceptions being the ambiguouswords finance and suit.
Although even in these casesthe correct two parts of speech obtain the emphasis, thedistribution of the weighting among them is somewhatdifferent.5 Summary and Future WorkA statistical approach has been presented which clus-ters contextual features (neighbor pairs) as observed ina large text corpus and derives syntactically orientedword classes from the clusters.
In addition, for each55word a probability of its occurrence as a memberof each of the classes is computed.Of course, many questions are yet to be ex-plored, among them the following: Can a singularvalue decomposition (to be in effect only tempo-rarily for the purpose of clustering) reduce theproblem of data sparseness?
Can biclustering (alsoreferred to as co-clustering or two-mode cluster-ing, i.e.
the simultaneous clustering of the rows andcolumns of a matrix) improve results?
Does the ap-proach scale to larger vocabularies?
Can it be extendedto word sense induction by looking at longer distanceequivalents to middle words and neighbor pairs (whichcould be homographs and pairs of words strongly as-sociated to them)?
All these are strands of research thatwe look forward to explore.Simulation Brown Corpus  Simulation Brown CorpusNoun Verb Adj.
NN VB JJ  Noun Verb Adj.
NN VB JJaccident 978 8 15 33 0 0 lunch 741 198 60 32 1 0belief 972 17 11 64 0 0 maintain 4 993 3 0 60 0birth 968 15 18 47 0 0 occur 15 973 13 0 43 0breath 946 21 33 51 0 0 option 984 10 7 5 0 0brief 132 50 819 8 0 63 pleasure 931 16 54 60 1 0broad 59 7 934 0 0 82 protect 4 995 1 0 34 0busy 22 22 956 0 1 56 prove 5 989 6 0 53 0catch 71 920 9 3 39 0 quick 47 14 938 1 0 58critical 51 13 936 0 0 57 rain 881 64 56 66 2 0cup 957 23 21 43 1 0 reform 756 221 23 23 3 0dangerous 37 29 934 0 0 46 rural 66 13 921 0 0 46discuss 3 991 5 0 28 0 screen 842 126 32 42 5 0drop 334 643 24 24 34 1 seek 8 955 37 0 69 0drug 944 10 46 20 0 0 serve 20 958 22 0 107 0empty 48 187 765 0 0 64 slow 43 141 816 0 8 48encourage 7 990 3 0 46 0 spring 792 130 78 102 6 0establish 2 995 2 0 58 0 strike 544 424 32 25 22 0expensive 55 14 931 0 0 44 suit 200 789 11 40 8 0familiar 42 17 941 0 0 72 surprise 818 141 41 44 5 3finance 483 473 44 9 18 0 tape 868 109 23 31 0 0grow 15 973 12 0 61 0 thank 14 983 3 0 35 0imagine 4 993 4 0 61 0 thin 32 58 912 0 2 90introduction 989 0 11 28 0 0 tiny 27 1 971 0 0 49link 667 311 23 12 4 0 wide 9 4 988 0 0 115lovely 41 7 952 0 0 44 wild 220 6 774 0 0 51Table 3: List of 50 words and their values (scaled by 1000) from each of the three cluster centroids.
Forcomparison, POS frequencies from the manually tagged Brown corpus are given.AcknowledgmentsThis research was supported by a Marie CurieIntra-European Fellowship within the 6th Frame-work Programme of the European Community.ReferencesClark, Alexander (2003).
Combining distributionaland morphological information for part of speechinduction.
Proceedings of 10th EACL Conference,Budapest, 59?66.Finch, Steven (1993).
Finding Structure in Language.PhD Thesis, University of Edinburgh.Freitag, Dayne (2004).
Toward unsupervised whole-corpus tagging.
Proc.
of 20th COLING, Geneva.Ku?era, Henry; Francis, W. Nelson (1967).
Compu-tational Analysis of Present-Day American Eng-lish.
Providence, Rhode Island: Brown UniversityPress.Mintz, Toben H. (2003).
Frequent frames as a cue forgrammatical categories in child directed speech.Cognition, 90, 91?117.Rapp, Reinhard (2005).
A practical solution to theproblem of automatic part-of-speech inductionfrom text.
Proceedings of the 43rd ACL Confer-ence, Companion Volume, Ann Arbor, MI, 77?80.Rapp, Reinhard (2007).
Part-of-speech discovery byclustering contextual features.
In: Reinhold Deckerand Hans-J.
Lenz (eds.
): Advances in Data Analy-sis.
Proceedings of the 30th Conference of the Ge-sellschaft f?r Klassifikation.
Heidelberg: Springer,627?634.Sch?tze, Hinrich (1993).
Part-of-speech induction fromscratch.
Proceedings of the 31st ACL Conference,Columbus, Ohio, 251?258.56
