Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1641?1650,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-domain Text Classification with Multiple Domains and DisparateLabel SetsHimanshu S. Bhatt, Manjira Sinha and Shourya RoyXerox Research Centre India, Bangalore, INDIA{Firstname.Lastname}@Xerox.ComAbstractAdvances in transfer learning have let gothe limitations of traditional supervisedmachine learning algorithms for being de-pendent on annotated training data fortraining new models for every new do-main.
However, several applications en-counter scenarios where models need totransfer/adapt across domains when the la-bel sets vary both in terms of count of la-bels as well as their connotations.
This pa-per presents first-of-its-kind transfer learn-ing algorithm for cross-domain classifica-tion with multiple source domains and dis-parate label sets.
It starts with identifyingtransferable knowledge from across multi-ple domains that can be useful for learningthe target domain task.
This knowledgein the form of selective labeled instancesfrom different domains is congregated toform an auxiliary training set which isused for learning the target domain task.Experimental results validate the efficacyof the proposed algorithm against strongbaselines on a real world social media andthe 20 Newsgroups datasets.1 IntroductionA fundamental assumption in supervised statisti-cal learning is that training and test data are inde-pendently and identically distributed (i.i.d.)
sam-ples drawn from a distribution.
Otherwise, goodperformance on test data cannot be guaranteedeven if the training error is low.
On the other hand,transfer learning techniques allow domains, tasks,and distributions used in training and testing tobe different, but related.
It works in contrast totraditional supervised techniques on the principleof transferring learned knowledge across domains.Pan and Yang, in their survey paper (2010), de-Figure 1: Cross-domain (a) sentiment classifica-tion and (b) subject classification.
Illustrates (a)invariant and (b) disparate label sets.scribed different transfer learning settings depend-ing on if domains and tasks vary as well as labeleddata is available in one/more/none of the domains.In this paper, we propose a generic solution formulti-source transfer learning where domains andtasks are different and no labeled data is availablein the target domain.
This is a relatively less char-tered territory and arguably a more generic settingof transfer learning.Motivating example: Consider a social mediaconsulting company helping brands to monitortheir social media channels.
Two problems typi-cally of interest are: (i) sentiment classification (isa post positive/negative/neutral?)
and (ii) subjectclassification (what was the subject of a post?
).While sentiment classification attempts to classifya post based on its polarity, subject classificationis towards identifying the subject (or topic) of thepost, as illustrated in Figure 1.
The companyhas been using standard classification techniquesfrom an off-the-shelf machine learning toolbox.While machine learning toolkit helps them to cre-ate and apply statistical models efficiently, thesame model can not be applied on a new collec-tion due to variations in data distributions acrosscollections1.
It requires a few hundreds of manu-ally labeled posts for every task on every collec-1A collection comprises comments/posts pertaining to aparticular client/product/services.
Domain and collection areused interchangeably.1641tion.
As social media are extremely high velocityand low retention channels, human labeling effortsact like that proverbial narrow bottleneck.
Needof the hour was to reduce, if not eliminate, thehuman-intensive labeling stage while continue touse machine learning models for new collections.Several transfer learning techniques exist in theliterature which can reduce labeling efforts re-quired for performing tasks in new collections.Tasks such as sentiment classification, named en-tity recognition (NER), part of speech (POS) tag-ging that have invariant label sets across domains,have shown to be greatly benefited from theseworks.
On the other hand, tasks like subject clas-sification that have disparate label sets across do-mains have not been able to gain at pace with theadvances in transfer learning.
Towards that we for-mulate the problem of Cross-domain classificationwith disparate label sets as learning an accuratemodel for the new unlabeled target domain givenlabeled data from multiple source domains whereall domains have (possibly) different label sets.Our contributions: To the best of our knowl-edge, this is the first work to explore the prob-lem of cross-domain text classification with mul-tiple source domains and disparate label sets.
Theother contributions of this work includes a sim-ple yet efficient algorithm which starts with iden-tifying transferable knowledge from across mul-tiple source domains useful for learning the tar-get domain task.
Specifically, it identifies rele-vant class-labels from the source domains suchthat the instances in those classes can induce class-separability in the target domain.
This transferableknowledge is accumulated as an auxiliary trainingset for an algorithm to learn the target domain clas-sification task followed by suitable transformationof the auxiliary training instances.Organization of the paper is as follows: Section2 presents the preliminaries and notation, Section3 summarizes the related work.
Section 4 and 5present the proposed algorithm and experimentalresults respectively.
Section 6 concludes the paper.2 Preliminaries and NotationsA domain D = {X , P (X)} is characterizedby two components: a feature space X and amarginal probability distribution P (X), whereX = {x1, x2, ...xn} ?
X .
A task T = {Y, f(?
)}also consists of two components: a label space Yand an objective predictive function f(?
).In our settings for cross-domain classificationwith disparate label sets, we assumeM source do-mains, denoted as DSi, where i = {1, 2, ..M}.Each source domain has different marginal distri-bution i.e.
P (XSi) 6= P (XSj) and different la-bel space i.e.
YSi6= YSj, ?i, j ?
M .
The labelspace across domains vary both in terms of countof class-labels as well as their connotations; how-ever, a finite set of labeled instances are availablefrom each source domain.
The target domain (DT)consists of a finite set of unlabeled instances, de-noted as tiwhere i = {1, .., N}.
Let YTbe thetarget domain label space with K class-labels.
Weassume that the number of classes in the targetdomain i.e.
K is known (analogous to clusteringwhere the number of clusters is given).3 Related WorkTable 1 summarizes different settings of transferlearning (Pan and Yang, 2010) and how this workdifferentiates from the existing literature2.
Thefirst scenario represents the ideal settings of tra-ditional machine learning (Mitchell, 1997) wherea model is trained on a fraction of labeled data andperforms well for the same task on the future un-seen instances from the same domain.The second scenario where the domains varywhile the tasks remain the same is referred to astransductive transfer learning.
This is the most ex-tensively studied settings in the transfer learningliterature and can be broadly categorized as singleand multi-source adaptation.
Single source adap-tation (Chen et al, 2009; Ando and Zhang, 2005;Daum?e III, 2009) primarily aims at minimizing thedivergence between the source and target domainseither at instance or feature levels.
The generalidea being identifying a suitable low dimensionalspace where transformed source and target do-mains data follow similar distributions and hence,a standard supervised learning algorithm can betrained (Daum?e III, 2009; Jiang and Zhai, 2007;Pan et al, 2011; Blitzer et al, 2007; Pan et al,2010; Dai et al, 2007; Bhatt et al, 2015).While several existing single source adaptationtechniques can be extended to multi-source adap-tation, the literature in multi-source adaptation canbe broadly categorized as: 1) feature representa-tion approaches (Chattopadhyay et al, 2012; Sunet al, 2011; Duan et al, 2009; Duan et al, 2012;2This is not the complete view of the transfer learningliterature; however, covers relevant work that helps moti-vate/differentiate the novel features of this paper.1642Table 1: Summarizing the related work and differentiating the novel features of the proposed algorithm.Scenario Settings Nature of DataLearningParadigmMain Concepts Our DifferentiationDS= DT,TS= TTTraditionalMachinelearningLabelled data insource domain(s)and unlabeled data intarget domainSource andtarget domainsare exactly thesameLearn models on training set andtest on future unseen dataAllows tasks across domains to bedifferent;a more general settingTransductiveLabelled data insource domain(s)andSingle sourcedomainadaptationLearning common sharedrepresentation; instance weighing,parameter transferExploits multiple sources each withdisparate label sets.DS6= DT,TS= TTTransferLearning unlabeled data fromthe target domainP(XS) 6= P(XT)Multi-sourceadaptationClassifier combination; efficientcombination of information frommultiple sources; FeaturerepresentationIntelligent selection of transferableknowledge from multiple sourcesfor adaptation.InductiveUnlabeled data insource domain(s)and labeled data intarget domainSelf-taughtlearningExtracts higher levelrepresentations from unlabeledauxiliary data to learninstance-to-label mapping withlabeled target instancesLearns instance-to-label mapping inthe unlabeled target domain usingmultiple labeled source domainshaving different data distributionsand label spaces.Noconditionson DS&DT, but,TS6= TTTransferLearningLabeled data isavailable in alldomainsMulti-tasklearningSimultaneously learns multipletasks within (or across) domain(s)by exploiting the common featuresubspace shared across the tasksLearns the optimal class distributionin an unlabeled target domain byminimizing the differences withmultiple labeled source domains.DS6= DT,TS6= TTKim et al(2015)Labeled data insource and targetdomainsTransferlearning withdisparate labelsetDisparate fine grained label setsacross domains, however, samecoarse grained labels set can beinvoked across domainsNo coarse-to-fine label mapping dueto heterogeneity of label sets,Assumes no labelled data in targetdomain.Bollegala et al, 2013; Crammer et al, 2008; Man-sour et al, 2009; Ben-David et al, 2010; Bhattet al, 2016) and 2) combining pre-trained classi-fiers (Schweikert and Widmer, 2008; Sun and Shi,2013; Yang et al, 2007; Xu and Sun, 2012; Sunet al, 2013).
Our work differentiates in intelli-gently exploiting selective transferable knowledgefrom multiple sources unlike existing approacheswhere multiple sources contribute in a brute-forcemanner.The third scenario where the tasks differ irre-spective of the relationship among domains is re-ferred to as inductive transfer learning.
Self-taughtlearning (Raina et al, 2007) and multi-task (Jiang,2009; Maurer et al, 2012; Xu et al, 2015; Kumarand Daume III, 2012) learning are the two mainlearning paradigms in this scenario and Table 1differentiates our work from these.This work closely relates to the fourth scenariowhere we allow domains to vary in the marginalprobability distributions and the tasks to vary dueto different label spaces3.
The closest prior workby Kim et al (2015) address a sequential label-ing problem in NLU where the fine grained labelsets across domains differ.
However, they assumethat there exists a bijective mapping between thecoarse and fine-grained label sets across domains.They learn this mapping using labeled instancesfrom the target domain to reduce the problem to astandard domain adaptation problem (Scenario 2).3This work do not consider scenario when domains varyin feature spaces and tasks vary in the objective predictivefunctions.Figure 2: Illustrates different stages of the pro-posed algorithm.However, this paper caters to multiple source do-mains with disparate label sets without assumingavailability of any labeled data from the target do-main or fine-to-coarse label mappings across do-mains.4 Cross-domain Classification withDisparate Label SetThe underlying philosophy of the proposed algo-rithm is to learn the target domain task by usingthe available information from multiple source do-mains.
To accomplish this, we have developedan algorithm to identify and extract partial trans-ferable knowledge from multiple sources.
Thisknowledge is then suitably transformed to induceclasses in the target domain using the class separa-tion from the source domains.
Different stages ofthe proposed algorithm, as shown in Figure 2, are1643elaborated in the next sections.4.1 Exploiting Multiple DomainsIf we had the mappings between the source andtarget domain label sets, we could have leveragedexisting transfer learning approaches.
However,heterogeneity of label sets across domains andthe unlabeled data from the target domain exac-erbate the problem.
Our objective is to leveragethe knowledge from multiple source domains toinduce class-separability in the target domain.
In-ducing class-separability refers to segregating thetarget domain into K classes using labeled in-stances from selective K source domain classes.Towards this, the proposed algorithm divideseach source domain into clusters/groups based onthe class-labels such that instances with the samelabel are grouped in one cluster.
All source do-mains are divided into Q clusters where Q =?Mm=1||Ym|| represents the count of class-labelsacross all sources.
||Ym|| being the count of class-labels in the mthsource domain.
Cqdenotes theqthcluster and ?qdenotes its centroid computed asthe average of all the members in the cluster.
Weassert that the target domain instances that havehigh similarity to a particular source domain clus-ter can be grouped together.
Given N target do-main instances and Q source domain clusters, amatrix R (dimension N ?
Q) is computed basedon the similarity of the target domain instanceswith the source clusters.
The ithrow of the ma-trix captures the similarity of the ithtarget domaininstance (ti) with all the source domain clusters.
Itcaptures how different source domain class-labelsare associated with the target domain instancesand hence, can induce class-separability in the tar-get domain.4.2 Extracting Transferable KnowledgeThe similarity matrix R associates target domaininstances to the source domain clusters in propor-tion to their similarity.
However, the objective is toselect the optimalK source domain clusters that fitthe maximum number of target domain instances.This problem is similar to the well-known combi-natorial optimization problem of Maximum Cov-erage (Vazirani, 2003) where given a collection ofP sets, we need to selectA sets (A < P ) such thatthe size of the union of the selected sets is maxi-mized.
In this paper, we are given Q source do-main clusters and need to select K clusters suchthat the corresponding number of associated tar-get domain instances is maximized.
As the Max-imum Coverage problem is NP-hard, we imple-ment a greedy algorithm for selecting the k sourcedomain clusters, as illustrated in Algorithm 1.Algorithm 1 Selecting K Source ClustersInput: A matrixR, K = number target domainclasses, l= number of selected cluster.Initialize: l = 0, Normalize R such that eachrow sums up to 1.repeat:1: Pick the column in R which has maximumsum of similarity scores for uncovered targetdomain instances.2: Mark elements in the chosen column as cov-ered.3: l = l + 1until: l = KOutput: K source domain clusters.A source domain contributes partially in termsof zero or more class-labels (clusters) identifiedusing the Algorithm 1.
Therefore, we refer to thelabeled instances from the selected clusters of asource domain as the partial transferable knowl-edge from that domain.
This partial transferableknowledge from across multiple source domainsis congregated to form an auxiliary training set,referred to as (AUX).4.3 Adapting to the Target DomainThe auxiliary training set comprises labeled in-stances from selected K source domain clusters4.Since, the auxiliary set is pulled out from multiplesource domains, it follows different data distribu-tion as compared to the target domain.
For a clas-sifier, trained on theK-class auxiliary training set,the distributional variations have to be normalizedso that it can generalize well on the target domain.In this research, we proposed to use an instanceweighting technique (Jiang and Zhai, 2007) tominimize the distributional variations by deferen-tially weighting instances in the auxiliary set.
In-tuitively, the auxiliary training instances similarto the target domain are assigned higher weightswhile training the classifier and vice versa.
Theweight for the ithinstance in the auxiliary setshould be proportional to the ratio(Pt(xi))(Pa(xi)).
How-ever, since the actual probability distributions4The K classes in auxiliary set induce class-separabilityin the target domain, however, the actual class-labels acrossthese two may not have any sort of coarse-to-fine mapping.1644Algorithm 2 Cross-domain Classification withDisparate Label SetsInput: M source domains, target domain in-stances (ti), i = (1, ..., N ), K = number oftarget domain classes.Process: Divide M sources into Q clusters s.t.Q =?Mq=1|Ym|.
Cqbe the qthcluster & ?qbeits centroid computed as shown in Eq 1.A: Exploiting Multiple Sources:for i = 0 : till N dofor q = 0 : till Q doR[i, q] = Sim(?q, ti)end forend forB: Extracting partial knowledge:1: Pick K columns from R using Algorithm 1.2: Construct AUX by congregating instancesfrom the selectedK source domain class-labels.C: Adapting to target domain:1: Minimize distributional variations using in-stance weighing technique.2: Train a K-class classifier using AUX .Output: K-class target domain classifier.
(Pa(x) and Pt(x) for the auxiliary set and targetdomain respectively) are unknown, the instancedifference is approximated as(Pt(xi|d=target))(Pa(xi|d=auxilliary)),where d is a random variable used to representwhether xicame from the auxiliary set or the tar-get domain.
To calculate this ratio, a binary classi-fier is trained using the auxiliary set and target do-main data with labels {-1} and {+1} respectively.The predicted probabilities from the classifier areused to estimate the ratio as the weight for the ithauxiliary instance xi.
Finally, a K-class classifieris trained on the weighted auxiliary training set toperform classification on the target domain data.4.4 AlgorithmAs shown in Figure 2, the step-by-step flow ofthe proposed algorithm is summarized below:1.
Divide M source domains into Q clusters,each represented as Cq, q = {1, 2, .., Q}.2.
Compute centroid of each cluster as the aver-age of the cluster members, as shown in Eq.1.?q=1||Cq||||Cq||?
(i=1;xi?Cq)xi(1)where ?qis the centroid, ||Cq|| is the mem-bership count and xiis the ithmember of Cq.3.
For target instances ti?i ?
N , computecosine similarity with all the source domaincluster centroids to form the matrix R (di-mensions: N ?Q), as shown in Eq.
2R[i, q] = Sim(?q, ti) =?q?
ti||?q|| ||ti||(2)4.
Run Algorithm 1 on R to select K optimalsource clusters (i.e.
columns of R).5.
Congregate labeled instances from the se-lected source domain clusters to form the K-class auxiliary training set.6.
Minimize the divergence between the auxil-iary set and target domain using the instanceweighing technique, described in Section 4.3.7.
Finally, train a K-class classifier on deferen-tially weighted auxiliary training instances toperform classification in the target domain.The K-class classifier trained on the auxiliarytraining set is an SVM classifier (Chih-Wei Hsuand Lin, 2003) with L2 ?
loss from the LIB-LINEAR library (Fan et al, 2008).
The classi-fier used in the instance weighing technique isagain an SVM classifier with RBF kernel.
Theproposed algorithm uses distributional embeddingi.e.
Doc2Vec (Le and Mikolov, 2014) to representinstances from the multiple source and target do-mains.
We used an open-source implementation ofDoc2Vec (Le and Mikolov, 2014) for learning 400dimensional vector representation using DBoW.5 Experimental EvaluationComprehensive experiments are performed toevaluate the efficacy of the proposed algorithmfor cross-domain classification with disparate la-bel sets across domains on two datasets.5.1 DatasetsThe first dataset is a real-world Online Social Me-dia (OSM) dataset which consists of 74 collec-tions.
Each collection comprises comments/tweetsthat are collected based on user-defined keywords.These keywords are fed to a listening enginewhich crawls the social media (i.e.
Twitter.com)and fetches comments matching the keywords.The task is to classify the comments in a collection1645Table 2: Illustrates variability in label sets acrosssome collections from the OSM dataset.Apple iPhone 6 Apple iOS 8 Apple iPad mini3Camera Locking apps & fea-turesRelease date & Fea-turesDesign Extensibility fea-turesApple play & NFCReview link General features re-lated marketingApple sim cardApple Play/NFC Camera features Touch IDComparison to An-droidPassword withtouch integrationiPad mini3 -disappointsPrice Health & fitness appApple watch Location & MapsFirmware updatesTable 3: Table illustrates the collections from theEMPATH database used in this research.Collection ID Domain #CategoriesColl 1 Huwaei 5Coll 2 Healthcare 9Coll 3 Whattsapp 8Coll 4 Apple iOS 8 8Coll 5 Apple iPhone 6 7into user-defined categories.
These user-definedcategories may vary across collections in terms ofcount as well as their connotations.
Table 2 showsan example of the user-defined categories for a fewcollections related to ?Apple?
products.
In the ex-periments, one collection is used as unlabeled tar-get collection and the remaining collections areused as the labeled source collections.
We ran-domly selected 5 target collections to report theperformance, as described in Table 3.The second dataset is the 20 Newsgroups (NG)(Lang, 1995) dataset which comprises 20, 000news articles organized into 6 groups with differ-ent sub-groups both in terms of count as well asconnotations, as shown in Figure 3(a).
Two differ-ent experiments are performed on this dataset.
Inthe first experiment (?Exp-1?
), one group is con-sidered as the target domain and the remaining 5groups as the source domains.
In the second ex-periment (?Exp-2?
), one sub-group from each ofthe first five groups5is randomly selected to syn-thesize a target domain while all the groups (withthe remaining sub-groups) are used as source do-mains.
Figure 3(b) shows an example on how tosynthesize target domains in ?Exp-2?.
There are720 possible target domains in this experiment andwe report the average performance across all pos-sible target domains, referred to as ?Grp 7?.
Thetask in both the experiments is to categorize thetarget domain into its K categories (sub-groups)using labeled data from multiple source domains.5Group-6 has only 1 sub-group, therefore, it is consideredfor synthesizing target domain in the experiments.Figure 3: Illustrates (a) different groups (b) targetdomain synthesis (?EXP 2?)
on the NG dataset.5.2 Evaluation MetricThe performance is reported in terms of classifi-cation accuracy on the target domain.
There is nodefinite mapping between the actual class-labelsin the target domain and the K categories (i.e.induced categories) in the auxiliary training set.Therefore, we sequentially evaluate all possibleone-to-one mappings between the K categories inthe auxiliary training set and target domain to re-port results for the best performing mapping.5.3 Experimental ProtocolThe performance of the proposed algorithm is sky-lined by the in-domain performance (Gold), i.e.a classifier trained and tested on the labeled tar-get domain data.
We also compared the perfor-mance with spherical K-means clustering (Dhillonand Modha, 2001) used to group the target domaindata into K categories against the ground truth,referred to CL.
Spherical K-means clustering isbased on cosine similarity and performs better forhigh-dimensional sparse data such as text.To compare with a baseline and an existingadaptation algorithm, we selected the most similarsource domain6with exactly K number of class-labels and report the performance on the best pos-sible mapping, as described in Section 5.2.
Tocompute the baseline (BL), a classifier trained onthe source domain is used to categorize the targetdomain.
A widely used domain adaptation algo-rithm, namely structural correspondence learning(SCL) (Blitzer et al, 2007) is also applied usingthe selected source domain.6The most similar source domain is selected using proxy-A distance (Blitzer et al, 2007) which has good correlationwith domain adaptation performance.1646Figure 4: Compares the performance of differenttechniques on the OSM dataset.Table 4: Summarizes the performance of the pro-posed algorithm on the OSM dataset.Coll ID (#) BL CL SCL W/O Proposed GoldColl 1 (5) 52.6 43.7 62.8 77.4 81.4 90.5Coll 2 (9) 38.6 31.8 58.8 72.5 77.6 84.4Coll 3 (8) 43.6 36.4 60.7 74.2 78.5 87.6Coll 4 (8) 44.7 38.8 62.5 78.8 82.1 92.5Coll 5 (7) 50.5 42.8 64.4 76.6 80.5 89.35.4 Results and AnalysisKey observations and analysis from the experi-mental evaluations are summarized below:5.4.1 Results on the OSM DatasetResults in Figure 4 and Table 4 show the effi-cacy of the proposed algorithm for cross-domainclassification with disparate label sets as it out-performs other approaches by at least 15%.
CollID(#) refers to the target collection and the corre-sponding count of class-labels.
Results in Table4 also compare the performance of the proposedtechnique without the distributional normalizationof the auxiliary training set, referred to as ?W/O?.Results suggest that suitably weighing instancesfrom the auxiliary training set mitigates the distri-butional variations and enhances the cross-domainperformance by at least 3.3%.5.4.2 Results on the 20Newsgroups DatasetResults in Table 5 show that the proposed algo-rithm outperforms other techniques for both theexperiments by at least 15 % and 18% respectivelyon the 20 Newsgroups dataset.
In Table 5, ?-?refers to the cases where a single source domainwith the same number of class-labels as in the tar-get domain is not available.
In ?Exp-1?
wherethe source and target categories vary in terms ofcounts as well as their connotations, the proposedalgorithm efficiently induces the classes in the un-labeled target domain using the partial transferableknowledge from multiple sources.
For ?Exp-2?, itis observed that the performance of the proposedalgorithm is better than the performance in ?Exp-1?
as the target categories have closely related cat-egories (from the same group) in the source do-Figure 5: Effects of selected source collections onthe OSM dataset.Table 5: Summarizes the performance of the pro-posed algorithm on the 20Newsgroups dataset.Target(#) BL CL SCL W/O Proposed GoldGrp 1 (5) - 48.6 - 79.4 80.8 85.6Grp 2 (4) 62.7 50.2 62.7 78.3 83.6 89.2Grp 3 (4) 64.3 54.8 64.4 81.6 85.3 90.4Grp 4 (3) 69.6 55.6 67.3 82.2 86.4 92.5Grp 5 (3) 69.7 56.4 70.3 83.6 85.3 91.2Grp 7 (5) - 52.8 - 84.6 88.4 93.8mains.
Table 5 reports the average performanceacross all the 720 possible combinations of targetdomains with a standard deviation of 2.6.5.4.3 Effect of Multiple Source DomainsTable 6 validates our assertion that multiplesources are necessary to induce class-separabilityin the target domain as a single source is not suf-ficient to cater to the heterogeneity of class-labelsacross domains.
It also suggests that the proposedalgorithm can learn class-separability in the tar-get domain by using arbitrary diverse class-labelsfrom different sources and does not necessarily re-quire class-labels to follow any sort of coarse-to-fine mapping across domains.To evaluate the effects of using multiplesources, further experiments were performed byvarying the number of available source domains.For the OSM dataset, we varied the number ofavailable source collections from 1 to 73 startingwith the most similar source collection and repeat-edly adding the next most similar collection in thepool of available collections.
We observe that eventhe most similar collection was not independentlysufficient to induce classes in the target collec-tion and it was favorable to exploit multiple col-lections.
Moreover, adding collections based onsimilarity to the target collection had a better like-lihood of achieving higher performance as com-pared to adding random collections.In another experiment, we first identified thesource collections which contributed to learningthe target task.
We removed these collections andapplied the proposed algorithm on the remainingsource collections.
Figure 5 shows the perfor-1647Table 6: Actual target domain class-labels and thecorresponding source domain class clusters usedto build the auxiliary training set.Target Collection:Apple iOS 8Associated Class-labels from multiplesource collectionsLocking apps &securityAnti Theft Features (Coll ID: 776 on AppleiOS 6 plus)ExtensibilityfeaturesApplication update (Coll ID:720 on AppleiOS Features)General featuresrelated marketingGeneral press (Coll ID: 163 on XBOXIssues)Camera features Camera (Coll ID: 775 on Apple iPhone 6 )Password withtouch integrationTouch ID (Coll ID: 803 on Apple iPadmini3)Health & fitness appReproductive health issues (Coll ID: 289 onHealthcare)Location & Maps Events (Coll ID: 502 on L?Oreal)Firmware updatesUpdates & patches (Coll ID: 478 on RiotGame Support v2)mance of the proposed algorithm on 5 such iter-ations of removing the contributing source collec-tions from the previous iteration.
We observed asignificant drop in the performance with each iter-ation which signifies the effectiveness of the pro-posed algorithm in extracting highly discriminat-ing transferable knowledge from multiple sources.5.4.4 Comparing with Domain AdaptationWe applied domain adaptation techniques con-sidering the auxiliary training set to be a singlesource domain with the same number of classesas that in the target domain.
We applied twoof the widely used domain adaptation techniques,namely SCL (Blitzer et al, 2007) and SFA (Pan etal., 2010) referred to as ?AuxSCL?
and ?AuxSFA?respectively.
Results in Table 7 suggest thatthe proposed algorithm significantly outperforms?AuxSCL?
and ?AuxSFA?
on the two datasets.Generally, existing domain adaptation techniquesare built on the co-occurrences of the common fea-tures with the domain specific features and hence,capture how domain specific features in one do-main behaves w.r.t to the domain specific featuresin the other domain.
They assume homogeneouslabels and expect the aligned features across do-mains to behave similarly for the prediction task.However, these features are misaligned when thelabel set across domains vary in terms of their con-notations.5.4.5 Effect of Different RepresentationsThe proposed algorithm uses Doc2Vec (Le andMikolov, 2014) for representing instances frommultiple domains.
However, the proposed algo-rithm can build on different representations andhence, we compare its performance with tradi-tional TF-IDF representation (including unigramsTable 7: Comparing the proposed algorithm withexisting domain adaptation algorithms.Dataset Target SCL SFA ProposedOSMColl 1 66.2 64.7 81.4Coll 2 63.8 62.6 77.6Coll 3 64,1 63.4 78.5Coll 4 64.2 65.2 82.1Coll 5 64.0 63.7 80.5Grp 1 65.2 64.2 80.8NGGrp 2 68.2 65.3 83.6Exp-1Grp 3 69.4 68.4 85.3Grp 4 70.3 69.2 86.4Grp 5 69.0 68.8 85.3NG Exp-2 Grp 7 72.6 70.2 88.4Table 8: Comparing different representations.Dataset Target TF-IDF TF-IDF +PCA Doc2VecOSMColl 1 70.6 76.8 81.4Coll 2 69.5 74.2 77.6Coll 3 70.2 75.5 78.5Coll 4 71.6 77.9 82.1Coll 5 70.8 76.8 80.5Grp 1 71.8 75.6 80.8NGGrp 2 73.6 77.5 83.6Exp-1Grp 3 77.4 81.1 85.3Grp 4 76.6 82.5 86.4Grp 5 75.5 81.4 85.3NG Exp-2 Grp 7 76.2 83.6 88.4and bigrams) and a dense representation using TF-IDF+PCA ( reduced to a dimension such that itcovers 90% of the variance).
We observe thatDoc2Vec representation clearly outperforms theother two representations as it addresses the draw-backs of bag-of n-gram models in terms of implic-itly inheriting the semantics of the words in a doc-ument and offering a more generalizable concisevector representation.6 ConclusionsThis paper presented the first study on cross-domain text classification in presence of multipledomains with disparate label sets and proposed anovel algorithm for the same.
It proposed to ex-tract partial transferable knowledge from acrossmultiple source domains which was beneficial forinducing class-separability in the target domain.The transferable knowledge was assimilated interms of selective labeled instances from differentsource domain to form a K-class auxiliary train-ing set.
Finally, a classifier was trained using thisauxiliary training set, following a distribution nor-malizing instance weighing technique, to performthe classification task in the target domain.
The ef-ficacy of the proposed algorithm for cross-domainclassification across disparate label sets will ex-pand the horizon for ML-based algorithms to bemore widely applicable in more general and prac-tically observed scenarios.1648ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In Proceedings of Association for Computa-tional Linguistics, pages 1?9.Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza,Fernando Pereira, and Jennifer Wortman Vaughan.
2010.A theory of learning from different domains.
MachineLearning, 79(1-2):151?175.Himanshu Sharad Bhatt, Deepali Semwal, and Shourya Roy.2015.
An iterative similarity based adaptation techniquefor cross-domain text classification.
In Proceedings ofConference on Natural Language Learning, pages 52?61.Himanshu Sharad Bhatt, Arun Rajkumar, and Shourya Roy.2016.
Multi-source iterative adaptation for cross-domainclassification.
In In Proceedings of International JointConference on Artificial Intelligence.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.
Bi-ographies, bollywood, boom-boxes and blenders: Domainadaptation for sentiment classification.
In Proceedingsof Association for Computational Linguistics, pages 440?447.Danushka Bollegala, David Weir, and John Carroll.
2013.Cross-domain sentiment classification using a sentimentsensitive thesaurus.
IEEE Transactions on Knowledge andData Engineering, 25(8):1719?1731.Rita Chattopadhyay, Qian Sun, Wei Fan, Ian Davidson,Sethuraman Panchanathan, and Jieping Ye.
2012.
Mul-tisource domain adaptation and its application to early de-tection of fatigue.
ACM Transactions on Knowledge Dis-covery from Data, 6(4):1?26.Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009.Extracting discriminative concepts for domain adaptationin text mining.
In Proceedings of International Confer-ence on Knowledge Discovery and Data Mining, pages179?188.Chih-Chung Chang Chih-Wei Hsu and Chih-Jen Lin.
2003.A practical guide to support vector classification.
Tech-nical report, Department of Computer Science, NationalTaiwan University.Koby Crammer, Michael Kearns, and Jennifer Wortman.2008.
Learning from multiple sources.
Journal of Ma-chine Learning Research, 9(1):1757?1774.Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu.2007.
Co-clustering based classification for out-of-domain documents.
In Proceedings of International Con-ference on Knowledge Discovery and Data Mining, pages210?219.Hal Daum?e III.
2009.
Frustratingly easy domain adaptation.arXiv preprint arXiv:0907.1815.Inderjit S. Dhillon and Dharmendra S. Modha.
2001.
Con-cept decompositions for large sparse text data using clus-tering.
Machine Learning, 42(1):143?175.Lixin Duan, Ivor W. Tsang, Dong Xu, and Tat-Seng Chua.2009.
Domain adaptation from multiple sources via aux-iliary classifiers.
In Proceedings International Conferenceon Machine Learning, pages 289?296.Lixin Duan, Dong Xu, and Ivor Wai-Hung Tsang.
2012.
Do-main adaptation from multiple sources: a domaindepen-dent regularization approach.
IEEE Transactions on Neu-ral Networks and Learning Systems, 23(3):504518.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: a libraryfor large linear classification.
Journal of Machine Learn-ing Research, 9:1871?1874.J.
Jiang and C. Zhai.
2007.
Instance weighting for do-main adaptation in NLP.
In Proceedings of Associationfor Computational Linguistics, volume 7, pages 264?271.Jing Jiang.
2009.
Multi-task transfer learning for weakly-supervised relation extraction.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2-Volume 2, pages1012?1020.
Association for Computational Linguistics.Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and MinwooJeong.
2015.
New transfer learning techniques for dis-parate label sets.
In Proceedings of Association for Com-putational Linguistics.Abhishek Kumar and Hal Daume III.
2012.
Learningtask grouping and overlap in multi-task learning.
arXivpreprint arXiv:1206.6417.Ken Lang.
1995.
NewsWeeder: Learning to filter netnews.In Proceedings of International Conference on MachineLearning.Quoc V. Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
In InternationalConference on Machine Learning, pages 1188?1196.Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.2009.
Domain adaptation with multiple sources.
In Ad-vances in Neural Information Processing Systems, pages1041?1048.Andreas Maurer, Massimiliano Pontil, and BernardinoRomera-Paredes.
2012.
Sparse coding for multitask andtransfer learning.
arXiv preprint arXiv:1209.0738.Thomas M. Mitchell.
1997.
Machine Learning.
McGraw-Hill, Inc., New York, NY, USA, 1 edition.Sinno Jialin Pan and Qiang Yang.
2010.
A survey on transferlearning.
Knowledge and Data Engineering, IEEE Trans-actions on, 22(10):1345?1359.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang,and Zheng Chen.
2010.
Cross-domain sentiment classi-fication via spectral feature alignment.
In Proceedings ofInternational Conference on World Wide Web, pages 751?760.Sinno Jialin Pan, , Ivor W. Tsang, James T. Kwok, and QiangYang.
2011.
Domain adaptation via transfer compo-nent analysis.
IEEE Transactions on Neural Networks,22(2):199?210.Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,and Andrew Y. Ng.
2007.
Self-taught learning: Transferlearning from unlabeled data.
In International Conferenceon Machine Learning, pages 759?766.1649Gabriele Schweikert and Christian Widmer.
2008.
An em-pirical analysis of domain adaptation algorithms for ge-nomic sequence analysis.
In Advances in Neural Informa-tion Processing Systems, pages 1433?1440.Shi-Liang Sun and Hong-Lei Shi.
2013.
Bayesian multi-source domain adaptations.
In International Conferenceon Machine Learning and Cybernetics, pages 24?28.Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan,and Jieping Ye.
2011.
A two-stage weighting frameworkfor multi-source domain adaptation.
In Advances in Neu-ral Information Processing Systems, pages 505?513.Shiliang Sun, Zhijie Xu, and Mo Yang.
2013.
Transfer learn-ing with part-based ensembles.
In Multiple Classifier Sys-tems, volume 7872, pages 271?282.Vijay V. Vazirani.
2003.
Approximation Algorithms.Springer-Verlag Berlin Heidelberg.Zhijie Xu and Shiliang Sun.
2012.
Multi-source transferlearning with multi-view adaboost.
In Proceedings of In-ternational Conference on Neural Information Process-ing, pages 332?339.Linli Xu, Aiqing Huang, Jianhui Chen, and Enhong Chen.2015.
Exploiting task-feature co-clusters in multi-tasklearning.
In Proceedings of Association for the Advance-ment of Artificial Intelligence, pages 1931?1937.Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007.Cross-domain video concept detection using adaptivesvms.
In Proceedings of International Conference onMultimedia, pages 188?197.1650
