SquibsFrom Annotator Agreement to Noise ModelsBeata Beigman Klebanov?Northwestern UniversityEyal Beigman?
?Northwestern UniversityThis article discusses the transition from annotated data to a gold standard, that is, a subsetthat is sufficiently noise-free with high confidence.
Unless appropriately reinterpreted, agreementcoefficients do not indicate the quality of the data set as a benchmarking resource: High overallagreement is neither sufficient nor necessary to distill some amount of highly reliable data fromthe annotated material.
A mathematical framework is developed that allows estimation of thenoise level of the agreed subset of annotated data, which helps promote cautious benchmarking.1.
IntroductionBy and large, the reason a computational linguist engages in an annotation project is tobuild a reliable data set for the eventual testing, and possibly training, of an algorithmperforming the task.
Hence, the crucial question regarding the annotated data set iswhether it is good for benchmarking.For classification tasks, the current practice is to infer this information from thevalue of an inter-annotator agreement coefficient such as the ?
statistic (Cohen 1960;Siegel and Castellan 1988; Carletta 1996).
If agreement is high, the whole of the data setis good for training and testing; the remaining disagreements are typically adjudicatedby an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju,Badulescu, andMoldovan 2006) or through discussion (Litman, Hirschberg, and Swerts2006), or, in case of more than two annotators, the majority label is chosen (Vieira andPoesio 2000).1 There are some studies where cases of disagreement were removed fromtest data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006).
If agreementis low, the whole data set is discarded as unreliable.
The threshold of acceptability seemsto have stabilized around ?
= 0.67 (Carletta 1996; Di Eugenio and Glass 2004).There is little understanding, however, of exactly how and how well the value of ?reflects the quality of the data for benchmarking purposes.
We develop a model of an-notation generation that allows estimation of the level of noise in a specially constructedgold standard.
A gold standard with a noise figure supports cautious benchmarking,?
Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu.??
Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu.1 In many studies, the procedure for handling disagreements is not clearly specified.
For example, Gildeaand Jurafsky (2002) mention a ?consistency check?
; in Lapata (2002), two annotators attained ?
= 0.78 on200 test instances, but it is not clear how cases of disagreements were settled.Submission received: 30 June 2008; revised submission received: 3 December 2008; accepted for publication:26 January 2009.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 4by requiring that the performance of an algorithm be better than baseline by more thanthat which can be attributed to noise.
Articulating an annotation generation model alsoallows us to shed light on the information ?
can contribute to benchmarking.2.
Annotation NoiseWe are interested in finding out which parts of the annotated data are sufficientlyreliable.
This question presupposes a division of instances into two types: reliableand unreliable, or, as we shall call them, easy and hard, under the assumption thatitems that are easy are reliably annotated, whereas items that are hard display con-fusion and disagreement.
The plausibility of separation into easy and hard instancesis supported by researchers conducting annotation projects: ?With many judgmentsthat characterize natural language, one would expect that there are clear cases as wellas borderline cases that are more difficult to judge?
(Wiebe, Wilson, and Cardie 2005,page 200).This suggests a model of annotation generation with latent variables for types, thus,for every instance i, there is a variable li with values E (easy) and H (hard).
Let n be thenumber of instances, k the number of annotators, and Xij the classification of the ith in-stance by the jth annotator.
An annotation generationmodel assigns a functional form tothe joint distribution conditioned on the latent variable P(Xi1, .
.
.
,Xik|li).
Similar modelshave been studied in biometrics (Aickin 1990; Hui and Zhou 1998; Albert, McShane,and Shih 2001; Albert and Dodd 2004).
The main assumption is that, conditioned on thetype, annotators agree on easy instances and independently flip a coin on hard ones.The joint distribution satisfies:P(Xi1= ...=Xik|li=E)=1; P(Xi1=b1, ...,Xik=bk|li=H)=?kj=1P(Xij=bj|li=H)We want to take only easy instances into the gold standard, so that it containsonly settled, trustworthy judgments.2 The problem is that the fact of being easy orhard is not directly observable, but has to be inferred from the observed annotations.In particular, some of the observed agreements will in fact be hard instances, sincecoin-flips could occasionally come out all-heads or all-tails.
Our objective is to estimate,with a given degree of confidence (?
), the proportion ?
of hard instances in the agreedannotations, based on the number of observed disagreements.
The value of ?
is the levelof annotation noise in the gold standard comprising agreed annotations.Let p be the probability that the annotators agree on a hard instance in a binaryclassification task:p=P(Xi1= ...=Xik|li=H)=?kj=1P(Xij=0|li=H)+?kj=1P(Xij=1|li=H)Denote by Ad the event that there are d disagreed instances; these are hard, and areassumed to be labeled by coin-flips.
Let Bh be the event that there are overall h hard2 On the status of hard instances, see Section 5.1.496Beigman Klebanov and Beigman From Annotator Agreement to Noise Modelsinstances; some of these may be unobserved as they surface as random agreements.
Wenote that P(Ad|Bh) =(hd)?
(1?
p)d ?
ph?d for d ?
h, hence:P(Bh|Ad) =P(Ad ?
Bh)P(Ad)=P(Ad|Bh) ?
P(Bh)?ni=d P(Ad|Bi) ?
P(Bi)=(hd)?
ph?d ?
P(Bh)?ni=d(id)?
pi?d ?
P(Bi)Let X be a random variable designating the number of coin-flips.
It follows thatP(X > t|Ad) =?ni=t+1(id)?
pi?d ?
P(Bi)?ni=d(id)?
pi?d ?
P(Bi)(1)Let t0 be the smallest integer for which P(X > t0|Ad) < 1?
?.
Given d observed dis-agreements, we estimate the noise level of the agreed subset of the annotations as atmost ?
= t0?dn?d , with confidence ?.3.
Relation to ?
Statistic3.1 The Case of High ?
with Two AnnotatorsSuppose 1,000 instances have been annotated by two people, such that 900 are instancesof agreement.
Both in the 900 agreed instances and in the 100 disagreed ones, thecategories were estimated to be equiprobable for both annotators.3 In this case p = 0.5,?
= 0.8,4 which is usually taken to be an indicator of sufficiently agreeable guidelines,and, by implication, of a high quality data set.
Our candidate gold standard is the 900instances of agreement.
What is its 95% confidence noise rate?We find, using ourmodel,that with more than 5% probability up to 125 agreements are due to coin-flipping, hence?
= 13.8%.5 This scenario is not hypothetical.
In Poesio and Vieira (1998) Experiment 1,the classification of definite descriptions into Anaphoric-or-Associative versus Unfa-miliar has n = 992, d = 121, p = 0.47, which, with 95% confidence, yields ?
= 15%.Let us reverse the question: For a two-annotator project with 1,000 instances, howmany disagreements could we tolerate, so that the agreed part is 95% noise-free with95% confidence?
Only 33 disagreements, corresponding to ?
= 0.93.
In practice, thismeans that a two-annotator project of this size is unlikely to produce a high-qualitygold standard, the high ?
notwithstanding.3.2 The Case of Low ?
with Five AnnotatorsSuppose now 1,000 instances are annotated by five people, with 660 agreements.
Withcategories equiprobable in both hard and easy instances, p = 0.0625.
The exact valueof ?
depends on the distribution of votes in the 340 disagreed cases, from ?
= 0.73when all disagreements are split 4-to-1, to ?
= 0.52 when all disagreements are split3-to-2.
Assuming disagreements are coin-flips, the most likely measurement would beabout ?
= 0.637, where the 340 observed coin-flips yielded the most likely pattern.6 Thisvalue of ?
is considered low, yet the 660 agreed items make a gold standard within the3 We estimate P(Xij=1|li=H) by the proportion of disagreed instances that annotator j put in category 1.4 For calculating ?, we use the version shown in Equation (2).5 In all our calculations P(B1) = ...=P(Bn ), that is, a priori, any number of hard instances is equiprobable.6 That is, there are twice as many 3-to-2 cases than 4-to-1, corresponding to(53)as opposed to(54).497Computational Linguistics Volume 35, Number 4noise rate of ?
= 5% with 95% confidence, according to our model.
Hence it is possiblefor the overall annotation to have low-ish ?, but the agreement of all five annotators,if observed sufficiently frequently, is reliable, and can be used to build a clean goldstandard.3.3 Interpreting the ?
Statistic in the Annotation Generation ModelThe ?
statistic is defined as ?
= PA?PE1?PE where PA is the observed agreement and PE is theagreement expected by chance, calculated from the marginals.
We use the Siegel andCastellan (1988) version, referred to as K in Artstein and Poesio (2008):PE =m?j=1p2j ; pj =?ni=1 aijnk; PA =1nn?i=1PAi ; PAi =?mj=1(aij2)(k2) (2)where n is the number of items; m is the number of categories; k is the number of anno-tators; and aij is the number of annotators who assigned the ith item to the jth category.Suppose there are h hard instances and e easy ones, and m = 2.
Suppose furtherthat all annotators flip the same coin on hard instances, and that the distribution of thecategories in easy and hard instances is the same and is given by q1, .
.
.
, qm.
Then theprobability for chance agreement between two annotators is q =?mj=1 q2j , of which PE isan estimator.
Agreement on a particular instance PAi is measured by the proportionof agreeing pairs of annotators out of all such pairs, and PA is an estimator of theexpected agreement across all instances.
Our model assumes perfect agreement on easyinstances and agreement with probability q on hard ones, so we expect to see e+q?hagreed instances, hence PA is an estimator ofe+qhe+h .
Putting these together, ?=PA?PE1?PEis an estimator ofe+qhe+h?q1?q =ee+h , the proportion of easy instances.7 In fact, Aickin (1990)shows that ?
is very close to this ratio when themarginal distribution over the categoriesis uniform, with a more substantial divergence for skewed category distributions.8The correspondence between ?
and the proportion of easy instances makes it clearwhy ?
is not a sufficient indicator of data quality for benchmarking.
For when ?
= 0.8,20% of the data are hard cases.
Using all data, especially for testing, is thus potentiallyhazardous, and the crucial question is: Can we zero in on the easy instances effectively,without admitting much noise?
This is exactly the question answered by the model.When the distribution of categories is the same in easy and hard instances anduniform, ?
can be used to address this question as well.
Recall that in the two-annotatorcase in Section 3.1, ?
= 0.8, that is, 80% of instances are estimated to be easy.
Becauseeasy cases are a subset of agreed ones in our model, 800 of the agreed 900 instances areeasy, giving an estimate of 11% noise in the gold standard.
Requiring 95% confidence innoise estimation, we found ?
= 13.8%, using our model.
Similarly, in the five-annotator7 The proportion of easy cases is positive, whereas the estimator ?
can be negative with non-negligibleprobability when e = O(?h).8 In Aickin (1990), category distribution on easy cases is derived from that in the hard cases.
The closer thecategories are to uniform distribution in the hard cases, the closer their distribution in hard cases is tothat in easy cases.
For example, if the categories are distributed uniformly in hard cases, they are also sodistributed in the easy ones.
If the categories are distributed ( 13 ,23 ) in the hard cases, they are distributed( 15 ,45 ) in the easy cases.
For this reason, in Aickin?s model, it is not possible to distinguish betweencategory imbalance (many more 0s than 1s) and differences in category distributions in easy and hardcases.
His simulations show that in cases of category imbalance (which imply, in his model, differences incategory distributions in easy and hard cases), ?
tends to underestimate the proportion of easy instances.498Beigman Klebanov and Beigman From Annotator Agreement to Noise Modelsscenario in Section 3.2, ?
= 0.637 tells us that about 637 out of 1,000 instances are easy;they are captured quite precisely by the 660 agreements, yielding a noise estimate of3.5%, again somewhat lower than the high confidence one we gave using the model.4.
Training and Testing in the Presence of Annotation NoiseWe discuss two uses of a gold standard within the benchmarking enterprise.
The datacould be used for testing, and, if there is enough of it and after an appropriate partition,for training as well.
We consider each case separately in the following sections.4.1 Testing with Annotation NoiseThe two questions one wants to answer using the data are: Howwell does an algorithmcapture the phenomenon?
For any two algorithms, which one is better?
Consider thealgorithm comparison situation.
Suppose we have a gold standard with L items ofwhich up to R are noise (?=RL ).
Two algorithms might differ in performance on the easycases, the hard ones, or both.
Because we cannot distinguish between easy and hardinstances in the gold standard, we are unable to attribute the difference in performancecorrectly.
Moreover, as the annotations of the hard instances are random coin-flips, thereis an expected difference in performance that is a result of pure chance.Suppose two algorithms perform equally well on easy instances; their performanceon the hard ones is as good as agreement-by-coin-flipping would allow.
Thus, thedifference in the number of ?correct?
answers on hard instances for algorithms A andB is a random variable S satisfying S =?Ri=1 Xi where X1, .
.
.
,XR are independent andidentically distributed random variables which obtain values?1 (A ?right?, B ?wrong?
)and 1 (A ?wrong?, B ?right?)
with probability 14 and 0 with probability12 , thus ?S = 0;?S =?R2 .
By Chebyshev?s inequality Pr(|S| > k?)
?1k2: that is, the chance differencebetween the algorithms will be within 4.5?
with 95% probability.9 In our example, L =900 and R = 125, hence a difference of up to 35 ?correct?
answers (3.9% of the goldstandard) can be attributed to chance.10This example shows that even if getting a clean data set is not feasible, it is impor-tant to report the noise rate of the data set that has been produced.
This would allowcalibrating the benchmarking procedure by requiring the difference between the twocompeting algorithms to be larger than the chance difference scale.Some perils of testing on noisy data were discussed in a recent article in this journalby Reidsma and Carletta (2008).
They showed that a machine-learning classifier issensitive to the type of noise in the data.
Specifically, if the noise is in the form ofcategory over-use (an annotator disproportionately favors a certain category), whenalgorithm performance is measured against the noisy data, accuracy estimates are ofteninflated relative to performance on the real data, uncorrupted by noise (see Figure 3(b)therein).
This is because ?when the observed data is used to test performance, some of9 For large R, normal approximation can be used with the tighter 2?
bound for 95% confidence.10 We note that because the difference attributable to coin-flipping is O(?
?L ), and assuming noise rateis constant, the scale of chance difference diminishes with larger data sets (see also footnote 9).The issue is more important when dealing with small-to-moderate data sets.
However, even fora 130K test set (Sections 22?24 of the Wall Street Journal corpus, standardly used as a test set inPOS-tagging benchmarks), it is useful to know the estimated noise rate, as it is not clear that allreported improvements in performance would come out significant.
For example, Shen, Satta, andJoshi (2007) summarize performance of five previously published and three newly reported algorithms,all between 97.10% and 97.33%.499Computational Linguistics Volume 35, Number 4the samples match not because the classifier gets the label right, but because it overusesthe same label as the human coder?
(Reidsma and Carletta 2008, page 232).
On theother hand, if disagreements are random classification noise (the label of any instancecan be flipped with a certain probability), a performance estimate based on observeddata would often be lower than performance on the real data, because the noise thatcorrupted it was ignored by the classifier (see Figure 2(d) therein).Reidsma and Carletta (2008) suggest that the community develops methods toinvestigate the patterns of disagreements between annotators to gain insight into the po-tential of incorrect performance estimation.
Althoughwe agree on the general point thathuman agreements and disagreements should bear directly on the practice of estimatingthe performance of an algorithm, we focus on improving the quality of performanceestimation.
We suggest (1) mitigating the effect of annotation noise on performanceestimation by using the least noisy part of the data set for testing, that is, a gold standardwith agreed items; (2) providing an estimate of the level of noise in the gold standard,which can be used to gauge the divergence between the estimate of performance usingthe gold standard from the real performance figure on the easy instances (i.e., on noise-free data), similarly to the algorithm comparison scenario provided herein.4.2 Learning with Annotation NoiseThe problem with noise in the training data is the potential for misclassification of easyinstances in the test data as a result of hard instances in the training data, the problemwe call hard case bias.Learning in the presence of noise is an active research area in machine learning.However, annotation noise is different from existing well-understood noise models.Specifically, random classification noise, where each instance has the same probability ofhaving its label flipped, is known to be tolerable in supervised learning (Blum et al 1996;Cohen 1997; Reidsma and Carletta 2008).
In annotation noise, coin-flipping is confinedto hard instances, which should not be assumed to be uniformly distributed across thefeature space.
Indeed, there is reason to believe that they form clusters; certain featurecombinations tend to give rise to hard instances.
The finding reported by Reidsma andop den Akker (2008) that a classifier trained on data from one annotator tended to agreemuch better with test data from the same annotator than with that of another annotatorexemplifies a situation where observed hard cases (i.e., cases where the annotatorsdisagree) constitute a pattern in the feature space that a classifier picks up.In a separate article, we establish a number of properties of learning under anno-tation noise (Beigman and Beigman Klebanov 2009).
We show that the 0-1 loss modelmay be vulnerable to annotation noise for small data sets, but becomes increasinglyrobust the larger the data set, with worst-case hard case bias of ?
( 1?n).
We also showthat learning with the popular voted-perceptron algorithm (Freund and Schapire 1999)could suffer a constant rate of hard case bias irrespective of the size of the data set.5.
Discussion5.1 The Status of Hard InstancesWe suggested that only the easy instances should be taken into the gold standard.
Thisis not to say that hard cases should be eliminated from the researcher?s attention; wemerely argue that they should not be used for testing algorithms for benchmarking500Beigman Klebanov and Beigman From Annotator Agreement to Noise Modelspurposes.
Hard cases are interesting for theory development, because this is where thetheory might have a difficulty, but they do not allow for a fair comparison, as theircorrect label cannot be determined under the current theory.
The agreed data embodiesthe well-articulated parts of the theory, which are ready for deployment as a goldstandard for machine learning.
Once the theory is improved to a stage where some ofthe previously hard cases receive an unproblematic treatment, those items can be addedto the data set, which can make the task more challenging for the machine.
Linguistictheories-in-the-making can have limited coverage; they do not immediately attain thestatus of medical conditions, for example, where there presumably exists a true labeleven for the hardest-to-diagnose cases.115.2 Plausibility of the ModelBeyond the separation into easy and hard instances, our model prescribes certain an-notator behavior for each type.
In our work on metaphor, we observed that certainmetaphor markups were retracted by their authors, when asked after 4?8 weeks torevisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008).
These wereapparently hard cases, with people resolving their doubts inconsistently on the twooccasions; coin-flipping is a reasonable first-cut model for such cases.
The model alsoaccommodates category over-use bias (Di Eugenio and Glass 2004; Artstein and Poesio2008; Reidsma and Carletta 2008), as P(Xij=bj|li=H) may vary across annotators.Still, this model is clearly a simplification.
For example, it is possible that thereis more than one degree of hardness, and annotator behavior changes accordingly.Another extension is modeling imperfect annotators, allowed to commit random errorson easy cases; this extension would be needed if a large number of annotators is used.Such extensions, as well as methods for estimating these more complex models,should clearly be put on the community?s research agenda.
The main contributionof the simple model is in outlining the trajectory from agreement to gold standardwith a noise estimate, and indicating the potential benefit of the latter to data uti-lization (low overall agreement does not preclude the existence of a reliable subset)and to prudent benchmarking.
Furthermore, the simple model helps us improve theunderstanding of the information provided by the ?
statistic, and to appreciate itslimitations.
It also allows us to see the benefit of adding annotators, as discussed in thenext section.5.3 Adding AnnotatorsIf we want the test data to be able to detect small advances in machines?
handling ofthe task, we need to produce gold standards with low noise levels.
The level of noisein agreed data depends on two parameters: (a) the number of agreed items, and (b) theprobability of chance agreement between annotators.
Although the first is not underthe researcher?s control once the data set is chosen, the second is, by changing thenumber of annotators.
Obviously, the more annotators are required to agree, the lowerp will be, and the smaller the number of agreements that can be attributed to coin-flipping.
If indeed 800 out of 1,000 items are easy, agreement between two annotatorscan only detect them with up to 13.8% noise.
Adding a third annotator means p = 0.25.11 As one of the anonymous reviewers pointed out, some medical conditions, such as autism, are also onlypartially understood.501Computational Linguistics Volume 35, Number 4We are most likely to observe 850 agreed instances, which would not contain morethan 7.7% noise, with 95% confidence.
Effectively, we got rid of about half the randomagreements.AcknowledgmentsWe thank Eli Shamir and Bei Yu for readingearlier drafts of this article, as well as theeditor and the anonymous reviewers forcomments that helped us improve thearticle significantly.ReferencesAickin, Mikel.
1990.
Maximum likelihoodestimation of agreement in the constantpredictive probability model, and itsrelation to Cohen?s kappa.
Biometrics,46(2):293?302.Albert, Paul and Lori Dodd.
2004.
Acautionary note on the robustness of latentclass models for estimating diagnosticerror without a gold standard.
Biometrics,60(2):427?435.Albert, Paul, Lisa McShane, and Joanna Shih.2001.
Latent class modeling approachesfor assessing diagnostic error without agold standard: With applications to p53immunohistochemical assays in bladdertumors.
Biometrics, 57(2):610?619.Artstein, Ron and Massimo Poesio.
2008.Inter-coder agreement for computationallinguistics.
Computational Linguistics,34(4):555?596.Beigman, Eyal and Beata Beigman Klebanov.2009.
Learning with annotation noise.
InProceedings of the 47th Annual Meeting of theAssociation for Computational Linguistics,Singapore.Beigman Klebanov, Beata, Eyal Beigman,and Daniel Diermeier.
2008.
Analyzingdisagreements.
In COLING 2008 Workshopon Human Judgments in ComputationalLinguistics, pages 2?7, Manchester.Blum, Avrim, Alan Frieze, Ravi Kannan,and Santosh Vempala.
1996.
Apolynomial-time algorithm for learningnoisy linear threshold functions.
InProceedings of the 37th Annual IEEESymposium on Foundations of ComputerScience, pages 330?338, Burlington, VT.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):249?254.Cohen, Edith.
1997.
Learning noisyperceptrons by a perceptron in polynomialtime.
In Proceedings of the 38th AnnualSymposium on Foundations of ComputerScience, pages 514?523, Miami Beach, FL.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20(1):37?46.Dagan, Ido, Oren Glickman, and BernardoMagnini.
2006.
The PASCAL recognisingtextual entailment challenge.
In ThePASCAL Recognising Textual EntailmentChallenge, Springer, Berlin, pages 177?190.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Freund, Y. and R. E. Schapire.
1999.
Largemargin classification using the perceptronalgorithm.Machine Learning, 37(3):277?296.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Girju, Roxana, Adriana Badulescu, and DanMoldovan.
2006.
Automatic discoveryof part-whole relations.
ComputationalLinguistics, 32(1):83?135.Hui, Siu and Xiao Zhou.
1998.
Evaluation ofdiagnostic tests without gold standards.Statistical Methods in Medical Research,7(4):354?370.Lapata, Maria.
2002.
The disambiguation ofnominalizations.
Computational Linguistics,28(3):357?388.Litman, Diane, Julia Hirschberg, and MarcSwerts.
2006.
Characterizing andpredicting corrections in spoken dialoguesystems.
Computational Linguistics,32(3):417?438.Markert, Katja and Malvina Nissim.
2002.Metonymy resolution as a classificationtask.
In Proceedings of the Empirical Methodsin Natural Language Processing Conference,pages 204?213, Philadelphia, PA.Palmer, Martha, Paul Kingsbury, and DanielGildea.
2005.
The proposition bank: Anannotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.Poesio, Massimo and Renata Vieira.
1998.A corpus-based investigation of definitedescription use.
Computational Linguistics,24(2):183?216.Reidsma, Dennis and Jean Carletta.
2008.Reliability measurement without limit.Computational Linguistics, 34(3):319?326.Reidsma, Dennis and Rieks op den Akker.2008.
Exploiting subjective annotations.In COLING 2008 Workshop on HumanJudgments in Computational Linguistics,pages 8?16, Manchester.502Beigman Klebanov and Beigman From Annotator Agreement to Noise ModelsShen, Libin, Giorgio Satta, and AravindJoshi.
2007.
Guided learning forbidirectional sequence classification.
InProceedings of the 45th Annual Meetingof the Association of ComputationalLinguistics, pages 760?767, Prague.Siegel, Sidney and N. John Castellan Jr.1988.
Nonparametric Statistics for theBehavioral Sciences.
McGraw-Hill,2nd edition.Snyder, Benjamin and Martha Palmer.
2004.The English all-words task.
In Senseval-3:3rd International Workshop on the Evaluationof Systems for the Semantic Analysis of Text,pages 41?43, Barcelona.Vieira, Renata and Massimo Poesio.
2000.
Anempirically based system for processingdefinite descriptions.
ComputationalLinguistics, 26(4):539?593.Wiebe, Janyce, Teresa Wilson, and ClaireCardie.
2005.
Annotating expressions ofopinions and emotions in language.Language Resources and Evaluation,39(2):165?210.503
