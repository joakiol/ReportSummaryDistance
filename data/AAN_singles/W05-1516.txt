Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 152?159,Vancouver, October 2005. c?2005 Association for Computational LinguisticsStrictly Lexical Dependency ParsingQin Iris Wang and Dale Schuurmans Dekang LinDepartment of Computing Science Google, Inc.University of Alberta 1600 Amphitheatre ParkwayEdmonton, Alberta, Canada, T6G 2E8 Mountain View, California, USA, 94043{wqin,dale}@cs.ualberta.ca lindek@google.comAbstractWe present a strictly lexical parsingmodel where all the parameters are basedon the words.
This model does not relyon part-of-speech tags or grammaticalcategories.
It maximizes the conditionalprobability of the parse tree given thesentence.
This is in contrast with mostprevious models that compute the jointprobability of the parse tree and the sen-tence.
Although the maximization ofjoint and conditional probabilities aretheoretically equivalent, the conditionalmodel allows us to use distributionalword similarity to generalize the ob-served frequency counts in the trainingcorpus.
Our experiments with the Chi-nese Treebank show that the accuracy ofthe conditional model is 13.6% higherthan the joint model and that the strictlylexicalized conditional model outper-forms the corresponding unlexicalizedmodel based on part-of-speech tags.1 IntroductionThere has been a great deal of progress in statisti-cal parsing in the past decade (Collins, 1996;Collins, 1997; Chaniak, 2000).
A common charac-teristic of these parsers is their use of lexicalizedstatistics.
However, it was discovered recently thatbi-lexical statistics (parameters that involve twowords) actually played much smaller role thanpreviously believed.
It was found in (Gildea,2001) that the removal of bi-lexical statistics froma state-of-the-art PCFG parser resulted very smallchange in the output.
Bikel (2004) observed thatthe bi-lexical statistics accounted for only 1.49%of the bigram statistics used by the parser.
Whenconsidering only bigram statistics involved in thehighest probability parse, this percentage becomes28.8%.
However, even when the bi-lexical statis-tics do get used, they are remarkably similar totheir back-off values using part-of-speech tags.Therefore, the utility of bi-lexical statistics be-comes rather questionable.
Klein and Manning(2003) presented an unlexicalized parser thateliminated all lexicalized parameters.
Its perform-ance was close to the state-of-the-art lexicalizedparsers.We present a statistical dependency parser thatrepresents the other end of spectrum where allstatistical parameters are lexical and the parserdoes not require part-of-speech tags or grammati-cal categories.
We call this strictly lexicalizedparsing.A part-of-speech lexicon has always been con-sidered to be a necessary component in any natu-ral language parser.
This is true in early rule-basedas well as modern statistical parsers and in de-pendency parsers as well as constituency parsers.The need for part-of-speech tags arises from thesparseness of natural language data.
They providegeneralizations of words that are critical for pars-ers to deal with the sparseness.
Words belongingto the same part-of-speech are expected to havethe same syntactic behavior.Instead of part-of-speech tags, we rely on dis-tributional word similarities computed automati-cally from a large unannotated text corpus.
One ofthe benefits of strictly lexicalized parsing is that152fundsinvestors continue  to  pour cash into moneyMany?0 1 2 3 4 5 6 7 8 9the parser can be trained with a treebank that onlycontains the dependency relationships betweenwords.
The annotators do not need to annotateparts-of-speech or non-terminal symbols (theydon?t even have to know about them), making theconstruction of the treebank easier.Strictly lexicalized parsing is especially benefi-cial for languages such as Chinese, where parts-of-speech are not as clearly defined as English.
InChinese, clear indicators of a word's part-of-speech such as suffixes -ment, -ous or functionwords such as the, are largely absent.
In fact,monolingual Chinese dictionaries that are mainlyintended for native speakers almost never containpart-of-speech information.In the next section, we present a method formodeling the probabilities of dependency trees.Section 3 applies similarity-based smoothing tothe probability model to deal with data sparseness.We then present experimental results with theChinese Treebank in Section 4 and discuss relatedwork in Section 5.2 A Probabilistic Dependency ModelLet S be a sentence.
The dependency structure Tof S is a directed tree connecting the words in S.Each link in the tree represents a dependency rela-tionship between two words, known as the headand the modifier.
The direction of the link is fromthe head to the modifier.
We add an artificial rootnode (?)
at the beginning of each sentence and adependency link from ?
to the head of the sen-tence so that the head of the sentence can betreated in the same way as other words.
Figure 1shows an example dependency tree.We denote a dependency link l by a triple (u, v,d), where u and v are the indices (u < v) of thewords connected by l, and d specifies the directionof the link l. The value of d is either L or R. If d =L, v is the index of the head word; otherwise, u isthe index of the head word.Dependency trees are typically assumed to beprojective (without crossing arcs), which meansthat if there is an arc from h to m, h is an ancestorof all the words between h and m. Let F(S) be theset of possible directed, projective trees spanningon S. The parsing problem is to find( ) ( )STPSFT |maxarg ?Generative parsing models are usually definedrecursively from top down, even though the de-coders (parsers) for such models almost alwaystake a bottom-up approach.
The model proposedhere is a bottom-up one.
Like previous ap-proaches, we decompose the generation of a parsetree into a sequence of steps and define the prob-ability of each step.
The probability of the tree issimply the product of the probabilities of the stepsinvolved in the generation process.
This schemerequires that different sequences of steps must notlead to the same tree.
We achieve this by defininga canonical ordering of the links in a dependencytree.
Each generation step corresponds to the con-struction of a dependency link in the canonicalorder.Given two dependency links l and l' with theheads being h and h' and the modifiers being mand m', respectively, the order between l and l' aredetermined as follows:?
If h ?
h' and there is a directed path from one(say h) to the other (say h?
), then l?
precedes l.?
If h ?
h' and there does not exist a directed pathbetween h and h?, the order between l and l?
isdetermined by the order of h and h?
in the sen-tence (h precedes h?
?
l precedes l?).?
If h = h' and the modifiers m and m?
are on dif-ferent sides of h, the link with modifier on theright precedes the other.?
If h = h' and the modifiers m and m?
are on thesame side of the head h, the link with its modi-fier closer to h precedes the other one.Figure 1.
An Example Dependency Tree.153For example, the canonical order of the links inthe dependency tree in Figure 1 is: (1, 2, L), (5, 6,R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4,R), (2, 3, L), (0, 3, L).The generation process according to the ca-nonical order is similar to the head outward gen-eration process in (Collins, 1999), except that it isbottom-up whereas Collins?
models are top-down.Suppose the dependency tree T is constructed insteps G1, ?, GN in the canonical order of the de-pendency links, where N is the number of wordsin the sentence.
We can compute the probabilityof T as follows:( )( )( )?
= ?==Ni iiNGGSGPSGGGPSTP1 1121,...,,||,...,,|Following (Klein and Manning, 2004), we re-quire that the creation of a dependency link fromhead h to modifier m be preceded by placing a leftSTOP and a right STOP around the modifier mand ?STOP between h and m.Let LwE  (andRwE ) denote the event that thereare no more modifiers on the left (and right) of aword w. Suppose the dependency link created inthe step i is (u, v, d).
If d = L, Gi is the conjunc-tion of the four events: RuE ,LuE ,LvE?
andlinkL(u, v).
If d = R, Gi consists of four events:LvE ,RvE ,RuE?
and linkR(u, v).The event Gi is conditioned on 11,...,, ?iGGS ,which are the words in the sentence and a forest oftrees constructed up to step i-1.
Let LwC  (andRwC )be the number of modifiers of w on its left (andright).
We make the following independence as-sumptions:?
Whether there is any more modifier of w onthe d side depends only on the number ofmodifiers already found on the d side of w.That is, dwE  depends only on w anddwC .?
Whether there is a dependency link from aword h to another word m depends only on thewords h and m and the number of modifiers ofh between m and h. That is,o linkR(u,v) depends only on u, v, and RuC .o linkL(u,v) depends only on u, v, and LvC .Suppose Gi corresponds to a dependency link (u,v, L).
The probability ( )11,...,,| ?ii GGSGP  can becomputed as:( )( )( )( ) ( )( )( ) ( )( )LvLLvLvRuRuLuLuiLLvRuLuiiCvuvulinkPCvEPCuEPCuEPGGSvulinkEEEPGGSGP,,|,,|1,|,|,...,,|,,,,,...,,|1111???
?=?= ?
?The events RwE  andLwE  correspond to theSTOP events in (Collins, 1999) and (Klein andManning, 2004).
They are crucial for modelingthe number of dependents.
Without them, theparse trees often contain some ?obvious?
errors,such as determiners taking arguments, or preposi-tions having arguments on their left (instead ofright).Our model requires three types of parameters:?
( )dwdw CwEP ,| , where w is a word, d is a di-rection (left or right).
This is the probability ofa STOP after taking dwC  modifiers on the dside.?
( )( )RuR CvuvulinkP ,,|,  is the probability of vbeing the ( 1+RuC )?th modifier of u on theright.?
( )( )LvL CvuvulinkP ,,|,  is the probability of ubeing the ( 1+LvC )?th modifier of v on theleft.The Maximum Likelihood estimations of theseparameters can be obtained from the frequencycounts in the training corpus:?
C(w, c, d): the frequency count of  w with cmodifiers on the d side.?
C(u, v, c, d): If d = L, this is the frequencycount words u and v co-occurring in a sen-tence and v has c modifiers between itself andu.
If d = R, this is the frequency count words uand v co-occurring in a sentence and u has cmodifiers between itself and v.?
K(u, v, c, d): similar to C(u, v, c, d) with anadditional constraint that linkd(u, v) is true.154( ) ( )( )?
?=ccdwdw dcwCdcwCCwEP',',,,,|  , where c = dwC ;( )( ) ( )( )RcvuC RcvuKCvuvulinkP RuR ,,, ,,,,,|, = ,where  c = RuC ;( )( ) ( )( )LcvuC LcvuKCvuvulinkP LvL ,,, ,,,,,|, = ,where  c = LvC .We compute the probability of the tree condi-tioned on the words.
All parameters in our modelare conditional probabilities where the left sides ofthe conditioning bar are binary variables.
In con-trast, most previous approaches compute jointprobability of the tree and the words in the tree.Many of their model parameters consist of theprobability of a word in a given context.We use a dynamic programming algorithmsimilar to chart parsing as the decoder for thismodel.
The algorithm builds a packed parse forestfrom bottom up in the canonical order of theparser trees.
It attaches all the right children be-fore attaching the left ones to maintain the canoni-cal order as required by our model.3 Similarity-based Smoothing3.1 Distributional Word SimilarityWords that tend to appear in the same contextstend to have similar meanings.
This is known asthe Distributional Hypothesis in linguistics (Harris,1968).
For example, the words test and exam aresimilar because both of them follow verbs such asadminister, cancel, cheat on, conduct, ... and both ofthem can be preceded by adjectives such as aca-demic, comprehensive, diagnostic, difficult, ...Many methods have been proposed to computedistributional similarity between words (Hindle,1990; Pereira et al, 1993; Grefenstette, 1994; Lin,1998).
Almost all of the methods represent a wordby a feature vector where each feature corre-sponds to a type of context in which the word ap-peared.
They differ in how the feature vectors areconstructed and how the similarity between twofeature vectors is computed.We define the features of a word w to be the setof words that occurred within a small context win-dow of w in a large corpus.
The context windowof an instance of w consists of the closest non-stop-word on each side of w and the stop-words inbetween.
In our experiments, the set of stop-wordsare defined as the top 100 most frequent words inthe corpus.
The value of a feature w' is defined asthe point-wise mutual information between the w'and w:( ) ( )( ) ( )????????
?='',log',wPwPwwPwwPMIwhere P(w, w?)
is the probability of w and w?
co-occur in a context window.The similarity between two vectors is computedas the cosine of the angle between the vectors.The following are the top similar words for theword keystone obtained from the English Giga-word Corpus:centrepiece 0.28, figment 0.27, fulcrum 0.21, culmi-nation 0.20, albatross 0.19, bane 0.19, pariahs 0.18,lifeblood 0.18, crux 0.18, redoubling 0.17, apotheo-sis 0.17, cornerstones 0.17, perpetuation 0.16, fore-runners 0.16, shirking 0.16, cornerstone 0.16,birthright 0.15, hallmark 0.15, centerpiece 0.15, evi-denced 0.15, germane 0.15, gist 0.14, reassessing0.14, engrossed 0.14, Thorn 0.14, biding 0.14, nar-rowness 0.14, linchpin 0.14, enamored 0.14, formal-ised 0.14, tenths 0.13, testament 0.13, certainties0.13, forerunner 0.13, re-evaluating 0.13, antithetical0.12, extinct 0.12, rarest 0.12, imperiled 0.12, remiss0.12, hindrance 0.12, detriment 0.12, prouder 0.12,upshot 0.12, cosponsor 0.12, hiccups 0.12, premised0.12, perversion 0.12, destabilisation 0.12, prefaced0.11, ?
?3.2 Similarity-based SmoothingThe parameters in our model consist of condi-tional probabilities P(E|C) where E is the binaryvariable linkd(u, v) or dwE  and the context C iseither [ ]dwCw,  or [ ]dwCvu ,, , which involves oneor two words in the input sentence.
Due to thesparseness of natural language data, the contextsobserved in the training data only covers a tinyfraction of the contexts whose probability distri-bution are needed during parsing.
The standardapproach is to back off the probability to wordclasses (such as part-of-speech tags).
We havetaken a different approach.
We search in the train-155ing data to find a set of similar contexts to C andestimate the probability of E based on its prob-abilities in the similar contexts that are observedin the training corpus.Similarity-based smoothing was used in (Daganet al, 1999) to estimate word co-occurrence prob-abilities.
Their method performed almost 40%better than the more commonly used back-offmethod.
Unfortunately, similarity-based smooth-ing has not been successfully applied to statisticalparsing up to now.In (Dagan et al, 1999), the bigram probabilityP(w2|w1) is computed as the weighted average ofthe conditional probability of w2 given similarwords of w1.
( ) ( )( ) ( )( )?
?= 11' 121 1112 '|',|wSwMLESIM wwPwnormwwsimwwPwhere ( )11 ', wwsim  denotes the similarity (or anincreasing function of the similarity) between w1and w?1, S(w1) denote the set of words that aremost similar to w1 and norm(w1) is the normaliza-tion factor ( ) ( )( )?
?= 11' 111 ',wSw wwsimwnorm .The underlying assumption of this smoothingscheme is that a word is more likely to occur afterw1 if it tends to occur after similar words of w1.We make a similar assumption: the probabilityP(E|C) of event E given the context C is computedas the weight average of P(E|C?)
where C?
is asimilar context of C and is attested in the trainingcorpus:( ) ( )( ) ( )( )??
?= OCSC MLESIM CEPCnormCCsimCEP''|',|where S(C) is the set of top-K most similar con-texts of C (in the experiments reported in this pa-per, K = 50); O is the set of contexts observed inthe training corpus, sim(C,C?)
is the similaritybetween two contexts  and  norm(C) is the nor-malization factor.In our model, a context is either  [ ]dwCw,  or [ ]dwCvu ,, .
Their similar contexts are defined as:[ ]( ) [ ] ( ){ }[ ]( ) [ ]{ })('),(',',',,',', 'vSvuSuCvuCvuSwSwCwCwSdwdwdwdw?
?=?=where S(w) is the set of top-K similar words of w(K = 50).Since all contexts used in our model contain atleast one word, we compute the similarity be-tween two contexts, sim(C, C?
), as the geometricaverage of the similarities between correspondingwords:[ ] [ ]( ) ( )[ ] [ ]( ) ( ) ( )',',,',',,,',,',,''vvsimuusimCvuCvusimwwsimCwCwsimdwdwdwdw?==Similarity-smoothed probability is only neces-sary when the frequency count of the context C inthe training corpus is low.
We therefore computeP(E | C) = ?
PMLE(E | C) + (1 ?
?)
PSIM(E | C)where the smoothing factor5||1||++=CC?
and |C| isthe frequency count of the context C in the train-ing data.A difference between similarity-based smooth-ing in (Dagan et al, 1999) and our approach isthat our model only computes probability distribu-tions of binary variables.
Words only appear asparts of contexts on the right side of the condition-ing bar.
This has two important implications.Firstly, when a context contains two words, weare able to use the cross product of the similarwords, whereas (Dagan et al, 1999) can only usethe similar words of one of the words.
This turnsout to have significant impact on the performance(see Section 4).Secondly, in (Dagan et al, 1999), the distribu-tion P(?|w?1) may itself be sparsely observed.When ( )12 '| wwPMLE  is 0, it is often due to datasparseness.
Their smoothing scheme thereforetends to under-estimate the probability values.This problem is avoided in our approach.
If a con-text did not occur in the training data, we do notinclude it in the average.
If it did occur, theMaximum Likelihood estimation is reasonablyaccurate even if the context only occurred a fewtimes, since the entropy of the probability distri-bution is upper-bounded by log 2.4 Experimental ResultsWe experimented with our parser on the ChineseTreebank (CTB) 3.0.
We used the same data splitas (Bikel, 2004): Sections 1-270 and 400-931 as156the training set, Sections 271-300 as testing andSections 301-325 as the development set.
TheCTB contains constituency trees.
We convertedthem to dependency trees using the same methodand the head table as (Bikel, 2004).
Parsing Chi-nese generally involve segmentation as a pre-processing step.
We used the gold standard seg-mentation in the CTB.The distributional similarities between the Chi-nese words are computed using the Chinese Gi-gaword corpus.
We did not segment the Chinesecorpus when computing the word similarity.We measure the quality of the parser by the un-directed accuracy, which is defined as the numberof correct undirected dependency links divided bythe total number of dependency links in the corpus(the treebank parse and the parser output alwayshave the same number of links).
The results aresummarized in Table 1.
It can be seen that the per-formance of the parser is highly correlated withthe length of the sentences.Max Sentence Length 10 15 20 40Undirected Accuracy 90.8 85.6 84.0 79.9Table 1.
Evaluation Results on CTB 3.0We also experimented with several alternativemodels for dependency parsing.
Table 2 summer-izes the results of these models on the test corpuswith sentences up to 40 words long.One of the characteristics of our parser is that ituses the similar words of both the head and themodifier for smoothing.
The similarity-basedsmoothing method in (Dagan et al, 1999) uses thesimilar words of one of the words in a bigram.
Wecan change the definition of similar context asfollows so that only one word in a similar contextof C may be different from a word in C (seeModel (b) in Table 2):[ ]( )[ ]{ } [ ]{ })(',',)(',,' ,, vSvCvuuSuCvu CvuS dwdwdw??
?=where w is either v or u depending on whether d isL or R. This change led to a 2.2% drop in accuracy(compared with Model (a) in Table 2), which weattribute to the fact that many contexts do not havesimilar contexts in the training corpus.Since most previous parsing models maximizethe joint probability of the parse tree and the sen-tence P(T, S) instead of P(T | S),  we also imple-mented a joint model (see Model (c) in Table 2):( ) ( ) ( )( )( ) ( )?= ???
?=NidhiidhidhRmiRmLmiLmiiiiiiiiChmPChEPCmEPCmEPSTP1 ,|,|1,|,|,where hi and mi are the head and the modifier ofthe i'th dependency link.
The probability ( )iidhii ChmP ,|  is smoothed by averaging theprobabilities ( )iidhii ChmP ,'| , where h?i is a similarword of hi, as in (Dagan et al, 1999).
The resultwas a dramatic decrease in accuracy from the con-ditional model?s 79.9%.
to 66.3%.Our use of distributional word similarity canbe viewed as assigning soft clusters to words.
Incontrast, parts-of-speech can be viewed as hardclusters of words.
We can modify both the condi-tional and joint models to use part-of-speech tags,instead of words.
Since there are only a smallnumber of tags, the modified models used MLEwithout any smoothing except using a small con-stant as the probability of unseen events.
Withoutsmoothing, maximizing the conditional model isequivalent to maximizing the joint model.
Theaccuracy of the unlexicalized models (see Model(d) and Model (e) in Table 2) is 71.1% which isconsiderably lower than the strictly lexicalizedconditional model, but higher than the strictlylexicalized joint model.
This demonstrated thatsoft clusters obtained through distributional wordsimilarity perform better than the part-of-speechtags when used appropriately.Models Accuracy(a) Strictly lexicalized conditional model 79.9(b)   At most one word is different in a similar context 77.7(c)  Strictly lexicalized  joint model 66.3(d)  Unlexicalized conditional mod-els 71.1(e)  Unlexicalized joint models 71.1Table 2.
Performance of Alternative Models1575 Related WorkPrevious parsing models (e.g., Collins, 1997;Charniak, 2000) maximize the joint probabilityP(S, T) of a sentence S and its parse tree T. Wemaximize the conditional probability P(T | S).
Al-though they are theoretically equivalent, the use ofconditional model allows us to take advantage ofsimilarity-based smoothing.Clark et al (2002) also computes a conditionalprobability of dependency structures.
While theprobability space in our model consists of all pos-sible non-projective dependency trees, their prob-ability space is constrained to all the dependencystructures that are allowed by a CombinatorialCategory Grammar (CCG) and a category diction-ary (lexicon).
They therefore do not need theSTOP markers in their model.
Another major dif-ference between our model and (Clark et al,2002) is that the parameters in our model consistexclusively of conditional probabilities of binaryvariables.Ratnaparkhi?s maximum entropy model (Rat-naparkhi, 1999) is also a conditional model.
How-ever, his model maximizes the probability of theaction during each step of the parsing process,instead of overall quality of the parse tree.Yamada and Matsumoto (2002) presented a de-pendency parsing model using support vector ma-chines.
Their model is a discriminative model thatmaximizes the differences between scores of thecorrect parse and the scores of the top competingincorrect parses.In many dependency parsing models such as(Eisner, 1996) and (MacDonald et al, 2005), thescore of a dependency tree is the sum of the scoresof the dependency links, which are computed in-dependently of other links.
An undesirable conse-quence of this is that the parser often createsmultiple dependency links that are separatelylikely but jointly improbable (or even impossible).For example, there is nothing in such models toprevent the parser from assigning two subjects toa verb.
In the DMV model (Klein and Manning,2004), the probability of a dependency link ispartly conditioned on whether or not there is ahead word of the link already has a modifier.
Ourmodel is quite similar to the DMV model, exceptthat we compute the conditional probability of theparse tree given the sentence, instead of the jointprobability of the parse tree and the sentence.There have been several previous approaches toparsing Chinese with the Penn Chinese Treebank(e.g., Bikel and Chiang, 2000; Levy and Manning,2003).
Both of these approaches employed phrase-structure joint models and used part-of-speechtags in back-off smoothing.
Their results wereevaluated with the precision and recall of thebracketings implied in the phrase structure parsetrees.
In contrast, the accuracy of our model ismeasured in terms of the dependency relation-ships.
A dependency tree may correspond to morethan one constituency trees.
Our results are there-fore not directly comparable with the precisionand recall values in previous research.
Moreover,it was argued in (Lin 1995) that dependency basedevaluation is much more meaningful for the appli-cations that use parse trees, since the semanticrelationships are generally embedded in the de-pendency relationships.6 ConclusionTo the best of our knowledge, all previous naturallanguage parsers have to rely on part-of-speechtags.
We presented a strictly lexicalized model fordependency parsing that only relies on word sta-tistics.
We compared our parser with an unlexical-ized parser that employs the same probabilisticmodel except that the parameters are estimatedusing gold standard tags in the Chinese Treebank.Our experiments show that the strictly lexicalizedparser significantly outperformed its unlexicalizedcounter-part.An important distinction between our statisticalmodel from previous parsing models is that all theparameters in our model are conditional probabil-ity of binary variables.
This allows us to take ad-vantage of similarity-based smoothing, which hasnot been successfully applied to parsing before.AcknowledgementsThe authors would like to thank Mark Steedmanfor suggesting the comparison with unlexicalizedparsing in Section 4 and the anonymous reviewersfor their comments.
This work was supported inpart by NSERC, the Alberta Ingenuity Centre forMachine Learning and the Canada Research158Chairs program.
Qin Iris Wang was also sup-ported by iCORE Scholarship.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4), pp.
479-511.Daniel M. Bikel and David Chiang.
2000.
Two Statisti-cal Parsing Models applied to the Chinese Treebank.In Proceedings of the second Chinese LanguageProcessing Workshop, pp.
1-6.Eugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of the Second Meeting ofNorth American Chapter of Association for Compu-tational Linguistics (NAACL-2000), pp.
132-139.Stephen Clark, Julia Hockenmaier and Mark Steedman.2002.
Building Deep Dependency Structures with aWide-Coverage CCG Parser.
In Proceedings of the40th Annual Meeting of the ACL, pp.
327-334.Michael Collins.
1996.
A New Statistical Parser Basedon Bigram Lexical Dependencies.
In Proceedings ofthe 34th Annual Meeting of the ACL, pp.
184-191.Santa Cruz.Michael Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proceedings of the35th Annual Meeting of the ACL (jointly with the 8thConference of the EACL), pp.
16-23.
Madrid.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD Dissertation,University of Pennsylvania.Ido Dagan, Lillian Lee and Fernando Pereira.
1999.Similarity-based models of cooccurrence probabili-ties.
Machine Learning, Vol.
34(1-3) special issueon Natural Language Learning, pp.
43-69.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of COLING-96, pp.
340-345, Copenhagen.Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Proceedings of EMNLP-2001, pp.
167-202.
Pittsburgh, PA.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Press, Bos-ton, MA.Zelig S. Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Donald Hindle.
1990.
Noun Classification from Predi-cate-Argument Structures.
In Proceedings of ACL-90, pp.
268-275.
Pittsburg, Pennsylvania.Dan Klein and Chris Manning.
2002.
Fast exact infer-ence with a factored model for natural languageparsing.
In Proceedings of Neural InformationProcessing Systems.Dan Klein and Chris Manning.
2003.
Accurate Unlexi-calized Parsing.
In Proceedings of the 41st AnnualMeeting of the ACL, pp.
423-430.Dan Klein and Chris Manning.
2004.
Corpus-BasedInduction of Syntactic Structure: Models of De-pendency and Constituency.
In Proceedings of the42nd Annual Meeting of the ACL, pp.
479-486.Roger Levy and Chris Manning.
2003.
Is it harder toparse Chinese, or the Chinese Treebank?
In Pro-ceedings of the 41st Annual Meeting of the ACL, pp.439-446.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof IJCAI-95, pp.1420-1425.Dekang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceeding of COLING-ACL98, pp.
768-774.
Montreal, Canada.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL-2005, pp.91-98.Fernando Pereira, Naftali Z. Tishby, and Lillian Lee.1993.
Distributional clustering of English words.
InProceedings of ACL-1993, pp.
183-190, Columbus,Ohio.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal Dependency Analysis with Support Vector Ma-chines.
In Proceedings of the 8th InternationalWorkshop on Parsing Technologies, pp.195-206.159
