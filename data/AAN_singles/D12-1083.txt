Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 904?915, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Novel Discriminative Framework for Sentence-Level Discourse AnalysisShafiq Joty and Giuseppe Carenini and Raymond T. Ng{rjoty, carenini, rng}@cs.ubc.caDepartment of Computer ScienceUniversity of British ColumbiaVancouver, BC, V6T 1Z4, CanadaAbstractWe propose a complete probabilistic discrim-inative framework for performing sentence-level discourse analysis.
Our framework com-prises a discourse segmenter, based on a bi-nary classifier, and a discourse parser, whichapplies an optimal CKY-like parsing algo-rithm to probabilities inferred from a DynamicConditional Random Field.
We show on twocorpora that our approach outperforms thestate-of-the-art, often by a wide margin.1 IntroductionAutomatic discourse analysis has been shown tobe critical in several fundamental Natural Lan-guage Processing (NLP) tasks including text gener-ation (Prasad et al2005), summarization (Marcu,2000b), sentence compression (Sporleder and Lap-ata, 2005) and question answering (Verberne et al2007).
Rhetorical Structure Theory (RST) (Mannand Thompson, 1988), one of the most influentialtheories of discourse, posits a tree representation ofa discourse, known as a Discourse Tree (DT), asexemplified by the sample DT shown in Figure 1.The leaves of a DT correspond to contiguous atomictext spans, also called Elementary Discourse Units(EDUs) (three in the example).
The adjacent EDUsare connected by a rhetorical relation (e.g., ELAB-ORATION), and the resulting larger text spans arerecursively also subject to this relation linking.
Aspan linked by a rhetorical relation can be eithera NUCLEUS or a SATELLITE depending on howcentral the message is to the author.
Discourse anal-ysis in RST involves two subtasks: (i) breaking thetext into EDUs (known as discourse segmentation)and (ii) linking the EDUs into a labeled hierarchicaltree structure (known as discourse parsing).Figure 1: Discourse structure of a sentence in RST-DT.Previous studies on discourse analysis have beenquite successful in identifying what machine learn-ing approaches and what features are more useful forautomatic discourse segmentation and parsing (Sori-cut and Marcu, 2003; Subba and Eugenio, 2009; du-Verle and Prendinger, 2009).
However, all the pro-posed solutions suffer from at least one of the fol-lowing two key limitations: first, they make strongindependence assumptions on the structure and thelabels of the resulting DT, and typically model theconstruction of the DT and the labeling of the rela-tions separately; second, they apply a greedy, sub-optimal algorithm to build the structure of the DT.In this paper, we propose a new sentence-leveldiscourse parser that addresses both limitations.
Thecrucial component is a probabilistic discriminativeparsing model, expressed as a Dynamic ConditionalRandom Field (DCRF) (Sutton et al2007).
Byrepresenting the structure and the relation of eachdiscourse tree constituent jointly and by explicitlycapturing the sequential and hierarchical dependen-cies between constituents of a discourse tree, ourDCRF model does not make any independence as-sumption among these properties.
Furthermore, our904parsing model supports a bottom-up parsing algo-rithm which is non-greedy and provably optimal.The discourse parser assumes that the input texthas been already segmented into EDUs.
As an addi-tional contribution of this paper, we propose a noveldiscriminative approach to discourse segmentationthat not only achieves state-of-the-art performance,but also reduces the time and space complexities byusing fewer features.
Notice that the combinationof our segmenter with our parser forms a completeprobabilistic discriminative framework for perform-ing sentence-level discourse analysis.Our framework was tested in a series of experi-ments.
The empirical evaluation indicates that ourapproach to discourse parsing outperforms the state-of-the-art by a wide margin.
Moreover, we show thisto be the case on two very different genres: news ar-ticles and instructional how-to-do manuals.In the rest of the paper, after discussing relatedwork, we present our discourse parser.
Then, wedescribe our segmenter.
The experiments and thecorpora we used are described next, followed by adiscussion of the key results and some error analysis.2 Related workAutomatic discourse analysis has a long history;see (Stede, 2011) for a detailed overview.
Sori-cut and Marcu (2003) present the publicly availableSPADE1 system that comes with probabilistic mod-els for sentence-level discourse segmentation andparsing based on lexical and syntactic features de-rived from the lexicalized syntactic tree of a sen-tence.
Their parsing algorithm finds the most proba-ble DT for a sentence, where the probabilities of theconstituents are estimated by their parsing model.A constituent (e.g., ATTRIBUTION-NS[(1,2),3] inFigure 1) in a DT has two components, first, the la-bel denoting the relation and second, the structureindicating which spans are being linked by the rela-tion.
The nuclearity statuses of the spans are builtinto the relation labels (e.g., NS[(1,2),3] means thatspan (1,2) is the NUCLEUS and it comes beforespan 3 which is the SATELLITE).
SPADE is limitedin several ways.
It makes an independence assump-tion between the label and the structure while mod-eling a constituent, and it ignores the sequential and1http://www.isi.edu/licensed-sw/spade/hierarchical dependencies between the constituentsin the parsing model.
Furthermore, SPADE reliesonly on lexico-syntactic features, and it follows agenerative approach to estimate the model param-eters for the segmentation and the parsing models.SPADE was trained and tested on the RST-DT cor-pus (Carlson et al2002), which contains human-annotated discourse trees for news articles.Subsequent research addresses the question ofhow much syntax one really needs in discourseanalysis.
Sporleder and Lapata (2005) focus ondiscourse chunking, comprising the two subtasksof segmentation and non-hierarchical nuclearity as-signment.
More specifically, they examine whetherfeatures derived via part of speech (POS) and chunktaggers would be sufficient for these purposes.
Theirresults on RST-DT turn out to be comparable toSPADE without using any features from the syntac-tic tree.
Later, Fisher and Roark (2007) demonstrateover 4% absolute ?performance gain?
in segmenta-tion, by combining the features extracted from thesyntactic tree with the ones derived via taggers.
Us-ing quite a large number of features in a binary log-linear model they achieve the state-of-the-art seg-mentation performance on the RST-DT test set.On the different genre of instructional manuals,Subba and Eugenio (2009) propose a shift-reduceparser that relies on a classifier to find the appro-priate relation between two text segments.
Theirclassifier is based on Inductive Logic Programming(ILP), which learns first-order logic rules from alarge set of features including the linguistically richcompositional semantics coming from a semanticparser.
They show that the compositional seman-tics improves the classification performance.
How-ever, their discourse parser implements a greedy ap-proach (hence not optimal) and their classifier disre-gards the sequence and hierarchical dependencies.Using RST-DT, Hernault et al2010) presentthe HILDA system that comes with a segmenterand a parser based on Support Vector Machines(SVMs).
The segmenter is a binary SVM classi-fier which relies on the same lexico-syntactic fea-tures used in SPADE, but with more context.
Thediscourse parser builds a DT iteratively utilizing twoSVM classifiers in each iteration: (i) a binary classi-fier decides which of the two adjacent spans to link,and (ii) a multi-class classifier then connects the se-905lected spans with the appropriate relation.
They usea very large set of features in their parser.
How-ever, taking a radically-greedy approach, they modelstructure and relations separately, and ignore the se-quence dependencies in their models.Recently, there has been an explosion of interestin Conditional Random Fields (CRFs) (Lafferty etal., 2001) for solving structured output classificationproblems, with many successful applications in NLPincluding syntactic parsing (Finkel et al2008), syn-tactic chunking (Sha and Pereira, 2003) and dis-course chunking (Ghosh et al2011) in Penn Dis-course Treebank (Prasad et al2008).
CRFs being adiscriminative approach to sequence modeling (i.e.,directly models the conditional p(y|x,?
)), have sev-eral advantages over its generative counterparts suchas Hidden Markov Models (HMMs) and MarkovRandom Fields (MRFs), which first model the jointp(y, x|?
), then infer the conditional p(y|x,?)).
Keyadvantages include the ability to incorporate arbi-trary overlapping local and global features, and theability to relax strong independence assumptions.
Ithas been advocated that CRFs are generally moreaccurate since they do not ?waste effort?
modelingcomplex distributions (i.e., p(x)) that are not rele-vant for the target task (Murphy, 2012).3 The Discourse ParserAssuming that a sentence is already segmented intoa sequence of EDUs e1, e2, .
.
.
en manually or by anautomatic segmenter (see Section 4), the discourseparsing problem is to decide which spans to con-nect (i.e., structure of the DT) and which relations(i.e., labels of the internal nodes) to use in the pro-cess of building the hierarchical DT.
To build theDTs effectively, a common assumption is that theyare binary trees (Soricut and Marcu, 2003; duVerleand Prendinger, 2009).
That is, multi-nuclear re-lations (e.g., LIST, JOINT, SEQUENCE) involvingmore than two EDUs are mapped to a hierarchi-cal right-branching binary tree.
For example, a flatLIST (e1, e2, e3, e4) is mapped to a right-branchingbinary tree LIST (e1, LIST (e2, LIST (e3, e4))).Our discourse parser has two components.
Thefirst component, the parsing model, assigns a proba-bility to every possible DT.
The second component,the parsing algorithm, finds the most probable DTamong the candidate discourse trees.3.1 Parsing ModelA DT can be represented as a set of constituentsof the form R[i,m, j], which denotes a rhetoricalrelation R that holds between the span containingEDUs i through m, and the span containing EDUsm+1 through j.
For example, the DT in Figure 1can be written as {ELABORATION-NS[1,1,2],ATTRIBUTION-NS[1,2,3]}.
Notice that a rela-tion R also indicates the nuclearity assignmentsof the spans being connected, which can be oneof NUCLEUS-SATELLITE (NS), SATELLITE-NUCLEUS (SN) and NUCLEUS-NUCLEUS (NN).Given the model parameters ?
and a candi-date DT T , for all the constituents c in T , ourparsing model estimates the conditional probabil-ity P (c|C,?
), which specifies the joint probabil-ity of the relation R and the structure [i,m, j]associated with the constituent c, given that chas a set of sub-constituents C. For instance,for the DT shown in Figure 1, our modelwould estimate P (R?
[1, 1, 2]|?
), P (R?
[2, 2, 3]|?
),P (R?
[1, 2, 3]|R??
[1, 1, 2],?)
etc.
for all R?
and R?
?ranging on the set of relations.
In what follows wedescribe our probabilistic parsing model to computeall these conditional probabilities P (c|C,?).
Wewill demonstrate how our approach not only modelsthe structure and the relation jointly, but it also cap-tures linear sequence dependencies and hierarchicaldependencies between constituents of a DT.Our novel parsing model is the Dynamic Condi-tional Random Field (DCRF) (Sutton et al2007)shown in Figure 2.
A DCRF is a generalizationof linear-chain CRFs to represent complex interac-tion between labels, such as when performing mul-tiple labeling tasks on the same sequence.
The ob-served nodes Wj in the figure are the text spans.A text span can be either an EDU or a concatena-tion of a sequence of EDUs.
The structure nodesSj?
{0, 1} in the figure represent whether text spansWj?1 and Wj should be connected or not.
The re-lation nodes Rj?
{1 .
.
.M} denote the discourse re-lation between spans Wj?1 and Wj , given that M isthe total number of relations in our relation set.
No-tice that we now model the structure and the relationjointly and also take the sequential dependencies be-tween adjacent constituents into consideration.906Figure 2: A Dynamic CRF as a discourse parsing model.We can obtain the conditional probabilities ofthe constituents (i.e., P (c|C,?))
of all candidateDTs for a sentence by applying the DCRF pars-ing model recursively at different levels, and bycomputing the posterior marginals of the relation-structure pairs.
To illustrate, consider the examplesentence in Figure 1 where we have three EDUse1, e2 and e3.
The DCRF model for the firstlevel is shown in Figure 3(a), where the (observed)EDUs are the spans in the span sequence.
Giventhis model, we obtain the probabilities of the con-stituents R[1, 1, 2] and R[2, 2, 3] by computing theposterior marginals P (R2, S2=1|e1, e2, e3,?)
andP (R3, S3=1|e1, e2, e3,?
), respectively.
At the sec-ond level (see Figure 3(b)), there are two possi-ble span sequences (e1:2, e3) and (e1, e2:3).
In thefirst sequence, EDUs e1 and e2 are linked intoa larger span, and in the second one, EDUs e2and e3 are connected into a larger span.
We ap-ply our DCRF model to the two possible span se-quences and obtain the probabilities of the con-stituents R[1, 2, 3] and R[1, 1, 3] by computingthe posterior marginals P (R3, S3=1|e1:2, e3,?)
andP (R2:3, S2:3=1|e1, e2:3,?
), respectively.Figure 3: DCRF model applied to the sequences at differ-ent levels in the example in Fig.
1.
(a) A sequence at thefirst level (b) Two possible sequences at the second level.To further clarify the process, let us as-sume that the sentence contains four EDUse1, e2, e3 and e4.
At the first level (Fig-ure 4(a)), there is only one possible span se-quence to which we apply our DCRF model.We obtain the probabilities of the constituentsR[1, 1, 2], R[2, 2, 3] and R[3, 3, 4] by computing theposterior marginals P (R2, S2=1|e1, e2, e3, e4,?
),P (R3, S3=1|e1, e2, e3, e4,?)
and P (R4, S4=1|e1,e2, e3, e4,?
), respectively.
At the second level(Figure 4(b)), there are three possible sequences(e1:2, e3, e4), (e1, e2:3, e4) and (e1, e2, e3:4).
Whenthe DCRF model is applied to the sequence(e1:2, e3, e4), we obtain the probabilities of theconstituent R[1, 2, 3] by computing the posteriormarginal P (R3, S3=1|e1:2, e3, e4,?).
Likewise, theposterior marginals P (R2:3, S2:3=1|e1, e2:3, e4,?
)and P (R4, S4=1|e1, e2:3, e4,?)
in the DCRF modelapplied to the sequence (e1, e2:3, e4) representsthe probabilities of the constituents R[1, 1, 3]and R[2, 3, 4], respectively.
Similarly, we at-tain the probabilities of the constituent R[2, 2, 4]from the DCRF model applied to the sequence(e1, e2, e3:4) by computing the posterior marginalP (R3:4, S3:4=1|e1, e2, e3:4,?).
At the third level(Figure 4(c)), there are three possible sequences(e1:3, e4), (e1, e2:4) and (e1:2, e3:4), to which we ap-ply our model and acquire the probabilities of theconstituents R[1, 3, 4], R[1, 1, 4] and R[1, 2, 4] bycomputing their respective posterior marginals.Figure 4: DCRF model applied to the sequences at differ-ent levels of a discourse tree.
(a) A sequence at the firstlevel, (b) Three possible sequences at the second level,(c) Two possible sequences at the third level.Our DCRF model is designed using MALLET(McCallum, 2002).
In order to avoid overfitting weregularize the DCRF model with l2 regularizationand learn the model parameters using the limited-memory BFGS (L-BFGS) fitting algorithm.
Sinceexact inference can be intractable in DCRF models,907we perform approximate inference (to compute theposterior marginals) using tree-based reparameteri-zation (Wainwright et al2002).3.1.1 Features Used in the Parsing ModelCrucial to parsing performance is the set of fea-tures used, as summarized in Table 1.
Note thatthese features are defined on two consecutive spansWj?1 and Wj of a span sequence.
Most of the fea-tures have been explored in previous studies.
How-ever, we improve some of these as explained below.Organizational features encode useful informa-tion about the surface structure of a sentence asshown by (duVerle and Prendinger, 2009).
We mea-sure the length of the spans in terms of the number ofEDUs and tokens in it.
However, in order to betteradjust to the length variations, rather than comput-ing their absolute numbers in a span, we choose tomeasure their relative numbers with respect to theirtotal numbers in the sentence.
For example, in a sen-tence containing three EDUs, a span containing twoof these EDUs will have a relative EDU number of0.67.
We also measure the distances of the spansfrom the beginning and to the end of the sentence interms of the number of EDUs.8 organizational featuresRelative number of EDUs in span 1 and span 2.Relative number of tokens in span 1 and span 2.Distances of span 1 in EDUs to the beginning and to the end.Distances of span 2 in EDUs to the beginning and to the end.8 N-gram featuresBeginning and end lexical N-grams in span 1.Beginning and end lexical N-grams in span 2.Beginning and end POS N-grams in span 1.Beginning and end POS N-grams in span 2.5 dominance set featuresSyntactic labels of the head node and the attachment node.Lexical heads of the head node and the attachment node.Dominance relationship between the two text spans.2 contextual featuresPrevious and next feature vectors.2 substructure featuresRoot nodes of the left and right rhetorical subtrees.Table 1: Features used in the DCRF parsing model.Discourse connectives (e.g., because, but), whenpresent, signal rhetorical relations between two textsegments (Knott and Dale, 1994; Marcu, 2000a).However, previous studies (e.g., Hernault et al(2010), Biran and Rambow (2011)) suggest that anempirically acquired lexical N-gram dictionary ismore effective than a fixed list of connectives, sincethis approach is domain independent and capableof capturing non-lexical cues such as punctuations.To build the lexical N-gram dictionary empiricallyfrom the training corpus we consider the first andlast N tokens (N?
{1, 2}) of each span and rankthem according to their mutual information2 withthe two labels, Structure and Relation.
Intuitively,the most informative cues are not only the most fre-quent, but also the ones that are indicative of the la-bels in the training data (Blitzer, 2008).
In additionto the lexical N-grams we also encode POS tags ofthe first and last N tokens (N?
{1, 2}) as features.Figure 5: A discourse segmented lexicalized syntactictree.
Boxed nodes form the dominance set D.Dominance set extracted from the Discourse Seg-mented Lexicalized Syntactic Tree (DS-LST) (Sori-cut and Marcu, 2003) has been shown to be a veryeffective feature in SPADE.
Figure 5 shows the DS-LST for our running example (see Figure 1 and 3).In a DS-LST, each EDU except the one with the rootnode must have a head node NH that is attached toan attachment node NA residing in a separate EDU.A dominance set D (shown at the bottom of Figure 5for our example) contains these attachment points ofthe EDUs in a DS-LST.
In addition to the syntacticand lexical information of the head and attachmentnodes, each element in D also represents a domi-nance relationship between the EDUs involved.
TheEDU with NA dominates the EDU with NH .
In or-2In contrast, HILDA ranks the N-grams by frequencies.908der to extract dominance set features for two consec-utive spans ei:j and ej+1:k, we first compute D fromthe DS-LST of the sentence.
We then extract theelement from D that holds across the EDUs j andj + 1.
In our running example, for the spans e1 ande2 (Figure 3(a)), the relevant dominance set elementis (1, efforts/NP)>(2, to/S).
We encode the syntac-tic labels and lexical heads of NH and NA and thedominance relationship (i.e., which of the two spansis dominating) as features in our model.We also incorporate more contextual informationby including the above features computed for theneighboring span pairs in the current feature vector.We incorporate hierarchical dependencies be-tween constituents in a DT by means of the sub-structure features.
For the two adjacent spans ei:jand ej+1:k, we extract the roots of the rhetoricalsubtrees spanning over ei:j (left) and ej+1:k (right).In our example (see Figure 1 and Figure 3 (b)),the root of the rhetorical subtree spanning over e1:2is ELABORATION-NS.
However, this assumes thepresence of a labeled DT which is not the case whenwe apply the parser to a new sentence.
This problemcan be easily solved by looping twice through build-ing the model and the parsing algorithm (describedbelow).
We first build the model without consideringthe substructure features.
Then we find the optimalDT employing our parsing algorithm.
This interme-diate DT will now provide labels for the substruc-tures.
Next we can build a new, more accurate modelby including the substructure features, and run againthe parsing algorithm to find the final optimal DT.3.2 Parsing AlgorithmOur parsing model above assigns a conditional prob-ability to every possible DT constituent for a sen-tence, the job of the parsing algorithm is to find themost probable DT.
Formally, this can be written as,DT ?
= argmax DTP (DT |?
)Our discourse parser implements a probabilisticCKY-like bottom-up algorithm for computing themost likely parse of a sentence using dynamic pro-gramming; see (Jurafsky and Martin, 2008) for adescription.
Specifically, with n number of EDUsin a sentence, we use the upper-triangular por-tion of the n ?
n Dynamic Programming Table(DPT).
The cell [i, j] in the DPT represents thespan containing EDUs i through j and stores theprobability of a constituent R[i,m, j], where m =argmax i?k?jP (R[i, k, j]).In contrast to HILDA which implements a greedyalgorithm, our approach finds a DT that is glob-ally optimal.
Our approach is also different fromSPADE?s implementation.
SPADE first finds thetree structure that is globally optimal, then it assignsthe most probable relations to the internal nodes.More specifically, the cell [i, j] in SPADE?s DPTstores the probability of a constituent R[i,m, j],where m = argmax i?k?jP ([i, k, j]).
Disregard-ing the relation label R while building the DPT, thisapproach may find a tree that is not globally optimal.4 The Discourse SegmenterOur discourse parser above assumes that the inputsentences have been already segmented into EDUs.Since it has been shown that discourse segmentationis a primary source of inaccuracy for discourse pars-ing (Soricut and Marcu, 2003), we have developedour own segmenter, that not only achieves state-of-the-art performance as shown later, but also reducesthe time complexity by using fewer features.Our segmenter implements a binary classifier todecide for each word (except the last word) in a sen-tence, whether to put an EDU boundary after thatword.
We use a Logistic Regression (LR) (i.e., dis-criminative) model with l2 regularization and learnthe model parameters using the L-BFGS algorithm,which gives quadratic convergence rate.
To avoidoverfitting, we use 5-fold cross validation to learnthe regularization strength parameter from the train-ing data.
We also use a simple bagging technique(Breiman, 1996) to deal with the sparsity of bound-ary tags.
Note that, our first attempt at this task im-plemented a linear-chain CRF model to capture thesequence dependencies between the tags in a dis-criminative way.
However, the binary LR classifier,using the same features, not only outperforms theCRF model, but also reduces the space complexity.4.1 Features Used in the Segmentation ModelOur set of features for discourse segmentation aremostly inspired from previous studies but used in anovel way as we describe below.Our first subset of features which we call SPADEfeatures, includes the lexico-syntactic patterns ex-909tracted from the lexicalized syntactic tree for thegiven sentence.
These features replicates the fea-tures used in SPADE, but used in a discriminativeway.
To decide on an EDU boundary after a tokenwk, we find the lowest constituent in the lexicalizedsyntactic tree that spans over tokens wi .
.
.
wj suchthat i?k<j.
The production that expands this con-stituent in the tree and its different variations, formthe feature set.
For example in Figure 5, the produc-tion NP(efforts)?PRP$(its)NNS(efforts)?S(to) andits different variations depending on whether theyinclude the lexical heads and how many non-terminals (up to two) to consider before and afterthe potential EDU boundary (?
), are used to de-termine the existence of a boundary after the wordefforts (see (Fisher and Roark, 2007) for details).SPADE uses these features in a generative way,meaning that, it inserts an EDU boundary if the rela-tive frequency (i.e., Maximum Likelihood Estimate(MLE)) of a potential boundary given the productionin the training corpus is greater than 0.5.
If the pro-duction has not been observed frequently enough, ituses its other variations to perform further smooth-ing.
In contrast, we compute the MLE estimates fora production and its other variations, and use thoseas features with/without binarizing the values.Shallow syntactic parse (or Chunk) and POS tagshave been shown to possess valuable cues for dis-course segmentation (Fisher and Roark, 2007).
Forexample, it is less likely that an EDU boundary oc-curs within a chunk.
We, therefore, annotate the to-kens of a sentence with chunk and POS tags by astate-of-the-art tagger3 and encode these as features.EDUs are normally multi-word strings.
Thus, atoken near the beginning or end of a sentence is un-likely to be the end of a segment.
Therefore, for eachtoken we include its relative position in the sentenceand distances to the beginning and end as features.It is unlikely that two consecutive tokens aretagged with EDU boundaries.
We incorporate con-textual information for a token by including theabove features computed for its neighboring tokens.We also experimented with different N-gram(N?
{1, 2, 3}) features extracted from the token se-quence, POS sequence and chunk sequence.
How-ever, since such features did not improve the seg-3http://cogcomp.cs.illinois.edu/page/softwarementation accuracy on the development set, theywere excluded from our final set of features.5 Experiments5.1 CorporaTo demonstrate the generality of our model, we ex-periment with two different genres.
First, we use thestandard RST-DT corpus (Carlson et al2002) thatcontains discourse annotations for 385 Wall StreetJournal news articles from the Penn Treebank (Mar-cus et al1994).
Second, we use the Instructionalcorpus developed by Subba and Eugenio (2009) thatcontains discourse annotations for 176 instructionalhow-to-do manuals on home-repair.The RST-DT corpus is partitioned into a trainingset of 347 documents (7673 sentences) and a test setof 38 documents (991 sentences), and 53 documents(1208 sentences) have been (doubly) annotated bytwo human annotators, based on which we computethe human agreement.
We use the human-annotatedsyntactic trees from Penn Treebank to train SPADEin our experiments using RST-DT as done in (Sori-cut and Marcu, 2003).
We extracted a sentence-levelDT from a document-level DT by finding the subtreethat exactly spans over the sentence.
By our count,7321 sentences in the training set, 951 sentencesin the test set and 1114 sentences in the doubly-annotated set have a well-formed DT in RST-DT.The Instructional corpus contains 3430 sentences intotal, out of which 3032 have a well-formed DT.This forms our sentence-level corpora for discourseparsing.
However, the existence of a well-formedDT in not a necessity for discourse segmentation,therefore, we do not exclude any sentence in our dis-course segmentation experiments.5.2 Experimental SetupWe perform our experiments on discourse pars-ing in RST-DT with the 18 coarser relations (seeFigure 6) defined in (Carlson and Marcu, 2001)and also used in SPADE and HILDA.
By attach-ing the nuclearity statuses (i.e., NS, SN, NN) tothese relations we get 39 distinct relations4.
Ourexperiments on the Instructional corpus considerthe same 26 primary relations (e.g., GOAL:ACT,CAUSE:EFFECT, GENERAL-SPECIFIC) used in4Not all relations take all the possible nuclearity statuses.910(Subba and Eugenio, 2009) and also treat the re-versals of non-commutative relations as separate re-lations.
That is, PREPARATION-ACT and ACT-PREPARATION are two different relations.
Attach-ing the nuclearity statuses to these relations gives 70distinct relations in the Instructional corpus.We use SPADE as our baseline model and applythe same modifications to its default setting as de-scribed in (Fisher and Roark, 2007), which deliversimproved performance.
Specifically, in testing, wereplace the Charniak parser (Charniak, 2000) with amore accurate reranking parser (Charniak and John-son, 2005).
We use the reranking parser in all ourmodels to generate the syntactic trees.
This parserwas trained on the sections of the Penn Treebank notincluded in the test set.
For a fair comparison, we ap-ply the same canonical lexical head projection rules(Magerman, 1995; Collins, 2003) to lexicalize thesyntactic trees as done in SPADE and HILDA.
Notethat, all the previous works described in Section 2,report their models?
performance on a particular testset of a specific corpus.
To compare our results withthe previous studies, we test our models on thosespecific test sets.
In addition, we show more generalperformance based on 10-fold cross validation.5.3 Parsing based on Manual SegmentationFirst, we present the results of our discourse parserbased on manual segmentation.
The parsing perfor-mance is assessed using the unlabeled (i.e., span)and labeled (i.e., nuclearity, relation) precision, re-call and F-score as described in (Marcu, 2000b, page143).
For brevity, we report only the F-scores in Ta-ble 2.
Notice that, our parser (DCRF) consistentlyoutperforms SPADE (SP) on the RST-DT test set5.Especially, on relation labeling, which is the hardestamong the three tasks, we get an absolute F-scoreimprovement of 9.5%, which represents a relativeerror rate reduction of 29.3%.
Our F-score of 77.1in relation labeling is also close to the human agree-ment (i.e., F-score of 83.0) on the doubly-annotateddata.
Our results on the RST-DT test set are con-sistent with the mean scores over 10-folds, when weperform 10-fold cross validation on RST-DT.The improvement is even larger on the Instruc-tional corpus, where we compare our mean results5The improvements are statistically significant (p < 0.01).over 10-folds with the results reported in Subba andEugenio (S&E) (2009) on a test set6, giving ab-solute F-score improvements of 4.8%, 15.5% and10.6% in span, nuclearity and relations, respectively.Our parser reduces the errors by 67.6%, 54.6% and28.6% in span, nuclearity and relations, respectively.RST-DT InstructionalTest set 10-fold Doubly S&E 10-foldScores SP DCRF DCRF Human ILP DCRFSpan 93.5 94.6 93.7 95.7 92.9 97.7Nuc.
85.8 86.9 85.2 90.4 71.8 87.2Rel.
67.6 77.1 75.4 83.0 63.0 73.6Table 2: Parsing results using manual segmentation.If we compare the performance of our model onthe two corpora, we see that our model is more accu-rate in finding the right tree structure (see Span) onthe Instructional corpus.
This may be due to the factthat sentences in the Instructional domain are rela-tively short and contain fewer EDUs than sentencesin the News domain, thus making it easier to findthe right tree structure.
However, when we comparethe performance on the relation labeling task, we ob-serve a decrease on the Instructional corpus.
Thismay be due to the small amount of data available fortraining and the imbalanced distribution of a largenumber of discourse relations in this corpus.To analyze the features, Table 3 presents the pars-ing results on the RST-DT test set using differentsubsets of features.
Every new subset of featuresappears to improve the accuracy.
More specifically,when we add the organizational features with thedominance set features (see S2), we get about 2%absolute improvement in nuclearity and relations.With N-gram features (S3), the gain is even higher;6% in relations and 3.5% in nuclearity, demonstrat-ing the utility of the N-gram features.
This is con-sistent with the findings of (duVerle and Prendinger,2009; Schilder, 2002).
Including the Contextual fea-tures (S4), we get further 3% and 2.2% improve-ments in nuclearity and relations, respectively.
No-tice that, adding the substructure features (S5) doesnot help much in sentence-level parsing, giving only6Subba and Eugenio (2009) report their results based on anarbitrary split between a training set and a test set.
We asked theauthors for their particular split.
However, since we could notobtain that information, we compare our model?s performancebased on 10-fold cross validation with their reported results.911an improvement of 0.8% in relations.
Therefore, onemay choose to avoid using this computationally ex-pensive feature in time-constrained scenarios.
How-ever, in the future, it will be interesting to see its im-portance in document-level parsing with large trees.Scores S1 S2 S3 S4 S5Span 91.3 92.1 93.3 94.6 94.6Nuclearity 78.2 80.3 83.8 86.8 86.9Relation 66.2 68.1 74.1 76.3 77.1Table 3: Parsing results based on manual segmentationusing different subsets of features on RST-DT test set.Feature subsets S1 = {Dominance set}, S2 = {Dominanceset, Organizational}, S3 = {Dominance set, Organiza-tional, N-gram}, S4 = {Dominance set, Organizational,N-gram, Contextual}, S5 (all) = {Dominance set, Orga-nizational, N-gram, Contextual, Substructure}.5.4 Evaluation of the Discourse SegmenterWe evaluate the segmentation accuracy with respectto the intra-sentential segment boundaries following(Fisher and Roark, 2007).
Specifically, if a sen-tence contains n EDUs, which corresponds to n?
1intra-sentence segment boundaries, we measure themodel?s ability to correctly identify these n ?
1boundaries.
Human agreement for this task is quitehigh (F-score of 98.3) on RST-DT.Table 4 shows the results of different models in(P)recision, (R)ecall, and (F)-score on the two cor-pora.
We compare our model?s (LR) results withHILDA (HIL), SPADE (SP) and the results reportedin Fisher and Roark (F&R) (2007) on the RST-DTtest set.
HILDA gives the weakest performance7.Our results are also much better than SPADE8, withan absolute F-score improvement of 4.9%, and com-parable to the results of F&R, even though we usefewer features.
Furthermore, we perform 10-foldcross validation on both corpora and compare withSPADE.
However, SPADE does not come with atraining module for its segmenter.
We reimple-mented this module and verified it on the RST-DTtest set.
Due to the lack of human-annotated syntac-tic trees in the Instructional corpus, we train SPADEin this corpus using the syntactic trees produced7Note that, the high segmentation accuracy reported in (Her-nault et al2010) is due to a less stringent evaluation metric.8The improvements are statistically significant (p<2.4e-06)by the reranking parser.
Our model delivers abso-lute F-score improvements of 3.8% and 8.1% on theRST-DT and the Instructional corpora, respectively,which is statistically significant in both cases (p <3.0e-06).
However, when we compare our results onthe two corpora, we observe a substantial decrease inperformance on the Instructional corpus.
This couldbe due to a smaller amount of data in this corpus andthe inaccuracies in the syntactic parser and taggers,which are trained on news articles.RST-DT InstructionalTest Set 10-fold 10-fold 10-foldHIL SP F&R LR SP LR SP LRP 77.9 83.8 91.3 88.0 83.7 87.5 65.1 73.9R 70.6 86.8 89.7 92.3 86.2 89.9 82.8 89.7F 74.1 85.2 90.5 90.1 84.9 88.7 72.8 80.9Table 4: Segmentation results of different models.5.5 Parsing based on Automatic SegmentationIn order to evaluate our full system, we feed ourdiscourse parser the output of our discourse seg-menter.
Table 5 shows the F-score results.
We com-pare our results with SPADE on the RST-DT test set.We achieve absolute F-score improvements of 3.6%,3.4% and 7.4% in span, nuclearity and relation, re-spectively.
These improvements are statistically sig-nificant (p<0.001).
Our system, therefore, reducesthe errors by 15.5%, 11.4%, and 17.6% in span, nu-clearity and relations, respectively.
These results arealso consistent with the mean results over 10-folds.RST-DT InstructionalTest set 10-fold 10-foldScores SPADE DCRF DCRF DCRFSpan 76.7 80.3 78.7 71.9Nuclearity 70.2 73.6 72.2 64.3Relation 58.0 65.4 64.2 54.8Table 5: Parsing results using automatic segmentation.For the Instructional corpus, the last column ofTable 5 shows the mean 10-fold cross validation re-sults.
We cannot compare with S&E because no re-sults were reported using an automatic segmenter.However, it is interesting to observe how much ourfull system is affected by an automatic segmenteron both RST-DT and the Instructional corpus (seeTable 2 and Table 5).
Nevertheless, taking into ac-count the segmentation results in Table 4, this is912not surprising because previous studies (Soricut andMarcu, 2003) have already shown that automaticsegmentation is the primary impediment to high ac-curacy discourse parsing.
This demonstrates theneed for a more accurate segmentation model in theInstructional genre.
A promising future directionwould be to apply effective domain adaptation meth-ods (e.g., easyadapt (Daume, 2007)) to improvethe segmentation performance in the Instructionaldomain by leveraging the rich data in RST-DT.5.6 Error Analysis and DiscussionThe results in Table 2 suggest that given a manuallysegmented discourse, our sentence-level discourseparser finds the unlabeled (i.e., span) discourse treeand assigns the nuclearity statuses to the spans at aperformance level close to human annotators.
We,therefore, look more closely into the performance ofour parser on the hardest task of relation labeling.Figure 6 shows the confusion matrix for the rela-tion labeling task using manual segmentation on theRST-DT test set.
The relation labels are ordered ac-cording to their frequency in the RST-DT trainingset and represented by their initial letters.
For exam-ple, EL represents ELABORATION and CA repre-sents CAUSE.
In general, errors can be explained bytwo different phenomena acting together: (i) the fre-quency of the relations in the training data, and (ii)the semantic (or pragmatic) similarity between therelations.
The most frequent relations (e.g., ELAB-ORATION) tend to confuse the less frequent ones(e.g., SUMMARY), and the relations which are se-mantically similar (e.g., CAUSE, EXPLANATION)confuse each other, making it hard to distinguish forthe computational models.
Notice that, the confu-sions caused by JOINT appears to be high consid-ering its frequency.
The confusion between JOINTand TEMPORAL may be due to the fact that both ofthese coarser relations9 contain finer relations (i.e.,list in JOINT and sequence in TEMPORAL), whichare semantically similar, as pointed out by Carlsonand Marcu (2001).
The confusion between JOINTand BACKGROUND may be explained by their dif-ferent (semantic vs. pragmatic) interpretation in theRST theory (Stede, 2011, page 85).9JOINT is actually not a relation, but is characterized byjuxtaposition of two EDUs without a relation.Figure 6: Confusion matrix for the relation labels onthe RST-DT test set.
Y-axis represents true and X-axisrepresents predicted labels.
The relation labels are TOPIC-COMMENT, EVALUATION, SUMMARY, MANNER-MEANS,COMPARISON, EXPLANATION, CONDITION, TEMPORAL,CAUSE, ENABLEMENT, BACKGROUND, CONTRAST, JOINT,SAME-UNIT, ATTRIBUTION, ELABORATION.Based on these observations we will pursue twoways to improve our discourse parser.
We need amore robust (e.g., bagging) method to deal with theimbalanced distribution of relations, along with abetter representation of semantic knowledge.
Forexample, compositional semantics (Subba and Eu-genio, 2009) and subjectivity (Somasundaran, 2010)can be quite relevant for identifying relations.6 ConclusionIn this paper, we have described a complete prob-abilistic discriminative framework for performingsentence-level discourse analysis.
Experiments indi-cate that our approach outperforms the state-of-the-art on two corpora, often by a wide margin.In ongoing work, we plan to generalize ourDCRF-based parser to multi-sentential text and alsoverify to what extent parsing and segmentation canbe jointly performed.
A longer term goal is to extendour framework to also work with graph structuresof discourse, as recommended by several recent dis-course theories (Wolf and Gibson, 2005).
Once weachieve similar performance on graph structures, wewill perform extrinsic evaluation to determine theirrelative utility for various NLP tasks.AcknowledgmentsWe are grateful to G. Murray, J. CK Cheung, the 3reviewers and the NSERC CGS-D award.913ReferencesOr Biran and Owen Rambow.
2011.
Identifying Justifi-cations in Written Dialogs by Classifying Text as Ar-gumentative.
Int.
J. Semantic Computing, 5(4):363?381.J.
Blitzer, 2008.
Domain Adaptation of Natural Lan-guage Processing Systems.
PhD thesis, University ofPennsylvania.L.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing, 24(2):123?140, August.L.
Carlson and D. Marcu.
2001.
Discourse Tagging Ref-erence Manual.
Technical Report ISI-TR-545, Univer-sity of Southern California Information Sciences Insti-tute.L.
Carlson, D. Marcu, and M. Okurowski.
2002.
RSTDiscourse Treebank (RST-DT) LDC2002T07.
Lin-guistic Data Consortium, Philadelphia.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics, pages 173?180, NJ, USA.
ACL.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of the 1st North AmericanChapter of the Association for Computational Linguis-tics Conference, pages 132?139, Seattle, Washington.ACL.M.
Collins.
2003.
Head-Driven Statistical Models forNatural Language Parsing.
Computational Linguis-tics, 29(4):589?637, December.H.
Daume.
2007.
Frustratingly Easy Domain Adapta-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages256?263, Prague, Czech Republic.
ACL.D.
duVerle and H. Prendinger.
2009.
A Novel DiscourseParser based on Support Vector Machine Classifica-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 665?673, Suntec, Singapore.ACL.J.
Finkel, A. Kleeman, and C. Manning.
2008.
Efficient,Feature-based, Conditional Random Field Parsing.
InProceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics, pages 959?967,Columbus, Ohio, USA.
ACL.S.
Fisher and B. Roark.
2007.
The Utility of Parse-derived Features for Automatic Discourse Segmenta-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages488?495, Prague, Czech Republic.
ACL.S.
Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.2011.
Shallow Discourse Parsing with ConditionalRandom Fields.
In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing, pages 1071?1079, Chiang Mai, Thailand.
AFNLP.H.
Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.2010.
HILDA: A Discourse Parser Using SupportVector Machine Classification.
Dialogue and Dis-course, 1(3):1?33.D.
Jurafsky and J. Martin, 2008.
Speech and LanguageProcessing, chapter 14.
Prentice Hall.A.
Knott and R. Dale.
1994.
Using Linguistic Phenom-ena to Motivate a Set of Coherence Relations.
Dis-course Processes, 18(1):35?62.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceedingsof the Eighteenth International Conference on Ma-chine Learning, pages 282?289, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.D.
Magerman.
1995.
Statistical Decision-tree Mod-els for Parsing.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,pages 276?283, Cambridge, Massachusetts.
ACL.W.
Mann and S. Thompson.
1988.
Rhetorical StructureTheory: Toward a Functional Theory of Text Organi-zation.
Text, 8(3):243?281.D.
Marcu.
2000a.
The Rhetorical Parsing of UnrestrictedTexts: A Surface-based Approach.
ComputationalLinguistics, 26:395?448.D.
Marcu.
2000b.
The Theory and Practice of DiscourseParsing and Summarization.
MIT Press, Cambridge,MA, USA.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1994.Building a Large Annotated Corpus of English:The Penn Treebank.
Computational Linguistics,19(2):313?330.A.
McCallum.
2002.
MALLET: A Machine Learningfor Language Toolkit.
http://mallet.cs.umass.edu.K.
Murphy.
2012.
Machine Learning A ProbabilisticPerspective (Forthcoming, August 2012).
MIT Press,Cambridge, MA, USA.R.
Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, andB.
Webber.
2005.
The Penn Discourse TreeBank as aResource for Natural Language Generation.
In Pro-ceedings of the Corpus Linguistics Workshop on Us-ing Corpora for Natural Language Generation, pages25?32, Birmingham, U.K.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,A.
Joshi, and B. Webber.
2008.
The Penn DiscourseTreeBank 2.0.
In Proceedings of the Sixth Interna-tional Conference on Language Resources and Eval-uation (LREC), pages 2961?2968, Marrakech, Mo-rocco.
ELRA.914F.
Schilder.
2002.
Robust Discourse Parsing via Dis-course Markers, Topicality and Position.
Natural Lan-guage Engineering, 8(3):235?255, June.F.
Sha and F. Pereira.
2003.
Shallow Parsing withConditional Random Fields.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology - Volume 1, pages 134?141, Ed-monton, Canada.
ACL.S.
Somasundaran, 2010.
Discourse-Level Relations forOpinion Analysis.
PhD thesis, University of Pitts-burgh.R.
Soricut and D. Marcu.
2003.
Sentence Level Dis-course Parsing Using Syntactic and Lexical Informa-tion.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy - Volume 1, pages 149?156, Edmonton, Canada.ACL.C.
Sporleder and M. Lapata.
2005.
Discourse Chunk-ing and its Application to Sentence Compression.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 257?264, Vancouver, BritishColumbia, Canada.
ACL.M.
Stede.
2011.
Discourse Processing.
Synthesis Lec-tures on Human Language Technologies.
Morgan AndClaypool Publishers, November.R.
Subba and B.
Di Eugenio.
2009.
An EffectiveDiscourse Parser that Uses Rich Linguistic Informa-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 566?574, Boulder, Colorado.
ACL.C.
Sutton, A. McCallum, and K. Rohanimanesh.
2007.Dynamic Conditional Random Fields: FactorizedProbabilistic Models for Labeling and Segmenting Se-quence Data.
Journal of Machine Learning Research(JMLR), 8:693?723.S.
Verberne, L. Boves, N. Oostdijk, and P. Coppen.2007.
Evaluating Discourse-based Answer Extrac-tion for Why-question Answering.
In Proceedings ofthe 30th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 735?736, Amsterdam, The Netherlands.
ACM.M.
Wainwright, T. Jaakkola, and A. Willsky.
2002.
Tree-based Reparameterization for Approximate Inferenceon Loopy Graphs.
In Advances in Neural InformationProcessing Systems 14, pages 1001?1008.
MIT Press.F.
Wolf and E. Gibson.
2005.
Representing DiscourseCoherence: A Corpus-Based Study.
ComputationalLinguistics, 31:249?288, June.915
