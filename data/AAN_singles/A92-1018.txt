A Pract i ca l  Par t -o f -Speech  TaggerDoug Cutting and Julian Kupiec and Jan Pedersen and Penelope SibunXerox Palo Alto Research Center3333 Coyote Hill Road, Palo Alto, CA 94304, USAAbstractWe present an implementation f a part-of-speechtagger based on a hidden Markov model.
Themethodology enables robust and accurate taggingwith few resource requirements.
Only a lexiconand some unlabeled training text are required.Accuracy exceeds 96%.
We describe implemen-tation strategies and optimizations which resultin high-speed operation.
Three applications fortagging are described: phrase recognition; wordsense disambiguation; and grammatical functionassignment.1 DesiderataMany words are ambiguous in their part of speech.
Forexample, "tag" can be a noun or a verb.
However, when aword appears in the context of other words, the ambiguityis often reduced: in '% tag is a part-of-speech label," theword "tag" can only be a noun.
A part-of-speech taggeris a system that uses context o assign parts of speech towords.Automatic text tagging is an important first step indiscovering the linguistic structure of large text corpora.Part-of-speech information facilitates higher-level analysis,such as recognizing noun phrases and other patterns intext.For a tagger to function as a practical component in alanguage processing system, we believe that a tagger mustbe:Robust  Text corpora contain ungrammatical con-structions, isolated phrases (such as titles), and non-linguistic data (such as tables).
Corpora are also likelyto contain words that are unknown to the tagger.
Itis desirable that a tagger deal gracefully with thesesituations.Efficient If a tagger is to be used to analyze arbi-trarily large corpora, it must be efficient--performingin time linear in the number of words tagged.
Anytraining required should also be fast, enabling rapidturnaround with new corpora and new text genres.Accurate A tagger should attempt o assign the cor-rect part-of-speech tag to every word encountered.Tunab le  A tagger should be able to take advantageof linguistic insights.
One should be able to correctsystematic errors by supplying appropriate a priori"hints."
It should be possible to give different hintsfor different corpora.Reusable The effort required to retarget a tagger tonew corpora, new tagsets, and new languages houldbe minimal.2 Methodology2.1 BackgroundSeveral different approaches have been used for buildingtext taggers.
Greene and Rubin used a rule-based ap-proach in the TAGGIT program \[Greene and Rubin, 1971\],which was an aid in tagging the Brown corpus \[Francis andKu~era, 1982\].
TAGGIT disambiguated 77% of the cor-pus; the rest was done manually over a period of severalyears.
More recently, Koskenniemi also used a rule-basedapproach implemented with finite-state machines \[Kosken-niemi, 1990\].Statistical methods have also been used (e.g., \[DeRose,1988\], \[Garside t al., 1987\]).
These provide the capabilityof resolving ambiguity on the basis of most likely interpre-tation.
A form of Markov model has been widely used thatassumes that a word depends probabilistically on just itspart-of-speech category, which in turn depends olely onthe categories of the preceding two words.Two types of training (i.e., parameter estimation) havebeen used with this model.
The first makes use of a taggedtraining corpus.
Derouault and Merialdo use a bootstrapmethod for training \[Derouault and Merialdo, 1986\].
Atfirst, a relatively small amount of text is manually taggedand used to train a partially accurate model.
The modelis then used to tag more text, and the tags are manu-ally corrected and then used to retrain the model.
Churchuses the tagged Brown corpus for training \[Church, 1988\].These models involve probabilities for each word in thelexicon, so large tagged corpora are required for reliableestimation.The second method of training does not require a taggedtraining corpus.
In this situation the Baum-Welch algo-rithm (also known as the forward-backward algorithm) canbe used \[Baum, 1972\].
Under this regime the model iscalled a hidden Markov model (HMM), as state transitions(i.e., part-of-speech categories) are assumed to be unob-servable.
Jelinek has used this method for training a texttagger \[Jelinek, 1985\].
Parameter smoothing can be con-veniently achieved using the method of deleted interpola-tion in which weighted estimates are taken from second-and first-order models and a uniform probability distribu-tion \[Jelinek and Mercer, 1980\].
Kupiec used word equiv-alence classes (referred to here as ambiguity classes) basedon parts of speech, to pool data from individual words \[Ku-piec, 1989b\].
The most common words are still representedindividually, as sufficient data exist for robust estimation.133However all other words are represented according to theset of possible categories they can assume.
In this manner,the vocabulary of 50,000 words in the Brown corpus canbe reduced to approximately 400 distinct ambiguity classes\[Kupiec, 1992\].
To further reduce the number of param-eters, a first-order model can be employed (this assumesthat a word's category depends only on the immediatelypreceding word's category).
In \[Kupiec, 1989a\], networksare used to selectively augment the context in a basic first-order model, rather than using uniformly second-order de-pendencies.2.2 Our approachWe next describe how our choice of techniques atisfies thecriteria listed in section 1.
The use of an HMM permitscomplete flexibility in the choice of training corpora.
Textfrom any desired domain can be used, and a tagger can betailored for use with a particular text database by trainingon a portion of that database.
Lexicons containing alter-native tag sets can be easily accommodated without anyneed for re-labeling the training corpus, affording furtherflexibility in the use of specialized tags.
As the resourcesrequired are simply a lexicon and a suitably large sam-ple of ordinary text, taggers can be built with minimaleffort, even for other languages, such as French (e.g., \[Ku-piec, 1992\]).
The use of ambiguity classes and a first-ordermodel reduces the number of parameters to be estimatedwithout significant reduction in accuracy (discussed in sec-tion 5).
This also enables a tagger to be reliably trained us-ing only moderate amounts of text.
We have produced rea-sonable results training on as few as 3,000 sentences.
Fewerparameters also reduce the time required for training.
Rel-atively few ambiguity classes are sufficient for wide cover-age, so it is unlikely that adding new words to the lexiconrequires retraining, as their ambiguity classes are alreadyaccommodated.
Vocabulary independence is achieved bypredicting categories for words not in the lexicon, usingboth context and suffix information.
Probabilities corre-sponding to category sequences that never occurred in thetraining data are assigned small, non-zero values, ensuringthat the model will accept any sequence of tokens, whilestill providing the most likely tagging.
By using the factthat words are typically associated with only a few part-of-speech categories, and carefully ordering the computation,the algorithms have linear complexity (section 3.3).3 H idden Markov  Mode l ingThe hidden Markov modeling component of our tagger isimplemented as an independent module following the spec-ification given in \[Levinson et al, 1983\], with special at-tention to space and time efficiency issues.
Only first-ordermodeling is addressed and will be presumed for the remain-der of this discussion.3.1 FormalismIn brief, an HMM is a doubly stochastic process that gen-erates sequence of symbolsS = { Si, S2,...,ST}, Si E W I< i<T,where W is some finite set of possible symbols, by compos-ing an underlying Markov process with a state-dependentsymbol generator (i.e., a Markov process with noise), i ThMarkov process captures the notion of sequence dependency and is described by a set of N states, a matrix ctransition probabilities A = {aij} 1 <_ i, j <_ N where aiis the probability of moving from state i to state j, andvector of initial probabilities H = {rq} 1 < i < N whereis the probability of starting in state i.
The symbol gererator is a state-dependent measure on V described bymatrix of symbol probabilities B = {bjk} 1 _< j <__ N an1 < k < M where M = IWI and bjk is the probabilitygenerating symbol s~ given that the Markov process is istate j.2In part-of-speech tagging, we will model word order d,pendency through an underlying Markov process that otcrates in terms of lexical tags, 'yet we will only be abto observe the sets of tags, or ambiguity classes, that aJpossible for individual words.
The ambiguity class of eacword is the set of its permitted parts of speech, only orof which is correct in context.
Given the parameters A,and H, hidden Markov modeling allows us to compute ttmost probable sequence of state transitions, and hence ttmostly likely sequence of lexical tags, corresponding tosequence of ambiguity classes.
In the following, N canidentified with the number of possible.tags, and W witthe set of all ambiguity classes.Applying an HMM consists of two tasks: estimating ttmodel parameters A, B and H from a training set; arcomputing the most likely sequence of underlying statransitions given new observations.
Maximum likeliho(estimates (that is, estimates that maximize the probabiliof the training set) can be found through application ofternating expectation in a procedure known as the BaurWelch, or forward-backward, algorithm \[Baum, 1972\].proceeds by recursively defining two sets of probabiliti,the forward probabilities,at+i(J)= \[ ~-~at(i)ai~\]i=i b,(St+i) 1 <t  <T- l ,  (where at( i )  = ribi(Si) for all i; and the backward probbilities,N= T-  i < t < i ,  (j= iwhere \[3T(j) = 1 for all j.
The forward probabiliat(i) is the joint probability of the sequence up to tirt, {Si, S2, .
.
.
,St} ,  and the event that the Markov prcess is in state i at time t. Similarly, the backwaprobability \[3t(j) is the probability of seeing the sequen{St+i, St+2 .... , ST} given that the Markov process isstate i at time t. It follows that the probability of tentire sequence isN NP = E E ?~t(i)ai~bJ(St+i)/3t+i(j)imi  j= ifor any t in the range l<t  <T-  1.aiFor an introduction to hidden Markov modeling see \[l:biner and Juang, 1986\].2In the following we will write hi(St ) for bjk if St = s~.3This is most conveniently evaluated at t = T - 1, in whica e P =134Given an initial choice for the parameters A, B, and IIthe expected number of transitions, 7ij, from state i tostate j conditioned on the observation sequence S may becomputed as follows:T-1  17ij = -fi E at(i)aijbj(St+l)~t+l(j).t= lHence we can estimate aij by:_ ET:  5'i = N Ej=l 7ij ET:I 1 at(i)~t(i)Similarly, bj~ and 7ri can be estimated as follows:bjk = Et~s,: , ,  at(j)~t(j)ET=I at(j)Zt (j)and(3)(4)1~i---- ~Ot1(i)~1(i).
(5)In summary, to find maximum likelihood estimates forA, B, and II, via the Baum-Welch algorithm, one choosessome starting values, applies equations 3-5 to computenew values, and then iterates until convergence.
It can beshown that this algorithm will converge, although possiblyto a non-global maximum \[Baum, 1972\].Once a model has been estimated, selecting the mostlikely underlying sequence of state transitions correspond-ing to an observation S can be thought of as a maxi-mization over all sequences that might generate S. Anefficient dynamic programming procedure, known as theViterbi algorithm \[Viterbi, 1967\], arranges for this com-putation to proceed in time proportional to T. SupposeV = {v(t)} 1 < t < T is a state sequence that generatesS, then the probability that V generates S is,TP(v) = %(ub~(1)(S1) H a~(t-1)~(t)b~(t)(St).t=2?
To find the most probable such sequence we start by defin-ing ?1(i) = ~rib~(S1) for 1 < i < N and then perform therecursionet( j)  = ~a<x\[?t-l(i)aij\]bj(St) (6)andCt(j) = max- tCt_ l ( i )I<i<Nfor 2 < t < T and i _< j _< N. The crucial observa-tion is-that-for each time t and each state i one needonly consider the most probable sequence arriving at statei at time t. The probability of the most probable se-quence is maxl<_i<.N\[?T(i)\] while the sequence itself canbe reconstructed by defining v(T) = maxl--<_li<g eT(i) andv(t - I) = et (q t )  for T > t > 2.3.2 Numer ica l  Stab i l i tyThe Baum-Welch algorithm (equations 1-5) and the Viter-bi algorithm (equation 6) involve operations on productsof numbers constrained to be between 0 and 1.
Since theseproducts can easily underflow, measures must be taken torescale.
One approach premultiplies the a and 13 probabil-ities with an accumulating product depending on t \[Levin-son et al, 1983\].
Let 51(i) = al(i) and definect = 5t i l<t<T.Now define &t(i) = ctK~t(i) and use  a in place of a inequation 1 to define & for the next iteration:5t+l( j )  = &t(i)aij bj(St+l) l<t<T-1 .Note that E in__=l  ~t(i)  = 1 for 1 < t < T. Similarly, let~T(i) = ~T(i) and define 3t(i) = ct~t(i) for T > t > 1whereN~t(i) = E aiJ bj(St+l)3t+l(j)j=lT- l<t<l .The scaled backward and forward probabilities, 5 and~, can be exchanged for the unscaled probabilities in equa-tions 3-5 without affecting the value of the ratios.
Tosee this, note that at(i) = C\[at(i) and ~t(i) = ~t(i)C/+lwhereJC~=Hct .Now, in terms of the scaled probabilities, equation 5, forexample, can be seen to be unchanged:(~ 1 (i)f}l (i)_EN=I aT(i) E~=l CTaT(i) = ~'i.A slight difficulty occurs in equation 3 that can be curedby the addition of a new term, ct+l, in each product of theupper sum:T-1  ^ ?
^ .
~~t=l at(z)aijbj(St+, )~t+l(J)Ct+l ^ET_~ll &t( i)~t( i) = a,j.Numerical instability in the Viterbi algorithm can beameliorated by operating on a logarithmic scale \[Levinsonet al, 1983\].
That is, one maximizes the log probability ofeach sequence of state transitions,log(P(v)) = + log(b (1)(Sl)) +TE log(a~(t_ 1)~(t)) + log(b~(t)(St)).t=2Hence, equation 6 is replaced byet(J) = max \[?t-1(i) + log(ao)\] + logbj(St).I< i<NCare must be taken with zero probabilities.
However, thiscan be elegantly handled through the use of IEEE negativeinfinity \[P754, 1981\].1353.3 Reduc ing  T ime Complex i tyAs can be seen from equations 1-5, the time cost of trainingis O(TN~).
Similarly, as given in equation 6, the Viterbialgorithm is also O(TN2).
However, in part-of-speech tag-ging, the problem structure dictates that the matrix ofsymbol probabilities B is sparsely populated.
That is,bij 3?
0 iff the ambiguity class corresponding to symbol jincludes the part-of-speech tag associated with state i. Inpractice, the degree of overlap between ambiguity classesis relatively low; some tokens are assigned unique tags, andhence have only one non-zero symbol probability.The sparseness of B leads one to consider restructuringequations 1-6 so a check for zero symbol probability canobviate the need for further computation.
Equation 1 isalready conveniently factored so that the dependence onbj(St+l) is outside the inner sum.
Hence, ifk is the averagenumber of non-zero entries in each row of B, the cost ofcomputing equation 1 can be reduced to O(kTN).Equations 2-4 can be similarly reduced by switching theorder of iteration.
For example, in equation 2, rather thanfor a given t computing/3t(i) for each i one at a time, onecan accumulate terms for all i in parallel.
The net effect ofthis rewriting is to place a bj(St+l) = 0 check outside theinnermost iteration.
Equations 3 and 4 submit to a similarapproach.
Equation 5 is already only O(N).
Hence, theoverall cost of training can be reduced to O(kTN), which,in our experience, amounts to an order of magnitude speed-upflThe time complexity of the Viterbi algorithm can also bereduced to O(kTN) by noting that bj(St) can be factoredout of the maximization of equation 6.3.4 Controlling Space ComplexityAdding up the sizes of the probabil ity matrices A, B, andH, it is easy to see that the storage cost for directly re-presenting one model is proportional to N(N -t- M + 1).Running the Baum-Welch algorithm requires storage forthe sequence of observations, the a and /3 probabilities,the vector {ci}, and copies of the A and B matrices (sincethe originals cannot be overwritten until the end of eachiteration).
Hence, the grand total of space required fortraining is proportional to T q- 2N(T q- N + M + 1).Since N and M are fixed by the model, the only param-eter that can be varied to reduce storage costs is T. Now,adequate training requires processing from tens of thou-sands to hundreds of thousands of tokens \[Kupiec, 1989a\].The training set can be considered one long sequence, itwhich case T is very large indeed, or it can be broken upinto a number of smaller sequences at convenient bound-aries.
In first-order hidden Markov modeling, the stochas-tic process effectively restarts at unambiguous tokens, suchas sentence and paragraph markers, hence these tokensare convenient points at which to break the training set.If the Baum-Weleh algorithm is run separately (from thesame starting point) on each piece, the resulting trainedmodels must be recombined in some way.
One obvious ap-proach is simply to average.
However, this fails if any two4An equivalent approach maintains a mapping from states ito non-zero symbol probabilities and simply avoids, in the in-ner iteration, computing products which must be zero \[Kupiec,1992\].states are indistinguishable (in the sense that they had thesame transition probabilities and the same symbol prob-abilities at start), because states are then not matchedacross trained models.
It is therefore important hat eachstate have a distinguished role, which is relatively easy toachieve in part-of-speech tagging.Our implementation of the Baum-Welch algorithmbreaks up the input into fixed-sized pieces of training text.The Baum-Welch algorithm is then run separately on eachpiece and the results are averaged together.Running the Viterbi algorithm requires storage for thesequence of observations, a vector of current maxes, ascratch array of the same size, and a matrix of ?
indices,for a total proportional to T + N(2 + T) and a grand total(including the model) of T -t- N(N H- M + T ?
3).
Again, Nand M are fixed.
However, T need not be longer than a sin-gle sentence, since, as was observed above, the HMM, andhence the Viterbi algorithm, restarts at sentence bound-aries.3.5 Mode l  Tun ingAn HMM for part-of-speech tagging can be tuned in avariety of ways.
First, the choice of tagset and lexicondetermines the initial model.
Second, empirical and a pri-ori information can influence the choice of starting valuesfor the Baum-Welch algorithm.
For example, counting in-stances of ambiguity classes in running text allows one toassign non-uniform starting probabilities in A for a partic-ular tag's realization as a particular ambiguity class.
Alter-natively, one can state a priori that a particular ambiguityclass is most likely to be the reflection of some subset of itscomponent tags.
For example, if an ambiguity class con-sisting of the open class tags is used for unknown words,one may encode the fact that most unknown words arenouns or proper nouns by biasing the initial probabilitiesin B.Another biasing of starting values can arises from not-ing that some tags are unlikely to be followed by others.For example, the lexical item "to" maps to an ambigu-ity class containing two tags, infinitive-marker and to-as-preposition, neither of which occurs in any other ambigu-ity class.
If nothing more were stated, the HMM wouldhave two states which were indistinguishable.
This canbe remedied by setting the initial transition probabilitiesfrom infinitive-marker to strongly favor transitions to suchstates as verb-uninflected and adverb.Our implementation allows for two sorts of biasing ofstarting values: ambiguity classes can be annotated withfavored tags; and states can be annotated with favoredtransitions.
These biases may be specified either as sets oras set complements.
Biases are implemented by replacingthe disfavored probabilities with a small constant (machineepsilon) and redistributing mass to the other possibilities.This has the effect of disfavoring the indicated outcomeswithout disallowing them; sufficient converse data can re-habilitate these values.4 Arch i tec tureIn support of this and other work, we have developed asystem architecture for text access \[Cutting et al, 1991\].This architecture defines five components for such systems:136SearchIndexAnalysisCorpus?4????
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
??
.
.
.~??????? "
' - .??o.O?
???oo....
""(further analysis)""" stem, tag l//~:/ Tagging -~ Training .
/ ?
~t ra inedHMM/ambiguityelass,<stem,tag>* ~/ambigu i tyc lassLexicontoken lTokenizer-...
character~'?o~,oQ.??
.
.
?
, .
, ?
???????~??
.
.
.
.
.
?
.
.
.
.
.
.
.
.
* .
.
.
.
, ?Figure 1: Tagger Modules in System Contextcorpus, which provides text in a generic manner; analysis,which extracts terms from the text; index which storesterm occurrence statistics; and search, which utilizes thesestatistics to resolve queries.The part-of-speech tagger described here is implementedas an analysis module.
Figure 1 illustrates the overall ar-chitecture, showing the tagger analysis implementation idetail.
The tagger itself has a modular architecture, isolat-ing behind standard protocols those elements which mayvary, enabling easy substitution of alternate implementa-tions.Also illustrated here are the data types which flow be-tween tagger components.
As an analysis implementation,the tagger must generate terms from text.
In this context,a term is a word stem annotated with part of speech.Text enters the analysis sub-system where the first pro-cessing module it encounters is the tokenizer, whose dutyis to convert ext (a sequence of characters) into a sequenceof tokens.
Sentence boundaries are also identified by thetokenizer and are passed as reserved tokens.The tokenizer subsequently passes tokens to the lexicon.Here tokens are converted into a set of stems, each anno-tated with a part-of-speech tag.
The set of tags identifiesan ambiguity class.
The identification of these classes isalso the responsibility of the lexicon.
Thus the lexicon de-livers a set of stems paired with tags, and an ambiguityc lass .The training module takes long sequences of ambiguityclasses as input.
It uses the Baum-Welch algorithm toproduce a trained HMM, an input to the tagging module.Training is typically performed on a sample of the corpusat hand, with the trained HMM being saved for subsequentuse on the corpus at large.The tagging module buffers sequences of ambiguityclasses between sentence boundaries.
These sequences aredisambiguated by computing the maximal path throughthe HMM with the Viterbi algorithm.
Operating at sen-tence granularity provides fast throughput without loss ofaccuracy, as sentence boundaries are unambiguous.
Theresulting sequence of tags is used to select the appropriatestems.
Pairs of stems and tags are subsequently emitted.The tagger may function as a complete analysis compo-nent, providing tagged text to search and indexing com-ponents, or as a sub-system of a more elaborate analysis,such as phrase recognition.4.1 Token izer  ImplementationThe problem of tokenization has been well addressed bymuch work in compilation of programming languages.
Theaccepted approach is to specify token classes with reg-ular expressions.
These may be compiled into a sin-gle deterministic finite state automaton which partitionscharacter streams into labeled tokens \[Aho et al, 1986,Lesk, 1975\].In the context of tagging, we require at least two to-ken classes: sentence boundary and word.
Other classesmay include numbers, paragraph boundaries and varioussorts of punctuation (e.g., braces of various types, com-mas).
However, for simplicity, we will henceforth assumeonly words and sentence boundaries are extracted.Just as with programming languages, with text it is notalways possible to unambiguously specify the required to-ken classes with regular expressions.
However the additionof a simple lookahead mechanism which allows specifica-tion of right context ameliorates this \[Aho et al, 1986,Lesk, 1975\].
For example, a sentence boundary in Englishtext might be identified by a period, followed by white-space, followed by an uppercase letter.
However the up-137percase letter must not be consumed, as it is the first com-ponent of the next token.
A lookahead mechanism allowsus to specify in the sentence-boundary regular expressionthat the final character matched should not be considereda part of the token.This method meets our stated goals for the overall sys-tem.
It is efficient, requiring that each character be exam-ined only once (modulo lookahead).
It is easily parameter-izable, providing the expressive power to concisely defineaccurate and robust token classes.4.2 Lex icon Imp lementat ionThe lexicon module is responsible for enumerating parts ofspeech and their associated stems for each word it is given.For the English word "does," the lexicon might return "do,verb" and "doe, plural-noun."
It is also responsible foridentifying ambiguity classes based upon sets of tags.We have employed a three-stage implementation:First, we consult a manually-constructed l xicon to findstems and parts of speech.
Exhaustive lexicons of this sortare expensive, if not impossible, to produce.
Fortunately,a small set of words accounts for the vast majority of wordoccurences.
Thus high coverage can be obtained withoutprohibitive ffort.Words not found in the manually constructed lexiconare generally both open class and regularly inflected.
Asa second stage, a language-specific method can be em-ployed to guess ambiguity classes for unknown words.
Formany languages (e.g., English and French), word suffixesprovide strong cues to words' possible categories.
Prob-abalistic predictions of a word's category can be madeby analyzing suffixes in untagged text \[Kupiec, 1992,Meteer e* al., 1991\].As a final stage, if a word is not in the manually con-structed lexicon, and its suffix is not recognized, a defaultambiguity class is used.
This class typically contains allthe open class categories in the language.Dictionaries and suffix tables are both efficiently imple-mentable as letter trees, or tries \[Knuth, 1973\], which re-quire that each character of a word be examined only onceduring a lookup.5 Per fo rmanceIn this section, we detail how our tagger meets the desider-ata that we outlined in section 1.5.1 EfficientThe system is implemented in Common Lisp \[Steele, 1990\].All timings reported are for a Sun SPARCStation2.
TheEnglish lexicon used contains 38 tags (M -- 38) and 174ambiguity classes (N -- 174).Training was performed on 25,000 words in articles se-lected randomly from Grolier's Encyclopedia.
Five itera-tions of training were performed in a total time of 115 CPUseconds.
Following is a time breakdown by component:Training: average #seconds per tokentokenizer lexicon 1 iteration 5 iterations total640 400 680 3400 4600Tagging was performed on 115,822 words in a collectionof articles by the journalist Dave Barry.
This required atotal of of 143 CPU seconds.
The time breakdown for thiswas as follows:Tagging: average #seconds per tokentokenizer lexicon Viterbi total604 388 233 1235It can be seen from these figures that training on a newcorpus may be accomplished in a matter of minutes, andthat tens of megabytes of text may then be tagged perhour.5.2 Accurate and RobustWhen using a lexicon and tagset built from the tagged textof the Brown corpus \[Francis and Ku~era, 1982\], trainingon one half of the corpus (about 500,000 words) and tag-ging the other, 96% of word instances were assigned thecorrect tag.
Eight iterations of training were used.
Thislevel of accuracy is comparable to the best achieved byother taggers \[Church, 1988, Merialdo, 1991\].The Brown Corpus contains fragments and ungrammat-icalities, thus providing a good demonstration of robust-ness.5.3 Tunable and ReusableA tagger should be tunable, so that systematic taggingerrors and anomalies can be addressed.
Similarly, it is im-portant that it be fast and easy to target the tagger tonew genres and languages, and to experiment with differ-ent tagsets reflecting different insights into the linguisticphenomena found in text.
In section 3.5, we describe howthe HMM implementation itself supports tuning.
In ad-dition, our implementation supports a number of explicitparameters to facilitate tuning and reuse, including specifi-cation of lexicon and training corpus.
There is also supportfor a flexible tagset.
For example, if we want to collapsedistinctions in the lexicon, such as those between positive,comparative, and superlative adjectives, we only have tomake a small change in the mapping from lexicon to tagset.Similarly, if we wish to make finer grain distinctions thanthose available in the lexicon, such as case marking on pro-nouns, there is a simple way to note such exceptions.6 ApplicationsWe have used the tagger in a number of applications.
Wcdescribe three applications here: phrase recognition; wordsense disambiguation; and grammatical function assign-ment.
These projects are part of a research effort to useshallow analysis techniques to extract content from unre-stricted text.6.1 Phrase  RecognitionWe have constructed a system that recognizes simpl~phrases when given as input the sequence of tags for a sen-tence.
There are recognizers for noun phrases, verb groupsadverbial phrases, and prepositional phrases.
Each of thes~phrases comprises a contiguous sequence of tags that satis.ties a simple grammar.
For example, a noun phrase can b~a unary sequence containing a pronoun tag or an arbitrar.ily long sequence of noun and adjective tags, possibly pre.ceded by a determiner tag and possibly with an embeddecpossessive marker.
The longest possible sequence is fount(e.g., "the program committee" but not "the program")138Conjunctions are not recognized as part of any phrase; forexample, in the fragment "the cats and dogs," "the cats"and "dogs" will be recognized as two noun phrases.
Prepo-sitional phrase attachment is not performed at this stage ofprocessing.
This approach to phrase recognition in somecases captures only parts of some phrases; however, ourapproach minimizes false positives, so that we can rely onthe recognizers' results.6.2 Word  Sense DisamblguatlonPart-of-speech tagging in and of itself is a useful tool inlexical disambiguation; for example, knowing that "dig" isbeing used as a noun rather than as a verb indicates theword's appropriate meaning.
But many words have multi-ple meanings even while occupying the same part of speech.To this end, the tagger has been used in the implementa-tion of an experimental noun homograph disambiguationalgorithm \[Hearst, 1991\].
The algorithm (known as Catch-Word) performs upervised training over a large text cor-pus, gathering lexical, orthographic, and simple syntacticevidence for each sense of the ambiguous noun.
After a pe-riod of training, CatchWord classifies new instances of thenoun by checking its context against hat of previously ob-served instances and choosing the sense for which the mostevidence is found.
Because the sense distinctions made arecoarse, the disambiguation can be accomplished withoutthe expense of knowledge bases or inference mechanisms.Initial tests resulted in accuracies of around 90% for nounswith strongly distinct senses.This algorithm uses the tagger in two ways: (i) to de-termine the part of speech of the target word (filteringout the non-noun usages) and (ii) as a step in the phraserecognition analysis of the context surrounding the noun.6.3 Grammatical Function AssignmentThe phrase recognizers also provide input to a system,Sopa \[Sibun, 1991\], which recognizes nominal argumentsof verbs, specifically, Subject, Object, and Predicative Ar-guments.
Sopa does not rely on information (such as arityor voice) specific to the particular verbs involved.
Thefirst step in assigning grammatical functions is to parti-tion the tag sequence of each sentence into phrases.
Thephrase types include those mentioned in section 6.1, addi-tional types to account for conjunctions, complementizers,and indicators of sentence boundaries, and an "unknown"type.
After a sentence has been partitioned, each simplenoun phrase is examined in the context of the phrase to itsleft and the phrase to its right.
On the basis of this localcontext and a set of rules, the noun phrase is marked asa syntactic Subject, Object, Predicative, or is not markedat all.
A label of Predicative is assigned only if it can bedetermined that the governing verb group is a form of apredicating verb (e.g., a form of "be").
Because this can-not always be determined, some Predicatives are labeledObjects.
If a noun phrase is labeled, it is also annotatedas to whether the governing verb is the closest verb groupto the right or to the left.
The algorithm has an accuracyof approximately 800"/o in assigning rammatical functions.AcknowledgmentsWe would like to thank Marti Hearst for her contributionsto this paper, Lauri Karttunen and Annie Zaenen for theirwork on lexicons, and Kris Halvorsen for supporting thisproject.References\[Aho et al, 1986\] A. V. Aho, R. Sethi, and J. D. Ullman.Compilers: Principles, Techniques and Tools.
Addison-Wesley, 1986.\[Baum, 1972\] L. E. Baum.
An inequality and associ-ated maximization technique in statistical estimation forprobabilistic functions of a Markov process.
Inequalities,3:1-8, 1972.\[Church, 1988\] K. W. Church.
A stochastic parts programand noun phrase parser for unrestricted text.
In Pro-ceedings of the Second Conference on Applied NaturalLanguage Processing (ACL), pages 136-143, 1988.\[Cutting et al, 1991\] D.R.
Cutting, J. Pedersen, and P.-K. Halvorsen.
An object-oriented architecture for textretrieval.
In Conference Proceedings of R\[AO'91, Intelli-gent Text and Image Handling, Barcelona, Spain, pages285-298, April 1991.\[DeRose, 1988\] S. DeRose.
Grammatical category disam-biguation by statistical optimization.
ComputationalLinguistics, 14:31-39, 1988.\[Derouault and Merialdo, 1986\] A. M. Derouault andB.
Merialdo.
Natural language modeling for phoneme-to-text transcription.
IEEE Transactions on PatternAnalysis and Machine Intelligence, PAMI-8:742-749,1986.\[Francis and Ku~era, 1982\] W. N. Francis and F. Ku~era.Frequency Analysis of English Usage.
Houghton Mifflin,1982.\[Garside t al., 1987\] R. Garside, G. Leech, and G. Samp-son.
The Computational Analysis of English.
Long.man,1987.\[Greene and Rubin, 1971\] B.
B. Greene and G. M. Rubin.Automatic grammatical tagging of English.
Technicalreport, Department of Linguistics, Brown University,Providence, Rhode Island, 1971.\[Hearst, 1991\] M. A. Hearst.
Noun homograph disam-biguation using local context in large text corpora.
InThe Proceedings of the 7th New OED Conference on Us-ing Corpora, pages 1-22, Oxford, 1991.\[Jelinek and Mercer, 1980\] F. Jelinek and R. L. Mercer.Interpolated estimation of markov source parametersfrom sparse data.
In Proceedings of the Workshop Pat-tern Recognition in Practice, pages 381-397, Amster-dam, 1980.
North-Holland.\[Jelinek, 1985\] F. Jelinek.
Markov source modeling of textgeneration.
In J. K. Skwirzinski, editor, Impact ofProcessing Techniques on Communication.
Nijhoff, Dor-drecht, 1985.\[Knuth, 1973\] D. Knuth.
The Art of Computer Program-ming, volume 3: Sorting and Searching.
Addison-Wesley, 1973.139\[Koskenniemi, 1990\] K. Koskenniemi.
Finte-state parsingand disambiguation.
In H. Karlgren, editor, COLING-90, pages 229-232, Helsinki University, 1990.\[Kupiec, 1989a\] J. M. Kupiec.
Augmenting a hiddenMarkov model for phrase-dependent word tagging.
InProceedings of the DARPA Speech and Natural LanguageWorkshop, pages 92-98, Cape Cod, MA, 1989.
MorganKaufmann.\[Kupiec, 1989b\] J. M. Kupiec.
Probabilistic models ofshort and long distance word dependencies in runningtext.
In Proceedings of the 1989 DARPA Speech andNatural Language Workshop, pages 290-295, Philadel-phia, 1989.
Morgan Kaufmann.\[Kupiec, 1992\] J. M. Kupiec.
Robust part-of-speech tag-ging using a hidden markov model, submitted to Com-puter Speech and Language, 1992.\[Lesk, 1975\] M. E. Lesk.
LEX - -  a lexical analyzer gen-erator.
Computing Science Technical Report 39, AT&TBell Laboratories, Murray Hill, New Jersey, 1975.\[Levinson et al, 1983\] S. E. Levinson, L. R. Rabiner, andM.
M. Sondhi.
An introduction to the application ofthe theory of probabilistic functions of a Markov processto automatic speech recognition.
Bell System Technical?
Journal, 62:1035-1074, 1983.\[Merialdo, 1991\] B. Merialdo.
Tagging text with a proba-blistic model.
In Proceedings of ICASSP-91, pages 809-812, Toronto, Canada, 1991.\[Meteer et al, 1991\] M. W. Meteer, R. Schwartz, andR.
Weischedel.
POST: Using probabilities in languageprocessing.
In Proceedings of the 12th InternationalJoint Conference on Artificial Intelligence, pages 960-965, 1991.\[P754, 1981\] IEEE Task P754.
A proposed standard forbinary floating-point arithmetic.
Computer, 14(3):51-62, March 1981.\[Rabiner and Juang, 1986\] L. R. Rabiner and B. H. Juang.An introduction to hidden markov models.
IEEE ASSPMagazine, January 1986.\[Sibun, 1991\] P. Sibun.
Grammatical function assignmentin unrestricted text.
internal report, Xerox Palo AltoResearch Center, 1991.\[Steele, 1990\] G. L. Steele, Jr. Common Lisp, The Lan-guage.
Digital Press, second edition, 1990.\[Viterbi, 1967\] A. J. Viterbi.
Error bounds for convolu-tional codes and an asymptotically optimal decoding al-gorithm.
In IEEE Transactions on Information Theory,pages 260-269, April 1967.140
