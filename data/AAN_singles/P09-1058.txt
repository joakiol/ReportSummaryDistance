Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPAn Error-Driven Word-Character Hybrid Modelfor Joint Chinese Word Segmentation and POS TaggingCanasai Kruengkrai??
and Kiyotaka Uchimoto?
and Jun?ichi Kazama?Yiou Wang?
and Kentaro Torisawa?
and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan?National Institute of Information and Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jpAbstractIn this paper, we present a discriminativeword-character hybrid model for joint Chi-nese word segmentation and POS tagging.Our word-character hybrid model offershigh performance since it can handle bothknown and unknown words.
We describeour strategies that yield good balance forlearning the characteristics of known andunknown words and propose an error-driven policy that delivers such balanceby acquiring examples of unknown wordsfrom particular errors in a training cor-pus.
We describe an efficient frameworkfor training our model based on the Mar-gin Infused Relaxed Algorithm (MIRA),evaluate our approach on the Penn ChineseTreebank, and show that it achieves supe-rior performance compared to the state-of-the-art approaches reported in the litera-ture.1 IntroductionIn Chinese, word segmentation and part-of-speech(POS) tagging are indispensable steps for higher-level NLP tasks.
Word segmentation and POS tag-ging results are required as inputs to other NLPtasks, such as phrase chunking, dependency pars-ing, and machine translation.
Word segmenta-tion and POS tagging in a joint process have re-ceived much attention in recent research and haveshown improvements over a pipelined fashion (Ngand Low, 2004; Nakagawa and Uchimoto, 2007;Zhang and Clark, 2008; Jiang et al, 2008a; Jianget al, 2008b).In joint word segmentation and the POS tag-ging process, one serious problem is caused byunknown words, which are defined as words thatare not found in a training corpus or in a sys-tem?s word dictionary1.
The word boundaries andthe POS tags of unknown words, which are verydifficult to identify, cause numerous errors.
Theword-character hybrid model proposed by Naka-gawa and Uchimoto (Nakagawa, 2004; Nakagawaand Uchimoto, 2007) shows promising propertiesfor solving this problem.
However, it suffers fromstructural complexity.
Nakagawa (2004) describeda training method based on a word-based Markovmodel and a character-based maximum entropymodel that can be completed in a reasonable time.However, this training method is limited by thegeneratively-trained Markov model in which in-formative features are hard to exploit.In this paper, we overcome such limitationsconcerning both efficiency and effectiveness.
Wepropose a new framework for training the word-character hybrid model based on the MarginInfused Relaxed Algorithm (MIRA) (Crammer,2004; Crammer et al, 2005; McDonald, 2006).We describe k-best decoding for our hybrid modeland design its loss function and the features appro-priate for our task.In our word-character hybrid model, allowingthe model to learn the characteristics of bothknown and unknown words is crucial to achieveoptimal performance.
Here, we describe ourstrategies that yield good balance for learningthese two characteristics.
We propose an error-driven policy that delivers this balance by acquir-ing examples of unknown words from particularerrors in a training corpus.
We conducted our ex-periments on Penn Chinese Treebank (Xia et al,2000) and compared our approach with the bestprevious approaches reported in the literature.
Ex-perimental results indicate that our approach canachieve state-of-the-art performance.1A system?s word dictionary usually consists of a wordlist, and each word in the list has its own POS category.
Inthis paper, we constructed the system?s word dictionary froma training corpus.513Figure 1: Lattice used in word-character hybrid model.Tag DescriptionB Beginning character in a multi-character wordI Intermediate character in a multi-character wordE End character in a multi-character wordS Single-character wordTable 1: Position-of-character (POC) tags.The paper proceeds as follows: Section 2 givesbackground on the word-character hybrid model,Section 3 describes our policies for correct pathselection, Section 4 presents our training methodbased on MIRA, Section 5 shows our experimen-tal results, Section 6 discusses related work, andSection 7 concludes the paper.2 Background2.1 Problem formationIn joint word segmentation and the POS tag-ging process, the task is to predict a pathof word hypotheses y = (y1, .
.
.
, y#y) =(?w1, p1?, .
.
.
, ?w#y, p#y?)
for a given charactersequence x = (c1, .
.
.
, c#x), where w is a word,p is its POS tag, and a ?#?
symbol denotes thenumber of elements in each variable.
The goal ofour learning algorithm is to learn a mapping frominputs (unsegmented sentences) x ?
X to outputs(segmented paths) y ?
Y based on training sam-ples of input-output pairs S = {(xt, yt)}Tt=1.2.2 Search space representationWe represent the search space with a lattice basedon the word-character hybrid model (Nakagawaand Uchimoto, 2007).
In the hybrid model,given an input sentence, a lattice that consistsof word-level and character-level nodes is con-structed.
Word-level nodes, which correspond towords found in the system?s word dictionary, haveregular POS tags.
Character-level nodes have spe-cial tags where position-of-character (POC) andPOS tags are combined (Asahara, 2003; Naka-gawa, 2004).
POC tags indicate the word-internalpositions of the characters, as described in Table 1.Figure 1 shows an example of a lattice for a Chi-nese sentence: ?
?
(Chongming isChina?s third largest island).
Note that some nodesand state transitions are not allowed.
For example,I and E nodes cannot occur at the beginning of thelattice (marked with dashed boxes), and the transi-tions from I to B nodes are also forbidden.
Thesenodes and transitions are ignored during the latticeconstruction processing.In the training phase, since several paths(marked in bold) can correspond to the correctanalysis in the annotated corpus, we need to se-lect one correct path yt as a reference for training.2The next section describes our strategies for deal-ing with this issue.With this search space representation, wecan consistently handle unknown words withcharacter-level nodes.
In other words, we useword-level nodes to identify known words andcharacter-level nodes to identify unknown words.In the testing phase, we can use a dynamic pro-gramming algorithm to search for the most likelypath out of all candidate paths.2A machine learning problem exists called structuredmulti-label classification that allows training from multiplecorrect paths.
However, in this paper we limit our considera-tion to structured single-label classification, which is simpleyet provides great performance.5143 Policies for correct path selectionIn this section, we describe our strategies for se-lecting the correct path yt in the training phase.As shown in Figure 1, the paths marked in boldcan represent the correct annotation of the seg-mented sentence.
Ideally, we need to build a word-character hybrid model that effectively learns thecharacteristics of unknown words (with character-level nodes) as well as those of known words (withword-level nodes).We can directly estimate the statistics of knownwords from an annotated corpus where a sentenceis already segmented into words and assigned POStags.
If we select the correct path yt that corre-sponds to the annotated sentence, it will only con-sist of word-level nodes that do not allow learningfor unknown words.
We therefore need to choosecharacter-level nodes as correct nodes instead ofword-level nodes for some words.
We expect thatthose words could reflect unknown words in thefuture.Baayen and Sproat (1996) proposed that thecharacteristics of infrequent words in a trainingcorpus resemble those of unknown words.
Theiridea has proven effective for estimating the statis-tics of unknown words in previous studies (Ratna-parkhi, 1996; Nagata, 1999; Nakagawa, 2004).We adopt Baayen and Sproat?s approach asthe baseline policy in our word-character hybridmodel.
In the baseline policy, we first count thefrequencies of words3 in the training corpus.
Wethen collect infrequent words that appear less thanor equal to r times.4 If these infrequent words arein the correct path, we use character-level nodesto represent them, and hence the characteristics ofunknown words can be learned.
For example, inFigure 1 we select the character-level nodes of theword ?
?
(Chongming) as the correct nodes.
Asa result, the correct path yt can contain both word-level and character-level nodes (marked with as-terisks (*)).To discover more statistics of unknown words,one might consider just increasing the thresholdvalue r to obtain more artificial unknown words.However, our experimental results indicate thatour word-character hybrid model requires an ap-propriate balance between known and artificial un-3We consider a word and its POS tag a single entry.4In our experiments, the optimal threshold value r is se-lected by evaluating the performance of joint word segmen-tation and POS tagging on the development set.known words to achieve optimal performance.We now describe our new approach to lever-age additional examples of unknown words.
In-tuition suggests that even though the system canhandle some unknown words, many unidentifiedunknown words remain that cannot be recoveredby the system; we wish to learn the characteristicsof such unidentified unknown words.
We proposethe following simple scheme:?
Divide the training corpus into ten equal setsand perform 10-fold cross validation to findthe errors.?
For each trial, train the word-character hybridmodel with the baseline policy (r = 1) us-ing nine sets and estimate errors using the re-maining validation set.?
Collect unidentified unknown words fromeach validation set.Several types of errors are produced by thebaseline model, but we only focus on those causedby unidentified unknown words, which can be eas-ily collected in the evaluation process.
As de-scribed later in Section 5.2, we measure the recallon out-of-vocabulary (OOV) words.
Here, we de-fine unidentified unknown words as OOV wordsin each validation set that cannot be recovered bythe system.
After ten cross validation runs, weget a list of the unidentified unknown words de-rived from the whole training corpus.
Note thatthe unidentified unknown words in the cross val-idation are not necessary to be infrequent words,but some overlap may exist.
Finally, we obtain theartificial unknown words that combine the uniden-tified unknown words in cross validation and in-frequent words for learning unknown words.
Werefer to this approach as the error-driven policy.4 Training method4.1 Discriminative online learningLet Yt = {y1t , .
.
.
, yKt } be a lattice consisting ofcandidate paths for a given sentence xt.
In theword-character hybrid model, the lattice Yt cancontain more than 1000 nodes, depending on thelength of the sentence xt and the number of POStags in the corpus.
Therefore, we require a learn-ing algorithm that can efficiently handle large andcomplex lattice structures.Online learning is an attractive method forthe hybrid model since it quickly converges515Algorithm 1 Generic Online Learning AlgorithmInput: Training set S = {(xt, yt)}Tt=1Output: Model weight vector w1: w(0) = 0;v = 0; i = 02: for iter = 1 to N do3: for t = 1 to T do4: w(i+1) = update w(i) according to (xt, yt)5: v = v +w(i+1)6: i = i+ 17: end for8: end for9: w = v/(N ?
T )within a few iterations (McDonald, 2006).
Algo-rithm 1 outlines the generic online learning algo-rithm (McDonald, 2006) used in our framework.4.2 k-best MIRAWe focus on an online learning algorithm calledMIRA (Crammer, 2004), which has the de-sired accuracy and scalability properties.
MIRAcombines the advantages of margin-based andperceptron-style learning with an optimizationscheme.
In particular, we use a generalized ver-sion of MIRA (Crammer et al, 2005; McDonald,2006) that can incorporate k-best decoding in theupdate procedure.
To understand the concept of k-best MIRA, we begin with a linear score function:s(x, y;w) = ?w, f(x, y)?
, (1)where w is a weight vector and f is a feature rep-resentation of an input x and an output y.Learning a mapping between an input-outputpair corresponds to finding a weight vector w suchthat the best scoring path of a given sentence isthe same as (or close to) the correct path.
Givena training example (xt, yt), MIRA tries to estab-lish a margin between the score of the correct paths(xt, yt;w) and the score of the best candidatepath s(xt, y?
;w) based on the current weight vectorw that is proportional to a loss function L(yt, y?
).In each iteration, MIRA updates the weight vec-tor w by keeping the norm of the change in theweight vector as small as possible.
With thisframework, we can formulate the optimizationproblem as follows (McDonald, 2006):w(i+1) = argminw?w ?w(i)?
(2)s.t.
?y?
?
bestk(xt;w(i)) :s(xt, yt;w)?
s(xt, y?
;w) ?
L(yt, y?)
,where bestk(xt;w(i)) ?
Yt represents a set of topk-best paths given the weight vector w(i).
Theabove quadratic programming (QP) problem canbe solved using Hildreth?s algorithm (Yair Cen-sor, 1997).
Replacing Eq.
(2) into line 4 of Al-gorithm 1, we obtain k-best MIRA.The next question is how to efficiently gener-ate bestk(xt;w(i)).
In this paper, we apply a dy-namic programming search (Nagata, 1994) to k-best MIRA.
The algorithm has two main searchsteps: forward and backward.
For the forwardsearch, we use Viterbi-style decoding to find thebest partial path and its score up to each node inthe lattice.
For the backward search, we use A?-style decoding to generate the top k-best paths.
Acomplete path is found when the backward searchreaches the beginning node of the lattice, and thealgorithm terminates when the number of gener-ated paths equals k.In summary, we use k-best MIRA to iterativelyupdate w(i).
The final weight vector w is the av-erage of the weight vectors after each iteration.As reported in (Collins, 2002; McDonald et al,2005), parameter averaging can effectively avoidoverfitting.
For inference, we can use Viterbi-styledecoding to search for the most likely path y?
fora given sentence x where:y?
= argmaxy?Ys(x, y;w) .
(3)4.3 Loss functionIn conventional sequence labeling where the ob-servation sequence (word) boundaries are fixed,one can use the 0/1 loss to measure the errors ofa predicted path with respect to the correct path.However, in our model, word boundaries varybased on the considered path, resulting in a dif-ferent numbers of output tokens.
As a result, wecannot directly use the 0/1 loss.We instead compute the loss function throughfalse positives (FP ) and false negatives (FN ).Here, FP means the number of output nodes thatare not in the correct path, and FN means thenumber of nodes in the correct path that cannotbe recognized by the system.
We define the lossfunction by:L(yt, y?)
= FP + FN .
(4)This loss function can reflect how bad the pre-dicted path y?
is compared to the correct path yt.A weighted loss function based on FP and FNcan be found in (Ganchev et al, 2007).516ID Template ConditionW0 ?w0?
for word-levelW1 ?p0?
nodesW2 ?w0, p0?W3 ?Length(w0), p0?A0 ?AS(w0)?
if w0 is a single-A1 ?AS(w0), p0?
character wordA2 ?AB(w0)?
for word-levelA3 ?AB(w0), p0?
nodesA4 ?AE(w0)?A5 ?AE(w0), p0?A6 ?AB(w0), AE(w0)?A7 ?AB(w0), AE(w0), p0?T0 ?TS(w0)?
if w0 is a single-T1 ?TS(w0), p0?
character wordT2 ?TB(w0)?
for word-levelT3 ?TB(w0), p0?
nodesT4 ?TE(w0)?T5 ?TE(w0), p0?T6 ?TB(w0), TE(w0)?T7 ?TB(w0), TE(w0), p0?C0 ?cj?, j ?
[?2, 2] ?
p0 for character-C1 ?cj , cj+1?, j ?
[?2, 1] ?
p0 level nodesC2 ?c?1, c1?
?
p0C3 ?T (cj)?, j ?
[?2, 2] ?
p0C4 ?T (cj), T (cj+1)?, j ?
[?2, 1] ?
p0C5 ?T (c?1), T (c1)?
?
p0C6 ?c0, T (c0)?
?
p0Table 2: Unigram features.4.4 FeaturesThis section discusses the structure of f(x, y).
Webroadly classify features into two categories: uni-gram and bigram features.
We design our featuretemplates to capture various levels of informationabout words and POS tags.
Let us introduce somenotation.
We write w?1 and w0 for the surfaceforms of words, where subscripts ?1 and 0 in-dicate the previous and current positions, respec-tively.
POS tags p?1 and p0 can be interpreted inthe same way.
We denote the characters by cj .Unigram features: Table 2 shows our unigramfeatures.
Templates W0?W3 are basic word-levelunigram features, where Length(w0) denotes thelength of the word w0.
Using just the surfaceforms can overfit the training data and lead to poorpredictions on the test data.
To alleviate this prob-lem, we use two generalized features of the sur-face forms.
The first is the beginning and endcharacters of the surface (A0?A7).
For example,?AB(w0)?
denotes the beginning character of thecurrent word w0, and ?AB(w0), AE(w0)?
denotesthe beginning and end characters in the word.
Thesecond is the types of beginning and end charac-ters of the surface (T0?T7).
We define a set ofgeneral character types, as shown in Table 4.Templates C0?C6 are basic character-level un-ID Template ConditionB0 ?w?1, w0?
if w?1 and w0B1 ?p?1, p0?
are word-levelB2 ?w?1, p0?
nodesB3 ?p?1, w0?B4 ?w?1, w0, p0?B5 ?p?1, w0, p0?B6 ?w?1, p?1, w0?B7 ?w?1, p?1, p0?B8 ?w?1, p?1, w0, p0?B9 ?Length(w?1), p0?TB0 ?TE(w?1)?TB1 ?TE(w?1), p0?TB2 ?TE(w?1), p?1, p0?TB3 ?TE(w?1), TB(w0)?TB4 ?TE(w?1), TB(w0), p0?TB5 ?TE(w?1), p?1, TB(w0)?TB6 ?TE(w?1), p?1, TB(w0), p0?CB0 ?p?1, p0?
otherwiseTable 3: Bigram features.Character type DescriptionSpace SpaceNumeral Arabic and Chinese numeralsSymbol SymbolsAlphabet AlphabetsChinese Chinese charactersOther OthersTable 4: Character types.igram features taken from (Nakagawa, 2004).These templates operate over a window of ?2characters.
The features include characters (C0),pairs of characters (C1?C2), character types (C3),and pairs of character types (C4?C5).
In addi-tion, we add pairs of characters and character types(C6).Bigram features: Table 3 shows our bigramfeatures.
Templates B0-B9 are basic word-level bigram features.
These features aim tocapture all the possible combinations of wordand POS bigrams.
Templates TB0-TB6 are thetypes of characters for bigrams.
For example,?TE(w?1), TB(w0)?
captures the change of char-acter types from the end character in the previ-ous word to the beginning character in the currentword.Note that if one of the adjacent nodes is acharacter-level node, we use the template CB0 thatrepresents POS bigrams.
In our preliminary ex-periments, we found that if we add more featuresto non-word-level bigrams, the number of featuresgrows rapidly due to the dense connections be-tween non-word-level nodes.
However, these fea-tures only slightly improve performance over us-ing simple POS bigrams.517(a) Experiments on small training corpusData set CTB chap.
IDs # of sent.
# of wordsTraining 1-270 3,046 75,169Development 301-325 350 6,821Test 271-300 348 8,008# of POS tags 32OOV (word) 0.0987 (790/8,008)OOV (word & POS) 0.1140 (913/8,008)(b) Experiments on large training corpusData set CTB chap.
IDs # of sent.
# of wordsTraining 1-270, 18,089 493,939400-931,1001-1151Development 301-325 350 6,821Test 271-300 348 8,008# of POS tags 35OOV (word) 0.0347 (278/8,008)OOV (word & POS) 0.0420 (336/8,008)Table 5: Training, development, and test datastatistics on CTB 5.0 used in our experiments.5 Experiments5.1 Data setsPrevious studies on joint Chinese word segmen-tation and POS tagging have used Penn ChineseTreebank (CTB) (Xia et al, 2000) in experiments.However, versions of CTB and experimental set-tings vary across different studies.In this paper, we used CTB 5.0 (LDC2005T01)as our main corpus, defined the training, develop-ment and test sets according to (Jiang et al, 2008a;Jiang et al, 2008b), and designed our experimentsto explore the impact of the training corpus size onour approach.
Table 5 provides the statistics of ourexperimental settings on the small and large train-ing data.
The out-of-vocabulary (OOV) is definedas tokens in the test set that are not in the train-ing set (Sproat and Emerson, 2003).
Note that thedevelopment set was only used for evaluating thetrained model to obtain the optimal values of tun-able parameters.5.2 EvaluationWe evaluated both word segmentation (Seg) andjoint word segmentation and POS tagging (Seg& Tag).
We used recall (R), precision (P ), andF1 as evaluation metrics.
Following (Sproat andEmerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) to-kens.
These performance measures can be calcu-lated as follows:Recall (R) = # of correct tokens# of tokens in test dataPrecision (P ) = # of correct tokens# of tokens in system outputF1 = 2 ?R ?
PR+ PROOV = # of correct OOV tokens# of OOV tokens in test dataRIV = # of correct IV tokens# of IV tokens in test dataFor Seg, a token is considered to be a cor-rect one if the word boundary is correctly iden-tified.
For Seg & Tag, both the word boundary andits POS tag have to be correctly identified to becounted as a correct token.5.3 Parameter estimationOur model has three tunable parameters: the num-ber of training iterations N ; the number of topk-best paths; and the threshold r for infrequentwords.
Since we were interested in finding anoptimal combination of word-level and character-level nodes for training, we focused on tuning r.We fixed N = 10 and k = 5 for all experiments.For the baseline policy, we varied r in the rangeof [1, 5] and found that setting r = 3 yielded thebest performance on the development set for boththe small and large training corpus experiments.For the error-driven policy, we collected unidenti-fied unknown words using 10-fold cross validationon the training set, as previously described in Sec-tion 3.5.4 Impact of policies for correct pathselectionTable 6 shows the results of our word-characterhybrid model using the error-driven and baselinepolicies.
The third and fourth columns indicate thenumbers of known and artificial unknown wordsin the training phase.
The total number of wordsis the same, but the different policies yield differ-ent balances between the known and artificial un-known words for learning the hybrid model.
Op-timal balances were selected using the develop-ment set.
The error-driven policy provides addi-tional artificial unknown words in the training set.The error-driven policy can improve ROOV as wellas maintain good RIV, resulting in overall F1 im-provements.518(a) Experiments on small training corpus# of words in training (75,169)Eval type Policy kwn.
art.
unk.
R P F1 ROOV RIVSeg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367(b) Experiments on large training corpus# of words in training (493,939)Eval Type Policy kwn.
art.
unk.
R P F1 ROOV RIVSeg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552Table 6: Results of our word-character hybrid model using error-driven and baseline policies.Method Seg Seg & TagOurs (error-driven) 0.9787 0.9367Ours (baseline) 0.9779 0.9360Jiang08a 0.9785 0.9341Jiang08b 0.9774 0.9337N&U07 0.9783 0.9332Table 7: Comparison of F1 results with previousstudies on CTB 5.0.Seg Seg & TagN&U07 Z&C08 Ours N&U07 Z&C08 OursTrial (base.)
(base.
)1 0.9701 0.9721 0.9732 0.9262 0.9346 0.93582 0.9738 0.9762 0.9752 0.9318 0.9385 0.93803 0.9571 0.9594 0.9578 0.9023 0.9086 0.90674 0.9629 0.9592 0.9655 0.9132 0.9160 0.92235 0.9597 0.9606 0.9617 0.9132 0.9172 0.91876 0.9473 0.9456 0.9460 0.8823 0.8883 0.88857 0.9528 0.9500 0.9562 0.9003 0.9051 0.90768 0.9519 0.9512 0.9528 0.9002 0.9030 0.90629 0.9566 0.9479 0.9575 0.8996 0.9033 0.905210 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225Avg.
0.9595 0.9590 0.9611 0.9085 0.9134 0.9152Table 8: Comparison of F1 results of our baselinemodel with Nakagawa and Uchimoto (2007) andZhang and Clark (2008) on CTB 3.0.Method Seg Seg & TagOurs (baseline) 0.9611 0.9152Z&C08 0.9590 0.9134N&U07 0.9595 0.9085N&L04 0.9520 -Table 9: Comparison of averaged F1 results (by10-fold cross validation) with previous studies onCTB 3.0.5.5 Comparison with best prior approachesIn this section, we attempt to make meaning-ful comparison with the best prior approaches re-ported in the literature.
Although most previousstudies used CTB, their versions of CTB and ex-perimental settings are different, which compli-cates comparison.Ng and Low (2004) (N&L04) used CTB 3.0.However, they just showed POS tagging resultson a per character basis, not on a per word basis.Zhang and Clark (2008) (Z&C08) generated CTB3.0 from CTB 4.0.
Jiang et al (2008a; 2008b)(Jiang08a, Jiang08b) used CTB 5.0.
Shi andWang (2007) used CTB that was distributed in theSIGHAN Bakeoff.
Besides CTB, they also usedHowNet (Dong and Dong, 2006) to obtain seman-tic class features.
Zhang and Clark (2008) indi-cated that their results cannot directly compare tothe results of Shi and Wang (2007) due to differentexperimental settings.We decided to follow the experimental settingsof Jiang et al (2008a; 2008b) on CTB 5.0 andZhang and Clark (2008) on CTB 4.0 since theyreported the best performances on joint word seg-mentation and POS tagging using the training ma-terials only derived from the corpora.
The perfor-mance scores of previous studies are directly takenfrom their papers.
We also conducted experimentsusing the system implemented by Nakagawa andUchimoto (2007) (N&U07) for comparison.Our experiment on the large training corpus isidentical to that of Jiang et al (Jiang et al, 2008a;Jiang et al, 2008b).
Table 7 compares the F1 re-sults with previous studies on CTB 5.0.
The resultof our error-driven model is superior to previousreported results for both Seg and Seg & Tag, andthe result of our baseline model compares favor-ably to the others.Following Zhang and Clark (2008), we firstgenerated CTB 3.0 from CTB 4.0 using sentenceIDs 1?10364.
We then divided CTB 3.0 intoten equal sets and conducted 10-fold cross vali-519dation.
Unfortunately, Zhang and Clark?s exper-imental setting did not allow us to use our error-driven policy since performing 10-fold cross val-idation again on each main cross validation trialis computationally too expensive.
Therefore, weused our baseline policy in this setting and fixedr = 3 for all cross validation runs.
Table 8 com-pares the F1 results of our baseline model withNakagawa and Uchimoto (2007) and Zhang andClark (2008) on CTB 3.0.
Table 9 shows a sum-mary of averaged F1 results on CTB 3.0.
Ourbaseline model outperforms all prior approachesfor both Seg and Seg & Tag, and we hope thatour error-driven model can further improve perfor-mance.6 Related workIn this section, we discuss related approachesbased on several aspects of learning algorithmsand search space representation methods.
Max-imum entropy models are widely used for wordsegmentation and POS tagging tasks (Uchimotoet al, 2001; Ng and Low, 2004; Nakagawa,2004; Nakagawa and Uchimoto, 2007) since theyonly need moderate training times while they pro-vide reasonable performance.
Conditional randomfields (CRFs) (Lafferty et al, 2001) further im-prove the performance (Kudo et al, 2004; Shiand Wang, 2007) by performing whole-sequencenormalization to avoid label-bias and length-biasproblems.
However, CRF-based algorithms typ-ically require longer training times, and we ob-served an infeasible convergence time for our hy-brid model.Online learning has recently gained popularityfor many NLP tasks since it performs comparablyor better than batch learning using shorter train-ing times (McDonald, 2006).
For example, a per-ceptron algorithm is used for joint Chinese wordsegmentation and POS tagging (Zhang and Clark,2008; Jiang et al, 2008a; Jiang et al, 2008b).Another potential algorithm is MIRA, which in-tegrates the notion of the large-margin classifier(Crammer, 2004).
In this paper, we first intro-duce MIRA to joint word segmentation and POStagging and show very encouraging results.
Withregard to error-driven learning, Brill (1995) pro-posed a transformation-based approach that ac-quires a set of error-correcting rules by comparingthe outputs of an initial tagger with the correct an-notations on a training corpus.
Our approach doesnot learn the error-correcting rules.
We only aim tocapture the characteristics of unknown words andaugment their representatives.As for search space representation, Ng andLow (2004) found that for Chinese, the character-based model yields better results than the word-based model.
Nakagawa and Uchimoto (2007)provided empirical evidence that the character-based model is not always better than the word-based model.
They proposed a hybrid approachthat exploits both the word-based and character-based models.
Our approach overcomes the limi-tation of the original hybrid model by a discrimi-native online learning algorithm for training.7 ConclusionIn this paper, we presented a discriminative word-character hybrid model for joint Chinese wordsegmentation and POS tagging.
Our approachhas two important advantages.
The first is ro-bust search space representation based on a hy-brid model in which word-level and character-level nodes are used to identify known and un-known words, respectively.
We introduced a sim-ple scheme based on the error-driven concept toeffectively learn the characteristics of known andunknown words from the training corpus.
The sec-ond is a discriminative online learning algorithmbased on MIRA that enables us to incorporate ar-bitrary features to our hybrid model.
Based on ex-tensive comparisons, we showed that our approachis superior to the existing approaches reported inthe literature.
In future work, we plan to applyour framework to other Asian languages, includ-ing Thai and Japanese.AcknowledgmentsWe would like to thank Tetsuji Nakagawa for hishelpful suggestions about the word-character hy-brid model, Chen Wenliang for his technical assis-tance with the Chinese processing, and the anony-mous reviewers for their insightful comments.ReferencesMasayuki Asahara.
2003.
Corpus-based Japanesemorphological analysis.
Nara Institute of Scienceand Technology, Doctor?s Thesis.Harald Baayen and Richard Sproat.
1996.
Estimat-ing lexical priors for low-frequency morphologi-cally ambiguous forms.
Computational Linguistics,22(2):155?166.520Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8.Koby Crammer, Ryan McDonald, and FernandoPereira.
2005.
Scalable large-margin online learn-ing for structured classification.
In NIPS Workshopon Learning With Structured Outputs.Koby Crammer.
2004.
Online Learning of Com-plex Categorial Problems.
Hebrew Univeristy ofJerusalem, PhD Thesis.Zhendong Dong and Qiang Dong.
2006.
Hownet andthe Computation of Meaning.
World Scientific.Kuzman Ganchev, Koby Crammer, Fernando Pereira,Gideon Mann, Kedar Bellare, Andrew McCallum,Steven Carroll, Yang Jin, and Peter White.
2007.Penn/umass/chop biocreative ii systems.
In Pro-ceedings of the Second BioCreative Challenge Eval-uation Workshop.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008a.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of ACL.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008b.
Wordlattice reranking for chinese word segmentation andpart-of-speech tagging.
In Proceedings of COLING.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields tojapanese morphological analysis.
In Proceedings ofEMNLP, pages 230?237.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML, pages 282?289.Ryan McDonald, Femando Pereira, Kiril Ribarow, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT/EMNLP, pages 523?530.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.University of Pennsylvania, PhD Thesis.Masaki Nagata.
1994.
A stochastic japanese mor-phological analyzer using a forward-DP backward-A* n-best search algorithm.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics, pages 201?207.Masaki Nagata.
1999.
A part of speech estimationmethod for japanese unknown words using a statis-tical model of morphology and context.
In Proceed-ings of ACL, pages 277?284.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.
Ahybrid approach to word segmentation and pos tag-ging.
In Proceedings of ACL Demo and Poster Ses-sions.Tetsuji Nakagawa.
2004.
Chinese and japanese wordsegmentation using word-level and character-levelinformation.
In Proceedings of COLING, pages466?472.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofEMNLP, pages 277?284.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP, pages 133?142.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layercrfs based joint decoding method for cascaded seg-mentation and labeling tasks.
In Proceedings of IJ-CAI.Richard Sproat and Thomas Emerson.
2003.
The firstinternational chinese word segmentation bakeoff.
InProceedings of the 2nd SIGHAN Workshop on Chi-nese Language Processing, pages 133?143.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-hara.
2001.
The unknown word problem: a morpho-logical analysis of japanese using maximum entropyaided by a dictionary.
In Proceedings of EMNLP,pages 91?99.Fei Xia, Martha Palmer, Nianwen Xue, Mary EllenOkurowski, John Kovarik, Fu dong Chiou, andShizhe Huang.
2000.
Developing guidelines andensuring consistency for chinese text annotation.
InProceedings of LREC.Stavros A. Zenios Yair Censor.
1997.
Parallel Op-timization: Theory, Algorithms, and Applications.Oxford University Press.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging on a single perceptron.
InProceedings of ACL.521
