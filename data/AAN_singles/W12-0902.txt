Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 10?18,Avignon, France, April 24 2012. c?2012 Association for Computational LinguisticsEmpiricist Solutions to Nativist Puzzles by means of Unsupervised TSGRens Bod Margaux SmetsInstitute for Logic, Language & Computation Institute for Logic, Language & ComputationUniversity of Amsterdam University of AmsterdamScience Park 904, 1098XH Amsterdam, NL Science Park 904, 1098XH Amsterdam, NLrens.bod@uva.nl margauxsmets@gmail.comAbstractWhile the debate between nativism and em-piricism exists since several decades, sur-prisingly few common learning problemshave been proposed for assessing the twoopposing views.
Most empiricist researchershave focused on a relatively small number oflinguistic problems, such as Auxiliary Front-ing or Anaphoric One.
In the current paperwe extend the number of common test casesto a much larger series of problems relatedto wh-questions, relative clause formation,topicalization, extraposition from NP andleft dislocation.
We show that these hardcases can be empirically solved by an unsu-pervised tree-substitution grammar inferredfrom child-directed input in the Adam cor-pus (Childes database).1 Nativism versus EmpiricismHow much knowledge of language is innate andhow much is learned through experience?
The na-tivist view endorses that there is an innate lan-guage-specific component and that humanlanguage acquisition is guided by innate rules andconstraints (?Universal Grammar?).
The empiricistview assumes that there is no language-specificcomponent and that language acquisition is theproduct of abstractions from empirical input bymeans of general cognitive capabilities.
Despitethe apparent opposition between these two views,the essence of the debate lies often in the relativecontribution of prior knowledge and linguistic ex-perience (cf.
Lidz et al 2003; Clark and Lappin2011; Ambridge & Lieven 2011).
Following thenativist view, the linguistic evidence is so hope-lessly underdetermined that innate components arenecessary.
This Argument from the Poverty of theStimulus can be phrased as follows (see Pullum &Scholz 2002 for a detailed discussion):(i) Children acquire a certain linguistic phe-nomenon(ii) The linguistic input does not give enoughevidence for acquiring the phenomenon(iii) There has to be an innate component forthe phenomenonIn this paper we will falsify step (ii) for a largenumber of linguistic phenomena that have beenconsidered ?parade cases?
of innate constraints(Crain 1991; Adger 2003; Crain and Thornton2006).
We will show that even if a linguistic phe-nomenon is not in a child?s input, it can be learnedby an ?ideal?
learner from a tiny fraction of child-directed utterances, namely by combining frag-ments from these utterances using the Adam cor-pus in the Childes database (MacWhinney 2000).Previous work on empirically solving na-tivist puzzles, focused on a relatively small set ofphenomena such as auxiliary fronting (Reali &Christiansen 2005; Clark and Eyraud 2006) andAnaphoric One (Foraker et al 2009).
Some of theproposed solutions were based on linear models,such as trigram models (Reali & Christiansen2005), though Kam et al (2008) showed that thesuccess of these models depend on accidental Eng-lish facts.
Other empiricist approaches have takenthe notion of structural dependency together with a10combination operation as minimal requirements(e.g.
Bod 2009), which overcomes the problemsraised by Kam et al (2008).
Yet, it remains anopen question which of the many other syntacticphenomena in the nativist literature can be ac-quired by such a general learning method on thebasis of child-directed speech.In this paper we will deal with a much lar-ger set of problems than used before in empiricistcomputational models.
These problems are well-known in the generativist literature (e.g.
Ross1967; Adger 2003; Borsley 2004) and are relatedto wh-questions, relative clause formation, topical-ization, extraposition and left dislocation.
It turnsout that these hard cases can be learned by a simpleunsupervised grammar induction algorithm thatreturns the sentence with the best-ranked deriva-tion for a particular phenomenon, using only a verysmall fraction of the input a child receives.2 MethodologyOur methodology is very simple: by means of aninduced Tree-Substitution Grammar or TSG (seeBod 2009 for an in-depth study), we compute fromthe alternative sentences of a syntactic phenome-non reported in the generativist literature -- seebelow -- the sentence with the best-ranked shortestderivation (see Section 3) according to the unsu-pervised TSG.
Next, we check whether this sen-tence corresponds with the grammatical sentence.For example, given a typical nativist prob-lem like auxiliary fronting, the question is: how dowe choose the correct sentence from among thealternatives (0) to (2):(0) Is the boy who is eating hungry?
(1) *Is the boy who eating is hungry?
(2) *Is the boy who is eating is hungry?According to Adger (2003), Crain (1991) and oth-ers, this phenomenon is regulated by an innateprinciple.
In our empiricist approach, instead, weparse all three sentences by our TSG.
Next, thesentence with the best-ranked shortest derivation iscompared with the grammatical expression.Ideally, rather than selecting from givensentences, we would like to have a model thatstarts with a certain meaning representation forwhich next the best sentence is generated.
In theabsence of such a semantic component, we let ourmodel select directly from the set of possible sen-tences as they are provided in the literature as al-ternatives, where we will mostly focus on theclassical work by Ross (1967) supplemented by themore recent work of Adger (2003) and Borsley(2004).
In section 9 we will discuss the shortcom-ings of our approach and suggest some improve-ments for future research.3 Grammar induction with TSG: thebest-ranked k-shortest derivationFor our induced grammar, we use the formalism ofTree-Substitution Grammar.
This formalism hasrecently generated considerable interest in the fieldof grammar induction (e.g.
Bod 2006; O?Donnellet al 2009; Post and Gildea 2009; Cohn et al2010).
As noted by Cohn et al (2010) and others,this formalism has a number of advantages.
Forexample, its productive units (elementary trees ofarbitrary size) allow for both structural and lexicalsensitivity (see Bod et al 2003), while grammars inthis formalism are still efficiently learnable from acorpus of sentences in cubic time and space.As an example, figure 1 gives two TSGderivations and parse trees for the sentence Shesaw the dress with the telescope.
Note that the firstderivation corresponds to the shortest derivation,as it consists of only two elementary trees.theNPPwith telescopePPNPsawVPVVPNPsheStheNPdress=theNPPwith telescopePPtheNPsawVPVVPNPsheS?dressPPtheNPdressNPVVPNPsheSsawVtheNPPwith telescopePP =?
?PPtheNPdressNPVVPNPsheSsaw theNPPwith telescopeFigure 1.
Two TSG derivations, resulting in differ-ent parse trees, for the sentence She saw the dresswith the telescopeOur induction algorithm is similar to Bod(2006) where first, all binary trees are assigned to aset of sentences, and next, the relative frequenciesof the subtrees in the binary trees (using a PCFG11reduction, see below) are used to compute the mostprobable trees.
While we will use Bod?s  method ofassigning all binary trees to a set of sentences, wewill not compute the most probable tree or sen-tence.
Instead we compute the k-shortest deriva-tions for each sentence after which the sum ofranks of the subtrees in the k derivations deter-mines the best-ranked shortest derivation (Bod2000).
This last step is important, since the shortestderivation alone is known to perform poorly(Bansal and Klein 2011).
In Zollmann and Sima?an(2005) it is shown that training by means of short-est derivations corresponds to maximum likelihoodtraining in the limit if the corpus grows to infinity.Our approach to focus on the k shortest deri-vation rather than the most probable tree or mostprobable sentence is partly motivated by our dif-ferent task: it is well-known that the probability ofa sentence decreases exponentially with sentencelength.
This is problematic since, when choosingamong alternative sentences, the longest sentencemay be (the most) grammatical.
Instead, by focus-ing on the (k-) shortest derivations this problemcan ?
at least partly ?
be overcome.From an abstract level, our grammar inductionalgorithm works as follows (see also Zollmann andSima?an 2005).
Given a corpus of sentences:1.
Divide the corpus into a 50% Extraction Cor-pus (EC) and a 50% Held out Corpus (HC).2.
Assign all unlabeled binary trees to the sen-tences in EC and store them in a parse forest.3.
Convert the subtrees from the parse forestsinto a compact PCFG reduction (Goodman2003).4.
Compute the k-shortest derivations for thesentences in HC using the PCFG reduction.5.
Compute the best-ranked derivation for eachsentence by the sum of the ranks of the sub-trees (where the most frequent subtrees getrank 1, next most frequent subtrees get rank 2,etc., thus the best-ranked derivation is the onewith the lowest total score).6.
Use the subtrees in the trees generated by thebest-ranked derivations to form the TSG (fol-lowing Zollmann & Sima?an 2005).The learning algorithm above does not inducePOS-tags.
In fact, in our experiments below we testdirectly on POS-strings.
This makes sense becausethe nativist constraints are also defined on catego-ries of words, and not on specific sentences.
Ofcourse, future work should also parse directly withword strings instead of with POS strings (for whichunsupervised POS-taggers may be used).Rather than using the (exponentially many)subtrees from the binary trees to construct our TSG,we convert them into a more compact homomor-phic PCFG.
We employ Goodman?s reductionmethod where each node in a tree is converted intoexactly 8 PCFG rules (Goodman 2003).
ThisPCFG reduction is linear in the number of nodes inthe corpus (Goodman 2003, pp.
130-133).The k-shortest derivations can be computedby Viterbi by assigning each elementary tree equalprobability (Bod 2000).
We follow the third algo-rithm in Huang and Chiang (2005), where first atraditional Viterbi-chart is created, which enumer-ates in an efficient way all possible subderivations.Next, the algorithm starts at the root node and re-cursively looks for the k-best derivations, wherewe used k = 100.
In addition, we employed thesize reduction technique developed in Teichmann(2011) for U-DOP/TSG.We used all 12K child-directed utterances inthe Adam corpus from the Childes database(MacWhinney 2000).
These utterances come withPOS-tags, which were stripped off the sentencesand fed to our TSG induction algorithm.
The child-directed sentences were randomly split into 50%EC and 50% HC.
The subtrees from EC were usedto derive a TSG for the POS-strings from HC.
Theresulting TSG consisted of 914,744 different sub-trees.
No smoothing was used.
With the methodol-ogy explained in Section 2, we used this TSG totest against a number of well-known nativist prob-lems from the literature (Ross 1967; Adger 2003).It may be important to stress that the Adamcorpus is based on only 2 hours of recordings perfortnight.
This corresponds to just a tiny fraction ofthe total number of utterances heard by Adam.Thus our TSG has access only to this very smallfraction of Adam?s linguistic input, and we do notassume that our model (let alne a child) literallystores all previously heard utterances.4 The problem of wh-questionsThe study of wh-questions or wh-movement is oneof oldest in syntactic theory (Ross 1967) and isusually dealt with by a specific set of ?island con-straints?, where islands are constituents out of12which wh-elements cannot move.
These con-straints are incorporated in the more recent Mini-malist framework (Adger 2003, pp.
389ff).
Ofcourse, our goal is different from Minimalism (orgenerative grammar in general).
Rather than tryingto explain the phenomenon by separate constraints,we try to model them by just one, more generalconstraint: the best-ranked (k-shortest) derivation.We do not intend to show that the constraints pro-posed by Ross, Adger and others are incorrect.
Wewant to demonstrate that these constraints can alsobe modeled by a more general principle.
Addition-ally, we intend to show that the phenomena relatedto wh-questions can be modeled by using only atiny fraction of child-directed speech.4.1 Unbounded scope of wh-questionsFirst of all we must account for the seemingly un-bounded scope of wh-movement: wh-questions canhave infinitely deep levels of embedding.
The puz-zle lies in the fact that children only hear construc-tions of level 1, e.g.
(3), but how then is it possiblethat they can generalize (certainly as adults) thissimple construction to more complex ones of lev-els 2 and 3 (e.g.
(4) and (5))?
(3) who did you steal from?
(4) who did he say you stole from?
(5) who did he want her to say you stole from?The initial nativist answer developed by Ross(1967) was to introduce a transformational rulewith variables, and in the more recent Minimalistframework it is explained by a complex interplaybetween the so-called Phase Impenetrability Con-straint and the Feature Checking Requirement(Adger 2003).Our model proposes instead to build con-structions like (4) and (5) by simply using frag-ments children heard before.
When we let ourinduced TSG parse sentence (3), we obtain the fol-lowing derivation consisting of 3 subtrees (wherethe operation ?o?
stands for leftmost node substitu-tion of TSG-subtrees).
For reasons of space, werepresent the unlabeled subtrees by squared brack-ets, and for reasons of readability we substitute thePOS-tags with the words.
(As mentioned above wetrained and tested only with POS-strings.
)[X [who [X [did X]]] o [X [X from]] o [X [yousteal]] =[X [who [X [did [X [[X [you steal]]  from]]]]]Although this derivation is not the shortest one interms of number of subtrees, it obtained the bestranking (sum of subtree ranks) among the 100-shortest derivations.
In fact, the derivation aboveconsists of three highly frequent subtrees with (re-spective) ranking of 1,153 + 7 + 488 = 1,648.
Theabsolute shortest derivation (k=1) consisted of onlyone subtree (i.e.
the entire tree) but had a rankingof 26,223.Sentences (4) and (5) could also be parsedby combinations of three subtrees, which in thiscase were also the shortest derivations.
The follow-ing is the shortest derivation for (4):[X [who [X [did he say X]]] o [X [X from]] o [X[you stole]] =[X [who [X [did he say [X [[X [you stole]]from]]]]]It is important to note that when looking at thespeech produced by Adam himself, he only pro-duced (3) but not (4) and (5) ?
and neither had heheard these sentences as a whole.
It thus turns outthat our induced TSG can deal with the presumedunbounded scope of wh-questions on the basis ofsimple combination of fragments heard before.4.2 Complex NP constraintThe first constraint-related problem we deal with isthe difference in grammaticality between sentences(4), (5) and (6), (7):(6) *who did you he say stole from?
(7) * who did you he want her to say stole from?The question usually posed is: how do childrenknow that they can generalize from what they hearin sentence (3) to sentences (4) and (5) but not to(6) and (7).
This phenomenon is dealt with in gen-erative grammar by introducing a specific restric-tion: the complex NP constraint (see Adger 2003).But we can also solve it by the best-ranked deriva-tion.
To do so, we compare sentences with thesame level of embedding, i.e.
(4) and (6), both of13level 2, and (5) and (7), of level 3.
We thus viewrespectively (4), (6) and (5), (7) as competing ex-pressions.It turns out that (6) like (4) can be derivedby minimally 3 subtrees, but with a worse rankingscore.
Similarly, (7) can also be derived by mini-mally 3 subtrees with a worse ranking score than(5).
Since we tested on POS-strings, the resultholds not only for these sentences of respectivelevels 2 and 3, but for all sentences of this type.Thus rather than assuming that the complex NPconstraint must be innate, it can be modelled byrecombining fragments from a fraction of previousutterances on the basis of the best-ranked deriva-tion.4.3 Left branch conditionThe second wh-phenomenon we will look into isknown as the Left Branch Condition (Ross 1967;Adger 2003).
This condition has to do with thedifference in grammaticality between (8) and (9):(8) which book did you read?
(9) *which did you read book?When we let our TSG parse these two sentences,we get the respective derivations (8?)
and (9?
),where for reasons of readability we now give thesubstree-yields only:(8?)
[X you read] o [which X] o [book did]ranking: 608 + 743 + 8,708 = 10,059(9?)
[which did X] o [you read book]ranking: 12,809 + 1 = 12,810Here we thus have a situation that, when looking atthe 100-best derivations, the subtree ranking over-rules the shortest derivation: although (9?)
isshorter than (8?
), the rank of (8?)
neverthelessoverrules (9?
), leading to the correct alternative.
Ofcourse, it has to be seen whether this perhaps coin-cidentally positive result can be confirmed on otherchild-directed corpora.4.4 Subject wh-questionsAn issue that is not considered in early work onwh-questions (such as Ross 1967), but covered inthe minimalist framework is the phenomenon thatarises with subject wh-questions.
We have to ex-plain how children know that (10) is the grammati-cal way of asking the particular question, and (11),(12) and (13) are not.
(10)  who kissed Bella(11) *kissed who Bella(12) *did who kiss Bella(13) *who did kiss BellaWhen we let our model parse these sentences, weobtain the following four derivations (where wegive again only the subtree-yields):(10?)
[who X] o [kissed Bella]ranking: 22 + 6,694 = 6,716(11?)
[X Bella] o [kissed who]ranking: 24 + 6,978 = 7,002(12?)
[did X Bella] o [who kiss]ranking: 4,230 + 8,527 = 12,757(13?)
[X kiss Bella] o [who did]ranking: 4,636 + 2,563 = 7,199Although all derivations are equally short, the best(= lowest) ranking score prefers the correct alterna-tive.4.5 Other wh-constraints modelled empiricallyBesides the constraints given above, there are vari-ous other constraints related to wh-questions.These include:?
Sentential Subject Constraint?
WH-questions in situ?
Embedded WH-questions?
WH-islands?
Superiority?
Coordinate Structure ConstraintAll but one of these constraints could be correctlymodelled by our TSG, preferring the correct alter-native on the basis of the best-ranked derivationand a fraction of a child?s input.
The only excep-14tion is the Coordinate Structure Constraint, as in(14) and (15):(14) you love chicken and what?
(15) *what do you love chicken and?Contrary to the ungrammaticality of (15), our TSGparser assigned the best rank to the derivation of(15).
Of course it has to be seen how our TSGwould perform on a corpus that is larger thanAdam.
Moreover, we will see that our TSG cancorrectly model the Coordinate Structure Con-straint for other phenomena, even on the basis ofthe Adam corpus.5 The problem of Relative clause formationA phenomenon closely related to wh-questions isrelative clause formation.
As in 4.2, generativ-ist/nativist approaches use the same complex NPconstraint to distinguish between the grammaticalsentence (16) and the ungrammatical sentence(17).
The complex NP constraint is in fact believedto be universal.
(16) the vampire who I read a book about is dan-gerous(17) *the vampire who I read a book which wasabout is dangerousIn (16), the ?moved?
phrase `the vampire' is takenout of the non-complex NP `a book about <thevampire>; in (17), however, `the vampire' is?moved?
out of the complex NP `a book which wasabout <the vampire>?, which is not allowed.Yet our TSG could also predict the correctalternative by means of the best ranked derivationalone, by respectively derivations (16?)
and (17?):(16?)
[the vampire X is dangerous] o [who I readX] o [a book about]ranking: 1,585,992 + 123,195 + 5,719 = 1,714,906(17?)
[the vampire X is dangerous] o [who I readX] o [a book which X] o [was about]ranking: 1,585,992 + 123,195 + 184,665 + 12,745= 1,906,597Besides the complex NP constraint, the phenome-non of relative clause formation also uses mostother constraints related to wh-questions: Leftbranch condition, Sentential Subject Constraint andCoordinate Structure Constraint.
All these con-straints could be modelled with the best-rankedderivation ?
this time including Coordinate struc-tures (as e.g.
(18) and (19)) that were unsuccess-fully predicted by our TSG for wh-questions.
(18) Bella loves vampires and werewolves who areunstable(19) *werewolves who Bella loves vampires andare unstable6 The problem of Extraposition from NPA problematic case for many nativist approaches isthe so-called ?Extraposition from NP?
problem forwhich only ad hoc solutions exist.
None of theconstraints previously mentioned can explain (20)and (21):(20) that Jacob picked Bella up who loves Edwardis possible(21) *that Jacob picked Bella up is possible wholoves EdwardAs Ross (1967), Borsley (2004) and others note,the Complex NP Constraint cannot explain (20)and (21), because it applies to elements of a sen-tence dominated by an NP, and here the movedconstituent `who loves Edward' is a sentencedominated by an NP.
Therefore, an additional con-cept needs to be introduced: `upward bounded-ness', where a rule is said to be upward bounded ifelements moved by that rule cannot be moved overthe boundaries of the first sentence above the ele-ments being operated on (Ross 1967; Borsley2004).Thus additional machinery is needed toexplain the phenomenon of Extraposition from NP.Instead, our notion of best ranked derivation needsno additional machinery and can do the job, asshown by derivations (20?)
and (21?):(20?)
[X is possible] o [that Jacob picked X] o[Bella up X] o [who loves Edward]ranking: 175 + 465,494 + 149,372 + 465,494 =1,080,535(21?)
[X is possible X] o [that Jacob picked X] o[Bella up] o [who loves Edward]15ranking: 3,257 + 465,494 + 176,910 + 465,494 =1,111,1557 The problem of TopicalizationAlso the phenomenon of Topicalization is sup-posed to follow the Complex NP constraint, Leftbranch condition, Sentential Subject Constraint andCoordinate Structure Constraint, all of which canagain be modelled by the best ranked derivation.For example, the topicalization in (22) is fine butin (23) it is not.
(22) Stephenie's book I read(23) * Stephenie's I read bookOur TSG predicts the correct alternative by meansof the best ranked derivation:(22?)
[X I read] o [Stephenie?s book]ranking: 608 + 2,784 = 3,392(23?)
[Stephenie?s X book] o [I read]ranking: 3,139 + 488 = 3,6278 The problem of Left dislocationThe phenomenon of Left dislocation provides aparticular challenge to nativist approaches since itshows that there are grammatical sentences that donot obey the Coordinate Structure Constraint (seeAdger 2003; Borsley 2004).
A restriction that ismentioned but not explained by Ross (1967), is thefact that in Left dislocation the moved constituentmust be moved to the left of the main clause.
In-stead, movement merely to the left of a subordinateclause results in an ungrammatical sentence.
Forexample, (24) is grammatical, because `Edward' ismoved to the left of the main clause.
Sentence(25), on the other hand, is ungrammatical, because`Edward' is only moved to the left of the subordi-nate clause `that you love <Edward>'.
(24) Edward, that you love him is obvious(25) *that Edward, you love him is obviousOur TSG has no problem in distinguishing be-tween these two alternatives, as is shown below:(24?)
[Edward X is obvious] o [that you love him]ranking: 590,659 + 57,785 = 648,444(25?)
[that X is obvious] o [Edward you love him]ranking: 876,625 + 415,940 = 1,292,5659 Discussion and conclusionWe have shown that an unsupervised TSG can cap-ture virtually all phenomena related to wh-questions in a simple and uniform way.
Further-more, we have shown that our model can be ex-tended to cover other phenomena, even phenomenathat fall out of the scope of the traditional nativistaccount.
Hence, for at least these phenomena, Ar-guments from Poverty of Stimulus can no longerbe invoked.
That is, step (ii) in Section 1 where itis claimed that children cannot learn the phenome-non on the basis of input alone, is refuted.Phenomenon          Succesful?Subject Auxiliary Fronting  yesWH-QuestionsUnbounded Scope   yesComplex NP Constraint  yesCoordinate Structure Constraint  noLeft Branch Condition   yesSubject WH-questions    yesWH in situ     yesSuperiority     yesExtended Superiority    yesEmbedded WH-questions  yesWH-islands    yesRelative Clause FormationComplex NP Constraint   yesCoordinate Structure Constraint yesSentential Subject Constraint   yesLeft Branch Condition   yesExtraposition from NP   yesTopicalizationComplex NP Constraint  yesCoordinate Structure Constraint  yesSentential Subject Constraint  yesLeft Branch Condition   yesLeft DislocationCoordinate Structure Constraint  yesRestriction     yesTable 1.
Overview of empiricist solutions to nativistproblems tested so far (using as input the child-directedsentences in the Adam corpus of the Childes database),and whether they were successful.16Table 1 gives an overview of all phenomena wehave tested so far with our model, and whetherthey can be successfully explained by the best-ranked k-shortest derivation (not all of these phe-nomena could be explicitly dealt with in the cur-rent paper).Previous empiricist computational modelsthat dealt with learning linguistic phenomena typi-cally focused on auxiliary fronting (and sometimeson a couple of other problems ?
see Clark and Ey-raud 2006).
MacWhinney (2004) also describesways to model some other language phenomenaempirically, but this has not resulted into a compu-tational framework.
To the best of our knowledge,ours is the first empiricist computational modelthat also deals with the problems of wh-questions,relative clause formation, topicalization, extraposi-tion from NP and left dislocation.Many other computational models of lan-guage learning focus either on inducing syntacticstructure (e.g.
Klein and Manning 2005), or onevaluating which sentences can be generated by amodel with which precision and recall (e.g.
Ban-nard et al 2009; Waterfall et al 2010).
Yet thatwork leaves the presumed ?hard cases?
from thegenerativist literature untouched.
This may be ex-plained by the fact that most empiricist models donot deal with the concept of (absolute) grammati-cality, which is a central concept in the generativistframework.
It may therefore seem that the two op-posing approaches are incommensurable.
But thisis only partly so: most empiricist models do havean implicit notion of relative grammaticality orsome other ranking method for sentences and theirstructures.
In some cases, like our model, the top-ranking can simply be equated with the notion ofgrammaticality.
In this way empiricist and genera-tivist models can be evaluated on the same prob-lems.There remains a question what our unsu-pervised TSG then exactly explains.
It may bequite successful in refuting step (ii) in the Argu-ment from the Poverty of the Stimulus, but it doesnot really explain where the preferences of chil-dren come from.
Actually it only explains thatthese preferences come from child-directed inputprovided by caregivers.
Thus the next question is:where do the caregivers get their preferences from?From their caregivers -- ad infinitum?
It is exactlythe goal of generative grammar to try to answerthese questions.
But as we have shown in this pa-per, these answers are motivated by an argumentthat does not hold.
Thus our work should be seenas (1) a refutation of this argument (of the Povertyof the Stimulus) and (2) an alternative approachthat can model all the hard phenomena on the basisof just one principle (the best-ranked derivation).The question where the preferences may eventuallycome from, should be answered within the field oflanguage evolution.While our TSG could successfully learn anumber of linguistic phenomena, it still has short-comings.
We already explained that we have onlytested on part of speech strings.
While this is notessentially different from how the nativist ap-proach defines their constraints (i.e.
on categoriesand functions of words, not on specific wordsthemselves), we believe that any final modelshould be tested on word strings.
Moreover, wehave tested only on English.
There is a major ques-tion how our approach performs on other lan-guages, for example, with rich morphology.So far, our model only ranks alternativesentences (for a certain phenomenon).
Ideally, wewould want to test a system that produces for agiven meaning to be conveyed the various possiblesentences ordered in terms of their rankings, fromwhich the top-ranked sentence is taken.
In the ab-sence of a semantic component in our model, wecould only test the already given alternative sen-tences and assess whether our model could predictthe correct one.Despite these problems, our main result isthat with just a tiny fraction of a child?s input thecorrect sentence can be predicted by an unsuper-vised TSG for virtually all phenomena related towh-questions as well as for a number of other phe-nomena that even fall out of the scope of the tradi-tional generativist account.Finally it should be noted that our result isnot in contrast with all generativist work.
For ex-ample, in Hauser et al (2002), it was proposed thatthe core language faculty comprises just recursivetree structure and nothing else.
The work presentedin this paper may be the first to show that one gen-eral grammar induction algorithm makes languagelearning possible for a much wider set of pheno-mena than has previously been endeavored.If empiricist models want to compete withgenerativist models, they should compete in thesame arena, with the same phenomena.17ReferencesD.
Adger, 2003.
Core syntax: A minimalist approach.Oxford University Press, 2003.B.
Ambridge and E. Lieven, 2011).
Child LanguageAcquisition.
Contrasting Theoretical Approaches.Cambridge University Press.M.
Bansal and D. Klein, 2011.
The Surprising Variancein Shortest-Derivation Parsing, Proceedings ACL-HLT 2011.C.
Bannard, E. Lieven and M. Tomasello, 2009.
Model-ing Children?s Early Grammatical Knowledge, Pro-ceedings of the National Academy of Sciences, 106,17284-89.R.
Bod, R. Scha and K. Sima?an (eds.
), 2003.
Data-Oriented Parsing, CSLI Publications/University ofChicago Press.R.
Bod, 2006.
An all-subtrees approach to unsupervisedparsing.
Proceedings ACL-COLING.R.
Bod, 2009.
From Exemplar to Grammar: A Proba-bilistic Analogy-based Model of Language Learn-ing.
Cognitive Science, 33(5), 752-793.R.
Borsley, 2004.
Syntactic Theory: A Unified Ap-proach, Oxford University Press.A.
Clark and R. Eyraud, 2006.
Learning AuxiliaryFronting with Grammatical Inference.
ProceedingsCONLL 2006.A.
Clark and S. Lappin, 2011.
Linguistic Nativism andthe Poverty of the Stimulus, Wiley-Blackwell.T.
Cohn, P. Blunsom, and S. Goldwater, 2010.
InducingTree-Substitution Grammars, Journal of MachineLearning Research, JMLR 11, 3053-3096.S.
Crain, 1991.
Language acquisition in the absence ofexperience.
Behavorial and Brain Sciences, 14, 597-612.S.
Crain and R. Thornton.
Acquisition of syntax andsemantics, 2006.
In M. Traxler and M. Gernsbacher,editors, Handbook of Psycholinguistics.
Elsevier.S.
Foraker, T. Regier, N. Khetarpal, A. Perfors, and J.Tenenbaum, 2009.
Indirect Evidence and the Pov-erty of the Stimulus: The Case of Anaphoric One.Cognitive Science, 33, 287-300.J.
Goodman, 2003.
Efficient parsing of DOP withPCFG-reductions.
In R. Bod, R. Scha & K.
Sima?an(Eds.
), Data-oriented parsing, 125?146.
CSLI Pubs.M.
Hauser, N. Chomsky and T. Fitch, 2002.
The facultyof language: What is it, who has it, and how did itevolve?
Science, 298, 1569?1579.L .
Huang  and D. Chiang, 2005.
Better k-best parsing.In Proceedings IWPT 2005, pp.
53?64.X.
Kam, L. Stoyneshka, L. Tornyova, J. Fodor and W.Sakas, 2008.
Bigrams and the Richness of theStimulus.
Cognitive Science, 32, 771-787.D.
Klein and C. Manning, 2005 Natural languagegrammar induction with a generative constituent-context model.
Pattern Recognition, 38, 1407?1419.J.
Lidz, S. Waxman and J. Freedman, 2003.
What in-fants know about syntax but couldn?t have learned:experimental evidence for syntactic structure at 18months.
Cognition, 89, B65?B73B.
MacWhinney, 2000.
The CHILDES project: Toolsfor analyzing talk.
Mawah, NJ: ErlbaumB.
MacWhinney, 2004.
A multiple process solution tothe logical problem of language acquisition.
Journalof Child Language, 3, 883- 914.T.
O?Donnell, N. Goodman, and J. Tenenbaum, 2009.Fragment grammar: Exploring reuse in hierarchicalgenerative processes.
Technical Report MIT-CSAIL-TR-2009-013, MIT.M.
Post and D. Gildea, 2009.
Bayesian learning of a treesubstitution grammar.
In Proceedings of the ACL-IJCNLP 2009.G.
Pullum and B. Scholz, 2002.
Empirical assessment ofstimulus povery arguments.
The Linguist Review,19(2002), 9-50.F.
Reali and M. Christiansen, 2005.
Uncovering therichness of the stimulus: structure dependence andindirect statistical evidence.
Cognitive Science, 29,1007-1028.J.
Ross, 1967.
Constraints on variables in syntax.
PhDthesis, Massachusetts Institute of Technology.C.
Teichmann, 2011.
Reducing the size of the represen-tation for the uDOP-estimate.
Proceedings EMNLP2011.H.
Waterfall, B. Sandbank, L. Onnis, and S. Edelman,2010.
An empirical generative framework for com-putational modeling of language acquisition.
Journalof Child Language, 37, 671-703.A.
Zollmann and K. Sima'an.
2005.
A Consistent andEfficient Estimator for Data-Oriented Parsing.In Journal of Automata, Languages and Combina-torics, 10 (2005), 367-388.18
