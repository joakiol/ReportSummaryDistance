Minimizing Speaker Variation Effects for Speaker-IndependentSpeech RecognitionXuedong HuangSchoo l  o f  Computer  Sc ienceCarnegie  Mel lon  Univers i tyPittsburgh, PA 15213ABSTRACTFor speaker-independent speech recognition, speaker variation isoneof the major error sources.
In this paper, aspeaker-independentnor-malization etwork is constructed such that speaker variation effectscan be minimized.
To achieve this goal, multiple speaker clustersare constructed from the speaker-independent training database.
Acodeword-dependent neural network is associated with each speakercluster.
The cluster that contains the largest number of speakersis designated as the golden cluster.
The objective function is tominimize distortions between acoustic data in each cluster and thegolden speakercluster.
Performanceevaluation showedthat speaker-normalized front-end reduced the error ate by 15% for the DARPAresource management speaker-independent speech recognition task.1.
INTRODUCTIONFor speaker-independent speech recognition, speaker varia-tion is one of the major error sources.
As a typical ex-ample, the error rate of a well-trained speaker-dependentspeech recognition system is three times less than that ofa speaker-independent speech recognition system \[11\].
Tominimize speaker variation effects, we can use either speaker-clustered models \[28, 11\] or speaker normalization techniques\[2, 24, 3, 25, 7\].
Speaker normalization is interesting since itsapplication isnot restricted to a specific type of speech recog-nition systems.
In comparison with speaker normalizationtechniques, peaker-clustered models will not only fragmentdata, but also increase the computational complexity sub-stantially, since multiple models have to be maintained andcompared uring recognition.Recently, nonlinear mapping based on neural networks hasattracted considerable attention because of the ability of thesenetworks to optimally adjust the parameters from the train-ing data to approximate the nonlinear elationship betweentwo observed spaces (see \[22, 23\] for a review), albeit muchremains to be clarified regarding practical applications.
Non-linear mapping of two different observation spaces is of greatinterest for both theoretical nd practical purposes.
In the areaof speech processing, nonlinear mapping has been applied tonoise enhancement \[1, 32\], articulatory motion estimation\[29, 18\], and speech recognition \[16\].
Neural networks havebeen used successfully to transform data of a new speaker toa reference speaker for speaker-adaptive speech recognition\[11\].
In this paper, we will study how neural networks can beemployed to minimize speaker variation effects for speaker-independent speech recognition.
The network is used as anonlinear mapping function to transform speech data betweentwo speaker clusters.
The mapping function we used is char-acterized by three important properties.
First, the assemblyof mapping functions enhances overall mapping quality.
Sec-ond, multiple input vectors are used simultaneously in thetransformation.
Finally, the mapping function is derived fromtraining data and the quality will dependent on the availableamount of training data.We used the DARPA Resource Management (RM) task \[271as our domain to investigate he performance of speaker nor-malization.
The 997-word RM task is a database querytask designed from 900 sentence templates \[271.
We usedword-pair grammar that has a test-set perplexity of about 60.The speaker-independent training speech database consists of3990 training sentences from 109 speakers \[26\]).
The test setcomprises of a total of 600 sentences from 20 speakers.
Weused all training sentences tocreate multiple speaker clusters.A codeword-dependent neural network is associated with eachspeaker cluster.
The cluster that contains the largest numberof speakers is designated as the golden cluster.
The objec-tive function is to minimize distortions between acoustic datain each cluster and the golden speaker cluster.
Performanceevaluation showed that speaker-normalized front-end reducedthe error rate by 15% for the DARPA resource managementspeaker-independent speech recognition task.This paper is organized as follows.
In Section 2, thespeech recognition system SPHINX-II is reviewed.
Section 3presents neural network architecture.
Section 4 discusses itsapplications to speaker-independent speech recognition.
Ourfindings are summarized in Section 5.2.
REV IEW OF THE SPHINX-H SYSTEMIn comparison with the SPHINX system \[20\], the SPHINX-IIsystem \[6\] reduced the word error rate by more than 50%through incorporating between-word coarticulation model-ing \[13\], high-order dynamics \[9\], sex-dependent shared-distribution semi-continuous hidden Markov models \[9, 15\].This section will review SPHINX-II, which will be used asour baseline system for this study \[6\].1912.1.
Signal ProcessingThe input speech signal is sampled at 16 kHz with a pre-emphasized filter, 1 - 0.9Z -1.
A Hamming window witha width of 20 msec is applied to speech signal every 10msec.
The 32-order LPC analysis is followed to computethe 12-order cepstral coefficients.
Bilinear transformation fcepstral coefficients i employed to approximate r el-scalerepresentation.
I  addition, relative power is also computedtogether with eepstral coefficients.
Speech features used inSPHINX-II include (t is in units of 10 msec) LPC cepstralcoefficients; 40-msec and 80-msec differenced LPC cepstralcoefficients; second-order differenced cepstral coefficients;and power, 40-msec differenced power, second-order differ-enced power.
These features are vector quantized into fourindependent codebooks by the Linde-Buzo-Gray algorithm\[21\], each of which has 256 entries.2.2.
TrainingTraining procedures are based on the forward-backward al-gorithm.
Word models are formed by concatenating pho-netic models; sentence models by concatenating word models.There are two stages at training.
The first stage is to gener-ate the shared output distribution mapping table.
Forty-eightcontext-independent discrete phonetic models are initially es-timated from the uniform distribution.
Deleted interpolation\[17\] is used to smooth the estimated parameters with the uni-form distribution.
Then context-dependent models have tobe estimated based on context-independent ones.
There are7549 triphone models in the DARPA RM task when bothwithin-word and between-word triphones are considered.
Tofacilitate training, one codebook discrete models were used,where acoustic feature consists of the cepstrai coefficients, 40-msec differenced cepstrum, power and 40-msec differencedpower.
After the 7549 discrete models are obtained, the dis-tribution clustering procedure \[14\] is then applied to create4500 distributions ( enones).
The second stage is to train 4-codebook models.
We first estimate 48 context independent,four-codebook discrete models with the uniform distribution.With these context independent models and the senone ta-ble, we then estimate the shared-distribution SCHMMs \[9\].Because of substantial difference between male and femalespeakers, two sets of sex-dependent SCHMMs are are sepa-rately trained to enhance the performance.To summarize, the configuration of the SPHINX-II systemhas:?
four codebooks of acoustic features,?
shared-distribution between-word and within-word tri-phone models,?
sex-dependent SCHMMs.2.3.
RecognitionIn recognition, a language network is pre-compiled torepre-sent he search space.
For each input utterance, the (artificial)sex is first determined automatically asfollows \[8, 31\].
As-sume each codeword occurs equally and assume codeword iis represented by a Gaussian density function N(x, Pi, ~i) .Then given a segment of speech x~, Prsex, the probability thatx~" is generated from codebook-sex is approximated by:~ log(N(x,, .,, .Z',))t iE~ewhere r/t is a set that contains the top N codeword indicesduring quantization for cepstrum data xt at time t. If Prrnale> Pry~mat~, then x~ belongs to male speakers.
Otherwise, x~is female speech.
After the sex is determined, only the modelsof the determined sex are activated uring recognition.
Thissaves both CPU time and memory requirement.
For eachinput utterance, the Viterbi beam search algorithm is used tofind out the optimal state sequence in the language network.3.
NEURAL NETWORK ARCHITECTURE3.1.
Codeword-Dependent Neural Networks(CDNN)When presented with a large amount of training data, a singlenetwork is often unable to produce satisfactory esults dur-ing training as each network is only suitable to a relativelysmall task.
To improve the mapping performance, breakingup a large task and modular construction are usually required\[5, 7\].
This is because the nonlinear relationship between twospeakers is very complicated, a simple network may not bepowerful enough.
One solution is to partition the mappingspaces into smaller egions, and to construct a neural networkfor each region as shown in Figure 1.
As each neural net-work is trained on a separate region in the acoustic space, thecomplexity of the mapping required of each network is thusreduced.
In Figure 1, the switch can be used to select he mostlikely network or top N networks based on some probabilitymeasures of acoustic similarity \[101.
Functionally, the assem-bly of networks is similar to a huge neural network.
However,each network in the assembly is learned independently withtraining data for the corresponding regions.
This reducesthe complexity of finding a good solution in a huge space ofpossible network configurations since strong constraints areintroduced inperforming complex constraint satisfaction i amassively interconnected network.Vector quantization (VQ) has been widely used for data com-pression in speech and image processing.
Here, it can beused to to partition original acoustic space into different pro-totypes (codewords).
This partition can be regarded as aprocedure to perform broad-acoustic pattern classification.192Output  switchI NN1 II NN 2 \].. INN kInput s b sw/tchFigure 1: Codeword-dependent neural networks (CDNN).The broad-acoustic patterns are automatically generated via aself-organization procedure based on the LBG algorithm \[21\].When the codeword-dependent neural network (CDNN) wasconstructed from the data in the corresponding cell, it wasfound that learning for the CDNN converges very quicklyin comparison with a huge neural network.
The larger thecodebook, the quicker it converges.
However, the size ofcodebook relies on the number of available training data sincecodeword-dependent structure fragments training data.
Thesize of codebook should be determined experimentally.Speaker normalization i volves acoustic data transformationfrom one speaker cluster to another.
In general, let X a =xl,xz,a a ...x\[ be a sequence of observations (frames) at time 1,2, .. t of speaker a.
Here, each observation at time k, x\[, isa multidimensional vector, which usually characterizes someshort-time spectral features.
For the sequence of speech obser-vations X a produced by speaker-cluster a, our goal is to find amapping function .Tt'(X a ) such that ~(X  a ) resembles the cor-responding sequence of observations produced by speakers inthe golden speaker cluster.
Speaker variations include manyfactors such as vocal tract, pitch, speaking speed, intensity,and cultural differences.
Unfortunately, given two differentspeakers, there is no simple mapping function that can ac-count for all these variations.
Consequently, we are mainlyconcerned with spectral normalization.
For each frame x a,we want to find out a mapping function to transform it to x b,the corresponding phonetic realization produced by speakerb.
We believe that x\[ can represent most important featuresproduced by the speaker.
Thus, our objective functions is tominimize:a) - x b) ( I )corresponding pairswhere ~D(x,y) denotes a predefined istortion measure be-tween frame x and y, and corresponding pairs are con-structed to approximate acoustic realizations of differentspeakers.
Even if we are only interested in spectral nor-malization, there is no analytic mapping solution.
Instead,stochastic approach as to be used to study the nonlinear re-lationship between the two observed spaces.
We need to havea set of supervision data (corresponding pairs in Equation 1)to extract the nonlinear relationship.It has been found that dynamic information plays an impor-tant role in speech recognition \[4, 20, 12\].
As frame to framenormalization lacks use of dynamic information, the architec-ture of normalization network is thus chosen to incorporatemultiple neighboring frames.
One of such architectures isshown in Figure 2.
Here, the current frame and its left andright neighboring frames are fed to the multi-layer neural net-work as inputs.
The network output is a normalized framecorresponding to the current input frame.
By using multipleinput frames for the network, the important dynamic informa-tion can be effectively used in estimating network parametersand in normalization.
In Figure 2, there are input layer, hid-den layer, and output layer.
Each arc k is associated withnormalized frameprevious frame current frame next frameFigure 2: A basic neural network architecture.a weight wk.
In the hidden and output layer, each node ischaracterized byan internal offset 0.
The hidden node is alsocharacterized by a nonlinear sigmoid function.
The input toeach hidden node and output node is a weighted sum of cor-responding inputs with the offset 0.
Both the internal offsetand arc weights are learned by the backpropagation algorithm\[30\], which uses a gradient search to minimize the objectivefunction.
If the dimension of observation space is d and thenumber of input frames is m, we will have dxm input unitsin the normalization network.
If we want to incorporate moreneighboring frames, this will definitely increase the number offree parameters in the network.
Although the increase in thenumber of free parameters lead to quick convergence duringtraining, this nevertheless may not lead to improved general-193ization capability.
Since the network is designed to normalizenew data from a given speaker to the reference speaker, goodgeneralitzation capability will be the most important concern.Therefore, a compromise has to be made between generaliza-tion capability and the number of free parameters.3.2.
Golden Speaker-Cluster SelectionSpeaker-dependent CDNNs have been used successfully forspeaker-adaptive speech recognition \[7\] (speaker-dependentmapping).
If we need to map multiple speakers to one goldenspeaker and simply construct a speaker-independent CDNN, itis unlikely that a single network will do the job.
With the samerational as CDNN for speaker-adaptive speech recognition,we can partition multiple speakers into speaker-clusters andconstruct cluster-dependent CDNN.For speaker clustering, we first generated 48 phonetic HMMfor each speaker in the speaker-independent training database.Thus, for each speaker, we have a set of output distributions.We then merge the two speaker-clusters iteratively that re-sulted in the least loss of information, and then move ele-ments from cluster to cluster to improve the overall quality.The clustering procedure used here is similar to the one usedfor generalized triphone clustering \[19\].
We can continue theclustering process until the specified speaker-clusters a e ob-tained.
The golden speaker-cluster is the one that contains thelargest number of speakers.
We generated two golden clustersfor male and female respectively.4.
EXPERIMENTAL EVALUATION4.1.
Experiment conditionsThrough this study, only the cepstral vectors are consideredfor normalization.
Once we have the normalized cepstral vec-tor, the first-order and second-order time derivatives can becomputed.
We first clustered all the speakers in the train-ing set into male and female clusters, and then generated 10speaker-clusters formale and 7 speaker-clusters for female.We selected two golden speaker-clusters for both male andfemale.
There were 13 and 6 speakers in the male and femalegolden cluster espectively.
To provide learning examples fornetwork learning, we first segmented all the training utter-ances into triphones using Viterbi alignment and then usedthe DTW algorithm to warp the data to the corresponding tri-phone pairs in the golden speaker-cluster.
Thus, for a givenframe of each training speaker, the desired output frame fornetwork learning is the golden speaker frame paired in theDTW optimal path.4.2.
Benchmark ExperimentsAs benchmark experiments, speaker-independent speechrecognition using SPHINX-II was first evaluated.
The worderror rate we used here reflects all three types of errors and iscomputed assubstitutions + deletions + insertions100 totaiwords + insertions (2)The average rror rate was 3.8% for speaker-independentspeech recognition.4.3.
Normalization ResultsThe input of the network consists of three frames from thenew speaker.
Here, 12 cepstral coefficients and energy areused together.
Thus, there are 93 input units in the network.The output of the network has 13 units corresponding the nor-malized frame, which is made to approximate he frame of thedesired reference speaker.
The energy output is discarded as itis relative unstable.
The objective function for network learn-ing is to minimize the distortion (mean squared error) betweenthe network output and the desired reference speaker frame.The network has one hidden layer with 20 hidden units.
Eachhidden unit is associated with the generalized S IGMOIDfunction, where c~, /~ and 7 are predefined to be 4.0, 1.8,2.0 respectively.
They are fixed for all the experiments con-ducted here.
The weights and offsets in the network wereinitialized with small random values.
The learning step andmomentum are controlled ynamically.
Experimental experi-ence indicates that 300 to 600 epochs are required to achieveacceptable distortion.
We created two golden speaker clustersfor male and female respectively.
There were seven femaleclusters and ten male clusters, which are designed accordingto the available amount of male/female training data.
For eachspeaker cluster, we built a cluster-dependent codebook (size16).
For the input speech signal, joint VQ pdfs are used to se-lect the top 2-5 clusters for normalization.
Thus, let Ai denotethe probability that acoustic vector belong to cluster i, and ,t'idenote the normalized vector using the ith cluster-dependentCDNN.
The normalized vector 32 can then be computed asX = ~'  ~,x, (3)With the same training conditions as used in SPHINX-II,when the speaker-normalized front-end is used, we reducedthe error ate from 3.8% to 3.3%, which represented 15% errorreduction.
The modest error reduction indicated the mappingquality still needs to be improved substantially.5.
SUMMARYIn this paper, the codeword-dependent neural network(CDNN) was presented for speaker-independent speechrecognition.
The network was used as a nonlinear mappingfunction to transform speech data between speakers in eachcluster and the golden speaker cluster.
Performance evalu-ation showed that speaker-normalized front-end reduced theerror rate by 15%, as shown in Figure 3, for the DARPA194m , .~ Speaker-IndependentContinuous Speechm31 Baseline Vocabulary = 1000;~ SPHINX Test Perplexity = 60+Between-Word8.0 Trlphone?
High-OrderDynamics7.06o + Sex.DependentSCHMM5.0+ Senone4.0 :~::,~:::;:,: + Speaker~!~.~i!i!i~ Normalization2.0  \[:i:~:i:!
:~:~1.0SPHINX-II System Summaryresource management speaker-independent speech recogni-tion.
If we compare the error rate of speaker-dependent andspeaker-independent systems, this 15 % error eduction isrela-tively small.
We believe that he quality of mapping functionsis extremely important if we want to bridge the gap betweenspeaker-dependent and speaker-independent systems.AcknowledgmentsThis research was sponsored by the Defense Advanced Re-search Projects Agency (DOD), Arpa Order No.
5167, undercontract number N00039-85-C-0163.
The authors would liketo express their gratitude to Professor R. Reddy for his en-couragement and support.References\[1\] Acero, A. and Stern, R. Environmental Robustness inAutomatic Speech Recognition.
in: IEEE InternationalConference on ,acoustics, Speech, and Signal Pro-cessing.
1990, pp.
849-852.\[2\] Choukri, K., Chollet, G., and Grenier, Y. Spectral trans-formations through cannonical correlation analysis forspeaker adapataion i  ASR.
in: IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing.
1986, pp.
2659-2552.\[3\] Class, E, Kaltenmeier, A., Regel, P., and Trottler,K.
Fast speaker adaptation for speech recognition.in: IEEE International Conference on Acoustics,Speech, and Signal Processing.
1990, pp.
133-136.\[4\] Furui, S. Speaker-Independent Isolated Word Recogni-tion Using Dynamic Features of Speech Spectrum.
IEEE\[51\[6\]\[7\]\[8\]\[9\]\[10\]\[111\[121\[131\[141Transactions on Acoustics, Speech, and Signal Pro-cessing, vol.
ASSP-34 (1986), pp.
52-59.Hampshire, J. and Waibel, A.
The Meta-Pi Network:Connectionist rapid adapatation for high-performancemulti-speakerphoneme recognition, in: IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing.
1990, pp.
165-168.Huang, X., Alleva, E, Hon, H., Hwang, M., and Rosen-reid, R. The SPHINX-H Speech Recognition System:An Overview.
Technical Report, no.
CMU-CS-92-112,School of Computer Science, Carnegie Mellon Univer-sity, Pittsburgh, PA, February 1992.Huang, X.
Speaker Adaptation Using Codeword-Dependent Neural Networks.
in: IEEE Workshop onSpeech Recognition, Arden House.
1991.Huang, X.
A Study on Speaker-Adaptive Speech Recog-nition, in: DARPA Speech and Language Workshop.Morgan Kaufmann Publishers, San Mateo, CA, 1991.Huang, X., Alleva, E, Hayamizu, S., Hon, H., Hwang,M., and Lee, K. Improved ttidden Markov Modelingfor Speaker-Independent Continuous Speech Recogni-tion.
in: DARPA Speech and Language Workshop.Morgan Kaufmann Publishers, Hidden Valley, PA, 1990,pp.
327-331.Huang, X., Ariki, Y., and Jack, M. Hidden MarkovModels for Speech Recognition.
Edinburgh UniversityPress, Edinburgh, U.K., 1990.Huang, X. and Lee, K. On Speaker-Independent,Speaker-Dependent, and Speaker-Adaptive SpeechRecognition.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
1991,pp.
877-880.Huang, X., Lee, K., Hon, H., and Hwang, M. ImprovedAcoustic Modeling for the SPHINX Speech RecognitionSystem.
in: IEEE International Conference on Acous-tics, Speech, and Signal Processing.
Toronto, Ontario,CANADA, 1991, pp.
345-348.Hwang, M., Hon, H., and Lee, K. Modeling Between-Word Coarticulation i Continuous Speech Recognition.in: Proceedings of Eurospeech.
Paris, FRANCE, 1989,pp.
5-8.Hwang, M. and Huang, X. Shared-Distribution Hid-den Markov Models for Speech Recognition.
TechnicalReport CMU-CS-91-124, Carnegie Mellon University,April 1991.195\[15\] Hwang, M. and Huang, X. Subphonetic Modeling withMarkov States - Senone.
in: IEEE International Con-ference on Acoustics, Speech, and Signal Processing.1992.\[16\] Iso, K. and Watanabe, T. Speaker-independnet wordrecognition using a neural prediction model, in: IEEEInternational Conference on Acoustics, Speech, andSignal Processing.
1990, pp.
441-444.\[17\] Jelinek, F. and Mercer, R. Interpolated Estimation ofMarkov Source Parameters from Sparse Data.
in: Pat-tern Recognition in Practice, edited by E. Gelsema ndL.
Kanal.
North-Holland Publishing Company, Amster-dam, the Netherlands, 1980, pp.
381-397.\[18\] Kobayashi, T., Yagyu, M., and Shirai, K. Applicationsof neural networks to articulatory motion estimation.in: IEEE International Conference on Acoustics,Speech, and Signal Processing.
1991, pp.
489-4920.\[19\] Lee, K. Context-Dependent Phonetic llidden MarkovModels for Continuous Speech Recognition.
IEEETransactions on Acoustics, Speech, and Signal Pro-cessing, April 1990, pp.
599--609.\[20\] Lee, K., Hon, H., and Reddy, R. An Overview of theSPHINX Speech Recognition System.
IEEE Transac-tions on Acoustics, Speech, and Signal Processing,January 1990, pp.
35-45.\[21\] Linde, Y., Buzo, A., and Gray, R. An Algorithm forVector Quantizer Design.
IEEE Transactions on Com-m unication, vol.
COM-28 (1980), pp.
84-95.\[22\] Lippmann, R. Neural Nets for Computing.
in: IEEEInternational Conference on Acoustics, Speech, andSignal Processing.
1988, pp.
1---6.\[23\] Lippmann, R. Review of Research on Neural Nets forSpeech.
in: Neural Computation.
1989.\[24\] Montacie, C., Choukri, K., and Chollet, G. Speechrecognition using temporal decomposition a d multi-layer feed-forward automata, in: IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing.
1989, pp.
409-412.\[i5\] Nakamura, S. and Shikano, K. A comparative studyof spectral mapping for speaker adaptation.
ICASSP,1990, pp.
157-160.\[26\] Pallett, D., Fiscus, J., and Garofolo, J. DARPA ResourceManagement Benchmark Test Results June 1990. in:DARPA Speech and Language Workshop.
MorganKaufmann Publishers, San Mateo, CA, 1990, pp.
298-305.\[27\] Price, P., Fisher, W., Bernstein, J., and Pallett, D. ADatabase for Continuous Speech Recognition i  a 1000-Word Domain.
in: IEEE International Conferenceon Acoustics, Speech, and Signal Processing.
1988,pp.
651--654.\[28\] Rabiner, L., Lee, C., Juang, B., and Wilpon, J. HMMClustering for Connected Word Recognition.
in: IEEEInternational Conference on Acoustics, Speech, andSignal Processing.
1989, pp.
405--408.\[29\] Rahim, M., Kleijn, W., Schroeter, J., and Goodyear,C.
Acoustic to articulatory parameter mapping usingan assembly of neural networks, in: IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing.
1991, pp.
485---488.\[30\] Rumelhart, D., Hinton, G., and Williams, R. Learn-ing Internal Representation by Error Propagation.
in:Learning Internal Representation by Error Propa-gation, by D. Rumelhart, G. Hinton, and R. Williams,edited by D. Rumelhart and J. McClelland.
MIT Press,Cambridge, MA, 1986.\[31\] Soong, F., Rosenberg, A., Rabiner, L., and Juang, B.A Vector Quantization Approach to Speaker Recogni-tion.
in: IEEE International Conference on Acous-tics, Speech, and Signal Processing.
1985, pp.
387-390.\[32\] Tamura, S. and Waibel, A.
Noise reduction using con-nectionist modelsnce Measure for Speech Recognition.in: IEEE International Conference on Acoustics,Speech, and Signal Processing.
1988, pp.
553-556.196
