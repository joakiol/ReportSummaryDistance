Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238?241,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsInducing Synchronous Grammars with Slice SamplingPhil BlunsomComputing LaboratoryOxford UniversityPhil.Blunsom@comlab.ox.ac.ukTrevor CohnDepartment of Computer ScienceUniversity of SheffieldT.Cohn@dcs.shef.ac.ukAbstractThis paper describes an efficient sampler forsynchronous grammar induction under a non-parametric Bayesian prior.
Inspired by ideasfrom slice sampling, our sampler is able todraw samples from the posterior distributionsof models for which the standard dynamic pro-graming based sampler proves intractable onnon-trivial corpora.
We compare our samplerto a previously proposed Gibbs sampler anddemonstrate strong improvements in terms ofboth training log-likelihood and performanceon an end-to-end translation evaluation.1 IntroductionIntractable optimisation algorithms abound in muchof the recent work in Natural Language Process-ing.
In fact, there is an increasing acceptance thatsolutions to many of the great challenges of NLP(e.g.
machine translation, summarisation, questionanswering) will rest on the quality of approximateinference.
In this work we tackle this problem inthe context of inducing synchronous grammars fora machine translation system.
We concern ourselveswith the lack of a principled, and scalable, algo-rithm for learning a synchronous context free gram-mar (SCFG) from sentence-aligned parallel corpora.The predominant approach for learning phrase-based translation models (both finite state or syn-chronous grammar based) uses a cascade of heuris-tics beginning with predicted word alignmentsand producing a weighted set of translation rules(Koehn et al, 2003).
Alternative approaches avoidsuch heuristics, instead learning structured align-ment models directly from sentence aligned data(e.g., (Marcu and Wong, 2002; Cherry and Lin,2007; DeNero et al, 2008; Blunsom et al, 2009)).Although these models are theoretically attractive,inference is intractable (at least O(|f |3|e|3)).
Theefficacy of direct estimation of structured alignmentmodels therefore rests on the approximations usedto make inference practicable ?
typically heuristicconstraints or Gibbs sampling.
In this work we showthat naive Gibbs sampling (specifically, Blunsom etal.
(2009)) is ineffectual for inference and reliant ona high quality initialisation, mixing very slowly andbeing easily caught in modes.
Instead, blocked sam-pling over sentence pairs allows much faster mixing,but done in the obvious way (following Johnson et al(2007)) would incur a O(|f |3|e|3) time complexity.Here we draw inspiration from the work ofVan Gael et al (2008) on inference in infinite hid-den Markov models to develop a novel algorithmfor efficient sampling from a SCFG.
We develop anauxiliary variable ?slice?
sampler which can dramati-cally reduce inference complexity, and thereby makeblocked sampling practicable on real translation cor-pora.
Our evaluation demonstrates that our algorithmmixes more quickly than the local Gibbs sampler, andproduces translation models which achieve state-of-the-art BLEU scores without using GIZA++ or sym-metrisation heuristics for initialisation.We adopt the generative model of Blunsom etal.
(2009) which creates a parallel sentence pairby a sequence (derivation) of SCFG productionsd = (r1, r2, ..., rn).
The tokens in each language canbe read off the leaves of the derivation tree whiletheir order is defined hierarchically by the produc-tions in use.
The probability of a derivation is definedas p(d|?)
=?r?d ?r where ?
are the model param-eters which are drawn from a Bayesian prior.
Wedeviate from that models definition of the prior overphrasal translations, instead adopting the hierarchicalDirichlet process prior from DeNero et al (2008),which incorporates IBM Model 1.
Blunsom et al(2009) describe a blocked sampler following John-son et al (2007) which uses the Metropolis-Hastingsalgorithm to correct proposal samples drawn froman approximating SCFG, however this is discountedas impractical due to the O(|f |3|e|3) complexity.Instead a Gibbs sampler is used which samples localupdates to the derivation structure of each traininginstance.
This avoids the dynamic program of the238blocked sampler but at the expense of considerablyslower mixing.Recently Bouchard-Co?te?
et al (2009) proposedan auxialliary variable sampler, possibly comple-mentary to ours, which was also evaluated on syn-chronous parsing.
Rather than slice sampling deriva-tions in a collapsed Bayesian model, this modelemployed a secondary proposal model (IBM Mod-els) and sampled expectations over rule parameters.2 Slice Sampling a SCFGIt would be advantageous to explore a middle groundwhere the scope of the dynamic program is limited tohigh probability regions, reducing the running timeto an acceptable level.
By employing the techniqueof slice sampling (Neal, 2003) we describe an algo-rithm which stochastically samples from a reducedspace of possible derivations, while ensuring thatthese samples are drawn from the correct distribu-tion.
We apply the slice sampler to the approximatingSCFG parameterised by ?, which requires samplesfrom an inside chart p(d|?)
(for brevity, we omit thedependency on ?
in the following).Slice sampling is an example of auxiliary variablesampling in which we make use of the fact that ifwe can draw samples from a joint distribution, thenwe can trivially obtain samples from the marginaldistributions: p(d) =?u p(d,u), where d is thevariable of interest and u is an auxiliary variable.Using a Gibbs sampler we can draw samples fromthis joint distribution by alternately sampling fromp(d|u) and p(u|d).
The trick is to ensure that u isdefined such that drawing samples from p(d|u) ismore efficient than from p(d).We define the variable u to contain a slice variableus for every cell of a synchronous parse chart forevery training instance:1S = {(i, j, x, y) | 0 ?
i < j ?
|f |, 0 ?
x < y ?
|e|}u = {us ?
R | 0 < us < 1, s ?
S}These slice variables act as cutoffs on the probabili-ties of the rules considered in each cell s: rule appli-cations rs with ?rs ?
us will be pruned from thedynamic program.21The dependence on training instances is omitted here andsubsequently for simplicity.
Each instance is independent, andtherefore this formulation can be trivially applied to a set.2Alternatively, we could naively sample from a pruned chartusing a fixed beam threshold.
However, this would not producesamples from p(d), but some other unknown distribution.Sampling p(u|d) Unlike Van Gael et al (2008),there is not a one-to-one correspondence between thespans of the rules in d and the set S, rather the deriva-tion?s rule spans form a subset of S .
This compli-cates our definition of p(u|d); we must provide sepa-rate accounts of how each us is generated dependingon whether there is a corresponding rule for s, i.e.,rs ?
d. We define p(u|d) =?s p(us|d), where:p(us|d) ={I(us<?rs )?rs, if rs ?
d?
(us; a, b) , else(1)which mixes a uniform distribution and a Beta dis-tribution3 depending on the existence of a rule rsin the derivation d.4 Eq.
1 is constructed such thatonly rules with probability greater than the rele-vant threshold, {rs | ?rs > us}, could have feasiblybeen part of a derivation resulting in auxiliary vari-able u.
This is critical in reasoning over the reverseconditional p(d|u) which only has to consider thereduced space of rules (formulation below in (4)).Trivially, the conditioning derivation is recoverable,?rs ?
d, ?rs ?
us.
We parameterise the ?
distribu-tion in (1) with a heavy skew towards zero in orderto limit the amount of pruning and thereby includemany competing derivations.5Sampling p(d|u) Recall the probability of aderivation, p(d) =?rs?d ?rs .
We draw samplesfrom the joint distribution, p(d,u), holding u fixed:p(d|u) ?
p(d,u) = p(d)?
p(u|d)=???rs?d?rs???
( ?us:rs?dI(us<?rs )?rs?
?us:rs 6?d ?
(us; a, b))=?us:rs?dI (us < ?rs)?us:rs 6?d?
(us; a, b) (2)=?us:rs?dI (us < ?rs)?
(us; a, b)?us?
(us; a, b) (3)?
?us:rs?dI (us < ?rs)?
(us; a, b)(4)In step (2) we cancel the ?rs terms while in step (3)we introduce ?
(us; a, b) terms to the numerator anddenominator for us : rs ?
d to simplify the range3Any distribution defined over {x ?
R | 0 ?
x ?
1} may beused in place of ?, however this may affect the efficiency of thesampler.4I(?)
returns 1 if the condition is true and 0 otherwise.5We experiment with a range of a < 1 while fixing b = 1.239System BLEU time(s) LLHMoses (default settings) 47.3 ?
?LB init.
36.5 ?
-257.1M1 init.
48.8 ?
-153.4M4 init.
49.1 ?
-151.4Gibbs LB init.
45.3 44 -135.4Gibbs M1 init.
48.2 40 -120.5Gibbs M4 init.
(Blunsom et al, 2009) 49.6 44 -110.3Slice (a=0.15, b=1) LB init.
47.3 180 -98.9Slice (a=0.10, b=1) M1 init.
50.4 908 -89.4Slice (a=0.15, b=1) M1 init.
49.9 144 -90.2Slice (a=0.25, b=1) M1 init.
49.2 80 -95.6Table 1: IWSLT Chinese to English translation.of the second product.
The last step (4) discards theterm?us ?
(us; a, b) which is constant wrt d. Thenet result is a formulation which factors with thederivation structure, thereby eliminating the need toconsider allO(|e|2|f |2) spans in S. Critically p(d|u)is zero for all spans failing the I (us < ?rs) condition.To exploit the decomposition of Equation 4 werequire a parsing algorithm that only explores chartcells whose child cells have not already been prunedby the slice variables.
The standard approach of usingsynchronous CYK (Wu, 1997) doesn?t posses thisproperty: all chart cells would be visited even if theyare to be pruned.
Instead we use an agenda basedparsing algorithm, in particular we extend the algo-rithm of Klein and Manning (2004) to synchronousparsing.6 Finally, we need a Metropolis-Hastingsacceptance step to account for intra-instance depen-dencies (the ?rich-get-richer?
effect).
We omit thedetails, save to state that the calculation cancels tothe same test as presented in Johnson et al (2007).73 EvaluationIn the following experiments we compare the slicesampler and the Gibbs sampler (Blunsom et al,2009), in terms of mixing and translation quality.
Wemeasure mixing in terms of training log-likelihood(LLH) after a fixed number of sampling iterations.Translations are produced using Moses (Koehn et al,2007), initialised with the word alignments from thefinal sample, and are evaluated using BLEU(Papineniet al, 2001).
The slice sampled models are restrictedto learning binary branching one-to-one (or null)alignments,8 while no restriction is placed on theGibbs sampler (both use the same model, so have6Moreover, we only sample values for us as they are visitedby the parser, thus avoiding the quartic complexity.7Acceptance rates averaged above 99%.8This restriction is not strictly necessary, however it greatlysimplifies the implementation and increases efficiency.comparable LLH).
Of particular interest is how thedifferent samplers perform given initialisations ofvarying quality.
We evaluate three initialisers: M4:the symmetrised output of GIZA++ factorised intoITG form (as used in Blunsom et al (2009)); M1:the output of a heavily pruned ITG parser using theIBM Model 1 prior for the rule probabilities;9 andLB: left-branching monotone derivations.10We experiment with the Chinese?English trans-lation task from IWSLT, as used in Blunsom et al(2009).11 Figure 1 shows LLH curves for the sam-plers initialised with the M1 and LB derivations, plusthe curve for Gibbs sampler with the M4 initialiser.12Table 1 gives BLEU scores on Test-05 for phrase-based translation models built from the 1500th sam-ple for the various models along with the averagetime per sample and their final log-likelihood.4 DiscussionThe results are particularly encouraging.
The slicesampler uniformly finds much better solutions thanthe Gibbs sampler regardless of initialisation.
Inparticular, the slice sampled model initialised withthe naive LB structure achieves a higher likelihoodthan the M4 initialised model, although this is notreflected in their relative BLEU scores.
In contrast theGibbs sampler is more significantly affected by itsinitialisation, only deviating slightly before becom-ing trapped in a mode, as seen in Fig.
1.
With suf-ficient (infinite) time both sampling strategies willconverge on the true posterior regardless of initiali-sation, however the slice sampler appears to be con-verging much faster than the Gibbs sampler.Interestingly, the initialisation heuristics (M1 andM4) outperform the default heuristics (Koehn et al,2007) by a considerable margin.
This is most likelybecause the initialisation heuristics force the align-ments to factorise with an ITG, resulting in moreaggressive pruning of spurious alignments which inturn allows for more and larger phrase pairs.9The following beam heuristics are employed: alignments tonull are only permitted on the longer sentence side; words areonly allowed to align to those whose relative sentence positionis within ?3 words.10Words of the longer sentence are randomly assigned to null.11We limit the maximum training sentence length to 40, result-ing in ?
40k training sentences.12The GIZA++ M4 alignments don?t readily factorise toword-based ITG derivations, as such we haven?t produced resultsfor this initialiser using the slice sampler.2400 50 100 150 200 250?140?130?120?110?100?90SamplesLog?likelihoodSlice (a=0.10 b=1) M1Slice (a=0.15 b=1) M1Slice (a=0.20 b=1) M1Slice (a=0.25 b=1) M1Gibbs M1Gibbs M40 200 400 600 800 1000?200?180?160?140?120?100SamplesLog?likelihoodSlice (a=0.15 b=1) M1Slice (a=0.15 b=1) LBGibbs M1Gibbs LBGibbs M4Figure 1: Training log-likelihood as a function of sampling iteration for Gibbs and slice sampling.While the LLHs for the slice sampled models andtheir BLEU scores appear correlated, this doesn?textend to comparisons with the Gibbs sampled mod-els.
We believe that this is because the GIZA++initialisation alignments also explain the data well,while not necessarily obtaining a high LLH underthe ITG model.
Solutions which score highly in onemodel score poorly in the other, despite both produc-ing good translations.The slice sampler is slower than the local Gibbssampler, its speed depending on the parameterisationof the Beta distribution (affecting the width of thebeam).
In the extreme, exhaustive search using thefull dynamic program is intractable on current hard-ware,13 and therefore we have achieved our aim ofmediating between local and blocked inference.This investigation has established the promiseof the SCFG slice sampling technique to providea scalable inference algorithm for non-parametricBayesian models.
With further development, thiswork could provide the basis for a family of prin-cipled inference algorithms for parsing models, bothmonolingual and synchronous, and other models thatprove intractable for exact dynamic programming.ReferencesP.
Blunsom, T. Cohn, C. Dyer, M. Osborne.
2009.A Gibbs sampler for phrasal synchronous grammarinduction.
In Proc.
ACL/IJCNLP, 782?790, Suntec,Singapore.
Association for Computational Linguistics.A.
Bouchard-Co?te?, S. Petrov, D. Klein.
2009.
Ran-domized pruning: Efficiently calculating expectations13Our implementation had not completed a single sample aftera week.in large dynamic programs.
In Advances in NeuralInformation Processing Systems 22, 144?152.C.
Cherry, D. Lin.
2007.
Inversion transduction grammarfor joint phrasal translation modeling.
In Proc.
SSST,Rochester, USA.J.
DeNero, A.
Bouchard-Co?te?, D. Klein.
2008.
Sam-pling alignment structure under a Bayesian translationmodel.
In Proc.
EMNLP, 314?323, Honolulu, Hawaii.M.
Johnson, T. Griffiths, S. Goldwater.
2007.
Bayesianinference for PCFGs via Markov chain Monte Carlo.In Proc.
HLT-NAACL, 139?146, Rochester, New York.D.
Klein, C. D. Manning, 2004.
Parsing and hypergraphs,351?372.
Kluwer Academic Publishers, Norwell, MA,USA, 2004.P.
Koehn, F. J. Och, D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT-NAACL, 81?88,Edmonton, Canada.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst.2007.
Moses: Open source toolkit for statisticalmachine translation.
In Proc.
ACL, Prague.D.
Marcu, W. Wong.
2002.
A phrase-based, joint proba-bility model for statistical machine translation.
In Proc.EMNLP, 133?139, Philadelphia.R.
Neal.
2003.
Slice sampling.
Annals of Statistics,31:705?767.K.
Papineni, S. Roukos, T. Ward, W. Zhu.
2001.
Bleu:a method for automatic evaluation of machine trans-lation.
Technical Report RC22176 (W0109-022), IBMResearch Division, Thomas J. Watson Research Center,2001.J.
Van Gael, Y. Saatci, Y. W. Teh, Z. Ghahramani.
2008.Beam sampling for the infinite hidden markov model.In ICML, 1088?1095, New York, NY, USA.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Compu-tational Linguistics, 23(3):377?403.241
