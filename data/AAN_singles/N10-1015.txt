Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127?135,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsJoint Parsing and Alignment with Weakly Synchronized GrammarsDavid Burkett John Blitzer Dan KleinComputer Science DivisionUniversity of California, Berkeley{dburkett,blitzer,klein}@cs.berkeley.eduAbstractSyntactic machine translation systems extractrules from bilingual, word-aligned, syntacti-cally parsed text, but current systems for pars-ing and word alignment are at best cascadedand at worst totally independent of one an-other.
This work presents a unified joint modelfor simultaneous parsing and word alignment.To flexibly model syntactic divergence, we de-velop a discriminative log-linear model overtwo parse trees and an ITG derivation whichis encouraged but not forced to synchronizewith the parses.
Our model gives absoluteimprovements of 3.3 F1 for English pars-ing, 2.1 F1 for Chinese parsing, and 5.5 F1for word alignment over each task?s indepen-dent baseline, giving the best reported resultsfor both Chinese-English word alignment andjoint parsing on the parallel portion of the Chi-nese treebank.
We also show an improvementof 1.2 BLEU in downstream MT evaluationover basic HMM alignments.1 IntroductionCurrent syntactic machine translation (MT) sys-tems build synchronous context free grammars fromaligned syntactic fragments (Galley et al, 2004;Zollmann et al, 2006).
Extracting such grammarsrequires that bilingual word alignments and mono-lingual syntactic parses be compatible.
Because ofthis, much recent work in both word alignment andparsing has focused on changing aligners to makeuse of syntactic information (DeNero and Klein,2007; May and Knight, 2007; Fossum et al, 2008)or changing parsers to make use of word align-ments (Smith and Smith, 2004; Burkett and Klein,2008; Snyder et al, 2009).
In the first case, how-ever, parsers do not exploit bilingual information.In the second, word alignment is performed with amodel that does not exploit syntactic information.This work presents a single, joint model for parsingand word alignment that allows both pieces to influ-ence one another simultaneously.While building a joint model seems intuitive,there is no easy way to characterize how word align-ments and syntactic parses should relate to eachother in general.
In the ideal situation, each pairof sentences in a bilingual corpus could be syntacti-cally parsed using a synchronous context-free gram-mar.
Of course, real translations are almost alwaysat least partially syntactically divergent.
Therefore,it is unreasonable to expect perfect matches of anykind between the two sides?
syntactic trees, muchless expect that those matches be well explained ata word level.
Indeed, it is sometimes the case thatlarge pieces of a sentence pair are completely asyn-chronous and can only be explained monolingually.Our model exploits synchronization where pos-sible to perform more accurately on both wordalignment and parsing, but also allows indepen-dent models to dictate pieces of parse trees andword alignments when synchronization is impossi-ble.
This notion of ?weak synchronization?
is pa-rameterized and estimated from data to maximizethe likelihood of the correct parses and word align-ments.
Weak synchronization is closely related tothe quasi-synchronous models of Smith and Eis-ner (2006; 2009) and the bilingual parse rerankingmodel of Burkett and Klein (2008), but those modelsassume that the word alignment of a sentence pair isknown and fixed.To simultaneously model both parses and align-127ments, our model loosely couples three separatecombinatorial structures: monolingual trees in thesource and target languages, and a synchronous ITGalignment that links the two languages (but is notconstrained to match linguistic syntax).
The modelhas no hard constraints on how these three struc-tures must align, but instead contains a set of ?syn-chronization?
features that are used to propagateinfluence between the three component grammars.The presence of synchronization features couplesthe parses and alignments, but makes exact inferencein the model intractable; we show how to use a vari-ational mean field approximation, both for comput-ing approximate feature expectations during train-ing, and for performing approximate joint inferenceat test time.We train our joint model on the parallel, goldword-aligned portion of the Chinese treebank.When evaluated on parsing and word alignment, thismodel significantly improves over independently-trained baselines: the monolingual parser of Petrovand Klein (2007) and the discriminative wordaligner of Haghighi et al (2009).
It also improvesover the discriminative, bilingual parsing modelof Burkett and Klein (2008), yielding the highestjoint parsing F1 numbers on this data set.
Finally,our model improves word alignment in the contextof translation, leading to a 1.2 BLEU increase overusing HMM word alignments.2 Joint Parsing and AlignmentGiven a source-language sentence, s, and a target-language sentence, s?, we wish to predict a sourcetree t, a target tree t?, and some kind of alignmenta between them.
These structures are illustrated inFigure 1.To facilitate these predictions, we define a condi-tional distribution P(t, a, t?|s, s?).
We begin with ageneric conditional exponential form:P(t, a, t?|s, s?)
?
exp ?>?
(t, a, t?, s, s?)
(1)Unfortunately, a generic model of this form is in-tractable, because we cannot efficiently sum overall triples (t, a, t?)
without some assumptions abouthow the features ?
(t, a, t?, s, s?)
decompose.One natural solution is to restrict our candidatetriples to those given by a synchronous context freegrammar (SCFG) (Shieber and Schabes, 1990).
Fig-ure 1(a) gives a simple example of generation froma log-linearly parameterized synchronous grammar,together with its features.
With the SCFG restric-tion, we can sum over the necessary structures usingthe O(n6) bitext inside-outside algorithm, makingP(t, a, t?|s, s?)
relatively efficient to compute expec-tations under.Unfortunately, an SCFG requires that all the con-stituents of each tree, from the root down to thewords, are generated perfectly in tandem.
The re-sulting inability to model any level of syntactic di-vergence prevents accurate modeling of the individ-ual monolingual trees.
We will consider the run-ning example from Figure 2 throughout the paper.Here, for instance, the verb phrase established insuch places as Quanzhou, Zhangzhou, etc.
in En-glish does not correspond to any single node in theChinese tree.
A synchronous grammar has no choicebut to analyze this sentence incorrectly, either by ig-noring this verb phrase in English or postulating anincorrect Chinese constituent that corresponds to it.Therefore, instead of requiring strict synchroniza-tion, our model treats the two monolingual trees andthe alignment as separate objects that can vary arbi-trarily.
However, the model rewards synchronizationappropriately when the alignment brings the treesinto correspondence.3 Weakly Synchronized GrammarsWe propose a joint model which still gives probabil-ities on triples (t, a, t?).
However, instead of usingSCFG rules to synchronously enforce the tree con-straints on t and t?, we only require that each of tand t?
be well-formed under separate monolingualCFGs.In order to permit efficient enumeration of all pos-sible alignments a, we also restrict a to the set ofunlabeled ITG bitrees (Wu, 1997), though again wedo not require that a relate to t or t?
in any particularway.
Although this assumption does limit the spaceof possible word-level alignments, for the domainwe consider (Chinese-English word alignment), thereduced space still contains almost all empiricallyobserved alignments (Haghighi et al, 2009).1 For1See Section 8.1 for some new terminal productions re-quired to make this true for the parallel Chinese treebank.128NP VPSNPVPIPb0b1b2Features?
( (IP, b0, S), s, s?
)?
( (NP, b1, NP), s, s?
)?
( (VP, b2, VP), s, s?
)NP VPSNPIPb0b1b2VPAPFeatures(IP, s)(b0, s, s?
)(NP, s)(b1, s, s?
)(VP, s)(b2, s, s?
)(S, s?
)(IP, b0)(NP, s?
)(b0, S)(AP, s?
)(b1, NP)(VP, s?
)(IP, b0, S)ParsingAlignmentSynchronization?E?E?E?E?F?F?F?A?A?A?!?!?!"?!
(a) Synchronous Rule (b) Asynchronous RuleFigure 1: Source trees, t (right), alignments, a (grid), and target trees, t?
(top), and feature decompositions for syn-chronous (a) and weakly synchronous (b) grammars.
Features always condition on bispans and/or anchored syntacticproductions, but weakly synchronous grammars permit more general decompositions.example, in Figure 2, the word alignment is ITG-derivable, and each of the colored rectangles is a bi-span in that derivation.There are no additional constraints beyond theindependent, internal structural constraints on t, a,and t?.
This decoupling permits derivations like thatin Figure 1(b), where the top-level syntactic nodesalign, but their children are allowed to diverge.
Withthe three structures separated, our first model is acompletely factored decomposition of (1).Formally, we represent a source tree t as a set ofnodes {n}, each node representing a labeled span.Likewise, a target tree t?
is a set of nodes {n?
}.2 Werepresent alignments a as sets of bispans {b}, indi-cated by rectangles in Figure 1.3 Using this notation,the initial model has the following form:P(t, a, t?|s, s?)
?
exp??
?n?t?>?F (n, s)+?b?a?>?A(b, s, s?)+?n??t?
?>?E(n?, s?)??
(2)Here ?F (n, s) indicates a vector of source node fea-tures, ?E(n?, s?)
is a vector of target node features,and ?A(b, s, s?)
is a vector of alignment bispan fea-tures.
Of course, this model is completely asyn-2For expositional clarity, we describe n and n?
as labeledspans only.
However, in general, features that depend on n orn?
are permitted to depend on the entire rule, and do in our finalsystem.3Alignments a link arbitrary spans of s and s?
(includingnon-constituents and individual words).
We discuss the relationto word-level alignments in Section 4.chronous so far, and fails to couple the trees andalignments at all.
To permit soft constraints betweenthe three structures we are modeling, we add a set ofsynchronization features.For n ?
t and b ?
a, we say that n b if n and bboth map onto the same span of s. We define b n?analogously for n?
?
t?.
We now consider threedifferent types of synchronization features.
Source-alignment synchronization features ?(n, b) are ex-tracted whenever n  b.
Similarly, target-alignmentfeatures ?(b, n?)
are extracted if b  n?.
Thesefeatures capture phenomena like that of bispan b7in Figure 2.
Here the Chinese noun?
synchronizeswith the ITG derivation, but the English projectionof b7 is a distituent.
Finally, we extract source-targetfeatures ?./(n, b, n?)
whenever nbn?.
These fea-tures capture complete bispan synchrony (as in bi-span b8) and can be expressed over triples (n, b, n?
)which happen to align, allowing us to reward syn-chrony, but not requiring it.
All of these licensingconditions are illustrated in Figure 1(b).With these features added, the final form of themodel is:P(t, a, t?|s, s?)
?
exp??
?n?t?>?F (n, s)+?b?a?>?A(b, s, s?)+?n??t?
?>?E(n?, s?
)+?nb?>?(n, b)+?bn?
?>?(b, n?)+?nbn?
?>?./(n, b, n?)??
(3)129We emphasize that because of the synchronizationfeatures, this final form does not admit any knownefficient dynamic programming for the exact com-putation of expectations.
We will therefore turn to avariational inference method in Section 6.4 FeaturesWith the model?s locality structure defined, wejust need to specify the actual feature function,?.
We divide the features into three types: pars-ing features (?F (n, s) and ?E(n?, s?
)), alignmentfeatures (?A(b, s, s?))
and synchronization features(?(n, b), ?(b, n?
), and ?./(n, b, n?)).
We detaileach of these in turn here.4.1 ParsingThe monolingual parsing features we use are sim-ply parsing model scores under the parser of Petrovand Klein (2007).
While that parser uses heavily re-fined PCFGs with rule probabilities defined at therefined symbol level, we interact with its posteriordistribution via posterior marginal probabilities overunrefined symbols.
In particular, to each unrefinedanchored production iAj ?
iBkCj , we associate asingle feature whose value is the marginal quantitylog P(iBkCj |iAj , s) under the monolingual parser.These scores are the same as the variational rulescores of Matsuzaki et al (2005).44.2 AlignmentWe begin with the same set of alignment featuresas Haghighi et al (2009), which are defined only forterminal bispans.
In addition, we include features onnonterminal bispans, including a bias feature, fea-tures that measure the difference in size betweenthe source and target spans, features that measurethe difference in relative sentence position betweenthe source and target spans, and features that mea-sure the density of word-to-word alignment poste-riors under a separate unsupervised word alignmentmodel.4Of course the structure of our model permits any of theadditional rule-factored monolingual parsing features that havebeen described in the parsing literature, but in the present workwe focus on the contributions of joint modeling.4.3 SynchronizationOur synchronization features are indicators for thesyntactic types of the participating nodes.
We de-termine types at both a coarse (more collapsedthan Treebank symbols) and fine (Treebank sym-bol) level.
At the coarse level, we distinguish be-tween phrasal nodes (e.g.
S, NP), synthetic nodesintroduced in the process of binarizing the grammar(e.g.
S?, NP?
), and part-of-speech nodes (e.g.
NN,VBZ).
At the fine level, we distinguish all nodesby their exact label.
We use coarse and fine typesfor both partially synchronized (source-alignment ortarget-alignment) features and completely synchro-nized (source-alignment-target) features.
The insetof Figure 2 shows some sample features.
Of course,we could devise even more sophisticated features byusing the input text itself.
As we shall see, however,our model gives significant improvements with thesesimple features alone.5 LearningWe learn the parameters of our model on the paral-lel portion of the Chinese treebank.
Although ourmodel assigns probabilities to entire synchronousderivations of sentences, the parallel Chinese tree-bank gives alignments only at the word level (1 by1 bispans in Figure 2).
This means that our align-ment variable a is not fully observed.
Because ofthis, given a particular word alignment w, we max-imize the marginal probability of the set of deriva-tions A(w) that are consistent with w (Haghighi etal., 2009).5L(?
)=log?a?A(wi)P(ti, a, t?i|si, s?i)We maximize this objective using standard gradientmethods (Nocedal and Wright, 1999).
As with fullyvisible log-linear models, the gradient for the ith sen-tence pair with respect to ?
is a difference of featureexpectations:?L(?)
=EP(a|ti,wi,t?i,si,s?i)[?
(ti, a, t?i, si, s?i)]?
EP(t,a,t?|si,s?i)[?
(t, a, t?, si, s?i)] (4)5We also learn from non-ITG alignments by maximizing themarginal probability of the set of minimum-recall error align-ments in the same way as Haghighi et al (2009)130NPNPINPPNPINPPVBNVPVBDVPNPSJJ NNS...were established in such places as Quanzhou Zhangzhou etc.?????????
?...NPPNNNPPPVPVVASNPVPb8b7b4Sample Synchronization FeaturesNP, b8,NPNN, b7?!
"( ) = CoarseSourceTarget?phrasal, phrasal?
: 1FineSourceTarget?NP,NP?
: 1?!
( ) = CoarseSourceAlign?pos?
: 1FineSourceAlign?NN?
: 1Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimalITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fullysynchronized bispan (b8).
The inset provides some examples of active synchronization features (see Section 4.3) onthese bispans.
On this example, the monolingual English parser erroneously attached the lower PP to the VP headed byestablished, and the non-syntactic ITG word aligner misaligned?
to such instead of to etc.
Our joint model correctedboth of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8.We cannot efficiently compute the model expecta-tions in this equation exactly.
Therefore we turn nextto an approximate inference method.6 Mean Field InferenceInstead of computing the model expectations from(4), we compute the expectations for each sentencepair with respect to a simpler, fully factored distri-bution Q(t, a, t?)
= q(t)q(a)q(t?).
Rewriting Q inlog-linear form, we have:Q(t, a, t?)
?
exp??
?n?t?n +?b?a?b +?n??t??n??
?Here, the ?n, ?b and ?n?
are variational parameterswhich we set to best approximate our weakly syn-chronized model from (3):??
= argmin?KL(Q?||P?
(t, a, t?|s, s?
))Once we have found Q, we compute an approximategradient by replacing the model expectations withexpectations under Q:EQ(a|wi)[?
(ti, a, t?i, si, s?i)]?
EQ(t,a,t?|si,s?i)[?
(t, a, t?, si, s?i)]Now, we will briefly describe how we compute Q.First, note that the parameters ?
of Q factor alongindividual source nodes, target nodes, and bispans.The combination of the KL objective and our par-ticular factored form of Q make our inference pro-cedure a structured mean field algorithm (Saul andJordan, 1996).
Structured mean field techniques arewell-studied in graphical models, and our adaptationin this section to multiple grammars follows stan-dard techniques (see e.g.
Wainwright and Jordan,2008).Rather than derive the mean field updates for ?,we describe the algorithm (shown in Figure 3) pro-cedurally.
Similar to block Gibbs sampling, we it-eratively optimize each component (source parse,target parse, and alignment) of the model in turn,conditioned on the others.
Where block Gibbs sam-pling conditions on fixed trees or ITG derivations,our mean field algorithm maintains uncertainty in131Input: sentence pair (s, s?
)parameter vector ?Output: variational parameters ?1.
Initialize?0n ?
?>?F (n, s)?0b?
?>?A(b, s, s?)?0n??
?>?E(n?, s?
)?0n ?
?t q?0(t)I(n ?
t), etc for ?0b , ?0n?2.
While not converged, for each n, n?, b inthe monolingual and ITG charts?in ?
?>(?F (n, s) +?b,nb ?i?1b ?(n, b)+?b,nb?n?,bn?
?i?1b ?i?1n?
?./(n, b, n?
))?in ?
?t q?
(t)I(n ?
t) (inside-outside)?ib ?
?>(?A(b, s, s?)
+?n,nb ?i?1n ?(n, b)+?n?,bn?
?i?1n?
?(b, n?)+?n,nb?n?,bn?
?i?1n ?i?1n?
?./(n, b, n?
))?b ?
?a q?
(a)I(b ?
a) (bitext inside-outside)updates for ?in?
, ?in?
analogous to ?in, ?in3.
Return variational parameters ?Figure 3: Structured mean field inference for the weaklysynchronized model.
I(n ?
t) is an indicator value forthe presence of node n in source tree t.the form of monolingual parse forests or ITG forests.The key components to this uncertainty are theexpected counts of particular source nodes, targetnodes, and bispans under the mean field distribution:?n =?tq?
(t)I(n ?
t)?n?
=?t?q?(t?)I(n?
?
t?
)?b =?aq?
(a)I(b ?
a)Since dynamic programs exist for summing overeach of the individual factors, these expectations canbe computed in polynomial time.6.1 PruningAlthough we can approximate the expectations from(4) in polynomial time using our mean field distribu-tion, in practice we must still prune the ITG forestsand monolingual parse forests to allow tractable in-ference.
We prune our ITG forests using the samebasic idea as Haghighi et al (2009), but we em-ploy a technique that allows us to be more aggres-sive.
Where Haghighi et al (2009) pruned bispansbased on how many unsupervised HMM alignmentswere violated, we first train a maximum-matchingword aligner (Taskar et al, 2005) using our super-vised data set, which has only half the precision er-rors of the unsupervised HMM.
We then prune ev-ery bispan which violates at least three alignmentsfrom the maximum-matching aligner.
When com-pared to pruning the bitext forest of our model withHaghighi et al (2009)?s HMM technique, this newtechnique allows us to maintain the same level of ac-curacy while cutting the number of bispans in half.In addition to pruning the bitext forests, we alsoprune the syntactic parse forests using the mono-lingual parsing model scores.
For each unrefinedanchored production iAj ?
iBkCj , we com-pute the marginal probability P(iAj ,i Bk,k Cj |s) un-der the monolingual parser (these are equivalent tothe maxrule scores from Petrov and Klein 2007).
Weonly include productions where this probability isgreater than 10?20.
Note that at training time, we arenot guaranteed that the gold trees will be includedin the pruned forest.
Because of this, we replace thegold trees ti, t?i with oracle trees from the pruned for-est, which can be found efficiently using a variant ofthe inside algorithm (Huang, 2008).7 TestingOnce the model has been trained, we still need todetermine how to use it to predict parses and wordalignments for our test sentence pairs.
Ideally, giventhe sentence pair (s, s?
), we would find:(t?, w?, t??)
= argmaxt,w,t?P(t, w, t?|s, s?
)= argmaxt,w,t?
?a?A(w)P(t, a, t?|s, s?
)Of course, this is also intractable, so we once againresort to our mean field approximation.
This yieldsthe approximate solution:(t?, w?, t??)
= argmaxt,w,t?
?a?A(w)Q(t, a, t?
)However, recall that Q incorporates the model?s mu-tual constraint into the variational parameters, which132factor into q(t), q(a), and q(t?).
This allows us tosimplify further, and find the maximum a posterioriassignments under the variational distribution.
Thetrees can be found quickly using the Viterbi insidealgorithm on their respective qs.
However, the sumfor computing w?
under q is still intractable.As we cannot find the maximum probability wordalignment, we provide two alternative approachesfor finding w?.
The first is to just find the ViterbiITG derivation a?
= argmaxa q(a) and then set w?to contain exactly the 1x1 bispans in a?.
The secondmethod, posterior thresholding, is to compute poste-rior marginal probabilities under q for each 1x1 cellbeginning at position i, j in the word alignment grid:m(i, j) =?aq(a)I((i, i+ 1, j, j + 1) ?
a)We then include w(i, j) in w?
if m(w(i, j)) > ?
,where ?
is a threshold chosen to trade off precisionand recall.
For our experiments, we found that theViterbi alignment was uniformly worse than poste-rior thresholding.
All the results from the next sec-tion use the threshold ?
= 0.25.8 ExperimentsWe trained and tested our model on the translatedportion of the Chinese treebank (Bies et al, 2007),which includes hand annotated Chinese and Englishparses and word alignments.
We separated the datainto three sets: train, dev, and test, according to thestandard Chinese treebank split.
To speed up train-ing, we only used training sentences of length ?
50words, which left us with 1974 of 2261 sentences.We measured the results in two ways.
First, wedirectly measured F1 for English parsing, Chineseparsing, and word alignment on a held out section ofthe hand annotated corpus used to train the model.Next, we further evaluated the quality of the wordalignments produced by our model by using them asinput for a machine translation system.8.1 Dataset-specific ITG TerminalsThe Chinese treebank gold word alignments includesignificantly more many-to-many word alignmentsthan those used by Haghighi et al (2009).
We areable to produce some of these many-to-many align-ments by including new many-to-many terminals intheentirecountryinrecentyearsbothsides???????
(a) 2x2theentirecountryinrecentyearsbothsides???????
(b) 2x3theentirecountryinrecentyearsbothsides???????
(c) Gapped 2x3Figure 4: Examples of phrasal alignments that can be rep-resented by our new ITG terminal bispans.our ITG word aligner, as shown in Figure 4.
Ourterminal productions sometimes capture non-literaltranslation like both sides or in recent years.
Theyalso can allow us to capture particular, systematicchanges in the annotation standard.
For example,the gapped pattern from Figure 4 captures the stan-dard that English word the is always aligned to theChinese head noun in a noun phrase.
We featurizethese non-terminals with features similar to thoseof Haghighi et al (2009), and all of the alignmentresults we report in Section 8.2 (both joint and ITG)employ these features.8.2 Parsing and Word AlignmentTo compute features that depend on external models,we needed to train an unsupervised word aligner andmonolingual English and Chinese parsers.
The un-supervised word aligner was a pair of jointly trainedHMMs (Liang et al, 2006), trained on the FBIS cor-pus.
We used the Berkeley Parser (Petrov and Klein,2007) for both monolingual parsers, with the Chi-nese parser trained on the full Chinese treebank, andthe English parser trained on a concatenation of thePenn WSJ corpus (Marcus et al, 1993) and the En-glish side of train.6We compare our parsing results to the mono-lingual parsing models and to the English-Chinesebilingual reranker of Burkett and Klein (2008),trained on the same dataset.
The results are inTable 1.
For word alignment, we compare to6To avoid overlap in the data used to train the monolingualparsers and the joint model, at training time, we used a separateversion of the Chinese parser, trained only on articles 400-1151(omitting articles in train).
For English parsing, we deemed itinsufficient to entirely omit the Chinese treebank data from themonolingual parser?s training set, as otherwise the monolingualparser would be trained entirely on out-of-domain data.
There-fore, at training time we used two separate English parsers: tocompute model scores for the first half of train, we used a parsertrained on a concatenation of the WSJ corpus and the secondhalf of train, and vice versa for the remaining sentences.133Test ResultsCh F1 Eng F1 Tot F1Monolingual 83.6 81.2 82.5Reranker 86.0 83.8 84.9Joint 85.7 84.5 85.1Table 1: Parsing results.
Our joint model has the highestreported F1 for English-Chinese bilingual parsing.Test ResultsPrecision Recall AER F1HMM 86.0 58.4 30.0 69.5ITG 86.8 73.4 20.2 79.5Joint 85.5 84.6 14.9 85.0Table 2: Word alignment results.
Our joint model has thehighest reported F1 for English-Chinese word alignment.the baseline unsupervised HMM word aligner andto the English-Chinese ITG-based word alignerof Haghighi et al (2009).
The results are in Table 2.As can be seen, our model makes substantial im-provements over the independent models.
For pars-ing, we improve absolute F1 over the monolingualparsers by 2.1 in Chinese, and by 3.3 in English.For word alignment, we improve absolute F1 by 5.5over the non-syntactic ITG word aligner.
In addi-tion, our English parsing results are better than thoseof the Burkett and Klein (2008) bilingual reranker,the current top-performing English-Chinese bilin-gual parser, despite ours using a much simpler setof synchronization features.8.3 Machine TranslationWe further tested our alignments by using them totrain the Joshua machine translation system (Li andKhudanpur, 2008).
Table 3 describes the results ofour experiments.
For all of the systems, we tunedRules Tune TestHMM 1.1M 29.0 29.4ITG 1.5M 29.9 30.4?Joint 1.5M 29.6 30.6Table 3: Tune and test BLEU results for machine transla-tion systems built with different alignment tools.
?
indi-cates a statistically significant difference between a sys-tem?s test performance and the one above it.on 1000 sentences of the NIST 2004 and 2005 ma-chine translation evaluations, and tested on 400 sen-tences of the NIST 2006 MT evaluation.
Our train-ing set consisted of 250k sentences of newswire dis-tributed with the GALE project, all of which weresub-sampled to have high Ngram overlap with thetune and test sets.
All of our sentences were oflength at most 40 words.
When building the trans-lation grammars, we used Joshua?s default ?tight?phrase extraction option.
We ran MERT for 4 itera-tions, optimizing 20 weight vectors per iteration ona 200-best list.Table 3 gives the results.
On the test set, we alsoran the approximate randomization test suggested byRiezler and Maxwell (2005).
We found that our jointparsing and alignment system significantly outper-formed the HMM aligner, but the improvement overthe ITG aligner was not statistically significant.9 ConclusionThe quality of statistical machine translation mod-els depends crucially on the quality of word align-ments and syntactic parses for the bilingual trainingcorpus.
Our work presented the first joint modelfor parsing and alignment, demonstrating that wecan improve results on both of these tasks, as wellas on downstream machine translation, by allowingparsers and word aligners to simultaneously informone another.
Crucial to this improved performanceis a notion of weak synchronization, which allowsour model to learn when pieces of a grammar aresynchronized and when they are not.
Although ex-act inference in the weakly synchronized model isintractable, we developed a mean field approximateinference scheme based on monolingual and bitextparsing, allowing for efficient inference.AcknowledgementsWe thank Adam Pauls and John DeNero for theirhelp in running machine translation experiments.We also thank the three anonymous reviewers fortheir helpful comments on an earlier draft of thispaper.
This project is funded in part by NSFgrants 0915265 and 0643742, an NSF graduate re-search fellowship, the CIA under grant HM1582-09-1-0021, and BBN under DARPA contract HR0011-06-C-0022.134ReferencesAnn Bies, Martha Palmer, Justin Mott, and Colin Warner.2007.
English Chinese translation treebank v 1.0.Web download.
LDC2007T02.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In EMNLP.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL.Victoria Fossum, Kevin Knight, and Steven Abney.
2008.Using syntax to improve word alignment for syntax-based statistical machine translation.
In ACL MTWorkshop.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In ACL.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In ACLSSST.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In HLT-NAACL.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Takuya Matsuzaki, Yusuki Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.Jon May and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In EMNLP.Jorge Nocedal and Stephen J. Wright.
1999.
NumericalOptimization.
Springer.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Workshop on Intrinsic and Extrinsic Eval-uation Methods for MT and Summarization, ACL.Lawrence Saul and Michael Jordan.
1996.
Exploit-ing tractable substructures in intractable networks.
InNIPS.Stuart M. Shieber and Yves Schabes.
1990.
Synchronoustree-adjoining grammars.
In ACL.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In HLT-NAACL.David A. Smith and Jason Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous grammarfeatures.
In EMNLP.David A. Smith and Noah A. Smith.
2004.
Bilin-gual parsing with factored estimation: using Englishto parse Korean.
In EMNLP.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In ACL.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005.A discriminative matching approach to word align-ment.
In EMNLP.Martin J Wainwright and Michael I Jordan.
2008.Graphical Models, Exponential Families, and Varia-tional Inference.
Now Publishers Inc., Hanover, MA,USA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Andreas Zollmann, Ashish Venugopal, Stephan Vogel,and Alex Waibel.
2006.
The CMU-AKA syntax aug-mented machine translation system for IWSLT-06.
InIWSLT.135
