Lookahead in Deterministic Left-Corner Parsing?James HENDERSONSchool of Informatics, University of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWUnited Kingdomjames.henderson@ed.ac.ukAbstractTo support incremental interpretation, anymodel of human sentence processing must notonly process the sentence incrementally, it mustto some degree restrict the number of analyseswhich it produces for any sentence prefix.
De-terministic parsing takes the extreme positionthat there can only be one analysis for any sen-tence prefix.
Experiments with an incremen-tal statistical parser show that performance isseverely degraded when the search for the mostprobable parse is pruned to only the most prob-able analysis after each prefix.
One methodwhich has been extensively used to address thedifficulty of deterministic parsing is lookahead,where information about a bounded numberof subsequent words is used to decide whichanalyses to pursue.
We simulate the effects oflookahead by summing probabilities over pos-sible parses for the lookahead words and usingthis sum to choose which parse to pursue.
Wefind that a large improvement is achieved withone word lookahead, but that more lookaheadresults in relatively small additional improve-ments.
This suggests that one word lookaheadis sufficient, but that other modifications to ourleft-corner parsing model could make determin-istic parsing more effective.1 IntroductionIncremental interpretation is a fundamentalproperty of the human parsing mechanism.
Tosupport incremental interpretation, any modelof sentence processing must not only process thesentence incrementally, it must to some degreerestrict the number of analyses which it pro-duces for any sentence prefix.
Otherwise theambiguity of natural language would make thenumber of possible interpretations at any pointin the parse completely overwhelming.
Deter-?
This work has been supported by the Department ofComputer Science, University of Geneva.ministic parsing takes the extreme position thatthere can only be one analysis for any sentenceprefix.
We investigate methods which makesuch a strong constraint feasible, in particularthe use of lookahead.In this paper we do not try to construct a sin-gle deterministic parser, but instead consider afamily of deterministic parsers and empiricallymeasure the optimal performance of a determin-istic parser in this family.
As has been previ-ously proposed by Brants and Crocker (2000),we take a corpus-based approach to this em-pirical investigation, using a previously definedstatistical parser (Henderson, 2003).
The sta-tistical parser uses an incremental history-basedprobability model based on left-corner parsing,and the parameters of this model are estimatedusing a neural network.
Performance of thisbasic model is state-of-the-art, making these re-sults likely to generalize beyond this specific sys-tem.We specify the family of deterministic parsersin terms of pruning the search for the most prob-able parse.
Both deterministic parsing and theuse of k-word lookahead are characterized asconstraints on pruning this search.
We then de-rive the optimal pruning strategy given theseconstraints and the probabilities provided bythe statistical parser?s left-corner probabilitymodel.
Empirical experiments on the accuracyof a parser which uses this pruning method in-dicate the best accuracy we could expect froma deterministic parser of this kind.
This al-lows us to compare different deterministic pars-ing methods, in particular the use of differentamounts of lookahead.In the remainder of this paper, we first dis-cuss how the principles of deterministic parsingcan be expressed in terms of constraints on thesearch strategy used by a statistical parser.
Wethen present the probability model used by thestatistical parser, the way a neural network isused to estimate the parameters of this proba-bility model, and the methods used to search forthe most probable parse according these param-eters.
Finally, we present the empirical experi-ments on deterministic parsing with lookahead,and discuss the implications of these results.2 Approximating OptimalDeterministic ParsingThe general principles of deterministic parsing,as proposed by Marcus (1980), are that parsingproceeds incrementally from left to right, andthat once a parsing decision has been made, itcannot be revoked or overridden by an alter-native analysis.
We translate the first principleinto the design of a statistical parser by using anincremental generative probability model.
Sucha model provides us with probabilities for par-tial parses which generate prefixes of the sen-tence and which do not depend on the wordsnot in this prefix.
We can then translate thesecond principle into constraints on how a sta-tistical parser chooses which partial parses topursue further as it searches for the most prob-able complete parse.The principle that decisions cannot be re-voked or overridden means that, given a se-quence of parser actions a1,..., ai?1 which wehave already chosen, we need to choose a sin-gle parser action ai before considering any sub-sequent parser action ai+1.
However, this con-straint does not prevent considering the effectsof multiple alternative parser actions for ai be-fore choosing between them.
This leaves a greatdeal of flexibility for the design of a determin-istic parser, because the set of actions definedby a deterministic parser does not have to bethe same as the basic decisions defined by ourprobability model.
We can combine any fi-nite sequence of decisions dj ,..., dj+l from ourprobability model into a single parser action ai.This combination allows a deterministic parserto consider the effects of the entire sequence ofdecisions dj ,..., dj+l before deciding whether tochoose it.
Different deterministic parser designswill combine the basic decisions into parser ac-tions in different ways, thereby imposing differ-ent constraints on how long a sequence of fu-ture decisions dj ,..., dj+l can be considered be-fore choosing a parser action.Once we have made a distinction between thebasic decisions of the probability model dj andthe parser actions ai = dj ,..., dj+l, it is conve-nient to express the choice of the parse a1,..., anas a search for the most probable d1,..., dm,where a1,..., an = d1,..., dm.
The search incre-mentally constructs partial parses and prunesthis search down to a single partial parse aftereach complete parser action.
In other words,given that the search has so far chosen the par-tial parse a1,..., ai?1 = d1,..., dj?1, the searchfirst considers all the possible partial parsesd1,..., dj?1, dj ,..., dj+l where there exists an ai =dj ,..., dj+l.
The search is then pruned down toonly the best d1,..., dj?1, dj ,..., dj+l from this set,and the search continues with all partial parsescontaining this prefix.
Thus the search is al-lowed to delay pruning for as many basic deci-sions as are combined into a single parser action.Rather than considering one single determin-istic parser design, in this paper we consider afamily of deterministic parser designs.
We thendetermine tight upper bounds on the perfor-mance of any deterministic parser in this fam-ily.
We define a family of deterministic parsersby starting with a particular incremental gen-erative probability model, and consider a rangeof ways to define parser action ai as finite se-quences dj ,..., dj+l of these basic decisions.We define the family of parser designs as al-lowing the combination of any sequence of de-cisions which occur between the parsing of twowords.
After a word has been incorporated intothe parse, this constraint allows the search toconsider all the possible decision sequences lead-ing up to the incorporation of the next word,but not beyond.
When the next word is reached,the search must again be pruned down to a sin-gle analysis.
This is a natural point to prune,because it is the position where new informa-tion about the sentence is available.
Given thisdefinition of the family of deterministic parsersand the fact that we are only concerned withan upper bound on a deterministic parser?s per-formance, there is no need to consider parserdesigns which require more pruning than this,since they will never perform as well as a parserwhich requires less pruning.Unfortunately, allowing the combination ofany sequence of decisions which occur betweenthe parsing of two words does not exactly corre-spond to the constraints on deterministic pars-ing.
This is because we cannot put a finite upperbound on the number of actions which occur be-tween two words.
Thus this class of parsers in-cludes non-deterministic parsers, and thereforeour performance results represent only an up-per bound on the performance which could beachieved by a deterministic parser in the class.However, there is good reason to believe thisis a tight upper bound.
Lexicalized theories ofsyntax all assume that the amount of informa-tion about the syntactic structure contributedby each word is finite, and that all the informa-tion in the syntactic structure is contributed bysome word.
Thus it should possible to distributeall the information about the structure acrossthe parse in such a way that a finite amountfalls in between each word.
The parsing orderwe use (a form of left-corner parsing) seems toachieve this fairly well, except for the fact thatit uses a stack.
Parsing right-branching struc-tures, such as are found in English, results inthe stack growing arbitrarily large, and then thewhole stack needs to be popped at the end ofthe sentence.
With the exception of these se-quences of popping actions, the number of ac-tions which occur between any two words couldbe bounded.
In our training set, the bound onthe number of non-popping actions between anytwo words could be set at just 4.In addition to designing parser actions tomake deterministic parsing easier, anothermechanism which is commonly used in deter-ministic parser designs is lookahead.
Withlookahead, information about words which havenot yet been incorporated into the parse can beused to decide what action to choose next.
Weconsider models where the lookahead consists ofsome small fixed-length prefix of the un-parsedportion of the sentence, which we call k-wordlookahead.
This mechanisms is constrained bythe requirement that the parser be incremental,since a deterministic parser with k-word looka-head can only provide an interpretation for theportion of the sentence which is k words be-hind what has been input so far.
Thus it is notpossible to include the entire unboundedly-longsentence in the lookahead.
The family of deter-ministic parsers with k-word lookahead wouldinclude parsers which sometimes choose parseractions without waiting to see all k words (andthus on average allow interpretation sooner),but because here we are only concerned withthe optimal performance achievable with a givenlookahead, we do not have to consider these al-ternatives.The optimal deterministic parser with looka-head will choose the partial parse which is themost likely to lead to the correct complete parsegiven the previous partial parse plus the k wordsof lookahead.
In other words, we are tryingto maximize P (at+1|a1,..., at, wt+1,..., wt+k),which is the same as maximizingP (at+1, wt+1,..., wt+k|a1,..., at) for the givenwt+1,..., wt+k.
(Note that any partial parsea1,..., at generates the words w1,..., wt, becausethe optimal deterministic parser designs weare considering all have parser actions whichcombine the entire portion of a parse betweenone word and another.)
We can compute thisprobability by summing over all parses whichinclude the partial parse a1,..., at+1 and whichgenerate the lookahead string wt+1,..., wt+k.P (at+1, wt+1,..., wt+k|a1,..., at) =?
(at+2,...,at+k) P (at+1, at+2,..., at+k|a1,..., at)where at+1,..., at+k generates wt+1,..., wt+k .Because the parser actions are defined in termsof basic decisions in the probability model, wecan compute this sum directly using the prob-ability model.
A real deterministic parser can-not actually perform this computation explic-itly, because it involves pursuing multiple anal-yses which are then discarded.
But ideally adeterministic parser should compute an esti-mate which approximates this sum.
Thus wecan compute the performance of a deterministicparser which makes the ideal use of lookaheadby explicitly computing this sum.
Again, thiswill be an upper bound on the performance ofa real deterministic parser, but we can reason-ably expect that a real deterministic parser canreach performance quite close to this ideal for asmall amount of lookahead.This approach to lookahead can also be ex-pressed in terms of pruning the search for thebest parse.
After pruning to a single par-tial parse a1,..., at which ends by generatingwt, the search is allowed to pursue multipleparses in parallel until they generate the wordwt+k.
The probabilities for these new partialparses are then summed to get estimates ofP (at+1, wt+1,..., wt+k|a1,..., at) for each possibleat+1, and these sums are used to choose a singleat+1.
The search is then pruned by removing allpartial parses which do not start with a1,..., at+1.The remaining partial parses are then contin-ued until they generate the word wt+k+1, andtheir probabilities are summed to decide how toprune to a single choice of at+2.By expressing the family of deterministicparsers with lookahead in terms of a pruningstrategy on a basic parsing model, we are able toeasily investigate the effects of different looka-head lengths on the maximum performance ofa deterministic parser in this family.
To com-plete the specification of the family of determin-istic parsers, we simple have to specify the basicparsing model, as done in the next section.3 A Generative Left-CornerProbability ModelAs with several previous statistical parsers(Collins, 1999; Charniak, 2000), we use a gen-erative history-based probability model of pars-ing.
Designing a history-based model of pars-ing involves two steps, first choosing a mappingfrom the set of phrase structure trees to the setof parses, and then choosing a probability modelin which the probability of each parser decisionis conditioned on the history of previous deci-sions in the parse.
For the model to be genera-tive, these decisions must include predicting thewords of the sentence.
To support incrementalparsing, we want to map phrase structure treesto parses which predict the words of the sen-tence in their left-to-right order.
To support de-terministic parsing, we want our parses to spec-ify information about the phrase structure treeat appropriate points in the sentence.
For thesereasons, we choose a form of left-corner parsing(Rosenkrantz and Lewis, 1970).In a left-corner parse, each node is introducedafter the subtree rooted at the node?s first childhas been fully parsed.
Then the subtrees for thenode?s remaining children are parsed in theirleft-to-right order.
In the form of left-cornerparsing we use, parsing a constituent starts bypushing the leftmost word w of the constituentonto the stack with a shift(w) action.
Parsing aconstituent ends by either introducing the con-stituent?s parent nonterminal (labeled Y ) witha project(Y) action, or attaching to the parentwith a attach action.More precisely, this parsing strategy is a ver-sion of left-corner parsing which first appliesright-binarization to the grammar, as is donein (Manning and Carpenter, 1997) except thatwe binarize down to nullary rules rather thanto binary rules.
This means that choosing thechildren for a node is done one child at a time,and that ending the sequence of children is aseparate choice.
We also extended the parsingstrategy slightly to handle Chomsky adjunctionstructures (i.e.
structures of the form [X [X .
.
.
][Y .
.
.]])
as a special case.
The Chomsky ad-junction is removed and replaced with a special?modifier?
link in the tree (becoming [X .
.
.
[modY .
.
.]]).
This means that the parser?s set ofbasic actions includes modify, as well as attach,shift(w), and project(Y).
We also compiled somefrequent chains of non-branching nodes (such as[S [VP .
.
.]])
into a single node with a new la-bel (becoming [S-VP .
.
.]).
All these grammartransforms are undone before any evaluation ofthe output trees is performed.Because this mapping from phrase structuretrees to sequences of parser decisions is one-to-one, finding the most probable phrase structuretree is equivalent to finding the parse d1,..., dmwhich maximizes P (d1,..., dm), as is done in gen-erative models.
Because this probability in-cludes the probabilities of the shift(wi) deci-sions, this is the joint probability of the phrasestructure tree and the sentence.
The probabil-ity model is then defined by using the chain rulefor conditional probabilities to derive the prob-ability of a parse as the multiplication of theprobabilities of each decision di conditioned onthat decision?s prior parse history d1,..., di?1.P (d1,..., dm) = ?iP (di|d1,..., di?1)The parameters of this probability model arethe P (di|d1,..., di?1).
Generative models are thestandard way to transform a parsing strategyinto a probability model, but note that we arenot assuming any bound on the amount of in-formation from the parse history which mightbe relevant to each parameter.4 Estimating the Parameters with aNeural NetworkThe most challenging problem in estimatingP (di|d1,..., di?1) is that the conditional includesan unbounded amount of information.
Theparse history d1,..., di?1 grows with the length ofthe sentence.
In order to apply standard prob-ability estimation methods, we use neural net-works to induce finite representations of this se-quence, which we will denote h(d1,..., di?1).
Theneural network training methods we use try tofind representations which preserve all the infor-mation about the sequences which are relevantto estimating the desired probabilities.P (di|d1,..., di?1) ?
P (di|h(d1,..., di?1))Of the previous work on using neural net-works for parsing natural language, by far themost empirically successful has been the workusing Simple Synchrony Networks (Henderson,2003).
Like other recurrent network architec-tures, SSNs compute a representation of an un-bounded sequence by incrementally computinga representation of each prefix of the sequence.At each position i, representations from earlierin the sequence are combined with features ofthe new position i to produce a vector of realvalued features which represent the prefix end-ing at i.
This representation is called a hiddenrepresentation.
It is analogous to the hiddenstate of a Hidden Markov Model.
As long asthe hidden representation for position i?1 is al-ways used to compute the hidden representationfor position i, any information about the entiresequence could be passed from hidden represen-tation to hidden representation and be includedin the hidden representation of that sequence.When these representations are then used to es-timate probabilities, this property means thatwe are not making any a priori hard indepen-dence assumptions.The difference between SSNs and most otherrecurrent neural network architectures is thatSSNs are specifically designed for process-ing structures.
When computing the his-tory representation h(d1,..., di?1), the SSN usesnot only the previous history representationh(d1,..., di?2), but also uses history representa-tions for earlier positions which are particularlyrelevant to choosing the next parser decision di.This relevance is determined by first assigningeach position to a node in the parse tree, namelythe node which is on the top of the parser?sstack when that decision is made.
Then therelevant earlier positions are chosen based onthe structural locality of the current decision?snode to the earlier decisions?
nodes.
In this way,the number of representations which informa-tion needs to pass through in order to flow fromhistory representation i to history representa-tion j is determined by the structural distancebetween i?s node and j?s node, and not just thedistance between i and j in the parse sequence.This provides the neural network with a lin-guistically appropriate inductive bias when itlearns the history representations, as explainedin more detail in (Henderson, 2003).
The factthat this bias is both structurally defined andlinguistically appropriate is the reason that thisparser performs so much better than previousattempts at using neural networks for parsing,such as (Costa et al, 2001).Once it has computed h(d1,..., di?1), the SSNuses standard methods (Bishop, 1995) to esti-mate a probability distribution over the set ofpossible next decisions di given these represen-tations.
This involves further decomposing thedistribution over all possible next parser actionsinto a small hierarchy of conditional probabili-ties, and then using log-linear models to esti-mate each of these conditional probability dis-tributions.
The input features for these log-linear models are the real-valued vectors com-puted by h(d1,..., di?1), as explained in more de-tail in (Henderson, 2003).As with many other machine learning meth-ods, training a Simple Synchrony Network in-volves first defining an appropriate learning cri-teria and then performing some form of gra-dient descent learning to search for the opti-mum values of the network?s parameters accord-ing to this criteria.
We use the on-line ver-sion of Backpropagation to perform the gradi-ent descent.
This learning simultaneously triesto optimize the parameters of the output com-putation and the parameters of the mappingh(d1,..., di?1).
With multi-layered networks suchas SSNs, this training is not guaranteed to con-verge to a global optimum, but in practice anetwork whose criteria value is close to the op-timum can be found.5 Searching for the most probableparseAs discussed in section 2, we investigate de-terministic parsing by translating the princi-ples of deterministic parsing into properties ofthe pruning strategy used to search the spaceof possible parses.
The complete parsing sys-tem alternates between using the search strat-egy to decide what partial parse d1,..., di?1 topursue further and using the SSN to estimatea probability distribution P (di|d1,..., di?1) overpossible next decisions di.
The probabilitiesP (d1,..., di) for the new partial parses are thenjust P (d1,..., di?1) ?
P (di|d1,..., di?1).
When nopruning applies, the partial parse with the high-est probability is chosen as the next one to beextended.Even in the non-deterministic version of theparser, we need to prune the search space.
Thisis because the number of possible parses is ex-ponential in the length of the sentence, andwe cannot use dynamic programming to com-pute the best parse efficiently because we do notmake any independence assumptions.
However,we have found that the search can be drasti-cally pruned without loss in accuracy, using asimilar approach to that used here to model de-terministic parsing.
After the prediction of eachword, we prune all partial parses except a fixedbeam of the most probable partial parses.
Dueto the use of the above left-corner parsing order,we have found that the beam can be as little as100 parses without having any measurable effecton accuracy.
Below we will refer to this beamwidth as the post-word search beam width.In addition to pruning after the prediction ofeach word, we also prune the search space in be-tween two words by limiting its branching fac-tor to at most 5.
This, in effect, just limits thenumber of labels considered for each new non-terminal.
We found that increasing the branch-ing factor had no effect on accuracy and littleeffect on speed.For the simulations of deterministic parsers,we always applied both the above pruningstrategies, in addition to the deterministic prun-ing.
This non-deterministic pruning reducesthe number of partial parses a1,..., at+1,..., at+kwhose probabilities are included in the sum usedto choose at+1 for the deterministic pruning.This approximation is not likely to have anysignificant effect on the choice of at+1, becausethe probabilities of the partial parses which arepruned by the non-deterministic pruning tendto be very small compared to the most prob-able alternatives.
The non-deterministic prun-ing also reduces the set of partial parses whichare chosen between during the subsequent de-terministic pruning.
But this undoubtedly hasno significant effect, since experimental resultshave shown that the level of non-deterministicpruning discussed above does not effect perfor-mance even without deterministic pruning.6 The ExperimentsTo investigate the effects of lookahead on ourfamily of deterministic parsers, we ran empiricalexperiments on the standard the Penn Treebank(Marcus et al, 1993) datasets.
The input tothe network is a sequence of tag-word pairs.1We report results for a vocabulary size of 508tag-word pairs (a frequency threshold of 200).We first trained a network to estimate the pa-rameters of the basic probability model.
We de-termined appropriate training parameters andnetwork size based on intermediate validation1We used a publicly available tagger (Ratnaparkhi,1996) to provide the tags.
This tagger is run before theparser, so there may be some information about futurewords which is available in the disambiguated tag whichis not available in the word itself.
We don?t think this hashad a significant impact on the results reported here, butcurrently we are working on doing the tagging internallyto the parser to avoid this problem.8082848688900 2 4 6 8 10 12 14 16deterministic recalldeterministic precisionnon-deterministic recallnon-deterministic precisionFigure 1: Labeled constituent recall and pre-cision as a function of the number of wordsof lookahead used by a deterministic parser.Curves reach their non-deterministic perfor-mance with large lookahead.results and our previous experience.2 Wetrained several networks and chose the best onesbased on their validation performance.
Thebest post-word search beam width for the non-deterministic parser was determined on the val-idation set, which was 100.To avoid repeated testing on the standardtesting set, we measured the performance ofthe different models on section 0 of the PennTreebank (which is not included in either thetraining or validation sets).
Standard measuresof accuracy for different lookahead lengths areplotted in figure 1.3 First we should note thatthe non-deterministic parser has state-of-the-art accuracy (89.0% F-measure), considering itsvocabulary size.
A moderately larger vocabu-lary version (4215 tag-word pairs) of this parserachieves 89.8% F-measure on section 0, wherethe best current result on the testing set is90.7% (Bod, 2003).As expected, the deterministic parsers doworse than the non-deterministic one, and thisdifference becomes less as the lookahead islengthened.
What is surprising about the curvesin figure 1 is that there is a very large increasein performance from zero words of lookahead2The best network had 80 hidden units for the historyrepresentation.
Weight decay regularization was appliedat the beginning of training but reduced to near 0 by theend of training.
Training was stopped when maximumperformance was reached on the validation set, using apost-word beam width of 5.3All our results are computed with the evalb programfollowing the standard criteria in (Collins, 1999).
Weused the standard training (sections 2?22, 39,832 sen-tences, 910,196 words) and validation (section 24, 1346sentence, 31507 words) sets (Collins, 1999).
Results ofthe nondeterministic parser average 0.2% worse on thestandard testing set, and average 0.8% better when alarger vocabulary (4215 tag-word pairs) is used.(i.e.
pruning the search to 1 alternative directlyafter every word) to one word of lookahead.
Af-ter one word of lookahead the curves show rel-atively moderate improvements with each addi-tional word of lookahead, converging to the non-deterministic level, as would be expected.4 Butbetween zero words of lookahead and one wordof lookahead there is a 5.6% absolute improve-ment in F-measure (versus a 0.9% absolute im-provement between one and two words of looka-head).
In other words, adding the first wordof lookahead results in a 2/3 reduction in thedifference between the deterministic and non-deterministic parser?s F-measure, while addingsubsequent words results in at most a 1/3 re-duction per word.7 DiscussionThe large improvement in performance whichresults from adding the first word of lookahead,as compared to adding the subsequent words,indicates that the first word of lookahead hasa qualitatively different effect on deterministicparsing.
We believe that one word of lookaheadis both necessary and sufficient for a model ofdeterministic parsing.The large gain provided by the first word oflookahead indicates that this lookahead is nec-essary for deterministic parsing.
Given the factthe with one word of lookahead the F-measureof the deterministic parser is only 2.7% belowthe maximum possible, it is unlikely that thefamily of deterministic parsers assumed here isso sub-optimal that the entire 5.6% improve-ment gained with one word lookahead is simplythe result of compensating for limitations in thechoice of this family.The performance curves in figure 1 also sug-gest that one word of lookahead is sufficient.
Webelieve the gain provided by more than one wordof lookahead is the result of compensating forlimitations in the family of deterministic parsersassumed here.
Any limitations in this familywill result in the deterministic search makingchoices before the necessary disambiguating in-formation is available, thereby leading to addi-tional errors.
As the lookahead increases, somepreviously mistaken choices will become disam-biguated by the additional lookahead informa-tion, thereby improving performance.
In thelimit as lookahead increases, the performance of4Note that when the lookahead length is longerthan the longest sentence, the deterministic and non-deterministic parsers become equivalent.the deterministic and non-deterministic parserswill become the same, no matter what familyof deterministic parsers has been specified.
Thesmooth curve of increasing performance as thelookahead is increased above one word is thetype of results we would expect if the lookaheadwere simply correcting mistakes in this way.Examples of possible limitations to the fam-ily of deterministic parsers assumed here includethe choice of the left-corner ordering of parserdecisions.
The left-corner ordering completelydetermines when each decision about the phrasestructure tree must be made.
If the family of de-terministic parsers had more flexibility in thisordering, then the optimal deterministic parsercould use an ordering which was tailored to thestatistics of the data, thereby avoiding beingforced to make decisions before sufficient infor-mation is available.8 ConclusionsIn this paper we have investigated issues in de-terministic parsing by characterizing these is-sues in terms of the search procedure used bya statistical parser.
We use a neural networkto estimate the probabilities for an incrementalhistory-based probability model based on left-corner parsing.
Using an unconstrained searchprocedure to try to find the most probable parseaccording to this probability model (i.e.
non-deterministic parsing) results in state-of-the-artaccuracy.
Deterministic parsing is simulatedby allowing the sequence of decisions betweentwo words to be combined into a single parseraction, and choosing the best single combinedaction based on the probability calculated us-ing the basic left-corner probability model.
Allparses which do not use this chosen action arethen pruned from the search.
When this prun-ing is applied directly after each word, there isa large reduction in accuracy (8.3% F-measure)as compared to the non-deterministic search.Given the pervasive ambiguity in natural lan-guage, it is not surprising that this drastic prun-ing strategy results in a large reduction in ac-curacy.
For this reason, deterministic parsersusually use some form of lookahead.
Looka-head gives the parser more information aboutthe sentence at the point when the choice ofthe next parser action takes place.
We sim-ulate the optimal use of k-word lookahead bysumming over all partial parses which continuethe given partial parse to the point where allk words in the lookahead have been generated.When expressed in terms of search, this meansthat the deterministic pruning is done k wordsbehind a non-deterministic search for the bestparse, based on a sum over the partial parsesfound by the non-deterministic search.
Whenaccuracy is plotted as a function of k (figure 1),we found that there is a large increase in accu-racy when the first word of lookahead is added(only 2.7% F-measure below non-deterministicsearch).
Further increases in the lookaheadlength have much less of an impact.We conclude that the first word of lookaheadis necessary for the success of any deterministicparser, but that additional lookahead is proba-bly not necessary.
The remaining error createdby this model of deterministic parsing is proba-bly best dealt with by investigating other aspectof the model of deterministic parsing assumedhere, in particular the strict adherence to theleft-corner parsing order.Despite the need to consider alternatives tothe left-corner parsing order, these results dodemonstrate that the left-corner parsing strat-egy proposed is surprisingly good at supportingdeterministic parsing.
This fact is importantin making the non-deterministic search strat-egy used with this parser tractable.
The obser-vations made in this paper could lead to moresophisticated search strategies which further in-crease the speed of this or similar parsers with-out significant reductions in accuracy.ReferencesChristopher M. Bishop.
1995.
Neural Networksfor Pattern Recognition.
Oxford UniversityPress, Oxford, UK.Rens Bod.
2003.
An efficient implementation ofa new DOP model.
In Proc.
10th Conf.
of Eu-ropean Chapter of the Association for Com-putational Linguistics, Budapest, Hungary.Thorsten Brants and Matthew Crocker.
2000.Probabilistic parsing and psychological plau-sibility.
In Proceedings of the EighteenthConference on Computational Linguistics(COLING-2000), Saarbru?cken / Luxemburg/ Nancy.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
1st Meeting of NorthAmerican Chapter of Association for Compu-tational Linguistics, pages 132?139, Seattle,Washington.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania, Philadel-phia, PA.F.
Costa, V. Lombardo, P. Frasconi, andG.
Soda.
2001.
Wide coverage incrementalparsing by learning attachment preferences.In Proc.
of the Conf.
of the Italian Associa-tion for Artificial Intelligence.James Henderson.
2003.
Inducing history rep-resentations for broad coverage statisticalparsing.
In Proc.
joint meeting of NorthAmerican Chapter of the Association forComputational Linguistics and the HumanLanguage Technology Conf., pages 103?110,Edmonton, Canada.Christopher D. Manning and Bob Carpenter.1997.
Probabilistic parsing using left cornerlanguage models.
In Proc.
Int.
Workshop onParsing Technologies, pages 147?158.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Mitchell Marcus.
1980.
A Theory of Syntac-tic Recognition for Natural Language.
MITPress, Cambridge, MA.Adwait Ratnaparkhi.
1996.
A maximum en-tropy model for part-of-speech tagging.
InProc.
Conf.
on Empirical Methods in NaturalLanguage Processing, pages 133?142, Univ.
ofPennsylvania, PA.D.J.
Rosenkrantz and P.M. Lewis.
1970.
De-terministic left corner parsing.
In Proc.
11thSymposium on Switching and Automata The-ory, pages 139?152.
