ACQUIRING DISAMBIGUATION RULES FROM TEXTDonald HindleAT~T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974-2070AbstractAn effective procedure for automatically acquiringa new set of disambiguation rules for an existingdeterministic parser on the basis of tagged text ispresented.
Performance of the automatically ac-quired rules is much better than the existing hand-written disambiguation rules.
The success of theacquired rules depends on using the linguistic in-formation encoded in the parser; enhancements tovarious components of the parser improves the ac-quired rule set.
This work suggests a path towardmore robust and comprehensive syntactic analyz-ers.1 IntroductionOne of the most serious obstacles to developingparsers to effectively analyze unrestricted Englishis the difficulty of creating sufllciently comprehen-sive grammars.
While it is possible to developtoy grammars for particular theoretically interest-ing problems, the sheer variety of forms in En-glish together with the complexity of interactionthat arises in a typical syntactic analyzer makeseach enhancement of parser coverage increasinglydifficult.
There is no question that we are stillquite far f~om syntactic analyzers that even beginto adequately model the grammatical variety ofEnglish.
To go beyond the current generation ofhand built grAmrnars for syntactic analysis it willbe necessary to develop means of acquiring someof the needed grammatical information from theregularities that appear in large corpora of natu-rally occurring text.This paper describes an implemented trainingprocedure for automatically acquiring symbolicrules for a deterministic parser on the basis of un-restricted textual input.
In particular, I describeexperiments in automatically acquiring a set ofrules for disambiguation f lexical category (partof speech).
Performance of the acquired rule setis much better than the set of rules for lexical dis-ambiguation written for the parser by hand overa period of several rules; the error rate is approx-imately half that of the hand written rules.
Fur-thermore, the error rate is comparable to recentprobabilistic approaches such as Church (1987)and Garside, Leech and Sampson (1987).
Thecurrent approach has the added advantage that,since the rules acquired depend on the parser'sgrammar in general, independent improvements inother modules of the parser can lead to improve-ment in the performance of the disambiguationcomponent.2 Categorial AmbiguityAmbiguity of part of speech is a pervasive char-acteristic of English; more than a third of theword tokens in the million-word "Brown Corpus"of written English (Francis and Kucera 1982) arecate$orially ambiguous.
It is possible to constructsentences in which every word is ambiguous, uchas the following,(1) Her hand had come to rest on that very book.But even without such contrived exaggeration,ambiguity of lsxical category is not a trivial prob-lem.
Nor can part of speech ambiguity be ig-nored in constructing models of natural anguageprocessing, since syntactic analysis (as well ashigher levels of analysis) depends on correctly dis-ambiguating the lexical category of both contentwords and function words like to and that.It may seem that disambiguating lexical cate-gory should depend on complex reasoning abouta variety of factors known to influence ambiguityin general, including semantic and pragmatic fac-tors.
No doubt some aspects of disambiguatinglexical category can be expressed in terms of suchhigher level decisions.
But if disambiguation infact depends on such higher level reasoning, thereis little hope of succeeding in disambiguationunrestricted text.118Fortunately, there is reason to believe that lex-ical disambiguation can proceed on more limitedsyntactic patterns.
Indeed, recent increased inter-est in the problem of disambiguating lexical cat-egory in English has led to significant progress indeveloping effective programs for assigning lexi-cal category in unrestricted text.
The most suc-cessful and comprehensive of these are based onprobabilistic modeling of category sequence andword category (Church 1987; Garside, Leech andSampson 1987; DeRose 1988).
These stochasticmethods show impressive performance: Church re-ports a success rate of 95 to 99%, and shows asample text with an error rate of less than onepercent.
What  may seem particularly surprisingis that these methods succeed essentially with-out reference to syntactic structure; purely sur-face lexical patterns are involved.
In contrastto these recent stochastic methods, earlier meth-ods based on categorical rules for surface patternsachieved only moderate success.
Thus for exam-ple, Klein and Simmons (1963) and Greene andRubin (1971) report success rates considerably be-low recent stochastic approaches.It is tempting to conclude from this contrastthat robust handling of unrestricted text de-mands general probabilistic methods in preferenceto deeper linguistic knowledge.
The Lancaster(UCREL)  group explicitly takes this position, sug-gesting: "... if we analyse quantitatively a suffi-ciently large amount of language data, we will beable to compensate for the computer's lack of so-phisticated knowledge and powers of inference, atleast to a considerable extent."
(Garside, Leechand Sampson 1987:3).In this paper, I want to emphasize a somewhatdifferent view of the role of large text corpora inbuilding robust models of natural language.
Inparticular, I will show that that large corpora ofnaturally occurring text can be used together withthe rule-based syntactic analyzers we have today- to build more effective linguistic analyzers.
As theinformation derived from text is incorporated intoour models, it will help increase the sophisticationof our linguistic models.
I suggest that in order tomove from our current impoverished natural lan-guage processing systems to more comprehensiveand robust linguistic models we must ask Can weacquire the linguistic information needed on thebasis of tezt?
If we can answer this questionaff~matively - and this paper presents evidencethat we can - then there is hope that we can makesome progress in constructing more adequate nat-ural language processing systems.It is important to emphasize that the ques-tion whether we can acquire linguistic informa-tion from text is independent of whether the modelis probabilistic, categorical, or some combinationof the two.
The issue is not, I believe, symbolicversus probabilistic rules, but rather whether wecan acquire the necessary linguistic information in-stead of building systems completely by hand.
Noalgorithm~ symbolic or otherwise, will succeed inlarge scale processing of natural text unless it canacquire some of the needed knowledge from sam-pies of naturally occurring text.3 Lexical D isambiguat ion ina Determinist ic  ParserThe focus of this paper is the problem of disam-biguating lexical category (part of speech) withina deterministic parser of the sort originated byMarcus (1980).
Fidditch is one such deterministicparser, designed to provide a syntactic analysis oftext as a tool for locating examples of various lin-guisticaUy interesting structures (Hindle 1983).
Ithas gradually been modified over the past severalyears to improve its ability to handle unrestrictedtext.Fidditch is designed to provide an annotatedsurface structure.
It aims to build phrase structuretrees, recovering complement relations and gappedelements.
It has?
a lexicon of about 100,000 words listing allpossible parts of speech for each word, alongwith root forms for inflected words.?
a morphological analyzer to assign part ofspeech and root form for words not in thelexicon?
a complementation lexicon for about 4000words?
a list of about 300 compound words, such asof cotJrse?
a set of about 350 regular grammar rules tobuild phrase structure?
a set of about 350 rules to disambiguate lexi-cal categoryBeing a deterministic parser, Fidditch pursues asingle path in analyzing a sentence and provides asingle analysis.
Of course, the parser is necessarilyfar from complete; neither its grammar rules norits lexicon incorporate all the information needed119to adequately describe English.
Therefore, it is tobe expected that the parser will encounter struc-tures that it does not recognize and will make er-rors of analysis.
When it is unable to provide acomplete analysis of text, it is designed to returna partial description and proceed.
Even with theinevitable rrors, it has proven useful for analyz-ing text.
(The parser has been used to analyzetens of millions of words of written text as wellas transcripts of speech in order to, for example,search for subject-verb-object triples.
)Rules for the parser are essentially pattern-action rules which match a single incomplete node(from a stack) and a buffer of up to three com-pleted constituents.
The patterns of parser rulescan refer only to limited aspects of the currentparser state.
Rules can mention the grammaticalcategory of the constituents in the buffer and thecurrent incomplete node.
Rules can also refer toa limited set (about 200) of specific words thatare grammatically distinguished (e.g.
be, of, as).Complementation rules of course refer to a largerset of specific lexical items.The model of the parser is that it recognizesgrammatical patterns; whenever it sees a patternof its rule base, it builds the associated structure;if it doesn't see a pattern, it does nothing.
At ev-ery step in the parse, the most specific pattern isselected..The more linguistic information in theparser, the better able it will be to recognize anddescribe patterns.
But when it does not recognizesome construction, it simply uses a more generalpattern to parse it.
This feature (i.e., matching themost specific pattern available, but always havingdefault analyses as more general patterns) is nec-essary both for analyzing unrestricted text and fortraining on the basis of unrestricted text.D isambiguat ion  ru lesOne of the possible rule actions of the parser is toselect a lexical category for an ambiguous word.In Fidditch about half of the 700 pattern-actionrules are disambiguation rules.A simple disambiguation rule, both existing inthe hand-written disambiguation rules and ac-quired by the training algorithm, looks like this:(9.)
\[PREP-{-TNS\] " -TNS \[N'ILV\]Rule (2) says that a word that can be a preposi-tion or a tense marker (i.e.
the word to) followedby a word which can be a noun or a verb is atense marker followed by a verb.
This rule is obvi-ously not always correct; there are two ways that120it can be overridden.
For rule (2), a previous rulemay have already disambiguated the PREP-t-TNS,for example by recognizing the phrase close to.
Al-ternatively, a more specific urrent rule may apply,for example recognizing the specific noun date into date.
In general, the parser provides a windowof attention that moves through a sentence fromthe beginning to the end.
A rule that, consideredin isolation, would match some sequence of wordsin a sentence, may not in fact apply, either be-cause a more specific rule matches, or because adifferent rule applied earlier.These disambiguation rules are obviously closelyrelated to the bigrams and trigrams of stochasticdisambiguation methods.
The rules differ in that1) they can refer to the 200 specified lexical items,and 9.)
they can refer to the current incompletenode.Disambiguation of lexical category must occurbefore the regular grammar ules can run; regu-lar grammar rules only match nodes whose lexicalcategory is disambiguated.
1The  grammat ica l  ca tegor iesFidditch has 46 lexical categories (incltlding 8punctuations), mostly encoding rather standardparts of speech, with inflections folded into thecategory set.
This is many fewer than the 87 sim-ple word tags of the Brown Corpus or of relatedtagging systems (see Garside, Leech and Samp-son 1987:165-183).
Most of the proliferation oftags in such systems is the result of encoding in-formation that is either lexically predictable orstructurally predictable.
For example, the Browntagset provides distinct ags for subjective and ob-jective uses of pronouns.
For I and me this dis-tinction is predictable both from the lexical itemsthemselves and from the structure in which theyoccur.
In Fidditch, both subjective and objectivepronouns are tagged simply as PRo.One of the motivations of the larger tagsets isto facilitate searching the corpus: using only theelaborated tags, it is possible to recover some lex-ical and structural distinctions.
When Fidditchis used to search for constructions, the syntacticstructure and lexical identity of items is availableand thus there is no need to encode it in the tagset.To use the tagged Brown Corpus for training andIMore recent approaches todeter~i-i~tic parsing mayallow categorial disamhiguation to occur ~fler some of thesyntactic properties ofphrases are noted (Marcus, Hindle,and Fleck 1983).
But in structure-b,,Hdln~ determlniRtlcparsers such ss Fidditch, lexical category must be disam-biguAted be/ore any ~m~r~ can he built.evaluating disambiguation rules, the Brown cate-gories were mapped onto the 46 lexical categoriesnative to Fidditch.E r ro rs  in  the  hand-wr i t ten  d i sam-b iguat ion  ru lesUsing the tagged Brown Corpus, we can ask howwell the disambiguation rules of Fidditch performin terms of the tagged Brown Corpus.
Compar-ing the part of speech assigned by Fidditch to the(transformed) Brown part of speech, we find about6.5% are assigned an incorrect category.
Approxi-mately 30% of the word tokens in the Brown Cor-pus are categorially ambiguous in the Fidditch lex-icon; it is this 30% that we are concerned with inacquiring disambignation rules.
For these ambigu-ons words, the error rate for the hand constructeddisambignation rules is about 19%.
That is, about1 out of 5 of the ambiguous word tokens are in-correctly disambiguated.
This means that there isa good chance that any given sentence wilt havean error in part of speech.
Obviously, there isconsiderable motivation for improving the lexicaldisambiguation.
I deed, errors in lexical categorydisambignation are the biggest source of error forthe parser.It has been my experience that the disambigna-tion rule set is particularly difficult to improve byhand.
The disambiguation rules make less syn-tactic sense than the regular grammar ules, andtherefore the effect of adding or deleting a ruleon the parser performance is hard to predict.
Inthe long run it is likely that these disambignationrules should be done away with, substituting dis-ambiguation by side effect as proposed by Milne(1986).
But in the meantime, we are faced withthe need to improve this model of lexical disana-bignation for a determinhtic parser.4 The  Tra in ing  ProcedureThe model of deterministic parsing proposed byMarcus (1980) has several properties that aid inacquisition of symbolic rules for syntactic analy-sis, and provide a natural way to resolve the twinproblems of discovering a) when it is necessary toacquire a new rule, and b) what new rule to ac-quire (see the discussion in Berwick 1985).
Thekey features of this niodel of parsing relevant oacquisition are:?
because the parser is deterministic and hasa limited window of attention, failure (andtherefore the need for a new rule) can be lo-calized.?
because the rules of the parser correspondclosely to the instantaneous description of thestate of the parser, it is easy to determine theform of the new rule.?
because there is a natural ordering of the rulesacquired, there is never any ambiguity aboutwhich rule to apply.
The ordering of newrules is fixed because more specific rules al-ways have precedence.These characteristics of the deterministic parserprovide a way to acquire a new set of lexical disam-biguation rules.
The idea is as follows.
Beginningwith a small set of disambiguation rules, proceedto parse the tagged Brown Corpus.
Check eachd~ambiguation action against the tags to see ifthe correct choice was made.
If an incorrect choicewas made, use the current state of the parser'to-gether with the current set of disambiguation rulesto create a new disambiguation rule to make thecorrect choice.Once a rule has been acquired in this manner,it may turn out that it is not a correct rule.
Al-though it worked for the triggering case, it may failon other cases.
If the rate of failure is sufficiently ~high, it is deactivated.An additional phase of acquisition would be togeneralize the rules to reduce the number of rulesand widen their applicability.
In the experimentsreported here, no genera~.ation has been done.This makes the rule set more redundant and lesscompact han necessary.
However, the simplicityof the rule patterns of this expanded rule set allow a compact encoding and an ei~cient patternmatching.The initial state for the training has the com-plete parser grammar - all the rules for buildingstructures - but only a minimal set of context in-dependent default disambiguation rules.
Specifi-cally, training begins with a set of rules which se-lect a default category for ambiguous words wordsignoring all context.
For example, the rule (3) saysthat a word that can be an adjective or a noun ora verb (appearing in the first buffer position) is anoun, no matter what the second and third bufferpositions show and no matter what the currentincomplete node is.
(3) A default  d lsambiguat ion rule= N \[*\] \[*\]In the absence of any other disambiguation rules(i.e.
before any training), this rule would declare121fleet, which according to Fidditch's lexicon is anXVJ-I-Nq-V, to be a noun.
There are 136 such de-fault disambiguation rules, one for each lexicallypossible combination of lexical categories.Acquisition of the disambiguation rules pro-ceeds in the course of parsing sentences.
In thisway, the current state of the parser - the sentenceas analyzed thus far - is available as a pattern forthe training.
At each step in parsing, before apply-ing any parser ule, the program checks whether anew disambiguation rule may be acquired.
If nei-ther the first nor the second buffer position con-tains an ambiguous word, no disambiguation canoccur, and no acquisition will occur.
When an am-biguous word is encountered in the first or secondbuffer position, the current set of disambiguationrules may change.New rule acquisitionThe training algorithm has two basic components.The first component - new rule acquis i t ion -first checks whether the currently selected dis-ambiguation rule correctly disambiguates thebiguous items in the buffer.
If the wrong choiceis made, then a new, more specific rule may beadded to the rule set to make the correct disam-biguation choice.
(Since the new rule is more spe-cific than the currently selected rule, it will haveprecedence over the older rule, and thus will makethe correct disambiguation for the current case,overriding any previous disamhiguation choice).The  pat tern  for the new rule is determinedby the current parse state together with the cur-rent set of disambiguation rules.
The new rule pat-tern must match the current state and also mustbe be more specific than any currently matchingdisambiguation rule.
(If an existing rule matchesthe current state, it must be doing the wrong dis-ambiguation, otherwise we would not be trying toacquire a new rule).
If there is no available morespecific pattern, no acquisition is possible, and thecurrent rule set reiD~ins.Although the patterns for rules are quite re-stricted, referring only to the data structures ofthe parser with a restricted set of categories, thereare nevertheless on the order of 109 possible dis-ambiguation rules.The  act ion for  the new rule is simply tochoose the correct part of speech.Rule deactivationThe second component of the rule acquisition -ru le  deact ivat ion  - comes into play when thecurrent disambiguation rule set makes the wrongdisambiguation and yet no new rule can be ac-quired (because there is no available more specificrule).
The incorrect rule may in this case be per-manently deactivated.
This deactivation occursonly when the proportion of incorrect applicationsreaches a given threshold (10 or 20% incorrect ruleapplications).Ideally we might expect hat each disambigua-tion rule would be completely correct; an incorrectapplication would count as evidence that the ruleis wrong.
However, this is an inappropriate ide-AliT.ation, for several reasons.
Most crucially, thegr~,m~atical coverage as well as the range of lin-guistic processes modeled in Fidditch, are limited.
(Note that this is a property of any current orforeseeable syntactic analyzer.)
Since the gram-mar itself is not complete, the parser will havemisanalyzed some constructions, leading to incor-rect pattern matching.
Moreover, some linguisticpatterns that determine disambiguation (such asfor example, the influence of parallelism) cannotbe incorporated into the current rules at all, lead-ing to occasional failure.
As the overall syntacticmodel is improved, such cases will become less andless f~equent, but they will never disappear alto-gether.
Finally, there are of course errors in thetagged input.
Thus, we can't demand perfectionof the trained rules; rather, we require that rulesreach a certain level of success.
For rules thatdisambiguate he first element (except he defaultdisambiguation rules), we require 80% success; forthe other rules, 90% success.
These cutoff fig-ures were imposed arbitrarily; other values maybe more appropriate.An example of a rule that is acquired and thendeactivated is the following.
(4) \[ADJ+N+V\] = ADJ \[*lThis rule correctly disambiguates some cases likesound health and light barbell but fails on a suffi-cient proportion (such cases as sound energy andlight intens/ty) that it is permanently deactivated.Interleaving of grammar and disam-biguationOne of the advantages of embedding the trainingof disambiguation rules in a general parser is thatindependent parser actions can make the disam-biguation more effective.
For example, adverbs122often occur in an auxiliary phrase, as in the phrasehas immediately left The parser effectively ignoresthe adverb immediately so that from its point ofview, has and left are contiguous.
This in turnallows the disambignation rules to see that has isthe leR context for left and to categorize left asa past participle (rather than a past tense or anadjective or a noun).5 The TrainingThe training text was 450 of the 500 samples thatmake up the Brown Corpus, tagged with part ofspeech transformed into the 46 grammatical cate-gories native to Fidditch.
Ten percent of the cor-pus, selected from a variety of genres, was heldback for testing the acquired set of disambigua-tion rules.The tr~inlng set (consisting of about a millionwords) was parsed, beginning with the defaultrule set and acquiring disambiguation rules as de-scribed above.
After parsing the training set once,a certain set of disambignation rules had been ac-quired.
Then it was parsed over again, a total offive times.
Each time, the rule set is further re-fined.
It is effective to reparse the same corpus be-cause the acquisition depends both on the sentenceparsed and on the current set of rules.
Therefore,the same sentence can induce different changes inthe rule set depending on the current state of therule set.After the five iterations, 35000 rules have beenacquired.
For the training set, overall error rateis less than 2% and error rate for the ambiguouswords is less than 5%.
Clearly, the acquired ruleseffectively model the training set.
Because the rulepatterns are simple, they can be efficiently indexedand applied.For the one tenth of the corpus held back (thetest set), the performance of the trained set ofrules is encouraging.
Overall, the error rate for thetest set is about 3%.
For the ambiguous words theerror rate is 10%.
Compared to the performance ofthe existing hand-written rules, this shows almosta 50% reduction in the error rate.
Additionallyof course, there is a great saving in developmenttime; to cut the error rate of the original hand-written rules in half by further hand effort wouldrequire an enormous amount of work.
In contrast,this training algorithm is automatic (though it de-pends of course on the hand-written parser andset of grammar ules, and on the significant effortin tagging the Brown Corpus, which was used for123training).It is harder to compare performance directly toother reported disambiguation procedures, sincethe part of speech categories used are different.The 10% error rate on ambiguous words is thesame as that reported by Garside, Leech andSampson (1987:55).
The program developed byChurch (1987), which makes ystematic use of rel-ative tag probabilities, has, I believe, a somewhatsmaller overall error rate.Add ing  lex ica l  re la t ionsh ipsThe current parser models complementation rela-tions only partially and it has no model at all ofwhat word can modify what word (except at thelevel of lexical category).
Clearly, a more com-prehensive system would reflect the fact, for ex-ample, that public apathy is known to be a noun-noun compound, though the word public might bea noun or an adjective.
One piece of evidence ofthe importance of such relationships i  the factthat more than one fourth of the errors are confu-sions of adjective use with noun use as premodifierin a noun phrase.
The current parser has no accessto the kinds of information relevant to such modifi-cation and compound relationships, and thus doesnot do well on this distinction.The claim of this paper is that the linguisticinformation embodied in the parser is useful todisambiguation, and that enhancing the linguis-tic information will result in improving the disam-bignation.
Adding that information about lexicalrelations to the parser, and making it available tothe disambignation procedure, should improve theaccuracy of the disambiguation rules.
In the longrun the parser should incorporate general mod-els of modification.
However, we can crudely addsome of this information to the disambiguationprocedure, and take advantage of complementa-tion information.For each word in the training set, all word pairsincluding that word that might be lexically condi-tioned modification or complementation relation-ships are recorded.
Any pair that occurs morethan once and always has the same lexical cate-gory is taken to be a lexically significant colloca-tion - either a complementation r a modificationrelationship.
For example, for the word study thefollowing lexical pairs are identified in the trainingset.bD \] \[NOUN\]\[NI \[NI\[VPPRT\] IN\]\[PP-ZPI\[N\]\[vl\[M\[vl\[Pp.zP\]\[NI\[Pm~P\]recent study, present study,psychological study, graduate study,own study, such study,theoretical studyuse study, place-name study,growth study, time-&-motion study,birefringence studyprolonged study, detailed studyunder studystudy dancestudy atstudy of, study on,study byObviously, only a small subset of the modifica-tion and complementation relations of English areincluded in this set.
But m\[qsing pairs cause notrouble, since more general disambiguation ruleswill apply.
This is an instance of the general strat-egy of the parser to use specific information whenit is available and to fall back on more general(and less accurate) information i case no specificpattern matches, permitting an incremental im-provement of the parser.
The set of lexical pairsdoes include many high frequency collocations in-volving potentially ambiguous words, such as closetO (ADJ PREP) and long time (ADJ N).The test set was reparsed using this lexical infor-mation.
The error rate for dis~mhiguation usingto these lexically related word pairs is quite small(3.5% of the ambiguous words), much better thanthe error rate of the disambiguation rules in gen-eral, resulting in an improved overall performancein disambiguation.
Although this is only a crudemodel of complementation and modification rela-tionships, it suggests how improvements in othermodules of the parser will result in improvementsin the disamhiguation.Us ing  grammat ica l  dependencyA second source of failure of the acquired isam-biguation rules is that the acquisition algorithmis not paying enough attention to the informationthe parser provides.The large difference in accuracy between thetraining set and the test set suggests that the ac-quired set of disambiguation rules are matchingidiosyncratic properties of the training set ratherthan general extensible properties; the rules aretoo powerful.
It seems that the rules that refer toall three items in the buffer are the culprit.
Forexample, the acquired rule124(5) \[M\[P P+TNS\] = 'rNs \[ +vl = vapplies to such cases as(6) Shall we flip a coin to see which of us goesfirst?
-~In effect, this rule duplicates the action of anotherrule(7) \[PREP'~t'TNS\] ----" TNS \[N'~V\] "--VIn short, the rule set does not have appropriateshift invariance.The problem with disamhiguation rule (5) isthat it refers to three items that are not in factsyntactically related: in sentence (6), there is nostructural relation between the noun coin and theinfinitive phrase to see.
It would be appropriate toonly acquire rules that refer to constituents thatoccur in construction with each other, since thepredictability of part of speech from local contextarises because of stract,ral relations among words;there should be no predictabifity across words thatate not structurally related.We should therefore be able to improve the setof disamhiguation rules by restricting new rules toonly those involving elements hat are in the samestructure.
We use the grammar as implemented inthe parser to decide what elements are related andthus to restrict he set of rules acquired.
Specif-ically, the following restriction on the acquisitionof new rules is proposed.All the buffer elements referred to bya disambiguation rule must appear to-gether in some other single rule.This rules out examples like rule (5) because nosingle parser grammar rule ever refers to the noun,the to and the following verb at the same time.However, a rule llke (7) is accepted because theparser grammar rule for infinitives does refer to toand the following verb st the same time.For training, an additional escape for rules wasadded: if the first element of the buffer is ambigu-ous s rule may use the second element to disam-biguate it whether or not there is any parser ulethat refers to the two together.
In these cases, if nonew rule were added, the default disamhiguationrules, which are notably ineffective, would match.
(The default rules have a success rate of only 55%compared to over 94% for the disambiguation rulesthat depend on context.)
Since the parser is notsufficiently complete to recognize all cases wherewords are related, this escape admits some localcontext even in the absence of parser internal rea-sons to do so.The training procedure was applied with thisnew constraint on rules, parsing the training setfive times to acquire a new rule set.
Restrictingthe rules to related elements had three notable f-fects.
First, the number of disambiguation rulesacquired was cut to nearly one third the numberfor the unrestricted rule set (about 12000 rules).Second, the difference between the tr~inlng setand the test set is reduced; the error rate differsby only one percent.
Finally, the performance ofthe restricted rule set is if anything slightly betterthan the unrestricted set (3427 errors for the re-stricted rules versus 3492 errors for the larger ruleset).
These results show the power of using thegrammatical information encoded in the parser todirect the attention of the disambiguation rules.6 ConclusionI have described a training algorithm that usesan existing deterministic parser together with acorpus of tagged text to acquiring rules for dis-ambiguating lexical category.
Performance of thetrained set of rules is much better than the pre-vious hand-written rule set (error rate reduced byhalf).
The success of the disambiguation proce-dure depends on the linguistic knowledge mbod-ied in the parser in a number of ways.It uses the data structures and linguistic cat-egories of the parser, focusing the rule acqui-sition mechanism on relevant elements.It is embedded in the parsing process sothat parser actions can set things up foracquisition (for example, adverbs axe in ef-fect removed within elements of the auxil-iary, restoring the contiguity of auxiliary ele-ments).It uses the grammar rules to identify wordsthat are grammatically related, and are there-fore relevant to disambiguation.It can use rough models of complementationand modification to help identify words thatare related.Finally, the parser always provides a defaultaction.
This permits the incremental im-provement of the parser, since it can take ad-vantage of more specific information when itis available, but it will always disambiguatesomehow, no matter whether it has acquiredthe appropriate rules or not.This work demonstrates the feasibility of acquiringthe linguistic information needed to analyze unre-stricted text from text itself.
Further improve-ments in syntactic analyzers will depend on suchautomatic acquisition of grammatical and lexicalfacts.ReferencesBerwick, Robert C. 1985.
The Acquisition of Syn-tactic Knowledge.
M IT  Press.Church, Kenneth.
1987.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
Proceedings Second A CL Conference onApplied Natural Language Processing.DeRose, Stephen J.
1988.
Grammatical categorydisambiguation by statistical optimization.Computational Linguistics 14.1.31-39.Francis, W. Nelson and Henry Kucera.
1982.
Fre-fuency Analysis of English Usage.
HoughtonMifflin Co.Garside, Roger, Geoffrey Leech, and  GeoffreySampson.
1987.
The Computational Analysisof English.
Longman.Greene, Barbara B. and Gerald M. Rubin.
1971.Automated grammatical tagging of English.Department of Linguistics, Brown University.Donald Hindle.
1983.
Deterministic parsing of syn-tactic non-fluencies.
Proceedings of the 21stAnnual Meeting of the Association for Com-putational Linguistics.Klein, S. and R.F.
Simmons.
1963.
A computa-tional approach to grammatical coding of En-glish words.
JACM 10:334-47.Marcus, Mitchell P. 1980.
A Theory of SyntacticRecognition for Natural Language.
MIT Press.Marcus, Mitchell P., Donald Hindle and MargaretFleck.
1983.
D-theory: talking about talkingabout trees.
Proceedings of the ~lst AnnualMeeting of the Association for ComputationalLinguistics.Milne, Robert.
1986.
Resolving Lexical Ambigu-ity in a Deterministic Parser.
ComputationalLinguistics 12.1, 1-12.125
