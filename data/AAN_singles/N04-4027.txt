Summarizing Email ThreadsOwen Rambow Lokesh Shrestha John Chen Chirsty LauridsenColumbia University Columbia University Microsoft Research Asia Columbia UniversityNew York, NY, USA New York, NY, USA Beijing, China New York, NY, USArambow@cs.columbia.edu, lokesh@cs.columbia.eduv-johnc@msrchina.research.microsoft.com, christy@columbia.eduAbstractSummarizing threads of email is different fromsummarizing other types of written communi-cation as it has an inherent dialog structure.
Wepresent initial research which shows that sen-tence extraction techniques can work for emailthreads as well, but profit from email-specificfeatures.
In addition, the presentation of thesummary should take into account the dialogicstructure of email communication.1 IntroductionIn this paper, we discuss work on summarizing emailthreads, i.e., coherent exchanges of email messagesamong several participants.1 Email is a written mediumof asynchronous multi-party communication.
This meansthat, unlike for example news stories but as in face-to-facespoken dialog, the email thread as a whole is a collabo-rative effort with interaction among the discourse partici-pants.
However, unlike spoken dialog, the discourse par-ticipants are not physically co-present, so that the writ-ten word is the only channel of communication.
Fur-thermore, replies do not happen immediately, so that re-sponses need to take special precautions to identify rele-vant elements of the discourse context (for example, byciting previous messages).
Thus, email is a distinct lin-guistic genre that poses its own challenges to summariza-tion.In the approach we propose in this paper, we followthe paradigm used for other genres of summarization,namely sentence extraction: important sentences are ex-tracted from the thread and are composed into a summary.Given the special characteristics of email, we predict thatcertain email-specific features can help in identifying rel-evant sentences for extraction.
In addition, in presentingthe extracted summary, special ?wrappers?
ensure that1The work reported in this paper was funded under the KDDprogram.
We would like to thank three anonymous reviewersfor very insightful and helpful comments.the reader can reconstruct the interactional aspect of thethread, which we assume is crucial for understanding thesummary.
We acknowledge that other techniques shouldalso be explored for email summarization, but leave thatto separate work.2 Previous and Related WorkMuresan et al (2001) describe work on summarizing in-dividual email messages using machine learning ap-proaches to learn rules for salient noun phrase extraction.In contrast, our work aims at summarizing whole threadsand at capturing the interactive nature of email.Nenkova and Bagga (2003) present work on generat-ing extractive summaries of threads in archived discus-sions.
A sentence from the root message and from eachresponse to the root extracted using ad-hoc algorithmscrafted by hand.
This approach works best when the sub-ject of the root email best describes the ?issue?
of thethread, and when the root email does not discuss morethan one issue.
In our work, we do not make any assump-tions about the nature of the email, and learn sentenceextraction strategies using machine learning.Newman and Blitzer (2003) also address the problemof summarizing archived discussion lists.
They clustermessages into topic groups, and then extract summariesfor each cluster.
The summary of a cluster is extractedusing a scoring metric based on sentence position, lexicalsimilarity of a sentence to cluster centroid, and a featurebased on quotation, among others.
While the approach isquite different from ours (due to the underlying clusteringalgorithm and the absence of machine learning to selectfeatures), the use of email-specific features, in particularthe feature related to quoted material, is similar.Lam et al (2002) present work on email summariza-tion by exploiting the thread structure of email conver-sation and common features such as named entities anddates.
They summarize the message only, though the con-tent of the message to be summarized is ?expanded?
us-ing the content from its ancestor messages.
The expandedmessage is passed to a document summarizer which isused as a black box to generate summaries.
Our work, incontrast, aims at summarizing the whole thread, and weare precisely interested in changing the summarization al-gorithm itself, not in using a black box summarizer.In addition, there has been some work on summarizingmeetings.
As discussed in Section 1, email is differentin important respects from (multi-party) dialog.
How-ever, some important aspects are related.
Zechner (2002),for example, presents a meeting summarization systemwhich uses the MMR algorithm to find sentences that aremost similar to the segment and most dissimilar to eachother.
The similarity weights in the MMR algorithm aremodified using three features, including whether a sen-tence belongs to a question-answer pair.
The use of thequestion-answer pair detection is an interesting proposalthat is also applicable to our work.
However, overall mostof the issues tackled by Zechner (2002) are not relevantto email summarization.3 The DataOur corpus consists of 96 threads of email sent duringone academic year among the members of the board ofthe student organization of the ACM at Columbia Uni-versity.
The emails dealt mainly with planning events ofvarious types, though other issues were also addressed.On average, each thread contained 3.25 email messages,with all threads containing at least two messages, and thelongest thread containing 18 messages.Two annotators each wrote a summary of the thread.We did not provide instructions about how to choose con-tent for the summaries, but we did instruct the annotatorson the format of the summary; specifically, we requestedthem to use the past tense, and to use speech-act verbs andembedded clauses (for example, Dolores reported she?dgotten 7 people to sign up instead of Dolores got 7 peo-ple to sign up).
We requested the length to be about 5%to 20% of the original text length, but not longer than 100lines.Writing summaries is not a task that competent nativespeakers are necessarily good at without specific train-ing.
Furthermore, there may be many different possiblesummary types that address different needs, and differ-ent summaries may satisfy a particular need.
Thus, whenasking native speakers to write thread summaries we can-not expect to obtain summaries that are similar.We then used the hand-written summaries to identifyimportant sentences in the threads in the following man-ner.
We used the sentence-similarity finder SimFinder(Hatzivassiloglou et al, 2001) in order to rate the sim-ilarity of each sentence in a thread to each sentence inthe corresponding summary.
SimFinder uses a combi-nation of lexical and linguistic features to assign a sim-ilarity score to an input pair of texts.
We excluded sen-tences that are being quoted, as well as signatures andthe like.
For each sentence in the thread, we retainedthe highest similarity score.
We then chose a threshold;sentences with SimFinder scores above this threshold arethen marked as ?Y?, indicating that they should be partof a summary, while the remaining sentences are marked?N?.
About 26% of sentences are marked ?Y?.
All sen-tences from the email threads along with their classifica-tion constitutes our data.
For annotator DB, we have 1338sentences, of which 349 are marked ?Y?, for GR (whohas annotated a subset of the threads that DB has anno-tated) there are 1296 sentences, of which 336 are marked?Y?.
Only 193 sentences are marked ?Y?
using the sum-maries of both annotators, reflecting the difference in thesummaries written by the two annotators.
The kappa forthe marking of the sentences is 0.29 (recall that this onlyindirectly reflects annotator choice).
Thus, our expecta-tion that human-written summaries will show great vari-ation was borne out; we discuss these differences furtherin Section 5.4 Features for Sentence ExtractionWe start out with features that are not specific to email.These features consider the thread as a single text.
Wecall this feature set basic.
Each sentence in the emailthread is represented by a feature vector.
We shall callthe sentence in consideration s, the message in which thesentence appears m, the thread in which the sentence ap-pears t, and the entire corpus c. (We omit some featureswe use for lack of space.)?
thread line num: The absolute position of s in t.?
centroid sim: Cosine similarity of s?s TF-IDF vec-tor (excluding stop words) with t?s centroid vector.The centroid vector is the average of the TF-IDFvectors of all the sentences in t. The IDF compo-nent is derived from the ACM Corpus.?
centroid sim local: Same as centroid sim exceptthat the inverse document frequencies are derivedfrom the thread.?
length: The number of content terms in s.?
tfidfsum: Sum of the TF-IDF weights of contentterms in s. IDF weights are derived from c.?
tfidfavg: Average TF-IDF weight of the contentterms in s. IDF weights are derived from c.?
t rel pos: Relative position of s in t: the number ofsentences preceding s divided by the total number ofsentences in t. All messages in a thread are orderedlinearly by the time they were sent.?
is Question: Whether s is a question, as determinedby punctuation.Ann.
Feature set ctroid basic basic+ fullDB Recall 0.255 0.315 0.370 0.421DB Precision 0.298 0.553 0.584 0.607DB F-measure 0.272 0.401 0.453 0.497GR Recall 0.291 0.217 0.193 0.280GR Precision 0.333 0.378 0.385 0.475GR F-measure 0.311 0.276 0.257 0.352Figure 1: Results for annotators DB and GR using differ-ent feature setsWe now add two features that take into account the di-vision of the thread into messages and the resulting dia-log structure.
The union of this feature set with basic iscalled basic+.?
msg num: The ordinality of m in t (i.e., the absoluteposition of m in t).?
m rel pos: Relative position of s in m: the numberof sentences preceding s divided by the total numberof sentences in m.Finally, we add features which address the specificstructure of email communication.
The full feature setis called full.?
subject sim: Overlap of the content words of thesubject of the first message in t with the contentwords in s.?
num of res: Number of direct responses to m.?
num Of Recipients: Number of recipients of m.?
fol Quote: Whether s follows a quoted portion in m.5 Experiments and ResultsThis section describes experiments using the machinelearning program Ripper (Cohen, 1996) to automaticallyinduce sentence classifiers, using the features describedin Section 4.
Like many learning programs, Ripper takesas input the classes to be learned, a set of feature namesand possible values, and training data specifying the classand feature values for each training example.
In our case,the training examples are the sentences from the threadsas described in Section 3.
Ripper outputs a classifica-tion model for predicting the class (i.e., whether a sen-tence should be in a summary or not) of future exam-ples; the model is expressed as an ordered set of if-thenrules.
We obtained the results presented here using five-fold cross-validation.
In this paper, we only evaluate theresults of the machine learning step; we acknowledge theneed for an evaluation of the resulting summaries usingDB only GR only avg maxRecall 0.421 0.280 0.212 0.268Precision 0.607 0.475 0.406 0.444F-measure 0.497 0.352 0.278 0.335Figure 2: Results for combining two annotators (last twocolumns) using full feature setword/string based similarity metric and/or human judg-ments and leave that to future publications.We show results for the two annotators and differentfeature sets in Figure 1.
First consider the results forannotator DB.
Recall that basic includes only standardfeatures that can be used for all text genres, and consid-ers the thread a single text.
basic+ takes the breakdownof the thread into messages into account.
full also usesfeatures that are specific to email threads.
We can seethat by using more features than the baseline set basic,performance improves.
Specifically, using email-specificfeatures improves the performance over the basic base-line, as we expected.
We also give a second baseline,ctroid, which we determined by choosing the top 20% ofsentences most similar to the thread centroid.
All resultsusing Ripper improve on this baseline.If we perform exactly the same experiments on thesummaries written by annotator GR, we obtain the re-sults shown in the bottom half of Figure 1.
The results aremuch worse, and the centroid-based baseline outperformsall but the full feature set.
We leave to further researchan explanation of why this may be the case; we speculatethat GR,as an annotator, is less consistent in her choice ofmaterial than is DB when forming a summary.
Thus, themachine learner has less regularity to learn from.
How-ever, we take this difference as evidence for the claim thatone should not expect great regularity in human-writtensummaries.Finally, we investigated what happens when we com-bine the data from both sources, DB and GR.
UsingSimFinder, we obtained two scores for each sentence, onethat shows the similarity to the most similar sentence inDB?s summary, and one that shows the similarity to themost similar sentence in GR?s summary.
We can com-bine these two scores and then use the combined score inthe same way that we used the score from a single anno-tator.
We explore two ways of combining the scores: theaverage, and the maximum.
Both ways of combining thescores result in worse scores than either annotator on hisor her own; the average is worse than the maximum (seeFigure 2).
We interpret these results again as meaningthat there is little convergence in the human-written sum-maries, and it may be advantageous to learn from oneparticular annotator.
(Of course, another option might beto develop and enforce very precise guidelines for the an-1 IF centroid sim local ?
0.32215 AND thread line num ?
4 AND isQuestion = 1AND tfidfavg ?
0.212141 AND tfidfavg ?
0.301707 THEN Y.2 IF centroid sim ?
0.719594 AND numOfRecipients ?
8 THEN Y.3 IF centroid sim local ?
0.308202 AND thread line num ?
4 AND tfidfmax ?
0.607829AND m rel pos ?
0.363636 AND t rel pos ?
0.181818 THEN Y.4 IF subject sim ?
0.333333 tfidfsum ?
2.83636 tfidfsum ?
2.64262 tfidfmax ?
0.675917 THEN Y.5 ELSE N.Figure 3: Sample rule set generated from DB data (simplified for reasons of space)Regarding ?acm home/bjarney?, on Apr 9, 2001, MurielDanslop wrote: Two things: Can someone be responsiblefor the press releases for Stroustrup?Responding to this on Apr 10, 2001, Theresa Feng wrote:I think Phil, who is probably a better writer than most ofus, is writing up something for dang and Dave to send outto various ACM chapters.
Phil, we can just use that as our?press release?, right?In another subthread, on Apr 12, 2001, Kevin Danquoitwrote: Are you sending out upcoming events for thisweek?Figure 4: Sample summary obtained with the rule set inFigure 3notators as to the contents of the summaries.
)A sample rule set obtained from DB data is shown inFigure 3.
Some rules are intuitively appealing: for ex-ample, rule 1 states that questions at the beginning of athread that are similar to entire thread should be retained,and rule 2 states that sentence which are very similar tothe thread and which have a high number of recipientsshould be retained.
However, some rules show signs ofoverfitting, for example rule 1 limits the average TF-IDFvalues to a rather narrow band.
Hopefully, more datawill alleviate the overfitting problem.
(The data collec-tion continues.
)6 Postprocessing Extracted SentencesExtracted sentences are sent to a module that wraps thesesentences with the names of the senders, the dates atwhich they were sent, and a speech act verb.
The speechact verb is chosen as a function of the structure of theemail thread in order to make this structure more appar-ent to the reader.
Further, for readability, the sentencesare sorted by the order in which they were sent.
An ex-ample can be seen in Figure 4.
Note that while the initialquestion is answered in the following sentence, two otherquestions are left unanswered in this summary (the an-swers are in fact in the thread).7 Future WorkIn future work, we will perform a qualitative error anal-ysis and investigate in more detail what characteristicsof DB?s summaries lead to better extractive summaries.We can use this insight to instruct human annotators, andto improve the automatic extraction.
We intend to learnpredictors for some other thread aspects such as threadcategory and question-answer pairs, and then use theseas input to the sentence extraction procedure.
For ex-ample, identifying question-answer pairs appears to beimportant for generating ?complete?
summaries, as illus-trated by the sample summary.
We also intend to performan evaluation based on human feedback.ReferencesWilliam Cohen.
1996.
Learning trees and rules withset-valued features.
In Fourteenth Conference of theAmerican Association of Artificial Intelligence.
AAAI.Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-combe, Regina Barzilay, Min-Yen Kan, and KathleenMcKeown.
2001.
SimFinder: A flexible cluster-ing tool for summarization.
In Proceedings of theNAACL Workshop on Automatic Summarization, Pitts-burgh, PA.Derek Lam, Steven L. Rohall, Chris Schmandt, andMia K. Stern.
2002.
Exploiting e-mail structure toimprove summarization.
In ACM 2002 Conference onComputer Supported Cooperative Work (CSCW2002),Interactive Posters, New Orleans, LA.Smaranda Muresan, Evelyne Tzoukermann, and JudithKlavans.
2001.
Combining Linguistic and Ma-chine Learning Techniques for Email Summarization.In Proceedings of the CoNLL 2001 Workshop at theACL/EACL 2001 Conference.Ani Nenkova and Amit Bagga.
2003.
Facilitating emailthread access by extractive summary generation.
InProceedings of RANLP, Bulgaria.Paula Newman and John Blitzer.
2003.
Summarizingarchived discussions: a beginning.
In Proceedings ofIntelligent User Interfaces.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Computational Linguistics, 28(4):447?485.
