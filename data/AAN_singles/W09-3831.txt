Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 206?209,Paris, October 2009. c?2009 Association for Computational LinguisticsUsing a maximum entropy-based tagger to improve a very fast vine parserAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkJonas KuhnDpt.
of LinguisticsUniversity of Potsdamkuhn@ling.uni-potsdam.deAbstractIn this short paper, an off-the-shelf maxi-mum entropy-based POS-tagger is used asa partial parser to improve the accuracy ofan extremely fast linear time dependencyparser that provides state-of-the-art resultsin multilingual unlabeled POS sequenceparsing.1 IntroductionThe dependency parsing literature has grown in alldirections the past 10 years or so.
Dependencyparsing is used in a wide variety of applications,and many different parsing techniques have beenproposed.Two dependency parsers have become morepopular than the rest, namely MSTParser (Mc-Donald et al, 2005) and MaltParser (Nivre etal., 2007).
MSTParser is slightly more accu-rate than MaltParser on most languages, especiallywhen dependencies are long and non-projective,but MaltParser is theoretically more efficient as itruns in linear time.
Both are relatively slow interms of training (hours, sometimes days), and rel-atively big models are queried in parsing.MSTParser and MaltParser can be optimized forspeed in various ways,1 but the many applicationsof dependency parsers today may turn model sizeinto a serious problem.
MSTParser typically takesabout a minute to parse a small standard test suite,say 2?300 sentences; the stand-alone version ofMaltParser may take 5?8 minutes.
Such parsingtimes are problematic in, say, a machine transla-tion system where for each sentence pair multiple1Recent work has optimized MaltParser considerably forspeed.
Goldberg and Elhadad (2008) speed up the MaltParserby a factor of 30 by simplifying the decision function for theclassifiers.
Parsing is still considerably slower than with ourvine parser, i.e.
a test suite is parsed in about 15?20 seconds,whereas our vine parser parses a test suite in less than twoseconds.target sentences are parsed (Charniak et al, 2003;Galley and Manning, 2009).
Since training takeshours or days, researchers are also more reluctantto experiment with new features, and it is verylikely that the features typically used in parsingare suboptimal in, say, machine translation.Conceptually simpler dependency parsers arealso easier to understand, which makes debugging,cross-domain adaption or cross-language adapta-tion a lot easier.
Finally, state-of-the-art depen-dency parsers may in fact be outperformed by sim-pler systems on non-standard test languages with,say, richer morphology or more flexible word or-der.Vine parsing is a parsing strategy that guaran-tees fast parsing and smaller models, but the ac-curacy of dependency-based vine parsers has beennon-competitive (Eisner and Smith, 2005; Dreyeret al, 2006).This paper shows how the accuracy ofdependency-based vine parsers can be improvedby 1?5% across six very different languages witha very small cost in training time and practicallyno cost in parsing time.The main idea in our experiments is to usea maximum entropy-based part-of-speech (POS)tagger to identify roots and tokens whose headsare immediately left or right of them.
These aretasks that a tagger can solve.
You simply readoff a tagged text from the training, resp.
test, sec-tion of a treebank and replace all tags of roots,i.e.
tokens whose syntactic head is an artificial rootnode, with a new tag ROOT.
You then train onthe training section and apply your tagger on thetest section.
The decisions made by the taggerare then, subsequently, used as hard constraints byyour parser.
When the parser then tries to find rootnodes, for instance, it is forced to use the roots as-signed by the tagger.
This strategy is meaningfulif the tagger has better precision for roots than theparser.
If it has better recall than the parser, the206parser may be forced to select roots only from theset of potential roots assigned by the tagger.
In ourexperiments, only the first strategy was used (sincethe tagger?s precision was typically better than itsrecall).The dependency parser used in our experimentsis very simple.
It is based on the Chu-Liu-Edmonds algorithm (Edmonds, 1967), which isalso used in the MSTParser (McDonald et al,2005), but it is informed only by a simple MLEtraining procedure and omits cycle contraction inparsing.
This means that it produces cyclic graphs.In the context of poor training, insisting on acyclicoutput graphs often compromises accuracy by >10%.
On top of this parser, which is super fast butoften does not even outperform a simple structuralbaseline, hard and soft constraints on dependencylength are learned discriminatively.
The speed ofthe parser allows us to repeatedly parse a tuningsection to optimize these constraints.
In particular,the tuning section (about 7500 tokens) is parseda fixed number of times for each POS/CPOS tagto find the optimal dependency length constraintwhen that tag is the tag of the head or dependentword.
In general, this discriminative training pro-cedure takes about 10 minutes for an average-sizedtreebank.
The parser only produces unlabeled de-pendency graphs and is still under development.While accuracy is below state-of-the-art results,our improved parser significantly outperforms adefault version of the MaltParser that is restrictedto POS tags only, on 5/6 languages (p ?
0.05),and it significantly outperforms the baseline vineparser on all languages.2 DataOur languages are chosen from different languagefamilies.
Arabic is a Semitic language, Czech isSlavic, Dutch is Germanic, Italian is Romance,Japanese is Japonic-Ryukyuan, and Turkish isUralic.
All treebanks, except Italian, were alsoused in the CONLL-X Shared Task (Buchholz andMarsi, 2006).
The Italian treebank is the lawsection of the TUT Treebank used in the Evalita2007 Dependency Parsing Challenge (Bosco et al,2000).3 ExperimentsThe Python/C++ implementation of the maximumentropy-based part-of-speech (POS) tagger firstdescribed in Ratnaparkhi (1998) that comes withthe maximum entropy library in Zhang (2004) wasused to identify arcs to the root node and to tokensimmediately left or right of the dependent.
Thiswas done by first extracting a tagged text fromeach treebank with dependents of the root node as-signed a special tag ROOT.
Similarly, tagged textswere extracted in which dependents of their im-mediate left, resp.
right neighbors, were assigned aspecial tag.
Our tagger was trained on the texts ex-tracted from the training sections of the treebanksand evaluated on the texts extracted from the testsections.
The number of gold standard, resp.
pre-dicted, ROOT/LEFT/RIGHT tags are presented inFigure 1.
Precision and f-score are also computed.Note that since our parser uses information fromour tagger as hard constraints, i.e.
it disregardsarcs to the root node or immediate neighbors notpredicted by our tagger, precision is really whatis important, not f-score.
Or more precisely, preci-sion indicates if our tagger is of any help to us, andf-score tells us to what extent it may be of help.4 ResultsThe results in Figure 2 show that using a maxi-mum entropy-based POS tagger to identify roots(ROOT), tokens with immediate left heads (LEFT)and tokens with immediate (RIGHT) heads im-proves the accuracy of a baseline vine parseracross the board for all languages measured interms of unlabeled attachment score (ULA), or de-creases are insignificant (Czech and Turkish).
Forall six languages, there is a combination of ROOT,LEFT and RIGHT that significantly outperformsthe vine parser baseline.
In 4/6 cases, absolute im-provements are ?
2%.
The score for Dutch is im-proved by > 4%.
The extended vine parser is alsosignificantly better than the MaltParser restrictedto POS tags on 5/6 languages.
MaltParser is prob-ably better than the vine parser wrt.
Japanese be-cause average sentence length in this treebank isvery short (8.9); constraints on dependency lengthdo not really limit the search space.In spite of the fact that our parser only uses POStags (except for the maximum entropy-based tag-ger which considers both words and tags), scoresare now comparable to more mature dependencyparsers: ULA excl.
punctuation for Arabic is70.74 for Vine+ROOT+LEFT+RIGHT which isbetter than six of the systems who participated inthe CONLL-X Shared Task and who had access toall data in the treebank, i.e.
tokens, lemmas, POS207Arabic Gold Predicted Precision F-scoreROOT 443 394 89.09 83.87LEFT 3035 3180 84.28 86.24RIGHT 313 196 82.14 63.26Czech Gold Predicted Precision F-scoreROOT 737 649 85.36 79.94LEFT 1485 1384 85.12 82.12RIGHT 1288 1177 87.51 83.57Dutch Gold Predicted Precision F-scoreROOT 522 360 74.44 60.77LEFT 1734 1595 87.02 83.39RIGHT 1300 1200 87.00 83.52Italian Gold Predicted Precision F-scoreROOT 100 58 74.36 65.17LEFT 1601 1640 90.30 91.39RIGHT 192 129 84.87 74.14Japanese Gold Predicted Precision F-scoreROOT 939 984 85.06 87.05LEFT 1398 1382 97.76 97.19RIGHT 2838 3016 92.27 95.08Turkish Gold Predicted Precision F-scoreROOT 694 685 85.55 84.99LEFT 750 699 91.70 88.47RIGHT 3433 3416 84.19 83.98Figure 1: Tag-specific evaluation of our tagger on the extracted texts.Arabic Czech Dutch Italian Japanese TurkishMaltParser 66.22 67.78 65.03 75.48 89.13 68.94Vine 67.99 66.70 65.98 75.50 83.15 68.53Vine+ROOT 68.68 66.65 66.21 78.06 83.82 68.45Vine+ROOT+LEFT 69.68 68.14 68.05 77.14 84.64 68.37Vine+RIGHT 68.50 67.38 68.18 78.55 84.17 69.87Vine+ROOT+RIGHT 69.20 67.32 68.40 78.29 84.78 69.79Vine+ROOT+LEFT+RIGHT 70.28 68.70 70.06 77.26 85.45 69.74Figure 2: Labeled attachment scores (LASs) for MaltParser limited to POS tags, our baseline vine parser(Vine) and our extensions of Vine.
Best scores bold-faced.208tags, features and dependency relations; not justthe POS tags as in our case.
In particular, our re-sult is 2.28 better than Dreyer et al (2006) whoalso use soft and hard constraints on dependencylengths.
They extend the parsing algorithm in Eis-ner and Smith (2005) to labeled k-best parsing anduse a reranker to find the best parse according topredefined global features.
ULA excl.
punctuationfor Turkish is 67.06 which is better than six of theshared task participants, incl.
Dreyer et al (2006)(60.45).The improvements come at an extremely lowcost.
The POS tagger simply stores its decisionsin a very small table, typically 5?10 cells per sen-tence, that is queried in no time in parsing.
Pars-ing a standard small test suite takes less than twoseconds, and the cost of the additional look-up istoo small to be measured.
The training time of themaximum entropy-based tagger is typically a mat-ter of seconds or half a minute.
Even running it onthe 1249k Prague Dependency Treebank (Czech)is only a matter of minutes.5 Conclusion and future workVine parsers are motivated by efficiency and ro-bustness (Dreyer et al, 2006), which has becomemore and more important over the last few years,but none of the systems introduced in the liter-ature provide competitive results in terms of ac-curacy.
Our experiments show how dependency-based vine parsers can be significantly improvedby using a maximum entropy-based POS taggerfor initial partial parsing with almost no cost interms of training and parsing time.Our choice of parser restricted us in a few re-spects.
Most importantly, our results are belowstate-of-the-art results, and it is not clear if thestrategy scales to more accurate parsers.
The strat-egy of using a POS tagger to do partial parsing andsubsequently forward high precision decisions toa parser only works on graph-based or constraint-based dependency parsers where previous deci-sions can be hardwired into candidate weight ma-trices by setting weights to 0.
It would be difficultif at all possible to implement in history-based de-pendency parsers such as MaltParser.
Experimentswill be performed with the MSTParser soon.Our parser also restricted us to considering un-labeled dependency graphs.
A POS tagger, how-ever, can also be used to identify grammaticalfunctions (subjects, objects, .
.
.
), for example,which may be used to hardwire dependency rela-tions into candidate weight matrices.
POS taggersmay also be used to identify other dependency re-lations or more fine-grained features that can im-prove the accuracy of dependency parsers.ReferencesCristina Bosco, Vincenzo Lombardo, Daniela Vassallo,and Leonardo Lesmo.
2000.
Building a treebank forItalian.
In LREC, pages 99?105, Athens, Greece.Sabine Buchholz and Erwin Marsi.
2006.
CONLL-Xshared task on multilingual dependency parsing.
InCONLL-X, pages 149?164, New York City, NY.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In MT Summit IX, New Or-leans, Louisiana.Markus Dreyer, David A. Smith, and Noah A. Smith.2006.
Vine parsing and minimum risk reranking forspeed and precision.
In CONLL-X, pages 201?205,New York City, NY.J.
Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71:233?240.Jason Eisner and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InIWPT?05, pages 30?41, Vancouver, Canada.Michel Galley and Cristopher Manning.
2009.Quadratic time dependency parsing for machinetranslation.
In ACL?09, Singapore, Singapore.
Toappear.Yoav Goldberg and Michael Elhadad.
2008.splitSVM: fast, space-efficient, non-heuristic, poly-nomial kernel computation for NLP applications.
InACL?08, Short Papers, pages 237?240, Columbus,Ohio.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In HLT-EMNLP2005, pages 523?530, Vancouver, British Columbia.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CONLL 2007 shared task ondependency parsing.
In EMNLP-CONLL?07, pages915?932, Prague, Czech Republic.Adwait Ratnaparkhi.
1998.
Maximum entropy mod-els for natural language ambiguity resolution.
Ph.D.thesis, University of Pennsylvania.Le Zhang.
2004.
Maximum entropy mod-eling toolkit for Python and C++.
Uni-versity of Edinburgh.
Available at home-pages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.209
