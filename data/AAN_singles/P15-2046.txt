Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 279?284,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Lexicalized Tree Kernel for Open Information ExtractionYing Xu?, Christoph Ringlstetter?, Mi-Young Kim?, Randy Goebel?,Grzegorz Kondrak?, Yusuke Miyao?
?Department of Computing Science, University of Alberta?Gini, Muenchen?National Institute of Informatics / JST, PRESTO?
{yx2,miyoung2,rgoebel,gkondrak}@ualberta.ca?c.ringlstetter@gini.net?yusuke@nii.ac.jpAbstractIn contrast with traditional relation ex-traction, which only considers a fixed setof relations, Open Information Extraction(Open IE) aims at extracting all typesof relations from text.
Because of datasparseness, Open IE systems typically ig-nore lexical information, and instead em-ploy parse trees and Part-of-Speech (POS)tags.
However, the same syntactic struc-ture may correspond to different relations.In this paper, we propose to use a lexical-ized tree kernel based on the word embed-dings created by a neural network model.We show that the lexicalized tree kernelmodel surpasses the unlexicalized model.Experiments on three datasets indicate thatour Open IE system performs better on thetask of relation extraction than the state-of-the-art Open IE systems of Xu et al(2013) and Mesquita et al (2013).1 IntroductionRelation Extraction (RE) is the task of recognizingrelationships between entities mentioned in text.In contrast with traditional relation extraction, forwhich a target set of relations is fixed a priori,Open Information Extraction (Open IE) is a gen-eralization of RE that attempts to extract all re-lations (Banko et al, 2007).
Although Open IEmodels that extract N-ary relations have been pro-posed, here we concentrate on binary relations.Most Open IE systems employ syntactic infor-mation such as parse trees and part of speech(POS) tags, but ignore lexical information.
How-ever, previous work suggests that Open IE wouldbenefit from lexical information because the samesyntactic structure may correspond to different re-lations.
For instance, the relation <Annacone,coach of, Federer> is correct for the sentence?Federer hired Annacone as a coach?, but not forthe sentence ?Federer considered Annacone as acoach,?
even though they have the same depen-dency path structure (Mausam et al, 2012).
Lex-ical information is required to distinguish the twocases.Here we propose a lexicalized tree kernel modelthat combines both syntactic and lexical informa-tion.
In order to avoid lexical sparsity issues, weinvestigate two smoothing methods that use wordvector representations: Brown clustering (Brownet al, 1992) and word embeddings created bya neural network model (Collobert and Weston,2008).
To our knowledge, we are the first to ap-ply word embeddings and to use lexicalized treekernel models for Open IE.Experiments on three datasets demonstrate thatour Open IE system achieves absolute improve-ments in F-measure of up to 16% over the cur-rent state-of-the-art systems of Xu et al (2013)and Mesquita et al (2013).
In addition, we ex-amine alternative approaches for including lexicalinformation, and find that excluding named enti-ties from the lexical information results in an im-proved F-score.2 System ArchitectureThe goal of the Open IE task is to extract fromtext a set of triples {< E1, R,E2>}, where E1and E2are two named entities, and R is a textualfragment that indicates the semantic relation be-tween the two entities.
We concentrate on binary,single-word relations between named entities.
Thecandidate relation words are extracted from depen-dency structures, and then filtered by a supervisedtree kernel model.Our system consists of three modules: entityextraction, relation candidate extraction, and tree279Figure 1: Our Open IE system structure.kernel filtering.
The system structure is outlinedin Figure 1.
We identify named entities, parse sen-tences, and convert constituency trees into depen-dency structures using the Stanford tools (Man-ning et al, 2014).
Entities within a fixed token dis-tance (set to 20 according to development results)are extracted as pairs {< E1, E2>}.
We thenidentify relation candidates R for each entity pairin a sentence, using dependency paths.
Finally,the candidate triples {< E1, R,E2>} are pairedwith their corresponding tree structures, and pro-vided as input to the SVM tree kernel.
Our OpenIE system outputs the triples that are classified aspositive.
In the following sections, we describe thecomponents of the system in more detail.3 Relation CandidatesRelation candidates are words that may repre-sent a relation between two entities.
We consideronly lemmatized nouns, verbs and adjectives thatare within two dependency links from either ofthe entities.
Following Wu and Weld (2010) andMausam et al (2012), we use dependency pat-terns rather than POS patterns, which allows us toidentify relation candidates which are farther awayfrom entities in terms of token distance.We extract the first two content words along thedependency path between E1and E2.
In the fol-lowing example, the path is E1?
encounter ?build ?
E2, and the two relation word candidatesbetween ?Mr.
Wathen?
and ?Plant Security Ser-vice?
are encounter and build, of which the latteris the correct one.If there are no content words on the dependencypath between the two entities, we instead considerwords that are directly linked to either of them.In the following example, the only relation candi-date is the word battle, which is directly linked to?Edelman.
?The relation candidates are manually annotatedas correct/incorrect in the training data for the treekernel models described in the following section.4 Lexicalized Tree KernelWe use a supervised lexicalized tree kernel to filternegative relation candidates from the results of thecandidate extraction module.
For semantic tasks,the design of input structures to tree kernels is asimportant as the design of the tree kernels them-selves.
In this section, we introduce our tree struc-ture, describe the prior basic tree kernel, and fi-nally present our lexicalized tree kernel function.4.1 Tree StructureIn order to formulate the input for tree kernelmodels, we need to convert the dependency pathto a tree-like structure with unlabelled edges.The target dependency path is the shortest paththat includes the triple and other content wordsalong the path.
Consider the following example,which is a simplified representation of the sen-tence ?Georgia-Pacific Corp.?s unsolicited $3.9billion bid for Great Northern Nekoosa Corp. washailed by Wall Street.?
The candidate triple iden-tified by the relation candidate extraction moduleis <Georgia-Pacific Corp., bid, Great NorthernNekoosa Corp.>.Our unlexicalized tree representation model issimilar to the unlexicalized representations of Xuet al (2013), except that instead of using the POStag of the path?s head word as the root, we cre-ate an abstract Root node.
We preserve the depen-dency labels, POS tags, and entity information astree nodes: (a) the top dependency labels are in-280(a) An un-lexicalized dependency tree.
(b) A lexicalized dependency tree.Figure 2: An unlexicalized tree and the corresponding lexicalized tree.cluded as children of the abstract Root node, otherlabels are attached to the corresponding parent la-bels; (b) the POS tag of the head word of the de-pendence path is a child of the Root; (c) other POStags are attached as children of the dependency la-bels; and (d) the relation tag ?R?
and the entity tags?NE?
are the terminal nodes attached to their re-spective POS tags.
Figure 2(a) shows the unlexi-calized dependency tree for our example sentence.Our lexicalized tree representation is derivedfrom the unlexicalized representation by attachingwords as terminal nodes.
In order to reduce thenumber of nodes, we collapse the relation and en-tity tags with their corresponding POS tags.
Fig-ure 2(b) shows the resulting tree for the examplesentence.4.2 Tree KernelsTree kernel models extract features from parsetrees by comparing pairs of tree structures.
Theessential distinction between different tree kernelfunctions is the ?
function that calculates simi-larity of subtrees.
Our modified kernel is based onthe SubSet Tree (SST) Kernel proposed by Collinsand Duffy (2002).
What follows is a simplified de-scription of the kernel; a more detailed descriptioncan be found in the original paper.The general function for a tree kernel modelover trees T1and T2is:K(T1, T2) =?n1?T1?n2?T2?
(n1, n2), (1)where n1and n2are tree nodes.
The ?
functionof SST kernel is defined recursively:1.
?
(n1, n2) = 0 if the productions (context-free rules) of n1and n2are different.2.
Otherwise, ?
(n1, n2) = 1 if n1and n2arematching pre-terminals (POS tags).3.
Otherwise,?
(n1, n2) =?j(1 + ?
(c(n1, j), c(n2, j)),where c(n, j) is the jth child of n.4.3 Lexicalized Tree KernelSince simply adding words to lexicalize a tree ker-nel leads to sparsity problems, a type of smoothingmust be applied.
Bloehdorn and Moschitti (2007)measure the similarity of words using WordNet.Croce et al (2011) employ word vectors createdby Singular Value Decomposition (Golub and Ka-han., 1965) from a word co-occurrence matrix.Plank and Moschitti (2013) use word vectors cre-ated by Brown clustering algorithm (Brown et al,1992), which is another smoothed word represen-tation that represents words as binary vectors.
Sri-vastava et al (2013) use word embeddings of Col-lobert and Weston (2008), but their tree kerneldoes not incorporate POS tags or dependency la-bels.We propose using word embeddings createdby a neural network model (Collobert and We-ston, 2008), in which words are represented byn-dimensional real valued vectors.
Each dimen-sion represents a latent feature of the word that re-flects its semantic and syntactic properties.
Next,we describe how we embed these vectors into treekernels.Our lexicalized tree kernel model is the same asSST, except in the following case: if n1and n2arematching pre-terminals (POS tags), then?
(n1, n2) = 1 +G(c(n1), c(n2)), (2)where c(n) denotes the word w that is the uniquechild of n, andG(w1, w2) = exp(??
?w1?w2?2)is a Gaussian function for two word vectors, whichis a valid kernel.We examine the contribution of different typesof words by comparing three methods of includinglexical information: (1) relation words only; (2) allwords (relation words, named entities, and otherwords along the dependency path fragment); and(3) all words, except named entities.
The wordsthat are excluded are assumed to be different; forexample, in the third method,G(E1, E2) is alwayszero, even if the entities, E1and E2, are the same.2815 ExperimentsHere we evaluate alternative tree kernel configura-tions, and compare our Open IE system to previ-ous work.We perform experiments on three datasets (Ta-ble 1): the Penn Treebank set (Xu et al, 2013),the New York Times set (Mesquita et al, 2013),and the ClueWeb set which we created for thisproject from a large collection of web pages.1Themodels are trained on the Penn Treebank trainingset and tested on the three test sets, of which thePenn Treebank set is in-domain, and the other twosets are out-of-domain.
For word embedding andBrown clustering representations, we use the dataprovided by Turian et al (2010).
The SVM param-eters, as well as the Brown cluster size and codelength, are tuned on the development set.Set train dev testPenn Treebank 750 100 100New York Times ?
300 500ClueWeb ?
450 250Table 1: Data sets and their size (number of sen-tences).Table 2 shows the effect of different smooth-ing and lexicalization techniques on the tree ker-nels.
In order to focus on tree kernel functions,we use the relation candidate extraction (Section3) and tree structure (Section 4.1) proposed inthis paper.
The results in the first two rows indi-cate that adding unsmoothed lexical informationto the method of Xu et al (2013) is not help-ful, which we attribute to data sparsity.
On theother hand, smoothed word representations do im-prove F-measure.
Surprisingly, a neural networkapproach of creating word embeddings actuallyachieves a lower recall than the method of Plankand Moschitti (2013) that uses Brown clustering;the difference in F-measure is not statistically sig-nificant according to compute-intensive random-ization test (Pad?o, 2006).With regards to lexicalization, the inclusion ofrelation words is important.
However, unlikePlank and Moschitti (2013), we found that it isbetter to exclude the lexical information of entitiesthemselves, which confirms the findings of Riedelet al (2013).
We hypothesize that the correctnessof a relation triple in Open IE is not closely re-1The Treebank set of (Xu et al, 2013), with minor correc-tions, and the ClueWeb set are appended to this publication.Smoothing Lexical info P R F1none (Xu13) none 85.7 72.7 78.7none all words 89.8 66.7 76.5Brown (PM13) relation only 88.7 71.2 79.0Brown (PM13) all words 84.5 74.2 79.0Brown (PM13) excl.
entities 86.2 75.8 80.7embedding relation only 93.9 69.7 80.0embedding all words 93.8 68.2 79.0embedding excl.
entities 95.9 71.2 81.7Table 2: The results of relation extraction with al-ternative smoothing and lexicalization techniqueson the Penn Treebank set (with our relation candi-date extraction and tree structure).lated to entities.
Consider the example mentionedin (Riedel et al, 2013): for relations like ?X vis-its Y?, X could be a person or organization, and Ycould be a location, organization, or person.Our final set of experiments evaluates the best-performing version of our system (the last rowin Table 2) against two state-of-the-art Open IEsystems: Mesquita et al (2013), which is basedon several hand-crafted dependency patterns; andXu et al (2013), which uses POS-based relationcandidate extraction and an unlexicalized tree ker-nel.
Tree kernel systems are all trained on thePenn Treebank training set, and tuned on the cor-responding development sets.The results in Table 3 show that our system con-sistently outperforms the other two systems, withabsolute gains in F-score between 4 and 16%.
Weinclude the reported results of (Xu et al, 2013)on the Penn Treebank set, and of (Mesquita et al,2013) on the New York Times set.
The ClueWebresults were obtained by running the respectivesystems on the test set, except that we used ourrelation candidate extraction method for the treekernel of (Xu et al, 2013).
We conclude that thesubstantial improvement on the Penn Treebank setcan be partly attributed to a superior tree kernel,and not only to a better relation candidate extrac-tion method.
We also note that word embeddingsstatistically outperform Brown clustering on theClueWeb set, but not on the other two sets.The ClueWeb set is quite challenging becauseit contains web pages which can be quite noisy.As a result we?ve found that a number of Open IEerrors are caused by parsing.
Conjunction struc-tures are especially difficult for both parsing andrelation extraction.
For example, our system ex-tracts the relation triple <Scotland, base, Scott>from the sentence ?Set in 17th century Scotland282P R F1Penn Treebank setXu et al (2013)* 66.1 50.7 57.4Brown (PM13) 82.8 65.8 73.3Ours (embedding) 91.8 61.6 73.8New York Times setMesquita et al (2013)* 72.8 39.3 51.1Brown (PM13) 83.5 44.0 57.6Ours (embedding) 85.9 40.7 55.2ClueWeb setXu et al (2013) 54.3 35.8 43.2Mesquita et al (2013) 63.3 29.2 40.0Brown (PM13) 54.1 31.1 39.5Ours (embedding) 45.8 51.9 48.7Table 3: Comparison of complete Open IE sys-tems.
The asterisks denote results reported in pre-vious work.and based on a novel by Sir Walter Scott, its highdrama...?
with the wrong dependency path Scot-landconj and?
basedprep by?
Scott.
In the future, wewill investigate whether adding information fromcontext words that are not on the dependency pathbetween two entities may alleviate this problem.6 ConclusionWe have proposed a lexicalized tree kernel modelfor Open IE, which incorporates word embeddingslearned from a neural network model.
Our sys-tem combines a dependency-based relation candi-date extraction method with a lexicalized tree ker-nel, and achieves state-of-the-art results on threedatasets.
Our experiments on different configu-rations of the smoothing and lexicalization tech-niques show that excluding named entity informa-tion is a better strategy for Open IE.In the future, we plan to mitigate the perfor-mance drop on the ClueWeb set by adding in-formation about context words around relationwords.
We will also investigate other ways of col-lapsing different types of tags in the lexicalizedtree representation.AcknowledgmentsWe would like to thank the anonymous review-ers for their helpful suggestions.
This work wasfunded in part by Alberta Innovates Center forMachine Learning (AICML), Natural Sciencesand Engineering Research Council of Canada(NSERC), Alberta Innovates Technology Futures(AITF), and National Institute of Informatics (NII)International Internship Program.ReferencesMichele Banko, Michael J Cafarella, Stephen Soderl,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In InternationalJoint Conference on Artificial Intelligence, pages2670?2676.Stephan Bloehdorn and Alessandro Moschitti.
2007.Structure and semantics for expressive text kernels.In Proceedings of the Sixteenth ACM Conferenceon Conference on Information and Knowledge Man-agement, CIKM ?07, pages 861?864, New York,NY, USA.
ACM.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Comput.
Linguist., 18(4):467?479, Decem-ber.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, ACL ?02, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Interna-tional Conference on Machine Learning, ICML.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, EMNLP ?11, pages1034?1046, Stroudsburg, PA, USA.
Association forComputational Linguistics.G.
Golub and W. Kahan.
1965.
Calculating the singu-lar values and pseudo-inverse of a matrix.
Journal ofthe Society for Industrial and Applied Mathematics:Series B, Numerical Analysis, page 205224.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Mausam, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open languagelearning for information extraction.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 523?534.
Asso-ciation for Computational Linguistics.Filipe Mesquita, Jordan Schmidek, and Denilson Bar-bosa.
2013.
Effectiveness and efficiency of open283relation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 447?457.
Association for Com-putational Linguistics.Sebastian Pad?o, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1498?1507, Sofia, Bulgaria, August.Association for Computational Linguistics.Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction withmatrix factorization and universal schemas.
In JointHuman Language Technology Conference/AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL?13), June.Shashank Srivastava, Dirk Hovy, and Eduard Hovy.2013.
A walk-based semantically enriched tree ker-nel over distributed word representations.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1411?1416, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 384?394,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 118?127,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,and Denilson Barbosa.
2013.
Open informationextraction with tree kernels.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 868?877, At-lanta, Georgia, June.
Association for ComputationalLinguistics.284
