Word Sense Disambiguation in a Korean-to-JapaneseMT System Using Neural NetworksYou-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok LeeDiv.
of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH)and Advanced Information Technology Research Center(AlTrc)San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784{prizer,sjkang,khmoon,jhlee}@postech.ac.krAbstractThis paper presents a method to resolveword sense ambiguity in aKorean-to-Japanese machine translationsystem using neural networks.
Theexecution of our neural network model isbased on the concept codes of a thesaurus.Most previous word sense disambiguationapproaches based on neural networks havelimitations due to their huge feature set size.By contrast, we reduce the number offeatures of the network to a practical size byusing concept codes as features rather thanthe lexical words themselves.IntroductionKorean-to-Japanese machine translation (MT)employs a direct MT strategy, where a Koreanhomograph may be translated into a differentJapanese equivalent depending on which senseis used in a given context.
Thus, word sensedisambiguation (WSD) is essential to theselection of an appropriate Japanese target word.Much research on word sense disambiguationhas revealed that several different types ofinformation can contribute to the resolution oflexical ambiguity.
These include surroundingwords (an unordered set of words surrounding atarget word), local collocations (a short sequenceof words near a target word, taking word orderinto account), syntactic relations (selectionalrestrictions), parts of speech, morphologicalforms, etc (McRoy, 1992, Ng and Zelle, 1997).Some researchers use neural networks intheir word sense disambiguation systemsBecause of its strong capability in classification(Waltz et al, 1985, Gallant, 1991, Leacock et al,1993, and Mooney, 1996).
Since, however, mostsuch methods require a few thousands offeatures or large amounts of hand-written datafor training, it is not clear that the same neuralnetwork models will be applicable to real worldapplications.We propose a word sense disambiguationmethod that combines both the neural net-basedapproach and the work of Li et al(2000),especially focusing on the practicality of themethod for application to real world MTsystems.
To reduce the number of input featuresof neural networks to a practical size, we useconcept codes of a thesaurus as features.In this paper, Yale Romanization is used torepresent Korean expressions.1 System ArchitectureOur neural network method consists of twophases.
The first phase is the construction of thefeature set for the neural network; the secondphase is the construction and training of theneural network.
(see Figure 1.
)For practical reasons, a reasonably smallnumber of features is essential to the design of aneural network.
To construct a feature set of areasonable size, we adopt Li?s method (2000),based on concept co-occurrence information(CCI).
CCI are concept codes of words whichco-occur with the target word for a specificsyntactic relation.In accordance with Li?s method, weautomatically extract CCI from a corpus byconstructing a Korean sense-tagged corpus.
Toaccomplish this, we apply a Japanese-to-KoreanMT system.
Next, we extract CCI from theconstructed corpus through partial parsing andscanning.
To eliminate noise and to reduce thenumber of CCI, refinement proceesing is appliedJapanese CorpusCOBALT-J/K(Japanese-to-KoreanMT system)Sense TaggedKorean CorpusPartial Parsing& Pattern ScanningRaw CCICCI RefinementProcessingRefined CCIFeature Set Construction Neural Net ConstructionFeature SetNetworkConstructionNeural NetworkNetworkLearningStored inMT DictionaryNetworkParametersFigure 1.
System Architecturenounnature  character          society institute  things0            1                     7          8          9astro- calen- animal        pheno-nomy    dar                          mena00      01              06             09goods drugs  food       stationary    machine90      91       92             96             99orga- ani- sin- intes- egg    sexnism  mal              ews   tine060    061               066   067    068   069supp- writing- count- belllies      tool      book960     961      962               969????????????????????
?????????????????????????????
?L1L2L3L4Figure 2.
Concept hierarchy of the Kadokawathesaurusto the extracted raw CCI.
After completingrefinement processing, we use the remainingCCI as features for the neural network.
Thetrained network parameters are stored in aKorean-to-Japanese MT dictionary for WSD intranslation.2 Construction of Refined Feature Set2.1 Automatic Construction of Sense-taggedCorpusFor automatic construction of the sense-taggedcorpus, we used a Japanese-to-Korean MTsystem called COBALT-J/K1.
In the transferdictionary of COBALT-J/K, nominal and verbalwords are annotated with concept codes of theKadokawa thesaurus (Ohno and Hamanishi,11CdCtnca1JN1TpThe quality of the constructed sense-taggedcorpus is a critical issue.
To evaluate the quality,we collected 1,658 sample sentences (29,420eojeols2) from the corpus and checked theirprecision.
The total number of errors was 789,and included such errors as morphologicalanalysis, sense ambiguity resolution andunknown words.
It corresponds to the accuracyof 97.3% (28,631 / 29,420 eojeols).Because almost all Japanese common nounsrepresented by Chinese characters aremonosemous little transfer ambiguity isexhibited in Japanese-to-Korean translation.
Inour test, the number of ambiguity resolutionerrors was 202 and it took only 0.69% of theoverall corpus (202 / 29,420 eojeols).Considering the fact that the overall accuracy ofthe constructed corpus exceeds 97% and only afew sense ambiguity resolution errors were.,981), which has a 4-level hierarchy of about,100 semantic classes, as shown in Figure 2.oncept nodes in level L1, L2 and L3 are furtherivided into 10 subclasses.We made a slight modification ofOBALT-J/K to enable it to produce Koreanranslations from a Japanese text, with allominal words tagged with specific conceptodes at level L4 of the Kadokawa thesaurus.
Asresult, a Korean sense-tagged corpus of,060,000 sentences can be obtained from theapanese corpus (Asahi Shinbun, Japaneseewspaper of Economics, etc.
).COBALT-J/K (Collocation-Based Languageranslator from Japanese to Korean) is a high-qualityractical MT system developed by POSTECH.found in the Japanese-to-Korean translation ofnouns, we regard the generated sense-taggedcorpus as highly reliable.2.2 Extraction of Raw CCIUnlike English, Korean has almost no syntacticconstraints on word order as long as the verbappears in the final position.
The variable wordorder often results in discontinuous constituentsInstead of using local collocations by word orderLi et al (2000) defined 13 patterns of CCI forhomographs using syntactically related words ina sentence.
Because we are concerned only with2 An Eojeol is a Korean syntactic unit consisting of acontent word and one or more function words.Table 2.
Concept codes and frequencies in CFP({<Ci,fi>}, type2, nwun(eye))Code Freq.
Code Freq.
Code Freq.
Code Freq.103 4 107 8 121 7 126 4143 8 160 5 179 7 277 4320 8 331 6 416 7 419 12433 4 501 13 503 10 504 11505 6 507 12 508 27 513 5530 6 538 16 552 4 557 7573 5 709 5 718 5 719 4733 5 819 4 834 4 966 4987 9 other* 210?
?other?
in the table means the set of concept codeswith the frequencies less than 4.Table 1.
Structure of CCI PatternsCCI type Structure of patterntype0 unordered co-occurrence wordstype1 noun + noun  or  noun + nountype2 noun + uy + nountype3 noun + other particles + nountype4 noun + lo/ulo + verbtype5 noun + ey + verbtype6 noun + eygey + verbtype7 noun + eyse + verbtype8 noun + ul/lul + verbtype9 noun + i/ka + verbtype10 verb + relativizer + nounnoun homographs, we adopt 11 patterns fromthem excluding verb patterns, as shown in Table1.
The words in bold indicate the targethomograph and the words in italic indicateKorean particles.For a homograph W, concept frequencypatterns (CFPs), i.e., ({<C1,f1>,<C2,f2>, ... ,<Ck,fk>}, typei, W(Si)), are extracted from thesense-tagged training corpus for each CCI type iby partial parsing and pattern scanning, where kis the number of concept codes in typei, fi is thefrequency of concept code Ci appearing in thecorpus, typei is an CCI type i, and W(Si) is ahomograph W with a sense Si.
All concepts inCFPs are three-digit concept codes at level L4 inthe Kadokawa thesaurus.
Table 2 demonstratesan example of CFP that can co-occur with thehomograph ?nwun(eye)?
in the form of the CCItype2 and their frequencies.2.3 CCI Refinement ProcessingThe extracted CCI are too numerous and toonoisy to be used in a practical system, and mustto be further selected.
To eliminate noise and toreduce the number of CCI to a practical size, weapply the refinement processing to the extractedCCI.
CCI refinement processing is composed of2 processes: concept code discrimination andconcept code generalization.2.3.1 Concept Code DiscriminationIn the extracted CCI, the same concept code mayappear for determining the different meanings ofa homograph.
To select the most probableconcept codes, which frequently co-occur withthe target sense of a homograph, Li defined thediscrimination value of a concept code usingShannon?s entropy (Shannon, 1951).
A conceptcode with a small entropy has a largediscrimination value.
If the discrimination valueof the concept code is larger than a threshold,the concept code is selected as usefulinformation for deciding the word sense.Otherwise, the concept code is discarded.2.3.2 Concept Code GeneralizationAfter concept discrimination, co-occurringconcept codes in each CCI type must be furtherselected and the code generalized.
To performcode generalization, Li adopted to Smadja?swork (Smadja, 1993) and defined the codestrength using a code frequency and a standarddeviation in each level of the concept hierarchy.The generalization filter selects the conceptcodes with a strength larger than a threshold.
Weperform this generalizaion processing on theKadokawa thesaurus level L4 and L3.After processing, the system stores theretyrefese33Bthusrefined conceptual patterns ({C1, C2, C3, ...},pei, W(Si)) as a knowledge source for WSD ofal texts.
These refined CCI are used as inputatures for the neural network.
The morepecific description of the CCI extraction isxplained in Li (2000).Construction of Neural Network.1 Neural Network Architectureecause of its strong capability for classification,e multilayer feedforward neural network issed in our sense classification system.
Ashown in Figure 3, each node in the input layerpresents a concept code in CCI of a target.
.CCI type i2CCI type i1input CCI type 0inputCCI type 1inputCCI type 8inputCCI type 2input7426022078080506960284193823239323nwun1 (snow)nwun2 (eye)...Figure 5.
The Resulting Network for ?nwun?wrnnTo determine a good topology for the network,we implemented a 2-layer (no hidden layer) anda 3-layer (with a single hidden layer of 5 nodes)network and compared their performance.
Thecomparison result is given in Section 5.Each homograph has a network of its own.Figure 43 demonstrates a construction exampleof the input layer for the homograph ?nwun?with the sense ?snow?
and ?eye?.
The left side isthe extracted CCI for each sense after refinementprocessing.
We construct the input layer for?nwun?
by merely integrating the concept codesin both senses.
The resulting input layer ispartitioned into several subgroups depending ontheir CCI types, i.e., type 0, type 1, type 2 andtype 8.
Figure 5 shows the overall networkarchitecture for ?nwun?.3fc3.2 Network LearningWe selected 875 Korean homographs requringthe WSD processing in a Korean-to-Japanesetranslation.
Among the selected nouns, 736nouns (about 84%) had two senses and the other139 nouns had more than 3 senses.
Using theextracted CCI, we constructed neural networksand trained network parameters for eachhomograph.
The training patterns were alsoextracted from the previously constructedsense-tagged corpus.The average number of input features (i.e.input nodes) of the constructed networks wasapproximately 54.1 and the average number ofsenses (i.e.
output nodes) was about 2.19.
In thecase of a 2-layer network, the total number ofparameters (synaptic weights) needed to betrained is about 118 (54.1?2.19) for eachhomograph.
This means that we merely needstorage for 118 floating point numbers (forsfere?
CCI type 0 : {26, 022}?
CCI type 1 : {080, 696}nwun1 (snow)CCI type 0inputCCI type 17426022078080Refined CCI4OckosFKThe concept codes in Figure 4 are simplified onesor the ease of illustration.
In reality there are 87?
CCI type 8 : {38, 239}Total 13 concept codesintegrate inputCCI type 8inputCCI type 2input13 nodesnwun2 (eye)?
CCI type 0 : {74, 078}?
CCI type 2 : {50, 028, 419}?
CCI type 8 : {23, 323}506960284193823239323Figure 4.
Construction of Input layer for ?nwun?.....Output(senses of thetarget word)Inputs Outputs..HiddenLayersinputCCI type ikinput...Figure 3.
Topology of Neural Networkord and each node in the output layerepresents the sense of a target word.
Theumber of hidden layers and the number ofodes in a hidden layer are another crucial issue.
oncept codes for ?nwun?.
Cynaptic weights) and 54 integers (for inputatures) for each homograph, which is aasonable size to be used in real applications.Word Sense Disambiguationur WSD approach is a hybrid method, whichombines the advantage of corpus-based andnowledge-based methods.
Figure 6 shows ourverall WSD algorithm.
For a given homograph,ense disambiguation is performed as follows.irst, we search a collocation dictionary.
Theorean-to-Japanese translation systemOBALT-K/J has an MWTU (Multi-Word{078}CCI type 0 CCI type 0inputCCI type 1inputCCI type 8inputCCI type 2inputCCI type 1nwunmwul-i   katuk-han   kunye-uy   nwun-ul   po-myeinput               : ???
???
???
??
??
?
[078] [274]concept code  : [503] [331]targetwordCCI type        : (type 0) (type 0) (type 2) (type 8)CCI type 2CCI type 8{none}{503}{331}0780227426028506960802393823419323Input LayerSimilarityCalculation{274}(0.000)(0.285)(0.250)(1.000)(0.000)(0.857)(0.000)(0.000)(0.000)(0.285)(0.000)(0.000)(0.250)similarity valuesFigure 7.
Construction of Input Pattern by UsingConcept Similarity CalculationNeural NetworksSelect the most frequent senseSuccessSuccessAnswerNONONOYESYESYESSelectional Restrictions of the VerbCollocation DictionarySuccessFigure 6.
The Proposed WSD AlgorithmTranslation Units) dictionary, which containsidioms, compound words, collocations, etc.
If acollocation of the target word exists in theMWTU dictionary, we simply determine thesense of the target word to the sense found in thedictionary.
This method is based on the idea of?one sense per collocation?.
Next, we verify theselectional restriction of the verb described inthe dictionary.
If we cannot find any matchedpatterns for selectional restrictions, we apply theneural network approach.
WSD in the neuralnetwork stage is performed in the following 3steps.Step 1.
Extract CCI from the context of thetarget word.
The window size of the context is asingle sentence.
Consider, for example, thesentence in Figure 7 which has the meaning of?Seeing her eyes filled with tears, ??.
Thetarget word is the homograph ?nwun?.
Weextract its CCI from the sentence by partialparsing and pattern scanning.
In Figure 7, thewords ?nwun?
and ?kunye(her)?
with the conceptcode 503 have the relation of <noun + uy +noun>, which corresponds to ?CCI type 2?
inTable 1.
There is no syntactic relation betweenthe words ?nwun?
and ?nwunmul(tears)?
with theconcept code 078, so we assign ?CCI type 0?
tothe concept code 078.Similarly, we can obtain all pairs of CCI typesand their concept codes appearing in the context.All the extracted <CCI-type: concept codes>pairs are as follows: {<type 0: 078,274>, <type2: 503>, <type 8: 331>}.Step 2.
Obtain the input pattern for thenetwork by calculating concept similaritiesbetween the features of the input nodes and theconcept code in the extracted <CCI-type:concept codes>.
Concept similarity calculationis performed only between the concept codeswith the same CCI-type.
The calculated conceptsimilarity score is assigned to each input node asthe input value to the network.Csim(Ci, Pj) in Equation 1 is used to calculatethe concept similarity between Ci and Pj, whereMSCA(Ci, Pj) is the most specific commonancestor of concept codes Ci and Pj, and weightis a weighting factor reflecting that Ci as adescendant of Pj is preferable to other cases.That is, if Ci is a descendant of Pj, we set weightto 1.
Otherwise we set weight to 0.5.weightPlevelClevelPCMSCAlevelPCCsimjijiji ?+?=)()()),((2),(  (1)The similarity values between the target(all 0.000)(0.375)(0.857)(0.667)(0.285)(0.250) (0.250)L1L2L3L4?CiP1P2P3P4P5 P5TOPFigure 8.
Concept Similarity on the KadokawaThesaurus Hierarchyconcept Ci and each Pj on the Kadokawathesaurus hierarchy are shown in Figure 8.These similarity values are computed usingEquation 1.
For example, in ?CCI-type 0?
partcalculation, the relation between the conceptcodes 274 and 26 corresponds to the relationbetween Ci and P4 in Figure 8.
So we assign thesimilarity 0.285 to the input node labeled by 26.As another example, the concept codes 503 and50 have a relation between Ci and P2 and weobtain the similarity 0.857.
If more than twoconcept codes exist in one CCI-type, such as<CCI-type 0: 078, 274>, the maximumsimilarity value among them is assigned to theinput node, as in Equation 2.In Equation 2, Ci is the concept code of theinput node, and Pj is the concept codes in the<CCI-type: concept codes> pair which has thesame CCI-type as Ci.By adopting this concept similarity calculation,we can achieve a broad coverage of the method.If we use the exact matching scheme instead ofconcept similarity, we may obtain only a fewconcept codes matched with the features.Consequently, sense disambiguation would failbecause of the absence of clues.Step 3.
Feed the obtained input pattern to theneural network and compute activation strengthsfor each output node.
Next, select the sense ofthe node that has a larger activation value thanall other output node.
If the activation strength islower than the threshold, it will be discarded andhis5 Experimental EvaluationFor an experimental evaluation, 10 ambiguousKorean nouns were selected, along with a totalof 500 test sentences in which one homographappears.
In order to follow the ambiguitydistribution described in Section 3.2, we set thenumber of test nouns with two senses to 8 (80%).The test sentences were randomly selected fromthe KIBS (Korean Information Base System)corpus.The experimental results are shown in Table 3,where result A is the case when the mostfrequent sense was taken as the answer.
Tocompare it with our approach (result C), we alsoperformed the experiment using Li?s method(result B).
For sense disambiguation, Li?smethod features which are similar to our method.However, unlike our method, which combinesall features by using neural networks, Liconsiders only one clue at each decision step.
Asshown in the table, our approach exceeded Li?s)),((max)( jiPi PCCsimCInputVal i=    (2)Table 3.
Comparison of WSD ResultsPrecision (%) Word Sense No (A) (B) (C)father & child 33 pwucarich man 1766 64 72liver 37 kancangsoy source 1374 84 74housework 39 kasawords of song 1178 68 82shoes 45 kwutwuword of mouth 590 70 92eye 42 nwunsnow 884 80 86container 41 yongki 82 72 88the network will not make any decisions.
Tprocess is represented in Figure 9. courage 9doctor 27 uysaintention 2354 80 84district 27 cikwuthe earth 2354 84 92whole body 39one?s past 6 censintelegraph 578 84 80one?s best 27military strength 13electric power 7cenlyekpast record 354 50 72Average Precision 71.4 73.6 82.2?
(A) : Baseline   (B) : Li?s method(C) : Proposed method (using a 2-layer NN)nwun1 (snow)nwun2 (eye)...threshold(0.000)(0.285)(0.250)(1.000)(0.000)(0.857)(0.000)(0.000)(0.000)(0.285)(0.000)(0.000)(0.250)Figure 9.
Sense Disambiguation for ?nwun?in most of the results except ?kancang?
and?censin?.
This result shows that word sensedisambiguation can be improved by combiningseveral clues together (e.g.
neural networks)rather than using them independently (e.g.
Li?smethod).The performance for each stage of theproposed method is shown in Table 4.
SymbolsCOL, VSR, NN and MFS in the table indicate 4stages of our method in Figure 6, respectively.In the NN stage, the 3-layer model did not showa performance superior  to the 2-layer modelbecause of the lack of training samples.
Sincethe 2-layer model has fewer parameters to betrained, it is more efficient to generalize forlimited training corpora than the 3-layer model.ConclusionTo resolve sense ambiguities inKorean-to-Japanese MT, this paper has proposeda practical word sense disambiguation methodusing neural networks.
Unlike most previousapproaches based on neural networks, we reducethe number of features for the network to apractical size by using concept codes rather thanlexical words.
In an experimental evaluation, theproposed WSD model using a 2-layer networkachieved an average precision of 82.2% with animprovement over Li?s method by 8.6%.
Thisresult is very promising for real world MTsystems.We plan further research to improve precisionand to expand our method for verb homographdisambiguation.AcknowledgementsThis work was supported by the Korea Scienceand Engineering Foundation (KOSEF)  throughthe Advanced Information Technology ResearchCenter(AITrc).Table 4.
Average Precision and Coveragefor Each Stage of thePproposed Method<Case 1 : 2-layer NN>COL VSR NN MFSAvg.
Prec 100.0% 91.2% 86.3% 56.1%Avg.
Cov 3.6% 6.8% 73.2% 16.4%<Case 2 : 3-layer NN>COL VSR NN MFSAvg.
Prec 100.0% 91.2% 87.1% 56.0%Avg.
Cov 3.6% 6.8% 72.5% 17.1%ReferencesGallant S. (1991) A Practical Approach forRepresenting Context and for Performing WordSense Disambiguation Using Neural Networks.Neural Computation, 3/3, pp.
293-309Leacock C., Twell G. and Voorhees E. (1993)Corpus-based Statistical Sense Resolution.
InProceedings of the ARPA Human LanguageTechnology Workshop, San Francisco, MorganKaufman, pp.
260-265Li H. F., Heo N. W., Moon K. H., Lee J. H. and LeeG.
B.
(2000) Lexical Transfer AmbiguityResolution Using Automatically-Extracted ConceptCo-occurrence Information.
International Journalof Computer Processing of Oriental Languages,13/1, pp.
53-68McRoy S. (1992) Using Multiple Knowledge Sourcesfor Word Sense Discrimination.
ComputationalLinguistics, 18/1, pp.
1-30Mooney R. (1996) Comparative Experiments onDisambiguating Word Senses: An Illustration ofthe Role of Bias in Machine Learning.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing,Philadelphia, PA, pp.
82-91Ng, H. T. and Zelle J.
(1997) Corpus-BasedApproaches to Semantic Interpretation in NaturalLanguage Processing.
AI Magazine, 18/4, pp.45-64Ohno S. and Hamanishi M. (1981) New SynonymDictionary.
Kadokawa Shoten, TokyoSmadja F. (1993) Retrieving Collocations from Text:Xtract.
Computational Linguistics, 19/1, pp.143-177Waltz D. L. and Pollack J.
(1985) Massively ParallelParsing: A Strongly Interactive Model of NaturalLanguage Interpretation.
Cognitive Science, 9, pp.51-74
