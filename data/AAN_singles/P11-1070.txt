Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693?702,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsWeb-Scale Features for Full-Scale ParsingMohit Bansal and Dan KleinComputer Science DivisionUniversity of California, Berkeley{mbansal,klein}@cs.berkeley.eduAbstractCounts from large corpora (like the web) canbe powerful syntactic cues.
Past work hasused web counts to help resolve isolated am-biguities, such as binary noun-verb PP attach-ments and noun compound bracketings.
Inthis work, we first present a method for gener-ating web count features that address the fullrange of syntactic attachments.
These fea-tures encode both surface evidence of lexi-cal affinities as well as paraphrase-based cuesto syntactic structure.
We then integrate ourfeatures into full-scale dependency and con-stituent parsers.
We show relative error re-ductions of 7.0% over the second-order depen-dency parser of McDonald and Pereira (2006),9.2% over the constituent parser of Petrov etal.
(2006), and 3.4% over a non-local con-stituent reranker.1 IntroductionCurrent state-of-the art syntactic parsers haveachieved accuracies in the range of 90% F1 on thePenn Treebank, but a range of errors remain.
Froma dependency viewpoint, structural errors can becast as incorrect attachments, even for constituent(phrase-structure) parsers.
For example, in theBerkeley parser (Petrov et al, 2006), about 20%of the errors are prepositional phrase attachment er-rors as in Figure 1, where a preposition-headed (IN)phrase was assigned an incorrect parent in the im-plied dependency tree.
Here, the Berkeley parser(solid blue edges) incorrectly attaches from debt tothe noun phrase $ 30 billion whereas the correct at-tachment (dashed gold edges) is to the verb rais-ing.
However, there are a range of error types, asshown in Figure 2.
Here, (a) is a non-canonical PPVBGVPNPNP ?
raising$ 30 billionPPfrom debt ?Figure 1: A PP attachment error in the parse output of theBerkeley parser (on Penn Treebank).
Guess edges are in solidblue, gold edges are in dashed gold and edges common in guessand gold parses are in black.attachment ambiguity where by yesterday afternoonshould attach to had already, (b) is an NP-internalambiguity where half a should attach to dozen andnot to newspapers, and (c) is an adverb attachmentambiguity, where just should modify fine and not theverb ?s.Resolving many of these errors requires informa-tion that is simply not present in the approximately1M words on which the parser was trained.
Oneway to access more information is to exploit sur-face counts from large corpora like the web (Volk,2001; Lapata and Keller, 2004).
For example, thephrase raising from is much more frequent on theWeb than $ x billion from.
While this ?affinity?
isonly a surface correlation, Volk (2001) showed thatcomparing such counts can often correctly resolvetricky PP attachments.
This basic idea has led to agood deal of successful work on disambiguating iso-lated, binary PP attachments.
For example, Nakovand Hearst (2005b) showed that looking for para-phrase counts can further improve PP resolution.In this case, the existence of reworded phrases likeraising it from on the Web also imply a verbal at-693SNPNP PP?/eKman +Xtton ,nFby yesterday afternoonVPKad aOread\ ?
PDTNP?
KaOIDTaPDTdozenPDTnewspapersQP VBZVP?
?sADVPRBjustADJPJJfineADJP(a) (b) (c)Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank).
Guess edges are insolid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black.tachment.
Still other work has exploited Web countsfor other isolated ambiguities, such as NP coordina-tion (Nakov and Hearst, 2005b) and noun-sequencebracketing (Nakov and Hearst, 2005a; Pitler et al,2010).
For example, in (b), half dozen is more fre-quent than half newspapers.In this paper, we show how to apply these ideasto all attachments in full-scale parsing.
Doing sorequires three main issues to be addressed.
First,we show how features can be generated for arbitraryhead-argument configurations.
Affinity features arerelatively straightforward, but paraphrase features,which have been hand-developed in the past, aremore complex.
Second, we integrate our featuresinto full-scale parsing systems.
For dependencyparsing, we augment the features in the second-orderparser of McDonald and Pereira (2006).
For con-stituent parsing, we rerank the output of the Berke-ley parser (Petrov et al, 2006).
Third, past systemshave usually gotten their counts from web searchAPIs, which does not scale to quadratically-manyattachments in each sentence.
Instead, we considerhow to efficiently mine the Google n-grams corpus.Given the success of Web counts for isolated am-biguities, there is relatively little previous researchin this direction.
The most similar work is Pitleret al (2010), which use Web-scale n-gram countsfor multi-way noun bracketing decisions, thoughthat work considers only sequences of nouns anduses only affinity-based web features.
Yates et al(2006) use Web counts to filter out certain ?seman-tically bad?
parses from extraction candidate setsbut are not concerned with distinguishing amongsttop parses.
In an important contrast, Koo et al(2008) smooth the sparseness of lexical features in adiscriminative dependency parser by using cluster-based word-senses as intermediate abstractions inaddition to POS tags (also see Finkel et al (2008)).Their work also gives a way to tap into corpora be-yond the training data, through cluster membershiprather than explicit corpus counts and paraphrases.This work uses a large web-scale corpus (Googlen-grams) to compute features for the full parsingtask.
To show end-to-end effectiveness, we incor-porate our features into state-of-the-art dependencyand constituent parsers.
For the dependency case,we can integrate them into the dynamic program-ming of a base parser; we use the discriminatively-trained MST dependency parser (McDonald et al,2005; McDonald and Pereira, 2006).
Our first-orderweb-features give 7.0% relative error reduction overthe second-order dependency baseline of McDon-ald and Pereira (2006).
For constituent parsing, weuse a reranking framework (Charniak and Johnson,2005; Collins and Koo, 2005; Collins, 2000) andshow 9.2% relative error reduction over the Berke-ley parser baseline.
In the same framework, wealso achieve 3.4% error reduction over the non-localsyntactic features used in Huang (2008).
Our web-scale features reduce errors for a range of attachmenttypes.
Finally, we present an analysis of influentialfeatures.
We not only reproduce features suggestedin previous work but also discover a range of newones.2 Web-count FeaturesStructural errors in the output of state-of-the-artparsers, constituent or dependency, can be viewedas attachment errors, examples of which are Figure 1and Figure 2.1 One way to address attachment errorsis through features which factor over head-argument1For constituent parsers, there can be minor tree variationswhich can result in the same set of induced dependencies, butthese are rare in comparison.694raising          $           from    debt?
(raising     from) ?
($     from)?
(head     arg)Figure 3: Features factored over head-argument pairs.pairs, as is standard in the dependency parsing liter-ature (see Figure 3).
Here, we discuss which web-count based features ?
(h, a) should fire over a givenhead-argument pair (we consider the words h anda to be indexed, and so features can be sensitive totheir order and distance, as is also standard).2.1 Affinity FeaturesAffinity statistics, such as lexical co-occurrencecounts from large corpora, have been used previ-ously for resolving individual attachments at least asfar back as Lauer (1995) for noun-compound brack-eting, and later for PP attachment (Volk, 2001; La-pata and Keller, 2004) and coordination ambigu-ity (Nakov and Hearst, 2005b).
The approach ofLauer (1995), for example, would be to take an am-biguous noun sequence like hydrogen ion exchangeand compare the various counts (or associated con-ditional probabilities) of n-grams like hydrogen ionand hydrogen exchange.
The attachment with thegreater score is chosen.
More recently, Pitler et al(2010) use web-scale n-grams to compute similarassociation statistics for longer sequences of nouns.Our affinity features closely follow this basic ideaof association statistics.
However, because a realparser will not have access to gold-standard knowl-edge of the competing attachment sites (see Attererand Schutze (2007)?s criticism of previous work),we must instead compute features for all possiblehead-argument pairs from our web corpus.
More-over, when there are only two competing attachmentoptions, one can do things like directly compare twocount-based heuristics and choose the larger.
Inte-gration into a parser requires features to be functionsof single attachments, not pairwise comparisons be-tween alternatives.
A learning algorithm can thenweight features so that they compare appropriatelyacross parses.We employ a collection of affinity features ofvarying specificity.
The basic feature is the core ad-jacency count feature ADJ, which fires for all (h, a)pairs.
What is specific to a particular (h, a) is thevalue of the feature, not its identity.
For example, ina naive approach, the value of the ADJ feature mightbe the count of the query issued to the web-corpus ?the 2-gram q = ha or q = ah depending on the or-der of h and a in the sentence.
However, it turns outthat there are several problems with this approach.First, rather than a single all-purpose feature likeADJ, the utility of such query counts will vary ac-cording to aspects like the parts-of-speech of h anda (because a high adjacency count is not equally in-formative for all kinds of attachments).
Hence, weadd more refined affinity features that are specificto each pair of POS tags, i.e.
ADJ ?
POS(h) ?POS(a).
The values of these POS-specific features,however, are still derived from the same queries asbefore.
Second, using real-valued features did notwork as well as binning the query-counts (we usedb = floor(logr(count)/5) ?
5) and then firing in-dicator features ADJ ?
POS(h) ?
POS(a) ?
b forvalues of b defined by the query count.
Adding stillmore complex features, we conjoin to the precedingfeatures the order of the words h and a as they occurin the sentence, and the (binned) distance betweenthem.
For features which mark distances, wildcards(?)
are used in the query q = h ?
a, where the num-ber of wildcards allowed in the query is proportionalto the binned distance between h and a in the sen-tence.
Finally, we also include unigram variants ofthe above features, which are sensitive to only one ofthe head or argument.
For all features used, we addcumulative variants where indicators are fired for allcount bins b?
up to query count bin b.2.2 Paraphrase FeaturesIn addition to measuring counts of the words presentin the sentence, there exist clever ways in whichparaphrases and other accidental indicators can helpresolve specific ambiguities, some of which are dis-cussed in Nakov and Hearst (2005a), Nakov andHearst (2005b).
For example, finding attestations ofeat : spaghetti with sauce suggests a nominal attach-ment in Jean ate spaghetti with sauce.
As anotherexample, one clue that the example in Figure 1 is695a verbal attachment is that the proform paraphraseraising it from is commonly attested.
Similarly, theattestation of be noun prep suggests nominal attach-ment.These paraphrase features hint at the correct at-tachment decision by looking for web n-gramswith special contexts that reveal syntax superficially.Again, while effective in their isolated disambigua-tion tasks, past work has been limited by both therange of attachments considered and the need to in-tuit these special contexts.
For instance, frequencyof the pattern The noun prep suggests noun attach-ment and of the pattern verb adverb prep suggestsverb attachment for the preposition in the phraseverb noun prep, but these features were not in themanually brainstormed list.In this work, we automatically generate a largenumber of paraphrase-style features for arbitrary at-tachment ambiguities.
To induce our list of fea-tures, we first mine useful context words.
We takeeach (correct) training dependency relation (h, a)and consider web n-grams of the form cha, hca,and hac.
Aggregating over all h and a (of a givenPOS pair), we determine which context words c aremost frequent in each position.
For example, for h =raising and a = from (see Figure 1), we look at webn-grams of the form raising c from and see that oneof the most frequent values of c on the web turns outto be the word it.Once we have collected context words (for eachposition p in {BEFORE, MIDDLE, AFTER}), weturn each context word c into a collection of featuresof the form PARA ?
POS(h) ?
POS(a) ?
c ?
p ?dir, where dir is the linear order of the attachmentin the sentence.
Note that h and a are head and ar-gument words and so actually occur in the sentence,but c is a context word that generally does not.
Forsuch features, the queries that determine their val-ues are then of the form cha, hca, and so on.
Con-tinuing the previous example, if the test set has apossible attachment of two words like h = lower-ing and a = with, we will fire a feature PARA ?VBG ?
IN ?
it ?
MIDDLE ?
?
with value (indi-cator bins) set according to the results of the querylowering it with.
The idea is that if frequent oc-currences of raising it from indicated a correct at-tachment between raising and from, frequent occur-rences of lowering it with will indicate the correct-ness of an attachment between lowering and with.Finally, to handle the cases where no induced con-text word is helpful, we also construct abstractedversions of these paraphrase features where the con-text words c are collapsed to their parts-of-speechPOS(c), obtained using a unigram-tagger trained onthe parser training set.
As discussed in Section 5, thetop features learned by our learning algorithm dupli-cate the hand-crafted configurations used in previouswork (Nakov and Hearst, 2005b) but also add nu-merous others, and, of course, apply to many moreattachment types.3 Working with Web n-GramsPrevious approaches have generally used search en-gines to collect count statistics (Lapata and Keller,2004; Nakov and Hearst, 2005b; Nakov and Hearst,2008).
Lapata and Keller (2004) uses the numberof page hits as the web-count of the queried n-gram (which is problematic according to Kilgarriff(2007)).
Nakov and Hearst (2008) post-processesthe first 1000 result snippets.
One challenge withthis approach is that an external search API is nowembedded into the parser, raising issues of bothspeed and daily query limits, especially if all pos-sible attachments trigger queries.
Such methodsalso create a dependence on the quality and post-processing of the search results, limitations of thequery process (for instance, search engines can ig-nore punctuation (Nakov and Hearst, 2005b)).Rather than working through a search API (orscraper), we use an offline web corpus ?
the Googlen-gram corpus (Brants and Franz, 2006) ?
whichcontains English n-grams (n = 1 to 5) and their ob-served frequency counts, generated from nearly 1trillion word tokens and 95 billion sentences.
Thiscorpus allows us to efficiently access huge amountsof web-derived information in a compressed way,though in the process it limits us to local queries.In particular, we only use counts of n-grams of theform x ?
y where the gap length is ?
3.Our system requires the counts from a large col-lection of these n-gram queries (around 4.5 million).The most basic queries are counts of head-argumentpairs in contiguous h a and gapped h ?
a configura-tions.2 Here, we describe how we process queries2Paraphrase features give situations where we query ?
h a696of the form (q1, q2) with some number of wildcardsin between.
We first collect all such queries overall trees in preprocessing (so a new test set requiresa new query-extraction phase).
Next, we exploit asimple but efficient trie-based hashing algorithm toefficiently answer all of them in one pass over then-grams corpus.Consider Figure 4, which illustrates the datastructure which holds our queries.
We first createa trie of the queries in the form of a nested hashmap.The key of the outer hashmap is the first word q1of the query.
The entry for q1 points to an innerhashmap whose key is the final word q2 of the querybigram.
The values of the inner map is an array of4 counts, to accumulate each of (q1q2), (q1 ?
q2),(q1 ?
?q2), and (q1 ?
?
?
q2), respectively.
We use k-grams to collect counts of (q1...q2) with gap length= k?
2, i.e.
2-grams to get count(q1q2), 3-grams toget count(q1 ?
q2) and so on.With this representation of our collection ofqueries, we go through the web n-grams (n = 2 to5) one by one.
For an n-gram w1...wn, if the first n-gram word w1 doesn?t occur in the outer hashmap,we move on.
If it does match (say q?1 = w1), thenwe look into the inner map for q?1 and check for thefinal word wn.
If we have a match, we increment theappropriate query?s result value.In similar ways, we also mine the most frequentwords that occur before, in between and after thehead and argument query pairs.
For example, to col-lect mid words, we go through the 3-gramsw1w2w3;if w1 matches q?1 in the outer hashmap and w3 oc-curs in the inner hashmap for q?1, then we store w2and the count of the 3-gram.
After the sweep, wesort the context words in decreasing order of count.We also collect unigram counts of the head and ar-gument words by sweeping over the unigrams once.In this way, our work is linear in the size of then-gram corpus, but essentially constant in the num-ber of queries.
Of course, if the number of queries isexpected to be small, such as for a one-off parse ofa single sentence, other solutions might be more ap-propriate; in our case, a large-batch setting, the num-ber of queries was such that this formulation waschosen.
Our main experiments (with no paralleliza-tion) took 115 minutes to sweep over the 3.8 billionand h a ?
; these are handled similarly.?????????
?Web N-grams Query Count-Triecounts?????????????????????????????SCAN????
hash????
hashFigure 4: Trie-based nested hashmap for collecting ngram web-counts of queries.n-grams (n = 1 to 5) to compute the answers to 4.5million queries, much less than the time required totrain the baseline parsers.4 Parsing ExperimentsOur features are designed to be used in full-sentenceparsing rather than for limited decisions about iso-lated ambiguities.
We first integrate our features intoa dependency parser, where the integration is morenatural and pushes all the way into the underlyingdynamic program.
We then add them to a constituentparser in a reranking approach.
We also verify thatour features contribute on top of standard rerankingfeatures.34.1 Dependency ParsingFor dependency parsing, we use thediscriminatively-trained MSTParser4, an im-plementation of first and second order MST parsingmodels of McDonald et al (2005) and McDonaldand Pereira (2006).
We use the standard splits ofPenn Treebank into training (sections 2-21), devel-opment (section 22) and test (section 23).
We usedthe ?pennconverter?5 tool to convert Penn trees fromconstituent format to dependency format.
FollowingKoo et al (2008), we used the MXPOST tagger(Ratnaparkhi, 1996) trained on the full training datato provide part-of-speech tags for the development3All reported experiments are run on all sentences, i.e.
with-out any length limit.4http://sourceforge.net/projects/mstparser5This supersedes ?Penn2Malt?
and is available athttp://nlp.cs.lth.se/software/treebank converter.
We followits recommendation to patch WSJ data with NP bracketing byVadas and Curran (2007).697Order 2 + Web features % Error Redn.Dev (sec 22) 92.1 92.7 7.6%Test (sec 23) 91.4 92.0 7.0%Table 1: UAS results for English WSJ dependency parsing.
Devis WSJ section 22 (all sentences) and Test is WSJ section 23(all sentences).
The order 2 baseline represents McDonald andPereira (2006).and the test set, and we used 10-way jackknifing togenerate tags for the training set.We added our first-order Web-scale features tothe MSTParser system to evaluate improvement overthe results of McDonald and Pereira (2006).6 Ta-ble 1 shows unlabeled attachments scores (UAS)for their second-order projective parser and the im-proved numbers resulting from the addition of ourWeb-scale features.
Our first-order web-scale fea-tures show significant improvement even over theirnon-local second-order features.7 Additionally, ourweb-scale features are at least an order of magnitudefewer in number than even their first-order base fea-tures.4.2 Constituent ParsingWe also evaluate the utility of web-scale featureson top of a state-of-the-art constituent parser ?
theBerkeley parser (Petrov et al, 2006), an unlexical-ized phrase-structure parser.
Because the underly-ing parser does not factor along lexical attachments,we instead adopt the discriminative reranking frame-work, where we generate the top-k candidates fromthe baseline system and then rerank this k-best listusing (generally non-local) features.Our baseline system is the Berkeley parser, fromwhich we obtain k-best lists for the development set(WSJ section 22) and test set (WSJ section 23) usinga grammar trained on all the training data (WSJ sec-tions 2-21).8 To get k-best lists for the training set,we use 3-fold jackknifing where we train a grammar6Their README specifies ?training-k:5 iters:10 loss-type:nopunc decode-type:proj?, which we used for all final ex-periments; we used the faster ?training-k:1 iters:5?
setting formost development experiments.7Work such as Smith and Eisner (2008), Martins et al(2009), Koo and Collins (2010) has been exploring more non-local features for dependency parsing.
It will be interesting tosee how these features interact with our web features.8Settings: 6 iterations of split and merge with smoothing.k = 1 k = 2 k = 10 k = 25 k = 50 k = 100Dev 90.6 92.3 95.1 95.8 96.2 96.5Test 90.2 91.8 94.7 95.6 96.1 96.4Table 2: Oracle F1-scores for k-best lists output by Berkeleyparser for English WSJ parsing (Dev is section 22 and Test issection 23, all lengths).on 2 folds to get parses for the third fold.9 The ora-cle scores of the k-best lists (for different values ofk) for the development and test sets are shown in Ta-ble 2.
Based on these results, we used 50-best listsin our experiments.
For discriminative learning, weused the averaged perceptron (Collins, 2002; Huang,2008).Our core feature is the log conditional likelihoodof the underlying parser.10 All other features are in-dicator features.
First, we add all the Web-scale fea-tures as defined above.
These features alone achievea 9.2% relative error reduction.
The affinity andparaphrase features contribute about two-fifths andthree-fifths of this improvement, respectively.
Next,we rerank with only the features (both local andnon-local) from Huang (2008), a simplified mergeof Charniak and Johnson (2005) and Collins (2000)(here configurational).
These features alone achievearound the same improvements over the baseline asour web-scale features, even though they are highlynon-local and extensive.
Finally, we rerank withboth our Web-scale features and the configurationalfeatures.
When combined, our web-scale featuresgive a further error reduction of 3.4% over the con-figurational reranker (and a combined error reduc-tion of 12.2%).
All results are shown in Table 3.115 AnalysisTable 4 shows error counts and relative reductionsthat our web features provide over the 2nd-orderdependency baseline.
While we do see substantialgains for classic PP (IN) attachment cases, we seeequal or greater error reductions for a range of at-tachment types.
Further, Table 5 shows how the to-9Default: we ran the Berkeley parser in its default ?fast?mode; the output k-best lists are ordered by max-rule-score.10This is output by the flag -confidence.
Note that baselineresults with just this feature are slightly worse than 1-best re-sults because the k-best lists are generated by max-rule-score.We report both numbers in Table 3.11We follow Collins (1999) for head rules.698Dev (sec 22) Test (sec 23)Parsing Model F1 EX F1 EXBaseline (1-best) 90.6 39.4 90.2 37.3log p(t|w) 90.4 38.9 89.9 37.3+ Web features 91.6 42.5 91.1 40.6+ Configurational features 91.8 43.8 91.1 40.6+ Web + Configurational 92.1 44.0 91.4 41.4Table 3: Parsing results for reranking 50-best lists of Berkeleyparser (Dev is WSJ section 22 and Test is WSJ section 23, alllengths).Arg Tag # Attach Baseline This Work % ERNN 5725 5387 5429 12.4NNP 4043 3780 3804 9.1IN 4026 3416 3490 12.1DT 3511 3424 3429 5.8NNS 2504 2319 2348 15.7JJ 2472 2310 2329 11.7CD 1845 1739 1738 -0.9VBD 1705 1571 1580 6.7RB 1308 1097 1100 1.4CC 1000 855 854 -0.7VB 983 940 945 11.6TO 868 761 776 14.0VBN 850 776 786 13.5VBZ 705 633 629 -5.6PRP 612 603 606 33.3Table 4: Error reduction for attachments of various child (argu-ment) categories.
The columns depict the tag, its total attach-ments as argument, number of correct ones in baseline (Mc-Donald and Pereira, 2006) and this work, and the relative errorreduction.
Results are for dependency parsing on the dev set foriters:5,training-k:1.tal errors break down by gold head.
For example,the 12.1% total error reduction for attachments of anIN argument (which includes PPs as well as comple-mentized SBARs) includes many errors where thegold attachments are to both noun and verb heads.Similarly, for an NN-headed argument, the majorcorrections are for attachments to noun and verbheads, which includes both object-attachment am-biguities and coordination ambiguities.We next investigate the features that were givenhigh weight by our learning algorithm (in the con-stituent parsing case).
We first threshold featuresby a minimum training count of 400 to focus onfrequently-firing ones (recall that our features arenot bilexical indicators and so are quite a bit moreArg Tag % Error Redn for Various Parent TagsNN IN: 18, NN: 23, VB: 30, NNP:20, VBN: 33IN NN: 11, VBD: 11, NNS: 20, VB:18, VBG: 23NNS IN: 9, VBD: 29, VBP: 21, VB:15, CC: 33Table 5: Error reduction for each type of parent attachment fora given child in Table 4.POShead POSarg Example (head, arg)RB IN back?
intoNN IN review?
ofNN DT The?
rateNNP IN Regulation?
ofVB NN limit?
accessVBD NN government?
clearedNNP NNP Dean?
IncNN TO ability?
toJJ IN active?
forNNS TO reasons?
toIN NN under?
pressureNNS IN reports?
onNN NNP Warner?
studioNNS JJ few?
plantsTable 6: The highest-weight features (thresholded at a count of400) of the affinity schema.
We list only the head and argu-ment POS and the direction (arrow from head to arg).
We omitfeatures involving punctuation.frequent).
We then sort them by descending (signed)weight.Table 6 shows which affinity features received thehighest weights, as well as examples of training setattachments for which the feature fired (for concrete-ness), suppressing both features involving punctua-tion and the features?
count and distance bins.
Withthe standard caveats that interpreting feature weightsin isolation is always to be taken for what it is,the first feature (RB?IN) indicates that high countsfor an adverb occurring adjacent to a preposition(like back into the spotlight) is a useful indicatorthat the adverb actually modifies that preposition.The second row (NN?IN) indicates that whether apreposition is appropriate to attach to a noun is wellcaptured by how often that preposition follows thatnoun.
The fifth row (VB?NN) indicates that whenconsidering an NP as the object of a verb, it is a goodsign if that NP?s head frequently occurs immediatelyfollowing that verb.
All of these features essentiallystate cases where local surface counts are good indi-699POShead mid-word POSarg Example (head, arg)VBN this IN leaned, fromVB this IN publish, inVBG him IN using, asVBG them IN joining, inVBD directly IN converted, intoVBD held IN was, inVBN jointly IN offered, byVBZ it IN passes, inVBG only IN consisting, ofVBN primarily IN developed, forVB us IN exempt, fromVBG this IN using, asVBD more IN looked, likeVB here IN stay, forVBN themselves IN launched, intoVBG down IN lying, onTable 7: The highest-weight features (thresholded at a count of400) of the mid-word schema for a verb head and prepositionargument (with head on left of argument).cators of (possibly non-adjacent) attachments.A subset of paraphrase features, which in theautomatically-extracted case don?t really correspondto paraphrases at all, are shown in Table 7.
Herewe show features for verbal heads and IN argu-ments.
The mid-words m which rank highly arethose where the occurrence of hma as an n-gramis a good indicator that a attaches to h (m of coursedoes not have to actually occur in the sentence).
In-terestingly, the top such features capture exactly theintuition from Nakov and Hearst (2005b), namelythat if the verb h and the preposition a occur witha pronoun in between, we have evidence that a at-taches to h (it certainly can?t attach to the pronoun).However, we also see other indicators that the prepo-sition is selected for by the verb, such as adverbs likedirectly.As another example of known useful featuresbeing learned automatically, Table 8 shows theprevious-context-word paraphrase features for anoun head and preposition argument (N ?
IN).Nakov and Hearst (2005b) suggested that the attes-tation of be N IN is a good indicator of attachment tothe noun (the IN cannot generally attach to forms ofauxiliaries).
One such feature occurs on this top list?
for the context word have ?
and others occur far-ther down.
We also find their surface marker / punc-bfr-word POShead POSarg Example (head, arg)second NN IN season, inThe NN IN role, ofstrong NN IN background, inour NNS IN representatives, inany NNS IN rights, againstA NN IN review, of: NNS IN Results, inthree NNS IN years, inIn NN IN return, forno NN IN argument, aboutcurrent NN IN head, ofno NNS IN plans, forpublic NN IN appearance, atfrom NNS IN sales, ofnet NN IN revenue, of, NNS IN names, ofyou NN IN leave, inhave NN IN time, forsome NN IN money, forannual NNS IN reports, onTable 8: The highest-weight features (thresholded at a count of400) of the before-word schema for a noun head and prepositionargument (with head on left of argument).tuation cues of : and , preceding the noun.
However,we additionally find other cues, most notably that ifthe N IN sequence occurs following a capitalized de-terminer, it tends to indicate a nominal attachment(in the n-gram, the preposition cannot attach left-ward to anything else because of the beginning ofthe sentence).In Table 9, we see the top-weight paraphrase fea-tures that had a conjunction as a middle-word cue.These features essentially say that if two heads w1and w2 occur in the direct coordination n-gram w1and w2, then they are good heads to coordinate (co-ordination unfortunately looks the same as comple-mentation or modification to a basic dependencymodel).
These features are relevant to a range ofcoordination ambiguities.Finally, Table 10 depicts the high-weight, high-count general paraphrase-cue features for arbitraryhead and argument categories, with those shownin previous tables suppressed.
Again, many inter-pretable features appear.
For example, the top entry(the JJ NNS) shows that when considering attachingan adjective a to a noun h, it is a good sign if the700POShead mid-CC POSarg Example (head, arg)NNS and NNS purchases, salesVB and VB buy, sellNN and NN president, officerNN and NNS public, mediaVBD and VBD said, addedVBZ and VBZ makes, distributesJJ and JJ deep, lastingIN and IN before, duringVBD and RB named, nowVBP and VBP offer, needTable 9: The highest-weight features (thresholded at a countof 400) of the mid-word schema where the mid-word was aconjunction.
For variety, for a given head-argument POS pair,we only list features corresponding to the and conjunction andh?
a direction.trigram the a h is frequent ?
in that trigram, the ad-jective attaches to the noun.
The second entry (NN- NN) shows that one noun is a good modifier ofanother if they frequently appear together hyphen-ated (another punctuation-based cue mentioned inprevious work on noun bracketing, see Nakov andHearst (2005a)).
While they were motivated on sep-arate grounds, these features can also compensatefor inapplicability of the affinity features.
For exam-ple, the third entry (VBD this NN) is a case whereeven if the head (a VBD like adopted) actually se-lects strongly for the argument (a NN like plan), thebigram adopted plan may not be as frequent as ex-pected, because it requires a determiner in its mini-mal analogous form adopted the plan.6 ConclusionWeb features are a way to bring evidence from alarge unlabeled corpus to bear on hard disambigua-tion decisions that are not easily resolvable based onlimited parser training data.
Our approach allows re-vealing features to be mined for the entire range ofattachment types and then aggregated and balancedin a full parsing setting.
Our results show that theseweb features resolve ambiguities not correctly han-dled by current state-of-the-art systems.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.
This research is sup-POSh POSa mid/bfr-word Example (h, a)NNS JJ b = the other?
thingsNN NN m = - auto?
makerVBD NN m = this adopted?
planNNS NN b = of computer?
productsNN DT m = current the?
proposalVBG IN b = of going?
intoNNS IN m = ?
clusters?
ofIN NN m = your In?
reviewTO VB b = used to?
easeVBZ NN m = that issue?
hasIN NNS m = two than?
minutesIN NN b = used as?
toolIN VBD m = they since?
wereVB TO b = will fail?
toTable 10: The high-weight high-count (thresholded at a count of2000) general features of the mid and before paraphrase schema(examples show head and arg in linear order with arrow fromhead to arg).ported by BBN under DARPA contract HR0011-06-C-0022.ReferencesM.
Atterer and H. Schutze.
2007.
Prepositional phraseattachment without oracles.
Computational Linguis-tics, 33(4):469476.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram corpus version 1.1.
LDC2006T13.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of ACL.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?70.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL.701Adam Kilgarriff.
2007.
Googleology is bad science.Computational Linguistics, 33(1).Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL.Mirella Lapata and Frank Keller.
2004.
The Web as abaseline: Evaluating the performance of unsupervisedWeb-based models for a range of NLP tasks.
In Pro-ceedings of HLT-NAACL.M.
Lauer.
1995.
Corpus statistics meet the noun com-pound: some empirical results.
In Proceedings ofACL.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of ACL-IJCNLP.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL.Preslav Nakov and Marti Hearst.
2005a.
Search en-gine statistics beyond the n-gram: Application to nouncompound bracketing.
In Proceedings of CoNLL.Preslav Nakov and Marti Hearst.
2005b.
Using the webas an implicit training set: Application to structuralambiguity resolution.
In Proceedings of EMNLP.Preslav Nakov and Marti Hearst.
2008.
Solving rela-tional similarity problems using the web as a corpus.In Proceedings of ACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proceedings ofCOLING-ACL.Emily Pitler, Shane Bergsma, Dekang Lin, , and KennethChurch.
2010.
Using web-scale n-grams to improvebase NP parsing performance.
In Proceedings of COL-ING.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofEMNLP.David Vadas and James R. Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proceedingsof ACL.Martin Volk.
2001.
Exploiting the WWW as a corpus toresolve PP attachment ambiguities.
In Proceedings ofCorpus Linguistics.Alexander Yates, Stefan Schoenmackers, and Oren Et-zioni.
2006.
Detecting parser errors using web-basedsemantic filters.
In Proceedings of EMNLP.702
