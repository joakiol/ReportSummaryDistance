Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 422?425,Prague, June 2007. c?2007 Association for Computational LinguisticsUPAR7: A knowledge-based system for headline sentiment taggingFran?ois-R?gis ChaumartinLattice/Talana ?
Universit?
Paris 730, rue du ch?teau des rentiers - 75013 Paris - Francefchaumartin@linguist.jussieu.fr / frc@proxem.comAbstractFor the Affective Text task at SemEval-2007, University Paris 7?s system firstevaluates emotion and valence on all wordsof a news headline (using enriched versionsof SentiWordNet and a subset of WordNet-Affect).
We use a parser to find the headword, considering that it has a major im-portance.
We also detect contrasts (be-tween positive and negative words) thatshift valence.
Our knowledge-based systemachieves high accuracy on emotion and va-lence annotation.
These results show thatworking with linguistic techniques and abroad-coverage lexicon is a viable ap-proach to sentiment analysis of headlines.1 Introduction1.1 ObjectivesThe detection of emotional connotations in texts isa recent task in computational linguistics.
Itseconomic stakes are promising; for example, acompany could detect, by analyzing the blo-gosphere, people?s opinion on its products.The goal of the SemEval task is to annotatenews headlines for emotions (using a predefinedlist: anger, disgust, fear, joy, sadness & surprise),and for valence (positive or negative).
A specificdifficulty here is related to the small number ofwords available for the analysis.1.2 Overall architectureOur system is mainly rule-based and uses a lin-guistic approach.
From a macroscopic point ofview, we follow the hypothesis that, in a news title,all the words potentially carry emotions.
If linguis-tic resources make it possible to detect these emo-tions individually, how can we deal with headlineswhere bad and good emotions appear at once?Our objective is to identify the expression whichcarries the main topic of the title.
One can considerthat this expression has a primary importance.We also seek to lay down rules for detectingspecific emotions.
For instance, surprise some-times comes from the contrast between good andbad news.
And sometimes, simple lexical elementsare characteristic of an emotion; a negation or amodal auxiliary in a title may be a relevant indica-tor of surprise.We describe here the techniques we imple-mented to address all these points.2 Components & resources usedThe system we employed for the Affective Textevaluation consists of the following components1:?
The SS-Tagger (a Part-of-Speech tagger)2,?
The Stanford Parser.We also used several lexical resources:?
WordNet version 2.1,?
A subset of WordNet-Affect,?
SentiWordNet.As the SS-Tagger is straightforward, we will notsay more about it here.
We will, however, discussthe remaining components and resources below.1  We used them through the Antelope NLP framework(www.proxem.com), which makes them easy to use.2 This fast PoS tagger uses an extension of Maximum EntropyMarkov Models.
See (Tsuruoka, Tsujii, 2005).4222.1 Choice of the Stanford ParserWe wished to use a syntactic parser for this task.We hesitated between two parsers producing adependency graph, the Link Grammar Parser(Sleator, Temperley, 1991) and the StanfordParser (Manning, Klein, 2002).As a news title is sometimes reduced to a nomi-nal group, without a verb, our experiments showedthat we should modify the title to make it ?gram-matically correct?.
Such a step is essential toobtain accurate results with a rule-based analyzersuch as the Link Grammar Parser.
On the otherhand, a statistical analyzer like the Stanford Parseris more tolerant with constructions which are notgrammatically correct.
That is why we chose it.2.2 WordNetWe used WordNet (Miller, 1995) as a semanticlexicon.
This well-known project, started in 1985at Princeton, offers a broad-coverage semanticnetwork of the English language, and is probablyone of the most popular NLP resources.In WordNet, words are grouped into sets ofsynonyms.
Various semantic relations exist be-tween these synsets (for example, hypernymy andhyponymy, antonymy, derivation?
).2.3 WordNet-AffectWordNet-Affect (Strapparava, Valitutti, 2004) is ahierarchy of ?affective domain labels?, with whichthe synsets representing affective concepts arefurther annotated.
We used the subset of WordNet-Affect provided as emotions lists by the SemEvalorganizers.
To improve it, we manually added tothe emotion lists new words that we found impor-tant on the task trial data.Nouns Verbs Adjectives AdverbsAnger 37 26 16 0Disgust 35 19 9 0Fear 71 26 20 4Joy 50 22 14 1Sadness 88 37 29 4Surprise 16 29 13 2Table 1: Counting of new words for each emotion.Nouns Verbs Adjectives AdverbscancerdangerpovertydemolishinjurekidnapcomatosenuclearviolentbloodydeadworseTable 2: Some words added for ?fear?
emotion.The synsets of emotions lists were considered asseeds; our system recursively propagated theiremotions to their neighbor synsets3.SentiWordNetSentiWordNet (Esuli, Sebastiani, 2006) describesitself as a lexical resource for opinion mining.SentiWordNet assigns to each synset of WordNetthree sentiment scores 4 : positivity, negativity,objectivity, the sum of which always equals 1.0.This resource has been created with a mix oflinguistics and statistics (using classifiers).
Theadvantage of this approach is to allow the auto-matic generation of emotion values for all thesynsets of WordNet.
The disadvantage is that, asall the results are not manually validated, someresulting classifications can appear incorrect5.We recursively propagate the positivity andnegativity values throughout neighbor synsets6.3 UPAR7 Affective Text system3.1 ?De-capitalization?
of common wordsA preliminary problem that we had to solve wasrelated to the Anglo-Saxon habit of putting initialcapital letters in all the words of a title.The first pass of our system thus detected newstitles that were ?improperly?
capitalized, and ?de-capitalizes?
their common words.For that, we used the SS-Tagger on the title; ac-cording to the part of speech of each word, infor-mation found in WordNet, and some hand-craftedrules7, the system chooses or not to keep the initial.The impact of this processing step is far fromnegligible, from the point of view of the StanfordParser.
Indeed, let us have a look at the differencebetween the parsing of the title, before (figure 1)and after (figure 2) this processing.3 Following relations such as Hyponym, Derivation, AdjectiveSimilar, Adjective Participle, Derivation and Pertainym.4 For instance, the synset ESTIMABLE#1 (deserving of respector high regard) has: Positivity = 0.75, Negativity = 0.00,Objectivity = 0.25.5  For example, RAPE#3 (the crime of forcing a woman tosubmit to sexual intercourse against her will) is classified withPositivity=0.25 and Negativity=0.0 despite the presence of theword ?crime?
in its gloss.6 Using WordNet?s relations such as Hyponym (for noun andverb), Antonym and Derivation.
For antonyms, positivity andnegativity values are exchanged.7 For instance, a word that cannot be any form of a WordNetlemma is probably a proper noun, and then we keep its initial.423Figure 1 : Output of the Stanford Parser with a title that is ?improperly?
capitalized.Figure 2 : Output of the Stanford Parser with a title that is ?properly?
capitalized.
(Words are tagged with the right part-of-speech, and dependencies are now correct.
)3.2 Individual words ratingFor the moment, we consider the output of theStanford Parser as an array of PoS-tagged words.We use WordNet?s morphology functions to findthe possible base form of each word.At this stage, an important question arose: waslexical disambiguation possible?
We thought not,because with short sentences, few relevant heuris-tics apply.
We chose another solution, by consider-ing that the emotion and valence values of a wordwere the linear combination of that of all its possi-ble meanings, balanced by the frequency of eachlemma.We detected emotion and valence values foreach word, by using our enriched version ofWordNet-Affect and SentiWordNet.In fact, we also detected some extra information:?
An additional 7th emotion, that looks like?compassion for people needing protection?.Our assumption is that certain words expressa subjacent need for protection.
For exam-ple, there is ?student?
behind ?school?, and?child?
behind ?adoption?.
So, we built a listof words designating something that needsprotection; we also include in this list wordssuch as ?troops?, ?shoppers???
We tried to detect acronyms relating totechnology; for this, we defined a list ofhigh-tech companies and a very basic regu-lar expression rule saying that a word (not inWordNet) containing numbers, or capitalsnot in first position, should be somethinghigh-tech.
(This very basic rule seems towork nicely on PS3, iPod, NASA?).
Weuse these high-tech indications to increasethe ?joy?
emotion.?
We counted lexical elements that we thinkare good indicators of surprise: negations,modal auxiliaries, question marks.At this stage, we begin some post-processing onindividual words.
Which factors cause anger ratherthat sadness?
We believe that human intention (toharm) causes the former emotion, while naturalfactors such as disease or climatic catastrophescause the latter.
So, we used a few rules related tothe WordNet noun hierarchy, based on the fact thatwhen a noun is a hyponym of a given synset, weboost some emotions:Does noun inherit from?
Emotions to boostUNHEALTHINESS Fear, sadnessATMOSPHERIC PHENOME-NONFear, sadnessAGGRESSION, HOSTILITY,WRONGFUL CONDUCTAnger, fear, sadness,disgustWEAPONRY, WEAPONSYSTEMAnger, fear, sadnessUNFORTUNATE PERSON Sadness, ?compassion?HUMAN WILL AngerTable 3: Hypernyms triggering an emotion boost.Then, the emotions found serve to update thevalence, by increasing positivity or negativity:Emotion Positivity NegativityJoy ++ --Anger, disgust, sadness,fear, ?compassion?-- ++Table 4: Emotions that change valence.4243.3 Global sentence ratingAt this stage, our system tries to find the mainsubject of the news title.
Again, we use the outputof the Stanford Parser, but this time, we make useof the dependency graph.
We consider that themain word is the root of the dependency graph, i.e.the word that is never a dependant word.
(Forinstance, in figure 2, the main word is ?predicts?.
)We think that the contribution of this main wordis much more important than that of the otherwords of the title8.
So, we multiply its individualvalence and emotion by 6.The last important part of linguistic processingis the detection of contrasts and accentuationsbetween ?good?
or ?bad?
things.
We search pat-terns like [noun?subject?verb] or [verb?directobject?noun] in the dependency graph, with verbsthat increase or decrease a quantity9 .
Using thevalence of the given noun, this gives our systemthe ability to detect very good news (?boosts(brain) power?)
or good news where somethingbad gets less important (?reduces risk?, ?slowsdecline?, ?hurricane weakens??
).4 ResultsCoase-grained Fine-grainedPearson Accuracy Precision RecallAnger 32.33 93.60 16.67 1.66Disgust 12.85 95.30 0.00 0.00Fear 44.92 87.90 33.33 2.54Joy 22.49 82.20 54.54 6.66Sadness 40.98 89.00 48.97 22.02Surprise 16.71 88.60 12.12 1.25Table 5: Results of the emotion annotation.Our rule-based system detects the six emotionsin news headlines with an average accuracy reach-ing 89.43% (coarse-grained evaluation).
However,recall is low.Coase-grained Fine-grainedPearson Accuracy Precision RecallValence 36.96 55.00 57.54 8.78Table 6: Results of the valence annotation.8 In sentences like ?study says?
?, ?scientists say?
?, ?policeaffirm?
?, the main head word is the verb of the relative.9 We ?rediscovered?
valence shifters (words that modify thesentiment expressed by a sentiment-bearing word, see (Po-lanyi and Zaenen, 2006)).The valence detection accuracy (55% in coarse-grained evaluation) is lower than in emotion anno-tation.
We attribute this difference to the fact that itis easier to detect emotions (that are given byindividual words) rather than valence, which needsa global understanding of the sentence.5 ConclusionEmotion and valence tagging is a complex andinteresting task.
For our first attempt, we designedand developed a linguistic rule-based system, usingWordNet, SentiWordNet and WordNet-Affectlexical resources, that delivers high accuracyresults.
In our future work, we will explore thepotential of simultaneously using a statisticalapproach, in order to improve recall of sentimentannotation.ReferencesAndrea Esuli, Fabrizio Sebastiani.
2006.
SentiWordNet:A Publicly Available Lexical Resource for OpinionMining.
Proceedings of LREC 2006, fifth interna-tional conference on Language Resources and Evalu-ation, pp.
417-422.Christopher Manning, Dan Klein.
2002.
Fast ExactInference with a Factored Model for Natural Lan-guage Parsing.
Advances in Neural InformationProcessing Systems 15 (NIPS 2002).George Miller.
1995.
WordNet: A lexical database.
Actsof ACM 38, 39-41.Livia Polanyi, Annie Zaenen.
2006.
Contextual ValenceShifters.
In J. G. Shanahan, Y. Qu, and J. Wiebe, edi-tors, Computing Attitude and Affect in Text: Theoryand Application.
Springer Verlag.Daniel Sleator, Davy Temperley.
1991.
Parsing Englishwith a Link Grammar.
Acts of Third InternationalWorkshop on Parsing Technologies.Carlo Strapparava, Alessandro Valitutti.
2004.
Word-Net-Affect: an affective extension of WordNet.
Pro-ceedings of the 4th International Conference on Lan-guage Resources and Evaluation (LREC 2004), pp.1083-1086.Yoshimasa Tsuruoka, Jun'ichi Tsujii.
2005.
Bidirec-tional Inference with the Easiest-First Strategy forTagging Sequence Data.
Proceedings ofHLT/EMNLP 2005, pp.
467-474.425
