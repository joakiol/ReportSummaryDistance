Proceedings of ACL-08: HLT, pages 897?904,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Cascaded Linear Model for Joint Chinese Word Segmentation andPart-of-Speech TaggingWenbin Jiang ?
Liang Huang ?
Qun Liu ?
Yajuan Lu?
?
?Key Lab.
of Intelligent Information Processing ?Department of Computer & Information ScienceInstitute of Computing Technology University of PennsylvaniaChinese Academy of Sciences Levine Hall, 3330 Walnut StreetP.O.
Box 2704, Beijing 100190, China Philadelphia, PA 19104, USAjiangwenbin@ict.ac.cn lhuang3@cis.upenn.eduAbstractWe propose a cascaded linear model forjoint Chinese word segmentation and part-of-speech tagging.
With a character-basedperceptron as the core, combined with real-valued features such as language models, thecascaded model is able to efficiently uti-lize knowledge sources that are inconvenientto incorporate into the perceptron directly.Experiments show that the cascaded modelachieves improved accuracies on both seg-mentation only and joint segmentation andpart-of-speech tagging.
On the Penn ChineseTreebank 5.0, we obtain an error reduction of18.5% on segmentation and 12% on joint seg-mentation and part-of-speech tagging over theperceptron-only baseline.1 IntroductionWord segmentation and part-of-speech (POS) tag-ging are important tasks in computer processing ofChinese and other Asian languages.
Several mod-els were introduced for these problems, for example,the Hidden Markov Model (HMM) (Rabiner, 1989),Maximum Entropy Model (ME) (Ratnaparkhi andAdwait, 1996), and Conditional Random Fields(CRFs) (Lafferty et al, 2001).
CRFs have the ad-vantage of flexibility in representing features com-pared to generative ones such as HMM, and usuallybehaves the best in the two tasks.
Another widelyused discriminative method is the perceptron algo-rithm (Collins, 2002), which achieves comparableperformance to CRFs with much faster training, sowe base this work on the perceptron.To segment and tag a character sequence, thereare two strategies to choose: performing POS tag-ging following segmentation; or joint segmentationand POS tagging (Joint S&T).
Since the typical ap-proach of discriminative models treats segmentationas a labelling problem by assigning each charactera boundary tag (Xue and Shen, 2003), Joint S&Tcan be conducted in a labelling fashion by expand-ing boundary tags to include POS information (Ngand Low, 2004).
Compared to performing segmen-tation and POS tagging one at a time, Joint S&T canachieve higher accuracy not only on segmentationbut also on POS tagging (Ng and Low, 2004).
Be-sides the usual character-based features, additionalfeatures dependent on POS?s or words can also beemployed to improve the performance.
However, assuch features are generated dynamically during thedecoding procedure, two limitation arise: on the onehand, the amount of parameters increases rapidly,which is apt to overfit on training corpus; on theother hand, exact inference by dynamic program-ming is intractable because the current predicationrelies on the results of prior predications.
As a result,many theoretically useful features such as higher-order word or POS n-grams are difficult to be in-corporated in the model efficiently.To cope with this problem, we propose a cascadedlinear model inspired by the log-linear model (Ochand Ney, 2004) widely used in statistical machinetranslation to incorporate different kinds of knowl-edge sources.
Shown in Figure 1, the cascadedmodel has a two-layer architecture, with a character-based perceptron as the core combined with otherreal-valued features such as language models.
We897CoreLinear Model(Perceptron)g1 =?i ?i ?
fi~?Outside-layerLinear ModelS = ?j wj ?
gj~wf1f2f|R|g1Word LM: g2 = Pwlm(W ) g2POS LM: g3 = Ptlm(T ) g3Labelling: g4 = P (T |W ) g4Generating: g5 = P (W |T ) g5Length: g6 = |W | g6SFigure 1: Structure of Cascaded Linear Model.
|R| denotes the scale of the feature space of the core perceptron.will describe it in detail in Section 4.
In this ar-chitecture, knowledge sources that are intractable toincorporate into the perceptron, can be easily incor-porated into the outside linear model.
In addition,as these knowledge sources are regarded as separatefeatures, we can train their corresponding models in-dependently with each other.
This is an interestingapproach when the training corpus is large as it re-duces the time and space consumption.
Experimentsshow that our cascaded model can utilize differentknowledge sources effectively and obtain accuracyimprovements on both segmentation and Joint S&T.2 Segmentation and POS TaggingGiven a Chinese character sequence:C1:n = C1 C2 .. Cnthe segmentation result can be depicted as:C1:e1 Ce1+1:e2 .. Cem?1+1:emwhile the segmentation and POS tagging result canbe depicted as:C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tmHere, Ci (i = 1..n) denotes Chinese character,ti (i = 1..m) denotes POS tag, and Cl:r (l ?
r)denotes character sequence ranges from Cl to Cr.We can see that segmentation and POS tagging taskis to divide a character sequence into several subse-quences and label each of them a POS tag.It is a better idea to perform segmentation andPOS tagging jointly in a uniform framework.
Ac-cording to Ng and Low (2004), the segmentationtask can be transformed to a tagging problem by as-signing each character a boundary tag of the follow-ing four types:?
b: the begin of the word?
m: the middle of the word?
e: the end of the word?
s: a single-character wordWe can extract segmentation result by splittingthe labelled result into subsequences of pattern s orbm?e which denote single-character word and multi-character word respectively.
In order to performPOS tagging at the same time, we expand boundarytags to include POS information by attaching a POSto the tail of a boundary tag as a postfix followingNg and Low (2004).
As each tag is now composedof a boundary part and a POS part, the joint S&Tproblem is transformed to a uniform boundary-POSlabelling problem.
A subsequence of boundary-POSlabelling result indicates a word with POS t only ifthe boundary tag sequence composed of its bound-ary part conforms to s or bm?e style, and all POStags in its POS part equal to t. For example, a tagsequence b NN m NN e NN represents a three-character word with POS tag NN .3 The PerceptronThe perceptron algorithm introduced into NLP byCollins (2002), is a simple but effective discrimina-tive training method.
It has comparable performance898Non-lexical-target InstancesCn (n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?CnCn+1 (n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?C?1C1 C?1C1=?/Lexical-target InstancesC0Cn (n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?C0CnCn+1 (n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?C0C?1C1 C0C?1C1 =U?/Table 1: Feature templates and instances.
Suppose we are considering the third character ?U?
in ?e?
U /?
?.to CRFs, while with much faster training.
The per-ceptron has been used in many NLP tasks, such asPOS tagging (Collins, 2002), Chinese word seg-mentation (Ng and Low, 2004; Zhang and Clark,2007) and so on.
We trained a character-based per-ceptron for Chinese Joint S&T, and found that theperceptron itself could achieve considerably high ac-curacy on segmentation and Joint S&T.
In followingsubsections, we describe the feature templates andthe perceptron training algorithm.3.1 Feature TemplatesThe feature templates we adopted are selected fromthose of Ng and Low (2004).
To compare with oth-ers conveniently, we excluded the ones forbidden bythe close test regulation of SIGHAN, for example,Pu(C0), indicating whether character C0 is a punc-tuation.All feature templates and their instances areshown in Table 1.
C represents a Chinese char-acter while the subscript of C indicates its posi-tion in the sentence relative to the current charac-ter (it has the subscript 0).
Templates immediatelyborrowed from Ng and Low (2004) are listed inthe upper column named non-lexical-target.
Wecalled them non-lexical-target because predicationsderived from them can predicate without consider-ing the current character C0.
Templates in the col-umn below are expanded from the upper ones.
Weadd a field C0 to each template in the upper col-umn, so that it can carry out predication accordingto not only the context but also the current char-acter itself.
As predications generated from suchtemplates depend on the current character, we namethese templates lexical-target.
Note that the tem-plates of Ng and Low (2004) have already con-tained some lexical-target ones.
With the two kindsAlgorithm 1 Perceptron training algorithm.1: Input: Training examples (xi, yi)2: ~??
03: for t?
1 .. T do4: for i?
1 .. N do5: zi ?
argmaxz?GEN(xi)?
(xi, z) ?
~?6: if zi 6= yi then7: ~??
~?
+?
(xi, yi)??
(xi, zi)8: Output: Parameters ~?of predications, the perceptron model will do exactpredicating to the best of its ability, and can backoff to approximately predicating if exact predicatingfails.3.2 Training AlgorithmWe adopt the perceptron training algorithm ofCollins (2002) to learn a discriminative model map-ping from inputs x ?
X to outputs y ?
Y , where Xis the set of sentences in the training corpus and Yis the set of corresponding labelled results.
Follow-ing Collins, we use a function GEN(x) generatingall candidate results of an input x , a representation?
mapping each training example (x, y) ?
X ?
Yto a feature vector ?
(x, y) ?
Rd, and a parametervector ~?
?
Rd corresponding to the feature vector.d means the dimension of the vector space, it equalsto the amount of features in the model.
For an inputcharacter sequence x, we aim to find an output F (x)satisfying:F (x) = argmaxy?GEN(x)?
(x, y) ?
~?
(1)?
(x, y) ?
~?
represents the inner product of featurevector ?
(x, y) and the parameter vector ~?.
We usedthe algorithm depicted in Algorithm 1 to tune theparameter vector ~?.899To alleviate overfitting on the training examples,we use the refinement strategy called ?averaged pa-rameters?
(Collins, 2002) to the algorithm in Algo-rithm 1.4 Cascaded Linear ModelIn theory, any useful knowledge can be incorporatedinto the perceptron directly, besides the character-based features already adopted.
Additional featuresmost widely used are related to word or POS n-grams.
However, such features are generated dy-namically during the decoding procedure so thatthe feature space enlarges much more rapidly.
Fig-ure 2 shows the growing tendency of feature spacewith the introduction of these features as well as thecharacter-based ones.
We noticed that the templatesrelated to word unigrams and bigrams bring to thefeature space an enlargement much rapider than thecharacter-base ones, not to mention the higher-ordergrams such as trigrams or 4-grams.
In addition, eventhough these higher grams were managed to be used,there still remains another problem: as the currentpredication relies on the results of prior ones, thedecoding procedure has to resort to approximate in-ference by maintaining a list of N -best candidates ateach predication position, which evokes a potentialrisk to depress the training.To alleviate the drawbacks, we propose a cas-caded linear model.
It has a two-layer architec-ture, with a perceptron as the core and another linearmodel as the outside-layer.
Instead of incorporat-ing all features into the perceptron directly, we firsttrained the perceptron using character-based fea-tures, and several other sub-models using additionalones such as word or POS n-grams, then trained theoutside-layer linear model using the outputs of thesesub-models, including the perceptron.
Since the per-ceptron is fixed during the second training step, thewhole training procedure need relative small timeand memory cost.The outside-layer linear model, similar to thosein SMT, can synthetically utilize different knowl-edge sources to conduct more accurate comparisonbetween candidates.
In this layer, each knowledgesource is treated as a feature with a correspondingweight denoting its relative importance.
Suppose wehave n features gj (j = 1..n) coupled with n corre-03000006000009000001.2e+0061.5e+0061.8e+0062.1e+0062.4e+0062.7e+0063e+0063.3e+0060  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22FeaturespaceIntroduction of featuresgrowing curveFigure 2: Feature space growing curve.
The horizontalscope X[i:j] denotes the introduction of different tem-plates.
X[0:5]: Cn (n = ?2..2); X[5:9]: CnCn+1 (n =?2..1); X[9:10]: C?1C1; X[10:15]: C0Cn (n =?2..2); X[15:19]: C0CnCn+1 (n = ?2..1); X[19:20]:C0C?1C1; X[20:21]: W0; X[21:22]: W?1W0.
W0 de-notes the current considering word, while W?1 denotesthe word in front of W0.
All the data are collected fromthe training procedure on MSR corpus of SIGHAN bake-off 2.sponding weights wj (j = 1..n), each feature gjgives a score gj(r) to a candidate r, then the totalscore of r is given by:S(r) =?j=1..nwj ?
gj(r) (2)The decoding procedure aims to find the candidater?
with the highest score:r?
= argmaxrS(r) (3)While the mission of the training procedure is totune the weights wj(j = 1..n) to guarantee that thecandidate r with the highest score happens to be thebest result with a high probability.As all the sub-models, including the perceptron,are regarded as separate features of the outside-layerlinear model, we can train them respectively withspecial algorithms.
In our experiments we traineda 3-gram word language model measuring the flu-ency of the segmentation result, a 4-gram POS lan-guage model functioning as the product of state-transition probabilities in HMM, and a word-POSco-occurrence model describing how much probablya word sequence coexists with a POS sequence.
Asshown in Figure 1, the character-based perceptron isused as the inside-layer linear model and sends itsoutput to the outside-layer.
Besides the output of theperceptron, the outside-layer also receive the outputs900of the word LM, the POS LM, the co-occurrencemodel and a word count penalty which is similar tothe translation length penalty in SMT.4.1 Language ModelLanguage model (LM) provides linguistic probabil-ities of a word sequence.
It is an important measureof fluency of the translation in SMT.
Formally, ann-gram word LM approximates the probability of aword sequence W = w1:m with the following prod-uct:Pwlm(W ) =m?i=1Pr(wi|wmax(0,i?n+1):i?1) (4)Similarly, the n-gram POS LM of a POS sequenceT = t1:m is:Ptlm(T ) =m?i=1Pr(ti|tmax(0,i?n+1):i?1) (5)Notice that a bi-gram POS LM functions as the prod-uct of transition probabilities in HMM.4.2 Word-POS Co-occurrence ModelGiven a training corpus with POS tags, we can traina word-POS co-occurrence model to approximatethe probability that the word sequence of the la-belled result co-exists with its corresponding POSsequence.
Using W = w1:m to denote the word se-quence, T = t1:m to denote the corresponding POSsequence, P (T |W ) to denote the probability that Wis labelled as T , and P (W |T ) to denote the prob-ability that T generates W , we can define the co-occurrence model as follows:Co(W,T ) = P (T |W )?wt ?
P (W |T )?tw (6)?wt and ?tw denote the corresponding weights of thetwo components.Suppose the conditional probability Pr(t|w) de-scribes the probability that the word w is labelled asthe POS t, while Pr(w|t) describes the probabilitythat the POS t generates the word w, then P (T |W )can be approximated by:P (T |W ) ?m?k=1Pr(tk|wk) (7)And P (W |T ) can be approximated by:P (W |T ) ?m?k=1Pr(wk|tk) (8)Pr(w|t) and Pr(t|w) can be easily acquired byMaximum Likelihood Estimates (MLE) over thecorpus.
For instance, if the word w appears N timesin training corpus and is labelled as POS t for ntimes, the probability Pr(t|w) can be estimated bythe formula below:Pr(t|w) ?
nN (9)The probability Pr(w|t) could be estimated throughthe same approach.To facilitate tuning the weights, we use two com-ponents of the co-occurrence model Co(W,T ) torepresent the co-occurrence probability of W and T ,rather than use Co(W,T ) itself.
In the rest of thepaper, we will call them labelling model and gener-ating model respectively.5 DecoderSequence segmentation and labelling problem canbe solved through a viterbi style decoding proce-dure.
In Chinese Joint S&T, the mission of the de-coder is to find the boundary-POS labelled sequencewith the highest score.
Given a Chinese charactersequence C1:n, the decoding procedure can proceedin a left-right fashion with a dynamic programmingapproach.
By maintaining a stack of size N at eachposition i of the sequence, we can preserve the top Nbest candidate labelled results of subsequence C1:iduring decoding.
At each position i, we enumer-ate all possible word-POS pairs by assigning eachPOS to each possible word formed from the charac-ter subsequence spanning length l = 1..min(i,K)(K is assigned 20 in all our experiments) and endingat position i, then we derive all candidate results byattaching each word-POS pair p (of length l) to thetail of each candidate result at the prior position of p(position i?
l), and select for position i a N -best listof candidate results from all these candidates.
Whenwe derive a candidate result from a word-POS pairp and a candidate q at prior position of p, we cal-culate the scores of the word LM, the POS LM, thelabelling probability and the generating probability,901Algorithm 2 Decoding algorithm.1: Input: character sequence C1:n2: for i?
1 .. n do3: L ?
?4: for l?
1 .. min(i, K) do5: w ?
Ci?l+1:i6: for t ?
POS do7: p?
label w as t8: for q ?
V[i?
l] do9: append D(q, p) to L10: sort L11: V[i]?
L[1 : N ]12: Output: n-best results V[n]as well as the score of the perceptron model.
In ad-dition, we add the score of the word count penalty asanother feature to alleviate the tendency of LMs tofavor shorter candidates.
By equation 2, we can syn-thetically evaluate all these scores to perform moreaccurately comparing between candidates.Algorithm 2 shows the decoding algorithm.Lines 3 ?
11 generate a N -best list for each char-acter position i.
Line 4 scans words of all possiblelengths l (l = 1..min(i,K), where i points to thecurrent considering character).
Line 6 enumeratesall POS?s for the word w spanning length l and end-ing at position i.
Line 8 considers each candidateresult in N -best list at prior position of the currentword.
Function D derives the candidate result fromthe word-POS pair p and the candidate q at prior po-sition of p.6 ExperimentsWe reported results from two set of experiments.The first was conducted to test the performance ofthe perceptron on segmentation on the corpus fromSIGHAN Bakeoff 2, including the Academia SinicaCorpus (AS), the Hong Kong City University Cor-pus (CityU), the Peking University Corpus (PKU)and the Microsoft Research Corpus (MSR).
The sec-ond was conducted on the Penn Chinese Treebank5.0 (CTB5.0) to test the performance of the cascadedmodel on segmentation and Joint S&T.
In all ex-periments, we use the averaged parameters for theperceptrons, and F-measure as the accuracy mea-sure.
With precision P and recall R, the balanceF-measure is defined as: F = 2PR/(P + R).0.9660.9680.970.9720.9740.9760.9780.980.9820.9840  1  2  3  4  5  6  7  8  9  10F-meassurenumber of iterationsPerceptron Learning CurveNon-lex + avgLex + avgFigure 3: Averaged perceptron learning curves with Non-lexical-target and Lexical-target feature templates.AS CityU PKU MSRSIGHAN best 0.952 0.943 0.950 0.964Zhang & Clark 0.946 0.951 0.945 0.972our model 0.954 0.958 0.940 0.975Table 2: F-measure on SIGHAN bakeoff 2.
SIGHANbest: best scores SIGHAN reported on the four corpus,cited from Zhang and Clark (2007).6.1 Experiments on SIGHAN BakeoffFor convenience of comparing with others, we focusonly on the close test, which means that any extraresource is forbidden except the designated train-ing corpus.
In order to test the performance of thelexical-target templates and meanwhile determinethe best iterations over the training corpus, we ran-domly chosen 2, 000 shorter sentences (less than 50words) as the development set and the rest as thetraining set (84, 294 sentences), then trained a per-ceptron model named NON-LEX using only non-lexical-target features and another named LEX us-ing both the two kinds of features.
Figure 3 showstheir learning curves depicting the F-measure on thedevelopment set after 1 to 10 training iterations.
Wefound that LEX outperforms NON-LEX with a mar-gin of about 0.002 at each iteration, and its learn-ing curve reaches a tableland at iteration 7.
Thenwe trained LEX on each of the four corpora for 7iterations.
Test results listed in Table 2 shows thatthis model obtains higher accuracy than the best ofSIGHAN Bakeoff 2 in three corpora (AS, CityUand MSR).
On the three corpora, it also outper-formed the word-based perceptron model of Zhangand Clark (2007).
However, the accuracy on PKUcorpus is obvious lower than the best score SIGHAN902Training setting Test task F-measurePOS- Segmentation 0.971POS+ Segmentation 0.973POS+ Joint S&T 0.925Table 3: F-measure on segmentation and Joint S&T ofperceptrons.
POS-: perceptron trained without POS,POS+: perceptron trained with POS.reported, we need to conduct further research on thisproblem.6.2 Experiments on CTB5.0We turned to experiments on CTB 5.0 to test the per-formance of the cascaded model.
According to theusual practice in syntactic analysis, we choose chap-ters 1?
260 (18074 sentences) as training set, chap-ter 271?
300 (348 sentences) as test set and chapter301?
325 (350 sentences) as development set.At the first step, we conducted a group of contrast-ing experiments on the core perceptron, the first con-centrated on the segmentation regardless of the POSinformation and reported the F-measure on segmen-tation only, while the second performed Joint S&Tusing POS information and reported the F-measureboth on segmentation and on Joint S&T.
Note thatthe accuracy of Joint S&T means that a word-POSpair is recognized only if both the boundary tags andthe POS?s are correctly labelled.The evaluation results are shown in Table 3.
Wefind that Joint S&T can also improve the segmen-tation accuracy.
However, the F-measure on JointS&T is obvious lower, about a rate of 95% to theF-measure on segmentation.
Similar trend appearedin experiments of Ng and Low (2004), where theyconducted experiments on CTB 3.0 and achieved F-measure 0.919 on Joint S&T, a ratio of 96% to theF-measure 0.952 on segmentation.As the next step, a group of experiments wereconducted to investigate how well the cascaded lin-ear model performs.
Here the core perceptron wasjust the POS+ model in experiments above.
Be-sides this perceptron, other sub-models are trainedand used as additional features of the outside-layerlinear model.
We used SRI Language ModellingToolkit (Stolcke and Andreas, 2002) to train a 3-gram word LM with modified Kneser-Ney smooth-ing (Chen and Goodman, 1998), and a 4-gram POSFeatures Segmentation F1 Joint S&T F1All 0.9785 0.9341All - PER 0.9049 0.8432All - WLM 0.9785 0.9340All - PLM 0.9752 0.9270All - GPR 0.9774 0.9329All - LPR 0.9765 0.9321All - LEN 0.9772 0.9325Table 4: Contribution of each feture.
ALL: all features,PER: perceptron model, WLM: word language model,PLM: POS language model, GPR: generating model,LPR: labelling model, LEN: word count penalty.LM with Witten-Bell smoothing, and we traineda word-POS co-occurrence model simply by MLEwithout smoothing.
To obtain their correspondingweights, we adapted the minimum-error-rate train-ing algorithm (Och, 2003) to train the outside-layermodel.
In order to inspect how much improvementeach feature brings into the cascaded model, everytime we removed a feature while retaining others,then retrained the model and tested its performanceon the test set.Table 4 shows experiments results.
We find thatthe cascaded model achieves a F-measure incrementof about 0.5 points on segmentation and about 0.9points on Joint S&T, over the perceptron-only modelPOS+.
We also find that the perceptron model func-tions as the kernel of the outside-layer linear model.Without the perceptron, the cascaded model (if wecan still call it ?cascaded?)
performs poorly on bothsegmentation and Joint S&T.
Among other features,the 4-gram POS LM plays the most important role,removing this feature causes F-measure decrementof 0.33 points on segmentation and 0.71 points onJoint S&T.
Another important feature is the labellingmodel.
Without it, the F-measure on segmentationand Joint S&T both suffer a decrement of 0.2 points.The generating model, which functions as that inHMM, brings an improvement of about 0.1 pointsto each test item.
However unlike the three fea-tures, the word LM brings very tiny improvement.We suppose that the character-based features usedin the perceptron play a similar role as the lower-order word LM, and it would be helpful if we traina higher-order word LM on a larger scale corpus.Finally, the word count penalty gives improvementto the cascaded model, 0.13 points on segmentation903and 0.16 points on Joint S&T.In summary, the cascaded model can utilize theseknowledge sources effectively, without causing thefeature space of the percptron becoming even larger.Experimental results show that, it achieves obviousimprovement over the perceptron-only model, aboutfrom 0.973 to 0.978 on segmentation, and from0.925 to 0.934 on Joint S&T, with error reductionsof 18.5% and 12% respectively.7 ConclusionsWe proposed a cascaded linear model for ChineseJoint S&T.
Under this model, many knowledgesources that may be intractable to be incorporatedinto the perceptron directly, can be utilized effec-tively in the outside-layer linear model.
This is asubstitute method to use both local and non-localfeatures, and it would be especially useful when thetraining corpus is very large.However, can the perceptron incorporate all theknowledge used in the outside-layer linear model?If this cascaded linear model were chosen, couldmore accurate generative models (LMs, word-POSco-occurrence model) be obtained by training onlarge scale corpus even if the corpus is not correctlylabelled entirely, or by self-training on raw corpus ina similar approach to that of McClosky (2006)?
Inaddition, all knowledge sources we used in the coreperceptron and the outside-layer linear model comefrom the training corpus, whereas many open knowl-edge sources (lexicon etc.)
can be used to improveperformance (Ng and Low, 2004).
How can we uti-lize these knowledge sources effectively?
We willinvestigate these problems in the following work.AcknowledgementThis work was done while L. H. was visitingCAS/ICT.
The authors were supported by NationalNatural Science Foundation of China, Contracts60736014 and 60573188, and 863 State Key ProjectNo.
2006AA010108 (W. J., Q. L., and Y. L.), and byNSF ITR EIA-0205456 (L. H.).
We would also liketo Hwee-Tou Ng for sharing his code, and Yang Liuand Yun Huang for suggestions.ReferencesStanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, USA.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of the 18th ICML, pages 282?289, Mas-sachusetts, USA.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proceedings of ACL 2006.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Proceedings of EMNLP.Franz Joseph Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30:417?449.Franz Joseph Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL2003, pages 160?167.Lawrence.
R. Rabiner.
1989.
A tutorial on hiddenmarkov models and selected applications in speechrecognition.
In Proceedings of IEEE, pages 257?286.Ratnaparkhi and Adwait.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of the EmpiricalMethods in Natural Language Processing Conference.Stolcke and Andreas.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 311?318.Nianwen Xue and Libin Shen.
2003.
Chinese word seg-mentation as lmr tagging.
In Proceedings of SIGHANWorkshop.Yue Zhang and Stephen Clark.
2007.
Chinese segmenta-tion with a word-based perceptron algorithm.
In Pro-ceedings of ACL 2007.904
