Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 138?143,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsFeatures for Detecting Hedge CuesNobuyuki ShimizuInformation Technology CenterThe University of Tokyoshimizu@r.dl.itc.u-tokyo.ac.jpHiroshi NakagawaInformation Technology CenterThe University of Tokyon3@dl.itc.u-tokyo.ac.jpAbstractWe present a sequential labeling approachto hedge cue detection submitted to the bi-ological portion of task 1 for the CoNLL-2010 shared task.
Our main approach isas follows.
We make use of partial syntac-tic information together with features ob-tained from the unlabeled corpus, and con-vert the task into one of sequential BIO-tagging.
If a cue is found, a sentence isclassified as uncertain and certain other-wise.
To examine a large number of fea-ture combinations, we employ a genetic al-gorithm.
While some features obtained bythis method are difficult to interpret, theywere shown to improve the performance ofthe final system.1 IntroductionResearch on automatically extracting factual in-formation from biomedical texts has become pop-ular in recent years.
Since these texts are abundantwith hypotheses postulated by researchers, onehurdle that an information extraction system mustovercome is to be able to determine whether or notthe information is part of a hypothesis or a factualstatement.
Thus, detecting hedge cues that indi-cate the uncertainty of the statement is an impor-tant subtask of information extraction (IE).
Hedgecues include words such as ?may?, ?might?, ?ap-pear?, ?suggest?, ?putative?
and ?or?.
They alsoincludes phrases such as ?.
.
.raising an intriguingquestion that.
.
.?
As these expressions are sparselyscattered throughout the texts, it is not easy to gen-eralize results of machine learning from a trainingset to a test set.
Furthermore, simply finding theexpressions listed above does not guarantee thata sentence contains a hedge.
Their function as ahedge cue depends on the surrounding context.The primary objective of the CoNLL-2010shared task (Farkas et al, 2010) is to detect hedgecues and their scopes as are present in biomedi-cal texts.
In this paper, we focus on the biologicalportion of task 1, and present a sequential labelingapproach to hedge cue detection.
The followingsummarizes the steps we took to achieve this goal.Similarly to previous work in hedge cue detec-tion (Morante and Daelemans, 2009), we first con-vert the task into a sequential labeling task basedon the BIO scheme, where each word in a hedgecue is labeled as B-CUE, I-CUE, or O, indicatingrespectively the labeled word is at the beginningof a cue, inside of a cue, or outside of a hedgecue; this is similar to the tagging scheme fromthe CoNLL-2001 shared task.
We then preparedfeatures, and fed the training data to a sequentiallabeling system, a discriminative Markov modelmuch like Conditional Random Fields (CRF), withthe difference being that the model parameters aretuned using Bayes Point Machines (BPM), andthen compared our model against an equivalentCRF model.
To convert the result of sequentiallabeling to sentence classification, we simply usedthe presence of a hedge cue, i.e.
if a cue is found, asentence is classified as uncertain and certain oth-erwise.To prepare features, we ran the GENIA tag-ger to add partial syntactic parse and named en-tity information.
We also applied Porter?s stem-mer (Jones and Willet, 1997) to each word in thecorpus.
For each stem, we acquired the distribu-tion of surrounding words from the unlabeled cor-pus, and calculated the similarity between thesedistributions and the distribution of hedge cues inthe training corpus.
Given a stem and its similari-ties to different hedge cues, we took the maximumsimilarity and discretized it.
All these features arepassed on to a sequential labeling system.
Usingthese base features, we then evaluated the effectsof feature combinations by repeatedly training thesystem and selecting feature combinations that in-creased the performance on a heldout set.
To au-138tomate this process, we employed a genetic algo-rithm.The contribution of this paper is two-fold.
First,we describe our system, outlined above, that wesubmitted to the CoNLL-2010 shared task in moredetail.
Second, we analyze the effects of partic-ular choices we made when building our system,especially the feature combinations and learningmethods.The rest of this paper is organized as follows.In Section 2, we detail how the task of sequentiallabeling is formalized in terms of linear classifi-cation, and explain the Viterbi algorithm requiredfor prediction.
We next present several algorithmsfor optimizing the weight vector in a linear classi-fier in Section 3.
We then detail the complete listof feature templates we used for the task of hedgecue detection in Section 4.
In order to evaluate theeffects of feature templates, in Section 5, we re-move each feature template and find that severalfeature templates overfit the training set.
We fi-nally conclude with Section 6.2 Sequential LabelingWe discriminatively train a Markov model us-ing Bayes Point Machines (BPM).
We will firstexplain linear classification, and then apply aMarkov assumption to the classification formal-ism.
Then we will move on to BPM.
Note thatwe assume all features are binary in this and up-coming sections as it is sufficient for the task athand.In the setting of sequential labeling, given theinput sequence x = (x1, x2, x3, ...xn), a systemis asked to produce the output sequence y =(y1, y2, y3, ...yn).
Considering that y is a class,sequential labeling is simply a classification witha very large number of classes.
Assuming that theproblem is one of linear classification, we may cre-ate a binary feature vector ?
(x) for an input x andhave a weight vector wy of the same dimensionfor each class y.
We choose a class y that has thehighest dot product between the input vector andthe weight vector for the class y.
For binary classi-fication, this process is very simple: compare twodot product values.
Learning is therefore reducedto specifying the weight vectors.To follow the standard notations in sequentiallabeling, let weight vectors wy be stacked intoone large vector w, and let ?
(x,y) be a binaryfeature vector such that w>?
(x,y) is equal tow>y ?(x).
Classification is to choose y such thaty = argmaxy?(w>?(x,y?
)).Unfortunately, a large number of classes createdout of sequences makes the problem intractable,so the Markov assumption factorizes y into a se-quence of labels, such that a label yi is affectedonly by the label before and after it (yi?1 and yi+1respectively) in the sequence.
Each structure, orlabel y is now associated with a set of the partsparts(y) such that y can be recomposed from theparts.
In the case of sequential labeling, parts con-sist of states yi and transitions yi ?
yi+1 betweenneighboring labels.
We assume that the featurevector for an entire structure y decomposes intoa sum over feature vectors for individual parts asfollows: ?
(x,y) =?r?parts(y) ?
(x, r).
Note thatwe have overloaded the symbol ?
to apply to eithera structure y or its parts r.The Markov assumption for factoring labels letsus use the Viterbi algorithm (much like a HiddenMarkov Model) in order to findy = argmaxy?
(w>?(x,y?
))= argmaxy?
(?nj=1w>?
(x, y?j)+?n?1j=1 w>?
(x, y?j ?
y?j+1)).3 OptimizationWe now turn to the optimization of the weight pa-rameter w. We compare three approaches ?
Per-ceptron, Bayes Point Machines and ConditionalRandom Fields, using our c++ library for struc-tured output prediction 1.Perceptron is an online update scheme thatleaves the weights unchanged when the predictedoutput matches the target, and changes them whenit does not.
The update is:wk := wk ?
?
(xi,y) + ?
(xi,yi).Despite its seemingly simple update scheme, per-ceptron is known for its effectiveness and perfor-mance (Collins, 2002).Conditional Random Fields (CRF) is a condi-tional modelP (y|x) = 1Zx exp(w>?
(x,y))where w is the weight for each feature and Zx is anormalization constant for each x.Zx =?yexp(w>?
(x,y))1Available at http://soplib.sourceforge.net/139for structured output prediction.
To fit the weightvector w using the training set {(xi,yi)}ni=1, weuse a standard gradient-descent method to find theweight vector that maximizes the log likelihood?ni logP (yi|xi) (Sha and Pereira, 2003).
Toavoid overfitting, the log likelihood is often pe-nalized with a spherical Gaussian weight prior:?ni logP (yi|xi) ?
C||w||2 .
We also evaluated thispenalized version, varying the trade-off parameterC.Bayes Point Machines (BPM) for structuredprediction (Corston-Oliver et al, 2006) is an en-semble learning algorithm that attempts to set theweight w to be the Bayes Point which approxi-mates to Bayesian inference for linear classifiers.Assuming a uniform prior distribution over w, werevise our belief of w after observing the trainingdata and produce a posterior distribution.
We cre-ate the final wbpm for classification using a poste-rior distribution as follows:wbpm = Ep(w|D)[w] =|V (D)|?i=1p(wi|D)wiwhere p(w|D) is the posterior distribution of theweights given the data D and Ep(w|D) is the ex-pectation taken with respect to this distribution.V (D) is the version space, which is the set ofweightswi that classify the training data correctly,and |V (D)| is the size of the version space.
Inpractice, to explore the version space of weightsconsistent with the training data, BPM trains a fewdifferent perceptrons (Collins, 2002) by shufflingthe samples.
The approximation of Bayes Pointwbpm is the average of these perceptron weights:wbpm = Ep(w|D)[w] ?K?k=11Kwk.The pseudocode of the algorithm is shown in Al-gorithm 3.1.
We see that the inner loop is simplya perceptron algorithm.4 Features4.1 Base FeaturesFor each sentence x, we have state features, rep-resented by a binary vector ?
(x, y?j) and transitionfeatures, again a binary vector ?
(x, y?j ?
y?j+1).For transition features, we do not utilize lexical-ized features.
Thus, each dimension of ?
(x, y?j ?Algorithm3.1: BPM(K,T, {(xi,yi)}ni=1)wbpm := 0;for k := 1 to KRandomly shuffle the sequential order ofsamples {(xi,yi)}ni=1wk := 0;for t := 1 to T # Perceptron iterationsfor i := 1 to n # Iterate shuffled samplesy := argmaxy?
(w>k ?(xi,y?
))if (y 6= yi)wk := wk ?
?
(xi,y) + ?
(xi,yi);wbpm := wbpm + 1Kwk;return (wbpm)y?j+1) is an indicator function that tests a com-bination of labels, for example, O?B-CUE, B-CUE?I-CUE or I-CUE?O.For state features ?
(x, y?j), the indicator func-tion for each dimension tests a combination ofy?j and lexical features obtained from x =(x1, x2, x3, ...xn).
We now list the base lexicalfeatures that were considered for this experiment.F 0 a token, which is usually a word.
As a part ofpreprocessing, words in each input sentenceare tokenized using the GENIA tagger 2.
Thistokenization coincides with Penn Treebankstyle tokenization 3.We add a subscript to indicate the position.
F 0j isexactly the input token xj .
From xj , we also createother lexical features such as F 1j , F 2j , F 3j , and soon.F 1 the token in lower case, with digits replacedby the symbol #.F 2 1 if the letters in the token are all capitalized,0 otherwise.F 3 1 if the token contains a digit, 0 otherwise.F 4 1 if the token contains an uppercase letter, 0otherwise.F 5 1 if the token contains a hyphen, 0 otherwise.2Available at: http:// www-tsujii.is.s.u-tokyo.ac.jp/ GE-NIA/ tagger/3A tokenizer is available at: http:// www.cis.upenn.edu/treebank/ tokenization.html140F 6 first letter in the token.F 7 first two letters in the token.F 8 first three letters in the token.F 9 last letter in the token.F 10 last two letters in the token.F 11 last three letters in the token.The features F 0 to F 11 are known to be usefulfor POS tagging.
We postulated that since mostfrequent hedge cues tend not to be nouns, thesefeatures might help identify them.The following three features are obtained byrunning the GENIA tagger.F 12 a part of speech.F 13 a CoNLL-2000 style shallow parse.
For ex-ample, B-NP or I-NP indicates that the tokenis a part of a base noun phrase, B-VP or I-VPindicates that it is part of a verb phrase.F 14 named entity, especially a protein name.F 15 a word stem by Porter?s stemmer 4.
Porter?sstemmer removes common morphologicaland inflectional endings from words in En-glish.
It is often used as part of an informa-tion retrieval system.Upon later inspection, it seems that Porter?sstemmer may be too aggressive in stemmingwords.
The word putative, for example, after be-ing processed by the stemmer, becomes simply put(which is clearly erroneous).The last nine types of features utilize the unla-beled corpus for the biological portion of sharedtask 1, provided by the shared task organizers.For each stem, we acquire a histogram of sur-rounding words, with a window size of 3, fromthe unlabeled corpus.
Each histogram is repre-sented as a vector; the similarity between his-tograms was then computed.
The similarity met-ric we used is called the Tanimoto coefficient, alsocalled extended/vector-based Jaccard coefficient.vi ?
vj||vi|| + ||vj || ?
vi ?
vjIt is based on the dot product of two vectors andreduces to Jaccard coefficient for binary features.4Available at: http://tartarus.org/ martin/PorterStemmer/This metric is known to perform quite well fornear-synonym discovery (Hagiwara et al, 2008).Given a stem and its similarities to different hedgecues, we took the maximum similarity and dis-cretized it.F 16 1 if similarity is bigger than 0.9, 0 otherwise....F 19 1 if similarity is bigger than 0.6, 0 otherwise....F 24 1 if similarity is bigger than 0.1, 0 otherwise.This concludes the base features we considered.4.2 Combinations of Base FeaturesIn order to discover combinations of base features,we implemented a genetic algorithm (Goldberg,1989).
It is an adaptive heuristic search algorithmbased on the evolutionary ideas of natural selec-tion and genetics.
After splitting the training setinto three partitions, given the first partition as thetraining set, the fitness is measured by the scoreof predicting the second partition.
We removedthe feature sets that did not score high, and intro-duced mutations ?
new feature sets ?
as replace-ments.
After several generations, surviving fea-ture sets performed quite well.
To avoid over fit-ting, occasionally feature sets were evaluated onthe third partition, and we finally chose the featureset according to this partition.The features of the submitted system are listedin Table 1.
Note that Table 1 shows the dimensionsof the feature vector that evaluate to 1 given x andy?j .
The actual feature vector is created by instan-tiating all the combinations in the table using thetraining set.Surprisingly, our genetic algorithm removedfeatures F 10 and F 11, the last two/three let-ters in a token.
It also removed the POS in-formation F 12, but kept the sequence of POStags F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3.
The reason forlonger sequences is due to our heuristics for muta-tions.
Occasionally, we allowed the genetic algo-rithm to insert a longer sequence of feature com-binations at once.
One other notable observationis that shallow parses and NEs are removed.
Be-tween the various thresholds from F 16 to F 24,it only kept F 19, discovering 0.6 as a similaritythreshold.141State ?
(x, y?j)y?jy?j , F 0j?2y?j , F 0j?1y?j , F 0jy?j , F 0j , F 19jy?j , F 0j?1, F 0j , F 0j+1, F 0j+2, F 0j+3, F 0j+4 ?
(1)y?j , F 0j+1y?j , F 0j+2y?j , F 1jy?j , F 2j ?
(2)y?j , F 3jy?j , F 4jy?j , F 4j?2, F 4j?1, F 4j , F 4j+1, F 4j+2y?j , F 5jy?j , F 5j , F 7j?1y?j , F 6jy?j , F 7jy?j , F 8jy?j , F 9j?1, F 9j , F 9j+1, F 9j+2, F 9j+3y?j , F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3y?j , F 15j , F 15j+1, F 15j+2, F 15j+3y?j , F 19j?2, F 19j?1, F 19j , F 19j+1, F 19j+2Table 1: Features for Sequential Labeling5 ExperimentsIn order to examine the effects of learning parame-ters, we conducted experiments on the test data af-ter it was released to the participants of the sharedtask.While BPM has two parameters, K and T , wefixed T = 5 and varied K, the number of percep-trons.
As increasing the number of perceptrons re-sults in more thorough exploration of the versionspace V (D), we expect that the performance ofthe classifier would improve as K increases.
Ta-ble 2 shows how the number of perceptrons affectsthe performance.TP stands for True Positive, FP for False Pos-itive, and FN for False Negative.
The evaluationmetrics were precision P (the number of true pos-K TP FP FN P (%) R (%) F1 (%)10 641 80 149 88.90 81.14 84.8420 644 79 146 89.07 81.52 85.1330 644 80 146 88.95 81.52 85.0740 645 81 145 88.84 81.65 85.0950 645 80 145 88.97 81.65 85.15Table 2: Effects of K in Bayes Point Machinesitives divided by the total number of elements la-beled as belonging to the positive class) recall R(the number of true positives divided by the to-tal number of elements that actually belong to thepositive class) and their harmonic mean, the F1score (F1 = 2PR/(P + R)).
All figures in thispaper measure hedge cue detection performance atthe sentence classification level, not word/phraseclassification level.
From the results, once thenumber of perceptrons hits 20, the performancestabilizes and does not seem to show any improve-ment.Next, in order to examine whether or not wehave overfitted to the training/heldout set, we re-moved each row of Table 1 and reevaluated theperformance of the system.
Reevaluation wasconducted on the labeled test set released by theshared task organizers after our system?s outputhad been initially evaluated.
Thus, these figuresare comparable to the sentence classification re-sults reported in Farkas et al (2010).TP FP FN P (%) R (%) F1 (%)1 647 79 143 89.12 81.90 85.362 647 80 143 89.00 81.90 85.301,2 647 81 143 88.87 81.90 85.24Table 3: Effects of removing features (1) or (2), orbothTable 3 shows the effect of removing (1), (2),or both (1) and (2), showing that they overfit thetraining data.
Removing any other rows in Ta-ble 1 resulted in decreased classification perfor-mance.
While there are other large combinationfeatures such as ones involving F 4, F 9, F 12, F 15and F 19, we find that they do help improving theperformance of the classifier.
Since these fea-tures seem unintuitive to the authors, it is likelythat they would not have been found without thegenetic algorithm we employed.
Error analysisshows that inclusion of features involving F 9 af-fects prediction of ?believe?, ?possible?, ?puta-tive?, ?assumed?, ?seemed?, ?if?, ?presumably?,?perhaps?, ?suggestion?, ?suppose?
and ?intrigu-ing?.
However, as this feature template is unfoldedinto a large number of features, we were unable toobtain further linguistic insights.In the following experiments, we used the cur-rently best performing features, that is, all fea-tures except (1) in Table 1, and trained the classi-fiers using the formalism of Perceptron and Con-ditional Random Fields besides Bayes Point Ma-142chines as we have been using.
The results in Table4 shows that BPM performs better than Percep-tron or Conditional Random Fields.
As the train-ing time for BPM is better than CRF, our choiceof BPM helped us to run the genetic algorithm re-peatedly as well.
After several runs of empiricaltuning and tweaking, the hyper-parameters of thealgorithms were set as follows.
Perceptron wasstopped at 40 iterations (T = 40).
For BPM, wefixed T = 5 and K = 20.
For Conditional Ran-dom Fields, we compared the penalized versionwith C = 1 and the unpenalized version (C = 0).The results in Table 4 is that of the unpenalizedversion, as it performed better than the penalizedversion.PerceptronTP FP FN P (%) R (%) F1 (%)671 128 119 83.98 84.94 84.46Conditional Random FieldsTP FP FN P (%) R (%) F1 (%)643 78 147 89.18 81.39 85.11Bayes Point MachinesTP FP FN P (%) R (%) F1 (%)647 79 143 89.12 81.90 85.36Table 4: Performance of different optimizationstrategies6 ConclusionTo tackle the hedge cue detection problem posedby the CoNLL-2010 shared task, we utilized aclassifier for sequential labeling following previ-ous work (Morante and Daelemans, 2009).
Anessential part of this task is to discover the fea-tures that allow us to predict unseen hedge expres-sions.
As hedge cue detection is semantic ratherthan syntactic in nature, useful features such asword stems tend to be specific to each word andhard to generalize.
However, by using a genetic al-gorithm to examine a large number of feature com-binations, we were able to find many features witha wide context window of up to 5 words.
Whilesome features are found to overfit, our analysisshows that a number of these features are success-fully applied to the test data yielding good general-ized performance.
Furthermore, we compared dif-ferent optimization schemes for structured outputprediction using our c++ library, freely availablefor download and use.
We find that Bayes PointMachines have a good trade-off between perfor-mance and training speed, justifying our repeatedusage of BPM in the genetic algorithm for featureselection.AcknowledgmentsThe authors would like to thank the reviewers fortheir comments.
This research was supported bythe Information Technology Center through theirgrant to the first author.
We would also like tothank Mr. Ono, Mr. Yonetsuji and Mr. Yamadafor their contributions to the library.ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof Empirical Methods in Natural Language Process-ing (EMNLP).Simon Corston-Oliver, Anthony Aue, Kevin Duh, andEric Ringger.
2006.
Multilingual dependency pars-ing using bayes point machines.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 160?167, June.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden.
ACL.David E. Goldberg.
1989.
Genetic Algorithmsin Search, Optimization, and Machine Learning.Addison-Wesley Professional.Masato Hagiwara, Yasuhiro Ogawa, and KatsuhikoToyama.
2008.
Context feature selection for distri-butional similarity.
In Proceedings of IJCNLP-08.Karen Spa?rk Jones and Peter Willet.
1997.
Readingsin Information Retrieval.
Morgan Kaufmann.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.In BioNLP ?09: Proceedings of the Workshop onBioNLP, pages 28?36.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceed-ings of the Human Language Technology Confer-ence (HLT).143
