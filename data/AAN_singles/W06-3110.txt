Proceedings of the Workshop on Statistical Machine Translation, pages 72?77,New York City, June 2006. c?2006 Association for Computational LinguisticsN -Gram Posterior Probabilities for Statistical Machine TranslationRichard Zens and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{zens,ney}@cs.rwth-aachen.deAbstractWord posterior probabilities are a com-mon approach for confidence estimationin automatic speech recognition and ma-chine translation.
We will generalize thisidea and introduce n-gram posterior prob-abilities and show how these can be usedto improve translation quality.
Addition-ally, we will introduce a sentence lengthmodel based on posterior probabilities.We will show significant improvements onthe Chinese-English NIST task.
The abso-lute improvements of the BLEU score isbetween 1.1% and 1.6%.1 IntroductionThe use of word posterior probabilities is a com-mon approach for confidence estimation in auto-matic speech recognition, e.g.
see (Wessel, 2002).This idea has been adopted to estimate confidencesfor machine translation, e.g.
see (Blatz et al, 2003;Ueffing et al, 2003; Blatz et al, 2004).
These confi-dence measures were used in the computer assistedtranslation (CAT) framework, e.g.
(Gandrabur andFoster, 2003).
The (simplified) idea is that the con-fidence measure is used to decide if the machine-generated prediction should be suggested to the hu-man translator or not.There is only few work on how to improvemachine translation performance using confidencemeasures.
The only work, we are aware of, is(Blatz et al, 2003).
The outcome was that the con-fidence measures did not result in improvements ofthe translation quality measured with the BLEU andNIST scores.
Here, we focus on how the ideas andmethods commonly used for confidence estimationcan be adapted and/or extended to improve transla-tion quality.So far, always word-level posterior probabilitieswere used.
Here, we will generalize this idea to n-grams.In addition to the n-gram posterior probabili-ties, we introduce a sentence-length model basedon posterior probabilities.
The common phrase-based translation systems, such as (Och et al, 1999;Koehn, 2004), do not use an explicit sentence lengthmodel.
Only the simple word penalty goes into thatdirection.
It can be adjusted to prefer longer orshorter translations.
Here, we will explicitly modelthe sentence length.The novel contributions of this work are to in-troduce n-gram posterior probabilities and sentencelength posterior probabilities.
Using these methods,we achieve significant improvements of translationquality.The remaining part of this paper is structured asfollows: first, we will briefly describe the baselinesystem, which is a state-of-the-art phrase-based sta-tistical machine translation system.
Then, in Sec-tion 3, we will introduce the n-gram posterior prob-abilities.
In Section 4, we will define the sentencelength model.
Afterwards, in Section 5, we willdescribe how these novel models can be used forrescoring/reranking.
The experimental results willbe presented in Section 6.
Future applications willbe described in Section 7.
Finally, we will concludein Section 8.722 Baseline SystemIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible tar-get language sentences, we will choose the sentencewith the highest probability:e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )} (1)The posterior probability Pr(eI1|fJ1 ) is modeled di-rectly using a log-linear combination of severalmodels (Och and Ney, 2002):Pr(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?I?,e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(2)The denominator is a normalization factor that de-pends only on the source sentence fJ1 .
Therefore,we can omit it during the search process.
As a deci-sion rule, we obtain:e?I?1 = argmaxI,eI1{ M?m=1?mhm(eI1, fJ1 )}(3)This approach is a generalization of the source-channel approach (Brown et al, 1990).
It has theadvantage that additional models h(?)
can be eas-ily integrated into the overall system.
The modelscaling factors ?M1 are trained with respect to the fi-nal translation quality measured by an error criterion(Och, 2003).We use a state-of-the-art phrase-based translationsystem as described in (Zens and Ney, 2004; Zenset al, 2005).
The baseline system includes the fol-lowing models: an n-gram language model, a phrasetranslation model and a word-based lexicon model.The latter two models are used for both directions:p(f |e) and p(e|f).
Additionally, we use a wordpenalty and a phrase penalty.3 N-Gram Posterior ProbabilitiesThe idea is similar to the word posterior probabili-ties: we sum the sentence posterior probabilities foreach occurrence of an n-gram.Let ?
(?, ?)
denote the Kronecker function.
Then,we define the fractional count C(en1 , fJ1 ) of an n-gram en1 for a source sentence fJ1 as:C(en1 , fJ1 ) =?I,e?I1I?n+1?i=1p(e?I1|fJ1 ) ?
?
(e?i+n?1i , en1 )(4)The sums over the target language sentences are lim-ited to an N -best list, i.e.
the N best translationcandidates according to the baseline model.
In thisequation, the term ?
(e?i+n?1i , en1 ) is one if and onlyif the n-gram en1 occurs in the target sentence e?I1starting at position i.Then, the posterior probability of an n-gram is ob-tained as:p(en1 |fJ1 ) =C(en1 , fJ1 )?e?n1C(e?n1 , fJ1 )(5)Note that the widely used word posterior proba-bility is obtained as a special case, namely if n is setto one.4 Sentence Length Posterior ProbabilityThe common phrase-based translation systems, suchas (Och et al, 1999; Koehn, 2004), do not use an ex-plicit sentence length model.
Only the simple wordpenalty goes into that direction.
It can be adjusted toprefer longer or shorter translations.Here, we will use the posterior probability of aspecific target sentence length I as length model:p(I|fJ1 ) =?eI1p(eI1|fJ1 ) (6)Note that the sum is carried out only over target sen-tences eI1 with the a specific length I .
Again, thecandidate target language sentences are limited to anN -best list.5 Rescoring/RerankingA straightforward application of the posterior prob-abilities is to use them as additional features ina rescoring/reranking approach (Och et al, 2004).The use of N -best lists in machine translation hasseveral advantages.
It alleviates the effects of thehuge search space which is represented in word73graphs by using a compact excerpt of the N best hy-potheses generated by the system.
N -best lists aresuitable for easily applying several rescoring tech-niques since the hypotheses are already fully gen-erated.
In comparison, word graph rescoring tech-niques need specialized tools which can traverse thegraph accordingly.The n-gram posterior probabilities can be usedsimilar to an n-gram language model:hn(fJ1 , eI1) =1I log( I?i=1p(ei|ei?1i?n+1, fJ1 ))(7)with:p(ei|ei?1i?n+1, fJ1 ) =C(eii?n+1, fJ1 )C(ei?1i?n+1, fJ1 )(8)Note that the models do not require smoothing aslong as they are applied to the same N -best list theyare trained on.If the models are used for unseen sentences,smoothing is important to avoid zero probabilities.We use a linear interpolation with weights ?n andthe smoothed (n ?
1)-gram model as generalizeddistribution.pn(ei|ei?1i?n+1, fJ1 ) = ?n ?C(eii?n+1, fJ1 )C(ei?1i?n+1, fJ1 )(9)+(1 ?
?n) ?
pn?1(ei|ei?1i?n+2, fJ1 )Note that absolute discounting techniques that areoften used in language modeling cannot be appliedin a straightforward way, because here we have frac-tional counts.The usage of the sentence length posterior prob-ability for rescoring is even simpler.
The resultingfeature is:hL(fJ1 , eI1) = log p(I|fJ1 ) (10)Again, the model does not require smoothing as longas it is applied to the same N -best list it is trainedon.
If it is applied to other sentences, smoothingbecomes important.
We propose to smooth the sen-tence length model with a Poisson distribution.p?
(I|fJ1 ) = ?
?p(I|fJ1 )+(1??)?
?I exp(??)I!
(11)We use a linear interpolation with weight ?.
Themean ?
of the Poisson distribution is chosen tobe identical to the mean of the unsmoothed lengthmodel:?
=?II ?
p(I|fJ1 ) (12)6 Experimental Results6.1 Corpus StatisticsThe experiments were carried out on the large datatrack of the Chinese-English NIST task.
The cor-pus statistics of the bilingual training corpus areshown in Table 1.
The language model was trainedon the English part of the bilingual training cor-pus and additional monolingual English data fromthe GigaWord corpus.
The total amount of lan-guage model training data was about 600M runningwords.
We use a fourgram language model withmodified Kneser-Ney smoothing as implemented inthe SRILM toolkit (Stolcke, 2002).To measure the translation quality, we use theBLEU score (Papineni et al, 2002) and the NISTscore (Doddington, 2002).
The BLEU score is thegeometric mean of the n-gram precision in com-bination with a brevity penalty for too short sen-tences.
The NIST score is the arithmetic mean ofa weighted n-gram precision in combination with abrevity penalty for too short sentences.
Both scoresare computed case-sensitive with respect to four ref-erence translations using the mteval-v11b tool1.
Asthe BLEU and NIST scores measure accuracy higherscores are better.We use the BLEU score as primary criterionwhich is optimized on the development set using theDownhill Simplex algorithm (Press et al, 2002).
Asdevelopment set, we use the NIST 2002 evaluationset.
Note that the baseline system is already well-tuned and would have obtained a high rank in thelast NIST evaluation (NIST, 2005).6.2 Translation ResultsThe translation results for the Chinese-English NISTtask are presented in Table 2.
We carried out experi-ments for evaluation sets of several years.
For theserescoring experiments, we use the 10 000 best trans-lation candidates, i.e.
N -best lists of size N=10 000.1http://www.nist.gov/speech/tests/mt/resources/scoring.htm74Table 1: Chinese-English NIST task: corpus statis-tics for the bilingual training data and the NIST eval-uation sets of the years 2002 to 2005.Chinese EnglishTrain Sentence Pairs 7MRunning Words 199M 213MVocabulary Size 223K 351KDictionary Entry Pairs 82KEval 2002 Sentences 878 3 512Running Words 25K 105K2003 Sentences 919 3 676Running Words 26K 122K2004 Sentences 1788 7 152Running Words 52K 245K2005 Sentences 1082 4 328Running Words 33K 148KUsing the 1-gram posterior probabilities, i.e.
theconventional word posterior probabilities, there isonly a very small improvement, or no improvementat all.
This is consistent with the findings of theJHU workshop on confidence estimation for statis-tical machine translation 2003 (Blatz et al, 2003),where the word-level confidence measures also didnot help to improve the BLEU or NIST scores.Successively adding higher order n-gram poste-rior probabilities, the translation quality improvesconsistently across all evaluation sets.
We alsoperformed experiments with n-gram orders beyondfour, but these did not result in further improve-ments.Adding the sentence length posterior probabilityfeature is also helpful for all evaluation sets.
For thedevelopment set, the overall improvement is 1.5%for the BLEU score.
On the blind evaluation sets,the overall improvement of the translation qualityranges between 1.1% and 1.6% BLEU.Some translation examples are shown in Table 3.7 Future ApplicationsWe have shown that the n-gram posterior probabil-ities are very useful in a rescoring/reranking frame-work.
In addition, there are several other potentialapplications.
In this section, we will describe two ofthem.7.1 Iterative SearchThe n-gram posterior probability can be used forrescoring as described in Section 5.
An alternative isto use them directly during the search.
In this secondsearch pass, we use the models from the first pass,i.e.
the baseline system, and additionally the n-gramand sentence length posterior probabilities.
As then-gram posterior probabilities are basically a kindof sentence-specific language model, it is straight-forward to integrate them.
This process can also beiterated.
Thus, using the N -best list of the secondpass to recompute the n-gram and sentence lengthposterior probabilities and do a third search pass,etc..7.2 Computer Assisted TranslationIn the computer assisted translation (CAT) frame-work, the goal is to improve the productivity of hu-man translators.
The machine translation systemtakes not only the current source language sentencebut also the already typed partial translation into ac-count.
Based on this information, the system suggestcompletions of the sentence.
Word-level posteriorprobabilities have been used to select the most ap-propriate completion of the system, for more detailssee e.g.
(Gandrabur and Foster, 2003; Ueffing andNey, 2005).
The n-gram based posterior probabili-ties as described in this work, might be better suitedfor this task as they explicitly model the dependencyon the previous words, i.e.
the given prefix.8 ConclusionsWe introduced n-gram and sentence length poste-rior probabilities and demonstrated their usefulnessfor rescoring purposes.
We performed systematicexperiments on the Chinese-English NIST task andshowed significant improvements of the translationquality.
The improvements were consistent amongseveral evaluation sets.An interesting property of the introduced meth-ods is that they do not require additional knowledgesources.
Thus the given knowledge sources are bet-ter exploited.
Our intuition is that the posterior mod-els prefer hypotheses with n-grams that are commonin the N -best list.The achieved results are promising.
Despite that,there are several ways to improve the approach.75Table 2: Case-sensitive translation results for several evaluation sets of the Chinese-English NIST task.Evaluation set 2002 (dev) 2003 2004 2005System NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]Baseline 8.49 30.5 8.04 29.5 8.14 29.0 8.01 28.2+ 1-grams 8.51 30.5 8.08 29.5 8.17 29.0 8.03 28.2+ 2-grams 8.47 30.8 8.03 29.7 8.12 29.2 7.98 28.1+ 3-grams 8.73 31.6 8.25 30.1 8.45 30.0 8.20 28.6+ 4-grams 8.74 31.7 8.26 30.1 8.47 30.1 8.20 28.6+ length 8.87 32.0 8.42 30.9 8.60 30.6 8.34 29.3Table 3: Translation examples for the Chinese-English NIST task.Baseline At present, there is no organization claimed the attack.Rescored At present, there is no organization claimed responsibility for the attack.Reference So far, no organization whatsoever has claimed responsibility for the attack.Baseline FIFA to severely punish football fraudRescored The International Football Federation (FIFA) will severely punish football?s deceptionReference FIFA will severely punish all cheating acts in the football fieldBaseline In more than three months of unrest, a total of more than 60 dead and 2000 injured.Rescored In more than three months of unrest, a total of more than 60 people were killed and morethan 2000 injured.Reference During the unrest that lasted more than three months, a total of more than 60 people diedand over 2,000 were wounded.For the decision rule in Equation 3, the modelscaling factors ?M1 can be multiplied with a constantfactor without changing the result.
This global fac-tor would affect the proposed posterior probabilities.So far, we have not tuned this parameter, but a properadjustment might result in further improvements.Currently, the posterior probabilities are com-puted on an N -best list.
Using word graphs insteadshould result in more reliable estimates, as the num-ber of hypotheses in a word graph is some orders ofa magnitude larger than in an N -best list.AcknowledgmentsThis material is partly based upon work supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023,and was partly funded by the European Union un-der the integrated project TC-STAR (Technologyand Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).ReferencesJ.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2003.
Confidence estimation for machine transla-tion.
Final report, JHU/CLSP Summer Workshop.http://www.clsp.jhu.edu/ws2003/groups/estimate/.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,A.
Kulesza, A. Sanchis, and N. Ueffing.
2004.
Con-fidence estimation for machine translation.
In Proc.20th Int.
Conf.
on Computational Linguistics (COL-ING), pages 315?321, Geneva, Switzerland, August.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85,June.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
ARPA Workshop on Human LanguageTechnology.S.
Gandrabur and G. Foster.
2003.
Confidence estima-tion for text prediction.
In Proc.
Conf.
on Natural Lan-76guage Learning (CoNLL), pages 95?102, Edmonton,Canada, May.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In 6th Conf.
of the Association for Machine Trans-lation in the Americas (AMTA 04), pages 115?124,Washington DC, September/October.NIST.
2005.
NIST 2005 machinetranslation evaluation official results.http://www.nist.gov/speech/tests/mt/mt05eval official results release20050801 v3.html, August.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 295?302, Philadelphia, PA, July.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proc.
Joint SIGDAT Conf.
on Empirical Methodsin Natural Language Processing and Very Large Cor-pora, pages 20?28, University of Maryland, CollegePark, MD, June.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In Proc.Human Language Technology Conf.
/ North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT-NAACL), pages 161?168,Boston,MA.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 311?318, Philadelphia, PA, July.W.
H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.Flannery.
2002.
Numerical Recipes in C++.
Cam-bridge University Press, Cambridge, UK.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Speech and Lan-guage Processing (ICSLP), volume 2, pages 901?904,Denver, CO, September.N.
Ueffing and H. Ney.
2005.
Application of word-level confidence measures in interactive statistical ma-chine translation.
In Proc.
of the 10th Annual Conf.
ofthe European Association for Machine Translation(EAMT), pages 262?270, Budapest, Hungary, May.N.
Ueffing, K. Macherey, and H. Ney.
2003.
Confi-dence Measures for Statistical Machine Translation.In Proc.
MT Summit IX, pages 394?401, New Orleans,LA, September.F.
Wessel.
2002.
Word Posterior Probabilities for LargeVocabulary Continuous Speech Recognition.
Ph.D.thesis, RWTH Aachen University, Aachen, Germany,January.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proc.
HumanLanguage Technology Conf.
/ North American Chapterof the Association for Computational Linguistics An-nual Meeting (HLT-NAACL), pages 257?264, Boston,MA, May.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHphrase-based statistical machine translation system.
InProceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 155?162, Pitts-burgh, PA, October.77
