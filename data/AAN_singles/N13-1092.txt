Proceedings of NAACL-HLT 2013, pages 758?764,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsPPDB: The Paraphrase DatabaseJuri Ganitkevitch1 Benjamin Van Durme1,2 Chris Callison-Burch2,31Center for Language and Speech Processing, Johns Hopkins University2Human Language Technology Center of Excellence, Johns Hopkins University3Computer and Information Science Department, University of PennsylvaniaAbstractWe present the 1.0 release of our para-phrase database, PPDB.
Its English portion,PPDB:Eng, contains over 220 million para-phrase pairs, consisting of 73 million phrasaland 8 million lexical paraphrases, as well as140 million paraphrase patterns, which cap-ture many meaning-preserving syntactic trans-formations.
The paraphrases are extractedfrom bilingual parallel corpora totaling over100 million sentence pairs and over 2 billionEnglish words.
We also release PPDB:Spa, acollection of 196 million Spanish paraphrases.Each paraphrase pair in PPDB contains aset of associated scores, including paraphraseprobabilities derived from the bitext data and avariety of monolingual distributional similar-ity scores computed from the Google n-gramsand the Annotated Gigaword corpus.
Our re-lease includes pruning tools that allow users todetermine their own precision/recall tradeoff.1 IntroductionParaphrases, i.e.
differing textual realizations of thesame meaning, have proven useful for a wide vari-ety of natural language processing applications.
Pastparaphrase collections include automatically derivedresources like DIRT (Lin and Pantel, 2001), theMSR paraphrase corpus and phrase table (Dolanet al 2004; Quirk et al 2004), among others.Although several groups have independently ex-tracted paraphrases using Bannard and Callison-Burch (2005)?s bilingual pivoting technique (seeZhou et al(2006), Riezler et al(2007), Snover etal.
(2010), among others), there has never been anofficial release of this resource.In this work, we release version 1.0 of the Para-Phrase DataBase PPDB,1 a collection of ranked En-glish and Spanish paraphrases derived by:?
Extracting lexical, phrasal, and syntactic para-phrases from large bilingual parallel corpora(with associated paraphrase probabilities).?
Computing distributional similarity scores foreach of the paraphrases using the Google n-grams and the Annotated Gigaword corpus.In addition to the paraphrase collection itself, weprovide tools to filter PPDB to only retain high pre-cision paraphrases, scripts to limit the collection tophrasal or lexical paraphrases (synonyms), and soft-ware that enables users to extract paraphrases forlanguages other than English.2 Extracting Paraphrases from BitextsTo extract paraphrases we follow Bannard andCallison-Burch (2005)?s bilingual pivoting method.The intuition is that two English strings e1 and e2that translate to the same foreign string f can be as-sumed to have the same meaning.
We can thus pivotover f and extract ?e1, e2?
as a pair of paraphrases,as illustrated in Figure 1.
The method extracts a di-verse set of paraphrases.
For thrown into jail, it ex-tracts arrested, detained, imprisoned, incarcerated,jailed, locked up, taken into custody, and throwninto prison, along with a set of incorrect/noisy para-phrases that have different syntactic types or that aredue to misalignments.For PPDB, we formulate our paraphrase collec-tion as a weighted synchronous context-free gram-mar (SCFG) (Aho and Ullman, 1972; Chiang, 2005)1Freely available at http://paraphrase.org.758... f?nf Landwirte , weil... 5 farmers were in Ireland ......oder wurden , gefoltertor have been , torturedfestgenommenthrown into jailfestgenommenimprisoned......
......Figure 1: Phrasal paraphrases are extracted via bilingualpivoting.with syntactic nonterminal labels, similar to Cohnand Lapata (2008) and Ganitkevitch et al(2011).An SCFG rule has the form:rdef= C ?
?f, e,?, ~?
?,where the left-hand side of the rule,C, is a nontermi-nal and the right-hand sides f and e are strings of ter-minal and nonterminal symbols.
There is a one-to-one correspondence, ?, between the nonterminalsin f and e: each nonterminal symbol in f has toalso appear in e. Following Zhao et al(2008), eachrule r is annotated with a vector of feature functions~?
= {?1...?N} which are combined in a log-linearmodel (with weights ~?)
to compute the cost of ap-plying r:cost(r) = ?N?i=1?i log?i.
(1)To create a syntactic paraphrase grammar wefirst extract a foreign-to-English translation gram-mar from a bilingual parallel corpus, using tech-niques from syntactic machine translation (Koehn,2010).
Then, for each pair of translation rules wherethe left-hand side C and foreign string f match:r1def= C ?
?f, e1,?1, ~?1?r2def= C ?
?f, e2,?2, ~?2?,we pivot over f to create a paraphrase rule rp:rpdef= C ?
?e1, e2,?p, ~?p?,with a combined nonterminal correspondency func-tion ?p.
Note that the common source side f im-plies that e1 and e2 share the same set of nonterminalsymbols.The paraphrase rules obtained using this methodare capable of making well-formed generalizationsof meaning-preserving rewrites in English.
Forinstance, we extract the following example para-phrase, capturing the English possessive rule:NP ?
the NP1 of NNS 2 | the NNS2 ?s NP1.The paraphrase feature vector ~?p is computedfrom the translation feature vectors ~?1 and ~?2 byfollowing the pivoting idea.
For instance, we esti-mate the conditional paraphrase probability p(e2|e1)by marginalizing over all shared foreign-languagetranslations f :p(e2|e1) ?
?fp(e2|f)p(f |e1).
(2)3 Scoring Paraphrases Using MonolingualDistributional SimilarityThe bilingual pivoting approach anchors para-phrases that share an interpretation because of ashared foreign phrase.
Paraphrasing methods basedon monolingual text corpora, like DIRT (Lin andPantel, 2001), measure the similarity of phrasesbased on distributional similarity.
This results in arange of different types of phrases, including para-phrases, inference rules and antonyms.
For instance,for thrown into prison DIRT extracts good para-phrases like arrested, detained, and jailed.
How-ever, it also extracts phrases that are temporarilyor causally related like began the trial of, crackeddown on, interrogated, prosecuted and ordered theexecution of, because they have similar distribu-tional properties.
Since bilingual pivoting rarely ex-tracts these non-paraphrases, we can use monolin-gual distributional similarity to re-rank paraphrasesextracted from bitexts (following Chan et al(2011))or incorporate a set of distributional similarity scoresas features in our log-linear model.Each similarity score relies on precomputed dis-tributional signatures that describe the contexts thata phrase occurs in.
To describe a phrase e, we gathercounts for a set of contextual features for each oc-currence of e in a corpus.
Writing the context vectorfor the i-th occurrence of e as ~se,i, we can aggre-gate over all occurrences of e, resulting in a distri-butional signature for e, ~se =?i ~se,i.
Following theintuition that phrases with similar meanings occur in759the long-termachieve25goals 23plans 97investment 10confirmed64revise43 the long-termthe long-termthe long-termthe long-termthe long-term....L-achieve = 25L-confirmed= 64L-revise = 43?R-goals= 23R-plans  = 97R-investment= 10?the long-term?=~sig?
(a) The n-gram corpus records the long-term as precededby revise (43 times), and followed by plans (97 times).
Weadd corresponding features to the phrase?s distributionalsignature retaining the counts of the original n-grams.long-term investment holding on todetamodtheJJ NN VBG IN TO DTNPPPVP?
?the long-term?=~sig?dep-det-R-investmentpos-L-TOpos-R-NNlex-R-investmentlex-L-todep-amod-R-investmentsyn-gov-NP syn-miss-L-NNlex-L-on-topos-L-IN-TOdep-det-R-NN dep-amod-R-NN(b) Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links , and featuresreflecting the phrase?s CCG-style label NP/NN are in-cluded in the context vector.Figure 2: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).similar contexts, we can then quantify the goodnessof e?
as a paraphrase of e by computing the cosinesimilarity between their distributional signatures:sim(e, e?)
= ~se ?
~se?|~se||~se?
|.A wide variety of features have been used to de-scribe the distributional context of a phrase.
Rich,linguistically informed feature-sets that rely on de-pendency and constituency parses, part-of-speechtags, or lemmatization have been proposed in worksuch as by Church and Hanks (1991) and Lin andPantel (2001).
For instance, a phrase is described bythe various syntactic relations such as: ?what verbshave this phrase as the subject?
?, or ?what adjectivesmodify this phrase??.
Other work has used simplern-gram features, e.g.
?what words or bigrams havewe seen to the left of this phrase??.
A substantialbody of work has focussed on using this type offeature-set for a variety of purposes in NLP (Lapataand Keller, 2005; Bhagat and Ravichandran, 2008;Lin et al 2010; Van Durme and Lall, 2010).For PPDB, we compute n-gram-based contextsignatures for the 200 million most frequent phrasesin the Google n-gram corpus (Brants and Franz,2006; Lin et al 2010), and richer linguistic signa-tures for 175 million phrases in the Annotated Gi-gaword corpus (Napoles et al 2012).
Our featuresextend beyond those previously used in the work byGanitkevitch et al(2012).
They are:?
n-gram based features for words seen to the leftand right of a phrase.?
Position-aware lexical, lemma-based, part-of-speech, and named entity class unigram and bi-gram features, drawn from a three-word win-dow to the right and left of the phrase.?
Incoming and outgoing (wrt.
the phrase) de-pendency link features, labeled with the corre-sponding lexical item, lemmata and POS.?
Syntactic features for any constituents govern-ing the phrase, as well as for CCG-style slashedconstituent labels for the phrase.Figure 2 illustrates the feature extraction for an ex-ample phrase.4 English Paraphrases ?
PPDB:EngWe combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn,2005), consisting of bitexts for the 19 European lan-guages, the 109 French-English corpus (Callison-Burch et al 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the UnitedNations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al 2006), Chinese and Arabic760Identity Paraphrases TotalLexical 0.6M 7.6M 8.1MPhrasal 4.9M 68.4M 73.2MSyntactic 46.5M 93.6M 140.1MAll 52.0M 169.6M 221.4MTable 1: A breakdown of PPDB:Eng size by paraphrasetype.
We distinguish lexical (i.e.
one-word) paraphrases,phrasal paraphrases and syntactically labeled paraphrasepatterns.newswire corpora used for the GALE machine trans-lation campaign,2 parallel Urdu-English data fromthe NIST translation task,3 the French portion ofthe OpenSubtitles corpus (Tiedemann, 2009), and acollection of Spanish-English translation memoriesprovided by TAUS.4The resulting composite parallel corpus has morethan 106 million sentence pairs, over 2 billion En-glish words, and spans 22 pivot languages.
To ap-ply the pivoting technique to this multilingual data,we treat the various pivot languages as a joint Non-English language.
This simplifying assumption al-lows us to share statistics across the different lan-guages and apply Equation 2 unaltered.Table 1 presents a breakdown of PPDB:Eng byparaphrase type.
We distinguish lexical (a singleword), phrasal (a continuous string of words), andsyntactic paraphrases (expressions that may con-tain both words and nonterminals), and separateout identity paraphrases.
While we list lexical andphrasal paraphrases separately, it is possible that asingle word paraphrases as a multi-word phrase andvice versa ?
so long they share the same syntacticlabel.5 Spanish Paraphrases ?
PPDB:SpaWe also release a collection of Spanish paraphrases:PPDB:Spa is extracted analogously to its Englishcounterpart and leverages the Spanish portions of thebitext data available to us, totaling almost 355 mil-lion Spanish words, in nearly 15 million sentencepairs.
The paraphrase pairs in PPDB:Spa are anno-2http://projects.ldc.upenn.edu/gale/data/Catalog.html3LDC Catalog No.
LDC2010T234http://www.translationautomation.com/Identity Paraphrases TotalLexical 1.0M 33.1M 34.1MPhrasal 4.3M 73.2M 77.5MSyntactic 29.4M 55.3M 84.7MAll 34.7M 161.6M 196.3MTable 2: An overview of PPDB:Spa.
Again, we parti-tion the resource into lexical (i.e.
one-word) paraphrases,phrasal paraphrases and syntactically labeled paraphrasepatterns.expectNNS VBPNPVPthe dataNP VPSto showJJeconomistsfew......S...RelArg0 Arg1Figure 3: To inspect our coverage, we use the PennTreebank?s parses to map from Propbank annotations toPPDB?s syntactic patterns.
For the above annotationpredicate, we extract VBP ?
expect, which is matchedby paraphrase rules like VBP ?
expect | anticipateand VBP ?
expect | hypothesize.
To search forthe entire relation, we replace the argument spanswith syntactic nonterminals.
Here, we obtain S ?NP expect S, for which PPDB has matching rules likeS ?
NP expect S | NP would hope S, and S ?NP expect S | NP trust S. This allows us to apply so-phisticated paraphrases to the predicate while capturingits arguments in a generalized fashion.tated with distributional similarity scores based onlexical features collected from the Spanish portionof the multilingual release of the Google n-gramcorpus (Brants and Franz, 2009), and the SpanishGigaword corpus (Mendonca et al 2009).
Table 2gives a breakdown of PPDB:Spa.6 AnalysisTo estimate the usefulness of PPDB as a resourcefor tasks like semantic role labeling or parsing, weanalyze its coverage of Propbank predicates andpredicate-argument tuples (Kingsbury and Palmer,2002).
We use the Penn Treebank (Marcus etal., 1993) to map Propbank annotations to patternswhich allow us to search PPDB:Eng for paraphrasesthat match the annotated predicate.
Figure 3 illus-7611 3 5-30 -25 -20 -15 -10 -5  0Avg.
Score Pruning Threshold 0 0.5 1-30 -25 -20 -15 -10 -5  0  0 50 100 150Coverage PP / Type(a) PPDB:Eng coverage of Propbank predicates(top), and average human judgment score (bottom)for varying pruning thresholds.0 0.2 0.4 0.6 0.8 1-30 -25 -20 -15 -10 -5  0  0 20 40 60 80 100 120 140 160Coverage Paraphrases / TypePruning ThresholdRelation Tokens CoveredParaphrases / TypeRelation Types Covered(b) PPDB:Eng?s coverage of Propbank predicateswith up to two arguments.
Here we consider rulesthat paraphrase the full predicate-argument expres-sion.Figure 4: An illustration of PPDB?s coverage of the manually annotated Propbank predicate phrases (4a) and binaryrelations with argument non-terminals (4b).
The curves indicate the coverage on tokens (solid) and types (dotted), aswell as the average number of paraphrases per covered type (dashed) at the given pruning level.trates this mapping.In order to quantify PPDB?s precision-recalltradeoff in this context, we perform a sweepover our collection, beginning with the full set ofparaphrase pairs and incrementally discarding thelowest-scoring ones.
We choose a simple estimatefor each paraphrase pair?s score by uniformly com-bining its paraphrase probability features in Eq.
1.The top graph in Figure 4a shows PPDB?s cover-age of predicates (e.g.
VBP ?
expect) at the typelevel (i.e.
counting distinct predicates), as well asthe token level (i.e.
counting predicate occurrencesin the corpus).
We also keep track of average num-ber of paraphrases per covered predicate type forvarying pruning levels.
We find that PPDB has apredicate type recall of up to 52% (accounting for97.5% of tokens).
Extending the experiment to fullpredicate-argument relations with up to two argu-ments (e.g.
S ?
NNS expect S), we obtain a 27%type coverage rate that accounts for 40% of tokens(Figure 4b).
Both rates hold even as we prune thedatabase down to only contain high precision para-phrases.
Our pruning method here is based on a sim-ple uniform combination of paraphrase probabilitiesand similarity scores.To gauge the quality of our paraphrases, the au-thors judged 1900 randomly sampled predicate para-phrases on a scale of 1 to 5, 5 being the best.
Thebottom graph in Figure 4a plots the resulting humanscore average against the sweep used in the cover-age experiment.
It is clear that even with a simpleweighing approach, the PPDB scores show a clearcorrelation with human judgements.
Therefore theycan be used to bias the collection towards greater re-call or higher precision.7 Conclusion and Future WorkWe present the 1.0 release of PPDB:Eng andPPDB:Spa, two large-scale collections of para-phrases in English and Spanish.
We illustrate theresource?s utility with an analysis of its coverage ofPropbank predicates.
Our results suggest that PPDBwill be useful in a variety of NLP applications.Future releases of PPDB will focus on expand-ing the paraphrase collection?s coverage with regardto both data size and languages supported.
Further-more, we intend to improve paraphrase scoring byincorporating additional sources of information, aswell as by better utilizing information present in thedata, like domain or topic.
We will also addresspoints of refinement such as handling of phrase am-biguity, and effects specific to individual pivot lan-guages.
Our aim is for PPDB to be a continuouslyupdated and improving resource.Finally, we will explore extensions to PPDB to in-clude aspects of related large-scale resources such aslexical-semantic hierarchies (Snow et al 2006), tex-tual inference rules (Berant et al 2011), relationalpatterns (Nakashole et al 2012), and (lexical) con-ceptual networks (Navigli and Ponzetto, 2012).762AcknowledgementsWe would like to thank Frank Ferraro for his Prop-bank processing tools.
This material is basedon research sponsored by the NSF under grantIIS-1249516 and DARPA under agreement num-ber FA8750-13-2-0017 (the DEFT program).
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Governmental purposes.
Theviews and conclusions contained in this publicationare those of the authors and should not be interpretedas representing official policies or endorsements ofDARPA or the U.S. Government.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice Hall.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Jonathan Berant, Jacob Goldberger, and Ido Dagan.2011.
Global learning of typed entailment rules.
InProceedings of ACL.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL/HLT.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.Thorsten Brants and Alex Franz.
2009.
Web 1T 5-gram,10 european languages version 1.
Linguistic DataConsortium, Philadelphia.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
In Pro-ceedings of WMT, pages 1?28, Athens, Greece, March.Tsz Ping Chan, Chris Callison-Burch, and Benjamin VanDurme.
2011.
Reranking bilingually extracted para-phrases using monolingual distributional similarity.
InEMNLP Workshop on GEMS.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.Kenneth Church and Patrick Hanks.
1991.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 6(1):22?29.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings of theCOLING.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of the COLING.Andreas Eisele and Yu Chen.
2010.
MultiUN: A multi-lingual corpus from united nation documents.
In Pro-ceedings of LREC, Valletta, Malta.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2012.
Monolingual distributionalsimilarity for text-to-text generation.
In Proceedingsof *SEM.
Association for Computational Linguistics.Paul Kingsbury and Martha Palmer.
2002.
From tree-bank to propbank.
In Proceedings of LREC.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of WMT, Prague, Czech Repub-lic, June.
Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing, 2(1).Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of LREC.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the Penn Treebank.
ComputationalLinguistics, 19(2).Angelo Mendonca, David Andrew Graff, and DeniseDiPersio.
2009.
Spanish Gigaword Second Edition.Linguistic Data Consortium.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
PATTY: a taxonomy of rela-tional patterns with semantic types.
In Proceedingsof EMNLP.Courtney Napoles, Matt Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In Proceedingsof AKBC-WEKEX 2012.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of EMNLP.763Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of the 45th Annual Meeting ofthe ACL.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2010.
Ter-plus: paraphrase, se-mantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2-3):117?127.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of the ACL/Coling.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?nielVarga.
2006.
The JRC-Acquis: A multilingualaligned parallel corpus with 20+ languages.
In Pro-ceedings of LREC, Genoa, Italy.Jo?rg Tiedemann.
2009.
News from OPUS: A collectionof multilingual parallel corpora with tools and inter-faces.
In Recent Advances in Natural Language Pro-cessing, volume 5.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of ACL, Short Papers.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and ShengLi.
2008.
Combining multiple resources to improveSMT-based paraphrasing model.
In Proceedings ofACL/HLT.Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof HLT/NAACL.764
