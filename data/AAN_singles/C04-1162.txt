PageRank on Semantic Networks,with Application to Word Sense DisambiguationRada Mihalcea, Paul Tarau, Elizabeth FigaUniversity of North TexasDallas, TX, USArada@cs.unt.edu, tarau@unt.edu, efiga@unt.eduAbstractThis paper presents a new open text word sensedisambiguation method that combines the useof logical inferences with PageRank-style algo-rithms applied on graphs extracted from natu-ral language documents.
We evaluate the ac-curacy of the proposed algorithm on severalsense-annotated texts, and show that it consis-tently outperforms the accuracy of other pre-viously proposed knowledge-based word sensedisambiguation methods.
We also explore andevaluate methods that combine several open-textword sense disambiguation algorithms.1 IntroductionGoogle?s PageRank link-analysis algorithm (Brinand Page, 1998), and variants like Kleinberg?s HITSalgorithm (Kleinberg, 1999), have been used for an-alyzing the link-structure of the World Wide Webto provide global, content independent ranking ofWeb pages.
Arguably, PageRank can be singledout as a key element of the paradigm-shift Googlehas triggered in the field of Web search technol-ogy, by providing a Web page ranking mechanismthat relies on the collective knowledge of Web ar-chitects rather than content analysis of individualWeb pages.
In short, PageRank is a way of decid-ing on the importance of a vertex within a graph, bytaking into account global information recursivelycomputed from the entire graph, rather than relyingonly on local vertex-specific information.
Apply-ing a similar line of thinking to lexical and semanticknowledge graphs like WordNet (Miller, 1995) sug-gests using the implicit knowledge incorporated intheir link structure for language processing applica-tions, where knowledge drawn from an entire textcan be used in making local ranking/selection deci-sions.In this paper, we explore the applicability ofPageRank to semantic networks, and show that suchgraph-based ranking algorithms can be successfullyused in language processing applications.
In partic-ular, we propose and experiment with a new unsu-pervised knowledge-based word sense disambigua-tion algorithm, which succeeds in identifying thesense of all words in open text with a precisionsignificantly higher than other previously proposedknowledge-based algorithms.The paper is organized as follows.
Section 2 re-views the problem of word sense disambiguation,and surveys related work.
Section 3 briefly describesthe PageRank algorithm, and shows how this algo-rithm can be adapted to the WordNet graph.
Sec-tion 4 introduces the PageRank-based word sensedisambiguation algorithm.
Combinations with otherknown algorithms are explored in Section 5.
Athorough empirical evaluation of the proposed algo-rithms on several sense-annotated texts is providedin section 6.2 Open Text Word Sense DisambiguationThe task of word sense disambiguation consists ofassigning the most appropriate meaning to a poly-semous word within a given context.
Applicationssuch as machine translation, knowledge acquisition,common sense reasoning, and others, require knowl-edge about word meanings, and word sense disam-biguation is considered essential for all these appli-cations.Most of the efforts in solving this problemwere concentrated so far toward targeted supervisedlearning, where each sense tagged occurrence of aparticular word is transformed into a feature vector,which is then used in an automatic learning process.The applicability of such supervised algorithms ishowever limited only to those few words for whichsense tagged data is available, and their accuracyis strongly connected to the amount of labeled dataavailable at hand.Instead, open-text knowledge-based approacheshave received significantly less attention1 .
While theperformance of such methods is usually exceeded bytheir supervised corpus-based alternatives, they havehowever the advantage of providing larger coverage.1We use the term knowledge-based to denote methods thatinvolve logical inferences and derivation of global propertiesthat extend the data in a dictionary and/or a corpus with newknowledge.
In our definition of knowledge-based approaches,the use of a corpus is not excluded.Knowledge-based methods for word sense disam-biguation are usually applicable to all words in opentext, while supervised corpus-based techniques tar-get only few selected words for which large corporaare made available.
Four main types of knowledge-based methods have been developed so far for wordsense disambiguation.Lesk algorithms.
First introduced by (Lesk,1986), these algorithms attempt to identify the mostlikely meanings for the words in a given contextbased on a measure of contextual overlap betweenthe dictionary definitions of the ambiguous words,or between the current context and dictionary defi-nitions provided for a given target word.Semantic similarity.
Measures of semantic simi-larity computed on semantic networks (Rada et al,1989).
Depending on the size of the context theyspan, these measures are in turn divided into twomain categories:(1) Local context ?
where the semantic measures areused to disambiguate words additionally connectedby syntactic relations (Stetina et al, 1998).
(2) Global context ?
where the semantic measuresare employed to derive lexical chains, which arethreads of meaning often drawn throughout an en-tire text (Morris and Hirst, 1991).Selectional preferences.
Automatically or semi-automatically acquired selectional preferences, asmeans for constraining the number of possiblesenses that a word might have, based on the relationit has with other words in context (Resnik, 1997).Heuristic-based methods.
These methods consistof simple rules that can reliably assign a sense tocertain word categories: one sense per collocation(Yarowsky, 1993), and one sense per discourse (Galeet al, 1992).In this paper, we propose a new open-text dis-ambiguation algorithm that combines informationdrawn from a semantic network (WordNet) withgraph-based ranking algorithms (PageRank).
Wecompare our method with other open-text wordsense disambiguation algorithms, and show that theaccuracy achieved through our new PageRank-basedmethod exceeds the performance obtained by otherknowledge-based methods.3 PageRank on Semantic NetworksIn this section, we briefly describe PageRank (Brinand Page, 1998), and describe the view of WordNetas a graph, which facilitates the application of thegraph-based ranking algorithm on this semantic net-work.3.1 The PageRank AlgorithmIterative graph-based ranking algorithms are essen-tially a way of deciding the importance of a vertexwithin a graph; in the context of search engines, itis a way of deciding how important a page is on theWeb.
In this model, when one vertex links to anotherone, it is casting a vote for that other vertex.
Thehigher the number of votes that are cast for a vertex,the higher the importance of the vertex.
Moreover,the importance of the vertex casting the vote deter-mines how important the vote itself is, and this in-formation is also taken into account by the rankingmodel.
Hence, the score associated with a vertex isdetermined based on the votes that are cast for it, andthe score of the vertices casting these votes.Let G = (V,E) be a directed graph with theset of vertices V and set of edges E, where E is asubset of V ?
V .
For a given vertex Vi, let In(Vi)be the set of vertices that point to it, and let Out(Vi)be the set of edges going out of vertex Vi.
ThePageRank score of vertex Vi is defined as follows:S(Vi) = (1 ?
d) + d ?
?j?In(Vi)S(Vj)|Out(Vj)|where d is a damping factor that can be set between0 and 1 2.Starting from arbitrary values assigned to eachnode in the graph, the PageRank computation it-erates until convergence below a given thresholdis achieved.
After running the algorithm, a fastin-place sorting algorithm is applied to the rankedgraph vertices to sort them in decreasing order.PageRank can be also applied on undirectedgraphs, in which case the out-degree of a vertex isequal to the in-degree of the vertex, and convergenceis usually achieved after a fewer number of itera-tions.3.2 WordNet as a GraphWordNet is a lexical knowledge base for Englishthat defines words, meanings, and relations betweenthem.
The basic unit in WordNet is a synset, whichis a set of synonym words or word phrases, andrepresents a concept.
WordNet defines several se-mantic relations between synsets, including ISArelations (hypernym/hyponym), PART-OF relations(meronym/holonym), entailment, and others.To represent WordNet as a graph, we use aninstance-centric data representation, which defines2The role of the damping factor d is to incorporate into thePageRank model the probability of jumping from a given ver-tex to another random vertex in the graph.
In the context ofWeb surfing, PageRank implements the ?random surfer model?,where a user clicks on links at random with a probability d, andjumps to a completely new page with probability 1 ?
d. Thefactor d is usually set at 0.85 (Brin and Page, 1998), and this isthe value we are also using in our implementation.synsets as vertices, and relations or sets of relationsas edges.
The graph can be constructed as an undi-rected graph, with no orientation defined for edges,or as a directed graph, in which case a direction is ar-bitrarily established for each relation (e.g.
hyponym?
hypernym).Given a subset of the WordNet synsets, as iden-tified in a given text or by other selectional crite-ria, and given a semantic relation, a graph is con-structed by identifying all the synsets (vertices) inthe given subset that can be linked by the given rela-tion (edges).
Relations can be also combined, for in-stance a graph can be constructed so that it accountsfor both the ISA and the PART-OF relations betweenthe vertices in the graph.4 PageRank-based Word SenseDisambiguationIn this section, we describe a new unsupervisedopen-text word sense disambiguation algorithm thatrelies on PageRank-style algorithms applied on se-mantic networks.4.1 Building the Text Synset GraphTo enable the application of PageRank-style algo-rithms to the disambiguation of all words in opentext, we have to build a graph that represents the textand interconnects the words with meaningful rela-tions.Since no a-priori semantic information is avail-able for the words in the text, we start with the as-sumption that every possible sense of a word is apotentially correct sense, and therefore all senses forall words are to be included in the initial search set.The synsets pertaining to all word senses form there-fore the vertices of the graph.
The edges between thenodes are drawn using synset relations available inWordNet, either explicitly encoded in the network,or derived by various means (see Sections 4.2, 4.3).Note that not all WordNet arcs are suitable forcombination with PageRank, as they sometimesidentify competing word senses which tend to sharetargets of incoming or outgoing links.
As our ob-jective is to differentiate between senses, we want tofocus on specific rather than shared links.
We calltwo synsets colexical if they represent two senses ofthe same word ?
that is, if they share one identicallexical unit.
For a given word or word phrase, colex-ical synsets will be listed as competing senses, fromwhich a given disambiguation algorithm should se-lect one.To ensure that colexical synsets do not ?contam-inate?
each other?s PageRank values, we have tomake sure that they are not linked together, andhence they compete through disjoint sets of links.This means that relations between synsets pertainingto various senses of the same word or word phraseare not added to the graph.
Consider for instancethe verb travel: it has six senses defined in Word-Net, with senses 2 and 3 linked by an ISA relation(travel#2 ISA travel#3).
Since the synsets pertain-ing to these two senses are colexical (they share thelexical unit travel), this ISA link is not added to thetext graph.4.2 Basic Semantic RelationsWordNet explicitly encodes a set of basic se-mantic relations, including hypernymy, hyponymy,meronymy, holonymy, entailment, causality, at-tribute, pertainimy.
WordNet 2.0 has also introducednominalizations ?
which link verbs and nouns per-taining to the same semantic class, and domain links?
a first step toward the classification of synsets,based on the ?ontology?
in which a given synset isrelevant to.
While the domain relations usually adda small number of links, their use tends to help fo-cusing on a dominant field which was observed tohelp the disambiguation process.4.3 Derived Semantic RelationsTwo or more basic WordNet relations can be com-bined together to form a new relation.
For in-stance, we can combine hypernymy and hyponymyto obtain the coordinate relation ?
which identifiessynsets that share the same hypernym.
For example,dog#1 and wolf#1 are coordinates, since they sharethe same hypernym canine#1.It is worth mentioning the composite relationxlink, which is a new global relation that we define,which integrates all the basic relations (nominaliza-tions and domain links included) and the coordinaterelation.
Shortly, two synsets are connected by anxlink relation if any WordNet-defined relation or acoordinate relation can be identified between them.4.4 The PageRank Disambiguation AlgorithmThe input to the disambiguation algorithm consistsof raw text.
The output is a text with word mean-ing annotations for all open-class words.
Given asemantic relation SR, which can be a basic or com-posite relation, the algorithm consists of the follow-ing main steps:Step 1: Preprocessing.During preprocessing, the text is tokenized and an-notated with parts of speech.
Collocations are iden-tified using a sliding window approach, where a col-location is considered to be a sequence of wordsthat forms a compound concept defined in WordNet.Named entities are also identified at this stage.Step 2: Graph construction.Build the text synset graph: for all open class wordsin the text, identify all synsets defined in Word-Net, and add them as vertices in the graph.
Wordspreviously assigned with a named entity tag, andmodal/auxiliary verbs are not considered.
For thegiven semantic relation SR, add an edge betweenall vertices in the graph that can be linked by therelation SR.Step 3: PageRank.Assign an initial small value to each vertex in thegraph.
Iterate the PageRank computation until itconverges - usually for 25-30 iterations.
In our im-plementation, vertices are initially assigned with avalue of 1.
Notice that the final values obtained af-ter PageRank runs to completion are not affected bythe choice of the initial value, only the number ofiterations to convergence may be different.Step 4: Assign word meanings.For each ambiguous word in the text, find thesynset that has the highest PageRank score, whichis uniquely identifying the sense of the word.
Ifnone of the synsets corresponding to the meaningsof a word could be connected with other synsets inthe graph using the given relation SR, the word isassigned with a random sense (when the WordNetsense order is not considered), or with the first sensein WordNet (when a sense order is available).The algorithm can be run on the entire text atonce, in which case the resulting graph is fairly large?
usually more than two thousands vertices ?
andhas high connectivity.
Alternatively, it can be runon smaller sections of the text, and in this case thegraphs have lower number of vertices and lower con-nectivity.
In the experiments reported in this paper,we are using the first option, since it results in richersynset graphs and ensures that most of the words areassigned a meaning using the PageRank sense dis-ambiguation algorithm.5 Related AlgorithmsWe overview in this section two other word sensedisambiguation algorithms that address all words inopen text: Lesk algorithm, and the most frequentsense algorithm3 .
We also propose two new hybridalgorithms that combine the PageRank word sensedisambiguation method with the Lesk algorithm andthe most frequent sense algorithm.5.1 The Lesk algorithmThe Lesk algorithm (Lesk, 1986) is one of the firstalgorithms used for the semantic disambiguation ofall words in open text.
The only resource requiredby the algorithm is a set of dictionary entries, one foreach possible word sense, and knowledge about theimmediate context where the sense disambiguationis performed.3The reason for choosing these algorithms over the othermethods mentioned in section 2 is the fact that they address allopen class words in a text.The main idea behind the original definition ofthe algorithm is to disambiguate words by findingthe overlap among their sense definitions.
Namely,given two words, W1 and W2, each with NW1 andNW2 senses defined in a dictionary, for each pos-sible sense pair W i1 and Wj2 , i=1..NW1, j=1..NW2,first determine their definitions overlap, by countingthe number of words they have in common.
Next,the sense pair with the highest overlap is selected,and consequently a sense is assigned to each of thetwo words involved in the initial pair.When applied to open text, the original defini-tion of the algorithm faces an explosion of wordsense combinations4 , and alternative solutions arerequired.
One solution is to use simulated anneal-ing, as proposed in (Cowie et al, 1992).
Anothersolution ?
which we adopt in our experiments ?
isto use a variation of the Lesk algorithm (Kilgarriffand Rosenzweig, 2000), where meanings of wordsin the text are determined individually, by findingthe highest overlap between the sense definitions ofeach word and the current context.
Rather than seek-ing to simultaneously determine the meanings of allwords in a given text, this approach determines wordsenses individually, and therefore it avoids the com-binatorial explosion of senses.5.2 Most Frequent SenseWordNet keeps track of the frequency of each wordmeaning within a sense-annotated corpus.
Thisintroduces an additional knowledge-element thatcan significantly improve the disambiguation perfor-mance.A very simple algorithm that relies on this infor-mation consists of picking the most frequent sensefor any given word as the correct one.
Given thatsense frequency distributions tend to decrease expo-nentially for less frequent senses, this guess usuallyoutperforms methods that use exclusively the con-tent of the document and associated dictionary in-formation.5.3 Combining PageRank and LeskWhen combining two different algorithms, we haveto ensure that their effects accumulate without dis-turbing each algorithms internal workings.The PageRank+Lesk algorithm consists in pro-viding a default ordering by Lesk (possibly aftershuffling WordNet senses to remove the sense fre-quency bias), and then applying PageRank, which4Consider for instance the text ?I saw a man who is 108years old and can still walk and tell jokes?, with nine open classwords, each with several possible senses : see(26), man(11),year(4), old(8), can(5), still(4), walk(10), tell(8), joke(3).
Giventhe total of 43,929,600 possible sense combinations, finding theoptimal combination using definition overlaps is not a tractableapproach.Size(words) Random Lesk PageRank PageRank+LeskSEMCORlaw 825 37.12% 39.62% 46.42% 49.36%sports 808 29.95% 33.00% 40.59% 46.18%education 898 37.63% 41.33% 46.88% 52.00%debates 799 40.17% 42.38% 47.80% 50.52%entertainment 802 39.27% 43.05% 43.89% 49.31%AVERAGE 826 36.82% 39.87% 45.11% 49.47%SENSEVAL-2d00 471 28.97% 43.94% 43.94% 47.77%d01 784 45.47% 52.65% 54.46% 57.39%d02 514 39.24% 49.61% 54.28% 56.42%AVERAGE 590 37.89% 48.73% 50.89% 53.86%AVERAGE (ALL) 740 37.22% 43.19% 47.27% 51.16%Table 1: Word Sense Disambiguation accuracy for PageRank, Lesk, PageRank+Lesk, and Random (no senseorder)will eventually reorder the senses.
With this ap-proach, senses that have similar PageRank valueswill keep their Lesk ordering.
As PageRank over-rides Lesk one can notice that in this case we pri-oritize PageRank, which tends to outperform Lesk.The resulting algorithm provides a combinationwhich improves over both algorithms individually,as shown in Section 6.5.4 Combining PageRank with the SenseFrequencyThe combination of PageRank with the WordNetsense frequency information is done in two steps:?
introduce the WordNet frequency ordering by re-moving the random permutation of senses?
use a formula which combines PageRank and ac-tual WordNet sense frequency informationWhile a simple product of the two ranks alreadyprovides an improvement over both algorithms thefollowing formula which prioritizes the first senseprovides the best results:Rank ={ 4?
FR ?
PR if N = 1FR ?
PR if N > 1where FR represents the WordNet sense frequency,PR represents the rank computed by PageRank, Nis the position in the frequency ordered synset list,and Rank represents the combined rank.6 Experimental EvaluationWe evaluate the accuracy of the word sense dis-ambiguation algorithms on a benchmark of sense-annotated texts, in which each open-class word ismapped to the meaning selected by a lexicographeras being the most appropriate one in the context ofa sentence.
We are using a subset of the SemCortexts (Miller et al, 1993) ?
five randomly selectedfiles covering different topics: news, sports, enter-tainment, law, and debates ?
as well as the dataset provided for the English all words task duringSENSEVAL-2.The average size of a file is 600-800 open classwords.
On each file, we run two sets of evaluations.
(1) One set consisting of the basic ?uninformed?version of the knowledge-based algorithms, wherethe sense ordering provided by the dictionary is nottaken into account at any point.
(2) A second set ofexperiments consisting of ?informed?
disambigua-tion algorithms, which incorporate the sense orderrendered by the dictionary.6.1 Uninformed AlgorithmsGiven that word senses are ordered in WordNet bydecreasing frequency of their occurrence in largesense annotated data, we explicitly remove this or-dering by applying a random permutation of thesenses with uniform distribution.
This randomiza-tion step ensures that any eventual bias introducedby the sense ordering is removed, and it enables us toevaluate the impact of the disambiguation algorithmwhen no information about sense frequency is avail-able.
In this setting, the following dictionary-basedalgorithms are evaluated and compared: PageRank,Lesk, combined PageRank-Lesk, and the randombaseline:PageRank.
The algorithm introduced in this paper,which selects the most likely sense of a word basedon the PageRank score assigned to the synsets cor-responding to the given word within the text graph.While experiments were performed using all seman-tic relations listed in Sections 4.2 and 4.3, we reporthere on the results obtained with the xlink relation,which was found to perform best as compared toother semantic relations.Lesk.
We are also experimenting with the Lesk al-gorithm described in section 5.1, which decides onthe correct sense of a word based on the highestSize(words) MFS Lesk PageRank PageRank+LeskSEMCORlaw 825 69.09% 72.65% 73.21% 73.97%sports 808 57.30% 64.21% 68.31% 68.31%education 898 64.03% 69.33% 71.65% 71.53%debates 799 66.33% 70.07% 71.14% 71.67%entertainment 802 59.72% 64.98% 66.02% 66.16%AVERAGE 826 63.24% 68.24% 70.06% 70.32%SENSEVAL-2d00 471 51.70% 53.07% 58.17% 57.74%d01 784 60.80% 64.28% 67.85% 68.11%d02 514 55.97% 62.84% 63.81% 64.39%AVERAGE 590 56.15% 60.06% 63.27% 63.41%AVERAGE (ALL) 740 60.58% 65.17% 67.51% 67.72%Table 2: Word Sense Disambiguation accuracy for PageRank, Lesk, PageRank+Lesk, and Most FrequentSense (WordNet sense order integrated)overlap between the dictionary sense definitions andthe context where the word occurs.PageRank + Lesk.
The PageRank and Lesk algo-rithms can be combined into one hybrid algorithm,as described in section 5.3.
First, we order the sensesbased on the score assigned by the the Lesk algo-rithm, and then apply PageRank on this reorderedset of senses.Random.
Finally, we are running a very simplesense annotation algorithm, which assigns a randomsense to each word in the text, and which representsa baseline for this set of ?uninformed?
word sensedisambiguation algorithms.Table 1 lists the disambiguation precision ob-tained by each of these algorithms on the evalua-tion benchmark.
On average, PageRank gives an ac-curacy of 47.27%, which brings a significant 7.7%error reduction with respect to the Lesk algorithm,and 19.0% error reduction over the random baseline.The best performance is achieved by a combinedPageRank and Lesk algorithm: 51.16% accuracy,which brings a 28.5% error reduction with respectto the random baseline.
Notice that all these algo-rithms rely exclusively on information drawn fromdictionaries, and do not require any information onsense frequency, which makes them highly portableto other languages.6.2 Informed AlgorithmsIn a second set of experiments, we allow the dis-ambiguation algorithms to incorporate the sense or-der provided by WordNet.
While this class ofalgorithms is informed by the use of global fre-quency information, it does not use any specificcorpus annotations and therefore it leans in grayarea between supervised and unsupervised methods.We are again evaluating four different algorithms:PageRank, Lesk, combined PageRank ?
Lesk, and abaseline consisting of assigning by default the mostfrequent sense.PageRank.
The PageRank-based algorithm intro-duced in this paper, combined with the WordNetsense frequency, as described in Section 5.4.Lesk.
The Lesk algorithm described in section 5.1,applied on an ordered set of senses.
This meansthat words that have two or more senses with a sim-ilar score identified by Lesk, will keep the WordNetsense ordering.PageRank + Lesk.
A hybrid algorithm, that com-bines PageRank, Lesk, and the dictionary sense or-der.
This algorithm consists of the method describedin Section 5.3, applied on the ordered set of senses.Most frequent sense.
Finally, we are running a sim-ple ?informed?
sense annotation algorithm, whichassigns by default the most frequent sense to eachword in the text (i.e.
sense number one in WordNet).Table 2 lists the accuracy obtained by each ofthese informed algorithms on the same benchmark.Again, the PageRank algorithm exceeds the otherknowledge-based algorithms by a significant mar-gin: it brings an error rate reduction of 21.3% withrespect to the most frequent sense baseline, and a7.2% error reduction over the Lesk algorithm.
Inter-estingly, combining PageRank and Lesk under thisinformed setting does not bring any significant im-provements over the individual algorithms: 67.72%obtained by the combined algorithm compared with67.51% obtained with PageRank only.6.3 DiscussionRegardless of the setting ?
fully unsupervised algo-rithms with no a-priori knowledge about sense or-der, or informed methods where the sense order ren-dered by the dictionary is taken into account ?
thePageRank-based word sense disambiguation algo-rithm exceeds the baseline by a large margin, andalways outperforms the Lesk algorithm.
Moreover,a hybrid algorithm that combines the PageRank andLesk methods into one single algorithm is found toimprove over the individual algorithms in the firstsetting, but brings no significant changes when thesense frequency is also integrated into the disam-biguation algorithm.
This may be explained by thefact that the additional knowledge element intro-duced by the sense order in WordNet increases theredundancy of information in these two algorithmsto the point where their combination cannot improveover the individual algorithms.The most closely related method is perhaps thelexical chains algorithm (Morris and Hirst, 1991) ?where threads of meaning are identified throughout atext.
Lexical chains however only take into accountpossible relations between concepts in a static way,without considering the importance of the conceptsthat participate in a relation, which is recursivelydetermined by PageRank.
Another related line ofwork is the word sense disambiguation algorithmproposed in (Veronis and Ide, 1990), where a largeneural network is built by relating words throughtheir dictionary definitions.The Analogy.
In the context of Web surfing,PageRank implements the ?random surfer model?,where a user surfs the Web by following links fromany given Web page.
In the context of text meaning,PageRank implements the concept of text cohesion(Halliday and Hasan, 1976), where from a certainconcept C in a text, we are likely to ?follow?
linksto related concepts ?
that is, concepts that have a se-mantic relation with the current concept C .Intuitively, PageRank-style algorithms work wellfor finding the meaning of all words in open textbecause they combine together information drawnfrom the entire text (graph), and try to identify thosesynsets (vertices) that are of highest importance forthe text unity and understanding.The meaning selected by PageRank from a set ofpossible meanings for a given word can be seen asthe one most recommended by related meanings inthe text, with preference given to the ?recommen-dations?
made by most influential ones, i.e.
the onesthat are in turn highly recommended by other relatedmeanings.
The underlying hypothesis is that in a co-hesive text fragment, related meanings tend to occurtogether and form a ?Web?
of semantic connectionsthat approximates the model humans build about agiven context in the process of discourse understand-ing.7 ConclusionsIn this paper, we showed that iterative graph-based ranking algorithms ?
originally designed forcontent-independent Web link analysis or for socialnetworks ?
turn into a useful source of informationfor natural language tasks when applied on semanticnetworks.
In particular, we proposed and evaluateda new approach for unsupervised knowledge-basedword-sense disambiguation that relies on PageRank-style algorithms applied on a WordNet-based con-cepts graph, and showed that the accuracy achievedthrough our algorithm exceeds the performance ob-tained by other knowledge-based algorithms.AcknowledgmentsThis work was partially supported by a National Sci-ence Foundation grant IIS-0336793.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scale hyper-textual Web search engine.
Computer Networks and ISDNSystems, 30(1?7):107?117.J.
Cowie, L. Guthrie, and J. Guthrie.
1992.
Lexical disam-biguation using simulated annealing.
In Proceedings of the5th International Conference on Computational LinguisticsCOLING-92, pages 157?161.W.
Gale, K. Church, and D. Yarowsky.
1992.
One sense perdiscourse.
In Proceedings of the DARPA Speech and NaturalLanguage Workshop, Harriman, New York.M.
Halliday and R. Hasan.
1976.
Cohesion in English.
Long-man.A.
Kilgarriff and R. Rosenzweig.
2000.
Framework and re-sults for English SENSEVAL.
Computers and the Humani-ties, 34:15?48.J.M.
Kleinberg.
1999.
Authoritative sources in a hyperlinkedenvironment.
Journal of the ACM, 46(5):604?632.M.E.
Lesk.
1986.
Automatic sense disambiguation using ma-chine readable dictionaries: How to tell a pine cone from anice cream cone.
In Proceedings of the SIGDOC Conference1986, Toronto, June.G.
Miller, C. Leacock, T. Randee, and R. Bunker.
1993.
Asemantic concordance.
In Proceedings of the 3rd DARPAWorkshop on Human Language Technology, pages 303?308,Plainsboro, New Jersey.G.
Miller.
1995.
Wordnet: A lexical database.
Communicationof the ACM, 38(11):39?41.J.
Morris and G. Hirst.
1991.
Lexical cohesion, the the-saurus, and the structure of text.
Computational Linguistics,17(1):21?48.R.
Rada, H. Mili, E. Bickell, and B. Blettner.
1989.
Devel-opment and application of a metric on semantic nets.
IEEETransactions on Systems, Man and Cybernetics, 19:17?30,Jan/Feb.P.
Resnik.
1997.
Selectional preference and sense disambigua-tion.
In Proceedings of ACL Siglex Workshop on TaggingText with Lexical Semantics, Why, What and How?, Wash-ington DC, April.J.
Stetina, S. Kurohashi, and M. Nagao.
1998.
General wordsense disambiguation method based on a full sentential con-text.
In Usage of WordNet in Natural Language Processing,Proceedings of COLING-ACL Workshop, Montreal, Canada,July.J.
Veronis and N. Ide.
1990.
Word sense disambiguation withvery large neural networks extracted from machine read-able dictionaries.
In Proceedings of the 13th InternationalConference on Computational Linguistics (COLING 1990),Helsinki, Finland, August.D.
Yarowsky.
1993.
One sense per collocation.
In Proceedingsof the ARPA Human Language Technology Workshop.
