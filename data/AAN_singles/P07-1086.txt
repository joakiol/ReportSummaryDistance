Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 680?687,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsCoordinate Noun Phrase Disambiguation in a Generative Parsing ModelDeirdre Hogan?Computer Science DepartmentTrinity College DublinDublin 2, Irelanddhogan@computing.dcu.ieAbstractIn this paper we present methods for im-proving the disambiguation of noun phrase(NP) coordination within the framework of alexicalised history-based parsing model.
Aswell as reducing noise in the data, we look atmodelling two main sources of informationfor disambiguation: symmetry in conjunctstructure, and the dependency between con-junct lexical heads.
Our changes to the base-line model result in an increase in NP coor-dination dependency f-score from 69.9% to73.8%, which represents a relative reductionin f-score error of 13%.1 IntroductionCoordination disambiguation is a relatively littlestudied area, yet the correct bracketing of coordina-tion constructions is one of the most difficult prob-lems for natural language parsers.
In the Collinsparser (Collins, 1999), for example, dependenciesinvolving coordination achieve an f-score as low as61.8%, by far the worst performance of all depen-dency types.Take the phrase busloads of executives and theirwives (taken from the WSJ treebank).
The coordi-nating conjunction (CC) and and the noun phrasetheir wives could attach to the noun phrase exec-utives, as illustrated in Tree 1, Figure 1.
Alterna-tively, their wives could be incorrectly conjoined tothe noun phrase busloads of executives as in Tree 2,Figure 1.?
Now at the National Centre for Language Technology,Dublin City University, Ireland.As with PP attachment, most previous attemptsat tackling coordination as a subproblem of parsinghave treated it as a separate task to parsing and itis not always obvious how to integrate the methodsproposed for disambiguation into existing parsingmodels.
We therefore approach coordination disam-biguation, not as a separate task, but from within theframework of a generative parsing model.As noun phrase coordination accounts for over50% of coordination dependency error in our base-line model we focus on NP coordination.
Us-ing a model based on the generative parsing modelof (Collins, 1999) Model 1, we attempt to improvethe ability of the parsing model to make the correctcoordination decisions.
This is done in the contextof parse reranking, where the n-best parses outputfrom Bikel?s parser (Bikel, 2004) are reranked ac-cording to a generative history-based model.In Section 2 we summarise previous work on co-ordination disambiguation.
There is often a consid-erable bias toward symmetry in the syntactic struc-ture of two conjuncts and in Section 3 we introducenew parameter classes to allow the model to prefersymmetry in conjunct structure.
Section 4 is con-cerned with modelling the dependency between con-junct head words and begins by looking at how thedifferent handling of coordination in noun phrasesand base noun phrases (NPB) affects coordinationdisambiguation.1 We look at how we might improvethe model?s handling of coordinate head-head de-pendencies by altering the model so that a common1A base noun phrase, as defined in (Collins, 1999), is a nounphrase which does not directly dominate another noun phrase,unless that noun phrase is possessive.6801.
NPNPNPBbusloadsPPof NPNPNPBexecutivesand NPNPBtheir wives2.
NPNPNPBbusloadsPPof NPNPBexecutivesand NPNPBtheir wivesFigure 1: Tree 1.
The correct noun phrase parse.Tree 2.
The incorrect parse for the noun phrase.parameter class is used for coordinate word prob-ability estimation in both NPs and NPBs.
In Sec-tion 4.2 we focus on improving the estimation ofthis parameter class by incorporating BNC data, anda measure of word similarity based on vector cosinesimilarity, to reduce data sparseness.
In Section 5 wesuggest a new head-finding rule for NPBs so that thelexicalisation process for coordinate NPBs is moresimilar to that of other NPs.Section 6 examines inconsistencies in the annota-tion of coordinate NPs in the Penn Treebank whichcan lead to errors in coordination disambiguation.We show how some coordinate noun phrase incon-sistencies can be automatically detected and cleanedfrom the data sets.
Section 7 details how the model isevaluated, presents the experiments made and givesa breakdown of results.2 Previous WorkMost previous attempts at tackling coordinationhave focused on a particular type of NP coordinationto disambiguate.
Both Resnik (1999) and Nakov andHearst (2005) consider NP coordinations of the formn1 and n2 n3 where two structural analyses are pos-sible: ((n1 and n2) n3) and ((n1) and (n2 n3)).
Theyaim to show more structure than is shown in treesfollowing the Penn guidelines, whereas in our ap-proach we aim to reproduce Penn guideline trees.To resolve the ambiguities, Resnik combines num-ber agreement information of candidate conjoinednouns, an information theoretic measure of semanticsimilarity, and a measure of the appropriateness ofnoun-noun modification.
Nakov and Hearst (2005)disambiguate by combining Web-based statistics onhead word co-occurrences with other mainly heuris-tic information sources.A probabilistic approach is presented in (Gold-berg, 1999), where an unsupervised maximum en-tropy statistical model is used to disambiguate coor-dinate noun phrases of the form n1 preposition n2cc n3.
Here the problem is framed as an attachmentdecision: does n3 attach ?high?
to the first noun, n1,or ?low?
to n2?In (Agarwal and Boggess, 1992) the task is toidentify pre-CC conjuncts which appear in text thathas been part-of-speech (POS) tagged and semi-parsed, as well as tagged with semantic labels spe-cific to the domain.
The identification of the pre-CC conjunct is based on heuristics which choose thepre-CC conjunct that maximises the symmetry be-tween pre- and post-CC conjuncts.Insofar as we do not separate coordination dis-ambiguation from the overall parsing task, our ap-proach resembles the efforts to improve coordi-nation disambiguation in (Kurohashi, 1994; Rat-naparkhi, 1994; Charniak and Johnson, 2005).In (Kurohashi, 1994) coordination disambiguationis carried out as the first component of a Japanesedependency parser using a technique which calcu-lates similarity between series of words from the leftand right of a conjunction.
Similarity is measuredbased on matching POS tags, matching words and athesaurus-based measure of semantic similarity.
Inboth the discriminative reranker of Ratnaparkhi etal.
(1994) and that of Charniak and Johnson (2005)features are included to capture syntactic parallelismacross conjuncts at various depths.3 Modelling Symmetry Between ConjunctsThere is often a considerable bias toward symme-try in the syntactic structure of two conjuncts, seefor example (Dubey et al, 2005).
Take Figure 2: Ifwe take as level 0 the level in the coordinate sub-681NP1(plains)NP2(plains)NP3(plains)DT6theJJ5highNNS4plainsPP7(of)IN8ofNP9(T exas)NNP10TexasCC11andNP11(states)NP12(states)DT15theJJ14northernNNS13statesPP16(of)IN17ofNP18(Delta)DT20theNNP19DeltaFigure 2: Example of symmetry in conjunct structure in a lexicalised subtree.tree where the coordinating conjunction CC occurs,then there is exact symmetry in the two conjuncts interms of non-terminal labels and head word part-of-speech tags for levels 0, 1 and 2.
Learning a biastoward parallelism in conjuncts should improve theparsing model?s ability to correctly attach a coordi-nation conjunction and second conjunct to the cor-rect position in the tree.In history-based models, features are limited tobeing functions of the tree generated so far.
The taskis to incorporate a feature into the model that cap-tures a particular bias yet still adheres to derivation-based restrictions.
Parses are generated top-down,head-first, left-to-right.
Each node in the tree inFigure 2 is annotated with the order the nodes aregenerated (we omit, for the sake of clarity, the gen-eration of the STOP nodes).
Note that when thedecision to attach the second conjunct to the headconjunct is being made (i.e.
Step 11, when the CCand NP(states) nodes are being generated) the sub-tree rooted at NP(states) has not yet been generated.Thus at the point that the conjunct attachment de-cision is made it is not possible to use informationabout symmetry of conjunct structure, as the struc-ture of the second conjunct is not yet known.It is possible, however, to condition on structureof the already generated head conjunct when build-ing the internal structure of the second conjunct.
Inour model when the structure of the second conjunctis being generated we condition on features whichare functions of the first conjunct.
When generat-ing a node Ni in the second conjunct, we retrievethe corresponding node NipreCC in the first conjunct,via a left to right traversal of the first conjunct.
Forexample, from Figure 2 the pre-CC node NP(Texas)is the node corresponding to NP(Delta) in the post-CC conjunct.
From NipreCC we extract information,such as its part-of-speech, for use as a feature whenpredicting a POS tag for the corresponding node inthe post-CC conjunct.When generating a second conjunct, instead ofthe usual parameter classes for estimating the prob-ability of the head label Ch and the POS label of adependent node ti, we created two new parameterclasses which are used only in the generation of sec-ond conjunct nodes:PccCh(Ch|?
(headC), Cp, wp, tp, tgp, depth) (1)Pccti(ti|?
(headC), dir, Cp, wp, tp, dist, ti 1, ti 2, depth)(2)where ?
(headC) returns the non-terminal label ofNipreCC for the node in question and ?
(headC) re-turns the POS tag of NipreCC .
Both functions return+NOMATCH+ if there is no NipreCC for the node.Depth is the level of the post-CC conjunct node be-ing generated.4 Modelling Coordinate Head WordsSome noun pairs are more likely to be conjoinedthan others.
Take again the trees in Figure 1.
Thetwo head nouns coordinated in Tree 1 are execu-tives and wives, and in Tree 2: busloads and wives.Clearly, the former pair of head nouns is more likelyand, for the purpose of discrimination, the modelwould benefit if it could learn that executives andwives is a more likely combination than busloadsand wives.Bilexical head-head dependencies of the typefound in coordinate structures are a somewhat dif-682ferent class of dependency to modifier-head depen-dencies.
In the fat cat, for example, there is clearlyone head to the noun phrase: cat.
In cats and dogshowever there are two heads, though in the parsingmodel just one is chosen, somewhat arbitrarily, tohead the entire noun phrase.In the baseline model there is essentially one pa-rameter class for the estimation of word probabili-ties:Pword(wi|H(i)) (3)where wi is the lexical head of constituent i andH(i) is the history of the constituent.
The history ismade up of conditioning features chosen from struc-ture that has already been determined in the top-down derivation of the tree.In Section 4.1 we discuss how though the coordi-nate head-head dependency is captured for NPs, it isnot captured for NPBs.
We look at how we mightimprove the model?s handling of coordinate head-head dependencies by altering the model so that acommon parameter class in (4) is used for coordi-nate word probability estimation in both NPs andNPBs.PcoordWord(wi|wp, H(i)) (4)In Section 4.2 we focus on improving the estimationof this parameter class by reducing data sparseness.4.1 Extending PcoordWord to Coordinate NPBsIn the baseline model each node in the tree is an-notated with a coordination flag which is set to truefor the node immediately following the coordinatingconjunction.
For coordinate NPs the head-head de-pendency is captured when this flag is set to true.
InFigure 1, discarding for simplicity the other featuresin the history, the probability of the coordinate headwives, is estimated in Tree 1 as:Pword(wi = wives|coord = true, wp = executives, ...)(5)and in Tree 2:Pword(wi = wives|coord = true, wp = busloads, ...) (6)where wp is the head word of the node to which thenode headed by wi is attaching and coord is the co-ordination flag.Unlike NPs, in NPBs (i.e.
flat, non-recursive NPs)the coordination flag is not used to mark whether anode is a coordinated head or not.
This flag is alwaysset to false for NPBs.
In addition, modifiers withinNPBs are conditioned on the previously generatedmodifier rather than the head of the phrase.2 Thismeans that in an NPB such as (cats and dogs), theestimate for the word cats will look like:Pword(wi = cats|coord = false, wp = and, ...) (7)In our new model, for NPs, when the coordinationflag is set to true, we use the parameter class in (4)to estimate the probability of one lexical head noun,given another.
For NPBs, if a noun is generated di-rectly after a CC then it is taken to be a coordinatehead, wi, and conditioned on the noun generated be-fore the coordinating conjunction, which is chosenas wp, and also estimated using (4).4.2 Estimating the PcoordWord parameter classData for bilexical statistics are particularly sparse.In order to decrease the sparseness of the coordinatehead noun data, we extracted from the BNC exam-ples of coordinate head noun pairs.
We extracted allnoun pairs occurring in a pattern of the form: nouncc noun, as well as lists of any number of nounsseparated by commas and ending in cc noun.3 Tothis data we added all head noun pairs from the WSJthat occurred together in a coordinate noun phrase,identified when the coordination flag was set to true.Every occurrence ni CC nj was also counted as anoccurrence of nj CC ni.
This further helps reducesparseness.The probability of one noun, ni being coordinatedwith another nj can be calculated simply as:Plex(ni|nj) =|ninj ||nj |(8)Again to reduce data sparseness, we introduce ameasure of word similarity.
A word can be rep-resented as a vector where every dimension of thevector represents another word type.
The values ofthe vector components, the term weights, are derivedfrom word co-occurrence counts.
Cosine similar-ity between two word vectors can then be used tomeasure the similarity of two words.
Measures of2A full explanation of the handling of coordination in themodel is given in (Bikel, 2004).3Extracting coordinate noun pairs from the BNC in sucha fashion follows work on networks of concepts describedin (Widdows, 2004).683similarity between words based on similarity of co-occurrence vectors have been used before, for exam-ple, for word sense disambiguation (Schu?tze, 1998)and for PP-attachment disambiguation (Zhao andLin, 2004).
Our measure resembles that of (Cara-ballo, 99) where co-occurrence is also defined withrespect to coordination patterns, although the exper-imental details in terms of data collection and vectorterm weights differ.We can now incorporate the similarity measureinto the probability estimate of (8) to give a newk-NN style method of estimating bilexical statisticsbased on weighting events according to the wordsimilarity measure:Psim(ni|nj) =?nx?N(nj)sim(nj , nx)|ninx|?nx?N(nj)sim(nj , nx)|nx|(9)where sim(nj, nx) is a similarity score betweenwords nj and nx and N(nj) is the set of words inthe neighbourhood of nj .
This neighbourhood canbe based on the k-nearest neighbours of nj , wherenearness is measured with the similarity function.In order to smooth the bilexical estimate in (9) wecombine it with another estimate, trained from WSJdata, by way of linear interpolation:PcoordWord(ni|nj) =?nj Psim(ni|nj) + (1?
?nj )PMLE(ni|ti) (10)where ti is the POS tag of word ni, PMLE(ni|ti)is the maximum-likelihood estimate calculated fromannotated WSJ data, and ?nj is calculated as in (11).In (11) we adapt the Witten-Bell method for thecalculation of the weight ?, as used in the Collinsparser, so that it incorporates the similarity measurefor all words in the neighbourhood of nj .
?nj =?nx?N(nj )sim(nj , nx)|nx|?nx?N(nj)sim(nj , nx)(|nx| + CD(nx))(11)where C is a constant that can be optimised usingheld-out data and D(nj) is the diversity of a wordnj: the number of distinct words with which nj hasbeen coordinated in the training set.The estimate in (9) can be viewed as the estimatewith the more general history context than that of (8)because the context includes not only nj but alsowords similar to nj .
The final probability estimatefor PcoordWord is calculated as the most specific es-timate, Plex, combined via regular Witten-Bell inter-polation with the estimate in (10).5 NPB Head-Finding RulesHead-finding rules for coordinate NPBs differ fromcoordinate NPs.4 Take the following two versionsof the noun phrase hard work and harmony: (c) (NP(NPB hard work and harmony)) and (d) (NP (NP(NPB hard work)) and (NP (NPB harmony))).
In thefirst example, harmony is chosen as head word of theNP; in example (d) the head of the entire NP is work.The choice of head affects the various dependenciesin the model.
However, in the case of two coordinateNPs which, as in the above example, cover the samespan of words and differ only in whether the coordi-nate noun phrase is flat as in (c) or structured as in(d), the choice of head for the phrase is not particu-larly informative.
In both cases the head words be-ing coordinated are the same and either word couldplausibly head the phrase; discrimination betweentrees in such cases should not be influenced by thechoice of head, but rather by other, salient featuresthat distinguish the trees.5We would like to alter the head-finding rules forcoordinate NPBs so that, in cases like those above,the word chosen to head the entire coordinate nounphrase would be the same for both base and non-base noun phrases.
We experiment with slightlymodified head-finding rules for coordinate NPBs.
Inan NPB such as NPB?
n1 CC n2 n3, the head rulesremain unchanged and the head of the phrase is (usu-ally) the rightmost noun in the phrase.
Thus, whenn2 is immediately followed by another noun the de-fault is to assume nominal modifier coordination andthe head rules stay the same.
The modification wemake to the head rules for NPBs is as follows: whenn2 is not immediately followed by a noun then thenoun chosen to head the entire phrase is n1.6 Inconsistencies in WSJ Coordinate NPAnnotationAn inspection of NP coordination error in the base-line model revealed inconsistencies in WSJ annota-4See (Collins, 1999) for the rules used in the baseline model.5For example, it would be better if discrimination waslargely based on whether hard modifies both work and harmony(c), or whether it modifies work alone (d).684tion.
In this section we outline some types of co-ordinate NP inconsistency and outline a method fordetecting some of these inconsistencies, which welater use to automatically clean noise from the data.Eliminating noise from treebanks has been previ-ously used successfully to increase overall parser ac-curacy (Dickinson and Meurers, 2005).The annotation of NPs in the Penn Treebank (Bieset al, 1995) follows somewhat different guidelinesto that of other syntactic categories.
Because theirinterpretation is so ambiguous, no internal structureis shown for nominal modifiers.
For NPs with morethan one head noun, if the only unshared modifiersin the constituent are nominal modifiers, then a flatstructure is also given.
Thus in (NP the Manhattanphone book and tour guide)6 a flat structure is givenbecause although the is a non-nominal modifier, it isshared, modifying both tour guide and phone book,and all other modifiers in the phrase are nominal.However, we found that out of 1,417 examplesof NP coordination in sections 02 to 21, involvingphrases containing only nouns (common nouns or amixture of common and proper nouns) and the co-ordinating conjunction, as many as 21.3%, contraryto the guidelines, were given internal structure, in-stead of a flat annotation.
When all proper nouns areinvolved this phenomenon is even more common.7Another common source of inconsistency in co-ordinate noun phrase bracketing occurs when a non-nominal modifier appears in the coordinate nounphrase.
As previously discussed, according to theguidelines the modifier is annotated flat if it isshared.
When the non-nominal modifier is un-shared, more internal structure is shown, as in:(NP (NP (NNS fangs)) (CC and) (NP (JJ pointed)(NNS ears))).
However, the following two struc-tured phrases, for example, were given a com-pletely flat structure in the treebank: (a) (NP (NP(NN oversight))(CC and) (NP (JJ disciplinary)(NNSprocedures))), (b) (NP (ADJP (JJ moderate)(CCand)(JJ low-cost))(NN housing)).
If we follow theguidelines then any coordinate NPB which endswith the following tag sequence can be automat-ically detected as incorrectly bracketed: CC/non-nominal modifier/noun.
This is because either the6In this section we do not show the NPB levels.7In the guidelines it is recognised however that proper namesare frequently annotated with internal structure.non-nominal modifier, which is unambiguously un-shared, is part of a noun phrase as (a) above, or itconjoined with another modifier as in (b).
We found202 examples of this in the training set, out of a totalof 4,895 coordinate base noun phrases.Finally, inconsistencies in POS tagging can alsolead to problems with coordination.
Take the bi-gram executive officer.
We found 151 examples inthe training set of a base noun phrase which endedwith this bigram.
48% of the cases were POS taggedJJ NN, 52% tagged NN NN.
8 This has repercussionsfor coordinate noun phrase structure, as the presenceof an adjectival pre-modifier indicates a structuredannotation should be given.These inconsistencies pose problems both fortraining and testing.
With a relatively large amountof noise in the training set the model learns to givestructures, which should be very unlikely, too higha probability.
In testing, given inconsistencies inthe gold standard trees, it becomes more difficultto judge how well the model is doing.
Although itwould be difficult to automatically detect the POStagging errors, the other inconsistencies outlinedabove can be detected automatically by simple pat-tern matching.
Automatically eliminating such ex-amples is a simple method of cleaning the data.7 Experimental EvaluationWe use a parsing model similar to that describedin (Hogan, 2005) which is based on (Collins, 1999)Model 1 and uses k-NN for parameter estimation.The n-best output from Bikel?s parser (Bikel, 2004)is reranked according to this k-NN parsing model,which achieves an f-score of 89.4% on section 23.For the coordination experiments, sections 02 to 21are used for training, section 23 for testing and theremaining sections for validation.
Results are forsentences containing 40 words or less.As outlined in Section 6, the treebank guide-lines are somewhat ambiguous as to the appropriatebracketing for coordinate NPs which consist entirelyof proper nouns.
We therefore do not include, in thecoordination test and validation sets, coordinate NPswhere in the gold standard NP the leaf nodes consistentirely of proper nouns (or CCs or commas).
In do-8According to the POS bracketing guidelines (Santorini,1991) the correct sequence of POS tags should be NN NN.685ing so we hope to avoid a situation whereby the suc-cess of the model is measured in part by how wellit can predict the often inconsistent bracketing deci-sions made for a particular portion of the treebank.In addition, and for the same reasons, if a goldstandard tree is inconsistent with the guidelines ineither of the following two ways the tree is not usedwhen calculating coordinate precision and recall ofthe model: the gold tree is a noun phrase which endswith the sequence CC/non-nominal modifier/noun;the gold tree is a structured coordinate noun phrasewhere each word in the noun phrase is a noun.9 Callthese inconsistencies type a and type b respectively.This left us with a coordination validation set con-sisting of 1064 coordinate noun phrases and a testset of 416 coordinate NPs from section 23.A coordinate phrase was deemed correct if theparent constituent label, and the two conjunct nodelabels (at level 0) match those in the gold subtree andif, in addition, each of the conjunct head words arethe same in both test and gold tree.
This follows thedefinition of a coordinate dependency in (Collins,1999).
Based on these criteria, the baseline f-scoresfor test and validation set were 69.1% and 67.1% re-spectively.
The coordination f-score for the oracletrees on section 23 is 83.56%.
In other words: if an?oracle?
were to choose from each set of n-best treesthe tree that maximised constituent precision and re-call, then the resulting set of oracle trees would havea NP coordination dependency f-score of 83.56%.For the validation set the oracle trees coordinationdependency f-score is 82.47%.7.1 Experiments and ResultsWe first eliminated from the training set al coordi-nate noun phrase subtrees, of type a and type b de-scribed in Section 7.
The effect of this on the vali-dation set is outlined in Table 1, step 2.For the new parameter class in (1) we found thatthe best results occurred when it was used only inconjuncts of depth 1 and 2, although the case basefor this parameter class contained head events fromall post-CC conjunct depths.
Parameter class (2) wasused for predicting POS tags at level 1 in right-of-head conjuncts, although again the sample contained9Recall from ?6 that for this latter case the noun phraseshould be flat - an NPB - rather than a noun phrase with internalstructure.Model f-score significance1.
Baseline 67.12.
NoiseElimination 68.7 ?
13.
Symmetry 69.9 > 2,?
14.
NPB head rule 70.6 NOT > 3, > 2,?
15.
PcoordWord WSJ 71.7 NOT > 4, > 3,?
26.
BNC data 72.1 NOT > 5, > 4,?
37. sim(wi, wp) 72.4 NOT > 6, NOT > 5,?
4Table 1: Results on the Validation Set.
1064 coordi-nate noun phrase dependencies.
In the significancecolumn > means at level .05 and ?
means at level.005, for McNemar?s test of significance.
Results arecumulative.events from all depths.For the PcoordWord parameter class we extracted9961 coordinate noun pairs from the WSJ train-ing set and 815,323 pairs from the BNC.
As pairsare considered symmetric this resulted in a total of1,650,568 coordinate noun events.
The term weightsfor the word vectors were dampened co-occurrencecounts, of the form: 1 + log(count).
For the es-timation of Psim(ni|nj) we found it too computa-tionally expensive to calculate similarity measuresbetween nj and each word token collected.
The bestresults were obtained when the neighbourhood of njwas taken to be the k-nearest neighbours of nj fromamong the set of word that had previously occurredin a coordination pattern with nj , where k is 1000.Table 1 shows the effect of the PcoordWord parame-ter class estimated from WSJ data only (step 5), withthe addition of BNC data (step 6) and finally with theword similarity measure (step 7).The result of these experiments, as well as thatinvolving the change in the head-finding heuristics,outlined in Section 5, was an increase in coordinatenoun phrase f-score from 69.9% to 73.8% on the testset.
This represents a 13% relative reduction in co-ordinate f-score error over the baseline, and, usingMcNemar?s test for significance, is significant at the0.05 level (p = 0.034).
The reranker f-score forall constituents (not excluding any coordinate NPs)for section 23 rose slightly from 89.4% to 89.6%, asmall but significant increase in f-score.10Finally, we report results on an unaltered coor-dination test set, that is, a test set from which no10Significance was calculated using the software available atwww.cis.upenn.edu/ dbikel/software.html.686noisy events were eliminated.
The baseline coordi-nation dependency f-score for all NP coordinationdependencies (550 dependencies) from section 23 is69.27%.
This rises to 72.74% when all experimentsdescribed in Section 7 are applied, which is also astatistically significant increase (p = 0.042).8 Conclusion and Future WorkThis paper outlined a novel method for modellingsymmetry in conjunct structure, for modelling thedependency between noun phrase conjunct headwords and for incorporating a measure of word sim-ilarity in the estimation of a model parameter.
Wealso demonstrated how simple pattern matching canbe used to reduce noise in WSJ noun phrase coor-dination data.
Combined, these techniques resultedin a statistically significant improvement in nounphrase coordination accuracy.Coordination disambiguation necessitates in-formation from a variety of sources.
Anotherinformation source important to NP coordinatedisambiguation is the dependency between non-nominal modifiers and nouns which cross CCsin NPBs.
For example, modelling this type ofdependency could help the model learn that thephrase the cats and dogs should be bracketed flat,whereas the phrase the U.S. and Washington shouldbe given structure.Acknowledgements We are grateful to the TCDBroad Curriculum Fellowship scheme and to theSFI Basic Research Grant 04/BR/CS370 for fund-ing this research.
Thanks to Pa?draig Cunningham,Saturnino Luz, Jennifer Foster and Gerard Hoganfor helpful discussions and feedback on this work.ReferencesRajeev Agarwal and Lois Boggess.
1992.
A Simple but UsefulApproach to Conjunct Identification.
In Proceedings of the30th ACL.Ann Bies, Mark Ferguson, Karen Katz and Robert MacIntyre.1995.
Bracketing Guidelines for Treebank II Style PennTreebank Project.
Technical Report.
University of Penn-sylvania.Dan Bikel.
2004.
On The Parameter Space of Generative Lex-icalized Statistical Parsing Models.
Ph.D. thesis, Universityof Pennsylvania.Sharon Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
In Proceedingsof the 37th ACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best Parsing and MaxEnt Discriminative Reranking.
In Pro-ceedings of the 43rd ACL.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania.Markus Dickinson and W. Detmar Meurers.
2005.
Prune dis-eased branches to get healthy trees!
How to find erroneouslocal trees in a treebank and why it matters.
In Proceedingsof the Fourth Workshop on Treebanks and Linguistic Theo-ries (TLT).Amit Dubey, Patrick Sturt and Frank Keller.
2005.
Parallelismin Coordination as an Instance of Syntactic Priming: Evi-dence from Corpus-based Modeling.
In Proceedings of theHLT/EMNP-05.Miriam Goldberg.
1999.
An Unsupervised Model for Statis-tically Determining Coordinate Phrase Attachment.
In Pro-ceedings of the 27th ACL.Deirdre Hogan.
2005. k-NN for Local Probability Estimationin Generative Parsing Models.
In Proceedings of the IWPT-05.Sadao Kurohashi and Makoto Nagao.
1994.
A Syntactic Anal-ysis Method of Long Japanese Sentences Based on the De-tection of Conjunctive Structures.
In Computational Lin-guistics, 20(4).Preslav Nakov and Marti Hearst.
2005.
Using the Web as anImplicit Training Set: Application to Structural AmbiguityResolution.
In Proceedings of the HLT/EMNLP-05.Adwait Ratnaparkhi, Salim Roukos and R. Todd Ward.
1994.
AMaximum Entropy Model for Parsing.
In Proceedings of theInternational Conference on Spoken Language Processing.Philip Resnik.
1999.
Semantic Similarity in a Taxonomy: AnInformation-Based Measure and its Application to Problemsof Ambiguity in Natural Language.
In Journal of ArtificialIntelligence Research, 11:95-130, 1999.Beatrice Santorini.
1991.
Part-of-Speech Tagging Guidelinesfor the Penn Treebank Project.
Technical Report.
Universityof Pennsylvania.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrimination.Computational Linguistics, 24(1):97-123.Dominic Widdows.
2004.
Geometry and Meaning.
CSLI Pub-lications, Stanford, USA.Shaojun Zhao and Dekang Lin.
2004.
A Nearest-NeighborMethod for Resolving PP-Attachment Ambiguity.
In Pro-ceedings of the IJCNLP-04.687
