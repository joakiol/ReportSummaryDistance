Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 420?429,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsFacilitating Translation Using Source Language Paraphrase LatticesJinhua Du, Jie Jiang, Andy WayCNGL, School of ComputingDublin City University, Dublin, Ireland{jdu, jjiang, away}@computing.dcu.ieAbstractFor resource-limited language pairs, coverageof the test set by the parallel corpus is animportant factor that affects translation qual-ity in two respects: 1) out of vocabularywords; 2) the same information in an inputsentence can be expressed in different ways,while current phrase-based SMT systems can-not automatically select an alternative wayto transfer the same information.
Therefore,given limited data, in order to facilitate trans-lation from the input side, this paper pro-poses a novel method to reduce the transla-tion difficulty using source-side lattice-basedparaphrases.
We utilise the original phrasesfrom the input sentence and the correspond-ing paraphrases to build a lattice with esti-mated weights for each edge to improve trans-lation quality.
Compared to the baseline sys-tem, our method achieves relative improve-ments of 7.07%, 6.78% and 3.63% in termsof BLEU score on small, medium and large-scale English-to-Chinese translation tasks re-spectively.
The results show that the proposedmethod is effective not only for resource-limited language pairs, but also for resource-sufficient pairs to some extent.1 IntroductionIn recent years, statistical MT systems have beeneasy to develop due to the rapid explosion in dataavailability, especially parallel data.
However, inreality there are still many language pairs whichlack parallel data, such as Urdu?English, Chinese?Italian, where large amounts of speakers exist forboth languages; of course, the problem is far worsefor pairs such as Catalan?Irish.
For such resource-limited language pairs, sparse amounts of paralleldata would cause the word alignment to be inac-curate, which would in turn lead to an inaccuratephrase alignment, and bad translations would re-sult.
Callison-Burch et al (2006) argue that lim-ited amounts of parallel training data can lead to theproblem of low coverage in that many phrases en-countered at run-time are not observed in the train-ing data and so their translations will not be learned.Thus, in recent years, research on addressing theproblem of unknown words or phrases has becomemore and more evident for resource-limited lan-guage pairs.Callison-Burch et al (2006) proposed a novelmethod which substitutes a paraphrase for an un-known source word or phrase in the input sentence,and then proceeds to use the translation of that para-phrase in the production of the target-language re-sult.
Their experiments showed that by translatingparaphrases a marked improvement was achieved incoverage and translation quality, especially in thecase of unknown words which previously had beenleft untranslated.
However, on a large-scale data set,they did not achieve improvements in terms of auto-matic evaluation.Nakov (2008) proposed another way to use para-phrases in SMT.
He generates nearly-equivalent syn-tactic paraphrases of the source-side training sen-tences, then pairs each paraphrased sentence withthe target translation associated with the originalsentence in the training data.
Essentially, thismethod generates new training data using para-phrases to train a new model and obtain more useful420phrase pairs.
However, he reported that this methodresults in bad system performance.
By contrast,real improvements can be achieved by merging thephrase tables of the paraphrase model and the orig-inal model, giving priority to the latter.
Schroederet al (2009) presented the use of word latticesfor multi-source translation, in which the multiplesource input texts are compiled into a compact lat-tice, over which a single decoding pass is then per-formed.
This lattice-based method achieved positiveresults across all data conditions.In this paper, we propose a novel method us-ing paraphrases to facilitate translation, especiallyfor resource-limited languages.
Our method doesnot distinguish unknown words in the input sen-tence, but uses paraphrases of all possible wordsand phrases in the source input sentence to build asource-side lattice to provide a diverse and flexiblelist of source-side candidates to the SMT decoderso that it can search for a best path and deliver thetranslation with the highest probability.
In this case,we neither need to change the phrase table, nor addnew features in the log-linear model, nor add newsentences in the training data.The remainder of this paper is organised as fol-lows.
In Section 2, we define the ?translation diffi-culty?
from the perspective of the source side, andthen examine how well the test set is covered bythe phrase table and the parallel training data .
Sec-tion 3 describes our paraphrase lattice method anddiscusses how to set the weights for the edges in thelattice network.
In Section 4, we report comparativeexperiments conducted on small, medium and large-scale English-to-Chinese data sets.
In Section 5,we analyse the influence of our paraphrase latticemethod.
Section 6 concludes and gives avenues forfuture work.2 What Makes Translation Difficult?2.1 Translation DifficultyWe use the term ?translation difficulty?
to explainhow difficult it is to translate the source-side sen-tence in three respects:?
The OOV rates of the source sentences in thetest set (Callison-Burch et al, 2006).?
Translatability of a known phrase in the inputsentence.
Some particular grammatical struc-tures on the source side cannot be directlytranslated into the corresponding structures onthe target side.
Nakov (2008) presents an ex-ample showing how hard it is to translate an En-glish construction into Spanish.
Assume that anEnglish-to-Spanish SMT system has an entryin its phrase table for ?inequality of income?,but not for ?income inequality?.
He argues thatthe latter phrase is hard to translate into Span-ish where noun compounds are rare: the correcttranslation in this case requires a suitable Span-ish preposition and a reordering, which are hardfor the system to realize properly in the targetlanguage (Nakov, 2008).?
Consistency between the reference and thetarget-side sentence in the training corpus.Nakov (2008) points out that if the target-sidesentence in the parallel corpus is inconsistentwith the reference of the test set, then in somecases, a test sentence might contain pieces thatare equivalent, but syntactically different fromthe phrases learned in training, which might re-sult in practice in a missed opportunity for ahigh-quality translation.
In this case, if we useparaphrases for these pieces of text, then wemight improve the opportunity for the transla-tion to approach the reference, especially in thecase where only one reference is available.2.2 CoverageAs to the first aspect ?
coverage ?
we argue thatthe coverage rate of the new words or unknownwords are more and more becoming a ?bottleneck?for resource-limited languages.
Furthermore, cur-rent SMT systems, either phrase-based (Koehn et al,2003; Chiang, 2005) or syntax-based (Zollmann andVenugopal, 2006), use phrases as the fundamentaltranslation unit, so how much the phrase table andtraining data can cover the test set is an importantfactor which influences the translation quality.
Ta-ble 1 shows the statistics of the coverage of the testset on English-to-Chinese FBIS data, where we cansee that the coverage of unigrams is very high, es-pecially when the data is increased to the mediumsize (200K), where unigram coverage is greater than90%.
Based on the observations of the unknown un-42120K Cov.
(%) 200K Cov.
(%)PL Tset PT Corpus in PT in Corpus PT Corpus in PT in Corpus1 5,369 3,785 4,704 70.5 87.61 4,941 5,230 92.03 97.412 24,564 8,631 15,109 35.14 61.51 16,803 21,071 68.40 85.783 37,402 4,538 12,091 12.13 32.33 12,922 22,531 34.55 60.244 41,792 1,703 6,150 4.07 14.72 5,974 14,698 14.29 35.175 43,008 626 2,933 1.46 6.82 2,579 8,425 5.99 19.596 43,054 259 1,459 0.6 3.39 1,192 4,856 2.77 11.287 42,601 119 821 0.28 1.93 581 2,936 1.36 6.898 41,865 51 505 0.12 1.21 319 1,890 0.76 4.519 40,984 34 341 0.08 0.83 233 1,294 0.57 3.1610 40,002 22 241 0.05 0.6 135 923 0.34 2.31Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of thetraining data.
?PL?
indicates the Phrase Length N , where {1 <= N <= 10}; ?20K?
and ?200K?
represent the sizesof the parallel data for model training and phrase extraction; ?Cov.?
indicates the coverage rate; ?Tset?
represents thenumber of unique phrases with the length N in the Test Set; ?PT?
represents the number of phrases of the Test Setoccur in the Phrase Table; ?Corpus?
indicates the number of phrases of the Test Set appearing in the parallel corpus;?in PT?
indicates the coverage of the phrases in the Test Set by the phrase table and correspondingly ?in Corpus?represents the coverage of the phrases in the Test Set by the Parallel Corpus.igrams, we found that most are named entities (NEs)such as person name, location name, etc.
From thebigram phrases, the coverage rates begin to signifi-cantly decline.
It can also be seen that phrases con-taining more than 5 words rarely appear either in thephrase table or in the parallel corpus, which indi-cates that data sparseness is severe for long phrases.Even if the size of the corpus is significantly in-creased (e.g.
from 20K to 200K), the coverage oflong phrases is still quite low.With respect to these three aspects of the transla-tion difficulty, especially for data-limited languagepairs, we propose a more effective method to makeuse of the paraphrases to facilitate translation pro-cess.3 Paraphrase Lattice for Input SentencesIn this Section, we propose a novel method to em-ploy paraphrases to reduce the translation difficultyand in so doing increase the translation quality.3.1 MotivationOur idea to build a paraphrase lattice for SMT is in-spired by the following points:?
Handling unknown words is a challenging issuefor SMT, and using paraphrases is an effectiveway to facilitate this problem (Callison-Burchet al, 2006);?
The method of paraphrase substitution does notshow any significant improvement, especiallyon a large-scale data set in terms of BLEU (Pa-pineni et al, 2002) scores (Callison-Burch etal., 2006);?
Building a paraphrase lattice might providemore translation options to the decoder so thatit can flexibly search for the best path.The major contributions of our method are:?
We consider all N -gram phrases rather thanonly unknown phrases in the test set, where{1 <= N <= 10};?
We utilise lattices rather than simple substitu-tion to facilitate the translation process;?
We propose an empirical weight estimationmethod to set weights for edges in the word lat-tice, which is detailed in Section 3.4.3.2 Paraphrase AcquisitionParaphrases are alternative ways to express the sameor similar meaning given a certain original word,phrase or segment.
The paraphrases used in ourmethod are generated from the parallel corporabased on the algorithm in (Bannard and Callison-Burch, 2005), in which paraphrases are identified422by pivoting through phrases in another language.In this algorithm, the foreign language translationsof an English phrase are identified, all occurrencesof those foreign phrases are found, and all Englishphrases that they translate as are treated as potentialparaphrases of the original English phrase (Callison-Burch et al, 2006).
A paraphrase has a probabilityp(e2|e1) which is defined as in (2):p(e2|e1) =?fp(f |e1)p(e2|f) (1)where the probability p(f |e1) is the probability thatthe original English phrase e1 translates as a particu-lar phrase f in the other language, and p(e2|f) is theprobability that the candidate paraphrase e2 trans-lates as the foreign language phrase.p(e2|f) and p(f |e1) are defined as the transla-tion probabilities which can be calculated straight-forwardly using maximum likelihood estimation bycounting how often the phrases e and f are alignedin the parallel corpus as in (2) and (3):p(e2|f) ?count(e2, f)?e2 count(e2, f)(2)p(f |e1) ?count(f, e1)?f count(f, e1)(3)3.3 Construction of Paraphrase LatticeTo present paraphrase options to the PB-SMT de-coder, lattices with paraphrase options are con-structed to enrich the source-language sentences.The construction process takes advantage of the cor-respondence between detected paraphrases and po-sitions of the original words in the input sentence,then creates extra edges in the lattices to allow thedecoder to consider paths involving the paraphrasewords.An toy example is illustrated in Figure 1: givena sequence of words {w1, .
.
.
, wN} as the input,two phrases ?
= {?1, .
.
.
, ?p} and ?
= {?1, .
.
.
, ?q}are detected as paraphrases for S1 = {wx, .
.
.
, wy}(1 ?
x ?
y ?
N ) and S2 = {wm, .
.
.
, wn}(1 ?
m ?
n ?
N ) respectively.
The followingsteps are taken to transform them into word lattices:1.
Transform the original source sentence intoword lattices.
N + 1 nodes (?k, 0 ?
k ?
N )... ...wx wm...
wy...1...Source sidesentenceGeneratedlattice!1 !2 ?
!p1  2 ?
qParaphrase AParaphrase B!1!2 ... !p2q... ... wn...wx wm wy wn... ...
...Figure 1: An example of lattice-based paraphrases for aninput sentenceare created, and N edges (referred to as ?ORG-E?
edges) labeled with wi (1 ?
i ?
N ) aregenerated to connect them sequentially.2.
Generate extra nodes and edges for each of theparaphrases.
Taking ?
as an example, firstly,p ?
1 nodes are created, and then p edges(referred as ?NEW-E?
edges) labeled with ?j(1 ?
j ?
p) are generated to connect node?x?1, p?
1 nodes and ?y?1.Via step 2, word lattices are generated by addingnew nodes and edges coming from paraphrases.Note that to build word lattices, paraphrases withmulti-words are broken into word sequences, andeach of the words produces one extra edge in theword lattices as shown in the bottom part in Figure 1.Figure 2 shows an example of constructing theword lattice for an input sentence which is from thetest set used in our experiments.1 The top part inFigure 2 represents nodes (double-line circles) andedges (solid lines) that are constructed by the orig-inal words from the input sentence, while the bot-tom part in Figure 2 indicates the final word latticewith the addition of new nodes (single-line circles)and new edges (dashed lines) which come from theparaphrases.
We can see that the paraphrase latticeincreases the diversity of the source phrases so that itcan provide more flexible translation options duringthe decoding process.1Figure 2 contains paths that are duplicates except for theweights.
We plan to handle this in future work.423Figure 2: An example of how to build a paraphrase lattice for an input sentence3.4 Weight EstimationEstimating and normalising the weight for each edgein the word lattice is a challenging issue when theedges come from different sources.
In this section,we propose an empirical method to set the weightsfor the edges by distinguishing the original (?ORG-E?)
and new (?NEW-E?)
edges in the lattices.
Theaim is to utilize the original sentences as the ref-erences to weight the edges from paraphrases, sothat decoding paths going through ?ORG-E?
edgeswill tend to have higher scores than those which use?NEW-E?
ones.
The assumption behind this is thatthe paraphrases are alternatives for the original sen-tences, so decoding paths going though them oughtto be penalised.Therefore, for all the ?ORG-E?
edges, theirweights in the lattice are set to 1.0 as the reference.Thus, in the log-linear model, decoding paths goingthough these edges are not penalised because theydo not come from the paraphrases.By contrast, ?NEW-E?
are divided into twogroups for the calculation of weights:?
For ?NEW-E?
edges which are outgoing edgesof the lattice nodes that come from the originalsentences, the probabilities p(es|ei)2 of their2es indicates the source phrase S, ei represents one of thecorresponding paraphrases are utilised to pro-duce empirical weights.
Supposing that a set ofparaphrases X = {x1, .
.
.
, xk} start at node Awhich comes from the original sentence, so thatX are sorted descendingly based on the proba-bilities p(es|ei), their corresponding edges fornode A are G = {g1, .
.
.
, gk}, then the weightsare calculated as in (4):w(ei) =1k + i(1 <= i <= k) (4)where k is a predefined parameter to trade offbetween decoding speed and the number ofpotential paraphrases being considered.
Thus,once a decoding path goes though one of theseedges, it will be penalised according to its para-phrase probabilities.?
For all other ?NEW-E?
edges, their weightsare set to 1.0, because the paraphrase penaltyhas been counted in their preceding ?NEW-E?edges.Figure 2 illustrates the weight estimation results.Nodes coming from the original sentences are drawnin double-line circles (e.g.
nodes 0 to 7), whileparaphrases of S.424nodes created from paraphrases are shown in single-line circles (e.g.
nodes 8 to 10).
?ORG-E?
edges aredrawn in solid lines and ?NEW-E?
edges are shownusing dashed lines.
As specified previously, ?ORG-E?
edges are all weighted by 1.0 (e.g.
edge labeled?the?
from node 0 to 1).
By contrast, ?NEW-E?edges in the first group are weighted by equation(4) (e.g.
edges in dashed lines start from node 0to node 2 and 8), while others in the second groupare weighted by 1.0 (e.g.
edge labeled ?training?from node 8 to 2).
Note that penalties of the pathsgoing through paraphrases are counted by equation(4), which is represented by the weights of ?NEW-E?
edges in the first group.
For example, startingfrom node 2, paths going to node 9 and 10 are pe-nalised because lattice weights are also consideredin the log-linear model.
However, other edges donot imply penalties since their weights are set to 1.0.The reason to set al weights for the ?ORG-E?edges to a uniform weight (e.g.
1.0) instead of alower empirical weight is to avoid excessive penal-ties for the original words.
For example, in Fig-ure 2, the original edge from node 3 to 4 (con-tinue) has a weight of 1.0, so the paths going thoughthe original edges from node 2 to 4 (will continue)have a higher lattice score (1.0 ?
1.0 = 1.0) thanthe paths going through the edges of paraphrases(e.g.
will resume (score: 0.125 ?
1.0 = 0.125) andwill go (score: 0.11 ?
1.0 = 0.11)), or any othermixed paths that goes through original edges andparaphrase edges, such as will continuous (score:1.0 ?
0.125 = 0.125).
The point is that we shouldhave more trust when translating the original words,but if we penalise (set weights < 1.0) the ?ORG-E?
edges whenever there is a paraphrase for them,then when considering the context of the lattice,paraphrases will be favoured systematically.
That iswhy we just penalise the ?NEW-E?
edges in the firstgroup and set other weights to 1.0.As to unknown words in the input sentence, evenif we give them a prioritised weight, they wouldbe severely penalised in the decoding process.
Sowe do not need to distinguish unknown words whenbuilding and weighting the paraphrase lattice.4 Experiments4.1 System and Data PreparationFor our experiments, we use Moses (Koehn et al,2007) as the baseline system which can supportlattice decoding.
We also realise a paraphrasesubstitution-based system (Para-Sub)3 based on themethod in (Callison-Burch, 2006) to compare withthe baseline system and our proposed paraphraselattice-based (Lattice) system.The alignment is carried out by GIZA++ (Ochand Ney, 2003) and then we symmetrized the wordalignment using the grow-diag-final heuristic.
Themaximum phrase length is 10 words.
Parameter tun-ing is performed using Minimum Error Rate Train-ing (Och, 2003).The experiments are conducted on English-to-Chinese translation.
In order to fully compare ourproposed method with the baseline and the ?Para-Sub?
system, we perform the experiments on threedifferent sizes of training data: 20K, 200K and 2.1million pairs of sentences.
The former two sizes ofdata are derived from FBIS,4 and the latter size ofdata consists of part of HK parallel corpus,5 ISI par-allel data,6 other news data and parallel dictionar-ies from LDC.
All the language models are 5-gramwhich are trained on the monolingual part of paralleldata.The development set (devset) and the test set forexperiments using 20K and 200K data sets are ran-domly extracted from the FBIS data.
Each set in-cludes 1,200 sentences and each source sentencehas one reference.
For the 2.1 million data set, weuse a different devset and test set in order to verifywhether our proposed method can work on a lan-guage pair with sufficient resources.
The devset isthe NIST 2005 Chinese-English current set whichhas only one reference for each source sentence andthe test set is the NIST 2003 English-to-Chinesecurrent set which contains four references for eachsource sentence.
All results are reported in BLEUand TER (Snover et al, 2006) scores.3We use ?Para-Sub?
to represent their system in the rest ofthis paper.4This is a multilingual paragraph-aligned corpus with LDCresource number LDC2003E14.5LDC number: LDC2004T08.6LDC number: LDC2007T09.42520K 200KSYS BLEU CI 95% pair-CI 95% TER BLEU CI 95% pair-CI 95% TERBaseline 14.42 [-0.81, +0.74] ?
75.30 23.60 [-1.03, +0.97] ?
63.56Para-Sub 14.78 [-0.78, +0.82] [+0.13, +0.60] 73.75 23.41 [-1.04, +1.00] [-0.46, +0.09] 63.84Lattice 15.44 [-0.85, +0.84] [+0.74, +1.30] 73.06 25.20 [-1.11, +1.15] [+1.19, +2.01] 62.37Table 2: Comparison between the baseline, ?Para-Sub?
and our ?Lattice?
(paraphrase lattice) method.The paraphrase data set used in our lattice-basedand the ?Para-Sub?
systems is same which is de-rived from the ?Paraphrase Phrase Table?7 of TER-Plus (Snover et al, 2009).
The parameter k in equa-tion 4 is set to 7.4.2 Paraphrase FilteringThe more edges there are in a lattice, the morecomplicated the decoding is in the search process.Therefore, in order to reduce the complexity of thelattice and increase decoding speed, we must fil-ter out some potential noise in the paraphrase table.Two measures are taken to optimise the paraphraseswhen building a paraphrase lattice:?
Firstly, we filter out all the paraphrases whoseprobability is less than 0.01;?
Secondly, given a source-side input sentence,we retrieve all possible paraphrases and theirprobabilities for source-side phrases which ap-pear in the paraphrase table.
Then we removethe paraphrases which are not occurred in the?phrase table?
of the SMT system.
This mea-sure intends to avoid adding new ?unknownwords?
to the source-side sentence.
Afterthis measure, we can acquire the final para-phrases which can be denoted as a quadru-ple < SEN ID,Span, Para, Prob >, where?SEN ID?
indicates the ID of the input sen-tence, ?Span?
represents the span of the source-side phrase in the original input sentence,?Para?
indicates the paraphrase of the source-side phrase, and ?Prob?
represents the probabil-ity between the source-side phrase and its para-phrase, which is used to set the weight of theedge in the lattice.
The quadruple is used toconstruct the weighted lattice.7http://www.umiacs.umd.edu/?snover/terp/downloads/terp-pt.v1.tgz.4.3 Experimental ResultsThe experimental results conducted on small andmedium-sized data sets are shown in Table 2.
The95% confidence intervals (CI) for BLEU scores areindependently computed on each of three systems,while the ?pair-CI 95%?
are computed relative tothe baseline system only for ?Para-Sub?
and ?Lat-tice?
systems.
All the significance tests use boot-strap and paired-bootstrap resampling normal ap-proximation methods (Zhang and Vogel, 2004).8Improvements are considered to be significant if theleft boundary of the confidence interval is largerthan zero in terms of the ?pair-CI 95%?.
It canbe seen that 1) our ?Lattice?
system outperformsthe baseline by 1.02 and 1.6 absolute (7.07% and6.78% relative) BLEU points in terms of the 20Kand 200K data sets respectively, and our system alsodecreases the TER scores by 2.24 and 1.19 (2.97%and 1.87% relative) points than the baseline system.In terms of the ?pair-CI 95%?, the left boundariesfor 20K and 200K data are respectively ?+0.74?
and?+1.19?, which indicate that the ?Lattice?
system issignificantly better than the baseline system on thesetwo data sets.
2) The ?Para-Sub?
system performsslightly better (0.36 absolute BLEU points) than thebaseline system on the 20K data set, but slightlyworse (0.19 absolute BLEU points) than the baselineon the 200K data set, which indicates that the para-phrase substitution method used in (Callison-Burchet al, 2006) does not work on resource-sufficientdata sets.
In terms of the ?pair-CI 95%?, the leftboundary for 20K data is ?+0.13?, which indicatesthat it is significantly better than the baseline sys-tem, while the left boundary is ?-0.46?
for 200Kdata, which indicates that the ?Para-Sub?
systemis significantly worse than the baseline system.
3)comparing the ?Lattice?
system with the ?Para-Sub?8http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm.426SYS BLEU CI 95% pair-CI 95% NIST TERBaseline 14.04 [-0.73, +0.40] ?
6.50 74.88Para-Sub 14.13 [-0.56, +0.56] [-0.18, +0.40] 6.52 74.43Lattice 14.55 [-0.75, +0.32] [+0.15,+0.83] 6.55 73.28Table 3: Comparison between the baseline and our paraphrase lattice method on a large-scale data set.system, the ?pair-CI 95%?
for 20K and 200K dataare respectively [+0.41, +0.92] and [+1.40, +2.17],which indicates that the ?Lattice?
system is signif-icantly better than the ?Para-Sub?
system on thesetwo data sets as well.
4) In terms of the two metrics,our proposed method achieves the best performance,which shows that our method is effective and consis-tent on different sizes of data.In order to verify our method on large-scaledata, we also perform experiments on 2.1 millionsentence-pairs of English-to-Chinese data as de-scribed in Section 4.1.
The results are shown in Ta-ble 3.
From Table 3, it can be seen that the ?Lattice?system achieves an improvement of 0.51 absolute(3.63% relative) BLEU points and a decrease of 1.6absolute (2.14% relative) TER points compared tothe baseline.
In terms of the ?pair-CI 95%?, the leftboundary for the ?Lattice?
system is ?+0.15?
whichindicates that it is significantly better than the base-line system in terms of BLEU.
Interestingly, in ourexperiment, the ?Para-Sub?
system also outperformsthe baseline on those three automatic metrics.
How-ever, in terms of the ?pair-CI 95%?, the left bound-ary for the ?Para-Sub?
system is ?-0.18?
which indi-cates that it is not significantly better than the base-line system in terms of BLEU.
The results also showthat our proposed method is effective and consistenteven on a large-scale data set.It also can be seen that the improvement on 2.1million sentence-pairs is less than that of the 20Kand 200K data sets.
That is, as the size of the train-ing data increases, the problems of data sparsenessdecrease, so that the coverage of the test set by theparallel corpus will correspondingly increase.
In thiscase, the role of paraphrases in decoding becomes alittle weaker.
On the other hand, it might become akind of noise to interfere with the exact translationof the original source-side phrases when decoding.Therefore, our proposed method may be more ap-propriate for language pairs with limited resources.5 Analysis5.1 Coverage of Paraphrase Test SetThe coverage rate of the test set by the phrase ta-ble is an important factor that could influence thetranslation result, so in this section we examine thecharacteristics of the updated test set that adds in theparaphrases.
We take the 200K data set to examinethe coverage issue.
Table 4 is an illustration to com-pare the new coverage and the old coverage (withoutparaphrases) on medium sized training data.PL Tset PT New Cov.
(%) Old Cov.
(%)1 9,264 8,994 97.09 92.032 32,805 25,796 78.63 68.403 39,918 15,708 39.35 34.554 42,247 6,479 15.34 14.295 43,088 2,670 6.20 5.996 43,066 1,204 2.80 2.777 42,602 582 1.37 1.368 41,865 319 0.76 0.769 40,984 233 0.57 0.5710 40,002 135 0.34 0.34Table 4: The coverage of the paraphrase-added test set bythe phrase table on medium size of the training data.From Table 4, we can see that the coverage of un-igrams, bigrams, trigrams and 4-grams goes up byabout 5%, 10%, 5% and 1%, while from 5-gramsthere is only a slight or no increase in coverage.These results show that 1) most of the paraphrasesthat are added in are lower-order n-grams; 2) theparaphrases can increase the coverage of the inputby handling the unknown words to some extent.However, we observed that most untranslatedwords in the ?Para-Sub?
and ?Lattice?
systems arestill NEs, which shows that in our paraphrase table,there are few paraphrases for the NEs.
Therefore,to further improve the translation quality using para-phrases, we also need to acquire the paraphrases forNEs to increase the coverage of unknown words.427Source:     whether or the albanian rebels can be genuinely disarmed completely is the main challenge to nato .Ref:           ??
??
??
?
??
??
?
??
?
??
??
?
??
??
?Baseline:   ??
?
??
??
?
?
??
disarmed    ??
?
??
?
??
??
?Para-Sub:  ??
??
??
??
??
??
??
??
?
??
?
??
??
?Lattice:      ??
??
??
??
??
??
??
?
??
??
?
??
?
??
??
?Figure 3: An example from three systems to compare the processing of OOVs5.2 Analysis on Translation ResultsIn this section, we give an example to show the ef-fectiveness of using paraphrase lattices to deal withunknown words.
The example is evaluated accord-ing to both automatic evaluation and human evalua-tion at sentence level.See Figure 3 as an illustration of how theparaphrase-based systems process unknown words.According to the word alignments between thesource-side sentence and the reference, the word?disarmed?
is translated into two Chinese words ????
and ????.
These two Chinese words are dis-continuous in the reference, so it is difficult for thePB-SMT system to correctly translate the single En-glish word into a discontinuous Chinese phrase.
Infact in this example, ?disarmed?
is an unknownwordand it is kept untranslated in the result of the base-line system.
In the ?Para-Sub?
system, it is trans-lated into ?`?
based on a paraphrase pair PP1 =?disarmed ?
disarmament ?
0.087?
and its transla-tion pair T1 = ?disarmament ?
`?.
The number?0.087?
is the probability p1 that indicates to whatextent these two words are paraphrases.
It can beseen that although ?`?
is quite different from themeaning of ?disarmed?, it is understandable for hu-man in some sense.
In the ?Lattice?
system, theword ?disarmed?
is translated into three Chinesewords ?
: / ???
based on a paraphrase pairPP2 = ?disarmed ?
demilitarized ?
0.099?
and itstranslation pair T2 = ?demilitarized ?
: / ???.
The probability p2 is slightly greater than p1.We argue that the reason that the ?Lattice?
systemselects PP2 and T2 rather than PP1 and T1 is be-cause of the weight estimation in the lattice.
Thatis, PP2 is more prioritised, while PP1 is more pe-nalised based on equation (4).From the viewpoint of human evaluation, theparaphrase pair PP2 is more appropriate than PP1,and the translation T2 is more similar to the origi-nal meaning than T1.
The sentence-level automaticevaluation scores for this example in terms of BLEUand TER metrics are shown in Table 5.SYS BLEU TERBaseline 20.33 66.67Para-Sub 21.78 53.33Lattice 23.51 53.33Table 5: Comparison on sentence-level scores in terms ofBLEU and TER metrics.The BLEU score of the ?Lattice?
system is muchhigher than the baseline, and the TER score is quitea bit lower than the baseline.
Therefore, from theviewpoint of automatic evaluation, the translationfrom the ?Lattice?
system is also better than thosefrom the baseline and ?Para-Sub?
systems.6 Conclusions and Future WorkIn this paper, we proposed a novel method usingparaphrase lattices to facilitate the translation pro-cess in SMT.
Given an input sentence, our methodfirstly discovers all possible paraphrases from aparaphrase database for N -grams (1 <= N <= 10)in the test set, and then filters out the paraphraseswhich do not appear in the phrase table in order toavoid adding new unknown words on the input side.We then use the original words and the paraphrasesto build a word lattice, and set the weights to priori-tise the original edges and penalise the paraphraseedges.
Finally, we import the lattice into the de-coder to perform lattice decoding.
The experimentsare conducted on English-to-Chinese translation us-ing the FBIS data set with small and medium-sizedamounts of data, and on a large-scale corpus of 2.1428million sentence pairs.
We also performed compar-ative experiments for the baseline, the ?Para-Sub?system and our paraphrase lattice-based system.
Theexperimental results show that our proposed systemsignificantly outperforms the baseline and the ?Para-Sub?
system, and the effectiveness is consistent onthe small, medium and large-scale data sets.As for future work, firstly we plan to propose apruning algorithm for the duplicate paths in the lat-tice, which will track the edge generation with re-spect to the path span, and thus eliminate duplicatepaths.
Secondly, we plan to experiment with anotherfeature function in the log-linear model to discountwords derived from paraphrases, and use MERT toassign an appropriate weight to this feature function.AcknowledgmentsMany thanks to the reviewers for their insight-ful comments and suggestions.
This work is sup-ported by Science Foundation Ireland (Grant No.07/CE/I1142) as part of the Centre for Next Genera-tion Localisation (www.cngl.ie) at Dublin City Uni-versity.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In 43rd An-nual meeting of the Association for ComputationalLinguistics, Ann Arbor, MI, pages 597?604.Chris Callison-Burch, Philipp Koehn and Miles Osborne.2006.
Improved statistical machine translation usingparaphrases.
In Proceedings of HLT-NAACL 2006:Proceedings of the Human Language Technology Con-ference of the North American Chapter of the ACL,NY, USA, pages 17?24.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In 43rd Annualmeeting of the Association for Computational Linguis-tics, Ann Arbor, MI, pages 263?270.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003: conference combining Hu-man Language Technology conference series and theNorth American Chapter of the Association for Com-putational Linguistics conference series, Edmonton,Canada, pages 48?54.Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, Wade Shen, C.Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin andEvan Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In ACL 2007: demoand poster sessions, Prague, Czech Republic, pages177?180.Preslav Nakov.
2008.
Improving English-Spanish sta-tistical machine translation: experiments in domainadaptation, sentence paraphrasing, tokenization, andrecasing.
In Proceedings of ACL-08:HLT.
Third Work-shop on Statistical Machine Translation, Columbus,Ohio, USA, pages 147?150.Franz Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
In 41st Annual meetingof the Association for Computational Linguistics, Sap-poro, Japan, pages 160?167.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: aMethod for Automatic Eval-uation of Machine Translation.
In 40th Annual meet-ing of the Association for Computational Linguistics,Philadelphia, PA, pages 311?318.Josh Schroeder, Trevor Cohn and Philipp Koehn.
2009.Word Lattices for Multi-source Translation.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL, Athens, Greece, pages 719?727.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas, Cam-bridge, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie J.Dorr andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, Athens,Greece, pages 259?268.Ying Zhang and Stephan Vogel.
2004.
Measuring Con-fidence Intervals for the Machine Translation Evalua-tion Metrics.
In Proceedings of the 10th InternationalConference on Theoretical and Methodological Issuesin Machine Translation (TMI), pages 85?94.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings of HLT-NAACL 2006: Proceedings ofthe Workshop on Statistical Machine Translation, NewYork, pages 138?141.429
