Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 907?916,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLearning Graph Walk Based Similarity Measures for Parsed TextEinat Minkov?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAeinat@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, USAwcohen@cs.cmu.eduAbstractWe consider a parsed text corpus as an in-stance of a labelled directed graph, wherenodes represent words and weighted directededges represent the syntactic relations be-tween them.
We show that graph walks, com-bined with existing techniques of supervisedlearning, can be used to derive a task-specificword similarity measure in this graph.
We alsopropose a new path-constrained graph walkmethod, in which the graph walk process isguided by high-level knowledge about mean-ingful edge sequences (paths).
Empirical eval-uation on the task of named entity coordinateterm extraction shows that this framework ispreferable to vector-based models for small-sized corpora.
It is also shown that the path-constrained graph walk algorithm yields bothperformance and scalability gains.1 IntroductionGraph-based similarity measures have been usedfor a variety of language processing applications.In this paper we assume directed graphs, wheretyped nodes denote entities and labelled directedand weighted edges denote the relations betweenthem.
In this framework, graph walks can be ap-plied to draw a measure of similarity between thegraph nodes.
Previous works have applied graphwalks to draw a notion of semantic similarity oversuch graphs that were carefully designed and man-ually tuned, based on WordNet reations (Toutanova?Current address: Nokia Research Center Cambridge, Cam-bridge, MA 02142, USA.et al, 2004; Collins-Thompson and Callan, 2005;Hughes and Ramage, 2007).While these and other researchers have usedWordNet to evaluate similarity between words, therehas been much interest in extracting such a measurefrom text corpora (e.g., (Snow et al, 2005; Pado?and Lapata, 2007)).
In this paper, we suggest pro-cessing dependency parse trees within the generalframework of directed labelled graphs.
We constructa graph that directly represents a corpus of struc-tured (parsed) text.
In the suggested graph scheme,nodes denote words and weighted edges representthe dependency relations between them.
We applygraph walks to derive an inter-word similarity mea-sure.
We further apply learning techniques, adaptedto this framework, to improve the derived corpus-based similarity measure.The learning methods applied include existinglearning techniques, namely edge weight tuning,where weights are associated with the edge types,and discriminative reranking of graph nodes, usingfeatures that describe the possible paths between agraph node and the initial ?query nodes?
(Minkovand Cohen, 2007).In addition, we outline in this paper a novelmethod for learning path-constrained graph walks.While reranking allows use of high-level featuresthat describe properties of the traversed paths, thesuggested algorithm incorporates this information inthe graph walk process.
More specifically, we allowthe probability flow in the graph to be conditionedon the history of the walk.
We show that this methodresults in improved performance as it directs proba-bility flow to meaningful paths.
In addition, it leads907to substantial gains in terms of runtime performance.The graph representation and the set of learningtechniques suggested are empirically evaluated onthe task of coordinate term extraction1 from smallto moderately sized corpora, where we comparethem against vector-based models, including a state-of-the-art syntactic distributional similarity method(Pado?
and Lapata, 2007).
It is shown that the graphwalk based approach gives preferable results for thesmaller datasets (and comparable otherwise), wherelearning yields significant gains in accuracy.There are several contributions of this paper.First, we represent dependency-parsed corporawithin a general graph walk framework, and deriveinter-word similarity measures using graph walksand learning techniques available in this framework.To our knowledge, the application of graph walks toparsed text in general, and to the extraction of coor-dinate terms in particular, is novel.
Another maincontribution of this paper is the path-constrainedgraph walk variant, which is a general learning tech-nique for calculating the similarity between graphnodes in directed and labelled graphs.Below we first outline our proposed scheme forrepresenting a dependency-parsed text corpus as agraph, and provide some intuitions about the asso-ciated similarity metric (Section 2).
We then givean overview of the graph-walk based similarity met-ric (Section 3), as well as the known edge weighttuning and reranking learning techniques (Section4).
We next present the proposed algorithm of path-constrained graph walks (Section 5).
The paper pro-ceeds with a review of related work (Section 6), adiscussion of the coordinate term extraction task,empirical evaluation and our conclusions (Sections7-9).2 Representing a Corpus as a GraphA typed dependency parse tree consists of directedlinks between words, where dependencies are la-belled with the relevant grammatical relation (e.g.,nominal subject, indirect object etc.).
We suggestrepresenting a text corpus as a connected graphof dependency structures, according to the schemeshown in Figure 1.
The graph shown in the figure1In particular, we focus on the extraction of named entityclasses.Figure 1: The suggested graph schema, demonstrated fora two-sentence corpus.includes the dependency analysis of two sentences:?boys like playing with all kinds of cars?, and ?girlslike playing with dolls?.
In the graph, each wordmention is represented as a node, which includes theindex of the sentence in which it appears, as wellas its position within the sentence.
Word mentionsare marked as circles in the figure.
The ?type?
ofeach word ?
henceforth a term node ?
is denoted bya square in the figure.
Each word mention is linkedto the corresponding term; for example, the nodes?like1?
and ?like2?
represent distinct word mentionsand both nodes are linked to the term ?like?.
Forevery edge in the graph, we add another edge in theopposite direction (not shown in the figure); for ex-ample, an inverse edge exists from ?like1?
to ?girls1?with an edge labelled as ?nsubj-inv?.
The resultinggraph is highly interconnected and cyclic.We will apply graph walks to derive an extendedmeasure of similarity, or relatedness, between wordterms (as defined above).
For example, starting fromthe term ?girls?, we will reach the semantically re-lated term ?boys?
via the following two paths:(1) girls mention??
girls1 nsubj??
like1 as?term??
like mention?
?like2 nsubj?inverse??
boys2 as?term??
boys(2) girls mention??
girls1 nsubj??
like1 partmod??
playing1as?term??
playing mention??
playing2 partmod?inverse??
like2nsubj?inverse??
boys2 as?term??
boys .Intuitively, in a graph representing a large cor-pus, terms that are more semantically related willbe linked by a larger number of connecting paths.
Inaddition, shorter connecting paths may be in generalmore meaningful.
In the next section we show that908the graph walk paradigm addresses both of these re-quirements.
Further, different edge types, as well asthe paths traversed, are expected to have varying im-portance in different types of word similarity (for ex-ample, verbs and nouns are associated with differentconnectivity patterns).
These issues are addressedusing learning.3 Graph Walks and Similarity QueriesThis section provides a quick overview of the graphwalk induced similarity measure.
For details, thereader is referred to previous publications (e.g.,(Toutanova et al, 2004; Minkov and Cohen, 2007)).In summary, similarity between two nodes in thegraph is defined by a weighted graph walk process,where an edge of type ` is assigned an edge weight,?`, determined by its type.2 The transition proba-bility of reaching node y from node x over a singletime step, Pr(x ??
y), is defined as the weightof their connecting edge, ?l, normalized by the to-tal outgoing weight from x.
Given these transitionprobabilities, and starting from an initial distribu-tion Vq of interest (a query), we perform a graphwalk for a finite number of steps K. Further, at eachstep of the walk, a proportion ?
of the probabilitymass at every node is emitted.
Thus, this model ap-plies exponential decay on path length.
The finalprobability distribution of this walk over the graphnodes, which we denote as R, is computed as fol-lows: R = ?Ki=1 ?iVqMi, where M is the transi-tion matrix.3 The answer to a query, Vq, is a list ofnodes, ranked by the scores in the final distributionR.
In this multi-step walk, nodes that are reachedfrom the query nodes by many shorter paths will beassigned a higher score than nodes connected overfewer longer paths.4 LearningWe consider a supervised setting, where we aregiven a dataset of example queries and labels overthe graph nodes, indicating which nodes are relevantto which query.
For completeness, we describe heretwo methods previously described by Minkov and2In this paper, we consider either uniform edge weights; or,learn the set of weights ?
from examples.3We tune K empirically and set ?
= 0.5, as in (Minkov andCohen, 2007).Cohen (Minkov and Cohen, 2007): a hill-climbingmethod that tunes the graph weights; and a rerankingmethod.
We also specify the feature set to be used bythe reranking method in the domain of parsed text.4.1 Weight TuningThere are several motivations for learning the graphweights ?
in this domain.
First, some dependencyrelations ?
foremost, subject and object ?
are in gen-eral more salient than others (Lin, 1998; Pado?
andLapata, 2007).
In addition, dependency relationsmay have varying importance per different notionsof word similarity (e.g., noun vs. verb similarity(Resnik and Diab, 2000)).
Weight tuning allows theadaption of edge weights to each task (i.e., distribu-tion of queries).The weight tuning method implemented in thiswork is based on an error backpropagation hillclimbing algorithm (Diligenti et al, 2005).
The al-gorithm minimizes the following cost function:E = 1N?z?Nez =1N?z?N12(pz ?
pOptz )2where ez is the error for a target node z defined as thesquared difference between the final score assignedto z by the graph walk, pz , and some ideal score ac-cording to the example?s labels, pOptz .4 Specifically,pOptz is set to 1 in case that the node z is relevantor 0 otherwise.
The error is averaged over a set ofexample instantiations of size N .
The cost functionis minimized by gradient descent where the deriva-tive of the error with respect to an edge weight ?`is derived by decomposing the walk into single timesteps, and considering the contribution of each nodetraversed to the final node score.4.2 Node RerankingReranking of the top candidates in a ranked listhas been successfully applied to multiple NLP tasks(Collins, 2002; Collins and Koo, 2005).
In essence,discriminative reranking allows the re-ordering ofresults obtained by methods that perform some formof local search, using features that encode higherlevel information.4For every example query, a handful of the retrieved nodesare considered, including both relevant and irrelevant nodes.909A number of features describing the set of pathsfrom Vq can be conveniently computed in the pro-cess of executing the graph walk, and it has beenshown that reranking using these features can im-prove results significantly.
It has also been shownthat reranking is complementary to weight tuning(Minkov and Cohen, 2007), in the sense that thetwo techniques can be usefully combined by tuningweights, and then reranking the results.In the reranking approach, for every training ex-ample i (1 ?
i ?
N ), the reranking algorithm isprovided with the corresponding output ranked listof li nodes.
Let zij be the output node ranked at rankj in li, and let pzij be the probability assigned to zijby the graph walk.
Each output node zij is repre-sented through m features, which are computed bypre-defined feature functions f1, .
.
.
, fm.
The rank-ing function for node zij is defined as:F (zij , ??)
= ?0log(pzij ) +m?k=1?kfk(zij)where ??
is a vector of real-valued parameters.
Givena new test example, the output of the model is theoutput node list reranked by F (zij , ??).
To learn theparameter weights ?
?, we here applied a boostingmethod (Collins and Koo, 2005) (see also (Minkovet al, 2006)).4.2.1 FeaturesWe evaluate the following feature templates.Edge label sequence features indicate whether a par-ticular sequence of edge labels `i occurred, in aparticular order, within the set of paths leading tothe target node zij .
Lexical unigram feature indi-cate whether a word mention whose lexical valueis tk was traversed in the set of paths leading tozij .
Finally, the Source-count feature indicates thenumber of different source query nodes that zij wasreached from.
The intuition behind this last fea-ture is that nodes linked to multiple query nodes,where applicable, are more relevant.
For exam-ple, for the query term ?girl?
in the graph depictedin Figure 1, the target node ?boys?
is describedby the features (denoted as feature-name.feature-value): sequence.nsubj.nsubj-inv (where mentionand as-term edges are omitted) , lexical.??like?
etc.In this work, the features encoded are binary.However, features can be assugned numeric weightsthat corresponds to the probability of the indicatorbeing true for any path between x and zij (Cohenand Minkov, 2006).5 Path-Constrained Graph WalkWhile node reranking allows the incorporation ofhigh-level features that describe the traversed paths,it is desirable to incorporate such information ear-lier in the graph walk process.
In this paper, wesuggest a variant of a graph-walk, which is con-strained by path information.
Assume that prelim-inary knowledge is available that indicates the prob-ability of reaching a relevant node after following aparticular edge type sequence (path) from the querydistribution Vq to some node x.
Rather than fix theedge weights ?, we can evaluate the weights of theoutgoing edges from node x dynamically, given thehistory of the walk (the path) up to this node.
Thisshould result in gains in accuracy, as paths that leadmostly to irrelevant nodes can be eliminated in thegraph walk process.
In addition, scalability gainsare expected, for the same reason.We suggest a path-constrained graph walk algo-rithm, where path information is maintained in acompact path-tree structure constructed based ontraining examples.
Each vertex in the path tree de-notes a particular walk history.
In applying the graphwalk, the nodes traversed are represented as a set ofnode pairs, comprised of the graph node and the cor-responding vertices in the path tree.
The outgoingedge weights from each node pair will be estimatedaccording to the respective vertex in the path tree.This approach needs to address two subtasks: learn-ing of the path-tree; and updating of the graph walkparadigm to co-sample from the graph and the pathtree.
We next describe these two components in de-tail.The Path-TreeWe construct a path-tree T using a training setof N example queries.
Let a path p be a sequenceof k < K edge types (where K is the maximumnumber of graph walk steps).
For each training ex-ample, we recover all of the connecting paths lead-ing to the top M (correct and incorrect) nodes.
Weconsider only acyclic paths.
Let each path p be as-sociated with its count, within the paths leading tothe correct nodes, denoted as C+p .
Similarly, the910Figure 2: An example path-tree.count within paths leading to the negatively labellednodes is denoted C?p .
The full set of paths observedis then represented as a tree.5 The leaves of thetree are assigned a Laplace-smoothed probability:Pr(p) = C+p +1C+p +C?p +2.Given path probabilities, they are propagatedbackwards to all tree vertices, applying the MAXoperator.6 Consider the example given in Figure 2.The path-tree in the figure includes three paths (con-structed from edge types k, l,m, n).
The top partof the figure gives the paths?
associated counts, andthe bottom part of the figure gives the derived outgo-ing edge probabilities at each vertex.
This path-treespecifies, for example, that given an edge of type lwas traversed from the root, the probability of reach-ing a correct target node is 0.9 if an edge of type nis followed, whereas the respective probability if anedge of type m is followed is estimated at a lower0.2.A Concurrent Graph-walkGiven a generated path tree, we apply path-constrained graph walks that adhere both to thetopology of the graph G, and to the path tree T .Walk histories of each node x visited in the walkare compactly represented as pairs < t, x >, wheret denotes the relevant vertex in the path tree.
Forexample, suppose that after one walk step, the main-tained node-history pairs include < T (l), x1 > and< T (m), x2 >.
If x3 is reached in the next walk step5The conversion to a tree is straight-forward, where identicalpath prefixes are merged.6Another possibility is to average the downstream cumula-tive counts at each vertex.
The MAX operation gave better re-sults in our experiments.Given: graph G, path-tree T , query distribution V0,number of steps KInitialize: for each xi ?
V0, assign a pair< root(T ), xi >Repeat for steps k = 0 to K:For each < ti, xi >?
Vk:Let L be the set of outgoing edge labels from xi, in G.For each lm ?
L:For each xj ?
G s.t., xi lm??
xj , add < tj , xj > toVk+1, where tj ?
T , s.t.
ti lm??
tj , with probabilityPr(xi|Vk) ?
Pr(lm|ti, T ).
(The latter probabilitiesshould be normalized with respect to xi.
)If ti is a terminal node in T , emit xi with probabilityPr(xi|Vk)?
Pr(ti|T ).Figure 3: Pseudo-code for path-constrained graph walkfrom both x1 and x2 over paths included in the path-tree, it will be represented by multiple node pairs,e.g., < T (l ?
n), x3 > and < T (m ?
l, x3 >.A pseudo-code for a path-constrained graph walk isgiven in Figure 3.
It is straight-forward to discardpaths in T that are associated with a lower proba-bility than some threshold.
A threshold of 0.5, forexample, implies that only paths that led to a major-ity of positively labelled nodes in the training set arefollowed.6 Related WorkGraph walks over typed graphs have been appliedto derive semantic similarity for NLP problems us-ing WordNet as a primary information source.
Forinstance, Hughes and Ramage (2007) constructed agraph which represented various types of word re-lations from WordNet, and compared random-walksimilarity to similarity assessments from human-subject trials.
Random-walk similarity has also beenused for lexical smoothing for prepositional wordattachment (Toutanova et al, 2004) and query ex-pansion (Collins-Thompson and Callan, 2005).
Incontrast to these works, our graph representation de-scribes parsed text and has not been (consciously)engineered for a particular task.
Instead, we in-clude learning techniques to optimize the graph-walk based similarity measure.
The learning meth-ods described in this paper can be readily applied to911other directed and labelled entity-relation graphs.7The graph representation described in this paperis perhaps most related to syntax-based vector spacemodels, which derive a notion of semantic similar-ity from statistics associated with a parsed corpus(Grefenstette, 1994; Lin, 1998; Pado?
and Lapata,2007).
In most cases, these models construct vectorsto represent each word wi, where each element in thevector for wi corresponds to particular ?context?
c,and represents a count or an indication of whetherwi occurred in context c. A ?context?
can refer tosimple co-occurrence with another word wj , to aparticular syntactic relation to another word (e.g., arelation of ?direct object?
to wj), etc.
Given theseword vectors, inter-word similarity is evaluated us-ing some appropriate similarity measure for the vec-tor space, such as cosine vector similarity, or Lin?ssimilarity (Lin, 1998).Recently, Pado?
and Lapata (Pado?
and Lapata,2007) have suggested an extended syntactic vectorspace model called dependency vectors, in whichrather than simple counts, the components of aword vector of contexts consist of weighted scores,which combine both co-occurrence frequency andthe importance of a context, based on properties ofthe connecting dependency paths.
They consideredtwo different weighting schemes: a length weight-ing scheme, assigning lower weight to longer con-necting paths; and an obliqueness weighting hierar-chy (Keenan and Comrie, 1977), assigning higherweight to paths that include grammatically salientrelations.
In an evaluation of word pair similar-ity based on statistics from a corpus of about 100million words, they show improvements over sev-eral previous vector space models.
Below we willcompare our framework to that of Pado?
and Lap-ata.
One important difference is that while Pado?
andLapata make manual choices (regarding the set ofpaths considered and the weighting scheme), we ap-ply learning to adjust the analogous parameters.7 Extraction of Coordinate TermsWe evaluate the text representation schema and theproposed set of graph-based similarity measures onthe task of coordinate term extraction.
In particular,7We refer the reader to the TextGraph workshop proceed-ings, http://textgraphs.org.we evaluate the extraction of named entities, includ-ing city names and person names from newswiredata, using word similarity measures.
Coordinateterms reflect a particular type of word similarity(relatedness), and are therefore an appropriate testcase for our framework.
While coordinate term ex-traction is often addressed by a rule-based (tem-plates) approach (Hearst, 1992), this approach wasdesigned for very large corpora such as the Web,where the availability of many redundant documentsallows use of high-precision and low-recall rules.In this paper we focus on relatively small corpora.Small limited text collections may correspond todocuments residing on a personal desktop, emailcollections, discussion groups and other specializedsets of documents.The task defined in the experiments is to retrievea ranked list of city or person names given a smallset of seeds.
This task is implemented in the graphas a query, where we let the query distribution Vq beuniform over the given seeds (and zero elsewhere).Ideally, the resulting ranked list will be populatedwith many additional city, or person, names.We compare graph walks to dependency vec-tors (DV) (Pado?
and Lapata, 2007),8 as well as toa vector-based bag-of-words co-occurrence model.DV is a state-of-the-art syntactic vector-based model(see Section 6).
The co-occurrence model representsa more traditional approach, where text is processedas a stream rather than syntactic structures.
In ap-plying the vector-space based methods, we computea similarity score between every candidate from thecorpus and each of the query terms, and then aver-age these scores (as the query distributions are uni-form) to construct a ranked list.
For efficiency, inthe vector-based models we limit the considered setof candidates to named entities.
Similarly, the graphwalk results are filtered to include named entities.9Corpora.
As the experimental corpora, we usethe training set portion of the MUC-6 dataset (MUC,1995) as well as articles from the Associated Press(AP) extracted from the AQUAINT corpus (Bilotti8We used the code from http://www.coli.uni-saarland.de/ pado/dv.html, and converted the underlyingsyntactic patterns to the Stanford dependency parser conven-tions.9In general, graph walk results can be filtered by variousword properties, e.g., capitalization pattern, or part-of-speech.912Corpus words nodes edges unique NEsMUC 140K 82K 244K 3KMUC+AP 2,440K 1,030K 3,550K 36KTable 1: Corpus statisticset al, 2007), all parsed using the Stanford depen-dency parser (de Marneffe et al, 2006).10 The MUCcorpus provides true named entity tags, while theAQUAINT corpus includes automatically generated,noisy, named entity tags.
Statistics on the experi-mental corpora and their corresponding graph rep-resentation are detailed in Table 1.
As shown, theMUC corpus contains about 140 thousand words,whereas the MUC+AP experimental corpus is sub-stantially larger, containing about 2.5 million words.We generated 10 queries, each comprised of 4 citynames selected randomly according to the distribu-tion of city name mentions in MUC-6.
Similarly,we generated a set of 10 queries that include 4 per-son names selected randomly from the MUC corpus.
(The MUC corpus was appended to AP, so that thesame query sets are applicable in both cases.)
Foreach task, we use 5 queries for training and tuningand the remaining queries for testing.8 Experimental ResultsExperimental setup.
We evaluated cross-validationperformance over the training queries in terms ofmean average precision for varying walk lengths K.We found that beyond K = 6 improvements weresmall (and in fact deteriorated for K = 9).
We there-fore set K = 6.
Weight tuning was trained usingthe training queries and two dozens of target nodesoverall.
In reranking, we set a feature count cutoffof 3, in order to avoid over-fitting.
Reranking wasapplied to the top 200 ranked nodes output by thegraph walk using the tuned edge weights.
Finally,path-trees were constructed using the top 20 correctnodes and 20 incorrect nodes retrieved by the uni-formly weighted graph walk.
In the experiments,we apply a threshold of 0.5 to the path constrainedgraph walk method.We note that for learning, true labels were used forthe fully annotated MUC corpus (we hand labelledall of the named entities of type location in the cor-pus as to whether they were city names).
However,10http://nlp.stanford.edu/software/lex-parser.shtml; sen-tences longer than 70 words omitted.noisy negative examples were considered for thelarger automatically annotated AP corpus.
(Specif-ically, for cities, we only considered city names in-cluded in the MUC corpus as correct answers.
)A co-occurrence vector-space model was appliedusing a window of two tokens to the right and tothe left of the focus word.
Inter-word similaritywas evaluated in this model using cosine similar-ity, where the underlying co-occurrence counts werenormalized by log-likelihood ratio (Pado?
and Lap-ata, 2007).
The parameters of the DV method wereset based on a cross validation evaluation (using theMUC+AP corpus).
The medium set of dependencypaths and the oblique edge weighting scheme werefound to perform best.
We experimented with co-sine as well as Lin similarity measure in combina-tion with the dependency vectors representation.
Fi-nally, given the large number of candidates in theMUC+AP corpus (Table 1), we show the results ofapplying the considered vector-space models to thetop, high-quality, entities retrieved with rerankingfor this corpus.11Test set results.
Figure 4 gives results for the cityname (top) and the person name (bottom) extractiontasks.
The left part of the figure shows results us-ing the MUC corpus, and its right part ?
using theMUC+AP corpus.
The curves show precision as afunction of rank in the ranked list, up to rank 100.
(For this evaluation, we hand-labeled all the top-ranked results as to whether they are city names orperson names.)
Included in the figure are the curvesof the graph-walk method with uniform weights(G:Uw), learned weights (G:Lw), graph-walk withreranking (Rerank) and a path-constrained graph-walk (PCW).
Also given are the results of the co-occurrence model (CO), and the syntactic vector-space DV model, using the Lin similarity measure(DV:Lin).
Performance of the DV model using co-sine similarity was found comparable or inferior tousing the Lin measure, and is omitted from the fig-ure for clarity.Several trends can be observed from the results.With respect to the graph walk methods, the graphwalk using the learned edge weights consistentlyoutperforms the graph walk with uniform weights.Reranking and the path-constrained graph walk,11We process the union of the top 200 results per each query.913MUC MUC+AP0.20.30.40.50.60.70.80.910  10  20  30  40  50  60  70  80  90  100PrecisionRank00.20.40.60.810  10  20  30  40  50  60  70  80  90  100PrecisionRankG:UwG:LwCODV:LinPCWRerank0.10.20.30.40.50.60.70.80.910  10  20  30  40  50  60  70  80  90  100PrecisionRank00.20.40.60.810  10  20  30  40  50  60  70  80  90  100PrecisionRankFigure 4: Test results: Precision at the top 100 ranks, for the city name extraction task (top) and person name extractiontask (bottom).however, yield superior results.
Both of these learn-ing methods utilize a richer set of features than thegraph walk and weight tuning, which can consideronly local information.
In particular, while the graphwalk paradigm assigns lower importance to longerconnecting paths (as described in Section 3), rerank-ing and the path-constrained walker allow to dis-card short yet irrelevant paths, and by that eliminatenoise at the top ranks of the retrieved list.
In gen-eral, the results show that edge sequences carry ad-ditional meaning compared with the individual edgelabel segments traversed.Out of the vector-based models, the co-occurrence model is preferable for the city nameextraction task, and the syntactic dependency vec-tors model gives substantially better performancefor person name extraction.
We conjecture that cityname mentions are less structured in the underlyingtext.
In addition, the syntactic weighting scheme ofthe DV model is probably not optimal for the case ofcity names.
For example, a conjunction relation wasfound highly indicative for city names (see below).However, this relation is not emphasized by the DVweighting schema.
As expected, the performance ofthe vector-based models improves for larger corpora(Terra and Clarke, 2003).
These models demonstrategood performance for the larger MUC+AP corpus,but only mediocre performance for the smaller MUCcorpus.Contrasting the graph-based methods with thevector-based models, the difference in performancein favor of reranking and PCW, especially for thesmaller corpus, can be attributed to two factors.
Thefirst factor is learning, which optimizes performancefor the underlying data.
A second factor is the incor-poration of non-local information, encoding proper-ties of the traversed paths.Models.
Following is a short description of themodels learned by the different methods and tasks.Weight tuning assigned high weights to edge typessuch as conj-and, prep-in and prep-from, nn, ap-pos and amod for the city extraction task.
For per-91424681012141618200  1  2  3  4  5  6Number of graphnodesvisited[log_2]Walk stepsG:UPCW:0PCW:0.5PCW:0.8Figure 5: The graph walk exponential spread is boundedby the path constrained walk.son extraction, prominent edge types included subj,obj, poss and nn.
(The latter preferences are sim-ilar to the linguistically motivated weights of DV.
)High weight features assigned by reranking for cityname extraction included, for example, lexical fea-tures such as ?based?
and ?downtown?, and edge bi-grams such as ?prep-in-Inverse?conj-and?
or ?nn-Inverse?nn?.
Positive highly predictive paths inthe constructed path tree included many symmetricpaths, such as ...?conj andInverse...?.conj and...,...?prep inInverse...?.prep in..., for the city nameextraction task.Scalability.
Figure 5 shows the number of graphnodes maintained in each step of the graph walk(logarithm scale) for a typical city extraction queryand the MUC+AP corpus.
As shown by the solidline, the number of graph nodes visited using theweighted graph walk paradigm grows exponentiallywith the length of the walk.
Applying a path-constrained walk with a threshold of 0 (PCW:0) re-duces the maximal number of nodes expanded (aspaths not observed in the training set are discarded).As shown, increasing the threshold leads to signifi-cant gains in scalability.
Overall, query processingtime averaged at a few minutes, using a commodityPC.9 Conclusion and Future DirectionsIn this paper we make several contributions.
First,we have explored a novel but natural representationfor a corpus of dependency-parsed text, as a labelleddirected graph.
We have evaluated the task of coor-dinate term extraction using this representation, andshown that this task can be performed using similar-ity queries in a general-purpose graph-walk basedquery language.
Further, we have successfully ap-plied learning techniques that tune weights assignedto different dependency relations, and re-score can-didates using features derived from the graph walk.Another orthogonal contribution of this paper isa path-constrained graph walk variant, where thegraph walk is guided by high level knowledge aboutmeaningful paths, learned from training examples.This method was shown to yield improved perfor-mance for the suggested graph representation, andimproved scalability compared with the local graphwalk.
The method is general, and can be readily ap-plied in similar settings.Empirical evaluation of the coordinate term ex-traction task shows that the graph-based frameworkperforms better than vector-space models for thesmaller corpus, and comparably otherwise.
Over-all, we find that the suggested model is suitable fordeep (syntactic) processing of small specialized cor-pora.
In preliminary experiments where we evalu-ated this framework on the task of extracting generalword synonyms, using a relatively large corpus of15 million words, we found the graph-walk perfor-mance to be better than DV using cosine similaritymeasures, but second to DV using Lin?s similaritymeasure.
While this set of results is incomplete, wefind that it is consistent with the results reported inthis paper.The framework presented can be enhanced in sev-eral ways.
For instance, WordNet edges and mor-phology relations can be readily encoded in thegraph.
We believe that this framework can be ap-plied for the extraction of more specialized no-tions of word relatedness, as in relation extraction(Bunescu and Mooney, 2005).
The path-constrainedgraph walk method proposed may be enhanced bylearning edge probabilities, using a rich set of fea-tures.
We are also interested in exploring a possi-ble relation between the path-constrained walk ap-proach and reinforcement learning.AcknowledgmentsThe authors wish to thank the anonymous reviewersand Hanghang Tong for useful advice.
This materialis based upon work supported by Yahoo!
Research.915ReferencesMatthew W. Bilotti, Paul Ogilvie, Jamie Callan, and EricNyberg.
2007.
Structured retrieval for question an-swering.
In SIGIR.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In HLT-EMNLP.William W. Cohen and Einat Minkov.
2006.
A graph-search framework for associating gene identifiers withdocuments.
BMC Bioinformatics, 7(440).Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?69.Kevyn Collins-Thompson and Jamie Callan.
2005.Query expansion using random walk models.
InCIKM.Michael Collins.
2002.
Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.In ACL.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC.Michelangelo Diligenti, Marco Gori, and Marco Mag-gini.
2005.
Learning web page scores by error back-propagation.
In IJCAI.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Dordrecht.Marti Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In COLING.Thad Hughes and Daniel Ramage.
2007.
Lexical seman-tic relatedness with random graph walks.
In EMNLP.Edward Keenan and Bernard Comrie.
1977.
Nounphrase accessibility and universal grammar.
Linguis-tic Inquiry, 8.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL.Einat Minkov and William W. Cohen.
2007.
Learning torank typed graph walks: Local and global approaches.In WebKDD/KDD-SNA workshop.Einat Minkov, William W. Cohen, and Andrew Y. Ng.2006.
Contextual search and name disambiguation inemail using graphs.
In SIGIR.1995.
Proceedings of the sixth message understandingconference (muc-6).
In Morgan Kaufmann Publish-ers, Inc. Columbia, Maryland.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2).Philip Resnik and Mona Diab.
2000.
Measuring verbsimilarity.
In CogSci.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS.Egidio Terra and C. L. A. Clarke.
2003.
Frequencyestimates for statistical word similarity measures.
InNAACL.Kristina Toutanova, Christopher D. Manning, and An-drew Y. Ng.
2004.
Learning random walk models forinducing word dependency distributions.
In ICML.916
