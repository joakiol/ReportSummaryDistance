INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 131?133,Utica, May 2012. c?2012 Association for Computational LinguisticsMidge: Generating Descriptions of Images?Margaret MitchellUniversity of Aberdeenm.mitchell@abdn.ac.ukXufeng HanStony Brook Universityxufhan@cs.stonybrook.eduJeff HayesSignWorks of Oregonjeff@signworksoforegon.comAbstractWe demonstrate a novel, robust vision-to-language generation system called Midge.Midge is a prototype system that connectscomputer vision to syntactic structures withsemantic constraints, allowing for the auto-matic generation of detailed image descrip-tions.
We explain how to connect vision de-tections to trees in Penn Treebank syntax,which provides the scaffolding necessary tofurther refine data-driven statistical generationapproaches for a variety of end goals.1 IntroductionThere has been a growing interest in tackling theproblem of how to describe an image using com-puter vision detections.
This problem is difficult inpart because computer vision detections are oftenwrong: State-of-the-art vision technology predictsthings that are not there, and misses things that areobvious to a human observer.
This problem is alsodifficult because it is not clear what kind of languageshould be generated ?
the language that makes up a?description?
can take many forms.At the bare minimum, an automatic vision-to-language system, given an image with a single de-tection of, for example, a dog, should be able to gen-erate a dog, and a longer phrase if requested.
To beuseful in real-world applications, it should be able tocreate basic descriptions that are as true as possibleto the image, as well as descriptions that guess prob-able information based on language analysis alone.To our knowledge, no current system provides thisfunctionality.
Midge is built based on these goals.Our approach converts object detections to de-scriptive sentences using a tree-generating deriva-tion process that fleshes out lexicalized syntactic?
Thanks to the CLSP 2011 summer workshopat Johns Hopkins for making this system possible.Midge is available to try online athttp://recognition.cs.stonybrook.edu:8080/?mitchema/midge/and http://mcvl.cewit.stonybrook.edu//?mitchema/midge/ andscreenshots at http://www.abdn.ac.uk/?r07mm9/midge/structure around object nouns.
Likely subtrees arelearned from a cleaned version of the Flickr dataset(Ordonez et al, 2011) parsed using the Berkeleyparser.
The final structures generated by the systemare present-tense declarative sentences in Penn Tree-bank syntax.With this in place, the system can generate a dog,a black dog sleeping, a furry black dog sleeping by acat, etc., while also suggesting further detectors forthe vision system to run.
Approaching the problemin this way, Midge provides a starting point for gen-eration to meet different goals: from automaticallycreating stories or summaries based on visual data,to suggesting phrases that a speech-impaired AACuser can select to assist in conversation.
There isstill much work to be done, but we believe that thebasic architecture used by this system is a solid start-ing point for generating a wide variety of descriptivecontent, and makes clear some of the issues a vision-to-language system must handle in order to generatenatural-sounding descriptions.2 BackgroundPrevious work on generating image descriptions canbe characterized as prioritizing among several goals:?
Creating language that is poetic or metaphori-cal (Li et al, 2011)?
Creating automatic captions with syntacticvariation based on semantic visual information(Farhadi et al, 2010)?
Creating language describing the scene in a ba-sic template-driven way, utilizing attribute de-tections (Kulkarni et al, 2011) or likely verbsfrom a language model (Yang et al, 2011)To meet one goal, other goals are often compro-mised.
Yang et al (2011) fill in likely verbs to formcomplete sentences, but limit the generated struc-tures to a simple template, without capturing natu-ral variation in sentence length or surface structure.131Li et al (2011) aim at more metaphorical and var-ied language, but the generated structures are oftensyntactically and semantically ill-formed.
Farhadi etal.
(2010) generate natural, varied, descriptive lan-guage, but this is created by copying captions di-rectly from similar images, resulting in captions thatare often not true to the actual query image content.Midge builds on ideas from these systems, ad-ditionally mapping the structures underlying visiondetections to syntactic structures and data-drivendistributional information underlying natural lan-guage descriptions.
With this in place, the door isopened for language and vision to communicate ata deep syntactic-semantic level.
The language com-ponents of the system can filter and expand on givenvisual information, and can also call back to the vi-sual system itself, specifying further detectors to run(or train) based on semantically related or expectedinformation.
We hope that this system not only ad-vances work in generating visual descriptions, butwork in training visual detectors as well.3 Vision to Language IssuesThe process of developing Midge brought to lightseveral key issues that any vision-to-language sys-tem aiming to generate descriptive, varied, human-like language must handle:Descriptiveness: Should the system include infor-mation about everything there is evidence for, limitthat information, or add to it?World knowledge: What sorts of things in an imageare remarkable, and should be mentioned, and whichmay go without saying?Object grouping: Which objects should be men-tioned together?
How do people divide objectsamong sentences when they describe an image?Which detections should not be mentioned?Noun ordering: In what order should the objects benamed?Reference plurals and sets: How should sets ofobjects be described as a whole?
Should the exactnumber be included (four chairs), a vague term (afew chairs) or a general plural form (chairs)?Modifier ordering: How should the different modi-fiers common to descriptions be ordered to make theutterances sound fluent?Determiner selection: When should objects betreated as given (the sky), new (a boy), mass (grass),or count (a blade)?Verb selection: Given that action/pose detection incomputer vision does not function reliably, shouldverbs be hallucinated from a language model alone?Should they be left out?Preposition selection: How should spatial relationsbetween objects be analyzed, and how does thistranslate to language describing the scene layout?Surface realization: What final lexicalization deci-sions need to be made to realize the generated stringswithin the output language?Final string selection: Given a set of possible out-puts, how is the final output string decided?Nonsense detections: How should the system han-dle computer vision detections that are often wrong?Many of these issues are well-suited to statisti-cal NLP techniques, and some (modifier ordering,final string selection) have already been addressed inthe NLP community.
Where appropriate, Midge in-corporates this technology alongside novel solutionsto issues that have not yet been heavily researched(determiner selection, nominal ordering).
We hopeto further refine Midge?s solutions as technology inthese areas advances.Separating Midge?s architecture into componentsthat handle each of these issues separately meansthat the system is flexible to change the kind of lan-guage it generates depending on the goals of theend user.
The system offers general solutions tothe issues listed above, and can have many of itsgoals changed if specified at run-time, resulting indifferent kinds of generated utterances.
Midge cansuccessfully create natural, varied descriptions thatadd descriptive content based on language modelingalone; it can also generate descriptions that are morelimited, but as true as possible to the image.4 Natural Language Generation in Midge- id: 1, type: 1, label: bus, score: 0.73, bbox: [65.0, 65.0, 415.0,191.0], attrs: {?blue?
: 0.01, ?furry?
:.02, .
.
.
, ?shiny?
: 0.69}- id: 2, type: 1, label: road, score: 0.95, bbox: [1.0, 95.0, 440.0,235.0], attrs: {?blue?
: 0.01, .
.
.
}- preps {1,2}: ?by?Figure 1: Computer Vision Out / Midge In (Excerpt)The input to Midge is the output of vision detec-tions, with detectors run for objects and attributeswithin each object?s bounding box.
In this demon-stration, we incorporate the Kulkarni et al (2011)vision detections.
This provides objects/stuff and as-sociated attributes, bounding boxes, and spatial rela-tions between object pairs derived from the bound-132ing boxes.
Object detections are based on Felzen-szwalb?s multi-scale deformable parts models, andstuff detections are based on linear SVMs for lowlevel region features.Language generation in Midge is driven by a lex-icalized derivation process that uses likely syntac-tic and distributional information for object nounsto create present-tense declarative sentences.
Objectdetections form the basis of the computer vision de-tections, and these in turn are linked to nouns thatform the basis of the generated output string.The syntactic trees used to collect and generatelikely subtrees for object nouns is outlined in Figure2.
Each anchor noun selects for a set of likely ad-jectives a, determiners d, prepositions p and presenttense verbs v.1 2NPNNbottleJJ* ?
(a)DT ?
(d)SVPVBZ ?
(v)NP{NN, bottle}3 4NPVPVB{G|N} ?
(v)NP{NN, bottle}NPPPIN ?
(p)NP {NN, bottle}5 6VPNP{NN, bottle}VB{G|N|Z} ?
(v)PPNP{NN, bottle}IN ?
(p)7 8VPPPNP{NN, bottle}IN ?
(p)VB{G|N|Z} ?
(v) VPVP* ?9 10SVPPPIN ?
(p)VBZ ?
(v)NP{NN, bottle} NPNP ?CCandNP ?11NPVPPPIN ?
(p)VB{G|N} ?
(v)NP{NN, bottle}Figure 2: Trees for generation.
Each {NN, noun} selectsfor its local subtrees.
?
marks a substitution site, * marks?
0 sister nodes of this type permitted.
Input: set of or-dered nouns, Output: trees preserving nominal ordering.5 ArchitectureMidge can be explained at a high level as a pipelinedsystem incorporating the following steps:Step 1: Run detectors for objects, stuff, action/poseand attributes; pass as <detection, score> pairs toMidge.
Vision output/NLG input is displayed in Fig-ure 1 and in the system demo.Step 2: Group objects together that will be men-tioned together.Step 3: Order objects within each group ?
this au-tomatically sets the subject and objects of the sen-tence.
Midge currently order nouns based on Word-Net hypernyms.Step 4: Create all tree structures that can be gen-erated from the object noun node.
(See Figure 2).Noun anchors select for adjectives (JJ), determin-ers (DT), prepositions (IN) and if specified, verbs(VBG, VBN, or VBZ).Step 5: Limit adjectives (JJ) to the set that are notmutually exclusive ?
different values for the same at-tribute class.
REG comes into play at this step.Step 6: Create all trees that combine following thegiven trees until all object nouns in a group are un-der one node (either NP or S).Step 7: Order selected adjectives.
We use the top-scoring ngram model from (Mitchell et al, 2011).Step 8: Choose final tree from set of generated trees.Users can select a longest-string or cross entropycalculation.ReferencesA.
Farhadi, M. Hejrati, P. Young Sadeghi, C. Rashtchian,J.
Hockenmaier, and D. A. Forsyth.
2010.
Everypicture tells a story: generating sentences for images.Proc.
ECCV 2010.G.
Kulkarni, V. Premraj, and S. Dhar, et al 2011.
Babytalk: Understanding and generating image descrip-tions.
Proc.
CVPR 2011.S.
Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi.2011.
Composing simple image descriptions usingweb-scale n-grams.
Proc.
CoNLL 2011.M.
Mitchell, A. Dunlop, and B. Roark.
2011.
Semi-supervised modeling for prenominal modifier order-ing.
Proc.
ACL 2011.V.
Ordonez, G. Kulkarni, and T. L. Berg.
2011.
Im2text:Describing images using 1 million captioned pho-tographs.
Proc.
NIPS 2011.Y.
Yang, C. L. Teo, H. Daume?
III, and Y. Aloimonos.2011.
Corpus-guided sentence generation of naturalimages.
Proc.
EMNLP 2011.133
