Coling 2008: Companion volume ?
Posters and Demonstrations, pages 47?50Manchester, August 2008ILP-based Conceptual Analysis for Chinese NPsPaul D. JiCenter for Language and PhilologyOxford UniversityPaul_dji@yahoo.com.ukStephen PulmanComputing LaboratoryOxford Universitysgp@clg.ox.ac.ukABSTRACTIn this paper, we explore a conceptual re-source for Chinese nominal phrases,which allows multi-dependency and dis-tinction between dependency and the cor-responding exact relation.
We also pro-vide an ILP-based method to learn map-ping rules from training data, and use therules to analyze new nominal phrases.1 IntroductionNominal phrases have long been a concern inlinguistic research and language processing (e.g.,Copestake and Briscoe, 2005; Giegerich, 2004).Generally, nominal phrases can be classified intotwo categories according to whether they containattributive clauses or not.
We focus on nominalphrases without attributive clauses.Closely related with nominal phrases, nominalcompounds or base NPs have also attracted agreat attention in language processing.
Gener-ally, nominal compounds refer to nominalphrases consisting of a series of nouns, whilebase NPs refer to non-recursive nominal phrases.However, such compounds or base NPs usuallyco-occur with other non-nominal words in run-ning texts, and it is impossible to separate themduring analysis.
Furthermore, there exist syntac-tic makers for attributive clauses, e.g., ?which?
or?who?
in English and ?
(of)?
in Chinese, nomi-nal phrases without attributive clauses tend to bea better linguistic category for theoretical andpractical investigation.To analyze NPs, we need first to determinewhat kinds of information are to be recognized.In this work, we focus on conceptual relatednessbetween words.
For example, in linguistics and?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.law books, linguistic and law are both conceptu-ally related with books, although linguisticsdoesn?t have a superficial syntactic relation withbooks.
Then, we need to fulfill two sub-tasks.One is about representation, i.e., what schemesare to be used.
The other is about analysis, i.e.,how to derive the formal representation.Regarding representation scheme, one possiblestrategy would be using syntactic structures, asare usually used in analysis for sentences.
How-ever, syntactic components for NPs, unlike thosefor sentences (e.g., V, VP, A, AP, and S, etc.
),are difficult to differentiate, and rules governingnominal phrases are especially difficult to deter-mine.
As an example, consider (bankloan interest), which is a nominal compoundconsisting of three serial nouns.
For such a NP, ifa rule with binary combination is used, it wouldproduce two structures for the unambiguous NP.If a rule with triple combination is used, as inChinese Treebank (Xue et al, 2005), it would bedifficult to disclose the lexical relation between(bank) and (loan).Another possible representation strategywould be using dependency structures (Mel?cuk,1988).
Under this strategy, a NP could be repre-sented as a dependency tree, which capturesvarious lexical control or dependency in thephrase.
However, traditional framework onlyfocuses on syntactic dependency, while concep-tual relatedness may exist without syntactic rela-tions.
For example, for  (eco-nomic development and law construction in Shang-hai), in traditional dependency analysis,(Shanghai) would depend on the conjunctionword (and), since conjunction words are usu-ally regarded as heads in coordinate structures.Although the relatedness may go downward fromthe head, it would be difficult to derive the relat-edness between   (Shanghai) and(economic) or (law), since the two words areeven not heads of the conjuncts   (eco-47nomic development) and (law develop-ment).As to analysis of NPs, there have been a lot ofwork on statistical techniques for lexical depend-ency parsing of sentences (Collins and Roark,2004; McDonald et al, 2005), and these tech-niques potentially can be used for analysis ofNPs if appropriate resources for NPs are avail-able.
However, these techniques are all meant tobuilding a dependency tree, while the conceptualrelatedness in NPs may form a graph, with multi-dependency allowed.
Additionally, these meth-ods generally suffer from the difficulty of localestimation from limited contexts and the struc-tural information is difficult to be exploited(Califf and Mooney, 2003).Recently, relational learning methods in gen-eral and inductive logic programming (ILP) inparticular have attracted a great of attention dueto their capability of going beyond finite featurevectors and exploiting unbounded structural in-formation from data (Califf and Mooney, 2003;Page et al, 2003; Srinivasan et al, 2003).In this work, we try to extend syntactic de-pendency to conceptual dependency to capturethe embedded lexical relatedness, and use ILP toanalyze nominal phrases, making use of thestructural information provided by the resourcesbased on conceptual dependency.2 Conceptual dependencyIn comparison with syntactic dependency, con-ceptual dependency may allow a word to be de-pendent on multiple words at the same time.
Forexample, in  (Economic devel-opment and law construction in Shanghai),(Shanghai) conceptually relates with both(economic) and (law), while in(activity of blood donation for university studentvolunteers),  (university student) relates onboth (volunteer) and (blood donation).In addition, syntactic dependency doesn?texactly specify what kind of relatedness heldbetween words, although the words denoting therelatedness may occur within NPs.
For example,1) is an ambiguous compound with two possibleinterpretations listed in 2).1)  (student discussion)2) i)  discussion by studentsii) discussion about studentsHowever, the dependency trees correspondingwith the two interpretations remain the same:(student) depends on   (discussion) in bothcases.
In fact, their difference lies in the exactsemantic relations held between the words:(student) is agent and patient of (discussion)in 2i) and 2ii) respectively.
This suggests thatonly syntactic dependency is not enough to re-flect conceptual difference.Notice that in (2), the relations between thetwo words (student) and (discussion) aredenoted by two proper nouns, agent and patient,which may never co-occur with them in runningtexts.
However, in some cases, some word co-occurring with two conceptually related wordsdo denote the relatedness exactly.
Consider(car in read color), where (color) relates withboth (car) and (red).
In the conceptual view,(color) can be seen as a feature of (car), and(red) can be seen as a kind of value for the fea-ture, as was also adopted in dealing with adjec-tives in WordNet (Fellbaum, 1988).
In this set-ting, (red) directly depends on (car), and(color) represents the relation between them.In building the resource for Chinese NPs,the conceptual relatedness is based on semanticreference, while the dependency is based on syn-tactic or potential syntactic relations.
The featurewords we adopt are mostly listed in a mediumclass, coded as Dn, in a Chinese thesaurus,Tongyici Cilin (henceafter Cilin, Mei et al,1982).
Function words (e.g.,  (from)), Partwords (e.g., (leg)) and Number words (e.g.,(count)) are also regarded as feature words.3 ILP-based AnalysisFig.
1 gives the overall structure of the analysisprocedure.ENDFig.
1 Overall structure of analysis procedureThe analysis consists of two phases, training andparsing.
During the training phase, rules arelearned for mapping from conceptual depend-ency graphs to word strings based on trainingexamples.
During the parsing phase, there arethree steps.
Search is to find candidate depend-ency graphs, Generation is to generate wordstrings from candidate dependency graphs usingRule LearningSearchGenerationEvaluationParsingTraining48the learned rules, and Evaluation is to comparethe generated word strings with the original NPs.3.1 Training: learning rulesFor each training sample, we have a nominalphrase and its corresponding conceptual depend-ency graph.
To learn the rules mapping from de-pendency graphs to word strings, we need to tagthe words with their sense labels, which denotethe synsets in the thesaurus (Mei et al, 1982).For the sense tagging, we used the same methodas in (Yarowsky, 1992) and used the minor cate-gories in the thesaurus as the synsets.Generally, a rule consists of two parts, Gr andSr.
Gr is a dependency sub-graph and Sr is asense label string.
Intuitively, conceptual con-figuration in Gr is represented by the label stringof Sr.To capture more structural information, weneed to find the maximal sub-graph in the train-ing data, whose corresponding labels form a con-tinuous substring in the training data.
But theproblem is NP hard, and we thus use heuristics tofind am optimally maximal sub-graphs.
How-ever, the search has a bias to larger sub-graphs,and to avoid the bias, we set the coverage of asub-graph as the penalty.
Here, the coverage ofthe sub-graph refers to the percentage of thenodes in the sub-graphs among all the nodes inthe training data.
The overall algorithm is:i) to find the most common edge in the training data,whose corresponding label strings are continuous;ii) to add another edge to the sub-graph, if the labelstrings corresponding with the new sub-graph arestill continuous until the coverage of the sub-graphdoesn?t increase.After finding such a sub-graph, we merge allthe nodes into one, and merge the sense labelstrings into one, and repeat the process until allthe nodes in the training data are covered Theresult of the learning is a set of rules, and eachrule specifies a sub-graph and a label string.For example, w got a rule which includes thesub-graph in Fig.
2 and sense label string in 3).Fig.
2.
Sub-graph in a rule.3) SL()SL()SL()For rule generalization, we don?t try to com-press the rule set, and simply use the sense hier-archy in the thesaurus, including the minor, me-dium and major classes.3.2 ParsingAfter training phase, we get a set of learnedrules.
During parsing, the task is to find a con-ceptual dependency graph for a new input data,which would generate the NP using the learnedrules.The optimal parsing can be implemented in agreedy manner.
First, one dependency with twowords is selected.
Then, another word is added ifthe resulted conceptual dependency graph gener-ates a word string which best matches the inputnominal phrase.
This process can be repeateduntil the graph includes all the words in the data.To compare the generated word string with theoriginal input, we use edit distance betweenthem, which is based on the times of operations(including adjacent move, deletion, insertion)needed to convert one word string to another.4 Experiments and EvaluationThere are 10,000 nominal phrases annotated inthe resource, and they were selected from 1,221articles form the corpora of China daily, 1992.Table 1 gives the statistics of the resource.num ?de?structureNominalcompoundDepend-ency withfeatureMulti-depend-ency10K 4,234 5,766 1,235 976Table 1.
Statistics of NP resourceHere, ?de?
structure refer to the phrase withword ?
?
(of).
Nominal compounds refer to thenominal phrases with no occurrence of ?
?
(of).Dependency with features refers to those taggedwith features, which also occur in the same NPs.Multi-dependency refers to the number of mono-dependencies occurring in the multi-dependency.We randomly selected 10% of the trainingdata as closed test data, and the other 90% or lessas training data.
To evaluate the performance ofthe dependency analysis, we used F-scores asevaluation measure as usual.
Fig.
4 shows theresults for overall dependency, multi-dependencyand dependency with features.
The results areaveraged over 10 random runs.(and)(boys)(some)4900.
20.
40.
60.
8110% 30% 50% 70% 90%Tr ai ni ng dat aF-Scoreover al  dependencymul t i - dependencydependency wi t h f eat uresFig.
3 Performance with varying training dataFig.
3 demonstrates that with more trainingdata, the performance generally improved.
Theperformance for dependency with featuresseemed better than that for overall dependency ormulti-dependency.
To check the reason, wefound that we treated the Amount words inNumber-Amount structures as features, and thesewords are generally easier to be identified, sincethey tend to be unambiguous.
Once they wererecognized as Amount words, the relevant de-pendency would be correctly identified.For an open test, we selected another 1,000nominal phrases from the same corpus, but fromdifferent time period (1994).
Such phrases wereannotated with the same standard as those train-ing data.
Fig.
4 shows the results with varyingtraining data.00.
20.
40.
60.
8110% 30% 50% 70% 90%Tr ai ni ng dat aF-Scorecl osed t est ( 1000)open t est (1000)Fig.
4 Comparison: Closed test and open testFig.
4 shows that the open test performance isgenerally worse than that of the closed test.
No-tice that although the test data was selected fromthe same resource, but with a different period,which may account for the different perform-ance.5 ConclusionIn this paper, we described a resource for lexicalconceptual dependency of Chinese nominalphrases.
Compared with other ones, it allowsmulti-dependency and distinguishes dependencyand relation, which exactly denotes what kinds ofdependency held.
We also provided an ILP-based analysis method, in which some rulesmapping from conceptual dependency to wordstrings are learned from the training data, andthen the rules are used to find the conceptual de-pendency graph for a new data.
Compared withother search strategies, this method makes use ofthe structural information and allows construc-tion of a dependency graph, not just a depend-ency tree.ReferencesCaliff, M.R.
and Mooney, R.J. 2003.
Bottom-Up Re-lational Learning of Pattern Matching Rules for In-formation Extraction, JMLR: 4:177-210.Collins M. and Roark, B.
2004.
Incremental parsingwith the perceptron algorithm.
In Proc.
of the 42rdAnnual Meeting of the ACL.Copestake, A. and Briscoe, T. 2005.
Noun com-pounds revisited.
In John I. Tait, editor, Charting aNew Course: Natural Language Processing andInformation Retrieval.
Springer, Berlin, 2005.Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Giegerich, H.J.
Compound or phrase?
English noun-plus-noun constructions and the stress criterion.English Language and Linguistics, 8(1):1-24,2004.McDonald, R., Pereira, F., Ribarov, K. and Haji?c, J.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
of HLT/EMNLP.Mei, J., Zhu, Y., Gao, Y., and Yin, H. 1982.
Tongyici Cilin.Shanghai Dictionary Press.Mel'cuk, I., 1988.
Dependency Syntax: Theory andPractice.
Albany.
State Univ.
of New York Press.Page, D. Srinivasan A.
2003.
ILP: A Short Look Backand a Longer Look Forward.
Journal of MachineLearning Research 4: 415-430Srinivasan, A. Ross D. K., Michael B.
2003.
An Em-pirical Study of the Use of Relevance Informationin Inductive Logic Programming.
Journal of Ma-chine Learning Research 4: 369-383.Xue, N.W., Xia, F., Chiou, F.D.and Palmer, M. ThePenn Chinese TreeBank: Phrase Structure Annota-tion of a Large Corpus.
Natural Language Engi-neering, 11(2): 207-238.Yarowsky, D. 1982.
Word-Sense DisambiguationUsing Statistical Models of Roget's CategoriesTrained on Large Corpora.
In Proceedings, COL-ING-92.
pp.
454-460, 1992.50
