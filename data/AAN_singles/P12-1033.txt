Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?319,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSmaller Alignment Models for Better Translations:Unsupervised Word Alignment with the `0-normAshish Vaswani Liang Huang David ChiangUniversity of Southern CaliforniaInformation Sciences Institute{avaswani,lhuang,chiang}@isi.eduAbstractTwo decades after their invention, the IBMword-based translation models, widely avail-able in the GIZA++ toolkit, remain the dom-inant approach to word alignment and an in-tegral part of many statistical translation sys-tems.
Although many models have surpassedthem in accuracy, none have supplanted themin practice.
In this paper, we propose a simpleextension to the IBM models: an `0 prior to en-courage sparsity in the word-to-word transla-tion model.
We explain how to implement thisextension efficiently for large-scale data (alsoreleased as a modification to GIZA++) anddemonstrate, in experiments on Czech, Ara-bic, Chinese, and Urdu to English translation,significant improvements over IBM Model 4in both word alignment (up to +6.7 F1) andtranslation quality (up to +1.4 Bleu).1 IntroductionAutomatic word alignment is a vital component ofnearly all current statistical translation pipelines.
Al-though state-of-the-art translation models use rulesthat operate on units bigger than words (like phrasesor tree fragments), they nearly always use wordalignments to drive extraction of those translationrules.
The dominant approach to word alignment hasbeen the IBM models (Brown et al, 1993) togetherwith the HMM model (Vogel et al, 1996).
Thesemodels are unsupervised, making them applicableto any language pair for which parallel text is avail-able.
Moreover, they are widely disseminated in theopen-source GIZA++ toolkit (Och and Ney, 2004).These properties make them the default choice formost statistical MT systems.In the decades since their invention, many mod-els have surpassed them in accuracy, but none hassupplanted them in practice.
Some of these modelsare partially supervised, combining unlabeled paral-lel text with manually-aligned parallel text (Moore,2005; Taskar et al, 2005; Riesa and Marcu, 2010).Although manually-aligned data is very valuable, itis only available for a small number of languagepairs.
Other models are unsupervised like the IBMmodels (Liang et al, 2006; Grac?a et al, 2010; Dyeret al, 2011), but have not been as widely adopted asGIZA++ has.In this paper, we propose a simple extension tothe IBM/HMM models that is unsupervised like theIBM models, is as scalable as GIZA++ because it isimplemented on top of GIZA++, and provides sig-nificant improvements in both alignment and trans-lation quality.
It extends the IBM/HMM models byincorporating an `0 prior, inspired by the princi-ple of minimum description length (Barron et al,1998), to encourage sparsity in the word-to-wordtranslation model (Section 2.2).
This extension fol-lows our previous work on unsupervised part-of-speech tagging (Vaswani et al, 2010), but enablesit to scale to the large datasets typical in wordalignment, using an efficient training method basedon projected gradient descent (Section 2.3).
Ex-periments on Czech-, Arabic-, Chinese- and Urdu-English translation (Section 3) demonstrate consis-tent significant improvements over IBM Model 4 inboth word alignment (up to +6.7 F1) and transla-tion quality (up to +1.4 Bleu).
Our implementationhas been released as a simple modification to theGIZA++ toolkit that can be used as a drop-in re-placement for GIZA++ in any existing MT pipeline.3112 MethodWe start with a brief review of the IBM and HMMword alignment models, then describe how to extendthem with a smoothed `0 prior and how to efficientlytrain them.2.1 IBM Models and HMMGiven a French string f = f1 ?
?
?
f j ?
?
?
fm and anEnglish string e = e1 ?
?
?
ei ?
?
?
e`, these models de-scribe the process by which the French string isgenerated by the English string via the alignmenta = a1, .
.
.
, a j, .
.
.
, am.
Each a j is a hidden vari-ables, indicating which English word ea j the Frenchword f j is aligned to.In IBM Model 1?2 and the HMM model, the jointprobability of the French sentence and alignmentgiven the English sentence isP(f, a | e) =m?j=1d(a j | a j?1, j)t( f j | ea j).
(1)The parameters of these models are the distortionprobabilities d(a j | a j?1, j) and the translation prob-abilities t( f j | ea j).
The three models differ in theirestimation of d, but the differences do not concern ushere.
All three models, as well as IBM Models 3?5,share the same t. For further details of these models,the reader is referred to the original papers describ-ing them (Brown et al, 1993; Vogel et al, 1996).Let ?
stand for all the parameters of the model.The standard training procedure is to find the param-eter values that maximize the likelihood, or, equiv-alently, minimize the negative log-likelihood of theobserved data:??
= arg min?(?
log P(f | e, ?
))(2)= arg min?????????
log?aP(f, a | e, ?)???????
(3)This is done using the Expectation-Maximization(EM) algorithm (Dempster et al, 1977).2.2 MAP-EM with the `0-normMaximum likelihood training is prone to overfitting,especially in models with many parameters.
In wordalignment, one well-known manifestation of overfit-ting is that rare words can act as ?garbage collectors?
(Moore, 2004), aligning to many unrelated words.This hurts alignment precision and rule-extractionrecall.
Previous attempted remedies include earlystopping, smoothing (Moore, 2004), and posteriorregularization (Grac?a et al, 2010).We have previously proposed another simpleremedy to overfitting in the context of unsuper-vised part-of-speech tagging (Vaswani et al, 2010),which is to minimize the size of the model using asmoothed `0 prior.
Applying this prior to an HMMimproves tagging accuracy for both Italian and En-glish.Here, our goal is to apply a similar prior in aword-alignment model to the word-to-word transla-tion probabilities t( f | e).
We leave the distortionmodels alone, since they are not very large, and thereis not much reason to believe that we can profit fromcompacting them.With the addition of the `0 prior, the MAP (maxi-mum a posteriori) objective function is??
= arg min?(?
log P(f | e, ?)P(?))(4)whereP(?)
?
exp(??????0)(5)and???
?0 =?e, f(1 ?
exp?t( f | e)?
)(6)is a smoothed approximation of the `0-norm.
Thehyperparameter ?
controls the tightness of the ap-proximation, as illustrated in Figure 1.
Substitutingback into (4) and dropping constant terms, we getthe following optimization problem: minimize?
log P(f | e, ?)
?
?
?e, fexp?t( f | e)?
(7)subject to the constraints?ft( f | e) = 1 for all e. (8)We can carry out the optimization in (7) with theMAP-EM algorithm (Bishop, 2006).
EM and MAP-EM share the same E-step; the difference lies in the3120 0.2 0.4 0.6 0.8 100.20.40.60.81Figure 1: The `0-norm (top curve) and smoothed approx-imations (below) for ?
= 0.05, 0.1, 0.2.M-step.
For vanilla EM, the M-step is:??
= arg min???????????
?e, fE[C(e, f )] log t( f | e)?????????
(9)again subject to the constraints (8).
The countC(e, f ) is the number of times that f occurs alignedto e. For MAP-EM, it is:??
= arg min?(?
?e, fE[C(e, f )] log t( f | e) ??
?e, fexp?t( f | e)?)
(10)This optimization problem is non-convex, and wedo not know of a closed-form solution.
Previously(Vaswani et al, 2010), we used ALGENCAN, a non-linear optimization toolkit, but this solution does notscale well to the number of parameters involved inword alignment models.
Instead, we use a simplerand more scalable method which we describe in thenext section.2.3 Projected gradient descentFollowing Schoenemann (2011b), we use projectedgradient descent (PGD) to solve the M-step (butwith the `0-norm instead of the `1-norm).
Gradientprojection methods are attractive solutions to con-strained optimization problems, particularly whenthe constraints on the parameters are simple (Bert-sekas, 1999).
Let F(?)
be the objective function in(10); we seek to minimize this function.
As in pre-vious work (Vaswani et al, 2010), we optimize eachset of parameters {t(?
| e)} separately for each En-glish word type e. The inputs to the PGD are theexpected counts E[C(e, f )] and the current word-to-word conditional probabilities ?.
We run PGD for Kiterations, producing a sequence of intermediate pa-rameter vectors ?1, .
.
.
, ?k, .
.
.
, ?K .
Each iteration hastwo steps, a projection step and a line search.Projection step In this step, we compute:?k=[?k ?
s?F(?k)]?
(11)This moves ?
in the direction of steepest descent(?F) with step size s, and then the function [?
]?projects the resulting point onto the simplex; thatis, it finds the nearest point that satisfies the con-straints (8).The gradient ?F(?k) is?F?t( f | e)= ?E[C( f , e)]t( f | e)+?
?exp?t( f | e)?
(12)In contrast to Schoenemann (2011b), we use anO(n log n) algorithm for the projection step due toDuchi et.
al.
(2008), shown in Pseudocode 1.Pseudocode 1 Project input vector u ?
Rn onto theprobability simplex.v = u sorted in non-increasing order?
= 0for i = 1 to n doif vi ?
1i(?ir=1 vr ?
1)> 0 then?
= iend ifend for?
= 1?(?
?r=1 vr ?
1)wr = max{vr ?
?, 0} for 1 ?
r ?
nreturn wLine search Next, we move to a point between ?kand ?kthat satisfies the Armijo condition,F(?k + ?m) ?
F(?k) + ?
(?F(?k) ?
?m)(13)where ?m = ?m(?k?
?k) and ?
and ?
are both con-stants in (0, 1).
We try values m = 1, 2, .
.
.
until theArmijo condition (13) is satisfied or the limit m = 20313Pseudocode 2 Find a point between ?k and ?kthatsatisfies the Armijo condition.Fmin = F(?k)?min = ?kfor m = 1 to 20 do?m = ?m(?k?
?k)if F(?k + ?m) < Fmin thenFmin = F(?k + ?m)?min = ?k + ?mend ifif F(?k + ?m) ?
F(?k) + ?
(?F(?k) ?
?m)thenbreakend ifend for?k+1 = ?minreturn ?k+1is reached.
(Note that we don?t allow m = 0 becausethis can cause ?k + ?m to land on the boundary ofthe probability simplex, where the objective func-tion is undefined.)
Then we set ?k+1 to the point in{?k} ?
{?k + ?m | 1 ?
m ?
20} that minimizes F.The line search algorithm is summarized in Pseu-docode 2.In our implementation, we set ?
= 0.5 and ?
=0.5.
We keep s fixed for all PGD iterations; we ex-perimented with s ?
{0.1, 0.5} and did not observesignificant changes in F-score.
We run the projectionstep and line search alternately for at most K itera-tions, terminating early if there is no change in ?kfrom one iteration to the next.
We set K = 35 for thelarge Arabic-English experiment; for all other con-ditions, we set K = 50.
These choices were made tobalance efficiency and accuracy.
We found that val-ues of K between 30 and 75 were generally reason-able.3 ExperimentsTo demonstrate the effect of the `0-norm on the IBMmodels, we performed experiments on four trans-lation tasks: Arabic-English, Chinese-English, andUrdu-English from the NIST Open MT Evaluation,and the Czech-English translation from the Work-shop on Machine Translation (WMT) shared task.We measured the accuracy of word alignments gen-erated by GIZA++ with and without the `0-norm,and also translation accuracy of systems trained us-ing the word alignments.
Across all tests, we foundstrong improvements from adding the `0-norm.3.1 TrainingWe have implemented our algorithm as an open-source extension to GIZA++.1 Usage of the exten-sion is identical to standard GIZA++, except that theuser can switch the `0 prior on or off, and adjust thehyperparameters ?
and ?.For vanilla EM, we ran five iterations of Model 1,five iterations of HMM, and ten iterations ofModel 4.
For our approach, we first ran one iter-ation of Model 1, followed by four iterations ofModel 1 with smoothed `0, followed by five itera-tions of HMM with smoothed `0.
Finally, we ran teniterations of Model 4.2We used the following parallel data:?
Chinese-English: selected data from the con-strained task of the NIST 2009 Open MT Eval-uation.3?
Arabic-English: all available data for theconstrained track of NIST 2009, excludingUnited Nations proceedings (LDC2004E13),ISI Automatically Extracted Parallel Text(LDC2007E08), and Ummah newswire text(LDC2004T18), for a total of 5.4+4.3 mil-lion words.
We also experimented on a largerArabic-English parallel text of 44+37 millionwords from the DARPA GALE program.?
Urdu-English: all available data for the con-strained track of NIST 2009.1The code can be downloaded from the first author?s websiteat http://www.isi.edu/?avaswani/giza-pp-l0.html.2GIZA++ allows changing some heuristic parameters forefficient training.
Currently, we set two of these to zero:mincountincrease and probcutoff.
In the default setting,both are set to 10?7.
We set probcutoff to 0 because we wouldlike the optimization to learn the parameter values.
For a faircomparison, we applied the same setting to our vanilla EMtraining as well.
To test, we ran GIZA++ with the default set-ting on the smaller of our two Arabic-English datasets with thesame number of iterations and found no change in F-score.3LDC catalog numbers LDC2003E07, LDC2003E14,LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,LDC2006E85, LDC2006E86, LDC2006E92, andLDC2006E93.314presidentof theforeignaffairsinstituteshuqinliuwasalsopresentat themeeting.u uwa`ijia?ouxue?hu?`uhu?`zha?nguliu?u ushu?q??nguhu?`jia`nsh?
?u u u  uza`izuo`u.over4000guestsfromhomeandabroadattendedtheopeningceremony.u  uzho?ngwa`iula?ib?
?nus?`qia?nu uduo?re?nuchu?x?
?uleu uka?imu`sh?`u.
(a) (b)it ?s extremelytroublesometo gettherevialand.uru?guo?uya`ou ulu`lu`zhua?nu uqu`dehua`neu,uhe?nuhe?nuhe?nuhe?numa?fandeu,afterthiswastakencareof , fourblockhouseswereblownup .uzhe`geuchu`l??wa?nuy??ho`uneu,ha?iuzha`uleus?`geudia?oba?ou.
(c) (d)Figure 2: Smoothed-`0 alignments (red circles) correct many errors in the baseline GIZA++ alignments (blacksquares), as shown in four Chinese-English examples (the red circles are almost perfect for these examples, exceptfor minor mistakes such as liu-shu?q?
?ng and meeting-za`izuo` in (a) and .-, in (c)).
In particular, the baseline systemdemonstrates typical ?garbage-collection?
phenomena in proper name ?shuqing?
in both languages in (a), number?4000?
and word ?la?ib??n?
(lit.
?guest?)
in (b), word ?troublesome?
and ?lu`lu`?
(lit.
?land-route?)
in (c), and ?block-houses?
and ?dia?oba?o?
(lit.
?bunker?)
in (d).
We found this garbage-collection behavior to be especially common withproper names, numbers, and uncommon words in both languages.
Most interestingly, in (c), our smoothed-`0 systemcorrectly aligns ?extremely?
to ?he?n he?n he?n he?n?
(lit.
?very very very very?)
which is rare in the bitext.315task data (M) system align F1 (%) word trans (M) ??sing.
Bleu (%)2008 2009 2010Chi-Eng 9.6+12baseline 73.2 3.5 6.2 28.7`0-norm 76.5 2.0 3.3 29.5difference +3.3 ?43% ?47% +0.8Ara-Eng 5.4+4.3baseline 65.0 3.1 4.5 39.8 42.5`0-norm 70.8 1.8 1.8 41.1 43.7difference +5.9 ?39% ?60% +1.3 +1.2Ara-Eng 44+37baseline 66.2 15 5.0 41.6 44.9`0-norm 71.8 7.9 1.8 42.5 45.3difference +5.6 ?47% ?64% +0.9 +0.4Urd-Eng 1.7+1.5baseline 1.7 4.5 25.3?
29.8`0-norm 1.2 2.2 25.9?
31.2difference ?29% ?51% +0.6?
+1.4Cze-Eng 2.1+2.3baseline 65.6 1.5 3.0 17.3 18.0`0-norm 72.3 1.0 1.4 17.9 18.4difference +6.7 ?33% ?53% +0.6 +0.4Table 1: Adding the `0-norm to the IBM models improves both alignment and translation accuracy across four differentlanguage pairs.
The word trans column also shows that the number of distinct word translations (i.e., the size of thelexical weighting table) is reduced.
The ??sing.
column shows the average fertility of once-seen source words.
ForCzech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST OpenMT Evaluation.
?Half of this test set was also used for tuning feature weights.?
Czech-English: A corpus of 4 million words ofCzech-English data from the News Commen-tary corpus.4We set the hyperparameters ?
and ?
by tuningon gold-standard word alignments (to maximize F1)when possible.
For Arabic-English and Chinese-English, we used 346 and 184 hand-aligned sen-tences from LDC2006E86 and LDC2006E93.
Sim-ilarly, for Czech-English, 515 hand-aligned sen-tences were available (Bojar and Prokopova?, 2006).But for Urdu-English, since we did not have anygold alignments, we used ?
= 10 and ?
= 0.05.
Wedid not choose a large ?, as the dataset was small,and we chose a conservative value for ?.We ran word alignment in both directions andsymmetrized using grow-diag-final (Koehn et al,2003).
For models with the smoothed `0 prior, wetuned ?
and ?
separately in each direction.3.2 AlignmentFirst, we evaluated alignment accuracy directly bycomparing against gold-standard word alignments.4This data is available at http://statmt.org/wmt10.The results are shown in the alignment F1 col-umn of Table 1.
We used balanced F-measure ratherthan alignment error rate as our metric (Fraser andMarcu, 2007).Following Dyer et al (2011), we also measuredthe average fertility, ?
?sing., of once-seen sourcewords in the symmetrized alignments.
Our align-ments show smaller fertility for once-seen words,suggesting that they suffer from ?garbage collec-tion?
effects less than the baseline alignments do.The fact that we had to use hand-aligned data totune the hyperparameters ?
and ?
means that ourmethod is no longer completely unsupervised.
How-ever, our observation is that alignment accuracy isactually fairly robust to the choice of these hyperpa-rameters, as shown in Table 2.
As we will see below,we still obtained strong improvements in translationquality when hand-aligned data was unavailable.We also tried generating 50 word classes usingthe tool provided in GIZA++.
We found that addingword classes improved alignment quality a little, butmore so for the baseline system (see Table 3).
Weused the alignments generated by training with wordclasses for our translation experiments.316?
model?0 10 25 50 75 100 250 500 750?HMM 47.5M4 52.10.5HMM 46.3 48.4 52.8 55.7 57.5 61.5 62.6 62.7M4 51.7 53.7 56.4 58.6 59.8 63.3 64.4 64.80.1HMM 55.6 60.4 61.6 62.1 61.9 61.8 60.2 60.1M4 58.2 62.4 64.0 64.4 64.8 65.5 65.6 65.90.05HMM 59.1 61.4 62.4 62.5 62.3 60.8 58.7 57.7M4 61.0 63.5 64.6 65.3 65.3 65.4 65.7 65.70.01HMM 59.7 61.6 60.0 59.5 58.7 56.9 55.7 54.7M4 62.9 65.0 65.1 65.2 65.1 65.4 65.3 65.40.005HMM 58.1 59.0 58.3 57.6 57.0 55.9 53.9 51.7M4 62.0 64.1 64.5 64.5 64.5 65.0 64.8 64.60.001HMM 51.7 52.1 51.4 49.3 50.4 46.8 45.4 44.0M4 59.8 61.3 61.5 61.0 61.8 61.2 61.0 61.2Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM modelfor Arabic-English alignment (?
= 0).word classes?direction system no yesP( f | e)baseline 49.0 52.1`0-norm 63.9 65.9difference +14.9 +13.8P(e | f )baseline 64.3 65.2`0-norm 69.2 70.3difference +4.9 +5.1Table 3: Adding word classes improves the F-score inboth directions for Arabic-English alignment by a little,for the baseline system more so than ours.Figure 2 shows four examples of Chinese-English alignment, comparing the baseline with oursmoothed-`0 method.
In all four cases, the base-line produces incorrect extra alignments that preventgood translation rules from being extracted whilethe smoothed-`0 results are correct.
In particular, thebaseline system demonstrates typical ?garbage col-lection?
behavior (Moore, 2004) in all four exam-ples.3.3 TranslationWe then tested the effect of word alignments ontranslation quality using the hierarchical phrase-based translation system Hiero (Chiang, 2007).
Weused a fairly standard set of features: seven in-herited from Pharaoh (Koehn et al, 2003), a sec-setting align F1 (%) Bleu (%)t( f | e) t(e | f ) 2008 20091st 1st 70.8 41.1 43.71st 2nd 70.7 41.1 43.82nd 1st 70.7 40.7 44.12nd 2nd 70.9 41.1 44.2Table 4: Optimizing hyperparameters on alignment F1score does not necessarily lead to optimal Bleu.
Thefirst two columns indicate whether we used the first- orsecond-best alignments in each direction (according toF1); the third column shows the F1 of the symmetrizedalignments, whose corresponding Bleu scores are shownin the last two columns.ond language model, and penalties for the gluerule, identity rules, unknown-word rules, and twokinds of number/name rules.
The feature weightswere discriminatively trained using MIRA (Chi-ang et al, 2008).
We used two 5-gram languagemodels, one on the combined English sides ofthe NIST 2009 Arabic-English and Chinese-Englishconstrained tracks (385M words), and another on2 billion words of English.For each language pair, we extracted grammarrules from the same data that were used for wordalignment.
The development data that were used fordiscriminative training were: for Chinese-Englishand Arabic-English, data from the NIST 2004 andNIST 2006 test sets, plus newsgroup data from the317GALE program (LDC2006E92); for Urdu-English,half of the NIST 2008 test set; for Czech-English,a training set of 2051 sentences provided by theWMT10 translation workshop.The results are shown in the Bleu column of Ta-ble 1.
We used case-insensitive IBM Bleu (closestreference length) as our metric.
Significance test-ing was carried out using bootstrap resampling with1000 samples (Koehn, 2004; Zhang et al, 2004).All of the tests showed significant improvements(p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu.For Urdu, even though we didn?t have manual align-ments to tune hyperparameters, we got significantgains over a good baseline.
This is promising for lan-guages that do not have any manually aligned data.Ideally, one would want to tune ?
and ?
to max-imize Bleu.
However, this is prohibitively expen-sive, especially if we must tune them separatelyin each alignment direction before symmetrization.We ran some contrastive experiments to investi-gate the impact of hyperparameter tuning on trans-lation quality.
For the smaller Arabic-English cor-pus, we symmetrized all combinations of the twotop-scoring alignments (according to F1) in each di-rection, yielding four sets of alignments.
Table 4shows Bleu scores for translation models learnedfrom these alignments.
Unfortunately, we find thatoptimizing F1 is not optimal for Bleu?using thesecond-best alignments yields a further improve-ment of 0.5 Bleu on the NIST 2009 data, which isstatistically significant (p < 0.05).4 Related WorkSchoenemann (2011a), taking inspiration from Bo-drumlu et al (2009), uses integer linear program-ming to optimize IBM Model 1?2 and the HMMwith the `0-norm.
This method, however, does notoutperform GIZA++.
In later work, Schoenemann(2011b) used projected gradient descent for the `1-norm.
Here, we have adopted his use of projectedgradient descent, but using a smoothed `0-norm.Liang et al (2006) show how to train IBM mod-els in both directions simultaneously by adding aterm to the log-likelihood that measures the agree-ment between the two directions.
Grac?a et al (2010)explore modifications to the HMM model that en-courage bijectivity and symmetry.
The modificationstake the form of constraints on the posterior dis-tribution over alignments that is computed duringthe E-step.
Mermer and Sarac?lar (2011) explore aBayesian version of IBM Model 1, applying sparseDirichlet priors to t. However, because this methodrequires the use of Monte Carlo methods, it is notclear how well it can scale to larger datasets.5 ConclusionWe have extended the IBM models and HMM modelby the addition of an `0 prior to the word-to-wordtranslation model, which compacts the word-to-word translation table, reducing overfitting, and, inparticular, the ?garbage collection?
effect.
We haveshown how to perform MAP-EM with this priorefficiently, even for large datasets.
The method isimplemented as a modification to the open-sourcetoolkit GIZA++, and we have shown that it signif-icantly improves translation quality across four dif-ferent language pairs.
Even though we have used asmall set of gold-standard alignments to tune ourhyperparameters, we found that performance wasfairly robust to variation in the hyperparameters, andtranslation performance was good even when gold-standard alignments were unavailable.
We hope thatour method, due to its simplicity, generality, and ef-fectiveness, will find wide application for trainingbetter statistical translation systems.AcknowledgmentsWe are indebted to Thomas Schoenemann for ini-tial discussions and pilot experiments that led tothis work, and to the anonymous reviewers fortheir valuable comments.
We thank Jason Riesa forproviding the Arabic-English and Chinese-Englishhand-aligned data and the alignment visualizationtool, and Chris Dyer for the Czech-English hand-aligned data.
This research was supported in partby DARPA under contract DOI-NBC D11AP00244and a Google Faculty Research Award to L. H.318ReferencesAndrew Barron, Jorma Rissanen, and Bin Yu.
1998.
Theminimum description length principle in coding andmodeling.
IEEE Transactions on Information Theory,44(6):2743?2760.Dimitri P. Bertsekas.
1999.
Nonlinear Programming.Athena Scientific.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.
2009.A new objective function for word alignment.
In Pro-ceedings of the NAACL HLT Workshop on Integer Lin-ear Programming for Natural Language Processing.Ondr?ej Bojar and Magdalena Prokopova?.
2006.
Czech-English word alignment.
In Proceedings of LREC.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19:263?311.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of EMNLP.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?208.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Computational Linguistics, 39(4):1?38.John Duchi, Shai Shalev-Shwartz, Yoram Singer, andTushar Chandra.
2008.
Efficient projections onto the`1-ball for learning in high dimensions.
In Proceed-ings of ICML.Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A.Smith.
2011.
Unsupervised word alignment with ar-bitrary features.
In Proceedings of ACL.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
Computational Linguistics, 33(3):293?303.Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar.2010.
Learning tractable word alignment modelswith complex constraints.
Computational Linguistics,36(3):481?504.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Cos?kun Mermer and Murat Sarac?lar.
2011.
Bayesianword alignment for statistical machine translation.
InProceedings of ACL HLT.Robert C. Moore.
2004.
Improving IBM word-alignment Model 1.
In Proceedings of ACL.Robert Moore.
2005.
A discriminative framework forbilingual word alignment.
In Proceedings of HLT-EMNLP.Franz Joseph Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30:417?449.Jason Riesa and Daniel Marcu.
2010.
Hierarchicalsearch for word alignment.
In Proceedings of ACL.Thomas Schoenemann.
2011a.
Probabilistic word align-ment under the L0-norm.
In Proceedings of CoNLL.Thomas Schoenemann.
2011b.
Regularizing mono- andbi-word models for word alignment.
In Proceedingsof IJCNLP.Ben Taskar, Lacoste-Julien Simon, and Klein Dan.
2005.A discriminative matching approach to word align-ment.
In Proceedings of HLT-EMNLP.Ashish Vaswani, Adam Pauls, and David Chiang.
2010.Efficient optimization of an MDL-inspired objectivefunction for unsupervised part-of-speech tagging.
InProceedings of ACL.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of COLING.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
In Proceed-ings of LREC.319
