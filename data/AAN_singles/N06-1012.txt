Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 89?95,New York, June 2006. c?2006 Association for Computational LinguisticsReducing Weight Undertrainingin Structured Discriminative LearningCharles Sutton, Michael Sindelar, and Andrew McCallumDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003 USA{casutton,mccallum}@cs.umass.edu, msindela@student.umass.eduAbstractDiscriminative probabilistic models are verypopular in NLP because of the latitude theyafford in designing features.
But traininginvolves complex trade-offs among weights,which can be dangerous: a few highly-indicative features can swamp the contributionof many individually weaker features, causingtheir weights to be undertrained.
Such a modelis less robust, for the highly-indicative featuresmay be noisy or missing in the test data.
Toameliorate this weight undertraining, we intro-duce several new feature bagging methods, inwhich separate models are trained on subsetsof the original features, and combined using amixture model or a product of experts.
Thesemethods include the logarithmic opinion poolsused by Smith et al (2005).
We evaluate fea-ture bagging on linear-chain conditional ran-dom fields for two natural-language tasks.
Onboth tasks, the feature-bagged CRF performsbetter than simply training a single CRF on allthe features.1 IntroductionDiscriminative methods for training probabilistic modelshave enjoyed wide popularity in natural language pro-cessing, such as in part-of-speech tagging (Toutanova etal., 2003), chunking (Sha and Pereira, 2003), named-entity recognition (Florian et al, 2003; Chieu and Ng,2003), and most recently parsing (Taskar et al, 2004).A discriminative probabilistic model is trained to maxi-mize the conditional probability p(y|x) of output labelsy given input variables x, as opposed to modeling thejoint probability p(y, x), as in generative models such asthe Naive Bayes classifier and hidden Markov models.The popularity of discriminative models stems from thegreat flexibility they allow in defining features: becausethe distribution over input features p(x) is not modeled,it can contain rich, highly overlapping features withoutmaking the model intractable for training and inference.In NLP, for example, useful features include word bi-grams and trigrams, prefixes and suffixes, membership indomain-specific lexicons, and information from semanticdatabases such as WordNet.
It is not uncommon to havehundreds of thousands or even millions of features.But not all features, even ones that are carefully engi-neered, improve performance.
Adding more features to amodel can hurt its accuracy on unseen testing data.
Onewell-known reason for this is overfitting: a model withmore features has more capacity to fit chance regulari-ties in the training data.
In this paper, however, we focuson another, more subtle effect: adding new features cancause existing ones to be underfit.
Training of discrimi-native models, such as regularized logistic regression, in-volves complex trade-offs among weights.
A few highly-indicative features can swamp the contribution of manyindividually weaker features, even if the weaker features,taken together, are just as indicative of the output.
Sucha model is less robust, for the few strong features may benoisy or missing in the test data.This effect was memorably observed by Dean Pomer-leau (1995) when training neural networks to drive vehi-cles autonomously.
Pomerleau reports one example whenthe system was learning to drive on a dirt road:The network had no problem learning and thendriving autonomously in one direction, butwhen driving the other way, the network waserratic, swerving from one side of the road tothe other.
.
.
.
It turned out that the networkwas basing most of its predictions on an easily-identifiable ditch, which was always on theright in the training set, but was on the leftwhen the vehicle turned around.
(Pomerleau,1995)The network had features to detect the sides of the road,and these features were active at training and test time,although weakly, because the dirt road was difficult to89detect.
But the ditch was so highly indicative that thenetwork did not learn the dependence between the roadedge and the desired steering direction.A natural way of avoiding undertraining is to train sep-arate models for groups of competing features?in thedriving example, one model with the ditch features, andone with the side-of-the-road features?and then averagethem into a single model.
This is same idea behind log-arithmic opinion pools, used by Smith, Cohn, and Os-borne (2005) to reduce overfitting in CRFs.
In this pa-per, we tailor our ensemble to reduce undertraining ratherthan overfitting, and we introduce several new combina-tion methods, based on whether the mixture is taken ad-ditively or geometrically, and on a per-sequence or per-transition basis.
We call this general class of methodsfeature bagging, by analogy to the well-known baggingalgorithm for ensemble learning.We test these methods on conditional random fields(CRFs) (Lafferty et al, 2001; Sutton and McCallum,2006), which are discriminatively-trained undirectedmodels.
On two natural-language tasks, we show thatfeature bagging performs significantly better than train-ing a single CRF with all available features.2 Conditional Random FieldsConditional random fields (CRFs) (Lafferty et al, 2001;Sutton and McCallum, 2006) are undirected graphicalmodels of a conditional distribution.
Let G be an undi-rected graphical model over random vectors y and x.As a typical special case, y = {yt} and x = {xt} fort = 1, .
.
.
, T , so that y is a labeling of an observed se-quence x.
For a given collection C = {{yc, xc}} ofcliques in G, a CRF models the conditional probabilityof an assignment to labels y given the observed variablesx as:p?
(y|x) = 1Z(x)?c?C?
(yc, xc), (1)where ?
is a potential function and the partition functionZ(x) = ?y?c?C ?
(yc, xc) is a normalization factorover all possible label assignments.We assume the potentials factorize according to a setof features {fk}, which are given and fixed, so that?
(yc, xc) = exp(?k?kfk(yc, xc))(2)The model parameters are a set of real weights?
= {?k},one weight for each feature.Many applications have used the linear-chain CRF, inwhich a first-order Markov assumption is made on thehidden variables.
In this case, the cliques of the condi-tional model are the nodes and edges, so that there arefeature functions fk(yt?1, yt, x, t) for each label transi-tion.
(Here we write the feature functions as potentially?
?
?
??
?0 2 4 6 8 100.500.550.600.650.700.75AlphaAccuracyStrong feature presentStrong feature removedFigure 1: Effect of a single strong feature drowning outweaker features in logistic regression on synthetic data.The x-axis indicates the strength of the strong feature.
Inthe top line, the strong feature is present at training andtest time.
In the bottom line, the strong feature is missingfrom the training data at test time.depending on the entire input sequence.)
Feature func-tions can be arbitrary.
For example, a feature functionfk(yt?1, yt, x, t) could be a binary test that has value 1 ifand only if yt?1 has the label ?adjective?, yt has the label?proper noun?, and xt begins with a capital letter.Linear-chain CRFs correspond to finite state machines,and can be roughly understood as conditionally-trainedhidden Markov models (HMMs).
This class of CRFsis also a globally-normalized extension to Maximum En-tropy Markov Models (McCallum et al, 2000) that avoidsthe label bias problem (Lafferty et al, 2001).Note that the number of state sequences is exponentialin the input sequence length T .
In linear-chain CRFs, thepartition function Z(x), the node marginals p(yi|x), andthe Viterbi labeling can be calculated efficiently by vari-ants of the dynamic programming algorithms for HMMs.3 Weight UndertrainingIn the section, we give a simple demonstration of weightundertraining.
In a discriminative classifier, such asa neural network or logistic regression, a few strongfeatures can drown out the effect of many individuallyweaker features, even if the weak features are just asindicative put together.
To demonstrate this effect, wepresent an illustrative experiment using logistic regres-sion, because of its strong relation to CRFs.
(Linear-90chain conditional random fields are the generalization oflogistic regression to sequence data.
)Consider random variables x1 .
.
.
xn, each distributedas independent standard normal variables.
The outputy is a binary variable whose probability depends on allthe xi; specifically, we define its distribution as y ?Bernoulli(logit(?i xi)).
The correct decision boundaryin this synthetic problem is the hyperplane tangent to theweight vector (1, 1, .
.
.
, 1).
Thus, if n is large, each xicontributes weakly to the output y.
Finally, we includea highly indicative feature xS = ?
?i xi + N (?
=0,?2 = 0.04).
This variable alone is sufficient to deter-mine the distribution of y.
The variable ?
is a parameterof the problem that determines how strongly indicativexS is; specifically, when ?
= 0, the variable xS is ran-dom noise.We choose this synthetic model by analogy to Pomer-leau?s observations.
The xi correspond to the side ofthe road in Pomerleau?s case?the weak features presentat both testing and training?and xS corresponds to theditch?the strongly indicative feature that is corrupted attest time.We examine how badly the learned classifier is de-graded when xS feature is present at training time butmissing at test time.
For several values of the weight pa-rameter ?, we train a regularized logistic regression clas-sifier on 1000 instances with n = 10 weak variables.
InFigure 1, we show how the amount of error caused byablating xS at test time varies according to the strengthof xS .
Each point in Figure 1 is averaged over 100randomly-generated data sets.
When xS is weakly in-dicative, it does not affect the predictions of the model atall, and the classifier?s performance is the same whetherit appears at test time or not.
When xS becomes stronglyindicative, however, the classifier learns to depend on it,and performs much more poorly when xS is ablated, eventhough exactly the same information is available in theweak features.4 Feature BaggingIn this section, we describe the feature bagging method.We divide the set of features F = {fk} into a collec-tion of possibly overlapping subsets F = {F1, .
.
.
FM},which we call feature bags.
We train individual CRFson each of the feature bags using standard MAP training,yielding individual CRFs {p1, .
.
.
pM}.We average the individual CRFs into a single com-bined model.
This averaging can be performed in severalways: we can average probabilities of entire sequences,or of individual transitions; and we can average using thearithmetic mean, or the geometric mean.
This yields fourcombination methods:1.
Per-sequence mixture.
The distribution over labelsequences y given inputs x is modeled as a mixtureof the individual CRFs.
Given nonnegative weights{?1, .
.
.
?m} that sum to 1, the combined model isgiven bypSM(y|x) =M?i=1?ipi(y|x).
(3)It is easily seen that if the sequence model is de-fined as in Equation 3, then the pairwise marginalsare mixtures as well:pSM(yt, yt?1|x) =M?i=1?ipi(yt, yt?1|x).
(4)The probabilities pi(yt, yt?1|x) are pairwisemarginal probabilities in the individual mod-els, which can be efficiently computed by theforward-backward algorithm.We can perform decoding in the mixture model bymaximizing the individual node marginals.
That is,to predict yt we computey?t = arg maxyt pSM(yt|x) = arg maxyt?i?ipi(yt|x),(5)where pi(yt|x) is computed by first runningforward-backward on each of the individual CRFs.In the results here, however, we compute themaximum probability sequence approximately, asfollows.
We form a linear-chain distributionpAPPX(y|x) = ?t pSM(yt|yt?1, x), and compute themost probable sequence according to pAPPX by theViterbi algorithm.
This is approximate because pSMis not a linear-chain distribution in general, evenwhen all the components are.
However, the dis-tribution pAPPX does minimize the KL-divergenceD(pSM?q) over all linear-chain distributions q.The mixture weights can be selected in a variety ofways, including equal voting, as in traditional bag-ging, or EM.2.
Per-sequence product of experts.
These are the log-arithmic opinion pools that have been applied toCRFs by (Smith et al, 2005).
The distribution overlabel sequences y given inputs x is modeled as aproduct of experts (Hinton, 2000).
In a product ofexperts, instead of summing the probabilities fromthe individual models, we multiply them together.Essentially we take a geometric mean instead ofan arithmetic mean.
Given nonnegative weights{?1, .
.
.
?m} that sum to 1, the product model isp(y|x) ?M?i=1(pi(y|x))?i .
(6)91The combined model can also be viewed as a condi-tional random field whose features are the log prob-abilities from the original models:p(y|x) ?
exp{ M?i=1?i log pi(y|x)}(7)By substituting in the CRF definition, it can be seenthat the model in Equation 7 is simply a single CRFwhose parameters are a weighted average of theoriginal parameters.
So feature bagging using theproduct method does not increase the family of mod-els that are considered: standard training of a singleCRF on all available features could potentially pickthe same parameters as the bagged model.Nevertheless, in Section 5, we show that this featurebagging method performs better than standard CRFtraining.The previous two combination methods combine theindividual models by averaging probabilities of en-tire sequences.
Alternatively, in a sequence modelwe can average probabilities of individual transitionspi(yt|yt?1, x).
Computing these transition proba-bilities requires performing probabilistic inference ineach of the original CRFs, because pi(yt|yt?1, x) =?y\yt,yt+1 p(y|yt?1, x).This yields two other combination methods:3.
Per-transition mixture.
The transition probabilitiesare modeled aspTM(yt|yt?1, x) =M?i=1?ipi(yt|yt?1, x) (8)Intuitively, the difference between per-sequence andper-transition mixtures can be understood genera-tively.
In order to generate a label sequence y givenan input x, the per-sequence model selects a mix-ture component, and then generates y using onlythat component.
The per-transition model, on theother hand, selects a component, generates y1 fromthat component, selects another component, gener-ates y2 from the second component given y1, and soon.4.
Per-transition product of experts.
Finally, we cancombine the transition distributions using a productmodelpSP(yt|yt?1, x) ?M?i=1p(yt|yt?1, x)?i (9)Each transition distribution is thus?similarly to theper-sequence case?an exponential-family distribu-tion whose features are the log transition proba-bilities from the individual models.
Unlike theper-sequence product, there is no weight-averagingtrick here, because the probabilities p(yt|yt?1, x)are marginal probabilities.Considered as a sequence distribution p(y|x),the per-transition product is a locally-normalizedmaximum-entropy Markov model (McCallum et al,2000).
It would not be expected to suffer from labelbias, however, because each of the features take thefuture into account; they are marginal probabilitiesfrom CRFs.Of these four combination methods, Method 2, the per-sequence product of experts, is originally due to Smith etal.
(2005).
The other three combination methods are asfar as we know novel.
In the next section, we comparethe four combination methods on several sequence label-ing tasks.
Although for concreteness we describe themin terms of sequence models, they may be generalized toarbitrary graphical structures.5 ResultsWe evaluate feature bagging on two natural languagetasks, named entity recognition and noun-phrase chunk-ing.
We use the standard CoNLL 2003 English data set,which is taken from Reuters newswire and consists ofa training set of 14987 sentences, a development set of3466 sentences, and a testing set of 3684 sentences.
Thenamed-entity labels in this data set corresponding to peo-ple, locations, organizations and other miscellaneous en-tities.
Our second task is noun-phrase chunking.
Weuse the standard CoNLL 2000 data set, which consists of8936 sentences for training and 2012 sentences for test-ing, taken from Wall Street Journal articles annotated bythe Penn Treebank project.
Although the CoNLL 2000data set is labeled with other chunk types as well, herewe use only the NP chunks.As is standard, we compute precision and recall forboth tasks based upon the chunks (or named entities forCoNLL 2003) asP = # correctly labeled chunks# labeled chunksR = # correctly labeled chunks# actual chunksWe report the harmonic mean of precision and recall asF1 = (2PR)/(P + R).For both tasks, we use per-sequence product-of-expertsfeature bagging with two feature bags which we manu-ally choose based on prior experience with the data set.For each experiment, we report two baseline CRFs, onetrained on union of the two feature sets, and one trainedonly on the features that were present in both bags, suchas lexical identity and regular expressions.
In both data92sets, we trained the individual CRFs with a Gaussianprior on parameters with variance ?2 = 10.For the named entity task, we use two feature bagsbased upon character ngrams and lexicons.
Both bagscontain a set of baseline features, such as word identityand regular expressions (Table 4).
The ngram CRF in-cludes binary features for character ngrams of length 2,3, and 4 and word prefixes and suffixes of length 2, 3,and 4.
The lexicon CRF includes membership featuresfor a variety of lexicons containing people names, places,and company names.
The combined model has 2,342,543features.
The mixture weight ?
is selected using the de-velopment set.For the chunking task, the two feature sets are selectedbased upon part of speech and lexicons.
Again, a set ofbaseline features are used, similar to the regular expres-sions and word identity features used on the named entitytask (Table 4).
The first bag also includes part-of-speechtags generated by the Brill tagger and the conjunctions ofthose tags used by Sha and Pereira (2003).
The secondbag uses lexicon membership features for lexicons con-taining names of people, places, and organizations.
In ad-dition, we use part-of-speech lexicons generated from theentire Treebank, such as a list of all words that appear asnouns.
These lists are also used by the Brill tagger (Brill,1994).
The combined model uses 536,203 features.
Themixture weight ?
is selected using 2-fold cross valida-tion.
The chosen model had weight 0.55 on the lexiconmodel, and weight 0.45 on the ngram model.In both data sets, the bagged model performs betterthan the single CRF trained with all of the features.
Forthe named entity task, bagging improves performancefrom 85.45% to 86.61%, with a substantial error reduc-tion of 8.32%.
This is lower than the best reported resultsfor this data set, which is 89.3% (Ando and Zhang, 2005),using a large amount of unlabeled data.
For the chunkingtask, bagging improved the performance from 94.34% to94.77%, with an error reduction of 7.60%.
In both datasets, the improvement is statistically significant (McNe-mar?s test; p < 0.01).On the chunking task, the bagged model also outper-forms the models of Kudo and Matsumoto (2001) andSha and Pereira (2003), and equals the currently-best re-sults of (Ando and Zhang, 2005), who use a large amountof unlabeled data.
Although we use lexicons that werenot included in the previous models, the additional fea-tures actually do not help the original CRF.
Only withfeature bagging do these lexicons improve performance.Finally, we compare the four bagging methods of Sec-tion 4: pre-transition mixture, pre-transition product ofexperts, and per-sequence mixture.
On the named en-tity data, all four models perform in a statistical tie, withno statistically significant difference in their performance(Table 1).
As we mentioned in the last section, the de-Model F1Per-sequence Product of Experts 86.61Per-transition Product of Experts 86.58Per-sequence Mixture 86.46Per-transition Mixture 86.42Table 1: Comparison of various bagging methods on theCoNLL 2003 Named Entity Task.Model F1Single CRF(Base Feat.)
81.52Single CRF(All Feat.)
85.45Combined CRF 86.61Table 2: Results for the CoNLL 2003 Named EntityTask.
The bagged CRF performs significantly better thana single CRF with all available features (McNemar?s test;p < 0.01).coding procedure for the per-sequence mixture is approx-imate.
It is possible that a different decoding procedure,such as maximizing the node marginals, would yield bet-ter performance.6 Previous WorkIn the machine learning literature, there is much work onensemble methods such as stacking, boosting, and bag-ging.
Generally, the ensemble of classifiers is generatedby training on different subsets of data, rather than dif-ferent features.
However, there is some literature withinunstructured classified on combining models trained onfeature subsets.
Ho (1995) creates an ensemble of de-cision trees by randomly choosing a feature subset onwhich to grow each tree using standard decision treelearners.
Other work along these lines include that of Bay(1998) using nearest-neighbor classifiers, and more re-cently Bryll et al(2003).
Also, in Breiman?s work on ran-dom forests (2001), ensembles of random decision treesare constructed by choosing a random feature at eachnode.
This literature mostly has the goal of improvingaccuracy by reducing the classifier?s variance, that is, re-ducing overfitting.In contrast, O?Sullivan et al (2000) specifically focuson increasing robustness by training classifiers to use allof the available features.
Their algorithm FeatureBoostis analogous to AdaBoost, except that the meta-learningalgorithm maintains weights on features instead of on in-stances.
Feature subsets are automatically sampled basedon which features, if corrupted, would most affect theensemble?s prediction.
They show that FeatureBoost ismore robust than AdaBoost on synthetically corruptedUCI data sets.
Their method does not easily extend to se-quence models, especially natural-language models withhundreds of thousands of features.93Model F1Single CRF(Base Feat.)
89.60Single CRF(All Feat.)
94.34(Sha and Pereira, 2003) 94.38(Kudo and Matsumoto, 2001) 94.39(Ando and Zhang, 2005) 94.70Combined CRF 94.77Table 3: Results for the CoNLL 2000 Chunking Task.The bagged CRF performs significantly better than a sin-gle CRF (McNemar?s test; p < 0.01), and equals the re-sults of (Ando and Zhang, 2005), who use a large amountof unlabeled data.wt = wwt begins with a capital letterwt contains only capital letterswt is a single capital letterwt contains some capital letters and some lowercasewt contains a numeric characterwt contains only numeric characterswt appears to be a numberwt is a string of at least two periodswt ends with a periodwt contains a dashwt appears to be an acronymwt appears to be an initialwt is a single letterwt contains punctuationwt contains quotation marksPt = PAll features for time t + ?
for all ?
?
[?2, 2]Table 4: Baseline features used in all bags.
In the abovewt is the word at position t, Pt is the POS tag at positiont, w ranges over all words in the training data, and Pranges over all chunk tags supplied in the training data.The ?appears to be?
features are based on hand-designedregular expressions.There is less work on ensembles of sequence models,as opposed to unstructured classifiers.
One example isAltun, Hofmann, and Johnson (2003), who describe aboosting algorithm for sequence models, but they boostinstances, not features.
In fact, the main advantage oftheir technique is increased model sparseness, whereas inthis work we aim to fully use more features to increaseaccuracy and robustness.Most closely related to the present work is that on log-arithmic opinion pools for CRFs (Smith et al, 2005),which we have called per-sequence mixture of experts inthis paper.
The previous work focuses on reducing over-fitting, combining a model of many features with severalsimpler models.
In contrast, here we apply feature bag-ging to reduce feature undertraining, combining severalmodels with complementary feature sets.
Our currentpositive results are probably not due to reduction in over-fitting, for as we have observed, all the models we test,including the bagged one, have 99.9% F1 on the train-ing set.
Now, feature undertraining can be viewed as atype of overfitting, because it arises when a set of fea-tures is more indicative in the training set than the test-ing set.
Understanding this particular type of overfittingis useful, because it motivates the choice of feature bagsthat we explore in this work.
Indeed, one contribution ofthe present work is demonstrating how a careful choiceof feature bags can yield state-of-the-art performance.Concurrently and independently, Smith and Osborne(2006) present similar experiments on the CoNLL-2003data set, examining a per-sequence mixture of experts(that is, a logarithmic opinion pool), in which the lexi-con features are trained separately.
Their work presentsmore detailed error analysis than we do here, while wepresent results both on other combination methods andon NP chunking.7 ConclusionDiscriminatively-trained probabilistic models have hadmuch success in applications because of their flexibil-ity in defining features, but sometimes even highly-indicative features can fail to increase performance.
Wehave shown that this can be due to feature undertrain-ing, where highly-indicative features prevent training ofmany weaker features.
One solution to this is feature bag-ging: repeatedly selecting feature subsets, training sepa-rate models on each subset, and averaging the individualmodels.On large, real-world natural-language processingtasks, feature bagging significantly improves perfor-mance, even with only two feature subsets.
In this work,we choose the subsets based on our intuition of whichfeatures are complementary for this task, but automati-cally determining the feature subsets is an interesting areafor future work.AcknowledgmentsWe thank Andrew Ng, Hanna Wallach, Jerod Weinman,and Max Welling for helpful conversations.
This workwas supported in part by the Center for Intelligent Infor-mation Retrieval, in part by the Defense Advanced Re-search Projects Agency (DARPA), in part by The Cen-tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS-0326249, and in part by The Central Intelligence Agency,the National Security Agency and National ScienceFoundation under NSF grant #IIS-0427594.
Any opin-ions, findings and conclusions or recommendations ex-pressed in this material are the author(s) and do not nec-essarily reflect those of the sponsor.94ReferencesYasemin Altun, Thomas Hofmann, and Mark Johnson.2003.
Discriminative learning for label sequences viaboosting.
In Advances in Neural Information Process-ing Systems (NIPS*15).Rie Ando and Tong Zhang.
2005.
A high-performancesemi-supervised learning method for text chunking.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?05), pages1?9, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.Stephen D. Bay.
1998.
Combining nearest neighborclassifiers through multiple feature subsets.
In ICML?98: Proceedings of the Fifteenth International Con-ference on Machine Learning, pages 37?45.
MorganKaufmann Publishers Inc.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32, October.Eric Brill.
1994.
Some advances in transformation-basedpart of speech tagging.
In AAAI ?94: Proceedingsof the twelfth national conference on Artificial intelli-gence (vol.
1), pages 722?727.
American Associationfor Artificial Intelligence.Robert Bryll, Ricardo Gutierrez-Osuna, and FrancisQuek.
2003.
Attribute bagging: improving accuracyof classifier ensembles by using random feature sub-sets.
Pattern Recognition, 36:1291?1302.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 160?163.
Edmonton,Canada.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Proceedings of CoNLL-2003.G.E.
Hinton.
2000.
Training products of experts by mini-mizing contrastive divergence.
Technical Report 2000-004, Gatsby Computational Neuroscience Unit.T.
K. Ho.
1995.
Random decision forests.
In Proc.
ofthe 3rd Int?l Conference on Document Analysis andRecognition, pages 278?282, Montreal, Canada, Au-gust.T.
Kudo and Y. Matsumoto.
2001.
Chunking with sup-port vector machines.
In Proceedings of NAACL-2001.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
Proc.
18th Inter-national Conf.
on Machine Learning.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov modelsfor information extraction and segmentation.
In Proc.17th International Conf.
on Machine Learning, pages591?598.
Morgan Kaufmann, San Francisco, CA.Joseph O?Sullivan, John Langford, Rich Caruana, andAvrim Blum.
2000.
Featureboost: A meta learningalgorithm that improves model robustness.
In Interna-tional Conference on Machine Learning.Dean Pomerleau.
1995.
Neural network vision for robotdriving.
In M. Arbib, editor, The Handbook of BrainTheory and Neural Networks.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof HLT-NAACL 2003.
Association for ComputationalLinguistics.Andrew Smith and Miles Osborne.
2006.
Usinggazetteers in discriminative information extraction.
InCoNLL-X, Tenth Conference on Computational Natu-ral Language Learning.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.Logarithmic opinion pools for conditional randomfields.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 18?25, Ann Arbor, Michigan, June.Association for Computational Linguistics.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors, Intro-duction to Statistical Relational Learning.
MIT Press.To appear.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,and Chris Manning.
2004.
Max-margin parsing.
InEmpirical Methods in Natural Language Processing(EMNLP04).Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In HLT-NAACL 2003.95
