2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200?210,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTowards Effective Tutorial Feedback for Explanation Questions:A Dataset and BaselinesMyroslava O. Dzikovska?
and Rodney D. Nielsen?
and Chris Brew?
?School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, UK?Computational Language & Education Research CenterUniversity of Colorado at Boulder, Boulder, CO 80309-0594, USA?Educational Testing Service, Princeton, NJ 08451, USAm.dzikovska@ed.ac.uk, rodney.nielsen@colorado.edu, cbrew@ets.orgAbstractWe propose a new shared task on grading stu-dent answers with the goal of enabling well-targeted and flexible feedback in a tutorial di-alogue setting.
We provide an annotated cor-pus designed for the purpose, a precise speci-fication for a prediction task and an associatedevaluation methodology.
The task is feasiblebut non-trivial, which is demonstrated by cre-ating and comparing three alternative baselinesystems.
We believe that this corpus will beof interest to the researchers working in tex-tual entailment and will stimulate new devel-opments both in natural language processingin tutorial dialogue systems and textual entail-ment, contradiction detection and other tech-niques of interest for a variety of computa-tional linguistics tasks.1 IntroductionIn human-human tutoring, it is an effective strategyto ask students to explain instructional material intheir own words.
Self-explanation (Chi et al, 1994)and contentful talk focused on the domain are cor-related with better learning outcomes (Litman et al,2009; Chi et al, 1994).
There has therefore beenmuch interest in developing automated tutorial dia-logue systems that ask students open-ended expla-nation questions (Graesser et al, 1999; Aleven etal., 2001; Jordan et al, 2006; VanLehn et al, 2007;Nielsen et al, 2009; Dzikovska et al, 2010a).
Inorder to do this well, it is not enough to simplyask the initiating question, because students needthe experience of engaging in meaningful dialogueabout the instructional content.
Thus, systems mustrespond appropriately to student explanations, andmust provide detailed, flexible and appropriate feed-back (Aleven et al, 2002; Jordan et al, 2004).In simple domains, we can adopt a knowledge en-gineering approach and build a domain model and adiagnoser, together with a natural language parser toproduce detailed semantic representations of studentinput (Glass, 2000; Aleven et al, 2002; Pon-Barryet al, 2004; Callaway et al, 2006; Dzikovska et al,2010a).
The advantage of this approach is that itallows for flexible adaptation of feedback to a va-riety of factors such as student performance.
Forexample, it is easy for the system to know if thestudent made the same error before, and adjust itsfeedback to reflect it.
Moreover, this approach al-lows for easy addition of new exercises : as long asan exercise relies on the concepts covered by the do-main model, the system can apply standard instruc-tional strategies to each new question automatically.However, this approach is significantly limited bythe requirement that the domain be small enough toallow comprehensive knowledge engineering, and itis very labor-intensive even for small domains.Alternatively, we can adopt a data-driven ap-proach, asking human tutors to anticipate in ad-vance a range of possible correct and incorrect an-swers, and associating each answer with an appro-priate remediation (Graesser et al, 1999; Jordan etal., 2004; VanLehn et al, 2007).
The advantageof this approach is that it allows more complex andinteresting domains and provides a good frameworkfor eliciting the necessary information from the hu-man experts.
A weakness of this approach, which200also arises in content-scoring applications such asETS?s c-rater (Leacock and Chodorow, 2003), is thathuman experts find it extremely difficult to predictwith any certainty what the full range of student re-sponses will be.
This leads to a lack of adaptivityand generality ?
if the system designers have failedto predict the full range of possibilities, students willoften receive the default feedback.
It is frustratingand confusing for students to repeatedly receive thesame feedback, regardless of their past performanceor dialogue context (Jordan, 2004).Our goal is to address the weaknesses of the data-driven approach by creating a framework for sup-porting more flexible and systematic feedback.
Ourapproach identifies general classes of error, such asomissions, incorrect statements and off-topic state-ments, then aims to develop general remediationstrategies for each error type.
This has the potentialto free system designers from the need to pre-authorseparate remediations for each individual question.A precondition for the success of this approach isthat the system be able to identify error types basedon the student response and the model answers.A contribution of this paper is to provide a newdataset that will enable researchers to develop clas-sifiers specifically for this purpose.
The hope is thatwith an appropriate dataset the data-driven approachwill be flexible and responsive enough to maintainstudent engagement.
We provide a corpus that is la-beled for a set of five student response types, developa precise definition of the corresponding supervisedclassification task, and report results for a variety ofsimple baseline classifiers.
This will provide a ba-sis for the development, comparison and evaluationof alternative approaches to the error classificationtask.
We believe that the natural language capabil-ities needed for this task will be directly applicableto a far wider range of tasks in educational assess-ment, information extraction and computational se-mantics.
This dataset is publicly available and willbe used in a community-wide shared task.2 CorpusThe data set we developed draws on two establishedsources ?
a data set collected and annotated duringan evaluation of the BEETLE II tutorial dialogue sys-tem (Dzikovska et al, 2010a) (henceforth, BEETLEcorpus) and a set of student answers to questionsfrom 16 science modules in the Assessing ScienceKnowledge (ASK) assessment inventory (LawrenceHall of Science, 2006) (henceforth, the Science En-tailments Bank or SCIENTSBANK).In both corpora, each question was associatedwith one or more reference answers provided bythe experts.
Student answers were evaluated againstthese reference answers and, using corpus-specificannotation schemes, assigned labels for correct-ness.
In order to reconcile the two different schemesand to cast the task in terms of standard supervisedmachine classification at the sentence level, we de-rived a new set of annotations, using the annotationscheme shown in Figure 1.Our label set has some similarity to the RTE5 3-way task (Bentivogli et al, 2009), which used ?en-tailment?, ?contradiction?
and ?unknown?
labels.The additional distinctions in our labels reflect typi-cal distinctions made by tutorial dialogue systems.They match our human tutors?
intuitions aboutthe general error types observed in student answersand corresponding teaching tactics.
For example,a likely response to ?partially correct incomplete?would be to tell the student that what they said so farwas correct but it had some gaps, and to encouragethem to fill in those gaps.
In contrast, the responseto ?contradictory?
would emphasize that there is amistake and the student needs to change their an-swer rather than just expand it.
Finally, the responseto ?irrelevant?
may encourage the student to addressrelevant concepts.
The ?non domain?
content couldbe an indicator that the student is frustrated or con-fused, and may require special attention.The annotations in the source corpora make somemore fine-grained distinctions based on the needs ofthe corresponding systems.
In principle, it is possi-ble to have answers that have both correct and con-tradictory parts, and acknowledge correct parts be-fore pointing out mistakes.
There are also distinctclasses of ?non domain?
utterances, e.g., social andmetacognitive statements, to which an ITS may wantto react differently (described in Section 2.1).
How-ever, these situations were rare in our corpora, andwe decided to use a single class for all contradictoryanswers and a single non-domain class.
This may beexpanded in the future as more data becomes avail-able for new versions of this challenge task.201Label Definitionnon domain does not contain domain content, e.g., a help request or ?I don?t know?correct the student answer is correctpartially correct incomplete the answer does not contradict the reference answer and includes somecorrect nuggets, but parts are missingcontradictory an answer that contradicts some part of the reference answerirrelevant contains domain content, but does not answer the questionFigure 1: The set of answer labels used in our taskWe further discuss the relationship with the taskof recognizing textual entailment in Section 5.
Inthe rest of this section, we describe our corpora anddiscuss how we obtained these labels from the rawdata available in our datasets.2.1 BEETLE II dataThe BEETLE corpus consists of the interactions be-tween students and the BEETLE II tutorial dialoguesystem (Dzikovska et al, 2010b).
The BEETLE IIsystem is an intelligent tutoring system that teachesstudents with no knowledge of high-school physicsconcepts in basic electricity and electronics.
In thefirst system evaluation, students spend 3-5 hours go-ing through prepared reading material, building andobserving circuits in the simulator and interactingwith a dialogue-based tutor.
The interaction wasby keyboard, with the computer tutor asking ques-tions, receiving replies and providing feedback via atext-based chat interface.
The data from 73 under-graduate volunteer participants at southeastern USuniversity were recorded and annotated to form theBEETLE human-computer dialogue corpus.The BEETLE II lesson material contains two typesof questions.
Factual questions require them to namea set of objects or a simple property, e.g., ?Whichcomponents in circuit 1 are in a closed path??
or?Are bulbs A and B wired in series or in parallel?.Explanation and definition questions require longeranswers that consist of 1-2 sentences, e.g., ?Whywas bulb A on when switch Z was open??
(expectedanswer ?Because it was still in a closed path with thebattery?)
or ?What is voltage??
(expected answer?Voltage is the difference in states between two ter-minals?).
From the full BEETLE evaluation corpus,we automatically extracted only the students?
an-swers to explanation and definition questions, sincereacting to them appropriately requires processingmore complex input than factual questions.The extracted answers were filtered to remove du-plicates.
In the BEETLE II lesson material thereare a number of similar questions and the tutor ef-fectively had a template answer such as ?TerminalX is connected to the negative/positive battery ter-minal?.
A number of students picked up on thisand used the same pattern in their responses (Stein-hauser et al, 2011).
This resulted in a number of an-swers to certain questions that came from differentspeakers but which were exact copies of each other.We removed such answers from the data set, sincethey were likely to be in both the training and testset, thus inflating our results.
Note that only exactmatches were removed: for example, answers thatwere nearly identical but contained spelling errorswere retained, since they would need to be handledin a practical system.Student utterances were manually labeled using asimplified version of the DEMAND coding scheme(Campbell et al, 2009) shown in Figure 2.
The utter-ances were first classified as related to domain con-tent, student?s metacognitive state, or social inter-action.
Utterances addressing domain content werefurther classified with respect to their correctness asdescribed in the table.
The Kappa value for thisannotation effort was ?
= 0.69.This annotation maps straightforwardly into ourset of labels.
The social and metacognitive state-ments are mapped to the ?non domain?
label;?pc some error?, ?pc?
and ?incorrect?
are mappedto the ?contradictory?
label; and the other classeshave a one-to-one correspondence with our task la-bels.2.2 SCIENTSBANK dataThe SCIENTSBANK corpus (Nielsen et al, 2008)consists of student responses to science assessment202Category Subcategory DescriptionMetacognitive positivenegativecontent-free expressions describing student knowledge, e.g., ?I don?tknow?Social positivenegativeneutralexpressions describing student?s attitudes towards themselves andthe computer (mostly negative in this data, e.g., ?You are stupid?
)Content the utterance addresses domain content.correct the student answer is fully correctpc some missing the student said something correct, but incompleteincorrect the student?s answer is completely incorrectpc some error the student?s answer contains correct parts, but some errors as wellpc the answer contains a mixture of correct, incorrect and missing partsirrelevant the answer may be correct or incorrect, but it is not answering thequestion.Figure 2: Annotation scheme used in the BEETLE corpusquestions.
Specifically, around 16k answers werecollected spanning 16 distinct science subject ar-eas within physical sciences, life sciences, earthsciences, space sciences, scientific reasoning andtechnology.
The tests were part of the Berke-ley Lawrence Hall of Science Assessing ScienceKnowledge (ASK) standardized assessments cover-ing material from their Full Option Science System(FOSS) (Lawrence Hall of Science, 2011).
The an-swers came from students in grades 3-6 in schoolsacross North America.The tests included a variety of questions includ-ing ?fill in the blank?
and multiple choice, but theSCIENTSBANK corpus only used a subset that re-quired students to explain their beliefs about top-ics, typically in one to two sentences.
We reviewedthe questions and a sample of the responses anddecided to filter the following types of questionsfrom the corpus, because they did not mesh withour goals.
First, we removed questions whose ex-pected answer was more than two full sentences(typically multi-step procedures), which were be-yond the scope of our task.
Second, we removedquestions where the expected answer was ill-definedor very open-ended.
Finally, the most frequent rea-son for removing questions was an extreme imbal-ance in the answer classifications (e.g., for manyquestions, almost all of the answers were labeled?partially correct incomplete?).
Specifically, we re-moved questions where more than 80% of the an-swers had the same label and questions with fewerthan three correct answers, since these questionswere unlikely to be useful in differentiating betweenthe quality of assessment systems.The SCIENTSBANK corpus was developed for thepurpose of assessing student responses at a very fine-grained level.
The reference answers were brokendown into several facets, which consisted roughlyof two key terms and the relation connecting them.Nielsen et al annotated student responses to indicatefor each reference answer facet whether the response1) implied the student understood the facet, 2) im-plied they held a contradictory belief, 3) included arelated, non-contradicting facet, or 4) left the facetunaddressed.
Reported agreement was 86.2% witha kappa statistic (Cohen, 1960) of 0.728, which is inthe range of substantial agreement.1Because our task focuses on answer classifica-tion rather than facet classification, we developed aset of rules indicating which combinations of facetsconstituted a correct answer.
We were then ableto compute an answer label from the gold-standardfacet annotations, as follows.
First, if any facetwas annotated as contradictory, the answer was alsolabeled ?contradictory?.
Second, if all of the ex-pected facets for any valid answer were annotatedas being understood, the answer was labeled ?cor-1These statistics were actually based on five labels, but wechose to combine the fifth, a self-contradiction, with other con-tradictions for the purposes of our task.203rect?.
Third, the remaining answers that includedsome but not all of the expected facets were la-beled ?partially correct incomplete?.
Fourth, if ananswer matched none of the expected facets, andhad not been previously labeled as ?contradictory?
itwas given the label ?irrelevant?.
Finally, all ?irrele-vant?
answers were reviewed manually to determinewhether they should be relabeled as ?non domain?.However, since Nielsen et al had already removedmost of the responses that originally fell into thiscategory, we only found 24 ?non domain?
answers.3 BaselinesWe established three baselines for our data set ?
astraightforward majority class baseline, an existingsystem baseline (BEETLE II system performance,which we report only for the BEETLE portion of thedataset), and the performance of a simple classifierbased on lexical similarity, which we report in orderto offer a substantial example of applying the sameclassifier to both portions of the dataset.3.1 BEETLE II system baselineThe interpretation component of the BEETLE IIsystem uses a syntactic parser and a set of hand-authored rules to extract the domain-specific se-mantic representations of student utterances fromthe text.
These representations were then matchedagainst the semantic representations of expected cor-rect answers supplied by tutors.
The resulting sys-tem output was automatically mapped into our targetlabels as discussed in (Dzikovska et al, 2012).3.2 Lexical similarity baselineTo provide a higher baseline that is compara-ble across both subsets of the data, we builta simple decision tree classifier using the Weka3.6.2 implementation of C4.5 pruned decision trees(weka.classifiers.trees.J48 class), with default pa-rameters.
As features, we used lexical similar-ity scores computed by the Text::Similaritypackage with default parameters2.
The code com-putes four similarity metrics ?
the raw number ofoverlapping words, F1 score, Lesk score and cosinescore.
We compared the learner response to the ex-pected answer(s) and the question, resulting in eight2http://search.cpan.org/dist/Text-Similarity/total features (the four values indicated above forthe comparison with the question and the highest ofeach value from the comparisons with each possibleexpected answer).This baseline is based on the lexical overlap base-line used in RTE tasks (Bentivogli et al, 2009).However, we measured overlap with the questiontext in addition to the overlap with the expectedanswers.
Students often repeat parts of the ques-tion in their answer and this needs to be takeninto account to differentiate, for example, ?par-tially correct incomplete?
and ?correct?
answers.4 Results4.1 Experimental SetupWe held back part of the data set for use as standardtest data in the future challenge tasks.
For BEETLE,this consisted of all student answers to 9 out of 56explanation questions asked by the system, plus ap-proximately 15% of the student answers to the re-maining 47 questions, sampling so that the distribu-tion of labels in test data was similar to the trainingdata.
For SCIENTSBANK, we used a previous train-test split (Nielsen et al, 2009).
For both data sets,the data was split so that in the future we can testhow well the different systems generalize: i.e., howwell they perform on answers to questions for whichthey have some sample student answers vs. howwell they perform on answers to questions that werenot in the training data (e.g., newly created questionsin a deployed system).
We discuss this in more detailin Section 5.In this paper, we report baseline performance onthe training set to demonstrate that the task is suf-ficiently challenging to be interesting and that sys-tems can be compared using our evaluation met-rics.
We preserve the true test data for use in theplanned large-scale system comparisons in a com-munity shared task.For the lexical similarity baseline, we use 10-foldcross-validation.3 For the BEETLE II system base-line, the language understanding module was de-3We did not take the student id into account explicitly duringcross-validation.
While there is some risk that the classifierswill learn features specific to the student, we concluded (basedon our understanding of data collection specifics for both datasets) that there is little enough overlap in cross-validation on thetraining data that this should not have a big effect on the results.204veloped based on eight transcripts, each taken fromthe interaction of a different student with an earlierversion of the system.
These sessions were com-pleted prior to the beginning of the experiment dur-ing which the BEETLE corpus was collected, and arenot included in the corpus presented here.
Thus, thedataset used in the paper constitutes unseen data forthe BEETLE II system.We process the two corpora separately becausethe additional system baseline is available for bee-tle, and because the corpora may be different enoughthat it will be helpful for shared task participants todevise processing strategies that are sensitive to theprovenance of the data.4.2 Evaluation MetricsTable 1 shows the distribution of codes in the anno-tated data.
The distribution is unbalanced, and there-fore in our evaluation results we report per-class pre-cision, recall and F1 scores, plus the averaged scoresusing two different ways to average over per-classevaluation scores, micro- and macro- averaging.For a set of classes C, each represented with Ncinstances in the test set, the macro-averaged recall isdefined asRmacro =1|C|?cCR(c)and the micro-averaged recall asRmicro =?cC1NcR(c)Micro- and macro-averaged precision and F1 are de-fined similarly.Micro-averaging takes class sizes into account, soa system that performs well on the most commonclasses will have a high micro-average score.
This isthe most commonly used classifier evaluation met-ric.
Note that, in particular, overall classificationaccuracy (defined as the number of correctly clas-sified instances out of all instances) is mathemat-ically equivalent to micro-averaged recall (Abuda-wood and Flach, 2011).
However, macro-averagingbetter reflects performance on small classes, and iscommonly used for unbalanced classification prob-lems (see, e.g., (Lewis, 1991)).
We report both val-ues in our results.BEETLE SCIENTSBANKLabel Count Freq.
Count Freq.correct 1157 0.42 2095 0.40partially correctincomplete626 0.23 1431 0.27contradictory 656 0.24 526 0.10irrelevant 86 0.03 1175 0.22non domain 204 0.07 24 0.005total 2729 5251Table 1: Distribution of annotated labels in the dataIn addition, we report the system scores on the bi-nary decision of whether or not the corrective feed-back should be issued (denoted ?corrective feed-back?
in the results table).
It assumes that a tutoringsystem using a classifier will give corrective feed-back if the classifiers returns any label other than?correct?.
Thus, every instance classified as ?par-tially correct incomplete?, ?contradictory?, ?irrele-vant?
or ?non domain?
is counted as true positiveif the hand-annotated label also belongs to this set(even if the classifier disagrees with the annotation);and as false positive if the hand-annotated label is?correct?.
This reflects the idea that students arelikely to be frustrated if the system gives correctivefeedback when their answer is in fact a fully accurateparaphrase of a correct answer.4.3 BEETLE baseline performanceThe detailed evaluation results for all baselines arepresented in Table 2.The majority class baseline is to assign ?correct?to every test instance.
It achieves 42% overall ac-curacy.
However, this is obviously at the expenseof serious errors; for example, such a system wouldtell the students that they are correct if they are say-ing something contradictory.
This is reflected in amuch lower macro-averaged F1 score.The BEETLE II system performs only slightly bet-ter than the baseline on the overall accuracy (0.44vs.
0.42 micro-averaged recall).
However, themacro-averaged F1 score of the BEETLE II systemis substantially higher (0.46 vs. 0.12).
The micro-averaged results show a similar pattern, although themajority-class baseline performs slightly better thanin the macro-averaged case, as expected.Comparing the BEETLE II parser to our lexical205similarity baseline, BEETLE II has lower overall ac-curacy, but performs similarly on micro- and macro-averaged scores.
BEETLE II precision is higher thanthat of the classifier in all cases except for the binarydecision as to whether corrective feedback shouldbe issued.
This is not unexpected given how the sys-tem was designed ?
since misunderstandings causeddialogue breakdown in pilot tests, the parser wasbuilt to prefer rejecting utterances as uninterpretablerather than assigning them an incorrect class, lead-ing to a considerably lower recall.
Around 31% ofutterances could not be interpreted.Our recent analysis shows that both incorrectinterpretations (in particular, confusions between?partially correct incomplete?
and ?contradictory?
)and rejections have significant negative effects onlearning gain (Dzikovska et al, 2012).
Classifierscan be tuned to reject examples where classificationconfidence falls below a given threshold, resultingin precision-recall trade-offs.
Our baseline classifierclassified all answer instances; exploring the possi-bilities for rejecting some low-confidence answers isplanned for future work.4.4 SCIENTSBANK baseline performanceThe accuracy of the majority class baseline (whichassumes all answers are ?correct?)
is 40% for SCI-ENTSBANK, about the same as it was for BEE-TLE.
The evaluation results, based on 10-fold cross-validation, for our simple lexical similarity classi-fier are presented in Table 3.
The lexical similar-ity based classifier outperforms the majority classbaseline by 0.18 and 3% on the macro-averagedF1-measure and accuracy, respectively.
The F1-measure for the two-way classification detecting an-swers which need corrective feedback is 0.66.The scores on SCIENTSBANK are noticeablylower than those for BEETLE.
The SCIENTSBANKincludes questions from 12 distinct science subjectareas, rather than a single area as in BEETLE.
Thisdecision tree classifier learns a function from theeight text similarity features to the desired answerlabel.
Because the features do not mention particularwords, the model can be applied to items other thanthe ones on which it was trained, and even to itemsfrom different subject areas.
However, the correctweighting of the textual similarity features dependson the extent and nature of the expected textual over-Predictn correct pc inc contra irrlvnt nondomcorrect 1213 553 209 392 2pc inc 432 497 128 241 2contra 115 109 58 74 3irrlvnt 335 272 131 468 17nondom 0 0 0 0 0Figure 4: Confusion matrix for lexical classifier on SCI-ENTSBANK.
Predictions in rows, gold labels in columnslap, which does vary from subject-area to subject-area.
We suspect that the differences between sub-ject areas made it hard for the decision-tree classi-fier to find a single, globally appropriate strategy.Nielsen (2009) reported the best results for classify-ing facets when training separate question-specificor even facet-specific classifiers.
Although separatetraining for each item reduces the amount of relevanttraining data for each classifier, it allows each clas-sifier to learn the specifics of how that item works.A comparison using this style of training would be areasonable next step,5 Discussion and Future WorkThe results presented satisfy two critical require-ments for a challenge task.
First, we have shown thatit is feasible to develop a system that performs sig-nificantly better than the majority class baseline.
Onthe macro-averaged F1-measure, our lexical clas-sifier outperformed the majority-class baseline by0.33 (on BEETLE) and 0.18 (on SCIENTSBANK)and by 13% and 3% on accuracy.
Second, we havealso shown, as is desired for a challenge task, thatthe task is not trivial.
With a system specificallydesigned to parse the BEETLE corpus answers, themacro-averaged F1-measure was just 0.46 and onthe binary decision regarding whether the responseneeded corrective feedback, it achieved just 0.63.One contribution of this work was to define a gen-eral classification scheme for student responses thatallows more specific learner feedback.
Another keycontribution was to unify two, previously incom-patible, large student response corpora under thiscommon annotation scheme.
The resultant corpuswill enable researchers to train learning algorithmsto classify student responses.
These classificationscan then be used by a dialogue manager to generatetargeted learner feedback.
The corpus is available206Classifier: majority lexical similarity BEETLE IIPredicted label prec.
recall F1 prec.
recall F1 prec.
recall F1correct 0.42 1.00 0.60 0.68 0.75 0.72 0.93 0.53 0.68partially correct incomplete 0.00 0.00 0.00 0.41 0.38 0.39 0.43 0.53 0.47contradictory 0.00 0.00 0.00 0.39 0.34 0.36 0.58 0.23 0.33irrelevant 0.00 0.00 0.00 0.05 0.02 0.03 0.23 0.17 0.20non domain 0.00 0.00 0.00 0.66 0.82 0.73 0.92 0.46 0.61macroaverage 0.09 0.20 0.12 0.44 0.46 0.45 0.62 0.39 0.46microaverage 0.18 0.42 0.25 0.53 0.55 0.54 0.71 0.44 0.53corrective feedback 0.00 0.00 0.00 0.80 0.74 0.77 0.73 0.56 0.63Table 2: Evaluation results for BEETLE corpusClassifier: lexical similarity BEETLE IIPredicted label corrct pc inc contra irrlvnt nondom corrct pc inc contra irrlvnt nondomcorrect 870 187 199 20 2 617 20 23 0 3part corr incmp 138 239 178 24 11 249 332 146 29 20contradictory 139 153 221 33 22 68 38 149 3 0irrelevant 3 20 12 2 1 4 22 23 15 1non domain 7 27 46 7 168 3 3 1 1 94uninterpretable n/a n/a n/a n/a n/a 216 211 314 38 86Figure 3: Confusion matrix for BEETLE corpus.
Predictions in rows, gold labels in columnsClassifier: baseline lexical similarityPredicted label prec.
recall F1 prec.
recall F1correct 0.40 1.00 0.57 0.51 0.58 0.54partially correct incomplete 0.00 0.00 0.00 0.38 0.35 0.36contradictory 0.00 0.00 0.00 0.16 0.11 0.13irrelevant 0.00 0.00 0.00 0.38 0.40 0.39non domain 0.00 0.00 0.00 0.00 0.00 0.00macroaverage 0.08 0.20 0.11 0.29 0.29 0.29microaverage 0.16 0.40 0.23 0.41 0.43 0.42corrective feedback 0.00 0.00 0.00 0.69 0.63 0.66Table 3: Evaluation results for SCIENTSBANK baselines207for general research purposes and forms the basisof SEMEVAL-2013 shared task ?Textual entailmentand paraphrasing for student input assessment?.4A third contribution of this work was to providebasic evaluation benchmark metrics and the corre-sponding evaluation scripts (downloadable from thesite above) for other researchers, including sharedtask participants.
This will facilitate the comparisonand, hence, the progress, of research.The work reported here is based on approximately8000 student responses to questions covering 12 dis-tinct science subjects and coming from a wide rangeof student ages.
These responses comprise the train-ing data for our task.
The vast majority of priorwork, including BEETLE II, which was included asa benchmark here, has been designed to provide ITSfeedback for relatively small, well-defined domains.The corpus presented in this paper is intended to en-courage research into more generalizable, domain-independent techniques.
Following Nielsen (2009),from whom the SCIENTSBANK corpus was adapted,our shared task evaluation corpus will be composedof three types of data: additional student responsesfor all of the questions in the training data (Un-seen Answers), student responses to questions thatwere not seen in the training data, but that are fromthe same subject areas (Unseen Questions), and re-sponses to questions from three entirely differentsubject areas (Unseen Domains), though in this casethe questions are still from the same general domain?
science.
Unseen Answers is the typical scenariofor the vast majority of prior work ?
training andtesting on responses to the same questions.
UnseenQuestions and Unseen Domains allow researchers toevaluate how well their systems generalize to nearand far domains, respectively.The primary target application for this work is in-telligent tutoring systems, where the classification ofresponses is intended to facilitate specific pedagogicfeedback.
Beneath the surface, the baseline systemsreported here are more similar to grading systemsthat use the approach of (Leacock and Chodorow,2003), which uses classifier technology to detect ex-pressions of facet-like concepts, then converts theresult to a numerical score, than to grading systemslike (Mohler et al, 2011), which directly produces a4See http://www.cs.york.ac.uk/semeval-2013/task4/numerical score, using support vector regression andsimilar techniques.
Either approach is reasonable,but we think that feedback is the more challeng-ing test of a system?s ultimate abilities, and there-fore a better candidate for the shared task.
The cor-pora from those systems, alongside with new cor-pora currently being collected in BEETLE and SCI-ENTSBANK domains, can serve as sources of datafor future tasks extensions.Future systems developed for this task can benefitfrom the large amount of existing work on recog-nizing textual entailment (Giampiccolo et al, 2007;Giampiccolo et al, 2008; Bentivogli et al, 2009)and on detecting contradiction (Ritter et al, 2008;De Marneffe et al, 2008).
However, there are sub-stantial challenges in applying the RTE tools directlyto this data set.
Our set of labels is more fine-grainedthan RTE labels to reflect the needs of intelligent tu-toring systems (see Section 2).
In addition, the top-performing systems in RTE5 3-way task, as well ascontradiction detection methods, rely on NLP toolssuch as dependency parsers and semantic role la-belers; these do not perform well on specializedterminology and language constructs coming from(typed) dialogue context.
We chose to use lexicalsimilarity as a baseline specifically because a simi-lar measure was used as a standard baseline in RTEtasks, and we expect that adapting the more complexRTE approaches for purposes of this task will resultin both improved results on our data set and new de-velopments in computational linguistics algorithmsused for RTE and related tasks.AcknowledgmentsWe thank Natalie Steinhauser, Gwendolyn Camp-bell, Charlie Scott, Simon Caine, Leanne Taylor,Katherine Harrison and Jonathan Kilgour for helpwith data collection and preparation.
The researchreported here was supported by the US ONR awardN000141010085 and by the Institute of EducationSciences, U.S. Department of Education, throughGrant R305A110811 to Boulder Language Tech-nologies Inc.
The opinions expressed are those ofthe authors and do not represent views of the Insti-tute or the U.S. Department of Education.208ReferencesTarek Abudawood and Peter Flach.
2011.
Learn-ing multi-class theories in ilp.
In The 20th Interna-tional Conference on Inductive Logic Programming(ILP?10).
Springer, June.V.
Aleven, O. Popescu, and K. R. Koedinger.
2001.Towards tutorial dialog to support self-explanation:Adding natural language understanding to a cogni-tive tutor.
In Proceedings of the 10th InternationalConference on Artificial Intelligence in Education(AIED ?01)?.Vincent Aleven, Octav Popescu, and KoedingerKoedinger.
2002.
Pilot-testing a tutorial dialoguesystem that supports self-explanation.
Lecture Notesin Computer Science, 2363:344?354.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernando Magnini.
2009.
The fifthPASCAL recognizing textual entailment challenge.
InNotebook papers and results, Text Analysis Confer-ence (TAC).Charles Callaway, Myroslava Dzikovska, Colin Mathe-son, Johanna Moore, and Claus Zinn.
2006.
Usingdialogue to learn math in the LeActiveMath project.In Proceedings of the ECAI Workshop on Language-Enhanced Educational Technology, pages 1?8, Au-gust.Gwendolyn C. Campbell, Natalie B. Steinhauser, My-roslava O. Dzikovska, Johanna D. Moore, Charles B.Callaway, and Elaine Farrow.
2009.
The DeMANDcoding scheme: A ?common language?
for represent-ing and analyzing student discourse.
In Proceedingsof 14th International Conference on Artificial Intelli-gence in Education (AIED), poster session, Brighton,UK, July.Michelene T. H. Chi, Nicholas de Leeuw, Mei-HungChiu, and Christian LaVancher.
1994.
Eliciting self-explanations improves understanding.
Cognitive Sci-ence, 18(3):439?477.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20(1):3746.M.C.
De Marneffe, A.N.
Rafferty, and C.D.
Manning.2008.
Finding contradictions in text.
Proceedings ofACL-08: HLT, pages 1039?1047.Myroslava Dzikovska, Diana Bental, Johanna D. Moore,Natalie B. Steinhauser, Gwendolyn E. Campbell,Elaine Farrow, and Charles B. Callaway.
2010a.
In-telligent tutoring with natural language support in theBeetle II system.
In Sustaining TEL: From Innovationto Learning and Practice - 5th European Conferenceon Technology Enhanced Learning, (EC-TEL 2010),Barcelona, Spain, October.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010b.
Beetle II: a systemfor tutoring and computational linguistics experimen-tation.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL-2010) demo session, Uppsala, Sweden, July.Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-hanna D. Moore.
2012.
Evaluating language under-standing accuracy with respect to objective outcomesin a dialogue system.
In Proceedings of the 13th Con-ference of the European Chapter of the Association forcomputational Linguistics, Avignon, France, April.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, andBill Dolan.
2007.
The third PASCAL recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 1?9, Prague, June.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, Elena Cabrio, and Bill Dolan.2008.
The fourth PASCAL recognizing textual entail-ment challenge.
In Proceedings of Text Analysis Con-ference (TAC) 2008, Gaithersburg, MD, November.Michael Glass.
2000.
Processing language input in theCIRCSIM-Tutor intelligent tutoring system.
In Pro-ceedings of the AAAI Fall Symposium on Building Di-alogue Systems for Tutorial Applications.A.
C. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, and R. Kreuz.
1999.
Autotutor: A simu-lation of a human tutor.
Cognitive Systems Research,1:35?51.Pamela W. Jordan, Maxim Makatchev, and Kurt Van-Lehn.
2004.
Combining competing language un-derstanding approaches in an intelligent tutoring sys-tem.
In James C. Lester, Rosa Maria Vicari, andFa?bio Paraguac?u, editors, Intelligent Tutoring Systems,volume 3220 of Lecture Notes in Computer Science,pages 346?357.
Springer.Pamela Jordan, Maxim Makatchev, Umarani Pap-puswamy, Kurt VanLehn, and Patricia Albacete.2006.
A natural language tutorial dialogue systemfor physics.
In Proceedings of the 19th InternationalFLAIRS conference.Pamela W. Jordan.
2004.
Using student explanationsas models for adapting tutorial dialogue.
In ValerieBarr and Zdravko Markov, editors, FLAIRS Confer-ence.
AAAI Press.Lawrence Hall of Science.
2006.
Assessing ScienceKnowledge (ask).
University of California at Berke-ley, NSF-0242510.Lawrence Hall of Science.
2011.
Full option sciencesystem.209Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.David D. Lewis.
1991.
Evaluating text categorization.
InProceedings of the workshop on Speech and NaturalLanguage, HLT ?91, pages 312?318, Stroudsburg, PA,USA.Diane Litman, Johanna Moore, Myroslava Dzikovska,and Elaine Farrow.
2009.
Using natural language pro-cessing to analyze tutorial dialogue corpora across do-mains and modalities.
In Proceedings of 14th Interna-tional Conference on Artificial Intelligence in Educa-tion (AIED), Brighton, UK, July.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graphalignments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 752?762, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Rodney D. Nielsen, Wayne Ward, James H. Martin, andMartha Palmer.
2008.
Annotating students?
under-standing of science concepts.
In Proceedings of theSixth International Language Resources and Evalua-tion Conference, (LREC08), Marrakech, Morocco.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2009.
Recognizing entailment in intelligent tutoringsystems.
The Journal of Natural Language Engineer-ing, 15:479?501.Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-beth Owen Bratt, and Stanley Peters.
2004.
Advan-tages of spoken language interaction in dialogue-basedintelligent tutoring systems.
In Proceedings of ITS-2004, pages 390?400.A.
Ritter, D. Downey, S. Soderland, and O. Etzioni.2008.
It?s a contradiction?no, it?s not: a case studyusing functional relations.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 11?20.Natalie B. Steinhauser, Gwendolyn E. Campbell,Leanne S. Taylor, Simon Caine, Charlie Scott, My-roslava O. Dzikovska, and Johanna D. Moore.
2011.Talk like an electrician: Student dialogue mimickingbehavior in an intelligent tutoring system.
In Proceed-ings of the 15th International Conference on ArtificialIntelligence in Education (AIED-2011).Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007.Developing pedagogically effective tutorial dialoguetactics: Experiments and a testbed.
In Proceedings ofSLaTE Workshop on Speech and Language Technol-ogy in Education, Farmington, PA, October.210
