Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 278?286,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMemory-enhanced Decoder for Neural Machine TranslationMingxuan Wang1 Zhengdong Lu2 Hang Li2 Qun Liu3,11Key Laboratory of Intelligent Information Processing,Institute of Computing Technology, Chinese Academy of Sciences{wangmingxuan,liuqun}@ict.ac.cn2Noah?s Ark Lab, Huawei Technologies{Lu.Zhengdong,HangLi.HL}@huawei.com3ADAPT Centre, School of Computing, Dublin City UniversityAbstractWe propose to enhance the RNN decoderin a neural machine translator (NMT) withexternal memory, as a natural but power-ful extension to the state in the decodingRNN.
This memory-enhanced RNN de-coder is called MEMDEC.
At each timeduring decoding, MEMDEC will read fromthis memory and write to this memoryonce, both with content-based addressing.Unlike the unbounded memory in previ-ous work(Bahdanau et al, 2014) to storethe representation of source sentence, thememory in MEMDEC is a matrix with pre-determined size designed to better cap-ture the information important for the de-coding process at each time step.
Ourempirical study on Chinese-English trans-lation shows that it can improve by 4.8BLEU upon Groundhog and 5.3 BLEUupon on Moses, yielding the best perfor-mance achieved with the same training set.1 IntroductionThe introduction of external memory has greatlyexpanded the representational capability of neu-ral network-based model on modeling se-quences(Graves et al, 2014), by providing flex-ible ways of storing and accessing information.More specifically, in neural machine translation,one great improvement came from using an arrayof vectors to represent the source in a sentence-level memory and dynamically accessing relevantsegments of them (alignment) (Bahdanau et al,2014) through content-based addressing (Graveset al, 2014).
The success of RNNsearch demon-strated the advantage of saving the entire sen-tence of arbitrary length in an unbounded mem-ory for operations of next stage (e.g., decoding).In this paper, we show that an external memorycan be used to facilitate the decoding/generationprocess thorough a memory-enhanced RNN de-coder, called MEMDEC.
The memory inMEMDEC is a direct extension to the state inthe decoding, therefore functionally closer to thememory cell in LSTM(Hochreiter and Schmid-huber, 1997).
It takes the form of a matrix withpre-determined size, each column (?a memorycell?)
can be accessed by the decoding RNN withcontent-based addressing for both reading andwriting during the decoding process.
This mem-ory is designed to provide a more flexible wayto select, represent and synthesize the informa-tion of source sentence and previously generatedwords of target relevant to the decoding.
Thisis in contrast to the set of hidden states of theentire source sentence (which can viewed as an-other form of memory) in (Bahdanau et al, 2014)for attentive read, but can be combined with it togreatly improve the performance of neural ma-chine translator.
We apply our model on English-Chinese translation tasks, achieving performancesuperior to any published results, SMT or NMT,on the same training data (Xie et al, 2011; Menget al, 2015; Tu et al, 2016; Hu et al, 2015)Our contributions are mainly two-folds?
we propose a memory-enhanced decoder for278neural machine translator which naturallyextends the RNN with vector state.?
our empirical study on Chinese-Englishtranslation tasks show the efficacy of theproposed model.Roadmap In the remainder of this paper, wewill first give a brief introduction to attention-based neural machine translation in Section 2,presented from the view of encoder-decoder,which treats the hidden states of source as anunbounded memory and the attention model asa content-based reading.
In Section 3, wewill elaborate on the memory-enhanced decoderMEMDEC.
In Section 4, we will apply NMT withMEMDEC to a Chinese-English task.
Then inSection 5 and 6, we will give related work andconclude the paper.2 Neural machine translation withattentionOur work is built on attention-basedNMT(Bahdanau et al, 2014), which repre-sents the source sentence as a sequence ofvectors after being processed by RNN or bi-directional RNNs, and then conducts dynamicalignment and generation of the target sentencewith another RNN simultaneously.Attention-based NMT, with RNNsearch as itsmost popular representative, generalizes the con-ventional notion of encoder-decoder in using aunbounded memory for the intermediate repre-sentation of source sentence and content-basedaddressing read in decoding, as illustrated inFigure 1.
More specifically, at time step t,RNNsearch first get context vector ct after read-ing from the source representation MS, which isthen used to update the state, and generate theword yt (along with the current hidden state st,and the previously generated word yi?1).Formally, given an input sequence x =[x1, x2, .
.
.
, xTx ] and the previously generatedsequence y<t = [y1, y2, .
.
.
, yt?1], the probabil-ity of next word yt isp(yt|y<t;x) = f(ct, yt?1, st), (1)Figure 1: RNNsearch in the encoder-decoder view.where st is state of decoder RNN at time step tcalculated asst = g(st?1, yt?1, ct).
(2)where g(?)
can be an be any activation function,here we adopt a more sophisticated dynamic op-erator as in Gated Recurrent Unit (GRU, (Cho etal., 2014)).
In the remainder of the paper, we willalso use GRU to stand for the operator.
The read-ing ct is calculated asct =j=Tx?j=1?t,jhj , (3)where hj is the jth cell in memory MS. Moreformally, hj = [??hj>,?
?hj>]> is the annotationsof xj and contains information about the wholeinput sequence with a strong focus on the partssurrounding xj , which is computed by a bidirec-tional RNN.
The weight ?t,j is computed by?t,j =exp(et,j)?k=Txk=1 exp(et,k).where ei,j = vTa tanh(Wast?1 + Uahj) scoreshow well st?1 and the memory cell hj match.This is called automatic alignment (Bahdanau etal., 2014) or attention model (Luong et al, 2015),but it is essentially reading with content-basedaddressing defined in (Graves et al, 2014).
Withthis addressing strategy the decoder can attend tothe source representation that is most relevant tothe stage of decoding.279Figure 2: Diagram of the proposed decoder MEMDEC with details.2.1 Improved Attention ModelThe alignment model ?t,j scores how well theoutput at position tmatches the inputs around po-sition j based on st?1 and hj .
It is intuitivelybeneficial to exploit the information of yt?1 whenreading from MS, which is missing from the im-plementation of attention-based NMT in (Bah-danau et al, 2014).
In this work, we build amore effective alignment path by feeding bothprevious hidden state st?1 and the context wordyt?1 to the attention model, inspired by the re-cent implementation of attention-based NMT1.Formally, the calculation of et,j becomeset,j = vTa tanh(Was?t?1 +Uahj),where?
s?t?1 = H(st?1, eyt?1) is an intermediatestate tailored for reading from MS with theinformation of yt?1 (its word embedding be-ing eyt?1) added;?
H is a nonlinear function, which can beas simple as tanh or as complex as GRU.In our preliminary experiments, we foundGRU works slightly better than tanh func-tion, but we chose the latter for simplicity.1github.com/nyu-dl/dl4mt-tutorial/tree/master/session23 Decoder with External MemoryIn this section we will elaborate on the proposedmemory-enhanced decoder MEMDEC.
In ad-dition to the source memory MS, MEMDEC isequipped with a buffer memory MB as an ex-tension to the conventional state vector.
Fig-ure 3 contrasts MEMDEC with the decoder inRNNsearch (Figure 1) on a high level.Figure 3: High level digram of MEMDEC.In the remainder of the paper, we will refer tothe conventional state as vector-state (denoted st)and its memory extension as memory-state (de-noted as MBt ).
Both states are updated at eachtime step in a interweaving fashion, while the out-put symbol yt is predicted based solely on vector-state st (along with ct and yt?1).
The diagram ofthis memory-enhanced decoder is given in Figure2.280Vector-State Update At time t, the vector-statest is first used to read MBrt?1 = readB(st?1,MBt?1) (4)which then meets the previous prediction yt?1 toform an ?intermediate?
state-vectors?t = tanh(Wrrt?1 +Wyeyt?1).
(5)where eyt?1 is the word-embedding associatedwith the previous prediction yt?1.
This pre-states?t is used to read the source memory MSct = readS(s?t,MS).
(6)Both readings in Eq.
(4) & (6) follow content-based addressing(Graves et al, 2014) (detailslater in Section 3.1).
After that, rt?1 is combinedwith output symbol yt?1 and ct to update the newvector-statest = GRU(rt?1,yt?1, ct) (7)The update of vector-state is illustrated in Fig-ure 4.Figure 4: Vector-state update at time t.Memory-State Update As illustrated in Fig-ure 5, the update for memory-state is simple afterthe update of vector-state: with the vector-statest+1 the updated memory-state will beMBt = write(st,MBt?1) (8)The writing to the memory-state is also content-based, with same forgetting mechanism sug-gested in (Graves et al, 2014), which we willelaborate with more details later in this section.Figure 5: Memory-state update at time t.Prediction As illustrated in Figure 6, the pre-diction model is same as in (Bahdanau et al,2014), where the score for word y is given byscore(y) = DNN([st, ct, eyt?1 ])>?y (9)where ?y is the parameters associated with theword y.
The probability of generating word y attime t is then given by a softmax over the scoresp(y|st, ct, yt?1) =exp(score(y))?y?
exp(score(y?
)).Figure 6: Prediction at time t.3.1 Reading Memory-StateFormally MBt?
?
Rn?m is the memory-state attime t?
after the memory-state update, where n isthe number of memory cells and m is the dimen-sion of vector in each cell.
Before the vector-stateupdate at time t, the output of reading rt is givenbyrt =j=n?j=1wRt (j)MBt?1(j)where wRt ?
Rn specifies the normalized weightsassigned to the cells in MBt .
Similar with thereading from MS ( a.k.a.
attention model), weuse content-based addressing in determining wRt .281More specifically, wRt is also updated from theone from previous time wRt?1 aswRt = gRt wRt?1 + (1?
gRt )w?Rt , (10)where?
gRt = ?
(wRgst) is the gate function, with pa-rameters wRg ?
Rm;?
w?t gives the contribution based on the cur-rent vector-state stw?Rt = softmax(aRt ) (11)aRt (i) = v>(WRaMBt?1(i) +URast?1), (12)with parameters WRa,URa ?
Rm?m and v ?Rm.3.2 Writing to Memory-StateThere are two types of operation on writing tomemory-state: ERASE and ADD.
Erasion is simi-lar to the forget gate in LSTM or GRU, which de-termines the content to be remove from memorycells.
More specifically, the vector ?ERSt ?
Rmspecifies the values to be removed on each dimen-sion in memory cells, which is than assigned toeach cell through normalized weights wWt .
For-mally, the memory-state after ERASE is given byM?Bt (i) = MBt?1(i)(1?wWt (i) ?
?ERSt ) (13)i = 1, ?
?
?
, nwhere?
?ERSt = ?
(WERSst) is parametrized withWERS ?
Rm?m;?
wWt (i) specifies the weight associated withthe ith cell in the same parametric form asin Eq.
(10)-(12) with generally different pa-rameters.ADD operation is similar with the update gate inLSTM or GRU, deciding how much current in-formation should be written to the memory.MBt (i) = M?Bt (i) +wWt (i)?ADDt?ADDt = ?
(WADDst)where ?ADDt ?
Rm and WADD ?
Rm?m.In our experiments, we have a peculiar but in-teresting observation: it is often beneficial to usethe same weights for both reading (i.e., wRt inSection 3.1) and writing (i.e., wWt in Section 3.2) for the same vector-state st. We conjecture thatthis acts like a regularization mechanism to en-courage the content of reading and writing to besimilar to each other.3.3 Some AnalysisThe writing operation in Eq.
(13) at time tcan be viewed as an nonlinear way to combinethe previous memory-state MBt?1 and the newlyupdated vector-state st, where the nonlinearitycomes from both the content-based addressingand the gating.
This is in a way similar to theupdate of states in regular RNN, while we con-jecture that the addressing strategy in MEMDECmakes it easier to selectively change some con-tent updated (e.g., the relatively short-term con-tent) while keeping other content less modified(e.g., the relatively long-term content).The reading operation in Eq.
(10) can ?extract?the content from MBt relevant to the alignment(reading from MS) and prediction task at time t.This is in contrast with the regular RNN decoderincluding its gated variants, which takes the en-tire state vector to for this purpose.
As one ad-vantage, although only part of the information inMBt is used at t, the entire memory-state, whichmay store other information useful for later, willbe carry over to time t + 1 for memory-state up-date (writing).4 Experiments on Chinese-EnglishTranslationWe test the memory-enhanced decoder to task ofChinese-to-English translation, where MEMDECis put on the top of encoder same as in (Bahdanauet al, 2014).4.1 Datasets and Evaluation metricsOur training data for the translation task con-sists of 1.25M sentence pairs extracted from LDCcorpora2, with 27.9M Chinese words and 34.5M2The corpora include LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,282English words respectively.
We choose NIST2002 (MT02) dataset as our development set,and the NIST 2003 (MT03), 2004 (MT04) 2005(MT05) and 2006 (MT06) datasets as our testsets.
We use the case-insensitive 4-gram NISTBLEU score as our evaluation metric as our eval-uation metric (Papineni et al, 2002).4.2 Experiment settingsHyper parameters In training of the neuralnetworks, we limit the source and target vocab-ularies to the most frequent 30K words in bothChinese and English, covering approximately97.7% and 99.3% of the two corpora respectively.The dimensions of word embedding is 512 andthe size of the hidden layer is 1024.
The dimem-sion of each cell in MB is set to 1024 and thenumber of cells n is set to 8.Training details We initialize the recurrentweight matrices as random orthogonal matrices.All the bias vectors were initialize to zero.
Forother parameters, we initialize them by samplingeach element from the Gaussian distribution ofmean 0 and variance 0.012.
Parameter optimiza-tion is performed using stochastic gradient de-scent.
Adadelta (Zeiler, 2012) is used to auto-matically adapt the learning rate of each param-eter ( = 10?6 and ?
= 0.95).
To avoid gra-dients explosion, the gradients of the cost func-tion which had `2 norm larger than a predefinedthreshold 1.0 was normalized to the threshold(Pascanu et al, 2013).
Each SGD is of a mini-batch of 80 sentences.
We train our NMT modelwith the sentences of length up to 50 words intraining data, while for moses system we use thefull training data.Memory Initialization Each memory cell isinitialized with the source sentence hidden statecomputed asMB(i) = m+ ?i (14)m = ?
(WINIi=Tx?i=0hi)/Tx (15)LDC2004T08 and LDC2005T06.where WINI ?
Rm?2?m; ?
is tanh function.
mmakes a nonlinear transformation of the sourcesentence information.
?i is a random vector sam-pled from N (0, 0.1).Dropout we also use dropout for our NMTbaseline model and MEMDEC to avoid over-fitting (Hinton et al, 2012).
The key idea is torandomly drop units (along with their connec-tions) from the neural network during training.This prevents units from co-adapting too much.In the simplest case, each unit is omitted witha fixed probability p, namely dropout rate.
Inour experiments, dropout was applied only on theoutput layer and the dropout rate is set to 0.5.
Wealso try other strategy such as dropout at wordembeddings or RNN hidden states but fail to getfurther improvements.Pre-training For MEMDEC, the objectivefunction is a highly non-convex function ofthe parameters with more complicated land-scape than that for decoder without exter-nal memory, rendering direct optimization overall the parameters rather difficult.
Inspiredby the effort on easing the training of verydeep architectures (Hinton and Salakhutdi-nov, 2006), we propose a simple pre-trainingstrategyFirst we train a regular attention-basedNMT model without external memory.
Thenwe use the trained NMT model to initializethe parameters of encoder and parameters ofMEMDEC, except those related to memory-state(i.e., {WRa,URa,v,wRg ,WERS,WADD}).
Afterthat, we fine-tune all the parameters of NMTwith MEMDEC decoder, including the parame-ters initialized with pre-training and those associ-ated with accessing memory-state.4.3 Comparison systemsWe compare our method with three state-of-the-art systems:?
Moses: an open source phrase-based trans-lation system 3: with default configurationand a 4-gram language model trained on thetarget portion of training data.3http://www.statmt.org/moses/283SYSTEM MT03 MT04 MT05 MT06 AVE.Groundhog 31.92 34.09 31.56 31.12 32.17RNNsearch?
33.11 37.11 33.04 32.99 34.06RNNsearch?
+ coverage 34.49 38.34 34.91 34.25 35.49MEMDEC 36.16 39.81 35.91 35.98 36.95Moses 31.61 33.48 30.75 30.85 31.67Table 1: Case-insensitive BLEU scores on Chinese-English translation.
Moses is the state-of-the-art phrase-based statisticalmachine translation system.
For RNNsearch, we use the open source system Groundhog as our baseline.
The strongbaseline, denoted RNNsearch?, also adopts feedback attention and dropout.
The coverage model on top of RNNsearch?
hassignificantly improved upon its published version (Tu et al, 2016), which achieves the best published result on this trainingset.
For MEMDEC the number of cells is set to 8.pre-training n MT03 MT04 MT05 MT06 Ave.N 4 35.29 37.36 34.58 33.32 35.11Y 4 35.39 39.16 35.33 35.02 36.22Y 6 35.63 39.29 35.61 34.92 36.58Y 8 36.16 39.81 35.91 35.98 36.95Y 10 36.46 38.86 34.46 35.00 36.19Y 12 35.92 39.09 35.31 35.12 36.37Table 2: MEMDEC performances of different memory size.?
RNNSearch: an attention-based NMTmodel with default settings.
We use the opensource system GroundHog as our NMTbaseline4.?
Coverage model: a state-of-the-art variantof attention-based NMT model (Tu et al,2016) which improves the attention mecha-nism through modelling a soft coverage onthe source representation.4.4 ResultsThe main results of different models are givenin Table 1.
Clearly MEMDEC leads to remark-able improvement over Moses (+5.28 BLEU) andGroundhog (+4.78 BLEU).
The feedback atten-tion gains +1.06 BLEU score on top of Ground-hog on average, while together with dropout addsanother +0.83 BLEU score, which constitute the1.89 BLEU gain of RNNsearch?
over Ground-hog.
Compared to RNNsearch?
MEMDEC is+2.89 BLEU score higher, showing the model-ing power gained from the external memory.
Fi-4https://github.com/lisa-groundhog/GroundHognally, we also compare MEMDEC with the state-of-the-art attention-based NMT with COVERAGEmechanism(Tu et al, 2016), which is about 2BLEU over than the published result after addingfast attention and dropout.
In this comparisonMEMDEC wins with big margin (+1.46 BLEUscore).4.5 Model selectionPre-training plays an important role in optimiz-ing the memory model.
As can be seen in Tab.2,pre-training improves upon our baseline +1.11BLEU score on average, but even without pre-training our model still gains +1.04 BLEU scoreon average.
Our model is rather robust to thememory size: with merely four cells, our modelwill be over 2 BLEU higher than RNNsearch?.This further verifies our conjecture the the exter-nal memory is mostly used to store part of thesource and history of target sentence.4.6 Case studyWe show in Table 5 sample translations fromChinese to English, comparing mainly MEMDEC284src ??????:???(2003?11???)?????,????????????????????????
?ref?All parties that signed the (November 2003 ceasefire) accord should finishthe cantoning of their fighters by January 5, 2004, at the latest,?
Ndayizeyesaid.MEMDEC UNK said, ?
the parties involved in the ceasefire agreement on November2003 will have to be completed by January 5, 2004.
?base ?The signing of the agreement (UNK-fire) agreement in the November2003 ceasefire must be completed by January 5, 2004.src ?????????????,????????????????????????????
?ref Members of the delegation told US Today that the Bush administration hadapproved the US delegation?
s visit to North Korea from January 6 to 10.MEMDEC The delegation told the US today that the Bush administration has approvedthe US delegation?s visit to north Korea from 6 to 10 january .base The delegation told the US that the Bush administration has approved the USto begin his visit to north Korea from 6 to 10 January.Table 3: Sample translations-for each example, we show the source(src), the human translation (ref),the translation fromour memory model MEMDEC and the translation from RNNsearch(equipped with fast attention and dropout).We italicisesome correct translation segments and highlight a few wrong ones in bold.and the RNNsearch model for its pre-training.
Itis appealing to observe that MEMDEC can pro-duce more fluent translation results and bettergrasp the semantic information of the sentence.5 Related WorkThere is a long thread of work aiming to im-prove the ability of RNN in remembering long se-quences, with the long short-term memory RNN(LSTM) (Hochreiter and Schmidhuber, 1997) be-ing the most salient examples and GRU (Cho etal., 2014) being the most recent one.
Those worksfocus on designing the dynamics of the RNNthrough new dynamic operators and appropri-ate gating, while still keeping vector form RNNstates.
MEMDEC, on top of the gated RNN, ex-plicitly adds matrix-form memory equipped withcontent-based addressing to the system, hencegreatly improving the power of the decoder RNNin representing the information important for thetranslation task.MEMDEC is obviously related to the recent ef-fort on attaching an external memory to neuralnetworks, with two most salient examples be-ing Neural Turing Machine (NTM) (Graves etal., 2014) and Memory Network (Weston et al,2014).
In fact MEMDEC can be viewed as aspecial case of NTM, with specifically designedreading (from two different types of memory)and writing mechanism for the translation task.Quite remarkably MEMDEC is among the rareinstances of NTM which significantly improvesupon state-of-the-arts on a real-world NLP taskwith large training corpus.Our work is also related to the recent work onmachine reading (Cheng et al, 2016), in whichthe machine reader is equipped with a memorytape, enabling the model to directly read all theprevious hidden state with an attention mecha-nism.
Different from their work, we use an ex-ternal bounded memory and make an abstractionof previous information.
In (Meng et al, 2015),Meng et.
al.
also proposed a deep architecture forsequence-to-sequence learning with stacked lay-ers of memory to store the intermediate represen-tations, while our external memory was appliedwithin a sequence.6 ConclusionWe propose to enhance the RNN decoder ina neural machine translator (NMT) with exter-nal memory.
Our empirical study on Chinese-English translation shows that it can significantlyimprove the performance of NMT.285References[Bahdanau et al2014] Dzmitry Bahdanau,Kyunghyun Cho, and Yoshua Bengio.
2014.
Neu-ral machine translation by jointly learning to alignand translate.
arXiv preprint arXiv:1409.0473.
[Cheng et al2016] Jianpeng Cheng, Li Dong, andMirella Lapata.
2016.
Long short-term memory-networks for machine reading.
arXiv preprintarXiv:1601.06733.
[Cho et al2014] Kyunghyun Cho, BartVan Merrie?nboer, Caglar Gulcehre, DzmitryBahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phraserepresentations using rnn encoder-decoder forstatistical machine translation.
arXiv preprintarXiv:1406.1078.
[Graves et al2014] Alex Graves, Greg Wayne, and IvoDanihelka.
2014.
Neural turing machines.
arXivpreprint arXiv:1410.5401.
[Hinton and Salakhutdinov2006] Geoffrey E Hintonand Ruslan R Salakhutdinov.
2006.
Reducing thedimensionality of data with neural networks.
Sci-ence, 313(5786):504?507.
[Hinton et al2012] Geoffrey E Hinton, Nitish Srivas-tava, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov.
2012.
Improving neural networksby preventing co-adaptation of feature detectors.
[Hochreiter and Schmidhuber1997] Sepp Hochreiterand Ju?rgen Schmidhuber.
1997.
Long short-termmemory.
Neural computation, 9(8):1735?1780.
[Hu et al2015] Baotian Hu, Zhaopeng Tu, ZhengdongLu, and Hang Li.
2015.
Context-dependent trans-lation selection using convolutional neural net-work.
[Luong et al2015] Minh-Thang Luong, Hieu Pham,and Christopher D Manning.
2015.
Effective ap-proaches to attention-based neural machine trans-lation.
arXiv preprint arXiv:1508.04025.
[Meng et al2015] Fandong Meng, ZhengdongLu, Zhaopeng Tu, Hang Li, and Qun Liu.2015.
A deep memory-based architecture forsequence-to-sequence learning.
arXiv preprintarXiv:1506.06442.
[Papineni et al2002] Kishore Papineni, SalimRoukos, Todd Ward, and Wei-Jing Zhu.
2002.Bleu: a method for automatic evaluation ofmachine translation.
In Proceedings of the 40thannual meeting on association for computa-tional linguistics, pages 311?318.
Association forComputational Linguistics.
[Pascanu et al2013] Razvan Pascanu, Caglar Gul-cehre, Kyunghyun Cho, and Yoshua Bengio.
2013.How to construct deep recurrent neural networks.arXiv preprint arXiv:1312.6026.
[Tu et al2016] Zhaopeng Tu, Zhengdong Lu, YangLiu, Xiaohua Liu, and Hang Li.
2016.
Model-ing coverage for neural machine translation.
ArXiveprints, January.
[Weston et al2014] Jason Weston, Sumit Chopra, andAntoine Bordes.
2014.
Memory networks.
arXivpreprint arXiv:1410.3916.
[Xie et al2011] Jun Xie, Haitao Mi, and Qun Liu.2011.
A novel dependency-to-string model for sta-tistical machine translation.
[Zeiler2012] Matthew D Zeiler.
2012.
Adadelta:an adaptive learning rate method.
arXiv preprintarXiv:1212.5701.286
