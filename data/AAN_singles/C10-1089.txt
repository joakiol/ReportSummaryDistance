Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788?796,Beijing, August 2010Entity-Focused Sentence Simplification for Relation ExtractionMakoto Miwa1 Rune S?tre1 Yusuke Miyao2 Jun?ichi Tsujii1,3,41Department of Computer Science, The University of Tokyo2National Institute of Informatics3School of Computer Science, University of Manchester4National Center for Text Miningmmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jpAbstractRelations between entities in text havebeen widely researched in the natu-ral language processing and information-extraction communities.
The region con-necting a pair of entities (in a parsedsentence) is often used to construct ker-nels or feature vectors that can recognizeand extract interesting relations.
Such re-gions are useful, but they can also incor-porate unnecessary distracting informa-tion.
In this paper, we propose a rule-based method to remove the informationthat is unnecessary for relation extraction.Protein?protein interaction (PPI) is usedas an example relation extraction problem.A dozen simple rules are defined on out-put from a deep parser.
Each rule specif-ically examines the entities in one targetinteraction pair.
These simple rules weretested using several PPI corpora.
The PPIextraction performance was improved onall the PPI corpora.1 IntroductionRelation extraction (RE) is the task of finding arelevant semantic relation between two given tar-get entities in a sentence (Sarawagi, 2008).
Someexample relation types are person?organizationrelations (Doddington et al, 2004), protein?protein interactions (PPI), and disease?gene as-sociations (DGA) (Chun et al, 2006).
Amongthe possible RE tasks, we chose the PPI extrac-tion problem.
PPI extraction is a major RE task;around 10 corpora have been published for train-ing and evaluation of PPI extraction systems.Recently, machine-learning methods, boostedby NLP techniques, have proved to be effec-tive for RE.
These methods are usually intendedto highlight or select the relation-related regionsin parsed sentences using feature vectors or ker-nels.
The shortest paths between a pair of enti-ties (Bunescu and Mooney, 2005) or pair-enclosedtrees (Zhang et al, 2006) are widely used as focusregions.
These regions are useful, but they can in-clude unnecessary sub-paths such as appositions,which cause noisy features.In this paper, we propose a method to removeinformation that is deemed unnecessary for RE.Instead of selecting the whole region betweena target pair, the target sentence is simplifiedinto simpler, pair-related, sentences using general,task-independent, rules.
By addressing particu-larly the target entities, the rules do not affect im-portant relation-related expressions between thetarget entities.
We show how rules of two groupscan be easily defined using the analytical capabil-ity of a deep parser with specific examination ofthe target entities.
Rules of the first group can re-place a sentence with a simpler sentence, still in-cluding the two target entities.
The other group ofrules can replace a large region (phrase) represent-ing one target entity, with just a simple mention ofthat target entity.
With only a dozen simple rules,we show that we can solve several simple well-known problems in RE, and that we can improvethe performance of RE on all corpora in our PPItest-set.7882 Related WorksThe general paths, such as the shortest path orpair-enclosed trees (Section 1), can only covera part of the necessary information for relationextraction.
Recent machine-learning methodsspecifically examine how to extract the missinginformation without adding too much noise.
Tofind more representative regions, some informa-tion from outside the original regions must beincluded.
Several tree kernels have been pro-posed to extract such regions from the parsestructure (Zhang et al, 2006).
Also the graphkernel method emphasizes internal paths with-out ignoring outside information (Airola et al,2008).
Composite kernels have been used to com-bine original information with outside informa-tion (Zhang et al, 2006; Miwa et al, 2009).The approaches described above are useful,but they can include unnecessary information thatdistracts learning.
Jonnalagadda and Gonzalez(2009) applied bioSimplify to relation extraction.BioSimplify is developed to improve their linkgrammar parser by simplifying the target sentencein a general manner, so their method might re-move important information for a given target re-lation.
For example, they might accidentally sim-plify a noun phrase that is needed to extract therelation.
Still, they improved overall PPI extrac-tion recall using such simplifications.To remove unnecessary information from a sen-tence, some works have addressed sentence sim-plification by iteratively removing unnecessaryphrases.
Most of this work is not task-specific;it is intended to compress all information in a tar-get sentence into a few words (Dorr et al, 2003;Vanderwende et al, 2007).
Among them, Vickreyand Koller (2008) applied sentence simplificationto semantic role labeling.
With retaining all argu-ments of a verb, Vickrey simplified the sentenceby removing some information outside of the verband arguments.3 Entity-Focused SentenceSimplificationWe simplify a target sentence using simple rulesapplicable to the output of a deep parser calledMogura (Matsuzaki et al, 2007), to remove noisyinformation for relation extraction.
Our methodrelies on the deep parser; the rules depend on theHead-driven Phrase Structure Grammar (HPSG)used by Mogura, and all the rules are written forthe parser Enju XML output format.
The deepparser can produce deep syntactic and semanticinformation, so we can define generally applica-ble comprehensive rules on HPSG with specificexamination of the entities.For sentence simplification in relation extrac-tion, the meaning of the target sentence itself isless important than maintaining the truth-value ofthe relation (interact or not).
For that purpose,we define rules of two groups: clause-selectionrules and entity-phrase rules.
A clause-selectionrule constructs a simpler sentence (still includ-ing both target entities) by removing noisy infor-mation before and after the relevant clause.
Anentity-phrase rule simplifies an entity-containingregion without changing the truth-value of the re-lation.
By addressing the target entities particu-larly, we can define rules for many applications,and we can simplify target sentences with lessdanger of losing relation-related mentions.
Therules are summarized in Table 1.Our method is different from the sentence sim-plification in other systems (ref.
Section 2).
First,our method relies on the parser, while bioSimplifyby Jonnalagadda and Gonzalez (2009) is devel-oped for the improvement of their parser.
Second,our method tries to keep only the relation-relatedregions, unlike other general systems includingbioSimplify which tried to keep all information ina sentence.
Third, our entity-phrase rules modifyonly the entity-containing phrases, while Vickreyand Koller (2008) tries to remove all informationoutside of the target verb and arguments.3.1 Clause-selection RulesIn compound or complex sentences, it is naturalto assume that one clause includes both the targetentities and the relation-related information.
It canalso be assumed that the remaining sentence parts,outside the clause, contain less related (or noisy)information.
The clause-selection rules simplify asentence by retaining only the clause that includesthe target entities (and by discarding the remain-der of the sentence).
We define three types of789Rule Group Rule Type # Example (original?
simplified )Sentence Clause 1 We show that A interacts with B.?
A interacts with B.Clause Selection Relative Clause 2 ... A that interacts with B.?
A interacts with B.Copula 1 A is a protein that interacts with B.?
A interacts with B.Apposition 2 a protein, A?
AEntity Phrase Exemplification 4 proteins, such as A?
AParentheses 2 a protein (A)?
ACoordination 3 protein and A?
ATable 1: Rules for Sentence Simplification.
(# is the rule count.
A and B are the target entities.
)(a) Sbbbbbbb \\\\\\\... VPbbbbbbb \\\\\\\N*ccccc [[[[[Vcc77(copular) ...bbbbbbb \\\\\\\...
ENTITY ... N* S-RELbbbbbbb \\\\\\\NP-RELNN...ccccc [[[[[...
ENTITY ...A is a protein that interacts with B .
(b) Sbbbbbbb \\\\\\\N*ccccc [[[[[...ccccc [[[[[...
ENTITY ... ...
ENTITY ...A interacts with B .Figure 1: Copula Rule.
(a) is simplified to (b).The arrows represent predicate?argument rela-tions.
(a) N*bbbbbbb \\\\\\\N* ...bbbbbbb ]]]]]]]]]]]]]PNRR55(apposition) N*ccccc [[[[[...
ENTITY ...protein , A(b) N*ccccc [[[[[...
ENTITY ...AFigure 2: Apposition Rule.clause-selection rules for sentence clauses, rela-tive clauses, and copula.
The sentence clause rulefinds the (smallest) clause that includes both tar-get entities.
It then replaces the original sentencewith the clause.
The relative clause rules con-struct a simple sentence from a relative clause andthe antecedent.
If this simple sentence includesthe target entities, it is used instead of the orig-inal sentence.
We define two rules for the casewhere the antecedent is the subject of the relativeclause.
One rule is used when the relative clauseincludes both the target entities.
The other rule isused when the antecedent includes one target en-tity and the relative clause includes the other tar-get entity.
The copula rule is for sentences thatinclude copular verbs (e.g.
be, is, become, etc).The rule constructs a simple sentence from a rel-ative clause with the subject of the copular verbas the antecedent subject of the clause.
The rulereplaces the target sentence with the constructedsentence, if the relative clause includes one targetentity and the subject of a copular verb includesthe other target entity, as shown in Figure 1.3.2 Entity-phrase RulesEven the simple clauses (or paths between twotarget entities) include redundant or noisy expres-sions that can distract relation extraction.
Someof these expressions are related to the target enti-ties, but because they do not affect the truth-valueof the relation, they can be deleted to make thepath simple and clear.
The target problem affectswhich expressions can be removed.
We definefour types of rules for appositions, exemplifica-tions, parentheses, and coordinations.
Two appo-sition rules are defined to select the correct ele-ment from an appositional expression.
One ele-ment modifies or defines the other element in ap-position, but the two elements represent the sameinformation from the viewpoint of PPI.
If the tar-get entity is in one of these elements, removing theother element does not affect the truth-value of theinteraction.
Many of these apposition expressionsare identified by the deep parser.
The rule to se-lect the last element is presented in Figure 2.
Fourexemplification rules are defined for the two ma-jor types of expressions using the phrases ?includ-ing?
or ?such as?.
Exemplification is representedby hyponymy or hypernymy.
As for appositions,the truth-value of the interaction does not changewhether we use the specific mention or the hyper-class that the mention represents.
Two parenthe-ses rules are defined.
Parentheses are useful forsynonyms, hyponyms, or hypernyms (ref.
the two7901: S ?
input sentence2: repeat3: reset rules {apply all the rules again}4: P ?
parse S5: repeat6: r ?
next rule {null if no more rules}7: if r is applicable to P then8: P ?
apply r to P9: S ?
sentence extracted from P10: break (Goto 3)11: end if12: until r is null13: until r is null14: return SFigure 3: Pseudo-code for sentence simplifica-tion.former rules).
Three coordination rules are de-fined.
Removing other phrases from coordinatedexpressions that include a target entity does notaffect the truth-value of the target relation.
Tworules are defined for simple coordination betweentwo phrases (e.g.
select left or right phrase), andone rule is defined to (recursively) remove oneelement from lists of more than two coordinatedphrases (while maintaining the coordinating con-junction, e.g.
?and?
).3.3 Sentence SimplificationTo simplify a sentence, we apply rules repeatedlyuntil no more applications are possible as pre-sented in Figure 3.
After one application of onerule, the simplified sentence is re-parsed beforeattempting to apply all the rules again.
This is be-cause we require a consistent parse tree as a start-ing point for additional applications of the rules,and because a parser can produce more reliableoutput for a partly simplified sentence than for theoriginal sentence.
Using this method, we can alsobacktrack and seek out conversion errors by exam-ining the cascade of partly simplified sentences.4 EvaluationTo elucidate the effect of the sentence simplifi-cation, we applied the rules to five PPI corporaand evaluated the PPI extraction performance.
Wethen analyzed the errors.
The evaluation settingswill be explained in Section 4.1.
The results of thePPI extraction will be explained in Section 4.2.
Fi-nally, the deeper analysis results will be presentedin Section 4.3.4.1 Experimental SettingsThe state-of-the-art PPI extraction systemAkaneRE by Miwa et al (2009) was used toevaluate our approach.
The system uses a com-bination of three feature vectors: bag-of-words(BOW), shortest path (SP), and graph features.Classification models are trained with a supportvector machine (SVM), and AkaneRE (withMogura) is used with default parameter settings.The following two systems are used for a state-of-the-art comparison: AkaneRE with multipleparsers and corpora (Miwa et al, 2009), andAirola et al (2008) single-parser, single-corpussystem.The rules were evaluated on the BioIn-fer (Pyysalo et al, 2007), AIMed (Bunescu et al,2005), IEPA (Ding et al, 2002), HPRD50 (Fun-del et al, 2006), and LLL (Ne?dellec, 2005) cor-pora1.
Table 2 shows the number of positive (in-teracting) vs. all pairs.
One duplicated abstract inthe AIMed corpus was removed.These corpora have several differences in theirdefinition of entities and relations (Pyysalo et al,2008).
In fact, BioInfer and AIMed target al oc-curring entities related to the corpora (proteins,genes, etc).
On the other hand, IEPA, HPRD50,and LLL only use limited named entities, basedeither on a list of entity names or on a named en-tity recognizer.
Only BioInfer is annotated forother event types in addition to PPI, includingstatic relations such as protein family member-ship.
The sentence lengths are also different.
Theduplicated pair-containing sentences contain thefollowing numbers of words on average: 35.8 inBioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 inHPRD50, and 33.4 in LLL.For BioInfer, AIMed, and IEPA, each corpus issplit into training, development, and test datasets2.The training dataset from AIMed was the onlytraining dataset used for validating the rules.
Thedevelopment datasets are used for error analysis.The evaluation was done on the test dataset, withmodels trained using training and development1http://mars.cs.utu.fi/PPICorpora/GraphKernel.html2This split method will be made public later.791BioInfer AIMed IEPA HPRD50 LLLpos all pos all pos all pos all pos alltraining 1,848 7,108 684 4,072 256 630 - - - -development 256 928 102 608 23 51 - - - -test 425 1,618 194 1,095 56 136 - - - -all 2,534 9,653 980 5,775 335 817 163 433 164 330Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.BioInfer AIMed IEPARule Applied F AUC Applied F AUC Applied F AUCNo Application 0 62.5 83.0 0 61.2 87.9 0 73.4 82.5Clause Selection 4,313 63.5 83.9 2,569 62.5 88.2 307 75.0 83.7Entity Phrase 22,066 60.5 80.9 7,784 61.2 86.1 1,031 72.7 83.3ALL 26,281 62.9 82.1 10,783 60.2 85.7 1,343 75.4 85.7Table 3: Performance of PPI Extraction on test datasets.
?Applied?
represents the number of times therules are applied on the corpus.
?No Application?
means PPI extraction without sentence simplification.ALL is the case all rules are used.
The top scores for each corpus are shown in bold.datasets).
Ten-fold cross-validation (CV) wasdone to facilitate comparison with other existingsystems.
For HPRD50 and LLL, there are insuf-ficient examples to split the data, so we use thesecorpora only for comparing the scores and statis-tics.
We split the corpora for the CV, and mea-sured the F -score (%) and area under the receiveroperating characteristic (ROC) curve (AUC) asrecommended in (Airola et al, 2008).
We counteach occurrence as one example because the cor-rect interactions must be extracted for each occur-rence if the same protein name occurs multipletimes in a sentence.In the experiments, the rules are applied in thefollowing order: sentence?clause, exemplifica-tion, apposition, parentheses, coordination, cop-ula, and relative-clause rules.
Furthermore, if thesame rule is applicable in different parts of theparse tree, then the rule is first applied closest tothe leaf-nodes (deepest first).
The order of therules is arbitrary; changing it does not affect theresults much.
We conducted five experiments us-ing the training and development dataset in IEPA,each time with a random shuffling of the order ofthe rules; the results were 77.8?0.26 in F -scoreand 85.9?0.55 in AUC.4.2 Performance of PPI ExtractionThe performance after rule application was bet-ter than the baseline (no application) on all thecorpora, and most rules could be frequently ap-plied.
We show the PPI extraction performance onRule Applied F AUCNo Application 0 72.9 84.5Sentence Clause 145 71.6 83.8Relative Clause 7 73.3 84.1Copula 0 72.9 84.5Clause Selection 152 71.4 83.4Apposition 64 73.2 84.6Exemplification 33 72.9 84.7Parentheses 90 72.9 85.1Coordination 417 73.6 85.4Entity Phrase 605 74.1 86.6ALL 763 75.0 86.6Table 4: Performance of PPI Extraction onHPRD50.Rule Applied F AUCNo Application 0 79.0 84.6Sentence Clause 135 81.3 85.2Relative Clause 42 78.8 84.6Copula 0 79.0 84.6Clause Selection 178 81.0 85.6Apposition 197 79.6 83.9Exemplification 0 79.0 84.6Parentheses 56 79.5 85.8Coordination 322 84.2 89.4Entity Phrase 602 83.8 90.1ALL 761 82.9 90.5Table 5: Performance of PPI Extraction on LLL.BioInfer, AIMed, and IEPA with rules of differentgroups in Table 3.
The effect of using rules ofdifferent types for PPI extraction from HPRD50and LLL is reported in Table 4 and Table 5.
Ta-ble 6 shows the number of times each rule wasapplied in an ?apply all-rules?
experiment.
Theusability of the rules depends on the corpus, anddifferent combinations of rules produce different792Rule B AIMed IEPA H LLLS.
Cl.
3,960 2,346 300 150 135R.
Cl.
287 212 17 5 24Copula 60 57 1 0 0Cl.
Sel.
4,307 2,615 318 155 159Appos.
3,845 1,100 99 69 198Exempl.
383 127 11 33 0Paren.
2,721 2,158 235 91 88Coord.
15,025 4,783 680 415 316E.
Foc.
21,974 8,168 1,025 608 602Sum 26,281 10,783 1,343 763 761Table 6: Distribution of the number of rules ap-plied when all rules are applied.
B:BioInfer, andH:HPRD50 corpora.Rules Miwa et al Airola et alF AUC F AUC F AUCB 60.0 79.8 68.3 86.4 61.3 81.9A 54.9 83.7 65.2 89.3 56.4 84.8I 77.8 88.7 76.6 87.8 75.1 85.1H 75.0 86.6 74.9 87.9 63.4 79.7L 82.9 90.5 86.7 90.8 76.8 83.4Table 7: Comparison with the results by Miwa etal.
(2009) and Airola et al (2008).
The resultswith all rules are reported.results.
For the clause-selection rules, the per-formance was as good as or better than the base-line for all corpora, except for HPRD50, whichindicates that the pair-containing clauses also in-clude most of the important information for PPIextraction.
Clause selection rules alone could im-prove the overall performance for the BioInfer andAIMed corpora.
Entity-phrase rules greatly im-proved the performance on the IEPA, HPRD50,and LLL corpora, although these rules degradedthe performance on the BioInfer and AIMed cor-pora.
These phenomena hold even if we use smallparts of the two corpora, so this is not because ofthe size of the corpora.We compare our results with the results byMiwa et al (2009) and Airola et al (2008) in Ta-ble 7.
On three of five corpora, our method pro-vides better results than the state-of-the-art systemby Airola et al (2008), and also provides com-parable results to those obtained using multipleparsers and corpora (Miwa et al, 2009) despitethe fact that our method uses one parser and onecorpus at a time.
We cannot directly compare ourresult with Jonnalagadda and Gonzalez (2009) be-cause the evaluation scheme, the baseline system,[FP?TN][Sentence, Parenthesis, Coordination] Tocharacterize the AAV functions mediating this effect,cloned AAV type 2 wild-type or mutant genomes weretransfected into simian virus 40 (SV40)-transformedhamster cells together with the six HSV replication genes(encoding UL5, UL8, major DNA-binding protein, DNApolymerase, UL42 , and UL52) which together arenecessary and sufficient for the induction of SV40 DNAamplification (R. Heilbronn and H. zur Hausen, J. Virol.63:3683-3692, 1989).
(BioInfer.d760.s0)[TP?FN][Coordination] Both the GT155-calnexin andthe GT155-CAP-60 interactions were dependent on thepresence of a correctly modified oligosaccharide groupon GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)[TN?TN][Coordination, Parenthesis] Leptin may act asa negative feedback signal to the hypothalamic control ofappetite through suppression of neuropeptide Y (NPY)secretion and stimulation of cocaine and amphetamineregulated transcript (CART) .
(IEPA.d190.s454)Figure 4: A rule-related error, a critical error, anda parser-related error.
Regions removed by therules are underlined, and target proteins are shownin bold.
Predictions, applied rules, and sentenceIDs are shown.
[FN?TP][Sentence, Coordination] WASp contains abinding motif for the Rho GTPase CDC42Hs as well asverprolin / cofilin-like actin-regulatory domains , but nospecific actin structure regulated by CDC42Hs-WASp hasbeen identified.
(BioInfer.d795.s0)[FN?TP][Parenthesis, Apposition] The protein Raf-1 , akey mediator of mitogenesis and differentiation, associateswith p21ras (refs 1-3) .
(AIMed.d124.s1055)[FN?TP][Sentence, Parenthesis] On the basis offar-Western blot and plasmon resonance (BIAcore)experiments, we show here that recombinant bovineprion protein (bPrP) (25-242) strongly interacts with thecatalytic alpha/alpha?
subunits of protein kinase CK2(also termed ?casein kinase 2?)
(IEPA.d197.s479)Figure 5: Correctly simplified cases.
The firstsentence is a difficult (not PPI) relation, which istyped as ?Similar?
in the BioInfer corpus.and test parts differ.4.3 AnalysisWe trained models using the training datasetsand classified the examples in the developmentdatasets.
Two types of analysis were performedbased on these results: simplification-based andclassification-based analysis.For the simplification-based analysis, we com-pared positive (interacting) and negative pair sen-tences that produce the exact same (inconsistent)sentence after protein names normalization and793BioInfer AIMed IEPABefore simplification FN FP TP TN FN FP TP TN FN FP TP TN Not AffectedAfter simplification TP TN FN FP TP TN FN FP TP TN FN FPNo Error 18 2 3 35 14 21 21 8 3 2 0 4 32No Application 3 2 0 3 0 7 8 0 0 1 0 1 7Number of Errors 0 2 0 32 4 2 1 4 0 0 0 0 1Number of Pairs 21 6 3 70 18 30 30 12 3 3 0 5 40Coordination 0 0 0 20 4 2 1 0 0 0 0 0 1Sentence 0 2 0 4 0 0 0 4 0 0 0 0 0Parenthesis 0 0 0 5 0 0 0 0 0 0 0 0 0Exemplification 0 0 0 2 0 0 0 0 0 0 0 0 0Apposition 0 0 0 1 0 0 0 0 0 0 0 0 0Table 8: Distribution of sentence simplification errors compared to unsimplified predictions with theirtypes (on the three development datasets).
TP, True Positive; TN, True Negative; FN, False Negative;FP, False Positive.
?No Error?
means that simplification was correct; ?No Application?
means that norule could be applied; Other rule names mean that an error resulted from that rule application.
?NotAffected?
means that the prediction outcome did not change.simplification in the training dataset.
The numbersof such inconsistent sentences are 7 for BioIn-fer, 78 for AIMed, and 1 for IEPA.
The few in-consistencies in BioInfer and IEPA are from er-rors by the rules, mainly triggered by parse errors.The frequent inconsistencies in AIMed are mostlyfrom inconsistent annotations.
For example, evenif all coordinated proteins are either interacting ornot, only the first protein mention is annotated asinteracting.For the classification-based analysis, wespecifically examine simplified pairs that werepredicted differently before and after the simplifi-cation.
Pairs predicted differently before and afterrule application were selected: 100 random pairsfrom BioInfer and all 90 pairs from AIMed.
ForIEPA, all 51 pairs are reported.
Simplified resultsare classified as errors when the rules affect a re-gion unrelated to the entities in the smallest sen-tence clause.
The results of analysis are shown inTable 8.
There were 34 errors in BioInfer, and 11errors in AIMed.
Among the errors, there werefive critical errors (in two sentences, in AIMed).Critical errors mean that the pairs lost relation-related mentions, and the errors are the only er-rors which caused the changes in the truth-valueof the relation.
There was also a rule-related er-ror (in BioInfer), which means that rules with cor-rect parse results affect a region unrelated to theentities, and parse errors (parser-related errors).Figure 4 shows the rule-related error in BioInfer,one critical error in AIMed, and one parser-relatederror in IEPA.5 DiscussionOur end goal is to provide consistent relationextraction for real tasks.
Here we discuss the?safety?
of applying our simplification rules, thedifficulties in the BioInfer and AIMed corpora, thereduction of errors, and the requirements for sucha general (PPI) extraction system.Our rules are applicable to sentences, with littledanger of changing the relation-related mentions.Figure 5 shows three successfully simplified cases(?No Error?
cases from Table 8).
The sentencesimplification leaves sufficient information to de-termine the value of the relation in these exam-ples.
Relation-related mentions remained for mostof the simplification error cases.
There were onlyfive critical errors, which changed the truth-valueof the relation, out of 46 errors in 241 pairs shownin Table 8.
Please note that some rules can bedangerous for other relation extraction tasks.
Forexample, the sentence clause rule could removemodality information (negation, speculation, etc.
)modifying the clause, but there are few such casesin the PPI corpora (see Table 8).
Also, the task ofhedge detection (Morante and Daelemans, 2009)can be solved separately, in the original sentences,after the interacting pairs have been found.
Forexample, in the BioNLP shared task challengeand the BioInfer corpus, interaction detection andmodality are treated as two different tasks.
Onceother NLP tasks, like static relation (Pyysalo et794al., 2009) or coreference resolution, become goodenough, they can supplement or even substitutesome of the proposed rules.There are different difficulties in the BioInferand AIMed corpora.
BioInfer includes more com-plicated sentences and problems than the othercorpora do, because 1) the apposition, coordi-nation, and exemplification rules are more fre-quently used in the BioInfer corpus than in theother corpora (shown in Table 6), 2) there weremore errors in the BioInfer corpus than in othercorpora among the simplified sentences (shownin Table 8), and 3) BioInfer has more words persentence and more relation types than the othercorpora.
AIMed contains several annotation in-consistencies as explained in Section 4.3.
Theseinconsistencies must be removed to properly eval-uate the effect of our method.Simplification errors are mostly caused byparse errors.
Our rule specifically examines a partof parser output; a probability is attached to thepart.
The probability is useful for defining the or-der of rule applications, and the n-best results bythe parser are useful to fix major errors such as co-ordination errors.
By using these modifications ofrule applications and by continuous improvementin parsing technology for the biomedical domain,the performance on the BioInfer and AIMed cor-pora will be improved also for the all rules case.The PPI extraction system lost the ability tocapture some of the relation-related expressionsleft by the simplification rules.
This indicatesthat the system used to extract some relations (be-fore simplification) by using back-off features likebag-of-words.
The system can reduce bad effectscaused by parse errors, but it also captures the an-notation inconsistencies in AIMed.
Our simpli-fication (without errors) can capture more generalexpressions needed for relation extraction.
To pro-vide consistent PPI relation extraction in a generalsetting (e.g.
for multiple corpora or for other pub-lic text collections), the parse errors must be dealtwith, and a relation extraction system that can cap-ture (only) general relation-related expressions isneeded.6 ConclusionWe proposed a method to simplify sentences, par-ticularly addressing the target entities for relationextraction.
Using a few simple rules applicableto the output of a deep parser called Mogura,we showed that sentence simplification is effec-tive for relation extraction.
Applying all the rulesimproved the performance on three of the fivecorpora, while applying only the clause-selectionrules raised the performance for the remaining twocorpora as well.
We analyzed the simplificationresults, and showed that the simple rules are ap-plicable with little danger of changing the truth-values of the interactions.The main contributions of this paper are: 1) ex-planation of general sentence simplification rulesusing HPSG for relation extraction, 2) presentingevidence that application of the rules improve re-lation extraction performance, and 3) presentationof an error analysis from two viewpoints: simpli-fication and classification results.As future work, we are planning to refine andcomplete the current set of rules, and to coverthe shortcomings of the deep parser.
Using theserules, we can then make better use of the parser?scapabilities.
We will also attempt to apply oursimplification rules to other relation extractionproblems than those of PPI.AcknowledgmentsThis work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT,Japan), Genome Network Project (MEXT, Japan),and Scientific Research (C) (General) (MEXT,Japan).795ReferencesAirola, Antti, Sampo Pyysalo, Jari Bjo?rne, TapioPahikkala, Filip Ginter, and Tapio Salakoski.
2008.A graph kernel for protein-protein interaction ex-traction.
In Proceedings of the BioNLP 2008 work-shop.Bunescu, Razvan C. and Raymond J. Mooney.
2005.A shortest path dependency kernel for relation ex-traction.
In HLT ?05: Proceedings of the confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages724?731.Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-ward M. Marcotte, Raymond J. Mooney, Arun K.Ramani, and Yuk Wah Wong.
2005.
Compara-tive experiments on learning information extractorsfor proteins and their interactions.
Artificial Intelli-gence in Medicine, 33(2):139?155.Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-DongKim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,and Jun?ichi Tsujii.
2006.
Extraction of gene-disease relations from medline using domain dictio-naries and machine learning.
In The Pacific Sympo-sium on Biocomputing (PSB), pages 4?15.Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.2002.
Mining medline: abstracts, sentences, orphrases?
Pacific Symposium on Biocomputing,pages 326?337.Doddington, George, Alexis Mitchell, Mark Przy-bocki, Lance Ramshaw, Stephanie Strassel, andRalph Weischedel.
2004.
The automatic contentextraction (ACE) program: Tasks, data, and evalua-tion.
In Proceedings of LREC?04, pages 837?840.Dorr, Bonnie, David Zajic, and Richard Schwartz.2003.
Hedge trimmer: A parse-and-trim approachto headline generation.
In in Proceedings of Work-shop on Automatic Summarization, pages 1?8.Fundel, Katrin, Robert Ku?ffner, and Ralf Zimmer.2006.
Relex?relation extraction using dependencyparse trees.
Bioinformatics, 23(3):365?371.Jonnalagadda, Siddhartha and Graciela Gonzalez.2009.
Sentence simplification aids protein-proteininteraction extraction.
In Proceedings of the 3rdInternational Symposium on Languages in Biologyand Medicine, pages 109?114, November.Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Efficient HPSG parsing with supertag-ging and cfg-filtering.
In IJCAI?07: Proceedings ofthe 20th international joint conference on Artificalintelligence, pages 1671?1676, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Miwa, Makoto, Rune S?tre, Yusuke Miyao, andJun?ichi Tsujii.
2009.
Protein-protein interac-tion extraction by leveraging multiple kernels andparsers.
International Journal of Medical Informat-ics, June.Morante, Roser and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the BioNLP 2009 Workshop, pages28?36, Boulder, Colorado, June.
Association forComputational Linguistics.Ne?dellec, Claire.
2005.
Learning language in logic -genic interaction extraction challenge.
In Proceed-ings of the LLL?05 Workshop.Pyysalo, Sampo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2007.
BioInfer: A corpus for infor-mation extraction in the biomedical domain.
BMCBioinformatics, 8:50.Pyysalo, Sampo, Antti Airola, Juho Heimonen, JariBjo?rne, Filip Ginter, and Tapio Salakoski.
2008.Comparative analysis of five protein-protein inter-action corpora.
In BMC Bioinformatics, volume9(Suppl 3), page S6.Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, andJun?ichi Tsujii.
2009.
Static relations: a piecein the biomedical information extraction puzzle.In BioNLP ?09: Proceedings of the Workshop onBioNLP, pages 1?9, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Sarawagi, Sunita.
2008.
Information extraction.Foundations and Trends in Databases, 1(3):261?377.Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,and Ani Nenkova.
2007.
Beyond sumbasic: Task-focused summarization with sentence simplifica-tion and lexical expansion.
Inf.
Process.
Manage.,43(6):1606?1618.Vickrey, David and Daphne Koller.
2008.
Sentencesimplification for semantic role labeling.
In Pro-ceedings of ACL-08: HLT, pages 344?352, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In ACL-44: Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 825?832.
Association forComputational Linguistics.796
