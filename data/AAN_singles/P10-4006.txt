Proceedings of the ACL 2010 System Demonstrations, pages 30?35,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsThe S-Space Package: An Open Source Package for Word Space ModelsDavid JurgensUniversity of California, Los Angeles,4732 Boelter HallLos Angeles, CA 90095jurgens@cs.ucla.eduKeith StevensUniversity of California, Los Angeles,4732 Boelter HallLos Angeles, CA 90095kstevens@cs.ucla.eduAbstractWe present the S-Space Package, an opensource framework for developing and eval-uating word space algorithms.
The pack-age implements well-known word spacealgorithms, such as LSA, and provides acomprehensive set of matrix utilities anddata structures for extending new or ex-isting models.
The package also includesword space benchmarks for evaluation.Both algorithms and libraries are designedfor high concurrency and scalability.
Wedemonstrate the efficiency of the referenceimplementations and also provide their re-sults on six benchmarks.1 IntroductionWord similarity is an essential part of understand-ing natural language.
Similarity enables meaning-ful comparisons, entailments, and is a bridge tobuilding and extending rich ontologies for evaluat-ing word semantics.
Word space algorithms havebeen proposed as an automated approach for de-veloping meaningfully comparable semantic rep-resentations based on word distributions in text.Many of the well known algorithms, such asLatent Semantic Analysis (Landauer and Dumais,1997) and Hyperspace Analogue to Language(Burgess and Lund, 1997), have been shown toapproximate human judgements of word similar-ity in addition to providing computational mod-els for other psychological and linguistic phenom-ena.
More recent approaches have extended thisapproach to model phenomena such as child lan-guage acquisition (Baroni et al, 2007) or seman-tic priming (Jones et al, 2006).
In addition, thesemodels have provided insight in fields outside oflinguistics, such as information retrieval, natu-ral language processing and cognitive psychology.For a recent survey of word space approaches andapplications, see (Turney and Pantel, 2010).The parallel development of word space modelsin different fields has often resulted in duplicatedwork.
The pace of development presents a needfor a reliable method for accurate comparisons be-tween new and existing approaches.
Furthermore,given the frequent similarity of approaches, weargue that the research community would greatlybenefit from a common library and evaluation util-ities for word spaces.
Therefore, we introduce theS-Space Package, an open source framework withfour main contributions:1. reference implementations of frequentlycited algorithms2.
a comprehensive, highly concurrent library oftools for building new models3.
an evaluation framework for testing mod-els on standard benchmarks, e.g.
the TOEFLSynonym Test (Landauer et al, 1998)4. a standardized interface for interacting withall word space models, which facilitates wordspace based applications.The package is written in Java and defines astandardized Java interface for word space algo-rithms.
While other word space frameworks ex-ist, e.g.
(Widdows and Ferraro, 2008), the focusof this framework is to ease the development ofnew algorithms and the comparison against exist-ing models.
Compared to existing frameworks,the S-Space Package supports a much wider vari-ety of algorithms and provides significantly morereusable developer utilities for word spaces, suchas tokenizing and filtering, sparse vectors andmatrices, specialized data structures, and seam-less integration with external programs for di-mensionality reduction and clustering.
We hopethat the release of this framework will greatly fa-cilitate other researchers in their efforts to de-velop and validate new word space models.
Thetoolkit is available at http://code.google.com/p/airhead-research/, which includes a wiki30containing detailed information on the algorithms,code documentation and mailing list archives.2 Word Space ModelsWord space models are based on the contextualdistribution in which a word occurs.
This ap-proach has a long history in linguistics, startingwith Firth (1957) and Harris (1968), the latterof whom defined this approach as the Distribu-tional Hypothesis: for two words, their similarityin meaning is predicted by the similarity of thedistributions of their co-occurring words.
Latermodels have expanded the notion of co-occurrencebut retain the premise that distributional similaritycan be used to extract meaningful relationships be-tween words.Word space algorithms consist of the same corealgorithmic steps: word features are extractedfrom a corpus and the distribution of these featuresis used as a basis for semantic similarity.
Figure 1illustrates the shared algorithmic structure of allthe approaches, which is divided into four compo-nents: corpus processing, context selection, fea-ture extraction and global vector space operations.Corpus processing normalizes the input to cre-ate a more uniform set of features on which the al-gorithm can work.
Corpus processing techniquesfrequently include stemming and filtering of stopwords or low-frequency words.
For web-gatheredcorpora, these steps also include removal of nonlinguistic tokens, such as html markup, or restrict-ing documents to a single language.Context selection determines which tokens in adocument may be considered for features.
Com-mon approaches use a lexical distance, syntac-tic relation, or document co-occurrence to definethe context.
The various decisions for selectingthe context accounts for many differences betweenotherwise similar approaches.Feature extraction determines the dimensions ofthe vector space by selecting which tokens in thecontext will count as features.
Features are com-monly word co-occurrences, but more advancedmodels may perform a statistical analysis to se-lect only those features that best distinguish wordmeanings.
Other models approximate the full setof features to enable better scalability.Global vector space operations are applied tothe entire space once the initial word features havebeen computed.
Common operations include al-tering feature weights and dimensionality reduc-Document-Based ModelsLSA (Landauer and Dumais, 1997)ESA (Gabrilovich and Markovitch, 2007)Vector Space Model (Salton et al, 1975)Co-occurrence ModelsHAL (Burgess and Lund, 1997)COALS (Rohde et al, 2009)Approximation ModelsRandom Indexing (Sahlgren et al, 2008)Reflective Random Indexing (Cohen et al, 2009)TRI (Jurgens and Stevens, 2009)BEAGLE (Jones et al, 2006)Incremental Semantic Analysis (Baroni et al, 2007)Word Sense Induction ModelsPurandare and Pedersen (Purandare and Pedersen, 2004)HERMIT (Jurgens and Stevens, 2010)Table 1: Algorithms in the S-Space Packagetion.
These operations are designed to improveword similarity by changing the feature space it-self.3 The S-Space FrameworkThe S-Space framework is designed to be extensi-ble, simple to use, and scalable.
We achieve thesegoals through the use of Java interfaces, reusableword space related data structures, and support formulti-threading.
Each word space algorithm is de-signed to run as a stand alone program and also tobe used as a library class.3.1 Reference AlgorithmsThe package provides reference implementationsfor twelve word space algorithms, which are listedin Table 1.
Each algorithm is implemented in itsown Java package, and all commonalities havebeen factored out into reusable library classes.The algorithms implement the same Java interface,which provides a consistent abstraction of the fourprocessing stages.We divide the algorithms into four categoriesbased on their structural similarity: document-based, co-occurrence, approximation, and WordSense Induction (WSI) models.
Document-basedmodels divide a corpus into discrete documentsand construct the vector space from word fre-quencies in the documents.
The documents aredefined independently of the words that appearin them.
Co-occurrence models build the vectorspace using the distribution of co-occurring wordsin a context, which is typically defined as a re-gion around a word or paths rooted in a parsetree.
The third category of models approximate31Corpus Processing Context Selection Feature Extraction Global OperationsVector SpaceToken FilteringStemmingBigrammingDimensionality ReductionFeature SelectionMatrix TransformsLexical DistanceIn Same DocumentSyntactic LinkWord Co-occurenceJoint ProbabilitiyApproximationCorpusFigure 1: A high-level depiction of common algorithmic steps that convert a corpus into a word spaceco-occurrence data rather than model it explic-itly in order to achieve better scalability for largerdata sets.
WSI models also use co-occurrence butalso attempt to discover distinct word senses whilebuilding the vector space.
For example, these al-gorithms might represent ?earth?
with two vectorsbased on its meanings ?planet?
and ?dirt.
?3.2 Data Structures and UtilitiesThe S-Space Package provides efficient imple-mentations for matrices, vectors, and specializeddata structures such as multi-maps and tries.
Im-plementations are modeled after the java.util li-brary and offer concurrent implementations whenmulti-threading is required.
In addition, the li-braries provide support for converting betweenmultiple matrix formats, enabling interaction withexternal matrix-based programs.
The package alsoprovides support for parsing different corpora for-mats, such as XML or email threads.3.3 Global Operation UtilitiesMany algorithms incorporate dimensionality re-duction to smooth their feature data, e.g.
(Lan-dauer and Dumais, 1997; Rohde et al, 2009),or to improve efficiency, e.g.
(Sahlgren et al,2008; Jones et al, 2006).
The S-Space Pack-age supports two common techniques: the Sin-gular Value Decomposition (SVD) and random-ized projections.
All matrix data structures are de-signed to seamlessly integrate with six SVD im-plementations for maximum portability, includingSVDLIBJ1 , a Java port of SVDLIBC2, a scalablesparse SVD library.
The package also providesa comprehensive library for randomized projec-tions, which project high-dimensional feature datainto a lower dimensional space.
The library sup-ports both integer-based projections (Kanerva etal., 2000) and Gaussian-based (Jones et al, 2006).The package supports common matrix trans-formations that have been applied to wordspaces: point wise mutual information (Dekang,1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/2http://tedlab.mit.edu/?dr/SVDLIBC/1998), term frequency-inverse document fre-quency (Salton and Buckley, 1988), and log en-tropy (Landauer and Dumais, 1997).3.4 MeasurementsThe choice of similarity function for the vectorspace is the least standardized across approaches.Typically the function is empirically chosen basedon a performance benchmark and different func-tions have been shown to provide application spe-cific benefits (Weeds et al, 2004).
To facili-tate exploration of the similarity function param-eter space, the S-Space Package provides sup-port for multiple similarity functions: cosine sim-ilarity, Euclidean distance, KL divergence, Jac-card Index, Pearson product-moment correlation,Spearman?s rank correlation, and Lin Similarity(Dekang, 1998)3.5 ClusteringClustering serves as a tool for building and refin-ing word spaces.
WSI algorithms, e.g.
(Puran-dare and Pedersen, 2004), use clustering to dis-cover the different meanings of a word in a cor-pus.
The S-Space Package provides bindings forusing the CLUTO clustering package3.
In addi-tion, the package provides Java implementationsof Hierarchical Agglomerative Clustering, Spec-tral Clustering (Kannan et al, 2004), and the GapStatistic (Tibshirani et al, 2000).4 BenchmarksWord space benchmarks assess the semantic con-tent of the space through analyzing the geomet-ric properties of the space itself.
Currently usedbenchmarks assess the semantics by inspecting therepresentational similarity of word pairs.
Twotypes of benchmarks are commonly used: wordchoice tests and association tests.
The S-SpacePackage supports six tests, and has an easily ex-tensible model for adding new tests.3http://glaros.dtc.umn.edu/gkhome/views/cluto32Word Choice Word AssociationAlgorithm Corpus TOEFL ESL RDWP R-G WordSim353 DeeseBEAGLE TASA 46.03 35.56 46.99 0.431 0.342 0.235COALS TASA 65.33 60.42 93.02 0.572 0.478 0.388HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042ISA TASA 41.33 18.75 33.72 0.245 0.150 0.286LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206P&P TASA 34.67 20.83 31.39 0.088 -0.036 0.216RI TASA 42.67 27.08 34.88 0.224 0.201 0.211RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200a Landauer et al (1997) report a score of 64.4 for this test, while Rohde et al (2009) report a score of 53.4.b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al, 2008)Table 2: A comparison of the implemented algorithms on common evaluation benchmarks4.1 Word ChoiceWord choice tests provide a target word and a listof options, one of which has the desired relation tothe target.
Word space models solve these tests byselecting the option whose representation is mostsimilar.
Three word choice benchmarks that mea-sure synonymy are supported.The first test is the widely-reported Test of En-glish as a Foreign Language (TOEFL) synonymtest from (Landauer et al, 1998), which consistsof 80 multiple-choice questions with four options.The second test comes from the English as a Sec-ond Language (ESL) exam and consists of 50question with four choices (Turney, 2001).
Thethird consists of 200 questions from the CanadianReader?s Digest Word Power (RDWP) (Jarmaszand Szpakowicz, 2003), which unlike the previ-ous two tests, allows the target and options to bemulti-word phrases.4.2 Word AssociationWord association tests measure the semantic re-latedness of two words by comparing word spacesimilarity with human judgements.
Frequently,these tests measure synonymy; however, othertypes of word relations such as antonymy (?hot?and ?cold?)
or functional relatedness (?doctor?and ?hospital?)
are also possible.
The S-SpacePackage supports three association tests.The first test uses data gathered by Rubensteinand Goodneough (1965).
To measure word simi-larity, word similarity scores of 51 human review-ers were gathered a set of 65 noun pairs, scored ona scale of 0 to 4.
The ratings are then correlatedwith word space similarity scores.Finkelstein et al (2002) test for relatedness.
353word pairs were rated by either 13 or 16 subjectson a 0 to 10 scale for how related the words are.This test is notably more challenging for wordspace models because human ratings are not tiedto a specific semantic relation.The third benchmark considers the antonym as-sociation.
Deese (1964) introduced 39 antonympairs that Greffenstette (1992) used to assesswhether a word space modeled the antonymy rela-tionship.
We quantify this relationship by measur-ing the similarity rank of each word in an antonympair, w1, w2, i.e.
w2 is the kth most-similar wordto w1 in the vector space.
The antonym score iscalculated as 2rankw1 (w2)+rankw2 (w1) .
The scoreranges from [0, 1], where 1 indicates that the mostsimilar neighbors in the space are antonyms.
Wereport the mean score for all 39 antonyms.5 Algorithm AnalysisThe content of a word space is fundamentallydependent upon the corpus used to construct it.Moreover, algorithms which use operations suchas the SVD have a limit to the corpora sizes they330500010000150002000025000100000  200000  300000  400000  500000  60000063.5M 125M 173M 228M 267M 296MSecondsNumber of documentsTokens in Documents (in millions)LSAVSMCOALSBEAGLEHALRIFigure 2: Processing time across different corpussizes for a word space with the 100,000 most fre-quent words01002003004005006007008002 3 4 5 6 7 8PercentageimprovementNumber of threadsRRIBEAGLECOALSLSAHALRIVSMFigure 3: Run time improvement as a factor of in-creasing the number of threadscan process.
We therefore highlight the differ-ences in performance using two corpora.
TASAis a collection of 44,486 topical essays introducedin (Landauer and Dumais, 1997).
The second cor-pus is built from a Nov. 11, 2009 Wikipedia snap-shot, and filtered to contain only articles with morethan 1000 words.
The resulting corpus consists of387,082 documents and 917 million tokens.Table 2 reports the scores of reference algo-rithms on the six benchmarks using cosine simi-larity.
The variation in scoring illustrates that dif-ferent algorithms are more effective at capturingcertain semantic relations.
We note that scores arelikely to change for different parameter configura-tions of the same algorithm, e.g.
token filtering orchanging the number of dimensions.As a second analysis, we report the efficiencyof reference implementations by varying the cor-pus size and number of threads.
Figure 2 reportsthe total amount of time each algorithm needs forprocessing increasingly larger segments of a web-gathered corpus when using 8 threads.
In all cases,only the top 100,000 words were counted as fea-tures.
Figure 3 reports run time improvements dueto multi-threading on the TASA corpus.Algorithm efficiency is determined by three fac-tors: contention on global statistics, contention ondisk I/O, and memory limitations.
Multi-threadingbenefits increase proportionally to the amount ofwork done per context.
Memory limitations ac-count for the largest efficiency constraint, espe-cially as the corpus size and number of featuresgrow.
Several algorithms lack data points forlarger corpora and show a sharp increase in run-ning time in Figure 2, reflecting the point at whichthe models no longer fit into 8GB of memory.6 Future Work and ConclusionWe have described a framework for developingand evaluating word space algorithms.
Many wellknown algorithms are already provided as part ofthe framework as reference implementations forresearches in distributional semantics.
We haveshown that the provided algorithms and librariesscale appropriately.
Last, we motivate further re-search by illustrating the significant performancedifferences of the algorithms on six benchmarks.Future work will be focused on providing sup-port for syntactic features, including dependencyparsing as described by (Pado?
and Lapata, 2007),reference implementations of algorithms that usethis information, non-linear dimensionality reduc-tion techniques, and more advanced clustering al-gorithms.ReferencesMarco Baroni, Alessandro Lenci, and Luca Onnis.2007.
Isa meets lara: A fully incremental wordspace model for cognitively plausible simulations ofsemantic learning.
In Proceedings of the 45th Meet-ing of the Association for Computational Linguis-tics.Curt Burgess and Kevin Lund.
1997.
Modeling pars-ing constraints with high-dimensional context space.Language and Cognitive Processes, 12:177210.Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-dows.
2009.
Reflective random indexing and indi-rect inference: A scalable method for discovery ofimplicit connections.
Journal of Biomedical Infor-matics, 43.J.
Deese.
1964.
The associative structure of some com-mon english adjectives.
Journal of Verbal Learningand Verbal Behavior, 3(5):347?357.34Lin Dekang.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the Joint An-nual Meeting of the Association for ComputationalLinguistics and International Conference on Com-putational Linguistics, pages 768?774.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Z. S.Rivlin, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTransactions of Information Systems, 20(1):116?131.J.
R. Firth, 1957.
A synopsis of linguistic theory 1930-1955.
Oxford: Philological Society.
Reprinted inF.
R. Palmer (Ed.
), (1968).
Selected papers of J. R.Firth 1952-1959, London: Longman.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In IJCAI?07: Pro-ceedings of the 20th international joint conferenceon Artifical intelligence, pages 1606?1611.Gregory Grefenstette.
1992.
Finding semantic similar-ity in raw text: The Deese antonyms.
In Workingnotes of the AAAI Fall Symposium on Probabilis-tic Approaches to Natural Language, pages 61?65.AAAI Press.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sthesaurus and semantic similarity.
In Conference onRecent Advances in Natural Language Processing,pages 212?219.Michael N. Jones, Walter Kintsch, and Doughlas J. K.Mewhort.
2006.
High-dimensional semantic spaceaccounts of priming.
Journal of Memory and Lan-guage, 55:534?552.David Jurgens and Keith Stevens.
2009.
Event detec-tion in blogs using temporal random indexing.
InProceedings of RANLP 2009: Events in EmergingText Types Workshop.David Jurgens and Keith Stevens.
2010.
HERMIT:Flexible Clustering for the SemEval-2 WSI Task.
InProceedings of the 5th International Workshop onSemantic Evaluations (SemEval-2010).
Associationof Computational Linguistics.P.
Kanerva, J. Kristoferson, and A. Holst.
2000.
Ran-dom indexing of text samples for latent semanticanalysis.
In L. R. Gleitman and A. K. Josh, editors,Proceedings of the 22nd Annual Conference of theCognitive Science Society, page 1036.Ravi Kannan, Santosh Vempala, and Adrian Vetta.2004.
On clusterings: Good, bad and spectral.
Jour-nal of the ACM, 51(3):497?515.Thomas K. Landauer and Susan T. Dumais.
1997.
Asolution to Plato?s problem: The Latent SemanticAnalysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104:211?240.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
In-troduction to Latent Semantic Analysis.
DiscourseProcesses, (25):259?284.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-Based Construction of Seman-tic Space Models.
Computational Linguistics,33(2):161?199.Amruta Purandare and Ted Pedersen.
2004.
Wordsense discrimination by clustering contexts in vectorand similarity spaces.
In HLT-NAACL 2004 Work-shop: Eighth Conference on Computational Natu-ral Language Learning (CoNLL-2004), pages 41?48.
Association for Computational Linguistics.Douglas L. T. Rohde, Laura M. Gonnerman, andDavid C. Plaut.
2009.
An improved model ofsemantic similarity based on lexical co-occurrence.Cognitive Science.
sumitted.H.
Rubenstein and J.
B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Communications of theACM, 8:627?633.M.
Sahlgren, A. Holst, and P. Kanerva.
2008.
Permu-tations as a means to encode order in word space.
InProceedings of the 30th Annual Meeting of the Cog-nitive Science Society (CogSci?08).G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
InformationProcessing & Management, 24:513?523.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Communica-tions of the ACM, 18(11):613?620.Robert Tibshirani, Guenther Walther, and TrevorHastie.
2000.
Estimating the number of clusters in adataset via the gap statistic.
Journal Royal StatisticsSociety B, 63:411?423.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter D. Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning (ECML-2001), pages 491?502.Julie Weeds, David Weir, and Diana McCarty.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th Interna-tional Conference on Computational LinguisticsCOLING?04, pages 1015?1021.Dominic Widdows and Kathleen Ferraro.
2008.
Se-mantic vectors: a scalable open source package andonline technology management application.
In Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08).35
