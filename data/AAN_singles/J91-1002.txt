Lexical Cohesion Computed by ThesauralRelations as an Indicator of the Structureof TextJane Morris*York UniversityGraeme HirsttUniversity of TorontoIn text, lexical cohesion is the result of chains of related words that contribute to the continuityof lexical meaning.
These lexical chains are a direct result of units of text being "about thesame thing," and finding text structure involves finding units of text that are about the samething.
Hence, computing the chains is useful, since they will have a correspondence to thestructure of the text.
Determining the structure of text is an essential step in determining thedeep meaning of the text.
In this paper, a thesaurus is used as the major knowledge base forcomputing lexical chains.
Correspondences between lexical chains and structural elements areshown to exist.
Since the lexical chains are computable, and exist in non-domain-specific text,they provide a valuable indicator of text structure.
The lexical chains also provide a semanticcontext for interpreting words, concepts, and sentences.1.
Lexical CohesionA text or discourse is not just a set of sentences, each on some random topic.
Rather,the sentences and phrases of any sensible text will each tend to be about the samethings - -  that is, the text will have a quality of unity.
This is the property of cohesion- -  the sentences "stick together" to function as a whole.
Cohesion is achieved throughback-reference, conjunction, and semantic word relations.
Cohesion is not a guaranteeof unity in text but rather a device for creating it.
As aptly stated by Hall iday andHasan (1976), it is a way of getting text to "hang together as a whole."
Their work oncohesion has underscored its importance as an indicator of text unity.Lexical cohesion is the cohesion that arises from semantic relationships betweenwords.
All that is required is that there be some recognizable relation between thewords.Halliday and Hasan have provided a classification of lexical cohesion based onthe type of dependency relationship that exists between words.
There are five basicclasses:1.
Reiteration with identity of reference:Example 11.
Mary bit into a peach.2.
Unfortunately the peach wasn't ripe.
* Department ofComputer Science, York University, North York, Ontario, Canada M3J 1P3t Department ofComputer Science, University of Toronto, Toronto, Ontario, Canada M5S 1A4(~) 1991 Association for Computational LinguisticsComputational Linguistics Volume 17, Number 12.
Reiteration without identity of reference:Example 21.
Mary ate some peaches.2.
She likes peaches very much.3.
Reiteration by means of superordinate:Example 31.
Mary ate a peach.2.
She likes fruit.4.
Systematic semantic relation (systematically classifiable):Example 41.
Mary likes green apples.2.
She does not like red ones.5.
Nonsystematic semantic relation (not systematically classifiable):Example 51.
Mary spent three hours in the garden yesterday.2.
She was digging potatoes.Examples 1, 2, and 3 fall into the class of reiteration.
Note that reiteration includesnot only identity of reference or repetition of the same word, but also the use ofsuperordinates, subordinates, and synonyms.Examples 4 and 5 fall into the class of collocation, that is, semantic relationshipsbetween words that often co-occur.
They can be further divided into two categoriesof relationship: systematic semantic, and nonsystematic semantic.Systematic semantic relationships can be classified in a fairly straightforward way.This type of relation includes antonyms, members of an ordered set such as {one, two,three}, members of an unordered set such as {white, black, red}, and part-to-whole r -lationships like {eyes, mouth, face}.
Example 5 is an illustration of collocation wherethe word relationship, {garden, digging}, is nonsystematic.
This type of relationshipis the most problematic, especially from a knowledge representation point of view.Such collocation relationships exist between words that tend to occur in similar lexicalenvironments.
Words tend to occur in similar lexical environments because they de-scribe things that tend to occur in similar situations or contexts in the world.
Hence,context-specific examples uch as {post office, service, stamps, pay, leave} are includedin the class.
(This example is from Ventola (1987), who analyzed the patterns of lex-ical cohesion specific to the context of service encounters.)
Another example of thistype is {car, lights, turning}, taken from example 14 in Section 4.2.
These words arerelated in the situation of driving a car, but taken out of that situation, they are notrelated in a systematic way.
Also contained in the class of collocation are word associa-tions.
Examples from Postman and Keppel (1970) are {priest, church}, {citizen, U.S.A.},and {whistle, stop}.
Again, the exact relationship between these words can be hard toclassify, but there does exist a recognizable r lationship.1.1 Lexical ChainsOften, lexical cohesion occurs not simply between pairs of words but over a succes-sion of a number of nearby related words spanning a topical unit of the text.
These22Morris and Hirst Lexical Cohesionsequences of related words will be called lexical chains.
There is a distance relation be-tween each word in the chain, and the words co-occur within a given span.
Lexicalchains do not stop at sentence boundaries.
They can connect a pair of adjacent wordsor range over an entire text.Lexical chains tend to delineate portions of text that have a strong unity of mean-ing.
Consider this example (sentences 31-33 from the long example given in Sec-tion 4.2):Example 6In front of me lay a virgin crescent cut out of pine bush.
A dozen houses were goingup, in various stages of construction, surrounded by hummocks of dry earth andstands of precariously tall trees nude halfway up their trunks.
They were the kind oftrees you might see in the mountains.A lexical chain spanning these three sentences i  {virgin, pine, bush, trees, trunks, trees}.Section 3 will explain how such chains are formed.
Section 4 is an analysis of thecorrespondence b tween lexical chains and the structure of the text.1.2 Why Lexical Cohesion Is ImportantThere are two major reasons why lexical cohesion is important for computational textunderstanding systems:.
Lexical chains provide an easy-to-determine context o aid in theresolution of ambiguity and in the narrowing to a specific meaning of aword.2.
Lexical chains provide a clue for the determination of coherence anddiscourse structure, and hence the larger meaning of the text.1.2.1 Word Interpretation in Context.
Word meanings do not exist in isolation.
Eachword must be interpreted in its context.
For example, in the context {gin, alcohol, sober,drinks}, the meaning of the noun drinks is narrowed own to alcoholic drinks.
In thecontext {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave,not a water wave, a physics wave, or a friendly hand wave.
In these examples, lexicalchains can be used as a contextual id to interpreting word meanings.In earlier work, Hirst (1987) used a system called "Polaroid Words" to providefor intrasentential lexical disambiguation.
Polaroid Words relied on a variety of cues,including syntax, selectional restrictions, case frames, and - -  most relevant here - -a notion of semantic distance or relatedness to other words in the sentences; a sensethat had such a relationship was preferred over one that didn't.
Relationships weredetermined by marker passing along the arcs in a knowledge base.
The intuition wasthat semantically related concepts will be physically close in the knowledge base, andcan thus be found by traversing the arcs for a limited distance.
But Polaroid Wordslooked only for possible relatedness between words in the same sentence; trying to findconnections with all the words in preceding sentences was too complicated and toolikely to be led astray.
The idea of lexical chains, however, can address this weakness inPolaroid Words; lexical chains provide a constrained easy-to-determine representationof context for consideration of semantic distance.1.2.2 Cohesion and Discourse Structure.
The second major importance of lexicalchains is that they provide a clue for the determination of coherence and discoursestructure.23Computational Linguistics Volume 17, Number 1When a chunk of text forms a unit within a discourse, there is a tendency forrelated words to be used.
It follows that if lexical chains can be determined, they willtend to indicate the structure of the text.We will describe the application of lexical cohesion to the determination of thediscourse structure that was proposed by Grosz and Sidner (1986).
Grosz and Sidnerpropose a structure common to all discourse, which could be used along with a struc-turally dependent focus of attention to delineate and constrain referring expressions.In this theory there are three interacting components: linguistic structure, intentionalstructure, and attentional state.Linguistic structure is the segmentation of discourse into groups of sentences,each fulfilling a distinct role in the discourse.
Boundaries of segments can be fuzzy,but some factors aiding in their determination are clue words, changes in intonation(not helpful in written text), and changes in aspect and tense.
When found, thesesegments indicate changes in the topics or ideas being discussed, and hence will havean effect on potential referents.The second major component of the theory is the intentional structure.
It is basedon the idea that people have definite purposes for engaging in discourse.
There isan overall discourse purpose, and also a discourse segment purpose for each of thesegments in the linguistic structure described above.
Each segment purpose specifieshow the segment contributes to the overall discourse purpose.
There are two structuralrelationships between these segments.
The first is called a dominance relation, whichoccurs when the satisfaction (i.e., successful completion) of one segment's intentioncontributes to the satisfaction of another segment's intention.
The second relation iscalled satisfaction precedence, which occurs when the satisfaction of one discourse seg-ment purpose must occur before the satisfaction of another discourse segment purposecan occur.The third component of this theory is the attentional state.
This is a stack-basedmodel of the set of things that attention is focused on at any given point in the dis-course.
It is "parasitic" on the intentional and linguistic structures, since for eachdiscourse segment there exists a separate focus space.
The dominance relations andsatisfaction precedence r lations determine the pushes and pops of this stack space.When a discourse segment purpose contributes to a discourse segment purpose of theimmediately preceding discourse segment, he new focus space is pushed onto thestack.
If the new discourse segment purpose contributes to a discourse segment pur-pose earlier in the discourse, focus spaces are popped off the stack until the discoursesegment that the new one contributes to is on the top of the stack.It is crucial to this theory that the linguistic segments be identified, and as statedby Grosz and Sidner, this is a problem area.
This paper will show that lexical chainsare a good indication of the linguistic segmentation.
When a lexical chain ends, thereis a tendency for a linguistic segment to end, as the lexical chains tend to indicate thetopicality of segments.
If a new lexical chain begins, this is an indication or clue thata new segment has begun.
If an old chain is referred to again (a chain return), it is astrong indication that a previous egment is being returned to.
We will demonstratethis in Section 4.1.3 Cohesion and CoherenceThe theory of coherence relations (Hobbs 1978; Hirst 1981; McKeown 1985) will now beconsidered in relation to cohesion.
There has been some confusion as to the differencesbetween the phenomena of cohesion and coherence, .g., Reichman (1985).
There is adanger of lumping the two together and losing the distinct contributions of each tothe understanding of the unity of text.24Morris and Hirst Lexical CohesionUltimately, the difference between cohesion and coherence is this: cohesion is a termfor sticking together; it means that the text all hangs together.
Coherence is a term formaking sense; it means that there is sense in the text.
Hence the term coherence r lationsrefers to the relations between sentences that contribute to their making sense.Cohesion and coherence relations may be distinguished in the following way.
Acoherence relation is a relation among clauses or sentences, uch as elaboration, sup-port, cause, or exemplification.
There have been various attempts to classify all possiblecoherence relations, but there is as yet no widespread agreement.
There does not exista general computationally feasible mechanism for identifying coherence relations.
Incontrast, cohesion relations are relations among elements in a text: reference, llipsis,substitution, conjunction, and lexical cohesion.Since cohesion is well'defined, one might expect hat it would be computationallyeasier to identify, because the identification of ellipsis, reference, substitution, conjunc-tion, and lexical cohesion is a straightforward task for people.
We will show below thatlexical cohesion is computationally feasible to identify.
In contrast, the identification ofa specific coherence relation from a given set is not a straightforward task, even forpeople.
Consider this example from Hobbs (1978):Example 71.
John can open Bill's safe.2.
He knows the combination.Hobbs identifies the coherence relation as elaboration.
But it could just as easily beexplanation.
This distinction depends on context, knowledge, and beliefs.
For example,if you questioned John's ability to open Bill's safe, you would probably identify therelation as explanation.
Otherwise you could identify it as elaboration.
Here is anotherexample:Example 81.
John bought a raincoat.2.
He went shopping yesterday on Queen Street and it rained.The coherence relation here could be elaboration (on the buying), or explanation (ofwhen, how, or why), or cause (he bought the raincoat because it was raining out).The point is that the identity of coherence relations is "interpretative," whereas theidentity of cohesion relations is not.
At a general evel, even if the precise coherencerelation is not known, the relation "is about the same thing" exists if coherence xists.In the example from Hobbs above, safe and combination are lexically related, which ina general sense means they "are about the same thing in some way."
In example 8,bought and shopping are lexically related, as are raincoat and rained.
This shows howcohesion can be useful in identifying sentences that are coherently related.Cohesion and coherence are independent, in that cohesion can exist in sentencesthat are not related coherently:Example 9Wash and core six apples.
Use them to cut out the material for your new suit.
Theytend to add a lot to the color and texture of clothing.
Actually, maybe you should usefive of them instead of six, since they are quite large.25Computational Linguistics Volume 17, Number 1Similarly, coherence can exist without extual cohesion:Example 10I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and abowl of rice.Of course, most sentences that relate coherently do exhibit cohesion as well.
11.4 The Importance of Both Cohesion and CoherenceHalliday and Hasan (1976) give two examples of lexical cohesion involving identityof reference:Example 111.
Wash and core six cooking apples.2.
Put them into a fireproof dish.Example 121.
Wash and core six cooking apples.2.
Put the apples into a fireproof dish.Reichman (1985, p. 180) writes "It is not the use of a pronoun that gives cohesionto the wash-and-core-apples t xt.
These utterances form a coherent piece of text notbecause the pronoun them is used but because they jointly describe a set of cookinginstructions" (emphasis added).
This is an example of lumping cohesion and coherencetogether as one phenomenon.
Pronominal reference is defined as a type of cohesion(Halliday and Hasan 1976).
Therefore the them in example 11 is an instance of it.
Theimportant point is that both cohesion and coherence are distinct phenomena creatingunity in text.Reichman also writes (1985, p. 1179) "that similar words (apples, them, apples) appearin a given stretch of discourse is an artifact of the content of discussion."
It follows thatif content is related in a stretch of discourse, there will be coherence.
Lexical cohesionis a computationally feasible clue to identifying a coherent stretch of text.
In example12, it is computationally trivial to get the word relationship between apples and apples,and this relation fits the definition of lexical cohesion.
Surely this simple indicatorof coherence is useful, since as stated above, there does not exist a computationallyfeasible method of identifying coherence in non-domain-specific text.
Cohesion is auseful indicator of coherence regardless of whether it is used intentionally by writersto create coherence, or is a result of the coherence of text.Hobbs (1978) sees the resolution of coreference (which is a form of cohesion)as being subsumed by the identification of coherence.
He uses a formal definitionof coherence relations, an extensive knowledge base of assertions and properties ofobjects and actions, and a mechanism that searches this knowledge source and makessimple inferences.
Also, certain elements must be assumed to be coreferential.He shows how, in example (7), an assumption of coherence allows the combinationto be identified as the combination of Bill's safe and John and he to be found to becoreferential.1 There is an interesting analogy between cohesion and syntax, and coherence and semantics.
Jabberwocky(Carroll 1872) is an example of syntax sticking text together without semantics.
Example 10 illustratescoherence sticking text together without cohesion.26Morris and Hirst Lexical CohesionBut lexical cohesion would also indicate that safe and combination can be assumedto be coreferential.
And more importantly, one should not be misled by chicken-and-egg questions when dealing with cohesion and coherence.
Rather, one should useeach where applicable.
Since the lexical cohesion between combination and safe is easyto compute, we argue that it makes sense to use this information as an indicator ofcoherence.2.
The Thesaurus and Lexical CohesionThe thesaurus was conceived by Peter Mark Roget, who described it as being the"converse" of a dictionary.
A dictionary explains the meaning of words, whereas athesaurus aids in finding the words that best express an idea or meaning.
In Section 3,we will show how a thesaurus can be used to find lexical chains in text.2.1 The Structure of the ThesaurusRoget's International Thesaurus, 4th Edition (1977) is composed of 1042 sequentially num-bered basic categories.
There is a hierarchical structure both above and below thislevel (see Figure 1).
Three structure levels are above the category level.
The topmostlevel consists of eight major classes developed by Roget in 1852: abstract relations,space, physics, matter, sensation, intellect, volition, and affections.
Each class is di-vided into (roman-numbered) subclasses, and under each subclass there is a (capital-letter-sequenced) sub-subclass.
These in turn are divided into the basic categories.Where applicable, categories are organized into antonym pairs.
For example, cate-gory 407 is Life, and category 408 is Death.Each category contains a series of numbered paragraphs to group closely relatedwords.
Within each paragraph, still finer groups are marked by semicolons.
In addition,a semicolon group may have cross-references or pointers to other related categoriesor paragraphs.
A paragraph contains words of only one syntactic ategory.
The nounparagraphs are grouped at the start of a category, followed by the paragraphs forClass 1 .
.
.Class 4: MatterI " "III Organic MatterA ...B Vitality407 Life1.
NOUNS life, living, vitality, being alive, having life, animation, ani-mate existence; liveliaess, animal spirits, vivacity, spriteliness; long llfe,longevity; viability; lifetime 110.5; immortality 112.3; birth 167; exis-tence 1; bio-, organ-; -biosis.2 .
.
.
.408 Death .
.
.iiFigure 1The structure of Roget's Thesaurus27Computational Linguistics Volume 17, Number 1Figure 2Index entry for the word lidLidclothing 231.35cover 228.5eyelid 439.9stopper 266.4verbs, adjectives, and so on.The thesaurus has an index, which allows for retrieval of words related to a givenone.
For each entry, a list of words suggesting its various distinct subsenses i given,and a category or paragraph number for each of these.
Figure 2 shows the index entryfor lid.
To find words related to lid in its sense of cover, one would turn to paragraph5 of category 228.
An index entry may be a pointer to a category or paragraph if thereare no subsenses to be distinguished.2.2 Differences from Traditional Knowledge BasesIn the structure of traditional artificial intelligence knowledge bases, such as frames orsemantic networks, words or ideas that are related are actually "physically close" inthe representation.
I  a thesaurus this need not be true.
Physical closeness has someimportance, as can be seen clearly from the hierarchy, but words in the index of thethesaurus often have widely scattered categories, and each category often points to awidely scattered selection of categories.The thesaurus simply groups words by idea.
It does not have to name or classifythe idea or relationship.
In traditional knowledge bases, the relationships must benamed.
For example, in a semantic net, a relationship might be isa or color-of, and ina frame database, there might be a slot for color or location.In Section 1, different ypes of word relationships were discussed: systematic se-mantic, nonsystematic semantic, word association, and words related by a commonsituation.
A factor common to all but situational relationships i  that there is a strongtendency for the word relationships to be captured in the thesaurus.
This holds evenfor the nonsystematic semantic relations, which are the most problematic by defini-tion.
A thesaurus simply groups related words without attempting to explicitly nameeach relationship.
In a traditional computer database, a systematic semantic relation-ship can be represented by a slot value for a frame, or by a named link in a semanticnetwork.
If it is hard to classify a relationship in a systematic semantic way, it will behard to represent the relationship in a traditional frame or semantic network formal-ism.
Of the 16 nonsystematic semantic lexical chains given as examples in Hallidayand Hasan (1976), 14 were found in Roget's Thesaurus (1977) using the relations givenin Section 3.2.2.
This represents an 87% hit rate (but not a big sample space).
Wordassociations show a strong tendency to be findable in a thesaurus.
Of the 16 wordassociation pairs given in Hirst (1987), 14 were found in Roget's Thesaurus (1977).
Sincetwo of the word senses were not contained in the thesaurus at all, this represents a100% hit rate among those that were.
Situational word relationships are not as likelyto be found in a general thesaurus.
An example of a situational relationship isbetweencar and lights, where the two words are clearly related in the situation involving a car'slights, but the relationship will not be found between them in a general thesaurus.28Morris and Hirst Lexical Cohesion3.
Finding Lexical Chains3.1 General MethodologyWe now describe a method of building lexical chains for use as an aid in determiningthe structure of text.
This section details how these lexical chains are formed, using athesaurus as the main knowledge base.
The method is intended to be useful for text inany general domain.
Unlike methods that depend on a full understanding of text, ourmethod is the basis of a computationally feasible approach to determining discoursestructure.We developed our method in the following way.
First, we took five texts, total-ing 183 sentences, from general-interest magazines (Reader's Digest, Equinox, The NewYorker, Toronto, and The Toronto Star).
Using our intuition (i.e., common sense and aknowledge of English), we identified the lexical chains in each text.
We then formal-ized our intuitions into an algorithm, using our experience with the texts to set valuesfor the following parameters (to be discussed below).?
thesaural relations?
transitivity of word relations?
distance (in sentences) allowable between words in a chainThe aim was to find efficient, plausible methods that will cover enough cases to ensurethe production of meaningful results.3.2 Forming Lexical Chains3.2.1 Candidate Words.
The first decision in lexical chain formation is which words inthe text are candidates for inclusion in chains.
As pointed out by Halliday and Hasan(1976), repetitive occurrences of closed-class words such as pronouns, prepositions,and verbal auxiliaries are obviously not considered.
Also, high-frequency words likegood, do, and taking do not normally enter into lexical chains (with some exceptionssuch as takings used in the sense of earnings).
For example, in (13) only the italicizedwords should be considered as lexical chain candidates:Example 13My maternal grandfather lived to be 111.
Zayde was lucid to the end, but a few yearsbefore he died the family assigned me the task of talking to him about his problem withalcohol.It should be noted that morphological nalysis on candidate words was done intu-itively, and would actually have to be formally implemented in an automated system.3.2.2 Building Chains.
Once the candidate words are chosen, the lexical chains can beformed.
For this work an abridged version of Roget's Thesaurus (1977) was used.
Thechains were built by hand.
Automation was not possible, for lack of a machine-readablecopy of the thesaurus.
Given a copy, implementation would clearly be straightforward.It is expected that research with an automated system and a large sample space of textwould give valuable information on the fine-tuning of the parameter settings used inthe general algorithm.Five types of thesaural relations between words were found to be necessary informing chains, but two (the first two below) are by far the most prevalent, constituting29Computational Linguistics Volume 17, Number 1over 90% of the lexical relationships.
The relationships are the following:1.
Two words have a category common in their index entries.
For example,residentialness and apartment both have category 189 in their index entries(see Figure 3.1).2.
One word has a category in its index entry that contains a pointer to acategory of the other word.
For example car has category 273 in its indexentry, and that contains a pointer to category 276, which is a category ofthe word driving (see Figure 3.2).3.
A word is either a label in the other word's index entry (see Figure 3.3b),or is in a category of the other word.
For example, blind has category 442in its index entry, which contains the word see (see Figure 3.3a).4.
Two words are in the same group, and hence are semantically related.For example, blind has category 442, blindness, in its index entry and seehas category 441, vision, in its index entry (see Figure 3.4).5.
The two words have categories in their index entries that both point to acommon category.
For example, brutal has category 851, which in turn(i)(2)word 1 index \]label 1:521 llabel 2:589 Jlabel 3:626 Jword 2 index I /I label 1:860 7 -  I label 2:521word 1 index \]label 2:589label 3: 6 2 6 ~thesaurus category 521thesaurus category 521~r860thesaurus category 860thesaurus category 521word 2word 2 index ~ .
~label 1:300label 2:860(3)(a)word 1 indexlabel 1:521label 2:589label 3:626word 1 indexlabel h 521(b )  word 2:589label 3:626Figure 3Thesaural Relations, parts (1)-(3)30Morris and Hirst Lexical Cohesion(4)(5) thesaurus category 521~ &457thesaurus category 23&457I thesaurus category 457 IFigure 3Continued.
Thesaural Relations, parts (4)-(5)has a pointer to category 830.
Terrified has category 860 that likewise hasa pointer to category 830 (see Figure 3.5).One must  consider how much transitivity to use when computing lexical chains.Specifically, if word a is related to word b, word b is related to word c, and word c isrelated to word d then is word a related to words c and d?Consider this chain: {cow, sheep, wool, scarf, boots, hat, snow}.
If unlimited transitivitywere allowed, then cow and snow would be considered related, which is definitelycounter intuitive.
Our intuition was to allow one transitive link: word a is relatedto word c but not to word d. It seemed that two or more transitive links would soseverely weaken the word relationship as to cause it to be nonintuitive.
Our analysisof our sample texts supported this.
To summarize, a transitivity of one link is sufficientto successfully compute the intuitive chains.
An automated system could be used totest this out extensively, varying the number  of transitive links and calculating theconsequences.
It is likely that it varies slightly with respect o style, author, or type oftext.There are two ways in which a transitive relation involving one link can causetwo words to be related.
In the first way, if word a is related to word b, and word bis related to word c, then word a is related to word c. In the second way, if word a isrelated to word b, and word a is related to word c, then word b is related to word c.But lexical chains are calculated only with respect o the text read so far.
For example,if word c is related to word a and to word b, then word a and word b are not related,since at the time of processing, they were not relatable.
Symmetry was not found tobe necessary for computing the lexical chains.We now consider how many sentences can separate two words in a lexical chainbefore the words should be considered unrelated.
Now, sometimes, several sentencesafter a chain has clearly stopped, it is returned to.
Such chain returns link togetherlarger expanses of text than are contained in single chains or chain segments.
Returns31Computational Linguistics Volume 17, Number 1to existing chains often correspond to intentional boundaries, as they occur after di-gressions or subintentions, thereby signalling a resumption of some structural textentity.Intuitively, the distance between words in a chain is a factor in chain formation.The distance will not be "large," because words in a chain co-relate due to recognizablerelations, and large distances would interfere with the recognition of relations.The five texts were analyzed with respect to distance between clearly relatedwords.
The analysis showed that there can be up to two or three intermediary sen-tences between a word and the preceding element of a chain segment with which itcan be linked.
At distances of four or more intermediary sentences, the word is onlyable to signal a return to an existing chain.
Returns happened after between 4 and19 intermediary sentences in the sample texts.
One significant fact emerged from thisanalysis: returns consisting of one word only were always made with a repetition ofone of the words in the returned-to chain.
Returns consisting of more than one worddid not necessarily use repetition - -  in fact in most cases, the first word in the returnwas not a repetition.The question of chain returns and when they can occur requires further esearch.When distances between relatable words are not tightly bound (as in the case ofreturns), the chances of incorrect chain linkages increase.
It is anticipated that chainreturn analysis would become integrated with other text processing tools in order toprevent this.
Also, we believe that chain strength analysis will be required for thispurpose.
Intuitively, some lexical chains are "stronger" than others, and possibly onlystrong chains can be returned to.
There are three factors contributing to chain strength.1.
Reiteration - -  the more repetitions, the stronger the chain.2.
Density - -  the denser the chain, the stronger it is.3.
Length - -  the longer the chain, the stronger it is.Ideally, some combination of values reflecting these three factors should result in achain strength value that can be useful in determining whether a chain is strongenough to be returned to.
Also, a strong chain should be more likely to have a struc-tural correspondence than a weak one.
It seems likely that chains could contain par-ticularly strong portions with special implications for structure.
These issues will notbe addressed here.3.2.3 Notation and Data Structures.
In the computation of lexical chains, the followinginformation is kept for each word in a chain:A word number, which is a sequential, chain-based number for eachword so that it can be uniquely identified.The sentence number in which the word occurs.The chain created so far.Each lexical relationship in a chain is represented as (u,v)~ where:?
u is the current word number,?
v is the word number of the related word,?
x is the transitive distance:32Morris and Hirst Lexical CohesionChain 1Word Sentence Lexical Chain1.
evade 152. feigning 15 (2~ 1) 23. escaped 16 (3, 1) o /3~ 2)~ 1Figure 4Lexical chain notation- -  0 means no transitive link was used to form the wordrelationship1 means one transitive link was used to form the wordrelationshipy is either- -  the number of the thesaural relationship between the two words(as given in Section 3.2.2)Tq whereT stands for transitively relatedq is the word number through which the transitiverelation is formed.A full example of this notation is shown in Figure 4.Figure 5 shows the generalized algorithm for computing lexical chains.
The pa-rameter values that we used are shown for the following:?
candidate words?
thesaural relations?
transitivity of word relations?
distance between words in a chain.The only parameter not addressed in this work is which (if any) chains should beeliminated from the chain-finding process.3.3 Problems and ConcernsThis section is a discussion of problems encountered during the computation of thelexical chains contained in our corpus of texts.
The text example used in this paper isin Section 4.2, and the chains found in the example are in Appendix A.3.3.1 Where the Thesaurus Failed to Find Lexical Relations.
The algorithm foundwell over 90% of the intuitive lexical relations in the five examples we studied.
Thefollowing is an analysis of when the thesaurus failed to find a relationship and why.One problem was when the relationship between words was due more to their"feel" than their meaning.
For example, in chain 6, the intuitive chain {hand-in-hand,matching, whispering, laughing, warm} was not entirely computable.
Only the italicizedwords were relatable.
The words in chain 6 are cohesive by virtue of being general, butstrong, "good" words related by their goodness, rather than by their specific meanings.Chain 10, {environment, setting, surrounding}, was not thesaurally relatable.
Setting was33Computational Linguistics Volume 17, Number 1REPEATREAD next wordIF word is suitable for lexical analysis (see section 3.2.1) THENCHECK for chains within a suitable span(up to 3 intermediary sentences, and no limitation onreturns):CHECK thesaurus for relationships ( ection 3.2.2).CHECK other knowledge sources(situational, general words, proper names).IF chain relationship is found THENINCLUDE word in chain.CALCULATE chain so far(allow one transitive link).END IFIF there are words that have not formed a chain for a suitablenumber of sentences (up to 3) THENELIMINATE words from the span.END IFCHECK new word for relevance to existing chains thatare suitable for checking.ELIMINATE chains that are not suitable for checking.END IFEND REPEATFigure 5Algorithm for Finding Lexical Chainsnot in the thesaurus, and while it seems as though environment and surrounding shouldbe thesaurally connected, they were not.Place names, street names, and people's names are generally not to be foundin Roget's Thesaurus (1977).
However, they are certainly contained in one's "mentalthesaurus."
Chain 1, which contains everal major Toronto street names, is a goodexample of this.
These names were certainly related to the rest of chain 1 in theauthors' mental thesaurus, ince we are residents of Toronto (and indeed the articleassumed a knowledge of the geography of the city).
In chain 5, the thesaurus did notconnect he words pine and trunk with the rest of the chain {virgin, bush, trees, trees}.In a general thesaurus, pecific information on, and classification of, plants, animals,minerals, etc., is not available.To summarize, there were few cases in which the thesaurus failed to confirm anintuitive lexical chain.
For those cases in which the thesaurus did fail, three missingknowledge sources became apparent.1.
General semantic relations between words of similar "feeling."2.
Situational knowledge.3.
Specific proper names.3.3.2 Problems with Distances and Chain Returns.
Occasionally the algorithm wouldcause two chains to merge together, whereas intuition would lead one to keep them34Morris and Hirst Lexical Cohesionseparate.
We found the following intuitively separate chain beginning in sentence38: {people, Metropolitan Toronto, people, urban, population, people, population, popula-tion, people}.
However, the algorithm linked this chain with chain 1, which runsthrough the entire example and consists of these words and others: {city, suburbs,traffic, community}.
Fortunately, this was a rare occurrence.
But note that there will becases in which lexical chains should be merged as a result of the intentional mergingof ideas or concepts in the text.Conversely, there were a few cases of unfortunate chain returns occurring wherethey were definitely counter intuitive.
In chain 3, word 4, wife, was taken as a one-word return to the chain {married, wife, wife}.
However, there is no intuitive reason forthis.4.
Using Lexical Chains to Determine Text StructureThis section describes how lexical chains formed by the algorithm given in Section3.2.3 can be used as a tool.4.1 Lexical Chains and Text StructureAny structural theory of text must be concerned with identifying units of text that areabout the same thing.
When a unit of text is about the same thing there is a strongtendency for semantically related words to be used within that unit.
By definition,lexical chains are chains of semantically related words.
Therefore it makes sense touse them as clues to the structure of the text.This section will concentrate on analyzing correspondences between lexical chainsand structural units of text, including:?
the correspondence of chain boundaries to structural unit boundaries;?
returns to existing chains and what they indicate about structural units;?
lexical chain strength and reliability of predicting correspondencesbetween chains and structural units;?
an analysis of problems encountered, and when extra textual informationis required to validate the correspondences between lexical chains andstructural components.The text structure theory chosen for this analysis was that of Grosz and Sidner (1986).it was chosen because it is an attempt at a general domain-independent theory oftext structure that has gained a significant acceptance in the field as a good standardapproach.The methodology we used in our analyses was as follows:1.
We determined the lexical chain structure of the text using the algorithmgiven in Section 3.2.3.
(In certain rare cases where the algorithm did notform intuitive lexical chains properly, it is noted, both in Section 3.4 andin the analysis in this section.
The intuitive chain was used for theanalysis; however the lexical chain data given in Appendix A show therare mismatches between intuition and the algorithm.)2.
We determined the intentional structure of the text using the theoryoutlined by Grosz and Sidner.35Computational Linguistics Volume 17, Number 1.
We compared the lexical structure formed in step 1 with the intentionalstructure formed in step 2, and looked for correspondences betweenthem.4.2 An ExampleExample 14 shows one of the five texts that we analyzed.
It is the first section of anarticle in Toronto magazine, December 1987, by Jay Teitel, entitled "Outland.
"2 Thetables in Appendix A show the lexical chains for the text.
(The other four texts andtheir analyses are given in Morris 1988.
)Example 141.
?I spent the first 19 years of my life in the suburbs, the initial 14 or so relativelycontented, the last four or five wanting mainly to be elsewhere.2.
The final two I remember vividly: I passed them driving to and from the Universityof Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness.3.
The car's lights never worked - -  every dusk turned into a kind of medieval raceagainst darkness, a panicky, mounfful rush north, away from everything I knew wasexciting, toward everything I knew was deadly.4.
I remember looking through the windows at the commuters mired in traffic besideme and actively hating them for their passivity.5.
I actually punched holes in the white vinyl ceiling of the Volks and then, by way ofpenance, wrote beside them the names and phone numbers of the girls I would callwhen I had my own apartment in the city.6.
One thing I swore to myself: I would never live in the suburbs again.7.
?My aversion was as much a matter of environment asit was traffic - -  one particularpiece of the suburban setting: the "cruel sun."8.
Growing up in the suburbs you can get used to a surprising number of things - -the relentless "residentialness" of your surroundings, the weird certainty ou havethat everything will stay vaguely new-looking and immune to historic soul no matterhow many years pass.9.
You don't notice the eerie silence that descends each weekday when every sound isdrained out of your neighbourhood along with all the people who've gone to work.10.
I got used to pizza, and cars, and the fact that the cultural hub of my communitywas the collective TV set.11.
But once a week I would step outside as dusk was about to fall and be absolutelybowled over by the setting sun, slanting huge and cold across the untreed front lawns,reminding me not just how barren and sterile, but how undefended life could be.12.
As much as I hated the suburban drive to school, I wanted to get away from thecruel suburban sun.13.
?When I was married a few years later, my attitude hadn't changed.14.
My wife was a city girl herself, and although er reaction to the suburbs was lessintense than mine, we lived in a series of apartments safely straddling Bloor Street.15.
But four years ago, we had a second child, and simultaneously the school my wifetaught at moved to Bathurst Street north of Finch Avenue.2 Q Jay Teitel.
Reprinted with kind permission of the author.36Morris and Hirst Lexical Cohesion16.
She was now driving 45 minutes north to work every morning, along a route thatwas perversely identical to the one I'd driven in college.17.
?We started looking for a house.18.
Our first limit was St. Clair - -  we would go no farther north.19.
When we took a closer look at the price tags in the area though, we conceded thatmaybe we'd have to go to Eglinton - -  but that was definitely it.20.
But the streets whose names had once been magical barriers, latitudes of tolerance,quickly changed to something else as the Sundays passed.21.
Eglinton became Lawrence, which became Wilson, which became Sheppard.22.
One wind-swept day in May I found myself sitting in a town-house developmentnorth of Steeles Avenue called Shakespeare Estates.23.
It wasn't until we stepped outside, and the sun, blazing unopposed over a countryclub, smacked me in the eyes, that I came to.24.
It was the cruel sun.25.
We got into the car and drove back to the Danforth and porches as fast as wecould, grateful to have been reprieved.26.
?And then one Sunday in June I drove north alone.27.
This time I drove up Bathurst past my wife's new school, hit Steeles, and keptgoing, beyond Centre Street and past Highway 7 as well.28.
I passed farms, a man selling lobsters out of his trunk on the shoulder of the road,a chronic care hospital, a country club and what looked like a mosque.29.
I reached a light and turned right.30.
I saw a sign that said Houses and turned right again.31.
?In front of me lay a virgin crescent cut out of pine bush.32.
A dozen houses were going up, in various stages of construction, surrounded byhummocks of dry earth and stands of precariously tall trees nude halfway up theirtrunks.33.
They were the kind of trees you might see in the mountains.34.
A couple was walking hand-in-hand up the dusty dirt roadway, wearing matchingblue track suits.35.
On a "front lawn" beyond them, several ittle girls with hair exactly the samecolour of blond as my daughter's were whispering and laughing together.36.
The air smelled of sawdust and sun.37.
?It was a suburb, but somehow different from any suburb I knew.38.
It felt warm.39.
?It was Casa Drive.40.
?In 1976 there were 2,124,291 people in Metropolitan Toronto, an area bordered bySteeles Avenue to the north, Etobicoke Creek on the west, and the Rouge River to theeast.41.
In 1986, the same area contained 2,192,721 people, an increase of 3 percent, all butnegligible on an urban scale.42.
In the same span of time the three outlying regions stretching across the top ofMetro - -  Peel, Durham, and York - -  increased in population by 55 percent, from814,000 to some 1,262,000.43.
Half a million people had poured into the crescent north of Toronto in the space ofa decade, during which time the population of the City of Toronto actually declinedas did the populations of the "old" suburbs with the exception of Etobicoke andScarborough.44.
If the sprawling agglomeration of people known as Toronto has boomed in thepast 10 years it has boomed outside the traditional city confines in a totally new city,a new suburbia containing one and a quarter million people.37Computational Linguistics Volume 17, Number 14.3 The Correspondences between Lexical and Intentional StructuresIn Figure 6 we show the intentional structure of the text of Section 4.2, and in Figure 7we show the correspondences between the lexical chains and intentions of the example.There is a clear correspondenc.e b tween chain 1, { .
.
.
.
driving, car's .
.
.
.
}, andintention I (changing attitudes to suburban life).
The continuity of the subject matter isreflected by the continuous lexical chain.
From sentence 40 to sentence 44, two words,population and people are used repetitively in the chain.
Population is repeated threetimes, and people is repeated five times.
If chain strength (indicated by the reiteration)were used to delineate "strong" portions of a chain, this strength information couldalso be used to indicate structural attributes of the text.
Specifically, sentences 40 to44 form intention 1.3 (why new suburbs exist), and hence a strong portion of the1 (1-44)Changing attitudes to suburban life.1.1 (1-25)Earlier aversion to suburban life.1.1.1 (1-~)Hatred of commuting.1.1.2 (8-12)The hated suburb environment.1.1.3 (13-25)How this old aversion to suburbs held, when a recent attempt was made tobuy a new house in the ~uburbs.1.1.3.1 (13-16)How life changed, giving author e~son to look for a new house.1.1.3.2 (17-22)Houses are too expensive in Metro Toronto, hence one must look in thesuburbs to buy a house.1.1.3.3 (23-25)The old familiar aversion to suburbs came back.1.2 (26-39)A new suburb that seems livable in and nice.1.2.1 (26-30)The drive to the new suburb.1.2.2 (31-33)The forested area.1.2.3 (34-39)The pleasant environment.1.3 (40-~4)Why the new suburbs exist.Figure 6The Intentional Structure of Example 14 (showing topics the writer intends to discuss)Chain IntentionChain Range Intention Range1 1-44 1 1-442.1 2-12 1.1.1, 1.1.2 1-122.2 16 end of 1.1.3.1 162.3 24 end of 1.1.3.3 253 13-15 1.1.3.1 13-164 19-20 1.1.3.2 17-225 31-33 1.2.2 31-336 34-38 1.2.3 34-397,8 1-3 1.1.1 1-79 7-8 1.1.2 8-12Figure 7Correspondences between lexical and intentional structures38Morris and Hirst Lexical Cohesionchain would correspond exactly to a structural unit.
In addition, drive was repeatedeight times between sentence 2 and sentence 26, corresponding to intention 1.1 (earlieraversion to suburban life).
Suburb was repeated eleven times throughout the entireexample, indicating the continuity in structure between sentences 1-44.Chain 2.1, {afflicted, darkness .
.
.
.
}, from sentence 2 to sentence 12, correspondsto intentions 1.1.1 (hatred of commuting) and 1.1.2 (hatred of suburbs).
More textualinformation is needed to separate intentions 1.1.1 and 1.1.2.
There is a one-word returnto chain 2 at sentences 16 and 24, strongly indicating that chain 2 corresponds tointention 1.1, which runs from sentence 1 to sentence 25.
Also, segment 2.2 coincideswith the end of intention 1.1.3.1 (how life changed), and segment 2.3 coincides withthe end of intention 1.1.3.3 (old familiar aversion to suburbs).
This situation illustrateshow chain returns help indicate the structure of the text.
If chain returns were notconsidered, chain 2 would end at sentence 12, and the structural implications of thetwo single-word returns would be lost.
It is intuitive that the two words perverse andcruel indicate links back to the rest of intention 1.1.
The link provided by the last return,cruel, is especially strong, since it occurs after the diversion describing the attempt tofind a nice house in the suburbs.
Cruel is the third reiteration of the word in chain 2.Chain 3, {married, wife .
.
.
.
}, corresponds to intention 1.1.3.1 (if the unfortunatechain return mentioned in section 3.4.2 is ignored) and chain 4 {conceded, tolerance},corresponds to intention 1.1.3.2 (expensive houses in Metro Toronto).
The boundariesof chain 4 are two sentences inside the boundaries of the intention.
The existence of alexical chain is a clue to the existence of a separate intention, and boundaries withinone or two sentences of the intention boundaries are considered to be close matches.Chain 5, {virgin, pine .
.
.
.
}, corresponds closely to intention 1.2.2 (forested area).Chain 6, {hand-in-hand, matching .
.
.
.
}, corresponds closely to intention 1.2.3 (pleasantenvironment).
Chains 7, {first, initial, final}, and 8, {night, dusk, darkness}, are a couple ofshort chains (three words long) that overlap.
They collectively correspond to intention1.1.1 (hatred of commuting).
The fact that they are short and overlapping suggeststhat they could be taken together as a whole.Chain 9, {environment, setting, surrounding}, corresponds to intention 1.1.2 (hatedsuburbs).
Even though the chain is a lot shorter in length than the intention, its pres-ence is a clue to the existence of a separate intention in its textual vicinity.
Since thelexical chain boundary is more than two sentences away from the intention boundary,other textual information would be required to confirm the structure.Overall, the lexical chains found in this example provide a good clue for thedetermination f the intentional structure.
In some cases, the chains correspond exactlyto an intention.
It should also be stressed, however, that the lexical structures cannotbe used on their own to predict an exact structural partitioning of the text.
This ofcourse was never expected.
As a good example of the limitations of the tool, intention1.2 (nice new suburb) starts in sentence 26, but there are no new lexical chains startingthere.
The only clue to the start of the new intention would be the ending of chain 2{afflicted, darkness .
.
.
.
}.This example also provides a good illustration (chain 2) of the importance of chainreturns being used to indicate a high-level intention spanning the length of the entirechain (including all segments).
Also, the returns coincided with intentional boundaries.5.
Conc lus ionsThe motivation behind this work was that lexical cohesion in text should correspondin some way to the structure of the text.
Since lexical cohesion is a result of a unit oftext being, in some recognizable semantic way, about a single topic, and text structure39Computational Linguistics Volume 17, Number 1analysis involves finding the units of text that are about the same topic, one shouldhave something to say about the other.
This was found to be true.
The lexical chainscomputed by the algorithm given in Section 3.2.3 correspond closely to the intentionalstructure produced from the structural analysis method of Grosz and Sidner (1986).This is important, since Grosz and Sidner give no method for computing the intentionsor linguistic segments that make up the structure that they propose.Hence the concept of lexical cohesion, defined originally by Halliday and Hasan(1976) and expanded in this work, has a definite use in an automated text under-standing system.
Lexical chains are shown to be almost entirely computable withthe relations defined in Section 3.2.2.
The computer implementation f this type ofthesaurus access would be a straightforward task involving traditional database tech-niques.
The program to implement the algorithm given in Section 3.2.3 would alsobe straightforward.
However, automated testing could help fine-tune the parameters,and would help to indicate any unfortunate chain linkages.
Although straightforwardfrom an engineering point of view, the automation would require a significant effort.
Amachine-readable th saurus with automated index searching and lookup is required.The texts we have analyzed, here and elsewhere (Morris 1988) are general-interestarticles taken from magazines.
They were chosen specifically to illustrate that lexicalcohesion, and hence this tool, is not domain-specific.5.1 Improvements  on Earlier ResearchThe methods used in this work improve on those from Halliday and Hasan (1976).Halliday and Hasan related words back to the first word to which they are tied, ratherthan forming explicit lexical chains that include the relationships tointermediate wordsin the chain.
They had no notions of transitivity, distance between words in a chain,or chain returns.
Their intent was not a computational means of finding lexical chains,and they did not suggest a thesaurus for this purpose.Ventola (1987) analyzed lexical cohesion and text structure within the framework ofsystemic linguistics and the specific domain of service ncounters such as the exchangeof words between a client at a post office and a postal worker.
Ventola's chain-buildingrule was that each lexical item is "taken back once to the nearest preceding lexicallycohesive item regardless of distance" (p. 131).
In our work the related words in a chainare seen as indicating structural units of text, and hence distance between wordsis relevant.
Ventola did not have the concept of chain returns, and transitivity wasallowed up to any level.
Her research was specific to the domain used.
She does notdiscuss a computational method of determining the lexical chains.Hahn (1985) developed a text parsing system that considers lexical cohesion.Nouns in the text are mapped irectly to the underlying model of the domain, whichwas implemented as a frame-structured knowledge base.
Hahn viewed lexical cohe-sion as a local phenomenon between words in a sentence and the preceding one.
Therewas also an extended recognizer that worked for cohesion contained within paragraphboundaries.
Recognizing lexical cohesion was a matter of searching for ways of relat-ing frames and slots in the database that are activated by words in the text.
Heavyreliance is put on the "formally clear cut model of the underlying domain" (Hahn 1985,p.
3).
However, general-interest ar icles uch as we analyzed o not have domains thatcan be a priori formally represented as frames with slot values in such a manner thatlexical cohesion will correspond irectly to them.
Our work uses lexical cohesion asit naturally occurs in domain-independent text as an indicator of unity, rather thanfitting a domain model to the lexical cohesion.
Hahn does not use the concept of chainreturns or transitivity.Sedelow and Sedelow (1986, 1987) have done a significant amount of research40Morris and Hirst Lexical Cohesionon the thesaurus as a knowledge source for use in a natural language understandingsystem.
They have been interested in the application of clustering patterns in the the-saurus.
Their student Bryan (1973) proposed a graph-theoretic model of the thesaurus.A boolean matrix is created with words on one axis and categories on the other.
Acell is marked as true if a word associated with a cell intersects with the categoryassociated with a cell.
Paths or chains in this model are formed by traveling alongrows or columns to other true cells.
Semantic "neighborhoods" are grown, consistingof the set of chains emanating from an entry.
It was found that without some conceptof chain strength, the semantic relatedness of these neighborhoods ecays, partly dueto homographs.
Strong links are defined in terms of the degree of overlap betweencategories and words.
A strong link exists where at least two categories contain morethan one word in common, or at least two words contain more than one category incommon.
The use of strong links was found to enable the growth of strong semanticchains with homograph disambiguation.This concept is different from that used in our work.
Here, by virtue of words co-occurring in a text and then also containing at least one category in common or beingin the same category, they are considered lexically related and no further strength isneeded.
We use the thesaurus as a validator of lexical relations that are possible dueto the semantic relations among words in a text.5.2 Further ResearchIt has already been mentioned that the concept of chain strength needs much fur-ther work.
The intuition is that the stronger a chain, the more likely it is to have acorresponding structural component.The integration of this tool with other text understanding tools is an area that willrequire a lot of work.
Lexical chains do not always correspond exactly to intentionalstructure, and when they do not, other textual information is needed to obtain thecorrect correspondences.
In the example given, there were cases where a lexical chaindid correspond to an intention, but the sentences spanned by the lexical chain andthe intention differed by more than two.
In these cases, verification of the possiblecorrespondence must be accomplished through the use of other textual informationsuch as semantics or pragmatics.
Cue words would be interesting toaddress, since suchinformation seems to be more computationally accessible than underlying intentions.It would be useful to automate this tool and run a large corpus of text throughit.
We suspect hat the chain-forming parameter settings (regarding transitivity anddistances between words) will be shown to vary slightly according to author's tyleand the type of text.
As it is impossible to do a complete and error-free lexical analysisof large text examples in a limited time-frame, automation is desirable.
It could helpshed some light on possible unfortunate chain linkages.
Do they become problematic,and if so, when does this tend to happen?
Research into limiting unfortunate linkagesand detecting when the method is likely to produce incorrect results should be done(cf.
Charniak 1986).Analysis using different heories of text structure was not done, but could proveinsightful.
The independence of different people's intuitive chains and structure as-signments was also not addressed by this paper.A practical limitation of this work is that it depends on a thesaurus as its knowl-edge base.
A thesaurus i as good as the work that went into creating it, and alsodepends on the perceptions, experience, and knowledge of its creators.
Since languageis not static, a thesaurus would have to be continually updated to remain current.
Fur-thermore, no one thesaurus exists that meets all needs.
Roget's Thesaurus, for example,is a general thesaurus that does not contain lexical relations pecific to the geography41Computational Linguistics Volume 17, Number 1of Africa or quantum mechanics.
Therefore, further work needs to be done on identi-fying other sources of word knowledge, such as domain-specific thesauri, dictionaries,and statistical word usage information, that should be integrated with this work.
Asan anonymous referee pointed out to us, Volks and Volkswagen were not included inthe chain containing driving and car.
These words were not in a general thesaurus,and were also missed by the authors!Section 1 mentioned that lexical chains would be also useful in providing a con-text for word sense disambiguation and in narrowing to specific word meanings.
Asan example of a chain providing useful information for word sense disambiguation,consider words I to 15 of chain 2.1 of the example: {afflicted, darkness, panicky, mournful,exciting, deadly, hating, aversion, cruel, relentless, weird, eerie, cold, barren, sterile .
.
.
.
}.
Inthe context of all of these words, it is clear that barren and sterile do not refer to aninability to reproduce, but to a cruel coldness.
The use of lexical chains for ambiguityresolution is a promising area for further research.AcknowledgmentsThanks to Robin Cohen, Jerry Hobbs,Eduard Hovy, Ian Lancashire, andanonymous referees for valuable discussionsof the ideas in this paper.
Thanks toChrysanne DiMarco, Mark Ryan, and JohnMorris for commenting on earlier drafts.This work was financially assisted by theGovernment of Ontario, the Department ofComputer Science of the University ofToronto, and the Natural Sciences andEngineering Research Council of Canada.We are grateful to Jay Teitel for allowing usto reprint ext from his article "Outland.
"ReferencesBryan, Robert M. (1973).
"Abstract hesauriand graph theory applications tothesaurus research," in Automatedlanguage analysis, edited by Sally YeatesSedelow, University of Kansas.Carroll, Lewis (1872).
Through the LookingGlass.Charniak, Eugene (1986).
"A neat theory ofmarker parsing."
In Proceedings, 5thNational Conference on Artificial Intelligence,Philadelphia, August 1986, 584-588.Grosz, Barbara nd Sidner, Candance (1986).
"Attention, intentions and the structure ofdiscourse."
Computational Linguistics,12(3), 175-204.Hahn, Udo (1985).
"On lexically distributedtext parsing.
A computational model forthe analysis of textuality on the level oftext cohesion and text coherence."
InLinking in text, edited by Ferenc Kiefer,Universit/it Konstanz.Halliday, Michael and Hasan, Ruqaiya(1976).
Cohesion in English.
LongmanGroup.Hirst, Graeme (1987).
Semantic Interpretationand the Resolution of Ambiguity.
Studies inNatural Language Processing.
CambridgeUniversity Press.Hirst, G. (1981).
Anaphora in NaturalLanguage Understanding: A Survey.
LectureNotes in Computer Science.
SpringerVerlag.Hobbs, Jerry (1978).
"Coherence andcoreference."
Technical note 168, SRIInternational.McKeown, K. (1985).
Text Generation: UsingDiscourse Strategies and Focus Constraints toGenerate Natural Language Text.
Studies inNatural Language Processing.
CambridgeUniversity Press.Morris, Jane (1988).
"Lexical cohesion, thethesaurus, and the structure of text.
"Technical report CSRI-219, Department ofComputer Science, University of Toronto.Postman, Leo and Keppel, Geoffrey, editors(1970).
Norms of Word Association.Academic Press.Reichman, Rachel (1985).
Getting Computersto Talk Like You and Me: Discourse Context,Focus, and Semantics (An ATN Model).
TheMIT Press.Roget, P. (1977).
Roget's InternationalThesaurus, Fourth Edition.
Harper andRow Publishers Inc.Sedelow, Sally and Sedelow, Walter (1987).
"Semantic space."
Computers andtranslation, 2, 235-245.Sedelow, Sally and Sedelow, Walter (1986).
"Thesaural knowledge representation."
InProceedings, 2nd Annual Conference oftheUniversity of Waterloo Centre for the NewOxford English Dictionary: Advances inLexicology.
University of Waterloo.Ventola, E. (1987).
The Structure of SocialInteraction: A Systemic Approach to theSemiotics of Service Encounters.
OpenLinguistic Series.
Frances PinterPublishers.42Morris and Hirst Lexical CohesionAppendix AChain 1Word Sentence Lexical Chain1.
suburbs2.
driving3.
Volkswagen4.
car's5.
lights6.
commuters7.
traffic8.
Volks9.
apartment10.
city11.
suburbs12.
traffic13.
suburban14.
suburbs15.
residentialness16.
neighbourhood17.
community18.
suburban19.
drive20.
suburban21.
city22.
suburbs23.
apartments24.
Bloor St.25.
Bathurst St.26.
Finch St.27.
driving28.
route29.
driven30.
house31.
St. Clair32.
Eglinton122334455567788910121212141414141515161616171819(4, 2)~(7, 2) 2 (7, 4)I(9, 1)~(10, 1)I (10, 2)~ (10, 4)o T2 (10, 7)I (10, 9)I(11, 1) 0 (11, 9-10)I (11, 2-7)1" 1?
(12, 2) 2 (12, 4-10)I (12, 7) 0 (12, 11)~ "1?
(13, 1-11) ?
(13, 9-10) 1 (13, 2-12)( I?
(14, 1-11-13) o (14, 9-10-13)I (14, 2-12)1 :"1?
(15, 1-9-10-13-14)I (15, 2-7-12)1 "1?
(16, 1-11-13-14)I (16, 9-10-13)1" 14(18, 1-11-13-14) 0 (18, 9-10-16) 1 (18, 2-12)1" l?
(19, 2) 0 (19, 7-10-12)I (19, 4) 2 (19, 1-9-11-13-14-15-16-18)~ 1?
(20, 9-10-16) 1 (20, 2-12-19)2 Tl?
(20, 1-11-13-14-18)o o (20, 9-10-16) 1 (20, 2-12-19)1 :"(21, 10) 0 (21, 1-2-7-9-13-14-15-16-19)1Tl?
(21,4-12)1" 19(22, 1-11-13-14-18-20) 0 (22, 9-10-16-21)I (22~ 2-12-19)1" 1?
(23, 9)0 o (23, 1-10-11-13-14-15-16-18-20-21-22)I(23, 2-4-7-12-19)1TM(27, 2-19)0 (27, 7-10-12-21)  (27, 4) 2 (27, 1-9-11-13-14-15-16-18-20-22-23)~ lo(28, 1-2-9-10-11-13-14-15-16-18-19-20-21-22-23-27)(28, 4-7-12)1T27(29, 2-19-27-29) o (29, 7-10-12-21)  (29, 4-28) 2 (29,1-9-11-13-14-15-16-18-20-22-23)1 :"1?
(30, 1-9-10-11-13-14-15-16-18-20-21-22-23) 1 ( 0, 2-4-7-12-19-27-28-29)5 l?43Computational Linguistics Volume 17, Number 1Chain 1 (continued)Word ' Sentence Lexical Chain33.
streets34.
Eglinton35.
Lawrence36.
Wilson37.
Sheppard38.
town-house39.
Steeles40.
car41.
drove42.
Danforth43.
porches44.
drove45.
drove46.
Bathurst47.
Steeles48.
Centre St.49.
Highway 750. trunk51.
road52.
light53.
turned54.
houses55.
turned56.
houses57.
roadway58.
lawn59.
suburb2O2121212122222525252526272727272728282929303032343537(33, 1-10-13-14-15-16-18-20-21-22-23-30)I ( 3, 2-4-7-12-19-27-28-29) ~1o(38, 30) o (38, 1-10-13-14-15-16-18-20-21-22-23)I(38, 2-4-7-12-19-27-28-29-33) ~ lo(40, 2-19-27-29)I (40, 4-7-10-12-21-28) T29(41, 2-19-27-29) o (41, 7-10-12-21)  (41, 4-28) 2 (41,1-9-11-13-14-15-16-18-20-22-30-38) 1Tlo(43, 33)I (43, 1-4-10-13-14-15-18-20-21-22-23-30-38-40) 2 (43, 16)1 T38 (43, 2-19-23-29)~ 4?
(44, 2-19-27-29-41){I (44, 7-10-12-21)I (44, 4-28) 2(44, 1-9-11-13-14-15-16-18-20-22-23-30-38)~ lo(45, 2-19-27-29-41-44) o (45, 7-10-12-21)I (45,4-28) 2 (45, 1-9-11-13-14-15-16-18-20-22-23-30-38)1T10(51, 1-9-10-11-13-14-15-16-18-20-21-22-23-28-30-38)~ (51, 43)02 (51, 7)~ 1?
(51, 16) T38(52, 5) ?
(54, 30-38) 0 (54, 1-9-10-11-13-14-15-18-20-21-22-23-33-43-52)I (54, 16-28)22 (54, 2-7-12-19-29-41-44)~ "1?
(55, 53) o(56, 30-38-54) o (56, 1-9-10-11-13-14-15-18-20-21-22-23-33-43-51)I (56, 16-28) 2 (56, 2-7-12-19-29-41-44)~ 1?
(57, 51) 0 (57, 1-9-10-11-13-14-15-16-18-20-21-22-23-28-30-38)I (57, 43) 2 (57, 7)~ 1?
(57, 16)~ 38(58, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-43-51-54-56-57) 1 (58, 28)o 5 (58, 2-12-19-27-29-41-44)1 :"1?
(58, 16)1 T56(59, 1-11-13-14-18-20-22) 0 (59, 30-38-56)I (59, 9-10-15-21-23-33-43-51)I (59, 16-28)o 2 (59, 2-7-12-19-29-41-44)1 " ?44Morris and Hirst Lexical CohesionChain 1 (continued)Word Sentence60.
suburb61.
people62.
MetropolitanToronto63.
Steeles64.
people65.
urban66.
Metro67.
Peel68.
Durham69.
York70.
population71.
people72.
Toronto73.
population74.
city75.
Toronto76.
population77.
suburbs78.
Etobicoke37404040414142424242424343434343434343Lexical Chain(60, 1-11-13-14-18-20-22-59)0O (60,30-38-56)I (60,9-10-15-21-23-33-43-51-54-56-57-59)I (60, 16-28)02(60, 2-7-12-19-29-41-4446-47) / l?
(61, 15)I (61, 1-9-10-11-13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60)02 (61,2-7-12-19-27-29-41-44)~ 1?
(61, 16-43-58) TM(62, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51-54-56-57-59-60)I (62, 2-7-12-19-27-29-41-44)/1?
(62, 16-43-58)02(64, 61) 0 (64, 15)I (64, 1-9-10-11-13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60-62)02 ( 5, 2-7-12-19-27-29-41-44)\[ 1?
(61, 16-43-58)/56(65, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51-54-56-57-59-60-62) 1 (65, 2-7-12-19-27-29-41-44)\[ 1?
(65, 16-43-58)02(66, 62)0 o (66, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51-54-56-57-59-60)1 (66,2-7-12-19-27-29-41-44)~ 1?
(66, 16-43-58-64)02(70, 30-38-54-56-61-64)I (70, 1-9-10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-65-66)02 (70, 43-58) 5 (70, 2-7-12-19-27-29-41-44)/1?
(70, 16)/64(71, 61-64) 0 (71, 15-70)I (71, 1-9-10-11-13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60-62-65-66)02(71, 2-7-12-19-27-29-41-44)/1?
(71, 16-43-58-64) TM(73, 70)o0 (73, 30-38-51-54-56-61-65-71)I (73, 1-9-10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-65-66) 2 (73, 43-58)0 s (73, 2-7-12-19-27-29-41-44)\[ l?
(73, 16)/64(74, 10-21) o (74, 1-2-7-9-11-12-13-14-15-18-19-20-22-23-27-29-30-33-38-41-44-51-54-56-57-59-60-62-65)I (74, 16-28-43-58-65-70-71-73)02 (74, 4-40)/47(76, 70-73)o 0 (76, 30-38-54-56-61-64-71) 1 (76, 1-9-10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-65-66-74) 2 (76, 43-58) 5 (76, 2-7-12-19-27-29-41-44)/1?
(76, 16)1T64(77, 1-11-13-14-18-20-22-59-60) o (77, 30-38-56-62-65-66-74)~ (77, 9-10-15-21-23-33-43-51) 1 (77, 16-28-64-70-71-72-73-76) 2 (77, 2-7-12-19-29-41-44-)1Tl?45Computational Linguistics Volume 17, Number 1Chain 1 (continued)Word Sentence Lexical Chain79.
Scarborough80.
people81.
Toronto82.
city83.
suburbia84.
people434444444444(80, 61-64-71) o (80, 15-70)I (80, 1-9-10-11-13-14-18-20~21-22-23-30-33-38-51-54-56-57-59-60-62-65-66-73~76-77)o 2 (80, 2-7-12-19-27-29-41-44) Tl?
(80,16-43-58) TM(82, 10-21-74) o (82, 1-2-7-9-11-12-13-14-15-18-19-20-22-23-27-29-30-33-38-41-44-46-47-51-54-56-57-59-60462-65-77)I (82, 16-28-43-58-64-70-71-73-76-80)02 (82, 4-40)1 T47(83, lq1-13-14-18-20-22-59-60-77) ?
(83, 30-38-56-82)~ (83, 9-10-15-21-23-33-43-51-82)I (83, 16-28-80) 2 (83, 2-7-12-19-29-41-44)1Tl?
(84, 61-64-71-80)  (84, 15-70-82)I (84, 1-9-10-11-13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60-62-65-66-73-76-77-82) 2 (84, 2-7-12-19-27-29-41-44) Tl?
(84, 16-43-58) TMChain 2, Segment 1Word Sentence Lexical Chain1.
afflicted2.
darkness3.
panicky4.
mournful5.
exciting6.
deadly7.
hating8.
aversion9.
cruel10.
relentless11.
weird12.
eerie13.
cold14.
barren15.
sterile16.
hated17.
cruel2333334778891111111212(2, 1) 2(3, 1)o 2 (3, 2) 5(4, 1)I (4, 2)I (4, 3)02(5, 1-4)o 2 (5, 2-3)5(6, 1-4) 2 (6, 2-3-5)o 5(7, 1-4)I (7, 2-3-5-6) 2(8, 7)I (8, 1-4) 2 (8, 2-3-5-6)5(9, 1-4-7)I (9, 2-3-5-6-8) 2(10, 9)I (10, 1-4-7)o 2 (10, 2-3-5-6-8)05(11, 3)I (11, 1-4-7-10)2o (11, 2-3-5-6-8) 5(12, 3-11)I (12, 1-4-7-10) 2 (12, 2-3-5-6-8)5(13, 3-6-7-8-11-12)I (13, 1-4-9) 2 (13, 2-3-5-6-10)5o(14, 6-7)2 (14, 1-2-3-4-5-8-9-10-11-12-13)~ 7(15, 14) 1 (15, 6-7)2 (15, 1-2-3-4-5-8-9-10-11-12-13) T7(16, 7) o (16, 1-4-6-8-9-13)I (16, 14-15)o 2 (16, 2-3-5-10-11-12)o 5(17, 9) 0 (17, 1-4-7-10)I (17, 2-3-5-6-8-11-12-13)O5(17, 14-15) T746Morris and Hirst Lexical CohesionChain 2, Segment 2Word18.
perverselySentence Lexical Chain16 (18, 10)2 (18, 1-2-3-4-5-6-7-8-9-11-12-13-16-17)~ '1?Chain 2, Segment 3Word Sentence Lexical Chain19.
cruel 24 (19, 9-17)~ (19, 1-4-7-10)~ (19, 2-3-5-6-8-11-12-13)05 (19, 14-15)~ 7Chain 3Word Sentence Lexical Chain1.
married2.
wife3.
wife4.
wife13141527(2, 1)~(3, 1)~ (3, 2)~(4, 2-3)~ (4, 1)~Chain 4Word Sentence Lexical Chain1.
conceded 192. tolerance 20 (2, 1)~Chain 5Word Sentence Lexical Chain1.
virgin2.
pine3.
bush4.
trees5.
trunks6.
trees313131323233(3, 1)~(4, 1)~ (4, 3)~(6, 4)~ (6, 1-3)~Word Sentence1.
hand-in-hand2.
matching3.
whispering4.
laughing5.
warm3434353538Chain 6Lexical Chain(5, 1) 1 (5, 4) 5Chain 7Word Sentence Lexical Chain1.
first2.
initial3.
final(2, 1) 1(3, 2-1) 347Computational Linguistics Volume 17, Number 1Chain 8Word Sentence Lexical Chain1.
night 22. dusk 3 (12, 1)023. darkness 3 (3, 1-2)~Chain 9Word Sentence Lexical Chain1.
environment 72. setting 73. surrounding 848
