Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 877?886,Honolulu, October 2008. c?2008 Association for Computational LinguisticsTwo Languages are Better than One (for Syntactic Parsing)David Burkett and Dan KleinComputer Science DivisionUniversity of California, Berkeley{dburkett,klein}@cs.berkeley.eduAbstractWe show that jointly parsing a bitext can sub-stantially improve parse quality on both sides.In a maximum entropy bitext parsing model,we define a distribution over source trees, tar-get trees, and node-to-node alignments be-tween them.
Features include monolingualparse scores and various measures of syntac-tic divergence.
Using the translated portionof the Chinese treebank, our model is trainediteratively to maximize the marginal likeli-hood of training tree pairs, with alignmentstreated as latent variables.
The resulting bi-text parser outperforms state-of-the-art mono-lingual parser baselines by 2.5 F1 at predictingEnglish side trees and 1.8 F1 at predicting Chi-nese side trees (the highest published numberson these corpora).
Moreover, these improvedtrees yield a 2.4 BLEU increase when used ina downstream MT evaluation.1 IntroductionMethods for machine translation (MT) have increas-ingly leveraged not only the formal machinery ofsyntax (Wu, 1997; Chiang, 2007; Zhang et al,2008), but also linguistic tree structures of either thesource side (Huang et al, 2006; Marton and Resnik,2008; Quirk et al, 2005), the target side (Yamadaand Knight, 2001; Galley et al, 2004; Zollmann etal., 2006; Shen et al, 2008), or both (Och et al,2003; Aue et al, 2004; Ding and Palmer, 2005).These methods all rely on automatic parsing of oneor both sides of input bitexts and are therefore im-pacted by parser quality.
Unfortunately, parsing gen-eral bitexts well can be a challenge for newswire-trained treebank parsers for many reasons, includingout-of-domain input and tokenization issues.On the other hand, the presence of translationpairs offers a new source of information: bilin-gual constraints.
For example, Figure 1 shows acase where a state-of-the-art English parser (Petrovand Klein, 2007) has chosen an incorrect structurewhich is incompatible with the (correctly chosen)output of a comparable Chinese parser.
Smith andSmith (2004) previously showed that such bilin-gual constraints can be leveraged to transfer parsequality from a resource-rich language to a resource-impoverished one.
In this paper, we show that bilin-gual constraints and reinforcement can be leveragedto substantially improve parses on both sides of abitext, even for two resource-rich languages.Formally, we present a log-linear model overtriples of source trees, target trees, and node-to-node tree alignments between them.
We considera set of core features which capture the scores ofmonolingual parsers as well as measures of syntacticalignment.
Our model conditions on the input sen-tence pair and so features can and do reference inputcharacteristics such as posterior distributions from aword-level aligner (Liang et al, 2006; DeNero andKlein, 2007).Our training data is the translated section of theChinese treebank (Xue et al, 2002; Bies et al,2007), so at training time correct trees are observedon both the source and target side.
Gold tree align-ments are not present and so are induced as latentvariables using an iterative training procedure.
Tomake the process efficient and modular to existingmonolingual parsers, we introduce several approxi-mations: use of k-best lists in candidate generation,an adaptive bound to avoid considering all k2 com-binations, and Viterbi approximations to alignmentposteriors.877Figure 1: Two possible parse pairs for a Chinese-English sentence pair.
The parses in a) are chosen by independentmonolingual statistical parsers, but only the Chinese side is correct.
The gold English parse shown in b) is further downin the 100-best list, despite being more consistent with the gold Chinese parse.
The circles show where the two parsesdiffer.
Note that in b), the ADVP and PP nodes correspond nicely to Chinese tree nodes, whereas the correspondencefor nodes in a), particularly the SBAR node, is less clear.We evaluate our system primarily as a parser andsecondarily as a component in a machine translationpipeline.
For both English and Chinese, we beginwith the state-of-the-art parsers presented in Petrovand Klein (2007) as a baseline.
Joint parse selectionimproves the English trees by 2.5 F1 and the Chi-nese trees by 1.8 F1.
While other Chinese treebankparsers do not have access to English side transla-tions, this Chinese figure does outperform all pub-lished monolingual Chinese treebank results on anequivalent split of the data.As MT motivates this work, another valuableevaluation is the effect of joint selection on down-stream MT quality.
In an experiment using asyntactic MT system, we find that rules extractedfrom joint parses results in an increase of 2.4BLEU points over rules extracted from independentparses.1 In sum, jointly parsing bitexts improvesparses substantially, and does so in a way that thatcarries all the way through the MT pipeline.2 ModelIn our model, we consider pairs of sentences (s, s?
),where we use the convention that unprimed vari-ables are source domain and primed variables aretarget domain.
These sentences have parse trees t(respectively t?)
taken from candidate sets T (T ?
).1It is anticipated that in some applications, such as tree trans-ducer extraction, the alignments themselves may be of value,but in the present work they are not evaluated.Non-terminal nodes in trees will be denoted by n(n?)
and we abuse notation by equating trees withtheir node sets.
Alignments a are simply at-most-one-to-one matchings between a pair of trees t andt?
(see Figure 2a for an example).
Note that we willalso mention word alignments in feature definitions;a and the unqualified term alignment will always re-fer to node alignments.
Words in a sentence are de-noted by v (v?
).Our model is a general log-linear (maximum en-tropy) distribution over triples (t, a, t?)
for sentencepairs (s, s?
):P(t, a, t|s, s?)
?
exp(w>?
(t, a, t?
))Features are thus defined over (t, a, t?)
triples; wediscuss specific features below.3 FeaturesTo use our model, we need features of a triple(t, a, t?)
which encode both the monolingual qualityof the trees as well as the quality of the alignmentbetween them.
We introduce a variety of features inthe next sections.3.1 Monolingual FeaturesTo capture basic monolingual parse quality, we be-gin with a single source and a single target featurewhose values are the log likelihood of the sourcetree t and the target tree t?, respectively, as given878by our baseline monolingual parsers.
These two fea-tures are called SOURCELL and TARGETLL respec-tively.
It is certainly possible to augment these sim-ple features with what would amount to monolin-gual reranking features, but we do not explore thatoption here.
Note that with only these two features,little can be learned: all positive weightsw cause thejointly optimal parse pair (t, t?)
to comprise the twotop-1 monolingual outputs (the baseline).3.2 Word Alignment FeaturesAll other features in our model reference the entiretriple (t, a, t?).
In this work, such features are de-fined over aligned node pairs for efficiency, but gen-eralizations are certainly possible.Bias: The first feature is simply a bias featurewhich has value 1 on each aligned node pair (n, n?
).This bias allows the model to learn a general prefer-ence for denser alignments.Alignment features: Of course, some alignmentsare better than others.
One indicator of a good node-to-node alignment between n and n?
is that a goodword alignment model thinks that there are manyword-to-word alignments in their bispan.
Similarly,there should be few alignments that violate that bis-pan.
To compute such features, we define a(v, v?
)to be the posterior probability assigned to the wordalignment between v and v?
by an independent wordaligner.2Before defining alignment features, we need todefine some additional variables.
For any node n ?
t(n?
?
t?
), the inside span i(n) (i(n?))
comprisesthe input tokens of s (s?)
dominated by that node.Similarly, the complement, the outside span, will bedenoted o(n) (o(n?
)), and comprises the tokens notdominated by that node.
See Figure 2b,c for exam-ples of the resulting regions.INSIDEBOTH =?v?i(n)?v??i(n?
)a(v, v?
)INSRCOUTTRG =?v?i(n)?v??o(n?
)a(v, v?
)INTRGOUTSRC =?v?o(n)?v??i(n?
)a(v, v?
)2It is of course possible to learn good alignments using lexi-cal indicator functions or other direct techniques, but given ourvery limited training data, it is advantageous to leverage countsfrom an unsupervised alignment system.Hard alignment features: We also define thehard versions of these features, which take countsfrom the word aligner?s hard top-1 alignment output?
:HARDINSIDEBOTH =?v?i(n)?v??i(n?)?
(v, v?
)HARDINSRCOUTTRG =?v?i(n)?v??o(n?)?
(v, v?
)HARDINTRGOUTSRC =?v?o(n)?v??i(n?)?
(v, v?
)Scaled alignment features: Finally, undesirablelarger bispans can be relatively sparse at the wordalignment level, yet still contain many good wordalignments simply by virtue of being large.
Wetherefore define a scaled count which measures den-sity rather than totals.
The geometric mean of spanlengths was a superior measure of bispan ?area?
thanthe true area because word-level alignments tend tobe broadly one-to-one in our word alignment model.SCALEDINSIDEBOTH =INSIDEBOTH?|i(n)| ?
|i(n?
)|SCALEDINSRCOUTTRG =INSRCOUTTRG?|i(n)| ?
|o(n?
)|SCALEDINTRGOUTSRC =INTRGOUTSRC?|o(n)| ?
|i(n?
)|Head word alignment features: When consider-ing a node pair (n, n?
), especially one which dom-inates a large area, the above measures treat allspanned words as equally important.
However, lex-ical heads are generally more representative thanother spanned words.
Let h select the headword ofa node according to standard head percolation rules(Collins, 2003; Bikel and Chiang, 2000).ALIGNHEADWORD = a(h(n), h(n?
))HARDALIGNHEADWORD = ?
(h(n), h(n?
))3.3 Tree Structure FeaturesWe also consider features that measure correspon-dences between the tree structures themselves.Span difference: We expect that, in general,aligned nodes should dominate spans of roughly thesame length, and so we allow the model to learn to879Figure 2: a) An example of a legal alignment on a Chinese-English sentence fragment with one good and one bad nodepair, along with sample word alignment posteriors.
Hard word alignments are bolded.
b) The word alignment regionsfor the good NP-NP alignment.
InsideBoth regions are shaded in black, InSrcOutTrg in light grey, and InTrgOutSrc ingrey.
c) The word alignment regions for the bad PP-NP alignment.penalize node pairs whose inside span lengths differgreatly.SPANDIFF = ||i(n)| ?
|i(n?
)||Number of children: We also expect that therewill be correspondences between the rules of theCFGs that generate the trees in each language.
Toencode some of this information, we compute in-dicators of the number of children c that the nodeshave in t and t?.NUMCHILDREN?|c(n)|, |c(n?)|?
= 1Child labels: In addition, we also encode whethercertain label pairs occur as children of matchednodes.
Let c(n, `) select the children of n with la-bel `.CHILDLABEL?`, `??
= |c(n, `)| ?
|c(n?, `?
)|Note that the corresponding ?self labels?
featureis not listed because it arises in the next section as atyped variant of the bias feature.3.4 Typed vs untyped featuresFor each feature above (except monolingual fea-tures), we create label-specific versions by conjoin-ing the label pair (`(n), `(n?)).
We use both thetyped and untyped variants of all features.4 TrainingRecall that our data condition supplies sentencepairs (s, s?)
along with gold parse pairs (g, g?).
Wedo not observe the alignments a which link theseparses.
In principle, we want to find weights whichmaximize the marginal log likelihood of what we doobserve given our sentence pairs:3w?
= arg maxw?aP(g, a, g?|s, s?, w) (1)= arg maxw?a exp(w>?
(g, a, g?))?(t,t?
)?a exp(w>?
(t, a, t?
))(2)There are several challenges.
First, the space ofsymmetric at-most-one-to-one matchings is #P-hard3In this presentation, we only consider a single sentence pairfor the sake of clarity, but our true objective was multiplied overall sentence pairs in the training data.880to sum over exactly (Valiant, 1979).
Second, evenwithout matchings to worry about, standard meth-ods for maximizing the above formulation would re-quire summation over pairs of trees, and we wantto assume a fairly generic interface to independentmonolingual parsers (though deeper joint modelingand/or training is of course a potential extension).As we have chosen to operate in a reranking modeover monolingual k-best lists, we have another is-sue: our k-best outputs on the data which trainsour model may not include the gold tree pair.
Wetherefore make several approximations and modifi-cations, which we discuss in turn.4.1 Viterbi AlignmentsBecause summing over alignments a is intractable,we cannot evaluate (2) or its derivatives.
However,if we restrict the space of possible alignments, thenwe can make this optimization more feasible.
Oneway to do this is to stipulate in advance that for eachtree pair, there is a canonical alignment a0(t, t?).
Ofcourse, we want a0 to reflect actual correspondencesbetween t and t?, so we want a reasonable definitionthat ensures the alignments are of reasonable qual-ity.
Fortunately, it turns out that we can efficientlyoptimize a given a fixed tree pair and weight vector:a?
= arg maxaP(a|t, t?, s, s?, w)= arg maxaP(t, a, t?|s, s?, w)= arg maxaexp(w>?
(t, a, t?
))This optimization requires only that we search foran optimal alignment.
Because all our features canbe factored to individual node pairs, this can be donewith the Hungarian algorithm in cubic time.4 Notethat we do not enforce any kind of domination con-sistency in the matching: for example, the optimalalignment might in principle have the source rootaligning to a target non-root and vice versa.We then define a0(t, t?)
as the alignment thatmaximizes w>0 ?
(t, a, t?
), where w0 is a fixed initialweight vector with a weight of 1 for INSIDEBOTH,-1 for INSRCOUTTRG and INTRGOUTSRC, and 04There is a minor modification to allow nodes not to match.Any alignment link which has negative score is replaced by azero-score link, and any zero-score link in the solution is con-sidered a pair of unmatched nodes.for all other features.
Then, we simplify (2) by fix-ing the alignments a0:w?
= arg maxwexp(w>?
(g, a0(g, g?
), g?))?(t,t?)
exp(w>?
(t, a0(t, t?
), t?
))(3)This optimization has no latent variables and istherefore convex and straightforward.
However,while we did use this as a rapid training procedureduring development, fixing the alignments a priori isboth unsatisfying and also less effective than a pro-cedure which allows the alignments a to adapt dur-ing training.Again, for fixed alignments a, optimizing w iseasy.
Similarly, with a fixed w, finding the optimala for any particular tree pair is also easy.
Anotheroption is therefore to use an iterative procedure thatalternates between choosing optimal alignments fora fixed w, and then reoptimizing w for those fixedalignments according to (3).
By iterating, we per-form the following optimization:w?
= arg maxwmaxa exp(w>?
(g, a, g?))?(t,t?)
maxa exp(w>?
(t, a, t?
))(4)Note that (4) is just (2) with summation replacedby maximization.
Though we do not know of anyguarantees for this EM-like algorithm, in practiceit converges after a few iterations given sufficienttraining data.
We initialize the procedure by settingw0 as defined above.4.2 Pseudo-gold TreesWhen training our model, we approximate the setsof all trees with k-best lists, T and T ?, producedby monolingual parsers.
Since these sets are notguaranteed to contain the gold trees g and g?, ournext approximation is to define a set of pseudo-goldtrees, following previous work in monolingual parsereranking (Charniak and Johnson, 2005).
We defineT?
(T?
?)
as the F1-optimal subset of T (T ?).
We thenmodify (4) to reflect the fact that we are seeking tomaximize the likelihood of trees in this subset:w?
= arg maxw?(t,t?)?(T?
,T?
?
)P(t, t?|s, s?, w) (5)where P(t, t?|s, s?, w) =maxa exp(w>?
(t, a, t?))?(t?,t??)?
(T,T ?)
maxa exp(w>?
(t?, a, t??
))(6)8814.3 Training Set PruningTo reduce the time and space requirements for train-ing, we do not always use the full k-best lists.
Toprune the set T , we rank all the trees in T from 1 tok, according to their log likelihood under the base-line parsing model, and find the rank of the leastlikely pseudo-gold tree:r?
= mint?T?rank(t)Finally, we restrict T based on rank:Tpruned = {t ?
T |rank(t) ?
r?
+ }where  is a free parameter of the pruning procedure.The restricted set T ?pruned is constructed in the sameway.
When training, we replace the sum over all treepairs in (T, T ?)
in the denominator of (6) with a sumover all tree pairs in (Tpruned, T ?pruned).The parameter  can be set to any value from 0to k, with lower values resulting in more efficienttraining, and higher values resulting in better perfor-mance.
We set  by empirically determining a goodspeed/performance tradeoff (see ?6.2).5 Joint SelectionAt test time, we have a weight vector w and soselecting optimal trees for the sentence pair (s, s?
)from a pair of k best lists, (T, T ?)
is straightforward.We just find:(t?, t??)
= arg max(t,t?)?
(T,T ?
)maxaP(t, a, t?|s, s?, w)= arg max(t,t?)?
(T,T ?)maxaw>?
(t, a, t?
)Note that with no additional cost, we can also findthe optimal alignment between t?
and t??:a?
= arg maxaw>?
(t?, a, t??
)5.1 Test Set PruningBecause the size of (T, T ?)
grows asO(k2), the timespent iterating through all these tree pairs can growunreasonably long, particularly when reranking a setof sentence pairs the size of a typical MT corpus.
Tocombat this, we use a simple pruning technique tolimit the number of tree pairs under consideration.Training Dev TestArticles 1-270 301-325 271-300Ch Sentences 3480 352 348Eng Sentences 3472 358 353Bilingual Pairs 2298 270 288Table 1: Sentence counts from bilingual Chinese tree-bank corpus.To prune the list of tree pairs, first we rank themaccording to the metric:wSOURCELL ?
SOURCELL +wTARGETLL ?
TARGETLLThen, we simply remove all tree pairs whose rank-ing falls below some empirically determined cutoff.As we show in ?6.3, by using this technique we areable to speed up reranking by a factor of almost 20without an appreciable loss of performance.6 Statistical Parsing ExperimentsAll the data used to train the joint parsing model andto evaluate parsing performance were taken from ar-ticles 1-325 of the Chinese treebank, which all haveEnglish translations with gold-standard parse trees.The articles were split into training, development,and test sets according to the standard breakdown forChinese parsing evaluations.
Not all sentence pairscould be included for various reasons, includingone-to-many Chinese-English sentence alignments,sentences omitted from the English translations, andlow-fidelity translations.
Additional sentence pairswere dropped from the training data because theyhad unambiguous parses in at least one of the twolanguages.
Table 1 shows how many sentences wereincluded in each dataset.We had two training setups: rapid and full.
In therapid training setup, only 1000 sentence pairs fromthe training set were used, and we used fixed align-ments for each tree pair rather than iterating (see?4.1).
The full training setup used the iterative train-ing procedure on all 2298 training sentence pairs.We used the English and Chinese parsers inPetrov and Klein (2007)5 to generate all k-best listsand as our evaluation baseline.
Because our bilin-gual data is from the Chinese treebank, and the data5Available at http://nlp.cs.berkeley.edu.882typically used to train a Chinese parser contains theChinese side of our bilingual training data, we hadto train a new Chinese grammar using only articles400-1151 (omitting articles 1-270).
This modifiedgrammar was used to generate the k-best lists thatwe trained our model on.
However, as we tested onthe same set of articles used for monolingual Chi-nese parser evaluation, there was no need to usea modified grammar to generate k-best lists at testtime, and so we used a regularly trained Chineseparser for this purpose.We also note that since all parsing evaluationswere performed on Chinese treebank data, the Chi-nese test sentences were in-domain, whereas theEnglish sentences were very far out-of-domain forthe Penn Treebank-trained baseline English parser.Hence, in these evaluations, Chinese scores tend tobe higher than English ones.Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al (2006)and DeNero and Klein (2007)6, trained on approxi-mately 1.7 million sentence pairs.
For our alignmentmodel we used an HMM in each direction, trained toagree (Liang et al, 2006), and we combined the pos-teriors using DeNero and Klein?s (2007) soft unionmethod.Unless otherwise specified, the maximum valueof k was set to 100 for both training and testing, andall experiments used a value of 25 as the  parameterfor training set pruning and a cutoff rank of 500 fortest set pruning.6.1 Feature AblationTo verify that all our features were contributing tothe model?s performance, we did an ablation study,removing one group of features at a time.
Table 2shows the F1 scores on the bilingual developmentdata resulting from training with each group of fea-tures removed.7 Note that though head word fea-tures seemed to be detrimental in our rapid train-ing setup, earlier testing had shown a positive effect,so we reran the comparison using our full trainingsetup, where we again saw an improvement whenincluding these features.6Available at http://nlp.cs.berkeley.edu.7We do not have a test with the basic alignment featuresremoved because they are necessary to compute a0(t, t?
).Baseline ParsersFeatures Ch F1 Eng F1 Tot F1Monolingual 84.95 76.75 81.15Rapid TrainingFeatures Ch F1 Eng F1 Tot F1All 86.37 78.92 82.91?Hard align 85.83 77.92 82.16?Scaled align 86.21 78.62 82.69?Head word 86.47 79.00 83.00?Span diff 86.00 77.49 82.07?Num children 86.26 78.56 82.69?Child labels 86.35 78.45 82.68Full TrainingFeatures Ch F1 Eng F1 Tot F1All 86.76 79.41 83.34?Head word 86.42 79.53 83.22Table 2: Feature ablation study.
F1 on dev set after train-ing with individual feature groups removed.
Performancewith individual baseline parsers included for reference. Ch F1 Eng F1 Tot F1 Tree Pairs15 85.78 77.75 82.05 1,463,28320 85.88 77.27 81.90 1,819,26125 86.37 78.92 82.91 2,204,98830 85.97 79.18 82.83 2,618,68640 86.10 78.12 82.40 3,521,42350 85.95 78.50 82.50 4,503,554100 86.28 79.02 82.91 8,997,708Table 3: Training set pruning study.
F1 on dev set aftertraining with different values of the  parameter for train-ing set pruning.6.2 Training Set PruningTo find a good value of the  parameter for train-ing set pruning we tried several different values, us-ing our rapid training setup and testing on the devset.
The results are shown in Table 3.
We selected25 as it showed the best performance/speed trade-off, on average performing as well as if we had doneno pruning at all, while requiring only a quarter thememory and CPU time.6.3 Test Set PruningWe also tried several different values of the rank cut-off for test set pruning, using the full training setup883Cutoff Ch F1 Eng F1 Tot F1 Time (s)50 86.34 79.26 83.04 174100 86.61 79.31 83.22 307200 86.67 79.39 83.28 509500 86.76 79.41 83.34 11821000 86.80 79.39 83.35 22472000 86.78 79.35 83.33 447610,000 86.71 79.37 83.30 20,549Table 4: Test set pruning study.
F1 on dev set obtainedusing different cutoffs for test set pruning.and testing on the dev set.
The results are in Table 4.For F1 evaluation, which is on a very small set ofsentences, we selected 500 as the value with the bestspeed/performance tradeoff.
However, when rerank-ing our entire MT corpus, we used a value of 200,sacrificing a tiny bit of performance for an extra fac-tor of 2 in speed.86.4 Sensitivity to kSince our bitext parser currently operates as areranker, the quality of the trees is limited by thequality of the k-best lists produced by the baselineparsers.
To test this limitation, we evaluated perfor-mance on the dev set using baseline k-best lists ofvarying length.
Training parameters were fixed (fulltraining setup with k = 100) and test set pruning wasdisabled for these experiments.
The results are in Ta-ble 5.
The relatively modest gains with increasing k,even as the oracle scores continue to improve, indi-cate that performance is limited more by the model?sreliance on the baseline parsers than by search errorsthat result from the reranking approach.6.5 Final ResultsOur final evaluation was done using the full trainingsetup.
Here, we report F1 scores on two sets of data.First, as before, we only include the sentence pairsfrom our bilingual corpus to fully demonstrate thegains made by joint parsing.
We also report scoreson the full test set to allow easier comparison with8Using a rank cutoff of 200, the reranking step takes slightlylonger than serially running both baseline parsers, and generat-ing k-best lists takes slightly longer than getting 1-best parses,so in total, joint parsing takes about 2.3 times as long as mono-lingual parsing.
With a rank cutoff of 500, total parsing time isscaled by a factor of around 3.8.Joint Parsing Oraclek Ch F1 Eng F1 Ch F1 Eng F11 84.95 76.75 84.95 76.7510 86.23 78.43 90.05 81.9925 86.64 79.27 90.99 83.3750 86.61 79.10 91.82 84.14100 86.71 79.37 92.23 84.73150 86.67 79.47 92.49 85.17Table 5: Sensitivity to k study.
Joint parsing and oracleF1 obtained on dev set using different maximum valuesof k when generating baseline k-best lists.F1 on bilingual data onlyParser Ch F1 Eng F1 Tot F1Baseline 83.50 79.25 81.44Joint 85.25 81.72 83.52F1 on full test setParser Ch F1 Eng F1 Tot F1Baseline 82.91 78.93 81.00Joint 84.24 80.87 82.62Table 6: Final evaluation.
Comparison of F1 on test setbetween baseline parsers and joint parser.past work on Chinese parsing.
For the latter evalu-ation, sentences that were not in the bilingual cor-pus were simply parsed with the baseline parsers.The results are in Table 6.
Joint parsing improvesF1 by 2.5 points on out-of-domain English sentencesand by 1.8 points on in-domain Chinese sentences;this represents the best published Chinese treebankparsing performance, even after sentences that lacka translation are taken into account.7 Machine TranslationTo test the impact of joint parsing on syntactic MTsystems, we compared the results of training an MTsystem with two different sets of trees: those pro-duced by the baseline parsers, and those produced byour joint parser.
For this evaluation, we used a syn-tactic system based on Galley et al (2004) and Gal-ley et al (2006), which extracts tree-to-string trans-ducer rules based on target-side trees.
We trained thesystem on 150,000 Chinese-English sentence pairsfrom the training corpus of Wang et al (2007), andused a large (close to 5 billion tokens) 4-gram lan-884Baseline Joint MosesBLEU 18.7 21.1 18.8Table 7: MT comparison on a syntactic system trainedwith trees output from either baseline monolingualparsers or our joint parser.
To facilitate relative compari-son, the Moses (Koehn et al, 2007) number listed reflectsthe default Moses configuration, including its full distor-tion model, and standard training pipeline.guage model for decoding.
We tuned and evaluatedBLEU (Papineni et al, 2001) on separate held-outsets of sentences of up to length 40 from the samecorpus.
The results are in Table 7, showing that jointparsing yields a BLEU increase of 2.4.98 ConclusionsBy jointly parsing (and aligning) sentences in atranslation pair, it is possible to exploit mutual con-straints that improve the quality of syntactic analy-ses over independent monolingual parsing.
We pre-sented a joint log-linear model over source trees,target trees, and node-to-node alignments betweenthem, which is used to select an optimal tree pairfrom a k-best list.
On Chinese treebank data, thisprocedure improves F1 by 1.8 on Chinese sentencesand by 2.5 on out-of-domain English sentences.
Fur-thermore, by using this joint parsing technique topreprocess the input to a syntactic MT system, weobtain a 2.4 BLEU improvement.AcknowledgementsWe would like to thank the anonymous reviewers forhelpful comments on an earlier draft of this paperand Adam Pauls and Jing Zheng for help in runningour MT experiments.ReferencesAnthony Aue, Arul Menezes, Bob Moore, Chris Quirk,and Eric Ringger.
2004.
Statistical machine trans-lation using labeled semantic dependency graphs.
InTMI.9Note that all numbers are single-reference BLEU scoresand are not comparable to multiple reference scores or scoreson other corpora.Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.2007.
English chinese translation treebank v 1.0.
Webdownload.
LDC2007T02.Daniel M. Bikel and David Chiang.
2000.
Two statisti-cal parsing models applied to the chinese treebank.
InSecond Chinese Language Processing Workshop.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In ACL.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In ACL.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
InCOLING-ACL.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In HLT-NAACL.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrase-based translation.In ACL.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.
Syn-tax for statistical machine translation.
Technical re-port, CLSP, Johns Hopkins University.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation.
Research report, IBM.RC22176.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.885Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In ACL.Libin Shen, Jinxi Xu, and Ralph Weishedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InACL.David A. Smith and Noah A. Smith.
2004.
Bilin-gual parsing with factored estimation: using englishto parse korean.
In EMNLP.Leslie G. Valiant.
1979.
The complexity of computingthe permanent.
In Theoretical Computer Science 8.Wen Wang, Andreas Stolcke, and Jing Zheng.
2007.Reranking machine translation hypotheses with struc-tured and web-based language models.
In IEEE ASRUWorkshop.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In COLING.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In ACL.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InACL.Andreas Zollmann, Ashish Venugopal, Stephan Vogel,and Alex Waibel.
2006.
The cmu-aka syntax aug-mented machine translation system for iwslt-06.
InIWSLT.886
