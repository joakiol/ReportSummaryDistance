Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1472?1482,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTesting and Comparing Computational Approaches for Identifying theLanguage of Framing in Political NewsEric P. S. Baumer1,2, Elisha Elovic2, Ying ?Crystal?
Qin2, Francesca Polletta3, Geri K. Gay1,21Communication2Information Science3SociologyCornell University University of California, IrvineIthaca, NY, USA Irvine, CA, USA{ericpsb, epe9, yq37, gkg1}@cornell.edu, polletta@uci.eduAbstractThe subconscious influence of framing on per-ceptions of political issues is well-documentin political science and communication re-search.
A related line of work suggests thatdrawing attention to framing may help reducesuch framing effects by enabling frame reflec-tion, critical examination of the framing un-derlying an issue.
However, definite guid-ance on how to identify framing does not ex-ist.
This paper presents a technique for iden-tifying frame-invoking language.
The paperfirst describes a human subjects pilot studythat explores how individuals identify fram-ing and informs the design of our technique.The paper then describes our data collectionand annotation approach.
Results show thatthe best performing classifiers achieve perfor-mance comparable to that of human annota-tors, and they indicate which aspects of lan-guage most pertain to framing.
Both technicaland theoretical implications are discussed.1 IntroductionContentious political issues are rarely understoodper se but rather through the lens of framing.
Termssuch as ?tax relief,?
?death taxes,?
?racial quotas,?
?death panels,?
and others have famously rallied cit-izens around fairly complex causes.
More gener-ally, research has shown that the way an issue isframed (Entman, 1993) ?
how a problem is defined,to what other problems and people it is linked, etc.?
has a significant impact on both perceptions ofthe issue and prescriptions for action.
A variety ofwork has shown that minor changes in language ?
?global warming?
vs. ?climate change?
(Schuldt etal., 2011), ?gay civil unions?
vs. ?homosexual mar-riage?
(Price et al, 2005), ?not allow?
vs.
?forbid?
(Rugg, 1941) ?
can significantly impact opinions.A related but less explored line of researchsuggests that ?frame reflection?
(Sch?on and Rein,1994), i.e., critical thinking about an issue?s fram-ing, can play an important role in understanding is-sues and reconciling conflicts.
Indeed, some recentwork suggests that drawing attention to framing mayhelp mitigate framing effects (Baumer et al, 2015).However, such reflection is no mean feat.
?Variousobservers have noted how subtly and unconsciously[framing] operates?
(Gamson and Modigliani, 1989,p.
7), making it difficult to acknowledge that anissue is being framed at all, let alne examine thatframing critically or consider alternatives.
Further-more, ?straightforward guidelines on how to identify[...] a frame in communication do not exist?
(Chongand Druckman, 2007, p. 106).To address this challenge, this paper comparesdifferent computational approaches for identifyingthe language of framing, specifically in politicalnews coverage.
The best performing classifiersachieve accuracy around 61% and F1 scores of 0.45to 0.46, outperforming a dummy baseline and ap-proaching or matching human performance of 73%accuracy and F1 score of 0.46.
This work makes twokey contributions.
First, compares different tech-niques for identifying language invoking conceptualframing, a novel yet important task.
Second, it offersevidence about what language is perceived as relatedto framing, helping to addressing the gap identifiedby (Chong and Druckman, 2007).14722 Related Work2.1 Conceptual FramingGenerally speaking, in order to make sense of theirinteractions, people frame their experiences (Goff-man, 1974).
Facts ?take on their meaning by be-ing embedded in a frame [...] that organizes themand gives them coherence?
(Gamson, 1989, p. 157).Frames help people ?locate, perceive, identify, andlabel,?
i.e., organize and give meaning to, informa-tion about experiences in the world.
A frame con-sists of a variety of components, including ?key-words, stock phrases, stereotype images, sources ofinformation?
(Entman, 1993, p. 52), ?metaphors,exemplars, catchphrases, depictions, [...] visual im-ages?
(Gamson and Modigliani, 1989, p. 3), andother devices that provide an interpretive lens or?package.?
Frames define what counts as a prob-lem, diagnose what is causing the problem, makemoral judgments about those involved, and sug-gest remedies for resolving the problem (Entman,1993).
Framing can significantly impact the per-ception of a variety of political issues.
(Schuldt etal., 2011) found that belief in ?global warming?
wassignificantly lower than in ?climate change,?
specif-ically among Republicans.
Gamson and Modigliani(1989) show how nuclear power is framed by suchphrases as ?atoms for peace,?
?we have created aFrankenstein,?
and ?the war being waged against theenvironment and our health.
?Crucially, framing differs from subjectivity, senti-ment, bias, and related constructs.
Subjectivity de-tection may not effectively identify well-established,codified frames (e.g., ?tax relief?
or ?racial quotas?
).Sentiment analysis focuses on assessing the valence(e.g., positive, neutral, or negative) of an entity?s de-scription.
Bias involves a clear, often intentional,preference shown in writing for one position or opin-ion on an issue.
In contrast, there does not exista one-to-one mapping between framing and opin-ions (Gamson and Modigliani, 1989).
For example,in late 2013, the international community was con-sidering what actions should be taken against Syr-ian president Bashar al-Assad for using chemicalweapons against rebelling citizens.
Some viewedthe situation as a humanitarian crisis and argued formilitary intervention.
Others argued that al-Assad?sactions were a threat to regional security, also argu-ing for military action.
Here, different framings areused to support the same position on an issue.In contrast, framing involves an ensemble ofrhetorical elements to create an ?interpretive pack-age?
(Gamson and Modigliani, 1989) that functionsby altering the relative salience or importance of dif-ferent aspects of an issue (Chong and Druckman,2007).
In the humanitarian crisis framing vs. re-gional security framing example above, a regionalsecurity framing does not negatively valence humansuffering; rather, it shifts the emphasis to make otherconcerns more apparent.
Thus, while we can drawon subjectivity, bias, and sentiment detection, iden-tifying framing requires features and techniques thatgo beyond any one of these related concepts.2.2 Related Computational ApproachesSome computational work explored concepts relatedto conceptual framing.
For example, Choi et al(2012) identify hedging in discussion of GMOs us-ing an SVM trained on n-grams from annotated cuephrases.
Greene and Resnik (2009) showed how ex-amining grammatical construction (i.e., syntax) canreveal implicit sentiment; for example, passive andactive voice imply different degrees of agency andcausality.
Recasens et al (2013) used edits fromWikipedia intended to enforce a neutral point ofview to identify biased sentences and the terms in-dicative of that bias.Relatively little work has been done on identi-fying frames per se.
Lind and Salo (2002) usedco-occurrence frequencies to examine the framingof feminism in news media.
Matthes and Kohring(2008) take a mixed methods approach, asking hu-man coders to annotate the occurrence of certainfeatures in a text problem definition, attribution ofcausation, moral evaluation, etc.
then cluster thetext using that coding to identify high-level frames.Boydstun et al (2013) suggest that approaches basedon hierarchical topic modeling may be an effectivemeans of identifying both issue-specific and genericframes.Such techniques, while useful from an analyticstandpoint, are not as directly relevant here.
Thework described in this paper does not aim to identifythe framing in a text.
Rather, it seeks to determinewhat language is perceived as being most related toframing, especially by lay-persons, as a means of1473supporting reflection on that framing.3 Exploratory Pilot StudyWhile the framing literature provides some guid-ance (Entman, 1993; Chong and Druckman, 2007),little work has explored how exactly non-elites goabout identifying framing (Chong and Druckman,2007).
Thus, we conducted a pilot investigating toexplore laypersons?
understandings and identifica-tion of framing.3.1 MethodsWe recruited undergraduate students at two majorUS universities.
Participants were asked to read anarticle that framed the issue of health care in terms ofequality (as opposed to cost).
Previous work showedthat reading this article was associated with strongersupport for a national healthcare system (Druckmanet al, 2012).
Participants were then asked to re-readthe article and highlight any words or phrases theybelieved were related to framing.
Specifically, par-ticipants were given the following prompt and in-structions:Political issues can often be complex, contentious, anddifficult to understand.
One way of making sense of theseissues, and the different positions that one can take onan issue, is to think about the frames that structure de-bate about the issue.
Frames help organize facts and in-formation.
They help define what counts as a problem,diagnose the problem?s causes, and suggest remedies forsolving the problem.
These ways of thinking have lots ofdifferent parts, including stereotypes, metaphors, images,catchphrases, and other elements.These different frames are often associated with a par-ticular way of talking about or communicating about anissue.
Certain words or phrases might suggest that oneor another frame.Please use the tool at the link below to highlight thewords or phrases that help you identify the framings usedin the article you just read.After a student finished highlighting the article,s/he also participated in a debriefing interview wherea researcher asked her or him about what s/he high-lighted and why.We recruited total of 47 students; 20 completedthe task in person, who completed the debriefing in-terview immediately, and 27 used an online anno-tation tool, who completed the debriefing over thephone within 24 hours.3.2 ResultsFirst, we sought to determine the extent to whichstudy participants?
annotations agreed with one an-other.
Do different people see the same words andphrases as being related to framing?
An intraclasscorrelation (ICC) among participants?
annotations of0.757 indicated that the annotators demonstrated amoderately high degree of agreement as to whichwords and phrases were most related to framing.As an example, one article about health care con-tained the sentence: ?A good doctor might recog-nize the regenerative powers of the body politic andcome up with a comprehensive treatment plan thatalso attacks root causesincluding the twin cancers ofracism and poverty.?
Three students highlighted theentire sentence, another three highlighted only thephrase ?twin cancers of racism and poverty,?
and onemore highlighted ?twinpoverty,?
?regenerative pow-ers,?
and ?body politic.
?Several important insights were also derived fromthe debriefing interviews.
Participants drew a dis-tinction between facts and opinions, the later beingmore relevant to framing.
For example, statisticswere rarely seen as related to framing.
Also, fram-ing often dealt not only with a word itself but alsowith aspects of its context and its relationships withother terms in the article.
For example, a partici-pant might highlight just the word ?but?
because itindicates an important rhetorical shift and, implic-itly, the article?s take on the issue.
Similarly, la-tent relationships between an individual word andthe article?s main argument also played an impor-tant role.
For example, the article participants reademphasized ?disparities?
between healthcare avail-able to the wealthy and to the working class.
Manyparticipants indicated that they would highlight anywords or phrases that drew attention to such dispari-ties.
These insights, in conjunction with the theoret-ical literature on conceptual framing, were used toguide feature selection.4 DataWe sought to develop a classifier that could auto-matically identify the language in a text that mostrelated to framing.
We chose to focus on news cover-age rather than, say, opinion and editorial columns.Framing likely occurs in a more apparent, poten-1474tially obvious fashion in opinion articles.
In ostensi-bly ?straight?
news, though, framing may be harderto identify.
Thus, this context would benefit morefrom a classifier that could automatically draw at-tention to frame-invoking language.For training data, we wanted political texts wherelay readers had indicated the words and phrases theyperceived as most related to framing.
Lay annona-tors were used instead of experts because the classi-fier?s purpose is to support frame reflection amongthe lay public.
Thus, the words and phrases the clas-sifier highlights should align with that population?sperception of framing.
To our knowledge, no suchdata set exists.
So, we used Mechanical Turk (Snowet al, 2008) and university students to build an an-notated dataset that could be used for training andtesting.4.1 CollectionWe began by collecting political newsarticles from top 15 online sources ofnews, as determined by Alexa rankings(http://www.alexa.com/topsites/category/Top/News).We excluded sources outside the US (e.g., BBC),news aggregators (e.g., Yahoo News, Google News),blogs (Huffington Post), and sites without a dedi-cated politics feed (e.g., USA Today, weather.com).Doing so left eight sources CNN, NYT, Fox, NBC,Washington Post, ABC, LA Times, Reuters.For each source, we collected all items on theirpolitics-specific RSS feed on two separate daysroughly six months apart to provide content aboutdiverse issues and events: Tuesday November 12,2013, and Thursday May 15, 2014.
We manu-ally removed duplicate posts, ?round-up?
style poststhat simply summarized and linked to other stories,video-only posts, and other non-textual content, re-sulting in a total of 205 documents.
Of these, werandomly selected 75 to be annotated.4.2 AnnotationEach article was annotated by five to 13 annotators,who were either Mechanical Turk (MTurk) workersor students at one of two major US universities.
Thetask first asked the annotator to read the article, thengave the same directions from the framing promptdescribed above.To encourage MTurk workers to pay attention tothe task and complete high-quality work, we pro-vided a scheme for bonus payments.
Every worda worker annotated that was also annotated by atleast two others (i.e., a majority of the 5 workersannotating each document) would earn the worker a$0.02 bonus (two cents).
Each word s/he annotatedthat was annotated by no other work would reducethe bonus by $0.005 (half a cent).
Workers werethen linked to our web tool where they could com-plete and submit their annotations.
Student annota-tors received no agreement-based incentive but weregranted extra course credit.4.3 Quality AssuranceCrowd workers do not always provide reliable anno-tations (Snow et al, 2008).
For example, we notedmultiple instances where annotators had only anno-tated about a dozen words in an article of severalhundred words.
These seemed likely to be casesin which the annotator was completing the task asquickly as possible without paying much attention.By comparing the annotations with those collectedduring our pilot study, in which participants?
justi-fications during the debriefing ensured higher atten-tion and quality, we developed the following criteriafor identifying questionable annotations.
Those thatdid not pass at least three of these five requirementswere removed from the analysis.1.
All Annotations Short ?
While some annota-tions of short words, such as conjunctions, could bemeaningful (see example above), we encountered anumber of annotations where every annotated phraseconsisted of only one or two words at a time.
Thus,we required that the average annotated contiguoussegment be at least 3 words long.2.
Few Words Annotated ?
When very fewwords in a document are annotated, we suspectthe annotator may have been completing the taskas quickly as possible and paying little attention.Thus, we required that each annotator?s work in-clude enough annotated words to total more than 5%of the document?s length.3.
Large Contiguous Passages without Annota-tion ?
While pilot study participants pointed outportions of a text that consisted of ?facts and fig-ures,?
these were generally relatively short.
Thus,we require the longest block of text without any an-notations to be no more than one third the length of1475the entire document.4.
Large Contiguous Passages Entirely Annotated?
Large, unbroken annotations rarely occurred inour pilot study.
Such annotations may also have in-dicated that the annotator was not attending to theentire content being annotated.
Thus, we requirethat the longest contiguous annotation be no morethan 120 words long.5.
Annotating Solely Stop Words ?
A number ofannotations include only very common words, suchas articles, prepositions, forms of the verb ?to be,?etc.
Single words such as ?but,?
?all,?
or ?not?
couldarguably be related to framing.
However, accountsfrom our pilot study participants made us less likelyto believe that other single words, such as ?an,?
?of,?or ?that,?
instantiated framing.
Thus, we require thatno more than 3 annotated passages consistent en-tirely of such stop words1.We also encountered two situations in which pairsof MTurk workers submitted virtually identical an-notations for multiple documents.
Following upwith the workers, we discovered that in one situationa husband and wife team had actually been workingtogether.
Based on our pilot study, we would expectannotators to agree to some degree, but we wouldnot want such agreement to arise because annota-tors were collaborating.
Thus, we also exclude theseannotations where we suspect the possibility of col-laboration.We only include in our dataset documents with atleast three valid annotations.
In total, the resultingdata set includes 74 articles containing 53,878 to-tal words (M=728.1 words per particle), each withthree to 11 valid annotations (Mdn=6).
The dataset includes 59,948 annotated words across 507 an-notations from 372 annotators (M=122.8 annotatedwords per article).
The full data set is available athttp://hdl.handle.net/1813/39216.5 Classifier Design and ImplementationAs argued above, drawing attention to framing-related language may both mitigate frame effects(Baumer et al, 2015) and facilitate frame reflection(Sch?on and Rein, 1994).
This section describes thefeatures used to train a classifier to identify framingalong with justifications for each, different subsetsof features that were tested, our classifier selection,and the training and testing methods.5.1 Features and SubsetsWe treat each word as a data point to be classifiedas framing-related or not.
Feature extraction beganwith splitting each article into sentences with NLTK(Bird et al, 2009) then using Stanford CoreNLP(Klein and Manning, 2003; De Marneffe et al,2006) to parse each sentence, obtain POSs and lem-matized forms for each word, etc.
We then constructa feature vector for each non-stop word.
Table 1 listsall features used.
The remainder of this subsectiondescribes the justificaton for each feature, as well asseveral subsets of features.Framing is often instantiated by specific ?key-words, stock phrases,?
(Entman, 1993, p. 52) or?catchphrases?
(Gamson and Modigliani, 1989, p.3).
Therefore, we include lexical features that cap-ture the specific words used.
Furthermore, many ofour pilot study participants pointed out that a givenword might be seen as related to framing because ofthe other words near which it appears.
Therefore,each of these features includes a contextual windowof up to two words before and after the word beingclassified (Recasens et al, 2013).Participants in our pilot study said physical loca-tions, such as the names of states, were often notrelated to framing.
However, they sometimes sawnames of political figures or experts as indicatingframing.
Thus, we include the named entity type asa feature, both of the word itself and of its context.Pilot study participants also mentioned that aword?s relationship to the remainder of a documentand its overall thesis played important roles.
A num-ber of similar structural aspects of the document,both explicit and latent, may help draw out these re-lationships.Several specific types of terms may be indica-tive of framing.
For example, ?depictions,?
?vi-sual images,?
and figurative language such as metah-por (Gamson and Modigliani, 1989) often invokeframes.
For imagery, we use previously establishedimagery ratings for 1818 common words (Paivioet al, 1968; van der Veur, 1975).
Words that ap-pear in this list are given an imagery rating, eitherlow (first quantile), medium (second and third quan-tile), or high (fourth quantile) imagery.
The con-text feature represents the average imagery of the1476Feature Description Feature Subset(s)Token ?1, ?2 The word token itself and the tokens in its context.
Lexical, TheoreticalLemma ?1, ?2 The lemmatized word and the lemmas in its context.
Lexical, TheoreticalBigrams and trigrams All bigrams and trigrams of which the word is a part.
Lexical, TheoreticalPOS ?1, ?2 The word?s and its context?s part(s) of speech.
GrammaticalRoot Whether the word is the sentence?s parse tree?s root.
GrammaticalRelation and Role The grammatical relations in which the word isinvolved and its role in those relations, e.g., passivesubject of a verb.GrammaticalNamed Entity ?1, ?2 Named entity type of the word and its context.
GrammaticalIn Title Whether the word appears in the article?s title.
DocumentSentence Lengeth The number of words in the sentence.
DocumentSentence Position Whether the word appears in the first third of thesentence, the middle third, or the last third.DocumentTFIDF The word?s tf-idf score, grouped into 8 bins ofincreasing size.DocumentImagery & Context Imagery rating of the word and average imagery ratingof its context (Paivio et al, 1968; van der Veur, 1975).Theoretical,DictionariesFigurativeness &ContextFigurativeness rating of the word and averagefigurativeness rating of its context (Gamson andModigliani, 1989; Turney et al, 2011).Theoretical,DictionariesAbstractness &ContextAbstractness rating of the word and averageabstractness rating of its context (Gamson andModigliani, 1989; Turney et al, 2011).Theoretical,DictionariesDictionaries &ContextOne feature each for whether word or context is asubjective word (Riloff and Wiebe, 2003), a report verb(Recasens et al, 2013), a hedge (Hyland, 2005), afactive verb (Hooper, 1975), an entailment (Berant etal., 2012), an assertive word (Hooper, 1975), a biasword (Recasens et al, 2013), a negative word (Liu etal., 2005), or a positive word (Liu et al, 2005).Theoretical,DictionariesTable 1: Name and description of all features for each word, as well as the set(s) to which each feature belongs.word?s context.
For figurative language, we usedTurney et al?s (Turney et al, 2011) approach ofmeasuring figurativeness based on the absolute dif-ference in the concreteness of two terms that aregrammatically related.
The figurativeness score fora given word is the average absolute difference be-tween its concreteness and the concreteness of ev-ery word with which it is in some grammatical re-lationship.
We also used Turney et al?s (Turney etal., 2011) approach to rate the individual abstract-ness of each word and its context.
Lastly, subjectivewords, hedges, entailments, and other terms that per-form specific psycholinguistic functions (Recasenset al, 2013) may also be useful in identifying fram-ing.
For each, we include one feature for the word it-self and one feature for the two-word context aroundthe word.The features used here are informed by a combi-nation of theoretical literature on framing, our ownpilot studies, and prior work in computational lin-guistics.
However, we have little means of know-ing a priori which of these features will be most im-portant or even necessary.
Therefore, in the interestof developing the most perspicacious model, we testseveral feature subsets, as noted in Table 1.
Lexicalfeatures involve only words that actually occur in thetext.
Grammatical features use only aspects of gram-matical structure.
Document features use various ex-1477plicit and latent aspects of the document?s structure.Theoretical features are those specifically mentionedin the theoretical literature on framing.
Finally, theDictionaries feature set tests whether there might bespecific terms that invoke framing.5.2 Training and TestingWe implemented and tested a variety of differ-ent classifiers, including Stochastic Gradient De-scent (SGD), Multinomial Na?
?ve Bayes, Perceptron,Nearest Neighbor, Logistic Regression, and PassiveAggressive classifiers.
In several tests, the Na?
?veBayes classifier performed best, so we used a Na?
?veBayes classifier throughout.As described above, our data include three to 11annotators?
annotations for each word in the corpus.To create training data, we aggregated these anno-tations such that any word highlighted by at leastone fourth of the annotators was treated as framing-related (i.e., true positive) for training and testingpurposes, and the remaining words were treated asnot frame-related (i.e., true negative).These data, in the combinations of features de-scribed above, were used to train our ensemble clas-sifier using scikit-learn (Pedregosa et al, 2011) us-ing ten-fold random shuffle cross-validation, as wellas a random dummy classifier based on class distri-butions in the training set.
Performance in terms off-score, accuracy, precision, and recall was averagedacross the ten folds.6 ResultsThis section summarizes results, highlighting someimportant aspects thereof, while the subsequent Dis-cussion section considers interpretation and broaderimplications.
Figure 1 presents a summary of re-sults for each feature set, including comparison withthe dummy baseline and the aggregate human per-formance.
The Document Structure feature set per-formed very poorly, identifying 0 true positives, sowe exclude it from the detailed results report.Since overall accuracy does not vary drastically,we focus on other performance metrics.
Except forthe Dictionaries, all feature sets perform statisticallysignificantly better than the dummy (ANOVA withTukey?s posthoc p < 0.001).
F1 scores among thetop three performers (Lexical, Theoretical, and AllFigure 1: Performance of each feature set, as well as thedummy baseline and human annotators, on accuracy, F1,precision, and recall.
Three feature sets (Lexical, Theo-retical, and All features) match human annotators on F1and outperform human recall, but humans demonstratehigher precision.features) are statistically indistinguishable.
Further-more, each of these three top performers matchesaggregate human annotator performance.Looking at precision and recall, we can see thatthe classifiers and the human annotators make differ-ent trade offs.
Precision for all feature sets is around34% (all statisticially significantly better than thedummy and significantly indistinguishable from oneanother), while human average precision is 91.5%.On the other hand, the three top performing featuresets (All, Lexical, and Theoretical) all achieve recallaround 70%, while average recall for the human an-notators is only 49.3%.It is also important to note that human perfor-mance is calculated by comparing each individualannotator to the aggregate of all the annotators.
Be-cause each individual is part of that aggregate, preci-sion scores for the humans are fairly high.
We con-sider comparison with human performance furtherin the discussion below.We also examine the most influential features forclassifiers using each of the three top performing1478feature sets.
For the Lexical feature set, the mostinformative features for negative cases are bigramsthat begin with quotation marks.
Since we manuallyassign all punctuation, including quotation marks,as true negatives, this is perhaps unsurprising.
Forpositive cases, many of the ?offset?
style featuresemerge as informative.
For example, words thatappear one or two words before a comma or pe-riod are more likely to involve framing.
Similarly,words that occur just before or just after preposi-tions or conjunctions ?
to, and, in, of, etc.
?
aremore likely classified as framing (i.e., positive).
Thisresult aligns with some of our pilot study findingsabout the relationships between such function termsthe words surrounding them.
Interestingly, though,these features do not resemble the catchphrases andkeywords described in the framing literature (Ent-man, 1993; Gamson and Modigliani, 1989).For the Theoretically Informed feature set,which adds in imagery, figurativeness, and otherdictionary-based features, roughly the same kindsof lexical features are most important in identify-ing negative cases.
For positive cases, constructssuch as descriptiveness and abstractness play impor-tant roles, but mostly when words in the context donot appear in these dictionaries, calling into ques-tion the importance of imagery, metaphor, and otherfigurative language (Gamson and Modigliani, 1989).Other dictionary-based features, such as being anentailment word, having an entailment in context,having a bias term in context, or having a subjec-tive term in context, become important in identifyingpositive cases.
Some of the lexical features, such asproximity to a comma or period, also remain infor-mative.With All features, which adds features for docu-ment structure and grammatical structure, two ma-jor differences occur.
First, some elements of docu-ment structure become important features for pos-itive cases, including sentence length and TFIDF.Second, various part of speech tags, mostly NNand NNP, become important to identifying framing.These findings suggest that the choice of terms usedto label concepts or entities can indicate framing.
In-deed, entity type, both of the word and its context,also emerges as an informative feature.7 DiscussionChiefly important among the results, three of ourfeature sets were able to achieve F1 scores on parwith those of human annotators.
This result pro-vides encouraging evidence that a machine classifiercan effectively accomplish the task of identifying thelanguage that invokes framing in political news cov-erage.That said, examining the results more closely ex-poses that, in order to achieve this level of perfor-mance, the classifier makes a trade-off.
Specifi-cally, the classifier is far more aggressive than hu-mans, resulting in significantly higher recall butlower precision.
We did experiment with differ-ent post-hoc decision thresholds to make the clas-sifier less aggressive.
However, as we the deci-sion threshold rose, recall fell much more quicklythan precision increased, resulting in lower overallF1 scores.
Moreover, this precision-recall trade offmay have different ramifications in different appli-cations.
For example, in terms of supporting framereflection, would it be harmful or distracting for amachine classifier to mark too many words as frame-invoking, thereby potentially overwhelming a poten-tial user?
Or would it be worse if the classifier weretoo sparse, missing certain important key words orphrases?
These are questions for later empiricalwork that incorporates the results of this classifierinto interactive systems to support frame reflection.Also, our results above identifying important fea-tures within the classifier contribute to and build onprior computational work.
For example, Recasenset al (Recasens et al, 2013) find entailment (Berantet al, 2012), implicature (Karttunen, 1971), subjec-tivity (Riloff and Wiebe, 2003), and other relatedconstructs helpful in identifying bias.
The resultsabove suggest that, when included, such featuresalso emerge as important for identifying the lan-guage of framing.
However, the feature sets that in-clude dictionaries of these terms do not perform sta-tistically significantly better than those feature setswithout them.
This result supports our initial asser-tion that bias and framing, while conceptually re-lated, are separate constructs that are each perceivedand instantiated via different linguistic cues.These findings also relate to recent efforts at iden-tifying which frame(s) are operating in a text (e.g.,1479Boydstun et al, 2013).
On the one hand, the perfor-mance of the Lexical feature set suggests that topicmodeling, especially including certain n-grams, mayprove an effective approach.
That said, Boydstun etal.
are more interested in determining which framesare at work in a text as a whole, whereas this pa-per focuses more on determining where within atext frames are invoked.
Thus, different computa-tional approaches and features may prove effectivefor each task.Interestingly, the grammatical structure featureset is not one of the top performers.
Given our ex-pectations, including the importance that pilot studyparticipants placed on structural relationships withinthe sentence, the role of grammatical constructionboth in metaphors (Turney et al, 2011) in framingmore broadly (Fairclough, 1999), and prior com-putational work on implicit sentiment (Greene andResnik, 2009), this result appears fairly surprising.It could be that grammatical structure alone is notsufficient to identify framing, but even combininggrammatical structure with other features does notsignificnatly improve performance.
Perhaps, then,grammatical construction matters less than the spe-cific words chosen.
On the other hand, lexical fea-tures may be topic-specific, such that even obtain-ing two samples six months apart still resulted inthe same buzzwords invoking framing.
Future workshould examine more closely the role that structuralfeatures play, or perhaps do not play, in invokingframing.This point also draws attention to some practicalimplications.
Performing a full grammatical parsecan be computationally intensive.
If using other fea-tures that do not require a full parse can achievecomparable performance, then perhaps real time ap-plications, such as analyzing live speeches as theyhappen, could employ only features that are rela-tively quicker and easier to extract.Finally, we note that the annotated data analyzedhere come from political news stories in US main-stream media.
Since these sources ostensibly strivefor impartiality, framing in these data may occurimplicitly or unconsciously.
Future work shouldcompare these results with similar analyses of textscontaining more explicit framing, such as opinioncolumns, campaign speeches, or political advertise-ments.
Differences in how framing is identified maygive important clues to how framing operates in dif-ferent contexts.
Similarly informative insights couldbe gained by comparing lay-persons?
annotationswith framing experts?.8 Conclusion?Facts have no intrinsic meaning.
They take on theirmeaning by being embedded in a frame?
(Gamson,1989, p. 157).
Given framing?s pervasive influence,this paper argues for the importance of computa-tional techniques that can identify and draw atten-tion to the language of framing.
Doing so can helpsupport frame reflection (Sch?on and Rein, 1994)and, thereby, deeper understanding of and engage-ment with political issues.This paper both develops a computational ap-proach to identifying framing and tests how welldifferent linguistic features indicate frame-invokinglanguage.
Results suggest grammatical structurealone as the most important indicator of framing.However, other data less computationally demand-ing to extract, such as lexical features (e.g., tokensand n-grams), can prove almost as effective.In sum, the paper makes two main contributions.First, it provides a technical contribution by iden-tifying a task of importance and demonstrating atechnique that performs close to as well as humans.Second, the paper makes a theoretical contribution,helping to provide ?guidelines on how to identify []a frame in communication?
(Chong and Druckman,2007, p. 106).
The data set of annotations releasedwith this paper may also prove a valuable resourcefor future analyses of framing.AcknowledgmentsThis material is based upon work supported by theNSF under Grant No.
IIS-1110932.
Thanks tothe Turker and student annotators, to Andrea Linfor research assistance, and to Cristian Danescu-Niculescu-Mizil and to Peter Turney for sharingtechnical resources.ReferencesEric P. S. Baumer, Francesca Polletta, Nicole Pierski, andGeri K. Gay.
2015.
A Simple Intervention to Re-duce Framing Effects in Perceptions of Global ClimateChange.
Environmental Communication (to appear).1480Jonathan Berant, Ido Dagan, Meni Adler, and JacobGoldberger.
2012.
Efficient tree-based approxima-tion for entailment graph learning.
In Proc ACL, pages117?125.Steven Bird, Edward Loper, and Ewan Klein.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia Inc.Amber E. Boydstun, Justin H. Gross, Philip Resnik, andNoah A. Smith.
2013.
Identifying Media Frames andFrame Dynamics Within and Across Policy Issues.
InNew Directions in Analyzing Text as Data Workshop,London.Eunsol Choi, Chenhao Tan, Lillian Lee, CristianDanescu-Niculescu-Mizil, and Jennifer Spindel.2012.
Hedge Detection as a Lens on Framing inthe GMO Debates: A Position Paper.
In Proc ACLWorkshop on Extra-Propositional Aspects of Meaningin Computational Linguistics, number July, pages70?79.Dennis Chong and James N. Druckman.
2007.
Fram-ing Theory.
Annual Review of Political Science,10(1):103?126, June.M.C.
De Marneffe, Bill MacCartney, and C.D.
Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In Proc LREC, Genoa, Italy.James N. Druckman, Jordan Fein, and Thomas J. Leeper.2012.
A Source of Bias in Public Opinion Stability.American Political Science Review, 106(02):430?454,May.Robert M. Entman.
1993.
Framing: Toward Clarificationof a Fractured Paradigm.
Journal of Communication,43(4):51?58.Norman Fairclough.
1999.
Global Capitalism and Crit-ical Awareness of Language.
Language Awareness,8(2):71?83.William A. Gamson and Andre Modigliani.
1989.
MediaDiscourse and Public Opinion on Nuclear Power: AConstructionist Approach.
The American Journal ofSociology, 95(1):1?37, July.William A. Gamson.
1989.
News as Framing.
AmericanBehavioral Scientist, 33(2):157?161, November.Erving Goffman.
1974.
Frame Analysis.
Harvard Uni-versity Press, Cambridge, MA.Stephan Greene and Philip Resnik.
2009.
More thanWords: Syntactic Packaging and Implicit Sentiment.In Proc HLT, number June, pages 503?511, Boulder,CO.Joan B. Hooper.
1975.
On Assertive Predicates.
InJ.
Kimball, editor, Syntax and Semantics, pages 91?124.
Academic Press, New York, volume 4 edition.Ken Hyland.
2005.
Metadiscourse: Exploring Interac-tion in Writing.
Continuum, London and New York.Lauri Karttunen.
1971.
Implicative Verbs.
Language,47(2):340?358.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
Sapporo, Japan.Rebecca Ann Lind and Colleen Salo.
2002.
The Framingof Feminists and Feminism in News and Public AffairsPrograms in U.S. Electronic Media.
Journal of Com-munication, 52(1):211?228.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion Observer: analyzing and comparing opinionson the Web.
In Proc WWW, pages 342?351.J?org Matthes and Matthias Kohring.
2008.
The Con-tent Analysis of Media Frames: Toward ImprovingReliability and Validity.
Journal of Communication,58(2):258?279.Allan Paivio, John C Yuille, and Stephen A Madigan.1968.
Concreteness, Imagery, and MeaningfulnessValues for 925 Nouns.
Journal of Experimental Psy-chology, 76(1).Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and Edouard Duchesnay.
2011.
Scikit-learn : Ma-chine Learning in Python.
Journal of Machine Learn-ing Research, 12:2825?2830.Vincent Price, Lilach Nir, and Joseph N. Cappella.
2005.Framing Public Discussion of Gay Civil Unions.
Pub-lic Opinion Quarterly, 69(2):171?212.Marta Recasens, Cristian Danescu-Niculescu-Mizil, andDan Jurafsky.
2013.
Linguistic Models for Analyzingand Detecting Biased Language.
In Proc ACL, pages1650?1659, Sofia, Bulgaria.Ellen Riloff and Janyce Wiebe.
2003.
Learning ex-traction patterns for subjective expressions.
In ProcEMNLP, pages 105?112.Donald Rugg.
1941.
Experiments in Wording Questions:II.
Public Opinion Quarterly, 5(1):91?92.Donald A. Sch?on and Martin Rein.
1994.
Frame Re-flection: Toward the Resolution of Intractable PolicyControversies.
Basic Books, New York.Jonathon P. Schuldt, S. H. Konrath, and N. Schwarz.2011.
?Global warming?
or ?climate change??
:Whether the planet is warming depends on questionwording.
Public Opinion Quarterly, 75(1):115?124,February.Rion Snow, Brendan O?Connor, Daniel Jurafsky, An-drew Y Ng, Dolores Labs, and Capp St. 2008.
Cheapand Fast But is it Good?
Evaluating Non-ExpertAnnotations for Natural Language Tasks.
In ProcEMNLP, number October, pages 254?263, Honolulu,HI.Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and Metaphorical Sense Identifica-tion through Concrete and Abstract Context.
In Proc1481EMNLP, volume 2, pages 680?690, Edinburgh, Scot-land.Barbara W. van der Veur.
1975.
Imagery Rating of 1,000Frequently Used Words.
Journal of Educational Psy-chology, 67(1):44?56.1482
