Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 971?978, Vancouver, October 2005. c?2005 Association for Computational LinguisticsAn Orthonormal Basis for Topic Segmentation in Tutorial DialogueAndrew Olney Zhiqiang CaiDepartment of Computer Science Institute for Intelligent SystemsUniversity of Memphis University of MemphisMemphis, TN 38152 Memphis, TN 38152aolney@memphis.edu zcai@memphis.eduAbstractThis paper explores the segmentation oftutorial dialogue into cohesive topics.
Alatent semantic space was created usingconversations from human to human tu-toring transcripts, allowing cohesion be-tween utterances to be measured usingvector similarity.
Previous cohesion-based segmentation methods that focus onexpository monologue are reapplied tothese dialogues to create benchmarks forperformance.
A novel moving windowtechnique using orthonormal bases of se-mantic vectors significantly outperformsthese benchmarks on this dialogue seg-mentation task.1 IntroductionEver since Morris and Hirst (1991)?s ground-breaking paper, topic segmentation has been asteadily growing research area in computationallinguistics, with applications in summarization(Barzilay and Elhadad, 1997), information retrieval(Salton and Allan, 1994), and text understanding(Kozima, 1993).
Topic segmentation likewise hasmultiple educational applications, such as questionanswering, detecting student initiative, and assess-ing student answers.There have been essentially two approaches totopic segmentation in the past.
The first of these,lexical cohesion, may be used for either linearsegmentation (Morris and Hirst, 1991; Hearst,1997) or hierarchical segmentation (Yarri, 1997;Choi, 2000).
The essential idea behind the lexicalcohesion approaches is that different topics willhave different vocabularies.
Therefore the lexicalcohesion within topics will be higher than the lexi-cal cohesion between topics, and gaps in cohesionmay mark topic boundaries.
The second major ap-proach to topic segmentation looks for distinctivetextual or acoustic markers of topic boundaries,e.g.
referential noun phrases or pauses (Passonneauand Litman, 1993; Passonneau and Litman, 1997).By using multiple markers and machine learningmethods, topic segmentation algorithms may bedeveloped using this second approach that have ahigher accuracy than methods using a singlemarker alone (Passonneau and Litman, 1997).The primary technique used in previous studies,lexical cohesion, is no stranger to the educationalNLP community.
Lexical cohesion measured bylatent semantic analysis (LSA) (Landauer and Du-mais, 1997; Dumais, 1993; Manning and Sch?tze,1999) has been used in automated essay grading(Landauer, Foltz, and Laham, 1998) and in under-standing student input during tutorial dialogue(Graesser et al, 2001).
The present paper investi-gates an orthonormal basis of LSA vectors, cur-rently used by the AutoTutor ITS to assess studentanswers (Hu et al, 2003), and how it may be usedto segment tutorial dialogue.The focus on dialogue distinguishes our workfrom virtually all previous work on topic segmen-tation: prior studies have focused on monologuerather than dialogue.
Without dialogue, previousapproaches have only limited relevance to interac-tive educational applications such as intelligenttutoring systems (ITS).
The only existing work ontopic segmentation in dialogue, Galley et al(2003), segments recorded speech between multi-ple persons using both lexical cohesion and dis-971tinctive textual and acoustic markers.
The presentwork differs from Galley et al (2003) in two re-spects, viz.
we focus solely on textual informationand we directly address the problem of tutorial dia-logue.In this study we apply the methods of Foltz et al(1998), Hearst (1994, 1997), and a new techniqueutilizing an orthonormal basis to topic segmenta-tion of tutorial dialogue.
All three are vector spacemethods that measure lexical cohesion to deter-mine topic shifts.
Our results show that the newusing an orthonormal basis significantly outper-forms the other methods.Section 2 reviews previous work, and Section 3reviews the vector space model.
Section 4 intro-duces an extension of the vector space modelwhich uses an orthonormal basis.
Section 5 out-lines the task domain of tutorial dialogue, and Sec-tion 6 presents the results of previous and thecurrent method on this task domain.
A discussionand comparison of these results takes place in Sec-tion 7.
Section 8 concludes.2 Previous workThough the idea of using lexical cohesion to seg-ment text has the advantages of simplicity and in-tuitive appeal, it lacks a unique implementation.An implementation must define how to representunits of text, compare the cohesion between units,and determine whether the results of comparisonindicate a new text segment.
Both Hearst (1994,1997) and Foltz et al (1998) use vector spacemethods discussed below to represent and compareunits of text.
The comparisons can be characterizedby a moving window, where successive overlap-ping comparisons are advanced by one unit of text.However, Hearst (1994, 1997) and Foltz et al(1998) differ on how text units are defined and onhow to interpret the results of a comparison.The text unit's definition in Hearst (1994, 1997)and Foltz et al (1998) is generally task dependent,depending on what size gives the best results.
Forexample, when measuring comprehension, Foltz etal.
(1998) use the unit of the sentence, as opposedto the more standard unit of the proposition, be-cause LSA is most correlated with comprehensionat that level.
However, when using LSA to seg-ment text, Foltz et al (1998) use the paragraph asthe unit, to "smooth out" the local changes in cohe-sion and become more sensitive to more globalchanges of cohesion.
Hearst likewise chooses alarge unit, 6 token-sequences of 20 tokens (Hearst,1994), but varies these parameters dependent onthe characteristics of the text to be segmented, e.g.paragraph size.Under a vector space model, comparisons areperformed by calculating the cosine of vectors rep-resenting text.
As stated previously, these com-parisons reflect the cohesion between units of text.In order to use these comparisons to segment text,however, one must have a criterion in place.
Foltzet al (1998), noting mean cosines of .16 forboundaries and .43 for non-boundaries, choose athreshold criterion of .15, which is two standarddeviations below the boundary mean of .43.
UsingLSA and this criterion, Foltz et al (1998) detectedchapter boundaries with an F-measure of .33 (seeManning and Sch?tze (1999) for a definition of F-measure).
Hearst (1994, 1997) in contrast uses arelative comparison of cohesion, by recasting vec-tor comparisons as depth scores.
A depth score iscomputed as the difference between a given vectorcomparison and its surrounding peaks, i.e.
the localmaxima of vector comparisons on either side of thegiven vector comparison.
The greater the differ-ence between a given comparison and its surround-ing peaks, the higher the depth score.
Once all thedepth scores are calculated for a text, those that arehigher than one standard deviation below the meanare taken as topic boundaries.
Using a vectorspace method without singular value decomposi-tion, Hearst (1997) reports an F-measure of .70when detecting topic shifts between paragraphs.Thus previous work suggests that the Hearst(1997) method is superior to that of Foltz et al(1998), having roughly twice the accuracy indi-cated by F-measure.
Although these two resultsused different data sets and are therefore not di-rectly comparable, one would predict based on thislimited evidence that the Hearst algorithm wouldoutperform the Foltz algorithm on other topic seg-mentation tasks.9723 The vector space modelThe vector space model is a statistical techniquethat represents the similarity between collections ofwords as a cosine between vectors (Manning andSch?tze, 1999).
The process begins by collectingtext into a corpus.
A matrix is created from thecorpus, having one row for each unique word inthe corpus and one column for each document orparagraph.
The cells of the matrix consist of asimple count of the number of times word i ap-peared in document j.
Since many words do notappear in any given document, the matrix is oftensparse.
Weightings are applied to the cells thattake into account the frequency of word i in docu-ment j and the frequency of word i across alldocuments, such that distinctive words that appearinfrequently are given the most weight.
Two col-lections of words of arbitrary size are compared bycreating two vectors.
Each word is associated witha row vector in the matrix, and the vector of a col-lection is simply the sum of all the row vectors ofwords in that collection.
Vectors are comparedgeometrically by the cosine of the angle betweenthem.LSA (Landauer and Dumais, 1997; Dumais1993) is an extension of the vector space modelthat uses singular value decomposition (SVD).SVD is a technique that creates an approximationof the original word by document matrix.
AfterSVD, the original matrix is equal to the product ofthree matrices, word by singular value, singularvalue by singular value, and singular value bydocument.
The size of each singular value corre-sponds to the amount of variance captured by aparticular dimension of the matrix.
Because thesingular values are ordered in decreasing size, it ispossible to remove the smaller dimensions and stillaccount for most of the variance.
The approxima-tion to the original matrix is optimal, in the leastsquares sense, for any number of dimensions onewould choose.
In addition, the removal of smallerdimensions introduces linear dependencies be-tween words that are distinct only in dimensionsthat account for the least variance.
Consequently,two words that were distant in the original spacecan be near in the compressed space, causing theinductive machine learning and knowledge acqui-sition effects reported in the literature (Landauerand Dumais, 1997).4 An orthonormal basisCohesion can be measured by comparing the co-sines of two successive sentences or paragraphs(Foltz, Kintsch, and Landauer, 1998).
However,cohesion is a crude measure: repetitions of a singlesentence will be highly cohesive (cosine of 1) eventhough no new information is introduced.
A varia-tion of the LSA algorithm using orthonormalizedvectors provides two new measures, ?informativ-ity?
and ?relevance?, which can detect how muchnew information is added and how relevant it is ina context (Hu et al, 2003).
The essential idea is torepresent context by an orthonormalized basis ofvectors, one vector for each utterance.
The basis isa subspace of the higher dimensional LSA space,in the same way as a plane or line is a subspace of3D space.
The basis is created by projecting eachutterance vector onto the basis of previous utter-ance vectors using a method known as the Gram-Schmidt process (Anton, 2000).
Each projectedutterance vector has two components, a componentparallel to the basis and a component perpendicularto the basis.
These two components represent ?in-formativity?
and ?relevance?, respectively.
Let usfirst consider ?relevance?.
Since each vector in thebasis is orthogonal, the basis represents all linearcombinations of what has been previously said.Therefore the component of a new utterance vectorthat is parallel to the basis is already representedby a linear combination of the existing vectors.?Informativity?
follows similarly: it is the perpen-dicular component of a new utterance vector thatcan not be represented by the existing basis vec-tors.
For example, in Figure 1, a new utterance cre-ates a new vector that can be projected to the basis,forming a triangle.
The leg of the triangle that liesVS 1VS 2InformativityRelevanceFigure 1.
Projecting a new utterance to the basis973along the basis indicates the ?relevance?
of therecent utterance to the basis; the perpendicular legindicates new information.
Accordingly, a re-peated utterance would have complete ?relevance?but zero new information.5 ProcedureThe task domain is a subset of conversations fromhuman-human computer mediated tutoring ses-sions on Newton?s Three Laws of Motion, inwhich tutor and tutee engaged in a chat room-styleconversation.
The benefits of this task domain aretwofold.
Firstly, the conversations are already tran-scribed.
Additionally, tutors were instructed tointroduce problems using a fixed set of scriptedproblem statements.
Therefore each topic shiftcorresponds to a distinct problem introduced by thetutor.
Clearly this problem would be trivial for acue phrase based approach, which could learn thefinite set of problem introductions.
However, thecurrent lexical approach does not have this luxury:words in the problem statements recur throughoutthe following dialogue.Human to human computer mediated physics tu-toring transcripts first were removed of all markup,translated to lower case, and each utterance wasbroken into a separate paragraph.
An LSA spacewas made with these paragraphs alone, approxi-mately one megabyte of text.
The conversationswere then randomly assigned to training (21 con-versations) and testing (22 conversations).
Theaverage number of utterances per topic, 16 utter-ances, and the average number of words per utter-ance, 32 words, were calculated to determine theparameters of the segmentation methods.
For ex-ample, a moving window size greater than 16 ut-terances implies that, in the majority ofoccurrences, the moving window straddles threetopics as opposed to the desired two.To replicate Foltz et al (1998), software waswritten in Java that created a moving window ofvarying sizes on the input text, and the softwareretrieved the LSA vector and calculated the cosineof each window.
Hearst (1994, 1997) was repli-cated using the JTextTile (Choi, 1999) Java soft-ware.
A variant of Hearst (1994, 1997) was createdby using LSA instead of the standard vector spacemethod.
The orthonormal basis method also useda moving window; however, in contrast to the pre-vious methods, the window is not treated just as alarge block of text.
Instead, the window consistsof two orthonormal bases, one on either side of anutterance.
That is, a region of utterances above thetest utterance is projected, utterance by utterance,into an orthonormal basis, and likewise a region ofutterances below the test utterance is projected intoanother orthonormal basis.
Then the test utteranceis projected into each orthonormal basis, yieldingmeasures of ?relevance?
and ?informativity?
withrespect to each.
Next the elements that make upeach orthonormal basis are aggregated into a block,and a cosine is calculated between the test utter-ance and the blocks on either side, producing atotal of six measures.Each tutoring session consists of the same 10problems, discussed between one of a set of 4 tu-tors and one of 18 subjects.
The redundancy pro-vides a variety of speaking and interaction styleson the same topic.Tutor: A clown is riding aunicycle in a straight line.She accidentally drops an eggbeside her as she continuesto move with constant veloc-ity.
Where will the egg landrelative to the point wherethe unicycle touches theground?
Explain.Student: The egg should landright next to the unicycle.The egg has a constant hori-zontal velocity.
The verti-cal velocity changes anddecreases as gravity pullsthe egg downward at a rate of9.8m/s^2.
The egg shouldtherefore land right next tothe unicycle.Tutor: Good!
There is onlyone thing I would like toknow.
What can you say aboutthe horizontal velocity ofthe egg compared to the hori-zontal velocity of the clown?Student: Aren't they thesame?All of the 10 problems are designed to require ap-plication of Newton?s Laws to be solved, and974therefore conversations share many terms such asforce, velocity, acceleration, gravity, etc.6 ResultsFor each method, the development set was firstused to establish the parameters such as text unitsize and classification criterion.
The methods,tuned to these parameters, were then applied to thetesting data.6.1 Foltz et al (1998)In order to replicate Foltz et al?s results, a text unitsize and window size needed to be chosen.
Theutterance was chosen as the text unit size, whichincluded single word utterances, full sentences, andmulti-sentence utterances.
To determine the mostappropriate window size, results from all sizes be-tween 1 and 16 (the average number of utterancesbetween topic shifts) were gathered.
The greatestdifference between the means for utterances thatintroduce a topic shift versus non-shift utterancesoccurs when the window contains four utterances.The standard deviation is uniformly low for win-dows containing more than two utterances andtherefore can be disregarded in choosing a windowsize.The optimal cosine threshold for classificationwas found using logistic regression (Garson, 2003)which establishes a relationship between the cosinethreshold and the log odds of classification.
Theoptimal cutoff was found to be shift odds = .17with associated F-measure of .49.
The logisticequation of best fit is:cosine)  (-13.345  1.887  odds)ln(shift ?+=F-measure of .49 is 48% higher than the F-measure reported by Foltz et al (1998) for seg-menting monologue.
On the testing corpus the F-measure is .52, which demonstrates good generali-zation for the logistic equation given.
Comparedthe F-measure of .33 reported by Foltz et al(1998), the current result is 58% higher.6.2 Hearst (1994, 1997)The JTextTile software was used to implementHearst (1994) on dialogue.
As with Foltz et al(1998), a text unit and window size had to be de-termined for dialogue.
Hearst (1994) recommendsusing the average paragraph size as the windowsize.
Using the development corpus's averagetopic length of 16 utterances as a reference point,F-measures were calculated for the combinationsof window size and text unit size in Table 1.The optimal combination of parameters (F-measure = .17) is a unit size of 16 words and awindow size of 16 units.
This combinationmatches Hearst (1994)'s heuristic of choosing thewindow size to be the average paragraph length.Table 1.
Unit vs. window size for Hearst methodOn the test set, this combination of parametersyielded an F-measure of .14 as opposed to the F-measure for monologue reported by Hearst (1997),.70.
For dialogue, the algorithm is 20% as effec-tive as it is for monologue.
It is unclear, however,exactly what part of the algorithm contributes tothis poor performance.
The two most obvious pos-sibilities are the segmentation criterion, i.e.
depthscores, or the standard vector space method.To further explore these possibilities, the Hearstmethod was augmented with LSA.
Again, the unitsize and window size had to be calculated.
Aswith Foltz, the unit size was taken to be the utter-ance.
The window size was determined by com-puting F-measures on the development corpus forall sizes between 1 and 16.
The optimal windowsize is 9, F-measure = .22.
Given the smallernumber of test cases, 22, this F-measure of .22 isnot significantly different from .17.
However, theFoltz method is significantly higher than both ofthese, p < .10.6.3 Orthonormal basisThe text unit used in the orthonormal basis is thesingle utterance.
The optimal window size, i.e.
theorthonormal basis size, was determined by creatinga logistic regression to calculate the maximum F-measure for several orthonormal basis sizes.
Thefindings of this procedure are listed in Table 2.Windowsize2 4 8 16 32Unitsize 8 .134 .129 .130 .146 .14416 .142 .133 .130 .171 .14032 .138 .132 .130 .151 .143975Table 2.
F-measure for orthonormal basis sizesF-measure monotonically increases until the or-thonormal basis holds six elements and holds rela-tively steady for larger orthonormal basis sizes.Since F-measure does not increase much over .72for greater orthonormal basis sizes, 6 was chosenas the most computationally efficient size for thestrength of the effect.
The logistic equation of bestfit is:)ityinformativ(2.771)relevance(-2.698)ityinformativ(-23.567)relevance(-30.843)cosine (16.70320.027odds)ln(shift22112?+?+?+?+?+=Where the index of 1 indicates a measure on thewindow preceding the utterance, and an index of 2indicates a measure on the window following theutterance.
In the regression, the cosine betweenthe utterance and the preceding window was notsignificant, p = .86.
This finding reflects the intui-tion that the cosine to the following window variesaccording to whether the following window is on anew topic, whereas the cosine to the precedingwindow is always high.
Additionally, measures of?relevance?
and ?informativity?
correspond to vec-tor length; all other measures did not contributesignificantly to the model and so were not in-cluded.The sign of the metrics illuminates their role inthe model.
The negative sign on the coefficientsfor relevance1, informativity1, and relevance2 indi-cates that they are inversely correlated with an ut-terance signaling the start of a new topic.
The onlysurprising feature is that informativity1 is nega-tively correlated instead of positively correlated:one would expect a topic shift to introduce newinformation.
There is possibly some edge effecthere, since the last move of a topic is often a sum-marizing move that shares many of the physicsterms present in the introduction of a new topic.On the other hand, the positive sign on cosine2 andinformativity2 indicates that the start of a new topicshould have elements in common with the follow-ing material and add new information to that mate-rial, as an overview would.
Beyond the sign, theexponentials of these values indicate how the twobasis metrics are weighted.
For example, wheninformativity2 is raised by one unit, a topic shift is16 times more likely.On the testing corpus the F-measure of the or-thonormal basis method is .67, which is signifi-cantly different from the performance of all threemethods mentioned above, p < .05.
Table 3 com-pares this result with the previous results in thecurrent study for segmenting dialogue.Method Hearst Hearst + LSA FoltzOrth.basisF .14 .22 .52 .67Table 3.
Comparison of dialogue segmentation methods7 DiscussionThe relative ranking of these results is not alto-gether surprising given the relationships betweeninferencing and LSA and between inferencing anddialogue.
Foltz et al (1998) found that LSAmakes simple bridging inferences in addition todetecting lexical cohesion.
These bridging infer-ences are a kind of collocational cohesion (Halli-day and Hassan, 1976) whereby words that co-occur in similar contexts become highly related inthe LSA space.
Therefore in applications wherethis kind of inferencing is required, one might ex-pect an LSA based method to excel.Similarly to van Dijk and Kintsch's model ofcomprehension (van Dijk and Kintsch, 1983), dia-logue can require inferences to maintain coher-ence.
According to Grice's Co-operative Principle,utterances lacking semantic coherence flout theMaxim of Relevance and license an inference(Grice, 1975):S1: Let?s go dancing.S2: I have an exam tomorrow.The "inference" in the sense of Foltz, Kintsch,and Landauer (1998) would be represented by ahigh cosine between these utterances, even thoughthey don't share any of the same words.
Dialoguegenerally tends to be less lexically cohesive andrequire more inferencing than expository mono-Size 3 4 5 6 8 10 15F .59 .63 .65 .72 .73 .72 .73976logue, so one might predict that LSA would excelin dialogue applications.However, LSA has a weakness: the cosinemeasure between two vectors does not changemonotonically as new word vectors are added toeither of the two vectors.
Accordingly, the addi-tion of a word vector can cause the cosine betweentwo text units to dramatically increase or decrease.Therefore the distinctive properties of individualwords can be lost with the addition of more wordsto a text unit.
This problem can be addressed byusing an orthonormal basis (Hu et al, 2003).
Byusing a basis, each utterance is kept independent,so ?inferencing?
can extend over both the entire setof utterances and the linear combination of any ofits subsets.
Accordingly, when ?inferencing?
overthe entire text unit is required, one would expect abasis method using LSA vectors to outperform astandard LSA method.
This expectation has beenput to the test recently by Olney & Cai (2005),who find that an orthonormal basis can signifi-cantly predict entailment on test data supplied bythe PASCAL Textual Entailment Challenge(PASCAL, 2004).Beyond relative performance rankings, moresupport for the above reasoning can be found in thedifference between Hearst and Hearst + LSA.
Re-call that in monologue, Hearst (1997) reports amuch larger F-measure than Foltz et al (1998), .70vs.
.33, albeit on different data sets.
In the presentdialogue corpus, these roles are reversed, .14 vs..52.
Possible reasons for this reversal are the seg-mentation criterion, the vector space method, orthe fact that Foltz has been trained on similar datavia regression and Hearst has not.
However, com-paring the Hearst algorithm with the Hearst + LSAalgorithm indicates that a 57% improvement stemsfrom the addition of LSA, keeping all other factorsconstant.
While this result is not statistically sig-nificant, the direction of the result supports the useof an ?inferencing?
vector space method for seg-menting dialogue.Unfortunately, the large difference in F-measurebetween the Foltz algorithm and the Hearst + LSAalgorithm is more difficult to explain.
These twomethods differ by their segmentation criterion andby their training (Foltz is a regression model andHearst is not).
It may be that Hearst (1994, 1997)?ssegmentation criterion, i.e.
depth scores, do nottranslate well to dialogue.
Perhaps the assignmentof segment boundaries based on the relative differ-ence between a candidate score and its surroundingpeaks is highly sensitive to cohesion gaps createdby conversational implicatures.
On the other handthe differences between these two methods may beentirely attributable to the amount of training theyreceived.
One way to separate the contributions ofthe segmentation criterion and training would be tocreate a logistic model using the Hearst + LSAmethod and to compare this to Foltz.The increased effectiveness of the orthonormalbasis method over the Foltz algorithm can also beexplained in terms of ?inferencing?.
Since ?infer-encing?
is overwhelmed by lexical cohesion (Foltzet al, 1998), the increase in window size for theFoltz algorithm deteriorates performance for awindow size greater than 4.
In contrast, the or-thonormal basis method becomes most effective asthe orthonormal basis size increases past 4.
Thisdichotomy illustrates that the Foltz algorithm is notcomplementary to an ?inferencing?
approach ingeneral.
Use of an orthonormal basis, on the otherhand, increases sensitivity to collocational cohe-sion without sacrificing lexical cohesion.8 ConclusionThis study explored the segmentation of tutorialdialogue using techniques that have previouslybeen applied to expository monologue and using anew orthonormal basis technique.
The techniquespreviously applied to monologue reversed theirroles of effectiveness when applied to dialogue.This role reversal suggests the predominance ofcollocational cohesion, requiring ?inferencing?,present in this tutorial dialogue.
The orthonormalbasis method, which we suggest has an increasedcapacity for ?inferencing?, outperformed both ofthe techniques previously applied to monologue,and demonstrates that segmentation of these tuto-rial dialogues most benefits from a method sensi-tive to lexical and collocational cohesion overlarge text units.AcknowledgementsThis research was supported by the National Sci-ence Foundation (SBR 9720314, REC 0089271,REC 0106965, REC 0126265) and the DoD Mul-tidisciplinary University Research Initiative(MURI) administered by ONR under grantN00014-00-1-0600.
Any opinions, findings, and977conclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of DoD, ONR, or NSF.ReferencesAnton, H.  (2000).
Elementary linear algebra.
8th edi-tion.
New York: John Wiley.Barzilay, R. & Elhadad, M. (1997).
Using LexicalChains for Text Summarization.
Proceedings of theIntelligent Scalable Text Summarization Workshop.Choi, F. (1999).
JTextTile: A free platform independenttext segmentation algorithm.http://www.cs.man.ac.uk/~choifChoi, F. (2000).
Advances in domain independent lineartext segmentation.
In Proceedings of the NAACL?00,May.van Dijk, T. A., & Kintsch, W. (1983).
Strategies ofDiscourse Comprehension.
New York: AcademicPress.Dumais, S. (1993).
LSI meets TREC: a status report.
InProceedings of the First Text Retrieval Conference(TREC1), 137-152.
NIST Special Publication 500-207.Foltz, P.W., Kintsch, W. & Landauer, T.K.
(1998).
Themeasurement of textual cohesion with latent semanticanalysis.
Discourse Processes, 25, 285-307.Galley, M., McKeown, K., Fosler-Lussier, E., & Jing,H.
(2003).
Discourse Segmentation of Multi-PartyConversation.
Proceedings of the ACL.Garson, D. Logistic Regression.
Accessed on April18th, 2003.  http://www2.chass.ncsu.edu/garson/pa765/logisticGraesser, A. C., Person, N. K., Harter, D., & the Tutor-ing Research Group.
(2001).
Teaching tactics anddialogue in AutoTutor.
International Journal of Arti-ficial Intelligence in Education, 12, 257-279.Grice, H.P.
(1975).
Logic and conversation.
In P. Cole& J. Morgan (Eds) Syntax and Semantics Vol 3.
41-58.
New York: Academic.Grosz, B.J.
& Sidner, C.L.
(1986).
Attention, Intentions,and the structure of discourse.
Computational Lin-guistics, 12 (3), 175-204.Halliday, M. A.
& Hassan, R. A.
(1976).
Cohesion inEnglish.
London: Longman.Hearst, M. (1994).
Multi-paragraph segmentation ofexpository text.
In Proceedings of the 32nd meetingof the Association for Computational Linguistics.
9-16.Hearst, M. (1997).
Text-Tiling: segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1), 33-64.Hu, X., Cai, Z., Louwerse, M., Olney, A.,  Penumatsa,P., and Graesser, A.
(2003).
An improved LSA algo-rithm to evaluate contributions in student dialogue.In Proceedings of the Eighteenth International JointConference on Artificial Intelligence (IJCAI-03),1489-1491.Kozima, H. (1993).
Text segmentation based on similar-ity between words.
In Proceedings of ACL '93, 286-288.Landauer, T. & Dumais, S. (1997).
A solution to Plato 'sproblem: the latent semantic analysis theory of acqui-sition, induction, and representation of knowledge.Psychological Review, 104, 211-240.Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
In-troduction to Latent Semantic Analysis.
DiscourseProcesses, 25, 259-284.Manning, C. & Sch?tze, H. (1999).
Foundations of Sta-tistical Natural Language Processing.
Cambridge:MIT Press.Morris, J.
& Hirst, G. (1991).
Lexical Cohesion Com-puted by Thesaural Relations as an Indicator of theStructure of Text.
Computational Linguistics, 17(1),21-48.Olney, A., & Cai, Z.
(2005).
An Orthonormal Basis forEntailment.
In Proceedings of the Eighteenth Interna-tional Florida Artificial Intelligence Research SocietyConference, 554-559.
Menlo Park, Calif.: AAAIPress.PASCAL.
2004.
Recognising Textual Entailment Chal-lenge.
Accessed on October 4th, 2004.http://www.pascal-network.org/Challenges/RTE/Passonneau, R. J.
& Litman, D. J.
(1993).
Intention-based Segmentation: Human Reliability and correla-tion with linguistic cues.
Proceedings of the ACL,148-155.Passonneau, R. J.
& Litman, D. J.
(1997).
Discoursesegmentation by human and automated means.
Com-putational Linguistics, 23(1), 103?139.Salton, G. & Allan, J.
(1994).
Automatic text decompo-sition and structuring.
In Proceedings of RIAO, 6?29,New York, NY.Yaari, Y.
(1997).
Segmentation of expository texts byhierarchical agglomerative clustering.
Proceedings ofthe RANLP'97.978
