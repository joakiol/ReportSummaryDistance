Context Dependent Modeling of Phones in ContinuousSpeech Using Decision TreesL.R.
Bahl, P.V.
de Souza, P.S.
Gopalakrishnan,D.
Nahamoo, M.A.
PichenyIBM Research DivisionThomas J. Watson Research CenterP.O.
Box 704, Yorktown Heights, NY 10598ABSTRACTIn a continuous peech recognition system it is impor-tant to model the context dependent variations in the pro-nunciations of words.
In this paper we present an automaticmethod for modeling phonological variation using decisiontrees.
For each phone we construct adecision tree that spec-ifies the acoustic realization of the phone as a function ofthe context in which it appears.
Several thousand sentencesfrom a natural anguage corpus spoken by several talkers areused to construct hese decision trees.
Experimental resultson a 5000-word vocabulary natural language speech recog-nition task are presented.INTRODUCTIONIt is well known that the pronunciation of a wordor subword unit such as a phone depends heavily onthe context.
This phenomenon has been studied ex-tensively by phoneticians who have constructed sets ofphonological rules that explain this context dependence\[8, 14\].
t\[owever, the use of such rules in recognitionsystems has not been extremely successful.
Perhaps, afundamental problem with this approach is that it re-lies on human perception rather than acoustic reality.Furthermore, this method only identifies gross changes,and the more subtle changes, which are generally unim-portant to humans but may be of significant value inspeech recognition by computers, are ignored.
Possibly,rules constructed with the aid of spectrograms wouldbe more useflfl, but this would be very tedious and dif-ficult.In this paper we describe an automatic method formodeling the context dependence of pronunciation.
\]nparticular, we expand on the use of decision trees formodeling allophonic variation, which we previously out-lined in \[4\].
Other researchers have modeled a.ll distinctsequences o\[ tri-phones (three consecutive phones) inan effort to capture phonological variations \[16, 10\].The method proposed in this paper has the advantagethat it allows us to account for much longer contexts.In the experiments reported in this paper, we modelthe pron0nciation of a phone as a function of the fivepreceding and five \[ollowing phones.
This method alsohas better powers o\[ generalization, i.e.
modeling con-texts tha.t do not occur in the training data,.Use of decision trees for identifying allophones havebeen considered in \[4, 7, 1\], \]5\].
However, apart from\[4\], these methods have either not been used in a rec-ognizer or have not provided significant improvementsover existing modeling methods.In the next section we describe the algorithnls usedfor constructing the decision trees.
In Section 3 wepresent recognition results for a 5000-word natural lan-guage continuous speech recognition task.
We alsopresent results showing the the effect of varying treesize and context on the recognition accuracy.
Conclud-ing remarks are presented in Section 4.CONSTRUCTING THE DECIS ION TREEThe data used for constructing tile decision trees isobtained from a database of 20,000 continuous peechnatural language sentences spoken by 10 different speak-ers.
For more details about this database, see \[4\].
Spec-tral feature vectors are extracted from the speech at arate of 100 frames per second.
These frames are la-beled by a vector quantizer using a common alpha-bet for all the si)eakers.
This data is used to train aset of phonetic Markov models for the words.
Usingthe trained phonetic Markov model statistics and theYit~rbi algorithm, the labeled speech is then a.ligned264against the phonetic basefoims.
This process resultsin an alignment of a sequence of phones (the phonesequence obtained by concatenating the phonetic base-rearms of the words in the entire training script) with theia.bel sequence produced by the w.ctor qua.ntizer, Foreach aligned phone we construct a data record whichcontains the identity of the current phone, denoted asP0, the context, i.e.
the identities of the K previousphones and K following phones in the phone sequence,denoted as P -K , .
.
.P -1 ,P1 , .
.
.P I c ,  and the label se-quence Migned against the current phone, denoted asy.
We partition this collection of data on the basis ofP0- Thus we have collected, for each phone in the phonealphabet, several thousand instances of label sequencesin various phonetic contexts.
Based on this annotateddata we construct a decision tree for each phone.If we had an unlimited supply of annotated data,we could solve the context dependence problem exhaus-tively by constructing a different model for each phonein each possible context.
Of course, we do not haveenough data.
to do this, but even if we could carry outthe exhaustive solution, it would take a large amount ofstorage to store all the different models.
Thus, becauseof limited data, and a need for parsimony, we com-bine the contexts into equivalence classes, and make amodel for each class.
Obviously, each equivalence classshould consist of contexts that result in similar labelstrings.
One effective way of constructing such equiv-alence classes is by the use of binary decision trees.Readers interested in this topic are urged to read Clas-sification and Regression Trees by Breiman, Friedman,Olshen and Stone \[6\].To construct a binary decision tree we begin witha collection of data, which in out case consists of allthe annotated samples for a particular phone.
We splitthis into two subsets, and then split each of these twosubsets into two smaller subsets, and so on.
The split-ting is done on the basis of binary questions about thecontext Pi, for i = :h l , .
.
.
?
K. In order to constructthe tree, we need to have a goodness-of-split evMua-tion function.
We base the goodness-of-split evaluationfunction on a, probabilistic measure that is related tothe homogeniety of a set of label strings.
Finally, weneed some stopping criteria.
We terminate splittingwhen the number of samples a.t a node falls below a.threshold, or if the goodness of the best split falls be-low a threshold.
The result is a binary tree in whicheach terminal node represents one equivalence class ofcontexts.
Using the label strings associated with a ter-minal node we can construct a fenonic Marker model\[or that node by the method described in \[1, 2\].
Duringrecognition, given a. pl, one and its context, we use thedecision tree o{ that phone to determine which modelsho,ld be used.
By answering the questions about thecontext at the nodes of the tree, we trace a path to a.t~rminal tmdp.
of the troo,~ which ~p~eifie.~ the model toI)e used.Let Q denote a set of binary questions about thecontext.
Let n denote a node in the tree, and m(q,n)the goodness of the split induced by question q 6 Q atnode n. We will need to distinguish between tested anduntested nodes.
A tested node is one on which we haveevaluated m(q,n) for all questions q 6 Q and eithersplit the node or designated it as a terminal node.
It iswell-known that the construction of an optimal binarydecision tree is an NP-hard problem.
We use a sub-optimal greedy algorithm to construct he tree, select-ing the best question from the set Q at each node.
Inoutline, the decision tree construction algorithm worksas follows.
We start with all samples at the toot node.In each iteration we select some untested node n andevaluate re(q, n) for all possible questions q 6 Q at thisnode.
If a stopping criterion is met, we declare noden as terminal, otherwise we associate the question qwith the highest value of m(n,q) with this node.
Wemake two new successor nodes.
All samples that an-swer positively to the question q a.~ce t~ansferred to theleft successor and all other samples are transferred tothe right successor.
We repeat these steps till all nodeshave been tested.
'.\['he most important aspects of this algorithm arethe set of questions Q, the goodness-of-split evaluationfunction re(q, n), and the stopping criteria.
We discusseach of these below.The Question SetLet P denote the aiphal)et of phones, and Np thesize of this alphal)et.
In our case Np = 55.
Thequestion set Q consists of questions of the form \[ IsI~ 6 S \] where ,5" C P. We start with singleton sub-sets of P, e.g.
$ = {p}, S = {t}, etc.
In addition,we use subsets corresponding to phonologically mean-inghfl classes of phones commonly used in the analysisof speech \[9\], e.g., $ = {p,t, k} (all unvoiced stops),,5' = {p, t, t;,b,d, g} (all stops), etc.
Each question isapplied to each element /~ for i = / :1 , .
.
.
?
K, of thecontext.
If there are Ns subsets in all, the number ofquestions NQ is given by NQ = 2KN~.
Thus these willbe NQ splits to be evaluated at each node of the tree.In our experiments K = 5 and Ns = 130, leading to a.total of 1300 questions.265Note that, in general, there are 2 NP different sub-sets of P, and, in principle, we could consider all 2K2 Npquestions.
Since this would be too expensive, we havechosen what we consider to be a meaningful subset ofall possible questions and consider only this .fixed setof questions during tree construction.
It is possible togeneralize the tree construction procedure to use vari-able questions which are constructed algorithmically aspart of the tree construction process, as in \[5, 13\].Furthermore, the type of questions we use are calledsimple questions, since each question is applied to oneelement of the context at a time.
It is possible to con-struct complex questions which deal with several con-text elements at once, as in \[5\].
Again, we did not usethis more complicated technique in the experiments re-ported in this paper.The Goodness -o f -Sp l i t  Eva luat ion  Funct ionWe derive the goodness-obsplit evaluation functionbased on a probabilistic model of collections of labelstrings.
Let .M denote a particular class of paramet-ric models that assign probabilities to label strings.For any model M 6 M let PrM(y) denote the prob-ability assigned to label string y.
Let }\]~ be the setof label strings associated with node n. PrM(Yn) =I-Iy~y, PrM(y) is a measure of how well the model Mfits the data at node n. Let Mn 6 .h4 be the best modelfor Yn, i.e.
PrM.
(I~z ) > FrM(}\]~ ) for all M. PrM.
(~ )is a measure of the purity of Y,.
If the label stringsin )~ are similar to each other, then PrM.
(}~) will belarge.
A question q will split the data at node n intotwo subsets based on the outcome of question q. Ourgoal is to pick q so as to make the successor nodes aspure as possible.
Let ~ and Y~ denote the subsets oflabel strings at the left and right successor nodes, re-spectively.
Obviously, )~ U Y~ = 1~.
Let Mt and )lI~be the corresponding best models for the two subsets.\]'henre(q, n) = log ((PrMt(I~)PrM~ (}5)) /PrM.
(1~)) (1)is a measure of the improvement in purity as a resultof the split.
Since our goal is to divide the strings inl, osubsets containing similar strings, this quantity servesus well as the goodness-of-split evaluation f, nction.Since, we will eventually use the strings at a ter-minal node to construct a Markov model, choosing Mto be a class of Markov models would be the naturalchoice.
Unfortunately, this choice of model is compn-tationa.lly very expensive.
To find the best model M,,we would have to train the model, using the forward-backward algorithm using all the data at the node n.Thus for computational reasons, we have chosen a sim-pler class of models - Poisson models of the type usedill \[3\] for the polling fast match.Recall that y is a sequence of acoustic labels al, a2,... at.
We make the simplifying assumption that the\]abels in tile sequence are independent of each other.The extent to which this approximation is inaccuratedepends on the length of the units being modeled.
Forstrings corresponding to single phones, the inaccuracyintroduced by this approximation is relatively small.However, it results in an evaluation function that iseasy to compute and leads to the construction of verygood decision trees in practice.A result of this assumption is that the order inwhich the labels occur is of no consequence.
Now, astring y can be fully characterized by its histogram,i.e.
the n,mber of times each label in the acoustic la-bel alphabet occurs in that string.
We represent hestring y by its histogram YlY>..YF, a. vector of lengthF where F is the size of the acoustic label alphabet andeach Yi is the number of times label i occurs in string y.We model each component Yi of the histogram by anindependent Poisson model with mean rate #i. Then,the probability assigned to y by M ise,..(y) = H (2)i=I Yi!The joint probability of all the strings in the set, Yn isthenFPrM(}~,) = I~ lq #~'e-'~ (3)y~Y.
i=1 Yi!It can be easily shown that PrM(}~ ) is maximized bychoosing the mean rate to be tile sample average, i.e.,the best model for )';, has as its mean rateJIt,,~- N,, ~ yi for i= l ,2 .
, .V  (4)' y615,Let Itli a,,d t*~i for i = 1 ,2 .
.
.
F ,  denote the optimalmean rates for Y} and I'~.
respectively.
Using these ex-pressions iu (1) and eliminating common terms, we canshow that the evaluation function is given byFre(q, n) = ~ {Nl#u log ttli+ N~#~i log #~ii=1- N .#, .
log (5)where Nt is the total number of strings at the left nodea.n(I ArT is the total number of strings at the right node266resulting from split q.
At each node, we select thequestion q that maximizes the evaluation function (5),The evaluation function given in equation (5) isvery general, and arises from several different model as-sumptions.
For example, if we assume that the lengthof each string is given by a Poisson distribution, andthe labels in a string are produced independently by amultinomial distribution, then the evaluation functionof equation (5) results.
There are also some interestingrelationships between this function and a minimizationof entropy formulation.
Due to space \]imitations, thedetails are omitted here.The s topp ing  c r i te r iaWe use two very simple stopping criteria.
If thevalue re(q, n) of the best split at a node n is less thana threshold Tm we designate it to be a terminal node.Also, if the number of samples at a node falls below athreshold T~ then we designate it to be a terminal node.The thresholds Tm and T~ ate selected empirically.Using the Decision Trees During Recogni-tionThe terminal nodes of a tree for a phone correspondto the different allophones of the phone.
We constructa fenonic Markov model for each terminal node fromthe label strings associated with the node.
The detailsof this procedure are described in \[1, 2\].During recognition, we construct Markov models forword sequences as follows.
We construct a sequence ofphones by concatenating the phonetic baseforms of thewords.
For each phone in this sequence, we use theappropriate decision tree and trace the path in the treecorresponding to the context provided by the phonesequence.
This leads to a terminal node, and we usethe fenonic Markov model associated with this node.By concatenating the fenonic Markov models for eachphone we obtain a Markov model for the entire wordsequence.For the last few phones in the phone sequence, theright context is not fully known.
For these phones,we make tentative models ignoring the unknown rightcontext.
When the sequence of words is extended, theright context for these phones will be available, and wecan replace the tentative models by the correct modelsand recompute the acoustic match probabilities.
Thisprocedure is quite simple and the details are omittedhere.EXPERIMENTAL RESULTSWe to~t~d tbi.
m~tho,l o11 ~ 5000.~word, continuo.~speech, natural language task.
The test vocabularyconsists of the 5000 most frequent words taken froma large quantity of IBM electronic mail.
The trainingdata consisted of 2000 sentences read by each of 10 dif-ferent talkers.
Tile first 500 sentences were the samefor each talker, while the other 1500 were different fromtalker to talker.
The training sentences were coveredby a 20,000 word vocabulary and the allophonic modelswere constructed for this vocabulary.
The test set con-sisted of 50 sentences (591 words) covered by our 5000word vocabulary, Interested readers can refer to \[4\] \[ormore details of the task and the recognition system.We constructed tire decision trees using the train-ing data.
described above.
The phone alphabet was ofsize 55, ff was chosen to be 5, and on the average thenumber of terminal nodes per decision tree and con-sequently the number of allophones per phone was 45.We tested the system with 10 talkers.
The error ratesreported here are \[or the same l0 talkers whose ut-terances were ,sed for constrncting the decision trees.Each talker provided roughly 2,000 sentences of train-ing data for constructing the vector quantizer proto-types and for training the Markov model parameters.Tests were also done using context independent pho-netic models.
In both, the same vector quantizer pro-totypes were used and the models were trained usingthe same data.
Table l shows the error rates for thephonetic (context independent) and aUophonic (con-text dependent) models for the \]0 talkers.
On the aver-age, the word error rate decreases from 10.3% to 5.9%.We tested the performance ofour Mlophonic modelsfor talkers who were not part o\[ the training database.The error rates for five new test talkers using the a,llo-pho,ic models are shown in Table 2.
As can be seen,the error rates obtained using the allophonic modelsare coral)arable to those given in Table \].We also trained and decoded using triphone-basedHMMs \[16\].
In these experiments, only intra-word tri-phone models were used; we did not attempt o con-struct cross-word triphone models.
The number of pho-netic models in our system is 55; approximately 10000triphone models were required to cower our 20000 wordvocabulary.
No attempt was made to cluster theseinto a smaller number as is done for generMized tri-phones \[10\].
Both phonetic and triphone models weretrained using the forward-backward algorithm in theusual manner; the triphone statistics were smoothedback onto the underlying phonetic models via deleted267estimation.
The topology of the triphone and and pho-netic models were seven-state models with independentdistributions for the beginning, middle, a.nd end of eachphone as described in \[12\].
Results are shown in thefourth column of Table 1.
These results are signifi-cantly worse than the results obtained with our allo-phonic models.
However, it should be noted that thesetri-phone models do not incorporate several techniquesthat are currently in use \[11\].Varying Context and Tree SizeThe number of preceding and following phones thatare used in the construction of the decision tree influ-ences the recognition performance considerably.
Weconstructed several decision trees that examine differ-ent amounts of context and the recognition error ratesobtained using these models is shown in Table 3.
Thesecond column shows results for models constructed us-ing decision trees that examine only one phone preced-ing and following the current one.
The third columnshows results for trees that examine two phones pre-ceding and following the current phone and so on tothe last column for trees that examine five precedingand following phones.
The stopping criterion used inall cases was the same, as was the training and test set.These results show that increasing the amount of con-text information improves the recognition performanceof the system using these models.An important issue in constructing decision treesis when to stop splitting the nodes.
As we generatemote and more nodes, the tree gets better and betterfor the training data but may not be appropriate fornew data.
In order to find an appropriate tree size, weconducted several decoding experiments using modelsconstructed from decision trees of various sizes builtfrom the same training data.. We constructed ecisiontrees of different sizes using the following scheme.
Wefirst constructed a set of decision trees using the algo-rithms given in Section 2, but without using the stop-ping criterion based on the goodness-of-split evaluationfunction.
The splitting is terminated only when we areleft with one sample at a node or when all samplesat a node have identical context so that no questioncan split the node.
The context used consisted of the5 preceding and following phones.
Now, sets of treesof varying sizes can be obtained from these large treesby pruning.
We store the vMue of the goodness-of-splitevaluation function re(q, n) obtained at each node.
Thetree for each phone is pruned back as follows.
We ex-amine all nodes n both of whose successor nodes areterminal nodes.
From among these we select the noden* which has the smallest value for the evaluation Nnc-tion re(q, n*).
If this va.lue is less than a theshold Tmwe discard this split, and mark the node n* as a leaf.This process is repeated until no more pruning can bedone.
By varying the pruning threshold ~n, we canobtain decision trees with different number of nodes.Table 4 shows the decoding error rates using modelsobtained for trees of various sizes.
The second columnshows the results obtained with trees having an aver-age of 23 terminal nodes (allphones) per phone.
Thethird, fourth, and fifth columns how the error rates for33, 45, and 85 Mlophones per phone respectively.
Thetraining and test sets were the same as that describedearlier in this section.
As can be seen, increasing thenumber of Mlophones beyond 45 did not result in in-creased accuracy.CONCLUSIONSAcoustic models used in continuous peech recog-nition systems should account for variations in pro-nunciation arising from contextual effects.
This paperdemonstrates that such effects can be discovered au-tomatically, and represented very effectively using bi-nary decision trees.
We have presented a method forconstructing and using decision trees for modeling allo-phonic variation.
Experiments with continuous peechrecognition show that this method is effective in reduc-ing the word error rate.REFERENCES\[1\] L.R.
Baltl, P.F.
Brown, P.V.
de Souz~, R.L.
Mercer,M.A.
Pieheny, "Automatic Construction of AcousticMarkov Models for Words," Proc.
International Sym-posium on SignM Processing and Its Applications, Bris-ha,e, Australia, \]987, pp.565-569.\[2\] L.R.
Bald, P.F.
Brown, P.V.
de Souza, R.L.
Mercer, andM.A.
Picheny, "Acoustic Markov Models Used in theTangora Speech Recognition System," Proc.
ICASSP-88, New York, NY, April 1988, pp.
497-500.\[3\] L.R.
Bahl, R. Bakis, P.V.
de Souza., and R.L.
Mercer,"Obt~htlng Candidate Words by Polling in a Large Vo-cab,lary Speech Recognition System," Proc.
ICASSP-88, New York, NY, April 1988, pp.
489-492.\[4\] L.R.
Bahl et.
al.,"Large Voca.bulary Natural LanguageContinuous Speech Recognition," Froc.
ICASSP-89,Glasgow, Scotland, May 1989, pp.465-467\[5\] L.R.
Bahl, P.F.
Brown, P.V.
de Souza, R.L.
Mercer,"A Tree-Based Language Model for Natural LanguageSpeech Recognitiop," IFBEE Transactions on ASSP, Vol.37, No.
7, July 1989, pp.1001-1008.\[6\] L. Breiman, J.}t.
Friedman, R.A. Olshen, C.J.
Stone,6'lass~fication a d Regression Trees, Wadsworth Statis-tlcs/Prob~bility Series, Behnont, CA, 1984.268SpeakerT1 10.5T2 17.8T3 13.4T4 12.5T5 2.9T6 14.4T7 8.3T8 3.2T9 5.9T10 13.7AverageModels UsedPhonetic I Allophonie \[ 'l~iphone8.39.56.67.61.97.83.63.03.67.68.612.09.08.82.011.78.52.95.910.210.3% 5.9% 8.0%Table 1: Recognition Error RateSpeaker Error rate with Allophonic ModelsT l lT12TI3T14T153.23.58.16.95.5Average 5.4%Table 2: Error Rate on New Set of Test Talkers\[7\] F.R.
Chen, J. Shrager, "Automatic Discovery of Con-textual Factors Describing Phonological Variation",Proc.
1989 DARPA Workshop on Speech and NaturalLanguage.\[8\] P.S.
Cohen and R.L.
Mercer, "The Phonological Com-ponent of an Automatic Speech Recognition System,"in Speech Recognition, D.R Reddy, editor, AcademicPress, NewYork, 1975, pp.275-320.\[9\] G. Pant, Speech Sounds and Features, MIT Press, Cam-bridge, MA, 1973.\[10\] K.F.
Lee, H.W.
Hon, M.Y.
Hwang, S. Mahajan, R.Reddy, "The Sphinx Speech Recognition System," ProcICASSP-89, Glasgow, Scotland, May 1989, pp.445-448\[11\] K.F.
Lee, et.
al., "Allophone Clustering for ContinuousSpeech Recognition", Proc.
ICASSP-90, Albuquerque,NM, April 1990, pp.749-752.\[12\] B. MSrialdo, "Multilevel decoding for very large sizedictionary speech recognition," IBM Journal of Re-search and Development, vol.
32, March 1988, pp.
227-237.\[13\] A. Nadas, D. Nahamoo, M.A.
Picheny and J. Powell,"An Iterative Flip-Flop Approximation of the Most In-formative Split in the Construction of Decision Trees,"Proc ICASSP-91, to appear.\[14\] B.T.
Oshika, V.W.
Zue, R.V.
Weeks, H. Nue andJ.
Auerbach, "The Role of Phonological Rules inSpeech Understanding Research," IEEE Transactionson ASSP, Vol.
ASSP-23, 1975, pp.
104-112.\[15\] M.A.
Randolph, "A Data-Driven Method for Dis-covering and Predicting Allophonic Variation", Proc.ICASSP-90, Albuquerque, NM, April 1990, pp.1177-1180.\[16\] R. Schwartz, Y. Chow, O. Kimball, S. Roucos, M. Kras-net, J. Makhoul, "Context-Dependent Modeling forAcoustic-Phonetic Recognition of Continuous Speech,"Proc.
ICASSP-85, April 1985SpeakerT1T2T3T4T5T6T7T8T9T10AverageAmount of Context UsedK=J K=2\ [K=3 K=5 J7.1- 9.5 I 8.611.7 161i5 9.07.8 6.87.8 6:6 7.49.5 \] 8.36.8 \[ 5.9 4.24.6 I 2.7 \[ 4.44.7 4.1 4.67.8 _~ 7.4 6.66 5% 6:28.3 I9.56.67.61.97.83.63.03.67.65.9%Table 3: Error Rates with Varying Context LengthSpeakerTlT2T3T4T5T6T7T8T9T10AverageAverage Number of Allophones23 133  45 85- - - - - -4 - - - ' - - - -9.6 19.1 8.3 8.011.2 I 10.8 9.5 9.68.1 16.8 6.6 5.87.8 17.6 7.6 6.41.91 1.9 1.9 3.29.3 I 9.0 7.8 7.85.4 I 4.9 3.6 4.13.4 I 4.2 3.0 3.24 .913 .9  3.6 4.46.8 J 6.3 7.6 6.96--.8%i 6.4% 5.9% 5.9%Table 4: Error Rates for Different Tree Sizes269
