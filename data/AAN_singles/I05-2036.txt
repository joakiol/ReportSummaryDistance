Representing semantics of texts - a non-statistical approachSvetlana HensmanSchool of ComputingDublin Institute of TechnologyKevin StreetDublin 8, IrelandSvetlana.Hensman@comp.dit.ieJohn DunnionIntelligent Information Retrieval GroupDepartment of Computer ScienceUniversity College DublinBelfield, Dublin 4, IrelandJohn.Dunnion@ucd.ieAbstractThis paper describes a non-statisticalapproach for semantic annotation ofdocuments by analysing their syntaxand by using semantic/syntactic behav-iour patterns described in VerbNet.
Weuse a two-stage approach, firstly iden-tifying the semantic roles in a sen-tence, and then using these roles to rep-resent some of the relations betweenthe concepts in the sentence and a listof noun behaviour patterns to resolvesome of the unknown (generic) rela-tions between concepts.
All outlinedalgorithms were tested on two corporawhich differs in size, type, style andgenre, and the performance does notvary significantly.1 IntroductionThis paper describes a system for semi-automaticconceptual graph acquisition using a combina-tion of linguistic resources, such as VerbNet andWordNet, together with semi-automatically com-piled domain-specific knowledge.
Such seman-tic information has a number of possible applica-tions, for example in the area of information re-trieval/extraction for enhancing the search meth-ods or in question-answering systems, allowingusers to communicate with the system in naturallanguage (English).We use conceptual graphs (CGs) (Sowa, 1984),a knowledge-representation formalism based onsemantic networks and the existential graphs ofC.
S. Pierce, to represent the semantics of doc-uments.
There are number of systems for gen-erating conceptual graphs representation of sen-tences: Sowa and Way (Sowa and Way, 1986)use a lexicon of canonical graphs which are com-bined to build a conceptual graph representationof a sentence, while Veraldi at al.
(Velardi et al,1988) describe a prototype of a semantic proces-sor for Italian sentences, which uses a manuallyacquired lexicon of about 850 word-sense defin-itions, each including 10 ?
20 surface semanticpatterns (SSPs) representing both usage informa-tion and semantic constraints.There are also systems aimed at extracting par-tial knowledge from texts, by either filling seman-tic templates (Hobbs et al, 1996) or by generatinga set of linguistic patterns for information extrac-tion (Harabagiu and Maiorano, 2000), to namebut a few.The following sections describe in more detailthe various aspects of our system, the experimentsthat we carried out to test the proposed algorithmsand finally draw some conclusions.2 System overviewWe use a two-step approach for constructing con-ceptual graph representations of texts: firstly,by using VerbNet and WordNet, we identifythe semantic roles in a sentence, and secondly,using these semantic roles and a set of syn-tactic/semantic rules we construct a conceptualgraph.To evaluate our algorithms we use test docu-ments from two corpora in different domains ?the Reuters-21578 text categorization test collec-209tion (Reuters, 1987) and the collection of aviationincident reports provided by the Irish Air Acci-dent Investigation Unit (AAIU) (Air Accident In-vestigation Unit, 2004).
All documents are parsedusing Eugene Charniak?s maximum entropy in-spired parser (Charniak, 2000).3 Semantic role identificationThere are number of different existing ap-proaches for identifying semantic roles, varyingfrom traditional parsing approaches, for exam-ple using HPSG grammars and Lexical Func-tional Grammars, that strongly rely on manually-developed grammars and lexicons, to data-drivenapproaches, for example AutoSlog (Riloff andSchmelzenbach, 1998).
In the domain of the AirTraveler Information System (Miller et al, 1996)the authors apply statistical methods to computethe probability that a constituent can fill in a se-mantic slot within a semantic frame.
Gildea andJurafsky (Gildea and Jurafsky, 2002) describe astatistical approach for semantic role labelling us-ing data collected from FrameNet by analysing anumber of features such as phrase type, grammat-ical function, position in the sentence, etc.Shi and Mihalcea (Shi and Mihalcea, 2004)propose a rule-based approach for semantic pars-ing using FrameNet and WordNet.
They extractrules from the tagged data provided by FrameNet,which specify the realisation (order and differentsyntactic features) for the present semantic roles.They also create a feature set representation ofthe sentence and match it to each of the extractedrules.
The result is the rule providing the mostfeature matches.
The authors do not provide anyinformation on how they select between differentmatches with the same score, or if there is anysemantic check on suitability of a phrase to re-alise a semantic role (FrameNet does not provideany restrictions on the semantic roles similar tothe selectional restrictions present in VerbNet).The approach we propose for semantic roleidentification uses information about each verb?sbehaviour, provided in VerbNet, and the Word-Net taxonomy to decide whether a phrase can bea suitable match for a semantic role.VerbNet (Kipper et al, 2000) is a computa-tional verb lexicon, based on Levin?s verb classes,that contains syntactic and semantic informationfor English verbs.
Each VerbNet class defines alist of members, a list of possible thematic roles,and a list of frames (patterns) of how these se-mantic roles can be realized in a sentence.WordNet (Fellbaum, 1998) is an English lex-ical database containing about 120 000 entriesof nouns, verbs, adjectives and adverbs, hier-archically organized in synonym groups (calledsynsets), and linked with relations such as hyper-nym, hyponym, holonym and others.To identify the semantic roles for a clause in asentence we identify and match the clause patternto each of the possible semantic frames for theclause verb (from VerbNet).
The result is a listof all possible semantic role assignments, fromwhich we must identify the correct one.3.1 Constructing sentence patterns for theverbs in a sentenceFor each sentence clause we construct a syntac-tical pattern, which is a flat parse representationthat identifies the main verb and the other maincategories of the clause.
As a sentence can havesubordinate clauses, we usually have more thanone syntactic pattern per sentence.
Each such pat-tern is processed individually.Using a constituency parser (such as Char-niak?s) is suitable in the majority of cases, butthere are some sentences where the correct set ofrole fillers cannot be identified by using the parsetree.
For example, for sentences such asThe price of oil will rise by 5 cents bythe end of the year.the phrase the price of oil will be identified asa possible role filler by our system, while the cor-rect result would have the price identified as theAttribute and oil as the Patient.
For such cases theuse of a dependency parser (such as a Link Gram-mar parser or a Functional Dependency Grammarparser) would be required.We also address some simple cases of pronounanaphoric reference.
For example, for patternssuch asIomega Corp said it has laid off over aquarter of its professional and manage-ment staff.210we identify the pronoun it as referring to thesubject of the verb in the main clause (which hereis Iomega Corp) if they agree by gender and num-ber.
In cases where the type of the concept repre-sented by the phrase is known, an agreement bytype is also required.Some cases of intersentential pronounanaphoric references are also resolved byanalysing the previous sentence context forsuitable candidates, that agree by gender, numberand type.
Agreement by type is present if thetype of the phrase is compatible (or the same) asthe type of the phrase it references.
For example,if the company refers to Iomega Corp, which islisted as an instance of the type organization, thenthe types of the two phrases are compatible, ascompany is defined as sub-type of organization.If agreement by type cannot be assured, thereference is not resolved.
The reference is onlyresolved if there is a single possibility for itsresolution.3.2 Extracting VerbNet semantic role framesEach verb can be described in VerbNet as a mem-ber of more than one class, and therefore the listof its possible semantic frames is a combinationof the semantic frames defined in each of theclasses in which it participates.We extract all the semantic frames in a classand consider them to be possible semantic framesfor each of the verbs that are members of thisclass.
Each verb class also defines a list of se-lectional constraints for the semantic roles.
Forexample, for all the verbs that are members of theVerbNet class get-13.5.1 one of the possible se-mantic role frames is:Agent[+animate OR +organization] V ThemePrep(from) Source[+concrete].The selectional constraints check is imple-mented using one or a combination of the fol-lowing techniques: hypernym relations defined inWordNet, pattern matching techniques, syntacticrules and some heuristics.3.3 Matching algorithmThe matching algorithm matches the sentencepattern against each of the possible semantic roleframes extracted from VerbNet.
We match theconstituents before and after the verb in the sen-tence pattern to the semantic roles before and afterthe verb in the semantic role frame.If the number of the available constituents inthe sentence pattern is less than the number ofthe required slots in the frame, the match fails.If there is more than one constituent available tofill a slot in a semantic frame, each of them isconsidered a different match.
If, for a seman-tic frame, we find a constituent for each of thesemantic role slots that complies with the selec-tional constraints, the algorithm considers this apossible match.Multiple results are identified when there aretwo or more phrases in a sentence that are possi-ble semantic role realisations, or if there are twoor more semantic frames for which matches werefound.
To select the correct role assignment weuse a weighting function that assigns scores toeach result and returns the one with the highestscore.
For each identified role the weighting func-tion adds one point if the role does not have anyselectional restrictions, and two points if there arerestrictions (including prepositional restrictions).The total score for a solution is the sum of thescores for each identified roles.
The solution withthe highest score is selected.For example, for the sentenceUSAir bought Piedmont for 69 dlrscash per share.the algorithm identifies two possible role as-signments:Agent[+animate OR +organization]matching NP(The company)Theme[] matching NP(the shares)Asset[+currency] matching PP(for 69dlrs cash per share)with weightframe1 = 2 + 1 + 2 = 5 and thesecond solutionAgent[+animate OR +organization]matching NP(The company)Theme[] matching the NP(the shares)with weightframe5 = 2 + 1 = 3Therefore, the algorithm returns the first set ofrole assignments as a result.2114 Building conceptual graphsThe conceptual graph representation of the sen-tence is built through the following steps: firstly,for each of the constituents of the sentence we re-cursively build a conceptual graph representation;then we link all the conceptual graphs represent-ing the constituents into a single graph; and fi-nally, we resolve the unknown (generic) relations.Each of these steps is described in more detail inthe following sub-sections.4.1 Building a conceptual graphrepresentation of a phraseThe first step involves building a conceptual graphfor a phrase.
Our general assumption is thateach lexeme in the sentence is represented us-ing a separate concept, therefore all nouns, adjec-tives, adverbs and pronouns are represented usingconcepts, while the determiners and numbers areused to specify the referent of the relevant concept(thus further specifying the concept).Below we illustrate the procedure for buildinga conceptual graph for some of the most commontypes of phrases. NP -> DT JJ NNFor phrases following this pattern we createtwo concepts - one for the NNwith a conceptreferent corresponding to the type of the de-terminer DT, and another concept represent-ing the adjective, and link both of them byan Attribute relation.
If the phrase containsmore than one adjective, each of them is rep-resented by a separate concept and they areall linked with Attribute relations to the con-cept representing the noun. NP -> NP , SBAR ,This pattern represents phrases where thenoun is further specified by the SBAR (forexample, The co-pilot, who was acting asa main pilot, landed the plane.)
For thesepatterns a conceptual graph is built for theSBAR and the head concept, if a WHNPphrase (e.g.
which or who), is replaced bythe concept created for the NP. PP -> IN NPFor such prepositional phrases we constructa conceptual graph representing the nounphrase.
We also keep track of the prepositionheading the prepositional phrase, as it is usedto mark the relation between this phrase andthe other relevant phrases in the sentence.4.2 Attaching all constituents to the verbOnce the graphs for each of the constituents areconstructed they are linked together in a singleconceptual graph.
As each of them describessome aspect of the concept represented by theverb, we link them to that concept.If the constituent already has an identified se-mantic role during the previous phase, the samerelation is used when constructing the concep-tual graph between the CG representing the con-stituent and the verb.
If the constituent does nothave any semantic roles identified, a relation witha generic label is used, which allows us to buildthe structure of the CG concentrating on the con-cepts involved, and to resolve the generic labelsat a later stage.
The generic labels used are ei-ther REL, or in the case of prepositional phrasesheaded with a proposition prep, REL prep (e.g.REL on).4.3 Resolving unknown relationsFinally we resolve some of the unknown (generic)relations in the conceptual graph.
We keep a data-base of the most common syntactic realisation ofrelations between concepts with specific types.An example of a relation correction rule is:Flight REL from City -> Flight Source Citywhere the left part of the rule represents the twoconcepts linked by a generic relation and the rightside represents the graph after the modification.All generic relations present after this step mustbe manually resolved by the user.
The system of-fers help by suggesting possible relations intro-duced by a preposition.
For example, the preposi-tion for can indicate Beneficiary (e.g.
a book forMary), Duration (e.g.
for three hours), etc.5 Query representationRepresentation of questions differs than represen-tation of declarative sentences and deserves spe-cial attention.
For sentences representing ques-tions we try to identify the statement that will212correspond to the question and then construct theconceptual graph in a similar way as for declara-tive sentences.5.1 Yes/No questionsWe process simple yes/no questions (questionsthat require a yes/no answer) that are constructedby a subject-verb inversion by applying a trans-formation to reverse the question to a declarativesentence.5.2 Wh questionThe parse tree of a sentence expressing awh question has the following general struc-ture: SBARQ ->WH phrase SQ ?
where theWH phrase is either WHNP, WHADVP or WHPPand represents the concept that triggers the query.The SQ represents the rest of the sentence.Similarly to yes/no questions, these type ofquestions are also transformed to declarative sen-tences.
The wh word (e.g.
who, what, where,when) is represented by a generic concept.
Therelation that attaches this concept to the verb de-pends on the type of the wh phrase and can be oneof the following:WHNPThese phrases are headed by the wh ques-tion words who, what or which.
The rela-tion between the wh phrase and the verb iseither identified from applying a suitable se-mantic frame for this verb, or it is a genericone, REL.WHADVPThese phrases represent an adverbial modi-fier for time, place or location.
If the phrasemarked as WHADVP is where the relation islocative; if it is when, the relation is tempo-ral; and if it is how, the relation is manner.WHPPSuch phrases are not processed by our sys-tem.6 Experimental resultsEach module of the system was evaluated sepa-rately.The first experiment we carried out was to es-timate the accuracy of the sentence frame con-structed by the role labelling module and it wasperformed on randomly selected 2% of the verbsin Reuters and 7% of the verbs in AAIU corpora.The parse trees produced by Charniak?s parserwere manually edited to avoid any errors due toincorrect parses.
The results showed that the sys-tem identified the correct set of possible candi-dates for semantic roles for 90% and 89% of theverbs in the Reuters and in the AAIU documentsrespectively.Further experiments were carried out to eval-uate the performance of the role assigning mod-ule.
As a testbed we randomly selected 2% ofthe verbs in Reuters and 15% of the verbs inthe AAIU documents.
From these, we analysedonly those cases where the verb is a member ofat least one VerbNet frame and the possible rolecandidates were correctly identified.
For 60% and70% of the remaining verbs, respectively, the al-gorithm identifies a single correct solution.
In3% and 4% of the cases respectively a partiallycorrect result is found (in the majority of suchcases it is Agent, Patient and Theme roles that arecorrectly identified, together with some incorrectones).In 11% and 9% of the cases for Reuters andAAIU, respectively, the algorithm identifies a setof possible solutions, containing the correct andseveral incorrect ones.
For these cases the weight-ing function identifies the correct solution in 38%of the of the cases for AAIU documents and 59%of the cases for the Reuters documents, while in40% and 21% of the cases, respectively, it identi-fies the correct and one or more incorrect results.We also evaluated the percentage of the syntac-tic patterns that the graph builder recognises: forAAUI and Reuters documents, respectively, wecan build a graph for 76% and 67% of the nounphrases, for 95% and 94% of the prepositionalphrases and for 91% and 97% of the subordinateclauses.7 ConclusionsIn this paper we have described an approachfor constructing conceptual graphs for Englishsentences, using VerbNet, WordNet and somedomain-specific knowledge.
The achieved accu-213racy is strongly influenced by the lack of VerbNetdescriptions of many verbs present in both cor-pora, as well as the lack of semantic frames forthe present verb sense.
Also, as the approach isnot statistical, it does not require large amount oftraining data.There are several other lexical resources cur-rently available that seem suitable for semanticrole identification, among them FrameNet andPropBank.
Our choice of VerbNet as a lexical re-source is based on our belief that a set of domain-independent descriptive role labels (such as thosedefined in VerbNet) is more suitable as it allowsfor generalisations.A drawback of both FrameNet and PropBankis that the roles do not include any selectionalrestrictions, which makes it hard for a non-statistical method to identify the correct filler foreach role.
As shown earlier, the selectional re-strictions defined for the semantic roles prove tobe a valuable asset when deciding if a phrase canbe a role filler.
While we can resolve the majorityof them by analysing the syntactic structure or byusing the WordNet hierarchy, some are more dif-ficult to resolve.
For example, the restriction soliddescribes an attribute or a state of an object, rela-tions which cannot be checked by using WordNet.FrameNet on the other hand defines usages notonly for verbs, but also for nouns.
As one of thecauses for the relatively poor performance of theconceptual graph building module is the lack of asufficient number of relation-correction rules, ourcurrent approach to increasing their number is try-ing to extract the rules from FrameNet.Work on the system is ongoing and effortsare continuing to implement a verb sense disam-biguation component.ReferencesAir Accident Investigation Unit.
2004.
Irish Air Acci-dent Investigation Unit Reports.
Available online:(http://www.aaiu.ie/).Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of NAACL-2000,pages 132?139.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, May.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28(3):245?288.Sanda Harabagiu and Steven Maiorano.
2000.
Ac-quisition of linguistic patterns for knowledge-basedinformation extraction.
In Proceedings of LREC-2000, Athens, June.Jerry Hobbs, Douglas Appelt, John Bear, David Is-rael, Megumi Kameyama, Mark Stickel, and MabryTyson.
1996.
FASTUS: A cascaded finite-statetransducer for extracting information from natural-language text.
In In Finite State Devices for NaturalLanguage Processing, Cambridge, MA.
MIT Press.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-Based Construction of a Verb Lexicon.In Proceedings of Seventeenth National Conferenceon Artificial Intelligence (AAAI-2000), pages 691 ?696, Austin, TX, July 30 - August 3.Scott Miller, David Stallard, Robert Bobrow, andRichard Schwartz.
1996.
A fully statistical ap-proach to natural language interfaces.
In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 55?61, SantaCruz, June.
Morgan Kaufmann Publishers, Inc.Reuters.
1987.
Reuters-21578 Text Cate-gorization Collection.
Available online:(http://kdd.ics.uci.edu/databases/reuters21578/-reuters21578.html).Ellen Riloff and Mark Schmelzenbach.
1998.
An Em-pirical Approach to Conceptual Case Frame Acqui-sition.
In Proceedings of the Sixth Workshop onVery Large Corpora.Lei Shi and Rada Mihalcea.
2004.
Open Text ParsingUsing FrameNet and WordNet.
In Daniel MarcuSusan Dumais and Salim Roukos, editors, Proceed-ings of HLT-NAACL 2004: Demonstration Papers,pages 247?250, Boston, Massachusetts, USA, May2 ?
May 7.
Association for Computational Linguis-tics.John F. Sowa and Eileen C. Way.
1986.
Imple-menting a semantic interpreter using conceptualgraphs.
IBM Journal of Research and Develop-ment, 30(1):57?69, January.John F. Sowa.
1984.
Conceptual Structures: Infor-mation Processing in Mind and Machine.
Addison-Wesley, Reading, M.Paola Velardi, Maria Teresa Pazienza, and Mario De-Giovanetti.
1988.
Conceptual graphs for the analy-sis and generation of sentences.
IBM Journal of Re-search and Development, 32(2):251?267,March.214
