Proceedings of the 7th Workshop on Statistical Machine Translation, pages 388?394,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsGHKM Rule Extraction and Scope-3 Parsing in MosesPhilip Williams and Philipp KoehnSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEH8 9AB, UKp.j.williams-2@sms.ed.ac.ukpkoehn@inf.ed.ac.ukAbstractWe developed a string-to-tree system forEnglish?German, achieving competitive re-sults against a hierarchical model baseline.We provide details of our implementation ofGHKM rule extraction and scope-3 parsingin the Moses toolkit.
We compare systemstrained on the same data using different gram-mar extraction methods.1 IntroductionOver the last few years, syntax-based rule extractionhas largely developed along two lines, one originat-ing in hierarchical phrase-based translation (Chiang,2005; Chiang, 2007) and the other in GHKM (Gal-ley et al, 2004; Galley et al, 2006).Hierarchical rule extraction generalizes the estab-lished phrase-based extraction method to produceformally-syntactic synchronous context-free gram-mar rules without any requirement for linguistic an-notation of the training data.
In subsequent work, theapproach has been extended to incorporate linguis-tic annotation on the target side (as in SAMT (Zoll-mann and Venugopal, 2006)) or on both sides (Chi-ang, 2010).In contrast, GHKM places target-side syntacticstructure at the heart of the rule extraction process,producing extended tree transducer rules that mapbetween strings and tree fragments.Ultimately, both methods define rules accordingto a sentence pair?s word-alignments.
Without anyrestriction on rule size they will produce an expo-nentially large set of rules and so in practice onlya subgrammar can be extracted.
It is the differingrule selection heuristics that distinguish these twoapproaches, with hierarchical approaches being mo-tivated by phrasal coverage and GHKM by target-side tree coverage.The Moses toolkit (Koehn et al, 2007) has in-cluded support for hierarchical phrase-based rule ex-traction since the decoder was first extended to sup-port syntax-based translation (Hoang et al, 2009).In this paper we provide some implementation de-tails for the recently-added GHKM rule extractorand for the related scope-3 decoding algorithm.
Wethen describe the University of Edinburgh?s GHKM-based English-German submission to the WMTtranslation task and present comparisons with hier-archical systems trained on the same data.
To ourknowledge, these are the first GHKM results pre-sented for English-German, a language pair with ahigh degree of reordering and rich target-side mor-phology.2 GHKM Rule Extraction in MosesA basic GHKM rule extractor was first developedfor Moses during the fourth Machine TranslationMarathon1 in 2010.
We have recently extended itto support several key features that are described inthe literature, namely: composition of rules (Gal-ley et al, 2006), attachment of unaligned sourcewords (Galley et al, 2004), and elimination of fullynon-lexical unary rules (Chung et al, 2011).We provide some basic implementation details inthe remainder of this section.
In section 4 we present1http://www.mtmarathon2010.info388TOPS-TOPPDSdasVAFINistNP-PDARTderNNFallPP-MNRAPPRvonPN-NKNEAlexanderNENikitinPUNC..it is the case of Alexander Nikitin .Figure 1: Sentence pair from training data.experimental results comparing performance againstMoses?
alternative rule extraction methods.2.1 Composed RulesComposition of minimal GHKM rules into larger,contextually-richer rules has been found to signif-icantly improve translation quality (Galley et al,2006).
Allowing any combination of adjacent min-imal rules without restriction is unfeasible and soin practice various constraints are imposed on com-position.
Our implementation includes three con-figurable parameters for this purpose, which wedescribe with reference to the example alignmentgraph shown in Figure 1.
All three are defined interms of the target tree fragment.Rule depth is defined as the maximum distancefrom the composed rule?s root node to any othernode within the fragment, not counting preterminalexpansions (such as NE ?
Nikitin).
By default, therule depth is limited to three.
If we consider thecomposition of rules rooted at the S-TOP node inFigure 1 then, among many other possibilities, thissetting permits the formation of a rule with the targetside:S-TOP ?
das ist der Fall von PN-NKsince the maximum distance from the rule?s rootnode to another node is three (to APPR or to PN-NK).However, a rule with the target side:S-TOP ?
das ist der Fall von NE Nikitinis not permitted since it has a rule depth of four(from S-TOP to either of the NE nodes).Node count is defined as the number of target treenodes in the composed rule, excluding target words.The default limit is 15, which for the example islarge enough to permit any possible composed rule(the full tree has a node count of 13).Rule size is the measure defined in De-Neefe et al (2007): the number of non-part-of-speech, non-leaf constituent labels in the target tree.The default rule size limit is three.2.2 Unaligned Source WordsUnaligned source words are attached to the treeusing the following heuristic: if there are alignedsource words to both the left and the right of an un-aligned source word then it is attached to the lowestcommon ancestor of its nearest such left and rightneighbours.
Otherwise, it is attached to the root ofthe parse tree.2.3 Unary Rule EliminationMoses?
chart decoder does not currently supportthe use of grammars containing fully non-lexicalunary rules (such as NP ?
X1 | NN1).
Unless the--AllowUnary option is given, the rule extractoreliminates these rules using the method described inChung et al (2011).2.4 Scope PruningUnlike hierarchical phrase-based rule extraction,GHKM places no restriction on the rank of the re-sulting rules.
In order that the grammar can beparsed efficiently, one of two approaches is usuallytaken: (i) synchronous binarization (Zhang et al,2006), which transforms the original grammar to aweakly equivalent form in which no rule has rankgreater than two.
This makes the grammar amenableto decoding with a standard chart-parsing algorithmsuch as CYK, and (ii) scope pruning (Hopkins andLangmead, 2010), which eliminates rules in order toproduce a subgrammar that can be parsed in cubictime.Of these two approaches, Moses currently sup-ports only the latter.
Both rule extractors prunethe extracted grammar to remove rules with scopegreater than three.
The next section describes theparsing algorithm that is used for scope-3 grammars.3893 Scope-3 Parsing in MosesHopkins and Langmead (2010) show that a sentenceof length n can be parsed using a scope-k grammarin O(nk) chart updates.
In this section, we describesome details of Moses?
implementation of their chartparsing method.3.1 The Grammar TrieThe grammar is stored in a trie-based data structure.Each edge is labelled with either a symbol from thesource terminal vocabulary or a generic gap sym-bol, and the trie is constructed such that for any pathoriginating at the root vertex, the sequence of edgelabels represents the prefix of a rule?s source right-hand-side (RHSs, also referred to as a rule pattern).Wherever a path corresponds to a complete RHSs,the vertex stores an associative array holding the setof grammar rules that share that RHSs.
The asso-ciative array maps a rule?s sequence of target non-terminal symbols to the subset of grammar rules thatshare those symbols.Figure 2 shows a sample of the grammar rules thatcan be extracted from the example alignment graphof Figure 1, and Figure 3 shows the correspondinggrammar trie.3.2 InitializationThe first step is to construct a secondary trie thatrecords all possible applications of rule patternsfrom the grammar to the sentence under consider-ation.
This trie is built during a single depth-firsttraversal of the grammar trie in which the terminaledge labels are searched for in the input sentence.
Ifa matching input word is found then the secondarytrie is extended by one vertex for each sentence posi-tion at which the word occurs and trie traversal con-tinues along that path.
A search for a gap label al-ways results in a match.
Edges in the secondary trieare labelled with the matching symbol and the posi-tion of the word in the input sentence (or a null po-sition for gap labels).
Each vertex in the secondarytrie stores a pointer to the corresponding grammartrie vertex.Once the secondary trie has been built, it is easyto determine the set of subspans to which each rulepattern applies.
A set of pairs is recorded againsteach subspan, each pair holding a pointer to a gram-mar trie vertex and a record of the sentence positionscovered by the symbols (which will be ambiguous ifthe pattern contains a sequence of k > 1 adjacentgap symbols covering more than k sentence posi-tions).After this initialization step, the secondary trie isdiscarded.3.3 Subspan ProcessingThe parsing algorithm proceeds by processing chartcells in order of increasing span width (i.e.
bottom-up).
At each cell, a stack lattice is constructed foreach rule pattern that was found during initialization.The stack lattice compactly represents all possibleapplications of that pattern over the span, togetherwith pointers to the underlying hypothesis stacks forevery gap.
A full path through the lattice corre-sponds to a single application context.
By selectinga derivation class (i.e.
target-side non-terminal la-bel) at each arc, the path can be bound to a set ofgrammar rules that differ only in the choice of targetwords or LHS label.Recall that for every rule pattern found duringinitialization, the corresponding grammar trie ver-tex was recorded and that the vertex holds an as-sociative array in which the keys are sequences oftarget-side non-terminal labels and the mapped val-ues are grammar rules (together with associated fea-ture model scores).
The algorithm now loops overthe associated array?s key sequences, searching thelattice for matching paths.
Where found, the gram-mar rule is bound with a sequence of underlyingstack pointers.
The cell?s stacks are then populatedby applying cube pruning (Chiang, 2007) to the setof bound grammar rules.4 ExperimentsThis section describes the GHKM-based English-German system submitted by the University of Ed-inburgh.
Subsequent to submission, a further set ofcomparative experiments were run using a hierarchi-cal phrase-based system and a hierarchical systemwith target side syntactic annotation.4.1 DataWe made use of all available English-German Eu-ropean and News Commentary data.
For the hi-erarchical phrase-based experiments, this totalled3901.
NP-PD ?
the case of Alexander Nikitin | der Fall von Alexander Nikitin2.
NP-PD ?
the case X1 | der Fall PP-MNR13.
NP-PD ?
X1 case X2 | ART1 Fall PP-MNR24.
PP-MNR ?
of X1 | von PN-NK15.
PP-MNR ?
of X1 X2 | von NE1 NE2Figure 2: A sample of the rules extractable from the alignment graph in Figure 1.
Rules are written in the formLHS ?
RHSs | RHSt .NikitinAlexanderof ?case??
?casethe of ?Figure 3: Example grammar trie.
The filled vertices holdassociative array values.2,043,914 sentence pairs.
For the target syntax ex-periments, the German-side of the parallel corpuswas parsed using the BitPar2 parser.
If a parsefailed then the sentence pair was discarded, leav-ing a total of 2,028,556 pairs.
The parallel corpuswas then word-aligned using MGIZA++ (Gao andVogel, 2008), a multi-threaded implementation ofGIZA++ (Och and Ney, 2003).We used all available monolingual German datato train seven 5-gram language models (one eachfor Europarl, News Commentary, and the five Newsdata sets).
These were interpolated using weightsoptimised against the development set and the re-sulting language model was used in experiments.We used the SRILM toolkit (Stolcke, 2002) withKneser-Ney smoothing (Chen and Goodman, 1998).The baseline system?s feature weights were tunedon the news-test2008 dev set (2,051 sentence pairs)using Moses?
implementation of minimum error ratetraining (Och, 2003).2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html4.2 Rule ExtractionFor the hierarchical phrase-based model we usedthe default Moses rule extraction settings, whichare taken from Chiang (2007).
For target-annotatedmodels, the syntactic constraints imposed by theparse trees reduce the grammar size significantly.This allows us to relax the rule extraction settings,which we have previously found to benefit transla-tion quality, without producing an unusably largegrammar.
We use identical settings to those used inWMT?s 2010 translation task (Koehn et al, 2010).Specifically, we relax the hierarchical phrase-basedextraction settings in the following ways:?
Up to seven source-side symbols are allowed.?
Consecutive source non-terminals are permit-ted.?
Single-word lexical phrases are allowed for hi-erarchical subphrase subtraction.?
Initial phrases are limited to 15 source words(instead of 10).By using the scope-3 parser we can also relax therestriction on grammar rank.
For comparison, weextract two target-annotated grammars, one with amaximum rank of two, and one with an unlimitedrank but subject to scope-3 pruning.GHKM rule extraction uses the default settings3as described in section 2.Table 1 shows the sizes of the extracted grammarsafter filtering for the newstest2011 test set.
Fil-tering removes any rule in which the source right-hand-side contains a sequence of terminals and gapsthat does not appear in any test set sentence.3GHKM rule extraction is now fully integrated into Moses?Experiment Management System (EMS) and can be enabled forstring-to-tree pipelines using the TRAINING:use-ghkm pa-rameter.391Experiment Grammar SizeHierarchical 118,649,771Target Syntax 12,748,259Target Syntax (scope-3) 40,661,639GHKM 27,002,733Table 1: Grammar sizes (distinct rule counts) after filter-ing for the newstest-2011 test set4.3 FeaturesOur feature functions include the n-gram languagemodel probability of the derivation?s target yield, itsword count, and various scores for the synchronousderivation.
We score grammar rules according to thefollowing functions:?
p(RHSs|RHSt,LHS), the noisy-channel trans-lation probability.?
p(LHS,RHSt|RHSs), the direct translationprobability.?
plex (RHSt|RHSs) and plex (RHSs|RHSt), thedirect and indirect lexical weights (Koehn et al,2003).?
ppcfg(FRAGt), the monolingual PCFG proba-bility of the tree fragment from which the rulewas extracted (GHKM and target-annotatedsystems only).
This is defined as ?ni=1 p(ri),where r1 .
.
.
rn are the constituent CFG rulesof the fragment.
The PCFG parameters are es-timated from the parse of the target-side train-ing data.
All lexical CFG rules are given theprobability 1.
This is similar to the pcfg featureused in Marcu et al (2006) and is intended toencourage the production of syntactically well-formed derivations.?
exp(?1/count(r)), a rule rareness penalty.?
exp(1), a rule penalty.
The main grammar andglue grammars have distinct penalty features.4.4 Decoder SettingsFor the submitted GHKM system we used a max-imum chart span setting of 25.
For the other sys-tems we used settings that matched the rule extrac-tion spans: 10 for hierarchical phrase-based, 15 fortarget syntax, and unlimited for GHKM.We used the scope-3 parsing algorithm (enabledusing the option -parsing-algorithm 1) forall systems except the hierarchical system, whichused the CYK+ algorithm (Chappelier and Rajman,1998).For all systems we set the ttable-limit pa-rameter to 50 (increased from the default value of20).
This setting controls the level of grammar prun-ing that is performed after loading: only the top scor-ing translations are retained for a given source RHS.4.5 ResultsFollowing the recommendation ofClark et al (2011), we ran the optimizationthree times and repeated evaluation with each setof feature weights.
Table 2 presents the averagedsingle-reference BLEU scores.
To give a roughindication of how much use the systems make ofsyntactic information for reordering, we also reportglue rule statistics taken from the 1-best derivations.There is a huge variation in decoding time be-tween the systems, much of which can be at-tributed to the differing chart span limits.
To givea comparison of system performance we selected an80-sentence subset of newstest2011, randomlychoosing ten sentences of length 1-10, ten of length11-20, and so on.
We decoded the test set four timesfor each system, discarding the first set of results (toallow for filesystem cache priming) and then aver-aging the remaining three.
Table 3 shows the totaldecoding times for each system and the peak virtualmemory usage4.
Figure 4 shows a plot of sentencelength against decoding time for the two GHKMsystems.5 ConclusionWe developed a GHKM-based string-to-tree systemfor English to German, achieving competitive resultscompared to a hierarchical model baseline.
We ex-tended the Moses toolkit to include a GHKM ruleextractor and scope-3 parsing algorithm and pro-vided details of our implementation.
We intend tofurther improve this system in future work.4The server has 142GB physical memory.
The decoder wasrun single-threaded in performance tests.
For the hierarchicalsystem we used an on-disk rule table, which reduces memoryrequirements at the cost of increased rule lookup time.
For allother systems we used in-memory rule tables.392newstest2009 newstest2010 newstest2011 Glue Rule AppsExperiment BLEU s.d.
BLEU s.d.
BLEU s.d.
Mean s.d.GHKM (max span 25) 15.2 0.1 16.7 0.1 15.4 0.1 3.1 0.3Hierarchical 15.2 0.0 16.4 0.1 15.5 0.0 13.9 0.5Target 14.6 0.1 16.0 0.1 14.9 0.1 8.4 5.0Target (scope-3) 14.7 0.0 16.4 0.2 15.0 0.0 9.7 1.2GHKM (no span limit) 15.0 0.3 16.6 0.1 15.2 0.2 1.9 1.3Table 2: Average BLEU scores and standard deviations over three optimization runs.
GHKM (max span 25) is thesubmitted system.
Also shown is the average number of rule applications per sentence for the 1-best output of thethree test sets, averaged over the three optimization runs.System Max Time (s) VM (MB)spanHierarchical 10 122 5,345Target 15 367 8,688Target (scope-3) 15 1,539 19,761GHKM 25 3,529 17,424GHKM None 11,196 18,060Table 3: Total decoding time and peak virtual memoryusage for the 80-sentence subset of newstest2011.01002003004005006007008000 10 20 30 40 50 60 70 80DecodingTime(seconds)Sentence LengthMax span 25No span limitFigure 4: Sentence length vs decoding time for theGHKM (max span 25) and GHKM (no limit) systemsAcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
Thiswork was supported by the EuroMatrixPlus projectfunded by the European Commission (7th Frame-work Programme) and made use of the resourcesprovided by the Edinburgh Compute and Data Facil-ity.5 The ECDF is partially supported by the eDIKTinitiative.6 This work was also supported in part un-der the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-C-0022.
The first author was supported by anEPSRC Studentship.ReferencesJ.-C. Chappelier and M. Rajman.
1998.
A generalizedCYK algorithm for parsing stochastic CFG.
In Pro-ceedings of the First Workshop on Tabulation in Pars-ing and Deduction, pages 133?137.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical report, Harvard University.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-5http://www.ecdf.ed.ac.uk6http://www.edikt.org.uk393tics, pages 1443?1452, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues concerning decoding with synchronouscontext-free grammar.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages413?417, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: Controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 176?181, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007).
June 28-30,2007.
Prague, Czech Republic.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL ?04.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL-44:Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 961?968, Morristown, NJ, USA.
Association forComputational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.
Aunified framework for phrase-based, hierarchical, andsyntax-based statistical machine translation.
In In Pro-ceedings of IWSLT, December 2009.Mark Hopkins and Greg Langmead.
2010.
SCFG decod-ing without binarization.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 646?655, Cambridge, MA,October.
Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn, Barry Haddow, Philip Williams, and HieuHoang.
2010.
More linguistic annotation for sta-tistical machine translation.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 115?120, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: statistical machine trans-lation with syntactified target language phrases.
InEMNLP ?06: Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 44?52, Morristown, NJ, USA.
Association forComputational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken Language Pro-cessing, Denver, Colorado, September 2002.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Compu-tational Linguistics, pages 256?263, Morristown, NJ,USA.
Association for Computational Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InStatMT ?06: Proceedings of the Workshop on Statisti-cal Machine Translation, pages 138?141, Morristown,NJ, USA.
Association for Computational Linguistics.394
