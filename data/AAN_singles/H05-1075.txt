Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 596?603, Vancouver, October 2005. c?2005 Association for Computational LinguisticsHandling Biographical Questions with ImplicatureDonghui Feng Eduard HovyInformation Sciences Institute Information Sciences InstituteUniversity of Southern California University of Southern CaliforniaMarina del Rey, CA, 90292 Marina del Rey, CA, 90292donghui@isi.edu hovy@isi.eduAbstractTraditional question answering systemsadopt the following framework: parsingquestions, searching for relevant docu-ments, and identifying/generating an-swers.
However, this framework does notwork well for questions with hidden as-sumptions and implicatures.
In this paper,we describe a novel idea, a cascadingguidance strategy, which can not onlyidentify potential traps in questions butfurther guide the answer extraction pro-cedure by recognizing whether there aremultiple answers for a question.
This isthe first attempt to solve implicature prob-lem for complex QA in a cascading fash-ion using N-gram language models asfeatures.
We here investigate questionswith implicatures related to biographyfacts in a web-based QA system, Power-Bio.
We compare the performances ofDecision Tree, Na?ve Bayes, SVM (Sup-port Vector Machine), and ME (Maxi-mum Entropy) classification methods.The integration of the cascading guidancestrategy can help extract answers forquestions with implicatures and producesatisfactory results in our experiments.1 MotivationQuestion Answering has emerged as a key area innatural language processing (NLP) to apply ques-tion parsing, information extraction, summariza-tion, and language generation techniques (Clark etal., 2004; Fleischman et al, 2003; Echihabi et al,2003; Yang et al, 2003; Hermjakob et al, 2002;Dumais et al, 2002).
Traditional question answer-ing systems adopt the framework of parsing ques-tions, searching for relevant documents, and thenpinpointing and generating answers.
However, thisframework includes potential dangers.
For exam-ple, to answer the question ?when did Beethovenget married?
?, a typical QA system would identifythe question target to be a ?Date?
and would applytechniques to identify the date Beethoven got mar-ried.
Since Beethoven never married, this directapproach is likely to deliver wrong answers.
Thetrick in the question is the implicature that Beetho-ven got married.
In the main task of QA track ofTREC 2003, the performances of most systems onproviding ?NIL?
when no answer is possible rangefrom only 10% to 30% (Voorhees, 2003).Just as some questions have no answer, othersmay have multiple answers.
For instance, with?who was Ronald Reagan?s wife?
?, a QA systemmay give only ?Nancy Davis?
as the answer.
How-ever, there is another correct answer: Jane Wyman.The problem here is the implicature in the questionthat Reagan only got married once.An implicature is anything that is inferred froman utterance but that is not a condition for the truthof the utterance (Gazdar, 1979; Levinson, 1983).Implicatures in questions either waste computa-tional effort or impair the performance of a QAsystem or both.
Therefore, when answering ques-tions, it is prudent to identify the questions withimplicatures before processing starts.In this paper, we describe a novel idea to solvethe problem: a strategy of cascading guidance.
Thisis the first attempt to solve implicature problem forcomplex QA in a cascading fashion using N-gram596language models as features.
The cascading guid-ance part is designed to be inserted immediatelybefore the search procedure to handle questionswith implicatures.
It can not only first identify thepotential ?no answer?
traps but also identifywhether multiple answers for this question arelikely.To investigate the performance of the cascadingguidance strategy, we here study two types ofquestions related to biography facts in a web-basedbiography QA system, PowerBio.
This web-basedQA system extracts biographical facts from theweb obtained by querying a web search engine(Google in our case).
Figure 1 provides the twotypes of questions we selected, which we refer toas SPOUSE_QUESTION and CHIL-D_QUESTION.Figure 1.
SPOUSE_QUESTION andCHILD_QUESTIONBoth types of questions have implicatures to jus-tify the use of the cascading guidance strategy.
In-tuitively, to answer these questions, we have twoissues related to implicatures to clarify:?
Does the person have a spouse/child??
What's the number of answers for this ques-tion?
(One or many?
)We therefore create two successive classifica-tion engines in the cascading classifier.For learning, our approach queries the searchengine with every person listed in the training set,extracts related features from the documents, andtrains the cascading classifiers.
For application,when a new question is given, the cascading classi-fier is applied before activation of the search sub-system.
We compare the performances of fourpopular classification approaches in the cascadingclassifier, namely Decision Tree, Na?ve Bayes,SVM (Support Vector Machine), and ME (Maxi-mum Entropy) classifications.The paper is structured as follows: related workis discussed in Section 2.
We introduce our cascad-ing guidance technique in Section 3, including De-cision Tree, Na?ve Bayes and SVM (SupportVector Machine) and ME (Maximum Entropy)classifications.
The experimental results are pre-sented in Section 4.
We discuss related issues andfuture work in Section 5.2 Related WorkQuestion Answering has attracted much attentionfrom the areas of Natural Language Processing,Information Retrieval and Data Mining (Fleisch-man et al, 2003; Echihabi et al, 2003; Yang et al,2003; Hermjakob et al, 2002; Dumais et al, 2002;Hermjakob et al, 2000).
It is tested in several ven-ues, including the TREC and CLEF Question An-swering tracks (Voorhees, 2003; Magnini et al,2003).
Most research efforts in the Question An-swering community have focused on factoid ques-tions and successful Question Answering systemstend to have similar underlying pipelines structures(Prager et al, 2004; Xu et al, 2003; Hovy et al,2000; Moldovan et al, 2000).Recently more techniques for answer extraction,answer selection, and answer validation have beenproposed (Lita et al, 2004; Soricut and Brill, 2004;Clark et al, 2004).Prager et al (2004) proposed applying constraintsatisfaction obtained by asking auxiliary questionsto improve system performance.
This approachrequires the creation of auxiliary questions, whichmay be complex to automate.Ravichandran and Hovy (2002) proposed auto-matically learning surface text patterns for answerextraction.
However, this approach will not work ifno explicit answers exist in the source.
The firstreason is that in that situation the anchors to learnthe patterns cannot be determined.
Secondly, mostof the facts without explicit values are not ex-pressed with long patterns including anchors.
Forexample, the phrase ?the childless marriage?
givesenough information that a person has no child.
Butit is almost impossible to learn such surface textpatterns following (Ravichandran and Hovy, 2002).Reported work on question processing focusesmainly on the problems of parsing questions, de-termining the question target for search subsystemI.
SPOUSE_QUESTIONE.g.
Who is <PERSON>?s wife?Who is <PERSON>?s husband?Whom did <PERSON> marry??II.
CHILD_QUESTIONE.g.
Who is <PERSON>?s son?Who is <PERSON>?s daughter?Who is <PERSON>?s child?
?597(Pasca and Harabagiu, 2001; Hermjakob et al,2000).
Saquete et al (2004) decompose complextemporal questions into simpler ones based on thetemporal relationships in the question.To date, there has been little published work onhandling implicatures in questions.
Just-In-TimeInformation Seeking Agents (JITISA) was pro-posed by Harabagiu (2001) to process questions indialogue and implicatures.
The agents are createdbased on pragmatic knowledge.
Traditional answerextraction and answer fusion approaches assumethe question is always correct and explicit answersdo exist in the corpus.
Reported work attempts torank the candidate answer list to boost the correctone into top position.
This is not enough whenthere may not be an answer for the question posed.For biographical fact extraction and generation,Zhou et al (2004) and Schiffman et al (2001) usesummarization techniques to generate human biog-raphies.
Mann and Yarowsky (2005) propose fus-ing the extracted information across documents toreturn a consensus answer.
In their approach, theydid not consider multiple values or no values forbiography facts, although multiple facts are com-mon for some biography attributes, such as multi-ple occupations, children, books, places ofresidence, etc.
In these cases a consensus answer isnot adequate.Our work differs from theirs because we are notonly working on information/answer extraction;the focus in this paper is the guidance for answerextraction of questions (or IE task for values) withimplicatures.
This work can be of great help forimmediate biographical information extraction.We describe details of the cascading guidancetechnique and investigate how it will help for ques-tion answering in Section 3.3 Cascading Guidance TechniqueWe turn to the Web by querying a web search en-gine (Google in our case) to find evidence to createguidance for answer extraction.3.1 Classification ProcedureThe cascading classifier is applied after the nameof the person and the answer types are identified.Figure 2 gives the pipeline of the classificationprocedure.With the identified person name, we query thesearch engine (Google) to obtain the top N webpages/documents.
A simple data cleaning programonly keeps the content texts in the web page, whichis broken up into separate sentences.
Followingthat, topic sentences are identified with the key-word topic identification technique.
For each topicwe provide a list of possible related keywords andany sentences containing both the person?s name(or reference) and at least one of the keywords willbe selected.
The required features are extractedfrom the topic sentences and passed to the cascad-ing classifier as supporting evidence to generateguidance for answer extraction.Figure 2.
Procedure of Cascading Classifier3.2 Feature ExtractionIntuitively, sentences elaborating a biographicalfact in a given topic should have similar styles(short patterns) of organizing words and phrases.Here, topic means an aspect of biographical facts,e.g., marriage, children, birthplace, and so on.
In-spired by this, we consider taking N-grams in sen-tences as our features.
However, N-gram featuresnot closely related to the topic will bring morenoise into the system.
Therefore, we only take theN-grams within a fixed-length window around thetopic keywords for features calculation, and passthem as evidence to cascading classifier.Classification ResultsSearch EnginePersonNameWebPagesData CleanerSentence breakerCascadingClassifierCleanTopicSentencesTopicIdentificationFeatureExtraction598For N-grams, instead of using the multiplicationof conditional probabilities of each word in the N-gram, we only consider the last conditional prob-ability (see below).
The reason is that the last con-ditional probability is a strong sign of the pattern?simportance and how this sequence of words is or-ganized.
Simply multiplying all the conditionalprobabilities will decrease the value and requirenormalization.
Realizing that in a set of documentsthe frequency of each N-gram is very importantinformation, we combine the last conditional prob-ability with the frequency.The computation for each feature of unigram,bigram and trigram are defined as the followingformulas:)(*)( iiunigram wfreqwpf =                             (1)),(*)|( 11 iiiibigram wwfreqwwpf ?
?=             (2)),,(*),|( 1212 iiiiiitrigram wwwfreqwwwpf ???
?=(3)We here investigate four kinds of classifiers,namely Decision Tree, Na?ve Bayes, Support Vec-tor Machine (SVM), and Maximum Entropy (ME).3.3 Classification ApproachesThe cascading classifier is composed of two suc-cessive parts.
Given the set of extracted features,the classification result could lead to different re-sponses to the question, either answering with ?novalue?
with strong confidence or directing the an-swer extraction model how many answers shouldbe sought.For text classification, there are several well-studied classifiers in the machine learning andnatural language processing communities.Decision Tree ClassificationThe Decision Tree classifier is simple and matcheshuman intuitions perfectly while it has been provedefficient in many application systems.
The basicidea is to break up the classification decision into aunion of a set of simpler decisions based on N-gram features.
Due to the large feature set, we useC5.0, the decision tree software package developedby RuleQuest Research (Quinlan, 1993), instead ofC4.5.Na?ve Bayes ClassificationThe Na?ve Bayes classifier utilizes Bayes' rule asfollows.
Supposing we have the featureset { }nfffF ,...,, 21= , the probability that personp belongs to a class c is given as:)|'(maxarg'FcPcc=     (4)Based on Bayes?
rule, we have)'()'|(maxarg)()'()'|(maxarg)|'(maxarg'''cPcFPFPcPcFPFcPcccc===(5)This was used for both successive classifiers of thecascading engine.SVM ClassificationSVM (Support Vector Machines) has attractedmuch attention since it was introduced in (Boser etal., 1992).
As a special and effective approach forkernel based methods, SVM creates non-linearclassifiers by applying the kernel trick to maxi-mum-margin hyperplanes.Suppose nipi ,...,1, =  represent the training setof persons, and the classes for classifications are},{ 21 ccC = (for simplicity, we represent theclasses with { }1,1?=C ).
Then the classificationtask requires the solution of the following optimi-zation problem (Hsu et al, 2003):01))((21min1,,??
?++ ?=iiiTiniiTbbpctosubjectM?????????
(6)We use the SVM classification packageLIBSVM (Chang and Lin, 2001) in our problem.ME ClassificationME (Maximum Entropy) classification is used hereto directly estimate the posterior probability forclassification.Suppose p represents the person and the classesfor classifications are { }21,ccC = , we have M fea-ture functions Mmpchm ,...,1),,( = .
For each fea-ture function, we have a modelparameter Mmm ,...,1, =?
.
The classification with599maximum likelihood estimation can be defined asfollows (Och and Ney, 2002):?
?
?===='1]),(exp[]),(exp[)|()|(1'1cMmmmMmmmpchpchpcppcP M???
(7)The decision rule to choose the most probableclass is (Och and Ney, 2002): { }??????==?=MmmmccpchpcPc1),(maxarg)|(maxarg??
(8)We use the published package YASMET 1  toconduct parameters training and classification.YASMET requires supervised learning for thetraining of maximum entropy model.The four classification approaches are assem-bled in a cascading fashion.
We discuss their per-formance next.4 Experiments and Results4.1 Experimental SetupWe download from infoplease.com 2  and biogra-phy.com 3  two corpora of people?s biographies,which include 24,975 and 24,345 bios respectively.We scan each whole corpus and extract peoplehaving spouse information.
To create the data set,we manually check and categorize each person ashaving multiple spouses, only one spouse, or nospouse.
Similarly, we obtained another list of per-sons having multiple children, only one child, andno child.
The sizes of data extracted are given inTable 1.Type Child SpouseNo_value 25 20One_value 35 32Multiple_values 107 43Table 1.
Extracted experimental dataFor the cascading classification, in the first step,when classifying whether a person has aspouse/child or not, we merge the last two subsets1 http://www.fjoch.com/YASMET.html2 http://www.infoplease.com/people.html3 http://www.biography.com/search/index.jspwith one value and multiple values into one.
Table2 presents the data used for each level of classifica-tion.class Child SpouseNo_value 25 20 First-levelClassification With_value 142 75One_value 35 32 Second-levelClassification Multiple_value 107 43Table 2.
Data set used for classificationTo investigate the performances of our cascad-ing classifiers, we divided the two sets into trainingset and testing set, with half of them in the trainingset and half in the testing set.4.2 Empirical ResultsFor each situation of the two questions, when theanswer type has been determined to be the child orspouse of a person, we send the person?s name toGoogle and collect the top N documents.
As de-scribed in Figure 2, topic sentences in each docu-ment are selected by keyword matching.
A windowwith the length of w is applied to the sentence.
Allword sequences in the window are selected for fea-ture calculation.
We take all the three N-gram lan-guage models (unigram, bigram, and trigram) inthe window for feature computation.
Table 3 givesthe sizes of the bigram feature sets for first-levelclassification as we take more and more documentsinto the system.Top N Docs Child Spouse1 3468 195810 27733 1232520 46431 2733130 61057 3663740 76687 4377150 87020 5086860 96393 6163270 108053 6771280 118947 7330690 130526 77370100 139722 82339Table 3.
Sizes of feature setsAs described in Section 3, the feature values areapplied in the classifiers.
Tables 4 and 5 give thebest performances of the 4 classifiers in the twosituations when we select the top N articles usingN-gram probability for feature computation.Due to the large size of the feature set, C5.0,SVM, and ME packages will not work at some600point as more documents are encountered.
The Na-?ve Bayes classification is more scalable as we useintermediate file to store probability tables.Precision First-levelClassificationSecond-levelClassificationC5.0 82.90% 65.70%Na?veBayes87.80% 72.86%SVM 84.15% 75.71%ME 86.59% 75.71%Table 4.
Precision scores for child classificationPrecision First-levelClassificationSecond-levelClassificationC5.0 80.90% 56.80%Na?veBayes83.00%59.46%SVM 78.72% 54.05%ME 78.72% 51.35%Table 5.
Precision scores for spouse classificationFeature # of timesidentified(out of 75)p(wi|wi-2,wi-1)and his wife 35  0.6786her husband , 33 0.3082and her husband 26   0.5476was married to 20 0.8621with his wife 14   0.875her second husband   13 0.6667her marriage to 13 0.5ex - wife           12 0.3333ex - husband 11    0.6667her first husband     10  0.75second husband ,     10       1his first wife 8 0.3333first husband ,  7        0.6667second wife ,   7 0.3333his first marriage     5        0.1667s second wife 5 0.75Table 6.
Example trigram features for second-levelclassification for Spouse (one or multiple values)The feature set has a large number of features.However, not all of them will be used for each per-son.
We studied the number of times features areidentified/used in the training and testing sets andtheir probabilities.
Table 6 presents a list of sometrigram features for second-level classification(one or multiple values) for Spouse.
Obviously,indicating features have a large probability as ex-pected.
The second column gives the number oftimes the feature is used out of the training andtesting set (75 persons in total).Will more complex N-gram features work bet-ter?Intuitively, being less ambiguous, more complexN-gram features carry more precise informationand therefore should work better than simple ones.We studied the performances for different N-gramlanguage model features.
Below are the results ofNa?ve Bayes first-level classification for Child,using different N-gram features.Top NDocsUnigram Bigram Trigram1 34.78% 54.35% 67.39%10 30.48% 79.27% 86.59%20 26.83% 82.93% 85.37%30 24.39% 81.71% 86.59%Table 7.
Comparisons of classification precisionsusing different N-gram features for childFrom Table 7, we can infer that bigram featureswork better than unigram features, and trigram fea-tures work better than bigrams when we select dif-ferent numbers of top N documents.
Trigramfeatures actually bring enough evidence in classifi-cation.
However, when we investigated 4-gramslanguage features in the collected data, most ofthem are very sparse in the feature space of all thecases.
Applying 4-grams or higher may not help inour task.Will more data/documents help?The performance of corpus-based statistical ap-proaches usually depends on the size of corpus.
Atraditional view for most NLP problems is thatmore data will help to improve the system?s per-formance.
However, for data collected from asearch engine, this may not be the case, since webdata is usually ambiguous and noisy.
We thereforeinvestigate the data size?s effect on system per-formance.
Figure 3 gives the precision curves ofthe Na?ve Bayes classifier for the first-level classi-fication for Child.Except for the case of top 1, where the topdocument alone may not contain too much usefulinformation on selected topics, precision scoresonly have slight variations for increasing numbersof documents.
For bigram features, over the top 50601through top 70 documents, the precision scoreseven get a little worse.Performances on Top N Docs00.10.20.30.40.50.60.70.80.911 10 20 30 40 50 60 70 80 90 100Top NPrecisionBigramTrigramFigure 3.
Performance on top N documents4.3 ExamplesEquipped with the cascading guiding strategy, weare able to handle questions containing implica-tures.
In our system, when we can determine theanswer type is child or spouse, the cascading guid-ing system will help the answer extraction part toextract answers from the designated corpus.
Figure4 gives two examples of the strategy.Figure 4.
Classification Example for questionFor the first question, the classifier recognizesthere is no spouse for the target person and returnsinformation for the answer generation.
The fea-tures used here are the first-level classification re-sult for SPOUSE_QUESTION.
For the secondquestion, the classifier recognizes the target personhas a child first, followed by recognizing that theanswer has multiple values.
In this way, the strat-egy integrated to the question answering systemcan improve the system?s performance by handlingquestions with implicatures.5 Discussion and Future WorkQuestions may have implicatures due to the flexi-bility of human language and conversation.
In realquestion-answering systems, failure to handle themmay either waste huge computation cost or impairsystem?s performance.
The traditional QA frame-work does not work well for questions containingimplicatures.
We describe a novel idea in this pa-per to identify potential traps in biographical ques-tions and recognize whether there are multipleanswers for a question.Question-Answering systems, even when fo-cused upon biographies, have to handle many facts,such as birth date, birth place, parents, training,accomplishments, etc.
These values can be ex-tracted using typical text harvesting approaches.However, when there are no values for some bio-graphical information, the task becomes muchmore difficult because text seldom explicitly statesa negative.
For example, the following two ques-tions require schools attended:?
Where did <person> graduate from??
What university did <person> attend?Our program scanned the two corpora of biosand found only 2 out 49320 bios explicitly statingthat the subject never attended any school.
There-fore, for some types of information, it will be muchharder to identify null values through evidencefrom text.
Some more complicated reasoning andinference may be required.
Classifiers for somebiographical facts may need to incorporate extraknowledge from other resources.
The inherent rela-tions between biography facts can also be used tovalidate each other.
For example, the relations ofmarriage and child, birth place and childhoodhome, etc.
may provide clues for cross-validation.We plan to investigate these problems in the future.AcknowledgementsWe wish to thank the anonymous reviewers fortheir helpful feedback and corrections.
Also wethank Lei Ding, Feng Pan, and Deepak Ravi-chandran for their valuable comments on this work.ReferencesBoser, B.E., Guyon, I. and Vapnik, V. 1992.
A trainingalgorithm for optimal margin classifiers.
Proceedingsof the ACM COLT 1992.Chang, C. and Lin, C. 2001.
LIBSVM -- A library forsupport vector machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm/Q1: Who is Sophia Smith?s spouse?Classified: <NO_SPOUSE>Answer: She did not marry.Q2: Who is John Ritter?s child?Classified: <HAVING_CHILD>Classified: <MULTIPLE_VALUES>?602Chu-Carroll, J., Czuba, K., Prager, J., and Ittycheriah,A.
2003.
In question answering, two heads are betterthan one.
Proceedings of HLT-NAACL-2003.Clark, S., Steedman, M. and Curran, J.R. 2004.
Object-extraction and question-parsing using CCG.
Proceed-ings EMNLP-2004, pages 111-118, Barcelona, Spain.Dumais, S., Banko, M., Brill, E., Lin, J., and Ng, A.2002.
Web question answering: is more always bet-ter?
Proceedings of SIGIR-2002.Echihabi, A. and Marcu, D. 2003.
A noisy channel ap-proach to question answering.
Proceedings of ACL-2003.Fleischman, M., Hovy, E.H., and Echihabi, A.
2003.Offline strategies for online question answering: an-swering questions before they are asked.
Proceedingsof ACL-2003.Gazdar, G. 1979.
Pragmatics: Implicature, presupposi-tion, and logical form.
New York: Academic Press.Harabagiu, S. 2001.
Just-In-Time Question Answering.Invited talk in Proceedings of the Sixth Natural Lan-guage Processing Pacific Rim Symposium 2001.Hermjakob, U., Echihabi, A., and Marcu, D. 2002.Natural language based reformulation resource andweb exploitation for question answering.
Proceed-ings of TREC-2002.Hermjakob, U., Hovy, E.H., and Lin, C. 2000.
Knowl-edge-based question answering.
TREC-2000.Hovy, E.H., Gerber, L., Hermjakob, U., Junk, M., andLin, C. 2000.
Question answering in Webclopedia.Proceedings of TREC-2000.Hovy, E.H., Hermjakob, U., Lin, C., and Ravichandran,D.
2002.
Using knowledge to facilitate factoid an-swer pinpointing.
Proceedings of COLING-2002.Hsu, C.-W., Chang, C.-C., and Lin, C.-J.
2003.
A Prac-tical Guide to Support Vector Classification.
Avail-able at:http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.Levinson, S. 1983.
Pragmatics.
Cambridge UniversityPress.Lita L.V.
and Carbonell, J.
2004.
Instance-based ques-tion answering: a data driven approach.
Proceedingsof EMNLP 2004.Magnini, B., Romagnoli, S., Vallin, A., Herrera, J., Pe-?as, A., Peinado, V., Verdejo, F., Rijke, M. 2003.The Multiple Language Question Answering Track atCLEF 2003.
CLEF 2003: 471-486.Mann, G. and Yarowsky, D. 2005.
Multi-field informa-tion extraction and cross-document fusion.
Proceed-ings of ACL-2005.Moldovan, D., Clark, D., Harabagiu, S., and Maiorano,S.
2003.
Cogex: A logic prover for question answer-ing.
Proceedings of ACL-2003.Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R.,Girju, R., Goodrum, R., and Rus, V. 2000.
The struc-ture and performance of an open-domain questionanswering system.
Proceedings of ACL-2000.Nyberg, E. et al 2003.
A multi strategy approach withdynamic planning.
Proceedings of TREC-2003.Och, F. J.and Ney, H. 2002.
Discriminative training andmaximum entropy models for statistical machinetranslation.
Proceedings of ACL 2002 pp.
295-302.Pasca, M. and Harabagiu, S. 2001.
High PerformanceQuestion/Answering.
Proceedings of SIGIR-2001.Prager, J. M., Chu-Carroll, J., and Czuba, K.W.. 2004.Question answering using constraint satisfaction.Proceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL'04).Quinlan, J. R. 1993.
C4.5: Programs for machine learn-ing.
Morgan Kaufmann, San Mateo, CA, 1993.Ravichandran, D. and Hovy, E.H. 2002.
Learning Sur-face Text Patterns for a Question Answering System.Proceedings of ACL-2002.Saquete, E., Mart?nez-Barco, P., Mu?oz, R., and Vicedo,J.L.
2004.
Splitting complex temporal questions forquestion answering systems.
Proceedings of ACL'04.Schiffman, B., Mani, I., and Concepcion, K.J.
2001.Producing biographical summaries: combining lin-guistic knowledge with corpus statistics.
Proceedingsof ACL/EACL-2001.Soricut, R. and Brill, E. 2004.
Automatic question an-swering: beyond the factoid.
Proceedings ofHLT/NAACL-2004, Boston, MA.Voorhees, E.M. 2003.
Overview of the trec 2003 ques-tion answering track.
Proceedings of TREC-2003.Xu, J., Licuanan, A., Weischedel, R. 2003.
TREC 2003QA at BBN: Answering Definitional Questions.
Pro-ceedings of TREC 2003.Yang, H., Chua, T.S., Wang, S., and Koh, C.K.
2003.Structured use of external knowledge for eventbasedopen domain question answering.
Proceedings ofSIGIR-2003.Zhou, L., Ticrea, M., and Hovy, E.H. 2004.
Multi-document biography summarization.
Proceedings ofEMNLP-2004.603
