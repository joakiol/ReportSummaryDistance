Support Vector Machines for Paraphrase Identificationand Corpus ConstructionChris Brockett and William B. DolanNatural Language Processing GroupMicrosoft ResearchOne Microsoft Way, Redmond, WA 98502, U.S.A.{chrisbkt, billdol}@microsoft.comAbstractThe lack of readily-available large cor-pora of aligned monolingual sentencepairs is a major obstacle to the devel-opment of Statistical Machine Transla-tion-based paraphrase models.
In thispaper, we describe the use of annotateddatasets and Support Vector Machinesto induce larger monolingual para-phrase corpora from a comparable cor-pus of news clusters found on theWorld Wide Web.
Features include:morphological variants; WordNetsynonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal stringfeatures such as word-based edit dis-tance.
Use of this technique dramati-cally reduces the Alignment Error Rateof the extracted corpora over heuristicmethods based on position of the sen-tences in the text.1 IntroductionParaphrase detection?the ability to determinewhether or not two formally distinct strings aresimilar in meaning?is increasingly recognizedas crucial to future applications in multiplefields including Information Retrieval, QuestionAnswering, and Summarization.
A growingbody of recent research has focused on the prob-lems of identifying and generating paraphrases,e.g., Barzilay & McKeown (2001), Lin & Pantel(2002), Shinyama et al (2002), Barzilay & Lee(2003), and Pang et al (2003).
One promisingapproach extends standard Statistical MachineTranslation (SMT) techniques (e.g., Brown et al,1993; Och & Ney, 2000, 2003) to the problemsof monolingual paraphrase identification andgeneration.
Finch et al (2004) have describedseveral MT based paraphrase systems within thecontext of improving machine translation output.Quirk et al (2004) describe an end-to-end para-phrase identification and generation system us-ing GIZA++ (Och & Ney, 2003) and amonotone decoder to generate information-preserving paraphrases.As with conventional SMT systems, SMT-based paraphrase systems require extensivemonolingual parallel training corpora.
However,while translation is a common human activity,resulting in large corpora of human-translatedbilingual sentence pairs being relatively easy toobtain across multiple domains and languagepairs, this is not the case in monolingual para-phrase, where naturally-occurring parallel dataare hard to come by.
The paucity of readilyavailable monolingual parallel training corporaposes a formidable obstacle to the developmentof SMT-based paraphrase systems.The present paper describes the extraction ofparallel corpora from clustered news articlesusing annotated seed corpora and an SVM clas-sifier, demonstrating that large parallel corporacan be induced by a classifier that includes mor-phological and synonymy features derived fromboth static and dynamic resources.2 BackgroundTwo broad approaches have dominated the lit-erature on constructing paraphrase corpora.
One1approach utilizes multiple translations of a sin-gle source language text, where the source lan-guage text guarantees semantic equivalence inthe target language texts (e.g., Barzilay &McKeown, 2001; Pang et al, 2003).
Such cor-pora are of limited availability, however, sincemultiple translations of the same document areuncommon in non-literary domains.The second strain of corpora construction in-volves mining paraphrase strings or sentencesfrom news articles, with document clusteringtypically providing the topical coherence neces-sary to boost the likelihood that any two arbi-trary sentences in the cluster are paraphrases.
Inthis vein, Shinyama et al (2002) use named en-tity anchors to extract paraphrases within a nar-row domain.
Barzilay & Lee (2003) employMultiple Sequence Alignment (MSA, e.g.,Durbin et al, 1998) to align strings extractedfrom closely related news articles.
Although theMSA approach can produce dramatic results, itis chiefly effective in extracting highly templaticdata, and appears to be of limited extensibility tobroad domain application (Quirk et al 2004).Recent work by Dolan, et al (2004) describesthe construction of broad-domain corpora ofaligned paraphrase pairs extracted from news-cluster data on the World Wide Web using twoheuristic strategies: 1) pairing sentences basedon a word-based edit distance heuristic; and 2) anaive text-feature-based heuristic in which thefirst two sentences of each article in a cluster arecross-matched with each other, their assumptionbeing that the early sentences of a news articlewill tend to summarize the whole article and arethus likely to contain the same information asother early sentences of other articles in thecluster.
The word-based edit distance heuristicyields pairs that are relatively clean but offerrelatively minor rewrites in generation, espe-cially when compared to the MSA model of(Barzilay & Lee, 2003).
The text-based heuristic,on the other hand, results in a noisy ?compara-ble?
corpus: only 29.7% of sentence pairs areparaphrases, resulting in degraded performanceon alignment metrics.
This latter technique,however, does afford large numbers of pairingsthat are widely divergent at the string level; cap-turing these is of primary interest to paraphraseresearch.
In this paper, we use an annotated cor-pus and an SVM classifier to refine the output ofthis second heuristic in an attempt to better iden-tify sentence pairs containing richer paraphrasematerial, and minimize the noise generated byunwanted and irrelevant data.3 Constructing a Classifier3.1 Sequential Minimal OptimizationAlthough any of a number of machine learningalgorithms, including Decision Trees, might beequally applicable here, Support Vector Ma-chines (Vapnik, 1995) have been extensivelyused in text classification  problems and withconsiderable success (Dumais 1998; Dumais etal., 1998; Joachims 2002).
In particular, SVMsare known to be robust in the face of noisy train-ing data.
Since they permit solutions in high di-mensional space, SVMs lend themselves readilyto bulk inclusion of lexical features such asmorphological and synonymy information.For our SVM, we employed an off-the-shelfimplementation of the Sequential Minimal Op-timization (SMO) algorithm described in Platt(1999).
1  SMO offers the benefit of relativelyshort training times over very large feature sets,and in particular, appears well suited to handlingthe sparse features encountered in natural lan-guage classification tasks.
SMO has been de-1 The pseudocode for SMO may be found in the appendix of Platt (1999)Edit Distance(e ?
12)San Jose Medical Center announcedWednesday that it would close itsdoors by Dec. 1, 2004.San Jose Medical Center has an-nounced that it will close itsdoors by Dec. 1, 2004.First TwoSentencesThe genome of the fungal pathogenthat causes Sudden Oak Death hasbeen sequenced by US scientistsResearchers announced Thursdaythey've completed the geneticblueprint of the blight-causingculprit responsible for Sudden OakDeathTable 1.
Paraphrase Examples Identified by Two Heuristics2ployed a variety of text classification tasks (e.g.,Dumais 1998; Dumais et al, 1998).3.2 DatasetsTo construct our corpus, we collected news arti-cles from news clusters on the World Wide Web.A database of 13,127,938 candidate sentencepairs was assembled from 9,516,684 sentencesin 32,408 clusters collected over a 2-year period,using simple heuristics to identify those sen-tence pairs that were most likely to be para-phrases, and thereby prune the overall searchspace.Word-based Levenshtein edit distanceof 1 < e ?
20; and a length ratio> 66%; ORBoth sentences in the first threesentences of each file; and lengthratio > 50%.From this database, we extracted three data-sets.
The extraction criteria, and characteristicsof these datasets are given in Table 2.
The datasets are labled L(evenshtein) 12, F(irst) 2 andF(irst) 3 reflecting their primary selection char-acteristics.
The L12 dataset represents the bestcase achieved so far, with Alignment ErrorRates beginning to approach those reported foralignment of closely parallel bilingual corpora.The F2 dataset was constructed from the firsttwo sentences of the corpus on the same as-sumptions as those used in Dolan et al (2004).To avoid conflating the two data types, however,sentence pairs with an edit distance of 12 or lesswere excluded.
Since this resulted in a corpusthat was significantly smaller than that desirablefor exploring extraction techniques, we also cre-ated a third data set, F3 that consisted of thecross-pairings of the first three sentences of eacharticle in each cluster, excluding those wherethe edit distance is e ?
12.3.3 Training DataOur training data consisted of 10,000 sentencepairs extracted from randomly held-out clustersand hand-tagged by two annotators according towhether in their judgment (1 or 0) the sentencepairs constituted paraphrases.
The annotatorswere presented with the sentences pairs in isola-tion, but were informed that they came fromrelated document sets (clusters).
A conservativeinterpretation of valid paraphrase was adopted:if one sentence was a superstring of the other,e.g., if a clause had no counterpart in the othersentence, the pair was counted as a non-paraphrase.
Wherever the two annotators dis-agreed, the pairs were classed as non-paraphrases.
The resultant data set contains 2968positive and 7032 negative examples.3.4 FeaturesSome 264,543 features, including overt lexicalpairings, were in theory available to the classi-fier.
In practice, however, the number of dimen-sions used typically fell to less than 1000 afterthe lowest frequency features are eliminated (seeTable 4.)
The main feature classes were:String Similarity Features: All sentence pairswere assigned string-based features, includ-ing absolute and relative length in words,number of shared words, word-based editdistance, and lexical distance, as measuredby converting the sentences into alphabet-ized strings of unique words and applyingword based edit distance.Morphological Variants: Another class offeatures was co-ocurrence of morphologicalvariants in sentence pairs.
Approximately490,000 sentences in our primary datasetswere stemmed using a rule-based stemmer,to yield a lexicon of 95,422 morphologicallyvariant word pairs.
Each word pair wastreated as a feature.
Examples are:orbit|orbitalorbiter|orbitingWordNet Lexical Mappings: Synonyms andhypernyms were extracted from WordNet,L12 F2 F3Corpus size 253,725 51,933 235,061Levenshteinedit distance 1 < e ?
12 e > 12 e > 12Sentence rangein article All First two First threeLength 5 < n < 30 5 < n < 30 5 < n < 30Length ratio 66% 50% 50%Shared words 3 3 3Table 2.
Characteristics of L(evenshtein) 12,F(irst) 2, and F(irst) 3 Data3(http://www.cogsci.princeton.edu/~wn/;Fellbaum, 1998), using the morphologicalvariant lexicon from the 490,000 sentencesas keywords.
The theory here is that as addi-tional paraphrase pairs are identified by theclassifier, new information will ?comealong for the ride,?
thereby augmenting therange of paraphrases available to be learned.A lexicon of 314,924 word pairs of the fol-lowing form created.
Only those pairs iden-tified as occurring in either training data orthe corpus to be classified were included inthe final classifier.operation|procedureoperation|workWord Association Pairs: To augment theabove resources, we dynamically extractedfrom the L12 corpus a lexicon of 13001possibly-synonymous word pairs using alog-likelihood algorithm described in Moore(2001) for machine translation.
To minimizethe damping effect of the overwhelmingnumber of identical words, these were de-leted from each sentence pair prior to proc-essing; the algorithm was then run on thenon-identical residue as if it were a bilingualparallel corpus.To deploy this data in the SVM feature set,a cutoff was arbitrarily selected that yielded13001 word pairs.
Some exemplars (notfound in WordNet) include:straight|consecutivevendors|suppliersFig.
1 shows the distribution of word pair-ings obtained by this method on the L12corpus in comparison with WordNet.
Ex-amination of the top-ranked 1500 wordpairs reveals that 46.53% are found inWordNet and of the remaining 53.47%,human judges rated 56% as good, yieldingan overall ?goodness score?
of 76.47%.Judgments were by two independent raters.For the purposes of comparison, we auto-matically eliminated pairs containing trivialsubstring differences, e.g., spelling errors,British vs. American spellings, singu-lar/plural alternations, and miscellaneousshort abbreviations.
All pairs on which theraters disagreed were discarded.
Also dis-carded were a large number of partialphrasal matches of the ?reported|according?and ?where|which?
type, where part of aphrase (?according to?, ?in which?)
wasmissing.
Although viewed in isolation thesedo not constitute valid synonym or hyper-rnym pairs, the ability to identify these par-tial matchings is of central importancewithin an SMT-framework of paraphrasealignment and generation.
These resultssuggest, among other things, that dynami-cally-generated lexical data of this kindmight be useful in increasing the coverageof hand-built synonymy resources.Composite Features: From each of the lexi-cal feature classes, we derived a set of moreabstract features that summarized the fre-quency with which each feature or class offeatures occurred in the training data, bothindependently, and in correlation with others.These had the effect of performing normali-zation for sentence length and other factors.Some examples are:No_of_List_2_Words (i.e., thecount of Wordnet matches)30.00%35.00%40.00%45.00%50.00%55.00%60.00%500100015002000Word PairsNot in WordnetIn WordNetFig.
1.
WordNet Coverage in Word Associa-tion Output4External_Matches_2_LED (i.e,,the ratio of total lexical matches toLevenshtein edit distance.
)4 Evaluation4.1 MethodologyEvaluation of paraphrase recognition within anSMT framework is highly problematic, since notechnique or data set is standardly recognized.Barzilay & Lee (2003) and Quirk et al (2004)use human evaluations of end-to-end generation,but these are not very useful here, since they addan additional layer of uncertainty into theevaluation, and depend to a significant extent onthe quality and functionality of the decoder.Dolan & Brockett (2005) report extraction pre-cision of 67% using a similar classifier, but withthe explicit intention of creating a corpus thatcontained a significant number of naturally-occuring paraphrase-like negative examples.Since our purpose in the present work  is non-application specific corpus construction, we ap-ply an automated technique that is widely usedfor reporting intermediate results in the SMTcommunity, and is being extended in other fieldssuch as summarization (Daum?
and Marcu,forthcoming), namely word-level alignment us-ing an off-the-shelf implementation of the SMTsystem GIZA++ (Och & Ney, 2003).
Below, weuse Alignment Error Rate (AER), which is in-dicative of how far the corpus is from providinga solution under a standard SMT tool.
This al-lows the effective coverage of an extracted cor-pus to be evaluated efficiently, repeatedlyagainst a single standard, and at little cost afterthe initial tagging.
Further, if used as an objec-tive function, the AER technique offers theprospect of using hillclimbing or other optimiza-tion techniques for non-application-specific cor-pus extraction.To create the test set, two human annotatorscreated a gold standard word alignment on heldout data consisting of 1007 sentences pairs.
Fol-lowing the practice of Och & Ney (2000, 2003),the annotators each created an initial annotation,categorizing alignments as either SURE (neces-sary) or POSSIBLE (allowed, but not required).
Inthe event of differences, annotators were askedto review their choices.
First pass inter-rateragreement was 90.28%, climbing to 94.43% onthe second pass.
Finally we combined the anno-tations into a single gold standard as follows: ifboth annotators agreed that an alignment wasSURE, it was tagged as SURE in the gold-standard; otherwise it was tagged as POSSIBLE.To compute Precision, Recall, and AlignmentError Rate (AER), we adhere to the formulaelisted in Och & Ney (2003).
Let A be the set ofalignments in the comparison, S be the set ofSURE alignments in the gold standard, and P bethe union of the SURE and POSSIBLE alignmentsin the gold standard:||||precisionAPA?= ; ||||recallSSA?=||||||AERSASAPA+?+?=4.2 BaselinesEvaluations were performed on the heuristi-cally-derived L12, F2, and F3 datasets using theabove formulation.
Results are shown in Table 3.L12 represents the best case, followed respec-tively by F3 and F2.
AERs were also computedseparately for identical (Id) and non-identical(Non-Id) word mappings in order to be able toCorpusSize(pairs)Precision Recall AER Id AER Non Id AERL12 ~254 K 87.42% 87.66% 12.46% 11.57% 21.25%F2 ~52 K 85.56% 83.31% 15.57% 13.19% 39.08%F3 ~235K 86.53% 81.57% 15.99% 14.24% 33.83%10K Trained ~24 K 86.93% 87.24% 12.92% 11.69% 24.70%MSR Trained  ~50 K 86.76% 86.39% 13.42% 11.92% 28.31%Table 3.
Precision, Recall and Alignment Error Rates5drill down on the extent to which new non-identical mappings are being learned from thedata.
A high Id error rate can be considered in-dicative of noise in the data.
The score that weare most interested in, however, is the Non-Idalignment error rate, which can be consideredindicative of coverage as represented by theGiza++ alignment algorithm?s ability to learnnew mappings from the training data.
It will beobserved that the F3 dataset non-Id AER issmaller than that of the F2 dataset: it appearsthat more data is having the desired effect.Following accepted SMT practice, we addeda lexicon of identical word mappings to thetraining data, since Giza++ does not directlymodel word identity, and cannot easily capturethe fact that many words in paraphrase sentencemay translate as themselves.
We did not add inword pairs derived from word association dataor other supplementary resources that mighthelp resolve matches between unlike but seman-tically similar words.4.3 Training on the 10K DataWe trained an SVM on the 10 K training setemploying 3-fold cross-validation on the train-ing set itself.
Validation errors were typically inthe region of 16-17%.
Linear kernels with de-fault parameters (tolerance=1e-3; margin sizecomputed automatically; error probability=0.5)were employed throughout.
Applying the SVMto the F3 data, using 946 features encountered inthe training data with frequency > 4, this classi-fier yielded a set of 24588 sentence pairs, whichwere then aligned using Giza++.The alignment result is shown in Table 3.
The?10K Trained?
row represents the results of ap-plying Giza++ to the data extracted by the SVM.Non-identical word AER, at 24.70%, shows a36.9% reduction in the non-identical word AERover the F2 dataset (which is approximatelydouble the size), and approximately 28% overthe original F3 dataset.
This represents a hugeimprovement in the quality of the data collectedby using the SVM and is within striking distanceof the score associated with the L12 best case.The difference is especially significant when itis considered that the newly constructed corpusis less than one-tenth the size of the best-casecorpus.
Table 5 shows sample extracted sen-tences.To develop insights into the relative contribu-tions of the different feature classes, we omittedsome feature classes from several runs.
The re-sults were generally indistinguishable, exceptfor non-Id AER, shown in Table 4, a fact thatmay be taken to indicate that string-based fea-tures such as edit distance still play a major role.Eliminating information about morphologicalalternations has the largest overall impact, pro-ducing a degradation of a 0.94 in on Non-IdAER.
Of the three feature classes, removal ofWordNet appears to have the least impact,showing the smallest change in Non-Id AER.When the word association algorithm is ap-plied to the extracted ~24K-sentence-pair set,degradation in word pair quality occurs signifi-cantly earlier than observed for  the L12 data;after removing ?trivial?
matches, 22.63% ofword pairs in the top ranked 800 were found inWordnet, while 25.3% of the remainder werejudged to be ?good?
matches.
This is equivalentto an overall ?goodness score?
of 38.25%.
Therapid degradation of goodness may be in partattributable to the smaller corpus size yielded bythe classifier.
Nevertheless, the model learnsmany valid new word pairs.
Given enough datawith which to bootstrap, it may be possible to doaway with static resources such as Wordnet, andrely entirely on dynamically derived data.4.4 Training on the MSR Training SetBy way of comparison, we also explored appli-cation of the SVM to the training data in theMSR Paraphrase corpus.
For this purpose weused the 4076-sentence-pair ?training?
sectionof the MSR corpus, comprising 2753 positiveand 1323 negative examples.
The results at de-fault parameter settings are given in Table 3,with respect to all features that were observed tooccur with frequency greater than 4.
Althoughthe 49914 sentence pairs yielded by using the    Dimensions Non Id AERAll (fq > 4) 946 24.70No Lexical Pairs 230 25.35No WordAssociation 470 25.35No WordNet  795 25.24No Morphology 813 25.64Table 4.
Effect of Eliminating Feature Classeson 10K Training Set6MSR Paraphrase Corpus is nearly twice that ofthe 10K training set, AER performance is meas-urably degraded.
Nevertheless, the MSR-trainedcorpus outperforms the similar-sized F12, yield-ing a reduction in Non-Id AER of a not insig-nificant 16%.The fact that the MSR training data does notperform as well as the 10 K training set probablyreflects its derivative nature, since it was origi-nally constructed with data collected using the10K training set, as described in Dolan &Brockett (2005).
The performance of the MSRcorpus is therefore skewed to reflect the biasesinherent in its original training, and thereforeexhibits the performance degradation commonlyassociated with bootstrapping.
It is also a sig-nificantly smaller training set, with a higherproportion of negative examples than in typicalin real world data.
It will probably be necessaryto augment the MSR training corpus with furthernegative examples before it can be utilized ef-fectively for training classifiers.5 Discussion and Future WorkThese results show that it is possible to use ma-chine learning techniques to induce a corpus oflikely sentential paraphrase pairs whose align-ment properties measured in terms of AER ap-proach those of a much larger, morehomogeneous dataset collected using a string-edit distance heuristic.
This result supports theidea that an abstract notion of paraphrase can becaptured in a high dimensional model.Future work will revolve around optimizingclassifiers for different domains, corpus typesand training sets.
It seems probable that the ef-fect of the 10K training corpus can be greatlyaugmented by adding sentence pairs that havebeen aligned from multiple translations usingthe techniques described in, e.g., Barzilay &McKeown (2001) and Pang et al (2003).6 ConclusionsWe have shown that supervised machinelearning techniques such as SVMs can signifi-cantly expand available paraphrase corpora, andachieve a reduction of noise as measured byAER on non-identical words.Although from the present research has fo-cused on ?ready-made?
news clusters found onthe web, nothing in this paper depends on theavailability of such clusters.
Given standardclustering techniques, the approach that we havedescribed for inductive classifier learning shouldin principle be applicable to any flat corpuswhich contains multiple sentences expressingsimilar content.
We expect also that the tech-niques described here could be extended to iden-tify bilingual sentence pairs in comparablecorpora, helping automate the construction ofcorpora for machine translation.The ultimate test of paraphrase identificationtechnologies lies in applications.
These arelikely to be in fields such as extractive multi-document summarization where paraphrase de-tection might eliminate sentences with compara-ble content and Question Answering, for bothidentifying sentence pairs with comparable con-tent and generating unique new text.
Such prac-young female chimps learn skillsearlier , spend more time studyingand tend to do better than youngmale chimpanzees - at least when itcomes to catching termites .young female chimpanzees are better stu-dents than males , at least when itcomes to catching termites , accordingto a study of wild chimps in tanzania 'sgombe national park .
Paraphrase(accepted)  a %%number%% -year-old girl wasarrested , handcuffed and takeninto custody on charges of stealinga rabbit and a small amount ofmoney from a neighbor 's home .sheriff 's deputies in pasco county ,fla. , this week handcuffed and ques-tioned a %%number%% -year-old girl whowas accused of stealing a rabbitand  %%money%%  from a neighbor 'shome .Non-Paraphrase(rejected)roy moore , the chief justice ofalabama , installed the two-tonsculpture in the rotunda of hiscourthouse in montgomery , and hasrefused to remove it .the eight associate justices of alabama's supreme court voted unani-mously  %%day%%  to overrule moore andcomply with u.s. district judge myronthompson 's order to remove the monu-ment .Table 5.
Sample Pairs Extracted and Rejected by the SVM Trained on the 10K Corpus7tical applications will only be possible oncelarge corpora are available to permit the devel-opment of robust paraphrase models on the scaleof the best SMT models.
We believe that thecorpus construction techniques that we have de-scribed here represent an important contributionto this goal.AcknowledgementsWe would like to thank Monica Corston-Oliver,Jeff Stevenson, Amy Muia and Margaret Salomeof Butler Hill Group LLC for their assistance inannotating and evaluating our data.
This paperhas also benefited from feedback from severalanonymous reviewers.
All errors and omissionsare our own.ReferencesRegina Barzilay and Katherine.
R. McKeown.
2001.Extracting Paraphrases from a parallel corpus.
InProceedings of the ACL/EACL.Regina Barzilay and  Lillian Lee.
2003.
Learning toParaphrase; an unsupervised approach using mul-tiple-sequence alignment.
In Proceedings ofHLT/NAACL 2003.P.
Brown, S. A. Della Pietra, V.J.
Della Pietra and R.L.
Mercer.
1993.
The Mathematics of StatisticalMachine Translation.
Computational Linguistics,Vol.
19(2): 263-311.Hal Daum?
III and Daniel Marcu.
(forthcoming)Induction of Word and Phrase Alignments forAutomatic Document Summarization.
To appearin Computational Linguistics.William.
B. Dolan, Chris Quirk, and Chris Brockett.2004.
Unsupervised Construction of Large Para-phrase Corpora: Exploiting Massively ParallelNews Sources.
Proceedings of COLING 2004,Geneva, Switzerland.William B. Dolan and Chris Brockett.
2005.
Auto-matically Constructing a Corpus of SententialParaphrases.
In Proceedings of The Third Interna-tional Workshop on Paraphrasing (IWP2005),Jeju, Republic of Korea.Susan Dumais.
1998.
Using SVMs for Text Catego-rization.
IEEE Intelligent Systems, Jul.-Aug. 1998:21-23Susan Dumais, John Platt, David Heckerman, Me-hran Sahami.
1998.
Inductive learning algorithmsand representations for text categorization.
In Pro-ceedings of the Seventh International Conferenceon Information and Knowledge Management.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological sequenceanalysis: Probabilistic models of proteins and nu-cleic acids.
Cambridge University Press.Christiane Fellbaum (ed.).
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Andrew Finch, Taro Watanabe, Yasuhiro Akiba andEiichiro Sumita.
2004.
Paraphrasing as MachineTranslation.
Journal of Natural Language Proc-essing, 11(5), pp 87-111.Thorsten Joachims.
2002.
Learning to Classify TextUsing Support Vector Machines: Methods, Theory,and Algorithms.
Kluwer Academic Publishers,Boston/Dordrecht/London.Microsoft Research Paraphrase Corpus.http://research.microsoft.com/research/downloads/default.aspxRobert C. Moore.
2001.
Towards a Simple and Ac-curate Statistical Approach to Learning Transla-tion Relationships among Words.
In Proceedingsof the Workshop on Data-Driven Machine Trans-lation, ACL 2001.Franz Joseph Och and H. Ney.
2000.
Improved Sta-tistical Alignment Models.
In Proceedings of the38th Annual Meeting of the ACL, Hong Kong,China, pp 440-447.Franz Joseph Och and Hermann Ney.
2003.
A Sys-tematic Comparison of Various Statistical Align-ment Models.
Computational Linguistics, 29 (1):19-52.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based Alignment of Multiple Translations:Extracting Paraphrases and Generating New Sen-tences.
In Proceedings of NAACL-HLT.John C. Platt.
1999.
Fast Training of Support VectorMachines Using Sequential Minimal Optimization.In Bernhard Sch?lkopf, Christopher J. C. Burgesand Alexander J. Smola (eds.).
1999.
Advances inKernel Methods: Support Vector Learning.
TheMIT Press, Cambridge, MA.
185-208.Quirk, Chris, Chris Brockett, and William B. Dolan.2004.
Monolingual Machine Translation for Para-phrase Generation, In Proceedings of the 2004Conference on Empirical Methods in NaturalLanguage Processing, 25-26 July 2004, BarcelonaSpain, pp.
142-149.Bernhard Sch?lkopf and Alexander J. Smola.
2002.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond.
TheMIT Press, Cambridge, MA.Y.
Shinyama, S. Sekine and K. Sudo 2002.
Auto-matic Paraphrase Acquisition from News Articles.In Proceedings of NAACL-HLT.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, New York.8
