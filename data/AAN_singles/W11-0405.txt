Proceedings of the Fifth Law Workshop (LAW V), pages 38?46,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsConsistency Maintenance in Prosodic Labeling for Reliable Prediction ofProsodic BreaksYoungim Jung Hyuk-Chul KwonDept.
Knowledge Resources at Korea Insti-tute of Science and Technology Information/Dept.
Computer Science and Engineering atPusan National University/245 Daehang-no Yuseong-gu, San 30, Jangjeon-dong, Geumjeon-gu305-806 Daejeon, Republic of Korea Busan, 609-735, Republic of Koreaacorn@kisti.re.kr hckwon@pusan.ac.krAbstractFor the implementation of the prosody predic-tion model, large scale annotated speech corpo-ra have been widely applied.
Reliability amongtranscribers, however, was too low for success-ful learning of an automatic prosodic prediction.This paper reveals our observations on perfor-mance deterioration of the learning model dueto inconsistent tagging of prosodic breaks in theestablished corpora.
Then, we suggest a methodfor consistent prosodic labeling among multipletranscribers.
As a result, we obtain a corpuswith consistent annotation of prosodic breaks.The estimated pairwise agreement of annotationof the main corpus is between 0.7477 and0.7916, and the value of K is between 0.7057and 0.7569.
Considering the estimated K, anno-tation of the main corpus has reliable consisten-cy among multiple transcribers.1 IntroductionThe naturalness and comprehensibility of text-to-speech (TTS) synthesis systems are strongly af-fected by the accuracy of prosody prediction fromtext input.
For the implementation of the prosodyprediction model, large annotated speech corporahave been widely applied to both linguistic re-search and speech processing technologies as in(Syrdal and McGory, 2000).
Since an increasingnumber of annotated speech corpora become avail-able, a number of self-learning or probabilisticmodels for prosodic prediction have been sug-gested.
To obtain reliable results from data-drivenmodels, the corpus must be large scale, noise-freeand annotated consistently.
However, due to thelimited range of tagged data with prosodic breaksthat is used to learn or establish stochastic modelsat present, reliable results cannot be obtained.
Thus,the reliability among transcribers was too low forsuccessful learning of a prosodic model(Wightman and Ostendorf, 1994).
In addition, theperformance of ASR systems degrades significant-ly when training data are limited or noisy as in(Alwan, 2008).In this study we propose a new methodology oftraining transcribers, annotating a corpus by mul-tiple transcribers, and validating the reliability ofintertranscriber agreement.
This paper is organizedas follows: we review related work on corpus an-notation for speech and language processing tasksand method of measuring the reliability of consis-tency among multiple annotators in Section 2.
Sec-tion 3 describes our observations on performancedeterioration of the learning model due to inconsis-tent tagging of prosodic breaks in the establishedcorpora.
In Section 4, we suggest a procedure ofconstructing a medium-scale corpus, which areaimed at maintaining consistency in prosodic labe-ling among multiple annotators.
Through a seriesof experiments during the training phase, the im-provement of the agreement of multiple annotatorsis shown.
The final experiment is performed inorder to guarantee labeling agreement among fiveannotators.
A brief summary and future work arepresented in the final section.2 Related WorkAs linguistically-annotated corpora became criticalresources, science of corpus annotation has beenhighlighted and evolved to reflect various interestsin the field as shown in (Ide, 2007).
In order to an-notate linguistic information to large-scale corpora,two methods have been used; existing natural lan-38guage processing (NLP) tools such as part-of-speech taggers, syntactic parsers, sentence boun-dary recognizers, named entity recognizers as havebeen used to generate annotations for ANC data(Ide and Suderman, 2006).
Big advantages of usingexisting tools are that much cost and time can besaved and that the annotation result is consistent.In addition, it could obtain reliable accuracies andreduce the prohibitive cost of hand-validation bycombining results of multiple NLP tools.
However,tagging for all other linguistic phenomena is stillmainly a manual effort as presented in (Eugenio,2000).
Thus, human annotators are required fortagging, correcting or validating the linguistic in-formation although human annotators are very ex-pensive and inconsistent in various aspects.Linguists and language engineers have recog-nized the importance of the consistency of annota-tion among multiple annotators while theyconstruct a large-scale corpus and have focused onhow to measure the inter-annotator agreement.Their annotators had difficulties in discriminatingone annotation category from others that are close-ly related to each other.
Fellbaum et al (1999) whoperformed a semantic annotation project whichaimed at linking each content word in a text to acorresponding synset in WordNet found out that,with increasing polysemy, both inter-annotator andannotator-expert matches decreased significantly.As to measure the rate of agreement, Fellbaum etal.
(1999) used a very simple measurement, thepercentage of agreement in semantic annotationtask.
A greedy algorithm for increasing the inter-annotator agreement has been suggested by Ng etal.
(1999).
However, automatic correction of themanual tagging cannot reflect natural linguisticinformation tagged by human.On the other hand, in prosodic annotation, the re-liable measurement of intertranscriber agreementwas studied by Beckman et al (1994) initially,since the goal of the original ToBI system design-ers was to design a system with ?reliability (agree-ment between different transcribers must be atleast 80%)?, ?coverage?, ?learnability?, and ?capa-bility?.
The designers and developers of adapta-tions of ToBI for other languages and dialects suchas G-ToBI, GlaToBI and K-ToBI have proved theusability of their labeling system rather than havesuggested the method of maintaining the intertran-scriber agreement based on the aforementionedcriteria (Grice et al, 1996; Mayo et al, 1996; Junet al, 2000).3 Problem Description3.1 Obtaining a Large Scale Speech Anno-tated CorpusIn order to design and implement a predictionmodel of prosodic break, annotated corpus shouldbe prepared.
Recorded speech files and text scriptsof Korean Broadcasting Station (KBS) News 9were collected and manual annotation was con-ducted by two linguistic specialists.
Each hand-labeled half of the selected script for prosodicbreaks was cross-checked with the other half.
Theresultant corpus had 47,368 eo-jeol1s.
The size ofthis corpus, however, does not seem to be suffi-cient.
An easy way to construct a larger-scale cor-pus is using existing corpora in the field.
To builda large volume of learning and testing data, anno-tated speech data from Postech speech groups wereobtained.
The Postech data included 122,025 eo-jeols from Munhwa Broadcasting Corporation(MBC) news.
Three types of break, viz., majorbreaks, minor breaks and no breaks, were anno-tated after each eo-jeol in KBS data (our initialdata) and MBC data.3.2 Performance Deterioration of LearningModels due to Inconsistent AnnotationKBS and MBC news data were selected, to ex-amine the effect of prosodic breaks in corpora con-structed by different groups on learning and testing.Only 46,526 eo-jeols were randomly sampled fromthe MBC News corpus, whereas the entire KBSNews data was used for learning and testing, toavoid potential side effects from the differing datasize.KBS MBC (Postech data)Training Data 38,243 37,258Testing Data  9,103 9,268Table 1 Size of Training and Test data1 An eo-jeol in Korean can be composed up of one morphemeor several concatenated morphemes of different linguisticfeatures which are equivalent to a phrase in English.
Thisspacing unit is referred as an ?eo-jeol?, ?word?, or ?morphemecluster?
in Koeran linguistic literatures.
We adopt ?eo-jeol?
inorder to refer to ?an alphanumeric cluster of morphemes with aspace on either side?.39C4.5 and CRFs were adapted in this experiment.The learning and testing was conducted in twophases.
First, learning and testing of the prosodicbreak prediction models used a corpus constructedby a single group.
Five-fold cross-validation wasused for evaluating the models.
Second, learningand evaluation of the models used a different cor-pus constructed by each group.
The ratio of train-ing to testing data (held-out data) was four to one.The results obtained from the first and secondphases of learning and testing are presented in Ta-ble 2.Algo-rithm1st Phase Precision(Learning -Testing)2nd Phase Precision(Learning -Testing)KBS-KBS MBC-MBC KBS-MBC MBC-KBSC4.5 85.30% 62.53% 38.78% 44.96%CRFs 84.65% 67.52% 37.96% 45.01%Table 2 Experimental Results for Impact Analysis ofInconsistent TaggingThe prediction models performed well with C4.5and CRFs learning algorithms when the model wastrained and tested with KBS news data.
However,its performance decreased drastically when themodel was initially trained with KBS news dataand subsequently tested with MBC news data.
Theperformance of the learning model trained withMBC news data also deteriorated when tested withKBS data.
These results suggest that serious per-formance deterioration is caused by data inconsis-tency rather than by the learning algorithm per se.3.3 Analysis on Inconsistent AnnotationThe deterioration of the performance presented inSection 3.2 is quite considerable, despite the factthat the same genre and level of prosodic breaklabeling system was selected.
After analyzing thedata, we identified three main reasons as follows.
(1) Perceptual Prominence of Prosodic LabelingSystemsDespite the fact that three types of prosodic breakhave been commonly used in the speech engineer-ing field for a considerable time as shown in (Os-tendorf and Veilleux, 1994), they have not beenclearly defined or referenced in standard prosodiclabeling conventions.
In particular, the notion ofthe minor break is rather vague, whereas those ofno break and major break are intuitively clear as in(Mayo et al, 1996).In the MBC news data labeled by Postech, sen-tences that had all prosodic breaks tagged as nobreak were frequently found, even if two longclauses exist in a sentence.
Most sentences hadbeen annotated only with no break.
The speakingrate of news announcers on air is relatively fast andno obvious audible break seems to exist in theirspeech.
However, Kim (1991) showed that evenwell-trained news announcers rarely read a sen-tence without breaks.
Therefore, minor breaks needto be recognized not only by the duration of thebreak, but also by the tonal changes or lengtheningof the final syllable as shown in (Kim, 1991; Jun,2006; Jung et al, 2008).
(2) Different Perceptibility of Prosodic Breaksamong TranscribersGrice et al (1996), Mayo et al (1996) and Jun et al(2000) have focused on reliability-agreement be-tween different transcribers as the main criterion ofevaluation.
This fact indicates that individual labe-ling of a single utterance can differ, because eachtranscriber?s recognition of the prosodic labelingsystem varies.
And, the perceptibility of each tran-scriber differs.
A large-scale corpus is necessaryfor modeling a data-driven framework, and thegreater the number of transcribers cooperating, thepoorer the intertranscriber agreement becomes.However, maintaining the intertranscriber agree-ments is often neglected as empirical work whenresearchers build and analyze a speech annotatedcorpus for implementation of the prosody model.
(3) Syntactic or Semantic AmbiguitiesA single sentence with syntactic ambiguities hasseveral different interpretations.
In spoken lan-guage, prosody prevents garden path sentences andenables resolution of syntactic ambiguity as shownin (Kjelgaard and Speer, 1999; Schafer, 1997).Sentences such as the one in the following exam-ple (E1) can be grammatically constructed withmultiple syntactic structures2.
(E1) ?????
????
???
????????
???????.a.
Gosogbeoseuga // jung-angseon-eul # chimbeom-hae /// maju-odeon # seung-yongchaleul  // deul-ibad-ass-seubnida?An express bus drove over the center line and2 In examples, letters in italics denote phonetic transliterationof Korean; hyphens in transliteration are used for segmenta-tion of syllables.40rammed into an oncoming car.?b.
Gosogbeoseuga /// jung-angseon-eul # chim-beomhae // maju-odeon # seung-yongchaleul  ///deul-ibad-ass-seubnida?An express bus rammed into an oncoming carwhich drove over the center line.
?#: no break,  //: minor break, ///: major breakThe prosodic phrasing in both (a) or (b) can be cor-rect, depending on the sentence?s syntactic struc-ture.
The pattern in (E1) is quite frequent inKorean, particularly in situations where the topic isbroad.
This kind of syntactic ambiguity needs to beresolved by semantic or pragmatic information,since it cannot be resolved using syntactic informa-tion only.As we previously mentioned, three main prob-lems arise when annotated speech data are bothconstructed by multiple labelers in a researchgroup and the data are collected from differentgroups.
Considering the impact of the quality ofannotated corpora on the data-driven models, theoverall procedure of corpus construction includingthe data collection and preprocess, labeling systemselection and intertranscriber agreement mainten-ance should be designed and then evaluated asshown in Section 4.4 Corpus Building4.1 Selection of Prosodic Labeling SystemIn this paper, we define seven types of prosodicbreak in combination with phrasal boundary tonessince a prosodic break cannot be separated from aboundary tone.
Our seven types are defined as fol-lows:(1) Major break with falling tone: For caseswith a strong phrasal disjuncture and a strongsubjective sense of pause.
The positions of majorbreaks generally correspond to the boundaries ofintonational phrases (marked ?///L?).
(2) Major break with rising tone: For caseswith a strong phrasal disjuncture but a weak sub-jective sense of pause length (marked ?///H?).
(3) Major break with middle tone: In real data,major breaks with middle tone (or major breakswithout tonal change) are observed as in (Lee,2004), although they have no definition or ex-planation in K-ToBI.
They have been observedin very fast speech such as headline news utter-ances (marked ?///M?).
(4) Minor break with rising tone: For caseswith a minimal phrasal disjuncture and no strongsubjective sense of pause.
The positions of mi-nor breaks correspond to the boundaries of ac-centual phrases with rising tone.
When anutterance is so fast that a pause cannot be recog-nized clearly, minor breaks are realized by tonalchanges or segment lengthening of the final syl-lable (marked ?//H?).
(5) Minor break with middle tone: For caseswith prosodic words in compound words, suchas compound nouns or compound verbs.
Breaksbetween noun groups in a compound word or be-tween verbs in a compound verb may be realizedwhen the overall length of a compound word islong, whereas a break is absent in a short com-pound word (marked ?//M?).
(6) Minor break with falling tone: For caseswith minimal phrasal disjuncture and no strongsubjective sense of pause.
The positions of mi-nor breaks correspond to the boundaries of ac-centual phrases with falling tone.
(7) No break: For internal phrase word bounda-ries.
There is no prosodic break between one-word modifiers and their one-word partners orbetween a word-level argument and its predicate,because the two words are syntactically and se-mantically combined (marked ?#?
).The seven types of prosodic break are mapped toK-ToBI break indices, enabling further reusabilityof the corpus labeled by the suggested break types.K-ToBI Suggested Prosodic BreaksBreakIndex0  No Break (#)1  Minor Break (//L)2  Minor Break (//H, //M)3  Major Break (///H, ///M, ///L)ToneIndexHa, H% HLa, L% LL+  MTable 3 Mapping between break indices of K-ToBI andthe suggested prosodic breaksJun et al (2000) showed that the tonal patternagreement for each word was approximately 36%41for all labelers and this low level of agreement ap-pears to be due to the nature of the tonal pattern.Although fourteen possible AP (Accent Phrase)tonal patterns exist, these variations are neithermeaningful nor phonologically correct.
We con-cluded that the final phrasal tones are sufficient forthe recognition of prosodic boundaries.4.2 Data Selection and PreprocessingIn this study, KBS news scripts (issued January,2005 ~ June, 2006) were collected as a raw corpusfrom web.
Although the speech rate of TV newsspeech is faster than that of general read speech,announcers are trained to speak Standard KoreanLanguage and to generate standard pronunciations,tones and breaks.
In addition, individual stylisticvariation is restricted in the announcer?s speech.The text formats of news scripts extracted fromthe web are unified.
Then, sentences or expressionsin news scripts differing from those in real sen-tences in multimedia files are revised according tothe real utterances of the announcer.
The selectionand revision of the sentences is performed accord-ing to the following criteria.1) Headline news sentences uttered by one femaleannouncer are collected.2) Minimum of five eo-jeols are included in onesentence.3) Real speech of news script read by the announc-er is considered as primary source of prosodicbreak tagging for transcribers.4) Sentences in the news script are deleted unlessthey are read by the announcer in real speech files.5) Between 1-3 eo-jeols in news scripts differingfrom those in speech files are revised according tothe real speech if there is no semantic change.6) Sentences in the news script differing consider-ably from those in speech files are deleted.7) Words or phrases in the news script differingfrom those in speech files due to spelling/grammarerrors are not corrected manually.
They are cor-rected automatically by the PNU grammar checker,which shows over 95% accuracy as in (Kwon etal., 2004).4.3 Training TranscribersThe most reliable method of maintaining the con-sistency and accuracy of prosodic breaks by mul-tiple transcribers is for each well-trainedtranscriber to annotate prosodic breaks in the entirecorpus.
Then the majority of the tagging resultsamong multiple transcribers are selected as an an-swer for the target eo-jeol.
However, this methodwhere all transcribers annotate the same corpus indepth is too time consuming and costly.
Due totime and cost constraints, most related studies usea simpler method.
If the size of the corpus is small,then a professional linguist annotates the entirecorpus as in (Maragoudakis et al, 2003).
If the sizeof corpus is large, more than two transcribers di-vide the corpus by the number of transcribers andeach transcriber annotates his/her own part as in(Wightman and Ostendorf, 1994; Viana et al,2003).
Unless the transcribers are trained and thereliability of the intertranscriber agreement is vali-dated, consistency of annotation  by multiple tran-scribers cannot be assured.
Hence, a method formaintaining the reliability of the intertranscriberagreement of prosodic breaks is suggested in thispaper.The overall procedure of training the transcribers,annotating the main corpus with prosodic breaksand validating the reliability of tagging consistencyamong multiple transcribers is illustrated in Figure1.Training TranscribersYESValidating ReliabilityEducation of casesGuideline EducationAnnotating identical datathoroughly by ntranscribersMeasuring intertranscriberagreementK>0.67New trainingdataNOMain corpus?Nparts?n transcribers annotateindividuallyAnnotating Main CorpusNew data for validatingintertranscriber agreementAnnotating identical datathoroughly by ntranscribersMeasuring intertranscriberagreementK: kappa coefficientK>0.67Validated reliability ofintertranscriber agreementYES+AnalysiscorpusEvaluationcorpus 1Evaluationcorpus 2Figure 1 Overall Procedure of Corpus BuildingFirstly, guidelines are provided for transcribers tofamiliarize themselves with the prosodic labelingsystem suggested in Section 4.1.
Secondly, in orderto improve the awareness of the length or strengthof each prosodic break type in detail, transcribersrepeatedly listen to speech files corresponding toseveral paragraphs in news scripts.
In addition,WaveSurfer Version.1.8.5, which is an open source42program for visualizing and manipulating speech,is utilized for transcribers to examine the pitchcontour, waveform, and power plot of speech files.In the training phase, five transcribers annotatethe same data with prosodic breaks at the sametime and then compare the results of their annota-tions, and discuss and repeatedly correct the vari-ous errors until reliable agreement among them isreached.
The data used for this intertranscriberagreement training is given in Table 4.1st 2nd 3rd 4th#  eo-jeols 422 544 491 711# sentences 35 49 42 32Table 4 Data used in intertranscribers trainingAfter mastering the guidelines and training witheach data set, specific reasons for inconsistencyamong transcribers were analyzed and their solu-tions were educated as follows:(1) Prosodic breaks were inserted due to announc-ers' emphasis on a certain eo-jeol, mistakes in read-ing the sentence or the habit of slowing down twoor three eo-jeols from the end of a sentence.
Sometranscribers recognized these as speakers?
errorsand corrected them in their annotations.
On theother hand, others annotated prosodic breaks ac-cording to what they heard, regardless of errors.Due to these differing policies on annotation, theresultant annotation of prosodic breaks amongtranscribers is not consistent, as shown in example(E2).
(E2) ??
?????
(///H, #)3 ???
?.deo-ug  simgaghaejigo       iss-seubnida.more   serious become       progress +EM4?
(sth) becomes more serious?Inconsistency derived from these speakers?
errorsshould be deleted.
(2) If the speech rate of the announcer is too fastfor some transcribers to perceive audible breaks3 The correct answer among different annotations is under-lined.4 Notes on abbreviations of Korean grammatical morphemesare as follows: EM for ending markers, TP for topical postpo-sition, LCM for locative case marker, OCM for objective casemarker, PEC for pre-ending denoting continuousbetween two eo-jeols, they omitted the minor break,whereas others put a minor break in the same place,as shown in (E3).
(E3) ???
(#, //L)  ??????
?geuleona        jilbyeonggwanlibonbu-neunhowever        Korea Center for Disease Control+TPand Prevention+TP?However, the Korea Center for Disease Controland Prevention?In this case, transcribers need to pay attention towhether the final tone of the target eo-jeol is risingor falling.
In order to reduce inconsistency derivedfrom missing breaks, transcribers repeatedly prac-tice while listening to similar patterns.
(3) If only one annotator selects a different type ofprosodic break than the others for the answer of thesame place, he/she must change his approach inannotating prosodic breaks.
(4) Wightman and Ostendorf (1994) and Ross andOstendorf (1996) have revealed that there is pro-sodic variability even for news speech data.
Theannouncer showed variability in the location,strength or length, and tonal change in our newsdata as well.
For example, the announcer occasio-nally put a minor break between two eo-jeols con-sisting of a time expression, as shown in (E4).
(E4)  a.
??
//H  2002?
?
?,jinan //H   2002nyeon   oneul,past       2002year    this day?
(on) this day 2002,?b.
??
#  2000?
1?jinan #   2000nyeon  1wolpast     2000year    January?
(in) January 2000,?For a time expression including less than four eo-jeols, no break should be marked in it.Discussion and education such cases describedabove after annotating new training data sets re-peats till the intertranscriber agreement is suffi-ciently high.
The intertranscriber agreement inannotating seven-level prosodic breaks includingtonal changes is shown in Table 5.Agreement Cumulative rate (%)431st 2nd  3rd  4thFive (all) agreed 43.84 50.55 55.80 57.67At least four agreed 60.90 68.20 73.52 75.53At least three agreed 81.75 87.50 90.84 91.70Table 5 Intertranscriber agreement in trainingThe cumulative rate of agreement of more thanhalf of the transcribers (n+1/2) is measured by ap-proximate figures.
Specifically, the rate of the in-tertranscriber agreement is calculated with thecumulative rate at which all five transcribersagreed, at least four of them agreed, and at leastthree of them agreed.
The resultant agreement ofthe first experiment is quite low, though the firstexperiment was performed after the transcribershad familiarized themselves with the guidelinesand studied many examples.
The intertranscriberagreement in annotating data with seven-level pro-sodic breaks increases continuously with repeatedtraining and experiments.
This indicates that edu-cating transcribers with guidelines and examples isnot sufficient, and training of transcribers is re-quired prior to annotation of the main corpus withspecified tagging classes by multiple transcribers.In order to review how accurately each individualtranscriber annotates the corpus, the annotationaccuracy of each individual transcriber is estimated.The prosodic break type for which at least three ofthem agreed is considered as the answer.
The an-notation result of each transcriber is compared tothe answer, and then the accuracy is estimated bycounting the number of annotations that match theanswers.
Table 6 shows the estimated annotationaccuracy of five transcribers from the 1st to the 4thexperiment.TranscriberEstimated accuracy (%)1st 2nd  3rd  4thA 94.51 84.00 86.32 91.56B 78.03 85.26 89.24 93.25C 78.03 93.05 94.39 94.02D 88.44 90.32 90.36 90.64E 82.37 83.79 84.08 89.11Table 6 Estimated accuracy of each  transcriberAlthough there are individual variations, the esti-mated accuracy of the transcribers increases steadi-ly.After the four experiments, the cumulative rate ofagreement of more than half of the transcribersreached 91.70% and the estimated accuracy of in-dividual transcribers increased to 89.11~94.02%.Hence, an objective and reliable measurement forintertranscriber agreement is required in order todecide whether the training is sufficient.The most commonly used methods to assess thelevel of agreement among transcribers are pairwiseanalysis and Kappa statistics.
The reliability ofintertranscriber agreement of the four experimentshas been assessed with these two measurementsand the result is given in Table 7.Measurement 1st  2nd   3rd   4thPairwise analysis 0.6385 0.6969 0.7375 0.7477Kappa statistics 0.5783 0.6464 0.6938 0.7057Table 7 Reliability of intertranscriber agreementSince the value of K is greater than 0.67 in the 3rdand 4th experiment, the intertranscriber agreementfor annotating prosodic breaks is considered tohave reached a reliable level as shown in (Carletta,1996).
Then annotation of the main corpus is per-formed.The main corpus comprising 29,686 eo-jeols isdivided into five parts.
Each partition is assigned tothe trained five transcribers and annotation is inde-pendently performed.
WaveSurfer, which is usedin the training phase, is also used in the annotationphase for the display and annotation of speech.Transcribers may openly discuss their annotations,even though they annotated different parts of themain corpus.4.4 Validation of Reliability of Intertran-scriber AgreementSince each individual transcriber annotated a dif-ferent part of the main corpus, the reliability ofintertranscriber agreement cannot be measured di-rectly.
We assume that intranscriber agreementdoes not change dramatically before and after  an-notation of the main corpus.Hence, another data set including 1,149 eo-jeols(46 sentences), with a size 1.5x larger than that ofthe data  set used in the 4th  experiment, is collectedand used instead, in order to validate the reliabilityof agreement.
Immediately after annotation of themain corpus, the final experiment is performedfollowing the procedure performed in the training44phase, except for the education steps.
The fivetranscribers annotated the same data in depth,however, they worked independently.
They werenot allowed to discuss prosodic labeling.
Pairwiseanalysis and Kappa statistics are used in measuringintertranscriber agreement on the validation dataset.
The pairwise agreement and K found in thevalidation experiment after annotation of the maincorpus was 0.79 and 0.76, respectively.Both agreement figures are greater than thosefound in the prior experiments, which were re-peated four times in the training phase.
Based onthis result, annotation of the main corpus is alsoconsidered to be part of training of transcribers.According to our assumption, the estimated inter-transcriber agreement of annotation of the maincorpus annotation is between the agreement of theprior and post experiments, as shown in Figure 2.Figure 2 Estimated intertranscriber reliability in annota-tion of main corpusThe estimated pairwise agreement of annotation ofthe main corpus is between 0.7477 and 0.7916, andthe value of K is between 0.7057 and 0.7569.
Con-sidering the estimated K, annotation of the maincorpus has reliable consistency among multipletranscribers.As a result, we obtained a corpus with consistentannotation of prosodic breaks.
The data used invalidation experiment is included as well.
The sta-tistics of the constructed corpus is shown in Table8.Data # eo-jeols # sentencesData set from valida-tion experiment1,149 46Main corpus 29,663 1,319Total 30,812 1,365Table 8 Size of resultant corpusIt took approximately three months for us to traintranscribers, annotate main corpus and validate thereliability of intertranscriber agreement in the maincorpus.
Considering the size of the constructedcorpus, three months might be regarded as a consi-derable amount of time for researchers who wantto build a large-scale annotated corpus.
However,most time was spent on analyzing the inconsisten-cies among transcribers in initial experiments dur-ing the training step.
Hence, if transcribers aretrained following the suggested method in this pa-per, the amount of time for transcribers to annotatethe target corpus with reliable consistency will de-crease dramatically compared with the time for alltranscribers to annotate prosodic breaks in the en-tire corpus.5 ConclusionsIn this study, potential problems in the construction,collection and utilization of a speech annotationcorpus have been identified, and a solution foreach type of problem has been suggested.
Theoverall procedure of training transcribers, taggingthe main corpus and validating the reliability ofintertranscriber agreement on the main corpus hasalso been specifically described.
As a result, weobtained a corpus with consistent annotation ofprosodic breaks.
The estimated pairwise agreementof annotation of the main corpus is between 0.7477and 0.7916 and K is between 0.7057 and 0.7569.The suggested method for constructing a consis-tently annotated corpus and validating the consis-tency of the resultant annotation must be appliedprior to implementation of data-driven models forpredicting prosodic breaks.
As our future work, theresultant corpus will be used for building a robustprediction model of prosodic boundary.In addition, the method can be utilized for seman-tic annotation tasks, discourse tagging and others,which have a similar problem due to the differingperceptions of transcribers in recognizing theclosely related categories.AcknowledgementThis work was supported by the National Re-search Foundation of Korea(NRF) grant funded bythe Korea government(MEST) (2010-0028784).45ReferencesAbeer Alwan.
2008.
Dealing with Limited and NoisyData in ASR: a Hybrid Knowledge-based and Statis-tical Approach, Proc.
Interspeech 2008, BrisbaneAustralia, , pp.
11-15.Amy J. Schafer.
1997.
Prosodic Parsing: The Role ofProsody in Sentence Comprehension, University ofMassachusetts.Ann K. Syrdal and Julia McGory.
2000.
Inter-transcriber Reliability of ToBI Prosodic Labeling,Proc.Interspeech 2000, pp.
235-238.Barbara Di Eugenio.
2000.
On the usage of Kappa toevaluate agreement on coding tasks, Proc.
Second In-ternational Conference on Language Resources andEvaluation, pp.441-444.Catherine Mayo, Matthew Aylett, D. Robert Ladd.
1996.Prosodic Transcription of Glasgow English: AnEvaluation Study of GlaToBI, Proc.
ESCA Work-shop on Intonation: Theory, Models and Applications,Athens Greece, pp.231-234.Christiane Fellbaum, Joachim Grabowski and ShariLandes.
1999.
Performance and Confidence in a Se-mantic Annotation Task, WordNet: An ElectronicLexical Database etd.
Fellbaum, MIT Press, London.Colin W. Wightman and Mari Ostendorf.
1994.
Auto-matic Labeling of Prosodic Patterns, IEEE Transac-tions on Speech and Audio Processing, 2(4):469-481.Hee Tou Ng, Chung Yong Lim and Shou King Foo.1999.
A Case Study on Inter-Annotator Agreementfor Word Sense Disambiguation, Proc.
ACLSIGLEX Workshop on Standardizing Lexical Re-sources pp.
9-13.Ho-Young Lee.
2004.
H and L are Not Enough in Into-national Phonology, Korean Journal of Linguistics,39:71-79.Hyuk-Chul Kwon, Mi-young Kang and Sung-Ja Choi.2004.
Stochastic Korean Word Spacing with Smooth-ing Using Korean Spelling Checker, ComputerProcessing of Oriental Languages, 17:239-252.Jean Carletta.
1996.
Assessing Agreement on Classifica-tion Tasks: The Kappa Statistic, Computational Lin-guistics,  22( 2):249-254.K.
Ross and M. Ostendorf.
1996.
Prediction of abstractprosodic labels for speech synthesis, ComputerSpeech and Language, 10(3):155-185.M.
C?u Viana, Lu?s C. Oliveira and Ana I. Mata.
2003.Prosodic Phrasing: Machine and Human Evaluation,International Journal of Speech Technology, 6:83-94.M.
Maragoudakis, P. Zervas, N. Fakotakis and G. Kok-kinakis.
2003.
A Data-Driven Framework for Intona-tional Phrase Break Prediction, Lecture Notes inComputer Science, 2807: 189-197.M.
Ostendorf and N. Veilleux.
1994.
A HierarchicalStochastic Model for Automatic Prediction of Pro-sodic Boundary Location, Computational Linguistics,20(1):27-54.Margaret M. Kjelgaard and Shari R. Speer.
1999.
Pro-sodic Facilitation and Interference in the Resolutionof Temporary Syntactic Closure Ambiguity, Journalof Memory and Language, 40:153-194.Martine Grice, Matthias Reyelt, Ralf Benzmuller, J?rgMayer and Anton Batliner.
1996.
Consistency inTranscription and Labelling of German Intonationwith GToBI, Proc.
Interspeech1996, pp.
1716-1719.Mary E. Beckman, John F. Pitrelli and Julia Hirschberg.1994.
Evaluation of Prosodic Transcription LabelingReliability in the ToBI Framework, Proc.
Interspeech1994, pp.
123-126.Nancy Ide.
2007.
Annotation Science From theory toPractice and Use: Data Structures for Linguistics Re-sources and Applications, Proc.
Bienniel GLDVConference, T?bingen, Germany.Nancy Ide and Keith Suderman.
2006.
Integrating Lin-guistic Resources: The American National CorpusModel, Proceedings of the Fifth Language Resourcesand Evaluation Conference, Genoa, Italy.Sangjun Kim.
1991.
Study on Broadcast Language,Hongwon, Seoul.Sun-Ah Jun.
2006.
Prosody in Sentence Processing:Korean vs. English, UCLA Working Papers in Pho-netics, 104:26-45.Sun-Ah Jun, Sook-Hyang Lee, Keeho Kim, Yong-JuLee.
2000.
Labler agreement in Transcribing KoreanIntonation with K-ToBI, Proc.
Interspeech 2000, pp.211-214.Youngim Jung, Sunho Cho, Aesun Yoon and Hyuk-Chul Kwon.
2008.
Prediction of Prosodic Break Us-ing Syntactic Relations and Prosodic Features, Ko-rean Journal of Cognitive Science, 19(1):89 -105.WaveSurfer.
WaveSurfer ver.1.8.5,http://crfpp.sourceforge.net/.46
