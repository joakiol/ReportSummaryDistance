Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 70?78,Beijing, August 2010More Languages, More MAP?
: A Study of Multiple Assisting Languagesin Multilingual PRFVishal Vachhani Manoj K. Chinnakotla Mitesh M. Khapra Pushpak BhattacharyyaDepartment of Computer Science and Engineering,Indian Institute of Technology Bombay{vishalv,manoj,miteshk,pb}@cse.iitb.ac.inAbstractMultilingual Pseudo-Relevance Feedback(MultiPRF) is a framework to improvethe PRF of a source language by takingthe help of another language called as-sisting language.
In this paper, we ex-tend the MultiPRF framework to includemultiple assisting languages.
We considerthree different configurations to incorpo-rate multiple assisting languages - a) Par-allel - all assisting languages combinedsimultaneously b) Serial - assisting lan-guages combined in sequence one afteranother and c) Selective - dynamically se-lecting the best feedback model for eachquery.
We study their effect on MultiPRFperformance.
Results using multiple as-sisting languages are mixed and it helps inboosting MultiPRF accuracy only in somecases.
We also observe that MultiPRF be-comes more robust with increase in num-ber of assisting languages.1 IntroductionPseudo-Relevance Feedback (PRF) (Buckley etal., 1994; Xu and Croft, 2000; Mitra et al, 1998)is known to be an effective technique to im-prove the effectiveness of Information Retrieval(IR) systems.
In PRF, the top ?k?
documentsfrom the ranked list retrieved using the initial key-word query are assumed to be relevant.
Later,these documents are used to refine the user queryand the final ranked list is obtained using theabove refined query.
Although PRF has beenshown to improve retrieval, it suffers from thefollowing drawbacks: (a) Lexical and SemanticNon-Inclusion: the type of term associations ob-tained for query expansion is restricted to onlyco-occurrence based relationships in the feedbackdocuments and (b) Lack of Robustness: due tothe inherent assumption in PRF, i.e., relevanceof top k documents, performance is sensitive tothat of the initial retrieval algorithm and as a re-sult is not robust.
Typically, larger coverage en-sures higher proportion of relevant documents inthe top k retrieval (Hawking et al, 1999).
How-ever, some resource-constrained languages do nothave adequate information coverage in their ownlanguage.
For example, languages like Hungarianand Finnish have meager online content in theirown languages.Multilingual Pseudo-Relevance Feedback(MultiPRF) (Chinnakotla et al, 2010a) is anovel framework for PRF to overcome the abovelimitations of PRF.
It does so by taking the help ofa different language called the assisting language.Thus, the performance of a resource-constrainedlanguage could be improved by harnessing thegood coverage of another language.
MulitiPRFshowed significant improvements on standardCLEF collections (Braschler and Peters, 2004)over state-of-art PRF system.
On the web, eachlanguage has its own exclusive topical coveragebesides sharing a large number of common topicswith other languages.
For example, informationabout Saudi Arabia government policies andregulations is more likely to be found in Arabiclanguage web and also information about a localevent in Spain is more likely to be covered inSpanish web than in English.
Hence, usingmultiple languages in conjunction is more likelyto ensure satisfaction of the user information needand hence will be more robust.In this paper, we extend the MultiPRF frame-work to multiple assisting languages.
We study70the various possible ways of combining the mod-els learned from multiple assisting languages.
Wepropose three different configurations for includ-ing multiple assisting languages in MultiPRF - a)Parallel b) Serial and c) Selective.
In Parallel com-bination, all the assisting languages are combinedsimultaneously using interpolation.
In Serial con-figuration, the assisting languages are applied insequence one after another and finally, in Selec-tive configuration, the best feedback model is dy-namically chosen for each query.
We experimentwith each of the above configurations and presentboth quantitative and qualitative analysis of the re-sults.
Results using multiple assisting languagesare mixed and it helps in boosting MultiPRF ac-curacy only in some cases.
We also observe thatMultiPRF becomes more robust with increase innumber of assisting languages.
Besides, we alsostudy the relation between number of assistinglanguages, coverage and the MultiPRF accuracy.The paper is organized as follows: Section 2,explains the Language Modeling (LM) based PRFapproach.
Section 3, describes the MultiPRF ap-proach.
Section 4 explains the various configu-rations to extend MultiPRF for multiple assistinglanguages.
Section 6 presents the results and dis-cussions.
Finally, Section 7 concludes the paper.2 PRF in the LM FrameworkThe Language Modeling (LM) Framework allowsPRF to be modeled in a principled manner.
In theLM approach, documents and queries are mod-eled using multinomial distribution over wordscalled document language model P (w|D) andquery language model P (w|?Q) respectively.
Fora given query, the document language models areranked based on their proximity to the query lan-guage model, measured using KL-Divergence.KL(?Q||D) =?wP (w|?Q) ?
logP (w|?Q)P (w|D)Since the query length is short, it is difficult to es-timate ?Q accurately using the query alone.
InPRF, the top k documents obtained through theinitial ranking algorithm are assumed to be rele-vant and used as feedback for improving the es-timation of ?Q.
The feedback documents con-tain both relevant and noisy terms from whichSymbol Description?Q Query Language Model?FL1 Feedback Language Model obtained from PRF in L1?FL2 Feedback Language Model obtained from PRF in L2?TransL1 Feedback Model Translated from L2 to L1t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1?, ?
Interpolation coefficients coefficients used in MultiPRFTable 1: Glossary of Symbols used in explaining MultiPRFthe feedback language model is inferred based ona Generative Mixture Model (Zhai and Lafferty,2001).Let DF = {d1, d2, .
.
.
, dk} be the top k doc-uments retrieved using the initial ranking algo-rithm.
Zhai and Lafferty (Zhai and Lafferty, 2001)model the feedback document setDF as a mixtureof two distributions: (a) the feedback languagemodel and (b) the collection model P (w|C).
Thefeedback language model is inferred using the EMAlgorithm (Dempster et al, 1977), which itera-tively accumulates probability mass on the mostdistinguishing terms, i.e.
terms which are morefrequent in the feedback document set than in theentire collection.
To maintain query focus the fi-nal converged feedback model, ?F is interpolatedwith the initial query model ?Q to obtain the finalquery model ?Final.
?Final = (1?
?)
?
?Q + ?
?
?F?Final is used to re-rank the corpus using theKL-Divergence ranking function to obtain the fi-nal ranked list of documents.
Henceforth, we referto the above technique as Model Based Feedback(MBF).3 Multilingual Pseudo-RelevanceFeedback (MultiPRF)Chinnakotla et al (Chinnakotla et al, 2010a;Chinnakotla et al, 2010b) propose the MultiPRFapproach which overcomes the fundamental limi-tations of PRF with the help of an assisting collec-tion in a different language.
Given a query Q inthe source language L1, it is automatically trans-lated into the assisting language L2.
The docu-ments in the L2 collection are ranked using thequery likelihood ranking function (John Laffertyand Chengxiang Zhai, 2003).
Using the top k doc-uments, they estimate the feedback model usingMBF as described in the previous section.
Simi-larly, they also estimate a feedback model using71the original query and the top k documents re-trieved from the initial ranking in L1.
Let the re-sultant feedback models be ?FL2 and ?FL1 respec-tively.
The feedback model estimated in the as-sisting language ?FL2 is translated back into lan-guage L1 using a probabilistic bi-lingual dictio-nary t(f |e) from L2 ?
L1 as follows:P (f |?TransL1 ) =??
e in L2t(f |e) ?
P (e|?FL2) (1)The probabilistic bi-lingual dictionary t(f |e) islearned from a parallel sentence-aligned corporain L1 ?
L2 based on word level alignments.
Theprobabilistic bi-lingual dictionary acts as a richsource of morphologically and semantically re-lated feedback terms.
Thus, the translation modeladds related terms in L1 which have their sourceas the term from feedback model ?FL2 .
The finalMultiPRF model is obtained by interpolating theabove translated feedback model with the originalquery model and the feedback model of languageL1 as given below:?MultiL1 = (1?
?
?
?)
?
?Q + ?
?
?FL1 + ?
?
?TransL1(2)In order to retain the query focus during backtranslation, the feedback model in L2 is interpo-lated with the translated query before translationof the L2 feedback model.
The parameters ?
and?
control the relative importance of the originalquery model, feedback model of L1 and the trans-lated feedback model obtained from L1 and aretuned based on the choice of L1 and L2.4 Extending MultiPRF to MultipleAssisting LanguagesIn this section, we extend the MultiPRF modeldescribed earlier to multiple assisting languages.Since each language produces a different feed-back model, there could be different ways of com-bining these models as suggested below.Parallel: One way is to include the new assist-ing language model using one more interpo-lation coefficient which gives the effect of us-ing multiple assisting languages in parallel.Serial: Alternately, we can have a serial combi-nation wherein language L2 is first assistedInitial Retrieval(LM Based Query Likelihood)Top ?k?ResultsPRF(Model Based Feedback)L  IndexFinal Ranked List Of Documents in LFeedback ModelInterpolationRelevance ModelTranslationKL-Divergence Ranking FunctionFeedback Model  ?L1Feedback Model ?LQuery in L Translated Query to L1ProbabilisticDictionaryL1?
LQuery Model ?QTranslated Query to LnInitial Retrieval(LM Based Query Likelihood)Top ?k?ResultsPRF(Model Based Feedback)L1 IndexRelevance ModelTranslationFeedback Model  ?LnInitial RetrievalTop ?k?ResultsPRF(Model Based Feedback)LnIndexProbabilisticDictionaryLn?
LFigure 1: Schematic of the Multilingual PRF Approach Us-ing Parallel AssistanceInitial Retrieval Algorithm(LM Based Query Likelihood)Top ?k?ResultsPRF(Model Based Feedback)FeedbackModel InterpolationKL-Divergence Ranking FunctionL IndexInitial Retrieval Algorithm(LM Based Query Likelihood)Top ?k?
ResultsPRF(Model Based Feedback)L2  IndexRelevance ModelTranslationL1 IndexFeedback Model    ?L1Query in L1Initial Retrieval Algorithm(LM Based Query Likelihood)Top ?k?
ResultsPRF(Model Based Feedback)FeedbackModel InterpolationFeedback Model    ?L2Query in L2Top ?k?
ResultsPRF(Model Based Feedback) KL Divergence RankingProbabilisticDictionaryL2 ?
L1Relevance ModelTranslationProbabilisticDictionaryL1?
LFeedback Model    ?LQuery Model ?QFigure 2: Schematic of the Multilingual PRF Approach Us-ing Serial Assistanceby language L3 and then this MultiPRF sys-tem is used to assist the source language L1.Selective: Finally, we can have selective assis-tance wherein we dynamically select whichassisting language to use based on the inputquery.Below we describe each of these systems in detail.4.1 Parallel CombinationThe MultiPRF model as explained in section 3 in-terpolates the query model of L1 with the MBFof L1 and the translated feedback model of theassisting language L2.
The most natural exten-sion to this approach is to translate the query intomultiple languages instead of a single languageand collect the feedback terms from the initial re-72Language CLEF Collection Identifier DescriptionNo.
ofDocumentsNo.
of UniqueTerms CLEF Topics (No.
of Topics)English EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)French FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)German DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)Finnish FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)Table 2: Details of the CLEF Datasets used for Evaluating the MultiPRF approach.
The number shown in brackets of the finalcolumn CLEF Topics indicate the actual number of topics used during evaluation.trieval of each of these languages.
The translatedfeedback models resulting from each of these re-trievals can then be interpolated to get the finalparallel MultiPRF model.
Specifically, if L1 is thesource language and L2, L3, .
.
.
Ln are assistinglanguages then final parallel MultiPRF model canbe obtained by generalizing Equation 2 as shownbelow:?MultiAssistL1 = (1?
?
?Xi?i) ?
?Q + ?
?
?F +Xi?i ?
?TransLi(3)The schematic representation of parallel combina-tion is shown in Figure 1.4.2 Serial CombinationLet L1 be the source language and let L2 and L3be two assisting languages.
A serial combinationcan then be achieved by cascading two MultiPRFsystems as described below:1.
Construct a MultiPRF system with L2 asthe source language and L3 as the assist-ing language.
We call this system as L2L3-MultiPRF system.2.
Next, construct a MultiPRF system with L1as the source language and L2L3-MultiPRFas the assisting system.As compared to a single assistance system whereonly L2 is used as the assisting language forL1, here the performance of language L2 is firstboosted using L3 as the assisting language.
Thisboosted system is then used for assisting L1.
Alsonote that unlike parallel assistance here we donot introduce an extra interpolation co-efficient inthe original MultiPRF model given in Equation 2.The schematic representation of serial combina-tion is shown in Figure 2.4.3 Selective AssistanceWe motivate selective assistance by posing thefollowing question: ?Given a source languageL1 and two assisting languages L2 and L3, isit possible that L2 is ideal for assisting somequeries whereas L3 is ideal for assisting someother queries??
For example, suppose L2 has arich collection of TOURISM documents whereasL3 has a rich collection of HEALTH documents.Now, given a query pertaining to TOURISM do-main one might expect L2 to serve as a better as-sisting language whereas given a query pertainingto the HEALTH domain one might expect L3 toserve as a better assisting language.
This intuitioncan be captured by suitably changing the interpo-lation model as shown below:?BestL = SelectBestModel(?FL ,?TransL1 ,?TransL2 ,?TransL12 )?MultiL1 = (1?
?)
?
?Q + ?
?
?BestL (4)where, SelectBestModel() gives the bestmodel for a particular query using the algorithmmentioned below which is based on minimizingthe query drift as described in (?):1.
Obtain the four feedback models, viz.,?FL ,?TransL1 ,?TransL2 ,?TransL122.
Build a language model (say, LM ) usingqueryQ and top-100 documents of initial re-trieval in language L.3.
Find the KL-Divergence between LM andthe four models obtained during step 1.4.
Select the model which has minimum KL-Divergence score from LM .
Call this model?BestL .5.
Get the final model by interpolating thequery model, ?Q, with ?BestL .735 Experimental SetupWe evaluate the performance of our system us-ing the standard CLEF evaluation data in six lan-guages, widely varying in their familial relation-ships - Dutch, German, English, French, Spanishand Finnish.
The details of the collections andtheir corresponding topics used for MultiPRF aregiven in Table 2.
Note that, in each experiment,we choose assisting collections such that the top-ics in the source language are covered in the as-sisting collection so as to get meaningful feedbackterms.
In all the topics, we only use the title field.We ignore the topics which have no relevant docu-ments as the true performance on those topics can-not be evaluated.We use the Terrier IR platform (Ounis et al,2005) for indexing the documents.
We performstandard tokenization, stop word removal andstemming.
We use the Porter Stemmer for Englishand the stemmers available through the Snowballpackage for other languages.
Other than these,we do not perform any language-specific process-ing on the languages.
In case of French, sincesome function words like l?, d?
etc., occur as pre-fixes to a word, we strip them off during index-ing and query processing, since it significantly im-proves the baseline performance.
We use standardevaluation measures like MAP, P@5 and P@10for evaluation.
Additionally, for assessing robust-ness, we use the Geometric Mean Average Preci-sion (GMAP) metric (Robertson, 2006) which isalso used in the TREC Robust Track (Voorhees,2006).
The probabilistic bi-lingual dictionaryused in MultiPRF was learnt automatically by run-ning GIZA++: a word alignment tool (Och andNey, 2003) on a parallel sentence aligned corpora.For all the above language pairs we used the Eu-roparl Corpus (Philipp, 2005).
We use GoogleTranslate as the query translation system as it hasbeen shown to perform well for the task (Wu etal., 2008).
We use two-stage Dirichlet smooth-ing with the optimal parameters tuned based onthe collection (Zhai and Lafferty, 2004).
We tunethe parameters of MBF, specifically ?
and ?, andchoose the values which give the optimal perfor-mance on a given collection.
We observe that theoptimal parameters ?
and ?
are uniform acrosscollections and vary in the range 0.4-0.48.
WeSourceLangsAssist.LangsMBF MultiPRF(L1)MultiPRF(L2)MultiPRF(L1,L2)ENDE-NLMAP 0.4495 0.4464 0.4471 0.4885(4.8)?P@5 0.4955 0.4925 0.5045 0.5164(2.4)P@10 0.4328 0.4343 0.4373 0.4463(2.1)DE-FIMAP 0.4495 0.4464 0.4545 0.4713(3.7)?P@5 0.4955 0.4925 0.5194 0.5224(1.2)P@10 0.4328 0.4343 0.4373 0.4507(3.1)NL-ESMAP 0.4495 0.4471 0.4566 0.4757(4.2)?P@5 0.4955 0.5045 0.5164 0.5224(0.6)P@10 0.4328 0.4373 0.4537 0.4448(2.4)ES-FRMAP 0.4495 0.4566 0.4563 0.48(5.1)?P@5 0.4955 0.5164 0.5075 0.5224(1.2)P@10 0.4328 0.4537 0.4343 0.4388(-3.3)ES-FIMAP 0.4495 0.4566 0.4545 0.48(5.1)?P@5 0.4955 0.5164 0.5194 0.5254(1.7)P@10 0.4328 0.4537 0.4373 0.4403(-3.0)FR-FIMAP 0.4495 0.4563 0.4545 0.4774(4.6)P@5 0.4955 0.5075 0.5194 0.5284(4.1)?P@10 0.4328 0.4343 0.4373 0.4373(0.7)FIEN-FRMAP 0.3578 0.3411 0.3553 0.3688(3.8)P@5 0.3821 0.394 0.397 0.4149(4.5)?P@10 0.3105 0.3463 0.3433 0.3433(0.1)NL-DEMAP 0.3578 0.3722 0.3796 0.3929(3.5)P@5 0.3821 0.406 0.403 0.4149(3.0)P@10 0.3105 0.3478 0.3582 0.3597(0.4)ES-DEMAP 0.3578 0.369 0.3796 0.4058(6.9)?P@5 0.3821 0.4119 0.403 0.4239(5.2)P@10 0.3105 0.3448 0.3582 0.3612(0.8)FR-DEMAP 0.3578 0.3553 0.3796 0.3988(5.1)?P@5 0.3821 0.397 0.403 0.406(0.7)P@10 0.3105 0.3433 0.3582 0.3507(-2.1)NL-ESMAP 0.3578 0.3722 0.369 0.3875(4.1)?P@5 0.3821 0.406 0.4119 0.4060.0)P@10 0.3105 0.3478 0.3448 0.3537(1.7)NL-FRMAP 0.3578 0.3722 0.3553 0.3875(4.1)?P@5 0.3821 0.406 0.397 0.409(0.7)P@10 0.3105 0.3478 0.3433 0.3463(-0.4)ES-FRMAP 0.3578 0.369 0.3553 0.3823(3.6)P@5 0.3821 0.4119 0.397 0.4119(0.0)P@10 0.3105 0.3448 0.3433 0.3418(-0.9)FR EN-ESMAP 0.4356 0.4658 0.4634 0.4803(3.1)P@5 0.4776 0.4925 0.4925 0.4985(1.2)P@10 0.4194 0.4358 0.4388 0.4493(3.1)?Table 3: Comparison of MultiPRF Multiple Assisting Lan-guages using parallel assistance framework with MultiPRFwith single assisting language.
Only language pairs wherepositive improvements were obtained are reported here.
Re-sults marked as ?
indicate that the improvement was sta-tistically significant over baseline (Maximum of MultiPRFwith single assisting language) at 90% confidence level (?
=0.01) when tested using a paired two-tailed t-test.uniformly choose the top ten documents for feed-back.6 Results and DiscussionTables ??
and ??
present the results for Multi-PRF with two assisting languages using paral-lel assistance and selective assistance framework.Out of the total 60 possible combinations, in Ta-ble ?
?, we only report the combinations wherewe have obtained positive improvements greaterthan 3%.
We observe most improvements in En-glish, Finnish and French.
We did not observe anyimprovements using the serial assistance frame-work over MultiPRF with single assisting lan-74SourceLangsAssist.LangsParallel Model Selective ModelEN DE-NLMAP 0.4651 0.4848P@5 0.5254 0.5224P@10 0.4493 0.4522NL-FIMAP 0.4387 0.4502P@5 0.5015 0.5164P@10 0.4284 0.4358DEEN-FRMAP 0.4097 0.4302P@5 0.594 0.5851P@10 0.5149 0.5179FR-ESMAP 0.4215 0.4333P@5 0.591 0.591P@10 0.5239 0.5209FR-NLMAP 0.4139 0.4236P@5 0.5701 0.5701P@10 0.5075 0.5134FR-FIMAP 0.3925 0.4055P@5 0.5101 0.5642P@10 0.4851 0.5NL-FIMAP 0.3974 0.4192P@5 0.5731 0.5612P@10 0.497 0.503ES EN-FIMAP 0.4436 0.4501P@5 0.6179 0.6269P@10 0.5567 0.5657DE-FIMAP 0.4542 0.465P@5 0.6269 0.6179P@10 0.5627 0.5582NL-FIMAP 0.4531 0.4611P@5 0.6269 0.6299P@10 0.5627 0.5627Table 4: Results showing the positive improvements of Mul-tiPRF with selective assistance framework over MultiPRFwith parallel assistance framework.guage.
Hence, we do not report their results asthe results were almost equivalent to single as-sisting language.
As shown in Table ?
?, selec-tive assistance does give decent improvements insome language pairs.
An interesting point to notein selective assistance is that it helps languageslike Spanish whose monolingual performance anddocument coverage are both high.6.1 Qualitative Comparison of FeedbackTerms using Multiple LanguagesIn this section, we qualitatively compare the re-sults of MultiPRF with two assisting languageswith that of MultiPRF with single assisting lan-guage, based on the top feedback terms obtainedby each model.
Specifically, in Table 5 we com-pare the terms obtained by MultiPRF using (i)Only L1 as assisting language, (ii) Only L2 as as-sisting language and (iii) Both L1 and L2 as as-sisting languages in a parallel combination.
Forexample, the first row in the above table showsthe terms obtained by each model for the En-glish query ?Golden Globes 1994?.
Here, L1 isFrench and L2 is Spanish.
Terms like ?Gold?and ?Prize?
appearing in the translated feedbackmodel of L1 cause a drift in the topic towards?Gold Prize?
resulting in a lower MAP score(0.33).
Similarly, the terms like ?forrest?
and?spielberg?
appearing in the translated feedbackmodel of L2 cause a drift in topic towards For-rest Gump and Spielberg Oscars resulting in aMAP score (0.5).
However, when the modelsfrom two languages are combined, terms whichcause a topic drift get ranked lower and as a resultthe focus of the query is wrenched back.
A sim-ilar observation was made for the English query?Damages in Ozone Layer?
using French (L1)and Spanish (L2) as assisting languages.
Here,terms from the translated feedback model of L1cause a drift in topic towards ?militri bacteria?whereas the terms from the translated feedbackmodel of L2 cause a drift in topic towards ?iraqwar?.
However, in the combined model theseterms get lower rank there by bringing back thefocus of the query.
For the Finnish query ?Lastenoikeudet?
(Children?s Rights), in German (L1),the topic drift is introduced by terms like ?las,gram, yhteis?.
In case of Dutch (L2), the querydrift is caused by ?mandy, richard, slovakia?
(L2)and in the case of combined model, these termsget less weightage and the relevant terms like?laps, oikeuks, vanhemp?
which are common inboth models, receive higher weightage causing animprovement in query performance.Next, we look at a few negative examples wherethe parallel combination actually performs poorerthan the individual models.
This happens whensome drift-terms (i.e., terms which can causetopic drift) get mutually reinforced by both themodels.
For example, for the German query?Konkurs der Baring-Bank?
(Bankruptcy of Bar-ing Bank) the term ?share market?
which was ac-tually ranked lower in the individual models getsboosted in the combined model resulting in a driftin topic.
Similarly, for the German query ?Ehren-Oscar fu?r italienische Regisseure?
(Honorary Os-car for Italian directors) the term ?head office?which was actually ranked lower in the individualmodels gets ranked higher in the combined modeldue to mutual reinforcement resulting in a topicdrift.75TOPIC NO.QUERIES(Meaning inEng.
)TRANSLATED ENGLISHQUERIES(Assisting Lang.
)L1M APL2M APL1 - L2M APRepresentative Terms with L1 asSingle Assisting Language (WithM eaning)Representative Terms with L2 asSingle Assisting Language (WithMeaning)Representative Terms with L1& L2 asAssisting Langs.
(With Meaning)English ?03TOPIC 165 Globes 1994Golden Globes 1994 (FR)Globos de Oro 1994 (ES) 0.33 0.5 1Gold, prize, oscar, nomin, best award,hollywood , actor, director ,actress, world,won ,list, winner, televi , foreign ,year, pressworld, nomin, film, award, delici, planet,earth, actress, list, drama, director, actor,spielberg, music, movie, forrest, hankoscar, nomin, best, award, hollywood actor,director, cinema, televi , music, actress,drama, role, hank, foreign, goldFinnish '03TOPIC 152Lasten oikeudet(Children?sRights)Rechte des Kindes (DE)Kinderrechten (NL) 0.2 0.25 0.37laps (child), oikeuks (rights), oikeud (rights),kind, oikeus (right), is?
(father), oikeut(justify ), vanhemp (parent), vanhem(parents), las, gram, yhteis , unicef, sunt,?
iti(mother), yleissopimnks (conventions)oikeuks (rights), laps (child), oikeud (right),mandy , richard, slovakia , t?h?nast (to date),tuomar (judge), tyto , kid, , nuor (youngpeople), nuort (young ), sano(said) ,perustam(establishing)laps (child), oikeuks (rights), oikeud (rights),oikeus (right), is?
(father, parent), vanhemp(parent), vanhem (parents), oikeut (justify),las, mandy , nuort (young ), richard, nuor(young people), slovakia , t?h?nast (to date),English ?03TOPIC 148Damages inOzone LayerDommages ?
la couched'ozone (FR)Destrucci?n de la capa deozono (ES)0.08 0.07 0.2 damag, militri, uv , layer, condition, chemic, bacteria, ban, radiat, ultravioletdamag, weather, atmospher, earth, problem,report, research, harm, iraq , war, scandal,illigel, latin, hairdamag, uv , layer,weather , atmospher, earth,problem, report, research , utraviolet , chemicGerman '03TOPIC 180Konkurs derBaring -Bank(Bankruptcy ofBaring Bank )Bankruptcy of Barings (EN)BaringsinKonkurssi (FI) 0.55 0.51 0.33zentralbank(central bank),bankrott(bankcruptcy), investitionsbank, sigapur, london ,britisch, index, tokio, england,werbung(advertising), japanfall, konkurs, bankrott(Bankruptcy),warnsignal(warning), ignoriert,zusammenbruch (collepse), london, singapur,britisch(british), dollar, tokio, druck(pressur),handel(trade)aktienmarkt(share market), investitionsbank ,bankrott, zentralbank (central bank), federal,singapur, london, britisch, index, tokio, dollar,druck, england, dokument(document)German '03TOPIC 198Ehren-Oscar f?ritalienischeRegisseure(Honorary Oscarfor Italiandirectors)Honorary Oscar for ItalianDirectors (EN)Kunnia -Oscar italialaisilleelokuvaohjaajille (FI)0.5 0.35 0.2Direktor(director), film, regierungschef(prime), best antonionis, antonionins, lieb,geschicht(history) , paris, preis, berlin,monitor, kameraGeneraldirektion(General director), film,ehrenmitglied, regisseur, direktor, verleih ,itali, oscar, award, antonioninsgeneraldirektion(head office),ehrenmitglied(honorable member),regierungschef(prime), regisseur(director),oscar, genossenschaftsbank (corporatebank)Table 5: Qualitative Comparison of MultiPRF Results using two assisting languages with single assisting language.6.2 Effect of Coverage on MultiPRFAccuracyA study of the results obtained for MultiPRF usingsingle assisting language and multiple assistinglanguages with different source languages showedthat certain languages are more suited to be ben-efited by assisting languages.
In particular, lan-guages having smaller collections are more likelyto be benefited if assisted by a language having alarger collection size.
For example, Finnish whichhas the smallest collection (55344 documents)showed maximum improvement when supportedby assisting language(s).
Based on this observa-tion, we plotted a graph of the collection size of asource language v/s the average improvement ob-tained by using two assisting languages to see iftheir exists a correlation between these two fac-tors.
As shown in Figure 3, there indeed exists ahigh correlation between these two entities.
Atone extreme, we have a language like Spanishwhich has the largest collection (454045 docu-ments) and is not benefited much by assisting lan-guages.
On the other extreme, we have Finnishwhich has the smallest collection size and is ben-efited most by assisting languages.454.045 (Spanish)294.809 (German)190.604 (Dutch) 169.477 (English)129.806 (French)55.344 (Finnish)0501001502002503003504004505000 1 2 3 4 5 6 7Coverage(No.of Docs in Thousands)Avg.
Improvement in MAP of MultiPRF using two Assisting Languages (%)Figure 3: Effect of Coverage on Average MultiPRF MAPusing Two Assisting Languages.6.3 Effect of Number of Assisting Languageson MultiPRF AccuracyAnother interesting question which needs to beaddressed is ?Whether it helps to use more thantwo assisting languages??
and if so ?Is there anoptimum number of assisting languages beyondwhich there will be no improvement??.
To an-swer these questions, we performed experimentsusing 1-4 assisting languages with each sourcelanguage.
As seen in Figure 4, in general as thenumber of assisting languages increases the per-formance saturates (typically after 3 languages).Thus, for 5 out of the 6 source languages, the per-formance saturates after 3 languages which is inline with what we would intuitively expect.
How-ever, in the case of German, on an average, the760.
350.
370.
390.
410.
430.
450.
470.
490 2 4 6MAPNo.
of.
Assisting Langs.EnglishAvg.
MAPMBF0.
350.
370.
390.
410.
430.
450.
470 2 4 6MAPNo.
of Assisting Langs.FrenchAvg.
MAPMBF0.
350.
370.
390.
410.
430.
450 2 4 6MAPNo.
of Assisting Langs.FinnishAvg.
MAPMBF0.
350.
370.
390.
410.
430.
450 2 4 6MAPNo.
of Assisting Langs.GermanAvg.
MAPMBF0.
350.
370.
390.
410.
430.
450.
470 2 4 6MAPNo.
of Assisting Langs.DutchAvg.
MAPMBF0.
350.
370.
390.
410.
430.
450.
470.
490 2 4 6MAPNo.
of Assisting Langs.SpanishAvg.
MAPMBFFigure 4: Effect of Number of Assisting Languages on Avg.
MultiPRF Performance with Multiple Assistance.00.
050.
10.
150.
20.
250.
30.
350.
4English French German Spanish Dutch FinnishAvg.
GMAPSource LanguageMBF1234Figure 5: Effect of Number of Assisting Languages on Ro-bustness measured through GMAP.performance drops as the number of assisting lan-guages is increased.
This drop is counter intuitiveand needs further investigation.6.4 Effect of Number of Assisting Languageson RobustnessOne of the primary motivations for including mul-tiple assisting languages in MultiPRF was to in-crease the robustness of retrieval through bettercoverage.
We varied the number of assisting lan-guages for each source and studied the averageGMAP.
The results are shown in Figure 5.
Weobserve that in almost all the source languages,the GMAP value increases with number of assist-ing languages and then reaches a saturation afterreaching three languages.7 ConclusionIn this paper, we extended the MultiPRF frame-work to multiple assisting languages.
We pre-sented three different configurations for includingmultiple assisting languages - a) Parallel b) Serialand c) Selective.
We observe that the results aremixed with parallel and selective assistance show-ing improvements in some cases.
We also observethat the robustness of MultiPRF increases withnumber of assisting languages.
We analyzed theinfluence of coverage of MultiPRF accuracy andobserved that it is inversely correlated.
Finally,increasing the number of assisting languages in-creases the MultiPRF accuracy to some extent andthen it saturates beyond that limit.
Many of theabove results (negative results of serial, selectiveconfigurations etc.)
require deeper investigationwhich we plan to take up in future.ReferencesBraschler, Martin and Carol Peters.
2004.
Cross-language evaluation forum: Objectives, results,achievements.
Inf.
Retr., 7(1-2):7?31.Buckley, Chris, Gerald Salton, James Allan, and AmitSinghal.
1994.
Automatic query expansion usingsmart : Trec 3.
In Proceedings of The Third TextREtrieval Conference (TREC-3, pages 69?80.Chinnakotla, Manoj K., Karthik Raman, and Push-pak Bhattacharyya.
2010a.
Multilingual pseudo-77relevance feedback: English lends a helping hand.In ACM SIGIR 2010, Geneva, Switzerland, July.ACM.Chinnakotla, Manoj K., Karthik Raman, and Push-pak Bhattacharyya.
2010b.
Multilingual pseudo-relevance feedback: Performance study of assistinglanguages.
In ACL 2010, Uppsala, Sweeden, July.ACL.Dempster, A., N. Laird, and D. Rubin.
1977.
Maxi-mum Likelihood from Incomplete Data via the EMAlgorithm.
Journal of the Royal Statistical Society,39:1?38.Hawking, David, Paul Thistlewaite, and Donna Har-man.
1999.
Scaling up the trec collection.
Inf.
Retr.,1(1-2):115?137.John Lafferty and Chengxiang Zhai.
2003.
Proba-bilistic Relevance Models Based on Document andQuery Generation.
In Language Modeling for Infor-mation Retrieval, volume 13, pages 1?10.
KluwerInternational Series on IR.Mitra, Mandar, Amit Singhal, and Chris Buckley.1998.
Improving automatic query expansion.
InSIGIR ?98: Proceedings of the 21st annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 206?214,New York, NY, USA.
ACM.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Ounis, I., G. Amati, Plachouras V., B.
He, C. Macdon-ald, and Johnson.
2005.
Terrier Information Re-trieval Platform.
In Proceedings of the 27th Euro-pean Conference on IR Research (ECIR 2005), vol-ume 3408 of Lecture Notes in Computer Science,pages 517?519.
Springer.Philipp, Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Robertson, Stephen.
2006.
On gmap: and other trans-formations.
In CIKM ?06: Proceedings of the 15thACM international conference on Information andknowledge management, pages 78?83, New York,NY, USA.
ACM.Voorhees, Ellen.
2006.
Overview of the trec 2005robust retrieval track.
In E. M. Voorhees and L.P. Buckland, editors, The Fourteenth Text REtrievalConference, TREC 2005, Gaithersburg, MD.
NIST.Wu, Dan, Daqing He, Heng Ji, and Ralph Grishman.2008.
A study of using an out-of-box commercialmt system for query translation in clir.
In iNEWS?08: Proceeding of the 2nd ACM workshop on Im-proving non english web searching, pages 71?76,New York, NY, USA.
ACM.Xu, Jinxi and W. Bruce Croft.
2000.
Improving the ef-fectiveness of information retrieval with local con-text analysis.
ACM Trans.
Inf.
Syst., 18(1):79?112.Zhai, Chengxiang and John Lafferty.
2001.
Model-based Feedback in the Language Modeling ap-proach to Information Retrieval.
In CIKM ?01: Pro-ceedings of the tenth international conference on In-formation and knowledge management, pages 403?410, New York, NY, USA.
ACM Press.Zhai, Chengxiang and John Lafferty.
2004.
A Study ofSmoothing Methods for Language Models appliedto Information Retrieval.
ACM Transactions on In-formation Systems, 22(2):179?214.78
