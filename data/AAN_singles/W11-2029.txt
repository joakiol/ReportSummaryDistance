Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 266?271,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsLearning to Balance Grounding Rationales for Dialogue SystemsJoshua Gordon  Susan L. EpsteinDepartment of Computer Science Department of Computer ScienceRebecca J. Passonneau Hunter College andCenter for Computational Learning Systems The Graduate Center of the City Universityof New York Columbia UniversityNew York, NY, USA New York, NY, USA(joshua|becky)@cs.columbia.edu susan.epstein@hunter.cuny.eduAbstractThis paper reports on an experiment thatinvestigates clarification subdialogues inintentionally noisy speech recognition.The architecture learns weights for mix-tures of grounding strategies from exam-ples provided by a human wizardembedded in the system.
Results indicatethat the architecture learns to eliminatemisunderstandings reliably despite highword error rate.1 IntroductionWe seek to develop spoken dialogue systems(SDSs) that communicate effectively despite un-certain input.
Our thesis is that a task-orientedSDS can perform well despite a high degree ofrecognizer noise by relying on context.
The SDSdescribed here uses FORRSooth, a semi-synchronous architecture under development fortask-oriented human-computer dialogue.
Ourimmediate goals are to reduce non-understandings of user utterances (where theSDS produces no interpretation) and to eliminatemisunderstandings (where the SDS misinterpretsuser utterances).
The experiment recounted hereinvestigates subdialogues consisting of an initialuser response to a system prompt, and any sub-sequent turns that might be needed to result infull understanding of the original response.
Ourprincipal finding is that a FORRSooth-basedSDS learns to build on partial understandingsand to eliminate misunderstandings despite noi-sy ASR.A FORRSooth-based SDS is intended to inte-ract effectively ?without the luxury of perfectcomponents?
(Paek and Horvitz, 2000), such ashigh-performance ASR.
FORRSooth relies onportfolios of strategies for utterance interpreta-tion and grounding, and learns to balance themfrom its experience.
Its confidence in its inter-pretations is dynamically calibrated against itspast experience.
At each user utterance, FORR-Sooth selects grounding actions modulated tobuild upon partial interpretations in subsequentexchanges with the user.The experiment presented here bootstraps theSDS with human expertise.
In a Wizard of Oz(WOz) study, a person (the wizard) replaces se-lected SDS components.
Knowledge is then ex-tracted from the wizard?s behavior to improvethe SDS.
FORRSooth uses the Relative SupportWeight Learning (RSWL) algorithm (Epstein andPetrovic, 2006) to learn weights that balance itsindividual strategies.
Training examples forgrounding strategies are based upon examplesproduced by an ablated wizard who was re-stricted to the same information and actions asthe system (Levin and Passonneau, 2006).Our domain is the Andrew Heiskell Brailleand Talking Book Library.
Heiskell?s patrons or-der their books by telephone, during conversa-tion with a librarian.
The next section of thispaper presents related work.
Subsequent sectionsdescribe the weight learning, the SDS architec-ture, and an experiment that challenges the ro-bustness of utterance interpretation andgrounding with intentionally noisy ASR.
We266conclude with a discussion of the results.2 Related WorkDespite increasingly accurate ASR methods, di-alogue systems often contend with noisy ASR,which can arise from performance phenomenasuch as filled pauses (er, um), false starts (fir-last name), or noisy transmission conditions.SDSs typically experience a higher WER whendeployed.
For example, the WER reported forCarnegie Mellon University?s Let?s Go Public!went from 17% under controlled conditions to68% in the field (Raux et al, 2005).To limit communication errors, an SDS canrely on strategies to detect and recover from in-correct recognition output (Bohus, 2007).
Onesuch strategy, to ask the user to repeat a poorlyunderstood utterance, can result in hyperarticula-tion and decreased recognition (Litman,Hirschberg and Swerts, 2006).
Prior work hasshown that users prefer explicit confirmationover dialogue efficiency (fewer turns) (Litmanand Pan, 1999).
We hypothesize that this resultsfrom an inherent tradeoff between efficiency anduser confidence.
We assume that evidence ofpartial understanding increases user confidencemore than evidence of non-understanding does.FORRSooth learns to ask more questions thatbuild on partial information, and to make fewerexplicit confirmations and requests to the user torepeat herself.While many techniques exist in the literaturefor semantic interpretation in task-oriented, in-formation-seeking dialogue systems, there is nosingle preferred approach.
SDSs rarely combinea portfolio of NLU (natural language under-standing) resources.
FORRSooth relies on ?mul-tiple processes for interpreting utterances (e.g.,structured parsing versus statistical techniques)?as in (Lemon, 2003).
These range from voicesearch (querying a database directly with ASRresults) to semantic parsing.Dialogue systems should ground their under-standing of the user?s objectives.
To limit com-munication errors, an SDS can rely on strategiesto detect and recover from incorrect recognitionoutput (Bohus, 2007).
In others?
work, thegrounding status of an utterance is typically bi-nary (i.e., understood or not) (Allen, Fergusonand Stent, 2001; Bohus and Rudnicky,2005; Paek and Horvitz, 2000) or ternary (i.e.,understood, misunderstood, not understood)(Bohus and Rudnicky, 2009).
FORRSooth?sgrounding decisions rely on a mixture of strate-gies, are based on degrees of evidence (Bohusand Rudnicky, 2009; Roque and Traum, 2009),and disambiguate among candidate interpreta-tions.
Work in (DeVault and Stone, 2009) ondisambiguation in task-oriented dialogue differsfrom ours in that it addresses genuine ambigui-ties rather than noise resulting from inaccurateASR.3 FORR and RSWLFORRSooth is based on FORR (FOr the RightReasons), an architecture for learning and prob-lem solving (Epstein, 1994).
FORR uses se-quences of decisions from multiple rationales tosolve problems.
Implementations have provedrobust in game learning, simulated pathfinding,and constraint solving.
FORR relies on an adap-tive, hierarchical mixture of resource-boundedprocedures called Advisors.
Each Advisor em-bodies a decision rationale.
Advisors?
opinions(comments) are combined to arrive at a decision.Each comment pairs an action with a strengththat indicates some degree of support for or op-position to that action.
An Advisor can makemultiple comments at once, and can base itscomments upon descriptives.
A descriptive is ashared data structure, computed on demand, andrefreshed only when required.
For each decision,FORR consults three tiers of Advisors, one tierat a time, until some tier reaches a decision.FORR learns weights for its tier-3 Advisorswith RSWL.
Relative support is a measure of thenormalized difference between the commentstrength (confidence) with which an Advisorsupports an action compared to other availablechoices.
RSWL learns Advisors?
weights fromtheir comments on training examples.
The de-gree of reinforcement (positive or negative) toan Advisor's weight is proportional to itsstrength and relative support for a decision.4 FORRSoothFORRSooth is a parallelized version of FORR.It models task-oriented dialogue with six FORR-based services that operate concurrently: INTE-267RACTION, INTERPRETATION, SATISFACTION,GROUNDING, GENERATION, and DISCOURSE.These services interpret user utterances with re-spect to system expectations, manage the con-versational floor, and consider competinginterpretations, partial understandings, and alter-native courses of action.
All services haveaccess to the same data, represented by descrip-tives.
In this section, we present background onSATISFACTION and INTERPRETATION, and pro-vide additional detail on GROUNDING.The role of SATISFACTION is to represent di-alogue goals, and to progress towards thosegoals through spoken interaction.
Dialogue goalsare represented as agreements.
An agreement isa subdialogue about a target concept (such as aspecific book) whose value must be groundedthrough collaborative dialogue between the sys-tem and the user (Clark and Schaefer, 1989).Agreements are organized into an agreementgraph that represents dependencies among them.Task-based agreements are domain specific,while grounding agreements are domain inde-pendent (cf.
(Bohus, 2007)).
An interpretationhypothesis represents the system?s belief that thevalue of a specific target (e.g., a full name or afirst name) occurred in the user?s speech.The role of INTERPRETATION is to formulatehypotheses representing the meaning of what theuser has said.
INTERPRETATION relies on tier-3Advisors (essentially, mixtures of heuristics).Each Advisor constructs comments on speechrecognition hypotheses.
A comment is a seman-tic concept (hypothesis) with an associatedstrength.
More than one Advisor can vote for thesame hypothesis.
Confidence in any one hypo-thesis is a function of votes, learned weights forAdvisors, and comment strengths.In previous work, we showed that INTERPRE-TATION Advisors can produce relatively reliablehypotheses given noisy ASR, with graceful de-gradation  as recognition performance decreases(Gordon, Passonneau and Epstein, 2011).
Forexample, at WER between 0.2 and 0.4, the con-cept accuracy of the top hypothesis was 80%.That work left open how to decide whether touse the top INTERPRETATION hypothesis.
HereFORRSooth learns how to assess its INTERPRE-TATION confidence, and what grounding actionsto take given different levels of confidence.Over the life of a FORRSooth SDS, INTER-PRETATION produces hypotheses for the valuesof target concepts.
FORRSooth records the meanand variance of the comment strengths for eachINTERPRETATION hypothesis, and uses them tocalculate INTERPRETATION?s merit.
Meritrepresents FORRSooth?s INTERPRETATION con-fidence as a dynamic, normalized estimate of thepercentile in which the value falls.
Merit compu-tations improve initially with use of the SDS,and can then shift with the user population andthe data.
FORRSooth?s approach differs fromsupervised confidence annotation methods thatlearn a fixed confidence threshold from a corpusof human-machine dialogues (Bohus, 2007).The role of GROUNDING is to monitor the sys-tem?s confidence in its interpretation of each us-er utterance, to provide evidence to the user ofits interpretation, and to elicit corroboration, fur-ther information, or tacit agreement.
To ground atarget concept, FORRSooth considers one ormore hypotheses for the value the user intended,and chooses a grounding action commensuratewith its understanding and confidence.GROUNDING updates the agreement graph byadding grounding agreements to elicit confirma-tions or rejections of target concepts, or to dis-ambiguate among target concepts.
A groundingagreement?s indicator target represents the ex-pectation of a user response.
Once a sufficientlyconfident INTERPRETATION hypothesis is boundto an indicator target, the grounding agreementexecutes side effects that strengthen or weakenthe hypothesis being grounded.
Recursivegrounding (where the system grounds the user?sresponse to the system?s previous grounding ac-tion) can result if the system?s expectation hasnot been met by the next system turn.GROUNDING makes two kinds of decisions,each with its own set of tier-3 Advisors.
Thefirst, commit bindings, indicates that the systemis confident in the value of a target concept.
Inthis experiment, decisions to commit to a valueare irrevocable.
The other kind of decision se-lects the next grounding utterance for any targetconcepts that have not yet been bound.
The deci-sion to ground a target concept is made by tier-3Advisors that consider the distribution of hypo-thesis merit, as well as the success or failure ofthe grounding actions taken thus far.2685 FX2FX2 is a FORRSooth SDS constructed for thecurrent experiment.
The ten FX2 INTERPRETA-TION Advisors are described in (Gordon,Passonneau and Epstein, 2011).
Here we de-scribe its GROUNDING actions and Advisors.FX2 can choose among six grounding actions.Given high confidence in a single interpretation,it commits to the binding of a target value with-out confirmation.
At slightly lower confidencelevels, it chooses to implicitly confirm a targetbinding, with or without a hedge (e.g., the tagquestion ?right??).
At even lower confidence,the grounding action is to explicitly confirm.Given competing interpretations with similarlyhigh confidence, the grounding action is to dis-ambiguate between the candidates.
Finally, FX2can request the user to repeat herself.We give two examples of the twenty-threeFX2 grounding Advisors.
Given two interpreta-tion hypotheses with similar confidence scores, adisambiguation Advisor votes to prompt the userto disambiguate between them.
The strength forthis grounding action is proportional to the ratioof the two hypotheses?
scores.
To avoid repeatedexecution of the same grounding action, onegrounding Advisor votes against actions to re-peat a prompt for the same target, especially ifASR confidence is low.
In FX2, RSWL facili-tates the use of multiple Advisors for INTERPRE-TATION and GROUNDING by learning weights forthem that reflect their relative reliability.
We de-scribe next how we collect training examplesthrough an ablated wizard experiment.6 Experimental DesignThis experiment tests FX2?s ability to learn IN-TERPRETATION and GROUNDING weights.
Ineach dialogue, FX2 introduces itself, promptsthe subject for her name or a book title, and thencontinues the dialogue until FX2 commits to abinding for the concept, or gives up.Four undergraduate native English speakers(two female, two male) participated.
Speech in-put and output was through a microphone head-set.
The PocketSphinx speech recognizerproduced ASR output (Huggins-Daines et al,2006) with Wall-Street Journal dictation acous-tic models adapted with ten hours of spontane-ous speech.
We built distinct trigram statisticallanguage models for each type of agreement us-ing names and titles from the Heiskell database.We collected three data sets, referenced hereas baseline, wizard, and learning.
Each had twoagreement graphs: UserName seeks a groundedvalue for the patron's full name, and BookTitleseeks a grounded value for a book title.
120 di-alogues were collected for each dataset.FX2 includes an optional wizard component.When active, the wizard component displays aGUI showing the current interpretation hypo-theses for target concepts, along with their re-spective merit.
A screen shot for the wizard GUIappears in Figure 1.A wizard dialogue activates the wizard com-ponent and uses INTERPRETATION as usual, butembeds a person (the wizard) in GROUNDING.The wizard?s purpose in this experiment is toprovide training data for GROUNDING.
Aftereach user turn, the wizard makes two decisionsbased on data from the GUI: whether to considerany target as grounded, and which in a set ofpossible grounding actions to use next.
The GUIdisplays what FX2 would choose for each deci-sion; the wizard can either accept or override it.Ordinarily, a FORR-based system begins withuniform Advisor weights and learns more ap-propriate values during its experience.
Becausecorrect interpretation and grounding are difficulttasks, however, we chose here to prime theseweights and hypothesis merits using training ex-amples collected during development.
Develop-ment data for INTERPRETATION included 200patron names, 400 book titles, and 50 indicatorFigure 1.
The wizard GUI displays hypotheses for a title from a user utterance.269concepts.
ASR output for each item, along withits correct value, became a training example.Development data for GROUNDING came from20 preliminary wizard dialogues.
The develop-ment data also served to prime hypothesis merit.Each subject had 30 dialogues with the sys-tem for the baseline dataset.
For the wizard dataset, FX2 used the same primed weights and me-rits as the baseline.
The wizard?s grounding ac-tions and the target graphs on which they werebased were saved as training examples.
Weightsfor GROUNDING Advisors were learned from thedevelopment data training examples and thetraining examples saved from the wizard data settogether before collecting the learned data set.7 Results and DiscussionWe assess system performance as follows.
Atrue positive (tp) here is a dialogue that made nogrounding errors and successfully grounded theroot task agreement; a false positive (fp) made atleast one grounding error (where the system en-tirely misunderstood the user).
A false negative(fn) occurs when the system gives up on thetask.
Precision is tp/(tp+fp), recall is tp/(tp+fn),and F is their mean.
We measure WER usingLevenshtein edit distance (Levenshtein, 1966).Because the audio data is not yet transcribed, weestimated average WER from the speaker's firstknown utterance (n=360).
Overall estimatedWER was 66% (54% male, 78% female).An ideal system engages in dialogues thathave high precision, high recall, and economicaldialogue length (as measured by number of sys-tem turns).
Table 1 reports that data.
There is asignificant increase in precision across the threedata sets, a small corresponding decrease in re-call, and an overall gain in F measure.
The pre-cision demonstrated by the system duringdialogues in the learned data set is as good orbetter than that reported for our best embeddedhuman wizards in full dialogue experiments(Ligorio, Epstein and Passonneau, 2010).Table 2 shows the distribution of the system'sgrounding actions for the three data sets.
Thegrounding actions in the learned data set aresimilar to the wizard?s; clearly, RSWL learnedwell.
Figure 2 illustrates an effective learnedgrounding behavior in which GROUNDING votesto abandon the current grounding strategy onturn 5 in favor of an approach designed to elicitan utterance from the user more likely to beprocessed correctly by the speech recognizer.Our experiment suggests that misunderstand-ings due to ASR errors can be significantly re-duced by a close integration between spokenlanguage understanding resources and groundingbehavior, together with the use of fine-grainedconfidence measures.
Despite intentionally noisyASR with an estimated average WER of 66%,FX2 did not experience a single misunderstand-ing during 120 clarification dialogues after it hadtrained with RSWL on examples provided by anembedded wizard.
After training, the system'sactions closely resembled the wizard's, indicat-ing that the system?s grounding strategies weresufficiently expressive to approximate the wi-zard's actions.
FX2 accommodates a variety ofindependent spoken language understanding re-sources.
It relies on RSWL and on merit, a self-normalizing estimate of the confidence percen-tile in which an interpretation hypothesis falls.Turn Utterance ASR1 What title would you like?2 Family and Friends family .friends.3I have two guesses.
The first isFamily and Friends.
Thesecond is Family Happiness.Is it either of these?4 The first one .nest.
.first.5 Let?s try something else.
Is the full title Family and Friends?6 Yes yesCondition Precision Recall F LengthBaseline 0.65 0.78 0.72 4.36Wizard 0.89 0.76 0.83 4.05Learned 1.00 0.71 0.86 3.86Condition Conf Disambig Repeat OtherBaseline 0.23 0.19 0.50 0.08Wizard 0.09 0.50 0.35 0.06Learned 0.15 0.52 0.32 0.01Table 1.
Performance across three data sets.
Table 2.
Distribution of grounding actions.Figure 2.
Example of learned GROUNDING behavior.The rightmost column is the top ASR hypothesis.Periods delimit unconfident words in the ASR.270ReferencesJames Allen, George Ferguson and Amanda Stent.2001.
An architecture for more realisticconversational systems.
Proc.
6th Int'l Conferenceon Intelligent User Interfaces.
ACM: 1-8.Dan Bohus.
2007.
Error awareness and recovery inconversational spoken language interfaces.
Ph.D.thesis, Carnegie Mellon University, Pittsburgh,PA.Dan Bohus and Alexander I. Rudnicky.
2005.
Errorhandling in the RavenClaw dialog managementframework.
Proc.
Human Language Technologyand Empirical Methods in Natural LanguageProcessing, ACL: 225-232.Dan Bohus and Alexander I. Rudnicky.
2009.
TheRavenClaw dialog management framework:Architecture and systems.
Comput.
Speech Lang.23(3): 332-361.Herbert H. Clark and Edward F. Schaefer.
1989.Contributing to discourse.
Cognitive Science13(2): 259 - 294.David Devault and Matthew Stone.
2009.
Learning tointerpret utterances using dialogue history.
Proc.12th Conference of the European Chapter of theAssociation for Computational Linguistics.
ACL:184-192.Susan L. Epstein.
1994.
For the Right Reasons: TheFORR Architecture for Learning in a SkillDomain.
Cognitive Science 18(3): 479-511.Susan L. Epstein and Smiljana Petrovic.
2006.Relative Support Weight Learning for ConstraintSolving.
AAAI Workshop on Learning for Search:115-122.Joshua B. Gordon, Rebecca J. Passonneau and SusanL.
Epstein.
2011.
Helping Agents Help TheirUsers Despite Imperfect Speech Recognition.AAAI Symposium Help Me Help You: Bridging theGaps in Human-Agent Collaboration.David Huggins-Daines, Mohit Kumar, Arthur Chan,Alan W. Black, Mosur Ravishankar and Alex I.Rudnicky.
2006.
Pocketsphinx: A Free, Real-TimeContinuous Speech Recognition System for Hand-Held Devices.
In Proc.
IEEE ICASSP, 2006.
185-188.Oliver Lemon.
2003.
Managing dialogue interaction:A multi-layered approach.
In Proc.
4th SIGDialWorkshop on Discourse and Dialogue.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
So-viet Physics Doklady.
10: 707-710.Esther Levin and Rebecca Passonneau.
2006.
A WOzVariant with Contrastive Conditions.
In Proc.
ofInterspeech 2006 Satelite Workshop: Dialogue onDialogues.Tiziana Ligorio, Susan L. Epstein and Rebecca J.Passonneau.
2010.
Wizards' dialogue strategies tohandle noisy speech recognition.
IEEE workshopon Spoken Language Technology (IEEE-SLT2010).
Berkeley, CA.Diane Litman, Julia Hirschberg and Marc Swerts.2006.
Characterizing and predicting corrections inspoken dialogue systems.
Comput.
Linguist.
32(3):417-438.Diane J. Litman and Shimei Pan.
1999.
Empiricallyevaluating an adaptable spoken dialogue system.Proc.
7th Int'l Conference on User Modeling.Springer-Verlag New York, Inc.: 55-64.Tim Paek and Eric Horvitz.
2000.
Conversation asaction under uncertainty.
Proc.
16th Conferenceon Uncertainty in Artificial Intelligence, MorganKaufmann Publishers Inc.: 455-464.Rebecca J. Passonneau, Susan L. Epstein, TizianaLigorio, Joshua B. Gordon and Pravin Bhutada.2010.
Learning about voice search for spokendialogue systems.
Human LanguageTechnologies: NAACL 2010.
ACL: 840-848.Antoine Raux, Brian Langner, Allan W. Black andMaxine Eskenazi.
2005.
Let's Go Public!
Taking aspoken dialog system to the real world.Interspeech 2005 (Eurospeech).
Lisbon, Portugal.Antonio Roque and David Traum.
2009.
Improving avirtual human using a model of degrees ofgrounding.
Proc.
IJCAI-2009.
Morgan KaufmannPublishers Inc.: 1537-1542.271
