Converting Dependency Structures to Phrase StructuresFei Xia and Martha PalmerUniversity of PennsylvaniaPhiladelphia, PA 19104, USAffxia,mpalmerg@linc.cis.upenn.edu1.
INTRODUCTIONTreebanks are of two types according to their annotation schemata:phrase-structure Treebanks such as the English Penn Treebank [8]and dependency Treebanks such as the Czech dependency Tree-bank [6].
Long before Treebanks were developed and widely usedfor natural language processing, there had been much discussion ofcomparison between dependency grammars and context-free phrase-structure grammars [5].
In this paper, we address the relationshipbetween dependency structures and phrase structures from a practi-cal perspective; namely, the exploration of different algorithms thatconvert dependency structures to phrase structures and the evalua-tion of their performance against an existing Treebank.
This worknot only provides ways to convert Treebanks from one type of rep-resentation to the other, but also clarifies the differences in repre-sentational coverage of the two approaches.2.
CONVERTING PHRASE STRUCTURESTO DEPENDENCY STRUCTURESThe notion of head is important in both phrase structures anddependency structures.
In many linguistic theories such as X-bartheory and GB theory, each phrase structure has a head that de-termines the main properties of the phrase and a head has severallevels of projections; whereas in a dependency structure the headis linked to its dependents.
In practice, the head information is ex-plicitly marked in a dependency Treebank, but not always so in aphrase-structure Treebank.
A common way to find the head in aphrase structure is to use a head percolation table, as discussed in[7, 1] among others.
For example, the entry (S right S/VP) in thehead percolation table says that the head child1 of an S node is thefirst child of the node from the right with the label S or VP.Once the heads in phrase structures are found, the conversionfrom phrase structures to dependency structures is straightforward,as shown below:(a) Mark the head child of each node in a phrase structure, usingthe head percolation table.1The head-child of a node XP is the child of the node XP that isthe ancestor of the head of the XP in the phrase structure..(b) In the dependency structure, make the head of each non-head-child depend on the head of the head-child.Figure 1 shows a phrase structure in the English Penn Treebank[8].
In addition to the syntactic labels (such as NP for a nounphrase), the Treebank also uses function tags (such as SBJ for thesubject) for grammatical functions.
In this phrase structure, the rootnode has two children: the NP and the VP.
The algorithm wouldchoose the VP as the head-child and the NP as a non-head-child,and make the head Vinkin of the NP depend on the head join ofthe VP in the dependency structure.
The dependency structure ofthe sentence is shown in Figure 2.
A more sophisticated versionof the algorithm (as discussed in [10]) takes two additional tables(namely, the argument table and the tagset table) as input and pro-duces dependency structures with the argument/adjunct distinction(i.e., each dependent is marked in the dependency structure as ei-ther an argument or an adjunct of the head).SVPNNPVinkenMDwillVPVBjoinNPDT NNthe boardas JJIN NPNNP CD29NovDT NNdirectoraNP-SBJPP-CLR NP-TMPnonexecutiveFigure 1: A phrase structure in the Penn TreebankjoinwillVinkennonexecutiveaboard asthe29NovdirectorFigure 2: A dependency tree for the sentence in Figure 1.
Headsare marked as parents of their dependents in an ordered tree.It is worth noting that quite often there is no consensus on whatthe correct dependency structure for a particular sentence shouldbe.
To build a dependency Treebank, the Treebank annotators mustdecide which word depends on which word; for example, they haveto decide whether the subject Vinken in Figure 1 depends on themodal verb will or the main verb join.
In contrast, the annotatorsfor phrase-structure Treebanks do not have to make such decisions.The users of phrase-structure Treebanks can modify the head per-colation tables to get different dependency structures from the samephrase structure.
In other words, phrase structures offer more flex-ibility than dependency structures with respect to the choices ofheads.The feasibility of using the head percolation table to identify theheads in phrase structures depends on the characteristics of the lan-guage, the Treebank schema, and the definition of the correct de-pendency structure.
For instance, the head percolation table for astrictly head-final (or head-initial) language is very easy to build,and the conversion algorithm works very well.
For the EnglishPenn Treebank, which we used in this paper, the conversion algo-rithm works very well except for the noun phrases with the appos-itive construction.
For example, the conversion algorithm wouldchoose the appositive the CEO of FNX as the head child of thephrase John Smith, the CEO of FNX, whereas the correct head childshould be John Smith.3.
CONVERTING DEPENDENCY STRUC-TURES TO PHRASE STRUCTURESThe main information that is present in phrase structures but notin dependency structures is the type of syntactic category (e.g., NP,VP, and S); therefore, to recover syntactic categories, any algorithmthat converts dependency structures to phrase structures needs toaddress the following questions:Projections for each category: for a category X, what kind ofprojections can X have?Projection levels for dependents: Given a category Y dependson a category X in a dependency structure, how far should Yproject before it attaches to X?s projection?Attachment positions: Given a category Y depends on a cate-gory X in a dependency structure, to what position on X?s projec-tion chain should Y?s projection attach?In this section, we discuss three conversion algorithms, each ofwhich gives different answers to these three questions.
To makethe comparison easy, we shall apply each algorithm to the depen-dency structure (d-tree) in Figure 2 and compare the output of thealgorithm with the phrase structure for that sentence in the EnglishPenn Treebank, as in Figure 1.Evaluating these algorithms is tricky because just like depen-dency structures there is often no consensus on what the correctphrase structure for a sentence should be.
In this paper, we mea-sure the performance of the algorithms by comparing their outputwith an existing phrase-structure Treebank (namely, the EnglishPenn Treebank) because of the following reasons: first, the Tree-bank is available to the public, and provides an objective althoughimperfect standard; second, one goal of the conversion algorithmsis to make it possible to compare the performance of parsers thatproduce dependency structures with the ones that produce phrasestructures.
Since most state-of-the-art phrase-structure parsers areevaluated against an existing Treebank, we want to evaluate theconversion algorithms in the same way; third, a potential appli-cation of the conversion algorithms is to help construct a phrase-structure Treebank for one language, given parallel corpora and thephrase structures in the other language.
One way to evaluate thequality of the resulting Treebank is to compare it with an existingTreebank.3.1 Algorithm 1According to X-bar theory, a category X projects to X?, whichfurther projects to XP.
There are three types of rules, as shown inFigure 3(a).
Algorithm 1, as adopted in [4, 3], strictly follows X-bar theory and uses the following heuristic rules to build phrasestructures:X YP(Spec)XPX?X?
WP(Mod)ZP(Arg)XWY Z(c) phrase structure(Arg) (Mod)(Spec)(1) XP -> YP X?
(2) X?
-> X?
WP(3) X?
-> X ZP(a) rules inX-bar theory(b) d-treeFigure 3: Rules in X-bar theory and Algorithm 1 (which isbased on it)Two levels of projections for any category: any category X hastwo levels of projection: X?
and XP.Maximal projections for dependents: a dependent Y alwaysprojects to Y?
then YP, and the YP attaches to the head?s projection.Fixed positions of attachment: Dependents are divided intothree types: specifiers, modifiers, and arguments.
Each type ofdependent attaches to a fixed position, as shown in Figure 3(c).The algorithm would convert the d-tree in Figure 3(b) to thephrase structure in Figure 3(c).
If a head has multiple modifiers,the algorithm could use either a single X?
or stacked X?
[3].
Figure4 shows the phrase structure for the d-tree in Figure 2, where thealgorithm uses a single X?
for multiple modifiers of the same head.2VPV?MDwillVBjoinNPVinkenNNPN?
PPP?INasNPV?nonexecutiveVPN?NovNNPN?NPN?NPCD29ADJPJJADJ?N?NNdirectorN?V?boardN?NNNPDTDT?DTPtheaDTDT?DTPFigure 4: The phrase structure built by algorithm 1 for the d-tree in Figure 23.2 Algorithm 2Algorithm 2, as adopted by Collins and his colleagues [2] whenthey converted the Czech dependency Treebank [6] into a phrase-structure Treebank, produces phrase structures that are as flat aspossible.
It uses the following heuristic rules to build phrase struc-tures:One level of projection for any category: X has only one levelof projection: XP.2To make the phrase structure more readable, we use N?
and NP asthe X?
and XP for all kinds of POS tags for nouns (e.g., NNP, NN,and CD).
Verbs and adjectives are treated similarly.Minimal projections for dependents: A dependent Y does notproject to Y P unless it has its own dependents.Fixed position of attachment: A dependent is a sister of itshead in the phrase structure.3The algorithm treats all kinds of dependents equally.
It convertsthe pattern in Figure 5(a) to the phrase structure in Figure 5(b).Notice that in Figure 5(b), Y does not project to YP because it doesnot have its own dependents.
The resulting phrase structure for thed-tree in Figure 2 is in Figure 6, which is much flatter than the oneproduced by Algorithm 1.Y WXP(dep) (dep) (dep)ZX(a) d-tree (b) phrase structureX ZP WPYFigure 5: The scheme for Algorithm 2VPVBjoinMD NPDTPPINnonexecutiveNPNNP CDNov 29NPNNJJDTtheNNboard asdirectorawillNNPVinkenFigure 6: The phrase structure built by Algorithm 2 for thed-tree in Figure 23.3 Algorithm 3The previous two algorithms are linguistically sound.
They donot use any language-specific information, and as a result there areseveral major differences between the output of the algorithms andthe phrase structures in an existing Treebank, such as the Penn En-glish Treebank (PTB).Projections for each category: Both algorithms assume that thenumbers of projections for all the categories are the same, whereasin the PTB the number of projections varies from head to head.
Forexample, in the PTB, determiners do not project, adverbs projectonly one level to adverbial phrases, whereas verbs project to VP,then to S, then to SBAR.4Projection levels for dependents: Algorithm 1 assumes themaximal projections for all the dependents, while Algorithm 2 as-sumes minimal projections; but in the PTB, the level of projectionof a dependent may depend on several factors such as the categoriesof the dependent and the head, the position of the dependent withrespect to the head, and the dependency type.
For example, when a3If a dependent Y has its own dependents, it projects to YP and YPis a sister of the head X; otherwise, Y is a sister of the head X.4S is similar to IP (IP is the maximal projection of INFL) in GBtheory, so is SBAR to CP (CP is the maximal projection of Comp);therefore, it could be argued that only VP is a projection of verbsin the PTB.
Nevertheless, because PTB does not mark INFL andComp, we treat S and SBAR as projections of verbs.noun modifies a verb (or VP) such as yesterday in he came yester-day, the noun always projects to NP, but when a noun N1modifiersanother noun N2, N1projects to NP if N1is to the right of N2(e.g., in an appositive construction) and it does not project to NP ifN1is to the left of N2.Attachment positions: Both algorithms assume that all the de-pendents of the same dependency type attach at the same level (e.g.,in Algorithm 1, modifiers are sisters of X?, while in Algorithm 2,modifiers are sisters of X); but in the PTB, that is not always true.For example, an ADVP, which depends on a verb, may attach toeither an S or a VP in the phrase structure according to the positionof the ADVP with respect to the verb and the subject of the verb.Also, in noun phrases, left modifiers (e.g., JJ) are sisters of the headnoun, while the right modifiers (e.g., PP) are sisters of NP.For some applications, these differences between the Treebankand the output of the conversion algorithms may not matter much,and by no means are we implying that an existing Treebank pro-vides the gold standard for what the phrase structures should be.Nevertheless, because the goal of this work is to provide an algo-rithm that has the flexibility to produce phrase structures that areas close to the ones in an existing Treebank as possible, we pro-pose a new algorithm with such flexibility.
The algorithm distin-guishes two types of dependents: arguments and modifiers.
Thealgorithm also makes use of language-specific information in theform of three tables: the projection table, the argument table, andthe modification table.
The projection table specifies the projec-tions for each category.
The argument table (the modification table,resp.)
lists the types of arguments (modifiers, resp) that a head cantake and their positions with respect to the head.
For example, theentry V !
V P !
S in the projection table says that a verb canproject to a verb phrase, which in turn projects to a sentence; theentry (P 0 1 NP/S) in the argument table indicates that a prepositioncan take an argument that is either an NP or an S, and the argumentis to the right of the preposition; the entry (NP DT/JJ PP/S) in themodification table says that an NP can be modified by a determinerand/or an adjective from the left, and by a preposition phrase or asentence from the right.Given these tables, we use the following heuristic rules to buildphrase structures:5One projection chain per category: Each category has a uniqueprojection chain, as specified in the projection table.Minimal projection for dependents: A category projects to ahigher level only when necessary.Lowest attachment position: The projection of a dependent at-taches to a projection of its head as lowly as possible.The last two rules require further explanation, as illustrated inFigure 7.
In the figure, the node X has three dependents: Y and Zare arguments, and W is a modifier of X.
Let?s assume that the al-gorithm has built the phrase structure for each dependent.
To formthe phrase structure for the whole d-tree, the algorithm needs toattach the phrase structures for dependents to the projection chainX0; X1; :::Xk of the head X.
For an argument such as Z, sup-pose its projection chain is Z0; Z1; :::Zu and the root of the phrasestructure headed by Z is Zs.
The algorithm would find the low-est position Xh on the head projection chain, such that Z has aprojection Zt that can be an argument of Xh 1 according to theargument table and Zt is no lower than Zs on the projection chainfor Z.
The algorithm then makes Zt a child of Xh in the phrasestructure.
Notice that based on the second heuristic rule (i.e., mini-mal projection for dependents), Zt does not further project to Zu in5In theory, the last two heuristic rules may conflict each other insome cases.
In those cases, we prefer the third rule over the second.In practice, such conflicting cases are very rare, if exist.this case although Zu is a valid projection of Z.
The attachment formodifiers is similar except that the algorithm uses the modificationtable instead of the argument table.6ZXY W(Arg) (Arg) (Mod)k0jj-1hlmnWWWW00h-1XXXXXXYX iYYYqZZ0PrsZtZu(b) phrase structure(a) d-treeFigure 7: The scheme for Algorithm 3MDwillVBjoinVPSNNPVinkenNPDT NNboardthePPNPDT JJ NNa directornonexecutiveINasCDNPNNPNov 29NP(a)(b)(c)(d)(e)(f)Figure 8: The phrase structure produced by Algorithm 3The phrase structure produced by Algorithm 3 for the d-tree inFigure 2 is in Figure 8.
In Figure 8, (a)-(e) are the phrase structuresfor five dependents of the head join; (f) is the projection chain forthe head.
The arrows indicate the positions of the attachment.
No-tice that to attach (a) to (f), the NNP Vinken needs to further projectto NP because according to the argument table, a VP can take anNP, but not an NNP, as its argument.In the PTB, a modifier either sister-adjoins or Chomsky-adjoinsto the modifiee.
For example, in Figure 1, the MD will Chomsky-adjoins whereas the NP Nov. 29 sister-adjoins to the VP node.
Toaccount for that, we distinguish these two types of modifiers in themodification table and Algorithm 3 is extended so that it would at-tach Chomsky-adjoining modifiers higher by inserting extra nodes.To convert the d-tree in Figure 2, the algorithm inserts an extra VPnode in the phrase structure in Figure 8 and attaches the MD willto the new VP node; the final phrase structure produced by the al-gorithm is identical to the one in Figure 1.3.4 Algorithm 1 and 2 as special cases of Al-gorithm 36Note that once Zt becomes a child of Xh, other dependents ofX (such as W) that are on the same side as Z but are further awayfrom X can attach only to Xh or higher on the projection chain ofX .Although the three algorithms adopt different heuristic rules tobuild phrase structures, the first two algorithms are special casesof the last algorithm; that is, we can design a distinct set of pro-jection/argument/modification tables for each of the first two al-gorithms so that running Algorithm 3 with the associated set of ta-bles for Algorithm 1 (Algorithm 2, respectively) would produce thesame results as running Algorithm 1 (Algorithm 2, respectively).For example, to produce the results of Algorithm 2 with the codefor Algorithm 3, the three tables should be created as follows:(a) In the projection table, each head X has only one projectionXP;(b) In the argument table, if a category Y can be an argument ofa category X in a d-tree, then include both Y and YP as argumentsof X;(c) In the modification table, if a category Y can be a modifierof a category X in a d-tree, then include both Y and YP as sister-modifiers of XP.4.
EXPERIMENTSSo far, we have described two existing algorithms and proposeda new algorithm for converting d-trees into phrase structures.
Asexplained at the beginning of Section 3, we evaluated the perfor-mance of the algorithms by comparing their output with an exist-ing Treebank.
Because there are no English dependency Treebanksavailable, we first ran the algorithm in Section 2 to produce d-treesfrom the PTB, then applied these three algorithms to the d-treesand compared the output with the original phrase structures in thePTB.7 The process is shown in Figure 9.tagset tableargument tablehead percolation tablemodification tableargument tablehead projection tableresultsnew phrased-treephr-struct => d-tree => phr-struct(alg1, alg2, alg3)structuresstructuresd-trees comparestructuresin the PTBphraseFigure 9: The flow chart of the experimentThe results are shown in Table 1, which use Section 0 of the PTB.The precision and recall rates are for unlabelled brackets.
The lastcolumn shows the ratio of the number of brackets produced by thealgorithms and the number of brackets in the original Treebank.From the table (especially the last column), it is clear that Algo-rithm 1 produces many more brackets than the original Treebank,resulting in a high recall rate but low precision rate.
Algorithm 2produces very flat structures, resulting in a low recall rate and highprecision rate.
Algorithm 3 produces roughly the same number ofbrackets as the Treebank and has the best recall rate, and its preci-sion rate is almost as good as that of Algorithm 2.The differences between the output of the algorithms and thephrase structures in the PTB come from four sources:(S1) Annotation errors in the PTB(S2) Errors in the Treebank-specific tables used by the algorithmsin Sections 2 and 3 (e.g., the head percolation table, the pro-jection table, the argument table, and the modification table)7Punctuation marks are not part of the d-trees produced by Lex-Tract.
We wrote a simple program to attach them as high as possi-ble to the phrase structures produced by the conversion algorithms.recall prec no-cross ave test/(%) (%) (%) cross goldAlg1 81.34 32.81 50.81 0.90 2.48Alg2 54.24 91.50 94.90 0.10 0.59Alg3 86.24 88.72 84.33 0.27 0.98Table 1: Performance of three conversion algorithms on theSection 0 of the PTB(S3) The imperfection of the conversion algorithm in Section 2(which converts phrase structures to d-trees)(S4) Mismatches between the heuristic rules used by the algorithmsin Section 3 and the annotation schemata adopted by the PTBTo estimate the contribution of (S1)?
(S4) to the differences be-tween the output of Algorithm 3 and the phrase structures in thePTB, we manually examined the first twenty sentences in Section0.
Out of thirty-one differences in bracketing, seven are due to(S1), three are due to (S2), seven are due to (S3), and the remainingfourteen mismatches are due to (S4).While correcting annotation errors to eliminate (S1) requires morehuman effort, it is quite straightforward to correct the errors in theTreebank-specific tables and therefore eliminate the mismatchescaused by (S2).
For (S3), we mentioned in Section 2 that the al-gorithm chose the wrong heads for the noun phrases with the ap-positive construction.
As for (S4), we found several exceptions(as shown in Table 2) to the one-projection-chain-per-category as-sumption (i.e., for each POS tag, there is a unique projection chain),an assumption which was used by all three algorithms in Section 3.The performance of the conversion algorithms in Section 2 and 3could be improved by using additional heuristic rules or statisticalinformation.
For instance, Algorithm 3 in Section 3 could use aheuristic rule that says that an adjective (JJ) projects to an NP if theJJ follows the determiner the and the JJ is not followed by a nounas in the rich are getting richer, and it projects to an ADJP in othercases.
Notice that such heuristic rules are Treebank-dependent.most likely projection other projection(s)JJ !
ADJP JJ !
NPCD !
NP CD !
QP !
NPVBN !
VP !
S VBN !
VP !
RRCNN !
NP NN !
NX !
NPVBG !
VP !
S VBG !
PPTable 2: Some examples of heads with more than one projectionchainEmpty categories are often explicitly marked in phrase-structures,but they are not always included in dependency structures.
We be-lieve that including empty categories in dependency structures hasmany benefits.
First, empty categories are useful for NLP applica-tions such as machine translation.
To translate a sentence from onelanguage to another, many machine translation systems first createthe dependency structure for the sentence in the source language,then produce the dependency structure for the target language, andfinally generate a sentence in the target language.
If the sourcelanguage (e.g., Chinese and Korean) allows argument deletion andthe target language (e.g., English) does not, it is crucial that thedropped argument (which is a type of empty category) is explic-itly marked in the source dependency structure, so that the machinetranslation systems are aware of the existence of the dropped argu-ment and can handle the situation accordingly.
The second benefitof including empty categories in dependency structures is that it canimprove the performance of the conversion algorithms in Section3, because the phrase structures produced by the algorithms wouldthen have empty categories as well, just like the phrase structuresin the PTB.
Third, if a sentence includes a non-projective construc-tion such as wh-movement in English, and if the dependency treedid not include an empty category to show the movement, travers-ing the dependency tree would yield the wrong word order.85.
CONCLUSIONWe have proposed a new algorithm for converting dependencystructures to phrase structures and compared it with two existingones.
We have shown that our algorithm subsumes the two ex-isting ones.
By using simple heuristic rules and taking as inputcertain kinds of Treebank-specific information such as the types ofarguments and modifiers that a head can take, our algorithm pro-duces phrase structures that are very close to the ones in an anno-tated phrase-structure Treebank; moreover, the quality of the phrasestructures produced by our algorithm can be further improved whenmore Treebank-specific information is used.
We also argue for in-cluding empty categories in the dependency structures.6.
ACKNOWLEDGMENTSThis research reported here was made possible by NSF underGrant NSF-89-20230-15 and by DARPA as part of the TranslingualInformation Detection, Extraction and Summarization (TIDES) pro-gram under Grant N66001-00-1-8915.7.
REFERENCES[1] M. Collins.
Three Generative, Lexicalised Models forStatistical Parsing.
In Proc.
of the 35th ACL, 1997.
[2] M. Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.
AStatistical Parser for Czech.
In Proc.
of ACL-1999, pages505?512, 1999.
[3] M. Covington.
An Empirically Motivated Reinterpretation ofDependency Grammar, 1994.
Research Report AI-1994-01.
[4] M. Covington.
GB Theory as Dependency Grammar, 1994.Research Report AI-1992-03.
[5] H. Gaifman.
Dependency Systems and Phrase-StructureSystems.
Information and Control, pages 304?337, 1965.
[6] J. Hajic?.
Building a Syntactically Annotated Corpus: ThePrague Dependency Treebank, 1998.
Issues of Valency andMeaning (Festschrift for Jarmila Panevova?.
[7] D. M. Magerman.
Statistical Decision-Tree Models forParsing.
In Proc.
of the 33rd ACL, 1995.
[8] M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
Buildinga Large Annotated Corpus of English: the Penn Treebank.Computational Lingustics, 1993.
[9] O. Rambow and A. K. Joshi.
A formal look at dependencygrammars and phrase structure grammars with specialconsidertaion of word-order phenomena.
In L. Wenner,editor, Recent Trends in Meaning-Text Theory.
JohnBenjamin, Amsterdam, Philadelphia, 1997.
[10] F. Xia, M. Palmer, and A. Joshi.
A Uniform Method ofGrammar Extraction and its Applications.
In Proc.
of JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora(EMNLP/VLC), 2000.8For more discussion of non-projective constructions, see [9].
