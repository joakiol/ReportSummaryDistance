Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62?69,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe impact of parse quality on syntactically-informed statistical machinetranslationChris Quirk and Simon Corston-OliverMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USA{chrisq,simonco}@microsoft.comAbstractWe investigate the impact of parse qualityon a syntactically-informed statistical ma-chine translation system applied to techni-cal text.
We vary parse quality by vary-ing the amount of data used to train theparser.
As the amount of data increases,parse quality improves, leading to im-provements in machine translation outputand results that significantly outperform astate-of-the-art phrasal baseline.1 IntroductionThe current study is a response to a questionthat proponents of syntactically-informed machinetranslation frequently encounter: How sensitive isa syntactically-informed machine translation sys-tem to the quality of the input syntactic analysis?It has been shown that phrasal machine translationsystems are not affected by the quality of the in-put word alignments (Koehn et al, 2003).
Thisfinding has generally been cast in favorable terms:such systems are robust to poor quality word align-ment.
A less favorable interpretation of these re-sults might be to conclude that phrasal statisticalmachine translation (SMT) systems do not standto benefit from improvements in word alignment.In a similar vein, one might ask whether con-temporary syntactically-informed machine trans-lation systems would benefit from improvementsin parse accuracy.
One possibility is that cur-rent syntactically-informed SMT systems are de-riving only limited value from the syntactic anal-yses, and would therefore not benefit from im-proved analyses.
Another possibility is that syn-tactic analysis does indeed contain valuable infor-mation that could be exploited by machine learn-ing techniques, but that current parsers are not ofsufficient quality to be of use in SMT.With these questions and concerns, let us be-gin.
Following some background discussion wedescribe a set of experiments intended to elucidatethe impact of parse quality on SMT.2 BackgroundWe trained statistical machine translation systemson technical text.
In the following sections weprovide background on the data used for training,the dependency parsing framework used to pro-duce treelets, the treelet translation framework andsalient characteristics of the target languages.2.1 Dependency parsingDependency analysis is an alternative to con-stituency analysis (Tesnie`re, 1959; Melc?uk, 1988).In a dependency analysis of syntax, words di-rectly modify other words, with no interveningnon-lexical nodes.
We use the terms child nodeand parent node to denote the tokens in a depen-dency relation.
Each child has a single parent, withthe lexical root of the sentence dependent on a syn-thetic ROOT node.We use the parsing approach described in(Corston-Oliver et al, 2006).
The parser is trainedon dependencies extracted from the English PennTreebank version 3.0 (Marcus et al, 1993) byusing the head-percolation rules of (Yamada andMatsumoto, 2003).Given a sentence x, the goal of the parser is tofind the highest-scoring parse y?
among all possibleparses y ?
Y :y?
= argmaxy?Ys(x,y) (1)The score of a given parse y is the sum of the62scores of all its dependency links (i, j) ?
y:s(x,y) = ?
(i, j)?yd(i, j) = ?
(i, j)?yw ?
f(i, j) (2)where the link (i, j) indicates a parent-child de-pendency between the token at position i and thetoken at position j.
The score d(i, j) of each de-pendency link (i, j) is further decomposed as theweighted sum of its features f(i, j).The feature vector f(i, j) computed for eachpossible parent-child dependency includes thepart-of-speech (POS), lexeme and stem of the par-ent and child tokens, the POS of tokens adjacentto the child and parent, and the POS of each to-ken that intervenes between the parent and child.Various combinations of these features are used,for example a new feature is created that combinesthe POS of the parent, lexeme of the parent, POSof the child and lexeme of the child.
Each featureis also conjoined with the direction and distanceof the parent, e.g.
does the child precede or followthe parent, and how many tokens intervene?To set the weight vector w, we train twentyaveraged perceptrons (Collins, 2002) on differentshuffles of data drawn from sections 02?21 of thePenn Treebank.
The averaged perceptrons are thencombined to form a Bayes Point Machine (Her-brich et al, 2001; Harrington et al, 2003), result-ing in a linear classifier that is competitive withwide margin techniques.To find the optimal parse given the weight vec-tor w and feature vector f(i, j) we use the decoderdescribed in (Eisner, 1996).2.2 Treelet translationFor syntactically-informed translation, we fol-low the treelet translation approach describedin (Quirk et al, 2005).
In this approach, trans-lation is guided by treelet translation pairs.
Here,a treelet is a connected subgraph of a dependencytree.
A treelet translation pair consists of a sourcetreelet S, a target treelet T , and a word alignmentA ?
S?
T such that for all s ?
S, there exists aunique t ?
T such that (s, t)?
A, and if t is the rootof T , there is a unique s ?
S such that (s, t) ?
A.Translation of a sentence begins by parsingthat sentence into a dependency representation.This dependency graph is partitioned into treelets;like (Koehn et al, 2003), we assume a uniformprobability distribution over all partitions.
Eachsource treelet is matched to a treelet translationpair; together, the target language treelets in thosetreelet translation pairs will form the target trans-lation.
Next the target language treelets are joinedto form a single tree: the parent of the root of eachtreelet is dictated by the source.
Let tr be the rootof the target language treelet, and sr be the sourcenode aligned to it.
If sr is the root of the sourcesentence, then tr is made the root of the target lan-guage tree.
Otherwise let sp be the parent of sr,and tp be the target node aligned to sp: tr is at-tached to tp.
Finally the ordering of all the nodesis determined, and the target tree is specified, andthe target sentence is produced by reading off thelabels of the nodes in order.Translations are scored according to a log-linearcombination of feature functions, each scoring dif-ferent aspects of the translation process.
We use abeam search decoder to find the best translation T ?according to the log-linear combination of models:T ?
= argmaxT{?f?F?
f f (S,T,A)}(3)The models include inverted and direct channelmodels estimated by relative frequency, lexicalweighting channel models following (Vogel et al,2003), a trigram target language model using mod-ified Kneser-Ney smoothing (Goodman, 2001),an order model following (Quirk et al, 2005),and word count and phrase count functions.
Theweights for these models are determined using themethod described in (Och, 2003).To estimate the models and extract the treelets,we begin from a parallel corpus.
First the cor-pus is word-aligned using GIZA++ (Och and Ney,2000), then the source sentence are parsed, andfinally dependencies are projected onto the targetside following the heuristics described in (Quirk etal., 2005).
This word aligned parallel dependencytree corpus provides training material for an ordermodel and a target language tree-based languagemodel.
We also extract treelet translation pairsfrom this parallel corpus.
To limit the combina-torial explosion of treelets, we only gather treeletsthat contain at most four words and at most twogaps in the surface string.
This limits the numberof mappings to be O(n3) in the worst case, wheren is the number of nodes in the dependency tree.2.3 Language pairsIn the present paper we focus on English-to-German and English-to-Japanese machine transla-63you can set this property using Visual BasicSie k?nnen diese Eigenschaft auch mit Visual Basic festlegenFigure 1: Example German-English and Japanese-English sentence pairs, with word alignments.tion.
Both German and Japanese differ markedlyfrom English in ways that we believe illumi-nate well the strengths of a syntactically-informedSMT system.
We provide a brief sketch of the lin-guistic characteristics of German and Japanese rel-evant to the present study.2.3.1 GermanAlthough English and German are closely re-lated ?
they both belong to the western branch ofthe Germanic family of Indo-European languages?
the languages differ typologically in ways thatare especially problematic for current approachesto statistical machine translation as we shall nowillustrate.
We believe that these typological differ-ences make English-to-German machine transla-tion a fertile test bed for syntax-based SMT.German has richer inflectional morphology thanEnglish, with obligatory marking of case, num-ber and lexical gender on nominal elements andperson, number, tense and mood on verbal ele-ments.
This morphological complexity, combinedwith pervasive, productive noun compounding isproblematic for current approaches to word align-ment (Corston-Oliver and Gamon, 2004).Equally problematic for machine translation isthe issue of word order.
The position of verbs isstrongly determined by clause type.
For exam-ple, in main clauses in declarative sentences, finiteverbs occur as the second constituent of the sen-tence, but certain non-finite verb forms occur in fi-nal position.
In Figure 1, for example, the English?can?
aligns with German ?ko?nnen?
in second po-sition and ?set?
aligns with German ?festlegen?
infinal position.Aside from verbs, German is usually charac-terized as a ?free word-order?
language: majorconstituents of the sentence may occur in variousorders, so-called ?separable prefixes?
may occurbound to the verb or may detach and occur at aconsiderable distance from the verb on which theydepend, and extraposition of various kinds of sub-ordinate clause is common.
In the case of extrapo-sition, for example, more than one third of relativeclauses in human-translated German technical textare extraposed.
For comparable English text thefigure is considerably less than one percent (Ga-mon et al, 2002).2.3.2 JapaneseWord order in Japanese is rather different fromEnglish.
English has the canonical constituent or-der subject-verb-object, whereas Japanese preferssubject-object-verb order.
Prepositional phrasesin English generally correspond to postpositionalphrases in Japanese.
Japanese noun phrases arestrictly head-final whereas English noun phrasesallow postmodifiers such as prepositional phrases,relative clauses and adjectives.
Japanese has lit-tle nominal morphology and does not obligatorilymark number, gender or definiteness.
Verbal mor-phology in Japanese is complex with morphologi-cal marking of tense, mood, and politeness.
Top-icalization and subjectless clauses are pervasive,and problematic for current SMT approaches.The Japanese sentence in Figure 1 illustratesseveral of these typological differences.
Thesentence-initial imperative verb ?move?
in the En-glish corresponds to a sentence-final verb in theJapanese.
The Japanese translation of the objectnoun phrase ?the camera slider switch?
precedesthe verb in Japanese.
The English preposition ?to?aligns to a postposition in Japanese.3 ExperimentsOur goal in the current paper is to measure theimpact of parse quality on syntactically-informedstatistical machine translation.
One method forproducing parsers of varying quality might be totrain a parser and then to transform its output, e.g.64by replacing the parser?s selection of the parent forcertain tokens with different nodes.Rather than randomly adding noise to theparses, we decided to vary the quality in ways thatmore closely mimic the situation that confronts usas we develop machine translation systems.
An-notating data for POS requires considerably lesshuman time and expertise than annotating syntac-tic relations.
We therefore used an automatic POStagger (Toutanova et al, 2003) trained on the com-plete training section of the Penn Treebank (sec-tions 02?21).
Annotating syntactic dependenciesis time consuming and requires considerable lin-guistic expertise.1 We can well imagine annotat-ing syntactic dependencies in order to develop amachine translation system by annotating first asmall quantity of data, training a parser, training asystem that uses the parses produced by that parserand assessing the quality of the machine transla-tion output.
Having assessed the quality of the out-put, one might annotate additional data and trainsystems until it appears that the quality of the ma-chine translation output is no longer improving.We therefore produced parsers of varying qualityby training on the first n sentences of sections 02?21 of the Penn Treebank, where n ranged from 250to 39,892 (the complete training section).
At train-ing time, the gold-standard POS tags were used.For parser evaluation and for the machine transla-tion experiments reported here, we used an auto-matic POS tagger (Toutanova et al, 2003) trainedon sections 02?21 of the Penn Treebank.We trained English-to-German and English-to-Japanese treelet translation systems on approxi-mately 500,000 manually aligned sentence pairsdrawn from technical computer documentation.The sentence pairs consisted of the English sourcesentence and a human-translation of that sentence.Table 1 summarizes the characteristics of this data.Note that German vocabulary and singleton countsare slightly more than double the correspondingEnglish counts due to complex morphology andpervasive compounding (see section 2.3.1).3.1 Parser accuracyTo evaluate the accuracy of the parsers trained ondifferent samples of sentences we used the tradi-1Various people have suggested to us that the linguisticexpertise required to annotate syntactic dependencies is lessthan the expertise required to apply a formal theory of con-stituency like the one that informs the Penn Treebank.
Wetend to agree, but have not put this claim to the test.75%80%85%90%95%0 10,000 20,000 30,000 40,000Sample sizeDependencyaccuracy.PTB Section 23Technical textFigure 2: Unlabeled dependency accuracy ofparsers trained on different numbers of sentences.The graph compares accuracy on the blind test sec-tion of the Penn Treebank to accuracy on a set of250 sentences drawn from technical text.
Punctu-ation tokens are excluded from the measurementof dependency accuracy.tional blind test section of the Penn Treebank (sec-tion 23).
As is well-known in the parsing commu-nity, parse quality degrades when a parser trainedon the Wall Street Journal text in the Penn Tree-bank is applied to a different genre or semantic do-main.
Since the technical materials that we weretraining the translation system on differ from theWall Street Journal in lexicon and syntax, we an-notated a set of 250 sentences of technical materialto use in evaluating the parser.
Each of the authorsindependently annotated the same set of 250 sen-tences.
The annotation took less than six hours foreach author to complete.
Inter-annotator agree-ment excluding punctuation was 91.8%.
Differ-ences in annotation were resolved by discussion,and the resulting set of annotations was used toevaluate the parsers.Figure 2 shows the accuracy of parsers trainedon samples of various sizes, excluding punctua-tion tokens from the evaluation, as is customaryin evaluating dependency parsers.
When mea-sured against section 23 of the Penn Treebank,the section traditionally used for blind evaluation,the parsers range in accuracy from 77.8% whentrained on 250 sentences to 90.8% when trainedon all of sections 02?21.
As expected, parse accu-racy degrades when measured on text that differsgreatly from the training text.
A parser trained on250 Penn Treebank sentences has a dependency65English German English JapaneseTraining Sentences 515,318 500,000Words 7,292,903 8,112,831 7,909,198 9,379,240Vocabulary 59,473 134,829 66,731 68,048Singletons 30,452 66,724 50,381 52,911Test Sentences 2,000 2,000Words 28,845 31,996 30,616 45,744Table 1: Parallel data characteristicsaccuracy of 76.6% on the technical text.
A parsertrained on the complete Penn Treebank trainingsection has a dependency accuracy of 84.3% onthe technical text.Since the parsers make extensive use of lexi-cal features, it is not surprising that the perfor-mance on the two corpora should be so similarwith only 250 training sentences; there were notsufficient instances of each lexical item to train re-liable weights or lexical features.
As the amountof training data increases, the parsers are able tolearn interesting facts about specific lexical items,leading to improved accuracy on the Penn Tree-bank.
Many of the lexical items that occur in thePenn Treebank, however, occur infrequently or notat all in the technical materials so the lexical infor-mation is of little benefit.
This reflects the mis-match of content.
The Wall Street Journal articlesin the Penn Treebank concern such topics as worldaffairs and the policies of the Reagan administra-tion; these topics are absent in the technical mate-rials.
Conversely, the Wall Street Journal articlescontain no discussion of such topics as the intrica-cies of SQL database queries.3.2 Translation qualityTable 2 presents the impact of parse quality on atreelet translation system, measured using BLEU(Papineni et al, 2002).
Since our main goal is toinvestigate the impact of parser accuracy on trans-lation quality, we have varied the parser trainingdata, but have held the MT training data, part-of-speech-tagger, and all other factors constant.
Weobserve an upward trend in BLEU score as moretraining data is made available to the parser; thetrend is even clearer in Japanese.2 As a baseline,we include right-branching dependency trees, i.e.,trees in which the parent of each word is its left2This is particularly encouraging since various peoplehave remarked to us that syntax-based SMT systems maybe disadvantaged under n-gram scoring techniques such asBLEU.EG EJPhrasal decoder 31.7?1.2 32.9?0.9Treelet decoderRight-branching 31.4?1.3 28.0?0.7250 sentences 32.8?1.4 34.1?0.92,500 sentences 33.0?1.4 34.6?1.025,000 sentences 33.7?1.5 35.7?0.939,892 sentences 33.6?1.5 36.0?1.0Table 2: BLEU score vs. decoder and parser vari-ants.
Here sentences refer to the amount of parsertraining data, not MT training data.neighbor and the root of a sentence is the firstword.
With this analysis, treelets are simply sub-sequences of the sentence, and therefore are verysimilar to the phrases of Phrasal SMT.
In English-to-German, this result produces results very com-parable to a phrasal SMT system (Koehn et al,2003) trained on the same data.
For English-to-Japanese, however, this baseline performs muchworse than a phrasal SMT system.
Althoughphrases and treelets should be nearly identicalunder this scenario, the decoding constraints aresomewhat different: the treelet decoder assumesphrasal cohesion during translation.
This con-straint may account for the drop in quality.Since the confidence intervals for many pairsoverlap, we ran pairwise tests for each system todetermine which differences were significant atthe p < 0.05 level using the bootstrap method de-scribed in (Zhang and Vogel, 2004); Table 3 sum-marizes this comparison.
Neither language pairachieves a statistically significant improvementfrom increasing the training data from 25,000pairs to the full training set; this is not surprisingsince the increase in parse accuracy is quite small(90.2% to 90.8% on Wall Street Journal text).To further understand what differences in de-pendency analysis were affecting translation qual-ity, we compared a treelet translation system that66Pharaoh Right-branching 250 2,500 25,000 39,892Pharaoh ?
> > > >Right-branching > > > >250 ?
> >2,500 > >25,000 ?
(a) English-GermanPharaoh Right-branching 250 2,500 25,000 39,892Pharaoh < ?
> > >Right-branching > > > >250 > > >2,500 > >25,000 ?
(b) English-JapaneseTable 3: Pairwise statistical significance tests.
> indicates that the system on the top is significantly betterthan the system on the left; < indicates that the system on top is significantly worse than the system onthe left; ?
indicates that difference between the two systems is not statistically significant.323334353637100 1000 10000 100000Parser training sentencesBLEUscoreJapaneseGermanFigure 3: BLEU score vs. number of sentencesused to train the dependency parserused a parser trained on 250 Penn Treebank sen-tences to a treelet translation system that useda parser trained on 39,892 Treebank sentences.From the test data, we selected 250 sentenceswhere these two parsers produced different anal-yses.
A native speaker of German categorized thedifferences in machine translation output as eitherimprovements or regressions.
We then examinedand categorized the differences in the dependencyanalyses.
Table 4 summarizes the results of thiscomparison.
Note that this table simply identifiescorrelations between parse changes and translationchanges; it does not attempt to identify a causallink.
In the analysis, we borrow the term ?NP[Noun Phrase] identification?
from constituencyanalysis to describe the identification of depen-dency treelets spanning complete noun phrases.There were 141 sentences for which the ma-chine translated output improved, 71 sentences forwhich the output regressed and 38 sentences forwhich the output was identical.
Improvements inthe attachment of prepositions, adverbs, gerundsand dependent verbs were common amongst im-proved translations, but rare amongst regressedtranslations.
Correct identification of the depen-dent of a preposition3 was also much more com-mon amongst improvements.Certain changes, such as improved root identifi-cation and final punctuation attachment, were verycommon across the corpus.
Therefore their com-mon occurrence amongst regressions is not verysurprising.
It was often the case that improve-ments in root identification or final punctuation at-tachment were offset by regressions elsewhere inthe same sentence.Improvements in the parsers are cases wherethe syntactic analysis more closely resembles theanalysis of dependency structure that results fromapplying Yamada and Matsumoto?s head-findingrules to the Penn Treebank.
Figure 4 shows dif-ferent parses produced by parsers trained on dif-3In terms of constituency analysis, a prepositional phraseshould consist of a preposition governing a single nounphrase67You can manipulate Microsoft Access objects from another application that also supports automation .ROOTYou can manipulate Microsoft Access objects from another application that also supports automation .ROOT(a) Dependency analysis produced by parser trained on 250 Wall Street Journal sentences.
(b) Dependency analysis produced by parser trained on 39,892 Wall Street Journal sentences.Figure 4: Parses produced by parsers trained on different numbers of sentences.ferent numbers of sentences.
The parser trainedon 250 sentences incorrectly attaches the prepo-sition ?from?
as a dependent of the noun ?ob-jects?
whereas the parser trained on the completePenn Treebank training section correctly attachesthe preposition as a dependent of the verb ?ma-nipulate?.
These two parsers also yield differentanalyses of the phrase ?Microsoft Access objects?.In parse (a), ?objects?
governs ?Office?
and ?Of-fice?
in turn governs ?Microsoft?.
This analy-sis is linguistically well-motivated, and makes atreelet spanning ?Microsoft Office?
available tothe treelet translation system.
In parse (b), theparser has analyzed this phrase so that ?objects?directly governs ?Microsoft?
and ?Office?.
Theanalysis more closely reflects the flat branchingstructure of the Penn Treebank but obscures theaffinity of ?Microsoft?
and ?Office?.An additional measure of parse utility for MTis the amount of translation material that can beextracted from a parallel corpus.
We increased theparser training data from 250 sentences to 39,986sentences, but held the number of aligned sentencepairs used train other modules constant.
The countof treelet translation pairs occurring at least twicein the English-German parallel corpus grew from1,895,007 to 2,010,451.4 ConclusionsWe return now to the questions and concernsraised in the introduction.
First, is a treelet SMTsystem sensitive to parse quality?
We have shownthat such a system is sensitive to the quality ofError category Regress ImproveAttachment of prep 1% 22%Root identification 13% 28%Final punctuation 18% 30%Coordination 6% 16%Dependent verbs 14% 32%Arguments of verb 6% 15%NP identification 24% 33%Dependent of prep 0% 7%Other attachment 3% 22%Table 4: Error analysis, showing percentage ofregressed and improved translations exhibiting aparse improvement in each specified categorythe input syntactic analyses.
With the less accu-rate parsers that result from training on extremelysmall numbers of sentences, performance is com-parable to state-of-the-art phrasal SMT systems.As the amount of data used to train the parser in-creases, both English-to-German and English-to-Japanese treelet SMT improve, and produce re-sults that are statistically significantly better thanthe phrasal baseline.In the introduction we mentioned the concernthat others have raised when we have presentedour research: syntax might contain valuable infor-mation but current parsers might not be of suffi-cient quality.
It is certainly true that the accuracyof the best parser used here falls well short of whatwe might hope for.
A parser that achieves 90.8%dependency accuracy when trained on the PennTreebank Wall Street Journal corpus and evalu-68ated on comparable text degrades to 84.3% accu-racy when evaluated on technical text.
Despite thedegradation in parse accuracy caused by the dra-matic differences between the Wall Street Journaltext and the technical articles, the treelet SMT sys-tem was able to extract useful patterns.
Researchon syntactically-informed SMT is not impeded bythe accuracy of contemporary parsers.One significant finding is that as few as 250sentences suffice to train a dependency parser foruse in the treelet SMT framework.
To date ourresearch has focused on translation from Englishto other languages.
One concern in applying thetreelet SMT framework to translation from lan-guages other than English has been the expenseof data annotation: would we require 40,000 sen-tences annotated for syntactic dependencies, i.e.,an amount comparable to the Penn Treebank, inorder to train a parser that was sufficiently accu-rate to achieve the machine translation quality thatwe have seen when translating from English?
Thecurrent study gives hope that source languages canbe added with relatively modest investments indata annotation.
As more data is annotated withsyntactic dependencies and more accurate parsersare trained, we would hope to see similar improve-ments in machine translation output.We challenge others who are conducting re-search on syntactically-informed SMT to verifywhether or to what extent their systems are sen-sitive to parse quality.ReferencesM.
Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP.Simon Corston-Oliver and Michael Gamon.
2004.Normalizing German and English inflectional mor-phology to improve statistical word alignment.
InR.
E. Frederking and K. B. Taylor, editors, Machinetranslation: From real users to research.
SpringerVerlag.Simon Corston-Oliver, Anthony Aue, Kevin Duh, andEric Ringger.
2006.
Multilingual dependency pars-ing using Bayes Point Machines.
In Proceedings ofHLT/NAACL.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of COLING, pages 340?345.Michael Gamon, Eric Ringger, Zhu Zhang, RobertMoore, and Simon Corston-Oliver.
2002.
Extrapo-sition: A case study in German sentence realization.In Proceedings of COLING, pages 301?307.Joshua Goodman.
2001.
A bit of progress in lan-guage modeling, extended version.
Technical Re-port MSR-TR-2001-72, Microsoft Research.Edward Harrington, Ralf Herbrich, Jyrki Kivinen,John C. Platt, and Robert C. Williamson.
2003.
On-line bayes point machines.
In Proc.
7th Pacific-AsiaConference on Knowledge Discovery and Data Min-ing, pages 241?252.Ralf Herbrich, Thore Graepel, and Colin Campbell.2001.
Bayes Point Machines.
Journal of MachineLearning Research, pages 245?278.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.Igor A. Melc?uk.
1988.
Dependency Syntax: Theoryand Practice.
State University of New York Press.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of theACL, pages 440?447, Hongkong, China, October.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theACL, pages 311?318, Philadelpha, Pennsylvania.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of the ACL.Lucien Tesnie`re.
1959.
?Ele?ments de syntaxe struc-turale.
Librairie C. Klincksieck.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT/EMNLP, pages 252?259.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,Ashish Venugopal, Bing Zhao, and Alex Waibel.2003.
The CMU statistical machine translation sys-tem.
In Proceedings of the MT Summit.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, pages 195?206.Ying Zhang and Stephan Vogel.
2004.
Measuring con-fidence intervals for mt evaluation metrics.
In Pro-ceedings of TMI.69
