Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104?108,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsThe Karlsruhe Institute of Technology Translation Systemsfor the WMT 2013Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues,Teresa Herrmann, Isabel Slawik and Alex WaibelKarlsruhe Institute of TechnologyKarlsruhe, Germanyfirstname.lastname@kit.eduAbstractThis paper describes the phrase-basedSMT systems developed for our partici-pation in the WMT13 Shared TranslationTask.
Translations for English?Germanand English?French were generated us-ing a phrase-based translation systemwhich is extended by additional modelssuch as bilingual, fine-grained part-of-speech (POS) and automatic cluster lan-guage models and discriminative wordlexica (DWL).
In addition, we combinedreordering models on different sentenceabstraction levels.1 IntroductionIn this paper, we describe our systems for theACL 2013 Eighth Workshop on Statistical Ma-chine Translation.
We participated in the SharedTranslation Task and submitted translations forEnglish?German and English?French using aphrase-based decoder with lattice input.The paper is organized as follows: the next sec-tion gives a detailed description of our systemsincluding all the models.
The translation resultsfor all directions are presented afterwards and weclose with a conclusion.2 System DescriptionThe phrase table is based on a GIZA++ wordalignment for the French?English systems.
Forthe German?English systems we use a Discrim-inative Word Alignment (DWA) as described inNiehues and Vogel (2008).
For every sourcephrase only the top 10 translation options are con-sidered during decoding.
The SRILM Toolkit(Stolcke, 2002) is used for training SRI languagemodels using Kneser-Ney smoothing.For the word reordering between languages, weused POS-based reordering models as described inSection 4.
In addition to it, tree-based reorderingmodel and lexicalized reordering were added forGerman?English systems.An in-house phrase-based decoder (Vogel,2003) is used to perform translation.
The trans-lation was optimized using Minimum Error RateTraining (MERT) as described in Venugopal etal.
(2005) towards better BLEU (Papineni et al2002) scores.2.1 DataThe Europarl corpus (EPPS) and News Commen-tary (NC) corpus were used for training our trans-lation models.
We trained language models foreach language on the monolingual part of thetraining corpora as well as the News Shuffle andthe Gigaword corpora.
The additional data such asweb-crawled corpus, UN and Giga corpora wereused after filtering.
The filtering work for this datais discussed in Section 3.For the German?English systems we use thenews-test2010 set for tuning, while the news-test2011 set is used for the French?English sys-tems.
For testing, news-test2012 set was used forall systems.2.2 PreprocessingThe training data is preprocessed prior to train-ing the system.
This includes normalizing specialsymbols, smart-casing the first word of each sen-tence and removing long sentences and sentencepairs with length mismatch.Compound splitting is applied to the Germanpart of the corpus of the German?English systemas described in Koehn and Knight (2003).3 Filtering of Noisy PairsThe filtering was applied on the corpora whichare found to be noisy.
Namely, the Giga English-French parallel corpus and the all the new web-crawled data .
The operation was performed using104an SVM classifier as in our past systems (Medi-ani et al 2011).
For each pair, the required lexicawere extracted from Giza alignment of the corre-sponding EPPS and NC corpora.
Furthermore, forthe web-crawled data, higher precision classifierswere trained by providing a larger number of neg-ative examples to the classifier.After filtering, we could still find English sen-tences in the other part of the corpus.
Therefore,we performed a language identification (LID)-based filtering afterwards (performed only on theFrench-English corpora, in this participation).4 Word ReorderingWord reordering was modeled based on POS se-quences.
For the German?English system, re-ordering rules learned from syntactic parse treeswere used in addition.4.1 POS-based Reordering ModelIn order to train the POS-based reordering model,probabilistic rules were learned based on the POStags from the TreeTagger (Schmid and Laws,2008) of the training corpus and the alignment.
Asdescribed in Rottmann and Vogel (2007), continu-ous reordering rules are extracted.
This modelingof short-range reorderings was extended so that itcan cover also long-range reorderings with non-continuous rules (Niehues and Kolss, 2009), forGerman?English systems.4.2 Tree-based Reordering ModelIn addition to the POS-based reordering, weapply a tree-based reordering model for theGerman?English translation to better address thedifferences in word order between German andEnglish.
We use the Stanford Parser (Rafferty andManning, 2008) to generate syntactic parse treesfor the source side of the training corpus.
Thenwe use the word alignment between source andtarget language to learn rules on how to reorderthe constituents in a German source sentence tomake it match the English target sentence word or-der better (Herrmann et al 2013).
The POS-basedand tree-based reordering rules are applied to eachinput sentence.
The resulting reordered sentencevariants as well as the original sentence order areencoded in a word lattice.
The lattice is then usedas input to the decoder.4.3 Lexicalized ReorderingThe lexicalized reordering model stores the re-ordering probabilities for each phrase pair.
Pos-sible reordering orientations at the incoming andoutgoing phrase boundaries are monotone, swapor discontinuous.
With the POS- and tree-basedreordering word lattices encode different reorder-ing variants.
In order to apply the lexicalized re-ordering model, we store the original position ofeach word in the lattice.
At each phrase boundaryat the end, the reordering orientation with respectto the original position of the words is checked.The probability for the respective orientation is in-cluded as an additional score.5 Translation ModelsIn addition to the models used in the baseline sys-tem described above, we conducted experimentsincluding additional models that enhance trans-lation quality by introducing alternative or addi-tional information into the translation modelingprocess.5.1 Bilingual Language ModelDuring the decoding the source sentence is seg-mented so that the best combination of phraseswhich maximizes the scores is available.
How-ever, this causes some loss of context informationat the phrase boundaries.
In order to make bilin-gual context available, we use a bilingual languagemodel (Niehues et al 2011).
In the bilingual lan-guage model, each token consists of a target wordand all source words it is aligned to.5.2 Discriminative Word LexiconMauser et al(2009) introduced the DiscriminativeWord Lexicon (DWL) into phrase-based machinetranslation.
In this approach, a maximum entropymodel is used to determine the probability of usinga target word in the translation.In this evaluation, we used two extensions tothis work as shown in (Niehues and Waibel, 2013).First, we added additional features to model theorder of the source words better.
Instead of rep-resenting the source sentence as a bag-of-words,we used a bag-of-n-grams.
We used n-grams up tothe order of three and applied count filtering to thefeatures for higher order n-grams.Furthermore, we created the training examplesdifferently in order to focus on addressing errorsof the other models of the phrase-based translation105system.
We first translated the whole corpus with abaseline system.
Then we only used the words thatoccur in the N-Best List and not in the reference asnegative examples instead of using all words thatdo not occur in the reference.5.3 Quasi-Morphological OperationsBecause of the inflected characteristic of theGerman language, we try to learn quasi-morphological operations that change the lexi-cal entry of a known word form to the out-of-vocabulary (OOV) word form as described inNiehues and Waibel (2012).5.4 Phrase Table AdaptationFor the French?English systems, we built twophrase tables; one trained with all data and theother trained only with the EPPS and NC cor-pora.
This is due to the fact that Giga corpus is bigbut noisy and EPPS and NC corpus are more reli-able.
The two models are combined log-linearly toachieve the adaptation towards the cleaner corporaas described in Niehues et al(2010).6 Language ModelsThe 4-gram language models generated by theSRILM toolkit are used as the main languagemodels for all of our systems.
For theEnglish?French systems, we use a good qualitycorpus as in-domain data to train in-domain lan-guage models.
Additionally, we apply the POSand cluster language models in different systems.For the German?English system, we build sepa-rate language models using each corpus and com-bine them linearly before the decoding by mini-mizing the perplexity.
Language models are inte-grated into the translation system by a log-linearcombination and receive optimal weights duringtuning by the MERT.6.1 POS Language ModelsFor the English?German system, we use the POSlanguage model, which is trained on the POS se-quence of the target language.
The POS tags aregenerated using the RFTagger (Schmid and Laws,2008) for German.
The RFTagger generates fine-grained tags which include person, gender, andcase information.
The language model is trainedwith up to 9-gram information, using the Germanside of the parallel EPPS and NC corpus, as wellas the News Shuffle corpus.6.2 Cluster Language ModelsIn order to use larger context information, we usea cluster language model for all our systems.
Thecluster language model is based on the idea shownin Och (1999).
Using the MKCLS algorithm, wecluster the words in the corpus, given a numberof classes.
Then words in the corpus are replacedwith their cluster IDs.
Using these cluster IDs,we train n-gram language models as well as aphrase table with this additional factor of clusterID.
Our submitted systems have diversed range ofthe number of clusters as well as n-gram.7 ResultsUsing the models described above we performedseveral experiments leading finally to the systemsused for generating the translations submitted tothe workshop.
The results are reported as case-sensitive BLEU scores on one reference transla-tion.7.1 German?EnglishThe experiments for the German to English trans-lation system are summarized in Table 1.
Thebaseline system uses POS-based reordering, DWAwith lattice phrase extraction and language modelstrained on the News Shuffle corpus and Giga cor-pus separately.
Then we added a 5-gram clusterLM trained with 1,000 word classes.
By adding alanguage model using the filtered crawled data wegained 0.3 BLEU on the test set.
For this we com-bined all language models linearly.
The filteredcrawled data was also used to generate a phrasetable, which brought another improvement of 0.85BLEU.
Applying tree-based reordering improvedthe BLEU score, and the performance had moregain by adding the extended DWL, namely us-ing both bag-of-ngrams and n-best lists.
Whilelexicalized reordering gave us a slight gain, weadded morphological operation and gained moreimprovements.7.2 English?GermanThe English to German baseline system uses POS-based reordering and language models using par-allel data (EPPS and NC) as shown in Table 2.Gradual gains were achieved by changing align-ment from GIZA++ to DWA, adding a bilinguallanguage model as well as a language model basedon the POS tokens.
A 9-gram cluster-based lan-guage model with 100 word classes gave us a106System Dev TestBaseline 24.15 22.79+ Cluster LM 24.18 22.84+ Crawled Data LM (Comb.)
24.53 23.14+ Crawled Data PT 25.38 23.99+ Tree Rules 25.80 24.16+ Extended DWL 25.59 24.54+ Lexicalized Reordering 26.04 24.55+ Morphological Operation - 24.62Table 1: Translation results for German?Englishsmall gain.
Improving the reordering using lexi-alized reordering gave us gain on the optimizationset.
Using DWL let us have more improvementson our test set.
By using the filtered crawled data,we gained a big improvement of 0.46 BLEU onthe test set.
Then we extended the DWL with bagof n-grams and n-best lists to achieve additionalimprovements.
Finally, the best system includeslattices generated using tree rules.System Dev TestBaseline 17.00 16.24+ DWA 17.27 16.53+ Bilingual LM 17.27 16.59+ POS LM 17.46 16.66+ Cluster LM 17.49 16.68+ Lexicalized Reordering 17.57 16.68+ DWL 17.58 16.77+ Crawled Data 18.43 17.23+ Extended DWL 18.66 17.57+ Tree Rules 18.63 17.70Table 2: Translation results for English?German7.3 French?EnglishTable 3 reports some remarkable improvementsas we combined several techniques on theFrench?English direction.
The baseline systemwas trained on parallel corpora such as EPPS, NCand Giga, while the language model was trainedon the English part of those corpora plus NewsShuffle.
The newly presented web-crawled datahelps to achieve almost 0.6 BLEU points moreon test set.
Adding bilingual language model andcluster language model does not show a significantimpact.
Further gains were achieved by the adap-tation of in-domain data into general-theme phrasetable, bringing 0.15 BLEU better on the test set.When we added the DWL feature, it notably im-proves the system by 0.25 BLEU points, resultingin our best system.System Dev TestBaseline 30.33 29.35+ Crawled Data 30.59 29.93+ Bilingual and Cluster LMs 30.67 30.01+ In-Domain PT Adaptation 31.17 30.16+ DWL 31.07 30.40Table 3: Translation results for French?English7.4 English?FrenchIn the baseline system, EPPS, NC, Giga and NewsShuffle corpora are used for language modeling.The big phrase tables tailored EPPC, NC and Gigadata.
The system also uses short-range reorderingtrained on EPPS and NC.
Adding parallel and fil-tered crawl data improves the system.
It was fur-ther enhanced by the integration of a 4-gram bilin-gual language model.
Moreover, the best config-uration of 9-gram language model trained on 500clusters of French texts gains 0.25 BLEU pointsimprovement.
We also conducted phrase-tableadaptation from the general one into the domaincovered by EPPS and NC data and it helps as well.The initial try-out with lexicalized reordering fea-ture showed an improvement of 0.23 points on thedevelopment set, but a surprising reduction on thetest set, thus we decided to take the system afteradaptation as our best English?French system.System Dev TestBaseline 30.50 27.77+ Crawled Data 31.05 27.87+ Bilingual LM 31.23 28.50+ Cluster LM 31.58 28.75+ In-Domain PT Adaptation 31.88 29.12+ Lexicalized Reordering 32.11 28.98Table 4: Translation results for English?French8 ConclusionsWe have presented the systems for our par-ticipation in the WMT 2013 Evaluation forEnglish?German and English?French.
All sys-tems use a class-based language model as wellas a bilingual language model.
Using a DWLwith source context improved the translation qual-ity of English?German systems.
Also for thesesystems, we could improve even more with atree-based reordering model.
Special handling107of OOV words improved German?English sys-tem, while for the inverse direction the languagemodel with fine-grained POS tags was helpful.
ForEnglish?French, phrase table adaptation helps toavoid using wrong parts of the noisy Giga corpus.9 AcknowledgementsThis work was partly achieved as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
The research lead-ing to these results has received funding fromthe European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementn?
287658.ReferencesTeresa Herrmann, Jan Niehues, and Alex Waibel.2013.
Combining Word Reordering Methods ondifferent Linguistic Abstraction Levels for Statisti-cal Machine Translation.
In Proceedings of the Sev-enth Workshop on Syntax, Semantics and Structurein Statistical Translation, Altanta, Georgia, USA,June.
Association for Computational Linguistics.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL, Bu-dapest, Hungary.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-based Lexicon Models.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1 - Volume 1, EMNLP ?09, Singapore.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The kit english-french translation systems for iwslt 2011.
In Pro-ceedings of the eight International Workshop onSpoken Language Translation (IWSLT).Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proc.
of Third ACL Workshop on Statistical Ma-chine Translation, Columbus, USA.Jan Niehues and Alex Waibel.
2012.
Detailed Analysisof different Strategies for Phrase Table Adaptationin SMT.
In Proceedings of the American MachineTranslation Association (AMTA), San Diego, Cali-fornia, October.Jan Niehues and Alex Waibel.
2013.
An MT Error-driven Discriminative Word Lexicon using SentenceStructure Features.
In Eighth Workshop on Statisti-cal Machine Translation (WMT 2013), Sofia, Bul-garia.Jan Niehues, Mohammed Mediani, Teresa Herrmann,Michael Heck, Christian Herff, and Alex Waibel.2010.
The KIT Translation system for IWSLT 2010.In Marcello Federico, Ian Lane, Michael Paul, andFranc?ois Yvon, editors, Proceedings of the seventhInternational Workshop on Spoken Language Trans-lation (IWSLT), pages 93?98.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, UK.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proceedings ofthe ninth conference on European chapter of the As-sociation for Computational Linguistics, EACL ?99,pages 71?76, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical Re-port RC22176 (W0109-022), IBM Research Divi-sion, T. J. Watson Research Center.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three German treebanks: lexicalized and un-lexicalized baselines.
In Proceedings of the Work-shop on Parsing German, Columbus, Ohio.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI, Sko?vde,Sweden.Helmut Schmid and Florian Laws.
2008.
Estimationof Conditional Probabilities with Decision Trees andan Application to Fine-Grained POS Tagging.
InCOLING 2008, Manchester, Great Britain.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of ICSLP, Denver,Colorado, USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Be-yond (WPT-05), Ann Arbor, MI.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural LanguageProcessing and Knowledge Engineering, Beijing,China.108
