Proceedings of the 6th Workshop on Statistical Machine Translation, pages 542?553,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsFrom n-gram-based to CRF-based Translation ModelsThomas Lavergne Josep Maria CregoLIMSI/CNRSBP 133F-91 403 Orsay Ce?dex{lavergne,jmcrego}@limsi.frAlexandre Allauzen Franc?ois YvonLIMSI/CNRS & Uni.
Paris SudBP 133F-91 403 Orsay Ce?dex{allauzen,yvon}@limsi.frAbstractA major weakness of extant statistical ma-chine translation (SMT) systems is their lackof a proper training procedure.
Phrase extrac-tion and scoring processes rely on a chain ofcrude heuristics, a situation judged problem-atic by many.
In this paper, we recast the ma-chine translation problem in the familiar termsof a sequence labeling task, thereby enablingthe use of enriched feature sets and exact train-ing and inference procedures.
The tractabil-ity of the whole enterprise is achieved throughan efficient implementation of the conditionalrandom fields (CRFs) model using a weightedfinite-state transducers library.
This approachis experimentally contrasted with several con-ventional phrase-based systems.1 IntroductionA weakness of existing phrase-based SMT systems,that has been repeatedly highlighted, is their lackof a proper training procedure.
Attempts to de-sign probabilistic models of phrase-to-phrase align-ments (e.g.
(Marcu and Wong, 2002)) have thus farfailed to overcome the related combinatorial prob-lems (DeNero and Klein, 2008) and/or to yield im-proved training heuristics (DeNero et al, 2006).Phrase extraction and scoring thus rely on a chainof heuristics see (Koehn et al, 2003), which evolvephrase alignments from ?symmetrized?
word-to-word alignments obtained with IBM models (Brownet al, 1990) and the like (Liang et al, 2006b; Dengand Byrne, 2006; Ganchev et al, 2008).
Phrasescoring is also mostly heuristic and relies on an op-timized interpolation of several simple frequency-based scores.
Overall, the training procedure oftranslation models within conventional phrase-based(or hierarchical) systems is generally considered un-satisfactory and the design of better estimation pro-cedures remains an active research area (Wuebker etal., 2010).To overcome the NP-hard problems that derivefrom the need to consider all possible permutationsof the source sentence, we make here a radicalsimplification and consider training the translationmodel given a fixed segmentation and reordering.This idea is not new, and is one of the groundingprinciple of n-gram-based approaches (Casacubertaand Vidal, 2004; Marin?o et al, 2006) in SMT.
Thenovelty here is that we will use this assumption to re-cast machine translation (MT) in the familiar termsof a sequence labeling task.This reformulation allows us to make use of theefficient training and inference tools that exists forsuch tasks, most notably linear CRFs (Lafferty etal., 2001; Sutton and McCallum, 2006).
It also en-ables to easily integrate linguistically informed (de-scribing morphological or morpho-syntactical prop-erties of phrases) and/or contextual features into thetranslation model.
In return, in addition to havinga better trained model, we also expect (i) to makeestimation less sensible to data sparsity issues and(ii) to improve the ability of our system to makethe correct lexical choices based on the neighbor-ing source words.
As explained in Section 2, thisreformulation borrows much from the general ar-chitecture of n-gram MT systems and implies tosolve several computational challenges.
In our ap-542proach, the tractability of the whole enterprise isachieved through an efficient reimplementation ofCRFs using a public domain library for weightedfinite-state transducers (WFSTs) (see details in Sec-tion 3).
This approach is experimentally contrastedwith more conventional n-gram based and phrase-based approaches on a standard benchmark in Sec-tion 4, where we also evaluate the benefits of variousfeature sets and training regimes.
We finally relateour new system with alternative proposals for train-ing discriminatively SMT systems in Section 5, be-fore drawing some lessons and discussing possibleextensions of this work.The main contribution of this work are thus (i) adetailed presentation of the CRF in translation in-cluding all necessary implementation details and (ii)an experimental study of various feature functionsand of various ways to integrate target side LM in-formation.2 MT as sequence labelingIn this section, we briefly review the n-gram basedapproach to SMT, originally introduced in (Casacu-berta and Vidal, 2004; Marin?o et al, 2006), whichconstitutes our starting point.
We then describe ournew proposal, which, in essence, consists in replac-ing the modeling of compound source-target trans-lation units by a conditional model where the prob-ability of each target side phrase is conditioned onthe source sentence.2.1 The n-gram based approach in SMTThe n-gram based approach of (Marin?o et al, 2006)is a variation of the standard phrase-based model,characterized by the peculiar form of the translationmodel.
In this approach, the translation model isbased on bilingual units called tuples.
Tuples arethe analogous of phrase pairs, as they represent amatching u = (e, f) between a source f and a tar-get e word sequence.
The probability of a sequenceof tuples is computed using a conventional n-grammodel as:p(u1 .
.
.
uI) =I?i=1p(ul|ui?1 .
.
.
ui?n+1).The probability of a sentence pair (f , e) is then ei-ther recovered by marginalization, or approximatedby maximization, over all possible joint segmenta-tions of f and e into tuples.As for any n-gram model, the parameters are es-timated using statistics collected in a training corpusmade of sequences of tuples derived from the par-allel sentences in a two step process.
First, a wordalignment is computed using a standard alignmentpipeline1 based on the IBM models.
Source wordsare then reordered so as to disentangle the align-ment links and to synchronize the source and tar-get texts.
Special care has to be paid to non-alignedsource words, which have to be collapsed with theirneighbor words.
A byproduct of this process is a de-terministic joint segmentation of parallel sentencesinto minimal bilingual units, the tuples, that consti-tute the basic elements in the model.
This process isillustrated on Figure 1, where the unfolding processenables the extraction of tuples such as: (demanda,said ) or (de nouveau, again).f : demanda de nouveau la femme voile?ee: the veiled dame said againf?
: la voile?e femme demanda de nouveauFigure 1: The tuple extraction processThe original (top) and reordered (bottom) Frenchsentence aligned with its translation.At test time, the source text is reordered so asto match the reordering implied by the disentangle-ment procedure.
Various proposals has been madeto perform such source side reordering (Collins etal., 2005; Xia and McCord, 2004), or even learn-ing reordering rules based on syntactic or morpho-syntactic information (Crego and Marin?o, 2007).The latter approach amounts to accumulate reorder-ing patterns during the training; test source sen-tences are then non-deterministically reordered inall possible ways yielding a word graph.
This graphis then monotonously decoded, where the score ofa translation hypothesis combines information fromthe translation models as well as from other infor-mation sources (lexicalized reordering model, target1Here, using the MGIZA++ package (Gao and Vogel, 2008).543side language model (LM), word and phrase penal-ties, etc).2.2 Translating with CRFsA discriminative version of the n-gram approachconsists in modeling P (e|f) instead of P (e, f),which can be efficiently performed with CRFs (Laf-ferty et al, 2001; Sutton and McCallum, 2006).
As-suming matched sequences of observations (x =xL1 ) and labels (y = yL1 ), CRFs express the con-ditional probability of labels as:P (yL1 |xL1 ) =1Z(xL1 ; ?
)exp(?TG(xL1 , yL1 )),where ?
is a parameter vector and G denotes a vec-tor of feature functions testing various properties ofx and y.
In the linear-chain CRF, each compo-nent Gk(xI1, yI1) of G is decomposed as a sum oflocal features: Gk(xI1, yI1) =?i gk(xI1, yi?1, yi)2.CRFs are trained by maximizing the (penalized) log-likelihood of a corpus containing observations andtheir labels.In principle, the data used to train n-gram trans-lation models provide all the necessary informationrequired to train a CRF3.
It suffices to consider thatthe alphabet of possible observations ranges over allpossible source side fragments, and that each tar-get side of a tuple is a potential label.
The modelthus defines the probability of a segmented targete?
= e?I1 given the segmented and reordered sourcesentence f?
= f?
I1 .
To complete the model, one justneeds to define a distribution over source segmen-tations P (f?
|f).
Given the deterministic relationshipbetween e and e?
expressed by the ?unsegmentation?function ?
which maps e?
with e = ?(e?
), we thenhave:P (e|f) =?f?
,e|?
(e)=eP (e?, f?
|f)=?f?
,e|?
(e)=eP (e?, |f?
, f)P (f?
|f)=?f?
,e|?
(e)=eP (e?, |f?
)P (f?
|f)2Assuming first order dependencies.3This is a significant difference with (Blunsom et al, 2008),as we do not need to introduce latent variables during training.In practice, we will only consider a restrictednumber of possible segmentation/reorderings of thesource, denoted L(f), and compute the best transla-tion e?
as ?(e??
), where:e??
= arg maxeP (e?|f)?
arg maxf?
?L(f),eP (e?, |f?
, f)P (f?
|f) (1)Even with these simplifying assumptions, thisapproach raises several challenging computationalproblems.
First, training a CRF is quadratic in thenumber of labels, of which we will have plenty (typ-ically hundreds of thousands).
A second issue is de-coding: as we need to consider at test time a combi-natorial number of possible source reorderings andsegmentations, we can no longer dispense with thecomputation of the normalizer Z(f?
; ?)
which is re-quired to compute P (e?, f?
|f) as P (f?
|f)P (e?|f?)
and tocompare hypotheses associated with different valuesof f?
.
We discuss our solutions to these problems inthe next section.3 Implementation issues3.1 TrainingBasic training The main difficulties in training arecaused by the unusually large number of labels, eachof which corresponds to a (small) sequence of targetwords.
Hopefully, each observation (source side tu-ple) occurs with a very small number of differentlabels.
A first simplification is thus to consider thatthe set of possible ?labels?
e?
for a source sequencef?
is limited to those that are seen in training: allthe other associations (f?
, e?)
are deemed impossible,which amounts to setting the corresponding param-eter value to ?
?.A second speed-up is to enforce sparsity in themodel, through the use of a `1 regularization term(Tibshirani, 1996): on the one hand, this greatly re-duces the memory usage; furthermore, sparse mod-els are also prone to various optimization of theforward-backward computations (Lavergne et al,2010).
As discussed in (Ng, 2004; Turian et al,2007), this feature selection strategy is well suitedto the task at hand, where the number of possiblefeatures is extremely large.
Optimization is per-544formed using the Rprop algorithm4 (Riedmiller andBraun, 1993), which provides the memory efficiencyneeded to cope with the very large feature sets con-sidered here.Training with a target language model One ofthe main strength of the phrase-based ?log-linear?models is their ability to make use of powerfultarget side language models trained on very largeamounts of monolingual texts.
This ability is crucialto achieve good performance and has to be preservedno matter the difficulties that occur when one movesaway from conventional phrase-based systems (Chi-ang, 2005; Huang and Chiang, 2007; Blunsom andOsborne, 2008; Ka?a?ria?inen, 2009).
It thus seemsappropriate to include a LM feature function in ourmodel or alternatively to define:P (e?|f?)
=1Z(f?
; ?
)PLM (e?)
exp(?TG(f?
, e?
)),where PLM is the target language model andZ(f?
; ?)
=?e PLM (e?)
exp(?TG(f?
, e?)).
Imple-menting this approach implies to deal with the lackof synchronization between the units of the trans-lation models, which are variable-length (possiblyempty) tuples, and the units of the language models,which are plain words.In practice, this extension is implemented by per-forming training and inference over a graph whosenodes are not only indexed by their position and theleft target context, but also by the required n-gram(target) history.
In most cases, for small values ofn such as considered in this study, the n-gram his-tory can be deduced from the left target tuple.
Themost problematic case is when the left target tupleis NULL, which require to copy the history from theprevious states.
As a consequence, for the values ofn considered here, the impact of this extension onthe total training time is limited.Reference reachability A recurring problem fordiscriminative training approaches is reference un-reachability (Liang et al, 2006a): this happens whenthe model cannot predict the reference translation,which means in our case that the probability of thereference cannot be computed.
In our implementa-tion, this only happens when the reference involves4Adapted to handle a locally non-differentiable objective.a tuple (f?
,e?)
that is too rare to be included in themodel.
As a practical workaround, when this hap-pens for a given training sentence, we make sureto ?locally?
augment the tuple dictionary with themissing part of the reference, which is then removedfor processing the rest of the training corpus.3.2 InferenceOur decoder is implemented as a cascade ofweighted finite-state transducers (WFSTs) using thefunctionalities of the OpenFst library (Allauzen etal., 2007).
This library provides many basic opera-tion for WFSTs, notably the left (pi1) and right (pi2)projections as well as the composition operation (?
).The related notions and algorithms are presented indetail in (Mohri, 2009), to which we refer the reader.In essence, our decoder is implemented of a finite-state cascade involoving the following steps: (i)source reordering and segmentation (ii) applicationof the translation model and (optionally) (iii) com-position with a target side language model, an ar-chitecture that is closely related to the proposal of(Kumar et al, 2006).
A more precise account ofthese various steps is given below, where we de-scribe the main finite-state transducers involved inour decoder:?
S, the acceptor for the source sentence f ;?
R, which implements segmentation and re-ordering rules;?
T , the tuple dictionary, associating source sidesequences with possible translations based onthe inventory of tuples;?
F , the feature matcher, mapping each featurewith the corresponding parameter value;Source reordering The computation of R mainlyfollows the approach of (Crego and Marin?o, 2007)and uses a part-of-speech tagged version of the re-ordered training data.
Each reordering pattern seenin training is generalized as a non-deterministic re-ordering rule which expresses a possible rearrange-ment of some subpart of the source sentence.
Eachrule is implemented as an elementary finite-statetransducer, and the set of possible word reorderingsis computed as the composition of these transducers.R is finally obtained by composing the result with a545transducer computing all the possible segmentationsof its input into sequences of source side tuples5.The output of S ?
R are sequences of source sidetuples f?
; each path in this transducer is addition-ally weighted with a simplistic n-tuple segmentationmodel, estimated using the source side of the paral-lel training corpus.
Note that these scores are nor-malized, so that the weight of each path labelled f?
inS ?R is logP (f?
|f).The feature matcher F The feature matcher isalso implemented as a series of elementary weightedtransducers, each transducer being responsible for agiven class of feature functions.
The simplest trans-ducer in this family deals with the class of unigramfeature functions, ie.
feature functions that only testthe current observation and label.
It is representedon the left part of Figure 3.2, where for the sake ofreadability we only display one example for eachtest pattern (here: an unconditional feature that al-ways returns true for a given label, a test on thesource word, and a test on the source POS label).As long as dependencies between source and/or tar-get symbols remain local, they can be captured byfinite-state transducers such as the ones on the midand right part of Figure 3.2, which respectively com-pute bigram target features, and joint bigram sourceand target features.The feature matcher F is computed as the com-position of these elementary transducers, where weonly include source and target labels that can occurgiven the current input sentence.
Weights in F areinterpreted in the tropical semiring.
exp(F ) is ob-tained by replacing weights w in F with exp(w) inthe real semiring.Decoding a word graph If the input segmentationand reordering were deterministically set, meaningthat the automaton I = pi1(S ?
R ?
T ) would onlycontain one path, decoding would amount to findingthe best path in S ?R ?
T ?F .
However, we need tocompute:arg maxeP (e?|f) = arg maxe?f?P (e?, f?
|f)= arg maxe?f?P (e?|f?
)P (f?
|f).5When none is found, we also consider a maximal segmen-tation into isolated words.This requires to compare model scores for mul-tiple source segmentations and reorderings f?
, henceto compute P (f?
|f) and P (e?|f?
), rather than just thenon-normalized value that is usually used in CRFs.Computing the normalizer Z(f?
; ?)
for all se-quences in S ?R is performed efficiently using stan-dard finite-state operations as :D = det(pi1(pi2(S ?R) ?
T ?
exp(F ))).In fact, determinization (in the real semiring) has theeffect of accumulating for each f?
the correspondingnormalizer Z(f?
; ?).
Replacing each weight w in Dby ?
log(w) and using the log semiring enables tocompute?
log(Z(f?
; ?)).
The best translation is thenobtained as: bestpath(pi2(S?R)?
?log(D)?T ?F )in the tropical semiring.Decoding and Rescoring with a target languagemodel An alternative manner of using a (large)target side language model is to use it for rescoringpurposes.
The consistent use of finite-state machinesand operations makes it fairly easy to include oneduring decoding : it suffices to perform the search inpi2(S?R)??
log(D)?T ?F ?L, where L representsa n-gram language model.
When combining severalmodels, notably a source segmentation model and/ora target language model for rescoring, we have madesure to rescale the (log)probabilities so as to balancethe language model scores with the CRF scores, andto use a fixed word bonus to make hypotheses of dif-ferent length more comparable.
All these parametersare tuned as part of the decoder development pro-cess.
It is finally noteworthy that, in our architecture,alternative decoding strategies, such as MBR (Ku-mar and Byrne, 2004) are also readily implemented.4 Experiments4.1 Corpora and metricsFor these experiments, we have used a medium sizetraining corpus, extracted from the datasets madeavailable for WMT 20116 evaluation campaign, andhave focused on one translation direction, fromFrench to English7.Translation model training uses the entire News-Commentary subpart of the WMT?2011 training6statmt.org/wmt117Results in the other direction suggest similar conclusions.5460le : the/?le,theDET : the/?DET,the?
: the/?the 0 1?
: the/0?
: cat/?the,cat0 1?
: the/0chat : cat/?chat,catFigure 2: Feature matchers.
The star symbol (*) matches any possible observation.French Englishsent?
token types token typestrain 115 K 3 339 K 60 K 2 816 K 58 Ktest 2008 2.0 K 55 K 9 K 49 K 8 Ktest 2009 2.5 K 72 K 11 K 65 K 10 Ktest 2010 2.5 K 69 K 10 K 61 K 9 KTable 1: Corpora used for the experimentsdata; for language models, we have considered twoapproaches (i) a ?large?
bigram model highly opti-mized using all the available monolingual data and(ii) a ?small?
trigram language model trained onjust the English side of the NewsCommentary cor-pus.
The regularization parameters used in trainingare tuned using the WMT 2009 test set; the variousparameters implied in the decoding are tuned (forBLEU) on WMT 2008 test set; the internal tests re-ported below are performed on the 2010 test lines(see Table 1) using the best parameters found duringtuning.
Various statistics regarding these corpora arereproduced on Table 1.All the training corpora were aligned usingMGIZA++ with standard parameters8, and pro-cessed in the standard tuple extraction pipeline.
Thedevelopment and test corpora were also processedanalogously.
For the sake of comparison, we alsotrained a standard n-gram-based and a Moses sys-tem (Koehn et al, 2007) with default parametersand a 3-gram target LM trained using only the tar-get side of our parallel corpus.
The development set(test 2009) was used to tune these two systems.
Allperformance are measured using BLEU (Papineni etal., 2002).8As part of a much larger batch of texts.4.2 FeaturesThe baseline system is composed only of transla-tion features [trs] and target bigram features [t2g].The former correspond to functions of the formgus,t(f?
, e?, i) = I(f?i = s ?
e?i = t), where sand t respectively denote source and target phrasesand I() is the indicator function.
These are alsogeneralized to part-of-speech and also to any pos-sible source phrase, giving rise to features such asgu?,t = (f?
, e?, i) = I(e?i = t).
Target bigram featurescorrespond to functions of the form gbt,t?(f?
, e?, i) =I(e?i?1 = t?
e?i = t?).
The last baseline feature is thecopy feature, which fires whenever the source andtarget segments are identical.Supplementary groups of features are consideredin further stages:?
suffix/prefix features [ix].
These features allowto generalize baseline features on the sourceside to fixed length prefixes and suffixes, thussmoothing the parameters.?
context features [ctx].
These features are sim-ilar to unigram features, but also test the leftsource tuple and the corresponding part-of-speech.?
segmentation features [seg].
These features aremeant to express a preference for longer tuplesand to regulate the number of target words persource word.
We consider the following featurefunctions (|e| denotes the length of e):?
target length features :gl?,l(f?
, e?, i) = I(|e?i| = l)?
source-target length features :gll,l?(f?
, e?, i) = I(|f?i| = l ?
|e?i| = l?)?
source-target length ratio :gll(f?
, e?, i) = I(round(| efi||ei|) = l)547Note that all these features are further condi-tioned on the target label.?
reordering features [ord].
These features aremeant to model preferences for specific lo-cal reordering patterns and take into accountneighbor source fragments in e?
together withthe current label.
Each source side segmentf?i is made of some source words that, priorto source reordering, were located at indicesi1 .
.
.
il, so that f?i = fi1 .
.
.
fil .
The high-est (resp.
lowest) index in this sequence is df?ie(resp.
bf?ic).
The leftmost (resp.
rightmost) in-dex is [f?i[ (resp.
]f?i]).Using these notations, our model includes thefollowing patterns:?
distortion features, measuring the gaps be-tween consecutive source fragments :gol,t(f?
, e?, i)=I(?
(f?i, e?i)= l ?
e?i= t),where ?
(f?i, e?i) ={bf?ic ?
df?i?1e if (df?i?1e ?
bf?ic)df?ie ?
bf?i?1c otherwise .?
lexicalized reordering, identifying mono-tone, swap and discontinuous configura-tions (Tillman, 2004).
The monotonoustest is defined as: gom(f?
, e?, i) =I(]ei?1] = [ei[); the swap and discon-tinuous configurations are defined analo-gously.?
?gappiness?
test : this feature is activatedwhenever the source indices i1...il containone or several gaps.4.3 Experiments and lessons learnedTraining time The first lesson learned is thattraining can be performed efficiently.
Our baselinesystem, which only contains trs and trg contains ap-proximately 87 million features, out of which a lit-tle bit more than 600K are selected.
Adding up allsupplementary features raises the number of param-eters to about 130M features, out of which 1.5M arefound useful.
All these systems require between 3and 5 hours to train9.
These numbers are obtainedwith a `1 penalty term ?
1, which offers a good bal-ance between accuracy and sparsity.9All experiments run on a server with 64G of memory andtwo Xeon processors with 4 cores at 2.27 Ghz.Test conditions In order to better assess thestrengths and weaknesses of our approach, we com-pare several test settings: the most favorable con-siders only one possible segmentation/reordering f?for each f , obtained through forced alignment withthe reference; we then consider the more challeng-ing case where the reordering is fixed, but severalsegmentations are considered; then the regular de-coding task, where both segmentation and reorder-ing are unknown and where the entire space of allsegmentations and reordering is searched.
For eachcondition, we also vary (i) the set of features usedand (ii) the target language model used, if any.Wherever applicable, we also report contrasts withn-gram-based systems subject to the same input andcomparable resources, varying the order of the tuplelanguage model, as well as with Moses.
Results arein Table 2.dev test # feat.decoding with optimal segmentation/reorderingCRF (trs,trg) 23.8 25.1 660KCRF +ctx 24.1 25.4 1.5MCRF +ix,ord,seg 24.3 25.6 1.5Mdecoding with optimal reorderingn-gram (2g,3g) 20.6 24.1 755Kn-gram (3g,3g) 21.5 25.2 755KCRF trs,trg - 22.8 660KCRF +ctx - 23.1 1.5MCRF +ix,ord,seg - 23.5 1.5Mregular decodingMoses (3g) 21.2 20.5n-gram (2g,3g) 20.6 20.2 755Kn-gram (3g,3g) 21.5 21.2 755KCRF (trs,trg) - 18.3 660KCRF +ctx - 18.8 1.5MCRF +ix,ord,seg - 19.1 1.5MCRF +ix,ord,seg+3g - 19.1 1.5MTable 2: Translation performanceExtending the feature set As expected, the useof increasingly complex feature sets seems benefi-cial in all experimented conditions.
It is noteworthythat throwing in reordering and contextual featuresis helping, even when decoding one single segmen-tation and reordering.
This is because these featuresdo not help to select the best input reordering, but548help choose the best target phrase.Searching a larger space Going from the sim-pler to the more difficult conditions yields signif-icant degradations in the model, as our best scoredrops down from 25.6 to 23.5 (with known reorder-ing) then to 19.1 (regular decoding).
This is a clearindication that our current segmentation/reorderingmodel is not delivering very useful scores.
A similarloss is incurred by the n-gram system, which loses4 bleu points between the two conditions.LM rescoring Our results to date with target sidelanguage models have proven inconclusive, whichmight explain why our best results remain betweenone and two BLEU points behind the n-gram basedsystem using comparable information.
Note alsothat preliminary experiments with incorporating alarge bigram during training have also failed to dateto provide us with improvements over the baseline.Summary In sum, the results accumulated duringthis first round of experiments tend to show that ourCRF model is still underperforming the more es-tablished baseline by approximately 1 to 1.5 BLEUpoint, when provided with comparable resources.Sources of improvements that have been clearlyidentified is the scoring of reordering and segmen-tations, and the use of a target language model intraining and/or decoding.5 Related workDiscriminative learning approaches have provensuccessful for many NLP tasks, notably thanks totheir ability to cope with flexible linguistic repre-sentations and to accommodate potentially redun-dant descriptions.
This is especially appealing formachine translation, where the mapping betweena source word or phrase and its target correlate(s)seems to involve an large array of factors, such as itsmorphology, its syntactic role, its meaning, its lexi-cal context, etc.
(see eg.
(Och et al, 2004; Gimpeland Smith, 2008; Chiang et al, 2009), for inspira-tion regarding potentially useful features in SMT).Discriminative learning requires (i) a parameter-ized scoring function and (ii) a training objective.The scoring function is usually assumed to be linearand ranks candidate outputs y for input x accord-ing to ?TG(x, y), where ?
is the parameter vector.
?andG deterministically imply the input/output map-ping as x ?
arg maxy ?TG(x, y).
Given a set oftraining pairs {xi, yi, i = 1 .
.
.
N}, parameters arelearned by optimizing some regularized loss func-tion of ?, so as to make the inferred input/outputmapping faithfully replicate the observed instances.Machine translation, like most NLP tasks, doesnot easily lend itself to that approach, due to thecomplexity of the input/output objects (word or la-bel strings, parse trees, dependency structures, etc).This complexity makes inference and learning in-tractable, as both steps imply the resolution ofthe arg max problem over a combinatorially largespace of candidates y.
Structured learning tech-niques (Bakir et al, 2007), developed over the lastdecade, rely on decompositions of these objects intosub-parts as part of a derivation process, and useconditional independence assumptions between sub-parts to render the learning and inference problemtractable.
For machine translation, this only pro-vides part of the solution, as the training data onlycontain pairs of word aligned sentences (f , e), butlack the explicit derivation h from f to e that is re-quired to train the model in a fully supervised way.The approach of (Liang et al, 2006a) circumventsthe issue by assuming that the hidden derivation hcan be approximated through forced decoding.
As-suming that h is in fact observed as the optimal(Viterbi) derivation h?
from f to e given the cur-rent parameter value10, it is straightforward to re-cast the training of a phrase-based system as a stan-dard structured learning problem, thus amenable totraining algorithms such as the averaged perceptronof (Collins, 2002).
This approximation is howevernot genuine, and the choice of the most appropriatederivation seems to raises intriguing issues (Watan-abe et al, 2007; Chiang et al, 2008).The authors of (Blunsom et al, 2008; Blunsomand Osborne, 2008) consider models for which it iscomputationally possible to marginalize out all pos-sible derivations of a given translation.
As demon-strated in these papers, this approach is tractableeven when the derivation process is a based on syn-chronous context-free grammars, rather that finite-state devices.
However, the computational cost as-10If one actually exists in the model, thus raising the issue ofreference reachability, see discussion in Section 3.549sociated with training and inference remains veryhigh, especially when using a target side languagemodel, which seems to preclude the application tolarge-scale translation tasks11.
The recent work of(Dyer and Resnik, 2010) proceeds from a similarvein: translation is however modeled as a two stepprocess, where a set of possible source reorderings,represented as a parse forest, are associated withpossible target sentences, using, as we do, a finite-state translation model.
This translation model istrained discriminatively by marginalizing out the(unobserved) reordering variables; inference can beperformed effectively by intersecting the input parseforest with a transducer representing translation op-tions.A third strategy is to consider a simpler class ofderivation process, which only partly describe themapping between f and e. This is, for instance,the approach of (Bangalore et al, 2007), where asimple bag-of-word representation of the target sen-tence is computed using a battery of boolean clas-sifiers (one for each target word).
In this approach,discriminative training is readily applicable, as therequired supervision is overtly present in examplesource-target pairs (f , e); however, a complemen-tary reshaping/reordering step is necessary to turnthe bag-of-word into a full-fledged translation.
Thiswork was recently revisited in (Mauser et al, 2009),where a conditional model predicting the presenceof each target phrase provides a supplementary scorefor the standard ?log-linear?
model.This line of research has been continued notablyin (Ka?a?ria?inen, 2009), which introduces an exponen-tial model of bag of phrases (allowing some over-lap), that enables to capture localized dependenciesbetween target words, while preserving (to some ex-tend) the efficiency of training and inference.
Su-pervision is here indirectly provided by word align-ment and correlated phrase extraction processesimplemented in conventional phrase-based systems(Koehn et al, 2003).
If this model seems to deliverstate-of-the-art performance on large-scale tasks, itdoes so at a very high computational cost.
More-over, for lack of an internal modeling of reorderingprocesses, this approach, like the bag-of-word ap-11For instance, the experiments reported in (Blunsom and Os-borne, 2008) use the English-Chinese BTEC, where the averagesentence length is lesser than 10.proach, seems only appropriate for language pairswith similar or related word ordering.The approach developed in this paper fills a gapbetween the hierarchical model of (Blunsom etal., 2008) and the phrase-based model (Ka?a?ria?inen,2009), with whom we share several important as-sumptions, such as the use of alignment informationto provide supervision, and the resort to a an ?ex-ternal?, albeit a more powerful, reordering compo-nent.
Using a finite-state model enables to processreasonably large corpora, and gives some hopes as tothe scalability of the whole enterprise; it also makesthe integration of a target side language model mucheasier than in hierarchical models.6 Discussion and future workIn this paper, we have given detailed description ofan original phrase-based system implementing a dis-criminative version of the n-gram model, where thetranslation model probabilities are computed withconditional random fields.
We have showed howto implement this approach using a memory effi-cient implementation of the optimization algorithmsneeded for training: in our approach, training a mid-scale translation system with hundred of thousandssentence pairs and millions of features only takes acouple of hours on a standalone desktop machine.Using `1 regularization has enabled to assess theusefulness of various families of features.We have also detailed a complete decoder im-plemented as a pipeline of finite-state transducers,which allows to efficiently combine several models,to produce n-best lists and word lattices.The results obtained in a series of preliminary ex-periments show that our system is already deliver-ing competitive translations, as acknowledged by acomparison with two strong phrase-based baselines.We have already started to implement various opti-mizations and to experiment with somewhat largerdatasets (up to 500K sentence pairs) and larger fea-ture sets, notably incorporating word sense disam-biguation features: this work needs to be contin-ued.
In addition, we intend to explore a numberof extensions of this architecture, such as imple-menting MBR decoding (Kumar and Byrne, 2004)or adapting the translation model to new domainsand conditions, using, for instance, the proposal of550(Daume III, 2007)12.One positive side effect of experimenting withnew translation models is that they help reevalu-ate the performance of the whole translation systempipeline: in particular, discriminative training seemsto be more sensible to alignments errors than the cor-responding n-gram system, which suggests to paymore attention to possible errors in the training data;we have also seen that the current reordering modeldefines a too narrow search space and delivers in-sufficiently discriminant scores: we will investigatevarious ways to further improve the computation andscoring of hypothetical source reorderings.AcknowledgementsThe authors wish to thank the reviewers for com-ments and suggestions.
This work was achieved aspart of the Quaero Programme, funded by OSEO,French State agency for innovation.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A general and efficient weighted finite-state trans-ducer library.
In Proceedings of the Ninth Interna-tional Conference on Implementation and Applicationof Automata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Go?khan Bakir, Thomas Hofmann, Bernhard Scho?lkopf,Alexander J.Smola, Ben Taskar, and S.V.N.
Vish-wanathan.
2007.
Predicting structured output.
MITPress.Srinivas Bangalore, Patrick Haffner, and Stephan Kan-thak.
2007.
Statistical machine translation throughglobal lexical selection and sentence reconstruction.In Proceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 152?159,Prague, Czech Republic.Phil Blunsom and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 215?223, Honolulu,Hawaii.12In a nutshell, this proposal amounts to having three dif-ferent parameters for each feature; one parameter is trainedas usual; the other two parameters are updated conditionally,depending whether the training instance comes from the in-domain or from the out-domain training dataset.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of ACL-08: HLT,pages 200?208, Columbus, Ohio.Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-cent J. Della Pietra, Frederick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 224?233, Honolulu, Hawaii.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001 newfeatures for statistical machine translation.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages218?226.
Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270, AnnArbor, Michigan.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 531?540, Ann Arbor, Michigan.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Josep M. Crego and Jose?
B. Marin?o.
2007.
ImprovingSMT by coupling reordering and decoding.
MachineTranslation, 20(3):199?215.Hal Daume III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 256?263, Prague, Czech Republic.
Association for Compu-tational Linguistics.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of ACL-08: HLT, Short Papers, pages 25?28, Columbus, Ohio.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperform551surface heuristics.
In Proceedings of the ACL work-shop on Statistical Machine Translation, pages 31?38,New York City, NY.Yonggang Deng and William Byrne.
2006.
MTTK: Analignment toolkit for statistical machine translation.
InProceedings of the Human Language Technology Con-ference of the NAACL, Companion Volume: Demon-strations, pages 265?268, New York City, USA.Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 858?866, Los Angeles,California.
Association for Computational Linguistics.Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008.Better alignments = better translations ?
In Pro-ceedings of ACL-08: HLT, pages 986?993, Columbus,Ohio.Qin Gao and Stephan Vogel.
2008.
Parallel implementa-tions of word alignment tool.
In SETQA-NLP ?08.Kevin Gimpel and Noah A. Smith.
2008.
Rich source-side context for statistical machine translation.
In Pro-ceedings of the Third Workshop on Statistical MachineTranslation, pages 9?17, Columbus, Ohio, June.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic.Matti Ka?a?ria?inen.
2009.
Sinuhe ?
statistical machinetranslation using a globally trained conditional expo-nential family translation model.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 1027?1036, Singapore.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistic, pages 127?133, Edmond-ton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.Annual Meeting of the Association for ComputationalLinguistics (ACL), demonstration session, pages 177?180, Prague, Czech Republic.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Daniel Marcu Susan Dumais and Salim Roukos,editors, HLT-NAACL 2004: Main Proceedings, pages169?176, Boston, Massachusetts, USA.
Associationfor Computational Linguistics.Shankar Kumar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer transla-tion template model for statistical machine translation.Natural Language Engineering, 12(1):35?75.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of the International Conference on Ma-chine Learning, pages 282?289.
Morgan Kaufmann,San Francisco, CA.Thomas Lavergne, Olivier Capp, and Franois Yvon.2010.
Practical very large scale crfs.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 504?513, Uppsala,Sweden.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006a.
An end-to-end discriminative ap-proach to machine translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 761?768, Syd-ney, Australia.Percy Liang, Ben Taskar, and Dan Klein.
2006b.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA.Daniel Marcu and Daniel Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In Proceedings of the 2002 Conference onEmpirical Methods in Natural Language Processing,pages 133?139.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrick Lambert, Jose?
A.R.
Fonollosa, andMarta R. Costa-Jussa`.
2006.
N-gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending statistical machine translation with discrimi-native and trigger-based lexicon models.
In Proceed-ings of the 2009 Conference on Empirical Methods inNatural Language Processing, pages 210?218, Singa-pore.Mehryar Mohri.
2009.
Weighted automata algorithms.In Manfred Droste, Werner Kuich, and Heiko Vogler,editors, Handbook of Weighted Automata, chapter 6,pages 213?254.
Springer Verlag.Andrew Y. Ng.
2004.
Feature selection, l1 vs. l2 regular-ization, and rotational invariance.
In Proceedings ofthe twenty-first international conference on Machinelearning, pages 78?86.Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,552Libin Shen, David Smith, Katherine Eng, Viren Jain,Zhen Jin, and Dragomir Radev.
2004.
A smorgasbordof features for statistical machine translation.
In HLT-NAACL 2004: Main Proceedings, pages 161?168,Boston, Massachusetts, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318.Martin Riedmiller and Heinrich Braun.
1993.
A directadaptive method for faster backpropagation learning:The RPROP algorithm.
In Proceedings of the IEEEInternational Conference on Neural Networks, pages586?591, San Francisco, USA.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning, Cam-bridge, MA.
The MIT Press.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
J.R.Statist.Soc.B, 58(1):267?288.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Susan Du-mais, Daniel Marcu, and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 101?104, Boston,Massachusetts, USA.J.
Turian, B. Wellington, and I.D.
Melamed.
2007.
Scal-able discriminative learning for natural language pars-ing and translation.
In Proc.
Neural Information Pro-cessing Systems (NIPS), volume 19, pages 1409?1417.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic.Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training phrase translation models with leaving-one-out.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages475?484, Uppsala, Sweden.Fei Xia and Michael McCord.
2004.
Improving a statis-tical mt system with automatically learned rewrite pat-terns.
In Proceedings of the 20th International Confer-ence on Computational Linguistics (COLING), pages508?514, Geneva, Switzerland.553
