Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369?378,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsConcept-to-text Generation via Discriminative RerankingIoannis Konstas and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABi.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractThis paper proposes a data-driven methodfor concept-to-text generation, the task ofautomatically producing textual output fromnon-linguistic input.
A key insight in our ap-proach is to reduce the tasks of content se-lection (?what to say?)
and surface realization(?how to say?)
into a common parsing prob-lem.
We define a probabilistic context-freegrammar that describes the structure of the in-put (a corpus of database records and text de-scribing some of them) and represent it com-pactly as a weighted hypergraph.
The hyper-graph structure encodes exponentially manyderivations, which we rerank discriminativelyusing local and global features.
We propose anovel decoding algorithm for finding the bestscoring derivation and generating in this set-ting.
Experimental evaluation on the ATIS do-main shows that our model outperforms acompetitive discriminative system both usingBLEU and in a judgment elicitation study.1 IntroductionConcept-to-text generation broadly refers to thetask of automatically producing textual output fromnon-linguistic input such as databases of records,logical form, and expert system knowledge bases(Reiter and Dale, 2000).
A variety of concept-to-text generation systems have been engineered overthe years, with considerable success (e.g., Dale etal.
(2003), Reiter et al (2005), Green (2006), Turneret al (2009)).
Unfortunately, it is often difficultto adapt them across different domains as they relymostly on handcrafted components.In this paper we present a data-driven ap-proach to concept-to-text generation that is domain-independent, conceptually simple, and flexible.
Ourgenerator learns from a set of database records andtextual descriptions (for some of them).
An exam-ple from the air travel domain is shown in Figure 1.Here, the records provide a structured representationof the flight details (e.g., departure and arrival time,location), and the text renders some of this infor-mation in natural language.
Given such input, ourmodel determines which records to talk about (con-tent selection) and which words to use for describingthem (surface realization).
Rather than breaking upthe generation process into a sequence of local deci-sions, we perform both tasks jointly.
A key insightin our approach is to reduce content selection andsurface realization into a common parsing problem.Specifically, we define a probabilistic context-freegrammar (PCFG) that captures the structure of thedatabase and its correspondence to natural language.This grammar represents multiple derivations whichwe encode compactly using a weighted hypergraph(or packed forest), a data structure that defines aweight for each tree.Following a generative approach, we could firstlearn the weights of the PCFG by maximising thejoint likelihood of the model and then perform gen-eration by finding the best derivation tree in the hy-pergraph.
The performance of this baseline systemcould be potentially further improved using discrim-inative reranking (Collins, 2000).
Typically, thismethod first creates a list of n-best candidates froma generative model, and then reranks them with arbi-trary features (both local and global) that are eithernot computable or intractable to compute within the369Database:Flightfrom todenver bostonDay Numbernumber dep/ar9 departureMonthmonth dep/araugust departureConditionarg1 arg2 typearrival time 1600 <Searchtype whatquery flight??expression:Text:?x.
f light(x)?
f rom(x,denver)?
to(x,boston)?day number(x,9)?month(x,august)?less than(arrival time(x),1600)Give me the flights leaving Denver August ninth coming back to Boston before 4pm.Figure 1: Example of non-linguistic input as a structured database and logical form and its corresponding text.
Weomit record fields that have no value, for the sake of brevity.baseline system.An appealing alternative is to rerank the hyper-graph directly (Huang, 2008).
As it compactly en-codes exponentially many derivations, we can ex-plore a much larger hypothesis space than wouldhave been possible with an n-best list.
Importantly,in this framework non-local features are computedat all internal hypergraph nodes, allowing the de-coder to take advantage of them continuously at allstages of the generation process.
We incorporatefeatures that are local with respect to a span of asub-derivation in the packed forest; we also (approx-imately) include features that arbitrarily exceed spanboundaries, thus capturing more global knowledge.Experimental results on the ATIS domain (Dahl etal., 1994) demonstrate that our model outperformsa baseline based on the best derivation and a state-of-the-art discriminative system (Angeli et al, 2010)by a wide margin.Our contributions in this paper are threefold: werecast concept-to-text generation in a probabilisticparsing framework that allows to jointly optimizecontent selection and surface realization; we repre-sent parse derivations compactly using hypergraphsand illustrate the use of an algorithm for generating(rather than parsing) in this framework; finally, theapplication of discriminative reranking to concept-to-text generation is novel to our knowledge and asour experiments show beneficial.2 Related WorkEarly discriminative approaches to text generationwere introduced in spoken dialogue systems, andusually tackled content selection and surface re-alization separately.
Ratnaparkhi (2002) concep-tualized surface realization (from a fixed meaningrepresentation) as a classification task.
Local andnon-local information (e.g., word n-grams, long-range dependencies) was taken into account with theuse of features in a maximum entropy probabilitymodel.
More recently, Wong and Mooney (2007)describe an approach to surface realization based onsynchronous context-free grammars.
The latter arelearned using a log-linear model with minimum er-ror rate training (Och, 2003).Angeli et al (2010) were the first to propose aunified approach to content selection and surface re-alization.
Their model operates over automaticallyinduced alignments of words to database records(Liang et al, 2009) and decomposes into a sequenceof discriminative local decisions.
They first deter-mine which records in the database to talk about,then which fields of those records to mention, andfinally which words to use to describe the chosenfields.
Each of these decisions is implemented asa log-linear model with features learned from train-ing data.
Their surface realization component per-forms decisions based on templates that are automat-ically extracted and smoothed with domain-specificknowledge in order to guarantee fluent output.Discriminative reranking has been employed inmany NLP tasks such as syntactic parsing (Char-niak and Johnson, 2005; Huang, 2008), machinetranslation (Shen et al, 2004; Li and Khudanpur,2009) and semantic parsing (Ge and Mooney, 2006).Our model is closest to Huang (2008) who alsoperforms forest reranking on a hypergraph, usingboth local and non-local features, whose weightsare tuned with the averaged perceptron algorithm(Collins, 2002).
We adapt forest reranking to gen-eration and introduce several task-specific featuresthat boost performance.
Although conceptually re-lated to Angeli et al (2010), our model optimizescontent selection and surface realization simultane-ously, rather than as a sequence.
The discriminativeaspect of two models is also fundamentally different.We have a single reranking component that applies370throughout, whereas they train different discrimina-tive models for each local decision.3 Problem FormulationWe assume our generator takes as input a set ofdatabase records d and produces text w that verbal-izes some of these records.
Each record r ?
d has atype r.t and a set of fields f associated with it.
Fieldshave different values f .v and types f .t (i.e., integeror categorical).
For example, in Figure 1, flight is arecord type with fields from and to.
The values ofthese fields are denver and boston and their type iscategorical.During training, our algorithm is given a corpusconsisting of several scenarios, i.e., database recordspaired with texts like those shown in Figure 1.
Thedatabase (and accompanying texts) are next con-verted into a PCFG whose weights are learned fromtraining data.
PCFG derivations are represented asa weighted directed hypergraph (Gallo et al, 1993).The weights on the hyperarcs are defined by a vari-ety of feature functions, which we learn via a dis-criminative online update algorithm.
During test-ing, we are given a set of database records with-out the corresponding text.
Using the learned fea-ture weights, we compile a hypergraph specific tothis test input and decode it approximately (Huang,2008).
The hypergraph representation allows usto decompose the feature functions and computethem piecemeal at each hyperarc (or sub-derivation),rather than at the root node as in conventional n-bestlist reranking.
Note that the algorithm does not sep-arate content selection from surface realization, bothsubtasks are optimized jointly through the proba-bilistic parsing formulation.3.1 Grammar DefinitionWe capture the structure of the database with a num-ber of CFG rewrite rules, in a similar way to howLiang et al (2009) define Markov chains in theirhierarchical model.
These rules are purely syn-tactic (describing the intuitive relationship betweenrecords, records and fields, fields and correspondingwords), and could apply to any database with sim-ilar structure irrespectively of the semantics of thedomain.Our grammar is defined in Table 1 (rules (1)?
(9)).Rule weights are governed by an underlying multi-nomial distribution and are shown in square brack-1.
S?
R(start) [Pr = 1]2.
R(ri.t)?
FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]3.
R(ri.t)?
FS(r j,start) [P(r j.t |ri.t) ??]4.
FS(r,r.
fi)?
F(r,r.
f j) FS(r,r.
f j) [P( f j | fi)]5.
FS(r,r.
fi)?
F(r,r.
f j) [P( f j | fi)]6.
F(r,r.
f )?W(r,r.
f ) F(r,r.
f ) [P(w |w?1,r,r.
f )]7.
F(r,r.
f )?W(r,r.
f ) [P(w |w?1,r,r.
f )]8.
W(r,r.
f )?
?
[P(?
|r,r.
f , f .t, f .v)]9.
W(r,r.
f )?
g( f .v)[P(g( f .v).mode |r,r.
f , f .t = int)]Table 1: Grammar rules and their weights shown insquare brackets.ets.
Non-terminal symbols are in capitals and de-note intermediate states; the terminal symbol ?corresponds to all words seen in the training set,and g( f .v) is a function for generating integer num-bers given the value of a field f .
All non-terminals,save the start symbol S, have one or more constraints(shown in parentheses), similar to number and gen-der agreement constraints in augmented syntacticrules.Rule (1) denotes the expansion from the startsymbol S to record R, which has the special starttype (hence the notation R(start)).
Rule (2) de-fines a chain between two consecutive records riand r j.
Here, FS(r j,start) represents the setof fields of the target r j, following the sourcerecord R(ri).
For example, the rule R(search1.t)?FS( f light1,start)R( f light1.t) can be interpreted asfollows.
Given that we have talked about search1,we will next talk about f light1 and thus emit itscorresponding fields.
R( f light1.t) is a non-terminalplace-holder for the continuation of the chain ofrecords, and start in FS is a special boundary fieldbetween consecutive records.
The weight of this ruleis the bigram probability of two records conditionedon their type, multiplied with a normalization fac-tor ?.
We have also defined a null record type i.e., arecord that has no fields and acts as a smoother forwords that may not correspond to a particular record.Rule (3) is simply an escape rule, so that the parsingprocess (on the record level) can finish.Rule (4) is the equivalent of rule (2) at the field371level, i.e., it describes the chaining of two con-secutive fields fi and f j. Non-terminal F(r,r.
f )refers to field f of record r. For example, the ruleFS( f light1, f rom) ?
F( f light1, to)FS( f light1, to),specifies that we should talk about the field to ofrecord f light1, after talking about the field f rom.Analogously to the record level, we have also in-cluded a special null field type for the emission ofwords that do not correspond to a specific recordfield.
Rule (6) defines the expansion of field F toa sequence of (binarized) words W, with a weightequal to the bigram probability of the current wordgiven the previous word, the current record, andfield.Rules (8) and (9) define the emission of words andinteger numbers from W, given a field type and itsvalue.
Rule (8) emits a single word from the vocabu-lary of the training set.
Its weight defines a multino-mial distribution over all seen words, for every valueof field f , given that the field type is categorical orthe special null field.
Rule (9) is identical but forfields whose type is integer.
Function g( f .v) gener-ates an integer number given the field value, usingeither of the following six ways (Liang et al, 2009):identical to the field value, rounding up or roundingdown to a multiple of 5, rounding off to the clos-est multiple of 5 and finally adding or subtractingsome unexplained noise.1 The weight is a multino-mial over the six generation function modes, giventhe record field f .The CFG in Table 1 will produce many deriva-tions for a given input (i.e., a set of database records)which we represent compactly using a hypergraph ora packed forest (Klein and Manning, 2001; Huang,2008).
Simplified examples of this representationare shown in Figure 2.3.2 Hypergraph RerankingFor our generation task, we are given a set ofdatabase records d, and our goal is to find the bestcorresponding text w. This corresponds to the bestgrammar derivation among a set of candidate deriva-tions represented implicitly in the hypergraph struc-ture.
As shown in Table 1, the mapping from d to wis unknown.
Therefore, all the intermediate multino-mial distributions, described in the previous section,define a hidden correspondence structure h, betweenrecords, fields, and their values.
We find the best1The noise is modeled as a geometric distribution.Algorithm 1: Averaged Structured PerceptronInput: Training scenarios: (di,w?,h+i )Ni=11 ??
02 for t?
1 .
.
.T do3 for i?
1 .
.
.N do4 (w?, h?)
= argmaxw,h?
??
(di,wi,hi)5 if (w?i ,h+i ) 6= (w?i, h?i) then6 ??
?+?
(di,w?i ,h+i )??
(di, w?i, h?i)7 return 1T ?Tt=11N ?Ni=1?itscoring derivation (w?, h?)
by maximizing over con-figurations of h:(w?, h?)
= argmaxw,h?
??
(d,w,h)We define the score of (w?, h?)
as the dot productbetween a high dimensional feature representation?= (?1, .
.
.
,?m) and a weight vector ?.We estimate the weights ?
using the averagedstructured perceptron algorithm (Collins, 2002),which is well known for its speed and good perfor-mance in similar large-parameter NLP tasks (Lianget al, 2006; Huang, 2008).
As shown in Algo-rithm 1, the perceptron makes several passes overthe training scenarios, and in each iteration it com-putes the best scoring (w?, h?)
among the candidatederivations, given the current weights ?.
In line 6,the algorithm updates ?
with the difference (if any)between the feature representations of the best scor-ing derivation (w?, h?)
and the the oracle derivation(w?,h+).
Here, w?
is the estimated text, w?
the gold-standard text, h?
is the estimated latent configurationof the model and h+ the oracle latent configuration.The final weight vector ?
is the average of weightvectors over T iterations and N scenarios.
This av-eraging procedure avoids overfitting and producesmore stable results (Collins, 2002).In the following, we first explain how we decodein this framework, i.e., find the best scoring deriva-tion (Section 3.3) and discuss our definition for theoracle derivation (w?,h+) (Section 3.4).
Our fea-tures are described in Section 4.2.3.3 Hypergraph DecodingFollowing Huang (2008), we also distinguish fea-tures into local, i.e., those that can be computedwithin the confines of a single hyperedge, and non-local, i.e., those that require the prior visit of nodesother than their antecedents.
For example, the372Alignment feature in Figure 2(a) is local, and thuscan be computed a priori, but the Word Trigramsis not; in Figure 2(b) words in parentheses are sub-generations created so far at each word node; theircombination gives rise to the trigrams serving asinput to the feature.
However, this combinationmay not take place at their immediate ancestors,since these may not be adjacent nodes in the hy-pergraph.
According to the grammar in Table 1,there is no direct hyperedge between nodes repre-senting words (W) and nodes representing the set offields these correspond to (FS); rather, W and FS areconnected implicitly via individual fields (F).
Note,that in order to estimate the trigram feature at theFS node, we need to carry word information in thederivations of its antecedents, as we go bottom-up.2Given these two types of features, we can thenadapt Huang?s (2008) approximate decoding algo-rithm to find (w?, h?).
Essentially, we perform bottom-up Viterbi search, visiting the nodes in reverse topo-logical order, and keeping the k-best derivations foreach.
The score of each derivation is a linear com-bination of local and non-local features weights.
Inmachine translation, a decoder that implements for-est rescoring (Huang and Chiang, 2007) uses the lan-guage model as an external criterion of the good-ness of sub-translations on account of their gram-maticality.
Analogously here, non-local features in-fluence the selection of the best combinations, byintroducing knowledge that exceeds the confines ofthe node under consideration and thus depend onthe sub-derivations generated so far.
(e.g., word tri-grams spanning a field node rely on evidence fromantecedent nodes that may be arbitrarily deeper thanthe field?s immediate children).Our treatment of leaf nodes (see rules (8) and (9))differs from the way these are usually handled inparsing.
Since in generation we must emit ratherthan observe the words, for each leaf node we there-fore output the k-best words according to the learnedweights ?
of the Alignment feature (see Sec-tion 4.2), and continue building our sub-generationsbottom-up.
This generation task is far from triv-ial: the search space on the word level is the size ofthe vocabulary and each field of a record can poten-tially generate all words.
Also, note that in decodingit is useful to have a way to score different output2We also store field information to compute structural fea-tures, described in Section 4.2.lengths |w|.
Rather than setting w to a fixed length,we rely on a linear regression predictor that uses thecounts of each record type per scenario as featuresand is able to produce variable length texts.3.4 Oracle DerivationSo far we have remained agnostic with respect tothe oracle derivation (w?,h+).
In other NLP taskssuch as syntactic parsing, there is a gold-standardparse, that can be used as the oracle.
In our gener-ation setting, such information is not available.
Wedo not have the gold-standard alignment between thedatabase records and the text that verbalizes them.Instead, we approximate it using the existing de-coder to find the best latent configuration h+ giventhe observed words in the training text w?.3 This issimilar in spirit to the generative alignment model ofLiang et al (2009).4 Experimental DesignIn this section we present our experimental setup forassessing the performance of our model.
We givedetails on our dataset, model parameters and fea-tures, the approaches used for comparison, and ex-plain how system output was evaluated.4.1 DatasetWe conducted our experiments on the Air Travel In-formation System (ATIS) dataset (Dahl et al, 1994)which consists of transcriptions of spontaneous ut-terances of users interacting with a hypothetical on-line flight booking system.
The dataset was orig-inally created for the development of spoken lan-guage systems and is partitioned in individual userturns (e.g., flights from orlando to milwaukee, showflights from orlando to milwaukee leaving after sixo?clock) each accompanied with an SQL query to abooking system and the results of this query.
Theseutterances are typically short expressing a specificcommunicative goal (e.g., a question about the ori-gin of a flight or its time of arrival).
This inevitablyresults in small scenarios with a few words that of-ten unambiguously correspond to a single record.
Toavoid training our model on a somewhat trivial cor-pus, we used the dataset introduced in Zettlemoyer3In machine translation, Huang (2008) provides a soft al-gorithm that finds the forest oracle, i.e., the parse among thereranked candidates with the highest Parseval F-score.
How-ever, it still relies on the gold-standard reference translation.373and Collins (2007) instead, which combines the ut-terances of a single user in one scenario and con-tains 5,426 scenarios in total; each scenario corre-sponds to a (manually annotated) formal meaningrepresentation (?-expression) and its translation innatural language.Lambda expressions were automatically con-verted into records, fields and values following theconventions adopted in Liang et al (2009).4 Givena lambda expression like the one shown in Figure 1,we first create a record for each variable and constant(e.g., x, 9, august).
We then assign record types ac-cording to the corresponding class types (e.g., vari-able x has class type flight).
Next, fields and val-ues are added from predicates with two argumentswith the class type of the first argument matchingthat of the record type.
The name of the predicatedenotes the field, and the second argument denotesthe value.
We also defined special record types, suchas condition and search.
The latter is introduced forevery lambda operator and assigned the categoricalfield what with the value flight which refers to therecord type of variable x.Contrary to datasets used in previous generationstudies (e.g., ROBOCUP (Chen and Mooney, 2008)and WEATHERGOV (Liang et al, 2009)), ATIS has amuch richer vocabulary (927 words); each scenariocorresponds to a single sentence (average lengthis 11.2 words) with 2.65 out of 19 record typesmentioned on average.
Following Zettlemoyer andCollins (2007), we trained on 4,962 scenarios andtested on ATIS NOV93 which contains 448 examples.4.2 FeaturesBroadly speaking, we defined two types of features,namely lexical and structural ones.
In addition,we used a generatively trained PCFG as a baselinefeature and an alignment feature based on the co-occurrence of records (or fields) with words.Baseline Feature This is the log score of a gen-erative decoder trained on the PCFG from Table 1.We converted the grammar into a hypergraph, andlearned its probability distributions using a dynamicprogram similar to the inside-outside algorithm (Liand Eisner, 2009).
Decoding was performed approx-4The resulting dataset and a technical report describ-ing the mapping procedure in detail are available fromhttp://homepages.inf.ed.ac.uk/s0793019/index.php?page=resourcesimately via cube pruning (Chiang, 2007), by inte-grating a trigram language model extracted from thetraining set (see Konstas and Lapata (2012) for de-tails).
Intuitively, the feature refers to the overallgoodness of a specific derivation, applied locally inevery hyperedge.Alignment Features Instances of this feature fam-ily refer to the count of each PCFG rule from Ta-ble 1.
For example, the number of times ruleR(search1.t)?
FS( f light1,start)R( f light1.t) is in-cluded in a derivation (see Figure 2(a))Lexical Features These features encourage gram-matical coherence and inform lexical selection overand above the limited horizon of the language modelcaptured by Rules (6)?(9).
They also tackle anoma-lies in the generated output, due to the ergodicity ofthe CFG rules at the record and field level:Word Bigrams/Trigrams This is a group ofnon-local feature functions that count word n-gramsat every level in the hypergraph (see Figure 2(b)).The integration of words in the sub-derivations isadapted from Chiang (2007).Number of Words per Field This feature functioncounts the number of words for every field, aimingto capture compound proper nouns and multi-wordexpressions, e.g., fields from and to frequently corre-spond to two or three words such as ?new york?
and?salt lake city?
(see Figure 2(d)).Consecutive Word/Bigram/Trigram This featurefamily targets adjacent repetitions of the same word,bigram or trigram, e.g., ?show me the show me theflights?.Structural Features Features in this category tar-get primarily content selection and influence appro-priate choice at the field level:Field bigrams/trigrams Analogously to the lexicalfeatures mentioned above, we introduce a series ofnon-local features that capture field n-grams, givena specific record.
For example the record flight in theair travel domain typically has the values <from to>(see Figure 2(c)).
The integration of fields in sub-derivations is implemented in fashion similar to theintegration of words.Number of Fields per Record This feature familyis a coarser version of the Field bigrams/trigrams374R(search1.t)FS(flight1.t,start) R(flight1.t)FS0,3(search1.t,start)w0(search1.t,type) ?
?
?
w1,2(search1.t,what)???showmewhat?
?
??????
?me theme f lightsthe f lights?
?
???
?FS2,6(flight1.t,start)F2,4(flight1.t,from) FS4,6(flight1.t,from)F4,6(flight1.t,to) ?| 2 words |(b)Word Trigrams (non-local)<show me the>, <show me flights>, etc.
(a)Alignment Features (local)<R(srch1.t)?
FS(fl1.t,st) R(fl1.t)>(c)Field Bigrams (non-local)<from to> | flight(d)Number of Words per Field (local)<2 | from>Figure 2: Simplified hypergraph examples with corresponding local and non-local features.feature, which is deemed to be sparse for rarely-seenrecords.Field with No Value Although records in the ATISdatabase schema have many fields, only a few areassigned a value in any given scenario.
For exam-ple, the flight record has 13 fields, of which only 1.7(on average) have a value.
Practically, in a genera-tive model this kind of sparsity would result in verylow field recall.
We thus include an identity featurefunction that explicitly counts whether a particularfield has a value.4.3 EvaluationWe evaluated three configurations of ourmodel.
A system that only uses the top scor-ing derivation in each sub-generation and in-corporates only the baseline and alignmentfeatures (1-BEST+BASE+ALIGN).
Our sec-ond system considers the k-best derivationsand additionally includes lexical features(k-BEST+BASE+ALIGN+LEX).
The number ofk-best derivations was set to 40 and estimatedexperimentally on held-out data.
And finally,our third system includes the full feature set(k-BEST+BASE+ALIGN+LEX+STR).
Note, thatthe second and third system incorporate non-localfeatures, hence the use of k-best derivation lists.5We compared our model to Angeli et al (2010)whose approach is closest to ours.6We evaluated system output automatically, usingthe BLEU-4 modified precision score (Papineni et5Since the addition of these features, essentially incursreranking, it follows that the systems would exhibit the exactsame performance as the baseline system with 1-best lists.6We are grateful to Gabor Angeli for providing us with thecode of his system.al., 2002) with the human-written text as reference.We also report results with the METEOR score(Banerjee and Lavie, 2005), which takes into ac-count word re-ordering and has been shown to cor-relate better with human judgments at the sentencelevel.
In addition, we evaluated the generated text byeliciting human judgments.
Participants were pre-sented with a scenario and its corresponding verbal-ization (see Figure 3) and were asked to rate the lat-ter along two dimensions: fluency (is the text gram-matical and overall understandable?)
and semanticcorrectness (does the meaning conveyed by the textcorrespond to the database input?).
The subjectsused a five point rating scale where a high numberindicates better performance.
We randomly selected12 documents from the test set and generated out-put with two of our models (1-BEST+BASE+ALIGNand k-BEST+BASE+ALIGN+LEX+STR) and Angeliet al?s (2010) model.
We also included the originaltext (HUMAN) as a gold standard.
We thus obtainedratings for 48 (12?
4) scenario-text pairs.
The studywas conducted over the Internet, using Amazon Me-chanical Turk, and was completed by 51 volunteers,all self reported native English speakers.5 ResultsTable 2 summarizes our results.
As can be seen, in-clusion of lexical features gives our decoder an ab-solute increase of 6.73% in BLEU over the 1-BESTsystem.
It also outperforms the discriminative sys-tem of Angeli et al (2010).
Our lexical featuresseem more robust compared to their templates.
Thisis especially the case with infrequent records, wheretheir system struggles to learn any meaningful infor-mation.
Addition of the structural features furtherboosts performance.
Our model increases by 8.69%375System BLEU METEOR1-BEST+BASE+ALIGN 21.93 34.01k-BEST+BASE+ALIGN+LEX 28.66 45.18k-BEST+BASE+ALIGN+LEX+STR 30.62 46.07ANGELI 26.77 42.41Table 2: BLEU-4 and METEOR results on ATIS.over the 1-BEST system and 3.85% over ANGELI interms of BLEU.
We observe a similar trend whenevaluating system output with METEOR.
Differ-ences in magnitude are larger with the latter metric.The results of our human evaluation study areshown in Table 5.
We carried out an Analysis ofVariance (ANOVA) to examine the effect of systemtype (1-BEST, k-BEST, ANGELI, and HUMAN) onthe fluency and semantic correctness ratings.
Meansdifferences were compared using a post-hoc Tukeytest.
The k-BEST system is significantly better thanthe 1-BEST and ANGELI (a < 0.01) both in termsof fluency and semantic correctness.
ANGELI issignificantly better than 1-BEST with regard to flu-ency (a < 0.05) but not semantic correctness.
Thereis no statistically significant difference between thek-BEST output and the original sentences (HUMAN).Examples of system output are shown in Table 3.They broadly convey similar meaning with the gold-standard; ANGELI exhibits some long-range repeti-tion, probably due to re-iteration of the same recordpatterns.
We tackle this issue with the inclusion ofnon-local structural features.
The 1-BEST systemhas some grammaticality issues, which we avoid bydefining features over lexical n-grams and repeatedwords.
It is worth noting that both our system andANGELI produce output that is semantically com-patible with but lexically different from the gold-standard (compare please list the flights and showme the flights against give me the flights).
This isexpected given the size of the vocabulary, but raisesconcerns regarding the use of automatic metrics forthe evaluation of generation output.6 ConclusionsWe presented a discriminative reranking frameworkfor an end-to-end generation system that performsboth content selection and surface realization.
Cen-tral to our approach is the encoding of generationas a parsing problem.
We reformulate the input (aset of database records and text describing some ofSystem FluencySemCor1-BEST+BASE+ALIGN 2.70 3.05k-BEST+BASE+ALIGN+LEX+STR 4.02 4.04ANGELI 3.74 3.17HUMAN 4.18 4.02Table 3: Mean ratings for fluency and semantic correct-ness (SemCor) on system output elicited by humans.Flightfrom tophoenix milwaukeeTimewhen dep/arevening departureDayday dep/arwednesday departureSearchtype whatquery flightHUMANANGELIk-BEST1-BESTgive me the flights from phoenix to milwaukee onwednesday eveningshow me the flights from phoenix to milwaukee onwednesday evening flights from phoenix to milwaukeeplease list the flights from phoenix to milwaukee onwednesday eveningon wednesday evening from from phoenix tomilwaukee on wednesday eveningFigure 3: Example of scenario input and system output.them) as a PCFG and convert it to a hypergraph.
Wefind the best scoring derivation via forest rerankingusing both local and non-local features, that we trainusing the perceptron algorithm.
Experimental eval-uation on the ATIS dataset shows that our model at-tains significantly higher fluency and semantic cor-rectness than any of the comparison systems.
Thecurrent model can be easily extended to incorporate,additional, more elaborate features.
Likewise, it canport to other domains with similar database struc-ture without modification, such as WEATHERGOVand ROBOCUP.
Finally, distributed training strate-gies have been developed for the perceptron algo-rithm (McDonald et al, 2010), which would allowour generator to scale to even larger datasets.In the future, we would also like to tackle morechallenging domains (e.g., product descriptions) andto enrich our generator with some notion of dis-course planning.
An interesting question is how toextend the PCFG-based approach advocated here soas to capture discourse-level document structure.376ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 502?512, Cambridge, MA.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, Michigan.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages173?180, Ann Arbor, Michigan, June.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language acqui-sition.
In Proceedings of International Conference onMachine Learning, pages 128?135, Helsinki, Finland.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of the 17th In-ternational Conference on Machine Learning, pages175?182, Stanford, California.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing, pages 1?8, Philadelphia, Penn-sylvania.Deborah A. Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the ATIStask: the ATIS-3 corpus.
In Proceedings of the Work-shop on Human Language Technology, pages 43?48,Plainsboro, New Jersey.Robert Dale, Sabine Geldof, and Jean-Philippe Prost.2003.
Coral: Using natural language generation fornavigational assistance.
In Proceedings of the 26thAustralasian Computer Science Conference, pages35?44, Adelaide, Australia.Giorgio Gallo, Giustino Longo, Stefano Pallottino, andSang Nguyen.
1993.
Directed hypergraphs and appli-cations.
Discrete Applied Mathematics, 42:177?201.Ruifang Ge and Raymond J. Mooney.
2006.
Discrimina-tive reranking for semantic parsing.
In Proceedings ofthe COLING/ACL 2006 Main Conference Poster Ses-sions, pages 263?270, Sydney, Australia.Nancy Green.
2006.
Generation of biomedical argu-ments for lay readers.
In Proceedings of the 5th In-ternational Natural Language Generation Conference,pages 114?121, Sydney, Australia.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594, Columbus, Ohio.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of the 7th Interna-tional Workshop on Parsing Technologies, pages 123?134, Beijing, China.Ioannis Konstas and Mirella Lapata.
2012.
Unsuper-vised concept-to-text generation with hypergraphs.
Toappear in Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, Montre?al, Canada.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 40?51, Suntec, Sin-gapore.Zhifei Li and Sanjeev Khudanpur.
2009.
Forest rerank-ing for machine translation with the perceptron algo-rithm.
In GALE Book.
GALE.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and the 44th Annual Meeting of the Associ-ation for Computational Linguistics, pages 761?768,Sydney, Australia.Percy Liang, Michael Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 91?99, Suntec, Singapore.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 456?464, Los Angeles, CA, June.
Associationfor Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings of377the 41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia.Adwait Ratnaparkhi.
2002.
Trainable approaches to sur-face natural language generation and their applicationto conversational dialog systems.
Computer Speech &Language, 16(3-4):435?455.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge UniversityPress, New York, NY.Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,and Ian Davy.
2005.
Choosing words in computer-generated weather forecasts.
Artificial Intelligence,167:137?169.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InHLT-NAACL 2004: Main Proceedings, pages 177?184, Boston, Massachusetts.Ross Turner, Yaji Sripada, and Ehud Reiter.
2009.
Gen-erating approximate geographic descriptions.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation, pages 42?49, Athens, Greece.Yuk Wah Wong and Raymond Mooney.
2007.
Gener-ation by inverting a semantic parser that uses statis-tical machine translation.
In Proceedings of the Hu-man Language Technology and the Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 172?179, Rochester, NY.Luke Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsing to log-ical form.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 678?687, Prague, CzechRepublic.378
