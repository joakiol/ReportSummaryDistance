SURVEY OF THE MESSAGE UNDERSTANDING CONFERENCESBeth M. SundheimNaval Command, Control & Ocean Surveillance Ctr.RDT&E Division (NRaD), Code 444San Diego, CA 92152-7420Nancy A. ChinchorScience Applications International Corporation10260 Campus Point Drive, M/S A2-FSan Diego, CA 92121ABSTRACTIn this paper, the Message Understanding Conferences arereviewed, and the natural anguage system evaluation that isunderway in preparation for the next conference is described.The role of the conferences in the evaluation of informationextraction systems is assessed in terms of the purposes ofthree broad classes of evaluation: progress, adequacy, anddiagnostic.
The conferences have measured systemperformance primarily to assess progress and the state of theart, but they have also been influenced by the concernsassociated with assessing adequacy and providingdiagnostics.
Challenges for the future of similar evaluationsare also discussed.1.
INTRODUCTIONMuch has happened since the last time a paper appearedin the ARPA workshop roceedings about the MessageUnderstanding Conferences \[11\].
The evaluationmethodology has been changing steadily, and moredemanding information extraction tasks have beendefined.
In response to the challenges of the evaluationtask and metrics, researchers have developed robust andefficient methods for working with large corpora nd haveconfronted prevalent text analysis issues that have so farconswained performance.These challenges have also resulted in a criticalrethinking of assumptions concerning the ideal system tosubmit for evaluation.
Is it a "generic" natural languagesystem with in-depth analysis capabilities and a well-defined internal representation language designed toaccomodate the translation of various kinds of textualinput into various kinds of output?
Or is it one that usesonly shallow processing techniques and does not presumeto be suitable for language processing tasks other thaninformation extraction?2.
REV IEW OF PAST MUCsThe first Message Understanding Conference (MUC) washeld in 1987, used ten narrative paragraphs from navalmessages as a training corpus and two others as test data,and had no defined evaluation task or metrics.Researchers from six organizations ran their systems onthe test data during the conference, then demonstrated andexplained how the systems analyzed the texts.
Two yearslater, the second MUC was held \[10\].
It made use of atraining corpus of 105 naval message narratives of fourdifferent ypes, a dry-run test set of 20 narratives, and afinal test set of five.
An information extraction task wasdefined that consisted of identifying ten different pieces ofinformation and representing them as slot fillers in atemplate resembling a semantic frame.
This taskemulates an information management applicationrequiring the culling of facts from a large body of freetext as a means to generate updates to a formatteddatabase.A rudimentary set of scoring standards was developed, andthe templates produced by the eight systems (includingfour of the six systems represented at the 1987evaluation) were scored by hand by comparison with ahand-generated answer key.
The nature of the corpus usedfor the second MUC was difficult enough that grammarcoverage and parsing efficiency were serious issues.
Thedomain was complex enough that the knowledgeengineering job was greatly facilitated by the availabilityof documentation presenting much of the essential,declarative domain knowledge in a structured format.After another two-year interval, MUC-3 was held in May,1991, followed by MUC-4 in June, 1992.
There arepublished proceedings for the third and fourth conferences\[8, 9\], including descriptions and test results of theparticipating systems (15 for MUC-3, 17 for MUC-4).A new corpus of 1,400 texts on the subject of LatinAmerican terrorism was used that includes 16 text types(transcribed speeches, newspaper a ticles, editorial reports,etc.).
The template developed for MUC-3 contained slotsfor 17 pieces of information; the number of information-bearing slots increased to 22 for MUC-4.
The scoringmetrics were refined and implemented for MUC-3 andMUC-4 in a semiautomated scoring program.For MUC-3, a study was carried out to measure thecomplexity of the MUC-3 terrorism task vis-a-vis thenaval task, and the scores obtained in the 1989 evaluationwere recomputed using the MUC-3 method of scoring\[5\].
Although these scores were lower, the conclusionwas that significant progress had been made, because theincrease in difficulty in the task more than offset thedecrease in scores.56It was possible to conduct a more refined study of theprogress from MUC-3 to MUC-4 \[12\] that showed thathigher levels of performance by nearly all veteransystems were achieved espite the relative difficulty of theMUC-4 test set that was used in the comparison anddespite increased strictness of the scoring with respect tospurious data generation.
The results of MUC-4 showthat higher recall is usually correlated with higherprecision 1, which is consistent with the results ofprevious evaluations and suggests that there is still avariety of techniques with potential for attaining evenhigher levels of performance in the future.
In absoluteterms, however, ecall and precision scores were still onlymoderate.According to an analysis of the effectiveness oftechniques used by MUC-3 systems \[4\], pattern-matchingtechniques (with hand-crafted or automatically acquiredpatterns) and probabilistic text categorzafion techniquesproved successful only when combined with linguistictechniques.
The use of robust processing includingrobust parsing was shown to correlate with the success ofthe system.
In a comparison of MUC-3 and MUC-4systems, minimal improvement from MUC-3 to MUC-4was demonstrated by the two systems that did not usel inguistically-based processing \[12\].
Severall inguistically-based MUC-3 systems improvedconsiderably via extensions made for MUC-4, as did oneMUC-3 system that was converted from a generic textunderstanding system to an information extraction systemthat maintains its basis in linguistics but is streamlinedfor speed and geared specifically to the demands ofinformation extraction.
However, other systems whichunderwent a complete overhaul for MUC-4 showed onlyslight progress or even a degradation i  performance.Error analyses point to the critical need for furtherresearch in areas uch as discourse reference r solution andinferencing.
For example, the inability to reliablydetermine whether a description found in one part of thetext refers or does not refer to something previouslydescribed inhibits both recall and precision because itcould result in the system either missing information orgenerating spurious information; the inability to pick upsubtle relevance indications (e.g., that persons describedas being "in" a place that was attacked could be targets ofthe attack) and not-so-subtle ones (e.g., that a vehiclewhose roof collapsed as a result of a bomb explosion wasdamaged by the explosion) places a limitation on recallbecause it results in missed information.
The ability to1 Recall is the ratio of correctly generated fills to the total number ofexpected fills; precision is the ratio of the correctly generated frdlsto the total number of generated fills.
Thus, shortfalls in recallreflect the amount of missing fills as well as incorrect fills, andshortfalls in precision reflect the amount of spurious fills as well asincorrect fills.
See \[2\] for detailed information on the formulation ofthese and other metrics, which are under review for MUC-5.take advantage of sophisticated approaches to discoursethat have already received computational treatment islimited by a dependence onerror-free outputs from earlierstages of processing.
Thus, there is a need for renewedattention to robust processing at the sentence l vel.3.
MUC-5We are in another one-year cycle this year, with MUC-5scheduled for August, 1993.
Over 20 organizations arecurrently planning to participate in the evaluation.Among the expected participants are the organizationsalready working on the Tipster Text extraction program,other MUC-4 veteran organizations, and six additionalparticipants, four of whom are from outside the UnitedStates.The final evaluation of the Tipster contractors' ystemswill be the MUC-5 evaluation.
There are four tasks, eachwith its own corpus: joint ventures in English and inJapanese and microelectronics n English and in Japanese.The Tipster-sponsored organizations will be evaluated onall tasks that they are contracted to work on; other MUC-5 participants are allowed to work on both languages ifthey want to but have been required to choose betweenthe two domains to keep them from spreading theirefforts too thin.The joint ventures task (in both languages) appears topose significantly greater challenges than themicroelectronics task, largely because the joint venturesarticles are less technical and more varied in style, aregenerally longer, and often discuss more than one jointventure.
The template includes over 40 content-bearingslots identifying and interrelating various facts about hejoint venture and the entities involved.
Themicroelectronics template has fewer slots; it coversfeatures of microchip fabrication processes and theorganizations mentioned in association with thoseprocesses.4.
ROLES IN  EVALUATIONThree broad types or purposes of evaluation have beenidentified and described by H. Thompson and M. King2:progress evaluation, adequacy evaluation, and diagnosticevaluation.
The MUC evaluations have been primarilyexamples of progress evaluation, which is defined as"assessing the actual state of a system with respect osome desired state of the same system, as when progressof a project owards ome goal is assessed."
However,2 These were outlined by Henry Thompson (University ofEdinburgh) at the Association for Machine Translation in theAmericas Evaluation Workshop in San Diego, CA, in November,1992, and further discussed in a subsequent personal communicationfrom Margaret King (ISSCO, Geneva).57the information extraction tasks that have been used forMUC are quite realistic in some respects, and there areways in which the evaluation metrics and scoringprocedures reflect the concern that the interests oftechnology consumers be accomodated to the extentpossible.
Their interest is in adequacy evaluation, whichis defined as "assessing the adequacy of a system withrespect to some intended use of that system, asexemplified by a potential customer investigatingwhether a system, either in its current state or aftermodification, will do what he requires, how well it willdo it and at what cost."
The third type, diagnosticevaluation, is defined as "assessing the state of a systemwith the intention of discovering where it fails and why,as exemplified by a research group examining their ownsystem."
There are ways in which the MUC evaluationspartially support this purpose as well, by providingquantitative data and by facilitating the collection ofqualitative data.4.1.
Progress EvaluationThere are at least three ways we look at progress: as anassessment of the current state of the art, as a measure ofprogress relative to the previous evaluation, and as ameasure of progress toward matching human performanceon the same task.
We expect the metrics to be applicableto both machines and humans, to provide a useful way tolook at how much of the expected ata the system isfinding and at the classes and numbers of errors it ismaking, and to offer a means for comparing performanceacross ystems.Using the metrics that have been developed so far, we cansay how systems are doing on particular informationextraction tasks with respect to correct, incorrect,spurious and missing data at various levels of granularity,and we can tell how a system's performance on the partsof the task that it tried to do compares to its performanceon the total task.
Repeated over time, the assessmentsmeasure progress of the systems as a group and asindividuals, although precise measurement has beencomplicated by the changes to the evaluationmethodology, task domain, and template design, and bythe radical system design changes made by some groups.Overall cross-system comparisons are possible given asingle-value metric \[2\] and statistical significance tests\[3\].
The most compelling research problems posed bythe task, e.g., suprasentential processing \[7\], aredramatically revealed.In the context of ARPA's Tipster program, humanperformance studies have been carried out with theanalysts who filled the answer-key templates.
One ofthese studies \[13\], which was conducted in the Englishjoint ventures domain, used 20 templates generatedindependently b four analysts and compared with a keyprepared by a fifth "expert" partly on the basis of theother four.
The results howed that the best performanceachieved was 82% recall and 84% precision, that a fairlysmall amount of variability existed between the two top-scoring humans, and that there was a sizable performancedifference between the top-scoring and the lowest-scoringhumans.An error analysis of these results showed that about halfof the approximately 20% total disagreement among theanalysts could be attributed to human error(misinterpretation, versight, data-entry error).
The restwas attributed to problems outside the human's control(gaps in template-filling guidelines, legitimate analyticaldifferences in text and guideline interpretation, and bugsin the template-fill ing tool).
Although humanperformance in this study is far from perfect, itnonetheless represents a challenging performanceobjective for computer systems.4.2.
Adequacy EvaluationAlthough the evaluation tasks emulate actual orhypothesized real-life tasks, they are unrealistic in certaincrucial respects, such as the complete autonomy of theextraction process.
Since the tasks are constrained inways such as this for the purposes of evaluation, it is notpossible to translate the evaluation results directly intoterms that reflect the specific requirements of anyparticular eal-life applications, even applications thatbear strong resemblances to the evaluation tasks.Nonetheless, we can consider the relevance of the MUCevaluation methodology to the problem of assessing theadequacy of systems and methods for real-life tasks.Decisions concerning choice of evaluation metrics havebeen motivated in part by an interest in establishing goodcommunications with technology consumers.
Ascommunications have improved, misconceptionsconcerning the presumed needs of technology consumersin terms of evaluation metrics have surfaced and are beingaddressed.
The result should be a small set of easily-understood metrics that provide insightful performancedata for consumers as well as producers.One example concerns the treatment of missing andspurious fills, which has been left as a variable so thattechnology consumers can decide to what extent hey areconcerned with absent or excess data in the database.However, it now appears that a strict and equal treatmentof both types of error is more meaningful to thetechnology consumers as well as to the technologyproducers.
Another example concerns the overall metricthat is computed primarily to enable systems to beranked.
The current metric was designed with thepresumed interests of technology consumers in mind, byincorporating variable weights for recall and precision and58by including a factor that rewards ystems for balancedperformance on those two measures.
However, there isstrong interest among some technology users and othersin replacing the current metric with the error rate (numberwrong divided by total possible).In addition to influencing the development of evaluationmetrics, the concerns of adequacy evaluation have affectedsome of the decisions programmed into the scoringsoftware.
All in all, the MUC evaluations have quiteconsciously responded to some of the presumed needs oftechnology consumers; it now appears that one of ourpriorities should be to eliminate some of theembellishments and complexities that have beenintroduced over the last few years.4.3.
Diagnostic EvaluationThe primary metrics of recall and precision and thesecondary ones of undergeneration a d overgenerationprovide diagnostic information in the sense that theyshow how accurate system performance is at the system'scurrent level of task coverage.
We rely on the evaluationparticipants for error analyses and qualitative assessmentsof their system's performance, using the metrics as onestarting point.
Attempts that have been made to use theinformation extraction task to reveal anguage analysiscapabilities directly have so far met with limited success.Although these attempts have stayed within the "black-box" information extraction evaluation paradigm byexamining only textual inputs in relation to template-filler outputs, they are diagnostic evaluations in the sensethat they seek to isolate specific aspects of text analysisfrom the information extraction task, making use of testsuites of examples elected from the overall extractiontask.One of the studies examined the results of informationextraction at the local level of processing (appositionhandling), and the other looked at the global level ofprocessing (discourse handling).
The former was carriedout for MUC-3 \[1\] and the latter for MUC-4 \[6\].
In bothstudies, there were conditions where the results conformedto expectations and conditions where they did not.
Bothstudies uffered from small test suites and a number ofuncontrolled variables.
Although there seems to be notheoretical impediment to conducting successful, fine-grained, task-oriented tests, these two efforts seem toshow that such tests cannot be designed as adjuncts to thebasic evaluation but rather require independentspecification i order to ensure adequate t st samples andan appropriately designed information extraction task.5.
CHALLENGES FOR THE FUTUREmeaningful and more directly usable by the variousinterested parties -- those doing the research anddevelopment, those watching, and those contemplatinguse.
To date, the results eem to have served those doingthe research and development well and the others not sowell.
Of benefit to all, however, have been thedevelopment of the shared tasks and the large prototypesystems, which have provided the basis for effectivecommunication.The pressures of the information extraction evaluationtasks and the pressures of the evaluations themselveshave resulted in increased attention to task-specificprocessing techniques.
These techniques are oftendesigned not only to improve the quantity and quality ofextracted information but also to shorten the developmentcycle and reduce the human effort associated with portingand extending the system.
At the extreme nd of thespectrum is a class of systems that exploit variousshallow processing techniques.
The performanceobjective of such systems is to at least come close to theestimated potential performance of an in-depthunderstanding system and to reach that level with muchless time and effort.
Thus, the contrasts in system designphilosophy and system architecture have grown, and thefoundation has been laid for an evaluation that couldreveal a lot about the near-term transition potential ofsome technologies and about he slrategies for addressingthe significant, longer-term research issues associatedwith the information extraction task.Although information extraction has served as anexcellent vehicle for elucidating the application potentialof current echnology, its utility as a vehicle for focusingattention on solving the hard, general problems of naturallanguage processing is not as great.
Many insights havebeen gained into the nature of natural language processingby experience in developing the large-scale systemsrequired to participate in the evaluation.
Nevertheless, somuch effort is involved simply to make it through theevaluation that it takes a disciplined effort to resistimplementing quick solutions to all the major issuesinvolved, whether they are well understood problems ornot.
This is especially true of the many MUCparticipants with severely limited resources, but it is alsotrue to some extent for those with more extensiveresources, who may feel the pressure of competition forhigh performance more keenly.
It is clearly of little useto anyone to ask a large number of research-orientedgroups to productize their systems and fine-tune them toa particular domain, just for the purposes of evaluation.The challenge to play a role in solving the hard naturallanguage processing problems is a challenge for theevaluators and participants alike.A major challenge for the immediate future of the MUCevaluations is to make the results more intuitively59ACKNOWLEDGEMENTSThe authors are especially indebted to the other membersof the MUC-5 program committee: Sean Boisen, LynnCarlson, Jim Cowie, Ralph Grishman, Jerry Hobbs, JoeMcCarthy, Mary Ellen Okurowski, Boyan Onyshkevych,and Carl Weir.
The authors' work is funded byARPA/SISTO under ARPA order 6359.R E F E R E N C E S. Chinchor, N., MUC-3 Linguistic Phenomena TestExperiment, in Proceedings of the Third MessageUnderstanding Conference (MUC-3), May, 1991,Morgan Kaufmann, pp.
31-45.. Chinchor, N., MUC-4 Evaluation Metrics, inProceedings ofthe Fourth Message UnderstandingConference (MUC-4), June, 1992, MorganKaufmann Publishers, pp.
22-29.. Chinchor, N., Statistical Significance of MUC-4Results, in Proceedings of the Fourth MessageUnderstanding Conference (MUC-4), June, 1992,Morgan Kaufmann Publishers, pp.
30-50.. Chinchor, N., Hirschman, L., and Lewis, D.D.,Evaluating Message Understanding Systems: AnAnalysis of the Third Message UnderstandingConference (MUC-3), to appear in ComputationalLinguistics, 19(3).. Hirschman, L., Comparing MUCK-II and MUC-3:Assessing the Difficulty of Different Tasks, inProceedings of the Third Message UnderstandingConference (MUC-3), May, 1991, MorganKaufmann Publishers, pp.
25-30.. Hirschman, L., An Adjunct Test for DiscourseProcessing in MUC-4, in Proceedings ofthe FourthMessage Understanding Conference (MUC-4), June,1992, Morgan Kaufmann Publishers, pp.
67-84.. Iwanska, L., et al, Computational Aspects ofDiscourse in the Context of MUC-3, in Proceedingsof the Third Message Understanding Conference(MUC-3), May, 1991, Morgan KaufmannPublishers, pp.
256-282..
Proceedings of the Third Message UnderstandingConference (MUC-3), May, 1991, MorganKaufmann Publishers..
Proceedings ofthe Fourth Message UnderstandingConference (MUC-4), June, 1992, MorganKaufmann Publishers.10.11.12.13.Sundheim, B., Plans for a Task-Oriented Evaluationof Natural Language Understanding Systems, inProceedings of the Speech and Natural LanguageWorkshop, February, 1989, Morgan KaufmannPublishers, pp.
197-202.Sundheim, B., Third Message UnderstandingEvaluation and Conference (MUC-3): Phase 1Status Report, in Proceedings of the Speech andNatural Language Workshop, February, 1991,Morgan Kaufmann Pubfishers, pp.
301-305.Sundheim, B., Overview of the Fourth MessageUnderstanding Evaluation and Conference, inProceedings ofthe Fourth Message UnderstandingConference (MUC-4), June, 1992, MorganKaufmann Publishers, pp.
3-21.Will, C. and Onyshkevych, B., Human Performancefor Information Extraction, unpublishedpresentation given at the Tipster 12-month meetingin San Diego, CA, September, 1992.60
