2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 347?351,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsInsertion and Deletion Models for Statistical Machine TranslationMatthias Huck and Hermann NeyHuman Language Technology and Pattern Recognition GroupComputer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany{huck,ney}@cs.rwth-aachen.deAbstractWe investigate insertion and deletion modelsfor hierarchical phrase-based statistical ma-chine translation.
Insertion and deletion mod-els are designed as a means to avoid the omis-sion of content words in the hypotheses.
Inour case, they are implemented as phrase-levelfeature functions which count the number ofinserted or deleted words.
An English word isconsidered inserted or deleted based on lex-ical probabilities with the words on the for-eign language side of the phrase.
Related tech-niques have been employed before by Och etal.
(2003) in an n-best reranking frameworkand by Mauser et al (2006) and Zens (2008)in a standard phrase-based translation system.We propose novel thresholding methods inthis work and study insertion and deletion fea-tures which are based on two different types oflexicon models.
We give an extensive exper-imental evaluation of all these variants on theNIST Chinese?English translation task.1 Insertion and Deletion ModelsIn hierarchical phrase-based translation (Chiang,2005), we deal with rules X ?
?
?, ?,?
?
where?
?, ??
is a bilingual phrase pair that may containsymbols from a non-terminal set, i.e.
?
?
(N ?VF )+ and ?
?
(N ?VE)+, where VF and VE are thesource and target vocabulary, respectively, and N isa non-terminal set which is shared by source and tar-get.
The left-hand side of the rule is a non-terminalsymbol X ?
N , and the ?
relation denotes a one-to-one correspondence between the non-terminals in?
and in ?.
Let J?
denote the number of terminalsymbols in ?
and I?
the number of terminal sym-bols in ?.
Indexing ?
with j, i.e.
the symbol ?j ,1 ?
j ?
J?, denotes the j-th terminal symbol onthe source side of the phrase pair ?
?, ?
?, and analo-gous with ?i, 1 ?
i ?
I?
, on the target side.With these notational conventions, we now de-fine our insertion and deletion models, each in bothsource-to-target and target-to-source direction.
Wegive phrase-level scoring functions for the four fea-tures.
In our implementation, the feature values areprecomputed and written to the phrase table.
Thefeatures are then incorporated directly into the log-linear model combination of the decoder.Our insertion model in source-to-target directionts2tIns(?)
counts the number of inserted words on thetarget side ?
of a hierarchical rule with respect to thesource side ?
of the rule:ts2tIns(?, ?)
=I??i=1J?
?j=1[p(?i|?j) < ?
?j](1)Here, [?]
denotes a true or false statement: The resultis 1 if the condition is true and 0 if the condition isfalse.
The model considers an occurrence of a tar-get word e an insertion iff no source word f existswithin the phrase where the lexical translation prob-ability p(e|f) is greater than a corresponding thresh-old ?f .
We employ lexical translation probabilitiesfrom two different types of lexicon models, a modelwhich is extracted from word-aligned training dataand?given the word alignment matrix?relies onpure relative frequencies, and the IBM model 1 lex-icon (cf.
Section 2).
For ?f , previous authors haveused a fixed heuristic value which was equal for all347f ?
Vf .
In Section 3, we describe how such a globalthreshold can be computed and set in a reasonableway based on the characteristics of the model.
Wealso propose several novel thresholding techniqueswith distinct thresholds ?f for each source word f .In an analogous manner to the source-to-target di-rection, the insertion model in target-to-source di-rection tt2sIns(?)
counts the number of inserted wordson the source side ?
of a hierarchical rule with re-spect to the target side ?
of the rule:tt2sIns(?, ?)
=J??j=1I?
?i=1[p(?j |?i) < ?
?i ] (2)Target-to-source lexical translation probabilitiesp(f |e) are thresholded with values ?e which may bedistinct for each target word e. The model consid-ers an occurrence of a source word f an insertion iffno target word e exists within the phrase with p(f |e)greater than or equal to ?e.Our deletion model, compared to the insertionmodel, interchanges the connection of the directionof the lexical probabilities and the order of sourceand target in the sum and product of the term.
Thesource-to-target deletion model thus differs from thetarget-to-source insertion model in that it employs asource-to-target word-based lexicon model.The deletion model in source-to-target directionts2tDel(?)
counts the number of deleted words on thesource side ?
of a hierarchical rule with respect tothe target side ?
of the rule:ts2tDel(?, ?)
=J??j=1I?
?i=1[p(?i|?j) < ?
?j](3)It considers an occurrence of a source word f a dele-tion iff no target word e exists within the phrase withp(e|f) greater than or equal to ?f .The target-to-source deletion model tt2sDel(?)
cor-respondingly considers an occurrence of a targetword e a deletion iff no source word f exists withinthe phrase with p(f |e) greater than or equal to ?e:tt2sDel(?, ?)
=I??i=1J?
?j=1[p(?j |?i) < ?
?i ] (4)2 Lexicon ModelsWe restrict ourselves to the description of thesource-to-target direction of the models.2.1 Word Lexicon from Word-Aligned DataGiven a word-aligned parallel training corpus, weare able to estimate single-word based translationprobabilities pRF(e|f) by relative frequency (Koehnet al, 2003).
With N(e, f) denoting counts ofaligned cooccurrences of target word e and sourceword f , we can computepRF(e|f) =N(e, f)?e?
N(e?, f).
(5)If an occurrence of e has multiple aligned sourcewords, each of the alignment links contributes witha fractional count.We denote this model as relative frequency (RF)word lexicon.2.2 IBM Model 1The IBM model 1 lexicon (IBM-1) is the first andmost basic one in a sequence of probabilistic genera-tive models (Brown et al, 1993).
For IBM-1, severalsimplifying assumptions are made, so that the proba-bility of a target sentence eI1 given a source sentencefJ0 (with f0 = NULL) can be modeled asPr(eI1|fJ1 ) =1(J + 1)II?i=1J?j=0pibm1(ei|fj) .
(6)The parameters of IBM-1 are estimated iterativelyby means of the Expectation-Maximization algo-rithm with maximum likelihood as training criterion.3 Thresholding MethodsWe introduce thresholding methods for insertion anddeletion models which set thresholds based on thecharacteristics of the lexicon model that is applied.For all the following thresholding methods, we dis-regard entries in the lexicon model with probabilitiesthat are below a fixed floor value of 10?6.
Again, werestrict ourselves to the description of the source-to-target direction.individual ?f is a distinct value for each f , com-puted as the arithmetic average of all entriesp(e|f) of any e with the given f in the lexiconmodel.348MT06 (Dev) MT08 (Test)NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]Baseline (with s2t+t2s RF word lexicons) 32.6 61.2 25.2 66.6+ s2t+t2s insertion model (RF, individual) 32.9 61.4 25.7 66.2+ s2t+t2s insertion model (RF, global) 32.8 61.8 25.7 66.7+ s2t+t2s insertion model (RF, histogram 10) 32.9 61.7 25.5 66.5+ s2t+t2s insertion model (RF, all) 32.8 62.0 26.1 66.7+ s2t+t2s insertion model (RF, median) 32.9 62.1 25.7 67.1+ s2t+t2s deletion model (RF, individual) 32.7 61.4 25.6 66.5+ s2t+t2s deletion model (RF, global) 33.0 61.3 25.8 66.1+ s2t+t2s deletion model (RF, histogram 10) 32.9 61.4 26.0 66.1+ s2t+t2s deletion model (RF, all) 33.0 61.4 25.9 66.4+ s2t+t2s deletion model (RF, median) 32.9 61.5 25.8 66.7+ s2t+t2s insertion model (IBM-1, individual) 33.0 61.4 26.1 66.4+ s2t+t2s insertion model (IBM-1, global) 33.0 61.6 25.9 66.5+ s2t+t2s insertion model (IBM-1, histogram 10) 33.7 61.3 26.2 66.5+ s2t+t2s insertion model (IBM-1, median) 33.0 61.3 26.0 66.4+ s2t+t2s deletion model (IBM-1, individual) 32.8 61.5 26.0 66.2+ s2t+t2s deletion model (IBM-1, global) 32.9 61.3 25.9 66.1+ s2t+t2s deletion model (IBM-1, histogram 10) 32.8 61.2 25.7 66.0+ s2t+t2s deletion model (IBM-1, median) 32.8 61.6 25.6 66.7+ s2t insertion + s2t deletion model (IBM-1, individual) 32.7 62.3 25.7 67.1+ s2t insertion + t2s deletion model (IBM-1, individual) 32.7 62.2 25.9 66.8+ t2s insertion + s2t deletion model (IBM-1, individual) 33.1 61.3 25.9 66.2+ t2s insertion + t2s deletion model (IBM-1, individual) 33.0 61.3 26.1 66.0+ source+target unaligned word count 32.3 61.8 25.6 66.7+ phrase-level s2t+t2s IBM-1 word lexicons 33.8 60.5 26.9 65.4+ source+target unaligned word count 34.0 60.4 26.7 65.8+ s2t+t2s insertion model (IBM-1, histogram 10) 34.0 60.3 26.8 65.2+ phrase-level s2t+t2s DWL + triplets + discrim.
RO 34.8 59.8 27.7 64.7+ s2t+t2s insertion model (RF, individual) 35.0 59.5 27.8 64.4Table 1: Experimental results for the NIST Chinese?English translation task (truecase).
s2t denotes source-to-targetscoring, t2s target-to-source scoring.
Bold font indicates results that are significantly better than the baseline (p < .1).global The same value ?f = ?
is used for all f .We compute this global threshold by averagingover the individual thresholds.1histogram n ?f is a distinct value for each f .
?f isset to the value of the n+1-th largest probabil-ity p(e|f) of any e with the given f .1Concrete values from our experiments are: 0.395847 forthe source-to-target RF lexicon, 0.48127 for the target-to-sourceRF lexicon.
0.0512856 for the source-to-target IBM-1, and0.0453709 for the target-to-source IBM-1.
Mauser et al (2006)mention that they chose their heuristic thresholds for use withIBM-1 between 10?1 and 10?4.all All entries with probabilities larger than the floorvalue are not thresholded.
This variant may beconsidered as histogram ?.
We only apply itwith RF lexicons.median ?f is a median-based distinct value for eachf , i.e.
it is set to the value that separates thehigher half of the entries from the lower half ofthe entries p(e|f) for the given f .4 Experimental EvaluationWe present empirical results obtained with the dif-ferent insertion and deletion model variants on the349Chinese?English 2008 NIST task.24.1 Experimental SetupTo set up our systems, we employ the open sourcestatistical machine translation toolkit Jane (Vilar etal., 2010; Vilar et al, 2012), which is freely avail-able for non-commercial use.
Jane provides efficientC++ implementations for hierarchical phrase extrac-tion, optimization of log-linear feature weights, andparsing-based decoding algorithms.
In our experi-ments, we use the cube pruning algorithm (Huangand Chiang, 2007) to carry out the search.We work with a parallel training corpus of 3.0MChinese-English sentence pairs (77.5M Chinese /81.0M English running words).
The counts forthe RF lexicon models are computed from a sym-metrized word alignment (Och and Ney, 2003), theIBM-1 models are produced with GIZA++.
Whenextracting phrases, we apply several restrictions, inparticular a maximum length of 10 on source andtarget side for lexical phrases, a length limit of five(including non-terminal symbols) for hierarchicalphrases, and no more than two gaps per phrase.The models integrated into the baseline are: phrasetranslation probabilities and RF lexical translationprobabilities on phrase level, each for both transla-tion directions, length penalties on word and phraselevel, binary features marking hierarchical phrases,glue rule, and rules with non-terminals at the bound-aries, source-to-target and target-to-source phraselength ratios, four binary features marking phrasesthat have been seen more than one, two, three orfive times, respectively, and an n-gram languagemodel.
The language model is a 4-gram with modi-fied Kneser-Ney smoothing which was trained withthe SRILM toolkit (Stolcke, 2002) on a large collec-tion of English data including the target side of theparallel corpus and the LDC Gigaword v3.Model weights are optimized against BLEU (Pa-pineni et al, 2002) with standard Minimum ErrorRate Training (Och, 2003), performance is measuredwith BLEU and TER (Snover et al, 2006).
We em-ploy MT06 as development set, MT08 is used as un-seen test set.
The empirical evaluation of all our se-tups is presented in Table 1.2http://www.itl.nist.gov/iad/mig/tests/mt/2008/4.2 Experimental ResultsWith the best model variant, we obtain a significantimprovement (90% confidence) of +1.0 points BLEUover the baseline on MT08.
A consistent trend to-wards one of the variants cannot be observed.
Theresults on the test set with RF lexicons or IBM-1, in-sertion or deletion models, and (in most of the cases)with all of the thresholding methods are roughly atthe same level.
For comparison we also give a resultwith an unaligned word count model (+0.4 BLEU).Huck et al (2011) recently reported substantialimprovements over typical hierarchical baseline se-tups by just including phrase-level IBM-1 scores.When we add the IBM-1 models directly, our base-line is outperformed by +1.7 BLEU.
We tried toget improvements with insertion and deletion mod-els over this setup again, but the positive effect waslargely diminished.
In one of our strongest setups,which includes discriminative word lexicon models(DWL), triplet lexicon models and a discriminativereordering model (discrim.
RO) (Huck et al, 2012),insertion models still yield a minimal gain, though.5 ConclusionOur results with insertion and deletion models forChinese?English hierarchical machine translationare twofold.
On the one hand, we achieved sig-nificant improvements over a standard hierarchicalbaseline.
We were also able to report a slight gainby adding the models to a very strong setup withdiscriminative word lexicons, triplet lexicon mod-els and a discriminative reordering model.
On theother hand, the positive impact of the models wasmainly noticeable when we exclusively applied lex-ical smoothing with word lexicons which are simplyextracted from word-aligned training data, whichis however the standard technique in most state-of-the-art systems.
If we included phrase-level lexicalscores with IBM model 1 as well, the systems barelybenefited from our insertion and deletion models.Compared to an unaligned word count model, inser-tion and deletion models perform well.AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.350ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The Mathemat-ics of Statistical Machine Translation: Parameter Es-timation.
Computational Linguistics, 19(2):263?311,June.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.
ofthe 43rd Annual Meeting of the Assoc.
for Computa-tional Linguistics (ACL), pages 263?270, Ann Arbor,MI, USA, June.Liang Huang and David Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.
InProc.
of the Annual Meeting of the Assoc.
for Com-putational Linguistics (ACL), pages 144?151, Prague,Czech Republic, June.Matthias Huck, Saab Mansour, Simon Wiesler, and Her-mann Ney.
2011.
Lexicon Models for Hierarchi-cal Phrase-Based Machine Translation.
In Proc.
ofthe Int.
Workshop on Spoken Language Translation(IWSLT), pages 191?198, San Francisco, CA, USA,December.Matthias Huck, Stephan Peitz, Markus Freitag, and Her-mann Ney.
2012.
Discriminative Reordering Exten-sions for Hierarchical Phrase-Based Machine Transla-tion.
In Proc.
of the 16th Annual Conference of the Eu-ropean Association for Machine Translation (EAMT),Trento, Italy, May.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of the Human Language Technology Conf.
/ NorthAmerican Chapter of the Assoc.
for ComputationalLinguistics (HLT-NAACL), pages 127?133, Edmonton,Canada, May/June.Arne Mauser, Richard Zens, Evgeny Matusov, Sas?aHasan, and Hermann Ney.
2006.
The RWTH Statisti-cal Machine Translation System for the IWSLT 2006Evaluation.
In Proc.
of the Int.
Workshop on SpokenLanguage Translation (IWSLT), pages 103?110, Ky-oto, Japan, November.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.
Syn-tax for Statistical Machine Translation.
Technical re-port, Johns Hopkins University 2003 Summer Work-shop on Language Engineering, Center for Languageand Speech Processing, Baltimore, MD, USA, August.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of the An-nual Meeting of the Assoc.
for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Evalu-ation of Machine Translation.
In Proc.
of the 40th An-nual Meeting of the Assoc.
for Computational Linguis-tics (ACL), pages 311?318, Philadelphia, PA, USA,July.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Conf.
of the Assoc.
for Machine Translationin the Americas (AMTA), pages 223?231, Cambridge,MA, USA, August.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Spoken Language Processing (ICSLP), volume 3,Denver, CO, USA, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open Source Hierarchical Transla-tion, Extended with Reordering and Lexicon Models.In ACL 2010 Joint Fifth Workshop on Statistical Ma-chine Translation and Metrics MATR, pages 262?270,Uppsala, Sweden, July.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2012.
Jane: an advanced freely available hier-archical machine translation toolkit.
Machine Trans-lation, pages 1?20.
http://dx.doi.org/10.1007/s10590-011-9120-y.Richard Zens.
2008.
Phrase-based Statistical MachineTranslation: Models, Search, Training.
Ph.D. thesis,RWTH Aachen University, Aachen, Germany, Febru-ary.351
