Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 273?280, Vancouver, October 2005. c?2005 Association for Computational LinguisticsPP-attachment disambiguation using large contextMarian Olteanu and Dan MoldovanHuman Language Technology Research InstituteThe University of Texas at DallasRichardson, TX 75080marian@hlt.utdallas.edumoldovan@utdallas.eduAbstractPrepositional Phrase-attachment is a com-mon source of ambiguity in natural lan-guage.
The previous approaches use lim-ited information to solve the ambiguity?
four lexical heads ?
although humansdisambiguate much better when the fullsentence is available.
We propose tosolve the PP-attachment ambiguity with aSupport Vector Machines learning modelthat uses complex syntactic and seman-tic features as well as unsupervised in-formation obtained from the World WideWeb.
The system was tested on severaldatasets obtaining an accuracy of 93.62%on a Penn Treebank-II dataset; 91.79% ona FrameNet dataset when no manually-annotated semantic information is pro-vided and 92.85% when semantic infor-mation is provided.1 Problem description1.1 PP-attachment ambiguity problemPrepositional Phrase-attachment is a source of ambi-guity in natural language that generates a significantnumber of errors in syntactic parsing.
For examplethe sentence ?I saw yesterday the man in the parkwith a telescope?
has 5 different semantic interpre-tations based on the way the prepositional phrases?in the park?
and ?with the telescope?
are attached:I saw yesterday [the man [in the park [with a tele-scope]]]; I saw yesterday [the man [in the park][with a telescope]]; I saw yesterday [the man [in thepark]] [with a telescope]; I saw yesterday [the man][in the park [with a telescope]] and I saw yesterday[the man] [in the park] [with a telescope].The problem can be viewed as a decision of at-taching a prepositional phrase (PP) to one of thepreceding head nouns or verbs.
The ambiguity ex-pressed by the number of potential parse trees gener-ated by Context-Free Grammars increases exponen-tially with the number of PPs.
For a PP that followsthe object of a verb there are 2 parse trees, for a chainof 2, 3, 4 and 5 PPs there are respectively 5, 14, 42and 132 parse trees.
Usually the average number ofconsecutive PPs in a sentence increases linearly withthe length of the sentence.Lexical and syntactic information alone is not suf-ficient to resolve the PP-attachment problem; of-ten semantic and/or contextual information is nec-essary.
For example, in ?I ate a pizza with an-chovies?, ?with anchovies?
attaches to the noun?pizza?, where as in ?I ate a pizza with friends.
?,?with friends?
attaches to the verb ?eat?
?
examplefound in (McLauchlan, 2001).
There are instancesof PP-attachment, like the one in ?I saw the car inthe picture?
that can be disambiguated only by usingcontextual discourse information.Usually, people don?t have much trouble in find-ing the right way to attach PPs.
But if one limitsthe information used for disambiguation of the PP-attachment to include only the verb, the noun repre-senting its object, the preposition and the main nounin the PP, the accuracy for human decision degradesfrom 93.2% to 88.2% (Ratnaparkhi et al, 1994) ona dataset extracted from Penn Treebank (Marcus et273al., 1993).1.2 MotivationSyntactic parsing is essential for many natural lan-guage applications such as Machine Translation,Question Answering, Information Extraction, Infor-mation Retrieval, Automatic Speech Recognition.Since parsing occurs early in the chain of NLPprocessing steps it has a large impact on the over-all system performance.2 ApproachOur approach to solve the PP-attachment ambigu-ity is based on a Support Vector Machines learner(Cortes and Vapnik, 1995).
The feature set containscomplex information extracted automatically fromcandidate syntax trees generated by parsing (Char-niak, 2000), trees that will be improved by more ac-curate PP-attachment decisions.
Some of these fea-tures were proven efficient for semantic informationlabeling (Gildea and Jurafsky, 2002).
The featureset alo includes unsupervised information obtainedfrom a very large corpus (World Wide Web).
Fea-tures containing manually annotated semantic infor-mation about the verb and about the objects of theverb have also been used.
We adopted the standardapproach to distinguish between verb and noun at-tachment; thus the classifier has to choose betweentwo classes: V when the prepositional phrase is at-tached to the verb and N when the prepositionalphrase is attached to the preceding head noun.3 DataTo be able to extract the required features from adataset instance, one must identify the verb, thephrase identifying the object of the verb that pre-cedes the prepositional phrase in question (np1)which usually is part of the predicate-argumentstructure of the verb, its head noun, the prepositionalphrase (np2), its preposition and its head noun (thesecond most important word in the PP).We have adopted the notation from (Collins andBrooks, 1995), where v is the verb, n1 is the headnoun of object phrase, p is the preposition and n2 isthe head noun of the prepositional phrase.Compared to our datasets, Ratnaparkhi?s dataset(Ratnaparkhi et al, 1994) contains only the lexicalheads v, n1, p and n2.
Thus, our methodology can-not be applied to Ratnaparkhi?s dataset (RRR).In our experiments we used two datasets:?
FN ?
extracted from FrameNet II 1.1 (Baker etal., 1998)?
TB2 ?
extracted from Penn Treebank-IITable 1 presents the datasets1.
The creation of thedatasets is described in details in (Olteanu, 2004).4 FeaturesThe experiments described in this paper use a setof discrete (alphanumeric) and continuous (numeric)features.
All features are fully deterministic, exceptthe features count-ratio and pp-count that are basedon information provided by an external resource- Google search engine (http://www.google.com).In describing the features, we will use the PennTreebank-II parse tree associated with the sentence?The Lorillard spokeswoman said asbestos wasused in ?very modest amounts?
in making paper forthe filters in the early 1950s and replaced with a dif-ferent type of filter in 1956?.Table 2 describes the features and the origin ofeach feature.
The preposition is the feature withthe most discriminative power, because of prefer-ences of particular prepositions to attach to verbsor nouns.
Table 3 shows the distribution of top10 most frequently used prepositions in the FN andTB2 datasets.The features were carefully designed so that,when they are extracted from gold parse trees, theydon?t provide more information useful for disam-biguation than when they are automatically gener-ated using a parser.
This claim is validated by theexperimental results that show a strong correlationbetween the results on the two datasets ?
one basedon automatically generated parse trees (FN) and onebased on gold parse trees (TB2).Next, we describe in further detail the featurespresented in Table 2.v-frame represents the frame of the verb ?
theframe to which the verb belongs, as it is present inFrameNet (manually annotated).
We used this fea-ture because the frame of the verb describes verywell the semantic behavior of the verb including thepredicate-argument structure of the verb, which en-tails the affinity of the verb for certain prepositions.1The datasets are available at http://www.utdallas.edu/?mgo031000/ppa/274FN TB2Source FrameNet annotation samples (British NationalCorpus)Penn Treebank-II(WSJ articles)Instance identifica-tionSemantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of theparse tree)Parse trees Automatically generated (Charniak) Gold standardTotal size 27,421 instances 60,699 instancesDistribution statistics 70.28% ambiguous verb attachments2.36:1 v-attch:n-attch35.71% ambiguous verb attachments1:1.8 v-attch:n-attchTraining / test sets 90% - 10% ?
homogenously distributed (one in every 10 instances is selected for the test set)Location of PP Both before and after verb Only after verbOther properties ?
Partial identification of ambiguous PP-attachment instances in the corpus, derived frommanual annotation of FEs (Olteanu, 2004)?
Semantic information readily availableTable 1: The datasets and their characteristicsFeature: description [origin]v-surface: surface form of the verb [Hindle?93, ...]n1-surface: surface form of n1.
May be morphologicallyprocessed [Hindle?93, ...]p: the preposition, lower-cased [Hindle?93, ...]n2-surface: surface form of n2.
May be morphologicallyprocessed [Ratnaparkhi?94, Collins?95, ...]n1-mp/n1-mpf: morph.
processing of n1 [Collins?95]n2-mp/n2-mpf: morph.
processing of n2 [Collins?95]v-lemma: lemma of the verb [Collins?95]path: path in the candidate parse tree between the verb andnp1 [Gildea?02]subcategorization: subcategorization of the verb [modifiedfrom Pradhan?03]v-pos: part-of-speech of the verbv-voice: voice of the verbn1-pos: part-of-speech of n1n1-lemma: lemma of n1.
May be morphologicallyprocessedn2-pos: part-of-speech of n2n2-lemma: lemma of n2.
May be morphologicallyprocessedposition: position of np1 relative to the verb [new]v-frame: frame of the verb [new in PPA]n1-sr: semantic role of np1 [new in PPA]n1-tr: thematic role of np1 [new in PPA]n1-preposition: preposition that heads np1, if np1 is a PP[new]n1-parent: label of the parent of np1 in the candidate parsetree [new in PPA]n1-np-label: label of np1 in the candidate parse tree [new inPPA]n2-det: determination of np2 [new]parser-vote: choice of the automatic parser in attaching PP[new in PPA]count-ratio: WWW statistics about verb-attachment vs.noun-attachment for that particular instance [new]pp-count: WWW statistics about co-occurrence of v and n2[new]n1-p-distance: the distance between n1 and p [new]Table 2: Features% of % v-att % of % v-attPrep.
FN FN TB2 TB2of 13.47% 6.17% 30.14% 2.74%to 13.27% 80.14% 9.55% 60.49%in 12.42% 73.64% 16.94% 42.58%for 6.87% 82.44% 8.95% 39.72%on 6.21% 75.51% 5.16% 47.73%with 6.17% 86.30% 3.79% 46.92%from 5.37% 75.90% 5.76% 52.76%at 4.09% 76.63% 3.21% 66.02%as 3.95% 86.51% 2.49% 51.69%by 3.53% 88.02% 3.27% 68.11%Table 3: Distribution of the first 10 most-frequentprepositions in the FN and TB2 datasetsn1-sr represents the semantic role of the objectphrase np1 ?
the label attached to the Frame Ele-ment (manual semantic annotation that can be foundin FrameNet).
This feature was introduced becauseof the relation between the underlying meaning ofnp1 and its semantic role.n1-tr represents the thematic role of the objectphrase np1 ?
a coarse-grained role based on the la-bel attached to the Frame Element (manual semanticannotation that can be found in FrameNet).
It wasintroduced to reduce data sparseness for the n1-srfeature.
The conversion from fine-grained semanticrole to coarse-grained semantic role is done auto-matically using a table that maps a pair of a frame-level semantic role (FE label) and a frame to a the-matic role.subcategorization contains a semi-lexicalizeddescription of the structure of the verb phrase.
Asubcategorization frame is closely related to the275predicate argument structure and to the underlyingmeaning of the verb.
It contains an ordered set of allthe phrase labels that are siblings of the verb, plus amarker for the verb.
If the child phrase of the verbis a PP, then the label will also contain the prepo-sition (the headword of the PP).
This feature is amodified form of the sub-categorization feature de-scribed in (Pradhan et al, 2003): the differences invarious part-of-speeches for the verb were ignoredand the preposition that heads a prepositional phraseis also attached to the label.
Therefore, for the sen-tence ?The stock declined in June by 4%?, the valuefor this feature is *-PPin-PPby.In the TB2 dataset the parse trees are gold stan-dard (contain the expected output value for PP-ambiguity resolution).
In the case of a verb attach-ment, if the selected PP is a child of the selected VP,then by applying the algorithm, the value of the fea-ture will contain the PP label plus the preposition.This clearly is a clue for the learner that the instanceis a verb attachment.
To overcome this problem fordatasets based on gold-standard parse trees, whencomputing the value of the subcategorization fea-ture the selected PP will not be used.
Figure 1 showsthe subcategorization for the phrase ?replaced witha different type of filter in 1956?.VPreplacedPPwithNPNPdifferentatypePPofNPfilterPPinNP1956Figure 1: Subcategorization feature: *-PPin-PPbypath expresses the syntactic relation between theverb v and the object phrase np1.
Its purpose isto describe the syntactic relation of np1 to the restof the clause by the syntactic relation of np1 withthe head of the clause ?
v. We adopted this featurefrom (Gildea and Jurafsky, 2002).
path describesthe chain of labels in the tree from v to np1, includ-ing the label of v and np1.
Ascending movementsand descending movements are depicted separately.We used two variants of this feature to determinethe optimum version for our problem ?
one with fullPOS of the verb and one with POS reduced to ?VB?.The experiments proved that the second variant pro-vides a better performance.
Figure 2 depicts the pathbetween ?replaced?
and ?a different type of filter?
:VBN?VP?PP?NP or VB?VP?PP?NP.VPreplacedPPwithNPNPdifferentatypePPofNPfilterPPinNP1956Figure 2: Example of a path featureposition indicates the position of the n1-p-n2 con-struction relative to the verb, i.e.
whether the prepo-sitional phrase in question lies before the verb or af-ter the verb in the sentence.
Position is very impor-tant in deciding the type of attachment, consideringthe totally different distribution of PPs constructionspreceding the verb and PPs constructions followingthe verb.Morphological processing applied to n1 and n2was inspired by the algorithm described in (Collinsand Brooks, 1995).
We analyzed the impact of dif-ferent levels of morphological processing by usingtwo types: partial morphological processing (onlynumbers and years are converted) ?
identified byadding -mp as a suffix to the name of this feature ?and full morphological processing (numbers, yearsand capitalized names) ?
identified by adding -mpfas a suffix to the name of this feature.
The purposeof morphological processing is data sparseness re-duction by clustering similar values for this feature.n1-parent represents the phrase label of the par-ent of np1 and it cannot be used on gold parse trees(TB2 dataset) because it will provide a clue aboutthe correct attachment type.276n2-det is called the determination of the preposi-tional phrase np2.
This novel feature tells if n2 ispreceded in np2 by a possessive pronoun or by a de-terminer.
This is used to differentiate between ?buybooks for children?
(which is probably a noun at-tachment) and ?buy books for her children?
(whichvery probably is a verb attachment).parser-vote feature represents the choice of theparser (Charniak?s parser) in the PP-attachment res-olution.
It cannot be used with gold-standard parsetrees because it will provide the right answer.count-ratio represents the estimated ratio be-tween the frequency of an unambiguous verb attach-ment construction based on v, p and n2 and the fre-quency of a probably unambiguous noun attachmentconstruction based on n1, p and n2 in a very largecorpus.
A very large corpus is required to overcomethe data sparseness inherent for complex construc-tions like those described above.We chose the World Wide Web as a corpus andGoogle as a query interface (see (Olteanu, 2004) fordetails).Let?s consider the estimated frequency of un-ambiguous verb-attachments and respectively noun-attachments defined as:fv = cv?p?n2cv ?
cp?n2fn = cn1?p?n2cn1 ?
cp?n2where:?
cv?p?n2 is the number of occurrences of thephrase ?v p n2?, ?v p?n2?
(where * symbolizesany word), ?v-lemma p n2?
or ?v-lemma p * n2?in World Wide Web, as reported by Google?
cv is the number of occurrences of the word ?v?or ?v-lemma?
in WWW?
cp?n2 is the number of occurrences of thephrase ?p n2?
or ?p ?
n2?
in WWW?
cn1?p?n2 is the number of occurrences of thephrase ?n1 p n2?
or ?v p ?
n2?
in WWW?
cn1 is the number of occurrences of the word?n1?
in WWWThe value for this feature is:count?
ratio = log10fvfn = log10cv?p?n2 ?
cn1cn1?p?n2 ?
cvWe chose logarithmic values for this feature be-cause experiments showed that logarithmic valuesprovide a higher accuracy than linear values.
Also,by experimentation we concluded that value bound-ing is helpful, and the feature was bounded to valuesbetween -3 and 3 on the logarithmic scale, unlessspecified otherwise in the experiment description.This feature resembles the approach adopted in(Volk, 2001).pp-count depicts the estimated count of occur-rences in World Wide Web of the prepositionalphrases based on p and n2.
The count is estimatedby cp?n2.
Therefore pp-count = log10(cp?n2 +cp??
?n2).n1-p-distance depicts the distance (in tokens) be-tween n1 and p. Let dn1?p be the distance be-tween n1 and p (d = 1 if there is no other to-ken between n1 and p).
Thus n1-p-distance =log10(1 + log10 dn1?p).5 Learning model and procedureWe used in our experiments a Support VectorMachines learner with Radial Basis Functionkernel as implemented in the LIBSVM toolkit(http://www.csie.ntu.edu.tw/?cjlin/libsvm/).We converted the feature tuples (containing dis-crete alphanumeric and continuous values) to multi-dimensional vectors using the following procedure:?
Discrete features: assign to each possible valueof each feature a dimension in the vector space,and to each feature value in each training or testexample put 1 in the dimension correspondingto the feature value and 0 in all other dimen-sions associated with that feature.?
Continuous features: assign a dimension andput the scaled value in the multi-dimensionalvector (all examples in training data will spanbetween 0 and 1 for that particular dimension).SVM training was preceded by finding the opti-mal ?
and C parameters required for training using2-fold cross validation, which was found to be supe-rior in model accuracy and training time over higherfolds cross-validations (Olteanu, 2004).The criterion for selecting the best set of featureswas the accuracy on the cross-validation.
Thus, thedevelopment of the models was performed entirely277on the training set, which acted also as a develop-ment set.
We later computed the accuracy on thetest set on some representative models.6 Experiments, results and analysisFor each dataset, we conducted experiments to de-termine an efficient combination of features and theaccuracy on test data for the best combination of fea-tures.
We also run the experimental procedure onthe original Ratnaparkhi?s dataset in order to com-pare SVM with other machine learning techniquesapplied to PP-attachment problem.
Table 4 summa-rizes the experiments performed on all datasets.% on dev % on testExperiment / x-valFN-basic-flw 86.25 86.44FN-lex-syn-flw 88.55 89.61FN-best-no-sem 90.93 91.79FN-best-sem 91.87 92.85TB2-basic 85.75 87.47TB2-best-no-www 92.06 92.81TB2-best 92.92 93.62RRR-basic 84.32 84.60RRR-basic-mpf 84.34 85.14Table 4: ResultsFN-basic-flw uses v-surface, n1-surface, p andn2-surface on examples that follow the verb.
FN-lex-syn-flw uses v-surface, v-pos, v-lemma, sub-categorization, path (full POS), position, n1-preposition, n1-surface, n1-pos, n1-lemma, n1-parent, p, n2-surface, n2-pos, n2-lemma, n2-det and parser-vote on examples that follow theverb.
FN-best-no-sem uses v-surface, v-pos, v-lemma, subcategorization, path (reduced POS),position, n1-preposition, n1-surface, n1-pos, n1-lemma-mpf, n1-parent, p, n2-surface, n2-pos,n2-lemma-mpf, n2-det, parser-vote, count-ratioand pp-count on all examples.
FN-best-sem usesthe same set of features as FN-best-no-sem plus v-frame and n1-sr.TB2-basic uses v-surface, n1-surface-mpf, pand n2-surface-mpf.
TB2-best-no-www uses v-surface, v-pos, v-lemma, subcategorization, path(reduced POS), n1-preposition, n1-surface, n1-mpf, n1-pos, n1-lemma, n1-np-label, p, n2-surface, n2-mpf and n1-p-distance.
TB2-best alsouses count-ratio and pp-count.RRR-basic uses v-surface, n1-surface, p andn2-surface.
RRR-basic-mpf uses v-surface, n1-surface-mpf, p and n2-surface-mpf.On the FN dataset, all features except v-voicehave a positive contribution to the system (n2-det,choice between semantic vs. thematic role and howshould morphological processing be applied is ques-tionable).
The negative impact for the v-voice fea-ture may be explained by the fact that the only sit-uation in which it may potentially help is extremelyrare: passive voice and the agent headed by ?by?
ap-pears after another argument of the verb (i.e.
: ?Thepainting was presented to the audience by its au-thor.?).
Moreover the PP-attachment based on thepreposition ?by?
is not highly ambiguous; as seenin Table 3 in the FrameNet dataset, 88% of the ?by?ambiguity instances are verb-attachments.The experiment with the highest cross-validationaccuracy has an accuracy of 92.85% on the test data.The equivalent experiment that doesn?t include man-ually annotated semantic information has an accu-racy of 91.79% on the test data.On TB2 dataset, the results are close to the resultsobtained on the FrameNet corpus, although the dis-tribution of noun and verb attachment differs consid-erably between the two data sets (70.28% are verb-attachments in FN2 and 35.71% in TB2).
The bestaccuracy in cross-validation is 92.92%, which leadsto an accuracy on test set of 93.62%.7 Comparison with previous workBecause we couldn?t use the standard dataset usedin PP-attachment resolution (Ratnaparkhi?s), we im-plemented back-off algorithm developed by Collinsand Brooks (1995) and applied it to our TB2 dataset.Both RRR and TB2 datasets are extracted from PennTreebank.
This algorithm, trained on TB2 trainingset, obtains an accuracy on TB2 test set of 86.1%(85.8% when no morphological processing is ap-plied).
The same algorithm provides an accuracy onRRR dataset of 84.5% (84.1% without morphologi-cal processing).
The difference in accuracy betweenthe two datasets is 1.6% (1.7% without morpholog-ical processing when using Collins and Brooks?s al-gorithm.The difference in accuracy between a SVM modelapplied to RRR dataset (RRR-basic experiment) andthe same experiment applied to TB2 dataset (TB2-278Description Accuracy Data Extra SupervisionAlways noun 55.0 RRRMost likely for each P 72.19 RRRMost likely for each P 72.30 TB2Most likely for each P 81.73 FNAverage human, headwords (Ratnaparkhi et al, 1994) 88.2 RRRAverage human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRRMaximum Likelihood-based (Hindle and Rooth, 1993) 79.7 APMaximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRRMaximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRRDecision trees (Ratnaparkhi et al, 1994) 77.7 RRRTransformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNetMaximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRRMaximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNetMemory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpaceMaximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRRNeural Nets (Alegre et al, 1999) 86.0 RRR WordNetBoosting (Abney et al, 1999) 84.4 RRRSemi-probabilistic (Pantel and Lin, 2000) 84.31 RRRMaximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSASVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRRNearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWSFN dataset, w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWWFN dataset, w/ semantic features (FN-best-sem) 92.85 FN PR-WWWTB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWWTable 5: Accuracy of PP-attachment ambiguity resolution (our results in bold)basic experiment) is 2.9%.
Also, the baseline ?
themost probable PP type for each preposition ?
is ap-proximately the same for the two datasets (72.19%on RRR and 72.30% on TB2).One may hypothesize that the majority of the al-gorithms for PP-attachment disambiguation obtainno more than 4% increase in accuracy on the TB2compared to the results on the RRR dataset.
Oneimportant difference between the two datasets is thesize ?
20,801 training examples in RRR vs. 54,629training examples in TB2.
We plan to implementmore algorithms described in literature in order toverify this statement.Table 5 summarizes the results in PP-attachmentambiguity resolution found in literature along withour best results.Other acronyms used in this table:?
AP ?
dataset of 13 million word sample of As-sociated Press news stories from 1999 (Hindleand Rooth, 1993).?
LexSpace - Lexical Space ?
a method to mea-sure the similarity of the words (Zavrel et al,1997).?
LSA ?
Latent Semantic Analysis ?
measure thelexical preferences between a preposition and anoun or a verb (McLauchlan, 2001)?
DWS ?
Distributional Word Similarity.
Wordsthat tend to appear in the same contexts tend tohave similar meanings (Zhao and Lin, 2004)?
PR-WWW ?
the probability ratio betweenverb-preposition-noun and noun-preposition-noun constructs measured using World WideWeb searching.8 ConclusionsThe Penn Treebank-II results indicate that thenew features used for the disambiguation of PP-attachment provide a very substantial improvementin accuracy over the base line (from 87.48% to93.62%).
This represents an absolute improvementof approximately 6.14%, equivalent to a 49% er-ror drop.
The performance of the system on PennTreebank-II exceeds the reported human expert per-formance on Penn Treebank-I (Ratnaparkhi et al,1994) by about 0.4%.
A significant improvementcomes from the unsupervised information collected279from a very large corpus; this method proved to beefficient to overcome the data sparseness problem.By analyzing the results from the FrameNetdataset, we conclude that the contribution of the goldsemantic features (frame and semantic role) is sig-nificant (1.05% difference in accuracy; 12.8% re-duction in the error).
We will further investigate thisissue by replacing gold semantic information withautomatically detected semantic information.
Ouradditional lexico-syntactic features increase the ac-curacy of the system from 86.44% to 89.61% forPPs following the verb.
This suggests that on theFrameNet dataset the proposed syntactic featureshave a considerable impact on the accuracy.The best TB2 feature set is approximately thesame as the best FN feature set in spite of the dif-ferences between the datasets (Parse trees: TB2 ?gold standard; FN ?
automatically generated.
PP-attachment ambiguity identification: TB2 ?
parsetrees; FN ?
a combination of trees and FE annota-tion.
Data source: TB2 ?
WSJ articles; FN ?
BNC).This fact suggests that the selected feature sets donot exploit particularities of the datasets and that thefeatures are relevant to the PP-attachment ambiguityproblem.ReferencesSteven Abney, Robert E. Schapire, and Yoram Singer.
1999.Boosting applied to tagging and PP Attachment.
In Proceed-ings of EMNLP/VLC-99, pages 38?45.Martha A. Alegre, Josep M. Sopena, and Agusti Lloberas.1999.
Pp-attachment: A committee machine approach.
InProceedings of EMNLP/VLC-99, pages 231?238.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.The Berkeley FrameNet Project.
In Proceedings of the17th international conference on Computational Linguistics,pages 86?90.Eric Brill and Philip Resnik.
1994.
A rule-based approachto prepositional phrase attachment disambiguation.
In Pro-ceedings of the 15th conference on Computational Linguis-tics, pages 1198?1204.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.In Proceedings of NAACL-2000, pages 132?139.Michael Collins and James Brooks.
1995.
Prepositional PhraseAttachment through a Backed-Off Model.
In Proceedings ofthe Thirds Workshop on Very Large Corpora, pages 27?38.Corinna Cortes and Vladimir Vapnik.
1995.
Support-VectorNetworks.
Machine Learning, 20(3):273?297.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic Labelingof Semantic Roles.
Computational Linguistics, 28(3):245?288.Donald Hindle and Mats Rooth.
1993.
Structural Ambi-guity and Lexical Relations.
Computational Linguistics,19(1):103?120.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Mark McLauchlan.
2001.
Maximum Entropy Models andPrepositional Phrase Ambiguity.
Master?s thesis, Universityof Edinburgh.Marian G. Olteanu.
2004.
Prepositional Phrase Attachmentambiguity resolution through a rich syntactic, lexical andsemantic set of features applied in support vector machineslearner.
Master?s thesis, University of Texas at Dallas.Patrick Pantel and Dekang Lin.
2000.
An unsupervised ap-proach to Prepositional Phrase Attachment using contextu-ally similar words.
In Proceedings of the 38th Meeting of theAssociation for Computational Linguistic, pages 101?108.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Mar-tin, and Daniel Jurafsky.
2003.
Semantic Role Parsing:Adding Semantic Structure to Unstructured Text.
In Pro-ceedings of the International Conference on Data Mining,pages 629?632.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.
AMaximum Entropy Model for Prepositional Phrase Attach-ment.
In Proceedings of the Human Language TechnologyWorkshop, pages 250?255.Adwait Ratnaparkhi.
1998.
Statistical Models for Unsuper-vised Prepositional Phrase Attachment.
In Proceedings ofthe 36th conference on Association for Computational Lin-guistics, pages 1079?1085.Jiri Stetina and Makoto Nagao.
1997.
Corpus based PP attach-ment ambiguity resolution with a semantic dictionary.
InProceedings of the Fifth Workshop on Very Large Corpora,pages 66?80.Bram Vanschoenwinkel and Bernard Manderick.
2003.
Aweighted polynomial information gain kernel for resolvingPrepositional Phrase attachment ambiguities with SupportVector Machines.
In Proceedings of the Eighteenth Inter-national Joint Conference on Artificial Intelligence, pages133?140.Martin Volk.
2001.
Exploiting the WWW as a corpus to re-solve PP attachment ambiguities.
In Proceedings of CorpusLinguistics, pages 601?606.Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997.Resolving PP attachment Ambiguities with Memory-BasedLearning.
In Proceedings of CoNLL-97, pages 136?144.Shaojun Zhao and Dekang Lin.
2004.
A Nearest-NeighborMethod for Resolving PP-Attachment Ambiguity.
In Pro-ceedings of IJCNLP-04.280
