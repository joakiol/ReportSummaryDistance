Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 90?99,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA practical and linguistically-motivated approachto compositional distributional semanticsDenis Paperno and Nghia The Pham and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)(denis.paperno|thenghia.pham|marco.baroni)@unitn.itAbstractDistributional semantic methods to ap-proximate word meaning with contextvectors have been very successful empir-ically, and the last years have seen a surgeof interest in their compositional exten-sion to phrases and sentences.
We presenthere a new model that, like those of Co-ecke et al (2010) and Baroni and Zam-parelli (2010), closely mimics the standardMontagovian semantic treatment of com-position in distributional terms.
However,our approach avoids a number of issuesthat have prevented the application of theearlier linguistically-motivated models tofull-fledged, real-life sentences.
We testthe model on a variety of empirical tasks,showing that it consistently outperforms aset of competitive rivals.1 Compositional distributional semanticsThe research of the last two decades has estab-lished empirically that distributional vectors forwords obtained from corpus statistics can be usedto represent word meaning in a variety of tasks(Turney and Pantel, 2010).
If distributional vec-tors encode certain aspects of word meaning, it isnatural to expect that similar aspects of sentencemeaning can also receive vector representations,obtained compositionally from word vectors.
De-veloping a practical model of compositionality isstill an open issue, which we address in this pa-per.
One approach is to use simple, parameter-free models that perform operations such as point-wise multiplication or summing (Mitchell and La-pata, 2008).
Such models turn out to be sur-prisingly effective in practice (Blacoe and Lap-ata, 2012), but they have obvious limitations.
Forinstance, symmetric operations like vector addi-tion are insensitive to syntactic structure, there-fore meaning differences encoded in word orderare lost in composition: pandas eat bamboo isidentical to bamboo eats pandas.
Guevara (2010),Mitchell and Lapata (2010), Socher et al (2011)and Zanzotto et al (2010) generalize the simpleadditive model by applying structure-encoding op-erators to the vectors of two sister nodes beforeaddition, thus breaking the inherent symmetry ofthe simple additive model.
A related approach(Socher et al, 2012) assumes richer lexical rep-resentations where each word is represented witha vector and a matrix that encodes its interactionwith its syntactic sister.
The training proposed inthis model estimates the parameters in a super-vised setting.
Despite positive empirical evalua-tion, this approach is hardly practical for general-purpose semantic language processing, since it re-quires computationally expensive approximate pa-rameter optimization techniques, and it assumestask-specific parameter learning whose results arenot meant to generalize across tasks.1.1 The lexical function modelNone of the proposals mentioned above, from sim-ple to elaborate, incorporates in its architecture theintuitive idea (standard in theoretical linguistics)that semantic composition is more than a weightedcombination of words.
Generally one of the com-ponents of a phrase, e.g., an adjective, acts asa function affecting the other component (e.g., anoun).
This underlying intuition, adopted fromformal semantics of natural language, motivatedthe creation of the lexical function model of com-position (lf ) (Baroni and Zamparelli, 2010; Co-ecke et al, 2010).
The lf model can be seen as aprojection of the symbolic Montagovian approachto semantic composition in natural language ontothe domain of vector spaces and linear operationson them (Baroni et al, 2013).
In lf, argumentsare vectors and functions taking arguments (e.g.,adjectives that combine with nouns) are tensors,with the number of arguments (n) determining the90order of tensor (n+1).
For example, adjectives, asunary functors, are modeled with 2-way tensors, ormatrices.
Tensor by vector multiplication formal-izes function application and serves as the generalcomposition method.Baroni and Zamparelli (2010) propose a practi-cal and empirically effective way to estimate ma-trices representing adjectival modifiers of nounsby linear regression from corpus-extracted exam-ples of noun and adjective-noun vectors.
Un-like the neural network approach of Socher etal.
(2011; 2012), the Baroni and Zamparellimethod does not require manually labeled data norcostly iterative estimation procedures, as it relieson automatically extracted phrase vectors and onthe analytical solution of the least-squares-errorproblem.The same method was later applied to matrixrepresentations of intransitive verbs and determin-ers (Bernardi et al, 2013; Dinu et al, 2013), al-ways with good empirical results.The full range of semantic types required fornatural language processing, including those ofadverbs and transitive verbs, has to include, how-ever, tensors of greater rank.
The estimationmethod originally proposed by Baroni and Zam-parelli has been extended to 3-way tensors rep-resenting transitive verbs by Grefenstette et al(2013) with preliminary success.
Grefenstette etal.
?s method works in two steps.
First, one esti-mates matrices of verb-object phrases from sub-ject and subject-verb-object vectors; next, transi-tive verb tensors are estimated from verb-objectmatrices and object vectors.1.2 Problems with the extension of the lexicalfunction model to sentencesWith all the advantages of lf, scaling it up to ar-bitrary sentences, however, leads to several issues.In particular, it is desirable for all practical pur-poses to limit representation size.
For example,if noun meanings are encoded in vectors of 300dimensions, adjectives become matrices of 3002cells, and transitive verbs are represented as ten-sors with 3003=27, 000, 000 dimensions.Estimating tensors of this size runs into datasparseness issues already for less common tran-sitive verbs.
Indeed, in order to train a transitiveverb tensor (e.g., eat), the method of Grefenstetteet al (2013) requires a sufficient number of dis-tinct verb object phrases with that verb (e.g., eatcake, eat fruits), each attested in combination witha certain number of subject nouns with sufficientfrequency to extract sensible vectors.
It is not fea-sible to obtain enough data points for all verbs insuch a training design.Things get even worse for other categories.Adverbs like quickly that modify intransitiveverbs have to be represented with 30022=8, 100, 000, 000 dimensions.
Modifiers of transi-tive verbs would have even greater representationsize, which may not be possible to store and learnefficiently.Another issue is that the same or similar itemsthat occur in different syntactic contexts are as-signed different semantic types with incompara-ble representations.
For example, verbs like eatcan be used in transitive or intransitive construc-tions (children eat meat/children eat), or in passive(meat is eaten).
Since predicate arity is encodedin the order of the corresponding tensor, eat andthe like have to be assigned different representa-tions (matrix or tensor) depending on the context.Deverbal nouns like demolition, often used with-out mention of who demolished what, would haveto get vector representations while the correspond-ing verbs (demolish) would become tensors, whichmakes immediately related verbs and nouns in-comparable.
Nouns in general would oscillate be-tween vector and matrix representations depend-ing on argument vs. predicate vs. modifier posi-tion (an animal runs vs. this is an animal vs. an-imal shelter).
Prepositions are the hardest, as thesyntactic positions in which they occur are mostdiverse (park in the dark vs. play in the dark vs.be in the dark vs. a light glowing in the dark).In all those cases, the same word has to bemapped to tensors of different orders.
Since eachof these tensors must be learned from examplesindividually, their obvious relation is missed.
Be-sides losing the comparability of the semantic con-tribution of a word across syntactic contexts, wealso worsen the data sparseness issues.The last, and related, point is that for the ten-sor calculus to work, one needs to model, for eachword, each of the constructions in the corpus thatthe word is attested in.
In its pure form lf doesnot include an emergency backoff strategy whenunknown words or constructions are encountered.For example, if we only observe transitive usagesof to eat in the training corpus, and encounter anintransitive or passive example of it in testing data,91the system would not be able to compose a sen-tence vector at all.
This issue is unavoidable sincewe don?t expect to find all words in all possibleconstructions even in the largest corpus.2 The practical lexical function modelAs follows from section 1.2, it would be desirableto have a compositional distributional model thatencodes function-argument relations but avoidsthe troublesome high-order tensor representationsof the pure lexical function model, with all thepractical problems that come with them.
We maystill want to represent word meanings in differ-ent syntactic contexts differently, but at the sametime we need to incorporate a formal connectionbetween those representations, e.g., between thetransitive and the intransitive instantiations of theverb to eat.
Last but not least, all items need toinclude a common aspect of their representation(e.g., a vector) to allow comparison across cate-gories (the case of demolish and demolition).To this end, we propose a new model of compo-sition that maintains the idea of function applica-tion, while avoiding the complications and rigidityof lf.
We call our proposal practical lexical func-tion model, or plf.
In plf, a functional word is notrepresented by a single tensor of arity-dependentorder, but by a vector plus an ordered set of matri-ces, with one matrix for each argument the func-tion takes.
After applying the matrices to the cor-responding argument vectors, a single representa-tion is obtained by summing across all resultingvectors.2.1 Word meaning representationIn plf, all words are represented by a vector, andfunctional words, such as predicates and modi-fiers, are also assigned one or more matrices.
Thegeneral form of a semantic representation for alinguistic unit is an ordered tuple of a vector andn ?
N matrices:1?~x,21x , .
.
.
,2nx?The number of matrices in the representationencodes the arity of a linguistic unit, i.e., the num-ber of other units to which it applies as a function.Each matrix corresponds to a function-argumentrelation, and words have as many matrices asmany arguments they take: none for (most) nouns,1Matrices associated with term x are symbolized2x.dog~dogrun ~run,2runchase~chase,2schase,2ochasegive~give,2sgive,2ogive,2iogivebig~big,2bigvery ~very,2nvery,2averyquickly~quickly,2squickly,2vquicklyTable 1: Examples of word representations.
Sub-scripts encode, just for mnemonic purposes, theconstituent whose vector the matrix combineswith: subject, object, indirect object, noun,adjective, verb phrase.one for adjectives and intransitive verbs, two fortransitives, etc.
The matrices formalize argumentslot saturation, operating on an argument vectorrepresentation through matrix by vector multipli-cation, as described in the next section.Modifiers of n-ary functors are represented byn+1-ary structures.
For instance, we treat adjec-tives that modify nouns (0-ary) as unary functions,encoded in a vector-matrix pair.
Adverbs have dif-ferent semantic types depending on their syntac-tic role.
Sentential adverbs are unary, while ad-verbs that modify adjectives (very) or verb phrases(quickly) are encoded as binary functions, repre-sented by a vector and two matrices.
The form ofsemantic representations we are using is shown inTable 1.22.2 Semantic compositionOur system incorporates semantic composition viatwo composition rules, one for combining struc-tures of different arity and the other for symmet-ric composition of structures with the same ar-ity.
These rules incorporate insights of two em-pirically successful models, lexical function andthe simple additive approach, used as the defaultstructure merging strategy.The first rule is function application, illustratedin Figure 1.
Table 2 illustrates simple cases offunction application.
For transitive verbs seman-tic composition applies iteratively as shown in thederivation of Figure 2.
For ternary predicates such2To determine the number and ordering of matrices rep-resenting the word in the current syntactic context, our plfimplementation relies on the syntactic type assigned to theword in the categorial grammar parse of the sentence.92?~x+2n+kx ?
~y,21x +21y , .
.
.
,2nx +2ny , .
.
.?
?~x,21x , .
.
.
,2nx , .
.
.
,2n+kx?
?~y,21y , .
.
.
,2ny?Figure 1: Function application: If two syntacticsisters have different arity, treat the higher-aritysister as the functor.
Compose by multiplying thelast matrix in the functor tuple by the argumentvector and summing the result to the functor vec-tor.
Unsaturated matrices are carried up to thecomposed node, summing across sisters if needed.dogs~dogsrun ~run,2rundogs run ~run +2run?~doghouse~housebig~big,2bigbig house~big +2big ?~houseTable 2: Examples of function application.as give in a ditransitive construction, the first stepin the derivation absorbs the innermost argumentby multiplying its vector by the third give matrix,and then composition proceeds like for transitives.The second composition rule, symmetric com-position applies when two syntactic sisters are ofthe same arity (e.g., two vectors, or two vector-matrix pairs).
Symmetric composition simplysums the objects in the two tuples: vector withvector, n-th matrix with n-th matrix.Symmetric composition is reserved for struc-tures in which the function-argument distinctionis problematic.
Some candidates for such treat-ment are coordination and nominal compounds,although we recognize that the headless analysis is2schase?~dogs+~chase+2ochase?~cats~dogs?~chase+2ochase?~cats,2schase?
?~chase,2schase,2ochase?~catsFigure 2: Applying function application twice toderive the representation of a transitive sentence.sing:~sing,2sing dance:~dance,2dancesing and dance:~sing +~dance,2sing +2dancerice:~rice cake:~cakerice cake~rice+~cakeTable 3: Examples of symmetric composition.not the only possible one here.
See two examplesof Symmetric Composition application in Table 3.Note that the sing and dance composition in Ta-ble 3 skips the conjunction.
Our current plf im-plementation treats most grammatical words, in-cluding conjunctions, as ?empty?
elements, thatdo not project into semantics.
This choice leadsto some interesting ?serendipitous?
treatments ofvarious constructions.
For example, since the cop-ula is empty, a sentence with a predicative adjec-tive (cars are red) is treated in the same way as aphrase with the same adjective in attributive posi-tion (red cars) ?
although the latter, being a phraseand not a full sentence, will later be embedded asargument in a larger construction.
Similarly, leav-ing the relative pronoun empty makes cars thatrun identical to cars run, although, again, the for-mer will be embedded in a larger construction laterin the derivation.We conclude our brief exposition of plf with analternative intuition for it: the plf model is alsoa more sophisticated version of the additive ap-proach, where argument words are adapted by ma-trices that encode the relation to their functors be-fore the sentence vector is derived by summing.2.3 Satisfying the desiderataLet us now outline how plf addresses the short-comings of lf listed in Section 1.2.
First, all is-sues caused by representation size disappear.
Ann-ary predicate is no longer encoded as an n+1-way tensor; instead we have a sequence of n ma-trices.
The representation size grows linearly, notexponentially, for higher semantic types, allowingfor simpler and more efficient parameter estima-tion, storage, and computation.As a consequence of our architecture, we nolonger need to perform the complicated step-by-step estimation for elements of higher arity.
In-deed, one can estimate each matrix of a com-plex representation individually using the simplemethod of Baroni and Zamparelli (2010).
For in-stance, for transitive verbs we estimate the verb-subject combination matrix from subject and verb-93boys~boyseat (intrans.
)~eat,2seatboys eat2seat?~boys+~eatmeat~meateat (trans.
)~eat,2seat,2oeatboys eat meat2seat?~boys+~eat+2oeat?~meat(is) eaten (pass.
)~eat,2oeatmeat is eaten~eat+2oeat?~meatTable 4: The verb to eat associated to different setsof matrices in different syntactic contexts.subject vectors, the verb-object combination ma-trix from object and verb-object vectors.
We ex-pect a reasonably large corpus to feature many oc-currences of a verb with a variety of subjects anda variety of objects (but not necessarily a varietyof subjects with each of the objects as required byGrefenstette et al?s training), allowing us to avoidthe data sparseness issue.The semantic representations we propose in-clude a semantic vector for constituents of any se-mantic type, thus enabling semantic comparisonfor words of different parts of speech (the case ofdemolition vs. demolish).Finally, the fact that we represent the predicateinteraction with each of its arguments in a sepa-rate matrix allows for a natural and intuitive treat-ment of argument alternations.
For instance, asshown in Table 4, one can distinguish the transi-tive and intransitive usages of the verb to eat bythe presence of the object-oriented matrix of theverb while keeping the rest of the representationintact.
To model passive usages, we insert the ob-ject matrix of the verb only, which will be multi-plied by the syntactic subject vector, capturing thesimilarity between eat meat and meat is eaten.So keeping the verb?s interaction with subjectand object encoded in distinct matrices not onlysolves the issues of representation size for arbi-trary semantic types, but also provides a sensiblebuilt-in strategy for handling a word?s occurrencein multiple constructions.
Indeed, if we encountera verb used intransitively which was only attestedas transitive in the training corpus, we can simplyomit the object matrix to obtain a type-appropriaterepresentation.
On the other hand, if the verb oc-curs with more arguments than usual in testingmaterials, we can add a default diagonal identitymatrix to its representation, signaling agnosticismabout how the verb relates to the unexpected argu-ment.
This flexibility makes our model suitable tocompute vector representations of sentences with-out stumbling at unseen syntactic usages of words.To summarize, plf is an extension of the lexi-cal function model that inherits its strengths andovercomes its weaknesses.
We still employ alinguistically-motivated notion of semantic com-position as function application and use distinctkinds of representations for different semantictypes.
At the same time, we avoid high order ten-sor representations, produce semantic vectors forall syntactic constituents, and allow for an elegantand transparent correspondence between differentsyntactic usages of a lexeme, such as the transi-tive, the intransitive, and the passive usages of theverb to eat.
Last but not least, our implementationis suitable for realistic language processing sinceit allows to produce vectors for sentences of arbi-trary size, including those containing novel syn-tactic configurations.3 Evaluation3.1 Evaluation materialsWe consider 5 different benchmarks that focus ondifferent aspects of sentence-level semantic com-position.
The first data set, created by EdwardGrefenstette and Mehrnoosh Sadrzadeh and in-troduced in Kartsaklis et al (2013), features 200sentence pairs that were rated for similarity by43 annotators.
In this data set, sentences havefixed adjective-noun-verb-adjective-noun (anvan)structure, and they were built in order to cru-cially require context-based verb disambiguation(e.g., young woman filed long nails is paired withboth young woman smoothed long nails and youngwoman registered long nails).
We also consider asimilar data set introduced by Grefenstette (2013),comprising 200 sentence pairs rated by 50 anno-tators.
We will call these benchmarks anvan1 andanvan2, respectively.
Evaluation is carried out bycomputing the Spearman correlation between theannotator similarity ratings for the sentence pairsand the cosines of the vectors produced by the var-ious systems for the same sentence pairs.The benchmark introduced by The Pham et al(2013) at the TFDS workshop (tfds below) wasspecifically designed to test compositional meth-ods for their sensitivity to word order and the se-mantic effect of determiners.
The tfds benchmarkcontains 157 target sentences that are matchedwith a set of (approximate) paraphrases (8 on av-94erage), and a set of ?foils?
(17 on average).
Thefoils have high lexical overlap with the targets butvery different meanings, due to different determin-ers and/or word order.
For example, the targetA man plays an acoustic guitar is matched withparaphrases such as A man plays guitar and Theman plays the guitar, and foils such as The manplays no guitar and A guitar plays a man.
Agood system should return higher similarities forthe comparison with the paraphrases with respectto that with the foils.
Performance is assessedthrough the t-standardized cross-target average ofthe difference between mean cosine with para-phrases and mean cosine with foils (Pham and col-leagues, equivalently, reported non-standardizedaverage and standard deviations).The two remaining data sets are larger and more?natural?, as they were not constructed by linguistsunder controlled conditions to focus on specificphenomena.
They are aimed at evaluating sys-tems on the sort of free-form sentences one en-counters in real-life applications.
The msrvid dataset from the SemEval-2012 Semantic Textual Sim-ilarity (STS) task (Agirre et al, 2012) consists of750 sentence pairs that describe brief videos.
Sen-tence pairs were scored for similarity by 5 subjectseach.
Following standard practice in paraphrasedetection studies (e.g., Blacoe and Lapata (2012)),we use cosine similarity between sentence pairs ascomputed by one of our systems together with twoshallow similarity cues: word overlap between thetwo sentences and difference in sentence length.We obtain a final similarity score by weighted ad-dition of the 3 cues, with the optimal weights de-termined by linear regression on separate msrvidtrain data that were also provided by the SemEvaltask organizers (before combining, we checkedthat the collinearity between cues was low).
Sys-tem scores are evaluated by their Pearson correla-tion with the human ratings.The final set we use is onwn, from the *SEM-2013 STS shared task (Agirre et al, 2013).
Thisset contains 561 pairs of glosses (from the Word-Net and OntoNotes databases), rated by 5 judgesfor similarity.
Our main interest in this set stemsfrom the fact that glosses are rarely well-formedfull sentences (consider, e.g., cause something topass or lead somewhere; coerce by violence, fillwith terror).
For this reason, they are very chal-lenging for standard parsers.
Indeed, we estimatedfrom a sample of 40 onwn glosses that the C&Cparser (see below) has only 45% accuracy on thisset.
Since plf needs syntactic information to con-struct sentence vectors compositionally, we test iton onwn to make sure that it is not overly sensi-tive to parser noise.
Evaluation proceeds as withmsrvid (cue weights are determined by 10-foldcross-validation).33.2 Semantic space construction andcomposition model implementationOur source corpus was given by the concatena-tion of ukWaC (wacky.sslmit.unibo.it),a mid-2009 dump of the English Wikipedia (en.wikipedia.org) and the British National Cor-pus (www.natcorp.ox.ac.uk), for a total ofabout 2.8 billion words.We collected a 30K-by-30K matrix by countingco-occurrence of the 30K most frequent contentlemmas (nouns, adjectives and verbs) within a 3-word window.
The raw count vectors were trans-formed into positive Pointwise Mutual Informa-tion scores and reduced to 300 dimensions by theSingular Value Decomposition.
All vectors werenormalized to length 1.
This setup was pickedwithout tuning, as we found it effective in previ-ous, unrelated experiments.4We consider four composition models.
The add(additive) model produces the vector of a sentenceby summing the vectors of all content words in it.Similarly, mult uses component-wise multiplica-tion of vectors for composition.
While these mod-els are very simple, a long experimental traditionhas proven their effectiveness (Landauer and Du-mais, 1997; Mitchell and Lapata, 2008; Mitchelland Lapata, 2010; Blacoe and Lapata, 2012).For the lf (lexical function) model, we constructfunctional matrix representations of adjectives, de-terminers and intransitive verbs.
These are trainedusing Ridge regression with generalized cross-validation from corpus-extracted vectors of nouns,3We did not evaluate on other STS benchmarks since theyhave characteristics, such as high density of named entities,that would require embedding our compositional models intomore complex systems, obfuscating their impact on the over-all performance.4With the multiplicative composition model we also triedNonnegative Matrix Factorization instead of Singular ValueDecomposition, because the negative values produced bySVD are potentially problematic for mult.
In addition, we re-peated the evaluation for the multiplicative and additive mod-els without any form of dimensionality reduction.
The over-all pattern of results did not change significantly, and thus forconsistency we report all models?
performance only for theSVD-reduced space.95as input, and phrases including those nouns as out-put (e.g., the matrix for red is trained from corpus-extracted ?noun, red-noun?
vector pairs).
Transi-tive verb tensors are estimated using the two-stepregression procedure outlined by Grefenstette etal.
(2013).
We did not attempt to train a lf modelfor the larger and more varied msrvid and onwndata sets, as this would have been extremely timeconsuming and impractical for all the reasons wediscussed in Section 1.2 above.Training plf (practical lexical function) pro-ceeds similarly, but we also build prepositionmatrices (from ?noun, preposition-noun?
vectorpairs), and for verbs we prepare separate subjectand object matrices.Since syntax guides lf and plf composition, wesupplied all test sentences with categorial gram-mar parses.
Every sentence in the anvan1 andanvan2 datasets has the form (subject) Adjective+ Noun + Transitive Verb + (object) Adjective +Noun, so parsing them is trivial.
All sentences intfds have a predictable structure that allows per-fect parsing with simple finite state rules.
In allthese cases, applying a general-purpose parser tothe data would have, at best, had no impact and,at worst, introduced parsing errors.
For msrvidand onwn, we used the output of the C&C parser(Clark and Curran, 2007).3.3 ResultsTable 5 summarizes the performance of our mod-els on the chosen tasks, and compares it to the stateof the art reported in previous work, as well as tovarious strong baselines.The plf model performs very well on both an-van benchmarks, outperforming not only add andmult, but also the full-fledged lf model.
Giventhat these data sets contain, systematically, transi-tive verbs, the major difference between plf and lflies in their representation of the latter.
Evidently,the separately-trained subject and object matricesof plf, being less affected by data sparseness thanthe 3-way tensors of lf, are better able to capturehow verbs interact with their arguments.
For an-van1, plf is just below the state of the art, whichis based on disambiguating the verb vector in con-text (Kartsaklis and Sadrzadeh, 2013), and lf out-performs the baseline, which consists in using theverb vector only as a proxy to sentence similar-ity.5On anvan2, plf outperforms the best model5We report state of the art from Kartsaklis and Sadrzadehmodels anvan anvan tfds msr onwn1 2 vidadd 8 22 -0.2 78 66mult 8 -4 -2.3 77 55lf 15 30 5.90 NA NAplf 20 36 2.7 79 67soa 22 27 11.4 87 75baseline 8 22 7.9 77 55Table 5: Performance of composition models onall evaluation sets.
Figures of merit follow previ-ous art on each set and are: percentage Spearmancoefficients for anvan1 and anvan2, t-standardizedaverage difference between mean cosines withparaphrases and with foils for tfds, percentagePearson coefficients for msrvid and onwn.
State-of-the-art (soa) references: anvan1: Kartsaklis andSadrzadeh (2013); anvan2: Grefenstette (2013);tfds: The Pham et al (2013); msrvid: B?ar etal.
(2012); onwn: Han et al (2013).
Baselines:anvan1/anvan2: verb vectors only; tfds: wordoverlap; msrvid/onwn: word overlap + sentencelength.reported by Grefenstette (2013) (an implementa-tion of the lexical function ideas along the lines ofGrefenstette and Sadrzadeh (2011a; 2011b)).
Andlf is, again, the only model, besides plf, that per-forms better than the baseline.In the tfds task, not surprisingly the add andmult models, lacking determiner representationsand being order-insensitive, fail to distinguish be-tween true paraphrases and foils (indeed, for themult model foils are significantly closer to the tar-gets than the paraphrases, probably because thelatter have lower content word overlap than thefoils, that often differ in word order and determin-ers only).
Our plf approach is able to handle deter-miners and word order correctly, as demonstratedby a highly significant (p < 0.01) difference be-tween paraphrase and foil similarity (average dif-ference in cosine .017, standard deviation .077).
Inthis case, however, the traditional lf model (aver-age difference .044, standard deviation .092) out-performs plf.
Since determiners are handled iden-tically under the two approaches, the culprit mustbe word order.
We conjecture that the lf 3-waytensor representation of transitive verbs leads toa stronger asymmetry between sentences with in-(2013) rather than Kartsaklis et al (2013), since only the for-mer used a source corpus that is comparable to ours.96verted arguments, and thus makes this model par-ticularly sensitive to word order differences.
In-deed, if we limit evaluation to those foils charac-terized by word order changes only, lf discrim-inates between paraphrases and foils even moreclearly, whereas the plf difference, while still sig-nificant, decreases slightly.The state-of-the-art row for tfds reports the lfimplementation by The Pham et al (2013), whichoutperforms ours.
The main difference is thatPham and colleagues do not normalize vectors likewe do.
If we don?t normalize, we do get larger dif-ferences for our models as well, but consistentlylower performance in all other tasks.
More wor-ryingly, the simple word overlap baseline reportedin the table sports a larger difference than our bestmodel.
Clearly, this baseline is exploiting the sys-tematic determiner differences in the foils and, in-deed, when it is evaluated on foils where onlyword order changes its performance is no longersignificant.On msrvid, the plf approach outperforms addand mult, although the difference between thethree is not big.
Our result stands in contrast withBlacoe and Lapata (2012), the only study we areaware of that compared a sophisticated composi-tion model (Socher et al?s 2011 model) to addand mult on realistic sentences, which attained thetop performance with the simple models for bothfigures of merit they used.6The best 2012 STSsystem (B?ar et al, 2012), obtained 0.87 correla-tion, but with many more and considerably morecomplex features than the ones we used here.
In-deed, our simple system would have obtained a re-spectable 25/89 ranking in the STS 2012 msrvidtask.
Still, we must also stress the impressive per-formance of our baseline, given by the combina-tion of the word overlap and sentence length cues.This suggests that the msrvid benchmark lacks thelexical and syntactic variety we would like to testour systems on.Our plf model is again the best on the onwnset (albeit by a small margin over add).
Thisis a very positive result, in the light of the factthat the parser has very low performance on theonwn glosses, thus suggesting that plf can pro-duce sensible semantic vectors from noisy syntac-6We refer here to the results reported in the er-ratum available at http://homepages.inf.ed.ac.uk/s1066731/pdf/emnlp2012erratum.pdf.
Theadd/mult advantage was even more marked in the original pa-per.tic representations.
Here the overlap+length base-line does not perform so well, and again the bestSTS 2013 system (Han et al, 2013) uses consider-ably richer knowledge sources and algorithms thanours.
Our plf-based method would have reached arespectable 20/90 rank in the STS 2013 onwn task.As a final remark, in all experiments the runningtime of plf was only slightly larger than for thesimpler models, but orders of magnitude smallerthan lf, confirming another practical side of ourapproach.4 ConclusionWe introduced an approach to compositional dis-tributional semantics based on a linguistically-motivated syntax-to-semantics type mapping, butsimple and flexible enough that it can produce rep-resentations of English sentences of arbitrary sizeand structure.We showed that our approach is competitiveagainst the more complex lexical function modelwhen evaluated on the simple constructions thelatter can be applied to, and it outperforms the ad-ditive and multiplicative compositionality modelswhen tested on more realistic benchmarks (wherethe full-fledged lexical function approach is dif-ficult or impossible to use), even in presence ofstrong noise in its syntactic input.
While our re-sults are encouraging, no current benchmark com-bines large-scale, real-life data with the syntacticvariety on which a syntax-driven approach to se-mantics such as ours could truly prove its worth.The recently announced SemEval 2014 Task 17isfilling exactly this gap, and we look forward to ap-ply our method to this new benchmark, as soon asit becomes available.One of the strengths of our framework is thatit allows for incremental improvement focused onspecific constructions.
For example, one could addrepresentations for different conjunctions (and vs.or), train matrices for verb arguments other thansubject and direct object, or include new types ofmodifiers into the model, etc.While there is potential for local improvements,our framework, which extends and improves onexisting compositional semantic vector models,has demonstrated its ability to account for full sen-tences in a principled and elegant way.
Our imple-mentation of the model relies on simple and effi-7http://alt.qcri.org/semeval2014/task1/97cient training, works fast, and shows good empiri-cal results.AcknowledgementsWe thank Roberto Zamparelli and the COM-POSES team for helpful discussions.
This re-search was supported by the ERC 2011 StartingIndependent Research Grant n. 283554 (COM-POSES).ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: apilot on semantic textual similarity.
In Proceedingsof *SEM, pages 385?393, Montreal, Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic Textual Similarity.
In Proceedingsof *SEM, pages 32?43, Atlanta, GA.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
UKP: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of *SEM, pages435?440, Montreal, Canada.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Marco Baroni, Raffaella Bernardi, and Roberto Zam-parelli.
2013.
Frege in space: A program forcompositional distributional semantics.
LinguisticIssues in Language Technology.
In press; http://clic.cimec.unitn.it/composes/materials/frege-in-space.pdf.Raffaella Bernardi, Georgiana Dinu, Marco Marelli,and Marco Baroni.
2013.
A relatedness benchmarkto test the role of determiners in compositional dis-tributional semantics.
In Proceedings of ACL (ShortPapers), pages 53?57, Sofia, Bulgaria.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.Stephen Clark and James Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of com-positional distributional semantic models.
In Pro-ceedings of ACL Workshop on Continuous VectorSpace Models and their Compositionality, pages 50?58, Sofia, Bulgaria.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011a.
Experimental support for a categorical com-positional distributional model of meaning.
In Pro-ceedings of EMNLP, pages 1394?1404, Edinburgh,UK.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011b.
Experimenting with transitive verbs in a Dis-CoCat.
In Proceedings of GEMS, pages 62?66, Ed-inburgh, UK.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
In Proceedings ofIWCS, pages 131?142, Potsdam, Germany.Edward Grefenstette.
2013.
Category-TheoreticQuantitative Compositional Distributional Modelsof Natural Language Semantics.
PhD thesis, Uni-versity of Oxford Essex.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.Lushan Han, Abhay Kashyap, Tim Finin,James Mayfield, and Jonathan Weese.
2013.UMBC EBIQUITY-CORE: Semantic textual sim-ilarity systems.
In Proceedings of *SEM, pages44?52, Atlanta, GA.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.Prior disambiguation of word tensors for construct-ing sentence vectors.
In Proceedings of EMNLP,pages 1590?1601, Seattle, WA.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman.
2013.
Separating disambiguation fromcomposition in distributional semantics.
In Pro-ceedings of CoNLL, pages 114?123, Sofia, Bulgaria.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.98Richard Socher, Eric Huang, Jeffrey Pennin, AndrewNg, and Christopher Manning.
2011.
Dynamicpooling and unfolding recursive autoencoders forparaphrase detection.
In Proceedings of NIPS, pages801?809, Granada, Spain.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Nghia The Pham, Raffaella Bernardi, Yao-ZhongZhang, and Marco Baroni.
2013.
Sentence para-phrase detection: When determiners and word or-der make the difference.
In Proceedings of the To-wards a Formal Distributional Semantics Workshopat IWCS 2013, pages 21?29, Potsdam, Germany.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.99
