Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 539?542, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticsbwbaugh : Hierarchical sentiment analysis with partial self-trainingWesley BaughDepartment of Computer ScienceUniversity of North Texasbrianbaugh@my.unt.eduAbstractUsing labeled Twitter training data fromSemEval-2013, we train both a subjectivityclassifier and a polarity classifier separately,and then combine the two into a single hier-archical classifier.
Using additional unlabeleddata that is believed to contain sentiment, weallow the polarity classifier to continue learn-ing using self-training.
The resulting system iscapable of classifying a document as neutral,positive, or negative with an overall accuracyof 61.2% using our hierarchical Naive Bayesclassifier.11 IntroductionMany people use social networks, such as Twitter,to connect and communicate with others.
Users ofsocial networks often share their experiences, suchas watching a recent movie or tv show, reading abook, or a newly tried product or service.
In addi-tion, social networks provide an avenue for discus-sion of current events, such as politics.
Many peopleand companies are often concerned with how othersperceive their product?which is sometimes them-selves, as is the case for politicians?or their service.By understanding and reacting to what the consumeris thinking, they can attempt to maximize their goodpress as well as to help minimize the bad.
It wouldtherefore be useful to use the information users ofsocial networks share to perform sentiment analysisin order to understand how people perceive targetsof interest.1 A working demo of the system will be available for a shorttime at: http://infertweet.bwbaugh.comIn general, sentiment analysis often involves theuse of machine learning, especially Naive Bayes,SVM, and MaxEnt classifiers [Jose].
Features gen-eral include n-grams and POS tags [Go et al 2009;Pak and Paroubek, 2010; Jose], as well as senti-ment lexicons [Jose].
Go et al[2009] achievedaround 82.5% accuracy for positive-negative polar-ity detection, Jose achieved around 76% accuracyfor subjective-objective classification, and Pak andParoubek [2010] achieved around 70% accuracy fora combined subjectivity-polarity classifier.While determining whether a document known tobe subjective is positive or negative (polarity detec-tion) is relatively easy, a currently more difficult taskin sentiment analysis is identifying whether a docu-ment is subjective or objective (subjectivity analy-sis).
Many approaches simply ignore the objectiveclass [Go et al 2009], which does not work for realworld problems as there are a substantial amount ofdocuments that are either partially or wholly objec-tive [Koppel and Schler, 2006].Many previous methods focus on either subjectiv-ity analysis or polarity detection.
Our method incor-porates both subtasks into a single overall system inorder to perform sentiment analysis.2 BackgroundThe sentiment analysis in Twitter task of SemEval-2013 [Wilson et al 2013] provides 9,864 la-beled tweets from Twitter to be used as a train-ing dataset.
Each instance is labeled as eitherpositive, negative, or neutral, and wasannotated through Amazon?s Mechanical Turk.
Theterms of service for Twitter puts restrictions on the539type of data that may be re-released, therefore par-ticipants SemEval-2013 Task 2 participants wererequired to download tweets directly from Twit-ter.
Due to deleted or otherwise unavailable tweets,this system was only able to download approxi-mately 8,750 training instances.
Additionally, adevelopment dataset was provided with 1,654 la-beled tweets, of which 340 are negative, 739 areneutral, and 575 are positive.
The providedtest set consisted of 3,813 instances, of which 601are negative, 1640 are neutral, and 1572 arepositive.In related work, Go et al[2009] generated an au-tomatically labeled noisy gold standard by search-ing for tweets that contained one of several emoti-cons2 (e.g.
:) or :() that were mapped to ei-ther the positive or negative class depend-ing on the type of emoticon in the text.
This sys-tem also collected approximately one million tweetsusing emoticons as a keyword search for match-ing, however the data remained unlabeled.
Thoughthese tweets are unlabeled, they are presumed to besubjective?either positive or negative butnot neutral?because of the intuitive associationof emoticons with sentiment.3 ApproachThe system uses a custom implementation of Multi-nomial Naive Bayes as the classifier.3 We createa hierarchical classifier, which in this case consistsof two binary classifiers.
The first-level is the sub-jectivity classifier, which can output objective(neutral) or subjective.
If the output of the firstlevel is subjective, then the second-level polar-ity classifier decides if the instance is positive ornegative.Both classifiers (subjective and polarity) aretrained on approximately 8,750 training instances,which come from the released SemEval-2013 train-ing dataset.
The subjective classifier is not given any2The term emoticon comes from a blending of the words?emotion?
and ?icon?.3 The machine learning components (Multinomial NaiveBayes) were written for this system as a Python library,and will be available on GitHub: https://github.com/bwbaugh/infer.
That toolkit was then used as a founda-tion for writing the code for the system, which will also beavailable on GitHub: https://github.com/bwbaugh/infertweet.Figure 1: A single multinomial classifier, which can out-put any class label.Figure 2: A hierarchical classifier, which in this case con-sists of two binary classifiers.
The first level is a sub-jectivity classifier, with an output of either subjective orneutral.
The second level is a sentiment polarity classi-fier, with an output of either positive or negative.additional training data.
The system then uses itscurrent model to classify approximately one millionunlabeled tweets that are believed to be subjective.The unlabeled tweets were classified one at a time.If the system classified the tweet as subjective, it wasused to train the polarity classifier only if the confi-dence in the predicted label was greater than 0.8.We stopped the system after approximately 910k to-tal training instances were used.The core features extracted are unigrams and bi-grams.
Bigrams had an additional start andend token at the beginning and end of the fulltext of the training instance.As part of a preprocessing step, we attempted tofind URLs in the text and replace them with a specialURL token.
We shortened characters repeatedmore than twice, such that ?haaaaaaate?
would be-come ?haate?.
We attempted to find dates in the textand replace them with a special DATE token.540training instances0.10.20.30.40.50.60.70.80.91.0performanceSingle ClassifierSemEval Positive Negative Neutral Accuracy103 104 105 106training instances0.10.20.30.40.50.60.70.80.91.0performanceHierarchical ClassifierSemEval Positive Negative Neutral AccuracyFigure 3: Performance of the single (non-hierarchical) and hierarchical classifiers on the development set vs. thenumber of training instances.
The performance metric for positive, negative and neutral is F-measure, SemEval is thesimple average of the positive and negative performance, and accuracy is the overall number of correct instances.
Thefirst 8,750 instances are labeled, while the rest are unlabeled instances that were added using self-training.4 Experiments4.1 DesignThe system was incrementally trained one tweet ata time, with the performance checked every so oftenby using the current model to classify the develop-ment set instances.
Once all of the labeled trainingdata had been used, the subjectivity classifier wasgiven no additional training instances, and the re-mainder of the subjectively charged unlabeled datawas used to train the polarity classifier.Variables experimented on included: extractingn-grams up to size 4 and trying all combinations;mapping Twitter usernames to a special token; map-ping substrings recognized as a date to a special to-ken; combining a negation token such as ?not?
to thefollowing token; deleting characters repeated morethan twice; mapping numbers to a special token;counting exclamation points; the confidence thresh-old above which the predicted label for an unlabeledinstance would be used for training.In addition to collecting unlabeled data usingemoticon keywords, we also experimented with us-ing sentences from Wikipedia as neutrally labeledtext, as well as using a random subsample of allEnglish-language tweets from the Twitter publicstream as a source of unlabeled data for any class.We also tried using a single non-hierarchical clas-sifier using each source of unlabeled data.4.2 Results4.2.1 SemEval-2013 development setUsing additional unlabeled data with the singlemultinomial classifier always resulted in overall de-541graded performance.Neg Neu PosNegNeuPos3 4 3339 36 6945 9 561Single ClassifierNeg Neu PosNegNeuPos208 71 61143 420 17687 135 353Hierarchical Classifier0.00.10.20.30.40.50.60.70.80.91.00.00.10.20.30.40.50.60.70.80.91.0Figure 4: The confusion matrix on the developmentset produced after training on a total of approximately970k training instances.
Rows are the true labels whilecolumns are the predicted labels.4.2.2 SemEval-2013 test setgs \ pred negative neutral positivenegative 324 203 74neutral 196 1168 276positive 233 498 841Table 1: Confusion matrix (hierarchical)class prec recall fscorenegative 0.4303 0.5391 0.4786neutral 0.6249 0.7122 0.6657positive 0.7061 0.5350 0.6088Table 2: Performance (hierarchical)The average F-score of the positive andnegative classes is 0.5437, which is the mainevaluation metric used by SemEval-2013 Task 2.The overall accuracy is 61.2%.4.2.3 DiscussionBy using a hierarchical classifier, we are able toprevent degradation of the performance of the clas-sifier on neutrally labeled instances by only applyingadditional training data to the polarity classifier.The use of additional unlabeled data results in anincrease in performance for the hierarchical classi-fier as seen in Figure 3.
However, the increase inperformance comes with an exponential increase inthe number of unlabeled instances.
Using appropri-ate feature selection for online algorithms, such asfeature hashing, a system like this could train indefi-nitely on additional data from a Twitter stream with-out running out of memory.The system?s lack of high-quality sources for ad-ditional objective-OR-neutral data?eitherlabeled or unlabeled?appears to be our biggest ob-stacle to increasing performance at this time.
Thepoor performance of the single multinomial classi-fier when given additional unlabeled data can alsolikely be attributed to this reason.
Identifying ad-ditional high-quality sources of neutral data wouldlikely go a long way towards improving the over-all system performance.
Active learning approachescould also be applied with the goal of improving thesubjectivity classifier.5 ConclusionUsing a hierarchical classifier comprised of twoNaive Bayes classifiers, we are able to improve theperformance of polarity detection with the additionof unlabeled data in an online setting by isolating thesubjectivity classifier.ReferencesAlec Go, Richa Bhayani, and Lei Huang.
Twit-ter sentiment classification using distant supervi-sion.
CS224N Project Report, Stanford, pages 1?12, 2009.Anthony K Jose.
Twitter sentiment analysis.Moshe Koppel and Jonathan Schler.
The importanceof neutral examples for learning sentiment.
Com-putational Intelligence, 22(2):100?109, 2006.Alexander Pak and Patrick Paroubek.
Twitter as acorpus for sentiment analysis and opinion mining.In Proceedings of LREC, volume 2010, 2010.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Alan Ritter, Sara Rosenthal, and Veselin Stoy-anov.
Semeval-2013 task 2: Sentiment analysisin twitter.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation, 2013.542
