A Stochastic Language Model using Dependencyand Its Improvement by Word ClusteringSh insuke  Mor i  *Tokyo  Research  Labo la tory ,IBM Japan ,  L td .1623-14  Sh imotsurumaYamatosh i ,  JapanMakoto NagaoKyoto UniversityYoshida-honmachi SakyoKyoto, JapanAbst ractIn this paper, we present a stochastic languagemodel for Japanese using dependency.
The predic-tion unit in this model is all attribute of "bunsetsu".This is represented by the product of the head of con-tent words and that of function words.
The relationbetween the attributes of "bunsetsu" is ruled by acontext-free grammar.
The word sequences axe pre-dicted from the attribute using word n-gram model.The spell of Unknow word is predicted using charac-ter n-grain model.
This model is robust in that it cancompute the probability of an arbitrary string andis complete in that it models from unknown word todependency at the same time.1 In t roduct ionAn effectiveness of stochastic language modeling asa methodology of natural language processing hasbeen attested by various applications to the recog-nition system such as speech recognition and to theanalysis ystem such as paxt-of-speech (POS) tagger.In this methodology a stochastic language modelwith some parameters i  built and they axe estimatedin order to maximize its prediction power (minimizethe cross entropy) on an unknown input.
Consid-ering a single application, it might be better to es-timate the parameters taking account of expectedaccuracy of recognition or analysis.
This method is,however, heavily dependent on the problem and of_fers no systematic solution, as fax as we know.
Themethodology of stochastic language modeling, how-ever, allows us to separate, from various frameworksof natural anguage processing, the language descrip-tion model common to them and enables us a sys-tematic improvement of each application.In this framework a description on a language isrepresented as a map from a sequence of alphabeticcharacters to a probability value.
The first modelis C. E. Shannon's n-gram model (Shannon, 1951).The parameters of the model are estimated from thefrequency of n character sequences of the alphabet(n-gram) on a corpus containing a large number ofsentences of a language.
This is the same model as0 This work is done when the auther was at Kyoto Univ.used in almost all of the recent practicM applicationsin that it describes only relations between sequentialelements.
Some linguistic phenomena, however, axebetter described by assuming relations between sep-axated elements.
And modeling this kind of phenom-ena, the accuracies of various application axe gener-ally augmented.As for English, there have been researches inwhich a stochastic ontext-free grammar (SCFG)(Fujisaki et ~1., 1989) is used for model descrip-tion.
Recently some researchers have pointed out theimportance of the lexicon and proposed lexicalizedmodels (Jelinek et al, 1994; Collins, 1997).
In thesemodels, every headword is propagated up throughthe derivation tree such that every parent receives aheadword from the head-child.
This kind of special-ization may, however, be excessive if the criterion ispredictive power of the model.
Research ~med atestimating the best specialization level for 2-grammodel (Mori et aL, 1997) shows a class-based modelis more predictive than a word-based 2-gram model,a completely lexicalized model, comparing cross en-tropy of a POS-based 2-graxa model, a word-based2-gram model and a class-based 2-graxa model, es-timated from information theoretical point of view.As for a parser based on a class-based SCFG, Chax-niak (1997) reports better accuracy than the abovelexicalized models, but the clustering method is notclear enough and, in addition, there is no reporton predictive power (cross entropy or perplexity).Hogenhout and Matsumoto (1997) propose a word-clustering method based on syntactic behavior, butno language model is discussed.
As the experimentsin the present paper attest, word-class relation isdependent on language model.In this paper, taking Japanese as the object lan-guage, we propose two complete stochastic languagemodels using dependency between bugsetsu, a se-quence of one or more content words followed byzero, one or more function words, and evaluate theirpredictive power by cross entropy.
Since the numberof sorts of bunsetsu is enormous, considering it as asymbol to be predicted would surely invoke the data-sparseness problem.
To cope with this problem we898use the concept of class proposed for a word n-grammodel (Brown et al, 1992).
Each bunsetsu is repre-sented by the class calculated from the POS of itslast content word and that of its last function word.The relation between bunsetsu, called dependency, isdescribed by a stochastic ontext-free grammar (Fu,1974) on the classes.
From the class of a bunsetsu,the content word sequence and the function word se-quence are independently predicted by word n-grammodels equipped with unknown word models (Moriand Yamaji, 1997).The above model assumes that the syntactic be-havior of each bunsetsu depends only on POS.
ThePOS system invented by grammarians may not al-ways be the best in terms of stochastic languagemodeling.
This is experimentally attested by thepaper (Mori et al, 1997) reporting comparisons be-tween a POS-based n-gram model and a class-basedn-gram model induced automatically.
SVe now pro-pose, based on this report, a word-clustering methodon the model we have mentioned above to success-fully improve the predictive power.
In addition, wediscuss a parsing method as an application of themodel.We also report the result of experiments con-ducted on EDR corpus (Jap, 1993) The corpus is di-vided into ten parts and the models estimated fromnine of them axe tested on the rest in terms of crossentropy.
As the result, the cross entropy of the POS-based dependency model is 5.3536 bits axtd that ofthe class-based ependency model estimated by ourmethod is 4.9944 bits.
This shows that the clus-tering method we propose improves the predictivepower of the POS-based model notably.
Addition-ally, a parsing experiment proved that the parserbased on the improved model has a higher accuracythan the POS-based one.2 Stochastic Language Model  basedon DependencyIn this section, we propose a stochastic languagemodel based on dependency.
Formally this model isbased on a stochastic ontext-free grammar (SCFG).The terminal symbol is the attribute of a bunsetsu,represented by the product of the head of the con-tent part and that of the function part.
From theattribute, a word sequence that matches the bun.setsu is predicted by a word-based 2-gram model,and unknown words axe predicted from POS by acharacter-based 2-gram model.2.1 Sentence  Mode lA Japanese sentence is considered as a sequence ofunits called bunsetsu composed of one or more con-tent words and function words.
Let Cont be a setof content words, Func a set of function words andSign a set of punctuation symbols.
Then bunsetsuis defined as follows:Bnst  = Cont+ Func * U Cont+ Func* Sign,where the signs "+" and "*" mean positive closureand Kleene closure respectively.
Since the relationsbetween bunsetsu known as dependency are not al-ways between sequential ones, we use SCFG to de-scribe them (Fu, 1974).
The first problem is howto choose terminal symbols.
The simplest way is toselect each bunsetsu as a terminal symbol.
In thiscase, however, the data-sparseness problem wouldsurely be invoked, since the number of possible bun-setsu is enormous.
To avoid this problem we use theconcept of class proposed for a word n-gram model(Brown et al, 1992).
All bunsetsu axe grouped bythe attribute defined as follows:attrib(b) (1)= qast(co.t(b)), last(f .
.c(b)),  Zast(sig.
(b))),where the functions cont, func  and sign take abun~etsu as their argument and return its contentword sequence, its function word sequence and itspunctuation respectively.
In addition, the functionlast(m) returns the POS of the last element of wordsequence m or NULL if the sequence has no word.Given the attribute, the content word sequence andthe function word sequence of the bunsetsu axe inde-pendently generated by word-based 2-gram models(Mori and Yamaji, 1997).2.2 Dependency  Mode lIn order to describe the relation between bunsetsucalled dependency, we make the generally acceptedassumption that no two dependency relations crosseach other, and we introduce a SCFG with the at-tribute of bunsetsu as terminals.
It is known, as acharacteristic of the Japanese language, that eachbunsetsu depends on the single bunsetsu appearingjust before it.
We say of two sequential bunsetsuthat the first to appear is the anterior and the sec-ond is the posterior.
We assume, in addition, thatthe dependency relation is a binary relation - thateach relation is independent of the others.
Thenthis relation is representing by the following form ofrewriting rule of CFG: B =~ AB,  where A is the at-tribute of the anterior bunsetsu and B is that of theposterior.Similarly to terminal symbols, non-terminal sym-bols can be defined as the attribute of bunsetsu.
Alsothey can be defined as the product of the attributeand some additional information to reflect the char-acteristics of the dependency.
It is reported that thedependency is more frequent between closer bunsetsuin terms of the position in the sentence (Maruyamaand Ogino, 1992).
In order to model these char-acteristics, we add to the attribute of bunsetsu an899(verb.
ending, period.
2.0)(noun, NULL.
comma, O, 0)kyou/noun ./sign(today)(noun.
postp.. NULL.
0.
0)Kyoto / noun daigaku / noun he/postp.
(Kyoto) (university) (to)ISCFG(verb.
ending, period.
0.0)" ~  j n-grami/verb ku/ending ./sign(go)Figure 1: Dependency model based on bunsetsuadditional information field holding the number ofbunsetsu depending on it.
Also the fact that a bun.setsu has a tendency to depend on a bunsetsu withcomma.
For this reason the number of bunsetsu withcomma depending on it is also added.
To avoiddata-sparseness problem we set an upper bound forthese numbers.
Let d be the number of bunsetsu de-pending on it and v be the number of bunsetsu withcomma depending on it, the set of terminal symbolsT and that of non-terminal symbols V is representedas follows (see Figure 1):T = attrib(b) ?
{0} ?
{0}V=attr ib(b)  ?
{1, 2, ""dmaz} x {0, 1, "''Vmaz}.It should be noted that terminal symbols have nobunsetsu depending on them.
It follows that allrewriting rules are in the following forms:S ~ (a, d, v) (2)(~ ,  d~, v , )~ (a,, d~, v~){~3, d~, ~)  (3)a 1 = a 3dl = min(ds + i, dm~.
)min(vs + 1, v,n~.
)vl = if sign(a2) = commav3 otherwisewhere a is the attribute of bunsetsu.The attribute sequence of a sentence is generatedthrough applications of these rewriting rules to thestart symbol S. Each rewriting rule has a probabilityand the probability of the attribute sequence is theproduct of those of the rewriting rules used for itsgeneration.
Taking the example of Figure 1, thisvalue is calculated as follows:P((noun,  JLL, comma, 0, 0)(noun, postp.,  NULL, 0, 0)(verb, ending, per iod,  0, 0))= P(S  ~ (verb, ending, per lod,  2, 0))?
P((verb,  ending, per iod,  2, O)=~ (noun, NULL, comma, 0, 0)(verb, ending, per iod,  1, 0))?
P((verb,  ending, per iod,  1, 0)=~ (noun, postp.,  NULL, 0, 0)(verb, ending, per iod,  0, 0)).The probability value of each rewriting rule is esti-mated from its frequency N in a syntactically anno-tated corpus as follows:P(S  ~ (a~, all, vl))N(S ::~ (al, dl, Va))N(s)N((al, dl, vl)=~ (a2, d2, v~)(a3, d3, v3))N((.I, dl, vl))In a word n-gram model, in order to cope withdata-sparseness problem, the interpolation tech-nique is applicable to SCFG.
The probability of theinterpolated model of grammars G1 and G2, whose900probabilities axe P1 and P2 respectively, is repre-sented as follows:P(A =~ a) = ~IPI(A =~ c~) +,~P2(A =~ a)0<~j  < l ( j= l ,  2) and ~,+~2=1 (4)where A E V and a E (VUT)*.
The coefficients areestimated by held-out method or deleted interpola-tion method (Jelinek et al, 1991).3 Word ClusteringThe model we have mentioned above uses the POSgiven manually for the attribute of bunsetsu.
Chang-ing it into some class may improve the predictivepower of the model.
This change needs only a slightreplacement in the model representing formula (1):the function last returns the class of the last word ofa word sequence rn instead of the POS.
The problemwe have to solve here is how to obtain such classesi.e.
word clustering.
In this section, we proposean objective function and a search algorithm of theword clustering.3.1 Ob ject ive  Funct ionThe aim of word clustering is to build a languagemodel with less cross entropy without referring tothe test corpus.
Similar reseaxch as been success-ful, aiming at an improvement of a word n-grammodel both in English and Japanese (Mori et al,1997).
So we have decided to extend this researchto obtain an optimal word-class relation.
The onlydifference from the previous research is the languagemodel.
In this case, it is a SCFG in stead of a n-gram model.
Therefore the objective function, calledaverage cross entropy, is defined as follows:my= __1 ~ H(Li,Mi), (5)m i----1where Li is the i-th learning corpus and Mi is thelanguage model estimated from the learning corpusexcluding the i-th learning corpus.3.2 A lgor i thmThe solution space of the word clustering is the set ofall possible word-class relations.
The caxdinality ofthe set, however, is too enormous for the dependencymodel to calculate the average cross entropy for allword-class relations and select the best one.
So weabandoned the best solution and adopted a greedyalgorithm as shown in Figure 2.4 Syntactic AnalysisSyntactic Analysis is defined as a function whichreceives a character sequence as an input, dividesit into a bunsetsu sequence and determines depen-dency relations among them, where the concatena-tion of character sequences of all the bunsetsu mustLet ml ,  m2, .
.
.
,  mn be .b4 sortedin the descending order of frequency.cl := {ml, m2, .
.
.
,  m,}c = {Cl}foreach i (1, 2, .
- - ,  n)f(mi) := clforeach i (1, 2, .
.
.
,  n)c := argmincecuc, ,~ -H(move(f, mi, c))i f  (-H(move(f, mi, c)) < H( f ) )  then/ :=  move(/, ms, c)update interpolation coeff?cients.i f  (c = c,e~) thenC := C u {c,,,,,,}i f f i liffi2i=3i=4update interpolation coefficientsc!
"- ........................................:" ................... i :::~.
::-:~., update interpolation coefficientsupdate interpolation coefficient.5Figure 2: The clustering algorithm.be equal to the input.
Generally there axe one ormore solutions for any input.
A syntactic analyzerchooses the structure which seems the most similarto the human decision.
There are two kinds of an-alyzer: one is called a rule-based analyzer, which isbased on rules described according to the intuitionof grarnmarians; the other is called a corpus-basedanalyzer, because it is based on a large number ofanalyzed examples.
In this section, we describe astochastic syntactic analyzer, which belongs to thesecond category.4.1 S tochast ic  Syntact i c  Ana lyzerA stochastic syntactic analyzer, based on a stochas-tic language model including the concept of depen-dency, calculates the syntactic tree (see Figure 1)with the highest probability for a given input x ac-cording to the following formula:rh = argmax P(Tia~)U~(T)=Z901Table 1: Corpus.
Table 2: Predictive power.#sentences #bunsetsu #wordlearning 174,524 1,610,832 4,251,085test 19,397 178,415 471,189#non-terminal crosslanguage model +#terminal  entropyPOS-based model 576 5.3536class-based model 10,752 4.9944= argmax P(TIx)P(x )W(T)=Z= argmax P(~\]T)P(T) ('."
Bayes' formula)W(T)=:v=argmaxP(T)  ('."
P(xlT ) = 1),W(T)=Zwhere to (T) represents he character sequence of thesyntactic tree T. P(T) in the last line is a stochas-tic language model including the concept of depen-dency.
We use, as such a model, the POS-based de-pendency model described in section 2 or the class-based dependency model described in section 3.4.2 So lut ion Search  A lgor i thmThe stochastic ontext-free grammar used for syn-tactic analysis consists of rewriting rules (see for-mula (3)) in Chom~ky normal form (Hopcroft andUllman, 1979) except for the derivation from thestart symbol (formula (2)).
It follows that a CKYmethod extended to SCFG, a dynamic-programmingmethod, is applicable to calculate the best solutionin O(n 3) time, where n is the number of input char-acters.
It should be noted that it is necessary tomultiply the probability of the derivation from thestart symbol at the end of the process.5 Evaluat ionWe constructed the POS-based dependency modeland the class-based ependency model to evaluatetheir predictive power.
In addition, we implementedparsers based on them which calculate the best syn-tactic tree from a given sequence of bun~etsu to ob-serve their accuracy.
In this section, we present heexperimental results and discuss them.5.1 Condi t ions  on the Exper imentsAs a syntactically annotated corpus we used EDRcorpus (Jap, 1993).
The corpus was divided intoten parts and the models estimated from nine ofthem were tested on the rest in terms of cross en-tropy (see Table 1).
The number of characters inthe Japanese writing system is set to 6,879.
Twoparameters which have not been determined yet inthe explanation of the models (dmaz and v,naz) axeboth set to 1.
Although the best value for each ofthem can also be estimated using the average crossentropy, they are fixed through the experiments.5.2 Eva luat ion  o f  Pred ic t ive  PowerFor the purpose of evaluating the predictive powerof the models, we calculated their cross entropy onthe test corpus.
In this process the annotated treeis used as the structure of the sentences in the testcorpus.
Therefore the probability of each sentencein the test corpus is not the summation over all itspossible derivations.
In order to compare the POS-based dependency model and the class-based epen-dency model, we constructed these models from thesame learning corpus and calculated their cross en-tropy on the same test corpus.
They are both inter-polated with the SCFG with uniform distribution.The processes for their construction are as follows:?
POS-based ependency model1.
estimate the interpolation coefficients inFormula (4) by the deleted interpolationmethod2.
count the frequency of each rewriting ruleon the whole learning corpus?
class-based ependency model1.
estimate the interpolation coefficients inFormula (4) by the deleted interpolationmethod2.
calculate an optimal word-class relation bythe method proposed in Section 3.3. count the frequency of each rewriting ruleon the whole learning corpusThe word-based 2-gram model for bunsetsu gener-ation and the character-based 2-gram model as anunknown word model (Mori and Yamaji, 1997) arecommon to the POS-based model and class-basedmodel.
Their contribution to the cross entropy isconstant on the condition that the dependency mod-els contain the prediction of the last word of the con-tent word sequence and that of the function wordsequence.Table 2 shows the cross entropy of each modelon the test corpus.
The cross entropy of the class-based dependency model is lower than that of thePOS-based dependency model.
This result attestsexperimentally that the class-based model estimatedby our clustering method is more predictive thanthe POS-based model and that our word clustering902Table 3: Accuracy of each model.language model cross entropy accuracyPOS-based model 5.3536 68.77%class-based model 4.9944 81.96%select always 53.10%the next bunsetsumethod is efficient at improvement of a dependencymodel.We also calculated the cross entropy of the class-based model which we estimated with a word 2-grammodel as the model M in the Formula (5).
The num-ber of terminals and non-terminals i 1,148,916 andthe cross entropy is 6.3358, which is much higherthan that of the POS-base model.
This result indi-cates that the best word-class relation for the depen-dency model is quite different from the best word-class relation for the n-gram model.
Comparing thenumber of the terminals and non-terminals, the bestword-class relation for n-gram model is exceedinglyspecialized for a dependency model.
We can con-clude that word-class relation depends on the lan-guage model.5.3 Eva luat ion  of  Syntact i c  Ana lys i sSVe implemented a parser based on the dependencymodels.
Since our models, equipped with a word-based 2-graan model for bunsetsu generation and thecharacter-based 2-gram as an unknown word model,can return the probability for amy input, we canbuild a parser, based on our model, receiving a char-acter sequence as input.
Its evaluation is not easy,however, because rrors may occur in bunsetsu gen-eration or in POS estimation of unknown words.
Forthis reason, in the following description, we assumea bunsetsu sequence as the input.The criterion we adopted is the accuracy of depen-dency relation, but the last bunsetsu, which has nobunsetsu to depend on, and the second-to-last bun-setsu, which depends always on the last bunsetsu,are excluded from consideration.Table 3 shows cross entropy and parsing accuracyof the POS-based dependency model and the class-based dependency model.
This result tells us ourword clustering method increases parsing accuracyconsiderably.
This is quite natural in the light of thedecrease of cross entropy.The relation between the learning corpus size andcross entropy or parsing accuracy is shown in Fig-ure 3.
The lower bound of cross entropy is the en-tropy of Japanese, which is estimated to be 4.3033bit (Mori and Yamaji, 1997).
Taking this fact intoconsideration, the cross entropy of both of the mod-els has stronger tendency to decrease.
As for ac-12104201'0,pOS-bm~l  ~M~/m~ldm=-Imsetl aep*t~'y  mad100%8O=.so24020i i i i t i 0101 102 10 ~ 104 105 106 107#characters in learning corpusFigure 3: Relation between cross entropy and pars-ing accuracy.curacy, there also is a tendency to get more accu-rate as the learning corpus size increases, but it is astrong tendency for the class-based model than forthe POS-based model.
It follows that the class-basedmodel profits more greatly from an increase of thelearning corpus size.6 Conc lus ionIn this paper we have presented ependency mod-els for Japanese based on the attribute of bunsetsu.They are the first fully stochastic dependency mod-els for Japanese which describes from character se-quence to syntactic tree.
Next we have proposeda word clustering method, an extension of deletedinterpolation technique, which has been proven tobe efficient in terms of improvement of the pre-dictive power.
Finally we have discussed parsersbased on our model which demonstrated a remark-able improvement in parsing accuracy by our word-clustering method.Re ferencesPeter F. Brown, Vincent J. Della Pietra, Peter V.deSouza, Jennifer C. Lal, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467-479.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Pro-ceedings of the l~th National Conference on Arti-ficial Intelligence, pages 598-603.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe 35th Annual Meeting of the Association forComputational Linguistics, pages 16-23.King Sun Fu.
1974.
Syntactic Methods in PatternRecognition, volume 12 of Mathematics in Scienceand Engineering.
Accademic Press.903T.
Fujisaki, F. Jelinek, J. Cocke, E. Black, andT.
Nishino.
1989.
A probabilistic parsing methodfor sentence disambiguation.
I  Proceedings oftheInternational Parsing Workshop.Wide R. ttogenhout and Yuji Matsumoto.
1997.
Apreliminary study of word clustering based on syn-tactic behavior.
In Proceedings of the Computa-tional Natural Language Learning, pages 16-24.John E. ttopcroft and Jeffrey D. UUman.
1979.
In-troduction to Automata Theory, Languages andComputation.
Addison-~,Vesley Publishing.Japan Electronic Dictionary Research Institute,Ltd., 1993.
EDR Electronic Dictionary TechnicalGuide.Fredelick Jelinek, Robert L. Mercer, and SalimRoukos.
1991.
Principles of lexical languagemodeling for speech recognition.
In Advances inSpeech Signal Processing, chapter 21, pages 651-699.
Dekker.F.
Jelinek, J. Lafferty, D. Magerman, R. Mercer,A.
Rantnaparkhi, and S. Roukos.
1994.
Decisiontree parsing using a hidden derivation model.
InProceedings of the ARPA Workshop on HumanLanguage Technology, pages 256-261.ttiroshi Maruyama nd Shiho Ogino.
1992.
A statis-tical property of japanese phrase-to-phrase modifi-cations.
Mathematical Linguistics, 18(7):348-352.Shinsuke Mort and Osamu Yamaji.
1997.
Anestimate of an upper bound for the entropyof japanese.
Transactions of Information Pro-cessing Society of Japan, 38(11):2191-2199.
(InJapanese).Shinsuke Mort, Masafumi Nishimura, and NobuyukiIto.
1997. l, Vord clustering for class-based lan-guage models.
Transactions of Information Pro-cessing Society of Japan, 38(11):2200-2208.
(InJapanese).C.
E. Shannon.
1951.
Prediction and entropy ofprinted english.
Bell System Technical Journal,30:50-64.904
