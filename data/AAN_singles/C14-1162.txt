Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1718?1727, Dublin, Ireland, August 23-29 2014.Towards multimodal modeling of physicians?
diagnosticconfidence and self-awareness using medical narrativesJoseph Bullard?Cecilia Ovesdotter Alm?Qi Yu?Pengcheng Shi?Anne Haake?
?College of Computing and Information Sciences?College of Liberal ArtsRochester Institute of Technologyjtb4478@cs.rit.educoagla|qi.yu|spcast|arhics@rit.eduAbstractMisdiagnosis is a problem in the medical field, often related to physicians?
cognitive errors.Overconfidence is considered a major cause of such errors.
Intelligent diagnostic support sys-tems could benefit from understanding how aware physicians are of their performance when theyestimate their confidence in a diagnosis (i.e.
a physician?s diagnostic self-awareness).
Shed-ding light on the cognitive processes related to such awareness could also help improve medicaleducation.
We use a multimodal dataset of medical narratives to computationally model diagnos-tic confidence and self-awareness based on physicians?
linguistic and eye movement behaviors.Dermatologists viewed images of cutaneous conditions, providing a description, diagnosis, andcertainty level for each image case, while their speech and eye movements were recorded.
Wedefine both a generalized and a personalized approach to binning confidence levels, used in clas-sification experiments.
We also introduce truly multimodal features, which focus on combininglinguistic and eye movement data into multimodal attributes.
Results indicate that combinationsof multiple modalities can outperform their constituent modalities in isolation for these problems.1 IntroductionMisdiagnosis in the medical field is estimated to be as high as 10%-15% (Berner and Graber, 2008;Croskerry, 2009).
Such errors can result in incorrect or delayed treatment, causing patients to experienceadditional suffering.
Graber et al.
(2002) describe three types of diagnostic errors: no-fault errors, result-ing from atypical disease presentation or limitations of medical knowledge; system errors, resulting fromproblems with the health care system; and cognitive errors, resulting from biases or faulty interpretationon the part of a physician.
Cognitive errors in particular have potential for substantial reduction througheducation and training aimed at developing clinicians?
metacognitive skills.
Understanding the cognitiveprocesses of physicians during diagnosis is also of critical importance for building human-centered di-agnostic support systems, which could help detect and flag problematic diagnostic self-awareness cases.Examples of cognitive errors include settling on a final diagnosis too early, without ever considering thecorrect diagnosis (Berner and Graber, 2008), or confirmation bias, in which only evidence to confirm adiagnostic hypothesis is considered (Croskerry, 2003).
Overconfidence is generally thought to be a majorcause of such errors (Berner and Graber, 2008; Croskerry, 2008).
For example, an overconfident physi-cian may not question her original thoughts or explore alternative diagnoses until later in the treatmentprocess.
In general, overconfidence may be a systemic problem, reinforced by patients?
preferences forconfident doctors, and by a professional environment that favors decisive actions (Katz, 1984).
Similarly,underconfidence can erode patients?
trust in their providers.
In this study, we view the interplay betweenconfidence1and correctness as a two-dimensional problem (see Figure 1).
Ideally, physicians wouldhave high confidence when correct and low confidence when incorrect, indicated by the upper-left andlower-right quadrants in Figure 1.This work is licensed under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1For consistency, this paper uses the term confidence, treated as interchangeable with certainty and similar synonymousexpressions which may have been used by clinicians in the medical narratives, such as sure, certain, confident, etc.1718AppropriateConfidenceOverconfidenceUnderconfidenceAppropriateConfidenceConfidentNot confidentCorrect IncorrectFigure 1: Two-dimensional view of the confidence and correctness relationship as it relates to diagnosticself-awareness.
A similar conceptual model is presented by Pon-Barry and Shieber (2011).
Ideally,physicians should have high confidence when they are correct and low confidence when incorrect.Contribution Diagnostic self-awareness is an important phenomenon with implications for clinicaltraining and practice, yet has received little focus from a computational perspective.
We report on com-putational modeling for predicting the confidence and correctness interplay in diagnosis using featuresof physicians?
speech, eye movements, and combinations thereof, as dermatologists performed medicalimage inspection tasks while narrating their diagnostic thought process.
In dermatology, visual expertiseand clinical knowledge are both important.
A motivation behind our multimodal approach is that medi-cal image inspection relies on both the physician?s visual perceptual expertise and conceptual knowledgebase, each of which can be regarded as expressed by eye movement behavior and linguistic behavior,respectively.
We aim to apply this decision modeling to intelligent diagnostic support and clinical tutor-ing systems.
Here we solve a foundational problem by successfully modeling the complex relationshipbetween physicians?
confidence in and correctness of their diagnoses.
We also make contributions inmultimodal and linguistic feature analysis: carefully assessing feature modalities that represent physi-cians?
behaviors, and introducing a novel multimodal feature type that focuses on fusing eye movementand verbal data.2 Previous WorkAlthough there are many causes of diagnostic errors (Graber et al., 2005), those resulting from cognitiveerrors may be the most challenging to reduce (Croskerry, 2003; Graber et al., 2002), while their reductionprovides high impact.
Examples of such errors include flawed perception, biased heuristics, and settlingon a final diagnosis too early (Graber et al., 2002), all of which can be caused by overconfidence (Bernerand Graber, 2008; Croskerry, 2008).
Underconfidence may also be a problem if it prevents a physicianfrom pursuing a correct diagnosis (Friedman et al., 2005).There is evidence for links between speech and confidence in terms of prosodic features, such aspitch and loudness (Scherer et al., 1973; Pon-Barry and Shieber, 2011; Kimble and Seidel, 1991), aswell as other characteristics of spoken language, such as speech disfluencies (Womack et al., 2012)and hedges (Smith and Clark, 1993).
Prosodic features have been identified and successfully used inintelligent tutoring systems (Liscombe et al., 2005), where a student?s confidence (or lack thereof) canplay a key role in effective system response.
In medical diagnosis, prosodic and lexical features havebeen useful indicators of physicians?
confidence and diagnostic correctness, individually (Womack et al.,2013; McCoy et al., 2012).
Other potentially useful information may be evident in speech as well.
Ina study by Womack et al.
(2012) on a similar dataset, the authors found a relationship between speechcharacteristics and physician experience: attending (experienced) physicians used more filled pauses andspoke more than resident (in-training) physicians.
Additionally, verbal features may expose differencesin diagnostic reasoning that may be useful predictors of confidence.
Rogers (1996) analyzed a dataset ofspoken chest X-ray examinations by radiologists, remarking that reasoning styles influence physicians?expectations, and confirmations or contradictions of those expectations can affect their self-reportedconfidence levels.1719Most relevant literature focuses on linguistic features.
Language, as the primary form of human ex-pression, is certainly critical.
However, analyzing meaning may require going beyond linguistic infer-ence, depending on the context or application.
Previous studies have successfully incorporated multipleexpressive modalities when examining linguistic and cognitive processes, such as facial expressions forvideo sentiment analysis (P?erez-Rosas et al., 2013) and pointing gestures for referring actions (Gatt andPaggio, 2013).
In such studies, the additional modalities were carefully chosen based on the nature ofthe performed tasks.
Here, we deal with experts (dermatologists) inspecting images (skin conditions) fordiagnostic purposes, a task that heavily involves their use of visual perceptual expertise, in addition toconceptual domain knowledge.
For this reason, we incorporate features of their eye movements in ourstudy.
There is evidence for ties between perceptual expertise and eye movements during image inspec-tion tasks (Li et al., 2012b), and we explore if such ties may also relate to a physician?s confidence anddiagnostic self-awareness.Integrating different expressive modalities is challenging.
Previous work involving multimodality haspredominantly treated each in isolation.
We further address this challenge by identifying and exploringtruly multimodal features that focus on combining verbal and eye movement data into complex multi-modal attributes, as it seems reasonable that the two modalities together could be more informative iflinked, and that such complex features represent a natural interactive extension of multimodal semantics.Evidence for ties between speech and eye movements specifically was found by Li et al.
(2012a), inwhich sequences of fixations and saccadic eye movements were identified to predominantly align withparticular conceptual units of thought (e.g.
primary lesion type) expressed verbally in medical narratives.3 Data Description and AnalysisThis study takes advantage of a dataset previously reported on by Womack et al.
(2013), which is brieflydescribed here for clarity, as Womack et al.
?s work ignored the eye movement data.
A group of 29 derma-tologists (11 attending physicians, 18 residents) were each shown a series of 30 images of dermatologicalconditions in random order and asked to narrate their diagnosis of each condition.
They were asked toprovide a description of the case, a list of differential diagnoses to consider, a final diagnosis, and theircertainty of their final diagnosis, as a percentage.
The physicians?
verbal descriptions were recorded asaudio and later manually transcribed in detail, including pauses, disfluencies, and other speech phenom-ena.2During this process, the physicians?
eye movements were also tracked.
Each image was displayedon a 22?
LCD monitor (1650x1050 pixels) with an attached 250Hz SensoMotoric Instruments REDremote eye-tracker while IViewX software was recording the eye movements.In this study, the time-aligned pair of verbal description and eye movements for one physician viewingone image is henceforth called a narrative.
Figure 2a shows an example of a verbal description forone narrative and Figure 2b shows a visualization of the corresponding eye movements.
The correctdiagnoses for all images were known for the experiment and each narrative was assigned a binary labelof correct or incorrect.3For the purposes of this multimodal study, 238 of the 870 narratives wereexcluded due to technical issues that had occurred with the eye tracking or audio capture equipment,or because the physicians had provided no confidence values for their diagnoses.
The remaining 632narratives were used for the analysis and experimentation reported on in this paper.3.1 Case Studies towards Understanding Physicians?
Confidence and CorrectnessThe physicians tended to evaluate their confidence towards the upper end of the spectrum, with a me-dian of 70% confident over all narratives.
But diagnostic confidence may be affected by many factors,including professional experience, case difficulty, and personality.
We examine both individual imagesand physicians at the extremes of confidence to gain insight into the relationship between confidence andcorrectness in the dataset.
Table 1 summarizes information for the three image cases that received the2Some transcription imperfections may occur.3A limited number of narratives in the dataset were labeled half correct if one of two final diagnoses given was correct, andpartially correct if the final diagnosis was too broad.
Here, we consider half to be correct, because in such cases the correctdiagnosis was still identified, but partial to be incorrect, because the correct diagnosis was technically not identified.1720... um two ... pa- ... pink to purplemacules ... on the ... volar wristdifferential diagnosis ... um fixeddrug eruption ... bites ... urticaria... uh ... diagnosis fixed drug erup-tion percent certainty fifty percentnext ...(a) Sample verbal description.
Ellipses(?...?)
show pauses.
(b) Sample eye movement visualization.
Circles represent fixations, where thecenter is the point of fixation and the radius is proportional to the time fixating atthat point.
Lines represent saccades (movements) between fixation points.Figure 2: Sample verbal description and eye movements for one narrative.
The final diagnosis is correctand the physician was 50% confident.Confidence Conf.
% Correct RankHighest100 100 290 100 590 100 1Lowest50 24 2550 35 2945 0 20Table 1: Images receiving highest and lowest me-dian confidence values.
Difficulty ranking pro-vided by a dermatology expert with 1 reflectingthe easiest image and 30 the most difficult.Confidence Conf.
% Correct Exp.Highest90 53 R85 50 A85 41 RLowest38 39 A30 48 R15 37 RTable 2: Most and least confident physicians bymedian confidence values given over all images.The last column shows experience level: experi-enced attending (A) or resident (R) physician.highest median confidence values and the three that received the lowest.
A domain expert (dermatolo-gist and clinical educator) who was not a subject in the experiment gave each image a unique difficultyranking from 1 to 30, where the image ranked number 1 was considered the easiest to support a correctdiagnosis, and 30 the most difficult.
As expected, the highest confidence images were among the easiest,and vice versa.
Accordingly, the higher confidence images were correctly diagnosed by every physician,while those receiving the lowest confidence were correctly diagnosed much less often.
The negativecorrelation between image difficulty and median physician confidence was significant using Spearman?srank correlation (rs= ?0.544, p < 0.005).
In other words, higher levels of case difficulty were associ-ated with lower levels of physician confidence.
In contrast, examination of the most and least confidentphysicians yields less intuitive results.
The physicians with the highest and lowest median confidencevalues are shown in the top and bottom halves of Table 2, respectively.
Notably, each of the two groupscontained both resident dermatologists-in-training and attending physicians with careers spanning mul-tiple decades.
Also, the most confident physicians were only correct roughly half of the time, and theleast confident physicians?
correctness appears quite similar.
While this may reflect the sample size, theobservation is interesting nonetheless.
Clearly, this points to how complicated diagnostic self-awarenessis, and how potentially useful it would be to computationally infer a physician?s self-awareness for diag-nostic cases based on their behaviors.17213.2 Confidence BinningNearly all confidence values given were multiples of five, or simply numbers close to 100, such as 99%.4This makes discretization preferable to using real-numbered values for confidence.
Additionally, theanalyses in Section 3.1 revealed patterns of over- or underconfidence in individual physicians.
What thisindicates is that ?high?
and ?low?
confidence involve different numerical values in the minds of differentphysicians.
This subjectivity could be problematic in doctor-patient interactions and it adds complexityfor predictive modeling involving confidence.
To explore the impact, we devise two alternative binarybinning schemes: generalized bins, based on the performance of all physicians in the dataset, and per-sonalized bins, based on each individual physician?s performance in the training data only.
In terms ofapplication, consider a diagnostic support system which could establish a history for each physician whouses it.
Such a system could implement a generalized binning scheme and predictive model for newusers, and later, after learning from repeated exposure to a given physician, switch to a model based onthat physician?s individual performance.
In addition, binning choice may be influenced by context: in aclinical tutoring system, it may be preferable to compare learners to experienced physicians as a targetpopulation.
For the generalized binning scheme, a confidence value greater than or equal to the medianover all physicians is considered high, while a value below is considered low.
This results in a slightimbalance towards high confidence (56% of narratives).5We construct the personalized binning schemesimilarly, but using a given physician?s own median confidence in the training data as the dividing line.In this case, high confidence accounts for 58% of the narratives, similar to that of the generalized bins.Calling a physician?s median confidence high lets us better distinguish the problem cases: cases of under-confidence should be strictly less than their ?typical?
confidence, while cases of overconfidence shouldbe at or above typical.
The binning scheme used does not affect the correctness value for each narrative,but it does change the distribution of high and low confidence, with the generalized scheme favoringover- and underconfidence, and the personalized scheme favoring appropriate confidence.
Arguably, thelatter is a better reflection of the expected: over- and underconfidence as the minority classes.4 Approach and MethodologyThere are many ways to approach the problem of predicting physicians?
diagnostic self-awareness.
Herewe formulate two classification problems, each tested under both binning schemes, yielding a total offour classification models.
We also outline the performance evaluation experiments for the models.4.1 Classification ProblemsWe define two classification problems based on the chart in Figure 1 (above).
First, we define ConfidenceOnly, which ignores correctness (the horizontal dimension of Figure 1) and predicts only confidence asa binary high or low.
Intuitively, low confidence might be considered a warning sign for a diagnosis,alerting a physician to seek additional insight or information.6This first problem was used as a steppingstone to explore and better understand confidence, before incorporating correctness.
Next, we defineConfidence & Correctness, which relates confidence with the correctness of the diagnosis (consideringall four quadrants in Figure 1, individually) to better address the more problematic, but interesting, cases.Distinguishing these four classes could be of use to intelligent tutoring or clinical support systems, whichcould respond differently to over- or underconfident users.
In general, the full separation of these classescould ultimately allow for deeper analysis of physician self-awareness.4.2 Model EvaluationBefore any development took place, the 632 narratives were randomly divided into three subsets: 442(70%) for training (dev-train), 95 (15%) for testing during development and tuning (dev-test), and 954There were only a few exceptions: one physician gave three values of 3%, another gave a 33% and a 66% (rounded downfrom ?two-thirds?
), and a third gave a 33%.
The latter three cases could also seem intuitive depending on how many conditionswere listed in the differential diagnosis.
For example, 66% might indicate that one disease seemed twice as likely as a another.5Other simple binning schemes dividing up the 0-100% range were explored, but this binary version allowed for a moresystematic approach to both generalized and personalized binning, without sacrificing performance.6Normally, a physician would likely administer tests after the differential diagnosis, before reaching a final diagnosis.1722(15%) for final summative evaluation after all development was completed (heldout-test).
All threesubsets have similar class distributions.
Each of the four classification models were evaluated in twoways: (1) by training the model on the union of the dev-train and dev-test sets and testing on the heldout-test set, and (2) by running 50 randomized iterations of 10-fold cross-validation on the entire collectionof 632 narratives.
The first evaluation experiment addresses the problem of overfitting by excluding theheldout-test set from all development, while the second addresses the problem of sampling bias in theinitial set divisions.
The results are described in Section 5.2.5 Models and ResultsHere we describe the development and performance of each of the four computational models outlinedin Section 4.
We report on logistic regression, which had the best performance in all metrics for allexperiments, after dimensionality reduction (see Section 5.1).
The feature selection and modeling wasimplemented in Python with the scikit-learn machine learning library (Pedregosa et al., 2011).5.1 Feature Extraction and SelectionA total of 60 features were examined (see Table 3).
The features represented three modalities, moti-vated by the task the physicians performed and knowledge about dimensions of clinical expertise in thisdomain: verbal, composed of lexical, prosodic, and structural features of the narratives; eye movement,consisting of features of fixations and saccadic eye movements; and truly multimodal features, consistingof overlapping or simultaneously occurring features from the other two modalities, to reflect integratedmultimodal semantics.
Continuing with the theme of personalization, we also created a fourth categoryof personal features, with demographics of the physician and statistics about their confidence and cor-rectness in the training data, in order to model their ?past?
performance.
The latter simulates how asystem could learn from experience with a particular physician.As discussed in Section 2, verbal features of confidence have been studied before, and many of theverbal features used here are inspired by previous work.
Some verbal features are based on word choice,such as amplifiers (e.g.
definitely, sure) and modals (e.g.
could, might),7while other have to do withsilences (or pauses) or prosody.
The eye movement and multimodal features are mostly concerned withfixations, as it seems intuitive that fixation may be associated with thoughtfulness about a particular areaof the image, which may in turn reflect a physician?s confidence.Initial feature selection was performed on the development data (dev-train and dev-test) usingscikit-learn?s random forest ensemble classifier.
This allowed for human-friendly inspection ofuseful features.
Random forests (Breiman, 2001) are an ensemble method in which numerous decisiontrees are constructed, each trained on a randomized subset of the development data, which allows for theutility of features to be evaluated on many sub-distributions of the data.
The importance of a feature canthen be approximated as the sum of the error reduction at each node that splits on that feature, weightedby the population size at that node.
This reflects the fact that features used near the root of the tree oftenhandle a larger number of individuals.
The importance values for all features will sum to 1.
We considerany feature that appeared in the top 20 of the ranked features for any model to be important, and all suchtypes of features are marked in bold in Table 3.
Interestingly, the useful features for all classificationmodels were almost the same, with a few transpositions in the ordering.
The exception was past confi-dence, which was useful under generalized, but disappeared under personalized, as expected, since thepersonalized scheme effectively normalizes each physician?s confidence values.Interpreting the results for the verbal features, silence duration (statistics about the durations of allsilences) and the duration of narrative were most useful.
Intuitively, this may relate to thoughtfulness orcontemplation.
Additionally, words per second, or speech rate, was also useful, again perhaps relating tomore careful or thorough inspection/diagnosis.
As discussed earlier, ties between speech and confidencehave been well-studied, while eye movements are underreported.
It seems intuitive that eye movement7Such word-choice features were mostly based on lexical lists, and some overlap may occur.
The cutaneousconditions feature contained multiword expressions.
These could be improved by using resources such as UMLS(http://www.nlm.nih.gov/research/umls/) or WordNet (http://wordnet.princeton.edu/).1723Verbal (29)Duration of narrativeNumber of silencesSilence duration (?, ?, ?
)Duration of initial silenceNumber of filled pausesWord type-token ratioWords per secondCutaneous conditions (n, %)Pronouns 1st (n, %)Pronouns 3rd (n, %)Modals (n, %)Amplifier words (n, %)Speculative words (n, %)Negations (n, %)Pitch (m, M , ?
)Intensity (m, M , ?
)Eye movement (11)Fixation duration (?, ?, ?)
Number of fixationsSaccade duration (?, ?, ?)
% image area fixatedSaccade amplitude (?, ?, ?
)Multimodal (14)% of initial silence time fixating% of total silent time fixating% of total fixation time silentWords per second during fixationPitch during fixations (?, range)Intensity during fixations (?, range)Pitch of filled pauses (m, M , ?
)Intensity of filled pauses (m, M , ?
)Personal (6)Attending vs. Resident Past correctnessYears of experiencePast confidence (m, M , ?
)Table 3: Features examined for classification (60 total), grouped by modality.
Symbols in parenthesesindicate statistics over all occurrences of a feature in a narrative: raw count (n), raw count divided bythe total number of words (%), sum (?
), mean (?
), standard deviation (?
), min (m), max (M ), range(range).
Useful features are boldfaced.
If a feature has multiple statistics, the useful ones are underlined.features may be more related to correctness.
For example, the most useful eye movement feature was% image area fixated, computed using a grid overlaid onto the image.
If more of the image was fixatedupon, then it may have contained more areas of interest, or more visual evidence may have been sought,which may also be related to case difficulty.
Similarly, features of saccade amplitude (the angle of asaccadic eye movement) may reflect physicians feeling a need to explore additional visual evidence byswitching focus between distant areas in an image.
It is not surprising that the useful individual featuresfrom verbal and eye movement modalities were also useful when combined as multimodal features.
Inparticular, simultaneous silence and fixation were the most useful, which again might indicate contem-plation and analytical cognitive processing.
This suggests that expression of confidence and diagnosticself-awareness is at least partially a multimodal phenomenon.Although the random forest method could be used for dimensionality reduction, we instead use Princi-ple Component Analysis (PCA) in evaluation below, as it gave better performance gains in development.The purpose of the random forest method was to examine which verbal, eye movement, and multimodalfeatures were most informative for classification, as we are interested in understanding how these modal-ities relate to confidence and correctness.
The latent features resulting from PCA are linear combinationsof the features, and thus would not allow for such inspection.
The number of PCA components wasoptimized for classification accuracy in cross-validation for each of the four classification models.
Eachproblem had a different number of principal components, indicating that both the binning scheme and theclassification problem type affected which features were identified as more collectively discriminativeby PCA.5.2 Results and EvaluationHeldout narratives We addressed the problem of overfitting by withholding 15% (n = 95) of thenarratives as an unseen final evaluation set.
All predictive models performed well above their respectivemajority class baselines (see Table 4).
The Confidence Only models were able to reach higher accuracy,precision, and recall than the joint Confidence & Correctness models.
The exception is the accuracyrelative to baseline for personalized Confidence Only, which may be due to its higher baseline.
As men-tioned in Section 3.2, the generalized binning scheme is biased towards over- and underconfidence, andthe personalized towards appropriate confidence.
The per-class metrics (not shown here) reflect this fact,with overconfidence having higher precision and recall under generalized binning than under personal-ized.
Additionally, under the personalized scheme underconfidence is particularly underrepresented andthus more difficult to predict.1724Binning Problem N Majority Class % BL % Acc.
P RGeneralizedConf.
Only 2 High Confidence 53 76 (+23) 0.76 0.76Conf.
& Corr.
4 Overconfidence 37 53 (+16) 0.42 0.42PersonalizedConf.
Only 2 High Confidence 65 77 (+12) 0.75 0.73Conf.
& Corr.
4 Appropriate High 37 53 (+16) 0.38 0.42Table 4: Performance metrics for the heldout-test set under each binning scheme with logistic regressionand PCA.
All four models performed well above the majority class baselines (% BL) of their respectiveproblems (each with N many class labels).
Precision (P) and recall (R) are each macro-averaged.Random cross-validation A potential drawback of the initial development strategy used here is thatthe initial random splits may bias classification models.
To address this problem, after the heldout testing,50 randomized iterations of 10-fold cross-validation were performed on the total collection of narratives,the results of which are in Table 5.
The personalized binning scheme was designed to mimic a sys-tem that could adapt to a physician?s performance history, and thus the statistics used for personalizedconfidence binning were recomputed on the training data within each individual cross-validation fold.It is therefore not possible to establish a baseline for the personalized confidence binning outside ofa given fold.
Instead, we take the mean of the percent accuracy above baseline from each test fold(1k?ki=1(accuracyi?
baselinei)).
All models performed well above their respective baselines, whichis in line with observations from heldout testing.Binning Generalized PersonalizedProblem C.O.
C&C C.O.
C&CAcc.
abovebaseline+14 +9 +13 +12Precision 0.70 0.25 0.69 0.32Recall 0.70 0.38 0.57 0.37Table 5: Performance metrics for logistic re-gression with 50 randomized iterations of cross-validation using all narratives for Confidence Only(C.O.)
and Confidence & Correctness (C&C).
Weaverage the accuracy above baseline from each in-dividual fold.
Precision and recall are each macro-averaged for each problem.Feature Generalized Personalizedmodality C.O.
C&C C.O.
C&CV +13 +9 +12 +11E +7 +6 +11 +10MM +7 +4 +6 +5V+E +13 +9 +13 +11V+MM +14 +8 +11 +11E+MM +10 +6 +13 +11V+E+MM +14 +9 +13 +12Table 6: Modality study with cross-validation forVerbal (V), Eye movement (E), and Multimodal(MM) features, measured in accuracy above re-spective baselines, averaged over all folds.
Mostmodality combinations equaled or slightly im-proved on constituent modalities in isolation.Modality study We also performed a study within the cross-validation testing to investigate the impactof different feature modality combinations on classification (see Table 6).
Importantly, the verbal modal-ity alone was more powerful than the eye movement or multimodal features, but most combinations ofmodalities resulted in slightly higher or equal accuracy compared to their isolated constituent modali-ties.
This suggests that, as we projected, considering multiple modalities of a physician?s behavior canhelp reveal their confidence and self-awareness, but also that verbal features are the most informative,likely since verbal expression is the primary means to tap into physicians?
rich and tacit conceptual un-derstanding of a diagnostic case.
The multimodal features, which focused on combining verbal and eyemovement data, did not improve performance over baselines as much as the simple combination of theindividual verbal and eye movement features.
One reason for this could be that a person?s speech andeye movements are not perfectly temporally aligned (Vaidyanathan et al., 2012), and this asynchronousrelationship may affect the meaningfulness of our multimodal feature measurements.
Additionally, theseeye movement features may be at a much finer spatial or temporal scale than the verbal features.17256 ConclusionsThis study examined a dataset of medical narratives consisting of verbal descriptions, eye movements,and self-reported confidence values, and used it to model physicians?
confidence in diagnosis, as wellas their diagnostic self-awareness.
The Confidence Only problem involves the expression of confidencebased on clinicians?
belief, but it is important to understand the relationship to clinicians?
actual diag-nostic performance.
This distinction is key because, while predicting confidence alone is a steppingstone, self-awareness is the ability to additionally align one?s confidence with unknown correctness,which involves human intuitive and analytical reasoning (another topic of interest to the medical field,see Hochberg et al.
(2014)).
Case studies of the most and least confident physicians revealed a com-plex relationship between confidence and correctness, and highlighted the need for exploring clinicalself-awareness.
We also defined a personalized binning scheme for physician confidence levels, takinginto account a physician?s past confidence when drawing the line between high and low confidence, andcompared this to a generalized binning scheme based on performance of all physicians.
In tandem, theseapproaches to confidence binning could be used by an intelligent diagnostic support system.We incorporated previously unused eye movement information from this dataset, and introduced trulymultimodal features which directly combined physicians?
verbal and eye movement behaviors.
Whilephysicians?
eye movement and multimodal features were not individually as powerful as verbal features,combinations of the three groups mostly produced classification improvements that were slightly betterthan, or at least as good as, their constituent feature groups in isolation.
The best performance for themajority of models was achieved by considering features from all three modalities.
This suggests thateye movements help convey confidence and diagnostic self-awareness.
The multimodal features did nothelp as much, which we believe is explained by the more flexible temporal relationship between speechand eye movements in the human mind.
We leave the multimodal alignment challenge to future work.Some pitch features implemented without speaker-dependent analysis were useful for classification, butfuture work may benefit from pitch feature representations that adapt to demographic variation.
Anotherarea for future work beyond the scope of this study includes examining alternative ways of combiningconfidence and correctness classes, such as merging the diagonals of Figure 1 into a binary classifica-tion of appropriate vs. inappropriate (i.e.
the union of over- and underconfidence).
Such alternativesmay present additional challenges for classification, but could also provide benefits for simpler clinicalsupport applications that may not be concerned with differentiating all four classes.AcknowledgementsThis work was supported by a seed award, and its dissemination partially by a Kodak Endowed Chairaward, both from the Golisano College of Computing and Information Sciences at RIT.
The original datacollection was supported by NIH grant 1 R21 LM01003901A1.
The content is solely the responsibilityof the authors and does not necessarily represent the official views of the National Institutes of Health.The authors also thank Rui Li, and appreciate the helpful comments from reviewers.ReferencesEta S. Berner and Mark L. Graber.
2008.
Overconfidence as a cause of diagnostic error in medicine.
The AmericanJournal of Medicine, 121(5A):S2?S23.Leo Breiman.
2001.
Random forests.
Machine Learning, 45:5?32.Pat Croskerry.
2003.
The importance of cognitive errors in diagnosis and strategies to minimize them.
AcademicMedicine, 78(8):775?780, August.Pat Croskerry.
2008.
Overconfidence in clinical decision making.
The American Journal of Medicine,121(5A):S24?S29.Pat Croskerry.
2009.
A universal model of diagnostic reasoning.
Academic Medicine, 84(8):1022?1028, August.Charles P. Friedman, Guido G. Gatti, Timothy M. Franz, Gwendolyn C. Murphy, Frederic M. Wolf, Paul S. Heck-erling, Paul L. Fine, Thomas M. Miller, and Arthur S. Elstein.
2005.
Do physicians know when their diagnosesare correct?
Journal of General Internal medicine, 20:334?339, April.1726Albert Gatt and Patrizia Paggio.
2013.
What and where: An empirical investigation of pointing gestures and de-scriptions in multimodal referring actions.
In Proceedings of the 14th European Workshop on Natural LanguageGeneration, pages 82?91, Sofia, Bulgaria, August 8-9.Mark Graber, Ruthanna Gordon, and Nancy Franklin.
2002.
Reducing diagnostic errors in medicine: What?s thegoal?
Academic Medicine, 77(10):981?992, October.Mark L. Graber, Nancy Franklin, and Ruthanna Gordon.
2005.
Diagnostic error in internal medicine.
Archives ofInternal Medicine, 165:1493?1499, July 11.Limor Hochberg, Cecilia Ovesdotter Alm, Esa M. Rantanen, Caroline M. DeLong, and Anne Haake.
2014.
Deci-sion style in a clinical reasoning corpus.
BioNLP 2014.Jay Katz.
1984.
Why doctors don?t disclose uncertainty.
Hastings Center Report, 14:35?44.Charles E. Kimble and Steven D. Seidel.
1991.
Vocal signs of confidence.
Journal of Nonverbal Behavior,15:99?105.Rui Li, Jeff Pelz, Pengcheng Shi, Cecilia Ovesdotter Alm, and Anne Haake.
2012a.
Learning eye movementpatterns for characterization of perceptual expertise.
In ETRA 2012 Proceedings of the Symposium on EyeTracking Research and Applications, pages 393?396, Santa Barbara, CA, March 28-30.Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.
2012b.
Learning image-derived eye movement patterns forcharacterization of perceptual expertise.
In Proceedings of CogSci 2012, pages 1900?1905.Jackson Liscombe, Julia Hirschberg, and Jennifer J. Venditti.
2005.
Detecting certainness in spoken tutorialdialogues.
In Proceedings of Interspeech 2005, pages 1837?1840, Lisbon, Portugal.Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake.
2012.
Link-ing uncertainty in physicians?
narratives to diagnostic correctness.
In Proceedings of the ACL-2012 Workshopon Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), pages 19?27, Jeju,Republic of Korea, 13 July.Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-napeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay.
2011.
Scikit-learn: Machine learning inPython.
Journal of Machine Learning Research, 12:2825?2830.Ver?onica P?erez-Rosas, Rada Mihalcea, and Louis-Phillippe Morency.
2013.
Utterance-level multimodal sentimentanalysis.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages973?982, Sofia, Bulgaria, August 4-9.Heather Pon-Barry and Stuart M. Shieber.
2011.
Recognizing uncertainty in speech.
EURASIP Journal onAdvances in Signal Processing, 2011(251753).Erika Rogers.
1996.
A study of visual reasoning in medical diagnosis.
In Proceedings of the Eighteenth AnnualConference of the Cognitive Science Society, pages 213?218, La Jolla, California, 12-15 July.Klaus R. Scherer, Harvey London, and Jared J. Wolf.
1973.
The voice of confidence: Paralinguistic cues andaudience evaluation.
Journal of Research in Personality, 7:31?44, June.Vicki L. Smith and Herbert H. Clark.
1993.
On the course of answering questions.
Journal of Memory andLanguage, 32(1):25?38.Preethi Vaidyanathan, Jeff Pelz, Wilson McCoy, Cara Calvelli, Cecilia Ovesdotter Alm, Pengcheng Shi, and AnneHaake.
2012.
Visualinguistic approach to medical image understanding.
In Proceedings of the AMIA 2012Annual Symposium, Chicago, Illinois, November.Kathryn Womack, Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and AnneHaake.
2012.
Disfluencies as extra-propositional indicators of cognitive processing.
In Proceedings of theACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 1?9, Jeju, Republic of Korea, 13 July.Kathryn Womack, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake.
2013.Markers of confidence and correctness in spoken medical narratives.
In Proceedings of Interspeech 2013, pages2549?2553, Lyon, France, August 25-29.1727
