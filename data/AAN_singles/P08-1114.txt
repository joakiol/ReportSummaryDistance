Proceedings of ACL-08: HLT, pages 1003?1011,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSoft Syntactic Constraints for Hierarchical Phrased-Based TranslationYuval Marton and Philip ResnikDepartment of Linguisticsand the Laboratory for Computational Linguistics and Information Processing (CLIP)at the Institute for Advanced Computer Studies (UMIACS)University of Maryland, College Park, MD 20742-7505, USA{ymarton, resnik} @t umiacs.umd.eduAbstractIn adding syntax to statistical MT, there isa tradeoff between taking advantage of lin-guistic analysis, versus allowing the modelto exploit linguistically unmotivated mappingslearned from parallel training data.
A num-ber of previous efforts have tackled this trade-off by starting with a commitment to linguisti-cally motivated analyses and then finding ap-propriate ways to soften that commitment.
Wepresent an approach that explores the trade-off from the other direction, starting with acontext-free translation model learned directlyfrom aligned parallel text, and then adding softconstituent-level constraints based on parsesof the source language.
We obtain substantialimprovements in performance for translationfrom Chinese and Arabic to English.1 IntroductionThe statistical revolution in machine translation, be-ginning with (Brown et al, 1993) in the early 1990s,replaced an earlier era of detailed language analy-sis with automatic learning of shallow source-targetmappings from large parallel corpora.
Over the lastseveral years, however, the pendulum has begun toswing back in the other direction, with researchersexploring a variety of statistical models that take ad-vantage of source- and particularly target-languagesyntactic analysis (e.g.
(Cowan et al, 2006; Zoll-mann and Venugopal, 2006; Marcu et al, 2006; Gal-ley et al, 2006) and numerous others).Chiang (2005) distinguishes statistical MT ap-proaches that are ?syntactic?
in a formal sense, go-ing beyond the finite-state underpinnings of phrase-based models, from approaches that are syntacticin a linguistic sense, i.e.
taking advantage of apriori language knowledge in the form of annota-tions derived from human linguistic analysis or tree-banking.1 The two forms of syntactic modeling aredoubly dissociable: current research frameworks in-clude systems that are finite state but informed bylinguistic annotation prior to training (e.g., (Koehnand Hoang, 2007; Birch et al, 2007; Hassan et al,2007)), and also include systems employing context-free models trained on parallel text without benefitof any prior linguistic analysis (e.g.
(Chiang, 2005;Chiang, 2007; Wu, 1997)).
Over time, however,there has been increasing movement in the directionof systems that are syntactic in both the formal andlinguistic senses.In any such system, there is a natural tension be-tween taking advantage of the linguistic analysis,versus allowing the model to use linguistically un-motivated mappings learned from parallel trainingdata.
The tradeoff often involves starting with a sys-tem that exploits rich linguistic representations andrelaxing some part of it.
For example, DeNeefe etal.
(2007) begin with a tree-to-string model, usingtreebank-based target language analysis, and find ituseful to modify it in order to accommodate useful?phrasal?
chunks that are present in parallel train-ing data but not licensed by linguistically motivatedparses of the target language.
Similarly, Cowan et al(2006) focus on using syntactically rich representa-tions of source and target parse trees, but they re-sort to phrase-based translation for modifiers within1See (Lopez, to appear) for a comprehensive survey.1003clauses.
Finding the right way to balance linguisticanalysis with unconstrained data-driven modeling isclearly a key challenge.In this paper we address this challenge from a lessexplored direction.
Rather than starting with a sys-tem based on linguistically motivated parse trees, webegin with a model that is syntactic only in the for-mal sense.
We then introduce soft constraints thattake source-language parses into account to a lim-ited extent.
Introducing syntactic constraints in thisrestricted way allows us to take maximal advantageof what can be learned from parallel training data,while effectively factoring in key aspects of linguis-tically motivated analysis.
As a result, we obtainsubstantial improvements in performance for bothChinese-English and Arabic-English translation.In Section 2, we briefly review the Hiero statis-tical MT framework (Chiang, 2005, 2007), uponwhich this work builds, and we discuss Chiang?s ini-tial effort to incorporate soft source-language con-stituency constraints for Chinese-English transla-tion.
In Section 3, we suggest that an insufficientlyfine-grained view of constituency constraints was re-sponsible for Chiang?s lack of strong results, andintroduce finer grained constraints into the model.Section 4 demonstrates the the value of these con-straints via substantial improvements in Chinese-English translation performance, and extends the ap-proach to Arabic-English.
Section 5 discusses theresults, and Section 6 considers related work.
Fi-nally we conclude in Section 7 with a summary andpotential directions for future work.2 Hierarchical Phrase-based Translation2.1 HieroHiero (Chiang, 2005; Chiang, 2007) is a hi-erarchical phrase-based statistical MT frameworkthat generalizes phrase-based models by permit-ting phrases with gaps.
Formally, Hiero?s trans-lation model is a weighted synchronous context-free grammar.
Hiero employs a generalization ofthe standard non-hierarchical phrase extraction ap-proach in order to acquire the synchronous rulesof the grammar directly from word-aligned paral-lel text.
Rules have the form X ?
?e?, f?
?, wheree?
and f?
are phrases containing terminal symbols(words) and possibly co-indexed instances of thenonterminal symbol X.2 Associated with each ruleis a set of translation model features, ?i(f?
, e?
); forexample, one intuitively natural feature of a rule isthe phrase translation (log-)probability ?(f?
, e?)
=log p(e?|f?)
, directly analogous to the correspondingfeature in non-hierarchical phrase-based models likePharaoh (Koehn et al, 2003).
In addition to thisphrase translation probability feature, Hiero?s fea-ture set includes the inverse phrase translation prob-ability log p(f?
|e?
), lexical weights lexwt(f?
|e?)
andlexwt(e?|f?
), which are estimates of translation qual-ity based on word-level correspondences (Koehn etal., 2003), and a rule penalty allowing the model tolearn a preference for longer or shorter derivations;see (Chiang, 2007) for details.These features are combined using a log-linearmodel, with each synchronous rule contributing?i?i?i(f?
, e?)
(1)to the total log-probability of a derived hypothesis.Each ?i is a weight associated with feature ?i, andthese weights are typically optimized using mini-mum error rate training (Och, 2003).2.2 Soft Syntactic ConstraintsWhen looking at Hiero rules, which are acquired au-tomatically by the model from parallel text, it is easyto find many cases that seem to respect linguisticallymotivated boundaries.
For example,X ?
?jingtian X1, X1 this year?,seems to capture the use of jingtian/this year asa temporal modifier when building linguistic con-stituents such as noun phrases (the election thisyear) or verb phrases (voted in the primary thisyear).
However, it is important to observe that noth-ing in the Hiero framework actually requires nonter-minal symbols to cover linguistically sensible con-stituents, and in practice they frequently do not.32This is slightly simplified: Chiang?s original formulationof Hiero, which we use, has two nonterminal symbols, X andS.
The latter is used only in two special ?glue?
rules that permitcomplete trees to be constructed via concatenation of subtreeswhen there is no better way to combine them.3For example, this rule could just as well be applied with X1covering the ?phrase?
submitted and to produce non-constituentsubstring submitted and this year in a hypothesis like The bud-get was submitted and this year cuts are likely.1004Chiang (2005) conjectured that there might bevalue in allowing the Hiero model to favor hy-potheses for which the synchronous derivation re-spects linguistically motivated source-language con-stituency boundaries, as identified using a parser.He tested this conjecture by adding a soft constraintin the form of a ?constituency feature?
: if a syn-chronous rule X ?
?e?, f??
is used in a derivation,and the span of f?
is a constituent in the source-language parse, then a term ?c is added to the modelscore in expression (1).4 Unlike a hard constraint,which would simply prevent the application of rulesviolating syntactic boundaries, using the feature tointroduce a soft constraint allows the model to boostthe ?goodness?
for a rule if it is constitent with thesource language constituency analysis, and to leaveits score unchanged otherwise.
The weight ?c, likeall other ?i, is set via minimum error rate train-ing, and that optimization process determines em-pirically the extent to which the constituency featureshould be trusted.Figure 1 illustrates the way the constituency fea-ture worked, treating English as the source languagefor the sake of readability.
In this example, ?c wouldbe added to the hypothesis score for any rule used inthe hypothesis whose source side spanned the minis-ter, a speech, yesterday, gave a speech yesterday, orthe minister gave a speech yesterday.
A rule trans-lating, say, minister gave a as a unit would receiveno such boost.Chiang tested the constituency feature forChinese-English translation, and obtained no signif-icant improvement on the test set.
The idea thenseems essentially to have been abandoned; it doesnot appear in later discussions (Chiang, 2007).3 Soft Syntactic Constraints, RevisitedOn the face of it, there are any number of possi-ble reasons Chiang?s (2005) soft constraint did notwork ?
including, for example, practical issues likethe quality of the Chinese parses.5 However, we fo-cus here on two conceptual issues underlying his useof source language syntactic constituents.4Formally, ?c(f?
, e?)
is defined as a binary feature, withvalue 1 if f?
spans a source constituent and 0 otherwise.
In thelatter case ?c?c(f?
, e?)
= 0 and the score in expression (1) isunaffected.5In fact, this turns out not to be the issue; see Section 4.Figure 1: Illustration of Chiang?s (2005) syntactic con-stituency feature, which does not distinguish among con-stituent types.First, the constituency feature treats all syntac-tic constituent types equally, making no distinctionamong them.
For any given language pair, however,there might be some source constituents that tend tomap naturally to the target language as units, andothers that do not (Fox, 2002; Eisner, 2003).
More-over, a parser may tend to be more accurate for someconstituents than for others.Second, the Chiang (2005) constituency featuregives a rule additional credit when the rule?s sourceside overlaps exactly with a source-side syntacticconstituent.
Logically, however, it might make sensenot just to give a rule X ?
?e?, f??
extra credit whenf?
matches a constituent, but to incur a cost when f?violates a constituent boundary.
Using the examplein Figure 1, we might want to penalize hypothesescontaining rules where f?
is the minister gave a (andother cases, such as minister gave, minister gave a,and so forth).6These observations suggest a finer-grained ap-proach to the constituency feature idea, retaining theidea of soft constraints, but applying them using var-ious soft-constraint constituency features.
Our firstobservation argues for distinguishing among con-stituent types (NP, VP, etc.).
Our second observa-tion argues for distinguishing the benefit of match-6This accomplishes coverage of the logically complete set ofpossibilities, which include not only f?
matching a constituentexactly or crossing its boundaries, but also f?
being properlycontained within the constituent span, properly containing it,or being outside it entirely.
Whenever these latter possibilitiesoccur, f?
will exactly match or cross the boundaries of someother constituent.1005ing constituents from the cost of crossing constituentboundaries.
We therefore define a space of new fea-tures as the cross product{CP, IP, NP, VP, .
.
.}
?
{=,+}.where = and + signify matching and crossing bound-aries, respectively.
For example, ?NP= would de-note a binary feature that matches whenever the spanof f?
exactly covers an NP in the source-side parsetree, resulting in ?NP= being added to the hypoth-esis score (expression (1)).
Similarly, ?VP+ woulddenote a binary feature that matches whenever thespan of f?
crosses a VP boundary in the parse tree,resulting in ?VP+ being subtracted from the hypoth-esis score.7 For readability from this point forward,we will omit ?
from the notation and refer to featuressuch as NP= (which one could read as ?NP match?
),VP+ (which one could read as ?VP crossing?
), etc.In addition to these individual features, we definethree more variants:?
For each constituent type, e.g.
NP, we definea feature NP_ that ties the weights of NP= andNP+.
If NP= matches a rule, the model score isincremented by ?NP_, and if NP+ matches, themodel score is decremented by the same quan-tity.?
For each constituent type, e.g.
NP, we define aversion of the model, NP2, in which NP= andNP+ are both included as features, with sepa-rate weights ?NP= and ?NP+.?
We define a set of ?standard?
linguistic labelscontaining {CP, IP, NP, VP, PP, ADJP, ADVP,QP, LCP, DNP} and excluding other labels suchas PRN (parentheses), FRAG (fragment), etc.8We define feature XP= as the disjunction of{CP=, IP=, .
.
., DNP=}; i.e.
its value equals 1for a rule if the span of f?
exactly covers a con-stituent having any of the standard labels.
The7Formally, ?VP+ simply contributes to the sum in expres-sion (1), as with all features in the model, but weight optimiza-tion using minimum error rate training should, and does, auto-matically assign this feature a negative weight.8We map SBAR and S labels in Arabic parses to CP and IP,respectively, consistent with the Chinese parses.
We map Chi-nese DP labels to NP.
DNP and LCP appear only in Chinese.
Weran no ADJP experiment in Chinese, because this label virtuallyaways spans only one token in the Chinese parses.definitions of XP+, XP_, and XP2 are analo-gous.?
Similarly, since Chiang?s original constituencyfeature can be viewed as a disjunctive ?all-labels=?
feature, we also defined ?all-labels+?,?all-labels2?, and ?all-labels_?
analogously.4 ExperimentsWe carried out MT experiments for translationfrom Chinese to English and from Arabic to En-glish, using a descendant of Chiang?s Hiero sys-tem.
Language models were built using the SRILanguage Modeling Toolkit (Stolcke, 2002) withmodified Kneser-Ney smoothing (Chen and Good-man, 1998).
Word-level alignments were obtainedusing GIZA++ (Och and Ney, 2000).
The base-line model in both languages used the feature setdescribed in Section 2; for the Chinese baseline wealso included a rule-based number translation fea-ture (Chiang, 2007).In order to compute syntactic features, we an-alyzed source sentences using state of the art,tree-bank trained constituency parsers ((Huang etal., 2008) for Chinese, and the Stanford parserv.2007-08-19 for Arabic (Klein and Manning,2003a; Klein and Manning, 2003b)).
In additionto the baseline condition, and baseline plus Chi-ang?s (2005) original constituency feature, experi-mental conditions augmented the baseline with ad-ditional features as described in Section 3.All models were optimized and tested using theBLEU metric (Papineni et al, 2002) with the NIST-implemented (?shortest?)
effective reference length,on lowercased, tokenized outputs/references.
Sta-tistical significance of difference from the baselineBLEU score was measured by using paired boot-strap re-sampling (Koehn, 2004).94.1 Chinese-EnglishFor the Chinese-English translation experiments, wetrained the translation model on the corpora in Ta-ble 1, totalling approximately 2.1 million sentencepairs after GIZA++ filtering for length ratio.
Chi-nese text was segmented using the Stanford seg-menter (Tseng et al, 2005).9Whenever we use the word ?significant?, we mean ?statis-tically significant?
(at p < .05 unless specified otherwise).1006LDC ID DescriptionLDC2002E18 Xinhua Ch/Eng Par News V1 betaLDC2003E07 Ch/En Treebank Par CorpusLDC2005T10 Ch/En News Mag Par Txt (Sinorama)LDC2003E14 FBIS Multilanguage TxtsLDC2005T06 Ch News Translation Txt Pt 1LDC2004T08 HK Par Text (only HKNews)Table 1: Training corpora for Chinese-English translationWe trained a 5-gram language model using theEnglish (target) side of the training set, pruning 4-gram and 5-gram singletons.
For minimum errorrate training and development we used the NISTMTeval MT03 set.Table 2 presents our results.
We first evaluatedtranslation performance using the NIST MT06 (nist-text) set.
Like Chiang (2005), we find that the orig-inal, undifferentiated constituency feature (Chiang-05) introduces a negligible, statistically insignificantimprovement over the baseline.
However, we findthat several of the finer-grained constraints (IP=,VP=, VP+, QP+, and NP=) achieve statisticallysignificant improvements over baseline (up to .74BLEU), and the latter three also improve signifi-cantly on the undifferentiated constituency feature.By combining multiple finer-grained syntactic fea-tures, we obtain significant improvements of up to1.65 BLEU points (NP_, VP2, IP2, all-labels_, andXP+).We also obtained further gains using combina-tions of features that had performed well; e.g., con-dition IP2.VP2.NP_ augments the baseline featureswith IP2 and VP2 (i.e.
IP=, IP+, VP= and VP+),and NP_ (tying weights of NP= and NP+; see Sec-tion 3).
Since component features in those combi-nations were informed by individual-feature perfor-mance on the test set, we tested the best perform-ing conditions from MT06 on a new test set, NISTMT08.
NP= and VP+ yielded significant improve-ments of up to 1.53 BLEU.
Combination conditionsreplicated the pattern of results from MT06, includ-ing the same increasing order of gains, with im-provements up to 1.11 BLEU.4.2 Arabic-EnglishFor Arabic-English translation, we used the train-ing corpora in Table 3, approximately 100,000 sen-Chinese MT06 MT08Baseline .2624 .2064Chiang-05 .2634 .2065PP= .2607DNP+ .2621CP+ .2622AP+ .2633AP= .2634DNP= .2640IP+ .2643PP+ .2644LCP= .2649LCP+ .2654CP= .2657NP+ .2662QP= .2674^+ .2071IP= .2680*+ .2061VP= .2683* .2072VP+ .2693**++ .2109*+QP+ .2694**++ .2091NP= .2698**++ .2217**++Multiple / conflated features:QP2 .2614NP2 .2621XP= .2630XP2 .2633all-labels+ .2633VP_ .2637QP_ .2641NP.VP.IP=.QP.VP+ .2646IP_ .2647IP2+VP2 .2649all-labels2 .2673*- .2070NP_ .2690**++ .2101^+IP2.VP2.NP_ .2699**++ .2105*+VP2 .2722**++ .2123**++all-labels_ .2731**++ .2125*++IP2 .2750**++ .2132**+XP+ .2789**++ .2175**++Table 2: Chinese-English results.
*: Significantly betterthan baseline (p < .05).
**: Significantly better thanbaseline (p < .01).
^: Almost significantly better thanbaseline (p < .075).
+: Significantly better than Chiang-05 (p < .05).
++: Significantly better than Chiang-05(p < .01).
-: Almost significantly better than Chiang-05(p < .075).1007LDC ID DescriptionLDC2004T17 Ar News Trans Txt Pt 1LDC2004T18 Ar/En Par News Pt 1LDC2005E46 Ar/En Treebank En TranslationLDC2004E72 eTIRR Ar/En News TxtTable 3: Training corpora for Arabic-English translationtence pairs after GIZA++ length-ratio filtering.
Wetrained a trigram language model using the Englishside of this training set, plus the English Gigawordv2 AFP and Gigaword v1 Xinhua corpora.
Devel-opment and minimum error rate training were doneusing the NIST MT02 set.Table 4 presents our results.
We first tested onon the NIST MT03 and MT06 (nist-text) sets.
OnMT03, the original, undifferentiated constituencyfeature did not improve over baseline.
Two individ-ual finer-grained features (PP+ and AdvP=) yieldedstatistically significant gains up to .42 BLEU points,and feature combinations AP2, XP2 and all-labels2yielded significant gains up to 1.03 BLEU points.XP2 and all-labels2 also improved significantly onthe undifferentiated constituency feature, by .72 and1.11 BLEU points, respectively.For MT06, Chiang?s original feature improved thebaseline significantly ?
this is a new result usinghis feature, since he did not experiment with Ara-bic ?
as did our our IP=, PP=, and VP= condi-tions.
Adding individual features PP+ and AdvP=yielded significant improvements up to 1.4 BLEUpoints over baseline, and in fact the improvement forindividual feature AdvP= over Chiang?s undifferen-tiated constituency feature approaches significance(p < .075).More important, several conditions combiningfeatures achieved statistically significant improve-ments over baseline of up 1.94 BLEU points: XP2,IP2, IP, VP=.PP+.AdvP=, AP2, PP+.AdvP=, andAdvP2.
Of these, AdvP2 is also a significant im-provement over the undifferentiated constituencyfeature (Chiang-05), with p < .01.
As we didfor Chinese, we tested the best-performing modelson a new test set, NIST MT08.
Consistent patternsreappeared: improvements over the baseline up to1.69 BLEU (p < .01), with AdvP2 again in thelead (also outperforming the undifferentiated con-stituency feature, p < .05).Arabic MT03 MT06 MT08Baseline .4795 .3571 .3571Chiang-05 .4787 .3679** .3678**VP+ .4802 .3481AP+ .4856 .3495IP+ .4818 .3516CP= .4815 .3523NP= .4847 .3537NP+ .4800 .3548AP= .4797 .3569AdvP+ .4852 .3572CP+ .4758 .3578IP= .4811 .3636** .3647**PP= .4801 .3651** .3662**VP= .4803 .3655** .3694**PP+ .4837** .3707** .3700**AdvP= .4823** .3711**- .3717**Multiple / conflated features:XP+ .4771 .3522all-labels2 .4898**+ .3536 .3572all-labels_ .4828 .3548VP2 .4826 .3552NP2 .4832 .3561AdvP.VP.PP.IP= .4826 .3571VP_ .4825 .3604all-labels+ .4825 .3600XP2 .4859**+ .3605^ .3613**IP2 .4793 .3611* .3593IP_ .4791 .3635* .3648**XP= .4808 .3659** .3704**+VP=.PP+.AdvP= .4833** .3677** .3718**AP2 .4840** .3692** .3719**PP+.AdvP= .4777 .3708** .3680**AdvP2 .4803 .3765**++ .3740**+Table 4: Arabic-English Experiments.
Results aresorted by MT06 BLEU score.
*: Better than baseline(p < .05).
**: Better than baseline (p < .01).
+: Bet-ter than Chiang-05 (p < .05).
++: Better than Chiang-05(p < .01).
-: Almost significantly better than Chiang-05(p < .075)10085 DiscussionThe results in Section 4 demonstrate, to our knowl-edge for the first time, that significant and sometimessubstantial gains over baseline can be obtained byincorporating soft syntactic constraints into Hiero?stranslation model.
Within language, we also seeconsiderable consistency across multiple test sets, interms of which constraints tend to help most.Furthermore, our results provide some insight intowhy the original approach may have failed to yield apositive outcome.
For Chinese, we found that whenwe defined finer-grained versions of the exact-matchfeatures, there was value for some constituencytypes in biasing the model to favor matching thesource language parse.
Moreover, we found thatthere was significant value in allowing the modelto be sensitive to violations (crossing boundaries)of source parses.
These results confirm that parserquality was not the limitation in the original work(or at least not the only limitation), since in our ex-periments the parser was held constant.Looking at combinations of new features, some?double-feature?
combinations (VP2, IP2) achievedlarge gains, although note that more is not neces-sarily better: combinations of more features did notyield better scores, and some did not yield any gainat all.
No conflated feature reached significance, butit is not the case that all conflated features are worsethan their same-constituent ?double-feature?
coun-terparts.
We found no simple correlation betweenfiner-grained feature scores (and/or boundary con-dition type) and combination or conflation scores.Since some combinations seem to cancel individ-ual contributions, we can conclude that the higherthe number of participant features (of the kinds de-scribed here), the more likely a cancellation effect is;therefore, a ?double-feature?
combination is morelikely to yield higher gains than a combination con-taining more features.We also investigated whether non-canonical lin-guistic constituency labels such as PRN, FRAG,UCP and VSB introduce ?noise?, by means of theXP features ?
the XP= feature is, in fact, simply theundifferentiated constituency feature, but sensitiveonly to ?standard?
XPs.
Although performance ofXP=, XP2 and all-labels+ were similar to that of theundifferentiated constituency feature, XP+ achievedthe highest gain.
Intuitively, this seems plausible:the feature says, at least for Chinese, that a transla-tion hypothesis should incur a penalty if it is trans-lating a substring as a unit when that substring is nota canonical source constituent.Having obtained positive results with Chinese, weexplored the extent to which the approach mightimprove translation using a very different sourcelanguage.
The approach on Arabic-English trans-lation yielded large BLEU gains over baseline, aswell as significant improvements over the undiffer-entiated constituency feature.
Comparing the twosets of experiments, we see that there are definitelylanguage-specific variations in the value of syntacticconstraints; for example, AdvP, the top performer inArabic, cannot possibly perform well for Chinese,since in our parses the AdvP constituents rarely in-clude more than a single word.
At the same time,some IP and VP variants seem to do generally wellin both languages.
This makes sense, since ?
atleast for these language pairs and perhaps more gen-erally ?
clauses and verb phrases seem to corre-spond often on the source and target side.
We foundit more surprising that no NP variant yielded muchgain in Arabic; this question will be taken up in fu-ture work.6 Related WorkSpace limitations preclude a thorough review ofwork attempting to navigate the tradeoff between us-ing language analyzers and exploiting unconstraineddata-driven modeling, although the recent literatureis full of variety and promising approaches.
We limitourselves here to several approaches that seem mostclosely related.Among approaches using parser-based syntacticmodels, several researchers have attempted to re-duce the strictness of syntactic constraints in or-der to better exploit shallow correspondences inparallel training data.
Our introduction has al-ready briefly noted Cowan et al (2006), who relaxparse-tree-based alignment to permit alignment ofnon-constituent subphrases on the source side, andtranslate modifiers using a separate phrase-basedmodel, and DeNeefe et al (2007), who modifysyntax-based extraction and binarize trees (follow-ing (Wang et al, 2007b)) to improve phrasal cov-1009erage.
Similarly, Marcu et al (2006) relax theirsyntax-based system by rewriting target-side parsetrees on the fly in order to avoid the loss of ?non-syntactifiable?
phrase pairs.
Setiawan et al (2007)employ a ?function-word centered syntax-based ap-proach?, with synchronous CFG and extended ITGmodels for reordering phrases, and relax syntac-tic constraints by only using a small number func-tion words (approximated by high-frequency words)to guide the phrase-order inversion.
Zollman andVenugopal (2006) start with a target language parserand use it to provide constraints on the extraction ofhierarchical phrase pairs.
Unlike Hiero, their trans-lation model uses a full range of named nonterminalsymbols in the synchronous grammar.
As an alter-native way to relax strict parser-based constituencyrequirements, they explore the use of phrases span-ning generalized, categorial-style constituents in theparse tree, e.g.
type NP/NN denotes a phrase likethe great that lacks only a head noun (say, wall) inorder to comprise an NP.In addition, various researchers have explored theuse of hard linguistic constraints on the source side,e.g.
via ?chunking?
noun phrases and translatingthem separately (Owczarzak et al, 2006), or by per-forming hard reorderings of source parse trees inorder to more closely approximate target-languageword order (Wang et al, 2007a; Collins et al, 2005).Finally, another soft-constraint approach that canalso be viewed as coming from the data-driven side,adding syntax, is taken by Riezler and Maxwell(2006).
They use LFG dependency trees on bothsource and target sides, and relax syntactic con-straints by adding a ?fragment grammar?
for un-parsable chunks.
They decode using Pharaoh, aug-mented with their own log-linear features (such asp(esnippet|fsnippet) and its converse), side by side to?traditional?
lexical weights.
Riezler and Maxwell(2006) do not achieve higher BLEU scores, butdo score better according to human grammaticalityjudgments for in-coverage cases.7 ConclusionWhen hierarchical phrase-based translation was in-troduced by Chiang (2005), it represented a new andsuccessful way to incorporate syntax into statisticalMT, allowing the model to exploit non-local depen-dencies and lexically sensitive reordering withoutrequiring linguistically motivated parsing of eitherthe source or target language.
An approach to incor-porating parser-based constituents in the model wasexplored briefly, treating syntactic constituency as asoft constraint, with negative results.In this paper, we returned to the idea of linguis-tically motivated soft constraints, and we demon-strated that they can, in fact, lead to substantialimprovements in translation performance when in-tegrated into the Hiero framework.
We accom-plished this using constraints that not only dis-tinguish among constituent types, but which alsodistinguish between the benefit of matching thesource parse bracketing, versus the cost of us-ing phrases that cross relevant bracketing bound-aries.
We demonstrated improvements for Chinese-English translation, and succeed in obtaining sub-stantial gains for Arabic-English translation, as well.Our results contribute to a growing body of workon combining monolingually based, linguisticallymotivated syntactic analysis with translation mod-els that are closely tied to observable parallel train-ing data.
Consistent with other researchers, we findthat ?syntactic constituency?
may be too coarse a no-tion by itself; rather, there is value in taking a finer-grained approach, and in allowing the model to de-cide how far to trust each element of the syntacticanalysis as part of the system?s optimization process.AcknowledgmentsThis work was supported in part by DARPA primeagreement HR0011-06-2-0001.
The authors wouldlike to thank David Chiang and Adam Lopez formaking their source code available; the StanfordParser team and Mary Harper for making theirparsers available; David Chiang, Amy Weinberg,and CLIP Laboratory colleagues, particularly ChrisDyer, Adam Lopez, and Smaranda Muresan, for dis-cussion and invaluable assistance.1010ReferencesAlexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the ACL Workshop onStatistical Machine Translation 2007.P.F.
Brown, S.A.D.
Pietra, V.J.D.
Pietra, and R.L.
Mercer.1993.
The mathematics of statistical machine transla-tion.
Computational Linguistics, 19(2):263?313.S.
F. Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Tech.Report TR-10-98, Comp.
Sci.
Group, Harvard U.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL-05, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL-05.Brooke Cowan, Ivona Kucerova, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proc.
EMNLP.S DeNeefe, K. Knight, W. Wang, and D. Marcu.
2007.What can syntax-based MT learn from phrase-basedMT?
In Proceedings of EMNLP-CoNLL.J.
Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In ACL Companion Vol.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proc.
EMNLP 2002.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING/ACL-06.H.
Hassan, K. Sima?an, and A.
Way.
2007.
Integratingsupertags into phrase-based statistical machine trans-lation.
In Proc.
ACL-07, pages 288?295.Zhongqiang Huang, Denis Filimonov, and Mary Harper.2008.
Accuracy enhancements for mandarin parsing.Tech.
report, University of Maryland.Dan Klein and Christopher D. Manning.
2003a.
Accu-rate unlexicalized parsing.
In Proceedings of ACL-03,pages 423?430.Dan Klein and Christopher D. Manning.
2003b.
Fastexact inference with a factored model for natural lan-guage parsing.
Advances in Neural Information Pro-cessing Systems, 15(NIPS 2002):3?10.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proc.
EMNLP+CoNLL, pages 868?876, Prague.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL, pages 127?133.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP.Adam Lopez.
(to appear).
Statistical machine transla-tion.
ACM Computing Surveys.
Earlier version: ASurvey of Statistical Machine Translation.
U. of Mary-land, UMIACS tech.
report 2006-47.
Apr 2007.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machine trans-lation with syntactified target language phrases.
InProc.
EMNLP, pages 44?52.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the ACL, pages 440?447.
GIZA++.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the ACL, pages 160?167.K.
Owczarzak, B. Mellebeek, D. Groves, J.
Van Gen-abith, and A.
Way.
2006.
Wrapper syntax forexample-based machine translation.
In Proceedingsof the 7th Conference of the Association for MachineTranslation in the Americas, pages 148?155.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002.
Corpusbasedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish results.
In Pro-ceedings of the Human Language Technology Confer-ence (ACL?2002), pages 124?127, San Diego, CA.Stefan Riezler and John Maxwell.
2006.
Grammaticalmachine translation.
In Proc.
HLT-NAACL, New York,NY.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007.Ordering phrases with function words.
In Proceedingsof the 45th Annual Meeting of the Association of Com-putational Linguistics, pages 712?719.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter.
In FourthSIGHAN Workshop on Chinese Language Processing.Chao Wang, Michael Collins, and Phillip Koehn.
2007a.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of EMNLP.Wei Wang, Kevin Knight, and Daniel Marcu.
2007b.
Bi-narizing syntax trees to improve syntax-based machinetranslation accuracy.
In Proc.
EMNLP+CoNLL 2007.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the SMT Workshop, HLT-NAACL.1011
