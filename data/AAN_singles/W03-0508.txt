Examining the consensus between human summaries: initialexperiments with factoid analysisHans van HalterenDepartment of Language and SpeechUniversity of Nijmegen, The NetherlandsSimone TeufelComputer LaboratoryCambridge University, UKAbstractWe present a new approach to summary evaluationwhich combines two novel aspects, namely (a) con-tent comparison between gold standard summaryand system summary via factoids, a pseudo-semanticrepresentation based on atomic information unitswhich can be robustly marked in text, and (b) useof a gold standard consensus summary, in our casebased on 50 individual summaries of one text.
Eventhough future work on more than one source text isimperative, our experiments indicate that (1) rank-ing with regard to a single gold standard summary isinsufficient as rankings based on any two randomlychosen summaries are very dissimilar (correlationsaverage ?
= 0.20), (2) a stable consensus summarycan only be expected if a larger number of sum-maries are collected (in the range of at least 30-40summaries), and (3) similarity measurement usingunigrams shows a similarly low ranking correlationwhen compared with factoid-based ranking.1 IntroductionIt is an understatement to say that measuring thequality of summaries is hard.
In fact, there is unan-imous consensus in the summarisation communitythat evaluation of summaries is a monstrously diffi-cult task.
In the past years, there has been quite alot of summarisation work that has effectively aimedat finding viable evaluation strategies (Spa?rck Jones,1999; Jing et al, 1998; Donaway et al, 2000).
Large-scale conferences like SUMMAC (Mani et al, 1999)and DUC (2002) have unfortunately shown weak re-sults in that current evaluation measures could notdistinguish between automatic summaries ?
thoughthey are effective enough to distinguish them fromhuman-written summaries.In principle, the best way to evaluate a summaryis to try to perform the task for which the sum-mary was meant in the first place, and measure thequality of the summary on the basis of degree ofsuccess in executing the task.
However, such extrin-sic evaluations are so time-consuming to set up thatthey cannot be used for the day-to-day evaluationneeded during system development.
So in practice,a method for intrinsic evaluation is needed, wherethe properties of the summary itself are examined,independent of its application.We think one of the reasons for the difficulty of anintrinsic evaluation is that summarisation has to callupon at least two hard subtasks: selection of infor-mation and production of new text.
Both tasks areknown from various NLP fields (e.g.
information re-trieval and information extraction for selection; gen-eration and machine translation (MT) for produc-tion) to be not only hard to execute, but also hard toevaluate.
This is caused for a large part by the factthat in both cases there is no single ?best?
result, butrather various ?good?
results.
It is hence no won-der that the evaluation of summarisation, combiningthese two, is even harder.
The general approach forintrinsic evaluations, then (Mani, 2001), is to sepa-rate the evaluation of the form of the text (quality)and its information content (informativeness).In this paper, we will focus on the latter, the in-trinsic evaluation of informativeness, and we will ad-dress two aspects: the (in)sufficiency of the singlehuman summary to measure against, and the infor-mation unit on which similarity measures are based.1.1 Gold standardsIn various NLP fields, such as POS tagging, systemsare tested by way of comparison against a ?gold stan-dard?, a manually produced result which is supposedto be the ?correct?, ?true?
or ?best?
result.
Thispresupposes, however, that there is a single ?best?result.
In summarisation there appears to be no ?onetruth?, as is evidenced by a low agreement betweenhumans in producing gold standard summaries bysentence selection (Rath et al, 1961; Jing et al,1998; Zechner, 1996), and low overlap measures be-tween humans when gold standards summaries arecreated by reformulation in the summarisers?
ownwords (e.g.
the average overlap for the 542 singledocument summary pairs in DUC-02 was only about47%).But even though the non-existence of any one goldstandard is generally acknowledged in the summari-sation community, actual practice nevertheless ig-nores this.
Comparisons against a single gold stan-dard are widely used, due to the expense of compil-ing summary gold standards and the lack of compos-ite measures for comparison to more than one goldstandard.In a related field, information retrieval (IR), theproblem of subjectivity of relevance judgements iscircumvented by extensive sampling: many differ-ent queries are collected to level out the differencehumans have in suggesting queries and in select-ing relevant documents.
While relevance judgementsbetween humans remain different, Voorhees (2000)shows that the relative rankings of systems are nev-ertheless stable across annotators, which means thatmeaningful IR measures have been found despite theinherent subjectivity of relevance judgements.Similarly, in MT, the recent Bleu measure alsouses the idea that one gold standard is not enough.In an experiment, Papineni et al (2001) based anevaluation on a collection of four reference trans-lations of 40 general news stories and showed theevaluation to be comparable to human judgement.Lin and Hovy (2002) examine the use of a multi-ple gold standard for summarisation evaluation, andconclude ?we need more than one model summaryalthough we cannot estimate how many model sum-maries are required to achieve reliable automatedsummary evaluation?.
We explore the differencesand similarities between various human summariesin order to create a basis for such an estimate, and asa side-effect, also re-examine the degree of differencebetween the use of a single summary gold standardand the use of a compound gold standard.1.2 Similarity measuresThe second aspect we examine is the similaritymeasure to be used for gold standard comparison.In principle, the comparison can be done via co-selection of extracted sentences (Rath et al, 1961;Jing et al, 1998; Zechner, 1996), by string-based sur-face measures (Lin and Hovy, 2002; Saggion et al,2002), or by subjective judgements of the amountof information overlap (DUC, 2002).
The rationalefor using information overlap judgement as the mainevaluation metric for DUC is the wish to measurethe meaning of sentences rather than use surface-based similarity such as co-selection (which does noteven take identical information expressed in differentsentences into account) and string-based measures.In the DUC competitions, assessors judge the infor-mational overlap between ?model units?
( elemen-tary discourse units (EDUs), i.e.
clause-like units,taken from the gold standard summary) and ?peerunits?
(sentences taken from the participating sum-maries) on the basis of the question: ?How muchof the information in a model unit is contained in apeer unit: all of it, most, some, any, or none.?
Thisoverlap judgement is done for each system-producedsummary, and weighted recall measures report howmuch gold standard information is present in thesummaries.However, Lin and Hovy (2002) report low agree-ment for two tasks: producing the human summaries(around 40%), and assigning information overlap be-tween them.
In those cases where annotators hadto judge a pair consisting of a gold standard sen-tence and a system sentence more than once (be-cause different systems returned the same sentence),they agreed with their own prior judgement in only82% of the cases.
This relatively low intra-annotatoragreement points to the fact that the overlap judge-ment remains a subjective task where judges willdisagree.
Lin and Hovy show the instability of theevaluation, expressed in system rankings.We propose a gold standard comparison basedon factoids, a pseudo-semantic representation of thetext, which measures information rather than stringsimilarity, like DUC, but which is more objectivethan DUC-style information overlap judgement.2 Data and factoid annotationOur goal is to compare the information content ofdifferent summaries of the same text.
In this ini-tial investigation we decided to focus on a singletext.
The text used for the experiment is a BBCreport on the killing of the Dutch politician PimFortuyn.
It is about 600 words long, and containsa mix of factual information and personal reactions.Our guidelines asked the human subjects to writegeneric summaries of roughly 100 words.
We askedthem to formulate the summary in their own words,so that we can also see which different textual formsare produced for the same information.Knowledge about the variability of expression isimportant both for evaluation and system building,and particularly so in in multi-document summarisa-tion, where redundant information is likely to occurin different textual forms.We used two types of human summarisers.
Thelargest group consisted of Dutch students of Englishand of Business Communications (with English asa chosen second language).
Of the 60 summarieswe received, we had to remove 20.
Summaries wereremoved if it was obvious from the summary thatthe student had insufficient skill in English or if theword count was too high (above 130 words).
A sec-ond group consisted of 10 researchers, who are eithernative or near-native English speakers.
With thisgroup there were no problems with language, for-mat or length, and we could use all 10 summaries.Our total number of summaries was thus 50.2.1 The factoid as atomic information unitsWe use atomic semantic units called factoids to rep-resent the meaning of a sentence.
For instance, werepresent the sentenceThe police have arrested a white Dutch man.by the union of the following factoids:FP20 A suspect was arrestedFP21 The police did the arrestingFP24 The suspect is whiteFP25 The suspect is DutchFP26 The suspect is maleNote that in this case, factoids correspond to ex-pressions in a FOPL-style semantics, which are com-positionally interpreted.
However, we define atom-icity as a concept which depends on the set of sum-maries we work with.
If a certain set of potentialfactoids always occurs together, this set of factoidsis treated as one factoid, because differentiation ofthis set would not help us in distinguishing the sum-maries.
If we had found, e.g., that there is no sum-mary that mentions only one of FP25 and FP26,those factoids would be combined into one new fac-toid ?FP27 The suspect is a Dutch man?.Our definition of atomicity means that the?amount?
of information associated with one factoidcan vary from a single word to an entire sentence.An example for a large chunk of information thatoccurred atomically in our texts was the fact thatthe victim wanted to become PM (FV71), a factoidwhich covers an entire sentence.
On the other hand,a single word may contain several factoids.
The word?gunman?
leads to two factoids: ?FP24 The perpe-trator is male?
and ?FA20 A gun was used in theattack?.The advantage of our functional, summary-set-dependent definition of atomicity is that the defi-nition of what counts as a factoid is more objec-tive than if factoids had to be invented by intuition,which is hard.
One possible disadvantage of our def-inition of atomicity (which is dependent on a givenset of summaries) is that the set of factoids usedmay have to be adjusted if further summaries areadded to the collection.
In practice, for a fixed setof summaries for experiments, this is less of an issue.We decompose meanings into separate (composi-tionally interpreted) factoids, if there are mentionsin our texts which imply information overlap.
Ifone summary contains ?was murdered?
and another?was shot dead?, we can identify the factoidsFA10 There was an attackFA40 The victim diedFA20 A gun was usedThe first summary contains only the first two fac-toids, whereas the second contains all three.
Thatway, the semantic similarity between related wordscan be expressed.2.2 Compositionality, generalisation andfactualityThe guidelines for manual annotation of summarieswith factoids stated that only factoids which areexplicitly expressed in the text should be marked.When we identified factoids in our actual summarycollection, most factoids turned out to be indepen-dent of each other, i.e.
the union of the factoids canbe compositionally interpreted.
However, there arerelations between factoids which are not as straight-forward.
For instance, in the case of ?FA21 Mul-tiple shots were fired?
and ?FA22 Six shots werefired?, FA22 implies FA21; any attempt to expressthe relationship between the factoids in a composi-tional way would result in awkward factoids.
Weaccept that there are factoids which are most natu-rally expressed as generalisations of other factoids,and record for each factoid a list of factoids that aremore general than it is, so that we can include theserelated factoids as well.
In one view of our data, if asummary states FA22, FA21 is automatically added.In addition to generality, there are two furthercomplicated phenomena we had to deal with.
Thefirst one is real inference, rather than generalisation,as in the following cases:FL52 The scene of the murder had tight securitychecksFL51 The scene of the murder was difficult toget intoFL50 It is unclear how the perpetrator got tothe victimFL52 implies (in the sense of real inference) FL51,which in turn implies FL50.
We again record infer-ence relations and automatically compute the transi-tive closure of all inferences, but we do not currentlyformally distinguish them from the simpler general-isation relations.The second phenomenon is the description of peo-ple?s opinions.
In our source document, quotationsof the reactions of several politicians were given.
Inthe summaries, our subjects often generalised thesereactions and produced statements such asDutch as well as international politicians have expressedtheir grief and disbelief.As more than one entity can be reported as sayingthe same thing, straightforward factoid union is notpowerful enough to accurately represent the attri-bution of opinions, as our notation does not containvariables for discourse referents and quoted state-ments.
We therefore revert to a separate set of fac-toids, which are multiplied-out factoids that com-bine the statement (what is being said) together witha description of who said it.
Elements of the descrip-tion can be interpreted in a compositional manner.For instance, the above sentence is expressed inour notation asOG10 Grief was expressedOG60 Dutch persons or organizations expressedgriefOG62 International persons or organizationsexpressed griefOG40 Politicians expressed griefOS10 Disbelief was expressedOS60 Dutch persons or organizations expresseddisbeliefOS62 International persons or organizationsexpressed disbeliefOS40 Politicians expressed disbeliefAnother problem with attribution of opinions isthat there is not always a clear distinction betweenfact and opinion.
For instance, the following sen-tence is presented as opinion in the original ?Geral-dine Coughlan in the Hague says it would have beendifficult to gain access to the media park.?
Never-theless, our summarisers often decided to representsuch opinions as facts, ie.
as ?The media park wasdifficult to gain entry to.?
?
in fact, in our data,every summary containing this factoid presents itas fact.
For now, we have taken the pragmatic ap-proach that the classification of factoids into factualand opinion factoids is determined by the actual rep-resentation of the information in the summaries (cf.FL51 above, where the first letter ?F?
stands forfactual, the first letter ?O?
for opinion).The factoid approach can capture much finershades of meaning differentiations than DUC-styleinformation overlap does ?
in an example from Linand Hovy (2002), an assessor judged some contentoverlap between ?Thousands of people are feareddead?
and ?3,000 and perhaps ... 5,000 people havebeen killed.?
In our factoid representation, a dis-tinction between ?killed?
and ?feared dead?
wouldbe made, and different numbers of people mentionedwould have been differentiated.2.3 Factoid annotationThe authors have independently marked the pres-ence of factoids in all summaries in the collection.Factoid annotation of a 100 word summary takesroughly half an hour.
Even with only short guide-lines, the agreement on which factoids are presentin a summary appears to be high.
The recall of anindividual annotator with regard to the consensusannotation is about 96%, and precision about 97%.This means that we can work with the current fac-toid presence table with reasonable confidence.Whereas single summaries contain between 32 and55 factoids, the collection as a whole contains 256different factoids.
Figure 1 shows the growth of thenumber of factoids with the size of the collection (1to 40 summaries).
We assume that the curve is Zip-fian.
This observation implies that larger numbersof summaries are necessary if we are looking for adefinitive factoid list of a document.Figure 1: Average number of factoids in collectionsof size 1?40The maximum number of possible factoids is notbounded by the number of factoids occurring in thedocument itself.
As we explained above, factoidscome into existence because they are observed in thecollection of summaries, and summaries sometimescontain factoids which are not actually present in thedocument.
Examples of such factoids are ?FP31 Thesuspect has made no statement?, which is true butnot stated in the source text, and ?FP23 The suspectwas arrested on the scene?, which is not even true.The reasons for such ?creative?
factoids vary fromthe expression of the summarisers?
personal knowl-edge or opinion to misinterpretation of the sourcetext.
In total we find 87 such factoids, 51 factualones and 36 incorrect generalisations of attribution.Of the remaining 169 ?correct?
factoids, most(125) are factual.
Within these factoids, we find74 generalisation links.
The rest of the factoids con-cern opinions and their attribution.
There are 18descriptions of opinion, with 11 generalisation links,and 26 descriptions of attribution, with 16 general-isation links.
For all types, we see that most factsare being represented at differing levels of generali-sation.
Some of the generalisation links are part of3- or 4-link hierarchies, e.g.
?FV40 Victim outspo-ken about/campaigning on immigration issues?
(26mentions) to ?FV41 Victim was anti- immigration?
(23) to ?FV42 Victim wanted to close borders to im-migration?
(9), or ?FV50 Victim outspoken aboutrace/religion issues?
(17 mentions) to ?FV51 Vic-tim outspoken about Islam/Muslims?
(16) to ?FV52Victim made negative remarks about Islam?
(14) to?FV53 Victim called Islam a backward religion?
(9).It is not surprising that more specific factoids areless frequent than their generalisations, but we ex-pect interesting correlations between a factoid?s im-portance and the degree and shape of the declineof its generalisation hierarchy, especially where fac-toids about the attribution of opinion are concerned.This is an issue for further research.3 Human summaries as benchmarkfor evaluationIf we plan to use human summaries as a refer-ence point for the evaluation of machine-made sum-maries, we are assuming that there is some consensusbetween the human summarisers as to which infor-mation is important enough to include in a summary.Whether such consensus actually exists is uncertain.In very broad terms, we can distinguish four possiblescenarios:1.
There is a good consensus between all humansummarisers.
A large percentage of the factoidspresent in the summaries is in fact present in alarge percentage of the summaries.
We can de-termine whether this is so by measuring factoidoverlap.2.
There is no such overall consensus between allsummarisers, but there are subsets of summaris-ers between whom consensus exists.
Each ofthese subsets has summarised from a particularpoint of view, even though a generic summarywas requested, and the point of view has ledto group consensus.
We can determine whetherthis is so by doing a cluster analysis on the fac-toid presence vectors.
We should find clustersif and only if group consensus exists.3.
There is no such thing as overall consensus, butthere is a difference in perceived importance be-tween the various factoids.
We can determinewhether this is the case by examining how ofteneach factoid is used in the summaries.
Factoidsthat are more important ought to be includedmore often.
In that case, it is still possible tocreate a consensus-like reference summary forany desired summary size.4.
There is no difference in perceived importanceof the various factoids at all.
Inclusion of fac-toids in summaries appears to be random.3.1 Factoid frequency and consensusWe will start by examining whether an importancehierarchy exists, as this can help us decide betweenscenario 1, 3 or 4.
If still necessary, we can check forgroup consensus later.If we count how often each factoid is used, itquickly becomes clear that we do not have to worryabout worst-case scenario 4.
There are clear differ-ences in the frequency of use of the factoids.
Onthe other hand, scenario 1 does not appear to bevery likely either.
There is full consensus on the in-clusion of only a meager 3 factoids, which can besummarised in 3 words:Fortuyn was murdered.If we accept some disagreement, and take the fac-toids which occur in at least 90% of the summaries,this increases the consensus summary to 5 factoidsand 6 words:Fortuyn, a politician, was shot dead.Setting our aims ever lower, 75% of the summariesinclude 6 further factoids and the summary goes upto 20 words:Pim Fortuyn, a Dutch right-wing politician, was shotdead before the election.
A suspect was arrested.
Fortuynhad received threats.A 50% threshold yields 8 more factoids and the47-word summary:Pim Fortuyn, a Dutch right-wing politician, was shotdead at a radio station in Hilversum.
Fortuyn was cam-paigning on immigration issues and was expected to dowell in the election.
He had received threats.
There wereshocked reactions.
Political campaigning was halted.
Thepolice arrested a man.If we want to arrive at a 100-word summary (ac-tually 104), we need to include 26 more factoids, andwe need to allow all factoids which occur in at least30% of the summaries:Pim Fortuyn was shot six times and died shortly after-wards.
He was attacked when leaving a radio station inthe (well-secured) media park in Hilversum.
The Dutchfar-right politician was campaigning on an anti- immi-gration ticket and was outspoken about Islam.
He wasexpected to do well in the upcoming election, getting atleast 15% of the votes.
Fortuyn had received threats.
Heexpected an attack and used bodyguards.
Dutch and in-ternational politicians were shocked and condemned theattack.
The Dutch government called a halt to politicalcampaigning.
The gunman was chased.
The police laterarrested a white Dutch man.
The motive is unknown.We conclude that the extreme scenarios, full con-sensus and full absence of consensus, can be rejectedfor this text.
This leaves the question whether thepartial consensus takes the form of clusters of con-senting summarisers.3.2 Summariser clustersIn order to determine whether the summarisers canbe assigned to groups within which a large amount ofconsensus can be found, we turn to statistical tech-niques.
We first form 256-dimensional binary vectorsrecording the presence of each of the factoids in each?2 ?1 0 1 2?2?1012R001R002R003R004R005R006R007R008R009R010S001S002S003S004S005S007S009S010S011S012S013S014S015S016S017S018S019S023S027S028S030S031S032S033S034S036S038S041S042S045S046S048S049S051S053S054S055S056S057 S061ConsFigure 2: Classical multi-dimensional scaling of dis-tances between factoid vectors into two dimensionssummariser?s summary.
We also added a vector forthe 104-word consensus summary above (?Cons?
).We then calculate the distances between the vari-ous vectors and use these as input for classical multi-dimensional scaling.
The result of scaling into twodimensions is shown in Figure 2.Only a few small clusters appear to emerge.
Al-though we certainly cannot conclude that there areno clusters, we would have expected more clearly de-limited groups of summarisers, i.e.
different pointsof view, if scenario 2 described the actual situation.For now we will assume that, for this document, sce-nario 3 is the most likely.3.3 The consensus summary as anevaluation toolTwo of the main demands on a gold standard genericsummary for evaluation are: a) that it contains theinformation deemed most important in the docu-ment and b) that two gold standard summaries con-structed along the same lines lead to the same, orat least very similar, ranking of a set of summarieswhich are evaluated.If we decide to use a single human summary asa gold standard, we in fact assume that this hu-man?s choice of important material is acceptable forall other summary users, which it the wrong assump-tion, as the lack of consensus between the varioushuman summaries shows.
We propose that the useof a reference summary which is based on the factoidimportance hierarchy described above, as it uses aless subjective indication of the relative importanceof the information units in the text across a popu-lation of summary writers.
The reference summarywould then take the form of a consensus summary,in our case the 100-word compound summary on thebasis of factoids over the 30% threshold.The construction of the consensus summary wouldindicate that demand a) will be catered for, but westill have to check demand b).
We can do this bycomputing rankings based on the F-measure for in-cluded factoids, and measuring the correlation coef-ficient ?
between them.As we do not have a large number of automaticsummaries of our text available, we use our 50 hu-man summaries as data, pretending that they aresummaries we wish to rank (evaluate).If we compare the rankings on the basis of sin-gle human summaries as gold standard, it turns outthat the ranking correlation ?
between two ?gold?standards is indeed very low at an average of 0.20(variation between -0.51 and 0.85).
For the consen-sus summary, we can compare rankings for variousnumbers of base summaries.
After all, the consensussummary should improve with the number of con-tributing base summaries and ought to approach anideal consensus summary, which would be demon-strated by a stabilizing derived ranking.We investigate if this assumption is correct by cre-ating pairs of samples of N=5 to 200 base summaries,drawn (in a way similar to bootstrapping) from ouroriginal sample of 50.
For each pair of samples, weautomatically create a pair of consensus summariesand then determine how well these two agree in theirranking.
Figure 3 shows how ?
increases with N(based on 1000 trials per N).
At N=5 and 10, ?
hasa still clearly unacceptable average 0.40 or 0.53.
Theaverage reaches 0.80 at 45, 0.90 at 95 and 0.95 at astaggering 180 base summaries.We must note, however, that we have to be care-ful with these measurements, since 40 of our 50starting summaries were made by less experiencednon-natives.
In fact, if we bootstrap pairs of N=10base summary samples (100 trials) on just the 10higher-quality summaries (created by natives andnear-natives), we get an average ?
of 0.74.
Thesame experiment on 10 different summaries fromthe other 40 (100 trials for choosing the 10, and foreach 100 trials to estimate average ?)
yields average?
?s ranging from 0.55 to 0.63.
So clearly the differ-ence in experience has its effect.
Even so, even the?better?
summaries lead to a ranking correlation of?=0.74 at N=10, which still is much lower than wewould like to see.
We estimate that with this type ofsummaries an acceptably stable ranking (?
around0.90) would be reached somewhere between 30 and40 summaries.3.4 Using unigrams instead of factoidsApart from the need for human summaries, thefactoid-based comparisons have another problem,5 15 25 35 45 55 65 75 85 95 110 125 140 155 170 185 200?0.20.00.20.40.60.81.0Figure 3: Correlation coefficient ?
between rankingsfor 50 summaries on the basis of two consensus sum-maries, each based on a size N base summary collec-tion, for N between 5 and 200viz.
the need for human interpretation when map-ping summaries to factoid lists.
The question iswhether simpler measures might not be equally in-formative.
We investigate this using unigram over-lap, following Papineni et al (2001) in their sug-gestion that unigrams best represent contents, whilelonger n-grams best represent fluency.Again, we reuse our 50 summaries as summaries tobe evaluated.
For each of these summaries, we cal-culate the F-measure for the included factoids withregard to the consensus summary shown above.
Ina similar fashion, we build a consensus unigram list,containing the 103 unigrams that occur in at least11 summaries, and calculate the F-measure for un-igrams.
The two measures are plotted against eachother in Figure 4.Some correlation is present (r = 0.48 and Spear-man?s ranking correlation ?
= 0.45), but there areclearly profound differences.
If we look at the rank-ings produced from these two F-measures, S054, onposition 16 on the basis of factoids, drops to posi-tion 37 on the basis of unigrams.
S046, on the otherhand, climbs from 42nd to 4th place when consid-ered by unigrams instead of factoids.
Apart fromthese extreme cases, these are also clear differencesin the top-5 for the two measurements: S030, S028,R001, S003 and S023 are the top-5 when measuringwith factoids, whereas S032, R002, S030, S046 andS028 are the top-5 when measuring with unigrams.It would seem that unigrams, though they are muchcheaper, are not a viable substitute for factoids.0.40 0.45 0.50 0.55 0.60 0.65 0.700.400.450.500.550.60F(factoids)F(unigrams)R001R002R003R004R005R006R007R008R009R010 S001S002S003S004S005S007 S009S010S011S012S013S014S015S016S017S018S019S023S027S028S030S031S032S033S034S036S038S041 S042S045S046S048S049S051S053S054S055S056S057S061Figure 4: F-measures of summarisers with regard toconsensus data: factoid-based versus unigram-based4 Discussion and future workFrom our experiences so far, it seems that both ourinnovations, viz.
using multiple summaries and mea-suring with factoids, appear to be worth pursuingfurther.
We summarise the results for our test textin the following:?
We observe a very wide selection of factoids inthe summaries, only few of which are includedby all summarisers.?
The number of factoids found if new summariesare considered does not tail off.?
There is a clear importance hierarchy of fac-toids which allows us to compile a consensussummary.?
If single summaries are used as gold standard,the correlation between rankings based on twosuch gold standard summaries is low.?
We could not find any large clusters of highlycorrelated summarisers in our data.?
Stability with respect to the consensus sum-mary can only be expected if a larger numberof summaries are collected (in the range of atleast 30-40 summaries).?
A unigram-based measurement shows only lowcorrelation with the factoid-based measure-ment.The information that is gained through multi-ple summaries with factoid-similarity is insufficientlyapproximated with the currently used substitutes,as the observations above show.
However, what wehave described here must clearly be seen as an initialexperiment, and there is yet much to be done.First of all, the notation of the factoid (currentlyflat atoms) needs to be made more expressive, e.g.by the addition of variables for discourse referentsand events, which will make factoids more similarto FOPL expressions, and/or by the use of a typingmechanism to indicate the various forms of infer-ence/implication.We also need to identify a good weighting schemeto be used in measuring similarity of factoid vec-tors.
The weighting should correct for the variationbetween factoids in information content, for theirdifferent position along an inference chain, and pos-sibly for their position in the summary.
It shouldalso be able to express some notion of importanceof the factoids, e.g.
as measured by the number ofsummaries containing the factoid.Something else to investigate is the presence anddistribution of factoids, types of factoids and rela-tions between factoids in summaries and summarycollections.
We have the strong feeling that someof our observations were tightly linked to the typeof text we used.
We would like to build a balancedcorpus of texts, of various subject areas and lengths,and their summaries, at several different lengths andpossibly even multi-document, so that we can studythis factor.
An open question is how many sum-maries we should try to get for each of the texts inthe corpus.
It is unlikely we will be able to collect50 summaries for each new text.
Furthermore, thetexts of the corpus should also be summarised by asmany machine summarisers as possible, so that wecan test ranking these on the basis of factoids, in arealistic framework.A final line of investigation is searching for ways toreduce the cost of factoid analysis.
The first reasonwhy this analysis is currently expensive is the needfor large summary bases for consensus summaries.There is yet hope that this can be circumvented byusing larger numbers of texts, as is the case in IRand in MT, where discrepancies prove to average outwhen large enough datasets are used.
Papineni et al,e.g., were able to show that the ranking with theirBleu measure of the five evaluated translators (twohuman and three machine) remained stable if onlya single reference translation was used, suggestingthat ?we may use a big corpus with a single referencetranslation, provided that the translations are not allfrom the same translator?.
Possibly a similar aver-aging effect will occur in the evaluation of summari-sation so that smaller summary bases can be used.The second reason is the need for human annotationof factoids.
Although simple unigram-based meth-ods prove insufficient, we will hopefully be able tocome a long way in automating factoid identificationon the basis of existing NLP techniques, combinedwith information gained about factoids in researchas described in the previous paragraph.
All in all,the use of consensus summaries and factoid analy-sis, even though expensive to set up for the moment,provides a promising alternative which could wellbring us closer to a solution to several problems insummarisation evaluation.ReferencesDonaway, Robert L., Kevin W. Drummey, and Laura A.Mather.
2000.
A comparison of rankings produced bysummarization evaluation measures.
In Proceedingsof the ANLP/NAACL 2000 Workshop on AutomaticSummarization.DUC.
2002.
Document Understanding Conference(DUC).
Electronic proceedings, http://www-nlpir.nist.gov/projects/duc/pubs.html.Jing, Hongyan, Regina Barzilay, Kathleen R. McKe-own, and Michael Elhadad.
1998.
SummarizationEvaluation Methods: Experiments and Analysis.
InDragomir R. Radev and Eduard H. Hovy, eds., Work-ing Notes of the AAAI Spring Symposium on Intelli-gent Text Summarization, 60?68.Lin, Chin-Yew, and Eduard Hovy.
2002.
Manual andautomatic evaluation of summaries.
In DUC 2002.Mani, Inderjeet.
2001.
Automatic Summarization.
JohnBenjamins.Mani, Inderjeet, Therese Firmin, David House, GaryKlein, Beth Sundheim, and Lynette Hirschman.
1999.The TIPSTER Summac Text Summarization Evalu-ation.
In Proceedings of EACL-99 , 77?85.Papineni, K, S. Roukos, T Ward, and W-J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL-02 , 311?318.Rath, G.J, A. Resnick, and T. R. Savage.
1961.
TheFormation of Abstracts by the Selection of Sentences.American Documentation 12(2): 139?143.Saggion, Horacio, Dragomir Radev, Simone Teufel, WaiLam, and Stephanie M. Strassel.
2002.
DevelopingInfrastructure for the Evaluation of Single and Multi-document Summarization Systems in a Cross-lingualEnvironment.
In Proceedings of LREC 2002 , 747?754.Spa?rck Jones, Karen.
1999.
Automatic Summaris-ing: Factors and Directions.
In Inderjeet Mani andMark T. Maybury, eds., Advances in Automatic TextSummarization, 1?12.
Cambridge, MA: MIT Press.Voorhees, Ellen.
2000.
Variations in relevance judge-ments and the measurement of retrieval effectiveness.Information Processing and Management 36: 697?716.Zechner, Klaus.
1996.
Fast Generation of Abstracts fromGeneral Domain Text Corpora by Extracting RelevantSentences.
In Proceedings of COLING-96 , 986?989.
