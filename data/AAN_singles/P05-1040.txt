Proceedings of the 43rd Annual Meeting of the ACL, pages 322?329,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDetecting Errors in Discontinuous Structural AnnotationMarkus DickinsonDepartment of LinguisticsThe Ohio State Universitydickinso@ling.osu.eduW.
Detmar MeurersDepartment of LinguisticsThe Ohio State Universitydm@ling.osu.eduAbstractConsistency of corpus annotation is anessential property for the many uses ofannotated corpora in computational andtheoretical linguistics.
While some re-search addresses the detection of inconsis-tencies in positional annotation (e.g., part-of-speech) and continuous structural an-notation (e.g., syntactic constituency), noapproach has yet been developed for au-tomatically detecting annotation errors indiscontinuous structural annotation.
Thisis significant since the annotation of po-tentially discontinuous stretches of ma-terial is increasingly relevant, from tree-banks for free-word order languages to se-mantic and discourse annotation.In this paper we discuss how the variationn-gram error detection approach (Dickin-son and Meurers, 2003a) can be extendedto discontinuous structural annotation.
Weexemplify the approach by showing how itsuccessfully detects errors in the syntacticannotation of the German TIGER corpus(Brants et al, 2002).1 IntroductionAnnotated corpora have at least two kinds of uses:firstly, as training material and as ?gold standard?testing material for the development of tools in com-putational linguistics, and secondly, as a source ofdata for theoretical linguists searching for analyti-cally relevant language patterns.Annotation errors and why they are a problemThe high quality annotation present in ?gold stan-dard?
corpora is generally the result of a manualor semi-automatic mark-up process.
The annota-tion thus can contain annotation errors from auto-matic (pre-)processes, human post-editing, or hu-man annotation.
The presence of errors creates prob-lems for both computational and theoretical linguis-tic uses, from unreliable training and evaluation ofnatural language processing technology (e.g., vanHalteren, 2000; Kve?to?n and Oliva, 2002, and thework mentioned below) to low precision and recallof queries for already rare linguistic phenomena.
In-vestigating the quality of linguistic annotation andimproving it where possible thus is a key issue forthe use of annotated corpora in computational andtheoretical linguistics.Illustrating the negative impact of annotation er-rors on computational uses of annotated corpora,van Halteren et al (2001) compare taggers trainedand tested on the Wall Street Journal (WSJ, Marcuset al, 1993) and the Lancaster-Oslo-Bergen (LOB,Johansson, 1986) corpora and find that the results forthe WSJ perform significantly worse.
They reportthat the lower accuracy figures are caused by incon-sistencies in the WSJ annotation and that 44% of theerrors for their best tagging system were caused by?inconsistently handled cases.
?Turning from training to evaluation, Padro andMarquez (1998) highlight the fact that the true ac-curacy of a classifier could be much better or worsethan reported, depending on the error rate of the cor-pus used for the evaluation.
Evaluating two taggerson the WSJ, they find tagging accuracy rates for am-322biguous words of 91.35% and 92.82%.
Given theestimated 3% error rate of the WSJ tagging (Marcuset al, 1993), they argue that the difference in perfor-mance is not sufficient to establish which of the twotaggers is actually better.In sum, corpus annotation errors, especially er-rors which are inconsistencies, can have a profoundimpact on the quality of the trained classifiers andthe evaluation of their performance.
The problem iscompounded for syntactic annotation, given the dif-ficulty of evaluating and comparing syntactic struc-ture assignments, as known from the literature onparser evaluation (e.g., Carroll et al, 2002).The idea that variation in annotation can indicateannotation errors has been explored to detect errorsin part-of-speech (POS) annotation (van Halteren,2000; Eskin, 2000; Dickinson and Meurers, 2003a)and syntactic annotation (Dickinson and Meurers,2003b).
But, as far as we are aware, the researchwe report on here is the first approach to error detec-tion for the increasing number of annotations whichmake use of more general graph structures for thesyntactic annotation of free word order languages orthe annotation of semantic and discourse properties.Discontinuous annotation and its relevance Thesimplest kind of annotation is positional in nature,such as the association of a part-of-speech tag witheach corpus position.
On the other hand, struc-tural annotation such as that used in syntactic tree-banks (e.g., Marcus et al, 1993) assigns a syntacticcategory to a contiguous sequence of corpus posi-tions.
For languages with relatively free constituentorder, such as German, Dutch, or the Slavic lan-guages, the combinatorial potential of the languageencoded in constituency cannot be mapped straight-forwardly onto the word order possibilities of thoselanguages.
As a consequence, the treebanks thathave been created for German (NEGRA, Skut et al,1997; VERBMOBIL, Hinrichs et al, 2000; TIGER,Brants et al, 2002) have relaxed the requirement thatconstituents have to be contiguous.
This makes itpossible to syntactically annotate the language dataas such, i.e., without requiring postulation of emptyelements as placeholders or other theoretically mo-tivated changes to the data.
We note in passing thatdiscontinuous constituents have also received somesupport in theoretical linguistics (cf., e.g., the arti-cles collected in Huck and Ojeda, 1987; Bunt andvan Horck, 1996).Discontinuous constituents are strings of wordswhich are not necessarily contiguous, yet form asingle constituent with a single label, such as thenoun phrase Ein Mann, der lacht in the German rel-ative clause extraposition example (1) (Brants et al,2002).1(1) EinaMannmankommtcomes,,derwholachtlaughs?A man who laughs comes.
?In addition to their use in syntactic annotation,discontinuous structural annotation is also rele-vant for semantic and discourse-level annotation?essentially any time that graph structures are neededto encode relations that go beyond ordinary treestructures.
Such annotations are currently employedin the mark-up for semantic roles (e.g., Kings-bury et al, 2002) and multi-word expressions (e.g.,Rayson et al, 2004), as well as for spoken languagecorpora or corpora with multiple layers of annota-tion which cross boundaries (e.g., Blache and Hirst,2000).In this paper, we present an approach to the de-tection of errors in discontinuous structural annota-tion.
We focus on syntactic annotation with poten-tially discontinuous constituents and show that theapproach successfully deals with the discontinuoussyntactic annotation found in the TIGER treebank(Brants et al, 2002).2 The variation n-gram methodOur approach builds on the variation n-gram al-gorithm introduced in Dickinson and Meurers(2003a,b).
The basic idea behind that approach isthat a string occurring more than once can occurwith different labels in a corpus, which we refer to asvariation.
Variation is caused by one of two reasons:i) ambiguity: there is a type of string with multiplepossible labels and different corpus occurrences ofthat string realize the different options, or ii) error:the tagging of a string is inconsistent across compa-rable occurrences.1The ordinary way of marking a constituent with brack-ets is inadequate for discontinuous constituents, so we insteadboldface and underline the words belonging to a discontinuousconstituent.323The more similar the context of a variation, themore likely the variation is an error.
In Dickin-son and Meurers (2003a), contexts are composedof words, and identity of the context is required.The term variation n-gram refers to an n-gram (ofwords) in a corpus that contains a string annotateddifferently in another occurrence of the same n-gramin the corpus.
The string exhibiting the variation isreferred to as the variation nucleus.2.1 Detecting variation in POS annotationIn Dickinson and Meurers (2003a), we explore thisidea for part-of-speech annotation.
For example, inthe WSJ corpus the string in (2) is a variation 12-gram since off is a variation nucleus that in one cor-pus occurrence is tagged as a preposition (IN), whilein another it is tagged as a particle (RP).2(2) to ward off a hostile takeover attempt by twoEuropean shipping concernsOnce the variation n-grams for a corpus havebeen computed, heuristics are employed to classifythe variations into errors and ambiguities.
The firstheuristic encodes the basic fact that the label assign-ment for a nucleus is dependent on the context: vari-ation nuclei in long n-grams are likely to be errors.The second takes into account that natural languagesfavor the use of local dependencies over non-localones: nuclei found at the fringe of an n-gram aremore likely to be genuine ambiguities than those oc-curring with at least one word of surrounding con-text.
Both of these heuristics are independent of aspecific corpus, annotation scheme, or language.We tested the variation error detection method onthe WSJ and found 2495 distinct3 nuclei for the vari-ation n-grams between the 6-grams and the 224-grams.
2436 of these were actual errors, making fora precision of 97.6%, which demonstrates the valueof the long context heuristic.
57 of the 59 genuineambiguities were fringe elements, confirming thatfringe elements are more indicative of a true ambi-guity.2To graphically distinguish the variation nucleus within avariation n-gram, the nucleus is shown in grey.3Being distinct means that each corpus position is only takeninto account for the longest variation n-gram it occurs in.2.2 Detecting variation in syntactic annotationIn Dickinson and Meurers (2003b), we decomposethe variation n-gram detection for syntactic annota-tion into a series of runs with different nucleus sizes.This is needed to establish a one-to-one relation be-tween a unit of data and a syntactic category annota-tion for comparison.
Each run detects the variationin the annotation of strings of a specific length.
Byperforming such runs for strings from length 1 tothe length of the longest constituent in the corpus,the approach ensures that all strings which are ana-lyzed as a constituent somewhere in the corpus arecompared to the annotation of all other occurrencesof that string.For example, the variation 4-gram from a yearearlier appears 76 times in the WSJ, where the nu-cleus a year is labeled noun phrase (NP) 68 times,and 8 times it is not annotated as a constituent andis given the special label NIL.
An example withtwo syntactic categories involves the nucleus nextTuesday as part of the variation 3-gram maturitynext Tuesday, which appears three times in the WSJ.Twice it is labeled as a noun phrase (NP) and once asa prepositional phrase (PP).To be able to efficiently calculate all variation nu-clei of a treebank, in Dickinson and Meurers (2003b)we make use of the fact that a variation necessar-ily involves at least one constituent occurrence ofa nucleus and calculate the set of nuclei for a win-dow of length i by first finding the constituents ofthat length.
Based on this set, we then find non-constituent occurrences of all strings occurring asconstituents.
Finally, the variation n-grams for thesevariation nuclei are obtained in the same way as forPOS annotation.In the WSJ, the method found 34,564 variationnuclei, up to size 46; an estimated 71% of the 6277non-fringe distinct variation nuclei are errors.3 Discontinuous constituentsIn Dickinson and Meurers (2003b), we argued thatnull elements need to be ignored as variation nucleibecause the variation in the annotation of a null el-ement as the nucleus is largely independent of thelocal environment.
For example, in (3) the null el-ement *EXP* (expletive) can be annotated a. as asentence (S) or b. as a relative/subordinate clause324(SBAR), depending on the properties of the clauseit refers to.
(3) a.
For cities losing business to suburban shop-ping centers , it *EXP* may be a wise busi-ness investment [S * to help * keep thosejobs and sales taxes within city limits] .b.
But if the market moves quickly enough , it*EXP* may be impossible [SBAR for thebroker to carry out the order] because the in-vestment has passed the specified price .We found that removing null elements as variationnuclei of size 1 increased the precision of error de-tection to 78.9%.Essentially, null elements represent discontinu-ous constituents in a formalism with a context-freebackbone (Bies et al, 1995).
Null elements are co-indexed with a non-adjacent constituent; in the pred-icate argument structure, the constituent should beinterpreted where the null element is.To be able to annotate discontinuous materialwithout making use of inserted null elements, sometreebanks have instead relaxed the definition of a lin-guistic tree and have developed more complex graphannotations.
An error detection method for such cor-pora thus does not have to deal with the problemsarising from inserted null elements discussed above,but instead it must function appropriately even ifconstituents are discontinuously realized.A technique such as the variation n-gram methodis applicable to corpora with a one-to-one map-ping between the text and the annotation.
Forcorpora with positional annotation?e.g., part-of-speech annotated corpora?the mapping is triv-ial given that the annotation consists of one-to-one correspondences between words (i.e., tokens)and labels.
For corpora annotated with morecomplex structural information?e.g., syntactically-annotated corpora?the one-to-one mapping is ob-tained by considering every interval (continuousstring of any length) which is assigned a categorylabel somewhere in the corpus.While this works for treebanks with continuousconstituents, a one-to-one mapping is more com-plicated to establish for syntactic annotation involv-ing discontinuous constituents (NEGRA, Skut et al,1997; TIGER, Brants et al, 2002).
In order to applythe variation n-gram method to discontinuous con-stituents, we need to develop a technique which iscapable of comparing labels for any set of corpuspositions, instead of for any interval.4 Extending the variation n-gram methodTo extend the variation n-gram method to handlediscontinuous constituents, we first have to definethe characteristics of such a constituent (section 4.1),in other words our units of data for comparison.Then, we can find identical non-constituent (NIL)strings (section 4.2) and expand the context intovariation n-grams (section 4.3).4.1 Variation nuclei: ConstituentsFor traditional syntactic annotation, a variation nu-cleus is defined as a contiguous string with a sin-gle label; this allows the variation n-gram methodto be broken down into separate runs, one for eachconstituent size in the corpus.
For discontinuoussyntactic annotation, since we are still interested incomparing cases where the nucleus is the same, wewill treat two constituents as having the same size ifthey consist of the same number of words, regard-less of the amount of intervening material, and wecan again break the method down into runs of differ-ent sizes.
The intervening material is accounted forwhen expanding the context into n-grams.A question arises concerning the word order ofelements in a constituent.
Consider the German ex-ample (4) (Mu?ller, 2004).
(4) weilbecausedertheMannmannomdertheFrauwomandatdastheBuchbookaccgab.gave?because the man gave the woman the book.
?The three arguments of the verb gab (?give?)
can bepermuted in all six possible ways and still result in awell-formed sentence.
It might seem, then, that wewould want to allow different permutations of nucleito be treated as identical.
If das Buch der Frau gabis a constituent in another sentence, for instance, itshould have the same category label as der Frau dasBuch gab.Putting all permutations into one equivalenceclass, however, amounts to stating that all order-325ings are always the same.
But even ?free word or-der?
languages are more appropriately called freeconstituent order; for example, in (4), the argumentnoun phrases can be freely ordered, but each argu-ment noun phrase is an atomic unit, and in each unitthe determiner precedes the noun.Since we want our method to remain data-drivenand order can convey information which might bereflected in an annotation system, we keep stringswith different orders of the same words distinct, i.e.,ordering of elements is preserved in our method.4.2 Variation nuclei: Non-constituentsThe basic idea is to compare a string annotated as aconstituent with the same string found elsewhere?whether annotated as a constituent or not.
So weneed to develop a method for finding all string oc-currences not analyzed as a constituent (and assignthem the special category label NIL).
FollowingDickinson and Meurers (2003b), we only look fornon-constituent occurrences of those strings whichalso occur at least once as a constituent.But do we need to look for discontinuous NILstrings or is it sufficient to assume only continuousones?
Consider the TIGER treebank examples (5).
(5) a. inondiesemthisPunktpointseienaresichSELFBonnBonnundandLondonLondonnichtnoteinigagreed..?Bonn and London do not agree on this point.?b.
inondiesemthisPunktpointseienaresichSELFBonnBonnundandLondonLondonoffensichtlichclearlynicht einignot agreed..In example (5a), sich einig (?SELF agree?)
formsan adjective phrase (AP) constituent.
But in ex-ample (5b), that same string is not analyzed as aconstituent, despite being in a nearly identical sen-tence.
We would thus like to assign the discontinu-ous string sich einig in (5b) the label NIL, so that thelabeling of this string in (5a) can be compared to itsoccurrence in (5b).In consequence, our approach should be able todetect NIL strings which are discontinuous?an is-sue which requires special attention to obtain an al-gorithm efficient enough to handle large corpora.Use sentence boundary information The firstconsideration makes use of the fact that syntactic an-notation by its nature respects sentence boundaries.In consequence, we never need to search for NILstrings that span across sentences.4Use tries to store constituent strings The sec-ond consideration concerns how we calculate theNIL strings.
To find every non-constituent string inthe corpus, discontinuous or not, which is identicalto some constituent in the corpus, a basic approachwould first generate all possible strings within a sen-tence and then test to see which ones occur as aconstituent elsewhere in the corpus.
For example,if the sentence is Nobody died when Clinton lied, wewould see if any of the 31 subsets of strings occuras constituents (e.g., Nobody, Nobody when, Clin-ton lied, Nobody when lied, etc.).
But such a gener-ate and test approach clearly is intractable given thatit generates generates 2n?
1 potential matches for asentence of n words.We instead split the task of finding NIL strings intotwo runs through the corpus.
In the first, we storeall constituents in the corpus in a trie data structure(Fredkin, 1960), with words as nodes.
In the sec-ond run through the corpus, we attempt to match thestrings in the corpus with a path in the trie, thus iden-tifying all strings occurring as constituents some-where in the corpus.Filter out unwanted NIL strings The final con-sideration removes ?noisy?
NIL strings from the can-didate set.
Certain NIL strings are known to be use-less for detecting annotation errors, so we should re-move them to speed up the variation n-gram calcu-lations.
Consider example (6) from the TIGER cor-pus, where the continuous constituent die Menschenis annotated as a noun phrase (NP).
(6) OhnewithoutdiesetheseAusgaben,expensessoaccording todietheWeltbank,world bankseienaredie Menschenthe peopletotesdeadKapitalcapital?According to the world bank, the people are dead capitalwithout these expenses.
?4This restriction clearly is syntax specific and other topo-logical domains need to be identified to make searching for NILstrings tractable for other types of discontinuous annotation.326Our basic method of finding NIL strings would de-tect another occurrence of die Menschen in the samesentence since nothing rules out that the other occur-rence of die in the sentence (preceding Weltbank)forms a discontinuous NIL string with Menschen.Comparing a constituent with a NIL string that con-tains one of the words of the constituent clearly goesagainst the original motivation for wanting to finddiscontinuous strings, namely that they show varia-tion between different occurrences of a string.To prevent such unwanted variation, we eliminateoccurrences of NIL-labeled strings that overlap withidentical constituent strings from consideration.4.3 Variation n-gramsThe more similar the context surrounding a varia-tion nucleus, the more likely it is for a variation inits annotation to be an error.
For detecting errors intraditional syntactic annotation (see section 2.2), thecontext consists of the elements to the left and theright of the nucleus.
When nuclei can be discontinu-ous, however, there can also be internal context, i.e.,elements which appear between the words forminga discontinuous variation nucleus.As in our earlier work, an instance of the a pri-ori algorithm is used to expand a nucleus into alonger n-gram by stepwise adding context elements.Where previously it was possible to add an elementto the left or the right, we now also have the option ofadding it in the middle?as part of the new, internalcontext.
But depending on how we fill in the internalcontext, we can face a serious tractability problem.Given a nucleus with j gaps within it, we need topotentially expand it in j + 2 directions, instead ofin just 2 directions (to the right and to the left).For example, the potential nucleus was werdenappears as a verb phrase (VP) in the TIGER corpus inthe string was ein Seeufer werden; elsewhere in thecorpus was and werden appear in the same sentencewith 32 words between them.
The chances of one ofthe middle 32 elements matching something in theinternal context of the VP is relatively high, and in-deed the twenty-sixth word is ein.
However, if wemove stepwise out from the nucleus in order to tryto match was ein Seeufer werden, the only optionsare to find ein directly to the right of was or Seeuferdirectly to the left of werden, neither of which oc-curs, thus stopping the search.In conclusion, we obtain an efficient applicationof the a priori algorithm by expanding the contextonly to elements which are adjacent to an elementalready in the n-gram.
Note that this was alreadyimplicitly assumed for the left and the right context.There are two other efficiency-related issuesworth mentioning.
Firstly, as with the variation nu-cleus detection, we limit the n-grams expansion tosentences only.
Since the category labels do not rep-resent cross-sentence dependencies, we gain no newinformation if we find more context outside the sen-tence, and in terms of efficiency, we cut off whatcould potentially be a very large search space.5Secondly, the methods for reducing the numberof variation nuclei discussed in section 4.2 have theconsequence of also reducing the number of possi-ble variation n-grams.
For example, in a test runon the NEGRA corpus we allowed identical stringsto overlap; this generated a variation nucleus of size63, with 16 gaps in it, varying between NP and NILwithin the same sentence.
Fifteen of the gaps can befilled in and still result in variation.
The filter for un-wanted NIL strings described in the previous sectioneliminates the NIL value from consideration.
Thus,there is no variation and no tractability problem inconstructing n-grams.4.3.1 Generalizing the n-gram contextSo far, we assumed that the context added aroundvariation nuclei consists of words.
Given that tree-banks generally also provide part-of-speech infor-mation for every token, we experimented with part-of-speech tags as a less restrictive kind of context.The idea is that it should be possible to find morevariation nuclei with comparable contexts if only thepart-of-speech tags of the surrounding words have tobe identical instead of the words themselves.As we will see in section 5, generalizing n-gramcontexts in this way indeed results in more variationn-grams being found, i.e., increased recall.4.4 Adapting the heuristicsTo determine which nuclei are errors, we can buildon the two heuristics from previous research (Dick-5Note that similar sentences which were segmented differ-ently could potentially cause varying n-gram strings not to befound.
We propose to treat this as a separate sentence segmen-tation error detection phase in future work.327inson and Meurers, 2003a,b)?trust long contextsand distrust the fringe?with some modification,given that we have more fringe areas to deal withfor discontinuous strings.
In addition to the rightand the left fringe, we also need to take into accountthe internal context in a way that maintains the non-fringe heuristic as a good indicator for errors.
Asa solution that keeps internal context on a par withthe way external context is treated in our previouswork, we require one word of context around everyterminal element that is part of the variation nucleus.As discussed below, this heuristic turns out to be agood predictor of which variations are annotation er-rors; expanding to the longest possible context, as inDickinson and Meurers (2003a), is not necessary.5 Results on the TIGER CorpusWe ran the variation n-grams error detection methodfor discontinuous syntactic constituents on v. 1 ofTIGER (Brants et al, 2002), a corpus of 712,332tokens in 40,020 sentences.
The method detecteda total of 10,964 variation nuclei.
From these wesampled 100 to get an estimate of the number of er-rors in the corpus which concern variation.
Of these100, 13 variation nuclei pointed to an error; with thispoint estimate of .13, we can derive a 95% confi-dence interval of (0.0641, 0.1959),6 which meansthat we are 95% confident that the true number ofvariation-based errors is between 702 and 2148.
Theeffectiveness of a method which uses context to nar-row down the set of variation nuclei can be judgedby how many of these variation errors it finds.Using the non-fringe heuristic discussed in theprevious section, we selected the shortest non-fringevariation n-grams to examine.
Occurrences of thesame strings within larger n-grams were ignored, soas not to artificially increase the resulting set of n-grams.When the context is defined as identical words,we obtain 500 variation n-grams.
Sampling 100 ofthese and labeling for each position whether it is anerror or an ambiguity, we find that 80 out of the 100samples point to at least one token error.
The 95%confidence interval for this point estimate of .80 is6The 95% confidence interval was calculated using the stan-dard formula of p?1.96qp(1?p)n , where p is the point estimateand n the sample size.
(0.7216, 0.8784), so we are 95% confident that thetrue number of error types is between 361 and 439.Note that this precision is comparable to the esti-mates for continuous syntactic annotation in Dick-inson and Meurers (2003b) of 71% (with null ele-ments) and 78.9% (without null elements).When the context is defined as identical parts ofspeech, as described in section 4.3.1, we obtain 1498variation n-grams.
Again sampling 100 of these, wefind that 52 out of the 100 point to an error.
Andthe 95% confidence interval for this point estimateof .52 is (0.4221, 0.6179), giving a larger estimatednumber of errors, between 632 and 926.Context Precision ErrorsWord 80% 361?439POS 52% 632?926Figure 1: Accuracy rates for the different contextsWords convey more information than part-of-speech tags, and so we see a drop in precision whenusing part-of-speech tags for context, but these re-sults highlight a very practical benefit of using ageneralized context.
By generalizing the context, wemaintain a precision rate of approximately 50%, andwe substantially increase the recall of the method.There are, in fact, likely twice as many errors whenusing POS contexts as opposed to word contexts.Corpus annotation projects willing to put in someextra effort thus can use this method of finding vari-ation n-grams with a generalized context to detectand correct more errors.6 Summary and OutlookWe have described the first method for finding er-rors in corpora with graph annotations.
We showedhow the variation n-gram method can be extendedto discontinuous structural annotation, and how thiscan be done efficiently and with as high a preci-sion as reported for continuous syntactic annotation.Our experiments with the TIGER corpus show thatgeneralizing the context to part-of-speech tags in-creases recall while keeping precision above 50%.The method can thus have a substantial practicalbenefit when preparing a corpus with discontinuousannotation.Extending the error detection method to handle328discontinuous constituents, as we have done, hassignificant potential for future work given the in-creasing number of free word order languages forwhich corpora and treebanks are being developed.Acknowledgements We are grateful to GeorgeSmith and Robert Langner of the University of Pots-dam TIGER team for evaluating the variation we de-tected in the samples.
We would also like to thankthe three ACL reviewers for their detailed and help-ful comments, and the participants of the OSU CLip-pers meetings for their encouraging feedback.ReferencesAnn Bies, Mark Ferguson, Karen Katz and RobertMacIntyre, 1995.
Bracketing Guidelines for Tree-bank II Style Penn Treebank Project.
Universityof Pennsylvania.Philippe Blache and Daniel Hirst, 2000.
Multi-levelannotation for spoken-language corpora.
In Pro-ceedings of ICSLP-00.
Beijing, China.Sabine Brants, Stefanie Dipper, Silvia Hansen,Wolfgang Lezius and George Smith, 2002.
TheTIGER Treebank.
In Proceedings of TLT-02.
So-zopol, Bulgaria.Harry Bunt and Arthur van Horck (eds.
), 1996.
Dis-continuous Constituency.
Mouton de Gruyter,Berlin and New York.John Carroll, Anette Frank, Dekang Lin, DetlefPrescher and Hans Uszkoreit (eds.
), 2002.
Pro-ceedings of the LREC Workshop ?Beyond PAR-SEVAL.
Towards Improved Evaluation Measuresfor Parsing Systems?, Las Palmas, Gran Canaria.Markus Dickinson and W. Detmar Meurers, 2003a.Detecting Errors in Part-of-Speech Annotation.
InProceedings of EACL-03.
Budapest, Hungary.Markus Dickinson and W. Detmar Meurers, 2003b.Detecting Inconsistencies in Treebanks.
In Pro-ceedings of TLT-03.
Va?xjo?, Sweden.Eleazar Eskin, 2000.
Automatic Corpus Correc-tion with Anomaly Detection.
In Proceedings ofNAACL-00.
Seattle, Washington.Edward Fredkin, 1960.
Trie Memory.
CACM,3(9):490?499.Erhard Hinrichs, Julia Bartels, Yasuhiro Kawata,Valia Kordoni and Heike Telljohann, 2000.
TheTu?bingen Treebanks for Spoken German, En-glish, and Japanese.
In Wolfgang Wahlster (ed.
),Verbmobil: Foundations of Speech-to-SpeechTranslation, Springer, Berlin, pp.
552?576.Geoffrey Huck and Almerindo Ojeda (eds.
), 1987.Discontinuous Constituency.
Academic Press,New York.Stig Johansson, 1986.
The Tagged LOB Corpus:Users?
Manual.
Norwegian Computing Centre forthe Humanities, Bergen.Paul Kingsbury, Martha Palmer and Mitch Marcus,2002.
Adding Semantic Annotation to the PennTreeBank.
In Proceedings of HLT-02.
San Diego.Pavel Kve?to?n and Karel Oliva, 2002.
Achievingan Almost Correct PoS-Tagged Corpus.
In PetrSojka, Ivan Kopec?ek and Karel Pala (eds.
), TSD2002.
Springer, Heidelberg, pp.
19?26.M.
Marcus, Beatrice Santorini and M. A.Marcinkiewicz, 1993.
Building a large an-notated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313?330.Stefan Mu?ller, 2004.
Continuous or Discontinu-ous Constituents?
A Comparison between Syn-tactic Analyses for Constituent Order and TheirProcessing Systems.
Research on Language andComputation, 2(2):209?257.Lluis Padro and Lluis Marquez, 1998.
On the Eval-uation and Comparison of Taggers: the Effect ofNoise in Testing Corpora.
In COLING/ACL-98.Paul Rayson, Dawn Archer, Scott Piao and TonyMcEnery, 2004.
The UCREL Semantic Analy-sis System.
In Proceedings of the Workshop onBeyond Named Entity Recognition: Semantic la-belling for NLP tasks.
Lisbon, Portugal, pp.
7?12.Wojciech Skut, Brigitte Krenn, Thorsten Brants andHans Uszkoreit, 1997.
An Annotation Schemefor Free Word Order Languages.
In Proceedingsof ANLP-97.
Washington, D.C.Hans van Halteren, 2000.
The Detection of Inconsis-tency in Manually Tagged Text.
In Anne Abeille?,Thorsten Brants and Hans Uszkoreit (eds.
), Pro-ceedings of LINC-00.
Luxembourg.Hans van Halteren, Walter Daelemans and Jakub Za-vrel, 2001.
Improving Accuracy in Word ClassTagging through the Combination of MachineLearning Systems.
Computational Linguistics,27(2):199?229.329
