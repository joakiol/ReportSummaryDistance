A Practical Solution to the Problem of Automatic Word Sense InductionReinhard RappUniversity of Mainz, FASKD-76711 Germersheim, Germanyrapp@mail.fask.uni-mainz.deAbstractRecent studies in word sense induction arebased on clustering global co-occurrence vec-tors, i.e.
vectors that reflect the overall be-havior of a word in a corpus.
If a word is se-mantically ambiguous, this means that thesevectors are mixtures of all its senses.
Inducinga word?s senses therefore involves the difficultproblem of recovering the sense vectors fromthe mixtures.
In this paper we argue that thedemixing problem can be avoided since thecontextual behavior of the senses is directlyobservable in the form of the local contexts ofa word.
From human disambiguation perform-ance we know that the context of a word isusually sufficient to determine its sense.
Basedon this observation we describe an algorithmthat discovers the different senses of an am-biguous word by clustering its contexts.
Themain difficulty with this approach, namely theproblem of data sparseness, could be mini-mized by looking at only the three main di-mensions of the context matrices.1 IntroductionThe topic of this paper is word sense induction,that is the automatic discovery of the possiblesenses of a word.
A related problem is word sensedisambiguation: Here the senses are assumed to beknown and the task is to choose the correct onewhen given an ambiguous word in context.Whereas until recently the focus of research hadbeen on sense disambiguation, papers like Pantel &Lin (2002), Neill (2002), and Rapp (2003) giveevidence that sense induction now also attracts at-tention.In the approach by Pantel & Lin (2002), allwords occurring in a parsed corpus are clustered onthe basis of the distances of their co-occurrencevectors.
This is called global clustering.
Since (bylooking at differential vectors) their algorithm al-lows a word to belong to more than one cluster,each cluster a word is assigned to can be consid-ered as one of its senses.
A problem that we seewith this approach is that it allows only as manysenses as clusters, thereby limiting the granularityof the meaning space.
This problem is avoided byNeill (2002) who uses local instead of global clus-tering.
This means, to find the senses of a givenword only its close associations are clustered, thatis for each word new clusters will be found.Despite many differences, to our knowledge al-most all approaches to sense induction that havebeen published so far have a common limitation:They rely on global co-occurrence vectors, i.e.
onvectors that have been derived from an entire cor-pus.
Since most words are semantically ambigu-ous, this means that these vectors reflect the sum ofthe contextual behavior of a word?s underlyingsenses, i.e.
they are mixtures of all senses occur-ring in the corpus.However, since reconstructing the sense vectorsfrom the mixtures is difficult, the question is if wereally need to base our work on mixtures or if thereis some way to directly observe the contextual be-havior of the senses thereby avoiding the mixingbeforehand.
In this paper we suggest to look at lo-cal instead of global co-occurrence vectors.
As canbe seen from human performance, in almost allcases the local context of an ambiguous word issufficient to disambiguate its sense.
This meansthat the local context of a word usually carries noambiguities.
The aim of this paper is to show howthis observation whose application tends to se-verely suffer from the sparse-data problem can besuccessfully exploited for word sense induction.2 ApproachThe basic idea is that we do not cluster theglobal co-occurrence vectors of the words (basedon an entire corpus) but local ones which are de-rived from the contexts of a single word.
That is,our computations are based on the concordance ofa word.
Also, we do not consider a term/term but aterm/context matrix.
This means, for each wordthat we want to analyze we get an entire matrix.Let us exemplify this using the ambiguous wordpalm with its tree and hand senses.
If we assumethat our corpus has six occurrences of palm, i.e.there are six local contexts, then we can derive sixlocal co-occurrence vectors for palm.
Consideringonly strong associations to palm, these vectorscould, for example, look as shown in table 1.The dots in the matrix indicate if the respectiveword occurs in a context or not.
We use binaryvectors since we assume short contexts wherewords usually occur only once.
By looking at thematrix it is easy to see that contexts c1, c3, and c6seem to relate to the hand sense of palm, whereascontexts c2, c4, and c5 relate to its tree sense.
Ourintuitions can be resembled by using a method forcomputing vector similarities, for example the co-sine coefficient or the (binary) Jaccard-measure.
Ifwe then apply an appropriate clustering algorithmto the context vectors, we should obtain the twoexpected clusters.
Each of the two clusters corre-sponds to one of the senses of palm, and the wordsclosest to the geometric centers of the clustersshould be good descriptors of each sense.However, as matrices of the above type can beextremely sparse, clustering is a difficult task, andcommon algorithms often deliver sub-optimal re-sults.
Fortunately, the problem of matrix sparse-ness can be minimized by reducing the dimension-ality of the matrix.
An appropriate algebraicmethod that has the capability to reduce the dimen-sionality of a rectangular or square matrix in anoptimal way is singular value decomposition(SVD).
As shown by Sch?tze (1997) by reducingthe dimensionality a generalization effect can beachieved that often improves the results.
The ap-proach that we suggest in this paper involves re-ducing the number of columns (contexts) and thenapplying a clustering algorithm to the row vectors(words) of the resulting matrix.
This works wellsince it is a strength of SVD to reduce the effectsof sampling errors and to close gaps in the data.c1 c2 c3 c4 c5 c6arm ?
?beach  ?
?coconut  ?
?
?finger ?
?hand ?
?
?shoulder ?
?tree  ?
?Table 1: Term/context matrix for the word palm.3 AlgorithmAs in previous work (Rapp, 2002), our compu-tations are based on a partially lemmatized versionof the British National Corpus (BNC) which hasthe function words removed.
Starting from the listof 12 ambiguous words provided by Yarowsky(1995) which is shown in table 2, we created aconcordance for each word, with the lines in theconcordances each relating to a context window of?20 words.
From the concordances we computed12 term/context-matrices (analogous to table 1)whose binary entries indicate if a word occurs in aparticular context or not.
Assuming that theamount of information that a context word pro-vides depends on its association strength to theambiguous word, in each matrix we removed allwords that are not among the top 30 first order as-sociations to the ambiguous word.
These top 30 as-sociations were computed fully automaticallybased on the log-likelihood ratio.
We used the pro-cedure described in Rapp (2002), with the onlymodification being the multiplication of the log-likelihood values with a triangular function thatdepends on the logarithm of a word?s frequency.This way preference is given to words that are inthe middle of the frequency range.
Figures 1 to 3are based on the association lists for the wordspalm and poach.Given that our term/context matrices are verysparse with each of their individual entries seemingsomewhat arbitrary, it is necessary to detect theregularities in the patterns.
For this purpose we ap-plied the SVD to each of the matrices, thereby re-ducing their number of columns to the three maindimensions.
This number of dimensions may seemlow.
However, it turned out that with our relativelysmall matrices (matrix size is the occurrence fre-quency of a word times the number of associationsconsidered) it was sometimes not possible to com-pute more than three singular values, as there aredependencies in the data.
Therefore, we decided touse three dimensions for all matrices.The last step in our procedure involves applying aclustering algorithm to the 30 words in each ma-trix.
For our condensed matrices of 3 rows and 30columns this is a rather simple task.
We decided touse the hierarchical clustering algorithm readilyavailable in the MATLAB (MATrix LABoratory)programming language.
After some testing withvarious similarity functions and linkage types, wefinally opted for the cosine coefficient and singlelinkage which is the combination that apparentlygave the best results.axes: grid/tools bass: fish/musiccrane: bird/machine drug: medicine/narcoticduty: tax/obligation motion: legal/physicalpalm: tree/hand plant: living/factorypoach: steal/boil sake: benefit/drinkspace: volume/outer tank: vehicle/containerTable 2: Ambiguous words and their senses.4 ResultsBefore we proceed to a quantitative evaluation,by looking at a few examples let us first give aqualitative impression of some results and considerthe contribution of SVD to the performance of ouralgorithm.
Figure 1 shows a dendrogram for theword palm (corpus frequency in the lemmatizedBNC: 2054) as obtained after applying the algo-rithm described in the previous section, with theonly modification that the SVD step was omitted,i.e.
no dimensionality reduction was performed.The horizontal axes in the dendrogram is dissimi-larity (1 ?
cosine), i.e.
0 means identical items and1 means no similarity.
The vertical axes has nospecial meaning.
Only the order of the words ischosen in such a way that line crossings areavoided when connecting clusters.As we can see, the dissimilarities among the top30 associations to palm are all in the upper half ofthe scale and not very distinct.
The two expectedclusters for palm, one relating to its hand and theother to its tree sense, have essentially been found.According to our judgment, all words in the upperbranch of the hierarchical tree are related to thehand sense of palm, and all other words are relatedto its tree sense.
However, it is somewhat unsatis-factory that the word frond seems equally similarto both senses, whereas intuitively we wouldclearly put it in the tree section.Let us now compare figure 1 to figure 2 whichhas been generated using exactly the same proce-dure with the only difference that the SVD step(reduction to 3 dimensions) has been conducted inthis case.
In figure 2 the similarities are generallyat a higher level (dissimilarities lower), the relativedifferences are bigger, and the two expected clus-ters are much more salient.
Also, the word frond isnow well within the tree cluster.
Obviously, figure2 reflects human intuitions better than figure 1, andwe can conclude that SVD was able to find theright generalizations.
Although space constraintsprevent us from showing similar comparative dia-grams for other words, we hope that this novel wayof comparing dendrograms makes it clearer whatthe virtues of SVD are, and that it is more than justanother method for smoothing.Our next example (figure 3) is the dendrogramfor poach (corpus frequency: 458).
It is also basedon a matrix that had been reduced to 3 dimensions.The two main clusters nicely distinguish betweenthe two senses of poach, namely boil and steal.The upper branch of the hierarchical tree consistsof words related to cooking, the lower one mainlycontains words related to the unauthorized killingof wildlife in Africa which apparently is an im-portant topic in the BNC.Figure 3 nicely demonstrates what distinguishesthe clustering of local contexts from the clusteringof global co-occurrence vectors.
To see this, let usbring our attention to the various species of ani-mals that are among the top 30 associations topoach.
Some of them seem more often affected bycooking (pheasant, chicken, salmon), others bypoaching (elephant, tiger, rhino).
According to thediagram only the rabbit is equally suitable for bothactivities, although fortunately its affinity to cook-ing is lower than it is for the chicken, and to poach-ing it is lower than it is for the rhino.That is, by clustering local contexts our algo-rithm was able to separate the different kinds ofanimals according to their relationship to poach.
Ifwe instead clustered global vectors, it would mostlikely be impossible to obtain this separation, asfrom a global perspective all animals have mostproperties (context words) in common, so they arelikely to end up in a single cluster.
Note that whatwe exemplified here for animals applies to all link-age decisions made by the algorithm, i.e.
all deci-sions must be seen from the perspective of the am-biguous word.This implies that often the clustering may becounterintuitive from the global perspective that ashumans we tend to have when looking at isolatedwords.
That is, the clusters shown in figures 2 and3 can only be understood if the ambiguous wordsthey are derived from are known.
However, this isexactly what we want in sense induction.In an attempt to provide a quantitative evaluationof our results, for each of the 12 ambiguous wordsshown in table 1 we manually assigned the top 30first-order associations to one of the two sensesprovided by Yarowsky (1995).
We then looked atthe first split in our hierarchical trees and assignedeach of the two clusters to one of the given senses.In no case was there any doubt on which wayround to assign the two clusters to the two givensenses.
Finally, we checked if there were any mis-classified items in the clusters.According to this judgment, on average 25.7 ofthe 30 items were correctly classified, and 4.3items were misclassified.
This gives an overall ac-curacy of 85.6%.
Reasons for misclassificationsinclude the following: Some of the top 30 associa-tions are more or less neutral towards the senses,so even for us it was not always possible to clearlyassign them to one of the two senses.
In othercases, outliers led to a poor first split, like if in fig-ure 1 the first split would be located between frondand the rest of the vocabulary.
In the case of sakethe beverage sense is extremely rare in the BNCand therefore was not represented among the top30 associations.
For this reason the clustering algo-rithm had no chance to find the expected clusters.5 Conclusions and prospectsFrom the observations described above we con-clude that avoiding the mixture of senses, i.e.clustering local context vectors instead of globalco-occurrence vectors, is a good way to deal withthe problem of word sense induction.
However,there is a  pitfall, as the matrices of local vectorsare extremely sparse.
Fortunately, our simulationssuggest that computing the main dimensions of amatrix through SVD solves the problem of sparse-ness and greatly improves clustering results.Although the results that we presented in thispaper seem useful even for practical purposes, wecan not claim that our algorithm is capable offinding all the fine grained distinctions that arelisted in manually created dictionaries such as theLongman Dictionary of Contemporary English(LDOCE), or in lexical databases such as WordNet.For future improvement of the algorithm we seetwo main possibilities:1) Considering all context words instead of onlythe top 30 associations would further reduce thesparse data problem.
However, this requires find-ing an appropriate association function.
This is dif-ficult, as for example the log-likelihood ratio, al-though delivering almost perfect rankings, has aninappropriate value characteristic: The increasein computed strengths is over-proportional forstronger associations.
This prevents the SVD fromfinding optimal dimensions.2) The principle of avoiding mixtures can be ap-plied more consequently if not only local instead ofglobal vectors are used, but if also the parts ofspeech of the context words are considered.
By op-erating on a part-of-speech tagged corpus thosesense distinctions that have an effect on part ofspeech can be taken into account.AcknowledgementsI would like to thank Manfred Wettler, RobertDale, Hinrich Sch?tze, and Raz Tamir for help anddiscussions, and the DFG for financial support.ReferencesNeill, D. B.
(2002).
Fully Automatic Word SenseInduction by Semantic Clustering.
CambridgeUniversity, Master?s Thesis, M.Phil.
in Com-puter Speech.Pantel, P.; Lin, D. (2002).
Discovering word sensesfrom text.
In: Proceedings of ACM SIGKDD,Edmonton, 613?619.Rapp, R. (2002).
The computation of word asso-ciations: comparing syntagmatic and paradigma-tic approaches.
Proc.
of 19th COLING, Taipei,ROC, Vol.
2, 821?827.Rapp, R. (2003).
Word sense discovery based onsense descriptor dissimilarity.
In: Ninth MachineTranslation Summit, New Orleans, 315?322.Sch?tze, H. (1997).
Ambiguity Resolution in Lan-guage Learning: Computational and CognitiveModels.
Stanford: CSLI Publications.Yarowsky, D. (1995).
Unsupervised word sensedisambiguation rivaling supervised methods.
In:Proc.
of 33rd ACL, Cambridge, MA, 189?196.Figure 1: Clustering results for palm without SVD.Figure 2: Clustering results for palm with SVD.Figure 3: Clustering results for poach with SVD.
