Analysis of Scene Identification Ability of Associative Memory withPictorial DictionaryTatsuhiko TSUNODA *, I-Iidehiko TANAKATanaka Hidehiko Laboratory, Departlnent ofElectrical EngineeringFaculty of Engineering, University of Tokyo, 7-3-1 Hongo, BunkyoqCu, Tokyo 113, Japan{tsunoda,t anaka} Qmtl.
t. u-tokyo, ac.j I)Abst rac tSemantic disambiguation depends on a process ofdefining the appropriate knowledge context.
Recentresearch directions suggest a eonnectionist approachwhich use dictionaries, but there remain problems ofscale, analysis, and interpretation.
IIere we focus onword disambiguation as scene selection, based on theOxford Pictorial English Dictionary.
We present a re-sults of a spatial-scene identification ability using ouroriginal associative mcmor~j, We show both theoreticaland experimental nalysis, based on a several differentmeasures including information entropy.1 In t roduct ion'the difficulty of semantic disambiguation in naturallanguage processing originates with the complexity ofdefining disambiguating knowledge contexts (BarwiseJ.
and Perry J., 1983).
These knowledge contextsmust provide unique interpretations for co-dependentwords, and help resolve "semantic garden path" se-quences.
For example, in "John shot some bucks,"aunique reading requires emantic agreement on "shot"and "bucks," suggesting either a hunting or gamblingcontext.
The semantic garden path can be illustratedby prefixing the above sentence with "John travelled tothe woods," which might suggest he hunting context,but then appending "The illegal csmino was hidden farfrom town," to dramatically change the interpretationsuggested by the first two sentences.The core of the problem is the disciplined and dy-namic construction of a disambiguating kvowledgecontext.
While it might be possible to write staticrules which provide disambiguating i formation i thecontext of complete knowledge, such rulmhased mod-els are both time and space inefficient.Recognizing these problems, Waltz D.L.
and PollackJ.B.
(1985) and Cottrell G.W.
(1989) proposed a f,~sci-hating connectionist approach, which uses early ideasfrom semantic networks to resolve semantic ambiguity*Supported by the Fellowships ofthe Japan Society for thePromotion ofScience for Japanese Junior Scientistsby dynamic spreading activation.
This spreading acti-wttion construction of disambiguating context is basedoil a high density associative cognitive model, butstill has problems: (1) no automated learning methodto adaptively construct he model, (2) non-scalable,and (3) no method of confirming hypothesized is-ambiguation.
Shastri L.(1988) proposes a similarstructure, which uses st statistical semantic network.Sharkey N.E.
(1989) has proposed asystem for process-ing script-based narratives based on combining localrepresentation a d relaxation techniques with ImrMleldistributed learning and mapping mechanisms.
Mi-ikkulainen's system DISCERN(Miikkulainen R., 1993)is also suggestive ofadaptive processing, and uses self-organizing representation f words and memory de-pending on semantics.
However, all of these modelsshare the problems enumerated above.Research directions for improvements suggest theuse of existing collections of machine-readM~le dictio-naries.
Ilecently, Nishlklmi M. et al (1992) has pro-posed a new relationship between language acquistionand learning based on scene analaysis.
Furthermore,Bookman L.A.(1993) has proposed a scalable architec-ture for integrating ~tqsociative and semantic memoryusing a thesaurus.
Based on this idea of using existingsources of word meanings, Veronis and Ide (Veronis .1.and Ide N.M., 1990; Ide N.M. and Veronis J., 1993) usesew~ral dictionaries and to improve the ratio of wordsdisambiguated to ambiguous words.In addition to ideas for the source of disambiguat-ing knowledge, many researchers have incorporatedsome kind of preference heuristics for improving tl,eefficiency of determining disambiguating constraints.Although these methods are essential for semantic pro-cessing they lack any coherent method for (1) evaluat-ing performance, and (2) acquiring new disaml)iguat-ing knowledge from real-world sensors.Of course all of these l)roblems result from the com-plexity of defining appropriate disambiguating knowl-edge contexts.
To help control and reduce this com-plexity, Kohonen T.(1984) has suggested the cla.ssifica-tion of dlsambiguating i formation i to flmr types: (1)spatial contact, (2) tenqmral contact, (3) similarity,(4) contrast.
Kohonen also emphmsizes the existence310of a contextual background in which primary percep-tions occur, but we clMm that this kind of information<:an be expressed in the existing four types.The previous approaches noted above can all beinterpreted as using a complex mixture of the infor-mation types proposed by Kohonen.
This coml>lex-ity makes it very difficult to identify or create a sta-ble mo<lel of learning the appropriate <lisan,biguatingknowledge from the real world.Our original contribution here is to propose a lmsiemethod of word disambiguation b~med on spatial sceneidentification, and to provide a detaile<l analysis of itsperformance.
The disambiguating knowledge is repre-sented in the form of a stochastic ~msociative memory,constructed fi-om the ()xford Pictorial English Dicti<>-nary (OPED).
This l>ietorial dictionary claims to l>ro:vide word sense meanings for most ordinary lift.'
scenes.The process of disambiguation is modelled as <leter-mining a unique mapping fi'om ambiguous input wor<lsto a particular l>ietorial <lictionary scene as modelle<l inthe ~msociative menmry.
The simple representatiml ofpietorial knowledge.
I)~med (m the OPED makes analy-sis simpler, and provides a potentially smooth (:onnee-tion to visual sensory data.2 Scene  Ident i f i ca t ionIn order to identify spatial scenes lmsed on inl)ut sen-tenees, some kind of information <>f detining each seell(~must exist.
As exph'dned in the OPEl),  "The dictio-nary is edited regarding the depiction of (weryday ob-jects and situations, in order to allow greater scopefor the treatment of these, objects and situatiovs inthe context of English-speaking countries" \[from l;'ofward in OPED\].
Each scene or pictorial entry i~, theOPED accompanied by a word list of entries f,'om thescene (see next section).
This bu,ldle of infi)rmation isthe basis for organizing our associate memory model.2.1 Constra intsHere we ~msume some constraints on the method ofrepresenting and using the OPED scenes:?
Only ordinal livivg scenes (384 scenes in(:lu(lingthousands of subseenes) are handled.
All scenesare hypothesized to be eonstructable by combina-tions of these scenes.?
Most of the words in OPEl) are noun terms ae-eoml)anied by adjective terms.
In this system,spatial-seenes are identified by using only thesewords.
No syntactical information is used.?
Compound words are dec<)mposed into primitiw'.words.?
The associative memory luus the ability to incre-mentally learn, but our analysis here uses a tixedset of scenes and words................................... ,Saqu~tlal mymbol Direct Logical 1 \[ r .
.
.
.
.
.
t .
.
.
.
uoI \[/0 I 12 I~tmc con ado on.
.
.
.
.
.
.
.
.
.
.
.  "
, "~ " " ~ _ _ _ _  \[3, Changa fo~:ut"?Z~-?oglcal process ldgT- -UK \ ]= 1 - - ?
: ?
rFigure l: PI)AI&CD architectureAmbiguous  DlsamblguatedFigure 2: ,qtrueture of OPED an{1 diagram ofPDAI&CD* Morphoh>gical nalysis is done by using the elec-tronie dictionary of Japatl Electronic DictionaryResear<:h inst itute (EDR).2.2 PDAI&CD and WAVEThe spatial scene identification system analyzed in thispaper is one moduh' of a general infi'rence architec-ture called l 'aralM l)istributed Associatiw."
Inferenceand Contradiction /)etection (PDAI&CD)(Tsunoda'\['.
and 'Fanak;t l\[., 1993), which uses an :msociatiw~.memory WAVE('\['sunoda T. an(\[ Tanaka H.) lmsed onneural networks and a logical veritieation system.
Wehaw~ previously presented itll application of that archi-tecture to semantic ?lisambiguation (Tsunoda T. andTanalat II., 1993).
It features a eognitive model of fastdisambiguation depending on context with bottom-upassociatiw:, memory together with a nmre precise top-(lown feedba(:k process (Fig.l).
After one scene is se-lected by previously inlmt words, the system can dis-ambiguate meaning of following words (as in the rightside of Fig.2).
In the.
future, we plan to combine natu-ral language proce.ssing with visual image from sensorydata.
Our representation f the spatial data fi'om theOPED is considered to be a simplest approximation ofsuch visual sensory images.311Table 1: Examples of semantic disambiguationEx .12Ambiguous Sentence # Classilied Meaningword (Context) scene of wordball Billiardslead(a)(a)(b)CarniwdKitchenAtom Iglobedancecordmetal2.3 Semantic DisambiguationWords in OPED have ditferent meanings correspond-ing to their use in ditferent scenes.
When a set of am-biguous words uniquely determines a scene, we con-clude that the words have been successfully disam-biguated.
We acknowledge that many other processesmay be involved in general word sense disambiguation,but use this scene-selection sense of word sense (lisain-biguation from here on.We illustrate typical two examples below.
The sys-tem with OPED and our associative memory can re(:-ognize these sentences and classify into each scene inthe dictionary.
Once a scene is identified, it assignseach ambiguous words uniquely.
We call it semanticaldisambiguation of words here.
The correspondances ofthe sentences and each meaning of word is summarizedin Table.1.1.
ba l l(a)(b)Tom shot a white cue ball with a cue.
Theball hit a red object ball and he thought it'slucky if it will ...Judy found that she was in a strangeworld.
Devils,dominos,pierrots,exotie girls,pirates,.
?, where am I?
'Oh!
', she said to her-self, a.s she found she wandered into a ball,2.
lead :(a) It's not sufficient o shield only by the lm-thick concrete?
The fission experiment re-quires additional 10cm-thick blocks of lea<l.Fission fragments released by the chain reac-tion of .
.
.
(b) He said to his son, "Please pull out the plugof the coffee grinder from the wall socket.
Becareful not to pull by the lea<l.
Ituum...hereI found the kettle.
"...Our system is able to disambiguate each meaning inthese examples actually.3 Representat ion and Process-ing Theory~ : : :  .
.
.
.
.
.
.
.
.
: .
.
.x ,  ......::~ i ~: :~,g !
::~+ ~zi~;~:~;: iL.
'~ i: : ,,',~if:t .......... :~ .:2:?
:'~ ">.
':"5-% 11711 words, 384 sceneswall 0 ,01\units o.o04~N,side 0 .008~- - - -~wall O.Ol--~all' ~',:,~ ;~ Ibookself 07251//row 0.7///...: ~- -~ l  ?
:...': ?
i .
:Figure 3: laving room scene and link example on theassociative memoryWAVEFigure 4: Weight of links and category selection3 .1  Representat ion  o f  OPEDThe Oxford Pictorial English Dictionary(OPED) h,~svery simple form of text and picture (Fig.3).
In thisexample, the upper part is a picture of a living roomscene, and the lower part consists of words of corre-sponding parts as follows:i wall units2 side wall3 bookselfOPP;I) has originally a hierachlcal structure of categorization (as in the left side of Fig.2), but we use themiddle level of it (shaded part in the figure), which ismost easily interl)retal~h!.To llrovide the associative memory model for l)ro -cessing words and selecting scenes, we, encode theOPED entries in tile WAVE model ms depicted inFig.3.
The weights between scene elements are au-tomatically learned during tile constructiou of the as-sociative memory.3.2  S impl i f ied  Mode l  o f  Assoc ia t iveMemory  WAVEThe aim of using m~sociative memory for identifica-tion is to select tile most likely scene based on incom-plete word data from sentences.
Ii and Ci are set tobe elements of input space SI, scene space So:, respec-tively, in an ideal state, the approl)riate scene Ci is312mfiquely indexed by z~ssociation from a complete inputvector: Ii A Ci.In the typical situation, however, the complete indexis not provided and we require a way of ranking cam-peting scenes by defining a weighted activation valuewhich depends on the i)artial inlmt, or set of ambigu-ous words, as follows:Ci = f (EWi f l J )  (1)J1f (x )  - (2) 1 + e-~'(a)where the weight of each compone.nt is given bytheconditional probability valueW~j - P (C i l6  ) (4)A maximum-likelihoad scene is selected by a winner-take-all network:c .
= .
, f i l ed  (5)This type of assaeiative meinory has following fea-tttres:?
Unlike correlative models (Amari S. and MaginuK., 1988), neither distortion of pattern nor pseudolocal minimum solutions arise from memorizingother patterns.?
Memory capacity is O(mn) compared to O(n "2)of correlative Inodel, where m is average immberof wards per scene, and n is the total number afpossible words.?
Unlike back-propagation learning algorithms, in-cremental earning is l)ossilflc at any time inWAVE.3.3  Reca l l ing  pro l )ab i l i ty  and  es t ima-t ion  of required quant i ty  of in fo fmat ionTile me`asure of scene selectivity is reduced to tile con-dition whether given words are unique to the SCelle.
Ifall input words are cOlnlnon to l)lura\] scenes, they cannot determine the original scene uniquely.
For exam-pie, tile system can not determine whether to choosecategory CA ar CB only by seeing element q}' in Fig.4.If 'a' or tile set {a, b} is given, it is able ta select CA.Here we estimate the selectivity by the ratio of suc-cessfld cases to all of possible cases ,as follaws(n is themlml}er of total elements, k is the number of elementsrelated to each scene, aim m is the total number ofscenes; incomplete information is dellned as a partialvector of elements number s (0 < s < k)).Tile pral)ability that s elements are shared si,nulta-neously by two patterns iskCs-t n-kCk.-.s-1 v(,, ,  k, ~) = (~)n CkTa extend this probal)ility to generalized cases ofm patterns, we use the munber s of elements of the(1)artial) input vector.
It can be estimated by countingthe negative ase where illore thall one pattern shareselelllents.1'(.,~, k, ,~, ,)0 (r)= (~v( , , ,< , . )
)  ..... ' - r ( ,~ ,k ,~-~, , ,0  (s)m - 2= (v, - p~) (~ 7,~I,: "'-~-~) (9)q~0m--2= vo,~ ) (m)q=:0v ,= v(n, <,.
), 7,~= v( , ,  k,,.
)r : : l  r= lThe results using this formula are shawn hi the nextsection.3.4  Infornmtion EntropyAs an alternative method of ewduation of spatial-see.he information of aPED,  we consider here self-information entropy and mntual-informatian e tropyalong with the information theory of ShannonC.E.(19,t8).
* Se l f - ln fo rmat ion  ent ropy :Fig.5 illustrates a talking scene.
Althoughsentences involving many ambiguous wards arehanded fr<>m the speaker to the listener, the lis-tener can disambiguate them with some kind ofknowkedge common to these people.
Conversely,the listner can determine scene 1)y the hande<l sen-tences.
The entropy of scene selection ainbiguityis reduced by the interaction.
We can define a con-cept of self-infarmation (SI) af the spatial-sceneidetification module as the entropy of ainbiguouswords or scenes.
Assuming equal probalfility tothe scene selection with no harmed ward, the en-tropy of the spatial-scene identitication can be cal-cualted.S lo -- - E I)( C J ) l"g2 I)( C J ) : log:, 38,1 = 8.59bitsJAfter the identiticatian, the meaning of eact, wordcan be selected according to each a selection dis-tril)ution flmctian updated by the Bayesian rule.S.\[1 = CE(C I X ) (11)= < -~r j~ l , ,~ l ' j~  > (12)j ir'ji = r (C j  I " i)  = P(~'i I @)  (13)Each P,j is equal to Wij as in Eq.(2).
<> repre-sents ensemble average over each xl.31,3sentencesListener I__L_._SpatialScenecommon knowledgeFigure 5: Common knowledge between speaker and lis-tener to disambiguate s mantics of handed sentences.Table 2: Mutual-information f OPEl)Scene entropy Mutual-inform.Without input 8.59 bits1 word input 0.80 bits 7.79 bits2 words inl)ut 0.32 bits 0.48 bitsMutua l - ln fo rmat ion  entropy:Mutual-information e tropy (MIE) can lye definedas the contribution of additional words to identifya scene, and consequently, tile selectiveness of thetarget word or scene.
In order to select a wordmeauing or scene fi'om the possible space Y, thespace C of M1 other words are considered in thecalculation of conditional entropy (CE).
Mutual-information entrot>y per word is calculated by fol-lowing formula:MIE(O;O') = CU(C l O ) -CE(C IO '  )Here, 0 is a set of previous tate parameters, and0 ~ is that of next one.
Mutual-inforamtion can lyeinterpreted ,as the reduction from a previous con-ditional entropy to corresponding updated con-ditional entrolyy with additional words.
We l)ro -vide a theoretical estimation of sclf-informatio,lof spatial-scenes with the dictionary in Table 2.Tile result suggests that it has the spa.tial-sceneidentification ability with a few words 1)rese,'va -tion.
It also supl)orts the consequence of a h)gical-summation algorithm shown in next section.4 Ana lyses  of ident i f icat ionmodu leHere we propose analyses of OPED and results of theo-retical simulations.
As formula (9) is expensive(11711!times), we use a Monte-Carlo simulation to abstract i scharacteristics.
Iteration thne in each case is 1,000.
* Fig.6 (a) shows a distribution of number of ele-ments involved in each scene in OPED.
It approx-imated a Gaussian distribution and has a average# Elemems i m .
.
.
.
.
tog(el .
.
.
.
IS?
nes per el?merit \]",o, ...... :2 o,Figure 6: (a) Distribution of number of elements perscene and (b) Distribution of number of scenes perelementswdue of 184.2.
This value is used ill the theoreti-cal simulations.?
Fig.6 (b) shows a distribution of number of sceneswhich are related to one element.
The regionwhere more than 100 scenes are related to oneword are those for trivial words like 'a', 'the', 'of','that',  'to', 'in', ~and', ~for', 'with', 's'.
Althoughwe could ignore these words for an actual appli-cation, we use them for fairness.?
Selection probability in the case that partialwords of scenes arc input to the mssoeiative men>cry is illustrated in Fig.7.
The recall rate in-cre`ases `as the input vector (set of words) becmnesmore similar to c:omplete vector (set of words) pat-tern.
Only about tlve words are enough to iden-tify each scene at recognition rate of 90 percent.Compared to the average, number of 184 wordsill each scene, this required mlmber is sufficientlysmall.
It proves good performance of the ` associa-tive memory used in this module.
'l~heoretical re-suits of a random distribution model is also shownin Fig.7.
The cause of the discrepancy betweenthe experiment and theoryis describe<l latter.
Thedotted line 'EXACT' ill the tlgure is a result ilS-ing logical-smnmation.
"File crossing point <>f the'OPED' line and the 'IgXACT' line.
is remarkable.Tile former has the adwmtage of expecting withrelatively high-probMfility (likelihood) using in-put words of small number.
Though with moreadditional words, the algorithm is deDated by thesimple logical-sumination.
As our architecturePDAI&CD uses dual-phase of expectation andevaluation, we can get a solution with maximum-likelihood slttisfying constraints automatically.?
Fig.8 shows tile distribution of mnnber of elementscontributing to identify each scene uniquely.?
In order to clarify tile discrepancy of tlle experi-mental an?l theoretical results, tile number of ele-l nents  overlal)lmd ill any two st:ones are connted.314Recalling ratio.o1.64).4~).21" I \[ I I5 10 15 20Number of elements of partial matchFigure 7: Recalling prollahility to number of partialinput elementsRecalling ratiot.
( ~ _ i3.13.13.,0.
:0JI'\[Tr;h~I I5 10 15 20Number of elements of partial matchFigure 8: Distribution of mmfller of partial inlmt ele-ments to identify scenesAs in Fig.9, tit(', number of overlal)ping (,lernentsin the.
the.oretieal e~dculation is very small com-pared to the experhr,ents with Of)El).
OPfi',D-2ill tile figure illustrates the same ,?alue withoutusing trivial words like 'a', 'the', 'of', ' that' ,  'to',' in', 'and', fief', 'with', 's'.
But the.
existence ofthese words can not explain the whole discrep-ancy.
This will be deserilled in the next sectionill more detail.
* As filrther investigation in order to explain tilediscrepancy of 'EXACT'(logical-sunnnation) and'OPED'(with our associative memory), distrilm-tion of weight v~tlues is shown in l,'ig.10.
I,~)/';ical-surnmation me.thod is achieved by a spe(:ial algo-rithm similar to the associative memory.
Only tileditferenee is that it uses equal weight value with-log(number)Figure 9: Distribution of number of elements comnmnto two  seel les4210.210g(number)650.4Distribution of weighl value0.G 0.8 1.0Vigure 10: Distribution of weight valueout any wtrianee, l lut in practic~tl, the experimen-tal result of 'OPED'  as ill \]'~ig.10 shows am exis-tence of enormous wtriance ill tile distrilmtion ofweight value.
Though tile varimme helps the selec-tivity with it few words, it disturhs the expectivitywith  lllOl'e thal l  l\]lrt!e w()rds eol ivers(qy, l \[el 'e wesumnmrize the interl)ret;ttion of the gaps ~tmonF,the theoretical expectation, the rest, It of logic~tl-summalion('\]';XAC'.l"), and the system('OPl~,l)'):1. l'~xsistem:e of trivial words in most of tileseelleS.2.
Variance of weight distribution.3.
l)ilference of characteristics hetwee.n algo-rithms.?
Abstracted results are summarized in Tabh.'.3.
Inthis table, the number of re.gistered words ill dic-tionary itself is ditferent from the nurnber of thetotal words analyzed hy our systern.
The diserep-alley arises mainly Dora the fact that we analyzedemnpound words into simple words (e.g.
'researchlaboratory' to 'research' ~'~ittl ' laboratory').315Table 3: Summarized resultsTotal ~ of scenes 384 scenesRegistered # of words 27,500 wordsTotal # of words 11,711 wordsAverage # of words / scene 184.2 wordsMm,~ # of words in one scene 478 wordsRequired # of words to 5 wordsidentify scenes at 90% ratioRequired # of words to 4 wordsidentify scenes at 90% ratioby exact match algorithmTheoretical estimation of 2 wordsrequired # of words toidentify scenes at 90% ratio5 SummaryWe analyzed the selectivity of our 384 living sceneswith many sets of words which are part of 11,711 wordsused in the dictionary OPED.
The average munber ofwords in one scene is about 184.
The probability of re-calling correct scenes with input partial words is difl'er-ent from the theoretical simulation of random assign-ment constructed with vMues of these parameters.
Un-like random generation of arbitrary symbols, seman-tics of natural anguage consists of highly-correlatedmeanings of words.
Although the theoretical simula-tion of the simplified model suggests a rough estima-tion of disambiguation requirements weshould analyzethe dictionary itself as in this paper.Another suggestive analysis is using Shannon's in-formation or entropy, which gives us more accurate.information depending on prol)ability of each phe-nomenon.
It shows how to estimate the amount ofsemantic ambiguity.Spatial-scene identification is one of the simplestkind of context necessary to disambiguate meaning ofwords an(\[ offer a new method for future integration ofnatural language processing and visual pattern recog-nition.6 AcknowledgementsThe authors acknowledge Randy Goebel, NancyIde, Jean Veronis, Hiroaki Kitano, Koiichi IIashida,Katashi Nagao and Lawrence A. Bookman for helpfuldiscussions and suggestions.
Also the authors thankKazuhiro Nala~tdai nd Satoshi Murakami for trans-formation of the pictorial dictionary into machine-readable one.
This research is supported by Fellow-ships of the Japan Society for the Promotion of Sci-ence for Japanese Junior Scientists and Grant-in-Aidfor Scientific Research on Priority Areas by the Min-istry of Educations, Science and Culture, Japan.References\[1\] Amari S. and Maginu K. (1988).
Statistical Neu-rodynamics of Associative Memory.
Neural Net-works, Vol.
1-I, pp.63-73.\[2\] Barwise .\].
and Perry J.
(1983).
Situation andAttitudes, MIT-Prcss.\[3\] Bookman L.A. (1993).
A ScMable Architecturefor Integrating Associative and Semantic Mem-ory.
Connection Science, Vol.
5.\[4\] Cottrell G.W.
(1989).
A Connectionist Approachto Word Sense Disambiguation, Pitman, MorganI(aufmann Pub.\[5\] Ide N.M. and Vcronis J.
(1993).
ExtractingKnowledge Bases from Machine-Readal)le Dic-tionaries: Have We Wasted Our Time?
In KB~ KS 93, pp.257-266.\[6\] Kohonen T. (1984).
Self-Organization and Asso-ciative Memory, Springer-Vcrlag.\[7\] Miikkulainen R. (1993).
Subsymbolic NaturalLanguage I)rocessing : An Inteyrated Model ofScripts, Lea:icon, and Memory., MIT-Press.\[8\] Nishikimi M., Nakashima II.
and Matsubara II.(1992).
Language Acquisition ,'us Learning.
InProceedings of COLING-92, pp.707-713.\[9\] Shannon C.E.
(1948).
A Mathematical Theoryof Communication.
Bell System 7~ch.
J., Vol.27,pp.373-423, 623-656.\[10\] Sharkey N.E.
(1989).
A PDP Learning Approachto Naural Language Understanding.
In Alexm~-der I.
Ed., Neural Computing Architectures :The Design of Brain-like Machines, MIT-Press,pp.92-116.\[11\] Shastri L. (1988).
Semantic Networks: An Evi-dential Formalization and its Connectionist Re-alization, Morgan Kauflnann.\[12\] Tsunod~t T. and Tanal~t It.
(1992).
SemanticAmbiguity Resolution by Parallel Distrit)utedAssociative Inference and Contradiction Detec-tion.
In Proceedings of LICNN-Nagoya93, Vol.
I,pp.163-166.\[131 Tsunoda T. and Tanata't H. (1993).
Winner As-sociatiw; Voting Engine (WAVE).
In Proceedingsof LlCNN-Beijing92, Vol.3, pp.589-594.\[141 Veronis J. and Ide N.M. (1990).
Word Sense Dis-ambiguation with Very Large Neural NetworksExtracted from Machine Readable Dictionaries.In Proceedings of COLING-90, pp.389-394.\[15\] Waltz D.L.
and Pollack J.B. (1985).
MassivelyParallel Parsing : A Strongly Interactive Modelof Natural Language Interpretation.
COGNI-TIVE SCIENCE, Vol.9, pp.51-74.316Generation
