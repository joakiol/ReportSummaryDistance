Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524?1534,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsNamed Entity Recognition in Tweets:An Experimental StudyAlan Ritter, Sam Clark, Mausam and Oren EtzioniComputer Science and EngineeringUniversity of WashingtonSeattle, WA 98125, USA{aritter,ssclark,mausam,etzioni}@cs.washington.eduAbstractPeople tweet more than 100 Million timesdaily, yielding a noisy, informal, but some-times informative corpus of 140-charactermessages that mirrors the zeitgeist in an un-precedented manner.
The performance ofstandard NLP tools is severely degraded ontweets.
This paper addresses this issue byre-building the NLP pipeline beginning withpart-of-speech tagging, through chunking, tonamed-entity recognition.
Our novel T-NERsystem doubles F1 score compared with theStanford NER system.
T-NER leverages theredundancy inherent in tweets to achieve thisperformance, using LabeledLDA to exploitFreebase dictionaries as a source of distantsupervision.
LabeledLDA outperforms co-training, increasing F1 by 25% over ten com-mon entity types.Our NLP tools are available at: http://github.com/aritter/twitter_nlp1 IntroductionStatus Messages posted on Social Media websitessuch as Facebook and Twitter present a new andchallenging style of text for language technologydue to their noisy and informal nature.
Like SMS(Kobus et al, 2008), tweets are particularly terseand difficult (See Table 1).
Yet tweets provide aunique compilation of information that is more up-to-date and inclusive than news articles, due to thelow-barrier to tweeting, and the proliferation of mo-bile devices.1 The corpus of tweets already exceeds1See the ?trending topics?
displayed on twitter.comthe size of the Library of Congress (Hachman, 2011)and is growing far more rapidly.
Due to the vol-ume of tweets, it is natural to consider named-entityrecognition, information extraction, and text miningover tweets.
Not surprisingly, the performance of?off the shelf?
NLP tools, which were trained onnews corpora, is weak on tweet corpora.In response, we report on a re-trained ?NLPpipeline?
that leverages previously-tagged out-of-domain text, 2 tagged tweets, and unlabeled tweetsto achieve more effective part-of-speech tagging,chunking, and named-entity recognition.1 The Hobbit has FINALLY started filming!
Icannot wait!2 Yess!
Yess!
Its official Nintendo announcedtoday that they Will release the Nintendo 3DSin north America march 27 for $2503 Government confirms blast n nuclear plants njapan...don?t knw wht s gona happen nw...Table 1: Examples of noisy text in tweets.We find that classifying named entities in tweets isa difficult task for two reasons.
First, tweets containa plethora of distinctive named entity types (Compa-nies, Products, Bands, Movies, and more).
Almostall these types (except for People and Locations) arerelatively infrequent, so even a large sample of man-ually annotated tweets will contain few training ex-amples.
Secondly, due to Twitter?s 140 characterlimit, tweets often lack sufficient context to deter-mine an entity?s type without the aid of background2Although tweets can be written on any subject, followingconvention we use the term ?domain?
to include text styles orgenres such as Twitter, News or IRC Chat.1524knowledge.To address these issues we propose a distantly su-pervised approach which applies LabeledLDA (Ra-mage et al, 2009) to leverage large amounts of unla-beled data in addition to large dictionaries of entitiesgathered from Freebase, and combines informationabout an entity?s context across its mentions.We make the following contributions:1.
We experimentally evaluate the performance ofoff-the-shelf news trained NLP tools when ap-plied to Twitter.
For example POS taggingaccuracy drops from about 0.97 on news to0.80 on tweets.
By utilizing in-domain, out-of-domain, and unlabeled data we are able tosubstantially boost performance, for exampleobtaining a 52% increase in F1 score on seg-menting named entities.2.
We introduce a novel approach to distant super-vision (Mintz et al, 2009) using Topic Models.LabeledLDA is applied, utilizing constraintsbased on an open-domain database (Freebase)as a source of supervision.
This approach in-creases F1 score by 25% relative to co-training(Blum and Mitchell, 1998; Yarowsky, 1995) onthe task of classifying named entities in Tweets.The rest of the paper is organized as follows.We successively build the NLP pipeline for Twitterfeeds in Sections 2 and 3.
We first present our ap-proaches to shallow syntax ?
part of speech tagging(?2.1), and shallow parsing (?2.2).
?2.3 describes anovel classifier that predicts the informativeness ofcapitalization in a tweet.
All tools in ?2 are usedas features for named entity segmentation in ?3.1.Next, we present our algorithms and evaluation forentity classification (?3.2).
We describe related workin ?4 and conclude in ?5.2 Shallow Syntax in TweetsWe first study two fundamental NLP tasks ?
POStagging and noun-phrase chunking.
We also discussa novel capitalization classifier in ?2.3.
The outputsof all these classifiers are used in feature generationfor named entity recognition in the next section.For all experiments in this section we use a datasetof 800 randomly sampled tweets.
All results (TablesAccuracy ErrorReductionMajority Baseline (NN) 0.189 -Word?s Most Frequent Tag 0.760 -Stanford POS Tagger 0.801 -T-POS(PTB) 0.813 6%T-POS(Twitter) 0.853 26%T-POS(IRC + PTB) 0.869 34%T-POS(IRC + Twitter) 0.870 35%T-POS(PTB + Twitter) 0.873 36%T-POS(PTB + IRC + Twitter) 0.883 41%Table 2: POS tagging performance on tweets.
By trainingon in-domain labeled data, in addition to annotated IRCchat data, we obtain a 41% reduction in error over theStanford POS tagger.2, 4 and 5) represent 4-fold cross-validation experi-ments on the respective tasks.32.1 Part of Speech TaggingPart of speech tagging is applicable to a wide rangeof NLP tasks including named entity segmentationand information extraction.Prior experiments have suggested that POS tag-ging has a very strong baseline: assign each wordto its most frequent tag and assign each Out of Vo-cabulary (OOV) word the most common POS tag.This baseline obtained a 0.9 accuracy on the Browncorpus (Charniak et al, 1993).
However, the appli-cation of a similar baseline on tweets (see Table 2)obtains a much weaker 0.76, exposing the challeng-ing nature of Twitter data.A key reason for this drop in accuracy is that Twit-ter contains far more OOV words than grammaticaltext.
Many of these OOV words come from spellingvariation, e.g., the use of the word ?n?
for ?in?
in Ta-ble 1 example 3.
Although NNP is the most frequenttag for OOV words, only about 1/3 are NNPs.The performance of off-the-shelf news-trainedPOS taggers also suffers on Twitter data.
The state-of-the-art Stanford POS tagger (Toutanova et al,2003) improves on the baseline, obtaining an accu-racy of 0.8.
This performance is impressive giventhat its training data, the Penn Treebank WSJ (PTB),is so different in style from Twitter, however it is ahuge drop from the 97% accuracy reported on the3We used Brendan O?Connor?s Twitter tokenizer1525Gold Predicted StanfordErrorT-POS Error ErrorReductionNN NNP 0.102 0.072 29%UH NN 0.387 0.047 88%VB NN 0.071 0.032 55%NNP NN 0.130 0.125 4%UH NNP 0.200 0.036 82%Table 3: Most common errors made by the Stanford POSTagger on tweets.
For each case we list the fraction oftimes the gold tag is misclassified as the predicted forboth our system and the Stanford POS tagger.
All verbsare collapsed into VB for compactness.PTB.
There are several reasons for this drop in per-formance.
Table 3 lists common errors made bythe Stanford tagger.
First, due to unreliable capi-talization, common nouns are often misclassified asproper nouns, and vice versa.
Also, interjectionsand verbs are frequently misclassified as nouns.
Inaddition to differences in vocabulary, the grammarof tweets is quite different from edited news text.For instance, tweets often start with a verb (wherethe subject ?I?
is implied), as in: ?watchng americandad.
?To overcome these differences in style and vocab-ulary, we manually annotated a set of 800 tweets(16K tokens) with tags from the Penn TreeBank tagset for use as in-domain training data for our POStagging system, T-POS.4 We add new tags for theTwitter specific phenomena: retweets, @usernames,#hashtags, and urls.
Note that words in these cate-gories can be tagged with 100% accuracy using sim-ple regular expressions.
To ensure fair comparisonin Table 2, we include a postprocessing step whichtags these words appropriately for all systems.To help address the issue of OOV words andlexical variations, we perform clustering to grouptogether words which are distributionally similar(Brown et al, 1992; Turian et al, 2010).
In particu-lar, we perform hierarchical clustering using Jcluster(Goodman, 2001) on 52 million tweets; each wordis uniquely represented by a bit string based on thepath from the root of the resulting hierarchy to theword?s leaf.
We use the Brown clusters resultingfrom prefixes of 4, 8, and 12 bits.
These clusters areoften effective in capturing lexical variations, for ex-4Using MMAX2 (Mu?ller and Strube, 2006) for annotation.ample, following are lexical variations on the word?tomorrow?
from one cluster after filtering out otherwords (most of which refer to days):?2m?, ?2ma?, ?2mar?, ?2mara?, ?2maro?,?2marrow?, ?2mor?, ?2mora?, ?2moro?, ?2mo-row?, ?2morr?, ?2morro?, ?2morrow?, ?2moz?,?2mr?, ?2mro?, ?2mrrw?, ?2mrw?, ?2mw?,?tmmrw?, ?tmo?, ?tmoro?, ?tmorrow?, ?tmoz?,?tmr?, ?tmro?, ?tmrow?, ?tmrrow?, ?tm-rrw?, ?tmrw?, ?tmrww?, ?tmw?, ?tomaro?,?tomarow?, ?tomarro?, ?tomarrow?, ?tomm?,?tommarow?, ?tommarrow?, ?tommoro?, ?tom-morow?, ?tommorrow?, ?tommorw?, ?tomm-row?, ?tomo?, ?tomolo?, ?tomoro?, ?tomorow?,?tomorro?, ?tomorrw?, ?tomoz?, ?tomrw?,?tomz?T-POS uses Conditional Random Fields5 (Laf-ferty et al, 2001), both because of their ability tomodel strong dependencies between adjacent POStags, and also to make use of highly correlated fea-tures (for example a word?s identity in addition toprefixes and suffixes).
Besides employing the Brownclusters computed above, we use a fairly standard setof features that include POS dictionaries, spellingand contextual features.On a 4-fold cross validation over 800 tweets,T-POS outperforms the Stanford tagger, obtaining a26% reduction in error.
In addition we include 40Ktokens of annotated IRC chat data (Forsythand andMartell, 2007), which is similar in style.
Like Twit-ter, IRC data contains many misspelled/abbreviatedwords, and also more pronouns, and interjections,but fewer determiners than news.
Finally, we alsoleverage 50K POS-labeled tokens from the PennTreebank (Marcus et al, 1994).Overall T-POS trained on 102K tokens (12K fromTwitter, 40K from IRC and 50K from PTB) resultsin a 41% error reduction over the Stanford tagger,obtaining an accuracy of 0.883.
Table 3 lists gainson some of the most common error types, for ex-ample, T-POS dramatically reduces error on inter-jections and verbs that are incorrectly classified asnouns by the Stanford tagger.2.2 Shallow ParsingShallow parsing, or chunking is the task of identi-fying non-recursive phrases, such as noun phrases,5We use MALLET (McCallum, 2002).1526Accuracy ErrorReductionMajority Baseline (B-NP) 0.266 -OpenNLP 0.839 -T-CHUNK(CoNLL) 0.854 9%T-CHUNK(Twitter) 0.867 17%T-CHUNK(CoNLL + Twitter) 0.875 22%Table 4: Token-Level accuracy at shallow parsing tweets.We compare against the OpenNLP chunker as a baseline.verb phrases, and prepositional phrases in text.
Ac-curate shallow parsing of tweets could benefit sev-eral applications such as Information Extraction andNamed Entity Recognition.Off the shelf shallow parsers perform noticeablyworse on tweets, motivating us again to annotate in-domain training data.
We annotate the same set of800 tweets mentioned previously with tags from theCoNLL shared task (Tjong Kim Sang and Buchholz,2000).
We use the set of shallow parsing features de-scribed by Sha and Pereira (2003), in addition to theBrown clusters mentioned above.
Part-of-speech tagfeatures are extracted based on cross-validation out-put predicted by T-POS.
For inference and learning,again we use Conditional Random Fields.
We utilize16K tokens of in-domain training data (using crossvalidation), in addition to 210K tokens of newswiretext from the CoNLL dataset.Table 4 reports T-CHUNK?s performance at shal-low parsing of tweets.
We compare against the off-the shelf OpenNLP chunker6, obtaining a 22% re-duction in error.2.3 CapitalizationA key orthographic feature for recognizing namedentities is capitalization (Florian, 2002; Downey etal., 2007).
Unfortunately in tweets, capitalizationis much less reliable than in edited texts.
In addi-tion, there is a wide variety in the styles of capital-ization.
In some tweets capitalization is informative,whereas in other cases, non-entity words are capital-ized simply for emphasis.
Some tweets contain alllowercase words (8%), whereas others are in ALLCAPS (0.6%).To address this issue, it is helpful to incorporateinformation based on the entire content of the mes-6http://incubator.apache.org/opennlp/P R F1Majority Baseline 0.70 1.00 0.82T-CAP 0.77 0.98 0.86Table 5: Performance at predicting reliable capitalization.sage to determine whether or not its capitalizationis informative.
To this end, we build a capitaliza-tion classifier, T-CAP, which predicts whether or nota tweet is informatively capitalized.
Its output isused as a feature for Named Entity Recognition.
Wemanually labeled our 800 tweet corpus as havingeither ?informative?
or ?uninformative?
capitaliza-tion.
The criteria we use for labeling is as follows:if a tweet contains any non-entity words which arecapitalized, but do not begin a sentence, or it con-tains any entities which are not capitalized, then itscapitalization is ?uninformative?, otherwise it is ?in-formative?.For learning , we use Support Vector Ma-chines.7 The features used include: the frac-tion of words in the tweet which are capitalized,the fraction which appear in a dictionary of fre-quently lowercase/capitalized words but are not low-ercase/capitalized in the tweet, the number of timesthe word ?I?
appears lowercase and whether or notthe first word in the tweet is capitalized.
Resultscomparing against the majority baseline, which pre-dicts capitalization is always informative, are shownin Table 5.
Additionally, in ?3 we show that fea-tures based on our capitalization classifier improveperformance at named entity segmentation.3 Named Entity RecognitionWe now discuss our approach to named entity recog-nition on Twitter data.
As with POS tagging andshallow parsing, off the shelf named-entity recog-nizers perform poorly on tweets.
For example, ap-plying the Stanford Named Entity Recognizer to oneof the examples from Table 1 results in the followingoutput:[Yess]ORG!
[Yess]ORG!
Its official[Nintendo]LOC announced today that theyWill release the [Nintendo]ORG 3DS in north[America]LOC march 27 for $2507http://www.chasen.org/?taku/software/TinySVM/1527The OOV word ?Yess?
is mistaken as a named en-tity.
In addition, although the first occurrence of?Nintendo?
is correctly segmented, it is misclassi-fied, whereas the second occurrence is improperlysegmented ?
it should be the product ?Nintendo3DS?.
Finally ?north America?
should be segmentedas a LOCATION, rather than just ?America?.
In gen-eral, news-trained Named Entity Recognizers seemto rely heavily on capitalization, which we know tobe unreliable in tweets.Following Collins and Singer (1999), Downey etal.
(2007) and Elsner et al (2009), we treat classi-fication and segmentation of named entities as sepa-rate tasks.
This allows us to more easily apply tech-niques better suited towards each task.
For exam-ple, we are able to use discriminative methods fornamed entity segmentation and distantly supervisedapproaches for classification.
While it might be ben-eficial to jointly model segmentation and (distantlysupervised) classification using a joint sequence la-beling and topic model similar to that proposed bySauper et al (2010), we leave this for potential fu-ture work.Because most words found in tweets are not partof an entity, we need a larger annotated dataset to ef-fectively learn a model of named entities.
We there-fore use a randomly sampled set of 2,400 tweets forNER.
All experiments (Tables 6, 8-10) report resultsusing 4-fold cross validation.3.1 Segmenting Named EntitiesBecause capitalization in Twitter is less informativethan news, in-domain data is needed to train modelswhich rely less heavily on capitalization, and alsoare able to utilize features provided by T-CAP.We exhaustively annotated our set of 2,400 tweets(34K tokens) with named entities.8 A convention onTwitter is to refer to other users using the @ sym-bol followed by their unique username.
We deliber-ately choose not to annotate @usernames as entitiesin our data set because they are both unambiguous,and trivial to identify with 100% accuracy using asimple regular expression, and would only serve toinflate our performance statistics.
While there is am-biguity as to the type of @usernames (for example,8We found that including out-of-domain training data fromthe MUC competitions lowered performance at this task.P R F1 F1 inc.Stanford NER 0.62 0.35 0.44 -T-SEG(None) 0.71 0.57 0.63 43%T-SEG(T-POS) 0.70 0.60 0.65 48%T-SEG(T-POS, T-CHUNK) 0.71 0.61 0.66 50%T-SEG(All Features) 0.73 0.61 0.67 52%Table 6: Performance at segmenting entities varying thefeatures used.
?None?
removes POS, Chunk, and capital-ization features.
Overall we obtain a 52% improvementin F1 score over the Stanford Named Entity Recognizer.they can refer to people or companies), we believethey could be more easily classified using featuresof their associated user?s profile than contextual fea-tures of the text.T-SEG models Named Entity Segmentation as asequence-labeling task using IOB encoding for rep-resenting segmentations (each word either begins, isinside, or is outside of a named entity), and usesConditional Random Fields for learning and infer-ence.
Again we include orthographic, contextualand dictionary features; our dictionaries included aset of type lists gathered from Freebase.
In addition,we use the Brown clusters and outputs of T-POS,T-CHUNK and T-CAP in generating features.We report results at segmenting named entities inTable 6.
Compared with the state-of-the-art news-trained Stanford Named Entity Recognizer (Finkelet al, 2005), T-SEG obtains a 52% increase in F1score.3.2 Classifying Named EntitiesBecause Twitter contains many distinctive, and in-frequent entity types, gathering sufficient trainingdata for named entity classification is a difficult task.In any random sample of tweets, many types willonly occur a few times.
Moreover, due to theirterse nature, individual tweets often do not containenough context to determine the type of the enti-ties they contain.
For example, consider followingtweet:KKTNY in 45min..........without any prior knowledge, there is not enoughcontext to determine what type of entity ?KKTNY?refers to, however by exploiting redundancy in thedata (Downey et al, 2010), we can determine it islikely a reference to a television show since it of-1528ten co-occurs with words such as watching and pre-mieres in other contexts.9In order to handle the problem of many infre-quent types, we leverage large lists of entities andtheir types gathered from an open-domain ontology(Freebase) as a source of distant supervision, allow-ing use of large amounts of unlabeled data in learn-ing.Freebase Baseline: Although Freebase has verybroad coverage, simply looking up entities and theirtypes is inadequate for classifying named entities incontext (0.38 F-score, ?3.2.1).
For example, accord-ing to Freebase, the mention ?China?
could refer toa country, a band, a person, or a film.
This prob-lem is very common: 35% of the entities in our dataappear in more than one of our (mutually exclusive)Freebase dictionaries.
Additionally, 30% of entitiesmentioned on Twitter do not appear in any Freebasedictionary, as they are either too new (for example anewly released videogame), or are misspelled or ab-breviated (for example ?mbp?
is often used to referto the ?mac book pro?
).Distant Supervision with Topic Models: Tomodel unlabeled entities and their possible types, weapply LabeledLDA (Ramage et al, 2009), constrain-ing each entity?s distribution over topics based onits set of possible types according to Freebase.
Incontrast to previous weakly supervised approachesto Named Entity Classification, for example the Co-Training and Na?
?ve Bayes (EM) models of Collinsand Singer (1999), LabeledLDA models each entitystring as a mixture of types rather than using a singlehidden variable to represent the type of each men-tion.
This allows information about an entity?s dis-tribution over types to be shared across mentions,naturally handling ambiguous entity strings whosementions could refer to different types.Each entity string in our data is associated with abag of words found within a context window aroundall of its mentions, and also within the entity itself.As in standard LDA (Blei et al, 2003), each bag ofwords is associated with a distribution over topics,Multinomial(?e), and each topic is associated with adistribution over words, Multinomial(?t).
In addi-tion, there is a one-to-one mapping between topicsand Freebase type dictionaries.
These dictionaries9Kourtney & Kim Take New York.constrain ?e, the distribution over topics for each en-tity string, based on its set of possible types, FB[e].For example, ?Amazon could correspond to a distribu-tion over two types: COMPANY, and LOCATION,whereas ?Apple might represent a distribution overCOMPANY, and FOOD.
For entities which aren?tfound in any of the Freebase dictionaries, we leavetheir topic distributions ?e unconstrained.
Note thatin absence of any constraints LabeledLDA reducesto standard LDA, and a fully unsupervised settingsimilar to that presented by Elsner et.
al.
(2009).In detail, the generative process that models ourdata for Named Entity Classification is as follows:for each type: t = 1 .
.
.
T doGenerate ?t according to symmetric Dirichletdistribution Dir(?
).end forfor each entity string e = 1 .
.
.
|E| doGenerate ?e over FB[e] according to Dirichletdistribution Dir(?FB[e]).for each word position i = 1 .
.
.
Ne doGenerate ze,i from Mult(?e).Generate the word we,i from Mult(?ze,i).end forend forTo infer values for the hidden variables, we applyCollapsed Gibbs sampling (Griffiths and Steyvers,2004), where parameters are integrated out, and theze,is are sampled directly.In making predictions, we found it beneficial toconsider ?traine as a prior distribution over types forentities which were encountered during training.
Inpractice this sharing of information across contextsis very beneficial as there is often insufficient evi-dence in an isolated tweet to determine an entity?stype.
For entities which weren?t encountered dur-ing training, we instead use a prior based on the dis-tribution of types across all entities.
One approachto classifying entities in context is to assume that?traine is fixed, and that all of the words inside theentity mention and context, w, are drawn based ona single topic, z, that is they are all drawn fromMultinomial(?z).
We can then compute the poste-rior distribution over types in closed form with asimple application of Bayes rule:P (z|w) ?
?w?wP (w|z : ?
)P (z : ?traine )During development, however, we found that ratherthan making these assumptions, using Gibbs Sam-1529Type Top 20 Entities not found in Freebase dictionariesPRODUCT nintendo ds lite, apple ipod, generation black, ipod nano, apple iphone, gb black, xperia, ipods, verizonmedia, mac app store, kde, hd video, nokia n8, ipads, iphone/ipod, galaxy tab, samsung galaxy, playstationportable, nintendo ds, vpnTV-SHOW pretty little, american skins, nof, order svu, greys, kktny, rhobh, parks & recreation, parks & rec, dawson?s creek, big fat gypsy weddings, big fat gypsy wedding, winter wipeout, jersey shores, idiot abroad, royle,jerseyshore, mr .
sunshine, hawaii five-0, new jersey shoreFACILITY voodoo lounge, grand ballroom, crash mansion, sullivan hall, memorial union, rogers arena, rockwoodmusic hall, amway center, el mocambo, madison square, bridgestone arena, cat club, le poisson rouge,bryant park, mandalay bay, broadway bar, ritz carlton, mgm grand, olympia theatre, consol energy centerTable 7: Example type lists produced by LabeledLDA.
No entities which are shown were found in Freebase; these aretypically either too new to have been added, or are misspelled/abbreviated (for example rhobh=?Real Housewives ofBeverly Hills?).
In a few cases there are segmentation errors.pling to estimate the posterior distribution over typesperforms slightly better.
In order to make predic-tions, for each entity we use an informative Dirich-let prior based on ?traine and perform 100 iterations ofGibbs Sampling holding the hidden topic variablesin the training data fixed (Yao et al, 2009).
Feweriterations are needed than in training since the type-word distributions, ?
have already been inferred.3.2.1 Classification ExperimentsTo evaluate T-CLASS?s ability to classify entitymentions in context, we annotated the 2,400 tweetswith 10 types which are both popular on Twitter,and have good coverage in Freebase: PERSON,GEO-LOCATION, COMPANY, PRODUCT, FACIL-ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND,and OTHER.
Note that these type annotations areonly used for evaluation purposes, and not used dur-ing training T-CLASS, which relies only on distantsupervision.
In some cases, we combine multi-ple Freebase types to create a dictionary of entitiesrepresenting a single type (for example the COM-PANY dictionary contains Freebase types /busi-ness/consumer company and /business/brand).
Be-cause our approach does not rely on any manuallylabeled examples, it is straightforward to extend itfor a different sets of types based on the needs ofdownstream applications.Training: To gather unlabeled data for inference,we run T-SEG, our entity segmenter (from ?3.1), on60M tweets, and keep the entities which appear 100or more times.
This results in a set of 23,651 dis-tinct entity strings.
For each entity string, we col-lect words occurring in a context window of 3 wordsfrom all mentions in our data, and use a vocabularyof the 100K most frequent words.
We run Gibbssampling for 1,000 iterations, using the last sampleto estimate entity-type distributions ?e, in additionto type-word distributions ?t.
Table 7 displays the20 entities (not found in Freebase) whose posteriordistribution ?e assigns highest probability to selectedtypes.Results: Table 8 presents the classification re-sults of T-CLASS compared against a majority base-line which simply picks the most frequent class(PERSON), in addition to the Freebase baseline,which only makes predictions if an entity appearsin exactly one dictionary (i.e., appears unambigu-ous).
T-CLASS also outperforms a simple super-vised baseline which applies a MaxEnt classifier us-ing 4-fold cross validation over the 1,450 entitieswhich were annotated for testing.
Additionally wecompare against the co-training algorithm of Collinsand Singer (1999) which also leverages unlabeleddata and uses our Freebase type lists; for seed ruleswe use the ?unambiguous?
Freebase entities.
Ourresults demonstrate that T-CLASS outperforms thebaselines and achieves a 25% increase in F1 scoreover co-training.Tables 9 and 10 present a breakdown of F1 scoresby type, both collapsing types into the standardclasses used in the MUC competitions (PERSON,LOCATION, ORGANIZATION), and using the 10popular Twitter types described earlier.Entity Strings vs.
Entity Mentions: DL-Cotrainand LabeledLDA use two different representationsfor the unlabeled data during learning.
LabeledLDAgroups together words across all mentions of an en-1530System P R F1Majority Baseline 0.30 0.30 0.30Freebase Baseline 0.85 0.24 0.38Supervised Baseline 0.45 0.44 0.45DL-Cotrain 0.54 0.51 0.53LabeledLDA 0.72 0.60 0.66Table 8: Named Entity Classification performance on the10 types.
Assumes segmentation is given as in (Collinsand Singer, 1999), and (Elsner et al, 2009).Type LL FB CT SP NPERSON 0.82 0.48 0.65 0.83 436LOCATION 0.74 0.21 0.55 0.67 372ORGANIZATION 0.66 0.52 0.55 0.31 319overall 0.75 0.39 0.59 0.49 1127Table 9: F1 classification scores for the 3 MUC typesPERSON, LOCATION, ORGANIZATION.
Results areshown using LabeledLDA (LL), Freebase Baseline (FB),DL-Cotrain (CT) and Supervised Baseline (SP).
N is thenumber of entities in the test set.Type LL FB CT SP NPERSON 0.82 0.48 0.65 0.86 436GEO-LOC 0.77 0.23 0.60 0.51 269COMPANY 0.71 0.66 0.50 0.29 162FACILITY 0.37 0.07 0.14 0.34 103PRODUCT 0.53 0.34 0.40 0.07 91BAND 0.44 0.40 0.42 0.01 54SPORTSTEAM 0.53 0.11 0.27 0.06 51MOVIE 0.54 0.65 0.54 0.05 34TV-SHOW 0.59 0.31 0.43 0.01 31OTHER 0.52 0.14 0.40 0.23 219overall 0.66 0.38 0.53 0.45 1450Table 10: F1 scores for classification broken down bytype for LabeledLDA (LL), Freebase Baseline (FB), DL-Cotrain (CT) and Supervised Baseline (SP).
N is the num-ber of entities in the test set.P R F1DL-Cotrain-entity 0.47 0.45 0.46DL-Cotrain-mention 0.54 0.51 0.53LabeledLDA-entity 0.73 0.60 0.66LabeledLDA-mention 0.57 0.52 0.54Table 11: Comparing LabeledLDA and DL-Cotraingrouping unlabeled data by entities vs. mentions.System P R F1COTRAIN-NER (10 types) 0.55 0.33 0.41T-NER(10 types) 0.65 0.42 0.51COTRAIN-NER (PLO) 0.57 0.42 0.49T-NER(PLO) 0.73 0.49 0.59Stanford NER (PLO) 0.30 0.27 0.29Table 12: Performance at predicting both segmentationand classification.
Systems labeled with PLO are evalu-ated on the 3 MUC types PERSON, LOCATION, ORGA-NIZATION.tity string, and infers a distribution over its possi-ble types, whereas DL-Cotrain considers the entitymentions separately as unlabeled examples and pre-dicts a type independently for each.
In order toensure that the difference in performance betweenLabeledLDA and DL-Cotrain is not simply due tothis difference in representation, we compare bothDL-Cotrain and LabeledLDA using both unlabeleddatasets (grouping words by all mentions vs. keep-ing mentions separate) in Table 11.
As expected,DL-Cotrain performs poorly when the unlabeled ex-amples group mentions; this makes sense, since Co-Training uses a discriminative learning algorithm,so when trained on entities and tested on individualmentions, the performance decreases.
Additionally,LabeledLDA?s performance is poorer when consid-ering mentions as ?documents?.
This is likely dueto the fact that there isn?t enough context to effec-tively learn topics when the ?documents?
are veryshort (typically fewer than 10 words).End to End System: Finally we present the endto end performance on segmentation and classifica-tion (T-NER) in Table 12.
We observe that T-NERagain outperforms co-training.
Moreover, compar-ing against the Stanford Named Entity Recognizeron the 3 MUC types, T-NER doubles F1 score.4 Related WorkThere has been relatively little previous work onbuilding NLP tools for Twitter or similar text styles.Locke and Martin (2009) train a classifier to recog-nize named entities based on annotated Twitter data,handling the types PERSON, LOCATION, and OR-GANIZATION.
Developed in parallel to our work,Liu et al (2011) investigate NER on the same 3types, in addition to PRODUCTs and present a semi-1531supervised approach using k-nearest neighbor.
Alsodeveloped in parallel, Gimpell et al (2011) build aPOS tagger for tweets using 20 coarse-grained tags.Benson et.
al.
(2011) present a system which ex-tracts artists and venues associated with musical per-formances.
Recent work (Han and Baldwin, 2011;Gouws et al, 2011) has proposed lexical normaliza-tion of tweets which may be useful as a preprocess-ing step for the upstream tasks like POS tagging andNER.
In addition Finin et.
al.
(2010) investigatethe use of Amazon?s Mechanical Turk for annotat-ing Named Entities in Twitter, Minkov et.
al.
(2005)investigate person name recognizers in email, andSingh et.
al.
(2010) apply a minimally supervisedapproach to extracting entities from text advertise-ments.In contrast to previous work, we have demon-strated the utility of features based on Twitter-specific POS taggers and Shallow Parsers in seg-menting Named Entities.
In addition we take a dis-tantly supervised approach to Named Entity Classi-fication which exploits large dictionaries of entitiesgathered from Freebase, requires no manually anno-tated data, and as a result is able to handle a largernumber of types than previous work.
Although wefound manually annotated data to be very beneficialfor named entity segmentation, we were motivatedto explore approaches that don?t rely on manual la-bels for classification due to Twitter?s wide range ofnamed entity types.
Additionally, unlike previouswork on NER in informal text, our approach allowsthe sharing of information across an entity?s men-tions which is quite beneficial due to Twitter?s tersenature.Previous work on Semantic Bootstrapping hastaken a weakly-supervised approach to classifyingnamed entities based on large amounts of unla-beled text (Etzioni et al, 2005; Carlson et al, 2010;Kozareva and Hovy, 2010; Talukdar and Pereira,2010; McIntosh, 2010).
In contrast, rather thanpredicting which classes an entity belongs to (e.g.a multi-label classification task), LabeledLDA esti-mates a distribution over its types, which is then use-ful as a prior when classifying mentions in context.In addition there has been been work on Skip-Chain CRFs (Sutton, 2004; Finkel et al, 2005)which enforce consistency when classifying multi-ple occurrences of an entity within a document.
Us-ing topic models (e.g.
LabeledLDA) for classifyingnamed entities has a similar effect, in that informa-tion about an entity?s distribution of possible typesis shared across its mentions.5 ConclusionsWe have demonstrated that existing tools for POStagging, Chunking and Named Entity Recognitionperform quite poorly when applied to Tweets.
Toaddress this challenge we have annotated tweets andbuilt tools trained on unlabeled, in-domain and out-of-domain data, showing substantial improvementover their state-of-the art news-trained counterparts,for example, T-POS outperforms the Stanford POSTagger, reducing error by 41%.
Additionally wehave shown the benefits of features generated fromT-POS and T-CHUNK in segmenting Named Entities.We identified named entity classification as a par-ticularly challenging task on Twitter.
Due to theirterse nature, tweets often lack enough context toidentify the types of the entities they contain.
In ad-dition, a plethora of distinctive named entity typesare present, necessitating large amounts of trainingdata.
To address both these issues we have presentedand evaluated a distantly supervised approach basedon LabeledLDA, which obtains a 25% increase in F1score over the co-training approach to Named En-tity Classification suggested by Collins and Singer(1999) when applied to Twitter.Our POS tagger, Chunker Named Entity Rec-ognizer are available for use by the researchcommunity: http://github.com/aritter/twitter_nlpAcknowledgmentsWe would like to thank Stephen Soderland, DanWeld and Luke Zettlemoyer, in addition to theanonymous reviewers for helpful comments on aprevious draft.
This research was supported in partby NSF grant IIS-0803481, ONR grant N00014-11-1-0294, Navy STTR contract N00014-10-M-0304, aNational Defense Science and Engineering Graduate(NDSEG) Fellowship 32 CFR 168a and carried outat the University of Washington?s Turing Center.1532ReferencesEdward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
In The49th Annual Meeting of the Association for Computa-tional Linguistics, Portland, Oregon, USA.
To appear.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.
Res.Avrim Blum and Tom M. Mitchell.
1998.
Combininglabeled and unlabeled sata with co-training.
In COLT,pages 92?100.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka, Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the third ACM interna-tional conference on Web search and data mining,WSDM ?10.Eugene Charniak, Curtis Hendrickson, Neil Jacobson,and Mike Perkowitz.
1993.
Equations for part-of-speech tagging.
In AAAI, pages 784?789.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In EmpiricalMethods in Natural Language Processing.Doug Downey, Matthew Broadhead, and Oren Etzioni.2007.
Locating complex named entities in web text.In Proceedings of the 20th international joint confer-ence on Artifical intelligence.Doug Downey, Oren Etzioni, and Stephen Soderland.2010.
Analysis of a probabilistic model of redundancyin unsupervised information extraction.
Artif.
Intell.,174(11):726?748.Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured generative models for unsupervisednamed-entity clustering.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, NAACL ?09.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: an ex-perimental study.
Artif.
Intell.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in Twitter data with crowd-sourcing.
In Proceedings of the NAACL Workshop onCreating Speech and Text Language Data With Ama-zon?s Mechanical Turk.
Association for ComputationalLinguistics, June.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, ACL ?05.Radu Florian.
2002.
Named entity recognition as a houseof cards: classifier stacking.
In Proceedings of the 6thconference on Natural language learning - Volume 20,COLING-02.Eric N. Forsythand and Craig H. Martell.
2007.
Lexicaland discourse analysis of online chat dialog.
In Pro-ceedings of the International Conference on SemanticComputing.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In ACL.Joshua T. Goodman.
2001.
A bit of progress in languagemodeling.
Technical report, Microsoft Research.Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-uard Hovy.
2011.
Contextual bearing on linguisticvariation in social media.
In ACL Workshop on Lan-guage in Social Media, Portland, Oregon, USA.
Toappear.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, April.Mark Hachman.
2011.
Humanity?s tweets: Just 20 ter-abytes.
In PCMAG.COM.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a #twitter.
InThe 49th Annual Meeting of the Association for Com-putational Linguistics, Portland, Oregon, USA.
To ap-pear.Catherine Kobus, Franc?ois Yvon, and Ge?raldineDamnati.
2008.
Normalizing sms: are two metaphorsbetter than one ?
In COLING, pages 441?448.Zornitsa Kozareva and Eduard H. Hovy.
2010.
Not allseeds are equal: Measuring the quality of text miningseeds.
In HLT-NAACL.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.In ACL.Brian Locke and James Martin.
2009.
Named entityrecognition: Adapting to microblogging.
In SeniorThesis, University of Colorado.1533Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.
In http://mallet.cs.umass.edu.Tara McIntosh.
2010.
Unsupervised discovery of nega-tive categories in lexicon bootstrapping.
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, EMNLP ?10.Einat Minkov, Richard C. Wang, and William W. Cohen.2005.
Extracting personal names from email: apply-ing named entity recognition to informal text.
In Pro-ceedings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, HLT ?05, pages 443?450, Morristown, NJ,USA.
Association for Computational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In Proceedings of ACL-IJCNLP2009.Christoph Mu?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy: New Re-sources, New Tools, New Methods, pages 197?214.
Pe-ter Lang, Frankfurt a.M., Germany.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled lda: a super-vised topic model for credit attribution in multi-labeledcorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 1 - Volume 1, EMNLP ?09, pages 248?256,Morristown, NJ, USA.
Association for ComputationalLinguistics.Christina Sauper, Aria Haghighi, and Regina Barzilay.2010.
Incorporating content structure into text anal-ysis applications.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 377?387, Morristown,NJ, USA.
Association for Computational Linguistics.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology - Volume 1, NAACL ?03.Sameer Singh, Dustin Hillard, and Chris Leggetter.
2010.Minimally-supervised extraction of entities from textadvertisements.
In Human Language Technologies:Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL HLT).Charles Sutton.
2004.
Collective segmentation and la-beling of distant entities in information extraction.Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 1473?1481.
Associ-ation for Computational Linguistics.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task: chunking.In Proceedings of the 2nd workshop on Learning lan-guage in logic and the 4th conference on Computa-tional natural language learning - Volume 7, ConLL?00.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inference onstreaming document collections.
In Proceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd annual meeting on Association forComputational Linguistics, ACL ?95.1534
