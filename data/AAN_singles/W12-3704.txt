Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11?18,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsMining Sentiments from TweetsAkshat Bakliwal, Piyush Arora, Senthil MadhappanNikhil Kapre, Mukesh Singh and Vasudeva VarmaSearch and Information Extraction Lab,International Institute of Information Technology, Hyderabad.
{akshat.bakliwal, piyush.arora}@research.iiit.ac.in,{senthil.m, nikhil.kapre, mukeshkumar.singh}@students.iiit.ac.in,vv@iiit.ac.inAbstractTwitter is a micro blogging website, whereusers can post messages in very short textcalled Tweets.
Tweets contain user opin-ion and sentiment towards an object or per-son.
This sentiment information is very use-ful in various aspects for business and gov-ernments.
In this paper, we present a methodwhich performs the task of tweet sentimentidentification using a corpus of pre-annotatedtweets.
We present a sentiment scoring func-tion which uses prior information to classify(binary classification ) and weight various sen-timent bearing words/phrases in tweets.
Us-ing this scoring function we achieve classifi-cation accuracy of 87% on Stanford Datasetand 88% on Mejaj dataset.
Using supervisedmachine learning approach, we achieve classi-fication accuracy of 88% on Stanford dataset.1 IntroductionWith enormous increase in web technologies, num-ber of people expressing their views and opinionsvia web are increasing.
This information is veryuseful for businesses, governments and individuals.With over 340+ million Tweets (short text messages)per day, Twitter is becoming a major source of infor-mation.Twitter is a micro-blogging site, which is popularbecause of its short text messages popularly knownas ?Tweets?.
Tweets have a limit of 140 characters.Twitter has a user base of 140+ million active users11As on March 21, 2012.
Source:http://en.wikipedia.org/wiki/Twitterand thus is a useful source of information.
Usersoften discuss on current affairs and share their per-sonals views on various subjects via tweets.Out of all the popular social media?s like Face-book, Google+, Myspace and Twitter, we chooseTwitter because 1) tweets are small in length, thusless ambigious; 2) unbiased; 3) are easily accessiblevia API; 4) from various socio-cultural domains.In this paper, we introduce an approach which canbe used to find the opinion in an aggregated col-lection of tweets.
In this approach, we used twodifferent datasets which are build using emoticonsand list of suggestive words respectively as noisy la-bels.
We give a new method of scoring ?PopularityScore?, which allows determination of the popular-ity score at the level of individual words of a tweettext.
We also emphasis on various types and levelsof pre-processing required for better performance.Roadmap for rest of the paper: Related work isdiscussed in Section 2.
In Section 3, we describeour approach to address the problem of Twittersentiment classification along with pre-processingsteps.Datasets used in this research are discussed inSection 4.
Experiments and Results are presented inSection 5.
In Section 6, we present the feature vectorapproach to twitter sentiment classification.
Section7 presents as discussion on the methods and we con-clude the paper with future work in Section 8.2 Related WorkResearch in Sentiment Analysis of user generatedcontent can be categorized into Reviews (Turney,2002; Pang et al, 2002; Hu and Liu, 2004), Blogs(Draya et al, 2009; Chesley, 2006; He et al, 2008),11News (Godbole et al, 2007), etc.
All these cat-egories deal with large text.
On the other hand,Tweets are shorter length text and are difficult toanalyse because of its unique language and struc-ture.
(Turney, 2002) worked on product reviews.
Tur-ney used adjectives and adverbs for performingopinion classification on reviews.
He used PMI-IRalgorithm to estimate the semantic orientation of thesentiment phrase.
He achieved an average accuracyof 74% on 410 reviews of different domains col-lected from Epinion.
(Hu and Liu, 2004) performedfeature based sentiment analysis.
Using Noun-Nounphrases they identified the features of the productsand determined the sentiment orientation towardseach feature.
(Pang et al, 2002) tested various ma-chine learning algorithms on Movie Reviews.
Heachieved 81% accuracy in unigram presence featureset on Naive Bayes classifier.
(Draya et al, 2009) tried to identify domain spe-cific adjectives to perform blog sentiment analysis.They considered the fact that opinions are mainlyexpressed by adjectives and pre-defined lexicons failto identify domain information.
(Chesley, 2006) per-formed topic and genre independent blog classifica-tion, making novel use of linguistic features.
Eachpost from the blog is classified as positive, negativeand objective.To the best of our knowledge, there is very lessamount of work done in twitter sentiment analy-sis.
(Go et al, 2009) performed sentiment analy-sis on twitter.
They identified the tweet polarity us-ing emoticons as noisy labels and collected a train-ing dataset of 1.6 million tweets.
They reported anaccuracy of 81.34% for their Naive Bayes classi-fier.
(Davidov et al, 2010) used 50 hashtags and 15emoticons as noisy labels to create a dataset for twit-ter sentiment classification.
They evaluate the effectof different types of features for sentiment extrac-tion.
(Diakopoulos and Shamma, 2010) worked onpolitical tweets to identify the general sentiments ofthe people on first U.S. presidential debate in 2008.
(Bora, 2012) also created their dataset based onnoisy labels.
They created a list of 40 words (pos-itive and negative) which were used to identify thepolarity of tweet.
They used a combination ofa minimum word frequency threshold and Cate-gorical Proportional Difference as a feature selec-tion method and achieved the highest accuracy of83.33% on a hand labeled test dataset.
(Agarwal et al, 2011) performed three class (pos-itive, negative and neutral) classification of tweets.They collected their dataset using Twitter streamAPI and asked human judges to annotate the datainto three classes.
They had 1709 tweets of eachclass making a total of 5127 in all.
In their research,they introduced POS-specific prior polarity featuresalong with twitter specific features.
They achievedmax accuracy of 75.39% for unigram + senti fea-tures.Our work uses (Go et al, 2009) and (Bora, 2012)datasets for this research.
We use Naive Bayesmethod to decide the polarity of tokens in the tweets.Along with that we provide an useful insight on howpreprocessing should be done on tweet.
Our methodof Senti Feature Identification and Popularity Scoreperform well on both the datasets.
In feature vec-tor approach, we show the contribution of individualNLP and Twitter specific features.3 ApproachOur approach can be divided into various steps.Each of these steps are independent of the other butimportant at the same time.3.1 BaselineIn the baseline approach, we first clean the tweets.We remove all the special characters, targets (@),hashtags (#), URLs, emoticons, etc and learn thepositive & negative frequencies of unigrams in train-ing.
Every unigram token is given two probabilityscores: Positive Probability (Pp) and Negative Prob-ability (Np) (Refer Equation 1).
We follow the samecleaning process for the test tweets.
After clean-ing the test tweets, we form all the possible uni-grams and check for their frequencies in the trainingmodel.
We sum up the positive and negative proba-bility scores of all the constituent unigrams, and usetheir difference (positive - negative) to find the over-all score of the tweet.
If tweet score is > 0 then it is12positive otherwise negative.Pf = Frequency in Positive Training SetNf = Frequency in Negative Training SetPp = Positive Probability of the token.= Pf/(Pf + Nf )Np = Negative Probability of the token.= Nf/(Pf + Nf )(1)3.2 Emoticons and Punctuations HandlingWe make slight changes in the pre-processing mod-ule for handling emoticons and punctuations.
Weuse the emoticons list provided by (Agarwal et al,2011) in their research.
This list2 is built fromwikipedia list of emoticons3 and is hand tagged intofive classes (extremely positive, positive, neutral,negative and extremely negative).
In this experi-ment, we replace all the emoticons which are taggedpositive or extremely positive with ?zzhappyzz?
andrest all other emoticons with ?zzsadzz?.
We appendand prepend ?zz?
to happy and sad in order to pre-vent them from mixing into tweet text.
At the end,?zzhappyzz?
is scored +1 and ?zzsadzz?
is scored -1.Exclamation marks (!)
and question marks (?
)also carry some sentiment.
In general, ?!?
is usedwhen we have to emphasis on a positive word and???
is used to highlight the state of confusion ordisagreement.
We replace all the occurrences of ?!
?with ?zzexclaimzz?
and of ???
with ?zzquestzz?.
Weadd 0.1 to the total tweet score for each ?!?
and sub-tract 0.1 from the total tweet score for each ???.
0.1is chosen by trial and error method.3.3 StemmingWe use Porter Stemmer4 to stem the tweet words.We modify porter stemmer and restrict it to step 1only.
Step 1 gets rid of plurals and -ed or -ing.3.4 Stop Word RemovalStop words play a negative role in the task of senti-ment classification.
Stop words occur in both pos-itive and negative training set, thus adding moreambiguity in the model formation.
And also, stop2http://goo.gl/oCSnQ3http://en.wikipedia.org/wiki/List of emoticons4http://tartarus.org/m?artin/PorterStemmer/words don?t carry any sentiment information andthus are of no use to us.
We create a list of stopwords like he, she, at, on, a, the, etc.
and ignorethem while scoring.
We also discard words whichare of length ?
2 for scoring the tweet.3.5 Spell CorrectionTweets are written in random form, without any fo-cus given to correct structure and spelling.
Spellcorrection is an important part in sentiment analy-sis of user- generated content.
Users type certaincharacters arbitrary number of times to put more em-phasis on that.
We use the spell correction algo-rithm from (Bora, 2012).
In their algorithm, theyreplace a word with any character repeating morethan twice with two words, one in which the re-peated character is placed once and second in whichthe repeated character is placed twice.
For examplethe word ?swwweeeetttt?
is replaced with 8 words?swet?, ?swwet?, ?sweet?, ?swett?, ?swweet?, and soon.Another common type of spelling mistakes oc-cur because of skipping some of characters from thespelling.
like ?there?
is generally written as ?thr?.Such types of spelling mistakes are not currentlyhandled by our system.
We propose to use phoneticlevel spell correction method in future.3.6 Senti FeaturesAt this step, we try to reduce the effect of non-sentiment bearing tokens on our classification sys-tem.
In the baseline method, we considered all theunigram tokens equally and scored them using theNaive Bayes formula (Refer Equation 1).
Here, wetry to boost the scores of sentiment bearing words.In this step, we look for each token in a pre-definedlist of positive and negative words.
We use the list ofof most commonly used positive and negative wordsprovided by Twitrratr5.
When we come across a to-ken in this list, instead of scoring it using the NaiveBayes formula (Refer Equation 1), we score the to-ken +/- 1 depending on the list in which it exist.
Allthe tokens which are missing from this list went un-der step 3.3, 3.4, 3.5 and were checked for their oc-currence after each step.5http://twitrratr.com/133.7 Noun IdentificationAfter doing all the corrections (3.3 - 3.6) on a word,we look at the reduced word if it is being convertedto a Noun or not.
We identify the word as a Nounword by looking at its part of speech tag in EnglishWordNet(Miller, 1995).
If the majority sense (mostcommonly used sense) of that word is Noun, wediscard the word while scoring.
Noun words don?tcarry sentiment and thus are of no use in our experi-ments.3.8 Popularity ScoreThis scoring method boosts the scores of the mostcommonly used words, which are domain specific.For example, happy is used predominantly for ex-pressing the positive sentiment.
In this method, wemultiple its popularity factor (pF) to the score ofeach unigram token which has been scored in theprevious steps.
We use the occurrence frequency ofa token in positive and negative dataset to decide onthe weight of popularity score.
Equation 2 showshow the popularity factor is calculated for each to-ken.
We selected a threshold 0.01 min support as thecut-off criteria and reduced it by half at every level.Support of a word is defined as the proportion oftweets in the dataset which contain this token.
Thevalue 0.01 is chosen such that we cover a large num-ber of tokens without missing important tokens, atthe same time pruning less frequent tokens.Pf = Frequency in Positive Training SetNf = Frequency in Negative Training Setif(Pf ?Nf ) > 1000)pF = 0.9;elseif((Pf ?Nf ) > 500)pF = 0.8;elseif((Pf ?Nf ) > 250)pF = 0.7;elseif((Pf ?Nf ) > 100)pF = 0.5;elseif((Pf ?Nf < 50))pF = 0.1;(2)Figure 1 shows the flow of our approach.Figure 1: Flow Chart of our Algorithm4 DatasetsIn this section, we explain the two datasets used inthis research.
Both of these datasets are built usingnoisy labels.4.1 Stanford DatasetThis dataset(Go et al, 2009) was built automat-ically using emoticons as noisy labels.
All thetweets which contain ?:)?
were marked positive andtweets containing ?:(?
were marked negative.
Tweetsthat did not have any of these labels or had bothwere discarded.
The training dataset has ?1.6 mil-lion tweets, equal number of positive and negativetweets.
The training dataset was annotated into twoclasses (positive and negative) while the testing datawas hand annotated into three classes (positive, neg-ative and neutral).
For our experimentation, we useonly positive and negative class tweets from the test-ing dataset for our experimentation.
Table 1 givesthe details of dataset.Training TweetsPositive 800,000Negative 800,000Total 1,600,000Testing TweetsPositive 180Negative 180Objective 138Total 498Table 1: Stanford Twitter Dataset144.2 MejajMejaj dataset(Bora, 2012) was built using noisy la-bels.
They collected a set of 40 words and manuallycategorized them into positive and negative.
Theylabel a tweet as positive if it contains any of the pos-itive sentiment words and as negative if it containsany of the negative sentiment words.
Tweets whichdo not contain any of these noisy labels and tweetswhich have both positive and negative words werediscarded.
Table 2 gives the list of words which wereused as noisy labels.
This dataset contains only twoclass data.
Table 3 gives the details of the dataset.Positive Labels Negative Labelsamazed, amused,attracted, cheerful,delighted, elated,excited, festive, funny,hilarious, joyful,lively, loving,overjoyed, passion,pleasant, pleased,pleasure, thrilled,wonderfulannoyed, ashamed,awful, defeated,depressed,disappointed,discouraged,displeased,embarrassed, furious,gloomy, greedy,guilty, hurt, lonely,mad, miserable,shocked, unhappy,upsetTable 2: Noisy Labels for annotating Mejaj DatasetTraining TweetsPositive 668,975Negative 795,661Total 1,464,638Testing TweetsPositive 198Negative 204Total 402Table 3: Mejaj Dataset5 ExperimentIn this section, we explain the experiments carriedout using the above proposed approach.5.1 Stanford DatasetOn this dataset(Go et al, 2009), we perform a seriesof experiments.
In the first series of experiments,we train on the given training data and test on thetesting data.
In the second series of experiments,we perform 5 fold cross validation using the trainingdata.
Table 4 shows the results of each of these ex-periments on steps which are explained in Approach(Section 3).In table 4, we give results for each step emoticonsand punctuations handling, spell correction, stem-ming and stop word removal mentioned in ApproachSection (Section 3).
The Baseline + All Combinedresults refers to combination of these steps (emoti-cons, punctuations, spell correction, Stemming andstop word removal) performed together.
Series 2 re-sults are average of accuracy of each fold.5.2 Mejaj DatasetSimilar series of experiments were performed onthis dataset(Bora, 2012) too.
In the first series ofexperiments, training and testing was done on therespective given datasets.
In the second series of ex-periments, we perform 5 fold cross validation on thetraining data.
Table 5 shows the results of each ofthese experiments.In table 5, we give results for each step emoticonsand punctuations handling, spell correction, stem-ming and stop word removal mentioned in ApproachSection (Section 3).
The Baseline + All Combinedresults refers to combination of these steps (emoti-cons, punctuations, spell correction, Stemming andstop word removal) performed together.
Series 2 re-sults are average of accuracy of each fold.5.3 Cross DatasetTo validate the robustness of our approach, we ex-perimented with cross dataset training and testing.We trained our system on one dataset and tested onthe other dataset.
Table 6 reports the results of crossdataset evaluations.6 Feature Vector ApproachIn this feature vector approach, we form features us-ing Unigrams, Bigrams, Hashtags (#), Targets (@),Emoticons, Special Symbol (?!?)
and used a semi-supervised SVM classifier.
Our feature vector com-prised of 11 features.
We divide the features intotwo groups, NLP features and Twitter specific fea-tures.
NLP features include frequency of positive15Method Series 1 (%) Series 2 (%)Baseline 78.8 80.1Baseline + Emoticons + Punctuations 81.3 82.1Baseline + Spell Correction 81.3 81.6Baseline + Stemming 81.9 81.7Baseline + Stop Word Removal 81.7 82.3Baseline + All Combined (AC) 83.5 85.4AC + Senti Features (wSF) 85.5 86.2wSF + Noun Identification (wNI) 85.8 87.1wNI + Popularity Score 87.2 88.4Table 4: Results on Stanford DatasetMethod Series 1 (%) Series 2 (%)Baseline 77.1 78.6Baseline + Emoticons + Punctuations 80.3 80.4Baseline + Spell Correction 80.1 80.0Baseline + Stemming 79.1 79.7Baseline + Stop Word Removal 80.2 81.7Baseline + All Combined (AC) 82.9 84.1AC + Senti Features (wSF) 86.8 87.3wSF + Noun Identification (wNI) 87.6 88.2wNI + Popularity Score 88.1 88.1Table 5: Results on Mejaj DatasetMethod Training Dataset Testing Dataset AccuracywNI + Popularity Score Stanford Mejaj 86.4%wNI + Popularity Score Mejaj Stanford 84.7%Table 6: Results on Cross Dataset evaluationNLP Unigram (f1) # of positive and negative unigramBigram (f2) # of positive and negative BigramTwitter SpecificHashtags (f3) # of positive and negative hashtagsEmoticons (f4) # of positive and negative emoticonsURLs (f5) Binary Feature - presence of URLsTargets (f6) Binary Feature - presence of TargetsSpecial Symbols (f7) Binary Feature - presence of ?!
?Table 7: Features and Description16Feature Set Accuracy (Stanford)f1 + f2 85.34%f3 + f4 + f7 53.77%f3 + f4 + f5 + f6 + f7 60.12%f1 + f2 + f3 + f4 + f7 85.89%f1 + f2 + f3 + f4 +f5 + f6 + f7 87.64%Table 8: Results of Feature Vector Classifier on StanfordDatasetunigrams matched, negative unigrams matched, pos-itive bigrams matched, negative bigrams matched,etc and Twitter specific features included Emoti-cons, Targets, HashTags, URLs, etc.
Table 7 showsthe features we have considered.HashTags polarity is decided based on the con-stituent words of the hashtags.
Using the list of pos-itive and negative words from Twitrratr6, we try tofind if hashtags contains any of these words.
If so,we assign the polarity of that to the hashtag.
Forexample, ?#imsohappy?
contains a positive word?happy?, thus this hashtag is considered as posi-tive hashtag.
We use the emoticons list providedby (Agarwal et al, 2011) in their research.
Thislist7 is built from wikipedia list of emoticons8 andis hand tagged into five classes (extremely positive,positive, neutral, negative and extremely negative).We reduce this five class list to two class by mergingextremely positive and positive class to single posi-tive class and rest other classes (extremely negative,negative and neutral) to single negative class.
Ta-ble 8 reports the accuracy of our machine learningclassifier on Stanford dataset.7 DiscussionIn this section, we present a few examples evaluatedusing our system.
The following example denotesthe effect of incorporating the contribution of emoti-cons on tweet classification.
Example ?Ahhh I can?tmove it but hey w/e its on hell I?m elated right now:-D?.
This tweet contains two opinion words, ?hell?and ?elated?.
Using the unigram scoring method,this tweet is classified neutral but it is actually posi-6http://twitrratr.com/7http://goo.gl/oCSnQ8http://en.wikipedia.org/wiki/List of emoticonstive.
If we incorporate the effect of emoticon ?
:-D?,then this tweet is tagged positive.
?:-D?
is a strongpositive emoticon.Consider this example, ?Bill Clinton Fail -Obama Win??.
In this example, there are two senti-ment bearing words, ?Fail?
and ?Win?.
Ideally thistweet should be neutral but this is tagged as a posi-tive tweet in the dataset as well as using our system.In this tweet, if we calculate the popularity factor(pF) for ?Win?
and ?Fail?, they come out to be 0.9and 0.8 respectively.
Because of the popularity fac-tor weight, the positive score domniates the negativescore and thus the tweet is tagged as positive.
It isimportant to identify the context flow in the text andalso how each of these words modify or depend onthe other words of the tweet.For calculating the system performance, we as-sume that the dataset which is used here is correct.Most of the times this assumption is true but thereare a few cases where it fails.
For example, thistweet ?My wrist still hurts.
I have to get it lookedat.
I HATE the dr/dentist/scary places.
:( Time towatch Eagle eye.
If you want to join, txt!?
is taggedas positive, but actually this should have been taggednegative.
Such erroneous tweets also effect the sys-tem performance.There are few limitations with the current pro-posed approach which are also open research prob-lems.1.
Spell Correction: In the above proposed ap-proach, we gave a solution to spell correctionwhich works only when extra characters are en-tered by the user.
It fails when users skip somecharacters like ?there?
is spelled as ?thr?.
Wepropose the use of phonetic level spell correc-tion to handle this problem.2.
Hashtag Segmentation: For handling hashtags,we looked for the existence of the positive ornegative words9 in the hashtag.
But there canbe some cases where it may not work correctly.For example, ?#thisisnotgood?, in this hashtagif we consider the presence of positive and neg-ative words, then this hashtag is tagged posi-tive (?good?).
We fail to capture the presenceand effect of ?not?
which is making this hash-9word list taken from http://twitrratr.com/17tag as negative.
We propose to devise and usesome logic to segment the hashtags to get cor-rect constituent words.3.
Context Dependency: As discussed in one ofthe examples above, even tweet text which islimited to 140 characters can have context de-pendency.
One possible method to address thisproblem is to identify the objects in the tweetand then find the opinion towards those objects.8 Conclusion and Future WorkTwitter sentiment analysis is a very important andchallenging task.
Twitter being a microblog suffersfrom various linguistic and grammatical errors.
Inthis research, we proposed a method which incorpo-rates the popularity effect of words on tweet senti-ment classification and also emphasis on how to pre-process the Twitter data for maximum informationextraction out of the small content.
On the Stanforddataset, we achieved 87% accuracy using the scor-ing method and 88% using SVM classifier.
On Me-jaj dataset, we showed an improvement of 4.77% ascompared to their (Bora, 2012) accuracy of 83.33%.In future, This work can be extended through in-corporation of better spell correction mechanisms(may be at phonetic level) and word sense disam-biguation.
Also we can identify the target and enti-ties in the tweet and the orientation of the user to-wards them.AcknowledgementWe would like to thank Vibhor Goel, Sourav Duttaand Sonil Yadav for helping us with running SVMclassifier on such a large data.ReferencesAgarwal, A., Xie, B., Vovsha, I., Rambow, O. and Pas-sonneau, R. (2011).
Sentiment analysis of Twitterdata.
In Proceedings of the Workshop on Languagesin Social Media LSM ?11.Bora, N. N. (2012).
Summarizing Public Opinions inTweets.
In Journal Proceedings of CICLing 2012,New Delhi, India.Chesley, P. (2006).
Using verbs and adjectives to auto-matically classify blog sentiment.
In In Proceedingsof AAAI-CAAW-06, the Spring Symposia on Compu-tational Approaches.Davidov, D., Tsur, O. and Rappoport, A.
(2010).
En-hanced sentiment learning using Twitter hashtags andsmileys.
In Proceedings of the 23rd International Con-ference on Computational Linguistics: Posters COL-ING ?10.Diakopoulos, N. and Shamma, D. (2010).
Characterizingdebate performance via aggregated twitter sentiment.In Proceedings of the 28th international conference onHuman factors in computing systems ACM.Draya, G., Planti, M., Harb, A., Poncelet, P., Roche,M.
and Trousset, F. (2009).
Opinion Mining fromBlogs.
In International Journal of Computer Informa-tion Systems and Industrial Management Applications(IJCISIM).Go, A., Bhayani, R. and Huang, L. (2009).
Twitter Sen-timent Classification using Distant Supervision.
InCS224N Project Report, Stanford University.Godbole, N., Srinivasaiah, M. and Skiena, S. (2007).Large-Scale Sentiment Analysis for News and Blogs.In Proceedings of the International Conference on We-blogs and Social Media (ICWSM).He, B., Macdonald, C., He, J. and Ounis, I.
(2008).
Aneffective statistical approach to blog post opinion re-trieval.
In Proceedings of the 17th ACM conference onInformation and knowledge management CIKM ?08.Hu, M. and Liu, B.
(2004).
Mining Opinion Features inCustomer Reviews.
In AAAI.Miller, G. A.
(1995).
WordNet: A Lexical Database forEnglish.
Communications of the ACM 38, 39?41.Pang, B., Lee, L. and Vaithyanathan, S. (2002).
Thumbsup?
Sentiment Classification using Machine LearningTechniques.Turney, P. D. (2002).
Thumbs Up or Thumbs Down?
Se-mantic Orientation Applied to Unsupervised Classifi-cation of Reviews.
In ACL.18
