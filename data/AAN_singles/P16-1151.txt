Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1600?1609,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDiscovery of Treatments from Text CorporaChristian FongStanford University655 Serra StreetStanford, CA 94305, USAchristianfong@stanford.eduJustin GrimmerStanford University616 Serra StreetStanford, CA 94305, USAjgrimmer@stanford.eduAbstractAn extensive literature in computationalsocial science examines how features ofmessages, advertisements, and other cor-pora affect individuals?
decisions, butthese analyses must specify the relevantfeatures of the text before the experiment.Automated text analysis methods are ableto discover features of text, but these meth-ods cannot be used to obtain the estimatesof causal effects?the quantity of inter-est for applied researchers.
We introducea new experimental design and statisticalmodel to simultaneously discover treat-ments in a corpora and estimate causal ef-fects for these discovered treatments.
Weprove the conditions to identify the treat-ment effects of texts and introduce the su-pervised Indian Buffet process to discoverthose treatments.
Our method enables usto discover treatments in a training set us-ing a collection of texts and individuals?responses to those texts, and then esti-mate the effects of these interventions ina test set of new texts and survey respon-dents.
We apply the model to an exper-iment about candidate biographies, recov-ering intuitive features of voters?
decisionsand revealing a penalty for lawyers and abonus for military service.1 IntroductionComputational social scientists are often inter-ested in inferring how blocks of text, such as mes-sages from political candidates or advertising con-tent, affect individuals?
decisions (Ansolabehereand Iyengar, 1995; Mutz, 2011; Tomz and Weeks,2013).
To do so, they typically attempt to estimatethe causal effect of the text: they model the out-come of interest, Y , as a function of the block oftext presented to the respondent, t, and define thetreatment effect of t relative to some other blockof text t?as Y (t) ?
Y (t?)
(Rubin, 1974; Hol-land, 1986).
For example, in industrial contextsresearchers design A/B tests to compare two po-tential texts for a use case.
Academic researchersoften design one text that has a feature of inter-est and another text that lacks that feature but isotherwise identical (for example, (Albertson andGadarian, 2015)).
Both kinds of experiments as-sume researchers already know the features of textto vary and offer little help to researchers whowould like to discover the features to vary.Topic models and related methods can discoverimportant features in corpora of text data, but theyare constructed in a way that makes it difficultto use the discovered features to estimate causaleffects (Blei et al, 2003).
Consider, for exam-ple, supervised latent Dirichlet alocation (sLDA)(Mcauliffe and Blei, 2007).
It associates a topic-prevalence vector, ?, with each document wherethe estimated topics depend upon both the con-tent of documents and a label associated with eachdocument.
If K topics are included in the model,then ?
is defined on the K ?
1-dimensional unitsimplex.
It is straightforward to define a treatmenteffect as the difference between two treatments ?and ??
(or points on the simplex) Y (?)
?
Y (??
).It is less clear how to define the marginal effectof any one dimension.
This is because biggervalues on some dimensions implies smaller val-ues on other dimensions, making the effect of anyone topic necessarily a combination of the differ-ences obtained when averaging across all the di-mensions (Aitchison, 1986; Katz and King, 1999).This problem will befall all topic models becausethe zero-sum nature of the topic-prevalence vectorimplies that increasing the prevalence of any onetopic necessarily decreases the prevalence of some1600other topic.
The result is that it is difficult (or im-possible) to interpret the effect of any one topicmarginalizing over the other topics.
Other appli-cations of topic models to estimate causal effectstreat text as the response, rather than the treatment(Roberts et al, 2016).
And still other methods re-quire a difficult to interpret assumption of how textmight affect individuals?
responses (Beauchamp,2011).To facilitate the discovery of treatments andto address the limitation of existing unsupervisedlearning methods, we introduce a new experimen-tal design, framework, and statistical model fordiscovering treatments within blocks of text andthen reliably inferring the effects of those treat-ments.
By doing so, we combine the utility ofdiscovering important features in a topic modelwith the scientific value of causal treatment ef-fects estimated in a potential outcomes frame-work.
We present a new statistical model?the su-pervised Indian Buffet Process?to both discovertreatments in a training set and infer the effectstreatments in a test set (Ghahramani and Griffiths,2005).
We prove that randomly assigning blocksof text to respondents in an experiment is suffi-cient to identify the effects of latent treatments thatcomprise blocks of text.Our framework provides the first of its kindapproach to automatically discover treatment ef-fects in text, building on literatures in both socialscience and machine learning (Blei et al, 2003;Beauchamp, 2011; Mcauliffe and Blei, 2007;Roberts et al, 2016).
The use of the trainingand test set ensures that this discovery does notcome at the expense of credibly inferring causaleffects, insulating the research design from con-cerns about ?p-hacking?
and overfitting (Ioanni-dis, 2005; Humphreys et al, 2013; Franco et al,2014).
Critically, we use a theoretical justificationfor our methodology: we select our particular ap-proach because it enables us to estimate causal ef-fect of interest.
Rather than demonstrating that ourmethod performs better at some predictive task,we prove that our method is able to estimate usefulcausal effects from the data.We apply our framework to study how featuresof a political candidate?s background affect voters?decisions.
We use a collection of candidate biogra-phies collected from Wikipedia to automaticallydiscover treatments in the biographies and then in-fer their effects.
This reveals a penalty for lawyersand career politicians and a bonus for military ser-vice and advanced degrees.
While we describeour procedure throughout the paper, we summa-rize our experimental protocol and strategy for dis-covering treatment effects in Table 1.Table 1: Experimental Protocol for Discoveringand Estimating Treatment Effects1) Randomly assign texts, Xj, to respon-dents2) Obtain response Yifor each respondent.3) Divide texts and responses into trainingand test set4) In training set:a) Use supervised Indian Buffet Pro-cess (sIBP) applied to documentsand responses to infer latent treat-ments in textsb) Model selection via quantitative fitand qualitative assessment5) In test set:a) Use sIBP trained on training set toinfer latent treatments on test setdocumentsb) Estimate effect of treatments withregression, with a bootstrap proce-dure to estimate uncertainty2 A Framework for DiscoveringTreatments from TextOur goal is to discover a set of features?treatments?underlying texts and then estimatethe effect of those treatments on some responsefrom an individual.
We first show that randomlyassigning texts to respondents is sufficient to iden-tify treatment effects.
We then provide a statisticalmodel for using both the text and responses to dis-cover latent features in the text that affect the re-sponse.
Finally, we show that we can use the map-ping from text to features discovered on a trainingset to estimate the presence of features in a testset, which allows us to estimate treatment effectsin the test set.16012.1 Randomizing Texts Identifies UnderlyingTreatment EffectsWhen estimating treatment effects, researchers of-ten worry that the respondents who received onetreatment systematically differ from those who re-ceived some other treatment.
In a study of adver-tising, if all of the people who saw one advertise-ment were men and all of the people who saw adifferent advertisement were women, it would beimpossible to tell whether differences in their re-sponses were driven by the fact that they saw dif-ferent advertisements or by their pre-existing dif-ferences.
Randomized experiments are the goldstandard for overcoming this problem (Gerber andGreen, 2012).
However, in text experiments, in-dividuals are randomly assigned to blocks of textrather than to the latent features of the text that weanalyze as the treatments.
In this section, we showthat randomly assigning blocks of text is sufficientto identify treatment effects.To establish our result, we suppose we have acorpora of J texts, X .
We represent a specifictext with Xj?
X , with Xj?
<D.
Through-out we will assume that we have standardized thevariableXjto be a per-document word usage ratewith each column normalized to have mean zeroand variance one.
We have a sample of N respon-dents from a population, with the response of in-dividual i to text j[i] given by the potential out-come Yi(Xj[i]).
We use the notation j[i] becausemultiple individuals may be assigned to the sametext; if i and i?are assigned to the same text, thenj[i] = j[i?].
We suppose that for each document jthere is a corresponding vector of K binary treat-ments Zj?
Z where Z contains all 2Kpossiblecombinations of treatments, {0, 1}K. The func-tion g : X ?
Z maps from the texts to the setof binary treatments: we will learn this functionusing the supervised Indian Buffet process intro-duced in the next section.
Note that distinct ele-ments of X may map to the same element of Z.To establish our identification result, we assume(Assumption 1) Yi(X) = Yi(Xj[i]) for all i. Thisassumption ensures that each respondent?s treat-ment assignment depends only on her assignedtext, a version of the Stable Unit Treatment ValueAssumption (SUTVA) for our application (Ru-bin, 1986).
We also assume (Assumption 2) thatYi(Xj[i]) = Yi(g(Xj[i])) for allXj[i]?
X and alli, or that Zj[i]is sufficent to describe the effect ofa document on individual i?s response.
Stated dif-ferently, we assume an individual would respondin the same way to two different texts if thosetexts have the same latent features.
We furthersuppose (Assumption 3) that texts are randomlyassigned to respondents according to probabilitymeasure h, ensuring that Yi(g(Xj[i])) ?
Xj[i]for allXj[i]?
X and for all individuals i.
This as-sumption ensures unobserved characteristics of in-dividuals are not confounding inferences about theeffects of texts.
The random assignment of textsto individuals induces a distribution over a prob-ability measure on treatment vectors Z, f(Z) =?X1(Z = g(X))h(X)dX .
Finally, we assume(Assumption 4) that f(Z) > 0 for all Z ?
Z .1This requires that every combination of treatmenteffects is possible from the documents in our cor-pus.
In practice, when designing our study wewant to ensure that the treatments are not aliasedor perfectly correlated.
If perfect correlation existsbetween factors, we are unable to disentangle theeffect of individual factors.In this paper we focus on estimating theAverage Marginal Component Specific Effectfor factor k (AMCEk) (Hainmueller et al,2014).2The AMCEkis useful for findingthe effect of one feature, k, when k interactswith the other features in some potentiallycomplicated way.
It is defined as the differ-ence in outcomes when the feature is presentand when it is not present, averaged over the val-ues of all of the other features.
Formally, AMCEk=?Z?kE [Y (Zk= 1,Z?k)?
Y (Zk= 0,Z?k)]m(Z?k)dZ?kwhere m(Z?k) is some analyst-defined densityon all elements but k of the treatment vector.For example, m(?)
can be chosen as the densityof Z?kin the population to obtain the marginalcomponent effect of k in the empirical population.The most commonly used m(?)
in applied workis uniform across all Z?k?s, and we follow thisconvention here.We now prove that assumptions 1, 2, 3, and 4are sufficient to identify the AMCEkfor all k.Proposition 1.
Assumptions 1, 2, 3, and 4 are suf-1Note for this assumption to hold it is necessary, but notsufficient that g is a surjection from X onto Z .2The procedure here can be understood as a method fordiscovering the treatments that are imposed by assumption inconjoint analysis, as presented by (Hainmueller et al, 2014).We deploy the regression estimator used in conjoint analy-sis as a subroutine of our procedure (see Step 5b in Table1), but otherwise our experimental design, statistical method,and proof is distinct.1602ficient to identify the AMCEkfor arbitrary k.Proof.
To obtain a useful form, we firstmarginalize over the documents to obtain,?Z?k?XE [Y (Zk= 1,Z?k)] f(Z?k|Zk=1,X) ?
E [Y (Zk= 0,Z?k)] f(Z?k|Zk=0,X)h(X)dXdZ?k=?Z?kE [Y (Zk= 1,Z?k)] f(Z?k|Zk= 1)?E [Y (Zk= 0,Z?k)] f(Z?k|Zk= 0)dZ?k.Where f(Z?k|Zk= 1) and f(Z?k|Zk= 0)are the induced distributions over latent featuresfrom averaging over documents.
If f(Z?k|Zk=0) = f(Z?k|Zk= 1) = m(Z?k) then this is theAMCEk.
Otherwise consider m(Z) > 0 for allZ ?
Z .
Because f(Z) > 0, f(Z?k|Zk= 0) > 0and f(Z?k|Zk= 1) > 0.
Thus, there exists con-ditional densities h(Z|Zk= 1) and h(Z|Zk=0) such thatf(Z?k|Zk=1)h(Z?k|Zk=1)=f(Z?k|Zk=0)h(Z?k|Zk=0)=m(Z?k)2.2 A Statistical Model for IdentifyingFeaturesThe preceding section shows that if we are ableto discover features in the data, we can estimatetheir AMCEs by randomly assigning texts to re-spondents.
We now present a statistical model fordiscovering those features.
As we argued in the in-troduction, it is difficult to use the topics obtainedfrom topic models like sLDA because the topicvector exists on the simplex.
When we comparethe outcomes associated with two different topicvectors, we do not know whether the change inthe response is caused by increasing the degree towhich the document about one topic or decreas-ing the degree to which it is about another, be-cause the former mathematically entails the latter.Other models, such as LASSO regression, wouldnecessarily suppose that the presence and absenceof words are the treatments (Hastie et al, 2001;Beauchamp, 2011).
This is problematic substan-tively, because it is hard to know exactly what thepresence or absence of a single word implies as atreatment in text.We therefore develop the supervised IndianBuffet Process (sIBP) to discover features in thedocument.
For our purposes, the sIBP has two es-sential properties.
First, it produces a binary topicvector, avoiding the complications of treatmentsassigned on the simplex.
Second, unlike the IndianBuffet Process upon which it builds (Ghahramaniand Griffiths, 2005), it incorporates informationabout the outcome associated with various texts,and therefore discovers features that explain boththe text and the response.3Figure 1 describes the posterior distribution forthe sIBP and a summary of the posterior is given inEquation 1.
We describe the model in three steps:the treatment assignment process, document cre-ation, and response.
The result is a model thatcreates a link between document content and re-sponse through a vector of treatment assignments.Treatment Assignment We assume that pi is aK-vector (where we take the limit as K ?
?
)where pikdescribes the population proportion ofdocuments that contain latent feature k. We sup-pose that pi is generated by the stick-breaking con-struction (Doshi-Velez et al, 2009).
Specifically,we suppose that ?k?
Beta(?, 1) for all K. Welabel pi1= ?1and for each remaining topic, weassume that pik=?kz=1?z.
For document j andtopic k, we suppose that zj,k?
Bernoulli(pik),which importantly implies that the occurrence oftreatments are not zero sum.
We collect the treat-ment vector for document j into Zjand collectall the treatment vectors into Z an Ntexts?
Kbinary matrix, where Ntextsrefers to number ofunique documents.
Throughout we will assumethat Ntexts= N or that the number of documentsand responses are equal and index the documentswith i.Document Creation We suppose that the doc-uments are created as a combination of latentfactors.
For topic k we suppose that Akis aD?dimensional vector that maps latent featuresonto observed text.
We collect the vectors intoA, a K ?
D matrix, and suppose that Xi?MVN(ZiA, ?2nID), where Xi,dis the standard-ized number of times word d appears in docu-ment i.
While it is common to model texts asdraws from multinomial distributions, the multi-3We note that there is a different model also called thesupervised Indian Buffet Process (Quadrianto et al, 2013).There are fundamental differences between the model pre-sented here and the sIBP in (Quadrianto et al, 2013).
Theiroutcome is a preference relation tuple, while ours is a real-valued scalar.
Because of this difference, the two models arefundamentally different.
This leads to a distinct data gener-ating process, model inference procedures, and inferences offeatures on the test set.
To leverage the analogy between LDAand sLDA vis a vis IBP and sIBP, we overload the term sIBPin our paper.
We expect that in future applications of sIBP, itwill be clear from the context which sIBP is being employed.1603ZXApiY?
?Figure 1: Graphical Model for the Supervised In-dian Buffet Processvariate normal distribution is useful for our pur-poses for two reasons.
First, we normalize our databy transforming each column X?,dto be mean 0and variance 1, ensuring that the multivariate nor-mal distribution captures the overall contours ofthe data.
Note that this implies that Xi,dcan benegative.
Second, we show that assuming a multi-variate normal for document generation results inparameters that capture the distinctive rate wordsare used for each latent feature (Doshi-Velez et al,2009).Response to Treatment Vector We assume thata K?vector of parameters ?
describes the re-lationship between the treatment vector and re-sponse.
Specifically, we use a standard parameter-ization and suppose that ?
?
Gamma(a, b), ?
?MVN(0, ?
?1) and that Yi?
Normal(Zi?, ??1).pik?
Stick-Breaking (?)zi,k?
Bernoulli(pik)Xi|Zi,A ?
MVN(ZiA, ?2XID)Ak?
MVN(0, ?2AID)Yi|Zi,?
?
Normal(Zi?, ??1)?
?
Gamma(a, b)?|?
?
MVN(0, ?
?1IK) (1)2.2.1 Inference for the Supervised IndianBuffet ProcessWe approximate the posterior distributionwith a variational approximation, buildingon the algorithm introduced in (Doshi-Velezet al, 2009).
We approximate the non-parametric posterior setting K to be largeand use a factorized approximation, assumingthat p(pi,Z,A,?, ?
|X,Y , ?, ?2A, ?2X, a, b) =q(pi)q(A)q(Z)q(?, ?
)A standard derivation that builds on (Doshi-Velez et al, 2009) leads to the following distribu-tional forms and update steps:?
q(piK) = Beta(pik|?k).
The update valuesare ?k,1=?K+?Ni=1?i,kand ?k,2= 1 +?Ni=1(1?
?i,k).?
q(Ak) = Multivariate Normal(Ak|?
?k,?k).The updated parameter values are,??k=[1?2X?Ni=1?i,k(Xi?
(?l:l 6=k?i,l??l))]?k?k=(1?2A+?Ni=1?i,k?2X)?1I?
q(?, ? )
= Multivariate Normal(?|m,S) ?Gamma(?
|c, d).
The updated parameter val-ues are,m = SE[ZT]YS = (E[ZTZ] + IK)?1c = a+N2d = b+YTY?
YTE[Z]SE[ZT]Y2Where typical element of E[ZT]j,k=?j,kand typical on-diagonal element ofE[ZTZ]k,k=?Ni=1?i,kand off-diagonal el-ement is E[ZTZ]j,k=?Ni=1?i,j?i,k.?
q(zi,k) = Bernoulli(zi,k|?i,k).
The updatedparameter values arevi,k= ?(?k,1)?
?(?k,2)?12?2X[?2?
?kXTi+(tr(?k) +??k?
?Tk) + 2?
?k(?l:l 6=k?i,l?
?Tl)]?c2d(?2mkYi+(dSk,kc?1+mTkmk)+2mk(?l:l 6=k?i,lml))?i,k=11+exp(?vi,k)where ?(?)
is the digamma function.
We re-peat the algorithm until the change in the pa-rameter vector drops below a threshold.To select the final model using the training setdata, we perform a two-dimensional line searchover values of ?
and ?X.4We then run the model4We assign ?A, a, and b values which lead to diffuse pri-ors.1604several times for each combination of values for ?and ?Xto evaluate the output at several differentlocal modes.
To create a candidate set of models,we use a quantitative measure that balances coher-ence and exclusivity (Mimno et al, 2011; Robertset al, 2014).
Let Ikbe the set of documents forwhich ?i,k?
0.5, and let ICkbe the complementof this set.
We identify the top ten words for inter-vention k as the ten words with the largest valuein Ak, tkand define Nk=?Ni=1I{?i,k?
0.5}.We then obtain measure CE for a particular modelCE =?Kk=1Nk?l,c?tkcov(XIk,l,XIk,c) ?
?Kk=1(N ?Nk)?l,c?tkcov(XICk,l,XICk,c)where here XIk,lrefers to the lthcolumn and Ikthrows ofX .
We make a final model selection basedon the model that provides the most substantivelyclear treatments.2.3 Inferring Treatments and EstimatingEffects in Test SetTo discover the treatment effects, we first supposethat we have randomly assigned a set of respon-dents a text based treatmentXiaccording to someprobability measure h(?)
and that we have ob-served their response Yi.
We collect the assignedtexts into X and the responses into Y .
As we de-scribe below, we will often assign each respondenttheir own distinctive message, with the probabil-ity of receiving any one message at1Nfor all re-spondents and messages.
We use the sIBP modeltrained our training set documents and responsesto infer the effect of those treatments among thetest set documents.
Separating the documents andresponses into training and test sets ensures thatAssumption 1, SUTVA, holds.
We learn the map-ping from texts to binary vectors in the trainingset, g?(?)
and then apply this mapping to the testset to infer the latent treatments present in the testset documents, without considering the test set re-sponses.
Dividing texts and responses into trainingand test sets provides a solution to SUTVA viola-tions present in other attempts at causal inferencein text analysis (Roberts et al, 2014).We approximate the posterior distributionfor the treatment vectors using the variationalapproximation from the training set parameters(??,???,?
?,?m,?S, c?,?d,??2X,?
?2A) and a modifiedupdate step on q(ztesti,k).
In this modified updatestep, we remove the component of the updatethat incorporates information about the outcome.Specifically for individual i in the test set forcategory k we have the following update stepvtesti,k= ?(??k,1)?
?(??k,2)?12(??2X)?[?2??
?k(XTi) + (tr(?
?k) +???k(??
?k)T) +2??
?k(?l:l 6=k?i,l(??
?lT))]?testi,k=11+exp(?vtesti,k).For each text in the test set we repeat this updateseveral times until ?testhas converged.
Note thatfor the test set we have excluded the componentof the model that links the latent features to theresponse, ensuring that SUTVA holds.With the approximating distribution q(Ztest) wethen measure the effect of the treatments in the testset.
Using the treatments, the most straightforwardmodel to estimate assumes that there are no inter-actions between each of the components.
Underthe no interactions assumption, we estimate the ef-fects of the treatments and infer confidence inter-vals using the following bootstrapping procedurethat incorporates uncertainty both from estimationof treatments and uncertainty about the effects ofthose treatments:1) For each respondent i and component k wedraw z?i,k?
Bernoulli(?testi,k), resulting in ma-trix?Z.2) Given the matrix?Z, we sample (Ytest,?Z)with replacement and for each sample esti-mate the regression Ytest= ?test?Z + .We repeat the bootstrap steps 1000 times, keeping?testfor each iteration.
The result of the proce-dure is a point estimate of the effects and confi-dence interval of the treatments under no interac-tions.
Technically, it is possible to estimate thetreatment effects in our variational approximation.But we estimate the effects in a second-stage re-gression because variational approximations tendto understate uncertainty, the bootstrap provides astraightforward method for including uncertaintyfrom estimation of the latent features and the ef-fect estimates, and it ensures that SUTVA is notviolated.3 Application: Voter Evaluations of anIdeal CandidateWe demonstrate our method in an experiment toassess how features of a candidate?s backgroundaffect respondents evaluations of the candidates.There is a rich literature in political science about1605the ideal attributes of political candidates (Canon,1990; Popkin, 1994; Carnes, 2012; Campbell andCowley, 2014).
We build on this literature and usea collection of candidate biographies to discoverfeatures of candidates?
backgrounds that votersfind appealing.
To uncover the features of can-didate biographies that voters are responsive towe acquired a collection of 1,246 Congressionalcandidate biographies from Wikipedia.
We thenanonymize the biographies?replacing names andremoving other identifiable information?to en-sure that the only information available to the re-spondent was explicitly present in the text.In Section 2.1 we show that a necessary con-dition for this experiment to uncover latent treat-ments is that each vector of treatments has non-zero probability of occuring.
This is equivalent toassuming that none of the treatments are aliased,or perfectly correlated (Hainmueller et al, 2014).Aliasing would be more likely if there are onlya few distinct texts that are provided to partici-pants in our experiment.
Therefore, we assigneach respondent in each evaluation round a dis-tinct candidate biography.
To bolster our statis-tical power, we ask our respondents to evaluateup to four distinct candidate biographies, resultingin each respondent evaluating 2.8 biographies onaverage.5After presenting the respondents witha candidate?s biography, we ask each respondentto rate the candidate using a feeling thermometer:a well-established social science scale that goesfrom 0 when a respodent is ?cold?
to a candidateto 100 when a respondent is ?warm?
to the candi-date.We recruited a sample of 1,886 participants us-ing Survey Sampling International (SSI), an onlinesurvey platform.
Our sample is census matchedto reflect US demographics on sex, age, race, andeducation.
Using the sample we obtain 5,303 to-tal observations.
We assign 2,651 responses to thetraining set and 2,652 to the test set.
We then applythe sIBP process to the training data.
To apply themodel, we standardize the feeling thermometer tohave mean zero and standard deviation 1.
We setK to a relatively low value (K = 10) reflectinga quantitative and qualitative search over K. Wethen select the final model varying the parameters5The multiple evaluations of candidate biographies isproblematic if there is spillover across rounds of our exper-iment.
We have little reason to believe observing one can-didate biography would systematically affect the response insubsequent rounds.and evaluating the CE score.Table 2 provides the top words for each of theten treatments the sIBP discovered in the trainingset.
We selected ten treatments using a combina-tion of guidance from the sIBP, assessment usingCE scores, and our own qualitative assessment ofthe models (Grimmer and Stewart, 2013).
Whileit is true that our final selection depends on humaninput, some reliance on human judgment at thisstage is appropriate.
If one set includes a treat-ment about military service but not legal trainingand another set includes a treatment about legaltraining but not military service, then model selec-tion is tantamount to deciding which hypothesesare most worthy of investigation.
Our CE scoresidentify sets of treatments that are most likely to beinteresting, but the human analyst should make thefinal decision about which hypotheses he wouldlike to test.
However, it is extremely importantfor the analyst to select a set of treatments firstand only afterwards estimate the effects of thosetreatments.
If the analyst observes the effects ofsome treatments and then decides he would like totest other sets, then the integrity of any p-valueshe might calculate are undermined by the multi-ple testing problem.
A key feature of our proce-dure is that it draws a clear line between the selec-tion of hypotheses to test (which leverages humanjudgment) and the estimation of effects (which ispurely mechanical).The estimated treatments cover salient featuresof Congressional biographies from the time periodthat we analyze.
For example, treatments 6 and 10capture a candidate?s military experience.
Treat-ment 5 and 7 are about previous political experi-ence and Treatment 3 and 9 refer to a candidate?seducation experience.
Clearly, there are many fea-tures of a candidate?s background missing, but thetreatments discovered provide a useful set of di-mensions to assess how voters respond to a candi-date?s background.
Further, the discovered treat-ments are a combination of those that are bothprevalent in the biographies and have an effect onthe thermometer rating.
The absence of biograph-ical features that we might think matters for can-didate evaluation could be because there are fewof those biographies in our data set, or because therespondents were unresponive to those features.After training the model on the training set, weapply it to the test set to infer the treatments in thebiographies.
We assume there are no interactions1606Treatment 1 Treatment 2 Treatment 3 Treatment 4 Treatment 5appointed fraternity director received electedschool graduated distinguished university washington university housegovernor war ii received years democraticworked chapter president death seatolder air force master arts company republicanlaw firm phi phd training servedelected reserve policy military committeegrandfather delta public including appointedoffice air master george washington defeatedlegal states air affairs earned bachelors officeTreatment 6 Treatment 7 Treatment 8 Treatment 9 Treatment 10united states republican star law warmilitary democratic bronze school law enlistedcombat elected germany law school united statesrank appointed master arts juris doctor assignedmarine corps member awarded student armymedal incumbent played earned juris airdistinguished political yale earned law states armyair force father football law firm yearstates air served maternal university school serviceair state division body president officerTable 2: Top Words for 10 Treatments sIBP Discovered?2.50.02.55.01 2 3 4 5 6 7 8 9 10FeatureEffect onFeeling ThermometerFigure 2: 95% Confidence Intervals for Effectsof Discovered Treatments: The mean value of thefeeling thermometer is 62.3between the discovered treatments in order to es-timate their effects.6Figure 2 shows the point es-timate and 95-percent confidence intervals, whichtake into account uncertainty in inferring the treat-ments from the texts and the relationship betweenthose treatments and the response.The treatment effects reveal intuitive, though in-teresting, features of candidate biographies that af-fect respondent?s evaluations.
For example, Fig-ure 2 reveals a distaste for political and legalexperience?even though a large share of Con-gressional candidates have previous political ex-6This assumption is not necessary for the framework wepropose here.
Interaction effects could be modeled, but itwould require us to make much stronger parametric assump-tions using a method for heterogeneous treatments such as(Imai and Ratkovic, 2013).perience and a law degree.
Treatment 5, which de-scribes a candidate?s previous political experience,causes an 2.26 point reduction in feeling ther-mometer evaluation (95 percent confidence inter-val, [-4.26,-0.24]).
Likewise, Treatment 9 showsthat respondents dislike lawyers, with the presenceof legal experience causing a 2.34 point reductionin feeling thermometer (95-percent confidence in-terval, [-4.28,-0.29]).
The aversion to lawyers isnot, however, an aversion to education.
Treat-ment 3, a treatment that describes advanced de-grees, causes a 2.43 point increase in feeling ther-mometer evaluations (95-percent confidence inter-val, [0.49,4.38]).In contrast, Figure 2 shows that there is a con-sistent bonus for military experience.
This isconsistent with intuition from political observersthat the public supports veterans.
For exam-ple, treatment 6, which describes a candidate?smilitary record, causes a 3.21 point increase infeeling thermometer rating (95-percent confidenceinterval, [1.34,5.12]) and treatment 10 causes a4.00 point increase (95-percent confidence inter-val, [1.53,6.45]).Because simultaneously discovering treatmentsfrom labeled data and estimating their averagemarginal component effects is a novel task, wecannot compare the performance of our frame-work against any benchmark.
Even so, one natu-ral question is whether the user could obtain muchmore coherent topics by foresaking the estimationof causal effects and using a more traditional topic1607modeling method.
We provide the topics discov-ered by sLDA in Table 3. sIBP discovered mostof the same features sLDA did.
Both find militaryservice, legal training, political background, andhigher education.
The Greek life feature is less co-herent in sIBP than it is in sLDA, and sLDA findsbusiness and ancestry features that sIBP does not.Both have a few incoherent treatments.
This com-parison suggests that sIBP does almost as well assLDA at identifying coherent latent features, whilealso facilitating the estimation of marginal treat-ment effects.4 ConclusionWe have presented a methodology for discover-ing treatments in text and then inferring the ef-fect of those treatments on respondents?
decisions.We prove that randomizing texts is sufficient toidentify the underlying treatments and introducethe supervised Indian Buffet process for discover-ing the effects.
The use of a training and test setensures that our method provides accurate confi-dence intervals and avoids the problems of over-fitting or ?p-hacking?
in experiments.
In an ap-plication to candidate biographies, we discover apenalty for political and legal experience and abonus for military service and non-legal advanceddegrees.Our methodology has a wide variety of appli-cations.
This includes numerous alternative ex-perimental designs, providing a methodology thatcomputational social scientists could use widelyto discover and then confirm the effects of mes-sages in numerous domains?including imagesand other high dimensional data.
The method-ology is also useful for observational data?forstudying the effects of complicated treatments,such as how a legislator?s roll call voting recordaffects their electoral support.AcknowledgmentsThis material is based upon work supported by theNational Science Foundation Graduate ResearchFellowship under Grant No.
DGE-114747.
Anyopinions, findings, and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofthe National Science FoundationReferencesJohn Aitchison.
1986.
The Statistical Analysis of Com-positional Data.
Chapman and Hall.Bethany Albertson and Shana Kushner Gadarian.2015.
Anxious Politics: Democratic Citizenship ina Threatening World.
Cambridge University Press.Stephen Ansolabehere and Shanto Iyengar.
1995.
Go-ing Negative: How Political Advertisements Shrinkand Polarize The Electorate.
Simon & Schuster, Inc.Nick Beauchamp.
2011.
A bottom-up approach tolinguistic persuasion in advertising.
The PoliticalMethodologist.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
Journal of MachineLearning and Research, 3:993?1022.Rosie Campbell and Philip Cowley.
2014.
What voterswant: Reactions to candidate characteristics in a sur-vey experiment.
Political Studies, 62(4):745?765.David T. Canon.
1990.
Actors, Athletes, and As-tronauts: Political Amateurs in the United StatesCongress.
University of Chicago Press.Nicholas Carnes.
2012.
Does the numerical underrep-resentation of the working class in congress matter?Legislative Studies Quarterly, 37(1):5?34.Finale Doshi-Velez, Kurt T. Miller, Jurgen Van Gael,and Yee Whye Teh.
2009.
Variational inference forthe indian buffet process.
Technical Report, Univer-sity of Cambridge.Annie Franco, Neil Malhotra, and Gabor Simonovits.2014.
Publication bias in the social sciences: Un-locking the file drawer.
Science, 345(6203):1502?1505.Alan S. Gerber and Donald P. Green.
2012.
FieldExperiment: Design, Analysis, and Interpretation.W.W.
Norton & Company.Zoubin Ghahramani and Thomas L Griffiths.
2005.
In-finite latent feature models and the indian buffet pro-cess.
In Advances in neural information processingsystems, pages 475?482.Justin Grimmer and Brandon M. Stewart.
2013.
Textas data: The promise and pitfalls of automatic con-tent analysis methods for political texts.
PoliticalAnalysis, 21(3):267?297.Jens Hainmueller, Daniel Hopkins, and Teppei Ya-mamoto.
2014.
Causal inference in conjoint anal-ysis: Understanding multi-dimensional choices viastated preference experiments.
Political Analysis,22(1):1?30.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2001.
The Elements of Statistical Learning.Springer.1608Treatment 1 Treatment 2 Treatment 3 Treatment 4 Treatment 5years father school family united stateswork n?ee medical father states armynational mother college white served unitedworked business public schools parents warboard irish attended year war iiyoung son county mother armylocal long city brother serveddirector family born married servicesocial ancestry schools years lieutenantcommunity descent studied played militaryTreatment 6 Treatment 7 Treatment 8 Treatment 9 Treatment 10fraternity elected company board law schoolmember republican officer graduated lawstudent served united staets college school lawphi army mexico bachelor arts attorneydelta democratic air force harvard juris doctorkappa member years state university barhall house representatives military professor law firmchi state national guard masters degree courtgraduated senate insurance high school law degreeson election business economics judgeTable 3: Top Words for 10 Treatments sLDA DiscoveredPaul Holland.
1986.
Statistics and causal infer-ence.
Journal of the American Statistical Associa-tion, 81(396):945?960.Macartan Humphreys, Raul Sanchez de la Sierra, andPeter van der Windt.
2013.
Fishing, commitment,and communication: A proposal for comprehensivenonbinding research registration.
Political Analysis,21(1):1?20.Kosuke Imai and Marc Ratkovic.
2013.
Estimatingtreatment effect heterogeneity in randomized pro-gram evaluation.
The Annals of Applied Statistics,7(1):443?470.John P. A. Ioannidis.
2005.
Why most published re-search findings are false.
PLoS Medicine, 2(8):696?701.Jonathan Katz and Gary King.
1999.
A statisticalmodel for multiparty electoral data.
The AmericanPolitical Science Review, 93(1):15?32.Jon D. Mcauliffe and David M. Blei.
2007.
Supervisedtopic models.
In Advances in Neural InformationProcessing Systems 20 (NIPS 2007).David Mimno, Hanna M Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 262?272.Diana C Mutz.
2011.
Population-Based Survey Exper-iments.
Princeton University Press.Samuel L Popkin.
1994.
The Reasoning Voter: Com-munication and Persuasion in Presidential Cam-paigns.
University of Chicago Press.Novi Quadrianto, Viktoriia Sharmanska, David A.Knowles, and Zoubin Ghahramani.
2013.
The su-pervised ibp: Neighbourhood preserving infinite la-tent feature models.
page 101.Margaret E Roberts, Brandon M Stewart, Dustin Tin-gley, Chris Lucas, Jetson Leder-Luis, Bethany Al-bertson, Shana Gadarian, and David Rand.
2014.Topic models for open ended survey responses withapplications to experiments.
American Journal ofPolitical Science.Margaret E. Roberts, Brandon M. Stewart, and Edo M.Airoldi.
2016.
A model of text for experimentationin the social sciences.
Journal of the American Sta-tistical Association.
Forthcoming.Don Rubin.
1974.
Estimating causal effects of treat-ments in randomized and nonrandomized studies.Journal of Educational Psychology, 66:688?701.Donald B. Rubin.
1986.
Statistics and causal in-ference: Comment: Which ifs have causal an-swers.
Journal of the American Statistical Associ-ation, 81(396):961?962.Michael Tomz and Jessica Weeks.
2013.
Public opin-ion and the democratic peace.
American PoliticalScience Review, 107(4):849?865.1609
