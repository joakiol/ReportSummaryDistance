Proceedings of the 12th Conference of the European Chapter of the ACL, pages 861?869,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCo-dispersion: A Windowless Approach to Lexical AssociationJustin WashtellUniversity of LeedsLeeds, UKwashtell@comp.leeds.ac.ukAbstractWe introduce an alternative approach to ex-tracting word pair associations from corpora,based purely on surface distances in the text.We contrast it with the prevailing window-based co-occurrence model and show it to bemore statistically robust and to disclose abroader selection of significant associative re-lationships - owing largely to the property ofscale-independence.
In the process we provideinsights into the limiting characteristics ofwindow-based methods which complement thesometimes conflicting application-oriented lit-erature in this area.1 IntroductionThe principle of using statistical measures of co-occurrence from corpora as a proxy for wordassociation - by comparing observed frequenciesof co-occurrence with expected frequencies - isrelatively young.
One of the most well knowncomputational studies is that of Church & Hanks(1989).
The method by which co-occurrences arecounted, now as then, is based on a device whichdates back at least to Weaver (1949): the contextwindow.
While variations on the specific notionof context have been explored (separation ofcontent and function words, asymmetrical andnon-contiguous contexts, the sentence or thedocument as context) and increasingly sophisti-cated association measures have been proposed(see Evert, 2007, for a thorough review) the basicprinciple ?
that of counting token frequencieswithin a context region ?
remains ubiquitous.Herein we discuss some of the intrinsic limi-tations of this approach, as are being felt in re-cent research, and present a principled solutionwhich does not rely on co-occurrence windowsat all, but instead on measurements of the surfacedistance between words.2 The impact of window sizeThe issue of how to determine appropriate win-dow size (and shape) has often been glossed overin the literature, with such parameters being de-termined arbitrarily, or empirically on a per-application basis, and often receiving little morethan a cursory mention under the description ofmethod.
For reasons that we will discuss how-ever, the issue has been receiving increasing at-tention.
Some have attempted to address it intrin-sically (Sahlgren 2006; Schulte im Walde &Melinger, 2008; Hung et al 2001); others no lessearnestly in the interests of specific applications(Lamjiri, 2003; Edmonds, 1997; Wang 2005;Choueka & Lusignan, 1985) (note that this di-vide is sometimes subtle).The 2008 Workshop on Distributional Lexi-cal Semantics, held in conjunction with theEuropean Summer School on Logic, Languageand Learning (ESSLLI) ?
hereafter the ESSLLIWorkshop - saw this issue (along with other?problem?
parameters in distributional lexicalsemantics) as one of its central themes, and wit-nessed many different takes upon it.
Interest-ingly, there was little consensus, with some stud-ies appearing on the surface to starkly contradictone-another.
It is now generally recognized thatwindow size is, like the choice of corpus or spe-cific association measure, a parameter which canhave a potentially profound impact upon the per-formance of applications which aim to exploitco-occurrence counts.One widely held (and upheld) intuition - ex-pressed throughout the literature, and echoed byvarious presenters at the ESSLLI Workshop - isthat whereas small windows are well suited tothe detection of syntactico-semantic associations,larger windows have the capacity to detectbroader ?topical?
associations.
More specifically,we can observe that small windows are unavoid-ably limited to detecting associations manifest atvery close distances in the text.
For example, a861window size of two words can only ever observebigrams, and cannot detect associations resultingfrom larger constructs, however ingrained in thelanguage (e.g.
?if ?
then?, ?ne ?
pas?, ?dear ...yours?).
This is not the full story however.
As,Rapp (2002) observes, choosing a window sizeinvolves making a trade-off between variousqualities.
So conversely for example, frequencycounts within large windows, though able to de-tect longer-range associations, are not readilyable to distinguish them from bigram style co-occurrences, and so some discriminatory power,and sensitivity to the latter, is lost.
Rapp (2002)calls this trade-off ?specificity?
; equivalent ob-servations were made by Church & Hanks(1989) and Church et al(1991), who refer to thetendency for large windows to ?wash out?,?smear?
or ?defocus?
those associations exhib-ited at smaller scales.In the following two sections, we presenttwo important and scarcely discussed facets ofthis general trade-off related to window size: thatof scale-dependence, and that concerning thespecific way in which the data sparseness prob-lem is manifest.2.1 Scale-dependenceIt has been shown that varying the size of thecontext considered for a word can impact uponthe performance of applications (Rapp, 2002;Yarowsky & Florian, 2002), there being no idealwindow size for all applications.
This is an ines-capable symptom of the fact that varying win-dow size fundamentally affects what is beingmeasured (both in the raw data sense and linguis-tically speaking) and so impacts upon the outputqualitatively.
As Church et al(1991) postulated,?It is probably necessary that the lexicographeradjust the window size to match the scale of phe-nomena that he is interested in?.In the case of inferential lexical semantics,this puts strict limits on the interpretation of as-sociation scores derived from co-occurrencecounts and, therefore, on higher-level featuressuch as context vectors and similarity measures.As Wang (2005) eloquently observes, with re-spect to the application of word sense disam-biguation, ?window size is an inherent parame-ter which is necessary for the observer to imple-ment an observation ?
[the result] has no mean-ing if a window size does not accompany?.
Moreprecisely, we can say that window-based co-occurrence counts (and any word-space modelswe may derive from them) are scale-dependent.It follows that one cannot guarantee there tobe an ?ideal?
window size within even a singleapplication.
Distributional lexical semantics of-ten defers to human association norms forevaluation.
Schulte im Walde & Melinger (2008)found that the correlation between co-occurrencederived association scores and human associationnorms were weakly dependent upon the windowsize used to calculate the former, but that certainassociations tended to be represented at certainwindow sizes, by virtue of the fact that the bestoverall correlation was found by combining evi-dence from all window sizes.
By identifying asingle window size (whether arbitrary or appar-ently optimum) and treating other evidence asextraneous, it follows that studies may tend todistance their findings from one another.As Church et al(1991) allude, in certainsituations the ability to tune analysis to a specificscale in this way may be desirable (for example,when explicitly searching for statistically signifi-cant bigrams, only a 2-token window will do).
Inother scenarios however, especially where atrade-off in aspects of performance is found be-tween scales, it can clearly be seen as a limita-tion.
And after all, is Church et als notionallexicographer really interested in those featuresmanifest at a specific scale, or is he interested ina specific linguistic category of features?
Not-withstanding grammatical notions of scale (theclause, the sentence etc), there is as yet little evi-dence to suggest how the two are linked.The existence of these trade-offs has ledsome authors towards creative solutions: lookingfor ways of varying window size dynamically inresponse to some performance measure, or si-multaneously exploiting more than one windowsize in order to maximize the pertinent informa-tion captured (Wang, 2005; Quasthoff, 2007;Lamjiri et al 2003).
When the scales at which anassociation is manifest are the quantity of interestand the subject of systematic study, we havewhat is known in scale-aware disciplines asmulti-scalar analysis, of which fractal analysis isa variant.
Although a certain amount has beenwritten about the fractal or hierarchical nature oflanguage, approaches to co-occurrence in lexicalsemantics remain almost exclusively mono-scalar, with the recent work of Quasthoff (2007)being a rare exception.2.2 Data sparsenessAnother facet of the general trade-off identifiedby Rapp (2002) pertains to how limitations in-862herent in the combination of data and co-occurrence retrieval method are manifest.When applying a small window, the numberof window positions which can be expected tocontain a specific pair of words will tend to below in comparison to the number of instances ofeach word type.
In some cases, no co-occurrencemay be observed at all between certain wordpairs, and zero or negative association may beinferred (even though we might reasonably ex-pect such co-occurrences to be feasible withinthe window, or know that a logical associationexists).
This is one manifestation of what iscommonly referred to as the data sparsenessproblem, and was discussed by Rapp (2002) as aside-effect of specificity.
It would of course beinaccurate to suggest that data sparseness itself isa response to window size; a larger window su-perficially lessens the sparseness problem byinviting more co-occurrences, but encounters thesame underlying paucity of information in a dif-ferent guise: as both the size and overlap be-tween the windows grow, the available informa-tion is increasingly diluted both within andamongst the windows, resulting in an over-smoothing of the data.
This phenomenon is wellillustrated in the extreme case of a single corpus-sized window where - in the absence of any ex-ternal information - observed and expected co-occurrence frequencies are equivalent, and it isnot possible to infer any associations at all.Addressing the sparseness problem with re-spect to corpus data has received considerableattention in recent years.
It is usually tackled byapplying explicit smoothing methods so as toallow the estimation of frequencies of unseen co-occurrences.
This may involve applying insightson the statistical limitations of working from afinite sample (add-?
smoothing, Good-Turingsmoothing), making inferences from words withsimilar co-occurrence patterns, or ?backing off?to a more general language model based on indi-vidual word frequencies, or even another corpus;for example, Keller & Lapata (2003) use theWeb.
All of these approaches attempt to mitigatethe data sparseness manifest in the observed co-occurrence frequencies; they do not presume toreduce data sparseness by improving the methodof observation.
Indeed, the general assumptionwould seem to be that the only way to minimizedata sparseness is to use more data.
However, wewill show that, similarly to Wang?s (2005) ob-servation concerning windowed measurements ingeneral, apparent data sparseness is as much amanifestation of the observation method as it isof the data itself; there may exist much pertinentinformation in the corpus which yet remains un-exploited.3 Proximity as associationComprehensive multi-scalar analyses (such asapplied by Quasthoff, 2007; and Schulte imWalde & Melinger, 2008) can be laborious andcomputationally expensive, and it is not yet clearhow to derive simple association scores andsuchlike from the dense data they generate (typi-cally a separate set of statistics for each windowsize examined).
There do exist however rela-tively efficient naturally scale-independent toolswhich are amenable to the detection of linguisti-cally interesting features in text.
In some do-mains the concept of proximity (or distance ?
wewill use the terms somewhat interchangeablyhere) has been used as the basis for straightfor-ward alternatives to various frequency-basedmeasures.
In biogeography, for example, the dis-persion or ?clumpiness?
of a population of indi-viduals can be accurately estimated by samplingthe distances between them (Clark & Evans,1954): a task more conventionally carried out by?quadrat?
sampling, which is directly analogousto the window-based methods typically used tomeasure dispersion or co-occurrence in a corpus(see Gries, 2008, for an overview of dispersion ina linguistic setting).
Such techniques are alsobeen used in archeology.
Washtell (2006) foundevidence to suggest that distance-based ap-proaches within the geographic domain can beboth more accurate and more efficient than theirwindow-based alternatives.In the present domain, the notion of prox-imity has been applied by Savick?
& Hlav?cov?
(2002) and Washtell (2007) - both in Gries(2008) - as an alternative to approaches based oncorpus division, for quantifying the dispersion ofwords within the text.
Hardcastle (2005) andWashtell (2007) apply this same concept tomeasuring word pair associations, the former viaa somewhat ad-hoc approach, the latter throughan extension of Clark-Evans (1954) dispersionmetric to the concept of co-dispersion: the ten-dency of unlike words to gravitate (or be simi-larly dispersed) in the text.
Terra & Clarke(2004) use a very similar approach in order togenerate a probabilistic language model, wherepreviously n-gram models have been used,The allusion to proximity as a fundamentalindicator of lexical association does in fact per-863meate the literature.
Halliday (1966), for exam-ple, in Church et al(1991) talked not explicitlyof frequencies within windows, but of identify-ing lexical associates via ?some measure of sig-nificant proximity, either a scale or at least acut-off point?.
For one (possibly practical) rea-son or another, the ?cut-off point?
has beenadopted and the intuition of proximity has sincebecome entrained within a distinctly frequency-oriented model.
By way of example, the notionof proximity has been somewhat more directlycourted in some window-based studies throughthe use of ?ramped?
or ?weighted?
windows(Lamjiri et al 2003; Bullinaria & Levy, 2007), inwhich co-occurrences appearing towards the ex-tremities of the window are discounted in someway.
As with window size however, the specificimplementations and resultant performances ofthis approach have been inconsistent in the litera-ture, with different profiles (even including thosewhere words are discounted towards the centreof the window) seeming to prove optimum undervarying experimental conditions (compare, forinstance, Bullinaria, 2008, and Shaol & West-bury, 2008, from the ESSLLI Workshop).Performance considerations aside, a problemarising from mixing the metaphors of frequencyand distance in this way is that the resultantmeasures become difficult to interpret; in thepresent case of association, it is not trivially ob-vious how one might establish an expected valuefor a window with a given profile, or apply andinterpret conditional probabilities and other well-understood association measures.1 At the veryleast, Wang?s (2005) observation is exacerbated.3.1 Co-dispersionBy doing away with the notion of a window en-tirely and focusing purely upon distance informa-tion, Halliday?s (1966) intuitions concerningproximity can be more naturally realized.
Underthe frequency regime, co-occurrence scores cor-respond directly to probabilities, which are wellunderstood (providing, as Wang, 2005, observes,that a window size is specified as a reference-frame for their interpretation).
It happens thatsimilarly intuitive mechanics apply within apurely distance-oriented regime - a fact realisedby Clark & Evans (1954), but not exploited byHardcastle (2005).
Co-dispersion, which is de-rived from the Clark-Evans metric (and moredescriptively entitled ?co-dispersion by nearest1Existing works do not go into detail on method, so itis possible that this is one source of discrepancies.neighbour?
- as there exist many ways to meas-ure dispersion), can be generalised as follows:)dist,,M(dist)freq,(freqnm=CoDispn1 ababbaab...)1(max +?Where, in the denominator, distabi is the in-ter-word distance (the number of interveningtokens plus one) between the ith occurrence ofword-type a in the corpus, and the nearest pre-ceding or following occurrence of word-type b(if one exists before encountering (1) anotheroccurrence of a or (2) the edge of the containingdocument).
M is the generalized mean.
In thenumerator, freqi is the total number of occur-rences of word-type i, n is the number of tokensin the corpus, and m is a constant based on theexpected value of the mean (e.g.
for the arithme-tic mean ?
as used by Clark & Evans - this is0.5).
Note that the implementation consideredhere does not distinguish word order; owing tothis, and the constraint (1), the measure is sym-metric.2Plainly put, co-dispersion calculates the ratioof the mean observed distance to the expecteddistance between word type pairs in the text; orhow much closer the word types occur, on aver-age, than would expected according to chance3.In this sense it is conceptually equivalent toPointwise Mutual Information (PMI) and relatedassociation measures which are concerned withgauging how more frequently two words occurtogether (in a window), than would be expectedby chance.Like many of its frequency-oriented cousins,co-dispersion can be used directly as a measureof association, with values in the range0>=CoDisp<=?
(with a value of 1 representingno discernible association); and as with thesemeasures, the logarithm can be taken in order topresent the values on a scale that more meaning-fully represents relative associations (as is thedefault with PMI).
Also as with PMI et al co-dispersion can have a tendency to give inflatedestimates where infrequent words are involved.To address this problem, a simple significance-2This constraint, which was independently adoptedby Terra & Clarke (2004), has significant computa-tional advantages as it effectively limits the searchdistance for frequent words.3The expected distance of an independent word-typepair is assumed to be half the distance betweenneighbouring occurrences of the more frequent word-type, were it uniformly distributed within the corpus.864corrected measure, more akin to a Z-Score or T-Score (Dennis, 1965; Church et al 1991) can beformed by taking (the root of) the number ofword-type occurrences into account (Sackett,2001).
The same principal can be applied to PMI,although in practice more precise significancemeasures such as Log-Likelihood are favoured.4These similarities aside, co-dispersion hasthe somewhat abstract distinction of being effec-tively based on degrees rather than probabilities.Although it is windowless (and therefore, as wewill show, scale-independent), it is not withoutanalogous constraints.
Just as the concept ofmean frequency employed by co-occurrence re-quires a definition of distance (window size), theconcept of distance employed by co-dispersionrequires a definition of frequency.
In the casepresented here, this frequency is 1 (the nearestneighbour).
Thus, whereas the assumption withco-occurrence is that the linguistically pertinentwords are those that fall within a fixed-sizedwindow of the word of interest, the assumptionunderpinning co-dispersion is that the relevantinformation lies (if at all) with the closestneighbouring occurrence of each word type.Among other things, this naturally favours theconsideration of nearby function words, whereas(generally less frequent) content words are con-sidered to be of potential relevance at some dis-tance.
That this may be a desirable property - orat least a workable constraint - is borne out bythe fact that other studies have experienced suc-cess by treating these two broad classes of wordswith separately sized windows (Lamjiri et al2003).4 Analyses4.1 Scale-independenceTable 1 shows a matrix of agreement betweenword-pair association scores produced by co-occurrence and co-dispersion as applied to theunlemmatised, untagged, Brown Corpus.
For co-occurrence, window sizes of ?1, ?3, ?10, ?32,and ?100 words were used (based on to a -somewhat arbitrary - scaling factor of ?10).The words used were a cross-section ofstimulus-response pairs from human associationexperiments (Kiss et al 1973), selected to give auniform spread of association scores, as used inthe ESSLLI Workshop shared task.
It is not ourpurpose in the current work to demonstrate com-4Although the heuristically derived MI2 and MI3(Daille, 1994) have gained some popularity.petitive correlations with human associationnorms (which is quite a specific research area)and we are making no cognitive claims here.Their use lends convenience and a (limited) de-gree of relevance, by allowing us to perform ourcomparison across a set of word-pairs which aredeigned to represent a broad spread of associa-tions according to some independent measure.Nonetheless, correlations with the associationnorms are presented as this was a straightforwardstep, and grounds the findings presented here in amore tangible context.Because the human stimulus-response rela-tionship is generally asymmetric (favouringcases where the stimulus word evokes the re-sponse word, but not necessarily vice-versa), theconditional probability of the response word wasused, rather than PMI which is symmetric.
Forthe windowless method, co-dispersion wasadapted equivalently - by multiplying the resul-tant association score by the number of wordpairings divided by the number of occurrences ofthe cue word.
These association scores were alsocorrected for statistical significance, as per Sack-ett (2001).
Both of these adjustments were foundto improve correlations with human scores acrossthe board, but neither impacts directly upon thecomparative analyses performed herein.
It is alsoworth mentioning that many human associationreproduction experiments employ higher-orderparadigmatic associations, whereas we use onlysyntagmatic associations.5 This is appropriate asour focus here is on the information captured atthe base level (from which higher order features?
paradigmatic associations, semantic categoriesetc - are invariably derived).
It can be seen in therightmost column of table 1 that, despite the lackof sophistication in our approach, all windowsizes and the windowless approach generatedstatistically significant (if somewhat less thanstate-of-the-art) correlations with the subset ofhuman association norms used.Owing to the relatively small size of the cor-pus, and the removal of stop-words, a large por-tion of the human stimulus-response pairs usedas our basis generated no association (nosmoothing was used as we are concerned at thislevel in raw evidence captured from the corpus).All correlations presented herein therefore con-sider only those word pairs for which there wassome evidence under the methods being com-5Though interestingly, work done by Wettler et al(2005) suggests that paradigmatic associations maynot be necessary for cognitive association models.865pared from which to generate a non-zero associa-tion score (however statistically insignificant).This number of word pairs, shown in squarebrackets in the leftmost column of table 1, natu-rally increases with window size, and is highestfor the windowless methods.Table 1: Matrix of agreement (corrected r2) betweenassociation retrieval methods; and correlations withsample association norms (r, and p-value).The coefficients of determination (correctedr2values) in the main part of table 1 show clearlythat, as window sizes diverge, their agreementover the apparent association of word pairs in thecorpus diminishes - to the point where there isalmost as much disagreement as there is agree-ment between windows whose size differs by adecimal order of magnitude.
While relativelysmall, the fact that there remains a degree of in-formation overlap between the smallest and larg-est windows in this study (18%), illustrates thatsome word pairs exhibit associative tendencieswhich markedly transcend scale.
It would followthat single window sizes are particularly impo-tent where such features are of holistic interest.The figures in the bottom row of table 1show, in contrast, that there is a more-or-lessconstant level of agreement between the win-dowless and windowed approaches, regardlessof the window size chosen for the latter.Figure 1 gives a good two-dimensional sche-matic approximation of these various relation-ships (in the style of a Venn diagram).
Analysisof partial correlations would give a more accu-rate picture, but is probably unnecessary in thiscase as the areas of overlap between methods arelarge enough to leave marginal room for misrep-resentation.
It is interesting to observe that co-dispersion appears to have a slightly higher af-finity for the associations best detected by smallwindows in this case.
Reassuringly nonetheless,the relative correlations with association normshere - and the fact that we see such significantoverlap ?
do indeed suggest that co-dispersion issensitive to useful information present in each ofthe various windowed methods.
Note that theregions in Figure 1 necessarily have similar ar-eas, as a correlation coefficient describes a sym-metric relationship.
The diagram therefore saysnothing about the amount of information cap-tured by each of these methods.
It is this issuewhich we will look at next.Figure 1: Approximate Venn representation of agree-ment between windowed and windowless associationretrieval methods.4.2 Statistical powerTo paraphrase Kilgariff (2005), language is any-thing but random.
A good language model is onewhich best captures the non-random structure oflanguage.
A good measuring device for any lin-guistic feature is therefore one which stronglydifferentiates real language from random data.The solid lines in figures 2a and 2b give an indi-cation of the relative confidence levels (p-values)attributable to a given association score derivedfrom windowed co-occurrence data.
Figure 2a isbased on a window size of ?10 words, and 2b?100 words.
The data was generated, MonteCarlo style, from a 1 million word randomlygenerated corpus.
For the sake of statistical con-venience and realism, the symbols in the corpuswere given a Zipf frequency distribution roughlymatching that of words found in the Brown cor-pus (and most English corpora).
Unlike with theprevious experiment, all possible word pairingswere considered.
PMI was used for measuringassociation, owing to its convenience and simi-larity to co-dispersion, but it should be noted thatthe specific formulation of the association meas-ure is more-or-less irrelevant in the present con-text, where we are using relative association lev-els between a real and random corpus as a proxyfor how much structural information is capturedfrom the corpus.866Figure 2a: Co-occurrence significances for a moderate(?10 words) window.Figure 2b: Co-occurrence significances for a large(?100 words) window.Precisely put, the figures show the percentageof times a given association score or lower wasmeasured between word types in a corpus whichis known to be devoid of any actual syntagmaticassociation.
The closer to the origin these lines,the fewer word instances were required to bepresent in the random corpus before high levelsof apparent association became unlikely, and sothe fewer would be required in a real corpus be-fore we could be confident of the import of ameasured level of association.
Consequently, ifword pairs in a real corpus exceed these levels,we say that they show significant association.The shaded regions in figures 2a and 2b showthe typical range of apparent association scoresfound in a real corpus ?
in this case the Browncorpus.
The first thing to observe is that both thespread of raw association scores and their sig-nificances are relatively constant across wordfrequencies, up to a frequency threshold which islinked to the window size.
This constancy existsin spite of a remarkable variation in the raw as-sociation scores, which are increasingly inflatedtowards the lower frequencies (indeed illustrat-ing the importance of taking statistical signifi-cance into account).
This observed constancy isintuitive where long-range associations betweenwords prevail: very infrequent words will tend toco-occur within the window less often than mod-erately frequent words - by simple virtue of theirnumber - yet when they do co-occur, the evi-dence for association is that much stronger ow-ing to the small size of the window relative totheir frequency.
Beyond the threshold governedby window size, there can be seen a sharp level-ling out in apparent association, accompanied byan attendant drop in overall significance.
This isa manifestation of Rapp?s specificity: as wordsbecome much more frequent than window size,the kinds of tight idiomatic co-occurrences andcompound forms which would otherwise implyan uncommonly strong association can no longerbe detected as such.A related observation is that, in spite of thelower random baseline exhibited by the largerwindow size, the actual significance of the asso-ciations it reports in a real corpus are, for allword frequencies, lower than those reported bythe smaller window: i.e.
quantitatively speaking,larger windows seem to observe less!
Evidently,apparent association is as much a function ofwindow size as it is of actual syntagmatic asso-ciation; it would be very tempting to interpret theassociation profiles in figures 2a or 2b, in isola-tion of each other or their baseline plots, as indi-cating some interesting scale-varying associativestructure in the corpus, where in fact they do not.Figure 3: Significances for windowless co-dispersion.60%867Figure 3 is identical to figures 2a and 2b (thesame random and real world corpora were used)but it represents the windowless co-dispersionmethod presented herein.
It can be seen that therandom corpus baseline comprises a smoothpower curve which gives low initial associationlevels, rapidly settling towards the expectedvalue of zero as the number of token instancesincreases.
Notably, the bulk of apparent associa-tion scores reported from the Brown Corpus are,while not necessarily greater, orders of magni-tude more significant than with the windowedexamples for all but the most frequent words(ranging well into the 99%+ confidence levels).This gain can only follow from the fact that moreinformation is being taken into account: not onlydo we now consider relationships that occur at allscales, as previously demonstrated, but we con-sider the exact distance between word tokens, asopposed to low-range ordinal values linked towindow-averaged frequencies.
There is no ob-servable threshold effect, and without a windowthere is no reason to expect one.
Accordingly,there is no specificity trade-off: while word pairsinteracting at very large distances are captured(as per the largest of windows), very close occur-rences are still rewarded appropriately (as per thesmallest of window).5 Conclusions and future directionWe have presented a novel alternative to co-occurrence for measuring lexical associationwhich, while based on similar underlying lin-guistic intuitions, uses a very different apparatus.We have shown this method to gather more in-formation from the corpus overall, and to be par-ticularly unfettered by issues of scale.
While theinformation gathered is, by definition, linguisti-cally relevant, relevance to a given task (such asreproducing human association norms or per-forming word-sense disambiguation), or superiorperformance with small corpora, does not neces-sarily follow.
Further work is to be conducted inapplying the method to a range of linguistictasks, with an initial focus on lexical semantics.In particular, properties of resultant word-spacemodels and similarity measures beg a thoroughinvestigation: while we would expect to gaindenser higher-precision vectors, there mightprove to be overriding qualitative differences.The relationship to grammatical dependency-based contexts which often out-perform contigu-ous contexts also begs investigation.It is also pertinent to explore the more fun-damental parameters associated with the win-dowless approach; the formulation of co-dispersion presented herein is but one interpreta-tion of the specific case of association.
In thesesenses there is much catching-up to do.At the present time, given the key role of win-dow size in determining the selection and appar-ent strength of associations under the conven-tional co-occurrence model - highlighted hereand in the works of Church et al(1991), Rapp(2002), Wang (2005), and Schulte im Walde &Melinger (2008) - we would urge that this is anissue which window-driven studies continue toconscientiously address; at the very least, scale isa parameter which findings dependent on distri-butional phenomena must be qualified in light of.AcknowledgementsKind thanks go to Reinhard Rapp, Stefan Gries,Katja Markert, Serge Sharoff and Eric Atwell fortheir helpful feedback and positive support.ReferencesJohn A. Bullinaria.
2008.
Semantic CategorizationUsing Simple Word Co-occurrence Statistics.
In:M. Baroni, S. Evert & A. Lenci (Eds), Proceedingsof the ESSLLI Workshop on Distributional LexicalSemantics: 1 - 8John A. Bullinaria  and Joe P. Levy.
2007.
ExtractingSemantic Representations from Word Co-occurrence Statistics: A Computational Study.
Be-havior Research Methods, 39:510 - 526.Yaacov Choueka and Serge Lusignan.
1985.
Disam-biguation by short contexts.
Computers and theHumanities.
19(3):147 - 157Kenneth W. Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information, and lexi-cography.
In Proceedings of the 27th Annual Meet-ing on Association For Computational Linguistics:76 - 83Kenneth W. Church, William A. Gale, Patrick Hanksand Donald Hindle.
1991.
Using statistics in lexi-cal analysis.
In: Lexical Acquisition: Using On-line Resources to Build a Lexicon, Lawrence Erl-baum: 115 - 164.P.
J. Clark and F. C. Evans.
1954.
Distance to nearestneighbor as a measure of spatial relationships inpopulations.Ecology.
35: 445 - 453.B?atrice Daille.
1994.
Approche mixte pour l'extrac-tion automatique de terminologie: statistiques lexi-cales et filtres linguistiques.
PhD thesis, Universit?Paris.868Sally F. Dennis.
1965.
The construction of a thesau-rus automatically from a sample of text.
In Pro-ceedings of the Symposium on Statistical Associa-tion Methods For Mechanized Documentation,Washington, DC: 61 - 148.Philip Edmonds.
1997.
Choosing the word most typi-cal in context using a lexical co-occurrence net-work.
In Proceedings of the Eighth Conference onEuropean Chapter of the Association For Computa-tional Linguistics: 507 - 509Stefan Evert.
2007.
Computational Approaches toCollocations: Association Measures,  Institute ofCognitive Science, University of Osnabruck,<http://www.collocations.de>.Manfred Wettler, Reinhard Rapp and Peter Sedlmeier.2005.
Free word associations correspond to conti-guities between words in texts.
Journal of Quantita-tive Linguistics, 12:111 - 122.Michael K. Halliday.
1966 Lexis as a LinguisticLevel, in Bazell, C., Catford, J., Halliday, M., andRobins, R.
(eds.
), In Memory of J. R. Firth, Long-man, London.David Hardcastle.
2005.
Using the distributional hy-pothesis to derive cooccurrence scores from theBritish National Corpus.
Proceedings of CorpusLinguistics.
Birmingham, UKKei Yuen Hung, Robert Luk, Daniel Yeung, KorrisChung and Wenhuo Shu.
2001.
Determination ofContext Window Size, International Journal ofComputer Processing of Oriental Languages,14(1): 71 - 80Stefan Gries.
2008.
Dispersions and Adjusted Fre-quencies in Corpora.
International Journal of Cor-pus Linguistics, 13(4)Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams, Compu-tational Limguistics, 29:459 ?
484Adam Kilgarriff.
2005.
Language is never ever everrandom.
Corpus Linguistics and Linguistic Theory1: 263 - 276.George Kiss, Christine Armstrong, Robert Milroy andJames Piper.
1973.
An associative thesaurus ofEnglish and its computer analysis.
In Aitken, A.J.,Bailey, R.W.
and Hamilton-Smith, N.
(Eds.
), TheComputer and Literary Studies.
Edinburgh Univer-sity Press.Abolfazl K. Lamjiri, Osama El Demerdash and LeilaKosseim.
2003.
Simple Features for StatisticalWord Sense Disambiguation, Proceedings of Sen-seval-3:3rd International Workshop on the Evalua-tion of Systems for the Semantic Analysis of Text:133 - 136.Uwe Quasthoff.
2007.
Fraktale Dimension vonW?rtern.
Unpublished manuscript.Reinhard Rapp.
2002.
The computation of word asso-ciations: comparing syntagmatic and paradigmaticapproaches.
In Proceedings of the 19th interna-tional Conference on Computational Linguistics.D.
L. Sackett.
2001.
Why randomized controlled trialsfail but needn't: 2.
Failure to employ physiologicalstatistics, or the only formula a clinician-trialist isever likely to need (or understand!).
CMAJ,165(9):1226 - 37.Magnus Sahlgren.
2006.
The Word-Space Model:using distributional analysis to represent syntag-matic and paradigmatic relations between words inhigh-dimensional vector space, PhD Thesis,Stockholm University.Petr Savick?
and Jana Hlav?cov?.
2002.
Measures ofword commonness.
Journal of QuantitativeLuiguistics, 9(3): 215 ?
31.Cyrus Shaoul, Chris Westbury.
2008.
Performance ofHAL-like word space models on semantic cluster-ing.
In: M. Baroni, S. Evert & A. Lenci (Eds), Pro-ceedings of the ESSLLI Workshop on Distribu-tional Lexical Semantics: 1 ?
8.Sabine Schulte im Walde and Alissa Melinger, A.2008.
An In-Depth Look into the Co-OccurrenceDistribution of Semantic Associates, Italian Journalof Linguistics, Special Issue on From Context toMeaning: Distributional Models of the Lexicon inLinguistics and Cognitive Science.Egidio Terra and Charles L. A. Clarke.
2004.
FastComputation of Lexical Affinity Models, Proceed-ings of the 20th International Conference on Com-putational Linguistics, Geneva, Switzerland.Xiaojie Wang.
2005.
Robust Utilization of Context inWord Sense Disambiguation, Modeling and UsingContext, Lecture Notes in Computer Science,Springer: 529-541.Justin Washtell.
2006.
Estimating Habitat Area &Related Ecological Metrics: From Theory TowardsBest Practice, BSc Dissertation, University ofLeeds.Justin Washtell.
2007.
Co-Dispersion by NearestNeighbour: Adapting a Spatial Statistic for the De-velopment of Domain-Independent Language Toolsand Metrics, MSc Thesis, University of Leeds.Warren Weaver.
1949 Translation.
Repr.
in: Locke,W.N.
and Booth, A.D.
(eds.)
Machine translationof languages: fourteen essays (Cambridge, Mass.
:Technology Press of the Massachusetts Institute ofTechnology, 1955), 15-23.
Association for Com-puting Machinery, 28(1):114-133.David Yarowsky and Radu Florian.
2002.
EvaluatingSense Disambiguation Performance Across Di-verse Parameter Spaces.
Journal of Natural Lan-guage Engineering, 8(4).869
