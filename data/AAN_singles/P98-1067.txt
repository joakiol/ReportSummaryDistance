Toward General-Purpose Learning for Information ExtractionDayne FreitagSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdayne?cs, crau.
eduAbst ractTwo trends are evident in the recent evolution ofthe field of information extraction: a preferencefor simple, often corpus-driven techniques overlinguistically sophisticated ones; and a broaden-ing of the central problem definition to includemany non-traditional text domains.
This devel-opment calls for information extraction systemswhich are as retctrgetable and general as possi-ble.
Here, we describe SRV, a learning archi-tecture for information extraction which is de-signed for maximum generality and flexibility.SRV can exploit domain-specific nformation,including linguistic syntax and lexical informa-tion, in the form of features provided to the sys-tem explicitly as input for training.
This pro-cess is illustrated using a domain created fromReuters corporate acquisitions articles.
Fea-tures are derived from two general-purpose NLPsystems, Sleator and Temperly's link grammarparser and Wordnet.
Experiments compare thelearner's performance with and without suchlinguistic information.
Surprisingly, in manycases, the system performs as well without thisinformation as with it.1 In t roduct ionThe field of information extraction (IE) is con-cerned with using natural anguage processing(NLP) to extract essential details from text doc-uments automatically.
While the problems ofretrieval, routing, and filtering have receivedconsiderable attention through the years, IE isonly now coming into its own as an informationmanagement sub-discipline.Progress in the field of IE has been away fromgeneral NLP systems, that must be tuned towork ill a particular domain, toward faster sys-tems that perform less linguistic processing ofdocuments and can be more readily targeted atnovel domains (e.g., (Appelt et al, 1993)).
Anatural part of this development has been theintroduction of machine learning techniques tofacilitate the domain engineering effort (Riloff,1996; Soderland and Lehnert, 1994).Several researchers have reported IE systemswhich use machine learning at their core (Soder-land, 1996; Califf and Mooney, 1997).
Ratherthan spend human effort tuning a system for anIE domain, it becomes possible to conceive oftraining it on a document sample.
Aside fromthe obvious savings in human development ef-fort, this has significant implications for infor-mation extraction as a discipline:Retargetability Moving to a novel domainshould no longer be a question of code mod-ification; at most some feature ngineeringshould be required.General i ty  It should be possible to handle amuch wider range of domains than previ-ously.
In addition to domains characterizedby grammatical prose, we should be able toperform information extraction in domainsinvolving less traditional structure, such asnetnews articles and Web pages.In this paper we describe a learning algorithmsimilar in spirit to FOIL (Quinlan, 1990), whichtakes as input a set of tagged ocuments, and aset of features that control generalization, andproduces rules that describe how to extract in-formation from novel documents.
For this sys-tem, introducing linguistic or any other infor-mation particular to a domain is an exercise infeature definition, separate from the central al-gorithm, which is constant.
We describe a set ofexperiments, involving a document collection ofnewswire articles, in which this learner is com-pared with simpler learning algorithms.4042 SRVIn order to be suitable for the widest possiblevariety of textual domains, including collectionsmade up of informal E-mail messages, WorldWide Web pages, or netnews posts, a learnermust avoid any assumptions about the struc-ture of documents that might be invalidated bynew domains.
It is not safe to assume, for ex-ample, that text will be grammatical, or that alltokens encountered will have entries in a lexiconavailable to the system.
Fundamentally, a doc-ument is simply a sequence of terms.
Beyondthis, it becomes difficult to make assumptionsthat are not violated by some common and im-portant domain of interest.At the same time, however, when structuralassumptions are justified, they may be criti-cal to the success of the system.
It should bepossible, therefore, to make structural informa-tion available to the learner as input for train-ing.
The machine learning method with whichwe experiment here, SRV, was designed withthese considerations in mind.
In experiments re-ported elsewhere, we have applied SRV to collec-tions of electronic seminar announcements andWorld Wide Web pages (Freitag, 1998).
Read-ers interested in a more thorough description ofSRV are referred to (Freitag, 1998).
Here, welist its most salient characteristics:?
Lack of  s t ruc tura l  assumpt ions .
SRVassumes nothing about the structure of afield instance 1 or the text in which it isembedded--only that an instance is an un-broken fragment of text.
During learningand prediction, SRV inspects every frag-ment of appropriate size.?
Token-or iented features.
Learning isguided by a feature set which is separatefrom the core algorithm.
Features de-scribe aspects of individual tokens, such ascapitalized, numeric, noun.
Rules can positfeature values for individual tokens, or forall tokens in a fragment, and can constrainthe ordering and positioning of tokens.?
Re lat iona l  features .
SRV also includes1We use the terms field and field instance for therather  generic IE concepts of slot and slot filler.
For anewswire article about  a corporate acquisition, for exam-ple, a field instance might be the text f ragment listingthe amount  paid as part  of the deal.a notion of relational features, such asnext-token, which map a given token to an-other token in its environment.
SRV usessuch features to explore the context of frag-ments under investigation.?
Top-down greedy  ru le  search.
SRVconstructs rules from general to specific,as in FOIL (Quinlan, 1990).
Top-downsearch is more sensitive to patterns in thedata, and less dependent on heuristics,than the bottom-up search used by sim-ilar systems (Soderland, 1996; Califf andMooney, 1997).?
Ru le  val idat ion.
Training is followed byvalidation, in which individual rules aretested on a reserved portion of the train-ing documents.
Statistics collected in thisway are used to associate a confidence witheach prediction, which are used to manip-ulate the accuracy-coverage trade-off.3 Case  StudySRV's default feature set, designed for informaldomains where parsing is difficult, includes nofeatures more sophisticated than those immedi-ately computable from a cursory inspection oftokens.
The experiments described here werean exercise in the design of features to capturesyntactic and lexical information.3.1 DomainAs part of these experiments we defined an in-formation extraction problem using a publiclyavailable corpus.
600 articles were sampledfrom the "acquisition" set in the Reuters corpus(Lewis, 1992) and tagged to identify instancesof nine fields.
Fields include those for the officialnames of the parties to an acquisition (acquired,purchaser, seller), as well as their short names(acqabr, purchabr, sellerabr), the location of thepurchased company or resource (acqloc), theprice paid (dlramt), and any short phrases um-marizing the progress of negotiations (status).The fields vary widely in length and frequencyof occurrence, both of which have a significantimpact on the difficulty they present for learn-ers.3.2 Feature Set Des ignWe augmented SRV's default feature set withfeatures derived using two publicly available405.---,---.---,---.--,,-+-Ce-+Ss*b+I I I I I IFirst Wisconsin Corp said.v it plans.v ...token."
Corp I \[token: soi  1 I oken: it  II lg_tag: nil | / lg_tag: "v" / | lg_ tag :  n i l  /~left_G / I ~left_S / I l\left C / IFigure 1: An example of link grammar featurederivation.NLP tools, the link grammar parser and Word-net.The link grammar parser takes a sentence asinput and returns a complete parse in whichterms are connected in typed binary relations("links") which represent syntactic relationships(Sleator and Temperley, 1993).
We mappedthese links to relational features: A token onthe right side of a link of type X has a cor-responding relational feature called left_)/ thatmaps to the token on the left side of the link.
Inaddition, several non-relational features, such aspart of speech, are derived from parser output.Figure 1 shows part of a link grammar parseand its translation into features.Our object in using Wordnet (Miller, 1995)is to enable 5RV to recognize that the phrases,"A bought B," and, "X acquired Y," are in-stantiations of the same underlying pattern.
Al-though "bought" and "acquired" do not belongto the same "synset" in Wordnet, they are nev-ertheless closely related in Wordnet by meansof the "hypernym" (or "is-a') relation.
To ex-ploit such semantic relationships we created asingle token feature, called wn_word.
In con-trast with features already outlined, which aremostly boolean, this feature is set-valued.
Fornouns and verbs, its value is a set of identifiersrepresenting all synsets in the hypernym path tothe root of the hypernym tree in which a wordoccurs.
For adjectives and adverbs, these synsetidentifiers were drawn from the cluster of closelyrelated synsets.
In the case of multiple Word-net senses, we used the most common sense ofa word, according to Wordnet, to construct thisset.3.3 Compet ing  Learners\?e compare the performance of 5RV with thatof two simple learning approaches, which makepredictions based on raw term statistics.
Rote(see (Freitag, 1998)), memorizes field instancesseen during training and only makes predic-tions when the same fragments are encounteredin novel documents.
Bayes is a statistical ap-proach based on the "Naive Bayes" algorithm(Mitchell, 1997).
Our implementation is de-scribed in (Freitag, 1997).
Note that althoughthese learners are "simple," they are not neces-sarily ineffective.
We have experimented withthem in several domains and have been sur-prised by their level of performance in somecases.4 ResultsThe results presented here represent averageperformances over several separate experiments.In each experiment, the 600 documents in thecollection were randomly partitioned into twosets of 300 documents each.
One of the twosubsets was then used to train each of the learn-ers, the other to measure the performance of thelearned extractors.\?e compared four learners: each of the twosimple learners, Bayes and Rote, and SRV withtwo different feature sets, its default feature set,which contains no "sophisticated" features, andthe default set augmented with the features de-rived from the link grammar parser and Word-net.
\?e will refer to the latter as 5RV+ling.Results are reported in terms of two metricsclosely related to precision and recall, as seen ininformation retrievah Accuracy, the percentageof documents for which a learner predicted cor-rectly (extracted the field in question) over alldocuments for which the learner predicted; andcoverage, the percentage of documents havingthe field in question for which a learner madesome prediction.4.1 Per fo rmanceTable 1 shows the results of a ten-fold exper-iment comparing all four learners on all ninefields.
Note that accuracy and coverage mustbe considered together when comparing learn-ers.
For example, Rote often achieves reasonableaccuracy at very low coverage.Table 2 shows the results of a three-fold ex-periment, comparing all learners at fixed cover-406Acc lCovAlg acqui redRote 59.6 18.5Bayes 19.8 100SRV 38.4 96.6SRVIng 38.0 95.6acqabrRote 16.1 42.5Bayes 23.2 100SRV 31.8 99.8SRVlng 35.5 99.2acqlocRote 6.4 63.1Bayes 7.0 100SRV 12.7 83.7SRVlng 15.4 80.2Ace IV  orpurchaser43.2 23.236.9 10042.9 97.942.4 96.3purchabr3.6 41.939.6 10041.4 99.643.2 99.3status42.0 94.533.3 10039.1 89.841.5 87.9Acc l Covseller38.5 15.215.6 10016.3 86.416.4 82.7sellerabr2.7 27.316.0 10014.3 95.114.7 91.8dlramt63.2 48.524.1 10050.5 91.052.1 89.4Table 1: Accuracy and coverage for all fourlearners on the acquisitions fields.age levels, 20% and 80%, on four fields whichwe considered representative of tile wide rangeof behavior we observed.
In addition, in order toassess the contr ibution of each kind of linguis-tic information (syntactic and lexical) to 5RV'sperformance, we ran experiments in which itsbasic feature set was augmented with only onetype or the other.4.2 D iscuss ionPerhaps surprisingly, but consistent with resultswe have obtained in other domains, there is noone algorithm which outperforms the others onall fields.
Rather than the absolute difficulty ofa field, we speak of the suitabil ity of a learner'sinductive bias for a field (Mitchell, 1997).
Bayesis clearly better  than SRV on the seller andsel lerabr  fields at all points on the accuracy-coverage curve.
We suspect this may be due, inpart, to the relative infrequency of these fieldsin the data.The one field for which the linguistic featuresoffer benefit at all points along the accuracy-coverage curve is acqabr.
2 We surmise that  twofactors contr ibute to this success: a high fre-quency of occurrence for this field (2.42 times2The acqabr differences in Table 2 (a 3-split exper-iment) are not significant at the 95% confidence l vel.However, the full 10-split averages, with 95% error mar-gins, are: at 20% coverage, 61.5+4.4 for SRV and68.5=1=4.2 for SRV-I-\[ing; at 80% coverage, 37.1/=2.0 forSRV and 42.4+2.1 for SRV+ling.Field 80%\[20%Rotep.r0h .... .. - -  ' 50.3acqabr .
.
.
.
24.4dlramt .
.
.
.
69.5s ta tus  46.7 65.3SRV+lingpurch .
.
.
.
48.5 56.3acqabr 44.3 75.4dlramt 57.1 61.9s ta tus  43.3 72.680%12o%Bayes40.6 55.929.3 50.645.9 71.439.4 62.1srv+lg46.3 63.540.4 71.455.4 67.338.8 74.880%120%SRV45.3 55.740.0 63.457.1 66.743.8 72.5srv- -wfl46.7 58.141.9 72.552.6 67.442.2 74.1Table 2: Accuracy from a three-split experimentat fixed coverage levels.A fragment is a acqabr, if:it contains exactly one token;the token (T) is capitalized;T is followed by a lower-case token;T is preceded by a lower-case token;T has a right AN-link to a token (U)with wn_word value "possession";U is preceded by a tokenwith wn_word value "stock";and the token two tokens before Tis not a two-character token.to purchase 4.5 m l n ~  common shares atacquire another 2.4 mln~-a6~treasury sharesFigure 2: A learned rule for acqabr using linguis-tic features, along with two fragments of match-ing text.
The AN-link connects a noun modifierto the noun it modifies (to "shares" in both ex-amples).per document  on average), and consistent oc-currence in a linguistically rich context.Figure 2 shows a 5RV+ling rule that  is ableto exploit both types of linguistic informa-tion.
The Wordnet  synsets for "possession" and"stock" come from the same branch in a hy-pernym tree- -"possess ion" is a generalizationof "s tock"3- -and both match the collocations"common shares" and "treasury shares."
Thatthe paths \[right_AN\] and \[right_AN prev_tok\]both connect to the same synset indicates thepresence of a two-word Wordnet  collocation.It is natural  to ask why SRV+ling does not3SRV, with its general-to-specific search bias, oftenemploys Wordnet his way--first more general synsets,followed by specializations of the same concept.407outperform SRV more consistently.
After all,the features available to SRV+ling are a supersetof those available to SRV.
As we see it, there aretwo basic explanations:?
Noise.
Heuristic choices made in handlingsyntactically intractable sentences and indisambiguating Wordnet word senses in-troduced noise into the linguistic features.The combination of noisy features and avery flexible learner may have led to over-fitting that offset any advantages the lin-guistic features provided.?
Cheap features  equal ly  effective.
Thesimple features may have provided mostof the necessary information.
For exam-ple, generalizing "acquired" and "bought"is only useful in the absence of enough datato form rules for each verb separately.4.3 Conc lus ionMore than similar systems, SRV satisfies the cri-teria of generality and retargetability.
The sep-aration of domain-specific information from thecentral algorithm, in the form of an extensiblefeature set, allows quick porting to novel do-mains.Here, we have sketched this porting process.Surprisingly, although there is preliminary evi-dence that general-purpose linguistic informa-tion can provide benefit in some cases, mostof the extraction performance can be achievedwith only the simplest of information.Obviously, the learners described here arenot intended to solve the information extractionproblem outright, but to serve as a source of in-formation for a post-processing component thatwill reconcile all of the predictions for a docu-ment, hopefully filling whole templates more ac-curately than is possible with any single learner.How this might be accomplished is one themeof our future work in this area.AcknowledgmentsPart of this research was conducted as part ofa summer internship at Just Research.
And itwas supported in part by the Darpa HPKB pro-gram under contract F30602-97-1-0215.Re ferencesDouglas E. Appelt, Jerry R. Hobbs, John Bear,David Israel, and Mabry Tyson.
1993.
FAS-408TUS: a finite-state processor for informationextraction from real-world text.
Proceedingsof IJCAI-93, pages 1172-1178.M.
E. Califf and R. J. Mooney.
1997.
Relationallearning of pattern-match rules for informa-tion extraction.
In Working Papers of ACL-97 Workshop on Natural Language Learning.D.
Freitag.
1997.
Using grammatical in-ference to improve precision in informa-tion extraction.
In Notes of the ICML-97Workshop on Automata Induction, Gram-matical Inference, and Language Acquisition.http://www.cs.cmu.edu/f)dupont/m197p/m197_GI_wkshp.tar.Dayne Freitag.
1998.
Information extractionfrom HTML: Application of a general ma-chine learning approach.
In Proceedings ofthe Fifteenth National Conference on Artifi-cial Intelligence (AAAI-98).D.
Lewis.
1992.
Representation a d Learningin Information Retrieval.
Ph.D. thesis, Univ.of Massachusetts.
CS Tech.
Report 91-93.G.A.
Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of theACM, pages 39-41, November.Tom M. Mitchell.
1997.
Machine Learning.The McGraw-Hilt Companies, Inc.J.
R. Quinlan.
1990.
Learning logical def-initions from relations.
Machine Learning,5(3):239-266.E.
Riloff.
1996.
Automatically generating ex-traction patterns from untagged text.
InProceedings of the Thirteenth National Con-ference on Artificial Intelligence (AAAI-96),pages 1044-1049.Daniel Sleator and Davy Temperley.
1993.Parsing English with a link grammar.
ThirdInternational Workshop on Parsing Tech-nologies.Stephen Soderland and Wendy Lehnert.
1994.Wrap-Up: a trainable discourse module forinformation extraction.
Journal of ArtificialIntelligence Research, 2:131-158.S.
Soderland.
1996.
Learning Text AnalysisRules for Domain-specific Natural LanguageProcessing.
Ph.D. thesis, University of Mas-sachusetts.
CS Tech.
Report 96-087.
