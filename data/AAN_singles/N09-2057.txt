Proceedings of NAACL HLT 2009: Short Papers, pages 225?228,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTree Linearization in English:Improving Language Model Based ApproachesKatja Filippova and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractWe compare two approaches to dependencytree linearization, a task which arises in manyNLP applications.
The first one is the widelyused ?overgenerate and rank?
approach whichrelies exclusively on a trigram language model(LM); the second one combines languagemodeling with a maximum entropy classifiertrained on a range of linguistic features.
Theresults provide strong support for the com-bined method and show that trigram LMs areappropriate for phrase linearization while onthe clause level a richer representation is nec-essary to achieve comparable performance.1 IntroductionTo date, many natural language processing appli-cations rely on syntactic representations and alsomodify them by compressing, fusing, or translatinginto a different language.
A syntactic tree emerg-ing as a result of such operations has to be lin-earized to a string of words before it can be out-put to the end-user.
The simple and most widelyused trigram LM has become a standard tool fortree linearization in English (Langkilde & Knight,1998).
For languages with less rigid word order,LM-based approaches have been shown to performpoorly (e.g., Marsi & Krahmer (2005) for Dutch),and methods relying on a range of linguistic fea-tures have been successfully applied instead (seeUchimoto et al (2000) and Ringger et al (2004),Filippova & Strube (2007) for Japanese and Germanresp.).
To our knowledge, none of the linearizationstudies have compared a LM-based method withan alternative.
Thus, it would be of interest todraw such a comparison, especially on English data,where LMs are usually expected to work well.As an improvement to the LM-based approach,we propose a combined method which distinguishesbetween the phrase and the clause levels:?
it relies on a trigram LM to order words withinphrases;?
it finds the order of clause constituents (i.e.,constituents dependent on a finite verb) with amaximum entropy classifier trained on a rangeof linguistic features.We show that such a differentiated approach is ben-eficial and that the proposed combination outper-forms the method which relies solely on a LM.Hence, our results challenge the widespread attitudethat trigram LMs provide an appropriate way to lin-earize syntactic trees in English but also indicatethat they perform well in linearizing subtrees cor-responding to phrases.2 LM-based ApproachTrigram models are easy to build and use, and it hasbeen shown that more sophisticated n-gram models(e.g., with higher n, complex smoothing techniques,skipping, clustering or caching) are often not worththe effort of implementing them due to data sparse-ness and other issues (Goodman, 2001).
This ex-plains the popularity of trigram LMs in a varietyof NLP tasks (Jurafsky & Martin, 2008), in partic-ular, in tree linearization where they have become225brotherspredet detthe ofneighbormyposspobjprepallFigure 1: A tree of the noun phrase all the brothers of myneighborde facto the standard tree linearization tool in ac-cordance with the ?overgenerate and rank?
principle:given a syntactic tree, one needs to consider all pos-sible linearizations and then choose the one with thelowest entropy.
Given a projective dependency tree1,all linearizations can be found recursively by gener-ating permutations of a node and its children.
Unfor-tunately, the number of possible permutations growsfactorially with the branching factor.
Hence it ishighly desirable to prohibit generation of clearly un-acceptable permutations by putting hard constraintsencoded in the English grammar.
The constraintswhich we implement in our study are the following:determiners, possessives, quantifiers and noun or ad-jective modifiers always precede their heads.
Con-junctions, coordinated elements, prepositional ob-jects always follow their heads.
These constraintsallow us to limit, e.g., the total of 96 (2 ?
2 ?
4!
)possibilities for the tree corresponding to the phraseall the brothers of my neighbor (see Figure 1) to onlytwo (all the brothers of my neighbor, the all brothersof my neighbor).Still, even with such constraints, in some cases thelist of possible linearizations is too long and has tobe reduced to the first N , where N is supposed to besufficiently large.
In our experiments we break thepermutation generation process if the limit of 20,000variants is reached.3 Combined ApproachThe LM approach described above has at least twodisadvantages: (1) long distance dependencies arenot captured, and (2) the list of all possible lineariza-tions can be huge which makes the search for the1Note that a phrase structure tree can be converted into adependency tree, and some PCFG parsers provide this option.best string unfeasible.
However, our combined ap-proach is based on the premise that trigram LMs arewell-suited for finding the order within NPs, PPs andother phrases where the head is not a finite verb.E.g., given a noun modified by the words big, redand the, a LM can reliably rank the correct orderhigher than incorrect ones ( the big red N vs. the redbig N, etc.
).Next, on the clause level, for every finite verb inthe tree we find the order of its dependents using themethod which we originally developed for German(Filippova & Strube, 2007), which utilizes a rangeof such linguistic features as PoS tag, syntactic role,length in words, pronominalization, semantic class,etc.2 For the experiments presented in this paper, wetrain two maximum entropy classifiers on all but thesemantic features:1.
The first classifier determines the best startingpoint for a sentence: for each constituent de-pendent on the verb it returns the probability ofthis constituent being the first one in a sentence.The subject and also adjuncts (e.g.
temporal ad-juncts like yesterday) are usually found in thebeginning of the sentence.2.
The second classifier is trained to determinewhether the precedence relation holds betweentwo adjacent constituents and is applied to allconstituents but the one selected by the firstclassifier.
The precedence relation defined bythis classifier has been shown to be transitiveand thus can be used to sort randomly orderedconstituents.
Note that we do not need to con-sider all possible orders to find the best one.Once the order within clause constituents as well asthe order among them is found, the verb is placedright after the subject.
The verb placing step com-pletes the linearization process.The need for two distinct classifiers can be illus-trated with the following example:(1) a [Earlier today] [she] sent [him] [an email].b [She] sent [him] [an email] [earlier today].c *[She] sent [earlier today] [him] [an email].2See the cited paper for the full list of features and imple-mentation details.226(1a,b) are grammatical while (1c) is hardly accept-able, and no simple precedence rule can be learnedfrom pairs of constituents in (1a) and (1b): the tem-poral adjunct earlier today can precede or followeach of the other constituents dependent on the verb(she, him, an email).
Thus, the classifier whichdetermines the precedence relation is not enough.However, an adequate rule can be inferred withan additional classifier trained to find good startingpoints: a temporal adjunct may appear as the firstconstituent in a sentence; if it is not chosen for thisposition, it should be preceded by the pronominal-ized subject (she), the indirect object (him) and theshort non-pronominalized object (an email).4 ExperimentsThe goal of our experiments is to check the follow-ing hypotheses:1.
That trigram LMs are well-suited for phraselinearization.2.
That there is a considerable drop in perfor-mance when one uses them for linearization onthe clause level.3.
That an approach which uses a richer represen-tation on the clause level is more appropriate.4.1 DataWe take a subset of the TIPSTER3 corpus ?
all WallStreet Journal articles from the period of 1987-92(approx.
72 mill.
words) ?
and automatically anno-tate them with sentence boundaries, part of speechtags and dependency relations using the Stanfordparser (Klein & Manning, 2003).
We reserve asmall subset of about 600 articles (340,000 words)for testing and use the rest to build a trigram LMwith the CMU toolkit (Clarkson & Rosenfeld, 1997,with Good-Turing smoothing and vocabulary size of30,000).
To train the maximum entropy classifierswe use about 41,000 sentences.4.2 EvaluationTo test the trigram-based approach, we generate allpossible permutations of clause constituents, place3Description at http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A.the verb right after the subject and then rank the re-sulting strings with the LM taking the informationon sentence boundaries into account.
To test thecombined approach, we find the best candidate forthe first position in the clause, then put the remain-ing constituents in a random order, and finally sortthem by consulting the second classifier.The purpose of the evaluation is to assess howgood a method is at reproducing the input from itsdependency tree.
We separately evaluate the perfor-mance on the phrase and the clause levels.
Whencomparing the two methods on the clause level, wetake the clause constituents as they are presentedin the input sentence.
Although English allows forsome minor variation in word order and it mighthappen that the generated order is not necessarilywrong if different from the original one, we do notexpect this to happen often and evaluate the perfor-mance rigorously: only the original order counts asthe correct one.
The default evaluation metric is per-phrase/per-clause accuracy:acc = |correct||total|Other metrics we use to measure how different agenerated order of N elements is from the correctone are:1.
Kendall?s ?
, ?
= 1 ?
4 tN(N?1) where t isthe minimum number of interchanges of con-secutive elements to achieve the right order(Kendall, 1938; Lapata, 2006).2.
Edit distance related di, di = 1 ?
mN where mis the minimum number of deletions combinedwith insertions to get to the right order (Ringgeret al, 2004).E.g., on the phrase level, the incorrectly generatedphrase the all brothers of my neighbor (?1-0-2-3-4-5?)
gets ?
= 0.87, di = 0.83.
Likewise, given theinput sentence from (1a), the incorrectly generatedorder of the four clause constituents in (1c) ?
?1-0-2-3?
?
gets ?
of 0.67 and di of 0.75.4.3 ResultsThe results of the experiments on the phrase and theclause levels are presented in Tables 1 and 2 respec-tively.
From the total of 5,000 phrases, 55 (about2271%) were discarded because the number of admis-sible linearizations exceeded the limit of 20,000.
Inthe first row of Table 1 we give the results for caseswhere, with all constraints applied, there were stillseveral possible linearizations (non-triv; 1,797); thesecond row is for all phrases which were longer thanone word (> 1; 2,791); the bottom row presents theresults for the total of 4,945 phrases (all).acc ?
dinon-triv 76% 0.85 0.94> 1 85% 0.90 0.96all 91% 0.94 0.98Table 1: Results of the trigram method on the phrase levelTable 2 presents the results of the trigram-based(TRIGRAM) and combined (COMBINED) methods onthe clause level.
Here, we filtered out trivial casesand considered only clauses which had at least twoconstituents dependent on the verb (approx.
5,000clauses in total).acc ?
diTRIGRAM 49% 0.49 0.81COMBINED 67% 0.71 0.88Table 2: Results of the two methods on the clause level4.4 DiscussionThe difference in accuracy between the performanceof the trigram model on the phrase and the clauselevel is considerable ?
76% vs. 49%.
The accuracyof 76% is remarkable given that the average lengthof phrases which counted as non-triv is 6.2 words,whereas the average clause length in constituents is3.3.
This statistically significant difference in per-formance supports our hypothesis that the ?overgen-erate and rank?
approach advocated in earlier studiesis more adequate for finding the optimal order withinphrases.
The ?
value of 0.85 also indicates that manyof the wrong phrase linearizations were near misses.On the clause level, where long distance dependen-cies are frequent, an approach which takes a rangeof grammatical features into account is more appro-priate ?
this is confirmed by the significantly betterresults of the combined method (67%).5 ConclusionsWe investigated two tree linearization methods inEnglish: the mainstream trigram-based approachand the one which combines a trigram LM on thephrase level with two classifiers trained on a rangeof linguistic features on the clause level.
The resultsdemonstrate (1) that the combined approach repro-duces the word order more accurately, and (2) thatthe performance of the trigram LM-based method onphrases is significantly better than on clauses.Acknowledgments: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by a KTFgrant (09.009.2004).
We would like to thank theanonymous reviewers for their feedback.ReferencesClarkson, P. & R. Rosenfeld (1997).
Statistical languagemodeling using the CMU-Cambridge toolkit.
In Proc.of EUROSPEECH-97, pp.
2707?2710.Filippova, K. & M. Strube (2007).
Generating constituentorder in German clauses.
In Proc.
of ACL-07, pp.
320?327.Goodman, J. T. (2001).
A bit of progress in languagemodeling.
Computer Speech and Language, pp.
403?434.Jurafsky, D. & J. H. Martin (2008).
Speech and LanguageProcessing.
Upper Saddle River, N.J.: Prentice Hall.Kendall, M. G. (1938).
A new measure of rank correla-tion.
Biometrika, 30:81?93.Klein, D. & C. D. Manning (2003).
Accurate unlexical-ized parsing.
In Proc.
of ACL-03, pp.
423?430.Langkilde, I.
& K. Knight (1998).
Generation that ex-ploits corpus-based statistical knowledge.
In Proc.
ofCOLING-ACL-98, pp.
704?710.Lapata, M. (2006).
Automatic evaluation of informationordering: Kendall?s tau.
Computational Linguistics,32(4):471?484.Marsi, E. & E. Krahmer (2005).
Explorations in sentencefusion.
In Proc.
of ENLG-05, pp.
109?117.Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets& S. Corston-Oliver (2004).
Linguistically informedstatistical models of constituent structure for orderingin sentence realization.
In Proc.
of COLING-04, pp.673?679.Uchimoto, K., M. Murata, Q. Ma, S. Sekine & H. Isahara(2000).
Word order acquisition from corpora.
In Proc.of COLING-00, pp.
871?877.228
