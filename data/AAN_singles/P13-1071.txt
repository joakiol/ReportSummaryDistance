Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 721?730,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe Impact of Topic Bias on Quality Flaw Prediction in WikipediaOliver Ferschke?, Iryna Gurevych??
and Marc Rittberger??
Ubiquitous Knowledge Processing LabDepartment of Computer Science, Technische Universita?t Darmstadt?
Information Center for EducationGerman Institute for Educational Research and Educational Informationhttp://www.ukp.tu-darmstadt.deAbstractWith the increasing amount of user gener-ated reference texts in the web, automaticquality assessment has become a key chal-lenge.
However, only a small amountof annotated data is available for trainingquality assessment systems.
Wikipediacontains a large amount of texts anno-tated with cleanup templates which iden-tify quality flaws.
We show that the dis-tribution of these labels is topically bi-ased, since they cannot be applied freelyto any arbitrary article.
We argue that itis necessary to consider the topical restric-tions of each label in order to avoid a sam-pling bias that results in a skewed classifierand overly optimistic evaluation results.We factor out the topic bias by extractingreliable training instances from the revi-sion history which have a topic distribu-tion similar to the labeled articles.
This ap-proach better reflects the situation a classi-fier would face in a real-life application.1 IntroductionUser generated content is the main driving forceof the increasingly social web.
Blogs, wikis andforums make up a large amount of the daily infor-mation consumed by web users.
The main proper-ties of user generated content are a low publicationthreshold and little or no editorial control, whichleads to a high variance in quality.
In order to nav-igate through large repositories of information effi-ciently and safely, users need a way to quickly as-sess the quality of the content.
Automatic qualityassessment has therefore become a key applicationin today?s information society.
However, there isa lack of training data annotated with fine-grainedquality information.Wikipedia, the largest encyclopedia on the web,contains so-called cleanup templates, which con-stitute a sophisticated system of user generated la-bels that mark quality problems in articles.
Re-cently, these cleanup templates have been used forautomatically identifying articles with particularquality flaws in order to support Wikipedia?s qual-ity assurance process in Wikipedia.
In a sharedtask (Anderka and Stein, 2012b), several systemshave shown that it is possible to identify the tenmost frequent quality flaws with high recall andfair precision.However, quality flaw detection based oncleanup template recognition suffers from a topicbias that is well known from other text classifica-tion applications such as authorship attribution orgenre identification.
We discovered that cleanuptemplates have implicit topical restrictions, i.e.they cannot be applied to any arbitrary article.
Asa consequence, corpora of flawed articles basedon these templates are biased towards particulartopics.
We argue that it is therefore not sufficientfor evaluating a quality flaw prediction systems tomeasure how well they can separate (topically re-stricted) flawed articles from a set of random out-liers.
It is rather necessary to determine reliablenegative instances with a similar topic distributionas the set of positive instances in order to factorout the sampling bias.
Related studies (Brooke andHirst, 2011) have proven that topic bias is a con-founding factor that results in misleading cross-validated performance while allowing only nearchance performance in practical applications.We present an approach for factoring out thebias from quality flaw corpora by mining reliablenegative instances for each flaw from the articlerevision history.
Furthermore, we employ the ar-ticle revision history to extract reliable positivetraining instances by using the version of eacharticle at the time it has first been identified asflawed.
This way, we avoid including articleswith outdated cleanup templates, a frequent phe-721nomenon that can occur when a template is notremoved after fixing a problem in an article.
Inour experiments, we focus on neutrality and styleflaws, since they are of particular high importancewithin the Wikipedia community (Stvilia et al,2008; Ferschke et al, 2012a) and are recognizedbeyond Wikipedia in applications such as uncer-tainty recognition (Szarvas et al, 2012) and hedgedetection (Farkas et al, 2010).2 Related WorkTopic bias is a known problem in text classifi-cation.
Mikros and Argiri (2007) investigate thetopic influence in authorship attribution.
Theyfound that even simple stylometric features, suchas sentence and token length, readability mea-sures or word length distributions show consider-able correlations with topic.
They argue that manyfeatures that were largely considered to be topicneutral are in fact topic-dependent variables.
Con-sequently, results obtained on multitopic corporaare prone to be biased by the correlation of authorswith specific topics.
Therefore, several authors in-troduce topic-controlled corpora for applicationssuch as author identification (Koppel and Schler,2003; Luyckx and Daelemans, 2005) or genre de-tection (Finn and Kushmerick, 2006).Brooke and Hirst (2011) measure the topic biasin the International Corpus of Learner Englishand found that it causes a substantial skew in clas-sifiers for native language detection.
In accor-dance with Mikros et al, the authors found thateven non-lexicalized meta features, such as vo-cabulary size or length statistics, depend on top-ics and cause cross-validated performance evalua-tions to be unrealistically high.
In a practical set-ting, these biased classifiers hardly exceed chanceperformance.As already noted above, a similar kind of topicbias negatively influences quality flaw detection inWikipedia.
Anderka et al (2012) automaticallyidentify quality flaws by predicting the cleanuptemplates in unseen articles with a one-class clas-sification approach.
Based on this work, a com-petition on quality flaw prediction has been es-tablished (Anderka and Stein, 2012b).
The win-ning team of the inaugural edition of the taskwas able to detect the ten most common qual-ity flaws with an average F1-Score of 0.81 us-ing a PU learning approach (Ferretti et al, 2012).With a binary classification approach, Ferschke etal.
(2012b) achieved an average F1-Score of 0.80,while reaching a higher precision than the winningteam.A closer examination of the aforementionedquality flaw detection systems reveals a systematicsampling bias in the training data, which leads toan overly optimistic performance evaluation andclassifiers that are biased towards particular arti-cle topics.
Our approach factors out the topic biasfrom the training data by mining topically con-trolled training instances from the Wikipedia revi-sion history.
The results show that flaw detectionis a much harder problem in a real-life scenario.3 Quality Flaws andFlaw Recognition in WikipediaQuality standards in Wikipedia are mainly definedby the featured article criteria1 and the WikipediaManual of Style2.
These policies define the char-acteristics excellent articles have to exhibit.
Othersets of quality criteria are adaptations or relax-ations of these standards, such as the good articlecriteria or the quality grading schemes of individ-ual interest groups in Wikipedia.In this work, we focus on quality flaws regard-ing neutrality and style problems.
We chose thesecategories due to their high importance within theWikipedia community (Stvilia et al, 2008; Fer-schke et al, 2012a) and due to their relevance tocontent outside of Wikipedia, such as blogs or on-line news articles.
According to the Wikipediapolicies3, an article has to be written from a neu-tral point of view.
Thus, authors must avoid stat-ing opinions and seriously contested assertions asfacts, avoid presenting uncontested factual asser-tions as mere opinions, prefer nonjudgmental lan-guage and indicate the relative prominence of op-posing views.
Furthermore, authors have to adhereto the stylistic guidelines defined in the Manual ofStyle.
While this subsumes a broad range of is-sues such as formatting and article structure, wefocus on the style of writing and disregard merestructural properties.Any articles that violate these criteria can bemarked with cleanup templates4 to indicate theirneed for improvement.
These templates canthus be regarded as proxies for quality flaws inWikipedia.1http://en.wikipedia.org/wiki/WP:FACR2http://en.wikipedia.org/wiki/WP:STYLE3http://en.wikipedia.org/wiki/WP:NPOV4http://en.wikipedia.org/wiki/WP:TM#Cleanup722Flaw Description Articles TemplatesAdvert The article appears to be written like an advertisement and is thus not neutral 7,332 2POV The neutrality of this article is disputed 5,086 10Globalize The article may not represent a worldwide view of the subject 1,609 1Peacock The article may contain wording that merely promotes the subject withoutimparting verifiable information1,195 1NeutralityWeasel The article contains vague phrasing that often accompanies biased or unver-ifiable information704 4Tone The tone of the article is not encyclopedic according to the Wikipedia Manualof Style4,563 6In-universe The article describes a work or element of fiction in a primarily in-universestylea2,227 1Copy-edit The article requires copy editing for grammar, style, cohesion, tone, orspelling1,954 6Trivia Contains lists of miscellaneous information 1,282 2Essay-like The article is written like a personal reflection or essay 1,244 1Confusing The article may be confusing or unclear to readers 1,084 1StyleTechnical The article may be too technical for most readers to understand 690 2a According to the Wikipedia Manual of Style, an in-universe perspective describes the article subject matter from theperspective of characters within a fictional universe as if it were real.Table 1: Neutrality and style flaw corpora used in this workTemplate Clusters Since several cleanup tem-plates might represent different manifestations ofthe same quality flaw, there is a 1 to n relation-ship between quality flaws and cleanup templates.For instance, the templates pov-check5, pov6 andnpov language7 can all be mapped to the sameflaw concerning the neutral point of view of an ar-ticle.
This aggregation of cleanup templates intoflaw-clusters is a subjective task.
It is not al-ways clear whether a particular template refers toan existing flaw or should be regarded as a sep-arate class.
Too many clusters will cause defini-tion overlaps (i.e.
similar cleanup templates areassigned to different clusters), while too few clus-ters will result in unclear flaw definitions, sinceeach flaw receives a wide range of possible mani-festations.Template Scope Another important aspect to beconsidered is the difference in the scope whichcleanup templates can have.
Inline-templates areplaced directly in the text and refer to the sentenceor paragraph they are placed in.
Templates witha section parameter, refer to the section they areplaced in.
The majority of templates, however, re-fer to a whole page.
The consideration of the tem-plate scope is of particular importance for qual-ity flaw recognition problems.
For example, thepresence of a cleanup template which marks a sin-gle section as not notable does not entail that thewhole article is not notable.5The article has been nominated for a neutrality check6The neutrality of the article is disputed7The article contains a non-neutral style of writingTopical Restriction A final aspect that has notbeen taken into account by related work is thatmany cleanup templates have restrictions concern-ing the pages they may be applied to.
A hard re-striction is the page type (or namespace) a tem-plate might be used in.
For example, some tem-plates can only be used in articles while others canonly be applied to discussion pages.
This is usu-ally enforced by maintenance scripts running onthe Wikimedia servers.
A soft restriction, on theother hand, are the topics of the articles a tem-plate can be used in.
Many cleanup templates canonly be applied to articles from certain subject ar-eas.
An example with a particularly obvious re-striction is the template in-universe (see Table 1),which should only be applied to articles about fic-tion.
This topical restriction is neither explicitlydefined nor automatically enforced, but it plays animportant role in the quality flaw recognition task,as the remainder of this paper will show.
Whileflaws merely concerning the structural or linguis-tic properties of an article are less restricted toindividual topics, they are still affected by a cer-tain degree of topical preference.
Many subjectareas in Wikipedia are organized in WikiProjects8,which have their own ways of reviewing and en-suring quality within their topical scope.
Depend-ing on the quality assurance processes establishedin a WikiProject, different importance is given toindividual types of flaws.
Thus, the distributionof cleanup templates regarding structural or gram-matical flaws is also biased towards certain topics.8http://en.wikipedia.org/wiki/WP:PROJ723We will henceforth subsume the concept of topicalpreference under the term topical restriction.Quality Flaw Recognition Based on the abovedefinition of quality flaws, we define the qual-ity flaw recognition task similar to Anderka etal.
(2012) as follows: Given a sample of articlesin which each article has been tagged with anycleanup template ?i from a specific template clus-ter T f thus marking all articles in the sample witha quality flaw f , it has to be decided whether ornot an unseen article suffers from f .4 Data Selection and Corpus CreationFor creating our corpora, we start with selecting allcleanup templates listed under the categories neu-trality and style in the typology of cleanup tem-plates provided by Anderka and Stein (2012a).Each of the selected templates serves as the nu-cleus of a template cluster that potentially repre-sents a quality flaw.
To each cluster, we add alltemplates that are synonymous to the nucleus.
Thesynonyms are listed in the template descriptionunder redirects or shortcuts.
Then we iterativelyadd all synonyms of the newly added template un-til no more redirects can be found.
Furthermore,we manually inspect the lists of similar templatesin the see also sections of the template descrip-tions and include all templates that refer to thesame concept as the other templates in the cluster.As mentioned earlier, this is a subjective task andlargely depends on the desired granularity of theflaw definitions.
We finally merge semanticallysimilar template clusters to avoid too fine grainedflaw distinctions.As a result, we obtain a total number of 94template clusters representing 60 style flaws and34 neutrality flaws.
From each of these clusters,we remove templates with inline or section scopedue to the reasons outlined in Section 3.
We alsoremove all templates that are restricted to pagesother than articles (e.g.
discussion or user pages).We use the Java Wikipedia Library (Zesch etal., 2008) to extract all articles marked with theselected templates.
We only regard flaws withat least 500 affected articles in the snapshot ofthe English Wikipedia from January 4, 2012.Table 1 lists the final sets of flaws used in thiswork.
For each flaw, the nucleus of the templatecluster is provided along with a description, thenumber of affected articles, and the cluster size.We make the corpora freely available for down-Flaw ?
F1Advert .60 .80Confusing .60 .80Copy-edit .00 .50Essay-like .60 .80Globalize: .60 .80In-universe .80 .90Peacock .70 .84POV .60 .80Technical .90 .95Tone .40 .70Trivia .20 .60Weasel .50 .74Table 2: Agreement of human annotator with goldstandardload under http://www.ukp.tu-darmstadt.de/data/wiki-flaws/.Agreement with Human RaterQuality flaw detection in Wikipedia is based on theassuption that cleanup templates are valid mark-ers of quality flaws.
In order to test the reliabil-ity of these user assigned templates as quality flawmarkers, we carried out an annotation study inwhich a human annotator was asked to perform thebinary flaw detection task manually.
Even thoughthe human performance does not necessarily pro-vide an upper boundary for the automatic classifi-cation task, it gives insights into potentially prob-lematic cases and ill-defined annotations.
The an-notator was provided with the template definitionsfrom the respective template information page asinstructions.
For each of the 12 article scope flaws,we extracted the plain text of 10 random flawedarticles and 10 random untagged articles.
The an-notator had to decide for each flaw individuallywhether a given text belonged to a flawed articleor not.
She was not informed about the ratio offlawed to untagged articles.Table 2 lists the chance corrected agreement(Cohen?s ?)
along with the F1 performance of thehuman annotations against the gold standard cor-pus.
The templates copy-edit and trivia yieldedthe lowest performance in the study.
Even thoughcopy-edit templates are assigned to whole articles,they refer to grammatical and stylistic problems ofrelatively small portions of the text.
This increasesthe risk of overlooking a problematic span of text,especially in longer articles.
The trivia template,on the other hand, designates sections that containmiscellaneous information that are not well inte-grated in the article.
Upon manual inspection, wefound a wide range of possible manifestations of724this flaw ranging from an agglomeration of inco-herent factoids to well-structured sections that didnot exactly match the focus of the article, which isthe main reason for the low agreement.5 Selection of Reliable TrainingInstancesIndependent from the classification approach usedto identify flawed articles, reliable training data isthe most important prerequisite for good predic-tions.
On the one hand, we need a set of examplesthat reliably represent a particular flaw, while onthe other hand, we need counterexamples whichreliably represent articles that do not suffer fromthe same flaw.
The latter aspect is most impor-tant for discriminative classification approaches,since they rely on negative instances for trainingthe classifier.
However, reliable negative instancesare also important for one-class classification ap-proaches, since it is only for the counterexam-ples (or outliers) that the performance of one-classclassifiers can be sufficiently evaluated.
It is fur-thermore important that the positive and the neg-ative instances do not differ systematically in anyrespect other than the presence or absence of therespective flaws, since any systematic differencewill bias the classifier.
In this context, the topicalrestrictions of cleanup templates have to be takeninto account.
In the following, we describe ourapproach to extracting reliable training instancesfrom the quality flaw corpora.5.1 Reliable PositivesIn previous work, the latest available versions offlawed articles have been used as positive traininginstances.
However, we found upon manual in-spection of the data that a substantial number ofarticles has been significantly edited between thetime t?, at which the template was first assigned,and the time te, at which the articles have been ex-tracted.
Using the latest version at time te can thusinclude articles in which the respective flaw hasalready been fixed without removing the cleanuptemplate.
Therefore, we use the revision of the ar-ticle at time t?
to assure that the flaw is still presentin the training instance.We use the Wikipedia Revision Toolkit (Fer-schke et al, 2011), an enhancement of the JavaWikipedia Library, to gain access to the revisionhistory of each article.
For every article in the cor-pus of positive examples for flaw f that is markedwith template ?
?
T f , we backtrack the revisionhistory chronologically, until we find the first revi-sion rt?
?1 that is not tagged with ?
.
We then addthe succeeding revision rt?
to the corpus of reliablepositives for flaw f .
In Section 6, we show thatthe classification performance improves for mostflaws when using reliable positives instead of thelatest available article versions.5.2 Reliable Negatives and TopicalRestrictionA central problem of the quality flaw recognitionapproach is the fact that there are no articles avail-able that are tagged to not contain a particularquality problem.
So far, two solutions to this issuehave been proposed in related work.
Anderka et al(2012) tackle the problem with a one-class classi-fier that is trained on the positive instances alonethus eradicating the need for negative instances inthe training phase.
However, in order to evalu-ate the classifier, a set of outliers is needed.
Theauthors circumvent this issue by evaluating theirclassifiers on a set of random untagged instancesand a set of featured articles and argue that theactual performance of predicting the quality flawslies between the two.Ferretti et al (2012) follow a two step classifica-tion approach (PU learning) that first uses a NaiveBayes classifier trained on positive instances andrandom untagged articles to pre-classify the data.In a second phase, they use the negatives identi-fied by the Naive Bayes classifier to train a Sup-port Vector Machine that produces the final predic-tions.
Even though the Naive Bayes classifier wassupposed to identify reliable negatives, the authorsfound no significant improvement over a randomselection of negative instances, which effectivelyrenders the PU learning approach redundant.None of the above approaches consider theissue of topical restriction mentioned in Sec-tion 3, which introduces a systematic bias to thedata.
Both approaches sample random negative in-stances Arnd for any given set of flawed articles A ffrom a set of untagged articles Au (see Fig.
1a).In order to factor out the article topics as a ma-jor characteristic for distinguishing flawed articlesfrom the set of outliers, reliable negative instancesArel have to be sampled from the restricted topicset Atopic that contains articles with a topic dis-tribution similar to the flawed articles in A f (seeFig.
1b).
This will avoid the systematic bias and725(a) Random negatives (b) Reliable negativesFigure 1: Sampling of negative instances for a given set of flawed articles (A f ).
Random negatives (Arnd)are sampled from articles without any cleanup templates (Au).
Reliable negatives (Arel) are sampled fromthe set of articles (Atopic) with the same topic distribution as A fresult in a more realistic performance evaluation.In the following, we present our approachto extracting reliable negative training instancesthat conform with the topical restrictions of thecleanup templates.
Without loss of generality, weassume that an article, from which a cleanup tem-plate ?
?
T f is deleted at a point in time d?, thearticle no longer suffers from flaw f at that pointin time.
Thus, the revision rd?
is a reliable negativeinstance for the flaw f .
Additionally, since the ar-ticle was once tagged with ?
?
T f , it belongs to thethe same restricted topic set Atopic as the positiveinstances for flaw f .We use the Apache Hadoop9 framework andWikiHadoop10, an input format for WikipediaXML dumps, for crawling the whole revision his-tory of the English Wikipedia on a compute clus-ter.
WikiHadoop allows each Hadoop mapper toreceive adjacent revision pairs, which makes itpossible to compare the changes made from onerevision to the next.
For every template ?
?
T f ,we extract all adjacent revision pairs (rd?
?1, rd?
), inwhich the first revision contains ?
and the seconddoes not contain ?.
Since there are occasions inwhich a template is replaced by another templatefrom the same cluster, we ensure that rd?
does alsonot contain any other template from cluster T f be-fore we finally add the revision to the set of reli-able negatives for flaw f .In the remainder of this section, we evaluate thetopical similarity between the positive and the neg-ative set of articles for each flaw using both ourmethod and the original approach.
In Wikipedia,9http://hadoop.apache.org10https://github.com/whym/wikihadoopthe topic of an article is captured by the categoriesassigned to it.
In order to compare two sets of arti-cles with respect to their topical similarity, we rep-resent each article set as a category frequency vec-tor.
Formally, we calculate for each set the vector~C = (wc1 ,wc2 , .
.
.
,wcn) with wci being the weightof category ci, i.e.
the number of times it occurs inthe set, and n being the total number of categoriesin Wikipedia.
We can then estimate the topicalsimilarity of two article sets by calculating the co-sine similarity of their category frequency vectors~C1 B A and ~C2 B B assim(A, B) = A ?
B?A?
?B?
=n?i=1Ai ?
Bi?
n?i=1(Ai)2 ??
n?i=1(Bi)2Table 3 gives an overview of the similarityscores between each positive training set and thecorresponding reliable negative set as well as be-tween each positive set and a random set of un-tagged articles.
We can see that the topics of arti-cles in the positive training sets are highly similarto the topics of the corresponding reliable negativearticles while they show little similarity to the ar-ticles in the random set.
This implies that the sys-tematic bias introduced by the topical restrictionhas largely been eradicated by our approach.Individual flaws have differently strong topicalrestrictions.
The strength of this restriction de-pends on the size of Atopic.
That is, a flaw such asin-universe is restricted to a very narrow selectionof articles, while a flaw such as copy edit can beapplied to most articles and rather shows a topicalpreference due to reasons outlined in Section 3.
It726Cosine SimilarityFlaw (A f , Arel) (A f , Arnd)Advert .996 .118Confusing .996 .084Copy-edit .993 .197Essay-like .996 .132Globalize .992 .023In-universe .996 .014Peacock .995 .310POV .994 .252Technical .995 .018Tone .996 .228Trivia .980 .184Weasel .976 .252Table 3: Cosine similarity scores between the cat-egory frequency vectors of the flawed article setsand the respective random or reliable negativesis therefore to be expected that that flaws with asmall Atopic are more prone to the topic bias.6 ExperimentsIn the following, we describe our system architec-ture and the setup of our experiments.
Our systemfor quality flaw detection follows the approach byFerschke et al (2012b), since it has been particu-larly designed as a modular system based on theUnstructured Information Management Architec-ture11, which makes it easy to extend.
Insteadof using Mallet (McCallum, 2002) as a machinelearning toolkit, we employ the Weka Data Min-ing Software (Hall et al, 2009) for classification,since it offers a wider range of state-of-the-art ma-chine learning algorithms.
For each of the 12 qual-ity flaws, we employ three different dataset config-urations.
The BASE configuration uses the newestversion of each flawed article as positive instancesand a random set of untagged articles as negativeinstances.
The RELP configuration uses reliablepositives, as described in Section 5.1, in combi-nation with random outliers.
Finally, the RELALLconfiguration employs reliable positives in com-bination with the respective reliable negatives asdescribed in Section 5.2.FeaturesAn extensive survey of features for quality flawrecognition has been provided by Anderka et al(2012).
We selected a subset of these features forour experiments and grouped them into four fea-ture sets in order to determine how well differ-ent combinations of features perform in the task.11http://uima.apache.orgCategory Feature type NONGRAMNGRAMNOWIKIALLLexical Article ngrams ?
?
?Info to noise ratio ?
?
?Network # External links ?
?# Outlinks ?
?# Outlinks per sentence ?
?# Language links ?
?References Has reference list ?
?# References ?
?# References per sentence ?
?Revision # Revisions ?
?# Unique contributors ?
?Structure # Empty sections ?
?Mean section size ?
?# Sections ?
?# Lists ?
?Question rate ?
?
?Readability ARI ?
?
?Coleman-Liau ?
?
?Flesch ?
?
?Flesch-Kincaid ?
?
?Gunning Fog ?
?
?Lix ?
?
?SMOG-Grading ?
?
?NamedEntity# Person entities?
?
?
?# Organization entities?
?
?
?# Location entities?
?
?
?Misc # Characters ?
?
?# Sentences ?
?
?# Tokens ?
?
?Average sentence length ?
?
?Article lead length ?
?Lead to article ratio ?
?# Discussions ?
??
newly introduced feature# number of instancesTable 4: Feature sets used in the experimentsTable 4 lists all feature types used in our experi-ments.Since the feature space becomes large due to thengram features, we prune it in two steps.
First,we filter the ngrams according to their documentfrequency in the training corpus.
We discard allngrams that occur in less than x% and more thany% of all documents.
Several values for x andy have been evaluated in parameter tuning ex-periments.
The best results have been achievedwith x=2 and y=90.
In a second step, we applythe Information Gain feature selection approach(Mitchell, 1997) to the remaining set to determinethe most useful features.Learning AlgorithmsWe evaluated several learning algorithms from theWeka toolkit with respect to their performance on727Algorithm Average F1SVM RBF Kernel 0.82AdaBoost (decision stumps) 0.80SVM Poly Kernel 0.79RBF Network 0.78SVM Linear Kernel 0.77SVM PUK Kernel 0.76J48 0.75Naive Bayes 0.72MultiBoostAB (decision stumps) 0.71Logistic Regression 0.60LibSVM One Class 0.67Table 5: Average F1-scores over all flaws on RELPusing all featuresthe quality flaw recognition task.
Table 5 showsthe average F1-score of each algorithm on theRELP dataset using all features.
The performancehas been evaluated with 10-fold cross validationon 2,000 documents split equally into positiveand negative instances.
One class classifiers aretrained on the positive instances alone.
We deter-mined the best parameters for each algorithms ina parameter optimization run and list the results ofthe best configuration.Overall, Support Vector Machines with RBFkernels yielded the best average results and out-performed the other algorithms on every flaw.
Weused a sequential minimal optimization (SMO) al-gorithm (Platt, 1998) to train the SVMs and useddifferent ?-values for the RBF kernel function.
Incontrast to Ferretti et al (2012), we did not see sig-nificant improvements when optimizing ?
for eachindividual flaw, so we determined one best settingfor each dataset.
Since SVMs with RBF kernelsare a special case of RBF networks that fit a sin-gle basis function to the data, we also used gen-eral RBF networks that can employ multiple ba-sis functions, but we did not achieve better resultswith that approach.One-class classification, as proposed by An-derka et al (2012), did not perform well withinour setup.
Even though we used an out-of-the-box one class classifier, we achieve similar re-sults as Anderka et al in their pessimistic setting,which best resembles our configuration.
However,the performance still lacks behind the other ap-proaches in our experiments.
The best perform-ing algorithm reported by Ferschke et al (2012b),AdaBoost with decision stumps as a weak learner,showed the second best results in our experiments.7 Evaluation and DiscussionThe SVMs achieve a similar cross-validated per-formance on all feature sets containing ngrams,showing only minor improvements for individ-ual flaws when adding non-lexical features.
Thissuggests that the classifiers largely depend onthe ngrams and that other features do not con-tribute significantly to the classification perfor-mance.
While structural quality flaws can bewell captured by special purpose features or in-tensional modeling, as related work has shown,more subtle content flaws such as the neutralityand style flaws are mainly captured by the word-ing itself.
Textual features beyond the ngram level,such as syntactic and semantic qualities of thetext, could further improve the classification per-formance of these flaws and should be addressedin future work.
Table 6 shows the performance ofthe SVMs with RBF kernel12 on each dataset us-ing the NGRAM feature set.
The average perfor-mance based on NOWIKI is slightly lower whileusing ALL features results in slightly higher aver-age F1-scores.
However, the differences are notstatistically significant and thus omitted.
Classi-fiers using the NONGRAM feature set achieved av-erage F1-scores below 0.50 on all datasets.
Theresults have been obtained by 10-fold cross vali-dation on 2,000 documents per flaw.The classifiers trained on reliable positives andrandom untagged articles (RELP) outperform therespective classifiers based on the BASE datasetfor most flaws.
This confirms our original hy-pothesis that using the appropriate revision of eachtagged article is superior to using the latest avail-able version from the dump.
The performance onthe RELALL dataset, in which the topic bias hasbeen factored out, yields lower F1-scores than thetwo other approaches.
Flaws that are restricted toa very narrow set of topics (i.e.
Atopic in Fig.
1bis small), such as the in-universe flaw, show thebiggest drop in performance.
Since the topicbias plays a major role in the quality flaw de-tection task, as we have shown earlier, the topic-controlled classifier cannot take advantage of thetopic information, while the classifiers trained onthe other corpora can make use of these charac-teristic as the most discriminative features.
In theRELALL setting, however, the differences betweenthe positive and negative instances are largely de-termined by the flaws alone.
Classifiers trained on12?=0.01 for BASE,RELP and ?=0.001 for RELALL728such a dataset therefore come closer to recogniz-ing the actual quality flaws, which makes themmore useful in a practical setting despite lowercross-validated scores.In addition to cross-validation, we performed across-corpus evaluation of the classifiers for eachflaw.
Therefore, we evaluated the performance ofthe unbiased classifiers (trained on RELALL) onthe biased data (RELP) and vice versa.
Hereby,the positive training and test instances remain thesame in both settings, while the unbiased data con-tains negative instances sampled from Arel and theunbiased data from Arnd (see Figure 1).
With theNGRAM feature set, the reliable classifiers outper-formed the unreliable classifiers on all flaws thatcan be well identified with lexical cues, such asAdvert or Technical.
In the biased case, we foundboth topic related and flaw specific ngrams amongthe most highly ranked ngram features.
In the un-biased case, most of the informative ngrams wereflaw specific expressions.
Consequently, biasedclassifiers fail on the unbiased dataset in whichthe positive and negative class are sampled fromthe same topics, which renders the highly rankedtopic ngrams unusable.
Flaws that do not largelyrely on lexical cues, however, cannot be predictedmore reliably with the unbiased classifier.
Thismeans that additional features are needed to de-scribe these flaw.
We tested this hypothesis by us-ing the full feature set ALL and saw a substantialimprovement on the side of the unbiased classifier,while the performance of the biased classifier re-mained unchanged.A direct comparison of our results to relatedwork is difficult, since neutrality and style flawshave not been targeted before in a similar manner.However, the Advert flaw was also part of the tenflaw types in the PAN Quality Flaw RecognitionTask (Anderka and Stein, 2012b).
The best systemachieved an F1 score of 0.839, which is just be-low the results of our system on the BASE dataset,which is similar to the PAN setup.8 ConclusionsWe showed that text classification based onWikipedia cleanup templates is prone to a topicbias which causes skewed classifiers and overlyoptimistic cross-validated evaluation results.
Thisbias is known from other text classification appli-cations, such as authorship attribution, genre de-tection and native language detection.
We demon-Flaw BASE RELP RELALLAdvert .86 .88 .75Confusing .76 .80 .70Copy edit .81 .73 .72Essay-like .79 .83 .64Globalize .85 .87 .69In-universe .96 .96 .69Peacock .77 .82 .69POV .75 .80 .71Technical .87 .88 .67Tone .70 .79 .69Trivia .72 .77 .70Weasel .69 .77 .72.79 .83 .70Table 6: F1 scores for the 10-fold cross validationof the SVMs with RBF kernel on all datasets usingNGRAM featuresstrated how to avoid the topic bias when creat-ing quality flaw corpora.
Unbiased corpora arenot only necessary for training unbiased classi-fiers, they are also invaluable resources for gaininga deeper understanding of the linguistic propertiesof the flaws.
Unbiased classifiers reflect much bet-ter the performance of quality flaw recognition ?inthe wild?, because they detect actual flawed ar-ticles rather than identifying the articles that areprone to certain quality due to their topic or subjectmatter.
In our experiments, we presented a systemfor identifying Wikipedia articles with style andneutrality flaws, a novel category of quality prob-lems that is of particular importance within andoutside of Wikipedia.
We showed that selectinga reliable set of positive training instances minedfrom the revision history improves the classifica-tion performance.
In future work, we aim to ex-tend our quality flaw detection system to not onlyfind articles that contain a particular flaw, but alsoto identify the flaws within the articles, which canbe achieved by leveraging the positional informa-tion of in-line cleanup templates.AcknowledgmentsThis work has been supported by the Volks-wagen Foundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806,and by the Hessian research excellence pro-gram ?Landes-Offensive zur EntwicklungWissenschaftlich-O?konomischer Exzellenz?
(LOEWE) as part of the research center ?DigitalHumanities?.729ReferencesMaik Anderka and Benno Stein.
2012a.
A Break-down of Quality Flaws in Wikipedia.
In 2nd JointWICOW/AIRWeb Workshop on Web Quality, pages11?18, Lyon, France.Maik Anderka and Benno Stein.
2012b.
Overview ofthe 1st International Competition on Quality FlawPrediction in Wikipedia.
In CLEF 2012 EvaluationLabs and Workshop ?
Working Notes Papers.Maik Anderka, Benno Stein, and Nedim Lipka.
2012.Predicting Quality Flaws in User-generated Content:The Case of Wikipedia.
In 35th International ACMConference on Research and Development in Infor-mation Retrieval, Portland, OR, USA.Julian Brooke and Graeme Hirst.
2011.
Native lan-guage detection with ?cheap?
learner corpora.
InLearner Corpus Research 2011 (LCR 2011).Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 shared task: learning to detect hedges and theirscope in natural language text.
In Proceedings ofthe Fourteenth Conference on Computational Natu-ral Language Learning, CoNLL ?10: Shared Task,pages 1?12, Stroudsburg, PA, USA.
Association forComputational Linguistics.Edgardo Ferretti, Donato Herna?ndez Fusilier, RafaelGuzma?n-Cabrera, Manuel Montes y Go?mez,Marcelo Errecalde, and Paolo Rosso.
2012.
On theUse of PU Learning for Quality Flaw Predictionin Wikipedia.
In CLEF 2012 Evaluation Labs andWorkshop ?
Working Notes Papers.Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.2011.
Wikipedia Revision Toolkit: Efficiently Ac-cessing Wikipedia?s Edit History.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies.
System Demonstrations, pages 97?102, Port-land, OR, USA.Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-tar.
2012a.
Behind the Article: Recognizing DialogActs in Wikipedia Talk Pages.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 777?786, Avignon, France.Oliver Ferschke, Iryna Gurevych, and Marc Rittberger.2012b.
FlawFinder: A Modular System for Pre-dicting Quality Flaws in Wikipedia.
In CLEF 2012Evaluation Labs and Workshop ?
Working Notes Pa-pers, Rome, Italy.Aidan Finn and Nicholas Kushmerick.
2006.
Learningto classify documents according to genre.
Journalof the American Society for Information Science andTechnology, 57(11):1506?1518.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11(1):10?18.Moshe Koppel and Jonathan Schler.
2003.
Exploit-ing stylistic idiosyncrasies for authorship attribution.In Workshop on Computational Approaches to StyleAnalysis and Synthesis, pages 69?72.K.
Luyckx and W. Daelemans.
2005.
Shallow textanalysis and machine learning for authorship attri-bution.
In Proceedings of the Fifteenth Meeting ofComputational Linguistics in the Netherlands (CLIN2004), pages 149?160.Andrew Kachites McCallum.
2002.
MALLET: A Ma-chine Learning for Language Toolkit.George K. Mikros and Eleni K. Argiri.
2007.
Inves-tigating topic influence in authorship attribution.
InProceedings of the SIGIR 2007 International Work-shop on Plagiarism Analysis, Authorship Identifica-tion, and Near-Duplicate Detection, PAN 2007, Am-sterdam, Netherlands.Thomas Mitchell.
1997.
Machine Learning.
McGraw-Hill Education, New York, NY, USA, 1st edition.John C Platt.
1998.
Fast training of support vectormachines using sequential minimal optimization.
InAdvances in Kernel Methods: Support Vector Learn-ing, pages 185?208, Cambridge, MA, USA.Besiki Stvilia, Michael B. Twidale, Linda C. Smith,and Les Gasser.
2008.
Information Quality WorkOrganization in Wikipedia.
Journal of the Ameri-can Society for Information Science and Technology,59(6):983?1001.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,Gyo?rgy Mo?ra, and Iryna Gurevych.
2012.
Cross-genre and cross-domain detection of semantic un-certainty.
Comput.
Linguist., 38(2):335?367.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting Lexical Semantic Knowledgefrom Wikipedia and Wiktionary.
In Proceedings ofthe 6th International Conference on Language Re-sources and Evaluation, Marrakech, Morocco.730
