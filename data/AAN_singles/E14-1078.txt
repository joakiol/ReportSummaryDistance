Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742?751,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsLearning part-of-speech taggers with inter-annotator agreement lossBarbara Plank, Dirk Hovy, Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140, DK-2300 Copenhagen Sbplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dkAbstractIn natural language processing (NLP) an-notation projects, we use inter-annotatoragreement measures and annotation guide-lines to ensure consistent annotations.However, annotation guidelines oftenmake linguistically debatable and evensomewhat arbitrary decisions, and inter-annotator agreement is often less thanperfect.
While annotation projects usu-ally specify how to deal with linguisti-cally debatable phenomena, annotator dis-agreements typically still stem from these?hard?
cases.
This indicates that some er-rors are more debatable than others.
In thispaper, we use small samples of doubly-annotated part-of-speech (POS) data forTwitter to estimate annotation reliabilityand show how those metrics of likely inter-annotator agreement can be implementedin the loss functions of POS taggers.
Wefind that these cost-sensitive algorithmsperform better across annotation projectsand, more surprisingly, even on data an-notated according to the same guidelines.Finally, we show that POS tagging mod-els sensitive to inter-annotator agreementperform better on the downstream task ofchunking.1 IntroductionPOS-annotated corpora and treebanks are collec-tions of sentences analyzed by linguists accord-ing to some linguistic theory.
The specific choiceof linguistic theory has dramatic effects on down-stream performance in NLP tasks that rely on syn-tactic features (Elming et al., 2013).
Variationacross annotated corpora in linguistic theory alsoposes challenges to intrinsic evaluation (Schwartzet al., 2011; Tsarfaty et al., 2012), as well asfor languages where available resources are mu-tually inconsistent (Johansson, 2013).
Unfortu-nately, there is no grand unifying linguistic the-ory of how to analyze the structure of sentences.While linguists agree on certain things, there isstill a wide range of unresolved questions.
Con-sider the following sentence:(1) @GaryMurphyDCU of @DemMattersIRLwill take part in a panel discussion on Octo-ber 10th re the aftermath of #seanref .
.
.While linguists will agree that in is a preposi-tion, and panel discussion a compound noun, theyare likely to disagree whether will is heading themain verb take or vice versa.
Even at a more basiclevel of analysis, it is not completely clear how toassign POS tags to each word in this sentence: ispart a particle or a noun; is 10th a numeral or anoun?Some linguistic controversies may be resolvedby changing the vocabulary of linguistic theory,e.g., by leaving out numerals or introducing adhoc parts of speech, e.g.
for English to (Marcuset al., 1993) or words ending in -ing (Manning,2011).
However, standardized label sets havepractical advantages in NLP (Zeman and Resnik,2008; Zeman, 2010; Das and Petrov, 2011; Petrovet al., 2012; McDonald et al., 2013).For these and other reasons, our annotators(even when they are trained linguists) often dis-agree on how to analyze sentences.
The strategyin most previous work in NLP has been to monitorand later resolve disagreements, so that the finallabels are assumed to be reliable when used as in-put to machine learning models.Our approachInstead of glossing over those annotation disagree-ments, we consider what happens if we embracethe uncertainty exhibited by human annotators742when learning predictive models from the anno-tated data.To achieve this, we incorporate the uncertaintyexhibited by annotators in the training of ourmodel.
We measure inter-annotator agreement onsmall samples of data, then incorporate this in theloss function of a structured learner to reflect theconfidence we can put in the annotations.
Thisprovides us with cost-sensitive online learning al-gorithms for inducing models from annotated datathat take inter-annotator agreement into consider-ation.Specifically, we use online structured percep-tron with drop-out, which has previously been ap-plied to POS tagging and is known to be robustacross samples and domains (S?gaard, 2013a).
Weincorporate the inter-annotator agreement in theloss function either as inter-annotator F1-scoresor as the confusion probability between annota-tors (see Section 3 below for a more detailed de-scription).
We use a small amounts of doubly-annotated Twitter data to estimate F1-scores andconfusion probabilities, and incorporate them dur-ing training via a modified loss function.
Specif-ically, we use POS annotations made by two an-notators on a set of 500 newly sampled tweetsto estimate our agreement scores, and train mod-els on existing Twitter data sets (described be-low).
We evaluate the effect of our modifiedtraining by measuring intrinsic as well as down-stream performance of the resulting models on twotasks, namely named entity recognition (NER) andchunking, which both use POS tags as input fea-tures.2 POS-annotated Twitter data setsThe vast majority of POS-annotated resourcesacross languages contain mostly newswire text.Some annotated Twitter data sets do exist for En-glish.
Ritter et al.
(2011) present a manually an-notated data set of 16 thousand tokens.
Theydo not report inter-annotator agreement.
Gimpelet al.
(2011) annotated about 26 thousand tokensand report a raw agreement of 92%.
Foster etal.
(2011) annotated smaller portions of data forcross-domain evaluation purposes.
We refer to thedata as RITTER, GIMPEL and FOSTER below.In our experiments, we use the RITTER splitsprovided by Derczynski et al.
(2013), and theOctober splits of the GIMPEL data set, version0.3.
We train our models on the concatenation ofRITTER-TRAIN and GIMPEL-TRAIN and evaluatethem on the remaining data, the dev and test setprovided by Foster et al.
(2011) as well as an in-house annotated data set of 3k tokens (see below).The three annotation efforts (Ritter et al., 2011;Gimpel et al., 2011; Foster et al., 2011) all useddifferent tagsets, however, and they also differ intokenization, as well as a wide range of linguisticdecisions.
We mapped all the three corpora to theuniversal tagset provided by Petrov et al.
(2012)and used the same dummy symbols for numbers,URLs, etc., in all the data sets.
Following (Fos-ter et al., 2011), we consider URLs, usernamesand hashtags as NOUN.
We did not change the tok-enization.The data sets differ in how they analyze many ofthe linguistically hard cases.
Consider, for exam-ple, the analysis of will you come out to in GIM-PEL and RITTER (Figure 1, top).
While Gimpelet al.
(2011) tag out and to as adpositions, Ritteret al.
(2011) consider them particles.
What is theright analysis depends on the compositionality ofthe construction and the linguistic theory one sub-scribes to.Other differences include the analysis of abbre-viations (PRT in GIMPEL; X in RITTER and FOS-TER), colon (X in GIMPEL; punctuation in RIT-TER and FOSTER), and emoticons, which can takemultiple parts of speech in GIMPEL, but are al-ways X in RITTER, while they are absent in FOS-TER.
GIMPEL-TRAIN and RITTER-TRAIN arealso internally inconsistent.
See the bottom of Fig-ure 1 for examples and Hovy et al.
(2014) for amore detailed discussion on differences betweenthe data sets.Since the mapping to universal tags couldpotentially introduce errors, we also annotateda data set directly using universal tags.
Werandomly selected 200 tweets collected over thespan of one day, and had three annotators tagthis set.
We split the data in such a way thateach annotator had 100 tweets: two annotatorshad disjoint sets, the third overlapped 50 itemswith each of the two others.
In this way, weobtained an initial set of 100 doubly-annotatedtweets.
The annotators were not provided withannotation guidelines.
After the first round ofannotations, we achieved a raw agreement of0.9, a Cohen?s ?
of 0.87, and a Krippendorff?s?
of 0.87.
We did one pass over the data toadjudicate the cases where annotators disagreed,743.
.
.will you come out to the.
.
.GIMPEL VERB PRON VERB ADP ADP DETRITTER VERB PRON VERB PRT PRT DETRITTER.
.
.you/PRON come/VERB out/PRT to/PRT.
.
.it/PRON comes/VERB out/ADP nov/NOUNGIMPEL.
.
.Advances/NOUN and/CONJ Social/NOUN Media/NOUN .../X.
.
.Journalists/NOUN and/CONJ Social/ADJ Media/NOUN experts/NOUNFigure 1: Annotation differences between (top) and within (bottom) two available Twitter POS data sets.or where they had flagged their choice as debat-able.
The final data set (lowlands.test),referred below to as INHOUSE, contained 3,064tokens (200 tweets) and is publicly availableat http://bitbucket.org/lowlands/costsensitive-data/, along with the dataused to compute inter-annotator agreement scoresfor learning cost-sensitive taggers, described inthe next section.3 Computing agreement scoresGimpel et al.
(2011) used 72 doubly-annotatedtweets to estimate inter-annotator agreement, andwe also use doubly-annotated data to computeagreement scores.
We randomly sampled 500tweets for this purpose.
Each tweet was anno-tated by two annotators, again using the univer-sal tag set (Petrov et al., 2012).
All annotatorswere encouraged to use their own best judgmentrather than following guidelines or discussing dif-ficult cases with each other.
This is in contrast toGimpel et al.
(2011), who used annotation guide-lines.
The average inter-annotator agreement was0.88 for raw agreement, and 0.84 for Cohen?s ?.Gimpel et al.
(2011) report a raw agreement of0.92.We use two metrics to provide a more detailedpicture of inter-annotator agreement, namelyF1-scores between annotators on individual partsof speech, and tag confusion probabilities, whichwe derive from confusion matrices.The F1-score relates to precision and recallin the usual way, i.e, as the harmonic meanbetween those two measure.
In more detail, giventwo annotators A1and A2, we say the precisionFigure 2: Inter-annotator F1-scores estimatedfrom 500 tweets.of A1relative to A2with respect to POS tag T insome data setX , denoted PrecT(A1(X), A2(X)),is the number of tokens both A1and A2predict tobe T over the number of times A1predicts a tokento be T .
Similarly, we define the recall with re-spect to some tag T , i.e., RecT(A1(X), A2(X)),as the number of tokens both A1and A2predictto be T over the number of times A2predictsa token to be T .
The only difference withrespect to standard precision and recall is thatthe gold standard is replaced by a second anno-tator, A2.
Note that PrecT(A1(X), A2(X)) =RecT(A2(X), A1(X)).
It follows from all ofthe above that the F1-score is symmetrical, i.e.,F1T(A1(X), A2(X)) = F1T(A2(X), A1(X)).The inter-annotator F1-scores over the 12POS tags in the universal tagset are presented inFigure 2.
It shows that there is a high agreementfor nouns, verbs and punctuation, while the agree-744Figure 3: Confusion matrix of POS tags obtainedfrom 500 doubly-annotated tweets.ment is low, for instance, for particles, numeralsand the X tag.We compute tag confusion probabilitiesfrom a confusion matrix over POS tags likethe one in Figure 3.
From such a matrix,we compute the probability of confusingtwo tags t1and t2for some data point x,i.e.
P ({A1(x), A2(x)} = {t1, t2}) as themean of P (A1(x) = t1, A2(x) = t2) andP (A1(x) = t2, A2(x) = t1), e.g., the confusionprobability of two tags is the mean of the prob-ability that annotator A1assigns one tag and A2another, and vice versa.We experiment with both agreement scores (F1and confusion matrix probabilities) to augment theloss function in our learner.
The next section de-scribes this modification in detail.4 Inter-annotator agreement lossWe briefly introduce the cost-sensitive perceptronclassifier.
Consider the weighted perceptron losson our ith example ?xi, yi?
(with learning rate ?
=1), Lw(?xi, yi?):?
(sign(w ?
xi), yi) max(0,?yiw ?
xi)In a non-cost-sensitive classifier, the weightfunction ?
(yj, yi) = 1 for 1 ?
i ?
N .
The1: X = {?xi, yi?
}Ni=1with xi= ?x1i, .
.
.
, xmi?2: I iterations3: w = ?0?m4: for iter ?
I do5: for 1 ?
i ?
N do6: y?
= arg maxy?Yw ?
?
(xi, y)7: w?
w+ ?
(y?, yi)[?
(xi, yi)??
(xi, y?
)]8: w?+ = w9: end for10: end for11: return w?/ = (N ?
I)Figure 4: Cost-sensitive structured perceptron (seeSection 3 for weight functions ?
).two cost-sensitive systems proposed only differ inhow we formulate ?
(?, ?).
In one model, the loss isweighted by the inter-annotator F1 of the gold tagin question.
This boils down to?
(yj, yi) = F1yi(A1(X), A2(X))where X is the small sample of held-out data usedto estimate inter-annotator agreement.
Note thatin this formulation, the predicted label is not takeninto consideration.The second model is slightly more expressiveand takes both the gold and predicted tags into ac-count.
It basically weights the loss by how likelythe gold and predicted tag are to be mistaken foreach other, i.e., (the inverse of) their confusionprobability:?
(yj, yi)) = 1?P ({A1(X), A2(X)} = {yj, yi})In both loss functions, a lower gamma valuemeans that the tags are more likely to be confusedby a pair of annotators.
In this case, the update issmaller.
In contrast, the learner incurs greater losswhen easy tags are confused.It is straight-forward to extend these cost-sensitive loss functions to the structured percep-tron (Collins, 2002).
In Figure 4, we provide thepseudocode for the cost-sensitive structured onlinelearning algorithm.
We refer to the cost-sensitivestructured learners as F1- and CM-weighted be-low.5 ExperimentsIn our main experiments, we use structured per-ceptron (Collins, 2002) with random corruptions745using a drop-out rate of 0.1 for regularization, fol-lowing S?gaard (2013a).
We use the LXMLStoolkit implementation1with default parameters.We present learning curves across iterations, andonly set parameters using held-out data for ourdownstream experiments.25.1 ResultsOur results are presented in Figure 5.
The top leftgraph plots accuracy on the training data per iter-ation.
We see that CM-weighting does not hurttraining data accuracy.
The reason may be thatthe cost-sensitive learner does not try (as hard) tooptimize performance on inconsistent annotations.The next two plots (upper mid and upper right)show accuracy over epochs on in-sample evalua-tion data, i.e., GIMPEL-DEV and RITTER-TEST.Again, the CM-weighted learner performs betterthan our baseline model, while the F1-weightedlearner performs much worse.The interesting results are the evaluations onout-of-sample evaluation data sets (FOSTER andIN-HOUSE) - lower part of Figure 5.
Here, bothour learners are competitive, but overall it is clearthat the CM-weighted learner performs best.
Itconsistently improves over the baseline and F1-weighting.
The former is much more expressiveas it takes confusion probabilities into account anddoes not only update based on gold-label uncer-tainty, as is the case with the F1-weighted learner.5.2 Robustness across regularizersDiscriminative learning typically benefits fromregularization to prevent overfitting.
The simplestis the averaged perceptron, but various other meth-ods have been suggested in the literature.We use structured perceptron with drop-out, butresults are relatively robust across other regular-ization methods.
Drop-out works by randomlydropping a fraction of the active features in eachiteration, thus preventing overfitting.
Table 1shows the results for using different regularizers,in particular, Zipfian corruptions (S?gaard, 2013b)and averaging.
While there are minor differencesacross data sets and regularizers, we observe thatthe corresponding cell using the loss function sug-gested in this paper (CM) always performs betterthan the baseline method.1https://github.com/gracaninja/lxmls-toolkit/2In this case, we use FOSTER-DEV as our developmentdata to avoid in-sample bias.6 Downstream evaluationWe have seen that our POS tagging model im-proves over the baseline model on three out-of-sample test sets.
The question remains whethertraining a POS tagger that takes inter-annotatoragreement scores into consideration is also effec-tive on downstream tasks.
Therefore, we eval-uate our best model, the CM-weighted learner,in two downstream tasks: shallow parsing?alsoknown as chunking?and named entity recogni-tion (NER).For the downstream evaluation, we used thebaseline and CM models trained over 13 epochs,as they performed best on FOSTER-DEV (cf.
Fig-ure 5).
Thus, parameters were optimized only onPOS tagging data, not on the downstream evalu-ation tasks.
We use a publicly available imple-mentation of conditional random fields (Laffertyet al., 2001)3for the chunking and NER exper-iments, and provide the POS tags from our CMlearner as features.6.1 ChunkingThe set of features for chunking include informa-tion from tokens and POS tags, following Sha andPereira (2003).We train the chunker on Twitter data (Ritter etal., 2011), more specifically, the 70/30 train/testsplit provided by Derczynski et al.
(2013) for POStagging, as the original authors performed crossvalidation.
We train on the 70% Twitter data (11ktokens) and evaluate on the remaining 30%, aswell as on the test data from Foster et al.
(2011).The FOSTER data was originally annotated forPOS and constituency tree information.
We con-verted it to chunks using publicly available conver-sion software.4Part-of-speech tags are the onesassigned by our cost-sensitive (CM) POS modeltrained on Twitter data, the concatenation of Gim-pel and 70% Ritter training data.
We did not in-clude the CoNLL 2000 training data (newswiretext), since adding it did not substantially improvechunking performance on tweets, as also shownin (Ritter et al., 2011).The results for chunking are given in Ta-ble 2.
They show that using the POS taggingmodel (CM) trained to be more sensitive to inter-annotator agreement improves performance over3http://crfpp.googlecode.com4http://ilk.uvt.nl/team/sabine/homepage/software.html7465 10 15 20 25Epochs747576777879808182Accuracy(%)TRAININGBASELINEF1CM5 10 15 20 25Epochs77.578.078.579.079.580.080.5Accuracy(%)GIMPEL-DEVBASELINEF1CM5 10 15 20 25Epochs83.584.084.585.085.586.086.587.0Accuracy(%)RITTER-TESTBASELINEF1CM5 10 15 20 25Epochs81.081.582.082.583.083.584.0Accuracy(%)FOSTER-DEVBASELINEF1CM5 10 15 20 25Epochs82.583.083.584.084.585.0Accuracy(%)FOSTER-TESTBASELINEF1CM5 10 15 20 25Epochs82.282.482.682.883.083.283.483.683.884.0Accuracy(%)IN-HOUSEBASELINEF1CMFigure 5: POS accuracy for the three models: baseline, confusion matrix loss (CM) and F1-weighted(F1) loss for increased number of training epochs.
Top row: in-sample accuracy on training (left) andin-sample evaluation datasets (center, right).
Bottom row: out-of-sample accuracy on various data sets.CM is robust on both in-sample and out-of-sample data.RITTER-TESTF1: All NP VP PPBL 76.20 78.61 74.25 86.79CM 76.42 79.07 74.98 86.19FOSTER-TESTF1: All NP VP PPBL 68.49 70.73 60.56 86.50CM 68.97 71.25 61.97 87.24Table 2: Downstream results on chunking.
OverallF1 score (All) as well as F1 for NP, VP and PP.the baseline (BL) for the downstream task ofchunking.
Overall chunking F1 score improves.More importantly, we report on individual scoresfor NP, VP and PP chunks, where we see consis-tent improvements for NPs and VPs (since bothnouns and verbs have high inter-annotator agree-ment), while results on PP are mixed.
This is tobe expected, since PP phrases involve adposition-als (ADP) that are often confused with particles(PRT), cf.
Figure 3.
Our tagger has been trainedto deliberately abstract away from such uncertaincases.
The results show that taking uncertainty inPOS annotations into consideration during train-ing has a positive effect in downstream results.
Itis thus better if we do not try to urge our modelsto make a firm decision on phenomena that neither747BASELINE CMRegularizer FOSTER-DEV FOSTER-TEST IN-HOUSE FOSTER-DEV FOSTER-TEST IN-HOUSEAveraging 0.827 0.837 0.830 0.831 0.844 0.833Drop-out 0.827 0.838 0.827 0.836 0.843 0.833Zipfian 0.821 0.835 0.833 0.825 0.838 0.836Table 1: Results across regularizers (after 13 epochs).linguistic theories nor annotators do agree upon.6.2 NERIn the previous section, we saw positive effects ofcost-sensitive POS tagging for chunking, and herewe evaluate it on another downstream task, NER.For the named entity recognition setup, we usecommonly used features, in particular featuresfor word tokens, orthographic features like thepresence of hyphens, digits, single quotes, up-per/lowercase, 3 character prefix and suffix infor-mation.
Moreover, we add Brown word clusterfeatures that use 2,4,6,8,..,16 bitstring prefixes es-timated from a large Twitter corpus (Owoputi etal., 2013).5For NER, we do not have access to carefullyannotated Twitter data for training, but rely onthe crowdsourced annotations described in Fininet al.
(2010).
We use the concatenation of theCoNLL 2003 training split of annotated data fromthe Reuters corpus and the Finin data for training,as in this case training on the union resulted in amodel that is substantially better than training onany of the individual data sets.
For evaluation, wehave three Twitter data set.
We use the recentlypublished data set from the MSM 2013 challenge(29k tokens)6, the data set of Ritter et al.
(2011)used also by Fromheide et al.
(2014) (46k tokens),as well as an in-house annotated data set (20k to-kens) (Fromheide et al., 2014).F1: RITTER MSM IN-HOUSEBL 78.20 82.25 82.58CM 78.30 82.00 82.77Table 3: Downstream results for named entityrecognition (F1 scores).Table 3 shows the result of using our POS mod-els in downstream NER evaluation.
Here we ob-serve mixed results.
The cost-sensitive model is5http://www.ark.cs.cmu.edu/TweetNLP/6http://oak.dcs.shef.ac.uk/msm2013/ie_challenge/able to improve performance on two out of thethree test sets, while being slightly below baselineperformance on the MSM challenge data.
Notethat in contrast to chunking, POS tags are just oneof the many features used for NER (albeit an im-portant one), which might be part of the reasonwhy the picture looks slightly different from whatwe observed above on chunking.7 Related workCost-sensitive learning takes costs, such as mis-classification cost, into consideration.
That is,each instance that is not classified correctly duringthe learning process may contribute differently tothe overall error.
Geibel and Wysotzki (2003) in-troduce instance-dependent cost values for the per-ceptron algorithm and apply it to a set of binaryclassification problems.
We focus here on struc-tured problems and propose cost-sensitive learn-ing for POS tagging using the structured percep-tron algorithm.
In a similar spirit, Higashiyamaet al.
(2013) applied cost-sensitive learning to thestructured perceptron for an entity recognition taskin the medical domain.
They consider the dis-tance between the predicted and true label se-quence smoothed by a parameter that they esti-mate on a development set.
This means that theentire sequence is scored at once, while we updateon a per-label basis.The work most related to ours is the recent studyof Song et al.
(2012).
They suggest that some er-rors made by a POS tagger are more serious thanothers, especially for downstream tasks.
They de-vise a hierarchy of POS tags for the Penn tree-bank tag set (e.g.
the class NOUN contains NN,NNS, NNP, NNPS and CD) and use that in anSVM learner.
They modify the Hinge loss thatcan take on three values: 0, ?, 1.
If an error oc-curred and the predicted tag is in the same class asthe gold tag, a loss ?
occurred, otherwise it countsas full cost.
In contrast to our approach, they letthe learner focus on the more difficult cases by oc-curring a bigger loss when the predicted POS tag748is in a different category.
Their approach is thussuitable for a fine-grained tagging scheme and re-quires tuning of the cost parameter ?.
We tacklethe problem from a different angle by letting thelearner abstract away from difficult, inconsistentcases as estimated from inter-annotator scores.Our approach is also related to the literatureon regularization, since our cost-sensitive lossfunctions are aimed at preventing over-fitting tolow-confidence annotations.
S?gaard (2013b;2013a) presented two theories of linguistic varia-tion and perceptron learning algorithms that reg-ularize models to minimize loss under expectedvariation.
Our work is related, but models varia-tions in annotation rather than variations in input.There is a large literature related to the issue oflearning from annotator bias.
Reidsma and op denAkker (2008) show that differences between anno-tators are not random slips of attention but ratherdifferent biases annotators might have, i.e.
differ-ent mental conceptions.
They show that a classi-fier trained on data from one annotator performedmuch better on in-sample (same annotator) datathan on data of any other annotator.
They proposetwo ways to address this problem: i) to identifysubsets of the data that show higher inter-annotatoragreement and use only that for training (e.g.
forspeaker address identification they restrict the datato instances where at least one person is in thefocus of attention); ii) if available, to train sepa-rate models on data annotated by different anno-tators and combine them through voting.
The lat-ter comes at the cost of recall, because they de-liberately chose the classifier to abstain in non-consensus cases.In a similar vein, Klebanov and Beigman (2009)divide the instance space into easy and hard cases,i.e.
easy cases are reliably annotated, whereasitems that are hard show confusion and disagree-ment.
Hard cases are assumed to be annotatedby individual annotator?s coin-flips, and thus can-not be assumed to be uniformly distributed (Kle-banov and Beigman, 2009).
They show that learn-ing with annotator noise can have deteriorating ef-fect at test time, and thus propose to remove hardcases, both at test time (Klebanov and Beigman,2009) and training time (Beigman and Klebanov,2009).In general, it is important to analyze the dataand check for label biases, as a machine learner isgreatly affected by annotator noise that is not ran-dom but systematic (Reidsma and Carletta, 2008).However, rather than training on subsets of data ortraining separate models ?
which all implicitly as-sume that there is a large amount of training dataavailable ?
we propose to integrate inter-annotatorbiases directly into the loss function.Regarding measurements for agreements, sev-eral scores have been suggested in the literature.Apart from the simple agreement measure, whichrecords how often annotators choose the samevalue for an item, there are several statistics thatqualify this measure by adjusting for other fac-tors, such as Cohen?s ?
(Cohen and others, 1960),the G-index score (Holley and Guilford, 1964), orKrippendorff?s ?
(Krippendorf, 2004).
However,most of these scores are sensitive to the label dis-tribution, missing values, and other circumstances.The measure used in this paper is less affected bythese factors, but manages to give us a good un-derstanding of the agreement.8 ConclusionIn NLP, we use a variety of measures to assessand control annotator disagreement to produce ho-mogenous final annotations.
This masks the factthat some annotations are more reliable than oth-ers, and which is thus not reflected in learned pre-dictors.
We incorporate the annotator uncertaintyon certain labels by measuring annotator agree-ment and use it in the modified loss function ofa structured perceptron.
We show that this ap-proach works well independent of regularization,both on in-sample and out-of-sample data.
More-over, when evaluating the models trained with ourloss function on downstream tasks, we observe im-provements on two different tasks.
Our resultssuggest that we need to pay more attention to an-notator confidence when training predictors.AcknowledgementsWe would like to thank the anonymous review-ers and Nathan Schneider for valuable commentsand feedback.
This research is funded by the ERCStarting Grant LOWLANDS No.
313695.ReferencesEyal Beigman and Beata Klebanov.
2009.
Learningwith annotation noise.
In ACL.Jacob Cohen et al.
1960.
A coefficient of agreement749for nominal scales.
Educational and psychologicalmeasurement, 20(1):37?46.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In ACL.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: overcoming sparse and noisy data.
InRANLP.Jakob Elming, Anders Johannsen, Sigrid Klerke,Emanuele Lapponi, Hector Martinez, and AndersS?gaard.
2013.
Down-stream effects of tree-to-dependency conversions.
In NAACL.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in Twitter data withcrowdsourcing.
In NAACL-HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Josef Le Roux, Joakim Nivre, Deirde Hogan, andJosef van Genabith.
2011.
From news to comments:Resources and benchmarks for parsing the languageof Web 2.0.
In IJCNLP.Hege Fromheide, Dirk Hovy, and Anders S?gaard.2014.
Crowdsourcing and annotating NER for Twit-ter #drift.
In Proceedings of LREC 2014.Peter Geibel and Fritz Wysotzki.
2003.
Perceptronbased learning with example dependent and noisycosts.
In ICML.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In ACL.Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Ue-hara.
2013.
Clinical entity recognition usingcost-sensitive structured perceptron for NTCIR-10MedNLP.
In NTCIR.Jasper Wilson Holley and Joy Paul Guilford.
1964.A Note on the G-Index of Agreement.
Educationaland Psychological Measurement, 24(4):749.Dirk Hovy, Barbara Plank, and Anders S?gaard.
2014.When POS datasets don?t add up: Combatting sam-ple bias.
In Proceedings of LREC 2014.Richard Johansson.
2013.
Training parsers on incom-patible treebanks.
In NAACL.Beata Klebanov and Eyal Beigman.
2009.
From an-notator agreement to noise models.
ComputationalLinguistics, 35(4):495?503.Klaus Krippendorf, 2004.
Content Analysis: An In-troduction to Its Methodology, second edition, chap-ter 11.
Sage, Thousand Oaks, CA.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: prob-abilistic models for segmenting and labeling se-quence data.
In ICML.Christopher D Manning.
2011.
Part-of-speech tag-ging from 97% to 100%: is it time for some linguis-tics?
In Computational Linguistics and IntelligentText Processing, pages 171?189.
Springer.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.
Uni-versal dependency annotation for multilingual pars-ing.
In ACL.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InNAACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In LREC.Dennis Reidsma and Jean Carletta.
2008.
Reliabil-ity measurement without limits.
Computational Lin-guistics, 34(3):319?326.Dennis Reidsma and Rieks op den Akker.
2008.
Ex-ploiting ?subjective?
annotations.
In Workshop onHuman Judgements in Computational Linguistics,COLING.Alan Ritter, Sam Clark, Oren Etzioni, et al.
2011.Named entity recognition in tweets: an experimentalstudy.
In EMNLP.Roy Schwartz, Omri Abend, Roi Reichart, and AriRappoport.
2011.
Neutralizing linguistically prob-lematic annotations in unsupervised dependencyparsing evaluation.
In ACL.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In NAACL.Anders S?gaard.
2013a.
Part-of-speech tagging withantagonistic adversaries.
In ACL.Anders S?gaard.
2013b.
Zipfian corruptions for robustpos tagging.
In NAACL.750Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, Seong-Bae Park, and Sang-Jo Lee.
2012.
A cost sensitivepart-of-speech tagging: differentiating serious errorsfrom minor errors.
In ACL.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Cross-framework evaluation for statisticalparsing.
In EACL.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In IJCNLP.Daniel Zeman.
2010.
Hard problems of tagset con-version.
In Proceedings of the Second InternationalConference on Global Interoperability for LanguageResources.751
