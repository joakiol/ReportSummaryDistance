Proceedings of NAACL HLT 2009: Short Papers, pages 57?60,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSpherical Discriminant Analysis in Semi-supervised Speaker Clustering?Hao TangDept.
of ECEUniversity of IllinoisUrbana, IL 61801, USAhaotang2@ifp.uiuc.eduStephen M. ChuIBM T. J. Watson Research CenterYorktown Heights, NY 10598, USAschu@us.ibm.comThomas S. HuangDept.
of ECEUniversity of IllinoisUrbana, IL 61801, USAhuang@ifp.uiuc.eduAbstractSemi-supervised speaker clustering refers tothe use of our prior knowledge of speakersin general to assist the unsupervised speakerclustering process.
In the form of an in-dependent training set, the prior knowledgehelps us learn a speaker-discriminative fea-ture transformation, a universal speaker priormodel, and a discriminative speaker subspace,or equivalently a speaker-discriminative dis-tance metric.
The directional scattering pat-terns of Gaussian mixture model mean su-pervectors motivate us to perform discrimi-nant analysis on the unit hypersphere ratherthan in the Euclidean space, which leads toa novel dimensionality reduction techniquecalled spherical discriminant analysis (SDA).Our experiment results show that in theSDA subspace, speaker clustering yields su-perior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA).1 IntroductionSpeaker clustering is a critical part of speaker di-arization (a.k.a.
speaker segmentation and cluster-ing) (Barras et al, 2006; Tranter and Reynolds,2006; Wooters and Huijbregts, 2007; Han et al,2008).
Unlike speaker recognition, where we havethe training data of a set of known speakers and thusrecognition can be done supervised, speaker cluster-ing is usually performed in a completely unsuper-vised manner.
The output of speaker clustering is theinternal labels relative to a dataset rather than real?This work was funded in part by DARPA contract HR0011-06-2-0001.speaker identities.
An interesting question is: Canwe do semi-supervised speaker clustering?
That is,can we make use of any available information thatcan be helpful to speaker clustering?Our answer to this question is positive.
Here,semi-supervision refers to the use of our priorknowledge of speakers in general to assist the un-supervised speaker clustering process.
In the formof an independent training set, the prior knowledgehelps us learn a speaker-discriminative feature trans-formation, a universal speaker prior model, and adiscriminative speaker subspace, or equivalently aspeaker-discriminative distance metric.2 Semi-supervised Speaker ClusteringA general pipeline of speaker clustering consistsof four essential elements, namely feature extrac-tion, utterance representation, distance metric, andclustering.
We incorporate our prior knowledgeof speakers into the various stages of this pipelinethrough an independent training set.2.1 Feature ExtractionThe most popular speech features are spectrum-based acoustic features such as mel-frequency cep-stral coefficients (MFCCs) and perceptual linear pre-dictive (PLP) coefficients.
In order to account forthe dynamics of spectrum changes over time, thebasic acoustic features are often supplemented bytheir first and second derivatives.
We pursue a dif-ferent avenue in which we augment the basic acous-tic features of every frame with those of the neigh-boring frames.
Specifically, the acoustic featuresof the current frame and those of the KL frames57to the left and KR frames to the right are con-catenated to form a high-dimensional feature vec-tor.
In the context-expanded feature vector space, welearn a speaker-discriminative feature transforma-tion by linear discriminant analysis (LDA) based onthe known speaker labels of the independent trainingset.
The resulting low-dimensional feature subspaceis expected to provide optimal speaker separability.2.2 Utterance RepresentationDeviating from the mainstream ?bag of acoustic fea-tures?
representation where the extracted acousticfeatures are represented by a statistical model suchas a Gaussian mixture model (GMM), we adopt theGMM mean supervector representation which hasemerged in the speaker recognition area (Campbellet al, 2006).
Such representation is obtained bymaximum a posteriori (MAP) adapting a universalbackground model (UBM), which has been finelytrained with all the data in the training set, to aparticular utterance.
The component means of theadapted GMM are stacked to form a column vectorconventionally called a GMM mean supervector.
Inthis way, we are allowed to represent an utteranceas a point in a high-dimensional space where tra-ditional distance metrics and clustering techniquescan be naturally applied.
The UBM, which can bedeemed as a universal speaker prior model inferredfrom the independent training set, imposes genericspeaker constraints to the GMM mean supervectorspace.2.3 Distance MetricIn the GMM mean supervector space, a naturallyarising distance metric is the Euclidean distancemetric.
However, it is observed that the supervec-tors show strong directional scattering patterns.
Thedirections of the data points seem to be more indica-tive than their magnitudes.
This observation moti-vates us to favor the cosine distance metric over theEuclidean distance metric for speaker clustering.Although the cosine distance metric can be usedin the GMM mean supervector space, it is optimalonly if the data points are uniformly spread in all di-rections in the entire space.
In a high-dimensionalspace, most often the data lies in or near a low-dimensional manifold or subspace.
It is advanta-geous to learn an optimal distance metric from thedata directly.The general cosine distance between two datapoints x and y can be defined and manipulated asfollows.d(x,y) = 1?
xTAy?xTAx?yTAy(1)= 1?
(A1/2x)T (A1/2y)?
(A1/2x)T (A1/2x)?
(A1/2y)T (A1/2y)= 1?
(WTx)T (WTy)?
(WTx)T (WTx)?
(WTy)T (WTy)The general cosine distance can be casted as thecosine distance between two transformed data pointsW Tx and W Ty where W T = A1/2.
In this sense,learning an optimal distance metric is equivalent tolearning an optimal linear subspace of the originalhigh-dimensional space.3 Spherical Discriminant AnalysisMost existing linear subspace learning techniques(e.g.
PCA and LDA) are based on the Euclideandistance metric.
In the GMM mean supervectorspace, we seek to perform discriminant analysis inthe cosine distance metric space.
We coin the phrase?spherical discriminant analysis?
to denote discrim-inant analysis on the unit hypersphere.
We definea projection from a d-dimensional hypersphere to ad?-dimensional hypersphere where d?
< dy = WTx?W Tx?
(2)We note that such a projection is nonlinear.
How-ever, under two mild conditions, this projection canbe linearized.
One is that the objective function forlearning the projection only involves the cosine dis-tance.
The other is that only the cosine distance isused in the projected space.
In this case, the norm ofthe projected vector y has no impact on the objectivefunction and distance computation in the projectedspace.
Thus, the denominator term of Equation 2can be safely dropped, leading to a linear projection.3.1 FormulationThe goal of SDA is to seek a linear transformationW such that the average within-class cosine similar-ity of the projected data set is maximized while the58average between-class cosine similarity of the pro-jected data set is minimized.
Assuming that there arec classes, the average within-class cosine similaritycan be written in terms of the unknown projectionmatrix W and the original data points xSW = 1cc?i=1Si (3)Si = 1|Di||Di|?yj ,yk?DiyTj yk?yTj yj?yTk yk= 1|Di||Di|?xj ,xk?DixTj WWTxk?xTj WWTxj?xTk WWTxkwhere |Di| denotes the number of data points in theith class.
Similarly, the average between-class co-sine similarity can be written in terms of W and xSB = 1c(c?
1)c?m=1c?n=1Smn (m 6= n) (4)Smn = 1|Dm||Dn|?yj?Dmyk?DnyTj yk?yTj yj?yTk yk= 1|Dm||Dn|?xj?Dmxk?DnxTj WWTxk?xTj WWTxj?xTk WWTxkwhere |Dm| and |Dn| denote the number of datapoints in the mth and nth classes, respectively.The SDA criterion is to maximize SW while min-imizing SBW = argmaxW(SW ?
SB) (5)Our SDA formulation is similar to the work of Maet al (2007).
However, we solve it efficiently in ageneral dimensionality reduction framework knownas graph embedding (Yan et al, 2007).3.2 Graph Embedding SolutionIn graph embedding, a weighted graph with vertexset X and similarity matrix S is used to characterizecertain statistical or geometrical properties of a dataset.
A vertex in X represents a data point and anentry sij in S represents the similarity between thedata points xi and xj .
For a specific dimensional-ity reduction algorithm, there may exist two graphs.The intrinsic graph {X,S(i)} characterizes the dataproperties that the algorithm aims to preserve andthe penalty graph {X,S(p)} characterizes the dataproperties that the algorithm aims to avoid.
The goalof graph embedding is to represent each vertex in Xas a low dimensional vector that preserves the simi-larities in S. The objective function isW=argminW?i 6=j ?f(xi,W )?f(xj ,W )?2(s(i)ij ?s(p)ij ) (6)where f(x,W ) is a general projection with param-eters W .
If we take the projection to be of the formin Equation 2, the objective function becomesW=argminW?i6=j???
?WT xi?WT xi??
WT xj?WT xj????
?2(s(i)ij ?s(p)ij ) (7)It is shown that the solution to the graph embed-ding problem of Equation 7 may be obtained bya steepest descent algorithm (Fu et al, 2008).
Ifwe expand the L2 norm terms of Equation 7, it isstraightforward to show that Equation 7 is equiva-lent to Equation 5 provided that the graph weightsare set to proper values, as follows.s(i)jk ?1c|Di||Di|if xj ,xk ?
Di, i = 1, ..., cs(p)jk ?1c(c?
1)|Dm||Dn| if xj ?
Dm,xk ?
Dnm,n = 1, ..., c,m 6= n (8)That is, by assigning appropriate values to theweights of the intrinsic and penalty graphs, the SDAoptimization problem in Equation 5 can be solvedwithin the elegant graph embedding framework.4 ExperimentsOur speaker clustering experiments are based on atest set of 630 speakers and 19024 utterances se-lected from the GALE database (Chu et al, 2008),which contains about 1900 hours of broadcastingnews speech data collected from various TV pro-grams.
An independent training set of 498 speak-ers and 18327 utterances is also selected from theGALE database.
In either data set, there are an aver-age of 30-40 utterances per speaker and the averageduration of the utterances is about 3-4 seconds.
Notethat there are no overlapping speakers in the two data59sets ?
speakers in the test set are not present in theindependent training set.The acoustic features are 13 basic PLP featureswith cepstrum mean subtraction.
In computing theLDA feature transformation using the independenttraining set, KL and KR are both set to 4, and the di-mensionality of the low-dimensional feature space isset to 40.
The entire independent training set is usedto train a UBM via the EM algorithm, and a GMMmean supervector is obtained for every utterance inthe test set via MAP adaptation.
The trained UBMhas 64 mixture components.
Thus, the dimension ofthe GMM mean supervectors is 2560.We employ the hierarchical agglomerative clus-tering technique with the ?ward?
linkage method.Our experiments are carried out as follows.
In eachexperiment, we perform 4 cases, each of which is as-sociated with a specific number of test speakers, i.e.,5, 10, 20, and 50, respectively.
In each case, thecorresponding number of speakers are drawn ran-domly from the test set, and all the utterances fromthe selected speakers are used for clustering.
Foreach case, 100 trials are run, each of which involvesa random draw of the test speakers, and the averageof the clustering accuracies across the 100 trials isrecorded.First, we perform speaker clustering in the orig-inal GMM mean supervector space using the Eu-clidean distance metric and the cosine distance met-ric, respectively.
The results indicate that the cosinedistance metric consistently outperforms the Eu-clidean distance metric.
Next, we perform speakerclustering in the reduced-dimensional subspaces us-ing the eigenvoice (PCA) and fishervoice (LDA)approaches, respectively.
The results show thatthe fishervoice approach significantly outperformsthe eigenvoice approach in all cases.
Finally, weperform speaker clustering in the SDA subspace.The results demonstrate that in the SDA subspace,speaker clustering yields superior performance thanthat in other reduced-dimensional subspaces (e.g.,PCA and LDA).
Table 1 presents these results.5 ConclusionThis paper proposes semi-supervised speaker clus-tering in which we learn a speaker-discriminativefeature transformation, a universal speaker priorMetric Subspace 5 10 20 50EucOrig 85.0 82.6 78.1 69.4PCA 85.5 82.9 79.3 69.9LDA 94.0 90.8 86.6 79.6Cos Orig 90.7 86.5 82.2 77.7SDA 98.0 94.7 90.0 85.9Table 1: Average speaker clustering accuracies (unit:%).model, and a speaker-discriminative distance metricthrough an independent training set.
Motivated bythe directional scattering patterns of the GMM meansupervectors, we peroform discriminant analysis onthe unit hypersphere rather than in the Euclideanspace, leading to a novel dimensionality reductiontechnique ?SDA?.
Our experiment results indicatethat in the SDA subspace, speaker clustering yieldssuperior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA).ReferencesC.
Barras, X. Zhu, S. Meignier, and J. Gauvain.
2006.Multistage speaker diarization of broadcast news.IEEE Trans.
ASLP, 14(5):1505?1512.W.
Campbell, D. Sturim, D. Reynolds.
2006.
Supportvector machines using GMM supervectors for speakerverification.
Signal Processing Letters 13(5):308-311.S.
Chu, H. Kuo, L. Mangu, Y. Liu, Y. Qin, and Q. Shi.2008.
Recent advances in the IBM GALE mandarintranscription system.
Proc.
ICASSP.Y.
Fu, S. Yan and T. Huang.
2008.
Correlation Met-ric for Generalized Feature Extraction.
IEEE Trans.PAMI 30(12):2229?2235.K.
Han, S. Kim, and S. Narayanan.
2008.
Strategies toImprove the Robustness of Agglomerative Hierarchi-cal Clustering under Data Source Variation for SpeakerDiarization.
IEEE Trans.
SALP 16(8):1590?1601.Y.
Ma, S. Lao, E. Takikawa, and M. Kawade.
2007.
Dis-criminant Analysis in Correlation Similarity MeasureSpace.
Proc.
ICML (227):577?584.S.
Tranter and D. Reynolds.
2006.
An Overview ofAutomatic Speaker Diarization Systems.
IEEE Trans.ASLP, 14(5):1557?1565.C.
Wooters and M. Huijbregts.
2007.
The ICSI RT07sSpeaker Diarization System.
LNCS.S.
Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, S. Lin.2007.
Graph Embedding and Extensions: A Gen-eral Framework for Dimensionality Reduction.
IEEETrans.
PAMI 29(1):40?51.60
