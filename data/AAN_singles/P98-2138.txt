Combining Trigram and Winnow in Thai OCR Error CorrectionSurapant MeknavinNational Electronics and Computer  Technology Center73/1 Rama VI Road, Rajthevi, Bangkok, Thai landsurapan@nectec.or.thBoonserm K i j s i r i ku l ,  Anan lada  Chot imongko l  and Cho lw ich  Nut teeDepartment of Computer  EngineeringChulalongkorn University, Thai landfengbks@chulkn.chula.ac.thAbst rac tFor languages that have no explicit word bound-ary such as Thai, Chinese and Japanese, cor-recting words in text is harder than in Englishbecause of additional ambiguities in locating er-ror words.
The traditional method handles thisby hypothesizing that every substrings in theinput sentence could be error words and tryingto correct all of them.
In this paper, we pro-pose the idea of reducing the scope of spellingcorrection by focusing only on dubious areas inthe input sentence.
Boundaries of these dubiousareas could be obtained approximately by ap-plying word segmentation algorithm and findingword sequences with low probability.
To gener-ate the candidate correction words, we used amodified edit distance which reflects the charac-teristic of Thai OCR errors.
Finally, a part-of-speech trigram model and Winnow algorithmare combined to determine the most probablecorrection.1 IntroductionOptical character ecognition (OCR) is usefulin a wide range of applications, such as officeautomation and information retrieval system.However, OCR in Thailand is still not widelyused, partly because existing Thai OCRs arenot quite satisfactory in terms of accuracy.
Re-cently, several research projects have focused onspelling correction for many types of errors in-cluding those from OCR (Kukich, 1992).
Nev-ertheless, the strategy is slightly different fromlanguage to language, since the characteristic ofeach language is different.Two characteristics of Thai which make thetask of error correction different from those ofother languages are: (1) there is no explicitword boundary, and (2) characters are writtenin three levels; i.e., the middle, the upper andthe lower levels.
In order to solve the prob-lem of OCR error correction, the first task isusually to detect error strings in the input sen-tence.
For languages that have explicit wordboundary such as English in which each wordis separated from the others by white spaces,this task is comparatively simple.
If the tok-enized string is not found in the dictionary, itcould be an error string or an unknown word.However, for the languages that have no ex-plicit word boundary such as Chinese, Japaneseand Thai, this task is much more complicated.Even without errors from OCR, it is difficult todetermine word boundary in these languages.The situation gets worse when noises are intro-duced in the text.
The existing approach forcorrecting the spelling error in the languagesthat have no word boundary assumes that allsubstrings in input sentence are error strings,and then tries to correct them (Nagata, 1996).This is computationally expensive since a largeportion of the input sentence is correct.
Theother characteristic of Thai writing system isthat we have many levels for placing Thai char-acters and several characters can occupy morethan one level.
These characters are easily con-nected to other characters in the upper or lowerlevel.
These connected characters cause diffi-culties in the process of character segmentationwhich then cause errors in Thai OCR.Other than the above problems specific toThai, real-word error is another source of er-rors that is difficult to correct.
Several previousworks on spelling correction demonstrated that836tonew !
upper levelmiddle levelbaseline?
@.I I lower levelconsonantFigure 1: No explicit word delimiter in Thaifeature-based approaches are very effective forsolving this problem.In this paper, a hybrid method for Thai OCRerror correction is proposed.
The method com-bines the part-of-speech (POS) trigram modelwith a feature-based model.
First, the POS tri-gram model is employed to correct non-word aswell as real-word errors.
In this step, the num-ber of non-word errors are mostly reduced, butsome real-word errors still remain because thePOS trigram model cannot capture some use-ful features in discriminating candidate words.A feature-based approach using Winnow algo-rithm is then applied to correct the remainingerrors.
In order to overcome the expensive com-putation cost of the existing approach, we pro-pose the idea of reducing the scope of correc-tion by using word segmentation algorithm tofind the approximate error strings from the in-put sentence.
Though the word segmentationalgorithm cannot give the accurate boundary ofan error string, many of them can give cluesof unknown strings which may be error strings.We can use this information to reduce the scopeof correction from entire sentence to a more nar-row scope.
Next, to capture the characteristicof Thai OCR errors, we have defined the modi-fied edit distance and use it to enumerate plau-sible candidates which deviate from the word inquestion within k-edit distance.2 Problems of Thai OCRThe problem of OCR error correction can bedefined as : given the string of charactersS = clc2...cn produced by OCR, find theword sequence W -- wlw2.
.
,  w~ that maximizesthe probability P(WIS ).
Before describing themethods used to model P(WIS),  below we listsome main characteristics of Thai that poses dif-ficulties for correcting Thai OCR error.?
Words are written consecutively withoutword boundary delimiters such as whitespace characters.
For example, the phrase"r~u~u~lJU" (Japan at present) in Figure1, actually consists of three words: "~du"(Japan), '%" (at), and "~u"  (present).Therefore, Thai OCR error correction hasto overcome word boundary ambiguity aswell as select the most probable correctioncandidate at the same time.
This is similarto the problem of Connected Speech Recog-nition and is sometimes called ConnectedText Recognition (Ingels, 1996).?
There are 3 levels for placing Thai charac-ters and some characters can occupy morethan one level.
For example, in Figure 2"~"  consists of characters in three levels, qi.e., ~, ,, ~ and ~ are in the top, the bot-tom, the middle and both the middle andtop levels, respectively.
The character thatoccupies more than one level like ~ usuallyconnects to other characters (~) and causeserror on the output of OCR, i.e., ~ maybe recognized as ~ or \].
Therefore, to cor-rect characters produced by OCR, not onlysubstitution errors but also deletion and in-sertion errors must be considered.
In addi-tion, in such a case, the candidates rankedby OCR output are unreliable and cannotbe used to reduce search space.
This isbecause the connected characters tend tohave very different features from the origi-nal separated ones.837tone consonant=ivowel 2I uppertoplheI middle levelbaselineI lower levelFigure 2: Three levels for placing Thai charac-ters3 Our  Methods3.1 Tr igram Mode lTo find W that maximizes P(WIS), we can usethe POS trigram model as follows.arg mwax P(WIS )= argmwaxP(W)P(SlW)/P(S ) (1)= argmwaxP(W)P(S\[W ) (2)The probability P(W) is given by the lan-guage model and can be estimated by the tri-gram model as:P(W) = P(W, T) = H P(ti\] ti-2,ti-1)P(wilti)(3)P(SIW ) is the characteristics of specificOCR, and can be estimated by collecting sta-tistical information from original text and thetext produced by OCR.
We assume that giventhe original word sequence W composed of char-acters vlv2... Vm, OCR produces the sequenceas string S (= ctc2.., an) by repeatedly apply-ing the following operation: substitute a char-acter with another; insert a character; or deletea character.
Let Si be the /-prefix of S thatis formed by first character to the/-characterof S (= clc2...ci), and similarly Wj is the j-prefix of W (= vlv2.., vj).
Using dynamic pro-gramming technique, we can calculate P(SIW )(= P(SnlWm)) by the following equation:P(SiIWj) = max(P(Si_llWj) * P(ins(ci)),P(SilWj_I) ?
P(del(vj)),P(Si-llW -l) ?
P(cilv )) (4)where P(ins(c)), P(del(v)) and P(clv ) are theprobabilities that letter c is inserted, letter v isdeleted and letter v is substituted with c, re-spectively.One method to do OCR error correction us-ing the above model is to hypothesize all sub-strings in the input sentence as words (Nagata,1996).
Both words in the dictionary that ex-actly match with the substrings and those thatapproximately match are retrieved.
To copewith unknown words, all other substrings notmatched must also be considered.
The wordlattice is then scanned to find the N-best wordsequences as correction candidates.
In general,this method is perfectly good, except in one as-pect: its time complexity.
Because it generatesa large number of hypothesized words and hasto find the best combination among them, it isvery slow.3.2 Selective T r ig ram ModelTo alleviate the above problem, we try to reducethe number of hypothesized words by generat-ing them only when needed.
Having analyzedthe OCR output, we found that a large por-tion of input sentence are correctly recognizedand need no approximation.
Therefore, insteadof hypothesizing blindly through the whole sen-tence, if we limit our hypotheses toonly dubiousareas, we can save considerable amount of time.Following is our algorithm for correcting OCRoutput...Find dubious areas: Find all substringsin the input sentence that exactly matchwords in the dictionary.
Each substringmay overlap with others.
The remainingparts of sentence which are not covered byany of these substrings are considered asdubious areas.Make hypotheses  for nonwords  andunknown words:(a) For each dubious tring obtained from1., the surrounding words are also con-sidered to form candidates for correc-tion by concatenating them with thedubious string.
For example, in "in-form at j off', j is an unknown stringrepresenting a dubious area, and in-form at and on are words.
In this838case, the unknown word and its sur-rounding known words are combinedtogether, esulting in "in/ormatjon" asa new unknown string.
(b) For each unknown string obtainedform 2(a), apply the candidate genera-tion routine to generate approximatelymatched words within k-edit distance.The value of k is varied proportionallyto the length of candidate word.
(c) All substrings except for ones thatviolate Thai spelling rules, i.e., leadby non-leading character, are hypoth-esized as unknown words.3.
F ind  good word sequences:  Findthe N-best word sequences accordingto equation (2).
For unknown words,P(wilUnknown word) is computed by us-ing the unknown word model in (Nagata,1996).4.
Make hypotheses  for real -word er-ror: For each word wi in N-best wordsequence where the local probabilitiesP(wi-1, wi, wi+l, ti-1, ti, ti+l) are below athreshold, generate candidate words by ap-plying the process imilar to step 2 exceptthat the nonword in step 2 is replaced withthe word wi.
Find the word sequenceswhose probabilities computed by equation(2) are better than original ones.5.
F ind  the  N-best  word sequences:From all word sequences obtained from step4, select the N-best ones.The candidate generation routine uses a mod-ification of the standard edit distance and em-ploys the error-tolerant finite-state recognitionalgorithm (Oflazer, 1996) to generate candidatewords.
The modified edit distance allows ar-bitrary number of insertion and/or deletion ofupper level and lower level characters, but al-lows no insertion or deletion of the middle levelcharacters.
In the middle level, it allows only ksubstitution.
This is to reflect he characteristicof Thai OCR which, 1. tends to merge severalcharacters into one when the character whichspans two levels are adjacent o characters inthe upper and lower level, and 2. rarely causesinsertion and deletion errors in the middle level.For example, applying the candidate generationroutine with 1 edit distance to the string "~"gives the set of candidates {~.
~,  ~.
~,  ~,~, ~,From our experiments, we found that the se-lective trigram model can deal with nonworderrors fairly well.
However, the model is notenough to correct real-word errors as well aswords with the same part of speech.
This isbecause the POS trigram model considers onlycoarse information of POS in a fixed restrictedrange of context, some useful information suchas specific word collocation may be lost.
Usingword N-gram could recover some word-level in-formation but requires an extremely large cor-pus to estimate all parameters accurately andconsumes vast space resources to store the hugeword N-gram table.
In addition, the modellosses generalized information at the level ofPOS.For English, a number of methods havebeen proposed to cope with real-word errors inspelling correction (Golding, 1995; Golding andRoth, 1996; Golding and Schabes, 1993; Tongand Evans, 1996).
Among them, the feature-based methods were shown to be superior toother approaches.
This is because the methodscan combine several kinds of features to deter-mine the appropriate word in a given context.For our task, we adopt a feature-based algo-rithm called Winnow.
There are two reasonswhy we select Winnow.
First, it has been shownto be the best performer in English context-sensitive spelling correction (Golding and Roth,1996).
Second, it was shown to be able to han-dle difficult disambiguation tasks in Thai (Mek-navin et al~ 1997).Below we describe Winnow algorithm that isused for correcting real-word error.3.3 Winnow A lgor i thm3.3.1 The a lgor i thmA Winnow algorithm used in our experiment isthe algorithm described in (Blum, 1997).
Win-now is a multiplicative weight updating and in-cremental lgorithm (Littlestone, 1988; Goldingand Roth, 1996).
The algorithm is originally de-signed for learning two-class (positive and neg-ative class) problems, and can be extended tomultiple-class problems as shown in Figure 3.Winnow can be viewed as a network of onetarget node connected to n nodes, called spe-cialists, each of which examines one feature and839Let Vh .
.
.
,  vm be the values of the target concept o be learned, and xi be the prediction of the/-specialist.1.
Initialize the weights wx, .
.
.
,  Wn of all the specialists to 1.2.
For Each  example x = {x l , .
.
.
,  Xn} Do(a) Let V be the value of the target concept of the example.
(b) Output ~)j = arg maxvie{vl,...,v,,,} ~'~i:xi=v i Wi(c) If the algorithm makes a mistake (~)j ~ V), then:i. for each xi equal to V, wi is updated to wi ?
o~ii.
for each xi equal to ?~j, wi is updated to wi ?where, c~ > 1 and/3 < 1 are promotion parameter and demotion parameter, and are set to 3/2 and1/2, respectively.Figure 3: The Winnow algorithm for learning multiple-class concept.predicts xi as the value of the target concept.The basic idea of the algorithm is that to ex-tract some useful unknown features, the algo-rithm asks for opinions from all specialists, eachof whom has his own specialty on one feature,and then makes a global prediction based on aweighted majority vote over all those opinionsas described in Step 2-(a) of Figure 3.
In our ex-periment, we have each specialist examine oneor two attributes of an example.
For example,a specialist may predict the value of the targetconcept by checking for the pairs "(attributel---- valuel) and (attribute2 = value2)".
Thesepairs are candidates of features we are trying toextract.A specialist only makes a prediction if its con-dition "(attributel = valuel)" is true in caseof one attribute, or both of its conditions "(at-tr ibutel -- value1) and (attibute2 -- value2)"are true in case of two attributes, and in thatcase it predicts the most popular outcome out ofthe last k times it had the chance to predict.
Aspecialist may choose to abstain instead of giv-ing a prediction on any given example in casethat it did not see the same value of an attributein the example.
In fact, we may have each spe-cialist examines more than two attributes, butfor the sake of simplification of preliminary ex-periment, let us assume that two attributes foreach specialist are enough to learn the targetconcept.The global algorithm updates the weight wiof any specialist based on the vote of that spe-cialist.
The weight of any specialist is initializedto 1.
In case that the global algorithm predictsincorrectly, the weight of the specialist that pre-dicts incorrectly is halved and the weight of thespecialist that predicts correctly is multiplied by3/2.
This weight updating method is the sameas the one used in (Blum, 1997).
The advan-tage of Winnow, which made us decide to usefor our task, is that it is not sensitive to extrairrelevant features (Littlestone, 1988).3.3.2 Constructing Confusion Set andDefining FeaturesTo employ Winnow in correcting OCR er-rors, we first define k-edit distance confusionset.
A k-edit distance confusion set S ={c, wl,  w2 , .
.
.
,  Wn} is composed of one centroidword c and words wl, w2 , .
.
.
,  Wn generated byapplying the candidate generation routine withmaximum k modified edit distance to the cen-troid word.
If a word c is produced by OCRoutput or by the previous tep, then it may becorrected as wl ,w2, .
.
.
,Wn or c itself.
For ex-ample, suppose that the centroid word is know,then all possible words in 1-edit distance con-fusion set are {know, knob, knop, knot, knew,enow, snow, known, now}.
Furthermore, wordswith probability lower than a threshold are ex-cluded from the set.
For example, if a specificOCR has low probability of substituting t withw, "knof' should be excluded from the set.Following previous works (Golding, 1995;Meknavin et al, 1997), we have tried two typesof features: context words and collocations.Context-word features is used to test for the840presence of a particular word within ?/- Mwords of the target word, and collocations testfor a pattern of up to L contiguous words and/orpart-of-speech tags around the target word.
Inour experiment M and L is set to 10 and 2,respectively.
Examples of features for discrimi-nating between snow and know include:(1) I {know, snow}(2) winter within ?10 wordswhere (1) is a collocation that tends to implyknow, and (2) is a context-word that tends toimply snow.
Then the algorithm should extractthe features ("word within ?10 words of thetarget word" = "winter") as well as ("one wordbefore the target word" -- 'T') as useful featuresby assigning them with high weights.3.3.3 Using the Network  to RankSentencesAfter networks of k-edit distance confusion setsare learned by Winnow, the networks are usedto correct the N-best sentences received fromPOS trigram model.
For each sentence, everyreal word is evaluated by the network whose thecentroid word is that real word.
The networkwill then output he centroid word or any wordin the confusion set according to the context.After the most probable word is determined, theconfidence l vel of that word will be calculated.Since every specialist has weight voting for thetarget word, we can consider the weight as con-fidence level of that specialist for the word.
Wedefine the confidence level of any word as allweights that vote for that word divided by allweights in the network.
Based on the confidencelevels of all words in the sentence, the averageof them is taken as the confidence l vel of thesentence.
The N-best sentences are then re-ranked according to the confidence l vel of thesentences.4 Exper imentsWe have prepared the corpus containing about9,000 sentences (140,000 words, 1,300,000 char-acters) for evaluating our methods.
The corpusis separated into two parts; the first part con-taining about 80 % of the whole corpus is usedas a training set for both the trigram modeland Winnow, and the rest is used as a test set.Based on the prepared corpus, experiments wereconducted to compare our methods.
The resultsTypeNon-word Errorl~al-word ErrorTotalError18.37%3.60%21.97%Table 1: The percentage of word error fromOCRTypeNon-word ErrorReal-word ErrorIntroduced ErrorTrigram82.16%75.71%1.42%Trigram +Winnow90.27%87.60%1.56%Table 2: The percentage of corrected word er-rors after applying Trigram and Winnoware shown in Table 1, and Table 2.Table 1 shows the percentage of word errorsfrom the entire text.
Table 2 shows the percent-age of corrected word errors after applying Tri-gram and Winnow.
The result reveals that thetrigram model can correct non-word and real-word, but introduced some new errors.
By thetrigram model, real-word errors are more diffi-cult to correct han non-word.
Combining Win-now to the trigram model, both types of errorsare further educed, and improvement of real-word error correction is more acute.The reason for better performance of Tri-gram+Winnow over Trigram alone is that theformer can exploit more useful features, i.e.,context words and collocation features, in cor-rection.
For example, the word "d~" (to bring)is frequently recognized as "~" (water) becausethe characters "~" is misreplaced with a sin-gle character " "~' by OCR.
In this case, Tri-gram cannot effectively recover the real-worderror "d~" to the correct word "~".
The word"d~" is effectively corrected by Winnow as thealgorithm found the context words that indicatethe occurence of "~" such as the words "=L~a"(evaporate) and "~"  (plant).
Note that thesecontext words cannot be used by Trigram tocorrect he real-word errors.8415 Conc lus ionWe have examined the application of the modi-fied edit distance, POS trigram model and Win-now algorithm to the task of Thai OCR errorcorrection.
The experimental result shows thatour proposed method reduces both non-word er-rors and reai-word errors effectively.
In futurework, we plan to test the method with muchmore data and to incorporate other sources ofinformation to improve the quality of correc-tion.
It is also interesting to examine howthe method performs when applied to human-generated misspellings.AcknowledgementWe would like to thank Paisarn Charoenporn-sawat who helps us run experiment with Win-now.
This work was partly supported by theThai Government Research Fund.Re ferencesAvrim Blum.
1997.
Empirical support for win-now and weighted-majority algorithm: Re-sults on a calendar scheduling domain.
Ma-chine Learning, 26.Andrew R. Golding and Dan Roth.
1996.
Ap-plying winnow to context-sensitive spellingcorrection.
In Proceedings of the ThirteenthInternational Conference on Machine Learn-ing.Andrew R. Golding and Yves Schabes.
1993.Combining trigram-based and featured-basedmethods for context-sensitive spelling cor-rection.
Technical Report TR-93-03a, Mit-subishi Electric Research Laboratory.Andrew R. Golding.
1995.
A bayesian hybridmethod for context-sensitive spelling correc-tion.
In Proceedings of the Third Workshopon Very Large Corpora.Peter Ingels.
1996.
Connected text recognitionusing layered HMMs and token passing.
InProceedings of the Second Conference on NewMethods in Language Processing.Karen Kukich.
1992.
Techniques for automati-cally correction words in text.
A CM Comput-ing Surveys, 24(4).Nick Littlestone.
1988.
Learning quickly whenirrelevant attributes abound: A new linear-threshold algorithm.
Machine Learning, 2.Surapant Meknavin, Paisarn Charoenporn-sawat, and Boonserm Kijsirikul.
1997.Feature-based Thai word segmentation.
InProceedings of Natural Language ProcessingPacific Rim Symposium '97.Masaaki Nagata.
1996.
Context-base spellingcorrection for Japanese OCR.
In Proceedingsof COLING '96.Kemai Oflazer.
1996.
Error-tolerant finite-staterecognition with applications to morphologi-cai analysis and spelling correction.
Compu-tational Linguistics, 22(1).Xiang Tong and David A. Evans.
1996.
Astatistical approach to automatic OCR errorcorrection in context.
In Proceedings of theFourth Workshop on Very Large Corpora.842
