Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689?697,Honolulu, October 2008. c?2008 Association for Computational LinguisticsOnline Methods for Multi-Domain Learning and AdaptationMark Dredze and Koby CrammerDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104 USA{mdredze,crammer}@cis.upenn.eduAbstractNLP tasks are often domain specific, yet sys-tems can learn behaviors across multiple do-mains.
We develop a new multi-domain onlinelearning framework based on parameter com-bination from multiple classifiers.
Our algo-rithms draw from multi-task learning and do-main adaptation to adapt multiple source do-main classifiers to a new target domain, learnacross multiple similar domains, and learnacross a large number of disparate domains.We evaluate our algorithms on two popularNLP domain adaptation tasks: sentiment clas-sification and spam filtering.1 IntroductionStatistical classifiers routinely process millions ofwebsites, emails, blogs and other text every day.Variability across different data sources means thattraining a single classifier obscures differences andseparate classifiers ignore similarities.
Similarly,adding new domains to existing systems requiresadapting existing classifiers.We present new online algorithms for three multi-domain learning scenarios: adapting existing classi-fiers to new domains, learning across multiple simi-lar domains and scaling systems to many disparatedomains.
Multi-domain learning combines char-acteristics of both multi-task learning and domainadaptation and drawing from both areas, we de-velop a multi-classifier parameter combination tech-nique for confidence-weighted (CW) linear classi-fiers (Dredze et al, 2008).
We focus on online algo-rithms that scale to large amounts of data.Next, we describe multi-domain learning and re-view the CW algorithm.
We then consider our threesettings using multi-classifier parameter combina-tion.
We conclude with related work.2 Multi-Domain LearningIn online multi-domain learning, each instance x isdrawn from a domain d specific distribution x ?
Ddover a vectors space RN and labeled with a domainspecific function fd with label y ?
{?1,+1} (forbinary classification.)
On round i the classifier re-ceives instance xi and domain identifier di and pre-dicts label y?i ?
{?1,+1}.
It then receives the truelabel yi ?
{?1,+1} and updates its prediction rule.As an example, consider a multi-user spam fil-ter, which must give high quality predictions fornew users (without new user data), learn on multi-ple users simultaneously and scale to thousands ofaccounts.
While a single classifier trained on allusers would generalize across users and extend tonew users, it would fail to learn user-specific prefer-ences.
Alternatively, separate classifiers would cap-ture user-specific behaviors but would not general-ize across users.
The approach we take to solv-ing multi-domain problems is to combine domain-specific classifiers.
In the adaptation setting, wecombine source domain classifiers for a new tar-get domain.
For learning across domains, we com-bine domain-specific classifiers and a shared classi-fier learned across all domains.
For learning acrossdisparate domains we learn which domain-specificand shared classifiers to combine.Multi-domain learning combines properties ofboth multi-task learning and domain adaptation.
As689in multi-task learning, we consider domains that arelabeled with different classification functions.
Forexample, one user may enjoy some emails that an-other user considers spam: differing in their classifi-cation function.
The goal of multi-task learning is togeneralize across tasks/domains (Dekel et al, 2006;Evgeniou and Pontil, 2004).
Furthermore, as in do-main adaptation, some examples are draw from dif-ferent distributions.
For example, one user may re-ceive emails about engineering while another aboutart, differing in their distribution over features.
Do-main adaptation deals with these feature distributionchanges (Blitzer et al, 2007; Jiang and Zhai, 2007).Our work combines these two areas by learning bothacross distributions and behaviors or functions.3 Confidence-Weighted Linear ClassifiersConfidence-weighted (CW) linear classification(Dredze et al, 2008), a new online algorithm, main-tains a probabilistic measure of parameter confi-dence, which may be useful in combining parame-ters from different domain distributions.
We sum-marize CW learning to familiarize the reader.Parameter confidence is formalized by a Gaussiandistribution over weight vectors with mean ?
?
RNand diagonal covariance ?
?
RN?N .
The values?j and ?j,j represent knowledge of and confidencein the parameter for feature j.
The smaller ?j,j ,the more confidence we have in the mean parametervalue ?j .
In this work we consider diagonal covari-ance matrices to scale to NLP data.A model predicts the highest probability label,arg maxy?
{?1}Prw?N (?,?)
[yi(w ?
xi) ?
0] .The Gaussian distribution over parameter vectors winduces a univariate Gaussian distribution over thescore Si = w ?
xi parameterized by ?, ?
and theinstance xi: Si ?
N(?i, ?2i), with mean ?i = ?
?xiand variance ?2i = x>i ?xi.The CW algorithm is inspired by the Passive Ag-gressive (PA) update (Crammer et al, 2006) ?which ensures a positive margin while minimizingparameter change.
CW replaces the Euclidean dis-tance used in the PA update with the Kullback-Leibler (KL) divergence over Gaussian distribu-tions.
It also replaces the minimal margin constraintwith a minimal probability constraint: with somegiven probability ?
?
(0.5, 1] a drawn classifier willassign the correct label.
This strategy yields the fol-lowing objective solved on each round of learning:min DKL (N (?,?)
?N (?i,?i))s.t.
Pr [yi (w ?
xi) ?
0] ?
?
,where (?i,?i) are the parameters on round i andw ?
N (?,?).
The constraint ensures that the re-sulting parameters(?i+1,?i+1)will correctly clas-sify xi with probability at least ?.
For conveniencewe write ?
= ?
?1 (?
), where ?
is the cumula-tive function of the normal distribution.
The opti-mization problem above is not convex, but a closedform approximation of its solution has the follow-ing additive form: ?i+1 = ?i + ?iyi?ixi and?
?1i+1 = ?
?1i + 2?i?xix>i for,?i=?(1+2??i)+?(1+2??i)2?8?(?i???2i)4?
?2i.Each update changes the feature weights ?, and in-creases confidence (variance ?
always decreases).We employ CW classifiers since they provide con-fidence estimates, which are useful for classifiercombination.
Additionally, since we require per-parameter confidence estimates, other confidencebased classifiers are not suitable for this setting.4 Multi-Classifier Parameter CombinationThe basis of our approach to multi-domain learningis to combine the parameters of CW classifiers fromseparate domains while respecting parameter confi-dence.
A combination method takes M CW classi-fiers each parameterized by its own mean and vari-ance parameters {(?m,?m)}Mm=1 and produces asingle combined classifier (?c,?c).
A simple tech-nique would be to average the parameters of classi-fiers into a new classifier.
However, this ignores thedifference in feature distributions.
Consider for ex-ample that the weight associated with some word ina source classifier has a value of 0.
This could eithermean that the word is very rare or that it is neutralfor prediction (like the work ?the?).
The informa-tion captured by the variance parameter allow us todistinguish between the two cases: an high-varianceindicates a lack of confidence in the value of the690weight vectors because of small number of exam-ples (first case), and vise-versa, small-variance indi-cates that the value of the weight is based on plentyof evidence.
We favor combinations sensitive to thisdistinction.Since CW classifiers are Gaussian distributions,we formalize classifier parameter combination asfinding a new distribution that minimizes theweighted-divergence to a set of given distributions:(?c,?c) = arg minM?mD((?c,?c)||(?m,?m) ; bm) ,where (since ?
is diagonal),D((?c,?c)||(?,?)
; b) =?Nf bfD((?cf ,?cf,f )||(?f ,?f,f )) .The (classifier specific) importance-weights bm ?RN+ are used to weigh certain parameters of somedomains differently in the combination.
When D isthe Euclidean distance (L2), we have,D((?cf ,?cf,f )||(?f ,?f,f )) =(?cf ?
?f )2 + (?cf,f ?
?f,f )2 .and we obtain:?cf =1?Mm bmfM?mbmf ?mf ,?cf,f =1?m?M bmfM?mbmf ?mf,f .
(1)Note that this is a (weighted) average of parameters.The other case we consider is when D is a weightedKL divergence we obtain a weighting of ?
by ?
?1:?cf =(M?m(?mf,f )?1bmf)?1 M?m(?mf,f )?1?mf bmf(?c)?1 =(MM?mbmf)?1 M?m(?mf )?1bfm .
(2)While each parameter is weighed by its variance inthe KL, we can also explicitly encode this behavioras bmf = a ?
?mf,f ?
0, where a is the initializa-tion value for ?mf,f .
We call this weighting ?vari-ance?
as opposed to a uniform weighting of param-eters (bmf = 1).
We therefore have two combinationmethods (L2 and KL) and two weighting methods(uniform and variance).5 DatasetsFor evaluation we selected two domain adaptationdatasets: spam (Jiang and Zhai, 2007) and sentiment(Blitzer et al, 2007).
The spam data contains twotasks, one with three users (task A) and one with 15(task B).
The goal is to classify an email (bag-of-words) as either spam or ham (not-spam) and eachuser may have slightly different preferences and fea-tures.
We used 700 and 100 training messages foreach user for task A and B respectively and 300 testemails for each user.The sentiment data contains product reviews fromAmazon for four product types: books, dvds, elec-tronics and kitchen appliances and we extended thiswith three additional domains: apparel, music andvideos.
We follow Blitzer et.
al.
for feature ex-traction.
We created different datasets by modify-ing the decision boundary using the ordinal ratingof each instance (1-5 stars) and excluding boundaryinstances.
We use four versions of this data:?
All - 7 domains, one per product type?
Books - 3 domains of books with the binarydecision boundary set to 2, 3 and 4 stars?
DVDs - Same as Books but with DVD reviews?
Books+DVDs - Combined Books and DVDsThe All dataset captures the typical domain adap-tation scenario, where each domain has the samedecision function but different features.
Booksand DVDs have the opposite problem: the samefeatures but different classification boundaries.Books+DVDs combines both issues.
Experimentsuse 1500 training and 100 test instances per domain.6 Multi-Domain AdaptationWe begin by examining the typical domain adapta-tion scenario, but from an online perspective sincelearning systems often must adapt to new users ordomains quickly and with no training data.
For ex-ample, a spam filter with separate classifiers trainedon each user must also classify mail for a newuser.
Since other user?s training data may have beendeleted or be private, the existing classifiers must becombined for the new user.691Train L2 KLTarget Domain All Src Target Best Src Avg Src Uniform Variance Uniform VarianceSpamuser0 3.85 1.80 4.80 8.26 5.25 4.63 4.53 4.32user1 3.57 3.17 4.28 6.91 4.53 3.80 4.23 3.83user2 3.30 2.40 3.77 5.75 4.75 4.60 4.93 4.67Sentimentapparel 12.32 12.02 14.12 21.15 14.03 13.18 13.50 13.48books 16.85 18.95 22.95 25.76 19.58 18.63 19.53 19.05dvd 13.65 17.40 17.30 21.89 15.53 13.73 14.48 14.15kitchen 13.65 14.40 15.52 22.88 16.68 15.10 14.78 14.02electronics 15.00 14.93 15.52 23.84 18.75 17.37 17.45 16.82music 18.20 18.30 20.75 24.19 18.38 17.83 18.10 18.22video 17.00 19.27 19.43 25.78 17.13 16.25 16.33 16.42Table 1: Test error for multi-source adaptation on sentiment and spam data.
Combining classifiers improves overselecting a single classifier a priori (Avg Src).We combine the existing user-specific classifiersinto a single new classifier for a new user.
Sincenothing is known about the new user (their deci-sion function), each source classifier may be useful.However, feature similarity ?
possibly measured us-ing unlabeled data ?
could be used to weigh sourcedomains.
Specifically, we combine the parametersof each classifier according to their confidence us-ing the combination methods described above.We evaluated the four combination strategies ?
L2vs.
KL, uniform vs. variance ?
on spam and sen-timent data.
For each evaluation, a single domainwas held out for testing while separate classifierswere trained on each source domain, i.e.
no targettraining.
Source classifiers are then combined andthe combined classifier is evaluated on the test data(400 instances) of the target domain.
Each classi-fier was trained for 5 iterations over the training data(to ensure convergence) and each experiment wasrepeated using 10-fold cross validation.
The CWparameter ?
was tuned on a single randomized runfor each experiment.
We include several baselines:training on target data to obtain an upper boundon performance (Target), training on all source do-mains together, a useful strategy if all source data ismaintained (All Src), selecting (with omniscience)the best performing source classifier on target data(Best Src), and the expected real world performanceof randomly selecting a source classifier (Avg Src).While at least one source classifier achieved highperformance on the target domain (Best Src), thecorrect source classifier cannot be selected withouttarget data and selecting a random source classifieryields high error.
In contrast, a combined classifieralmost always improved over the best source domainclassifier (table 1).
That some of our results improveover the best training scenario is likely caused by in-creased training data from using multiple domains.Increases over all available training data are very in-teresting and may be due to a regularization effect oftraining separate models.The L2 methods performed best and KL improved7 out of 10 combinations.
Classifier parameter com-bination can clearly yield good classifiers withoutprior knowledge of the target domain.7 Learning Across DomainsIn addition to adapting to new domains, multi-domain systems should learn common behaviorsacross domains.
Naively, we can assume that thedomains are either sufficiently similar to warrantone classifier or different enough for separate clas-sifiers.
The reality is often more complex.
Instead,we maintain shared and domain-specific parametersand combine them for learning and prediction.Multi-task learning aims to learn common behav-iors across related problems, a similar goal to multi-domain learning.
The primary difference is the na-ture of the domains/tasks: in our setting each domainis the same task but differs in the types of features inaddition to the decision function.
A multi-task ap-proach can be adapted to our setting by using ourclassifier combination techniques.692Spam SentimentMethod Task A Task B Books DVD Books+DVD AllSingle 3.88 8.75 23.7 25.11 23.26 16.57Separate 5.46 14.53 22.22 21.64 21.23 21.89Feature Splitting 4.16 8.93 15.65 16.20 14.60 17.45MDR 4.09 9.18 15.65 15.12 13.76 17.45MDR+L2 4.27 8.61 12.70 14.95 12.73 17.16MDR+L2-Var 3.75 7.52 12.90 14.21 12.52 17.37MDR+KL 4.32 9.22 13.51 13.81 13.32 17.20MDR+KL-Var 4.02 8.70 14.93 14.03 14.22 18.40Table 2: Online training error for learning across domains.Spam SentimentMethod Task A Task B Books DVD Books+DVD AllSingle 2.11 5.60 18.43 18.67 19.08 14.09Separate 2.43 8.5 18.87 15.97 16.45 17.23Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73MDR 1.94 5.69 9.97 8.33 8.20 14.73MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daume?
(2007).We seek to learn domain specific parametersguided by shared parameters.
Dekel et al (2006)followed this approach for an online multi-task algo-rithm, although they did not have shared parametersand assumed that a training round comprised an ex-ample from each task.
Evgeniou and Pontil (2004)achieved a similar goal by using shared parametersfor multi-task regularization.
Specifically, they as-sumed that the weight vector for problem d could berepresented aswc = wd+ws, wherewd are task spe-cific parameters and ws are shared across all tasks.In this framework, all tasks are close to some under-lying meanws and each one deviates from this meanby wd.
Their SVM style multi-task objective mini-mizes the loss ofwc and the norm ofwd andws, witha tradeoff parameter allowing for domain deviancefrom the mean.
The simple domain adaptation al-gorithm of feature splitting used by Daume?
(2007)is a special case of this model where the norms areequally weighted.
An analogous CW objective is:min1?1DKL(N(?d,?d)?N(?di ,?di))+1?2DKL (N (?s,?s) ?N (?si ,?si ))s.t.
Prw?N (?c,?c) [yi (w ?
xi) ?
0] ?
?
.
(3)(?d,?d)are the parameters for domain d, (?s,?s)for the shared classifier and (?c,?c) for the com-bination of the domain and shared classifiers.
Theparameters are combined via (2) with only two ele-ments summed - one for the shared parameters s andthe other for the domain parameters d .
This capturesthe intuition of Evgeniou and Pontil: updates en-force the learning condition on the combined param-eters and minimize parameter change.
For conve-nience, we rewrite ?2 = 2?
2?1, where ?1 ?
[0, 1].If classifiers are combined using the sum of the indi-vidual weight vectors and ?1 = 0.5, this is identicalto feature splitting (Daume?)
for CW classifiers.The domain specific and shared classifiers can be693updated using the closed form solution to (3) as:?s = ?si + ?2?yi?cxi(?s)?1 = (?si )?1 + 2?2?
?xixTi?d = ?di + ?1?yi?cixi(?d)?1 = (?di )?1 + 2?1?
?xixTi(4)We call this objective Multi-Domain Regulariza-tion (MDR).
As before, the combined parametersare produced by one of the combination methods.On each round, the algorithm receives instance xiand domain di for which it creates a combined clas-sifier (?c,?c) using the shared (?s,?s) and domainspecific parameters(?d,?d).
A prediction is is-sued using the standard linear classifier predictionrule sign(?c ?
x) and updates follow (4).
The ef-fect is that features similar across domains quicklyconverge in the shared classifier, sharing informa-tion across domains.
The combined classifier re-flects shared and domain specific parameter confi-dences: weights with low variance (i.e.
greater con-fidence) will contribute more.We evaluate MDR on a single pass over a streamof instances from multiple domains, simulating areal world setting.
Parameters ?1 and ?
are iter-atively optimized on a single randomized run foreach dataset.
All experiments use 10-fold CV.
In ad-dition to evaluating the four combination methodswith MDR, we evaluate the performance of a sin-gle classifier trained on all domains (Single), a sep-arate classifier trained on each domain (Separate),Feature Splitting (Daume?)
and feature splitting withoptimized ?1 (MDR).
Table 3 shows results on testdata and table 2 shows online training error.In this setting, L2 combinations prove best on 5of 6 datasets, with the variance weighted combina-tion doing the best.
MDR (optimizing ?1) slightlyimproves over feature splitting, and the combinationmethods improve in every case.
Our best result isstatistically significant compared to Feature Split-ting using McNemar?s test (p = .001) for Task B,Books, DVD, Books+DVD.
While a single or sepa-rate classifiers have a different effect on each dataset,MDR gives the best performance overall.8 Learning in Many DomainsSo far we have considered settings with a smallnumber of similar domains.
While this is typicalof multi-task problems, real world settings presentmany domains which do not all share the same be-haviors.
Online algorithms scale to numerous ex-amples and we desire the same behavior for numer-ous domains.
Consider a spam filter used by a largeemail provider, which filters billions of emails formillions of users.
Suppose that spammers controlmany accounts and maliciously label spam as legiti-mate.
Alternatively, subsets of users may share pref-erences.
Since behaviors are not consistent acrossdomains, shared parameters cannot be learned.
Weseek algorithms robust to this behavior.Since subsets of users share behaviors, these canbe learned using our MDR framework.
For example,discovering spammer and legitimate mail accountswould enable intra-group learning.
The challenge isthe online discovery of these subsets while learningmodel parameters.
We augment the MDR frame-work to additionally learn this mapping.We begin by generalizing MDR to include kshared classifiers instead of a single set of shared pa-rameters.
Each set of shared parameters representsa different subset of domains.
If the correspondingshared parameters are known for a domain, we coulduse the same objective (3) and update (4) as before.If there are many fewer shared parameters than do-mains (k  D), we can benefit from multi-domainlearning.
Next, we augment the learning algorithmto learn a mapping between the domains and sharedclassifiers.
Intuitively, a domain should be mappedto shared parameters that correctly classify that do-main.
A common technique for learning such ex-perts in the Weighted Majority algorithm (Little-stone and Warmuth, 1994), which weighs a mixtureof experts (classifiers).
However, since we require ahard assignment ?
pick a single shared parameterset s ?
rather than a mixture, the algorithm reducesto picking the classifier s with the fewest mistakesin predicting domain d. This requires tracking thenumber of mistakes made by each shared classifieron each domain once a label is revealed.
For learn-ing, the shared classifier with the fewest mistakesfor a domain is selected for an MDR update.
Clas-sifier ties are broken randomly.
While we experi-694Figure 1: Learning across many domains - spam (left) and sentiment (right) - with MDR using k shared classifiers.Figure 2: Learning across many domains - spam (left) and sentiment (right) - with no domain specific parameters.mented with more complex techniques, this simplemethod worked well in practice.
When a new do-main is added to the system, it takes fewer exam-ples to learn which shared classifier to use instead oflearning a new model from scratch.While this approach adds another free parameter(k) that can be set using development data, we ob-serve that k can instead be fixed to a large constant.Since only a single shared classifier is updated eachround, the algorithm will favor selecting a previ-ously used classifier as opposed to a new one, usingas many classifiers as needed but not scaling up to k.This may not be optimal, but it is a simple.To evaluate a larger number of domains, we cre-ated many varying domains using spam and senti-ment data.
For spam, 6 email users were created bysplitting the 3 task A users into 2 users, and flippingthe label of one of these users (a malicious user),yielding 400 train and 100 test emails per user.
Forsentiment, the book domain was split into 3 groupswith binary boundaries at a rating of 2, 3 or 4.
Eachof these groups was split into 8 groups of which halfhad their labels flipped, creating 24 domains.
Thesame procedure was repeated for DVD reviews butfor a decision boundary of 3, 6 groups were created,and for a boundary of 2 and 4, 3 groups were createdwith 1 and 2 domains flipped respectively, resultingin 12 DVD domains and 36 total domains with var-ious decision boundaries, features, and inverted de-cision functions.
Each domain used 300 train and100 test instances.
10-fold cross validation with onetraining iteration was used to train models on these695two datasets.
Parameters were optimized as before.Experiments were repeated for various settings ofk.
Since L2 performed well before, we evaluatedMDR+L2 and MDR+L2-Var.The results are shown in figure 1.
For both spamand sentiment adding additional shared parametersbeyond the single shared classifier significantly re-duces error, with further reductions as k increases.This yields a 45% error reduction for spam and a38% reduction for sentiment over the best baseline.While each task has an optimal k (about 5 for spam,2 for sentiment), larger values still achieve low error,indicating the flexibility of using large k values.While adding parameters clearly helps for manydomains, it may be impractical to keep domain-specific classifiers for thousands or millions of do-mains.
In this case, we could eliminate the domain-specific classifiers and rely on the k shared clas-sifiers only, learning the domain to classifier map-ping.
We compare this approach using the best resultfrom MDR above, again varying k. Figure 2 showsthat losing domain-specific parameters hurts perfor-mance, but is still an improvement over baselinemethods.
Additionally, we can expect better perfor-mance as the number of similar domains increases.This may be an attractive alternative to keeping avery large number of parameters.9 Related WorkMulti-domain learning intersects two areas of re-search: domain adaptation and multi-task learning.In domain adaptation, a classifier trained for a sourcedomain is transfered to a target domain using eitherunlabeled or a small amount of labeled target data.Blitzer et al (2007) used structural correspondencelearning to train a classifier on source data withnew features induced from target unlabeled data.
Ina complimentary approach, Jiang and Zhai (2007)weighed training instances based on their similarityto unlabeled target domain data.
Several approachesutilize source data for training on a limited numberof target labels, including feature splitting (Daume?,2007) and adding the source classifier?s predictionas a feature (Chelba and Acero, 2004).
Others haveconsidered transfer learning, in which an existingdomain is used to improve learning in a new do-main, such as constructing priors (Raina et al, 2006;Marx et al, 2008) and learning parameter functionsfor text classification from related data (Do and Ng,2006).
These methods largely require batch learn-ing, unlabeled target data, or available source dataat adaptation.
In contrast, our algorithms operatepurely online and can be applied when no target datais available.Multi-task algorithms, also known as inductivetransfer, learn a set of related problems simultane-ously (Caruana, 1997).
The most relevant approachis that of Regularized Multi-Task Learning (Evge-niou and Pontil, 2004), which we use to motivateour online algorithm.
Dekel et al (2006) gave a sim-ilar online approach but did not use shared parame-ters and assumed multiple instances for each round.We generalize this work to both include an arbi-trary classifier combination and many shared classi-fiers.
Some multi-task work has also considered thegrouping of tasks similar to our learning of domainsubgroups (Thrun and O?Sullivan, 1998; Bakker andHeskes, 2003).There are many techniques for combining the out-put of multiple classifiers for ensemble learning ormixture of experts.
Kittler et al (Mar 1998) providea theoretical framework for combining classifiers.Some empirical work has considered adding versusmultiplying classifier output (Tax et al, 2000), usinglocal accuracy estimates for combination (Woods etal., 1997), and applications to NLP tasks (Florian etal., 2003).
However, these papers consider combin-ing classifier output for prediction.
In contrast, weconsider parameter combination for both predictionand learning.10 ConclusionWe have explored several multi-domain learningsettings using CW classifiers and a combinationmethod.
Our approach creates a better classifier fora new target domain than selecting a random sourceclassifier a prior, reduces learning error on multipledomains compared to baseline approaches, can han-dle many disparate domains by using many sharedclassifiers, and scales to a very large number of do-mains with a small performance reduction.
Thesescenarios are realistic for NLP systems in the wild.This work also raises some questions about learningon large numbers of disparate domains: can a hi-696erarchical online clustering yield a better represen-tation than just selecting between k shared parame-ters?
Additionally, how can prior knowledge aboutdomain similarity be included into the combinationmethods?
We plan to explore these questions in fu-ture work.Acknowledgements This material is based uponwork supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
FA8750-07-D-0185.ReferencesB.
Bakker and T. Heskes.
2003.
Task clustering and gat-ing for bayesian multi?task learning.
Journal of Ma-chine Learning Research, 4:83?99.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
In As-sociation for Computational Linguistics (ACL).Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28:41?75.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmax- imum entropy classifier: Little data can help alot.
In Empirical Methods in Natural Language Pro-cessing (EMNLP).Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Hal Daume?.
2007.
Frustratingly easy domain adaptation.In Association for Computational Linguistics (ACL).Ofer Dekel, Philip M. Long, and Yoram Singer.
2006.Online multitask learning.
In Conference on LearningTheory (COLT).Chuong B.
Do and Andrew Ng.
2006.
Transfer learningfor text classification.
In Advances in Neural Informa-tion Processing Systems (NIPS).Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.In International Conference on Machine Learning(ICML).Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi-task learning.
In Conference onKnowledge Discovery and Data Mining (KDD).Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Conference on ComputationalNatural Language Learning (CONLL).Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In Association forComputational Linguistics (ACL).J.
Kittler, M. Hatef, R.P.W.
Duin, and J. Matas.
Mar1998.
On combining classifiers.
Pattern Analy-sis and Machine Intelligence, IEEE Transactions on,20(3):226?239.N.
Littlestone and M. K. Warmuth.
1994.
The weightedmajority algorithm.
Information and Computation,108:212?261.Zvika Marx, Michael T. Rosenstein, Thomas G. Diet-terich, and Leslie Pack Kaelbling.
2008.
Two algo-rithms for transfer learning.
In Inductive Transfer: 10years later.Rajat Raina, Andrew Ng, and Daphne Koller.
2006.Constructing informative priors using transfer learn-ing.
In International Conference on Machine Learn-ing (ICML).David M. J.
Tax, Martijn van Breukelen, Robert P. W.Duina, and Josef Kittler.
2000.
Combining multipleclassifiers by averaging or by multiplying?
PatternRecognition, 33(9):1475?1485, September.S.
Thrun and J. O?Sullivan.
1998.
Clustering learningtasks and the selective cross?task transfer of knowl-edge.
In S. Thrun and L.Y.
Pratt, editors, Learning ToLearn.
Kluwer Academic Publishers.Kevin Woods, W. Philip Kegelmeyer Jr., and KevinBowyer.
1997.
Combination of multiple classifiersusing local accuracy estimates.
IEEE Transactions onPattern Analysis andMachine Intelligence, 19(4):405?410.697
