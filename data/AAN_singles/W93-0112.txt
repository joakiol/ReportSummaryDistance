Structural Methods for Lexica l /Semant ic  PatternsScott A. WatermanBrande is  Un ivers i tyComputer  Science Dept .Wa l tham,  MA 02254-9110emai l :  waterman@cs .brande is .eduABSTRACTThis paper represents initial work on corpus methods for ac-quiring lexical/semantic pattern lexicons for text understand-ing.
Recently, implementors of information extraction (IE)systems have moved away from using conventional syntac-tic parsing methods, instead adopting a variety of patternbased approaches for complex IE tasks.
While there has beenmuch work towards automated acquisition of lexicons for con-ventional syntactic processing, little progress has been madetowards those for pattern systems, due primarily, in the au-thor's opinion, to a lack of a linguistic framework in whichto view their use.
In combining a functional view of both de-notational semantics and syntactic structure, this paper pro-vides a basis for examining the structural constraints betweenthe two.
This functional viewpoint is the starting point formethods to investigate the characteristics of the interactionbetween text and denotation, from the perspective of pattern-based systems.
An approach for determining and exploitingthese structural constraints i outlined in terms of buildinghierarchical lexical structures for text understanding.
Exper-iment results for such a method are given, demonstrating thefunctionality of the approach.1.
In t roduct ion .Recently, implementors of information extraction (IE)systems have moved away from using conventional syn-tactic parsing methods, instead adopting a variety ofpattern based approaches for complex IE tasks (such asthe MUC contests and the ARPA-sponsored TIPSTERresearch).
These pattern-based systems use short andfairly specific lexical patterns to specify the relation be-tween strings in the source text and particular entriesin a problem-dependent k owledge representation.
Thisone-step rocess ubstitutes for a conventional two-levelprocess of a full syntactic parse followed by semanticinterpretation.
With considerably less time and devel-opment effort (notably demonstrated by \[11, 8\]), thesesystems achieve performance comparable to more stan-dard systems that rely heavily on full syntactic analy-sis (\[9, 5\]).
However, because these pattern-based sys-tems are still viewed as linguistically ungrounded andsomewhat ad hoc, formal work in the application andacquisition of lexical patterns has lagged system devel-opment.
In most current systems, patterns are producedthrough tedious hand analysis of text (\[11, 4, 8\]), whilesystem coverage is gained either through extensive lin-guistic knowledge on the part of the researcher (in judg-ing appropriate pattern generalizations), or by generat-ing and testing massive numbers of patterns.One exception to hand analysis is Lehnert's work in \[12\],in which machine learning techniques are used to inferpossible patterns for extraction.
While this AutoSlogtechnique has dramatically reduced system developmenttime, the inference techniques use only sparse linguis-tic information, provide no means of generalizing pat-terns across domains, and still require that the rules bechecked by the researcher for applicability.1 .1.
A theoret i ca l  f rameworkBy placing pattern-based approaches in a lexical seman-tic framework, such as Pustejovsky's Generative Lexicontheory (\[16\]), my aim is to provide a basis for pattern-based understanding systems which can be used to relatepattern-based approaches to more conventional linguis-tic approaches.
Within such a framework, methods forpattern acquisition can be studied and developed andthe effectiveness of patterns can be assessed.My main contention is that this framework can be devel-oped by viewing the lexieal patterns as structural map-pings from text to denotation in a compositional lexi-cal semantics, merging the distinction between syntacticand semantic analysis, and obviating the need for sep-arate syntactic and semantic processing systems.
Thisinterpretation follows directly from an appeal to func-tional semantic principles.
In the framework I present,patterns indexed to individual words relate semantic in-terpretations to lexical constraints, in a manner dictatedby context.
Patterns for multiple words in context can becombined to provide a consistent interpretation for largeconstructions - - a mechanism that could be viewed as alexically distributed semantic grammar.A combined approach to pattern acquisition is out-lined here, with two orthogonal methods whose combi-nation leads to the construction of organized sets of lex-128teal/semantic patterns which relate strings in the sourcetext to denotations in a predicate structured knowledgerepresentation.
A key feature of the mechanism is thatthese resulting patterns are organized hierarchically interms of the specificity of both syntactic and semanticconstraints.The methods presented are based on the strncturalprop-erties of the text and denotation, attacking the prob-lem of pattern acquisition from the separate directionsof a purely syntactic approach and a purely semanticapproach.
The natural merger of the two methods re-sults in an automatic machine learning technique for theproduction of lexical/semantic patterns, relating use incontext o denotation.Related works describing corpus techniques for deducinglexical structure (\[lS, 19\]) and semantically marked se-lectional frames (\[6\]) suggest hat lexical/semantic pat-terns can be induced from corpora, but do not directlyapply to the generation of these distributed patterns.2.
In fo rmat ion  Ext rac t ionThe recent Message Understanding Conferences (MUCs)and APt.PA Tipster project have posed a complex andfairly specific problem in information extraction (IE).The problem given is that of creating semantic tem-plates or frames to correspond to newswire and newspa-per articles.
The expressiveness of the templates is re-stricted and somewhat skeletal, capturing the bare factsof the text, and not its complete meaning.
Hobbs (\[8\])has argued effectively that the problem is not one offull text understanding, but specifically one of informa-tion extraction ---: many types of information, such asspeaker attitude, intensional constructs, facts not rele-vant to the chosen domain, etc., are not required; only arepresentation-specific set of domain information is thetarget for extraction.These types of systems provide a useful groundwork forthe study of text interpretation systems because of therelative lack of difficulty in representing and manipulat-ing the resulting knowledge structures.
Although deno-tational structures for the type of factual informationrequired in IE can be quite complex, they are still farmore tractable than representations of speaker attitude,opaque contexts, or intensionai constructions.For example, in the ongoing TIPSTER project, informa-tion in only two specific domains is to be extracted -one domain is joint ventures and business ownership, theother the microelectronics industry.
The domains arefurther restricted by the particular hierarchy of predi-cate types used in the knowledge representation.
Eachdomain has a set of templates (a particular implementa-tion of frames) which rigidly define what types of factsand relations from the text are representable.2.1.
Mapp ing-  IE  : text~- -~KRThese information extraction tasks, as a subset of textunderstanding tasks, can be viewed as mapping prob-lems, in which the problem is to find the proper repre-sentation, in terms of templates, for the source text.
Theproblem is one of mapping from the strings of the sourcetext to a problem-dependent knowledge representationscheme.The template knowledge representation used in the TxP-STER/MUC tasks is based on a frame-like system com-monly known as the entity-relation, or ER., model.The El:t model codes information as multi-place rela-tions.
Typically, each type of relation has a fixed numberof arguments, each of which is an entity in the model.Entities can either be atomic - -  in the case of Ill'STEP,.atoms can be strings from text or items from a prede-termined hierarchy of types - -  or they can be composite,referring to other relational structures.Objects referenced in text often participate in more thanone relationship.
For example, the direct object of a sen-tence will often be the subject of a subordinate clause,either explicitly, or by pronominal reference.
In a strictEl:t model, this direct object would have to be repre-sented twice, once for each clause.
By a slight extension,atoms in the ER model can be generalized to objectswhich can take multiple references.
Thus, no real atomsappear in relations, but only references to atoms, or toother relations.
This model is often termed an object-oriented model, but because of the overloading of thatname in so many fields, I prefer to call these modelsreference-relation models (RR).
The important extentionfrom the ER model is that relations themselves may I)etreated as objects of reference by other relations.3.
Lex ica l  Semant icsThe structure of the denotational representation is im-portant not only for its expressiveness, but also in itsrelationship to the structure of the language it is to bederived from.
In part, the structure of the language isdetermined by the semantic onstraints of relations thatare conveyed by its use.
If the model is accurate nough,these constraints will be reflected in the representation.Many, if not most, semantic theories used in computa-tional linguistics today assume some degree of function-ality in language - -  words act as operators, or take ar-guments, or act as logical quantifiers over the things de-noted in their context.
The corresponding grammatical129theories (e.g.
CFG, LFG, HPSG, GB) assume a par-allel functional structure, incorporating notions of com-binational categories, argument structure, or selectionalframes into the lexical representation.
These structuresuse individual words or constituent phrases as functionalobjects, projecting expectations or constraints o subcat-cgorize for arguments to be incorporated into the func-tion.3.1.
Structural ly specified semanticrelationsThe functional semantics of the operators, then, spec-ify the nature and lexical appearance of the arguments.The appearance of a particular head will generate x-pectations for the number and kind of arguments to befound, and dictate the semantic relation to be appliedto them - -  because we have seen the operator, we canexpect to find its operands in the vicinity.
Further, ifthese operands do not have distinct types, we will needsome other mechanism, such as position or order, to beablc to distinguish them.
In this way, the need for syn-tactic structure is driven by typing ambiguities in thesemantics.There is an immediate parallel between the semanticspecification of function/argument s ructure and thespecification of the reference-relation representations:the function is analogous to the predicate relation, whilethe arguments are the referenced components of the re-lation.
In computational linguistic models, this sortof functional semantics has proved very useful in pro-viding a mechanism for deriving frame-like denotationswhen processing language (predicate logic and unifica-tion frames, two of the more popular denotation schemes,can both be transformed to general RR models).
In fact,it is often the case that the relations of the RR modelare the same as the semantic relations pecified by thelanguage.
(Whether this is because of a desire for rep-resentational efficiency or for other reasons I will leaveunexplored.
)Semant ica l ly  specif ied s t ruc tura l  in terpretat ion :We can rephrase the relation between a functional headand its arguments in the following way: since the headrequires a particular semantic relation to its arguments,an argument with an otherwise ambiguous interpreta-tion must be treated as being of the type required bythe head.
Because we know the interpretation of the op-erator, we can constrain the various arguments to havea corresponding and consistent interpretation.This type of argument disambiguation is exhibited in thephenomenon of type coercion (\[16\]).3.2.
The syntax-semantics boundaryIn terms of the function-argument structure or reference-relation representations, words or categories with similartype amibiguities and similar argument number are de-scribed as being syntactically similar, while differing ininterpretation.
On the other side, categories with similarfunctional or relational type are said to have similar se-mantics, even though the number and typical realizationof arguments might differ considerably.As the specificity of the relational constraints varies,the distinction between the two can also vary.
Somehighly cased languages (e.g.
Japanese and Latin) haveloose syntactic constraints; the case marking developsconstraints for the consistent semantic incorporation ofthe various arguments within the functional scope of theheads.
Other languages, such as English, have a muchmore definite word order, where the configuration of ar-guments and heads constrains their semantic relation-ships.
Some constructions, such as idiomatic expres-sions, have both completely a fixed syntax and seman-tics.
Poetic use has both a freedom of word order anda loose interpretation.
Each form of linguistic construc-tion, however, has a consistency of interpretation derivedfrom its components.By using a mechanism of language interpretation thatexplicitly examines the degree of specificity in argumentposition and in argument ype, and especially their in-teraction with one another in use, one should be betterable to achieve the goals of interpretation; that is, torelate the text to a particular denotation.4.
The Generative LexiconTheoretical approaches to lexical semantics have begunto incorporate this merging of syntactic and semanticdescription.
The incorporation of argument structure orselectional frames is a large step in this direction.
Whilethe notion of argument structure is usually reserved forverbs, some theories, such as Pustejovsky's generativelexicon (GL), extend the idea to include all lexical cat-egories (\[16, 17\]).
For the purposes of this discussion,we can consider the GL lexicon to carry two sorts ofselectional information with every term:?
Qualia structure, which provide semantic type con-straints on arguments.
These constraints are usedboth in deriving expectations for the syntactic formof arguments, and in coercing ambiguous or polyse-mous arguments into the required types (\[16\]).?
Cospecifications, which constrain the syntactic real-izations and ordering of arguments in relation to thelexical entry and to each other.
These constraints130are specified much like regular expressions, and canprovide varying degrees of 'fit' to the syntax.In addition to these selectional constraints, each termhas a mapping from the arguments to a predicate logicdenotation, detailing the relationship in which the argu-ments participate.These three together embody what Pustejovsky calls alexieal-eouceptuai p radigm (LCP), a representation ofthe expression of a particular concept, and the paradig-matic usage in context of the lexical entry to expressthat concept (\[19\]).It is easy to see how a theoretical approach such as GLcan be operationalized: A local grammar, correspond-ing to the cospecifications, and indexed off the lexicalentry, could be used in conjunction with a type match-ing system which imposes the semantic onstraints ofthe qualia structure.
The resulting mechanism, whenmatched against text, could place the matching argu-ments appropriately in the predicate denotation to re-turn an interpretation of the text.This system, which by conjoining argument ype andpositional information avoids making a distinction be-tween separate syntactic and semantic analysis, wouldbe a pattern system.This system has been implemented, in part, in theD1BEROT information extraction system (\[4\]).5.
Pat ternsPattern-based extraction systems combine syntactic andsemantic processing through the use of patterns.
Pat-terns consist of lexically specified syntactic templatesthat are matched to text, in much the same way as reg-ular expressions, that are applied along with type con-straints on substrings of the match.
These patterns areiexically indexed local grammar fragments, annotatedwith semantic relations between the various argumentsand the knowledge representation.
In the most generalsystem, the units of matching could range from singlelexical items to phrasal components or variables witharbitrary type constraints.
The variables in the patterncan be mapped directly into the knowledge representa-tion, or, through type constraints, used as abstract spec-ifications on the syntax.
Pattern-based systems operateby combining numerous local parses, without relying ona full syntactic analysis.5 .1.
DIDEROT, a pat tern  exampleFor example, in the DIDEROT project (\[4\]), a pattern isrepresented as a GL structure (GLS) which gives the syn-gls (establ ish,syn( .
.
.  )
,args( \[argl (AI,syn( \[type (np) \] ) ,qualia( \[formal ( \[code_2, organization\] )\] ) ),arg2 (h2,syn( \[type (np)\] ),qualia( \[formal ( \[code_2, organization\] )\] ) ),arE3 (A3,syn( \[type (np)\]),qualia( \[formal( \[code_2, j oint_organ\] )\] ) )\] ),qualia( \[formal (t ie_up_icp) \] ),?ospe?
( \[\[hi ,*, se l f  ,* ,A2,* ,with,  A3\],\[A1, and,A3,*, self ,*,A2\],\[A 1, together, with, A3, * ,  se l f ,  *,  A2\] ,\ [A2 , i s , to ,be ,se l f , * ,w i th ,  A3\],\ [A l , * , s igned ,* ,agreement , * , se l f ,  A2\],\[At, *, self, *, joint, venture, A2,.ith, A3\],\[self, include ,h2\],\[A2, I~as ,self, with, h3\] \] ),types (tie_up_verb),t empl at e_ s emant i c s (pt_t ie_up,tie_up( \[AI ,A3\] ,A2,_,existing,_))).Figure 1: A GLS for 'establish'tactic context along with mappings from text variablesto an predicate logic knowledge representation.
A typi-cal set of patterns used to cxtract joint-venture vents,indexed here from the word 'establish', is given in fig-ure 1.The GL cospecification i formation is contained in thccospec field.
The index variable ' se l f '  is used to rcferto an appearance of any of the morphological forms of'establish'.
These forms are given in the gyn( .
.
.  )
) field(omitted here for brevity).
Literals, such as 'venture' or'agreement' must match the text exactly.
The args fieldindicates that argument variables AI and A2 must bc re-alized syntactically as type(np) ,  where np designates aclass of strings which are heuristically noun phrases.
Theargument variables are further restricted to the semantictype path \ [code_2, jo int_organ) \ ] .
The type path es-tablishes a region in a type hierarchy which must containthe type of the argument (\[20\]).
The last component ofthe cospec, '*', is a Kleene star over all tokens - -  any-thing or nothing may appear in this position.Because of the difficulty and expense of deriving pat-terns, GLSs cannot be produced for every term of im-portance.
Rather, large segments of the lexicon are stat-ically typed in a sublexicon less intricate than the GLS131lexicon.
When the GLS is applied to text, the matchingof argument variables is accomplished either by calls toGLSs of the appropriate type, or by the invocation ofsmall heuristic grammars.
These small grammars com-bine the type information of their constituents to matchthe constraints of the governing GLS.These grammars are used especially for proper namerecognition.
Both company names and human names arematched using small grammars based on part-of-speechtags and the sublexicon typing.
Some company namesare keyed from semantic indicators uch as 'Corp.'
and'Inc.
', while many human and place names are identifiedfrom a large fixed name lexicon.Overall, other pattern-based systems operate in muchthe same manner, varying somewhat in the amount ofmachinery for pattern-matching, and the richness of thetyping systems.6.
The  cur rent  s ta te  o f  Pat ternAcqu is i t ionThe TIPSTER and MUC projects have provided a wealthof knowledge about building pattern-based systems.
Thehardest and most time-consuming task involved is cer-tainly the acquisition of patterns, which is still done pri-marily by tedious hand analysis.
Working backwardsfrom the key templates (hand generated knowledge rep-resentations of texts as interpretted by the project spon-sors), one can, by careful reading of the text, usually findthose segments of text which corresponds to the repre-sentation entries Although the key templates are orig-inally created by a researcher doing a careful reading,the correspondence b tween text segments and the keytemplates has not been recorded, making the process er-ror prone and leaving the text open for reinterpretation.The next step, that of correlating the text with the rep-resentation and deriving a pattern which captures the re-lation, is the most tedious and difficult part of the task.Typing constraints for each class of predicate must beremembered by the researcher performing the task, andinteractions between patterns must be identified and an-alyzed for possible interference.Here is a short (and most likely incomplete) review ofthe state-of-the-art in pattern acquisition, as it exists inthe IE community:CIRCUS (Lehnert et al \[11\]) - -  Handwritten CN (con-cept node) patterns for partial template extraction.Many man-hours were spent reading text, extracting allpossibly relevant contexts.
Patterns were checked byrunning the system.
A knowledge-poor method withgood coverage due to large numbers of trials.Shogun (Jacobs et al - -  Handwritten AWK scripts.
De-rived from compiled lists of company names, locations,and other semi-regular semantic types.
Also from re-searcher analysis of these in context.
Designed to aug-ment or replace previous methods with similar function-ality.FASTUS (Hobbs, Appelt, et al\[8\]) - -  Handwritten reg-ular expression-like patterns for partial template xtrac-tion.
Years of linguistic system building expertise im-proved pattern generality and helped avoid interactionsbetween patterns.DIDEROT (Cowie, Pustejovsky, et al\[4\]) - -  Patterns forfull template xtraction.
Initial patterns automaticallyderived from structured dictionary entries \[2, 25\] givemoderately effective high level patterns.
Partly auto~mated tuning to corpus usage.
Hand analysis of contextsand addition of patterns was used to complete coverage.CIRCUS + AutoSlog (Lehnert et al\[12\]) - -  Automatedreference from template to text, using machine learninginference techniques, gives much of the coverage previ-ously provided by hand-analysis.
Patterns must still becorrected by the researcher.The AutoSlog approach has obtained the most signif-icant benefit from automated acquisition.
In this sys-tem, a sentence containing a string which correspondsto a template ntry is analyzed for part-of-speech andmajor phrasal boundaries.
If the string entry from thetemplate aligns with one of the phrases, a pattern is gemcrated corresponding to the observed syntactic structure.However, since the generated AutoSlog patterns are pro-duced from single occurrences of context patterns, theyare not likely to capture patterns generalizing to vary-ing contexts.
In addition, the acquisition method is soclosely tied only to specific parts of the knowledge repre-sentation (in that string entries only are matched) thatextending the coverage, or generalizing the domain ap-pears to be as difficult as porting to entirely new do-mains.7.
S t ruc tura l  S imi la r i ty  C lus ter ingThe pattern systems described here attempt o relatethe use of terms in context o corresponding denotations.One of the major assumptions made here, as well as in allalgorithmic omputational linguistic systems, is one ofconsislency of use and meaning - -  that a term or phrase(or any linguistic structure) used in a particular fashionwill give rise to a particular denotation.
The goals ofany grammar induction or lexical semantic acquisitionproblem are to define those particulars - -  to find the132distinguishing features of the usage as they relate to thefeatures of the denotation.The approach given here chooses to focus only on thestructural features of usage and denotation.
By classify-ing features relevant o the text~--,denotation mapping,the aim is to provide a vocabulary and mechanism forderiving and evaluating interpretation procedures.It has been noted already that there exist paradig-matic usages of terms to express particular concepts (theLCPs).
It is not a large leap to venture also that partic-ular concepts have paradigmatic expressions in words -idiomatic expressions, 'stock phrases' and proper namesbeing the most obvious examples.
The relationship be-tween the two can be approached from both directions -by classifying the uses of a word in terms of their conven-tional expression of a concept, or by classifying the ex-pressions of a concept in terms of the words used.
Theseclassifications create a vocabulary that can be used tocompare and relate words with concepts.This work provides a step in forming such a vocab-ulary by examining methods for classifying the struc-tural properties of the words and denotations separately,and in suggesting methods by which they could be uni-fied.
Classification methods for both lexical and seman-tic structure are outlined here.
An experimental im-plementation of the lexical approach is presented in thelatter sections of the paper.7 .1.
Lex ica l  s t ruc tureWithout considering its semantics, the use of a word canbe expressed solely by its lexical environment, or con-text.
Grammar-driven systems as well as pattern sys-tems achieve their performance by relying on the ex-pected structural properties of the language.
We canexpress the consistencies and paradigms in the usage ofa word in explicit terms of the similarities and commonstructural properties of the lexical environment in whichthat word appears,A large collection of usages could be analyzed to findnatural classes of context, defined purely in terms ofthe lexical environment, o give a vocabulary of contexttypes that can be used to compare and relate differingwords.
The similarities of context would be determinedby the structural similarities of their component stringsof words.
The presence and relative ordering of identicalwords, words belonging to the same structural similar-ity classes, or phrasal components, recursively defined interms of context types, would be the environment fea-tures necessary for determining these classes.Groups of contexts could be organized into context ypesbased on these similarity measures, with group member-ship determined by similarity.
The contexts could beassembled into a hierarchical structure, in which groupsof high similarity combine to form higher-order clustersencompassing the structural features of their componentgroups.Word classes could be defined inductively on this treeof context types by classifying words according to thesets of context ypes in which they have appeared.
Thehierarchy of context ypes and word classes encodes thespecificity of the relation to the category.
Lower levelsof the hierarchy have strict context constraints, whilehigher levels, combining the classes beneath them, placelooser constraints on context patterns.
By studying thelexical context classes in relation to the semantic prop-erties of the terms, we could illuminate those features ofcontext which correlate with, and in theory constrain,their semantic properties.An experimental method for performing these sorts ofclassification is presented in the later part of this pa-per, using string edit distance as a metric of similarity,and agglomerative clustering techniques to provide theclassification structure.7.2.
Semant ic  s t ruc tureIn an analogous way, the predicate denotations of textcould be classified purely from their structural proper-ties.
In exactly the same manner as for context classes,relation predicates could be grouped hierarchically basedon their structural features.
The features one could useto derive predicate classes include predicate arity, speci-ficity, argument ypes, and structure depth, as well asa semantic type hierarchy or lattice defined for specificdomain.The large databases of parallel text and denotations that,would be necessary for this are not as freely available astext corpora for study.
Representations would have to begenerated by hand.
However, the work in template fillingand analysis contributed by the research community tothe TIPSTER effort has shown that deriving a sufficientvolume is not out of the question.This classification of predicate structure would providea basis for examining the constraints which predicatestructure nforces on lexical realization.7.3.
I n tegrat ionThe natural integration of these two lines of study wouldresult in a vocabulary of semantic and lexical classes thatwould enable the correlation of the lexieal structure of atext with its denotational structure, and the derivation133of structural mappings between the two.As an example of the benefits this integration might giveto interpretation or IE systems, consider the followingexample, from the TIPSTER/MUC-5 domain:Imagine a researcher developing the domain-dependentvocabulary for an IE system.
Assume that the systemhas a classification of the structural properties of gen-eral text, and has also a type hierarchy for general anddomain-specific representations.The researcher has annotated a short segment of textwith its interpretation in the problem domain.
(Seefig.
2).
In the figure, the indices relate segments of textto their corresponding denotations.
SMALL CAPS areused in the denotation to indicate known quantities inthc domain specific type hierarchy; mixed case is usedfor unknown types.\[A\[BIBM\]B is jointly developing \[cpractical X-ray toolsfor \[Dthe manufacture of \[Gdevices\]G \[Bbasedon 0.25 micron or small geometries\]E\]D\]C with\[fUotorola\]F\]A.DEVELOPMENTAAGENT:"IBM" B"Motorola" FPRODUCT:"tools" cTYPE:X-RAYUSE:MANUFACTUREDPiODUCT: "devices" aFEATURE_SIZE:0.25 /~M EFigure 2: A segment of text, marked against a predicateinterpretationNow that the researcher has provided a connection be-tween text and denotation, the system can use the clas-sifications of context and mapping types as a vocabu-lary to describe the relation.
For instance, it is nowknown that ' IBM', and also 'Motorola', can be AGENTarguments, and specifically the AGENT arguments of aDEVELOPMENT predicate.
The system probably has anLCP encoding the co-agentive functionality of'with',  butnow learns specifically that the DEVELOPMENT predicateallows this behavior, and that a configuration giving thatinterpretation is:\[A1 .
.
.
PRODUCT with A2\]This knowledge can augment both the LCP for 'with'and the mapping structures for DEVELOPMENT relations.Once the system has been provided with more text-denotation pairs particular to the domain, it may finda correlation between lexical structures containing theword 'developing' and DEVELOPMENT predicate struc-tures, and then postulate mappings between the two,building an LCP for 'developing'.
Or, relying more heav-ily on general structural knowledge, the system could usean existing LCP for the word 'is', as represented by thesyntactic pattern\[ARGlis'X'ARG2 ...\]and the predicate structureX-PREDARG1ARG 2(where the word 'X' is correlated with the predicate X-PRED).
This general mapping for 'is' could be used topostulate a correlation between 'developing' and DEVEL-OPMENT.Only through the development of a catalog and vocabu-lary of structural descriptions, however, could one hopeto build a system such as this.8.
Edit  D is tanceOne method for judging the similarity between stringsof lexical items (tokens) is the edit distance formulatedby Levenshtein (\[13\]).
This is a similarity measure basedon the minimum number of token insertions, deletions,and substitutions (mutations) required to transform onestring into another.
A generalization of this edit distancecan be made by assigning differing weights to insertionsof particular tokens or classes of tokens, and by also as-signing weights to token substitution pairs.
Straight-forward computational methods for finding the edit dis-tance between two strings (\[22, 24\]) have been used ona variety of problems in biology, genetics, speech andhandwriting analysis (\[21\]), as well as in syntactic anal-ysis of formal anguages (\[14\]).
(For a good introductionwith applications to many domains, see \[21\].
)To demonstrate the generalized edit distance, considerthe two strings:134the path that is the paththe way that i:~ not the wayThe first string can be transformed into the second bya number of insertion, deletion, and substitution oper-ations.
Substitutions are commonly counted as two op-erations, since they give the same effect as a deletion-insertion combination.
In this example, 'not' could beinserted; 'path' could be substituted by 'way', then thesecond 'path' deleted at the end, then 'way' inserted;'that'  could be deleted then reinserted, and then 'not'inserted; etc.
Many different sequences lead to the sameresult, but there will be a minimum number of opera-tions required for the transformation.After a short inspection, we could expect a minimumof 5 operations in this case - two for each change from'path' to 'way', and one for the insertion of 'not'.This distance measure can be generalized to compensatefor different similarities between types of tokens.
For in-stance, if one decides that 'way' and 'path' are more sim-ilar to each other than either is, say, to 'is' or 'the', thenit would be good to have the substitution of 'path ' - 'way'amount o less than the possible substitution 'path'- ' is ' .To accomplish this, a cost can be associated with eachoperation, perhaps even a different cost for each sortof insertion or substitution.
Then a transformation ofminimum cost, rather than minimum operations, can bedefined.
If one makes the simple assumption that a sub-stitution costs no more than the corresponding deletion-insertion pair, then this minimum cost can be shown toobey metric properties, and defines the generalized editdistance between the two strings, with larger distancescorresponding to less similar strings.There is a straightforward method for computing editdistance.
In a prime example of dynamic programming,the edit distance is computed for every pair of initialsubstrings of the two strings under study, with resultsfor shorter substrings combining to give results for longersubstrings.More explicitly, let our two strings be A =(a0,a l , .
.
.
,am)  and B = (bo,bl,...,bn), where a, is theith token in string A, starting with token 1.
We let thefirst component of the the string, a0, be a null token,representing an empty position into which we can insert.Define also the initial substring Ai = (a0, h i , .
.
.
,  hi) ofa string to be the first i tokens, including the null tokenat the beginning.The computation starts by assigning D(A0, B0)) = 0,the cost of transforming a0 to b0, the null token to itself.AB I I I- I  the path \]that is the \[ path- 0 1 2 3 4 5 6the 1 0 1 2 3 4 5way 2 1 2 3 4 5 6that 3 2 3 2 3 4 5is 4 3 4 3 2 3 4not 5 4 5 4 3 4 4the 6 5 6 5 4 3 4way 7 6 7 6 5 4 5Figure 3: Dynamic programming for edit distance ( ' - '  isthe null token)Each subsequent step in the computation proceeds withthe simple rule:D(Ai, Bj_i) + Din.~rt(bj)D(Ai, Bj) = min D(A,_,, Bj) + Dinsert(a,)D( Ai-1, Bj -1 )  "l- Dsubstitute ( ai , bj )where Dinsert(X) is the cost for inserting z, andDsubstitute($~, y) is the cost of substituting x for y.Starting with D(0,0), one can fill each D(i,j) in a ta-ble, ending at D(m, n), the edit distance between thetwo strings.
The table is filled from upper left to lowerright, as each entry is computed from its upper, leftward,and diagonal neighbors using the minimum rule above.Figure 3 gives this table for the example strings.8 .1 .
S t r ing  a l ignmentsAs a by-product of the edit distance computation, onecan create an alignment of the two strings.
This align-ment matches the elements of the two sequences in linearorder and shows the correspondence b tween tokens andsubstrings of the two matched strings.
An alignment canbe generated irectly from the table created in the editdistance computation by following the path of minimachosen during the computation from the upper left cor-ner to the lower right.
Rightward travel along this pathcorresponds to insertion of a token from string A, down-ward travel to tokens from string B, and diagonal pathsto substitutions.
(Multiple minimum paths may result,giving alternate but equivalent alignments.
)The alignment created from our two example strings (fig-ure 4) gives the correspondence b tween the tokens of thetwo initial strings.
From the figure, it is easy to see thestructural similarities of the two strings.Alignments can be created for sets of more than two135p i the i pat  i that i isl j the i path Ithe way that is not the wayFigure 4: A string alignment tablestrings.
These can be expressed in terms of extendedalignment tables, with added rows corresponding to theadditional strings.
These alignment ables could fur-ther be abstracted to probabilistic descriptions of thesequences, using either a zero-order or Markov chain de-scription.
Chan and Wang (\[3\]) have used syntheses,zero-order probabilistic descriptions of alignment tablesin order to generalize the edit distance and capture thenotion of distance between two sets of sequences.
Tech-niques such as this may prove useful in later work.9.
Context  C lus ter ingIll keeping with a straightforward approach to this pre-liminary work, a simple clustering technique was chosento produce hierarchical sets of keyword contexts withsimilar structural properties.
In this approach, contextsjudged most similar in terms of a generalized edit dis-tance were grouped into clusters.
This technique is sim-ilar to some methods used in automatic grammar induc-t, ion (\[14\]).Clustering was chosen over grammar induction or otherabstract echniques for the simple reason that the resultis more easily explained from the data.
The resultantgroupings indicate exactly which data contribute, andalignments or syntheses can help to determine the exactnature of the contribution.
Grammar induction tech-niques give results so far abstracted from the data thatanalysis is often unclear.The clustering procedure used was the group averagemethod, a variety of agglomerative hierarchical cluster-ing often used in biological and genetic studies in nu-merical taxonomy (\[1\]).
The technique is agglomerativein that groups of increasing size are built from smallergroups.
It is hierarchical in that each the members of acluster retain their pre-existing cluster organization, asopposed to a flat structure in which the origins of clustermembers are not retained.The hierarchy produced by the clustering algorithm isuseful in judging similarity in a variety of ways.
Compar-ing the clusters at one similarity level with those groupseither above or below in the hierarchy gives a good in-dication of which properties are responsible for the indi-cated level similarity.
Properties of the data may becomeapparent due to their uniform presence (or absence) ata given level in the hierarchy.1369.1.
Loca l i ty  in the  ed i t  d i s tanceThere is a degree to which purely configurational (syn-tactic) considerations are local in nature.
Syntactic well-formedness and syntactic interactions are properties andbehaviors that seem to have a high locality of effect.
Thepresence of phrasal constituents in almost every syntac-tic theory is evidence of the degree to which this beliefis held - -  phrasal boundaries mark the limits of localsyntactic interactions for most word classes.
Only someword classes, such as verbs and event-denoting nouns,seem to affect the placement and configuration of moredistant constituents.
Most word types seem to affect(and, conversely, are affected by) primarily the configu-ration in their immediate vicinity.In order to highlight he locality of these configurationaleffects, the edit distance used in the experiments wasmodified so as to decrease the importance of token dis-tance from the keyword.
One would like to weight neartokens more heavily, but without ignoring the contri-butions of distant ones.
A window function (sometimescalled a step function) would be simplest, but would onlycount near tokens and completely discount far ones.
Alinear dropoff unction would be able to include contri-butions of all tokens, but because some strings are verylong, it would necessitate a slow dropoff if even the verydistant okens were to contribute to the measure.In the end, a geometrically decreasing weight functionwas chosen, due to its useful properties:?
Near tokens are weighted more heavily than far to-kens.?
All tokens in the string still contribute to the dis-tance measure.?
A half power distance can be defined, which helpsin the understanding and analysis of the results.The half power distance is the distance for which thetokens on one side (those near the keyword) account forhalf of the total possible edit distance, while those onthe other side (farther from the keyword) account forthe remainder.
This helps give a more intuitive read-ing for the resulting distance, with an effective windowaround the keyword which can be treated equally withthe remainder of the string.The implementation of this geometric dropoff requiresonly a small change to the original dynamic program-ming algorithm for edit distance.
The table-filling rulebecomes:D(Ai, Bj) =D(Ai, Bj-1) + L i+j x Dinsert(bj)min D(Ai.1, Bj) + L i+j x Dinsert(ai)D(Ai-1, Bj-1) + L i+j X Dsubstitute(ai, bj)where L is the locality factor, which is defined in termsof the half power distance, Ph:1 l /Ph  Lp , ,  _ 1 L= ~ , so that  -3"9 .2 .
P rob lem~-spec i f i c  we ightsWhile it would be ideal to perform the analysis usingonly perfect equality of lexical items as a criterion, boththe number of contexts required for useful generalization,and the immense computational cost of performing suchexperiments are prohibitive.
In order to make the testprocedures tractable in these experiments, lexical itemswere not treated uniformly as purely lexical tokens.
Theinput was first divided into word classes based on stan-dard part-of-speech lassification, and edit distance costswere assigned on the basis of those classes.The text was initially tagged using a stochastic part-ohspeech tagger (\[15\]).
The 48 tag types used were dividedinto 12 equivalence classes (verbs, nouns, determiners,adjectives, etc.)
in order to simplify weight assignment.To give members of a given class a higher self-similarity,intra-class ubstitutions were assigned lower cost thaninter-class ubstitutions.
Perfect lexical equality was stillaccorded a cost of zero.These particular classes were chosen on the basis of gen-eral linguistic knowledge with respect o the underlyingfunctional aspects~ of the theory.
It is hoped that in lateranalyses, untagged text can be used in the system fromend to end, with context type and word classificationscoming as a result of the pattern clustering scheme.10.
Context  method  resu l t sThe context clustering algorithm described above wasrun using a variety of different keywords.
Two exam-pies, of and without, are given to provide a basis of com-parison with other methods in grammar induction andselectional frame acquisition.
One example of a wordrelevant o the TIPSTER project is given, to illustrateapplications of the similarity clustering technique in ac-quiring domain-specific lexicons.199 occurrences of of, 197 of without and 150 of jointwere chosen randomly from the 1988 Wall Street Jour-nal \[26\], part-of-speech tagged, and clustered using thelocalized edit distance and the group average clusteringmethod.
The half-power distance used was 6, giving 3 to-kens per string.
Because of processing constraints, onlythe right-hand side of each lexical environment was usedin the clustering.
In order to achieve clusters of equalsignificance correlating both sides of the context, with-out assuming some intrinsic cross-correlation, the samplesize would need to be increased ramatically.The results of the clustering are given in two forms.Dendogram tree structures are shown on the final page.These diagram are presented in order to provide a rela-tive indication of the structuring properties of the tech-nique - to show that the clustering algorithm used pro-vides more than a flat grouping.
In these tree diagrams,the vertical scale represents he similarity of merged clus-ters.
Two segments of the tree joined by a horizontalsegment indicates the merger of two clusters whose dis-tance corresponds to the height of the commcting seg-ment.
Higher connections correspond to greater distance(less similarity).In addition to the dendograms, the context strings ofsome of the significant clusters are given as aligmnentsin figures 5 through 13.
The context strings are indexedby number, matching identical (although almost illegiblysmall) indices on the diagrams.10.1 .
P repos i t iona l  a rgumentsThe prepositional keyword of was used to test whetherthe method could extract general noun-phrase structure(NPs being the usual right-hand complement of of).Clusters representing the expected short NP patterns,such as \[DEW N\], \[DEW Adj N\], and \[DEW N-plural\] weregenerated.Two of the more interesting low level clusters are illus-trated in figures 5 and 6.
Figure 6 is a cluster whichgroups genetive NPs as the argument to of.
Figure 5 il-lustrates phrasal delineation by punctuation, promisingperhaps that the method could also derive the syntac-tic phrase-structuring properties .and conventional usesof punctuation.Another test was run with the prepositional keywordwithout, again to test the for NP structure, and to illus-trate semantic subtyping of the arguments.
Most of theargument clusters found were phrases denoting an eventor action, either with a nominal event head (figure 7), orwith a participial phrase (figure 8).The clustering for without also revealed as significant ileidiomatic expression 'without admitting or denying x,'137121: of the asahan author i ty - I96: of the dealer ,136: of the gross national product84: of the old one63: of the proposed actionsFigure 5: of: the \[MOD\] NOUN DELIMITERwhere x is a term carrying negative connotations (fig-ure 9).10 .2 .
Domain -spec i f i c  vocabu lary :  jo in tA trial using an exemplary word from the TIPSTER do-main was also run, to test whether the method couldextract paradigmatic use carrying semantic information.The word joint was selected because of its semantic re-latedness to the cooperative nature of the business tie-upevents (the domain of the TIPSTER task), and becauseof its observed heavy use in relevant context.
150 oc-currences of joint were taken randomly from the samecorpus, and clustered using the same techniques as forof and without.The simplest clusters for joint are of the form 'joint x',where x is a group behavior or a group (figure 10).
Thiskind of semantic ollocation information can also be de-rived through statistical hi-gram analysis (\[7, 18\]).Tile phrasal clusters produced by the method, however,cannot be obtained with bi-gram methods.
Figures 11,12and 13 illustrate clusters of paradigmatic usage of jointill the business reporting domain.
These clusters reflecttile semantic ollocations that can be expected to appearwith joint.
The appearance of these clusters shows thatthe paradigmatic use embodied by the LCP is derivableby purely structural exical methods.The more structured clusters shown here for joint (fig-ures 11, 12, and 13) give patterns with direct applicabil-ity to IE systems.
In fact, these patterns were derivedpreviously through other techniques and are currentlyused in the DIDEROT system to trigger extraction of jointventure events.11.
Conc lus ionThis paper has presented a linguistic framework in whichto view the use of pattern-based extraction systems for85: of the code 's spirit47: of the dollar 's recent rise146: of the company 's quarterly dividend182: of the president-elect's favorite phrasesFigure 6: of: the N's NP119: without a significant correction38: without a significant retreat19: without a proper hearingll:lwithout a legislative vote42:lwithout a bone168: without  any coattai ls155: without any results61: wi thout  any author izat ion whatsoever36: without any congressional uthorization136: without any prior regulatory approvalFigure 7: w i thout :  \[alany \] EVENT-NOMINAL93:7:138:166:4:120:192:186:20:134:47:16:150:159:without raising tax rateswithout raising taxeswithout hurting customer~without telling themwithout recognizing itwithout borrowing moneywithout using installment noteswithout taking a strikewithout fomenting a revolutiorwithout complying with federal disclosurewithout putting up any cashwithout buying any shareswithout ever entering the courthousewithout bail pending a hearingFigure 8: w i thout :  xing Y100:91:115:104:105:146:145:140:117:52:34:38:39:50:joint bidjoint bidjoint effortjoint appearancesjoint appearanceoint venturesoint venturesoint venturesoint venturesoint ventureoint ventureoint venturesoint venturesoint chiefsFigure 10: jo in t :  cooperative122: joint venture of enron corp and sonat inc I26: joint venture of sammis corp and transameric~ corp124: joint venture of general motors corp and allied-signal incFigure 11: jo in t :  venture of x coRP and v INC138text understanding.
The framework isbased on the func-tional aspects of denotational lexical semantics, treatingthe lexical and semantic omponents of an expression asmutual constraining parts, each imposing constraints onthe structure of the other.The viewpoint leads to an investigation of the lexical-semantic interaction in terms of a classification of struc-tural properties.
The two ends of the spectrum canbe analyzed separately, bringing independent s ructuralclassifications to bear on the analysis of the interaction.Methods were outlined for creating classifications of thissort, to create hierarchical descriptions of context andpredicate types, which form a descriptive vocabulary foranalyzing the interaction of lexical and semantic proper-ties in use.Experiments were performed on structural clustering oflexical context, using a localized edit distance as a mea-sure of similarity.
These experiments showed that struc-ture clustering can derive the lexical information re-quired for constructing LCPs.Future direct ions: Obviously, the current level ofthese techniques i not sufficient o automatically createpatterns mapping lexical structure to semantic denota-tions.
What they do show, however, is that edit-distanceclustering is a useful technique for extracting the syntac-tic portions of such patterns - from a set of less than 200contexts in each case we see significant clusters, identi-cal to patterns used in an existing IE system.
Furtherwork is needed in order to fold the semantic mappinginto the clustering process.
Metrics are needed for clas-sifying both semantic structure and for the integratedmappings.
One solution might be to augment the stringedit distance with a predicate-similarity metric based ontree-matching, with the relational structure treated as atree of predicates and arguments.
This combined metriccould provide a measure of similarity for classifying thestructural mappings themselves.Much of the community has discussed the need for se-mantically marked text, much like that in the exampleof figure 2, over which to run machine learning methodssuch as these.
A collection of text with relations explic-itly marked out would provide an ideal set of learningexamples for the clustering technique shown, and for ex-tension into methods integrating the semantic and syn-tactic clustering.Because of the cost in analysis time, the creation of sucha collection is currently unreasonable.
In parallel re-search, I am constructing tools to allow the researcherto easily mark text relative to an arbitrary RR knowl-edge representation scheme.The similarity measure could benefit from further re-search.
As it is given, the edit distance provides no dis-tinction between contiguous substring matches and arbi-trary subsequence matching.
A measurement for rever-sals - the alternation of a pair AB with BA, for tokens(or substrings) A and B - would be useful, &s this sortof swapping is common in natural anguage.
There havebeen some attempts toward this in the genetics commu-nity, but no significant success has been achieved.The metric also could benefit from more advanced meth-ods of comparing sets of strings, rather than pairs only.In order for these techniques to be most effective in de-riving lexical structure, the comparison metric shouldgive credit explicitly to those substructures responsiblefor the assessed similarity.
The present metric can onlydo this on a pair-wise basis.
The syntheses presented in\[3\] provide one method for extension to sets of strings,probabilistic grammars another.The method also suffers from its computational complex-ity.
The clustering method is O(n~), where n is the num-ber of contexts, while the edit-distance computations areO(k2), where k is the average context length, making theentire method O(k2n2).
The context length, k is rela-tively fixed, but the number of separate contexts n isunbounded.
For large numbers of context strings, thecomputational cost is prohibitive.
However, there is asimple parallel reduction of the clustering which bringsthe cost down to a tractable O(nk ~) for n processors.
Ihave begun to experiment with this algorithm on a CM-5parallel computer.References related to edit distance and context evalu-ation, primarily from the biological iterature, are con-tinually coming to my attention.
Unfortunately, I havenot had ample opportunity to judge their relation to thepresent work.Acknowledgements: I would like to thank James Puste-jovsky and Paul Buitelaar for useful discussions indevelopingthis material.
I would also like to thank the anonymous re-viewers for their helpful comments in its improvement.139195: without admitt ing or denying wrongdoing59: without admitt ing or denying guilt123: without admitt ing or denying any wrongdoing194: without admitt ing or denying wrongdoing122: without admitt ing or denying the allegationsFigure 9: w i thout :  admitt ing or denying x131:lJoint venture with bp america !nc \[l l9:\] joint venture with icn pharmaceuticals mc I73:l joint venture with aaa development corp I\] 67:lJoint venture with komori printing machinery co II 10:l joint venture agreement with pt astra international inc I \[ 16:lJoint venture with french publisher hachette sa IFigure 12: jo int :  venture with x INC.113: joint venture of dow chemical co , detroit , and corning glass works coming , n YY ork64: joint venture of dow chemical co in midland , mich , and corning glass works in corning, n98: joint venture of landmark land corp , carmel , calif , and ranieri wilson co newFigure 13: jo int :  venture of x co.  LOCATIONx and Y co.  LOCATIONy1402J 'ii|: !8"| | =?
| lI I, !l }?w?
||1 .
}.__r":4| -~!| ?r| ?I I~-t-O-elReferences1.
Anderberg, M.R.
(1973) Cluster Analysis for Applica-tions, Academic Press, New York2.
Boguraev, B. and Briscoe, T., Eds.
(1989) Computa-tional Lexicography for Natural Language Processing,Longman, London.3.
Chan, S.C., and Wang, A.K.C.
(1991) "Synthesis andrecognition of sequences," IEEE Trans.
on PatternAnalysis and Machine Intelligence 13-12 pp.
1245-12554.
Cowie, J., Guthrie, L., Pustejovsky, J., Wakao, T.,Wang, J., and Waterman, S. (1993) "The Diderot In-formation Extraction System," to appear in Proc.
FirstPA CLING Conference, Vancouver.5.
Grishman, R., Macleod, C., and Sterfing, J.
(1992) "NewYork University: Description of the PROTEUS System&s Used for MUG-4," in Fourth Message Understand-ing ConIerence (MUC-4), Morgan Kaufmann Publish-ers, San Mateo6.
Grishman, R., Sterling, J.
(1992) "Acquisition of selec-tional patterns," in COLING 92, the Proceeding of the14 th International Conf.
on Computational Linguistics,Nantes, France7.
llindle, D., and Rooth, M. (1991) "Structural Ambiguityand Lexical Relations," Proceedings of the 29 th AnnualMeeting of the ACL.8.
Hobbs, J.R., Appelt, D., Tyson, M., Bear, J., and Isreal,D.
(1992) "SRI International: Description of the FAS-TUS System used for MUC-4," in Fourth Message Un-derstanding Conference (MUC-4), Morgan KaufmannPublishers, San Mateo9.
Hobbs, J., Stickel, M., Appelt, D., and Paul, M. (1990)"Interpretation as Abduction," SRI International AICenter Technical Note 449, Palo Alto10.
Itogeweg, P., and Hesper, B.
(1984) "The alignmentof sets of sequences and the construction of phyletictrees: An integrated method," J. Molecular Evolution,20 pp.
175-18611.
Lehnert, W., Cardie, C., Fisher, D., Riloff, E., andWilliams, R. (1991) "Description of the CIRCUS Systemas Used for MUC-3," in Third Message UnderstandingCon\]erence (MUC-3), San Diego, pp.
223-23312.
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J.,Riloff, E., and Soderland, S. "University of Mas-sachusetts: MUC-4 Test Results and Analysis," inFourth Message Understanding Conference (MUC-~),Morgan Kaufmann Publishers, San Mateo13.
Levenshtein, V.I.
(1966) "Binary codes capable of cor-recting deletions, insertions, and reversals."
Cyberneticsand Control Theory 10-8 pp.
707-710; Russian original(1965) Doklady Akademii Nauk SSR 163-4 pp.
845-84814.
Ira, S.Y., and Fu, K.S.
(1977) "A clustering procedurefor syntactic patterns," IEEE Trans.
on Systems, Man,and Cybernetics, Oct. 1977.15.
Mcteer, M., Schwartz,R, and Weischedel,R.
(1991)"Empirical Studies in Part of Speech Labelling," Proc.o\] the 4th DARPA Workshop on Speech and NaturalLanguage, Morgan Kaufman Publishers, San Mateo,pp.
331-33616.
Pustejovsky, J.
(1991) "The Generative Lexicon," Com-putational Linguistics, 17-4.17.
Pustejovsky, J.
(forthcoming) The Generative Lexicon:A Theory of Computational Lexicai Semantics, MITPress, Cambridge.18.
Pustejovsky, J.
(1992) "The Acquisition of Lexical Se-mantic Knowledge From Large Corpora," in Proceed-ings of the Filth DARPA Workshop on Speech ~ NaturalLanguage19.
Pustejovsky, J .
and Anick, P. (1988) "3'he SemanticInterpretation of Nominals," Proc.
of the 12 th Inter-national Con\]erence on Computational Linguistics, Bu-dapest.20.
Pustejovsky, J. and Boguraev, B.
(1993) Lexical knowl-edge representation and natural language processing.Artificial Intelligence, 1993.21.
Sankoff, D., and Kruskal, J.B., eds.
(1983) Timewarps, string edits, and macromolecules, Addison-Wesley, Reading, MA22.
Sellers, P.tI.
(1974) "An algorithm for the distance be-tween two finite sequences," J. Comb.
Thy A16 pp.
253-25823.
Smadja, F. (1989) "Macrocoding the Lexicon with Co-occurrence Knowledge," First Int'l Language Acquisi-tion Workshop, IJCAI 8924.
Wagner, R.A., and Fischer, M.J. (1974) "The string-to-string correction problem," J. ACM 21 pp.
168-17325.
Wilks, Y., Fass, D., Gou, C.M., McDonald, J.E.,Plate, T., and Slator, B.M.
(1990) "Providing MachineTractable Dictionary Tools," Machine Translation 526.
The Wall Street Journal, (1988) Dow Jones, Inc.27.
Zernik; U.
(1990) "Lexical Acquisition: Where is theSemantics?"
Machine Translation 5, pp.
155-174142
