Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140?148,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsRelation Annotation for Understanding Research PapersYuka Tateisi?
Yo Shidahara?
Yusuke Miyao?
Akiko Aizawa?
?National Institute of Informatics, Tokyo, Japan{yucca,yusuke,aizawa}@nii.ac.jp?Freelance Annotatoryo.shidahara@gmail.comAbstractWe describe a new annotation scheme forformalizing relation structures in researchpapers.
The scheme has been developedthrough the investigation of computer sci-ence papers.
Using the scheme, we arebuilding a Japanese corpus to help developinformation extraction systems for digitallibraries.
We report on the outline of theannotation scheme and on annotation ex-periments conducted on research abstractsfrom the IPSJ Journal.1 IntroductionPresent day researchers need services for search-ing research papers.
Search engines and pub-lishing companies provide specialized search ser-vices, such as Google Scholar, Microsoft Aca-demic Search, and Science Direct.
Academic so-cieties provide archives of journal articles and/orconference proceedings such as the ACL Anthol-ogy.
These services focus on simple keyword-based searches as well as extralinguistic relationsamong research papers, authors, and research top-ics.
However, because contemporary research isbecoming increasingly complicated and interre-lated, intelligent content-based search systems aredesired (Banchs, 2012).
A typical query in compu-tational linguistics could be what tasks have CRFsbeen used for?, which includes the elements ofa typical schema for searching research papers;researchers want to find relationships between atechnique and its applications (Gupta and Man-ning, 2011).
Answers to this query can be foundin various forms in published papers, for example,(1) CRF-based POS tagging has achieved state-of-the-art accuracy.
(2) CRFs have been successfully applied to se-quence labeling problems including POS taggingand named entity recognition.
(3) We apply feature reduction to CRFs and showits effectiveness in POS tagging.
(4) This study proposes a new method for the ef-ficient training of CRFs.
The proposed method isevaluated for POS tagging tasks.Note that the same semantic relation, i.e., theuse of CRFs for POS tagging, is expressed by var-ious syntactic constructs: internal structures of thephrase in (1), clause-level structures in (2), inter-clause structures in (3), and discourse-level struc-tures in (4).
This implies that an integrated frame-work is required to represent semantic relations forphrase-level, clause-level, inter-clause level, anddiscourse-level structures.
Another interesting factis that we can recognize various fragments of in-formation from single texts.
For example, fromsentence (1), we can identify CRF is applied toPOS tagging, state-of-the-art accuracy is achievedfor POS tagging, and CRFs achieve high POS tag-ging accuracy, all of which is valuable content fordifferent search requests.
This indicates that weneed a framework that can cover (almost) all con-tent in a text.In this paper we describe a new annotationscheme for formalizing typical schemas for repre-senting relations among concepts in research pa-pers, such as techniques, resources, and effects.Our study aims to establish a framework for rep-resenting the semantics of research papers to helpconstruct intelligent search systems.
In particular,we focus on the formalization of typical schemasthat we believe exemplify common query charac-teristics.From the above observations, we have de-veloped the following criteria for our proposedframework: use the same scheme for annotatingcontents in all levels of linguistic structures, an-notate (almost) all contents presented in texts, andcapture relations necessary for surveying researchpapers.
We investigated 71 computer science ab-stracts (498 sentences) and defined an annotation140scheme comprising 16 types of semantic relations.Computer science is particularly suitable for ourpurpose because it is primarily concerned with ab-stract concepts rather than concrete entities, whichare typically the primary focus of empirical sci-ences such as physics and biology.
In addition,computer and computational methods can be ap-plied to an extraordinarily wide range of top-ics; computer science papers might discuss a bustimetable (for automatic optimization), a person?spalm (as a device for projecting images), or look-ing over another person!Gs shoulder (to obtain pass-words).
Therefore, to annotate all computer sci-ence papers, we cannot develop predefined entityontologies, which is the typical approach taken inbiomedical text mining (Kim et al 2011).However, most computer science papers havecharacteristic schemata: the papers describe aproblem, postulate a method, apply the method tothe problem using particular data or devices, andperform experiments to evaluate the method.
Thetypical schemata clearly represent the structure ofinterests in this research field.
Therefore, we canfocus on typical schemata, such as application ofa method to a problem and evaluation of a methodfor a task.
As we will demonstrate in this paper,the proposed annotation scheme can cover almostall content, from phrase levels to discourse levels,in computer science papers.Note that this does not necessarily mean that ourframework can only be applied to computer sci-ence literature.
The characteristics of the schematadescribed above are universal in contemporary sci-ence and engineering, and many other activities inhuman society.
Thus, the framework presented inthis study can be viewed as a starting point for re-search focusing on representative schemata of hu-man activities.2 Related WorkTraditionally, research on searching research pa-pers has focused more on the social aspects ofpapers and their authors, such as citation linksand co-authorship analysis implemented in theaforementioned services.
Recently, research oncontent-based analysis of research papers has beenemerging.For example, methods of document zoning havebeen proposed for research papers in biomedicine(Mizuta et al 2006; Agarwal and Yu, 2009; Li-akata et al 2010; Guo et al 2011; Varga etal., 2012), and chemistry and computational lin-guistics (Teufel et al 2009).
Zoning providesa sentence-based information structure of papersto help identify the components such as the pro-posed method and the results obtained in the study.As such, zoning can narrow down the sections ofa paper in which the answer to a query can befound.
However, zoning alone cannot always cap-ture the relation between the concepts described inthe sections as it focuses on relation at a sentencelevel.
For example, the examples (1), (2), (3) in theprevious section require intra-sentence analysis tocapture the relation between CRF and POS tag-ging.
Our annotation scheme, which can be seenas conplementary to zoning, attempts to providea structure for capturing the relationship betweenconcepts at a finer-grained level than a sentence.Establishing semantic relations among scien-tific papers has also been studied.
For example,the ACL Anthology Searchbench (Scha?fer et al2011) provides querying by predicate-argumentrelations.
The system accepts specifications ofsubject, predicate, and object, and searches fortexts that semantically match the query using theresults from an HPSG parser.
It can also searchby topics automatically extracted from the papers.Gupta and Manning (2011) proposed a method forextracting Focus, Domain, and Technique from pa-pers in the ACL anthology: Focus is a researcharticle?s main contribution, Domain is an applica-tion domain, and Technique is a method or a toolused to achieve the Focus.
The change in these as-pects over time is traced to measure the influenceof research communities on each other.
Fukuda etal.
(2012) developed a method of technical trendanalysis that can be applied to both patent appli-cations and academic papers, using the distribu-tion of named entities.
However, as processes andfunctions are key concepts in computer science,elements are often described in a unit with its owninternal structures which include data, systems,and other entities as substructures.
Thus, tech-nical concepts such as technique cannot be cap-tured fully by extracting named entities.
Guptaand Manning (2011) analyzed the internal struc-tures of concepts syntactically using a dependencyparser, but did not further investigate the structuresemantically.In addition to the methodological aspects of re-search, i.e., what techniques are applied to whatdomain, a research paper can include other infor-141mation that we also want to capture, such as howthe author evaluates current systems and methodsor the previous efforts of others.
An attempt toidentify the evaluation and other meta-aspects ofscientific papers was made by Thompson et al(2011), which, on top of the biomedical eventsannotated in the GENIA event corpus (Kim etal., 2008), annotated meta-knowledge such as thecertainty level of the author, polarity (positive?negative), and manner (strong?weak) of events, aswell as source (whether the event is attributed tothe current study or previous studies), along withthe clue mentioned in the text.
For in-domainrelations within and between the events, they re-lied on the underlying GENIA annotation, whichmaps events and their participants to a subset ofGene Ontology (The Gene Ontology Consortium,2000), a standard ontology in genome science.We cannot assume the existence of standard do-main ontology in the variety of domains to whichcomputer systems are applied, as was mentionedin Section 1.
On the other hand, using domain-general linguistic frameworks, such as FrameNet(Ruppenhofer et al 2006) or the Lexical Concep-tual Structure (Jackendoff, 1990) is also not sat-isfactory for our purpose.
These frameworks at-tempt to identify the relations lexicalized by verbsand their case arguments; however, they do notconsider discourse or other levels of linguistic rep-resentation.
In addition, relying on a linguistic the-ory requires that annotators understand linguistics.Most computer scientists, the best candidates forperforming the annotation task, would not have thenecessary knowledge of linguistics and would re-quire training, which would increase costs for cor-pus annotation.3 Annotation SchemeThe principle is to employ a uniform structure torepresent semantic relations in scientific papersin phrase-level, clause-level, inter-clause level,and discourse-level structures.
For this purpose,a bottom-up strategy that identifies relations be-tween the entities mentioned is used.
This strat-egy is similar to dependency parsing/annotation,which identifies the relations between constituentsto find the overall structure of sentences.We did not want the relations to be uncondi-tionally concrete and domain-specific, because, asmentioned in the previous section, new conceptsand relations that may not be expressed by pre-In this paper, we propose a novel strategy forparallel preconditioning of large scale linearsystems by means of a two-level approximateinverse technique with AISM method.
Accord-ing to the numerical results on an origin 2400 byusing MPI, the proposed parallel technique ofcomputing the approximate inverse makes thespeedup of about 136.72 times with 16 proces-sors.Figure 1: Sample Abstractdefined (concrete, domain-specific) concepts andrelations may be created.
For the same reason,we did not set specific entity types on the basis ofdomain ontology.
We simply classified entities as?general object,?
?specific object,?
and ?measure-ment.
?To illustrate our scheme, consider the two-sentence abstract1 shown in Figure 12.In the first sentence, we can read that a methodcalled two-level approximate inverse is used forparallel preconditioning (1), the preconditioningis applied to large-scale linear systems, the AISMmethod is a subcomponent or a substage of thetwo-level technique, and the author claims that theuse of two-level approximate inverse is a novelstrategy.In the second sentence, we can read that theauthor has conducted a numerical experiment,the experiment was conducted on an origin 2400(a computer system), message Passing Interface(MPI, a standardized method for message passing)was used in the experiment, the proposed paralleltechnique was 136.72 times quicker than existingmethods, and the speedup was achieved using 16processors.In addition, by comparing the two sentences, wecan determine that the proposed parallel techniquein the second sentence refers to the parallel pre-conditioning using two-level approximate inversementioned in the first sentence.
Consequently, wecan infer the author?s claim that the parallel pre-conditioning using two-level approximate inverseachieved 136.72 times speedup.We define binary relations includingAPPLY TO(A, B) (A method A is appliedto achieve the purpose B or used for do-ing B), EVALUATE(A, B) (A is evaluated as1Linjie Zhang, Kentaro Moriya and Takashi Nodera.2008.
Two-level Parallel Computation for Approximate In-verse with AISM Method.
IPSJ Journal, 48 (6): 2164-2168.2Although the annotation was done for abstracts inJapanese, we present examples in English except where wediscuss issues that we believe are specific to Japanese.142APPLY TO(two-level approximate inverse, parallel preconditioning)APPLY TO(parallel preconditioning, large scale linear systems)SUBCONCEPT(AISM method, two-level approximate inverse)EVALUATE(two-level approximate inverse, novel)RESULT(numerical results, 136.72 times speedup)CONDITION(origin 2400, 136.72 times speedup)APPLY TO(MPI, numerical results)EVALUATE(the proposed parallel technique, 136.72 times speedup)CONDITION(16 processors, 136.72 times speedup)EQUIVALENCE(the proposed parallel technique, two-level approximate inverse)Figure 2: Relations Found in the Sentences in Figure 1B), SUBCONCEPT(A, B) (A is a part of B),RESULT(A, B) (The result of experiment A is B),CONDITION(A, B) (The condition A holds insituation B), and EQUIVALENCE(A, B) (A andB refer to the same entity), with which we canexpress the relations mentioned in the example, asshown in Figure 2.Note that it is the use of two-level approximateinverse for parallel preconditioning(A) that the au-thor claims to be novel.
However, the relation in Ais already represented by the first APPLY TO rela-tion.
Consequently, it is sufficient to annotate theEVALUATE relation between two-level approxi-mate inverse and novel.
This is approximatelyequivalent to paraphrasing the use of two-level ap-proximate inverse for parallel preconditioning isnovel as two-level approximate inverse used forparallel preconditioning is novel.
The same holdsfor the equivalence relation involving the proposedmethod.Expressing the content as the set of relations fa-cilitates discovery of a concept that plays a par-ticular role in the work.
For example, if a readerwants to know the method for achieving paral-lel preconditioning, X, which satisfies the relationAPPLY TO(X, parallel preconditioning) must besearched for.
By using the APPLY TO relationsmentioned in Figure 2 and inference on an is-a re-lation expressed by the SUBCONCEPT, we can ob-tain the result that AISM method is used for paral-lel preconditioning.After a series of trial annotations on 71 abstractsfrom the IPSJ Journal (a monthly peer-reviewedjournal published by the Information ProcessingSociety of Japan), the following tag set was fixed.The annotation was conducted by the two of theauthors of this paper.3.1 Entity and Relation TypesThe current tag set has 16 relation types and threeentity types.
An entity is whatever can be an argu-Type Definition ExampleOBJECT the name of concrete entities such asa system, a person, and a companyOrigin2400, SGIMEASURE value, measurement, necessity, obli-gation, expectation, and possibilitynovel,136.72TERM any otherTable 1: Entity Tagsment or a participant in a relation.
Entity typesare OBJECT, MEASURE, or TERM, as shown inTable 1.
Note that, unlike most schemes wherethe term entity refers to a nominal (named entity),in our scheme, almost all syntactic types of con-tent words can be an entity, including numbers,verbs, adjectives, adverbs, and even some auxil-iaries.
The 16 types of relations are shown in Ta-ble 2.
They are binary relations are directed fromA to B.All relations except EVALUATE COMPARE, andATTRIBUTE can hold between any types of en-tity.
EVALUATE and COMPARE relations holdbetween an entity (of any type) and an entityof the MEASURE type.
The entities involvedin an ATTRIBUTE relation must not be of theMEASURE type.The INPUT and OUTPUT relations were intro-duced to deal with the distinction between the dataand method used in computer systems.
We ex-tend the use of the scheme to annotate the in-ner structure of sentences and predicates, by es-tablishing the relations between verbs and theircase elements.
For example, in automaticallygenerated test data, obviously test data is anoutput of the action of generate, and automati-cally is the manner of generation.
We annotatethe test data as an OUTPUT and automaticallyas an ATTRIBUTE of generate.
In another ex-ample, a protocol that combines biometrics andzero-knowledge proof, the protocol is the productof an action of combining biometrics and zero-143Type Definition ExampleAPPLY TO(A, B) A method A is applied to achieve the purpose B or used forconducting BCRFA-based taggerBRESULT(A, B) A results in B in the sense that B is either an experimentalresult, a logical conclusion, or a side effect of AexperimentA shows the increaseB in F-score compared to the baselinePERFORM(A, B) A is the agent of an intentional action B a frustrated playerA of a gameBINPUT(A, B) A is the input of a system or a process B, A is somethingobtained for BcorpusA for trainingBOUTPUT(A, B) A is the output of a system or a processB, A is somethinggenerated from Ban imagea displayedB on a palmTARGET(A, B) Ais the target of an action B, which does not suffer alteration to driveB a busAORIGIN(A, B) A is the starting point of action B to driveB from ShinjukuADESTINATION(A, B) A is the ending point of action B an image displayedB on a palmACONDITION(A, B) The condition A holds in situation B, e.g, time, location, ex-perimental conditiona surveyB conducted in IndiaaATTRIBUTE(A, B) A is an attribute or a characteristic of B accuracyA of the taggerBSTATE(A, B) A is the sentiment of a person B other than the author, e.g.
auser of a computer system or a player of a gamea frustratedA playerB of a gameEVALUATE(A, B) A is evaluated as B in comparison to C experiment shows an increaseBCOMPARE(C, B) in F?scoreA compared to the baselineCSUBCONCEPT(A, B) A is-a, or is a part-of B a corpusA such as PTBaEQUIVALENCE(A, B) terms A and B refer to the same entity: definition, abbrevia-tion, or coreferenceDoSB (denial ?
of ?
serviceA) attackSPLIT(A, B) a term is split by parenthesical expressions into A and B DoSB (denial-of-service) attackATable 2: Relation Tagsknowledge proof.
Therefore, both biometrics andzero-knowledge proof are annotated as INPUTs ofcombines, and protocol is annotated as OUTPUTof combines.
This scheme is not only used forcomputer-related verbs, but is further extendedto any verb phrases or phrases with nominalizedverbs.
In change in a situation, situation is an-notated as both INPUT and OUTPUT of change.It is as if we regard change as a machine thatchanges something, and when we input a situa-tion, the change-machine processes it and outputa different situation.
Similarly, in evolution of mo-bile phones, mobile phones is annotated as bothINPUT and OUTPUT of evolution.
Here we re-gard evolution as a machine, and when we input(old-style) mobile phones, the evolution-machineprocesses them and outputs (new-style) mobilephones.
We have found that a wide variety of pred-icates can be interpreted using these relations.3.2 Other FeaturesAlthough we aim to annotate all possible relationsmentioned, some conventions are introduced to re-duce the workload.First, we do not annotate the structure withinentities.
No nested entities are allowed, and com-pound words are treated as a single word.
In ad-dition, polarity (negation) is not expressed as a re-lation but as a part of an entity.
We assume thatthe internal structure of entities can be analyzedby mechanisms such as technical term recognition.On the other hand, nested and crossed relations areallowed.Second, we do not annotate words that indicatethe existence of relations.
This is because the re-lations are usually indicated by case markers andpunctuation 3 and marking them up was found tobe a considerable mental workload.
In addition,words and phrases that directly represent the re-lations themselves are not annotated as entities.For example, in CG iteration was applied to theproblem, we directly CG relation and the problemdirectly with APPLY TO and skip the phrase wasapplied to.Third, relations other than EQUIVALENCE andSUBCONCEPT are annotated within a sentence.We assume that the discourse-level relation can beinferred by the composition of relations.In addition, the annotation of frequent verbs andtheir case elements was examined in the trial pro-cess.
Verbs were classified, according to the pat-tern of the annotated relation with the case ele-ments.
For example, verbs semantically similar toassemble and compile form a class.
The semanticrole of the direct object of these verbs varies bycontext.
For example, the materials in phrases likecompile source codes or the product in phrases like3This is in the case with Japanese.
In languages such asEnglish, there may be no trigger words, as the semantic rela-tions are often expressed by the structure of sentences.144compile the driver from the source codes.
In ourscheme, the former is the INPUT of the verb, andthe latter is the OUTPUT of the verb.
Another ex-ample is the class of verbs that includes learn andobtain.
The direct object (what is learned) is theINPUT to the system but is also the result or anoutput of the learning process.
In such cases, wedecided that both INPUT and OUTPUT should beannotated between the verb and its object.Other details of annotation fixed in the processof trial annotation include:1) The span of entities, which is determined to bethe longest possible sequences delimited by casesuffix (-ga,-wo, etc.)
in the case of nominals and toseparate the -suru suffix of verbs and the -da suffixof adjectives but retain other conjugation suffixes;2) How to annotate evaluation sentences involv-ing nouns derived from adjectives that imply eval-uation and measurement, such as necessity, diffi-culty, and length.
The initial agreement was thatwe would consider that they lose MEASURE-nesswhen nominalized; however, with the similarity ofJapanese expressions hitsuyou/mondai de aru (isnecessary/problematic) and hitsuyou/mondai gaaru(there is a necessity/problem), there was con-fusion about which word should be the MEASUREargument necessary for the EVALUATE relation.It was determined that, for example, in hit-suyou/mondai de aru, de aru, a copula, is ig-nored and hitsuyou/mondai is the MEASURE.
Inhitsuyou/mondai ga aru, aru is the MEASURE;3) How to annotate phrases like the tagger wasbetter in precision, where it can be understood thatthe system is evaluated as being better in precision.While what is actually measured in the evaluationprocess described in the paper is the precision (anattribute) of the tagger and the sentence has almostthe same meaning as the tagger?s precision wasbetter, the surface (syntactic) subject of is betteris the tagger.
This can lead to two possibilitiesfor the target of the EVALUATE relation.
We de-cided that the EVALUATE relation holds betweenprecision and better, and the ATTRIBUTE relationholds between precision and tagger, as illustratedin Figure 3.A set of annotation guidelines was compiled asthe result of the trial annotation, including the clas-sifications and the pattern of annotation on fre-quent verbs and their arguments.Figure 3: Annotation of the tagger was better inprecisionEntity RelationConunt % Conunt %Total 1895 100.0 Total 2269 100.0OK 1658 87.5 OK 1110 48.9Type 56 3.0 Type 250 11.0Span 67 3.5 Direction 6 0.3Direction+Type 106 4.7None 114 6.0 None 797 35.1Table 3: Tag Counts4 Annotation ExperimentWe conducted an experiment on another 30 ab-stracts (197 sentences) from the IPSJ Journal.
Thetwo annotators who participated in the develop-ment of the guidelines annotated the abstracts in-dependently, and inter-annotator discrepancy waschecked.
The annotation was performed man-ually using the brat annotation tool(Stenetorp etal., 2012).
No automatic preprocessing was per-formed.
Figure 4 shows the annotation results forthe abstract shown in Figure 1.
The 30 pairs of an-notation results were aligned automatically; Theresults are shown in Tables 3, 4, and 5.Table 3 shows the matches between the twoannotators.
?Total?
denotes the count of enti-ties/relations that at least one annotator found,?OK?
denotes complete matches, ?Type?
denotescases where two annotations on the same spanhave different entity/relation types, ?Span?
de-notes entities where two annotations partiallyoverlap, ?Direction?
denotes the count of relationswhere (only) the direction is different, and ?Direc-tion+Type?denotes relations where the same pairof entities were in different types of relation andin opposite directions, and ?None?
denotes caseswhere no counterpart was found in the other re-sult.Tables 4 and 5 are the confusion matrices forentity type and relation type, respectively.
Thedifferences in the span and direction are ignored.Agreement in F-score calculated in the same man-ner as in Brants (2000) for each relation is shownin column F, with the overall (micro-average) F-score shown in the bottom row of column F.If we assume the number of cases that none of145Figure 4: Annotation Results with bratTERM OBJECT MEASURE NONE Total F(%)TERM 1458 2 38 14 1512 94.9OBJECT 0 17 0 0 17 94.4MEASURE 28 0 238 18 284 83.8None 74 0 8 X 82Total 1560 19 284 32 93.0Table 4: Confusion Matrix for Entitythe annotators recognized (the value of the cellX in the tables) to be zero, the observed agree-ment and Cohen?s ?
coefficient are 90.3% and70.0% for entities, and 49.3% and 43.5% for re-lations, respectively.
If we ignore the count for thecases where one annotator did not recognize theentity/relation (?None?
rows and columns in thetables), the observed agreement and ?
are 96.1%and 89.3% for entities, and 76.1% and 74.3% forrelations, respectively.
The latter statistics indi-cate the agreement on types for entities/relationsthat both annotators recognized.These results show that entity annotation wasconsistent between the annotators but the agree-ment for relation annotation varied, depending onthe relation type.
Table 5 shows that agreementfor DESTINATION, ORIGIN, EVALUATE, andSPLIT was reasonably high, but was low forCONDITION and TARGET.
The rise in agreement(simple and ?)
by excluding cases where only oneannotator recognized the relation indicate that theproblem is recognition, rather than classification,of relations4.From the investigation of the annotated text, thefollowing was found:(1) ATTRIBUTE/CONDITION decision was in-consistent in phrases involving EVALUATE rela-tion, such as the disk space is smaller for the im-age (Figure 5).
The EVALUATE relation betweenthe disk space and smaller was agreed; however,the two annotators recognized different relationsbetween the image and other words.
One annota-4The same observation was true for entitiestor recognized the ATTRIBUTE relation betweenthe disk space and the image (?the disk space as afeature of the image is smaller?).
The other recog-nized the CONDITION relation between the imageand smaller (?the disk space is smaller in the caseof the image?).
(2) We were not in complete agreement aboutskipping phrases that directly represent a relation.The expressions to be skipped in the 71 trial ab-stracts were listed in the guidelines; however, it isdifficult to exhaust all such expressions.
(3) In the case of some verbs, an argument canbe INPUT and OUTPUT simultaneously (Section3.1).
We agreed that an object that undergoes alter-ation in a process should be tagged as both INPUTand OUTPUT but one that does not undergo al-teration or which is just moved is the TARGET.Conflicts occurred for verbs that denote preven-tion of some situations such as prevent, avoid, andsuppress, as illustrated in Figure 6.
One annota-tor claimed that the possibility of DoS attacks isreduced to zero; hence the argument of the verbshould be annotated with INPUT and OUTPUT.The other claims that since the DoS attack itselfdoes not change, it is a TARGET.
(4) In a coordination expression, logical inferencemay be implicitly stated.
For example, in it re-quires the linguistic knowledge and is costly, thereason for costly is likely to be the need for lin-guistic knowledge, i.e., employment of an expertlinguist.
However, the relation is not readily ap-parent.
We wanted to capture the relation in suchcases, but the disagreement shows that it is diffi-cult to judge such a relation consistently.
(5) The decision on whether to split expressionslike XX dekiru and XX kanou (can/able to XX) wasalso problematic.
The guideline was to split them.This contradicts the decision for the compoundwords in general that we do not split them; how-ever, we determined that dekiru/kanou cases had146APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F(%)APPLY TO 136 9 0 2 1 1 2 10 1 0 0 3 0 0 1 0 65 231 53.0ATTRIBUTE 14 154 0 19 6 0 9 5 1 0 7 1 0 0 3 0 28 247 59.7COMPARE 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 4 11 54.5CONDITION 4 11 1 77 0 0 1 4 0 0 0 5 0 0 0 0 49 152 48.7DESTINATION 6 0 0 0 39 0 0 0 0 1 0 0 0 0 0 0 4 50 77.2EQUIVALENCE 4 1 0 1 0 54 0 0 0 0 0 0 0 0 4 0 23 87 60.0EVALUATE 0 11 0 0 0 0 215 3 0 9 0 0 0 0 0 1 41 280 76.1INPUT 12 2 0 0 0 1 4 96 0 11 0 0 0 0 0 9 15 150 58.7ORIGIN 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 2 18 78.0OUTPUT 2 1 0 3 0 0 4 23 0 141 0 0 0 0 0 18 37 229 56.5PERFORM 1 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 2 22 74.5RESULT 8 1 0 0 0 0 1 1 0 0 0 38 0 0 0 0 22 71 54.3SPLIT 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 80.0STATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0SUBCONCEPT 14 10 0 3 0 4 5 0 0 2 0 0 0 0 81 0 34 153 58.1TARGET 6 2 1 3 2 0 7 12 0 14 1 0 0 0 0 42 6 96 47.7None 75 67 3 55 3 33 37 23 5 92 2 22 1 0 37 10 X 465Total 282 269 11 164 51 93 285 177 23 270 29 69 3 0 126 80 332 59.8Table 5: Confusion Matrix for RelationFigure 5: ATTRIBUTE/CONDITION DisagreementFigure 6: INPUT/OUTPUT/TARGET Disagreementto be exceptions because the possibility of XX isexpressed by dekiru/kanou and it seemed naturalto relate XX and dekiru/kanou with EVALUATE.Unfortunately, confusion about splitting them re-mains.5 ConclusionsWe set up a scheme to annotate the content of re-search papers comprehensively.
Sixteen semanticrelations were defined, and guidelines for anno-tating semantic relations between concepts usingthe relations were established.
The experimen-tal results on 30 abstracts show that fairly goodagreement was achieved, and that while entity-and relation-type determination can be performedconsistently, determining whether a relation existsbetween particular pairs of entities remains prob-lematic.
We also found several discrepancy pat-terns that should be resolved and included in a fu-ture revision of the guidelines.Traditionally, in semantic annotation of textsin the science/engineering domains, corpus cre-ators focus on specific types of entities or eventsin which they are interested.
On the other hand,we did not assume such specific types of entitiesor events, and we attempted to design a schemethat annotates more general relations in computerscience/engineering domain.Although the annotation is conducted for com-puter science abstracts in Japanese, we believe thescheme can be used for other languages, or forthe broader science/engineering domains.
The an-notated corpus can provide data for constructingcomprehensive semantic relation extraction sys-tems.
This would be challenging but worthwhilesince such systems are in great demand.
Suchrelation extraction systems will be the basis forcontent-based retrieval and other applications, in-cluding paraphrasing and translation.The abstracts annotated in the course of the ex-periment have been cleaned up and are availableon request.
We are planning to increase the vol-ume and make the corpus widely available.In the future, we will assess machine-learningperformance and incorporate the relation extrac-tion mechanisms into search systems.
Comparisonof the annotated structure and the structures thatcan be given by existing semantic theories couldbe an interesting theoretical subject for future re-search.AcknowledgmentsThis study was partially supported by the JapanMinistry of Education, Culture, Sports, Scienceand Technology Grant-in-Aid for Scientific Re-search (B) No.
22300031.147ReferencesShashank Agarwal and Hong Yu.
2009.
Automaticallyclassifying sentences in full-text biomedical articlesinto introduction, methods, results and discussion.Bioinformatics, 25(23):3174?3180.Rafael E. Banchs, editor.
2012.
Proceedings of theACL-2012 Special Workshop on Rediscovering 50Years of Discoveries.
Association for ComputationalLinguistics.Thorsten Brants.
2000.
Inter-annotator agreement fora German newspaper corpus.
In Proceedings of theSecond International Conference on Language Re-sources and Evaluation.Satoshi Fukuda, Hidetsugu Nanba, and ToshiyukiTakezawa.
2012.
Extraction and visualization oftechnical trend information from research papersand patents.
In Proceedings of the 1st InternationalWorkshop on Mining Scientific Publications.Yufan Guo, Anna Korhonen, and Thierry Poibeau.2011.
A weakly-supervised approach to argumen-tative zoning of scientific documents.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing, pages 273?283.Sonal Gupta and Christopher D Manning.
2011.
An-alyzing the dynamics of research by extracting keyaspects of scientific papers.
In Proceedings of 5thIJCNLP.Ray Jackendoff.
1990.
Semantic Structures.
The MITPress.Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics, 9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011.Overview of bionlp shared task 2011.
In Proceed-ings of BioNLP Shared Task 2011 Workshop, pages1?6.Maria Liakata, Simone Teufel, Advaith Siddharthan,and Colin Batchelor.
2010.
Corpora for concep-tualisation and zoning of scientific papers.
In Pro-ceedings of LREC 2010.Yoko Mizuta, Anna Korhonen, Tony Mullen, and NigelCollier.
2006.
Zone analysis in biology articles as abasis for information extraction.
International Jour-nal of Medical Informatics, 75(6):468?487.Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2006.
FrameNet II: Extended Theory andPractice.
International Computer Science Institute.Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rgSteffen, and Rui Wang.
2011.
The ACL anthologysearchbench.
In Proceedings of the ACL-HLT 2011System Demonstrations, pages 7?13.Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a web-based tool for NLP-assistedtext annotation.
In Proceedings of the Demonstra-tions Session at EACL.Simone Teufel, Advaith Siddharthan, and Colin Batch-elor.
2009.
Towards discipline-independent ar-gumentative zoning: evidence from chemistry andcomputational linguistics.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 1493?1502.The Gene Ontology Consortium.
2000.
Gene ontol-ogy: tool for the unification of biology.
Nature Ge-netics, 25(1):25?29.Paul Thompson, Raheel Nawaz, John McNaught, andSophia Ananiadou.
2011.
Enriching a biomedi-cal event corpus with meta-knowledge annotation.BMC Bioinformatics, 12.Andrea Varga, Daniel Preotiuc-Pietro, and FabioCiravegna.
2012.
Unsupervised document zoneidentification using probabilistic graphical models.In Proceedings of LREC 2012, pages 1610?1617.148
