Translating into Free Word Order LanguagesBery l  Hof fmanCent re  for Cogn i t ive  Sc ienceUn ivers i ty  of  Ed inburgh2 Bucc leuch  P laceEd inburgh ,  EH8 9LW, U.K.hoffman~cogsci, ed.
ac.
ukAbst ractIn this paper, I discuss machine trans-lation of English text into a relatively"free" word order language, specificallyTurkish.
I present algorithms thatuse contextual information to determinewhat the topic and the focus of each sen-tence should be, in order to generate thecontextually appropriate word orders inthe target language.1 In t roduct ionLanguages uch as Catalan, Czech, Finnish, Ger-man, Hindi, Hungarian, Japanese, Polish, Rus-sian, Turkish, etc.
have much freer word orderthan English.
For example, all six permutationsof a transitive sentence are grammatical in Turk-ish (although SOV is the most common).
Whenwe translate an English text into a "free" word or-der language, we are faced with a choice betweenmany different word orders that are all syntacti-cally grammatical but are not all felicitous or con-textually appropriate.
In this paper, I discuss ma-chine translation (MT) of English text into 2hrk-ish and concentrate on how to generate the appro-priate word order in the target language based oncontextual information.The most comprehensive project of this type ispresented in (Stys/Zemke, 1995) for MT into Pol-ish.
They use the referential form and repeatedmention of items in the English text in order topredict the salience of discourse entities and or-der the Polish sentence according to this salienceranking.
They also rely on statistical data, choos-ing the most frequently used word orders.
I arguefor a more generative approach: a particular in-formation structure (IS) can be determined fromthe contextual information and then can be usedto generate the felicitous word order.
This paperconcentrates on how to determine the IS from con-textual information using centering, old vs. newinformation, and contrastiveness.
(Hajifiov?/etal,1993; Steinberger, 1994) present approaches thatdetermine the IS by using cues such as word order,definiteness, and complement semantic types (e.g.temporal adjuncts vs arguments) in the som:cclanguage, English.
I believe that we cannot relyupon cues in the source language in order to de-termine the IS of the translated text.
Instead, Iuse contextual informati<)n i the target languageto determine the IS of sentences in the target lan-guage.In section 2, I discuss the Information Struc-ture, and specifically th<~ topic and the focus innaturally occurring Turkish data.
Then, in section3, I present algorithms for determining the topicand the focus, and show that we can generate con-textually appropriate word orders in '\[~/rkish usingthese algorithms in a simple MT implementation.2 In fo rmat ion  St ructure\]n the Information Structure (IS) that I use forTurkish, a sentence is first divided into a topicand a comment.
The topic is the maiu ele-ment that the sentence is about, and the com-ment is the information conveyed about this toI)ic.Within the comment, we tind the focus, the mostinformation-bearing const.itnent in the senten(:e,and the ground, the rest of the sentence.
The focus is the new or important information in the sen-tence and receives prosodic prominence in speech.In Turkish, the pragmatic fimction of topic isassigned to the sentence-initial position mM thefocus to the immediately preverbM position, fol-lowing (Erguvanh, 11984).
The rest of the sentenceforms the ground.In (Iloffman, 1995; Iloffman, 1995b), I showthat the information structure components oftopic and focus can be suecessfiflly used in gener-ating the context-appropriate nswer to databasequeries.
Determining the topic and focns is fairlyeasy in the context of a simple question, howeverit is much more complica.ted in a text.
In the fol-556The Cb in SOY se i i tences .Cb = Subject 14 (47%)Cb = Object 6 (20%)Cb = Subj or Obj?
6 (20%)Cb = Subj or Other Obj?
0 (0%)No Cb 4 (1.3%)TOTAL 3OTim Cb in OSV sento, nco.s.Cb = Subject 4 (13%)C'b = Object :t6 (53%)Cb = Sub,i or Ob.i?
- 6 (2()%)-Cb = Sul)j or Other ()b.i'?
2 U%)No Cb 2 (7%)TO'rn l ,  30Figure 1: The Cb it, SOV a,nd OSV Sentences.lowing sections, I will describe the characteristicsof topic, focus, and ground components of the 1Sin natural ly occurring texts analyzed in (l loffman,1995b) and allude to possible algorithms for deter-mining them.
The algorithms will then be spelledout in section 3.An example text from the cortms 1 is shown be-low.
The noncanonical OSV word order in (1)b iscontextual ly appropr iate because the object t)ro-noun is a discourse-old topic that links the se.n-tence to the previous context, and the sul)jeet,"your father",  is a discourse-new focus that is be-ing contrasted with other relatives.
Discourse-oldentities are those that  were previously mentionedin the discourse while discourse-new entil, ics arethose that  were not (Prince, 1992).O) a .b.Bu defteri de gok say(lira ben.This notebk-acc too much like-l)st-lS I.
'As for this notebook, I like it very much.
'Bunu da baban ml verdi?
(OSV)This-Ace too father-2S Quest give-Past?
'Did your FATHER, give this to you?
'(CHILDES lba.cha)Many people have suggested that "free" wordorder languages order information from old to newinformation.
However, the Old-to-New orderingprim:iple is a generalization to which exceptionscan be found.
1 believe that the order in whichspeakers place old vs. new items in a sentence re-flects the information structures that are awdlableto the speakers.
The ordering is actually tile ' Ibpicfollowed by the Focus.
Tile qbpic tends to bediscourse-old in lbrmation and the focus disconrse-new.
However, it is possible to have a disconrse-NEW topic and a discourse-OLD focus, as wc willsee in the following sections, which explains theexceptions to the Old-To-New ordering principle.1The data was collected fi'om transcribed conver-sations, contemporary novels, and adult speedl fromthe CHILDES corpus.2.1 Top icAlthough humans can intuitively determine whal,the tol)ic of a sentence is, the tradit ional delinition(what tim sentence is about) is too vague to be im-plemented in a COmlml, ational system, l proposeheuristics based on familiarit,y and salience to de-termine discourse-old seal;ante topics, ~tt~?l heu ris~ties based on grammat ica l  reb~tions Ibr discou rse-new t.opics.
Speakers can shill; Loa  new topicat the start, of a new discourse sag/ileal., ;ts iH(2)a.
Or they can continue ta.lking about Lh(~ sam(,(liscours(>o\[(I tot)it , as iu (2)1).
(2) a.
\[Mary\]m went to lhe I,ookstore.b.
\[She\]./.
I)ought a new book on linguistics.A discourse-old topic often serves 1.o liuk thesentence to the previous context l)y evoking afamil iar and sMient discourse entity.
(~enteriugTheory ((~rosz/etal, 1{)95) provides a measure ofsaliency based on the obserwrtions t;hat salientdiscourse entities are often mentioned rel)ea.1;edlywithin a discourse segment and are oft.an r(mlizedas pronouns.
(rl~lran, 1995) provides a.
(:OUlpre-hensive study of null and overt subjects in Turk-ish using Centering Theory, and \[ inw~stigate theinteraction between word order and (',catering inTurkish in (I loffman, 1996).In the Centering Algor i t l .n,  each nt,l, era.nce ina discom:se is associated with a ranked list of dis-course entities called the forward-lookiug eent.ers(Cf list;) that contains every (lis(:ours(~ entity thatis reMized in thai; utteraltce.
The Cf list is usuallyranked according to a hierarchy of granmmtica\]relal, ions, e.g.
subjects are aSSllllled to \])e l l loresalient than objects.
The backward looking cen-ter (Cb) is the most salient member  of t,he Cf listthat links the era'rent utterance to the iwevious ut-terance.
The Cb of an utterance is delined as thehighest ranke(l element of the previous u tterance'sCf list that also occurs iu the curren(, utterance.If there is a pronoun in the sentence, it ia likelyto be the (Jb.
As we.
will see, the (~,b has much incommon with a sentence- tol)ic.557Discourse-OldInferrableD-New, Hearer-OldS-initsov,osv55 (85%)8 (13%)i (2%)IPV Post-Vso v,os_v ovs, svoo_43 (67%) 56 (93%)10 06%) 4 (7%)1 (2%) 0* D-New, Hearer-New 0 10 (15%) 0TOTAL 64 64 60Figure 2: Given/New Status in Different Sentence PositionsThe Cb analyses of the canonical SOV and thenoncanonical OSV word orders in 251rkish aresummarized in Figure 1 (forthcoming study in(Hoffman, 1996)).
As expected, the subject isoften the Cb in the SOV sentences.
However,in the OSV sentences, the object, not the sub-ject, is most often the Cb of the utterance.
Acomparison of the 20 discourses in the first tworows 2 of the tables in Figure 1 using the chi-square test shows that the association betweensentence-position and Cb is statistically signifi-cant (X 2 = 10.10, p < 0.001).
a Thus, the Cb,when it is not dropped, is often placed in the sen-tence initial topic position in Turkish regardless ofwhether it is the subject or the object of the sen-tence.
The intditive reason for this is that speak-ers want to form a coherent discourse by imme-diately linking each sentence to the previous onesby placing the Cb and discourse-old topic in thesentence-initial position.There are also situations where no Cb ordiscourse-old topic can be found.
Then, adiscourse-new topic can be placed in the sentence-initial position to start a new discourse seg-ment.
Discourse-new topics are often subjects orsituation-setting adverbs (e.g.
yesterday, in themorning, in the garden) in 3Mrkish.2.2 FocusThe term focus has been used with many differ-ent meanings.
Focusing is often associated withnew information, but it is well-known that old in-formation, for example pronouns, can be focusedas well.
I think part of the confusion lies in thedistinction between contrastive and presentational2The centering analysis is inconclusive in somecases because the subject and the object in the sen-tence are realized with the same referential form (e.g.both as overt pronouns or as full NPs).ZAlternatively, using the canonical SOV sentencesas the expected frequencies, the observed frequenciesfor the noncanonical OSV sentences significantly di-verge from the expected frequencies (X 2 = 8.8, p <0.005).focus.
Focusing discourse-new information is of-ten called presentational or informational focus asshown in (3)a. Broad/wide focus (focus projec-tion) is also possible where the rightmost elementin the phrase is accented, but the whole phrase isin focus.
However, we can also use focusing in or-der to contrast one item with another, and in thiscase the focus can be discourse-old or discourse-new, e.g.
(3)b.
(3) a.
What did Mary do this summer?She \[wandered around TURKEY\]F.b.
It wasn't \[ME\],., - It was \[HF, R\]e.(VMlduvf, 1992) defines fbcns as the mostinformation-bearing constituent, and this defini-tion encompasses both contrastive and presenta-tional focusing.
I use this definition of focus aswell.
However, as will see, we still need two differ-ent algorithms in order to determine which itemsare in focus in the target sentence in MT.
We mustcheck to see if they are discourse-new informat ionas well as checking if they are being contrastedwith another item in the discourse model.In Turkish, items that are presentationally orcontrastively focused are placed in the immedi-ately preverbM (IPV) position and receive the pri-mary accent of the phrase.
4 As seen in Figure 2,brand-new discourse ntities are found in the,,IPVposition, but never in other positions in the sen-tence in my Turkish corpus.
The distribution ofbrand-new (the starred line of the table) versusdiscourse-old information (the rest of the table 5)is statistically significant, (X 2 = 10.847, p < .001).This supports the association of discourse-new \[b-cus with the IPV position.4Some languages uch as Greek and Russian treatpresentational nd contrastive focus differently inword order.5 lnferrables refer to entities that the hearer can eas-ily accmnmodate based on entities already in the dis-.course model or the situation.
Hearer-old entities arewell-known to the speaker and hearer but not neces-sarily mentioned in the prior discourse (Prince, 1992).They both behave like discourse-oM entities.558However, as can be seen in Figure 2, mostof the focused subjects in the OSV sentences inmy corpus were actually discourse-old informa-tion.
Discourse-old entities that occur in the IPVposition are contrastively focused.
In (Rooth,1985)'s alternative-set heory, a contrastively fo-cused item is interpreted by constructing a setof alternatives from which the focused item mustbe distinguished.
Generalizing from his work, wecan determine whether an entity should be con-trastively focused by seeing if we can construct analternative set from the discourse model.2.3 GroundThose items that do not play a role in IS of thesentence as the topic or the focus form the groundof the sentence.
In Turkish, discourse-old informa-tion that is not the topic or focus can be(4) a. dropped,b.
postposed to the right of the verb,c.
or placed unstressed between the topic andthe focus.Postposing plays a backgrounding fnnction inTurkish, and it is very common.
Often, speak-ers will drop only those items that are very salient(e.g.
mentioned just in the previous sentence) andpostpose the rest of the discourse-old items, lIow-ever, the conditions for dropping arguments canbe very complex.
(Turan, 1995) shows that thereare semantic considerations; for instance, genericobjects are often dropped, but specific objectsare often realized as overt pronouns and fronted.Thus, the conditions governing dropping and post-posing are areas that require more research.3 The  Implementat ionIn order to simplify the MT implementation, Iconcentrate on translating short and simple En-glish texts into Turkish, using an interlingua rep-resentation where concepts in the semantic repre-sentation map onto at most one word in the En-glish or Turkish lexicons.
The translation pro-ceeds sentence by sentence (leaving aside ques-tions of aggregation, etc.
), but contextual infor-mation is used during the incremental generationof the target text.
These simplifications allowme to test out the algorithms for determining thetopic and the focus presented in this section.In the implementation, first, an English sen-tence is parsed with a Combinatory CategorialGrammar, CCG, (Steedman, 1985).
The semanticrepresentation is then sent to the sentence plan-ner for Turkish.
The Sentence Planner uses thealgorithms in the following subsections to deter-mine the topic, focus, and ground from the givensemantic representation ~md the discourse model.Then, the sentence planner sends the semanticrepresentation and the information strncture ithas determined to the sentence realization com-ponent for Turkish.
This component consists of ahead-driven bottom up generation algorithm thatuses the semantic as well as the information strnc-ture features given by the planner to choose an ap-propriate head in the lexicon.
The grammar usedfor the generation of 3hlrkish is a lexicalist formal-ism called Multiset-CCG (Hoffman, 1995; Iloff-man, 1995b), an extension of CCGs.
Multiset-CCG was developed in order to capture formaland descriptive properties of "free" and restrictedword order in simple and complex sentences (withdiscontinuous constituents and long distance de-pendencies).
Mnltiset-CCG captures the context-dependent meaning of word order in 'Fnrkish bycompositionally deriving the predicate-argumentstructure and the information strnctm'e of a sen-tence in parallel.The following sections describe the algorithmsused by the sentence plauner to determine the ISof the 'lSlrkish sentence, given the semantic repre-sentation of a parsed English sentence.3.1 The  Top ic  A lgor i thmAs each sentence is translated, we update the dis-course model, and keep track of the forward look-ing centers list (Cflist) of the last processed sen-tence.
This is simply a list of all the discourseenities realized in that sentence ranked accordingto the theta-role hierarchy found in the semanticrepresentation.
Thus, the Cf list for the reI)re-sentation give(Pat, Chris, book) is the ranked list\ [Pat ,Chr i s ,book \ ] ,  where the subject is assmnedto be more salient than the objects.Given the semantic representation for the sen-tence, the discourse model of the text processe(lso far, and the ranked C\[ lists of the current andprevious sentences in the discourse, the follow-ing algorithm determines the topic of (;he sen-tence.
First, the algorithm tries to choose themost salient discourse-old entity as the sentencetopicf  If there is no discourse-old entity realizedin the sentence, then a situation-setting adverb o,the subject is chosen as the discourse-new topic.l.
Compare the current Cf list with the previoussentence's Cf list; and choose the firs( itemthat is a member of both of the ranked lists(the Cb).6(Stys/Zemke, 1995) use the saliency ranking toorder the whole sentence in Polish.
tIowever, \[ I)clievethat there is a distinct notion of topic and fo(:as inTurkish.5592.
If 1 fails: Choose the first item in the currentsentence's Cf list that is discourse-old (i.e.
isalready in the discourse model).3.
If 2 fails: If there is a situation-setting ad-verb in the semantic representation (i.e.
apredicate modifying the main event, in rep-resentation), choose it as the discourse-newtopic.4.
If 3 fails: choose the first item in the Cf list(i.e.
the subject) as the discourse-new topic.Note that the determination of the sentencetopic is distinct from the question of how to realizethe salient Cb/topic (e.g.
as a dropped or overtpronoun or full NP).
In the MT domain, this canbe determined by the referential form in the sourcetext.
This trick can also be used for accommodat-ing inferrable or hearer-old entities that behave asif they are discourse-old even though they are lit-erally discourse-new.
If an item that is not; in thediscourse model is nonetheless realized as a defi-nite NP in the source text, the speaker is treatingthe entity as discourse-old.
This is very similar to(Stys/Zemke, 1995)'s MT system which uses thereferential form in the source text to predict thetopicality of a phrase in the target text.3.2 The Focus  Algor i thmGiven the rest of the semantic representation forthe sentence and the discourse model of the textprocessed so far, the following algorithm deter-mines the focus of the sentence.
The first step isto determine presentational focusing of discourse-new information.
Note that the focus, unlike thetopic, can contain more than one element; this al-lows broad focus as well as narrow focusing.
Ifthere is no discourse-new information, the secondstep in the algorithm allows contrastive focusingof discourse-old information.
In order to constructthe alternative sets, a small knowledge base is usedto determine the semantic type (agent, object, orevent) of the entities in the discourse model.1.
If there are any discourse-new entities (i.e.not in the discourse model) in the sentence,put their semantic representations into focus,2.
Else for each discourse ntity realized in thesentence,(a) Look up its semantic type in the KB andconstruct an alternative set that consistsof all objects of that type in the discoursemodel,(b) If the constructed alternative set is notempty, put the discourse ntity's eman-tic representation i to the focus.Once the topic and focus are determined, the re-mainder of the semantic representation is assignedas the gronnd.
For now, items in the ground are ei-ther generated in between the topic and the focusor post-posed behind the verb as backgroundedinformation.
Further research is needed to disa.m-biguate the use of the two possible word orders.Further esearch is also needed on the exact roleof verbs in the IS.
Verbs can be in the focus orthe ground in Turkish; this cannot be seen in theword order, but it is distinguished by sententialstress for narrow focus readings.
The algorithmabove works for verbs since I place events thatare realized as verbs in the sentence into the dis-course model as well.
ltowever, verbs are usu-ally not in focus unless they are surprising or con-trastive or in a discourse-initiM context.
Thus, thealgorithm needs to be extended to a(:comnaodatediscourse-new verbs that are nonetheless expectedin some way into the ground component.
In addi-tion, verbs often participate in broad focus read-ings, and fllrther research is needed to account forthe observation that broad focus readings are onlyavailable in canonical word orders.3.3 ExamplesThe English text in (5) is translated using theword orders in (6) following the Mgorithrns givenabove.
In (6), the numbers following T and F indi-cate the step in the respective algorithm which de-termined the topic or focus for that sentence.
Notethat the inappropriate word orders (indicated by#)  cannot be generated by the algorithm.
(5) a. Pat will meet Chris today.b.
There is a tMk at four.c.
Chris is giving the talk.d.
Pat cannot come.
(6) a.b.Bugiin Pat Chris'le bulu~acak.
(AdvSOV)Today Pat Chris-with meet-flit.
(T:3,F~I)D6rtde bir konu~ma vat.
(AdvSV,#SAdvV)Four-Lot one talk exist.
(T:3,F:I)c. Konu~mayl Chris w'.riyor.
(OSV,#SOV)Talk-Ace Chris give-Prog.
(T:I,F:2)d.Pat gelemiyecek.
(SV,@VS)Pat come-Neg-Fu|;.
('F:2,F:I for the verb)The algorithms can also utilize long distancescrambling in 3~rkish, i.e.
constructions wherean element of an embedded clause has been ex-560tracted and scrambled into the matrix clause inorder to play a role in the IS of the matrix clause.For example the b sentence in the following text istranslated using long distance scrambling because"the talk" is the Cb of the utterance and there-fore the best sentence topic, even though it is theargument of an embedded clause.
(7) a.
There is a talk at four.b.
Pat thinks that Chris will give the talk.
(8) a. D6rtde bir konu~ma var.
(AdvSV)Four-Lot one talk exist.b.Konu+mayh Pat \[Chris'in ei verecegini\]Taik-Acci Pat \[Chris-gen ci givc-ger-as-a<:c\]samyor.
(O281 \[S2V2\]V1)think-Prog.
(T:I,F:I)4 Conc lus ionsIn the machine translation task from Fnglish intoa "free" word order language, it is crucial tochoose the contextnally appropriate word order inthe target language.
In this paper, I discussed howto determine the appropriate word order usingcontextual information in translating into Turk-ish.
I presented algorithms for deterndning thetopic and the focus of the sentence.
These algo-rithms are sensitive to whether the information isold or new in the discourse model (incrementallyconstructed from the translated text); whetherthey refer to salient entities (using Centering The-ory); and whether they can be contrasted withother entities in the discourse model.
Once the imformation structure for a semantic representationis constructed using these algorithms, the sentencewith the contextually appropriate word order isgenerated in the target language using MultisetCCG, a grammar which integrates syntax and in-formation structure.ReferencesEser Emine Erguvanh.
1984.
The l,'uuction ofWord Order in Turkish Grammar.
Universityof California Press.Barbara Grosz and Aravind K. Joshi and ScottWeinstein.
1995.
Centering: A Framework forModelling the Local Coherence of Discourse.Computational Linguistics.Haji~ov& Eva, Petr Sgall, and liana Skounm,lowt11993.
Identifying Topi(: and Focus 1)y an Auto=marie Procedure.
l'rocccdings of the ,%,:th Coat-ference of the Eurolwan Chapter of the As.soci-ali(m for Computational Linguistic.<Beryl tIott'man.
1995.
Integrating Frec Word O>der Syntax and Information Structure.
t'roeced-ings of the European A ssoeiation for Com.puta-tiou,I Linguistics (I';A CL).Beryl Hoffman.
1995.
7he Computational Anah, l-sis of the Syntax and Inte~Tnvtatimt of "\[i','ee';Word Order in Turki.~h.
Ph.I).
dissertation.1RCS q>ch Report 95-17. l)ept, of (~on,puterand Information Science.
\[ Miversil;y of I'eJmsyl-vania.Beryl lloffman, to appear 1996.
Word Order, in-fbrmation Structure, and Centering in Turkish.Centering in Discourse.
eds.
F, llcn I'rin<:e, Ar-avind .loshi, and Marilyn Walker.
Oxford (hal-versify I)ress.Ellen F. Prince.
The ZPG Letter: Subjects, l)ef-initeness and Information Status.
Discoursedescro~tion: diw'rse analyses of a \])rod rais-ing t,e.vt, eds.
Thonrl)son, S. and Mann, W.Philadelphia: ,lohn Beujamins ILV.
pl),2(,)5325.
1992.Mats l{.ooth.
1985.
Association with l,'o-cus.
Ph.D. Dissertation.
lJniversity of Mas-sachusel;t,s.
Amherst.Mark Steedman.
1985.
Dependencies mid <:oordi-nation in the grammar of l)uteh and Englislr,Language, 61:523 568.l{alfSteinberger.
1994.
Tr<mting Free Word Orderin Ma<:hine Translation.
Coling, Kyol,o, Jal0nl\].Malgorzata E. Stys and Stefan S. Zemke.
\] 995.
In:corporating l)iscourse Aspects in English- l>olishMT: Towards Robust Implementation.
I{cccnlAdvanees in NLI'.
{)rnit Turan.
1995.
Null vs. Ow'~rt ,5'ubjer:ls i~7}lrkish Discourse: g Centering An~dysis.
Uni:versil,y of Pennsylvania, Linguistics l>h.l), dis-sertation.Fmric Va.llduvL 1992.
The l'nformational Corn.po-rtent.
New York: Garla,d.561
