Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 157?162,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsPrompt-based Content Scoring for Automated Spoken Language AssessmentKeelan EvaniniEducational Testing ServicePrinceton, NJ 08541, USAkevanini@ets.orgShasha XieMicrosoftSunnyvale, CA 94089shxie@microsoft.comKlaus ZechnerEducational Testing ServicePrinceton, NJ 08541, USAkzechner@ets.orgAbstractThis paper investigates the use of prompt-based content features for the automated as-sessment of spontaneous speech in a spokenlanguage proficiency assessment.
The resultsshow that single highest performing prompt-based content feature measures the numberof unique lexical types that overlap with thelistening materials and are not contained ineither the reading materials or a sample re-sponse, with a correlation of r = 0.450 withholistic proficiency scores provided by hu-mans.
Furthermore, linear regression scor-ing models that combine the proposed prompt-based content features with additional spokenlanguage proficiency features are shown toachieve competitive performance with scoringmodels using content features based on pre-scored responses.1 IntroductionA spoken language proficiency assessment shouldprovide information about how well the non-nativespeaker will be able to perform a wide range of tasksin the target language.
Therefore, in order to providea full evaluation of the non-native speaker?s speak-ing proficiency, the assessment should include sometasks eliciting unscripted, spontaneous speech.
Thisgoal, however, is hard to achieve in the context ofa spoken language assessment which employs auto-mated scoring, due to the difficulties in developingaccurate automatic speech recognition (ASR) tech-nology for non-native speech and in extracting validand reliable features.
Because of this, most spo-ken language proficiency assessments which use au-tomated scoring have focused on restricted speech,and have included tasks such as reading a word / sen-tence / paragraph out loud, answering single-wordfactual questions, etc.
(Chandel et al 2007; Bern-stein et al 2010).In order to address this need, some automatedspoken language assessment systems have also in-cluded tasks which elicit spontaneous speech.
How-ever, these systems have focused primarily on a non-native speaker?s pronunciation, prosody, and fluencyin their scoring models (Zechner et al 2009), sincethese types of features are relatively robust to ASRerrors.
Some recent studies have investigated theuse of features related to a spoken response?s con-tent, such as (Xie et al 2012).
However, the ap-proach to content scoring taken in that study requiresa large amount of responses for each prompt to beprovided with human scores in order to train thecontent models.
This approach is not practical for alarge-scale, high-stakes assessment which regularlyintroduces many new prompts into the assessment?obtaining the required number of scored training re-sponses for each prompt would be quite expensiveand could lead to potential security concerns for theassessment.
Therefore, it would be desirable to de-velop an approach to content scoring which does notrequire a large amount of actual responses to trainthe models.
In this paper, we propose such a methodwhich uses the stimulus materials for each promptcontained in the assessment to evaluate the contentin a spoken response.1572 Related WorkThere has been little prior work concerning auto-mated content scoring for spontaneous spoken re-sponses (a few recent studies include (Xie et al2012) and (Chen and Zechner, 2012)); however, sev-eral approaches have been investigated for writtenresponses.
A standard approach for extended writ-ten responses (e.g., essays) is to compare the con-tent in a given essay to the content in essays thathave been provided with scores by human raters us-ing similarity methods such as Content Vector Anal-ysis (Attali and Burstein, 2006) and Latent SemanticAnalysis (Foltz et al 1999).
This method thus re-quires a relatively large set of pre-scored responsesfor each test question in order to train the contentmodels.
For shorter written responses (e.g., short an-swer questions targeting factual content) approacheshave been developed that compare the similarity be-tween the content in a given response and a modelcorrect answer, and thus do not necessarily requirethe collection of pre-scored responses.
These ap-proaches range from fully unsupervised text-to-textsimilarity measures (Mohler and Mihalcea, 2009) tosystems that incorporate hand-crafted patterns iden-tifying specific key concepts (Sukkarieh et al 2004;Mitchell et al 2002).For extended written responses, it is less practicalto make comparisons with model responses, due tothe greater length and variability of the responses.However, another approach that does not requirepre-scored responses is possible for test questionsthat have prompts with substantial amounts of in-formation that should be included in the answer.
Inthese cases, the similarity between the response andthe prompt materials can be calculated, with the hy-pothesis that higher scoring responses will incorpo-rate certain prompt materials more than lower scor-ing responses.
This approach was taken by (Gure-vich and Deane, 2007) which demonstrated thatlower proficiency non-native essay writers tend touse more content from the reading passage, which isvisually accessible and thus easier to comprehend,than the listening passage.
The current study inves-tigates a similar approach for spoken responses.3 DataThe data used in this study was drawn from TOEFLiBT, an international assessment of academic En-glish proficiency for non-native speakers.
For thisstudy, we focus on a task from the assessment whichelicits a 60 second spoken response from the testtakers.
In their response, the test takers are askedto use information provided in reading and listen-ing stimulus materials to answer a question concern-ing specific details in the materials.
The responsesare then scored by expert human raters on a 4-pointscale using a scoring rubric that takes into accountthe following three aspects of spoken English pro-ficiency: delivery (e.g., pronunciation, prosody, flu-ency), language use (e.g., grammar, lexical choice),and topic development (e.g., content, discourse co-herence).
For this study, we used a total of 1189responses provided by 299 unique speakers to fourdifferent prompts1 (794 responses from 199 speak-ers were used for training and 395 responses from100 speakers were used for evaluation).4 MethodologyWe investigated several variations of simple featuresthat compare the lexical content of a spoken re-sponse to following three types of prompt materials:1) listening passage: a recorded lecture or dialoguecontaining information relevant to the test question(the number of words contained in each of the fourlistening passages used in this study were 213, 223,234, and 318), 2) reading passage: an article or es-say containing additional information relevant to thetest question (the number of words contained in thetwo reading passages were 94 and 111), and 3) sam-ple response: a sample response provided by the testdesigners containing the main ideas expected in amodel answer (the number of words contained in thefour sample responses were 41, 74, 102, and 133).The following types of features were investi-gated for each of the materials: 1) stimulus cosine:the cosine similarity between the spoken responseand the various materials, 2) tokens/response,types/response: the number of word tokens / typesthat occur in both the spoken response and each of1Two out of the four tasks in this study had only listeningmaterials; responses to these tasks are not included in the resultsfor the features which require reading materials.158the materials, divided by the number of word to-kens / types in the response,2 and 3) unique tokens,unique types: the number of word tokens / types thatoccur in both the spoken response and one or twoof the materials, but do not occur in the remainingmaterial(s).As a baseline, we also compare the proposedcontent features based on the prompt materials tocontent features based on collections of scored re-sponses to the same prompts.
This type of featurehas been shown to be effective for content scoringboth in non-native essays (Attali and Burstein, 2006)and spoken responses (Xie et al 2012), and is com-puted by comparing the content in a test response tocontent models trained using responses from each ofthe score points.
It is defined as follows:?
Simi: the similarity score between the wordsin the spoken response and a content modeltrained from responses receiving score i (i ?1, 2, 3, 4 in this study)The Simi features were trained on a corpus of7820 scored responses (1955 for each of the fourprompts), and we investigated two different meth-ods for computing the similarity between the testresponses and the content models: Content VectorAnalysis using the cosine similarity metric (CVA)and Pointwise Mutual Information (PMI).The spoken responses were processed using anHMM-based triphone ASR system trained on 800hours of non-native speech (approximately 15% ofthe training data consisted of responses to the fourtest questions in this study), and the ASR hypothe-ses were used to compute the content features.35 ResultsWe first examine the performance of each of theindividual features by calculating their correlationswith the holistic English speaking proficiency scoresprovided by expert human raters.
These results for2Dividing the number of matching word tokens / types bythe number of word tokens in the response factors out the over-all length of the response from the calculation of the feature.3Transcriptions were not available for the spoken responsesused in this study, so the exact WER of the ASR system is un-known.
However, the WER of the ASR system on a comparableset of spoken responses is 28%.the training partition are presented in Table 1.4Feature Set Feature rstimulus cosinelistening 0.384reading 0.176sample 0.384tokens/responselistening 0.022reading 0.096sample 0.121types/responselistening 0.426reading 0.142sample 0.128unique tokensL?RS 0.116L?RS?
0.162LR?S 0.219LR?S?
0.337unique typesL?RS 0.140L?RS?
0.166LR?S 0.259LR?S?
0.450CVASim1 0.091Sim2 0.186Sim3 0.261Sim4 0.311PMISim1 0.191Sim2 0.261Sim3 0.320Sim4 0.361Table 1: Correlations of individual content features withholistic human scores on the training partitionAs Table 1 shows, some of the individual contentfeatures based on the prompt materials obtain highercorrelations with human scores than the baselineCVA and PMI features based on scored responses.Next, we investigated the overall contribution of thecontent features to a scoring model that takes intoaccount features from various aspects of speakingproficiency.
To show this, we built a baseline lin-ear regression model to predict the human scores us-ing 9 features from 4 different aspects of speaking4For the unique tokens and unique types features, each rowlists how the prompt materials were used in the similarity com-parison as follows: R = reading, L = listening, S = sample,and ?
indicates no lexical overlap between the spoken responseand the material.
For example, L?RS indicates content from thetest response that overlapped with both the reading passage andsample response but was not contained in the listening material.159proficiency (fluency, pronunciation, prosody, andgrammar) produced by SpeechRater, an automatedspeech scoring system (Zechner et al 2009), asshown in Table 2.Category FeaturesFluency normalized number of silences> 0.15 sec, normalized numberof silences > 0.495 sec, averagechunk length, speaking rate, nor-malized number of disfluenciesPronunciation normalzied Acoustic Modelscore from forced alignmentusing a native speaker AM,average normalized phone du-ration differnce compared to areference corpusProsody mean deviation of distance be-tween stressed syllablesGrammar Language Model scoreTable 2: Baseline speaking proficiency features used inthe scoring modelIn order to investigate the contribution of the vari-ous types of content features to the scoring model,linear regression models were built by adding thefeatures from each of the feature sets in Table 1 tothe baseline features.
The models were trained usingthe 794 responses in the training set and evaluatedon the 395 responses in the evaluation set.
Table 3presents the resulting correlations both for the indi-vidual responses (N=395) as well as the sum of allfour responses from each speaker (N=97).5As Table 3 shows, all of the scoring models us-ing feature sets with the proposed content featuresbased on the prompt materials outperform the base-line model.
While none of the models incorporat-ing features from a single feature set outperformsthe baseline CVA model using features based onscored responses, a model incorporating all of theproposed prompt-based content features, all prompt-based, does outperform this baseline.
Furthermore,a model incorporating all of the content features(both the proposed features and the baseline CVA /PMI features), all content, outperforms a model us-5Three speakers were removed from the evaluation set forthis analysis since they provided fewer than four responses.Feature Set response r speaker rBaseline 0.607 0.687+ types/response 0.612 0.701+ tokens/response 0.615 0.700+ unique tokens 0.616 0.695+ stimulus cosine 0.630 0.716+ unique types 0.658 0.761+ CVA 0.665 0.762+ all prompt-based 0.677 0.779+ PMI 0.723 0.818+ CVA and PMI 0.723 0.818+ all content 0.742 0.838Table 3: Performance of scoring models with the additionof content featuresing only the baseline CVA and PMI features.66 Discussion and ConclusionThis paper has demonstrated that the use of contentscoring features based solely on the prompt stimu-lus materials and a sample response is a viable al-ternative to using features based on content mod-els trained on large sets of pre-scored responses forthe automated assessment of spoken language profi-ciency.
Under this approach, automated scoring sys-tems for large-scale spoken language assessmentsinvolving spontaneous speech can begin to addressan area of spoken language proficiency (content ap-propriateness) which has mostly been neglected insystems that have been developed to date.
Com-pared to an approach using pre-scored responses fortraining the content models, the proposed approachis much more cost effective and reduces the riskthat test materials will be seen by test takers priorto the assessment; both of these attributes are cru-cial benefits for large-scale, high-stakes language as-sessments.
Furthermore, the proposed prompt-basedcontent features, when combined in a linear regres-sion model with other speaking proficiency features,outperform a baseline set of CVA content featureswhich use models trained on pre-scored responses,6While the prompt-based content features do result in im-provements, neither of these two differences are statistically sig-nificant at ?
= 0.05 using the Hotelling-Williams Test, sinceboth the magnitude of the increase and the size of the data setare relatively small.160and they add further improvement to a model incor-porating the higher performing baseline with PMIcontent features.The results in Table 1 indicate that the indi-vidual features based on overlapping lexical types(types/response and unique types) perform slightlybetter than the ones based on overlapping lexical to-kens (tokens/response and unique tokens).
This sug-gests that it is important for test takers to use a rangeof concepts that are contained in the stimulus mate-rials in their responses.
Similarly to the result from(Gurevich and Deane, 2007), Table 1 also shows thatthe features measuring overlap between the responseand the listening materials typically perform betterthan the features measuring overlap between the re-sponse and the reading materials; the best individ-ual feature, LR?S?
for unique types, measures theamount of overlap with lexical types that are con-tained in the listening stimulus, but absent from thereading stimulus and sample response.
This indi-cates that the use of content from the listening ma-terials is a better differentiator among students ofdiffering language proficiency levels than readingmaterials, likely because test takers generally havemore difficulty understanding the content from lis-tening materials.Table 1 also shows the somewhat counterintu-itive result that features based on no lexical over-lap with the sample response produce higher corre-lations than features based on lexical overlap withthe sample response, when there is lexical overlapwith the listening materials and no overlap with thereading materials.
That is, the LR?S?
feature out-performs the LR?S feature for both the unique typesand unique tokens features sets.
However, as shownin Section 4, the sample responses varied widelyin length (ranging from 41 to 133 words), and allwere substantially shorter than the listening materi-als, which ranged from 213 to 318 words.
Therefore,it is likely that many of the important lexical itemsfrom the sample response are also contained in thelistening materials.
Thus, the LR?S feature providedless information than the LR?S?
feature.The features used in this study are all based onsimple lexical overlap statistics, and are thus triv-ial to implement.
Future research will investigatemore sophisticated methods of text-to-text similar-ity for prompt-based content scoring, such as thoseused in (Mohler and Mihalcea, 2009).
Furthermore,future research will address the validity of the pro-posed features by ensuring that there are ways to fil-ter out responses that are too similar to the stimulusmaterials, and thus indicate that the test taker simplyrepeated the source verbatim.7 AcknowledgmentsThe authors would like to thank Yigal Attali for shar-ing his ideas about prompt-based content scoring.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater R?
V.2.
The Journal of Technol-ogy, Learning, and Assessment, 4(3):3?30.Jared Bernstein, Alistair Van Moere, and Jian Cheng.2010.
Validating automated speaking tests.
LanguageTesting, 27(3):355?377.Abhishek Chandel, Abhinav Parate, Maymon Madathin-gal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal,Om Deshmuck, and Ashish Verma.
2007.
Sensei:Spoken language assessment for call center agents.
InProceedings of ASRU.Miao Chen and Klaus Zechner.
2012.
Using an ontol-ogy for improved automated content scoring of spon-taneous non-native speech.
In Proceedings of the7th Workshop on Innovative Use of NLP for Build-ing Educational Applications, NAACL-HLT, Montre?al,Canada.
Association for Computational Linguistics.Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.1999.
The intelligent essay assessor: Applications toeducational technology.
Interactive Multimedia Elec-tronic Journal of Computer-Enhanced Learning, 1(2).Olga Gurevich and Paul Deane.
2007.
Documentsimilarity measures to distinguish native vs. non-native essay writers.
In Proceedings of NAACL HLT,Rochester, NY.Tom Mitchell, Terry Russell, Peter Broomhead, andNicola Aldridge.
2002.
Towards robust computerisedmarking of free-text responses.
In Proceedings ofthe 6th International Computer Assisted Assessment(CAA) Conference, Loughborough.Michael Mohler and Rada Mihalcea.
2009.
Text-to-text semantic similarity for automatic short answergrading.
In Proceedings of the European Chapter ofthe Association for Computational Linguistics (EACL2009), Athens, Greece.Jana Sukkarieh, Stephen Pulman, and Nicholas Raikes.2004.
Auto-marking 2: An update on the UCLES-Oxford University research into using computationallinguistics to score short, free text responses.
In161International Association of Educational Assessment,Philadelphia.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speech scor-ing.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 103?111, Montre?al, Canada.
Associationfor Computational Linguistics.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken En-glish.
Speech Communication, 51(10):883?895.162
