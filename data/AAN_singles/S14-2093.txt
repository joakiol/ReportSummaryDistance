Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 532?540,Dublin, Ireland, August 23-24, 2014.SemantiKLUE: Robust Semantic Similarity at Multiple Levels UsingMaximum Weight MatchingThomas Proisl and Stefan Evert and Paul Greiner and Besim KabashiFriedrich-Alexander-Universit?t Erlangen-N?rnberg (FAU)Department Germanistik und KomparatistikProfessur f?r KorpuslinguistikBismarckstr.
6, 91054 Erlangen, Germany{thomas.proisl,stefan.evert,paul.greiner,besim.kabashi}@fau.deAbstractBeing able to quantify the semantic similar-ity between two texts is important for manypractical applications.
SemantiKLUE com-bines unsupervised and supervised tech-niques into a robust system for measuringsemantic similarity.
At the core of the sys-tem is a word-to-word alignment of twotexts using a maximum weight matchingalgorithm.
The system participated in threeSemEval-2014 shared tasks and the com-petitive results are evidence for its usabilityin that broad field of application.1 IntroductionSemantic similarity measures the semantic equiv-alence between two texts ranging from total dif-ference to complete semantic equivalence and isusually encoded as a number in a closed interval,e.
g. [0,5].
Here is an example for interpreting thenumeric similarity scores taken from Agirre et al.
(2013, 33):0.
The two sentences are on different topics.1.
The two sentences are not equivalent, but areon the same topic.2.
The two sentences are not equivalent, butshare some details.3.
The two sentences are roughly equivalent, butsome important information differs/missing.4.
The two sentences are mostly equivalent, butsome unimportant details differ.5.
The two sentences are completely equivalent,as they mean the same thing.Systems capable of reliably predicting the semanticsimilarity between two texts can be beneficial for aThis work is licensed under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/broad range of NLP applications, e. g. paraphrasing,MT evaluation, information extraction, questionanswering and summarization.A general system for semantic similarity aimingat being applicable in such a broad scope has tobe able to adapt to the use case at hand, becausedifferent use cases might, for example, require dif-ferent similarity scales: For one application, twotexts dealing roughly with the same topic shouldget a high similarity score, whereas for another ap-plication being able to distinguish between subtledifferences in meaning might be important.
Thethree SemEval-2014 shared tasks focussing on se-mantic similarity (cf.
Sections 3, 4 and 5 for moredetailed task descriptions) provide a rich testbedfor such a general system, as the individual tasksand subtasks have slightly different objectives.In the remainder of this paper, we describeSemantiKLUE, a general system for measuring se-mantic similarity between texts that we built basedon our experience from participating in the *SEM2013 shared task on ?Semantic Textual Similarity?
(Greiner et al., 2013).2 System DescriptionSemantiKLUE operates in two stages.
In the first,unsupervised stage, a number of similarity mea-sures are computed.
Those measures are the samefor all tasks and range from simple heuristics to dis-tributional approaches to resource-heavy methodsbased on WordNet and dependency structures.
Theidea is to have a variety of similarity measures thatcan capture small differences in meaning as wellas broad thematical similarities.
In the second, su-pervised stage, all similarity measures obtained inthis way are passed to a support vector regressionlearner that is trained on the available gold standarddata in order to obtain a final semantic similarityscore.
This way, the proper similarity scale for agiven task can be learned.
The few remaining out-liers in the predictions for new text pairs are cut532off to fit the interval required by the task definition([0,4] or [0,5]).Our submissions for the individual tasks werecreated using incomplete versions from differentdevelopmental stages of the system.
In the follow-ing sections we describe the current version of thecomplete system for which we also report compa-rable results for all tasks (cf.
Sections 3?
5).The whole system is implemented in Python.2.1 PreprocessingWe use Stanford CoreNLP1for part-of-speech tag-ging, lemmatizing and parsing the input texts.
Weutilize the CCprocessed variant of the Stanford De-pendencies (collapsed dependencies with propaga-tion of conjunct dependencies; de Marneffe andManning (2008, 13?15)) to create a graph represen-tation of the texts using the NetworkX2(Hagberget al., 2008) module.
All the similarity measuresdescribed below are computed on the basis of thatgraph representation.
It is important to keep inmind that by basing all computations on the Stan-ford Dependencies model we effectively ignoremost of the prepositions when using measures thatwork on tokens.3For some tasks, we perform someadditional task-specific preprocessing steps priorto parsing, cf.
task descriptions below.2.2 Simple MeasuresWe use four simple heuristic similarity measuresthat need very little preprocessing.
The first twoare word form overlap and lemma overlap betweenthe two texts.
We take the sets of word form to-kens/lemmatized tokens in text A and text B andcalculate the Jaccard coefficient:overlap =|A?B||A?B|.The third is a heuristic for the difference in textlength that was used by Gale and Church (1993) asa similarity measure for aligning sentences:zi=di?d, where di= bi?
?Nj=1bj?Nj=1ajai.For each of the N text pairs we calculate the differ-ence dibetween the observed length of text B and1http://nlp.stanford.edu/software/corenlp.shtml2http://networkx.github.com/3That is because in the CCprocessed variant of the StanfordDependencies most prepositions are ?collapsed?
into depen-dency relations and are therefore represented as edges and notas vertices in the graph.the expected length of text B based on the lengthof text A.
By dividing that difference diby the stan-dard deviation of all those differences, we obtainour heuristic zi.The fourth is a binary feature expressing whetherthe two texts differ in their use of negation.
Wecheck if one of the texts contains any of the lem-mata no, not or none and the other doesn?t.
Thatfeature is motivated by the comparatively largenumber of sentences in the SICK dataset (Marelliet al., 2014b) that mainly differ in their use of nega-tion, e. g. sentence pair 42 in the training data thathas a gold similarity score of 3.4:?
Two people are kickboxing and spectators arewatching?
Two people are kickboxing and spectators arenot watching2.3 Measures Based on DistributionalDocument SimilarityWe obtain document similarity scores from twolarge-vocabulary distributional semantic models(DSMs).The first model is based on a 10-billion wordWeb corpus consisting of Wackypedia and ukWaC(Baroni et al., 2009), UMBC WebBase (Han etal., 2013), and UKCOW 2012 (Sch?fer and Bild-hauer, 2012).
Target terms and feature terms arePOS-disambiguated lemmata.4We use parame-ters suggested by recent evaluation experiments:co-occurrence counts in a symmetric 4-word win-dow, the most frequent 30,000 lexical words asfeatures, log-likelihood scores with an additionallog-transformation, and SVD dimensionality re-duction of L2-normalized vectors to 1000 latentdimensions.
This model provides distributionalrepresentations for 150,000 POS-disambiguatedlemmata as target terms.The second model was derived from the secondrelease of the Google Books N-Grams database(Lin et al., 2012), using the dependency pairs pro-vided in this version.
Target and feature terms arecase-folded word forms; co-occurrence ounts arebased on direct syntactic relations.
Here, the mostfrequent 50,000 word forms were used as features.All other parameters are identical to the first DSM.This model provides distributional representationsfor 250,000 word forms.We compute bag-of-words centroid vectors foreach text as suggested by (Sch?tze, 1998).
For each4e.g.
can_N for the noun can533text pair and DSM, we calculate the cosine similar-ity between the two centroid vectors as a measureof their semantic similarity.
We also determine thenumber of unknown words in both texts accordingto both DSMs as additional features.2.4 Alignment-based MeasuresWe also use features based on word-level similar-ity.
We separately compute similarities betweenwords using state-of-the-art WordNet similaritymeasures and the two distributional semantic mod-els described above.
The words from both textsare then aligned using those similarity scores tomaximize the similarity total.
We use two typesof alignment: One-to-one alignment where somewords in the longer text remain unaligned and one-to-many alignment where all words are aligned.The one-to-many alignment is based on the one-to-one alignment and aligns each previously unalignedword in the longer text to the most similar word inthe shorter text.
The discussion of the alignmentalgorithm is based on the former case.2.4.1 Alignment via Maximum WeightMatchingWe opt for a graphical solution to the alignmentproblem.
The similarities between the words fromboth texts can be modelled as a bipartite graph inwhich every word from text A is a vertice on theleft-hand side of the graph and every word fromtext B a vertex on the right-hand side.
Weightededges connect every word from text A to everyword from text B.
The weight of an edge corre-sponds to the similarity between the two words itconnects.
In order to obtain an optimal one-to-onealignment we have to select edges in such a waythat no two edges share a common vertice and thatthe sum of the edge weights is maximized.
Thatcorresponds to the problem of finding the maxi-mum weight matching in the graph.
SemantiKLUEutilizes the NetworkX implementation of Galil?s(1986) algorithms for finding that maximum weightmatching.Figure 1 visualizes the one-to-one alignment be-tween two sentences.
For the one-to-many align-ment, the previously unaligned words are alignedas indicated by the dashed lines.2.4.2 Measures Based on DistributionalWord SimilaritiesFor each of the two DSMs described in Section 2.3we compute the best one-to-one and the best one-AwomanisusingamachinemadeforsewingAwomanissewingwithamachineFigure 1: Alignment between a sentence pair fromthe SICK data set.to-many alignment using the cosine similarity be-tween two words as edge weight.
For each ofthose two alignments we compute the followingtwo similarity measures: I) the arithmetic mean ofthe cosines between all the aligned words from textA and text B and II) the arithmetic mean ignoringidentical word pairs.In addition to those eight measures, we use thelemma-based DSM for computing the distributionof cosines between lemma pairs.
For both align-ments, we categorize the cosines between alignedlemma pairs into five heuristically determined inter-vals ([0.2,0.35), [0.35,0.5), [0.5,0.7), [0.7,0.999),[0.999,1.0])5and use the proportions as features.Intuitively, the top bins correspond to links betweenidentical words, paradigmatically related wordsand topically related words.
All in all, we use atotal of 18 features computed from the DSM-basedalignments.2.4.3 Measures Based on WordNetWe utilize two state-of-the-art (Budanitsky andHirst, 2006) WordNet similarity measures for cre-ating alignments: Leacock and Chodorow?s (1998)normalized path length and Lin?s (1998) universalsimilarity measure.
For both of those similaritymeasures we compute the best one-to-one and thebest one-to-many alignment.
For each alignmentwe compute the following two similarity measures:I) the arithmetic mean of the similarities betweenthe aligned words from text A and text B and II) thearithmetic mean ignoring identical word pairs.5Values in the interval [0.0,0.2) are discarded as theywould be collinear with the other features.534We also include the number of unknown wordsin both texts according to WordNet as additionalfeatures.2.5 Measures Using the DependencyStructureWe expect that the information encoded in the de-pendency structure of the texts can be beneficial indetermining the semantic similarity between them.Therefore, we use three heuristics for measuringsimilarity on the level of syntactic dependencies.The first simply measures the overlap of depen-dency relation labels between the two texts (cf.
Sec-tion 2.2).
The second utilizes the fact that the Stan-ford Dependencies are organized in a hierarchy (deMarneffe and Manning, 2008, 11?12) to computeLeacock and Chodorow?s normalized path lengthsbetween individual dependency relations.
Thatmeasure for the similarity between dependency re-lations is then used to determine the best one-to-onealignment between dependency relations from textA and text B and to compute the arithmetic meanof the similarities between the aligned dependencyrelations.
The third heuristic gives an indicationof the quality of the one-to-one alignment and canbe used to distinguish texts that contain the samewords in different syntactic structures.
It uses theone-to-one alignment created with similarity scoresfrom the lemma-based DSM (cf.
Section 2.4.2) tocompute the average overlap of neighbors for allaligned word pairs.
The overlap of neighbors isdetermined by computing the Jaccard coefficientof sets NAand NB.
Set NAcontains all words fromtext B that are aligned to words from text A thatare connected to the target word via a single depen-dency relation.
NBcontains all words from text Bthat are connected to the word aligned to the targetword in text A via a single dependency relation.2.6 Experimental FeaturesAs an experiment, we included features from a com-mercial text clustering software that is currentlybeing developed by our team (Greiner and Evert, inpreparation).
We used this tool ?
which combinesideas from Latent Semantic Indexing and distribu-tional semantics with multiple clustering steps ?
asa black box.We loaded all training, development and testitems for a given task into the system and appliedthe clustering algorithm.
However, we did notmake use of the resulting topic clusters.
Instead, wecomputed cosine similarities for each pair (s1,s2)of sentences (or other textual units) based on the in-ternal representation.
In addition, we computed theaverage neighbour rank of the two sentences, basedon the rank of s2among the nearest neighbours ofs1and vice versa.Since these features are generated from the taskdata themselves, they should adapt automaticallyto the range of meaning differences present in agiven data set.2.7 Machine LearningUsing all the features described above, we have atotal of 39 individual features that measure seman-tic similarity between two texts (cf.
Sections 2.2 to2.5) and two experimental features (cf.
Section 2.6).In order to obtain a single similarity score, we usethe scikit-learn6(Pedregosa et al., 2011) implemen-tation of support vector regression.
In our cross-validation experiments we got the best results withan RBF kernel of degree 2 and a penaltyC= 0.7, sothose are the parameters we use in our experiments.The SemEval-2014 Task 1 also includes a classi-fication subtask for which we use the same 39+2features for training a support vector classifier.Cross-validation suggests that the best parametersetting is a polynomial kernel of degree 2 and apenalty C = 2.5.3 SemEval-2014 Task 13.1 Task DescriptionThe focus of the shared task on ?Evaluation of com-positional distributional semantic models on fullsentences through semantic relatedness and textualentailment?
(Marelli et al., 2014a) lies on the com-positional nature of sentence semantics.
By usinga specially created data set (Marelli et al., 2014b)that tries to avoid multiword expressions and otheridiomatic features of language outside the scope ofcompositional semantics, it provides a testbed forsystems implementing compositional variants ofdistributional semantics.
There is also an additionalsubtask for detecting the entailment relation (entail-ment, neutral, contradiction) between to sentences.Although SemantiKLUE lacks a truly sophisti-cated component for dealing with compositionalsemantics (besides trying to incorporate the depen-dency structure of the texts), the system takes theseventh place in the official ranking by Pearsoncorrelation with a correlation coefficient of 0.7806http://scikit-learn.org/535(best of 17 systems: 0.828).
In the entailment sub-task, the system even takes the fourth place with anaccuracy of 0.823 (best of 18 systems: 0.846).3.2 ExperimentsThe official runs we submitted for this task werecreated by a work-in-progress version of Semanti-KLUE that did not contain all the features de-scribed above.
In this section, we report on somepost-hoc experiments with the complete system us-ing all the features as well as various subsets offeatures.
See Table 1 for an overview of the results.Run r ?
MSE Acc.primary run 0.780 0.736 0.403 0.823best run 0.782 0.738 0.398 0.823complete system 0.798 0.754 0.373 0.820no deps 0.793 0.748 0.383 0.817no deps, no WN 0.763 0.713 0.432 0.793complete + experimental 0.801 0.757 0.367 0.823only DSM alignment 0.729 0.670 0.484 0.746only WordNet 0.708 0.636 0.515 0.715only simple 0.676 0.667 0.561 0.754only DSM document 0.660 0.568 0.585 0.567only deps 0.576 0.565 0.688 0.614Table 1: Results for task 1 (Pearson?s r, Spearman?s?
, mean squared error and accuracy).The whole system as described above, withoutthe experimental features, performs even a bit bet-ter in the semantic similarity subtask (taking place6) and only slightly worse in the entailment subtask(still taking place 4) than the official submissions.Adding the experimental features slightly improvesthe results but does not lead to a better position inthe ranking.We are particularly interested in the impact ofthe resource-heavy features derived from the de-pendency structure of the texts and from Word-Net.
If we use the complete system without thedependency-based features (emulating the case ofa language for which we have access to a WordNet-like resource but not to a parser), we get resultsthat are only marginally worse than those for thecomplete system and lead to the same places in therankings.
Additionally leaving out WordNet has abigger impact and results in places 9 and 8 in therankings.Regarding the individual feature groups, theDSM-alignment-based measures are the best fea-ture group for predicting semantic similarity andthe simple heuristic measures are the best featuregroup for predicting entailment.4 SemEval-2014 Task 34.1 Task DescriptionUnlike the other tasks, which focus on similar-sizedtexts, the shared task on ?Cross-Level SemanticSimilarity?
(Jurgens et al., 2014) is about measur-ing semantic similarity between textual units ofdifferent lengths.
It comprises four subtasks com-paring I) paragraphs to sentences, II) sentences tophrases, III) phrases to words and IV) words toword senses (taken from WordNet).
Due to thenature of this task, performance in it might be es-pecially useful as an indicator for the usefulness ofa system in the area of summarization.SemantiKLUE takes the fourth place out of 38 inboth the official ranking by Pearson correlation andthe alternative ranking by Spearman correlation.4.2 Additional PreprocessingFor the official run we perform some additional pre-processing on the data for the two subtasks on com-paring phrases to words and words to word senses.On the word level we combine the word with theglosses of all its WordNet senses and on the wordsense level we replace the WordNet sense indica-tion with its corresponding lemmata and gloss.
Asour post-hoc experiments show that has a nega-tive effect on performance in the phrase-to-wordsubtask.
Therefore, we skip the additional prepro-cessing on that level for our experiments describedbelow.4.3 ExperimentsFor each of the four subtasks, we perform thesame experiments as described in Section 3.2: Wecompare the official run submitted from a work-in-progress version of SemantiKLUE with the re-sults from the whole system; we see how the sys-tem performs without dependency-based featuresand WordNet-based features; we try out the experi-mental features; we determine the most importantfeature group for the subtask.
Table 2 gives anoverview of the results.4.3.1 Paragraph to SentenceOur submitted run takes the fifth place (ties withanother system) in the official ranking by Pearsoncorrelation with a correlation coefficient of 0.817(best of 34 systems: 0.837) and seventh place inthe alternative ranking by Spearman correlation.536Run Paragraph to sent.
Sent.
to phrase Phrase to word Word to senser ?
r ?
r ?
r ?official 0.817 0.802 0.754 0.739 0.215 0.218 0.314 0.327complete system 0.817 0.802 0.754 0.739 0.284 0.289 0.316 0.330no deps 0.815 0.802 0.752 0.739 0.309 0.313 0.312 0.329no deps, no WN 0.813 0.802 0.736 0.721 0.335 0.335 0.234 0.248complete + experimental 0.816 0.800 0.752 0.738 0.292 0.298 0.318 0.330only DSM alignment 0.799 0.789 0.724 0.711 0.302 0.301 0.216 0.216only WordNet 0.787 0.769 0.664 0.641 0.186 0.171 0.313 0.311only simple 0.807 0.793 0.686 0.672 0.128 0.121 0.089 0.093only DSM document 0.629 0.624 0.546 0.558 0.247 0.240 0.144 0.148only deps 0.655 0.621 0.449 0.440 0.036 0.057 ?0.080 ?0.076Table 2: Results for task 3 (Pearson?s r and Spearman?s ?
).The complete SemantiKLUE system gives identicalresults.
Leaving out the resource-heavy featuresbased on the dependency structure and WordNetdiminishes the results only very slightly, thoughit still resolves the tie and puts the system on thesixth place in the Pearson ranking.
Adding theexperimental features to the complete system has aminor negative effect.Probably due to the length of the texts, our sim-ple heuristic measures surpass the DSM-alignment-based measures as the best feature group for pre-dicting semantic similarity.4.3.2 Sentence to PhraseIn this subtask, SemantiKLUE takes the fourthplace in both the official ranking with a Pearsoncorrelation coefficient of 0.754 (best of 34 systems:0.777) and in the alternative ranking by Spearmancorrelation.
The complete system performs iden-tically to our submitted run and leaving out thedependency-based features has little impact on theresults.
Additionally also leaving out the WordNet-based features has more impact on the results andputs the system on the eighth place in the officialranking.
Just as in the paragraph-to-sentence sub-task, adding the experimental features to the com-plete system has a slightly negative effect.For this subtask, the DSM-alignment-based mea-sures are clearly the feature group that yields thebest results.4.3.3 Phrase to WordFor our submitted run we performed the additionalpreprocessing described in Section 4.2 resultingin the eleventh place in the official ranking witha Pearson correlation coefficient of 0.215 (best of22 systems: 0.415) and the 14th place in the alter-native ranking by Spearman correlation.
For ourexperiments with the complete system we skip thatadditional preprocessing step, i. e. we do not addthe WordNet glosses to the word, and drasticallyimprove the results, putting our system on the thirdplace in the official ranking.
Even more interestingis the observation that leaving out the resource-heavy features further improves the results, puttingthe system on the second place.
In consistencywith those observations, the DSM-alignment-basedmeasures are not only the strongest individual fea-ture group but also yield better results when takenalone than the complete system.In contrast to the first two subtasks, adding theexperimental features to the complete systems hasa slightly positive effect here.4.3.4 Word to SenseIn the word-to-sense subtask, SemantiKLUE takesthe third place in both the official ranking with aPearson correlation coefficient of 0.316 (best of20 systems: 0.381) and in the alternative rank-ing by Spearman correlation.
The complete sys-tem performs slightly better than our submittedrun and adding the experimental features givesanother marginal improvement.
Leaving out thedependency-based features has little impact butalso leaving out the WordNet-based features sev-erly hurts performance.
The reason for that be-haviour becomes clear when we look at the resultsfor the individual feature groups: the WordNet-based measures are clearly the strongest featuregroup for predicting the semantic similarity be-tween words and word senses.5 SemEval-2014 Task 105.1 Task DescriptionThe shared task on ?Multilingual Semantic TextualSimilarity?
(Agirre et al., 2014) is a continuationof the SemEval-2012 and *SEM 2013 shared tasks537Run deft-forumdeft-newsheadlinesimagesOnWNtweet-newsw.meanbest run 0.349 0.643 0.733 0.773 0.855 0.640 0.694complete (all training data) 0.432 0.638 0.660 0.736 0.810 0.659 0.676best overall training data 0.464 0.672 0.657 0.771 0.836 0.690 0.700best overall, no deps 0.457 0.675 0.636 0.764 0.834 0.690 0.694best overall, no deps, no WN 0.426 0.653 0.617 0.719 0.780 0.636 0.654best overall + experimental 0.466 0.674 0.673 0.772 0.849 0.687 0.706best individual training data 0.475 0.706 0.711 0.788 0.852 0.715 0.727best individ., no deps 0.465 0.700 0.699 0.781 0.848 0.722 0.722best individ., no deps, no WN 0.448 0.722 0.677 0.752 0.791 0.706 0.697best individ.
+ experimental 0.475 0.711 0.715 0.795 0.864 0.721 0.733Table 3: Results for task 10.on semantic textual similarity (Agirre et al., 2012;Agirre et al., 2013).
It comprises two subtasks:English semantic textual similarity and Spanishsemantic textual similarity.
For each subtask, thereare sentence pairs from various genres.We only participate in the English subtask andtake the 13th place out of 38 with a weighted meanof Pearson correlation coefficients of 0.694 (bestsystem: 0.761).5.2 ExperimentsFrom participating in the *SEM 2013 shared taskon semantic textual similarity (Greiner et al., 2013)we already know that the composition of the train-ing data is one of the strongest influences on systemperformance in this task.
As the individual data setsare not very similar to each other, we tried to comeup with a good subset of the available training datafor each data set.
In doing so, we were moderatelysuccessful as the results in Table 3 show.
Run-ning the complete system with all of the availabletraining data on all test data sets results in a lowerweighted mean than our submitted run.
If we stickto using the same training data for all test data setsand optimize the subset of the training data we use,we achieve a slightly better result than our submit-ted run (the optimal subset consists of the FNWN,headlines, MSRpar, MSRvid and OnWN data sets).Using that optimal subset of the training data andadding the experimental features to the completesystem has a minor positive effect on the weightedmean, with the biggest impact on the headlines andOnWN data sets.
Using the complete system with-out the dependency-based features gives roughlythe same results but omitting all resource-heavyfeatures has clearly a negative impact on the re-sults.In another experiment we try to optimize ourstrategy of finding the best subset of the trainingdata for each test data set.
Doing that gives us aconsiderably higher weighted mean than using thesame training data for every test data set, puttingour system on the eighth place.
Using the completesystem, we find that the best training data subsetsfor the individual test data sets are those shown inTable 4.test set training setsdeft-forum FNWN, headlines, MSRviddeft-news FNWN, MSRpar, MSRvidheadlines FNWN, headlines, MSRparimages FNWN, MSRpar, MSRvidOnWN FNWN, MSRvid, OnWNtweet-news FNWN, headlines, MSRpar, MSRvidTable 4: Optimal subsets of training data for usewith the complete SemantiKLUE system.If we add the experimental features to the com-plete system and still optimize the training data sub-sets, we get a small boost to the results.
Leaving outthe dependency-based features does not really hurtperformance but also omitting the WordNet-basedfeatures has a negative impact on the results.6 ConclusionSemantiKLUE is a robust system for predicting thesemantic similarity between two texts that can alsobe used to predict entailment.
The system achievesgood or very good results in three SemEval-2014tasks representing a broad variety of semantic simi-larity problems (cf.
Table 5 for an overview of theresults of all subtasks).
Our two-staged strategy ofcomputing several similarity measures and usingthem as input for a machine learning mechanism538Subtask submitted run complete system winner scorescore rank score rankTask 1, similarity 0.780 7/17 0.798 6/17 0.828Task 1, entailment 0.823 4/18 0.820 4/18 0.846Task 3, par-2-sent 0.817 5/34 0.817 5/34 0.837Task 3, sent-2-phr 0.754 4/34 0.754 4/34 0.777Task 3, phr-2-word 0.215 11/22 0.284 3/22 0.415Task 3, word-2-sense 0.314 3/20 0.316 3/20 0.381Task 3 overall N/A 4/38 N/A 3/38 N/ATask 10, deft-forum 0.349 20/38 0.464 12/38 0.531Task 10, deft-news 0.643 22/37 0.672 19/37 0.785Task 10, headlines 0.733 15/37 0.657 20/37 0.784Task 10, images 0.773 16/37 0.771 17/37 0.834Task 10, OnWN 0.855 3/36 0.836 7/36 0.875Task 10, tweet-news 0.640 20/37 0.690 12/37 0.792Task 10 overall 0.694 13/38 0.700 13/38 0.761Table 5: Overview of results.proves itself to be adaptable to the needs of theindividual tasks.Using the maximum-weight-matching algorithmfor aligning words from both texts that have similardistributional semantics leads to very sound fea-tures.
Even without the resource-heavy features,the system yields competitive results.
In some usecases, those expensive features are almost negligi-ble.
Without being dependent on the availability ofresources like a dependency parser or a WordNet-like lexical database, SemantiKLUE can easily beadapted to other languages.Our experimental features from the commercialtopic clustering software are useful in some cases;in others at least they do not hurt performance.We feel that the heuristics based on the depen-dency structure of the texts do not exhaust all thepossibilities that dependency parsing has to offer.In the future we would like to try out more mea-sures based on those structures.
Probably somekind of graph edit distance incorporating the sim-ilarities between both dependency relations andwords might turn out to be a powerful feature.ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2012.
SemEval-2012 task6: A pilot on semantic textual similarity.
In FirstJoint Conference on Lexical and Computational Se-mantics, pages 385?393.
ACL.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), volume 1: Proceedings of the MainConference and the Shared Task, pages 32?43.
ACL.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
SemEval-2014 task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014).Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky WideWeb: A collection of very large linguistically pro-cessed Web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating WordNet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Marie-Catherine de Marneffe and Christopher D. Man-ning, 2008.
Stanford typed dependencies manual.Stanford University.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Zvi Galil.
1986.
Efficient algorithms for findingmaximum matching in graphs.
Computing Surveys,18(1):23?38.Paul Greiner and Stefan Evert.
in preparation.
TheKlugator Engine: A distributional approach to openquestions in market research.Paul Greiner, Thomas Proisl, Stefan Evert, and BesimKabashi.
2013.
KLUE-CORE: A regression modelof semantic textual similarity.
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM), volume 1: Proceedings of the Main Con-ference and the Shared Task, pages 181?186.
ACL.Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart.2008.
Exploring network structure, dynamics, andfunction using NetworkX.
In G?el Varoquaux,539Travis Vaught, and Jarrod Millman, editors, Pro-ceedings of the 7th Python in Science Conference(SciPy2008), pages 11?15, Pasadena, CA.Lushan Han, Abhay L. Kashyap, Tim Finin,James Mayfield, and Johnathan Weese.
2013.UMBC_EBIQUITY-CORE: Semantic textualsimilarity systems.
In Proceedings of the SecondJoint Conference on Lexical and ComputationalSemantics.
ACL.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 task 3:Cross-level semantic similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalua-tion (SemEval-2014).Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity forword sense identification.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database,pages 265?283.
MIT Press, Cambridge, MA.Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,Jon Orwant, Will Brockman, and Slav Petrov.
2012.Syntactic annotations for the Google Books NgramCorpus.
In Proceedings of the ACL 2012 SystemDemonstrations, pages 169?174, Jeju Island, Korea.ACL.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, pages296?304, San Francisco, CA.
Morgan Kaufmann.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014a.
SemEval-2014 task 1: Evaluation ofcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014).Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zampar-elli.
2014b.
A SICK cure for the evaluation of com-positional distributional semantic models.
In Pro-ceedings of LREC 2014, Reykjavik.
ELRA.Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and ?douard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Roland Sch?fer and Felix Bildhauer.
2012.
Buildinglarge corpora from the web using a new efficienttool chain.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Eval-uation (LREC ?12), pages 486?493, Istanbul, Turkey.ELRA.Hinrich Sch?tze.
1998.
Automatic word sense discrim-ination.
Computational Linguistics, 24(1):97?123.540
