Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57?60Manchester, August 2008Random Graph Model Simulations of Semantic Networks for Associative Concept DictionariesHiroyuki Akama Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan akama@dp.hum.titech.ac.jp Terry Joyce Tama University 802 Engyo Fujisawa-shi Kanagawa-ken 252-0805, Japan terry@tama.ac.jpJaeyoung Jung Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan catherina@dp.hum.titech.ac.jp Maki Miyake Osaka University 1-8 Machikaneyama-cho Toyonaka-shi Osaka 560-0043, Japan mmiyake@lang.osaka-u.ac.jpAbstractWord association data in dictionary form can be simulated through the combina-tion of three components: a bipartite graph with an imbalance in set sizes; a scale-free graph of the Barab?si-Albert model; and a normal distribution con-necting the two graphs.
Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data.
1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological ex-periments where the participants are typically asked to provide the semantically-related re-sponse word that comes to mind on presentation of a stimulus word.
Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al, 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al, 1973).
Two ACDs for Japanese are Ishizaki?s Associative Concept Dictionary (IACD) (Oka-moto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007).
While there are a number of practical applica-tions for ACDs, three are singled out for mention                                                 ?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.here.
The first is in the area of artificial intelli-gence, where ACDs can contribute to the devel-opment of intelligent information retrieval sys-tems for societies requiring increasingly sophisti-cated navigation methods.
A second application is in the field of medicine, where ACDs could be used in developing systems that seek to prevent dementia by checking higher brain functions with a brain dock.
Finally, within educational settings, ACDs can greatly facilitate language learning through the manifestation of inherent cultural modes of thinking.
The typical format of an ACD is to list the stimulus words (cue words) and their response words together with some statistics relating to the word pairing.
The stimulus words are generally basic words determined in advance by the ex-perimenter, while the response words are seman-tically associated words provided by respondents on presentation of the stimulus word.
The statis-tics for the word pairing include, for example, measured or calculated indices of distance or perhaps some classification of the semantic rela-tionship between the pair of words.
In order to mathematically analyze the struc-ture of ACDs, the raw association data is often transformed into some form of graph or complex network representation, where the vertices stand for words and the edges indicate an associative relationship (Joyce and Miyake, 2007).
However, to our knowledge, there have been no attempts at mathematically simulating an ACD as a way of determining in advance the architectural design of a dictionary.
One reason is that it is a major challenge to compute maximum likelihood esti-mations (MLEs) or Monte-Carlo simulations for graph data (Snijder, 2005).
Thus, it is extremely difficult to predict dependences for unknown57factors such as the lexical distribution across a predetermined and controllable dictionary framework starting simply from a list of basic words.
Accordingly, we propose an easier and more basic approach to constructing an ACD model by combining random graph models to simulate graph features in terms of degree distri-butions and clustering results.
2 Degree distributions for ACDs 2.1 Typical local skew It is widely known that Barab?si and Albert (1999) have suggested that the degree distributions of scale-free network structures correspond to a power law, expressed asrddxP?== )(  (where d stands for degree andr  is a small number, such as 2 or 3).
This type of distribution is also known as Zipf's law describing the typical frequency distribution of words in a document and plots on a log scale as a falling diagonal stroke.
However, in the degree distribution of ACDs, there is always a local skew, as a local peak or bump with a low hemline.
Figure 1 presents two degree distributions; for the IACD (upper) ( r  = 1.8) and the JWAD (lower) ( r  = 2.3).
Figure 1.
Degree distributions for actual data  The plots indicate a combination of heteroge-neous distributions, consisting of a single degreedistribution represented as a bell form with a steep slope on the right side.
However, what is most interesting here is that throughout the dis-tribution range the curves remain regular and continuous, with an absence of any ruptures or fractures both before and after the local peaks.
When actual ACD data is examined, one finds that as response words are not linked together, almost all the words located in the skewed part are stimulus words (which we refer to as peak words in this study), while the items before the local peak are less frequent response words that have a strong tendency to conform to a decaying distribution.
It is therefore relatively natural to divide all word pairs into two types of graph: either a bipartite graph for new response words that are not already part of the stimulus list and a graph that conforms to Zipf's law for the fre-quencies of response words that are already pre-sent in the stimulus list.
For the first type, new response words are represented as nodes only with incoming links, generating a bipartite graph with two sets of different sizes.
This bipartite graph would exhibit the decaying distribution due to low-frequency response words prior to the local peak.
In the second type of graph, response words are represented as nodes with both incom-ing and outgoing links.
This second type is simi-lar to a scale-free graph, such as that incorpo-rated within the Barab?si-Albert (BA) model.
2.2 Bipartite Graph and BA Model A bipartite graph is a graph consisting of vertices that are divided into two independent sets, S and R, such that every edge connects to one S vertex and one R vertex.
The graph can be represented by an adjacency matrix with diagonal zero sub-matrices, where the values of the lower right sub-matrices would all be zero were it not for the appearances of some stimulus words as response words.
The lower right section is exactly where the extremely high degrees of hubs are produced, which far exceed the average numbers of response words.
Thus, we adopt an approach to generating a scale-free graph that reflects Zipf's law for fre-quency distributions.
According to the BA model, the probability that a node receives an additional link is proportional to its degree.
Here, we im-plement the principle of preferential attachment formulated by Bollob?s (2003):?=+=?tTttxTdxmdNxP11)(/)()(    (1),0.000010.00010.0010.010.111 10 100 1000kP(k)datak^(-r)0.000010.00010.0010.010.111 10 100 1000kP(k)datak^(-r)58with the addition of one condition that is specific to ACDs, which we explain below.
The BA model starts with a small number,0m  of vertices, and at each time step, T , a new vertex with m  edges is added and linked to m  different vertices that already exist in the graph.1+tN  represents a random set of m  early vertices, )(idtthe degree of vertexiin the process at time t .
The probability that a new vertex will be connected to a vertexidepends on the connectivity of that vertex, as expressed by Equation (1).
However, we specifically assume that m  is a random natural number that is smaller than0m , because in actual data the ratio of stimulus words among all responses words for each stimulus word is obviously far from constant.
Moreover, the graph for the BA model here should be regarded as being a directed graph, because the very reason that hubs emerge within semantic network representations of ACDs is that the number of incoming edges is much larger than the expected number of nodes for each possible in-degree.
In contrast, out-degree is limited by the number of responses for each stimulus wordi, which is represented as )(ic .
Let )(ic  follow a normal distribution with a mean cm and a small variance value 2?
(which is not constant but nearly so) to smoothly combine the distribution of the bipartite graph and the power distribution.
If a directed adjacency matrix for the network exclusively between stimulus words is expressed as)(ijBD, then the sum of the non-zero values for each row in a random bipartite graph introducing new response words will be??
?iijBDiC )()((The vertices of stimulus words with the subscript j are linked with the vertex of the stimulus word i).
Thus, new response words?words that are not stimulus words?will be randomly allocated within a bipartite graph according to Equation (2):???
?==iijBDicrliP ))()(()1),((1            (2), where r is the approximate number of such words.
Equation (2) will yield the lower left and the upper right sections of the complete adja-cency matrix A  for the ACD model.
The subse-quent sub-matrix tP  refers to the transposition of the prior sub-matrix P .
The adjacency matrix in Equation (3) represents a pseudo bipartite graph structure where the upper left section is a zero sub-matrix (because there are no intercon-nections among new response words), but the lower right section is not.
Here,ijB  (not )(ijBD , but the undirected counterpart to it), which corre-sponds to the BA model, is taken as a subsection of the adjacency matrix that must be non-directed for the whole composition.???????
?=ijtBPPOA           (3)The key to understanding Equation (3) is to real-ize that P  is conditionally dependent onijB , because we assume a normal distribution for the number of non-zero values at each row in the lower section of A .
2.3 Simulation Results Taking into account the approximate numbers of possible new response words, in other words, the balance in sizes between the two sets in the bipartite graph, we built a composition of partial random graphs that could represent an adjacency matrix of the ACD model.
Figure 2 presents one of the results obtained for the following conditions:3000,1,5,3,10,900====== rcmmtm?
.
As the Figure shows, the local peak and the accompanying hemline in the degree distribution are clearly simulated by the complex combination of random graphs.10 20 30 40-8-6-4-2Figure 2.
Degree distribution of an ACD model  The degree distribution for the artificial net-work is consistent with the features observed for actual ACD data, where more than 96% of the stimulus words in each data set are distributed across the peak section of the degree distribution, which is why we have referred to them as peak words.
Moreover, it is easy to verify that without the assumption of a normal distribution for )(ic , distinct fractures emerge in the artificial curve where new response words in the bipartite struc-Local peak59ture would be distinguished from stimulus words located at initial points of the local peak.
3 Markov Clustering of ACDs 3.1 MCL This section introduces the graph clustering method that is applied to both the real and artifi-cial ACD data in order to compare them.
Markov Clustering (MCL) proposed by Van Dongen (2001) is well known as a scalable unsupervised cluster algorithm for graphs that decomposes a whole graph into small coherent groups by simu-lating the probability movements of a random walker across the graph.
It is believed that when MCL is applied to semantic networks, it yields clusters of words that share certain similarities in meaning or appear to be related to common con-cepts.
3.2 MCL Results The clustering results for the ACD model created by combining random graphs reveal that each of the resultant clusters contains only one stimulus word surrounded by several response words.
This result is somewhat strange because there are dense connections between stimulus words, which would lead us to assume that clusters would have multiple stimulus word.
However, the results of applying MCL clustering to the graph for the ACD model are in reality highly influenced by the sub-structure of the bipartite graph and less dependent on the scale-free structure.
Nevertheless, the result is quite similar to results observed with real data.
On examining MCL clustering results for different ACD semantic networks, we have observed that MCL clusters tend to consist of one word node with a relatively high degree and some other words with relatively low degrees.
On closer inspection of the graph, it is possible to see several supporter nodes that gather around one leader node, forming a kind of small conceptual community.
This suggests that the highest degree word for each cluster becomes a representative for that particular cluster consisting of some other low degree words.
In short, MCL clustering is executed based on such high degree words that tend to have relatively low curvature values (Dorow, 2005) compared to their high average degree values.4 Conclusion In this paper, we have proposed a basic approach to simulating word association dictionary data through the application of graph methodologies.
This modeling is expected not only to provide insights into the structures of real ACD data, but also to predict, by manipulating the model pa-rameters, possible forms for future ACDs.
Future research will focus on constructing an exponen-tial random graph model for ACDs based on Markov Chain Monte Carlo (MCMC) methods.
References Barab?si, Albert-L?szl?
and R?ka Albert.
1999.
Emergence of scaling in random networks, Science.
286:509-512.
Bollob?s, B?la.
2003.
Mathematical Results on Scale-free Random Graphs, http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf Dorow, Beate et al 2005.
Using Curvature and Markov Clustering in Graphs for Lexical Acquisi-tion and Word Sense Discrimination, MEANING-2005,2nd Workshop organized by the MEANING Project, February,3rd-4th.
Joyce, Terry and Maki Miyake.
2007.
Capturing the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Japanese Word Associations, Large-Scale Knowl-edge Resources.
Construction and Application, Springer Verlag:116-131.
Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J.
1973.
An associative thesaurus of English and its computer analysis, In Aitken, A.J., Bailey, R.W.
and Hamilton-Smith, N.
(Eds.
), The Computer and Literary Studies, Edinburgh University Press.
Nelson, Douglas L., Cathy L. McEvoy, & Thomas A. Schreiber.
1998.
The University of South Florida word association, rhyme, and word fragment norms, Retrieved August 31, 2005, from http://www.usf.edu/FreeAssociation Okamato, Jun and Shun Ishizaki.
2001.
Associative Concept Dictionary and its Comparison Electronic Concept Dictionaries.
PACLING2001-4th Confer-ence of the Pacific Association for Computational Linguistics:214-220.
Snijders, Tom A.B., Philippa E. Pattison, Garry L.Robins, Mark S. Handcock, 2005.
New Specifi-cations for Exponential Random Graph Models, http://stat.gamma.rug.nl/SnijdersPattisonRobinsHandcock2006.pdf Steyvers, Mark and Josh Tenenbaum.
2005.
The Large-Scale Structure of Semantic Networks, Sta-tistical Analyses and a Model of Semantic Growth, Cognitive Science.
29 (1):41-78.6061
