Text Classification by Bootstrapping withKeywords, EM and ShrinkageAndrew McCal lum ttmccallum@justresearch.comS Just Research4616 Henry StreetPittsburgh, PA 15213Kamal  Nigam tknigam@cs.cmu.edutSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213Abst rac tWhen applying text classification to com-plex tasks, it is tedious and expensiveto hand-label the large amounts of train-ing data necessary for good performance.This paper presents an alternative ap-proach to text classification that requiresno labeled documentsi instead, it uses asmall set of keywords per class, a classhierarchy and a large quantity of easily-obtained unlabeled ocuments.
The key-words are used to assign approximate la-bels to the unlabeled ocuments by term-matching.
These preliminary labels be-come the starting point for a bootstrap-ping process that learns a naive Bayes clas-sifier using Expectation-Maximization andhierarchical shrinkage.
When classifying acomplex data set of computer science re-search papers into a 70-leaf topic hierar-chy, the keywords alone provide 45% accu-racy.
The classifier learned by bootstrap-ping reaches 66% accuracy, a level close tohuman agreement.1 In t roduct ionWhen provided with enough labeled training exam-ples, a variety of text classification algorithms canlearn reasonably accurate classifiers (Lewis, 1998;Joachims, 1998; Yang, 1999; Cohen and Singer,1996).
However, when applied to complex domainswith many classes, these algorithms often require x-tremely large training sets to provide useful classifi-cation accuracy.
Creating these sets of labeled datais tedious and expensive, since typically they mustbe labeled by a person.
This leads us to considerlearning algorithms that do not require such largeamounts of labeled data.While labeled data is difficult to obtain, un-labeled data is readily available and plentiful.Castelli and Cover (1996) show in a theoreticalframework that unlabeled ata can indeed be usedto improve classification, although it is exponentiallyless valuable than labeled data.
Fortunately, unla-beled data can often be obtained by completely auto-mated methods.
Consider the problem of classifyingnews articles: a short Perl script and a night of au-tomated Internet downloads can fill a hard disk withunlabeled examples of news articles.
In contrast, itmight take several days of human effort and tediumto label even one thousand of these.In previous work (Nigam et al, 1999) it has beenshown that with just a small number of labeled ocu-ments, text classification error can be reduced by upto 30% when the labeled documents are augmentedwith a large collection of unlabeled ocuments.This paper considers the task of learning text clas-sifiers with no labeled documents at all.
Knowledgeabout the classes of interest is provided in the formof a few keywords per class and a class hierarchy.Keywords are typically generated more quickly andeasily than even a small number of labeled docu-ments.
Many classification problems naturally comewith hierarchically-organized classes.
Our algorithmproceeds by using the keywords to generate prelim-inary labels for some documents by term-matching.Then these labels, the hierarchy, and all the unla-beled documents become the input to a bootstrap-ping algorithm that produces a naive Bayes classi-fier.The bootstrapping algorithm used in this papercombines hierarchical shrinkage and Expectation-Maximization (EM) with unlabeled ata.
EM is aniterative algorithm for maximum likelihood estima-tion in parametric estimation problems with missingdata.
In our scenario, the class labels of the docu-ments are treated as missing data.
Here, EM worksby first training a classifier with only the documents52Computer Sciencecomputer, university, science, system, paperSoftware Programming OS Artificial ... Hardware & HC1 InformationEngineering programming distributed Intelligence Architecture computer Retrievalsoftware language system learning circuits system informationdesign logic systems university design multimedia textengineering university network computer computer university documentstools programs time based university paper classificationSemantics Garbage Compiler" " NLP Machine Planning Knowledge ... Interface Cooperative Multimediasemantics Collection Design language Learning planning Representation Design collaborative multimediadenotationel garbage compiler natural learning temporal knowledge interface cscw reallanguage collection code processing a lgor i thm reasoning representation design work timeconstruction memory parallel information algorithms plan language user provide datatypes optimization data text university problems system sketch group mediaregion language networks natural interfacesFigure 1: A subset of Cora's topic hierarchy.
Each node contains its title, and the five most probable words, ascalculated by naive Bayes and shrinkage with vertical word redistribution (Hofmann and Puzicha, 1998).
Wordsamong the initial keywords for that class are indicated in plain font; others are in italics.preliminarily-labeled by the keywords, and then usesthe classifier to re-assign probahilistically-weightedclass labels to all the documents by calculating theexpectation ofthe missing class labels.
It then trainsa new classifier using all the documents and iterates.We further improve classification by incorporatingshrinkage, a statistical technique for improving pa-rameter estimation in the face of sparse data.
Whenclasses are provided in a hierarchical relationship,shrinkage is used to estimate new parameters by us-ing a weighted average of the specific (but unreli-able) local class estimates and the more general (butalso more reliable) ancestors of the class in the hier-archy.
The optimal weights in the average are cal-culated by an EM process that runs simultaneouslywith the EM that is re-estimating the class labels.Experimental evaluation of this bootstrapping ap-proach is performed on a data set of thirty-thousandcomputer science research papers.
A 70-leaf hier-archy of computer science and a few keywords foreach class are provided as input.
Keyword matchingalone provides 45% accuracy.
Our bootstrapping al-gorithm uses this as input and outputs a naive Bayestext classifier that achieves 66% accuracy.
Inter-estingly, this accuracy approaches estimated humanagreement levels of 72%.The experimental domain in this paper originatesas part of the Ra research project, an effort to builddomain-specific search engines on the Web with ma-chine learning techniques.
Our demonstration sys-tem, Cora, is a search engine over computer scienceresearch papers (McCallum et al, 1999).
The boot-strapping classification algorithm described in thispaper is used in Corn to place research papers intoa Yahoo-like hierarchy specific to computer science.The-search engine, including this hierarchy, is pub-licly available at www.
cora.justresearch, com.2 Generating Preliminary Labelswith KeywordsThe first step in the bootstrapping process is to usethe keywords to generate preliminary labels for asmany of the unlabeled ocuments as possible.
Eachclass is given just a few keywords.
Figure 1 showsexamples of the number and type of keywords givenin our experimental domain--the human-providedkeywords are shown in the nodes in non-italic font.In this paper, we generate preliminary labels fromthe keywords by term-matching in a rule-list fashion:for each document, we step through the keywordsand place the document in the category of the firstkeyword that matches.
Finding enough keywords toobtain broad coverage while simultaneously findingsufficiently specific keywords to obtain high accuracyis very difficult; it requires intimate knowledge of thedata and a lot of trial and error.As a result, classification by keyword matching isboth an inaccurate and incomplete.
Keywords tendto provide high-precision and low-recall; this brittle-ness will leave many documents unlabeled.
Somedocuments will match keywords from the wrongclass.
In general we expect he low recall of the key-words to be the dominating factor in overall error.In our experimental domain, for example, 59% of theunlabeled ocuments do not contain any keywords.Another method of priming bootstrapping withkeywords would be to take each set of keywords as a53labeled mini-document containing just a few words.This could then be used as input to any standardlearning algorithm.
Testing this, and other keywordlabeling approaches, is an area of ongoing work.3 The Bootstrapping AlgorithmThe goal of the bootstrapping step is to generatea naive Bayes classifier from the inputs: the (inac-curate and incomplete) preliminary labels, the un-labeled data and the class hierarchy.
One straight-forward method would be to simply take the unla-beled documents with preliminary labels, and treatthis as labeled data in a standard supervised set-ting.
This approach provides only minimal benefitfor three reasons: (1) the labels are rather noisy,(2) the sample of preliminarily-labeled documentsis skewed from the regular document distribution(i.e.
it includes only documents containing key-words), and (3) data are sparse in comparison tothe size of the feature space.
Adding the remain-ing unlabeled data and running EM helps counterthe first and second of these reasons.
Adding hier-archical shrinkage to naive Bayes helps counter thefirst and third of these reasons.
We begin a detaileddescription of our bootstrapping algorithm with ashort overview of standard naive Bayes text classi-fication, then proceed by adding EM to incorporatethe unlabeled ata, and conclude by explaining hi-erarchical shrinkage.
An outline of the entire algo-rithm is presented in Table 1.3.1 The  naive Bayes  f rameworkWe build on the framework of multinomial naiveBayes text classification (Lewis, 1998; McCallumand Nigam, 1998).
It is useful to think of naiveBayes as estimating the parameters of a probabilis-tic generative model for text documents.
In thismodel, first the class of the document is" selected.The words of the document are then generated basedon the parameters for the class-specific multinomial(i.e.
unigram model).
Thus, the classifier parame-ters consist of the class prior probabilities and theclass-conditioned word probabilities.
For formally,each class, cj, has a document frequency relative toall other classes, written P(cj).
For every wordwt in the vocabulary V, P(wtlcj) indicates the fre-quency that the classifier expects word wt to occurin documents in class cj.In the standard supervised setting, learning of theparameters i accomplished using a set of labeledtraining documents, 79.
To estimate the word prob-ability parameters, P (wt I cj), we count the frequencywith which word wt occurs among all word occur-rences for documents in class cj.
We supplement?
Inputs: A collection 79 of unlabeled ocuments, aclass hierarchy, and a few keywords for each class.?
Generate preliminary labels for as many of the unla-beled documents as possible by term-matching withthe keywords in a rule-list fashion.?
Initialize all the Aj's to be uniform along each pathfrom a leaf class to the root of the class hierarchy.?
Iterate the EM algorithm:?
(M-step) Build the maximum likelihoodmultinomial at each node in the hierarchygiven the class probability estimates for eachdocument (Equations 1 and 2).
Normalize allthe Aj's along each path from a leaf class to theroot of the class hierarchy so that they sum to1.?
(E-step) Calculate the expectation of theclass labels of each document using the clas-sifter created in the M-step (Equation 3).
In-crement the new )~j's by attributing each wordof held-out data probabilistically to the ances-tors of each class.?"
Output:  A naive Bayes classifier that takes an un-labeled document and predicts a class label.Table 1: An outline of the bootstrapping algorithm de-scribed in Sections 2 and 3.this with Laplace smoothing that primes each esti-mate with a count of one to avoid probabilities ofzero.
Let N(wt,di) be the count of the number oftimes word we occurs in document di, and defineP(cj\[di) E {0, 1}, as given by the document's classlabel.
Then, the estimate of the probability of wordwt in class cj is:1 + ~a,~v N(wt, di)P(cjldi )P(wtlc~) =IVl + N(w., di)e(cjldi ) "(1)The class prior probability parameters are set in thesame way, where ICI indicates the number of classes:P(cj) = 1 + Ea, ev P(cjldi)ICl + IVl (2)Given an unlabeled ocument and a classifier, wedetermine the probability that the document be-longs in class cj using Bayes' rule and the naiveBayes assumption--that the words in a documentoccur independently of each other given the class.
Ifwe denote Wd~,k to be the kth word in document di,then classification becomes:54P(cjld~) c(P(cj)P(di lc j )Id, I(X P(Cj) H P(Wd'.
~lcj)" (3)k~lEmpirically, when given a large number of train-ing documents, naive Bayes does a good job ofclassifying text documents (Lewis, 1998).
Morecomplete presentations of naive Bayes for textclassification are provided by Mitchell (1997) andMcCallum and Nigam (1998).3.2 Adding unlabeled ata with EMIn the standard supervised setting, each documentcomes with a label.
In our bootstrapping sce-nario, the preliminary keyword labels are both in-complete and inaccurate--the keyword matchingleaves many many documents unlabeled, and la-bels some incorrectly.
In order to use the entiredata set in a naive Bayes classifier, we use theExpectation-Maximization (EM) algorithm to gen-erate probabilistically-weighted class labels for allthe documents.
This results in classifier parametersthat are more likely given all the data.EM is a class of iterative algorithms for maximumlikelihood or maximum a posteriori parameter esti-mation in problems with incomplete data (Dempsteret al, 1977).
Given a model of data generation, anddata with some missing values, EM iteratively usesthe current model to estimate the missing values,and then uses the missing value estimates to im-prove the model.
Using all the available data, EMwill locally maximize the likelihood of the parame-ters and give estimates for the missing values.
Inour scenario, the class labels of the unlabeled ataare the missing values.In implementation, EM is an iterative two-stepprocess.
Initially, the parameter estimates are setin the standard naive Bayes way from just thepreliminarily labeled documents.
Then we iter-ate the E- and M-steps.
The E-step calculatesprobabilistically-weighted class labels, P(cjldi), forevery document using the classifier and Equation 3.The M-step estimates new classifier parameters us-ing all the documents, by Equations 1 and 2, whereP(cjldi) is now continuous, as given by the E-step.We iterate the E- and M-steps until the classifierconverges.
The initialization step from the prelimi-nary labels identifies each mixture component witha class and seeds EM so that the local maxima thatit finds correspond well to class definitions.In previous work (Nigam et al, 1999), we haveshown this technique significantly increases textclassification accuracy when given limited amountsof labeled ata and large amounts of unlabeled ata.The expectation here is that EM will both correctand complete the labels for the entire data set.3.3 Improving sparse data est imates withshrinkageEven when provided with a large pool of documents,naive Bayes parameter stimation during bootstrap-ping will suffer from sparse data because naive Bayeshas so many parameters to estimate (\[V\[IC I + IC\[).Using the provided class hierarchy, we can integratethe statistical technique shrinkage into the boot-strapping algorithm to help alleviate the sparse dataproblem.Consider trying to estimate the probability of theword "intelligence" in the class NLP.
This wordshould clearly have non-negligible probability there;however, with limited training data we may be un-lucky, and the observed frequency of "intelligence"in NLP may be very far from its true expected value.One level up the hierarchy, however, the Artificial In-telligence class contains many more documents (theunion of all the children).
There, the probabilityof the word "intelligence" can be more reliably esti-mated.Shrinkage calculates new word probability esti-mates for each leaf class by a weighted average ofthe estimates on the path from the leaf to the root.The technique balances a trade-off between speci-ficity and reliability.
Estimates in the leaf are mostspecific but unreliable; further up the hierarchy es-timates are more reliable but unspecific.
We cancalculate mixture weights for the averaging that areguaranteed to maximize the likelihood of held-outdata with the EM algorithm.One can think of hierarchical shrinkage as a gener-ative model that is slightly augmented from the onedescribed in Section 3.1.
As before, a class (leaf) isselected first.
Then, for each word position in thedocument, an ancestor of the class (including itself)is selected according to the shrinkage weights.
Then,the word itself is chosen based on the multinomialword distribution of that ancestor.
If each word inthe training data were labeled with which ancestorwas responsible for generating it, then estimatingthe mixture weights would be a simple matter ofmaximum likelihood estimation from the ancestoremission counts.
But these ancestor labels are notprovided in the training data, and hence we use EMto fill in these missing values.
We use the term ver-tical EM to refer to this process that calculates an-cestor mixture weights; we use the term horizontalEM to refer to the process of filling in the missing55class (leaf) labels on the unlabeled ocuments.
Bothvertical and horizontal EM run concurrently, withinterleaved E- and M-steps.More formally, let {pl(wt\[cj),...,pk(wtlcj) } beword probability estimates, where pl(wt\[cj) is themaximum likelihood estimate using training datajust in the leaf, P2(wtlcj) is the maximum likeli-hood estimate in the parent using the training datafrom the union of the parent's children, pk-1 (w~lcj)is the estimate at the root using all the training data,and pk(wtlcj) is the uniform estimate (Pk(wtlcj) =1/IVI).
The interpolation weights among cj's "an-cestors" (which we define to include cj itself) arewritten {A}, A~,..., A~}, where Ea:lk Aja = 1.
Thenew word probability estimate based on shrinkage,denoted P(wt\[cj), is thenr'(wtlc3) = AJPl(wtlc~) +.
.
.
+ A~pk(wtlcy).
(4)The Aj vectors are calculated by the iterations ofEM.
In the E-step we calculate for each class cjand each word of unlabeled held out data, ~,  theprobability that the word was generated by the ithancestor.
In the M-step, we normalize the sum ofthese expectations to obtain new mixture weights,kj.
Without the use of held out data, all the mix-ture weight would concentrate in the leaves.Specifically, we begin by initializing the A mixtureweights for each leaf to a uniform distribution.
Let/3~ (di,k) denote the probability that the ath ancestorof cj was used to generate word occurrence di,k.
TheE-step consists of estimating the/Ts:ICj) (5)5 (di,k) = A npm(wd',  IcJ) "In the M-step, we derive new and guaranteed im-proved weights, A, by summing and normalizing theX~ = Ed,.~en P(c~ldi) (6)The E- and M-steps iterate until the ~'s con-verge.
These weights are then used to calculatenew shrinkage-based word probability estimates, asin Equation 4.
Classification of new test documentsis performed just as before (Equation 3), where theLaplace estimates of the word probability estimatesare replaced by shrinkage-based estimates.A more complete description of hierarchicalshrinkage for text classification is presented byMcCallum et al (1998).4 Related WorkOther research efforts in text learning have also usedbootstrapping approaches.
The co-training algo-rithm (Blum and Mitchell, 1998) for classificationworks in cases where the feature space is separableinto naturally redundant and independent parts.
Forexample, web pages can be thought of as the text onthe web page, and the collection of text in hyperlinkanchors to that page.A recent paper by Riloff and Jones (1999) boot-straps a dictionary of locations from just a small setof known locations.
Here, their mutual bootstrapalgorithm works by iteratively identifying syntacticconstructs indicative of known locations, and identi-fying new locations using these indicative constructs.The preliminary labeling by keyword matchingused in this paper is similar to the seed collocationsused by Yarowsky (1995).
There, in a word sensedisambiguation task, a bootstrapping algorithm isseeded with some examples of common collocationswith the particular sense of some word (e.g.
the seed"life" for the biological sense of "plant").5 Experimental ResultsIn this section, we provide empirical evidence thatbootstrapping a text classifier from unlabeled atacan produce ahigh-accuracy text classifier.
As a testdomain, we use computer science research papers.We have created a 70-1ear hierarchy of computer sci-ence topics, part of which is shown in Figure 1.
Cre-ating the hierarchy took about 60 minutes, duringwhich we examined conference proceedings, and ex-plored computer science sites on the Web.
Select-ing a few keywords associated with each node tookabout 90 minutes.
A test set was created by experthand-labeling of a random sample of 625 researchpapers from the 30,682 papers in the Cora archive atthe time we began these experiments.
Of these, 225(about one-third) did not fit into any category, andwere discarded--resulting i  a 400 document testset.
Labeling these 400 documents took about sixhours.
Some of these papers were outside the areaof computer science (e.g.
astrophysics papers), butmost of these were papers that with a more completehierarchy would be considered computer science pa-pers.
The class frequencies of the data are not tooskewed; on the test set, the most populous class ac-counted for only 7% of the documents.Each research paper is represented as the wordsof the title, author, institution, references, and ab-stract.
A detailed description of how these seg-ments are automatically extracted is provided else-where (McCallum et al, 1999; Seymore t al., 1999).56MethodKeywo~dNBNBNB+EM+SNBNB+SHuman# Lab100399# P-Lab12,65712,65712,657# Unlab Acc- -  45%- -  30%- -  47%18,025 66%- -  47%- -  63%- -  72%Table 2: Classification results with different techniques:keyword matching, human agreement, aive Bayes (NB),and naive Bayes combined with hierarchical shrink-age (S), and EM.
The classification accuracy (Acc),and the number of labeled (Lab), keyword-matchedpreliminarily-labeled (P-Lab), and unlabeled (Unlab)documents used by each method are shown.Words occurring in fewer than five documents andwords on a standard stoplist were discarded.
Nostemming was used.
Bootstrapping was performedusing the algorithm outlined in Table 1.Table 2 shows classification results with differentclassification techniques used.
The rule-list classifierbased on the keywords alone provides 45%.
(The43% of documents in the test set containing no key-words cannot be assigned a class by the rule~list clas-sifter, and are counted as incorrect.)
As an inter-esting time comparison, about 100 documents couldhave been labeled in the time it took to generatethe keyword lists.
Naive Bayes accuracy with 100labeled documents i only 30%.
With 399 labeleddocuments (using our test set in a leave-one-out-fashion), naive Bayes reaches 47%.
When runningthe bootstrapping algorithm, 12,657 documents aregiven preliminary labels by keyword matching.
EMand shrinkage incorporate he remaining 18,025 doc-uments, "fix" the preliminary labels and leverage thehierarchy; the resulting accuracy is 66%.
As an in-teresting comparison, agreement on the test set be-tween two human experts was 72%.A few further experiments reveal some of theinner-workings ofbootstrapping.
If we build a naiveBayes classifier in the standard supervised way fromthe 12,657 preliminarily abeled ocuments he clas-sifter gets 47% accuracy.
This corresponds to theperformance for the first iteration of bootstrapping.Note that this matches the accuracy of traditionalnaive Bayes with 399 labeled training documents,but that it requires less than a quarter the hu-man labeling effort.
If we run bootstrapping with-out the 18,025 documents left unlabeled by keywordmatching, accuracy reaches 63%.
This indicates thatshrinkage and EM on the preliminarily labeled doc-uments is providing substantially more benefit hanthe remaining unlabeled ocuments.One explanation for the small impact of the 18,025documents left unlabeled by keyword matching isthat many of these do not fall naturally into thehierarchy.
Remember that about one-third of the30,000 documents fall outside the hierarchy.
Mostof these will not be given preliminary labels by key-word matching.
The presence of these outlier docu-ments kews EM parameter estimation.
A more in-clusive computer science hierarchy would allow theunlabeled ocuments o benefit classification more.However, even without a complete hierarchy, wecould use these documents if we could identify theseoutliers.
Some techniques for robust estimation withEM are discussed by McLachlan and Basford (1988).One specific technique for these text hierarchies i  toadd extra leaf nodes containing uniform word dis-tributions to each interior node of the hierarchy inorder to capture documents not belonging in any ofthe predefined topic leaves.
This should allow EMto perform well even when a large percentage ofthedocuments do not fall into the given classificationhierarchy.
A similar approach is also planned for re-search in topic detection and tracking (TDT) (Bakeret al, 1999).
Experimentation with these techniquesis an area of ongoing research.6 Conc lus ions  and  Future  WorkThis paper has considered building a text classifierwithout labeled training documents.
In its place,our bootstrapping algorithm uses a large pool of un-labeled documents and class-specific knowledge inthe form of a few keywords per class and a classhierarchy.
The bootstrapping algorithm combinesExpectation-Maximization and hierarchical shrink-age to correct and complete preliminary labelingprovided by keyword matching.
Experimental re-sults show that accuracies close to human agreementcan be obtained by the bootstrapping algorithm.In future work we plan to refine our probabilis-tic model to allow for documents to be placed ininterior hierarchy nodes, documents to have mul-tiple class assignments, and classes to be modeledwith multiple mixture components.
We are also in-vestigating principled methods of re-weighting theword features for "semi-supervised" clustering thatwill provide better discriminative training with un-labeled ata.A c k n o w l e d g e m e n t sKamal Niga~rt was supported in part by the DarpaHPKB program under contract F30602-97-1-0215.57ReferencesD.
Baker, T. Hofmann, A. McCallum, and Y. Yang.1999.
A hierarchical probabilistic model for nov-elty detection in text.
Technical report, Just Re-search, http://www.cs.cmu.edu/~mccallum.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT'98.V.
Castelli and T. M. Cover.
1996.
The relativevalue of labeled and unlabeled samples in pat-tern recognition with an unknown mixing param-eter.
IEEE Transactions on Information Theory,42(6):2101-2117.W.
Cohen and Y.
Singer.
1996.
Context-sensitivelearning methods for text categorization.
In SI-GIR '96.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1-38.T.
Hofmann and J. Puzicha.
1998.
Statistical mod-els for co-occurrence data.
Technical Report AIMemo 1625, AI Lab, MIT.T.
Joachims.
1998.
Text categorization with Sup-port Vector Machines: Learning with many rele-vant features.
In ECML-98.D.
D. Lewis.
1998.
Naive (Bayes) at forty: Theindependence assumption i information retrieval.In ECML-98.A.
McCallum and K. Nigam.
1998.
A comparisonof event models for naive Bayes text classification.In AAAL98 Workshop on Learning \]or Text Cat-egorization.
Tech.
rep. WS-98-05, AAAI Press.http://www.cs.cmu.edu/Nmccallum.A.
McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.1998.
Improving text clasification by shrinkage ina hierarchy of classes.
In ICML-98.Andrew McCallum, Kamal Nigam, Jason Rennie,and Kristie Seymore.
1999.
Using machine learn-ing techniques to build domain-specific search en-gines.
In IJCAI-99.
To appear.G.J.
McLachlan and K.E.
Basford.
1988.
MixtureModels.
Marcel Dekker, New York.T.
M. Mitchell.
1997.
Machine Learning.
McGraw-Hill, New York.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.1999.
Text classification from labeled and unla-beled documents using EM.
Machine Learning.To appear.E.
Riloff and R. Jones.
1999.
Learning dictionariesfor information extraction using multi-level boot-strapping.
In AAAI-99.
To appear.K.
Seymore, A. McCallum, and R. Rosenfeld.
1999.Learning hidden Markov model structure for in-formation extraction.
In AAAI-99 Workshop onMachine Learning for Information Extraction.
Toappear.Y.
Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Journal of In-formation Retrieval.
To appear.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In A CL-95.513
