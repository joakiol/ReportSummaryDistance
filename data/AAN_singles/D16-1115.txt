Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1072?1077,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAutomatic Features for Essay Scoring ?
An Empirical StudyFei Dong and Yue ZhangSingapore University of Technology and Designfei dong@mymail.sutd.edu.sg and yue zhang@sutd.edu.sgAbstractEssay scoring is a complicated processing re-quiring analyzing, summarizing and judgingexpertise.
Traditional work on essay scoringfocused on automatic handcrafted features,which are expensive yet sparse.
Neural mod-els offer a way to learn syntactic and semanticfeatures automatically, which can potentiallyimprove upon discrete features.
In this pa-per, we employ convolutional neural network(CNN) for the effect of automatically learn-ing features, and compare the result with thestate-of-art discrete baselines.
For in-domainand domain-adaptation essay scoring tasks,our neural model empirically outperforms dis-crete models.1 IntroductionAutomatic essay scoring (AES) is the task of build-ing a computer-based grading system, with the aimof reducing the involvement of human raters as faras possible.
AES is challenging since it relies notonly on grammars, but also on semantics, discourseand pragmatics.
Traditional approaches treat AESas a classification (Larkey, 1998; Rudner and Liang,2002), regression (Attali and Burstein, 2004; Phandiet al, 2015), or ranking classification problem (Yan-nakoudakis et al, 2011; Chen and He, 2013), ad-dressing AES by supervised learning.
Features aretypically bag-of-words, spelling errors and lengths,such word length, sentence length and essay length,etc.
Some grammatical features are considered toassess the quality of essays (Yannakoudakis et al,2011).
A drawback is feature engineering, whichcan be time-consuming, since features need to becarefully handcrafted and selected to fit the appro-riate model.
A further drawback of manual featuretemplates is that they are sparse, instantiated by dis-crete pattern-matching.
As a result, parsers and se-mantic analyzers are necessary as a preprocessingstep to offer syntactic and semantic patterns for fea-ture extraction.
Given variable qualities of studentessays, such analyzers can be highly unreliable.Neural network approaches have been shown tobe capable of inducing dense syntactic and semanticfeatures automatcially, giving competitive results tomanually designed features for several tasks (Kalch-brenner et al, 2014; Johnson and Zhang, 2014; dosSantos and Gatti, 2014).
In this paper, we empir-ically investigate a neural network method to learnfeatures automatically for AES, without the need ofexternal pre-processing.
In particular, we build a hi-erarchical CNN model, with one lower layer repre-senting sentence structures and one upper layer rep-resenting essay structure based on sentence repre-sentations.
We compare automatically-induced fea-tures by the model with state-of-art baseline hand-crafted features.
Empirical results show that neuralfeatures learned by CNN are very effective in essayscoring task, covering more high-level and abstractinformation compared to manual feature templates.2 Related WorkFollowing the first AES system Project Essay Grade(PEG) been developed in 1966 (Page, 1994), a num-ber of commercial systems have come out, suchas IntelliMetric 2, Intelligent Essay Assessor (IEA)(Foltz et al, 1999) and e-rater system (Attali andBurstein, 2004).
The e-rater system now plays a1072second human rater?s role in the Test of English asa Foreign Language (TOEFL) and Graduate RecordExamination (GRE).
The e-rater extracts a numberof complex features, such as grammatical error andlexical complexity, and uses stepwise linear regres-sion.
IEA uses Latent Semantic Analysis (LSA)(Landauer et al, 1998) to create semantic vectors foressays and measure the semantic similarity betweenthe vectors.In the research literature, Larkey (1998) and Rud-ner and Liang (2002) treat AES as classification us-ing bag-of-words features.
Other recent work for-mulates the task as a preference ranking problem(Yannakoudakis et al, 2011; Chen and He, 2013).Yannakoudakis et al (2011) formulated AES as apairwise ranking problem by ranking the order ofpair essays based on their quality.
Features consistof word, POS n-grams features, complex grammati-cal features and so on.
Chen and He (2013) formu-lated AES into a listwise ranking problem by con-sidering the order relation among the whole essaysand features contain syntactical features, grammarand fluency features as well as content and prompt-specific features.
Phandi et al (2015) use correlatedBayesian Linear Ridge Regression (cBLRR) focus-ing on domain-adaptation tasks.
All these previousmethods use discrete handcrafted features.Recently, Alikaniotis et al (2016) also employa neural model to learn features for essay scor-ing automatically, which leverages a score-specificword embedding (SSWE) for word representationsand a two-layer bidirectional long-short term mem-ory network (LSTM) to learn essay representations.Alikaniotis et al (2016) show that by combiningSSWE, LSTM outperforms traditional SVM model.On the other hand, using LSTM alone does not givesignificantly more accuracies compared to SVM.This conforms to our preliminary experiments withthe LSTM structure.
Here, we use CNN withoutany specific embeddings and show that our neuralmodels could outperform baseline discrete modelson both in-domain and cross-domain senarios.CNN has been used in many NLP applications,such as sequence labeling (Collobert et al, 2011) ,sentences modeling (Kalchbrenner et al, 2014), sen-tences classification (Kim, 2014), text categorization(Johnson and Zhang, 2014; Zhang et al, 2015) andsentimental analysis (dos Santos and Gatti, 2014),Feature Type Feature DescriptionLength Number of characters, words,sentences, etc.POS Relative and absolute number ofbad POS n-gramsPrompt Relative and absolute number ofwords and their synonyms in theessay appearing in the promptBag-of-words Count of useful unigrams andbigrams (unstemmed, stemmedand spell corrected)Table 1: Feature description used by EASE.etc.
In this paper, we explore CNN representa-tion ability for AES tasks on both in-domain anddomain-adaptation settings.3 BaselineBayesian Linear Ridge Regression (BLRR) andSupport Vector Regression (SVR) (Smola and Vap-nik, 1997) are chosen as state-of-art baselines.
Fea-ture templates follow (Phandi et al, 2015), extractedby EASE1, which are briefly listed in Table 1.
?Use-ful n-grams?
are determined using the Fisher test toseparate the good scoring essays and bad scoring es-says.
Good essays are essays with a score greaterthan or equal to the average score, and the remainderare considered as bad scoring essays.
The top 201 n-grams with the highest Fisher values are chosen asthe bag of features and these top 201 n-grams consti-tute useful n-grams.
Correct POS tags are generatedusing grammatically correct texts, which is done byEASE.
The POS tags that are not included in thecorrect POS tags are treated as bad POS tags, andthese bad POS tags make up the ?bad POS n-grams?features.The features tend to be highly useful for thein-domain task since the discrete features of sameprompt data share the similar statistics.
However,for different prompts, features statistics vary signif-icantly.
This raises challenges for discrete featurepatterns.ML-?
(Phandi et al, 2015) was proposed to ad-dress this issue.
It is based on feature augmentation,incorporating explicit correlation into augmentedfeature spaces.
In particular, it expands baseline fea-ture vector x to be ?s(x) = (?x, (1?
?2)1/2x) and?t(x) = (x,0p) for source and target domain data1https://github.com/edx/ease1073Figure 1: Hierarchical CNN structurein R2p respectively, with ?
being the correlation be-tween source and target domain data.
Then BLRRand maximum likelihood estimation are used to theoptimize correlation.
All the baseline models re-quire POS-tagging as a pre-processing step, extract-ing syntactic features based on POS-tags.4 ModelWord Representations We use word embeddingwith an embedding matrix Ew ?
Rdw?Vw wheredw is the embedding dimension, and Vw representswords vocabulary size.
A word vector zi is repre-sented by zi = Ewwi where wi is the i-th word ina sentence.
In contrast to the baseline models, ourCNN model does not rely on POS-tagging or otherpre-processing.CNNModel We take essay scoring as a regressiontask and employ a two-layer CNN model, in whichone convolutional layer is used to extract sentencesrepresentations, and the other is stacked on sentencevectors to learn essays representations.
The archi-tecture is depicted in Figure 1.
Given an input sen-tence z1, z2, ..., zn, a convolution layer with a filterw ?
Rh?k is applied to a window of h words toproduce n-grams features.
For instance, a feature ciis generated from a window of words zi:i+h?1 byci = f(w ?
zi:i+h?1 + b) , b ?
R is the bias termand f is the non-linear activation function rectifiedlinear unit (ReLU).The filter is applied to the all possible win-dows in a sentence to produce a feature map c =[c1, c2, ..., cm?h+1].
For cj of the j-th sentence inan essay, max-pooling and average pooling func-tion are used to produce the sentence vector sj =Set #Essays Genre Avg Len.
Range Med.1 1783 ARG 350 2-12 82 1800 ARG 350 1-6 33 1726 RES 150 0-3 14 1772 RES 150 0-3 15 1805 RES 150 0-4 26 1800 RES 150 0-4 27 1569 NAR 250 0-30 168 723 NAR 650 0-60 36Table 2: Details of the ASAP data; the last two columnsare score range and median scores.
For genre, ARG spec-ifies argumentative essays, RES means response essaysand NAR denotes narrative essays.max{cj} ?
avg{cj}.
The second convolutionallayer takes s1, s2,..., sn as inputs, followed by pool-ing layer (max-pooling and average-pooling) and afully-connected hidden layer.
The hidden layer di-rectly connects to output layer which generates ascore.5 Experiments5.1 SetupData We use the Automated Student AssessmentPrize (ASAP)2 dataset as evaluation data for ourtask, which contains 8 prompts of different genresas listed in Table 2.
The essay scores are scaled intothe range from 0 to 1.
The settings of data prepara-tion follow (Phandi et al, 2015).
We use quadraticweighted kappa (QWK) as the metric.
For domain-adaptation (cross-domain) experiments, we follow(Phandi et al, 2015), picking four pairs of essayprompts, namely, 1?2, 3?4, 5?6 and 7?8, where1?2 denotes prompt 1 as source domain and prompt2https://www.kaggle.com/c/asap-aes/data1074Parameter Parameter Name Valuedw Word embedding dimension 100hwrd Word context window size 5hsent Sentence context window size 3kwrd Word convolution units 50ksent Sentence convolution units 50p Hidden size 50drop rate Dropout rate 0.5batch size Batch size 4?
Learning rate 0.01Table 3: Neural Model Hyper-parametersFigure 2: In-domain results2 as target domain.
All source domain essays areused as training data.
Target domain data are ran-domly divided into 5 folds, where one fold is usedas test data, and other 4 folds are collected togetherto sub-sample target domain train data.
The sub-sampled sizes are 10, 25, 50, 100, with the largersampled sets containing the smaller ones.
And werepeated sub-sampling 5 times for each target train-ing number to alleviate bias.Hyper-parameters We use Adagrad for optimiza-tion.
Word embeddings are randomly initialized andthe hyper-parameter settings are listed in Table 3.5.2 ResultsIn-domain The in-domain results are shown inFigure 2.
The average values of all 8 prompt setsare listed in Table 4.
For the in-domain task, CNNoutperforms the baseline model SVR on all promptsof essay sets, and is competitive to BLRR.
For thestatistical significance, neural model is significantlybetter than baseline models with the p-value lessthan 10?5 at the confidence level of 95%.
The av-erage kappa value over 8 prompts is close to that ofhuman raters.Cross-domain The domain-adaptation results areshown in Table 5.
It can be seen that our CNNModel BLRR SVR CNN HumanAvg 0.725 0.682 0.734 0.754Std dev 0.0025 0.0033 0.0029 ?Table 4: Indomain average kappa value and standard de-viation over all 8 prompts.Pairs Method nt = 10 25 50 1001?2 ML-?
0.365 0.437 0.521 0.559CNN 0.546 0.569 0.563 0.5593?4 ML-?
0.435 0.540 0.590 0.619CNN 0.628 0.656 0.659 0.6625?6 ML-?
0.415 0.600 0.678 0.718CNN 0.647 0.700 0.714 0.7507?8 ML-?
0.328 0.438 0.496 0.551CNN 0.570 0.590 0.568 0.587Table 5: Cross-domain results.model outperforms ML-?
on almost all pairs ofadaptation experiments.
ML-?
domain-adaptationmethod?s performance improves as the size of tar-get domain training data increases.
However, com-pared to ML-?, target training data size has less im-pact on our neural model.
Even if the target train-ing size is small, the neural model still gives strongperformance.
This results from the fact that neu-ral model could learn more high-level and abstractfeatures compared to traditional models with hand-crafted discrete features.
We plot the confusion ma-trix between truth and model prediction on test datain Figure 4, which shows that prediction scores ofneural model tend to be closer to true values, whichis very important in our task.5.3 Feature AnalysisTo visualize the features learned by our model, weuse t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008), pro-jecting 50-dimensional features into 2-dimensionalspace.
We take two domain pairs 3?4 and 5?6as examples on the cross-domain task, extractingfully-connected hidden-layer features for target do-main data using model trained on source domaindata.
The results are showed in Figure 3.
The base-line discrete features are more concentrated, whichshows that patterns on source prompt are weak indifferentiating target prompt essays.
By using ML-?and leveraging 100 target prompt training examples,the discrete features patterns are more scattered, in-creasing the differentiating power.
In contrast, CNN1075(a) (b) (c)(d) (e) (f)Figure 3: Visualization of discrete and neural features using t-SNE (each value represents an essay of the correspond-ing score).
Top: Set 4 (3?4), Bottom: Set 6 (5?6).
(a) discrete features; (b) ML-?
features, nt = 100; (c) neuralfeatures; (d) discrete features; (e) ML-?
features, nt = 100; (f) neural features.
(a) (b)(c) (d)Figure 4: Confusion matrix of true and prediction scoresby two different models on test data when target trainingsize nt = 10.
(a) ML-?
on 1?2; (b) CNN model on1?2; (c) ML-?
on 5?6; (d) CNN model on 5?6.features trained on source prompt are sparse whenused directly on the target prompt.
This shows thatneural features learned by the CNN model can betterdifferentiate essays of different qualities.
Withoutmanual templates, such features automatically cap-ture subtle and complex information that is relevantto the task.6 ConclusionWe empirically investigated a hierarchical CNNmodel for automatic essay scoring, showing au-tomatically learned features competitive to dis-crete handcrafted features for both in-domain anddomain-adaptation tasks.
The results demonstratelarge potential for deep learning in AES.AcknowledgmentsWe thank the anonymous reviewers for their con-structive comments, which helped to improve thepaper.
This work is supported by NSFC61572245and T2MOE201301 from Singapore Ministry of Ed-ucation.1076ReferencesDimitrios Alikaniotis, Helen Yannakoudakis, and MarekRei.
2016.
Automatic text scoring using neural net-works.
arXiv preprint arXiv:1606.04289.Yigal Attali and Jill Burstein.
2004.
Automated essayscoring with e-rater R?
v. 2.0.
ETS Research ReportSeries, 2004(2):i?21.Hongbo Chen and Ben He.
2013.
Automated essayscoring by maximizing human-machine agreement.
InEMNLP, pages 1741?1752.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.C?
?cero Nogueira dos Santos and Maira Gatti.
2014.
Deepconvolutional neural networks for sentiment analysisof short texts.
In COLING, pages 69?78.Peter W Foltz, Darrell Laham, and Thomas K Landauer.1999.
Automated essay scoring: Applications to edu-cational technology.
In proceedings of EdMedia, vol-ume 99, pages 40?64.Rie Johnson and Tong Zhang.
2014.
Effective use ofword order for text categorization with convolutionalneural networks.
arXiv preprint arXiv:1412.1058.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
arXiv preprint arXiv:1404.2188.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.Thomas K Landauer, Peter W Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse processes, 25(2-3):259?284.Leah S Larkey.
1998.
Automatic essay grading us-ing text categorization techniques.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 90?95.
ACM.Ellis Batten Page.
1994.
Computer grading of studentprose, using modern concepts and software.
The Jour-nal of experimental education, 62(2):127?142.Peter Phandi, Kian Ming A Chai, and Hwee Tou Ng.2015.
Flexible domain adaptation for automated es-say scoring using correlated linear regression.Lawrence M Rudner and Tahung Liang.
2002.
Auto-mated essay scoring using bayes?
theorem.
The Jour-nal of Technology, Learning and Assessment, 1(2).Alex Smola and Vladimir Vapnik.
1997.
Support vectorregression machines.
Advances in neural informationprocessing systems, 9:155?161.Laurens Van der Maaten and Geoffrey Hinton.
2008.
Vi-sualizing data using t-sne.
Journal of Machine Learn-ing Research, 9(2579-2605):85.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies-Volume 1, pages180?189.
Association for Computational Linguistics.Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for text classi-fication.
In Advances in Neural Information Process-ing Systems, pages 649?657.1077
