Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 15?23,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSemantic Enrichment of Text with Background KnowledgeAnselmo Pe?as Eduard HovyUNED NLP & IR Group USC Information Sciences InstituteJuan del Rosal, 16 4676 Admiralty Way28040 Madrid, Spain Marina del Rey, CA 90292-6695anselmo@lsi.uned.es hovy@isi.eduAbstractTexts are replete with gaps, information omit-ted since authors assume a certain amount ofbackground knowledge.
We describe the kindof information (the formalism and methods toderive the content) useful for automated fill-ing of such gaps.
We describe a stepwise pro-cedure with a detailed example.1 IntroductionAutomated understanding of connected text re-mains an unsolved challenge in NLP.
In contrastto systems that harvest information from large col-lections of text, or that extract only certain pre-specified kinds of information from single texts,the task of extracting and integrating all informa-tion from a single text, and building a coherent andrelatively complete representation of its full con-tent, is still beyond current capabilities.A significant obstacle is the fact that text alwaysomits information that is important, but that peoplerecover effortlessly.
Authors leave out informationthat they assume is known to their readers, since itsinclusion (under the Gricean maxim of minimality)would carry an additional, often pragmatic, import.The problem is that systems cannot perform therecovery since they lack the requisite backgroundknowledge and inferential machinery to use it.In this research we address the problem ofautomatically recovering such omitted informationto ?plug the gaps?
in text.
To do so, we describethe background knowledge required as well as aprocedure for recognizing where gaps exist anddetermining which kinds of background knowl-edge are needed.We are looking for the synchronization betweenthe text representation achievable by current NLPand a knowledge representation (KR) scheme thatcan permit further inference for text interpretation.1.1 VisionClearly, producing a rich text interpretation re-quires both NLP and KR capabilities.
The strategywe explore is the enablement of bidirectionalcommunication between the two sides from thevery beginning of the text processing.
We assumethat the KR system doesn?t require a full represen-tation of the text meaning, but can work with a par-tial interpretation, namely of the material explicitlypresent in the text, and can then flesh out this in-terpretation as required for its specific task.
Al-though the NLP system initially provides simplerrepresentations (even possibly ambiguous orwrong ones), the final result contains the semanticsof the text according to the working domain.In this model, the following questions arise:How much can we simplify our initial text repre-sentation and still permit the attachment of back-ground knowledge for further inference andinterpretation?
How should background knowl-edge be represented for use by the KR system?How can the incompleteness and brittleness typicalof background knowledge (its representational in-flexibility, or limitation to a single viewpoint orexpressive phrasing) (Barker 2007) be overcome?In what sequence can a KR system enrich an initialand/or impoverished reading, and how can the en-richment benefit subsequent text processing?1.2 ApproachAlthough we are working toward it, we do not yethave such a system.
The aim of our current workis to rapidly assemble some necessary pieces andexplore how to (i) attach background knowledge toflesh out a simple text representation and (ii) thereby make explicit the meanings attached to some ofits syntactic relations.
We begin with an initialsimple text representation, a background knowl-edge base corresponding to the text, and a simple15formalized procedure to attach elements from thebackground knowledge to the entities and implicitrelations present in the initial text representation.Surprisingly, we find that some quite simpleprocessing can be effective if we are able to con-textualize the text under interpretation.For our exploratory experiments, we are work-ing with a collection of 30,000 documents in thedomain of US football.
We parsed the collectionusing a standard dependency parser (Marneffe andManning, 2008; Klein and Maning, 2003) and, af-ter collapsing some syntactic dependencies, ob-tained the simple textual representations shown inSection 2.
From them, we built a BackgroundKnowledge Base by automatically harvestingpropositions expressed in the collection (Section3).
Their frequency in the collection lead the en-richment process: given a new text in the samedomain, we build exactly the same kind of repre-sentation, and attach the background knowledgepropositions as related to the text (Section 4).Since this is an exploratory sketch, we cannotprovide a quantitative evaluation yet, but the quali-tative study over some examples suggest that thissimple framework is promising enough to start along term research (Section 5).
Finally, we con-clude with the next steps we want to follow and thekind of evaluation we plan to do.2 Text RepresentationThe starting text representation must capture thefirst shot of what?s going on in the text, takingsome excerpts into account and (unfortunately)losing others.
After the first shot, in accord withthe purpose of the reading, we will ?contextualize?each sentence, expanding its initial representationwith the relevant related background knowledge inour base.During this process of making explicit the im-plicit semantic relations (which we call contextu-alization or interpretation) it will become apparentwhether we need to recover some of the discardedelements, whether we need to expand some others,etc.
So the process of interpretation is identifiedwith the growing of the context (according to theKB) until the interpretation is possible.
This is re-lated to some well-known theories such as theTheory of Relevance (Sperber and Wilson, 1995).The particular method we envisage is related toInterpretation as Abduction (Hobbs et al 1993).How can the initial information be representedso as to enable the context to grow into an interpre-tation?
We hypothesize that:1.
Behind certain syntactic dependencies thereare semantic relations.2.
In the case of dependencies between nouns,this semantic relation can be made more ex-plicit using verbs and/or prepositions.
Theknowledge base must help us find them.We look for a semantic representation closeenough to the syntactic representation we can ob-tain from the dependency graph.
The main syntac-tic dependencies we want to represent in order toenable enrichment are:1.
Dependencies between nouns such as noun-noun compounds (nn) or possessive (poss).2.
Dependencies between nouns and verbs,such as subject and object relations.3.
Prepositions having two nouns as argu-ments.
Then the preposition becomes the la-bel for the relation between the two nouns,being the object of the preposition the targetof the relation.For these selected elements, we produce two verysimple transformations of the syntactic dependencygraph:1.
Invert the direction of the syntactic depend-ency for the modifiers.
Since we work withthe hypothesis that behind a syntactic de-pendency there is a semantic relation, we re-cord the direction of the semantic relation.2.
Collapse the syntactic dependencies be-tween verb, subject, and object into a singlesemantic relation.
Since we are assumingthat the verb is the more explicit expressionof a semantic relation, we fix this in the ini-tial representation.
The subject will be thesource of the relation and the object will bethe target of the relation.
When the verb hasmore arguments we consider its expansionas a new node as referred in Section 4.4.Figure 1 shows the initial minimal representa-tion for the sentence we will use for our discus-sion:San_Francisco's Eric_Davis intercepteda Steve_Walsh pass on the next series toset_up a seven-yard Young touchdown passto Brent_Jones.Notice that some pieces of the text are lost in theinitial representation of the text as for example ?onthe next series?
or ?seven-yard?.163    Background Knowledge BaseThe Background Knowledge Base (BKB) is builtfrom a collection in the domain of the texts wewant to semanticize.
The collection consists of30,826 New York Times news about Americanfootball, similar to the kind of texts we want tointerpret.
The elements in the BKB (3,022,305 intotal) are obtained as a result of applying generalpatterns over dependency trees.
We take advantageof the typed dependencies (Marneffe and Manning,2008) produced by the Stanford parser (Klein andManing, 2003).3.1 Types of elements in the BKBWe distinguish three elements in our BackgroundKnowledge Base: Entities, Propositions, and Lexi-cal relations.
All of them have associated their fre-quency in the reference collection.EntitiesWe distinguish between entity classes and entityinstances:1.
Entity classes: Entity classes are denoted bythe nouns that participate in a copulative rela-tion or as noun modifier.
In addition, we intro-duce two special classes: Person and Group.These two classes are related to the use of pro-nouns in text.
Pronouns ?I?, ?he?
and ?she?
arelinked to class Person.
Pronouns ?we?
and?they?
are linked to class Group.
For example,the occurrence of the pronoun ?he?
in ?Hethrew a pass?
would produce an additionalcount of the proposition ?person:throw:pass?.2.
Entity Instances: Entity instances are indicatedby proper nouns.
Proper nouns are identifiedby the part of speech tagging.
Some of theseinstances will participate in the ?has-instance?relation (see below).
When they participate ina proposition they produce proposition in-stances.Figure 1.
Representation of the sentence: San_Francisco's Eric_Davis intercepted a Steve_Walshpass on the next series to set_up a seven-yard Young touchdown pass to Brent_Jones.PropositionsFollowing Clark and Harrison (2009) we callpropositions the tuples of words that have somedetermined pattern of syntactic relations amongthem.
We focus on NVN, NVNPN and NPNproposition types.
For example, a NVNPN propo-sition is a full instantiation of:Subject:Verb:Object:Prep:ComplementThe first three elements are the subject, the verband the direct object.
Fourth is the preposition thatattaches the PP complement to the verb.
For sim-plicity, indirect objects are considered as a Com-plement with the preposition ?to?.The following are the most frequent NVNpropositions in the BKB ordered by frequency.NVN 2322 'NNP':'beat':'NNP'NVN 2231 'NNP':'catch':'pass'NVN 2093 'NNP':'throw':'pass'NVN 1799 'NNP':'score':'touchdown'NVN 1792 'NNP':'lead':'NNP'NVN 1571 'NNP':'play':'NNP'NVN 1534 'NNP':'win':'game'NVN 1355 'NNP':'coach':'NNP'NVN 1330 'NNP':'replace':'NNP'NVN 1322 'NNP':'kick':'goal'NVN 1195 'NNP':'win':'NNP'NVN 1155 'NNP':'defeat':'NNP'NVN 1103 'NNP':'gain':'yard'The ?NNP?
tag replaces specific proper nounsfound in the proposition.When a sentence has more than one comple-ment, a new occurrence is counted for each com-plement.
For example, given the sentence?Steve_Walsh threw a pass to Brent_Jonesin the first quarter?, we would add a count toeach of the following propositions:17Steve_Walsh:throw:passSteve_Walsh:throw:pass:to:Brent_JonesSteve_Walsh:throw:pass:in:quarterNotice that right now we include only the headsof the noun phrases in the propositions.We call proposition classes the propositions thatonly involve instance classes (e.g., ?per-son:throw:pass?
), and proposition instancesthose that involve at least one entity instance (e.g.,?Steve_Walsh:throw:pass?
).Proposition instances are useful for the trackingof a entity instance.
For example,?'Steve_Walsh':'supplant':'John_Fourcade':'as':'quarterback'?.
When a proposition in-stance is found, it is stored also as a propositionclass replacing the proper nouns by a special word(NNP) to indicate the presence of a entity instance.The enrichment of the text is based on the use ofmost frequent proposition classes.Lexical relationsAt the moment, we make use of the copulativeverbs (detected by the Stanford?s parser) in orderto extract ?is?, and ?has-instance?
relations:1.
Is: between two entity classes.
They denote akind of identity between both entity classes,but not in any specific hierarchical relationsuch as hyponymy.
Neither is a relation ofsynonymy.
As a result, is somehow a kind ofunderspecified relation that groups those morespecific.
For example, if we ask the BKB whata ?receiver?
is, the most frequent relations are:290 'person':is:'receiver'29 'player':is:'receiver'16 'pick':is:'receiver'15 'one':is:'receiver'14 'receiver':is:'target'8 'end':is:'receiver'7 'back':is:'receiver'6 'position':is:'receiver'The number indicates the number of times therelation appears explicitly in the collection.2.
Has-instance: between an entity class and anentity instance.
For example, if we ask for in-stances of team, the top 10 instances with moresupport in the collection are:192 'team':has-instance:'Jets'189 'team':has-instance:'Giants'43 'team':has-instance:'Eagles'40 'team':has-instance:'Bills'36 'team':has-instance:'Colts'35 'team':has-instance:'Miami'35 'team':has-instance:'Vikings'34 'team':has-instance:'Cowboys'32 'team':has-instance:'Patriots'31 'team':has-instance:'Dallas'But we can ask also for the possible classes ofan instance.
For example, all the entity classes for?Eric_Davis?
are:12 'cornerback':has-instance:'Eric_Davis'1 'hand':has-instance:'Eric_Davis'1 'back':has-instance:'Eric_Davis'There are other lexical relations as ?part-of?
and?is-value-of?
in which we are still working.
Forexample, the most frequent ?is-value-of?
relationsare:5178 '[0-9]-[0-9]':is-value-of:'lead'3996 '[0-9]-[0-9]':is-value-of:'record'2824 '[0-9]-[0-9]':is-value-of:'loss'1225 '[0-9]-[0-9]':is-value-of:'season'4 Enrichment procedureThe goal of the enrichment procedure is to deter-mine what kind of events and entities are involvedin the text, and what semantic relations are hiddenby some syntactic dependencies such as noun-nouncompound or some prepositions.4.1 Fusion of nodesSometimes, the syntactic dependency ties two ormore words that form a single concept.
This is thecase with multiword terms such as ?tight end?,?field goal?, ?running back?, etc.
In these cases,the meaning of the compound is beyond the syn-tactic dependency.
Thus, we shouldn?t look for itsexplicit meaning.
Instead, we activate the fusion ofthe nodes into a single one.However, there are some open issues related tothe cases were fusion is not preferred.
Otherwise,the process could be done with standard measureslike mutual information, before the parsing step(and possibly improving its results).The question is whether the fusion of the wordsinto a single expression allows or not the consid-eration of possible paraphrases.
For example, inthe case of ?field:nn:goal?, we don?t find otherways to express the concept in the BKB.
However,in the case of ?touchdown:nn:pass?
we can find,for example, ?pass:for:touchdown?
a significantamount of times, and we want to identify them asequivalent expressions.
For this reason, we find notconvenient to fuse these cases.184.2 Building context for instancesSuppose we wish to determine what kind of entity?Steve Walsh?
is in the context of the syntacticdependency ?Steve_Walsh:nn:pass?.
First, welook into the BKB for the possible entity classes ofSteve_Walsh previously found in the collection.
Inthis particular case, the most frequent class is?quarterback?
:40 'quarterback':has-instance:'Steve_Walsh'2 'junior':has-instance:'Steve_Walsh'But, what happens if we see ?Steve_Walsh?
forthe first time?
Then we need to find evidence fromother entities in the same syntactic context.
Wefound that ?Marino?, ?Kelly?, ?Elway?,?Dan_Marino?, etc.
appear in the same kind ofproposition (?N:nn:pass?)
where we found?Steve_Walsh?, each of them supported by 24, 17,15 and 10 occurrences respectively.
However,some of the names can be ambiguous.
For exam-ple, searching for ?Kelly?
in our BKB yields:153 'quarterback':has-instance:'Jim_Kelly'19 'linebacker':has-instance:'Joe_Kelly'17 'quarterback':has-instance:'Kelly'14 'quarterback':has-instance:'Kelly_Stouffer'10 'quarterback':has-instance:'Kelly_Ryan'8 'quarterback':has-instance:'Kelly_Holcomb'7 'cornerback':has-instance:'Brian_Kelly'Whereas others are not so ambiguous:113 'quarterback':has-instance:'Dan_Marino'6 'passer':has-instance:'Dan_Marino'5 'player':has-instance:'Dan_Marino'Taking this into account, we are able to infer thatthe most plausible class for an entity involved in a?NNP:nn:pass?
proposition is a quarterback.4.3 Building context for dependenciesNow we want to determine the meaning behindsuch syntactic dependencies as?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,?Young:nn:pass?
or ?pass:to:Brent_Jones?.We have two ways for adding more meaning tothese syntactic dependencies: find the most appro-priate prepositions to describe them, and find themost appropriate verbs.
Whether one, the other orboth is more useful has to be determined during thereasoning system development.Finding the prepositionsThere are several types of propositions in theBKB that involve prepositions.
The most relevantare NPN and NVNPN.
In the case of ?touch-down:nn:pass?, preposition ?for?
is clearly the bestinterpretation for the ?nn?
dependency:NPN 712 'pass':'for':'touchdown'NPN 24 'pass':'include':'touchdown'NPN 3 'pass':'with':'touchdown'NPN 2 'pass':'of':'touchdown'NPN 1 'pass':'in':'touchdown'NPN 1 'pass':'follow':'touchdown'NPN 1 'pass':'to':'touchdown'In the case of ?Steve_Walsh:nn:pass?
and?Young:nn:pass?, assuming they are quarterbacks,we can ask for all the prepositions between ?pass?and ?quarterback?
:NPN 23 'pass':'from':'quarterback'NPN 14 'pass':'by':'quarterback'NPN 2 'pass':'of':'quarterback'NPN 1 'pass':'than':'quarterback'NPN 1 'pass':'to':'quarterback'Notice how lower frequencies involve morenoisy options.If we don?t have any evidence on the instanceclass, and we know only that they are instances,the pertinent query to the BKB obtains:NPN 1305 'pass':'to':'NNP'NPN 1085 'pass':'from':'NNP'NPN 147 'pass':'by':'NNP'NPN 144 'pass':'for':'NNP'In the case of ?Young:nn:pass?
(in ?Youngpass to Brent Jones?
), there exists already thepreposition ?to?
(?pass:to:Brent_Jones?
), so themost promising choice become the second,?pass:from:Young?, which has one order of magni-tude more occurrences than the following.In the case of ?Steve_Walsh:nn:pass?
(in ?EricDavis intercepted a Steve Walsh pass?)
we can useadditional information: we know that?Eric_Davis:intercept:pass?.
So, we can try tofind the appropriate preposition using NVNPNpropositions in the following way:Eric_Davis:intercept:pass:P:Steve_Walsh?Asking the BKB about the propositions that in-volve two instances with ?intercept?
and ?pass?
weget:NVNPN 48 'NNP':'intercept':'pass':'by':'NNP'NVNPN 26 'NNP':'intercept':'pass':'at':'NNP'NVNPN 12 'NNP':'intercept':'pass':'from':'NNP'We could also query the BKB with the classeswe already found for ?Eric_Davis?
(cornerback,player, person):NVNPN 11 'person':'intercept':'pass':'by':'NNP'NVNPN 4 'person':'intercept':'pass':'at':'NNP'NVNPN 2 'person':'intercept':'pass':'in':'NNP'19NVNPN 2 'person':'intercept':'pass':'against':'NNP'NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP'All these queries accumulate evidence over a cor-rect preposition ?by?
(?pass:by:Steve_Walsh?
).However, an explicit entity classification wouldmake the procedure more robust.Finding the verbsNow the exercise is to find a verb able to givemeaning to the syntactic dependencies such as?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,?Young:nn:pass?
or ?pass:to:Brent_Jones?.We can ask the BKB what instances (NNP) dowith passes.
The most frequent propositions are:NVN 2241 'NNP':'catch':'pass'NVN 2106 'NNP':'throw':'pass'NVN 844 'NNP':'complete':'pass'NVN 434 'NNP':'intercept':'pass'NVNPN 758 'NNP':'throw':'pass':'to':'NNP'NVNPN 562 'NNP':'catch':'pass':'for':'yard'NVNPN 338 'NNP':'complete':'pass':'to':'NNP'NVNPN 255 'NNP':'catch':'pass':'from':'NNP'Considering the evidence of ?Brent_Jones?
be-ing instance of ?end?
(tight end), if we ask theBKB about the most frequent relations between?end?
and ?pass?
we find:NVN 28 'end':'catch':'pass'NVN 6 'end':'drop':'pass'So, in this case, the BKB suggests that the syn-tactic dependency ?pass:to:Brent_Jones?
means?Brent_Jones is an end catching a pass?.
Or inother words, that ?Brent_Jones?
has a role of?catch-ER?
with respect to ?pass?.If we want to accumulate more evidence on thiswe can consider NVNPN propositions includingtouchdown.
We only find evidence for the mostgeneral classes (NNP and person):NVNPN 189 'NNP':'catch':'pass':'for':'touchdown'NVNPN 26 'NNP':'complete':'pass':'for':'touchdown'NVNPN 84 'person':'catch':'pass':'for':'touchdown'NVNPN 18 'person':'complete':'pass':'for':'touchdown'This means, that when we have ?touchdown?,we don?t have counting for the second option?Brent_Jones:drop:pass?, while ?catch?
becomesstronger.In the case of ?Steve_Walsh:nn:pass?
we hy-pothesize that ?Steve_Walsh?
is a quarterback.Asking the BKB about the most plausible relationbetween a quarterback and a pass we find: Figure 2.
Graphical representation of the enrichedtext.20NVN 98 'quarterback':'throw':'pass'NVN 27 'quarterback':'complete':'pass'Again, if we take into account that it is a?touchdown:nn:pass?, then only the second op-tion ?Steve_Walsh:complete:pass?
is consistentwith the NVNPN propositions.So, in this case, the BKB suggests that the syn-tactic dependency ?Steve_Walsh:nn:pass?
means?Steve_Walsh is a quarterback completing a pass?.Finally, with respect to ?touchdown:nn:pass?,we can ask about the verbs that relate them:NVN 14 'pass':'set_up':'touchdown'NVN 6 'pass':'score':'touchdown'NVN 5 'pass':'produce':'touchdown'Figure 2 shows the graphical representation ofthe sentence after some enrichment.4.4 Expansion of relationsSometimes, the sentence shows a verb with severalarguments.
In our example, we have?Eric_David:intercept:pass:on:series?.
Inthese cases, the relation can be expanded and be-come a node.In our example, the new node is the eventualityof ?intercept?
(let?s say ?intercept-ION?),?Eric_Davis?
is the ?intercept-ER?
and ?pass?
isthe ?intercept-ED?.
Then, we can attach the miss-ing information to the new node (see Figure 3).Figure 3.
Expansion of the "intercept" relation.In addition, we can proceed with the expansionof the context considering this new node.
For ex-ample, we are working with the hypothesis that?Steve_Walsh?
is an instance of quarterback andthus, its most plausible relations with pass are?throw?
and ?complete?.
However, now we canask about the most frequent relation between?quarterback?
and ?interception?.
The most fre-quent is ?quarterback:throw:interception?supported 35 times in the collection.
From this,two actions can be done: reinforce the hypothesisof ?throw:pass?
instead of ?complete:pass?, andadd the hypothesis that?Steve_Walsh:throw:interception?.Finally, notice that since ?set_up?
doesn?t needto accommodate more arguments, we can maintainthe collapsed edge.4.5 Constraining the interpretationsSome of the inferences being performed are localin the sense that they involve only an entity and arelation.
However, these local inferences must becoherent both with the sentence and the completedocument.To ensure this coherence we can use additionalinformation as a way to constrain different hy-potheses.
In section 4.3 we showed the use ofNVNPN propositions to constrain NVN ones.Another example is the case of?Eric_Davis:intercept:pass?.
We can ask theBKB for the entity classes that participate in suchkind of proposition:NVN 75 'person':'intercept':'pass'NVN 14 'cornerback':'intercept':'pass'NVN 11 'defense':'intercept':'pass'NVN 8 'safety':'intercept':'pass'NVN 7 'group':'intercept':'pass'NVN 5 'linebacker':'intercept':'pass'So the local inference for the kind of entity?Eric_Davis?
is (cornerback) must be coherentwith the fact that it intercepted a pass.
In this case?cornerback?
and ?person?
are properly reinforced.In some sense, we are using these additional con-strains as shallow selectional preferences.5 EvaluationThe evaluation of the enrichment process is a chal-lenge by itself.
Eventually, we will use extrinsicmeasures such as system performance on a QAtask, applied first after reading a text, and then asecond time after the enrichment process.
This willmeasure the ability of the system to absorb and useknowledge across texts to enrich the interpretationof the target text.
In the near term, however, it re-mains unclear which intrinsic evaluation measuresto apply.
It is not informative simply to count thenumber of additional relations one can attach torepresentation elements, or to count the increase indegree of interlinking of the nodes in the represen-tation of a paragraph.216 Related WorkTo build the knowledge base we take an approachclosely related to DART (Clark and Harrison,2009) which in turn is related to KNEXT (VanDurme and Schubert, 2008).
It is also more dis-tantly related to TextRunner (Banko et al 2007).Like DART, we make use of a dependencyparser instead of partial parsing.
So we capturephrase heads instead complete phrases.
The maindifferences between the generation of our BKBand the generation of DART are:1.
We use the dependencies involving copula-tive verbs as a source of evidence for ?is?and ?has-instance?
relations.2.
Instead of replacing proper nouns by ?per-son?, ?place?, or ?organization?, we con-sider all of them just as instances in ourBKB.
Furthermore, when a proposition con-tains a proper noun, we count it twice: oneas the original proposition instance, and asecond replacing the proper nouns with ageneric tag indicating that there was a name.3.
We make use of the modifiers that involvean instance (proper noun) to add counting tothe ?has-instance?
relation.4.
Instead of replacing pronouns by ?person?or ?thing?, we replace them by ?person?,?group?
or ?thing?, taking advantage of thepreposition number.
This is particular usefulfor the domain of football where players andteams are central.5.
We add a new set of propositions that relatetwo clauses in the same sentence (e.g.,Floyd:break:takle:add:touchdown).
Wetagged these propositions NVV, NVNV,NVVN and NVNVN.6.
Instead of an unrestricted domain collection,we consider documents closely related to thedomain in which we want to interpret texts.The consideration of a specific domain collec-tion seems a very powerful option.
Ambiguity isreduced inside a domain so the counting for propo-sitions is more robust.
Also frequency distributionof propositions is different from one domain intoanother.
For example, the list of the most frequentNVN propositions in our BKB (see Section 3.1) is,by itself, an indication of the most salient and im-portant events in the American football domain.7 Conclusion and Future WorkThe task of inferring omitted but necessary infor-mation is a significant part of automated text inter-pretation.
In this paper we show that even simplekinds of information, gleaned relatively straight-forwardly from a parsed corpus, can be quite use-ful.
Though they are still lexical and not evenstarting to be semantic, propositions consisting ofverbs as relations between nouns seem to provide asurprising amount of utility.
It remains a researchproblem to determine what kinds and levels ofknowledge are most useful in the long run.In the paper, we discuss only the propositionsthat are grounded in instantial statements aboutplayers and events.
But for true learning by read-ing, a system has to be able to recognize when theinput expresses general rules, and to formulatesuch input as axioms or inferences.
In addition,augmenting that is the significant challenge ofgeneralizing certain kinds of instantial propositionsto produce inferences.
At which point, for exam-ple, should the system decide that ?all footballplayers have teams?, and how should it do so?How to do so remains a topic for future work.A further topic of investigation is the time atwhich expansion should occur.
Doing so at ques-tion time, in the manner of traditional task-orientedback-chaining inference, is the obvious choice, butsome limited amount of forward chaining at read-ing time seems appropriate too, especially if it cansignificantly assist with text processing tasks, inthe manner of expectation-driven understanding.Finally, as discussed above, the evaluation ofour reading augmentation procedures remains to bedeveloped.AcknowledgmentsWe are grateful to Hans Chalupsky and DavidFarwell for their comments and input along thiswork.
This work has been partially supported bythe Spanish Government through the "ProgramaNacional de Movilidad de Recursos Humanos delPlan Nacional de I+D+i 2008-2011 (GrantPR2009-0020).
We acknowledge the support ofDARPA contract number: FA8750-09-C-0172.22References1.
Banko, M., Cafarella, M., Soderland, S.,Broadhead, M., Etzioni, O.
2007.
Open Infor-mation Extraction from the Web.
IJCAI 2007.2.
Barker, K. 2007.
Building Models by ReadingTexts.
Invited talk at the AAAI 2007 SpringSymposium on Machine Reading, StanfordUniversity.3.
Clark, P. and Harrison, P. 2009.
Large-scaleextraction and use of knowledge from text.The Fifth International Conference on Knowl-edge Capture (K-CAP 2009).http://www.cs.utexas.edu/users/pclark/dart/4.
Hobbs, J.R., Stickel, M., Appelt, D. and Mar-tin, P., 1993.
Interpretation as Abduction.
Arti-ficial Intelligence, Vol.
63, Nos.
1-2, pp.
69-142.http://www.isi.edu/~hobbs/interp-abduct-ai.pdf5.
Klein, D. and Manning, C.D.
2003.
AccurateUnlexicalized Parsing.
Proceedings of the 41stMeeting of the Association for ComputationalLinguistics, pp.
423-4306.
Marneffe, M. and Manning, C.D.
2008.
TheStanford typed dependencies representation.
InCOLING 2008 Workshop on Cross-frameworkand Cross-domain Parser Evaluation.7.
Sperber, D. and Wilson, D. 1995.
Relevance:Communication and cognition (2nd ed.)
Ox-ford, Blackwell.8.
Van Durme, B., Schubert, L. 2008.
OpenKnowledge Extraction through CompositionalLanguage Processing.
Symposium on Seman-tics in Systems for Text Processing, STEP2008.23
