A model of syntactic disambiguation based on lexicalized grammarsYusuke MiyaoDepartment of Computer Science,University of Tokyoyusuke@is.s.u-tokyo.ac.jpJun?ichi TsujiiDepartment of Computer Science,University of TokyoCREST, JST(Japan Science and Technology Corporation)tsujii@is.s.u-tokyo.ac.jpAbstractThis paper presents a new approach to syntac-tic disambiguation based on lexicalized gram-mars.
While existing disambiguation mod-els decompose the probability of parsing re-sults into that of primitive dependencies of twowords, our model selects the most probableparsing result from a set of candidates allowedby a lexicalized grammar.
Since parsing re-sults given by the lexicalized grammar cannotbe decomposed into independent sub-events,we apply a maximum entropy model for fea-ture forests, which allows probabilistic model-ing without the independence assumption.
Ourapproach provides a general method of produc-ing a consistent probabilistic model of parsingresults given by lexicalized grammars.1 IntroductionRecent studies on the automatic extraction of lexicalizedgrammars (Xia, 1999; Chen and Vijay-Shanker, 2000;Hockenmaier and Steedman, 2002a) allow the modelingof syntactic disambiguation based on linguistically moti-vated grammar theories including LTAG (Chiang, 2000)and CCG (Clark et al, 2002; Hockenmaier and Steed-man, 2002b).
However, existing models of disambigua-tion with lexicalized grammars are a mere extension oflexicalized probabilistic context-free grammars (LPCFG)(Collins, 1996; Collins, 1997; Charniak, 1997), whichare based on the decomposition of parsing results into thesyntactic/semantic dependencies of two words in a sen-tence under the assumption of independence of the de-pendencies.
While LPCFG models have proved that theincorporation of lexical associations (i.e., dependenciesof words) significantly improves the accuracy of parsing,this idea has been naively inherited in the recent studieson disambiguation models of lexicalized grammars.However, the disambiguation models of lexicalizedgrammars should be totally different from that of LPCFG,because the grammars define the relation of syntax andsemantics, and can restrict the possible structure of pars-ing results.
Parsing results cannot simply be decomposedinto primitive dependencies, because the complete struc-ture is determined by solving the syntactic constraintsof a complete sentence.
For example, when we applya unification-based grammar, LPCFG-like modeling re-sults in an inconsistent probability model because themodel assigns probabilities to parsing results not allowedby the grammar (Abney, 1997).
We have only two waysof adhering to LPCFG models: preserve the consistencyof probability models by abandoning improvements tothe lexicalized grammars using complex constraints (Chi-ang, 2000), or ignore the inconsistency in probabilitymodels (Clark et al, 2002).This paper provides a new model of syntactic disam-biguation in which lexicalized grammars can restrict thepossible structures of parsing results.
Our modeling aimsat providing grounds for i) producing a consistent proba-bilistic model of lexicalized grammars, as well as ii) eval-uating the contributions of syntactic and semantic prefer-ences to syntactic disambiguation.
The model is com-posed of the syntax and semantics probabilities, whichrepresent syntactic and semantic preferences respectively.The syntax probability is responsible for determining thesyntactic categories chosen by words in a sentence, andthe semantics probability selects the most plausible de-pendencies of words from candidates allowed by the syn-tactic categories yielded by the syntax probability.
Sincethe sequence of syntactic categories restricts the possi-ble structure of parsing results, the semantics probabil-ity is a conditional probability without decompositioninto the primitive dependencies of words.
Recently usedmachine learning methods including maximum entropymodels (Berger et al, 1996) and support vector machines(Vapnik, 1995) provide grounds for this type of model-ing, because it allows various dependent features to beincorporated into the model without the independence as-sumption.The above approach, however, has a serious deficiency:a lexicalized grammar assigns exponentially many pars-ing results because of local ambiguities in a sentence,which is problematic in estimating the parameters of aprobability model.
To cope with this, we adopted analgorithm of maximum entropy estimation for featureforests (Miyao and Tsujii, 2002; Geman and Johnson,2002), which allows parameters to be efficiently esti-mated.
The algorithm enables probabilistic modelingof complete structures, such as transition sequences inMarkov models and parse trees, without dividing theminto independent sub-events.
The algorithm avoids expo-nential explosion by representing a probabilistic event bya packed representation of a feature space.
If a completestructure is represented with a feature forest of a tractablesize, the parameters can be efficiently estimated by dy-namic programming.A series of studies on parsing with wide-coverage LFG(Johnson et al, 1999; Riezler et al, 2000; Riezler et al,2002) have had a similar motivation to ours.
Their mod-els have also been based on a discriminative model toselect a parsing result from all candidates given by thegrammar.
A significant difference is that we apply max-imum entropy estimation for feature forests to avoid theinherent problem with estimation: the exponential explo-sion of parsing results given by the grammar.
They as-sumed that parsing results would be suppressed to a rea-sonable number through using heuristic rules, or by care-fully implementing a fully restrictive and wide-coveragegrammar, which requires a considerable amount of effortto develop.
Our contention is that this problem can besolved in a more sophisticated way as is discussed in thispaper.
Another difference is that our model is separatedinto syntax and semantics probabilities, which will ben-efit computational/linguistic investigations into the rela-tion between syntax and semantics, and allow separateimprovements to both models.Overall, the approach taken in this paper is differentfrom existing models in the following respects.?
Since it does not require the assumption of inde-pendence, the probability model is consistent withlexicalized grammars with complex constraints in-cluding unification-based grammar formalism.
Ourmodel can assign consistent probabilities to parsingresults of lexicalized grammars, while the traditionalmodels assign probabilities to parsing results not al-lowed by the grammar.?
Since the syntax and semantics probabilities are sep-arate, we can improve them individually.
For exam-ple, the syntax model can be improved by smooth-ing using the syntactic classes of words, while thesemantics model should be able to be improved byusing semantic classes.
In addition, the model canbe a starting point that allows the theory of syntaxand semantics to be evaluated through consulting anextensive corpus.We evaluated the validity of our model through experi-ments on a disambiguation task of parsing the Penn Tree-bank (Marcus et al, 1994) with an automatically acquiredLTAG grammar.
To assess the contribution of the syntaxand semantics probabilities to the accuracy of parsing andto evaluate the validity of applying maximum entropy es-timation for feature forests, we compared three modelstrained with the same training set and the same set of fea-tures.
Following the experimental results, we concludedthat i) a parser with the syntax probability only achievedhigh accuracy with the lexicalized grammar, ii) the in-corporation of preferences for lexical association throughthe semantics probability resulted in significant improve-ments, and iii) our model recorded an accuracy that wasquite close to the traditional model, which indicated thevalidity of applying maximum entropy estimation for fea-ture forests.In what follows, we first describe the existing modelsfor syntactic disambiguation, and discuss problems withthem in Section 2.
We then define the general form forparsing results of lexicalized grammars, and introduceour model in Section 3.
We prove the validity of our ap-proach through a series of experiments in Section 4.2 Traditional models for syntacticdisambiguationThis section reviews the existing models for syntactic dis-ambiguation from the viewpoint of representing parsingresults of lexicalized grammars.
In particular, we dis-cuss how the models incorporate syntactic/semantic pref-erences for syntactic disambiguation.
The existing stud-ies are based on the decomposition of parsing results intoprimitive lexical dependencies where syntactic/semanticpreferences are combined.
This traditional scheme ofsyntactic disambiguation can be problematic with lexi-calized grammars.
Throughout the discussion, we referto the example sentence ?What does your student want towrite?
?, whose parse tree is in Figure 1.2.1 Lexicalized parse treesThe first successful work on syntactic disambiguationwas based on lexicalized probabilistic context-free gram-mar (LPCFG) (Collins, 1997; Charniak, 1997).
AlthoughLPCFG is not exactly classified into lexicalized grammarformalism, we should mention these studies since theydemonstrated that lexical dependencies were essential toimproving the accuracy of parsing.whatdoesyour wantto writeSSSVPVPNPstudentFigure 1: A parse tree for ?What does your student wantto write?
?whatdoesyour wantto writeSSSVPVPNPstudent writewantwantwantwantstudentFigure 2: A lexicalized parse treeA lexicalized parse tree is an extension of a parse treethat is achieved by augmenting each non-terminal with itslexical head.
There is an example of a lexicalized parsetree in Figure 2, which is a lexicalized version of the onein Figure 1.
A lexicalized parse tree is represented bya set of branchings in the tree1: T = {?whi, wni, ri?
},where whiis a head word, wnithe head word of anon-head, and ria grammar rule corresponding to eachbranching.
LPCFG models yield a probability of thecomplete parse tree T = {?whi, wni, ri?}
by the prod-uct of probabilities of branchings in it.p(T ) =?ip(whi, wni, ri|?
),where ?
is a condition of the probability, which is usuallythe nonterminal symbol of the mother node.
Since eachbranching is augmented with the lexical heads of non-terminals in the rule, the model can capture lexical de-pendencies, which increase the accuracy.
This is becauselexical dependencies approximately represent the seman-tic preference of a sentence.
As is well known, a syntacticstructure is not accurately disambiguated only with syn-tactic preferences, and the incorporation of approximate1For simplicity, we have assumed parse trees are only com-posed of binary branchings.semantic preferences was the key to improving the accu-racy of syntactic disambiguation.We should note that this model has the following threedisadvantages.1.
The model fails to represent some linguistic depen-dencies, including long-distance dependencies andargument/modifier distinctions.
Since an existingstudy incorporates these relations ad hoc (Collins,1997), they are apparently crucial in accurate dis-ambiguation.
This is also problematic for providinga sufficient representation of semantics.2.
The model assumes the statistical independence ofbranchings, which is apparently not preserved.
Forexample, the ambiguity of PP-attachments should beresolved by considering three words: the modifiee ofthe PP, its preposition, and the object of the PP.3.
The preferences of syntax and semantics are com-bined in the lexical dependencies of two words,i.e., features for syntactic preference and those forsemantic preference are not distinguished in themodel.
Lexicalized grammars formalize the con-straints of the relations between syntax and seman-tics, but the model does not assume the existenceof such constraints.
The model prevents further im-provements to the syntax/semantics models; in addi-tion to the linguistic analysis of the relation betweensyntax and semantics.2.2 Derivation treesRecent work on the automatic extraction of LTAG (Xia,1999; Chen and Vijay-Shanker, 2000) and disambigua-tion models (Chiang, 2000) has been the first on the sta-tistical model for syntactic disambiguation based on lexi-calized grammars.
However, the models are based on thelexical dependencies of elementary trees, which is a sim-ple extension of the LPCFG.
That is, the models are stillbased on decomposition into primitive lexical dependen-cies.Derivation trees, the structural description in LTAG(Schabes et al, 1988), represent the association of lex-ical items i.e., elementary trees.
In LTAG, all syntacticconstraints of words are described in an elementary tree,and the dependencies of elementary trees, i.e., a deriva-tion tree, describe the semantic relations of words moredirectly than lexicalized parse trees.
For example, Fig-ure 3 has a derivation tree corresponding to the parsetree in Figure 12.
The dotted lines represent substitu-tion while the solid lines represent adjunction.
We shouldnote that the relations captured by ad-hoc augmentation2The nodes in a derivation tree are denoted with the namesof the elementary trees, while we have omitted details.what does student want towriteyourFigure 3: A derivation treeof lexicalized parse trees, such as the distinction of argu-ments/modifiers and unbounded dependencies (Collins,1997), are elegantly represented in derivation trees.
For-mally, a derivation tree is represented as a set of depen-dencies: D = {?
?i, ?
?j, ri?
}, where ?iis an elemen-tary tree, ?
?irepresents a node in ?jwhere substitu-tion/adjunction has occurred, and riis a label of the ap-plied rule, i.e., adjunction or substitution.A probability of derivation tree D = {?
?i, ?
?j, ri?}
isgenerally defined as follows (Schabes et al, 1988; Chi-ang, 2000).p(D) =?ip(?i|?
?j, ri)Note that each probability on the right represents the syn-tactic/semantic preference of a dependency of two lexicalitems.
We can readily see that the model is very similarto LPCFG models.The first problem with LPCFG is partially solvedby this model, since the dependencies not representedin LPCFG (e.g., long-distance dependencies and ar-gument/modifier distinctions) are elegantly represented,while some relations (e.g., the control relation between?want?
and ?student?)
are not yet represented.
However,the other two problems remain unsolved in this model.In particular, when we apply Feature-Based LTAG (FB-LTAG), the above probability is no longer consistent be-cause of the non-local constraints caused by feature uni-fication (Abney, 1997).2.3 Dependency structuresA disambiguation model for wide-coverage CCG (Clarket al, 2002) aims at representing deep linguistic depen-dencies including long-distance dependencies and con-trol relations.
This model can represent all the syntac-tic/semantic dependencies of words in a sentence.
How-ever, the statistical model is still a mere extension ofLPCFG, i.e., it is based on decomposition into primitivelexical dependencies.In this model, a lexicalized grammar defines the map-ping from a sentence into dependency structures, whichrepresent all the necessary dependencies of words in asentence, including long-distance dependencies and con-trol relations.
There is an example in Figure 4, whichwhat doesstudent want towriteyourARG1ARG2ARG1MODIFYMODIFYFigure 4: A dependency structurecorresponds to the parse tree in Figure 1.
Note that thisrepresentation includes a dependency not represented inthe derivation tree (the control relation between ?want?and ?student?).
A dependency structure is formally de-fined as a set of dependencies: S = {?whi, wni, ?i?
},where whiand wniare a head and argument word of thedependency, and ?iis an argument position of the headword filled by the argument word.An existing model assigns a probability value to de-pendency structure S = {?whi, wni, ?i?}
as follows.p =?ip(wni|whi, ?i)Primitive probability is approximated by the relative fre-quency of lexical dependencies of two words in a trainingcorpus.Since dependency structures include all necessary de-pendency relations, the first problem with LPCFG is nowcompletely solved.
However, the third problem still re-mains unsolved.
The probability of a complete parse treeis defined as the product of probabilities of primitive de-pendencies of two words.
In addition, the second prob-lem is getting worse; the independence assumption is ap-parently violated in this model, since the possible depen-dency structures are restricted by the grammar.
The prob-ability model is no longer consistent.3 Probability Model based on LexicalizedGrammarsThis section introduces our model of syntactic disam-biguation, which is based on the decomposition of theparsing model into the syntax and semantics models.
Theconcept behind it is that the plausibility of a parsing re-sult is determined by i) the plausibility of syntax, and ii)selecting the most probable semantics from the structuresallowed by the given syntax.
This section formalizes thegeneral form of statistical models for disambiguation ofparsing including lexicalized parse trees, derivation trees,and dependency structures.
Problems with the existingmodels are then discussed, and our model is introduced.Suppose that a set W of words and a set C of syn-tactic categories (e.g., nonterminal symbols of CFG, ele-mentary trees of LTAG, feature structures of HPSG (Sagand Wasow, 1999)) are given.
A lexicalized grammar isLexicalized parse tree?write, what, S?
write S?,?write, does, S?
does S?,?write, student, S?
NP VP?,?student, your, NP?
your student?,?write, want, VP?
want VP?,?write, to, VP?
to write?Derivation tree?write, what, SUBST?,?write, does, ADJ?,?write, student, SUBST?,?student, your, ADJ?,?write, want, ADJ?,?write, to, ADJ?Dependency structure?write, what, ARG2?,?write, does, MODIFY?,?write, student, ARG1?,?student, your, MODIFY?,?write, want, MODIFY?,?want, student, ARG1?,?write, to, MODIFY?Figure 5: Parsing results of lexicalized grammarsthen defined as a tuple G = ?L, R?, where L = {l =?w, c?|w ?
W , c ?
C} is a lexicon and R is a set ofgrammar rules.
A parsing result of lexicalized gram-mars is defined as a labeled graph structure A = {a|a =?lh, ln, d?
}, where a is an edge representing the depen-dency of head lhand argument lnlabeled with d. Forexample, the lexicalized parse tree in Figure 2 is repre-sented in this form as in Figure 5, as well as the derivationtree and the dependency structure.Given the above definition, the existing models dis-cussed in Section 2 yield a probability P (A|w) for givensentence w as in the following general form.P (A|w) =?a?Ap(a|?
),In short, the probability of the complete structure is de-fined as the product of probabilities of lexical depen-dencies.
For example, p(a|?)
corresponds to the prob-ability of branchings in LPCFG models, that of substi-tution/adjunction in derivation tree models, and that ofprimitive dependencies in dependency structure models.The models, however, have a crucial weakness withlexicalized grammar formalism; probability values areassigned to parsing results not allowed by the grammar,i.e., the model is no longer consistent.
Hence, the disam-biguation model of lexicalized grammars should not bedecomposed into primitive lexical dependencies.A possible solution to this problem is to directly es-timate p(A|w) by applying a maximum entropy model(Berger et al, 1996).
However, such modeling will leadus to extensive tweaking of features that is theoreticallyunjustifiable, and will not contribute to the theoreticalinvestigation of the relations of syntax and semantics.Since lexicalized grammars express all syntactic con-straints by syntactic categories of words, we have as-sumed that we first determine which syntactic category cshould be chosen, and then determine which argument re-lations are likely to appear under the constraints imposedby the syntactic categories.
Formally,p(A|w) = p(c|w)p(A|c).The first probability in the above formula is the prob-ability of syntactic categories, i.e., the probability of se-lecting a sequence of syntactic categories in a sentence.Since syntactic categories in lexicalized grammars deter-mine the syntactic constraints of words, this expresses thesyntactic preference of each word in a sentence.
Note thatour objective is not only to improve parsing accuracy butalso to investigate the relation between syntax and seman-tics.
We have not adopted the local contexts of words asin the supertaggers in LTAG (Joshi and Srinivas, 1994)because they partially include the semantic preferencesof a sentence.
The probability is purely unigram to se-lect the probable syntactic category for each word.
Theprobability is then given by the product of probabilitiesto select a syntactic category for each word from a set ofcandidate categories allowed by the lexicon.p(c|w) =?ip(ci|wi)The second describes the probability of semantics,which expresses the semantic preferences of relating thewords in a sentence.
Note that the semantics probabil-ity is dependent on the syntactic categories determinedby the syntax probability, because in lexicalized grammarformalism, a series of syntactic categories determines thepossible structures of parsing results.
Parsing results areobtained by solving the constraints given by the grammar.Hence, we cannot simply decompose semantics probabil-ity into the dependency probabilities of two words.
Wedefine semantics probability as a discriminative modelthat selects the most probable parsing result from a setof candidates given by parsing.Since semantics probability cannot be decomposedinto independent sub-events, we applied a maximum en-tropy model, which allowed probabilistic modeling with-out the independence assumption.
Using this model, wecan assign consistent probabilities to parsing results withcomplex structures, such as ones represented with featurestructures (Abney, 1997; Johnson et al, 1999).
Givenparsing result A, semantics probability is defined as fol-lows:p(A|c) = 1Zcexp???s?S(A)?(s)??Zc=?A??A(c)exp???s??S(A?)?(s?)??
,where S(A) is a set of connected subgraphs of A, ?
(s)is a weight of subgraph s, and A(c) is a set of parsingresults allowed by the sequence of syntactic categories c.Since we aim at separating syntactic and semantic pref-erences, feature functions for semantic probability distin-guish only words, not syntactic categories.
We shouldnote that subgraphs should not be limited to an edge, i.e.,the lexical dependency of two words.
By taking morethan one edge as a subgraph, we can represent the depen-dency of more than two words, although existing mod-els do not adopt such dependencies.
Various ambigui-ties should be resolved by considering the dependencyof more than two words; e.g.
PP-attachment ambiguityshould be resolved by the dependency of three words.Consequently, the probability model takes the follow-ing form.p(A|w) ={?ip(ci|wi)}???1Zcexp???s?S(A)?(s)????
?However, this model has a crucial flaw: the maxi-mum likelihood estimation of semantics probability isintractable.
This is because the estimation requires Zcto be computed, which requires summation over A(c),exponentially many parsing results.
To cope with thisproblem, we applied an efficient algorithm of maximumentropy estimation for feature forests (Miyao and Tsu-jii, 2002; Geman and Johnson, 2002).
This enabledthe tractable estimation of the above probability, whena set of candidates are represented in a feature forest of atractable size.Here, we should mention that the disadvantages of thetraditional models discussed in Section 2 have been com-pletely solved by this model.
It can be applied to anyparsing results given by a lexicalized grammar, does notrequire the independence assumption, and is defined as acombination of syntax and semantics probabilities, wherethe semantics probability is a discriminative model thatselects a parsing result from the set of candidates givenby the syntax probability.4 ExperimentsThe model proposed in Section 3 is generally applica-ble to any lexicalized grammars, and this section reportsthe evaluation of our model with a wide-coverage LTAGgrammar, which is automatically acquired from the PennTreebank (Marcus et al, 1994) Sections 02?21.
Thegrammar was acquired by an algorithm similar to (Xia,1999), and consisted of 2,105 elementary trees, where1,010 were initial trees and 1,095 were auxiliary ones.The coverage of the grammar against Section 22 (1,700sentences) was 92.6% (1,575 sentences) in a weak sense(i.e., the grammar could output a structure consistent withthe bracketing in the test corpus), and 68.0% (1,156 sen-tences) in a strong sense (i.e., the grammar could outputexactly the correct derivation).Since the grammar acquisition algorithm could outputderivation trees for the sentences in the training corpus(Section 02?21), we used them as a training set of theprobability model.
The model of syntax probability wasestimated with syntactic categories appearing in the train-ing set.
For estimating the semantics probability, a parserproduced all possible derivation trees for each sequenceof syntactic categories (corresponding to each sentence)in the training set, and the obtained derivation trees, i.e.,A(c), are passed to a maximum entropy estimator.
By ap-plying the grammar acquisition algorithm to Section 22,we obtained the derivation trees of the sentences in thissection, and from this set we prepared a test set by elim-inating non-sententials, long sentences (including morethan 40 words), sentences not covered by the grammar,and sentences that caused time-outs in parsing.
The re-sulting set consisted of 917 derivation trees.The following three disambiguation models were pre-pared using the training set.syntax Only composed of the syntax probability, i.e.,p(c|w)traditional Similar to our model, but semantics proba-bility p(A|c) was decomposed into the probabilitiesof the primitive dependencies of two words as in thetraditional modeling, i.e., this model is an inconsis-tent probability modelour model The model by maximum entropy estimationfor feature forestsThe syntax probability was a unigram model, and con-texts around the word such as previous words/categorieswere not used.
Hence, it includes only syntactic prefer-ences of words.
The semantics parts of traditional andour model were maximum entropy models, where ex-actly the same set of features were used, i.e., the differ-ence between the two models was only in an event repre-sentation: derivation trees were decomposed into primi-tive dependencies in traditional, while in our model theywere represented by a feature forest without decompo-sition.
Hence, we can evaluate the effects of applyingmaximum entropy estimation for feature forests by com-paring our model with traditional.
While our model al-lowed features to be incorporated that were not limitedto the dependencies of two words (Section 3), the modelsused throughout the experiments only included featuresof the dependencies of two words.
The semantics proba-bilities were developed with two sets of features includ-exact partialsyntax 73.4 77.3traditional 79.2 83.4our model 79.6 83.6Table 1: Accuracy of dependencies (1)exact partialsyntax 73.4 77.3traditional 79.6 83.6our model 78.9 82.8Table 2: Accuracy of dependencies (2)ing surface forms/POSs of words, the labels of dependen-cies (substitution/adjunction), and the distance betweentwo words.
The first feature set had 283,755 featuresand the other had 150,156 features excluding fine-grainedfeatures of the first set.
There were 701,819 events fortraditional, and 32,371 for our model.
The difference inthe number of events was caused by the difference in theunits of events, i.e., an event corresponded to a depen-dency in traditional, while it corresponded to a sentencein our model.The parameters of the models were estimated by thelimited-memory BFGS algorithm (Nocedal, 1980) witha Gaussian distribution as the prior probability distri-bution for smoothing (Chen and Rosenfeld, 1999) im-plemented in a maximum entropy estimator for featureforests (Miyao, 2002).
The estimation for traditional wasconverged in 67 iterations in 127 seconds, and our modelin 29 iterations in 111 seconds on a Pentium III 1.26-GHzCPU with 4 GB of memory.
These results reveal that theestimation with our model is comparatively efficient withtraditional.
The parsing algorithm was CKY-style pars-ing with beam thresholding, which was similar to onesused in (Collins, 1996; Clark et al, 2002).
Althoughwe needed to compute normalizing factor Zcto obtainprobability values, we used unnormalized products as thepreference score for beam thresholding, following (Clarket al, 2002).
We did not use any preprocessing such assupertagging (Joshi and Srinivas, 1994) and the parsersearched for the most plausible derivation tree from thederivation forest in terms of the probability given by thecombination of syntax and semantics probabilities.Tables 1 and 2 list the accuracy of dependencies, i.e.,edges in derivation trees, for each model with two setsof features for the semantics model3.
Since in derivationtrees each word in a sentence depends on one and onlyone word (see Figure 3), the accuracy is the number of3Since the features of the syntax part were not changed, theresults for syntax are exactly the same.correct edges divided by the number of all edges in thetree.
The exact column indicates the ratio of dependen-cies where the syntactic category, the argument position,and the dependee head word of the argument word arecorrectly output.
The partial column shows the ratio ofdependencies where the words are related regardless ofthe label.
We should note that the exact measure is a verystringent because the model must select the correct syn-tactic category from 2,105 categories.First, we can see that syntax achieved a high level ofaccuracy although it was not quite sufficient yet.
Wethink this was because the grammar could adequately re-strict the possible structure of parsing results, and the dis-ambiguation model tried to search for the most probablestructure from the candidates allowed by the grammar.Second, traditional and our model recorded significantlyhigher accuracy than syntax.
The accuracy of our modelwas almost matched traditional, which proved the valid-ity of probabilistic modeling with maximum entropy es-timation for feature forests.
The differences between tra-ditional and our model were insignificant and the resultsproved that a consistent probability model of parsing canbe built without the independence assumption, and attainsperformance that rivals the traditional models in terms ofparsing accuracy.We should note that accuracy can further be improvedwith our model because it allows other features to be in-corporated that were not used in these experiments be-cause the model is not rely on the decomposition intothe dependencies of two words.
Another possibility toincrease the accuracy is to refine the LTAG grammar.
Al-though we assumed that all syntactic constraints wereexpressed with syntactic categories (Section 3), i.e., el-ementary trees, the grammar used in the experimentswere not augmented with feature structures and not suffi-ciently restrictive to eliminate syntactically invalid struc-tures.
Since our model did not include the preferences ofsyntactic relations of words, we expect the refinement ofthe grammar will greatly improve the accuracy.5 ConclusionThis paper described a novel model for syntactic dis-ambiguation based on lexicalized grammars, where themodel selects the most probable parsing result from thecandidates allowed by a lexicalized grammar.
Since lex-icalized grammars can restrict the possible structure ofparsing results, the probabilistic model cannot simplybe decomposed into independent events as in the ex-isting disambiguation models for parsing.
By apply-ing a maximum entropy model for feature forests, weachieved probabilistic modeling without decomposition.Through experiments, we proved the syntax-only modelcould record with high level of accuracy with a lexical-ized grammar, and maximum entropy estimation for fea-ture forests could attain competitive accuracy comparedto the traditional model.
We see this work as the first stepin the application of linguistically motivated grammars tothe parsing of real-world texts as well as the evaluation oflinguistic theories by consulting extensive corpora.Future work should include the application of ourmodel to other lexicalized grammars including HPSG.The development of sophisticated parsing strategies isalso required to improve the accuracy and efficiency ofparsing.
Since parsing results of lexicalized grammarssuch as HPSG and CCG can include non-local dependen-cies, we cannot simply apply well-known parsing strate-gies, such as beam thresholding, which assume the localcomputation of preference scores.
Further investigationsmust be left for future research.ReferencesSteven P. Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4).Adam L. Berger, Stephen A. Della Pietra, and Vincent.J.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of 14th National Conference on Artificial Intelli-gence, pages 598?603.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussianprior for smoothing maximum entropy models.
Tech-nical Report CMUCS-99-108, Carnegie Mellon Uni-versity.John Chen and K. Vijay-Shanker.
2000.
Automated ex-traction of TAGs from the Penn Treebank.
In Proceed-ings of 6th IWPT.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of ACL 2000, pages 456?463.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building deep dependency structures with awide-coverage CCG parser.
In Proceedings of 40thACL.Michael Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings of 34thACL, pages 184?191.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of 35thACL.Stuart Geman and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of stochasticunification-based grammars.
In Proceedings of 40thACL, pages 279?286.Julia Hockenmaier and Mark Steedman.
2002a.
Acquir-ing compact lexicalized grammars from a cleaner tree-bank.
In Proceedings of 3rd LREC.Julia Hockenmaier and Mark Steedman.
2002b.
Gen-erative models for statistical parsing with Combina-tory Categorial Grammar.
In Proceedings of 40th ACL,pages 335?342.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceedings of 37thACL, pages 535?541.Aravind K. Joshi and B. Srinivas.
1994.
Disambiguationof super parts of speech (or supertags): Almost pars-ing.
In Proceedings of 17th COLING, pages 161?165.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InARPA Human Language Technology Workshop.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proceedings ofHLT 2002.Yusuke Miyao.
2002.
Amis ?
a maximum entropy es-timator for feature forests.
Available via http://www-tsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.Jorge Nocedal.
1980.
Updating quasi-Newton matriceswith limited storage.
Mathematics of Computation,35:773?783.Stefan Riezler, Detlef Prescher, Jonas Kuhn, and MarkJohnson.
2000.
Lexicalized stochastic modeling ofconstraint-based grammars using log-linear measuresand EM training.
In Proceedings of 38th ACL.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of 40th ACL.Ivan A.
Sag and ThomasWasow.
1999.
Syntactic Theory?
A Formal Introduction.
CSLI Lecture Notes no.
92.CSLI Publications.Yves Schabes, Anne Abeille?, and Aravind K. Joshi.1988.
Parsing strategies with ?lexicalized grammars?
:Application to tree adjoining grammars.
In Proceed-ings of 12th COLING, pages 578?583.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of 5th NLPRS.
