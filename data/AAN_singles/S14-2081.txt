Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 465?470,Dublin, Ireland, August 23-24, 2014.Potsdam: Semantic Dependency Parsing by Bidirectional Graph-TreeTransformations and Syntactic Parsing?Zeljko Agi?cUniversity of Potsdamzagic@uni-potsdam.deAlexander KollerUniversity of Potsdamkoller@ling.uni-potsdam.deAbstractWe present the Potsdam systems that par-ticipated in the semantic dependency pars-ing shared task of SemEval 2014.
Theyare based on linguistically motivated bidi-rectional transformations between graphsand trees and on utilization of syntactic de-pendency parsing.
They were entered inboth the closed track and the open trackof the challenge, recording a peak averagelabeled F1score of 78.60.1 IntroductionIn the semantic dependency parsing (SDP) task ofSemEval 2014, the meaning of a sentence is repre-sented in terms of binary head-argument relationsbetween the lexical units ?
bi-lexical dependencies(Oepen et al., 2014).
Since words can be seman-tic dependents of multiple other words, this frame-work results in graph representations of sentencemeaning.
For the SDP task, three such annotationlayers are provided on top of the WSJ text of thePenn Treebank (PTB) (Marcus et al., 1993):?
DM: the reduction of DeepBank HPSG anno-tation (Flickinger et al., 2012) into bi-lexicaldependencies following (Oepen and L?nning,2006; Ivanova et al., 2012),?
PAS: the predicate-argument structures derivedfrom the training set of the Enju HPSG parser(Miyao et al., 2004) and?
PCEDT: a subset of the tectogrammatical anno-tation layer from the English side of the PragueCzech-English Dependency Treebank (Cinkov?aet al., 2009).The three annotation schemes provide three di-rected graph representations for each PTB sen-This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/tence, with word forms as nodes and labeled de-pendency relations as edges pointing from func-tors to arguments.
The SDP-annotated PTB text issplit into training (sections 00?19), development(sec.
20) and testing sets (sec.
21).
This in turnmakes the SDP parsing task a problem of data-driven graph parsing, in which systems are to betrained for producing dependency graph represen-tations of sentences respecting the three underly-ing schemes.While a number of theoretical and preliminarycontributions to data-driven graph parsing exist(Sagae and Tsujii, 2008; Das et al., 2010; Joneset al., 2013; Chiang et al., 2013; Henderson etal., 2013), our goal here is to investigate the sim-plest approach that can achieve competitive per-formance.
Our starting point is the observationthat the SDP graphs are relatively tree-like.
On it,we build a system for data-driven graph parsing by(1) transforming dependency graphs into depen-dency trees in preprocessing, (2) training and us-ing syntactic dependency parsers over these treesand (3) transforming their output back into graphsin postprocessing.
This way, we inherit the accu-racy and speed of syntactic dependency parsers.The secondary benefit is insight into the struc-ture of the semantic representations, as graph-treetransformations can make the phenomena that re-quire non-tree-like structures more explicit.2 Data and SystemsWe present the basic statistics for the SDP train-ing sets in Table 1.
The graphs contain no cycles,i.e., all SDP meaning representations are directedacyclic graphs (DAGs).
DM and PAS are auto-matically derived from HPSG annotations, whilePCEDT is based on manual tectogrammatical an-notation.
This is reflected in more than half of thePCEDT graphs being disjoint sets of dependencytrees, i.e., forests.
The number of forests in DMand PAS is negligible, on the other hand.
The edge465Feature DM PAS PCEDTSentences 32,389 32,389 32,389Tokens 742,736 742,736 742,736Edge labels 52 43 71Cyclic graphs 0 0 0Forests 810 418 18,527Treewidth (undirected) 1.30 1.71 1.45Tree labelsLOCAL 79 77 124DFS 79 81 133Table 1: Basic statistics for the training sets.label set of PCEDT is also substantially larger thanthe label sets of DM and PAS.2.1 BaselineA directed acyclic graph is a dependency tree inthe sense of (Nivre, 2006) if any two nodes areconnected by exactly one simple path.
In otherwords, a DAG is a dependency tree if there areno disconnected (singleton) nodes and if there areno node reentrancies, i.e., all nodes have an in-degree of 1.
We calculate the average treewidthof SDP graphs by converting them to undirectedgraphs and applying the algorithm of (Gogate andDechter, 2004).
As we show in Table 1, thetreewidth is low for all three representations.
Thelow treewidth indicates that, even if the SDP se-mantic representations are graphs and not trees,these graphs are very tree-like and, as such, easilytransformed into trees as there are not many edgesthat would require deletion.
Thus, one could per-form a lossy graph-to-tree conversion by (a) de-tecting singleton nodes and attaching them triv-ially and (b) detecting reentrant nodes and deletingall but one incoming edge.The official SDP baseline system1(Oepen et al.,2014) is based precisely on this principle: single-tons are attached to their right neighbors, only theedges to the closest predicates are kept for reen-trant nodes, with a preference for leftward predi-cates in ties, and all remaining nodes with an in-degree of 0 are attached to the root.
Two dummylabels are introduced in the process: root for at-tachments to root and null for the remaining newattachments.
The baseline is thus limited by thelossy approach to graph-to-tree reductions and thelack of linguistic motivation for these particular re-duction operations.
Here, we aim at introducing1http://alt.qcri.org/semeval2014/task8/index.php?id=evaluationFigure 1: Distributions of node indegrees for (a)all nodes and (b) source nodes of edges participat-ing in reentrancies.Figure 2: Distributions of parts of speech for reen-trancy source nodes with zero indegree.
Ten mostfrequent parts of speech are displayed.less lossy and more linguistically motivated reduc-tions.2.2 Local Edge FlippingFurthermore, inspecting the distribution of nodeindegrees in the SDP data in Figure 1, we maketwo important observations: (1) from its left his-togram, that most of the nodes in all three annota-tions have an indegree of 0 or 1, and (2) from itsright histogram, that most source nodes of edgescausing reentrancies themselves have an indegreeof 0.
Figure 2 deepens this observation by provid-ing a part-of-speech distribution of source nodesin reentrancies.
It shows that the edges in DMand PAS are systematically pointed from modi-466System DM PAS PCEDTBASELINE 66.19 57.66 90.70LOCAL 89.93 88.73 91.86DFS 95.52 93.98 92.85Table 2: Upper bound LF scores on the develop-ment set for LOCAL and DFS conversion comparedto the baseline.
This score indicates the quality ofgraph-tree transformation as no parsing is done.Dataset P R F1DM 73.30 62.99 67.76PAS 76.03 72.12 74.02PCEDT 79.40 78.52 78.96Table 3: Top node detection accuracy with CRFson the development set for the three annotations.Precision (P), recall (R) and the F1scores relate tomarking tokens with the binary top node flag.fiers to modifiees, while coordinating conjunctionsin PCEDT introduce the coordinated nodes.
Weconclude that edges in reentrancies, for which thesource nodes have zero indegree, could be flippedby changing places of their source and target nodesand encoding the switch in the edge labels by ap-pending the suffix flipped to the existing labels.This is the basis for our first system: LOCAL.In it, we locally flip all edges in reentrancies forwhich the source node has zero indegree and runthe BASELINE conversion on the resulting graphs.We apply this conversion on the training data, usethe converted training sets to train syntactic de-pendency parsers (Bohnet, 2010) and utilize theparsing models on the development and test data.The parsing outputs are converted back to graphsby simply re-flipping all the edges denoted asflipped.2.3 Depth-first Edge FlippingOur second system, DFS, is based on depth-firstsearch graph traversal and edge flipping.
In it, wecreate a undirected copy of the input graph andconnect all nodes with zero indegree to the root us-ing dummy edges.
We do a depth-first traversal ofthis graph, starting from the root, while perform-ing edge lookup in the original DAG.
For each DFSedge traversal in the undirected copy, we check ifthe direction of this edge in the original DAG isidentical or reversed to the traversal direction.
Ifit is identical, we keep the existing edge.
If wetraverse the edge against its original direction, weDM PAS PCEDTclosed LAS UAS LAS UAS LAS UASLOCAL 79.09 81.35 81.93 83.79 81.16 89.60DFS 82.02 83.74 87.06 87.93 79.94 88.04openLOCAL 80.86 82.73 85.16 86.18 82.04 90.79DFS 84.23 85.77 88.42 89.26 80.82 89.02Table 4: Syntactic dependency parsing accuracyof our systems before the tree-to-graph transfor-mations, given as a set of labeled (LAS) and un-labeled (UAS) attachment scores.
The scores aregiven for the development set.reverse it.
Finally, we delete the dummy edges andconvert the resulting graph to a dependency tree byrunning the baseline, to connect the singletons totheir neighbors, and to attach predicates with zeroindegree and sentence-final nodes to the root.We illustrate our graph-to-tree transformationsLOCAL and DFS on a gold standard graph from thetraining data in Figure 3.
It shows how DFS man-ages to preserve more edges than LOCAL by per-forming traversal flipping, while LOCAL flips onlythe edges that have source nodes with zero inde-gree.
On the other hand, DFS performs more flip-ping operations than LOCAL, but as Table 1 shows,this does not result in substantial increase of thelabel sets.2.4 Parsing and Top Node DetectionThe same syntactic parser and top node detectorare used in both LOCAL and DFS.
Both systemsran in the closed SDP track, with no additionalfeatures for learning, and in the open track, wherethey used the SDP companion data, i.e., the out-puts of a syntactic dependency parser (Bohnet andNivre, 2012) and phrase-based parser (Petrov etal., 2006) as additional features.
Our choice ofparser was based on the high non-projectivity ofthe resulting trees, while parsers of (Bohnet andNivre, 2012; Bohnet et al., 2013) could also beused, among others.
We use the parser out ofthe box, i.e., without any parameter tuning or ad-ditional features other than what was previouslylisted for the open track.Top node detection is implemented separately,by training a sequence labeling model (Laffertyet al., 2001; Kudo, 2005) on tokens and part-of-speech tags from the training sets.
Its accuracyis given in Table 3.
We use only the tokens andparts of speech as features for these models, and467Figure 3: Illustration of graph-to-tree transformations of a gold standard graph for LOCAL and DFS.
Edgelabels are omitted.
The sentence (PAS, #20415005): Who that winner will be is highly uncertain.we design our feature set by adapting the chunkingtemplate from the CRF++ toolkit documentation.2We note that this model can be improved by, e.g.,adding the open track companion features to thefeature set, but they were not used in the experi-ments we present here.3Our graph-to-tree conversions expand the labelsets by appending the edge flip flag.
The sizes ofthe new label sets are given in Table 1 in compar-ison to the original ones.
The increase in size isexpected to affect the parsing accuracy.
The pars-ing accuracies on the development sets are givenin Table 4.
The scores correlate with the labelset sizes, with a notable difference between the la-beled (LAS) and unlabeled (UAS) attachment scorefor PCEDT.
The LOCAL approach tends to out-perform DFS for PCEDT, while DFS parsers alsosignificantly outperform LOCAL for DM and PAS.The open track parsers tend to perform a little bet-ter as they make use of the additional features.In Table 2, we measure the theoretical maxi-mum accuracy for parsers based on our two con-versions in comparison with the baseline.
There,we run BASELINE, LOCAL and DFS on the devel-opment set and convert the trees back to graphsright away, i.e., without the parsing step, so asto observe the dissipation of the conversion.
Thescores show that LOCAL and DFS outperformBASELINE by a large margin, while the maximumaccuracy for DFS is larger than the one for LOCAL,1 point for PCEDT and around 5 points for DMand PAS.
This is due to DFS performing non-localedge flipping, thus preserving more edges.
Theparsing scores from Table 4 and the maximum ac-curacy from Table 2 show that our systems are not2http://crfpp.googlecode.com/svn/trunk/doc/index.html3The recall would increase by 15 points, amounting to a10 point increase in F1for top node detection in DM.closed opendev LF UF LF UFLOCAL 76.70 82.01 77.87 83.19DFS 78.49 83.78 80.03 85.31testLOCAL 75.94 81.58 76.79 82.52DFS 77.34 82.99 78.60 84.32Table 5: Overall accuracy for our LOCAL and DFSsystems, i.e., averaged labeled and unlabeled F1scores over the three annotations.as lossy in graph-tree conversions as the baseline,while they pay the price in the number of new la-bels in actual parsing and, subsequently, in the ac-curacy of the dependency parsers.
Thus, LAS andUAS for the baseline are 1-2 points higher than thescores in Table 4 for DM and PCEDT, while ourscores are 3-4 points higher for PAS.3 Results and DiscussionAs in the official SDP scoring, we express theresults in terms of labeled and unlabeled preci-sion (LP, UP) and recall (LR, UR), their harmonicmeans, the F1scores (LF, UF), and sentence-levelexact matches (LM, UM).
The official SDP scorerreports on two variants of these scores: the onetaking into account the virtual edges to top nodesand the one excluding those edges.
The former isless relaxed as it requires the top nodes to be pre-dicted, and this is the only one we use in this re-port.
We note that for our systems, the scores with-out the virtual edges are approximately 2 pointshigher for all the metrics.The overall scores are given in Table 5.
There,we provide the labeled and unlabeled F1scores onthe development and test data in the closed andopen track, averaged for all three annotations.
Theopen track systems consistently score approxi-468closed track DM PAS PCEDTLP LR LF LM LP LR LF LM LP LR LF LMLOCAL 83.39 72.88 77.78 4.53 88.18 74.00 80.47 2.00 72.25 67.10 69.58 6.38DFS 79.36 79.34 79.35 9.05 88.15 81.60 84.75 7.72 69.68 66.25 67.92 5.86?4.03 +6.46 +1.57 +4.52 ?0.03 +7.60 +4.28 +5.72 ?2.57 ?0.85 ?1.66 ?0.52UP UR UF UM UP UR UF UM UP UR UF UMLOCAL 85.47 74.70 79.72 5.04 89.70 75.28 81.86 2.23 86.36 80.21 83.17 19.44DFS 81.56 81.54 81.55 10.31 89.62 82.96 86.16 7.86 83.37 79.27 81.27 17.51?3.91 +6.84 +1.83 +5.27 ?0.08 +7.69 +4.30 +5.63 ?3.00 ?0.94 ?1.91 ?1.93open track DM PAS PCEDTLP LR LF LM LP LR LF LM LP LR LF LMLOCAL 84.54 73.80 78.80 4.53 89.72 75.08 81.75 2.00 72.52 67.33 69.83 6.08DFS 81.32 80.91 81.11 10.46 89.41 82.61 85.88 8.46 70.35 67.33 68.80 5.79?3.22 +7.11 +2.31 +5.93 ?0.31 +7.53 +4.13 +6.46 ?2.17 +0.00 ?1.03 ?0.29UP UR UF UM UP UR UF UM UP UR UF UMLOCAL 86.43 75.45 80.57 5.49 90.99 76.14 82.91 2.30 87.32 81.07 84.08 19.73DFS 83.37 82.95 83.16 11.94 90.78 83.87 87.19 8.75 84.46 80.83 82.60 18.47?3.06 +7.50 +2.59 +6.45 ?0.22 +7.73 +4.28 +6.45 ?2.86 ?0.24 ?1.48 ?1.26Table 6: Breakdown of the scores for our LOCAL and DFS systems on the test sets.
We provide labeledand unlabeled precision (LP, UP), recall (LR, UR), F1scores (LF, UF) and exact matches (LM, UM) forall three annotations in both the closed and the open evaluation track.mately 1 point higher than their closed track coun-terparts, apparently taking advantage of the ad-ditional features available in training and testing.The DFS system is 2 points better than LOCAL inall scenarios, owing to the higher maximum cover-age of the original graphs in the conversions.
Thelarge label sets amount to a difference of approxi-mately 6 points between the labeled and unlabeledaccuracies in favor of the latter attachment.Table 6 is a breakdown of the scores in Table 5across the three annotations and the two tracks.Here, we pair the F1scores with the correspond-ing precision and recall scores.
We also explicitlydenote the differences in scores between LOCALand DFS.
For DM and PAS, the score patternsare very similar: due to the larger label set andless regular edge flipping, DFS has a 3-4 pointslower precision than LOCAL, while its recall is 6-8points higher, amounting to the overall improve-ment of approximately 4 points F1.
In contrast, onthe PCEDT data, LOCAL outperforms DFS by ap-proximately 1.5 points.
We note that the label setsfor PCEDT are much larger than for DM and PASand that the favorable reentrancies in PCEDT aremuch less frequent to begin with (see Table 1, Ta-ble 2 and Figure 2).
At 14 points F1, the discrep-ancy between the labeled and unlabeled scores ismuch higher for PCEDT than for DM and PAS,for which we observe a 1-2 point difference.The exact match scores (LM, UM) favor DFSover LOCAL by approximately 5 points for DMand PAS, while LOCAL is better than DFS forPCEDT by 1-2 points.
In absolute terms, the PASscores are higher than those for DM and PAS inboth our systems.
This difference between thetoken-level and the sentence-level scores stemsfrom the properties of our graph-tree transforma-tions as, e.g., certain edges in undirected cyclescould not be addressed by our edge inversions.At approximately 81, 86 and 70 points F1forDM, PAS and PCEDT, in this contribution wehave shown that focusing on graph-tree transfor-mations for the utilization of a syntactic depen-dency parser lets us achieve good overall perfor-mance in the semantic dependency parsing task.
Inthe future, we will further investigate what trans-formations are appropriate for different styles ofgraph-based semantic representations, and whatwe can learn from this both for improving SDPparser accuracy and for making linguistically mo-tivated design choices for graph-based seman-tic representations.
Furthermore, we will extendour system to cover inherently non-tree-like struc-tures, such as those induced by control verbs.Acknowledgements We are grateful to StephanOepen for all the discussions on the properties ofthe SDP datasets, and for providing the infrastruc-ture for running the systems.
We also thank theanonymous reviewers for their valuable insight.469ReferencesBernd Bohnet and Joakim Nivre.
2012.
A Transition-Based System for Joint Part-of-Speech Tagging andLabeled Non-Projective Dependency Parsing.
InProc.
EMNLP-CoNLL, pages 1455?1465.Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,Rich?ard Farkas, Filip Ginter, and Jan Haji?c.
2013.Joint Morphological and Syntactic Analysis forRichly Inflected Languages.
TACL, 1:415?428.Bernd Bohnet.
2010.
Top Accuracy and Fast Depen-dency Parsing is not a Contradiction.
In Proc.
COL-ING, pages 89?97.David Chiang, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, Bevan Jones, and KevinKnight.
2013.
Parsing Graphs with HyperedgeReplacement Grammars.
In Proc.
ACL, pages924?932.Silvie Cinkov?a, Josef Toman, Jan Haji?c, Krist?yna?Cerm?akov?a, V?aclav Klime?s, Lucie Mladov?a,Jana?Sindlerov?a, Krist?yna Tom?s?u, and Zden?ek?Zabokrtsk?y.
2009.
Tectogrammatical Annotationof the Wall Street Journal.
The Prague Bulletin ofMathematical Linguistics, 92:85?104.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic Frame-Semantic Parsing.
In Proc.
NAACL, pages 948?956.Dan Flickinger, Yi Zhang, and Valia Kordoni.
2012.DeepBank: A Dynamically Annotated Treebank ofthe Wall Street Journal.
In Proc.
TLT, pages 85?96.Vibhav Gogate and Rina Dechter.
2004.
A CompleteAnytime Algorithm for Treewidth.
In Proc.
UAI,pages 201?208.James Henderson, Paola Merlo, Ivan Titov, andGabriele Musillo.
2013.
Multilingual Joint Pars-ing of Syntactic and Semantic Dependencies with aLatent Variable Model.
Computational Linguistics,39(4):949?998.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who Did What to Whom?A Contrastive Study of Syntacto-Semantic Depen-dencies.
In Proc.
Linguistic Annotation Workshop,pages 2?11.Bevan Keeley Jones, Sharon Goldwater, and MarkJohnson.
2013.
Modeling Graph Languages withGrammars Extracted via Tree Decompositions.
InProc.
FSMNLP, pages 54?62.Taku Kudo.
2005.
CRF++: Yet another CRFtoolkit.
Software available at http://crfpp.sourceforge.net/.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proc.
ICML, pages 282?289.Mitchell Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2004.
Corpus-oriented Grammar Development forAcquiring a Head-Driven Phrase Structure Grammarfrom the Penn Treebank.
In Proc.
IJCNLP, pages684?693.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Stephan Oepen and Jan Tore L?nning.
2006.Discriminant-Based MRS Banking.
In Proc.
LREC,pages 1250?1255.Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Dan Flickinger, Jan Haji?c, AngelinaIvanova, and Yi Zhang.
2014.
SemEval 2014 Task8: Broad-Coverage Semantic Dependency Parsing.In Proceedings of the 8th International Workshop onSemantic Evaluation.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proc.
COLING-ACL, pages 433?440.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-ReduceDependency DAG Parsing.
In Proc.
COLING, pages753?760.470
