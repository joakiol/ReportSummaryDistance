Proceedings of the 6th Workshop on Statistical Machine Translation, pages 413?419,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsILLC-UvA translation system for EMNLP-WMT 2011Maxim Khalilov and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamP.O.
Box 942421090 GE Amsterdam, The Netherlands{m.khalilov,k.simaan}@uva.nlAbstractIn this paper we describe the Institute forLogic, Language and Computation (Uni-versity of Amsterdam) phrase-based statisti-cal machine translation system for English-to-German translation proposed within theEMNLP-WMT 2011 shared task.
The mainnovelty of the submitted system is a syntax-driven pre-translation reordering algorithmimplemented as source string permutation viatransfer of the source-side syntax tree.1 IntroductionFor the WMT 2011 shared task, ILLC-UvA submit-ted two translations (primary and secondary) for theEnglish-to-German translation task.
This year, wedirected our research toward addressing the wordorder problem for statistical machine translation(SMT) and discover its impact on output translationquality.
We reorder the words of a sentence of thesource language with respect to the word order ofthe target language and a given source-side parsetree.
The difference from the baseline Moses-basedtranslation system lies in the pre-translation step, inwhich we introduce a discriminative source stringpermutation model based on probabilistic parse treetransduction.The idea here is to permute the order of the sourcewords in such a way that the resulting permutationallows as monotone a translation process as possibleis not new.
This approach to enhance SMT by usinga reordering step prior to translation has proved to besuccessful in improving translation quality for manytranslation tasks, see (Genzel, 2010; Costa-jussa` andFonollosa, 2006; Collins et al, 2005), for example.The general problem of source-side reordering isthat the number of permutations is factorial in n, andlearning a sequence of transductions for explaininga source permutation can be computationally ratherchallenging.
We propose to address this problem bydefining the source-side permutation process as thelearning problem of how to transfer a given sourceparse tree into a parse tree that minimizes the diver-gence from target word order.Our reordering system is inspired by the directiontaken in (Tromble and Eisner, 2009), but differs indefining the space of permutations, using local prob-abilistic tree transductions, as well as in the learn-ing objective aiming at scoring permutations basedon a log-linear interpolation of a local syntax-basedmodel with a global string-based (language) model.The reordering (novel) and translation (standard)components are described in the following sections.The rest of this paper is structured as follows.
After abrief description of the phrase-based translation sys-tem in Section 2, we present the architecture and de-tails of our reordering system (Section 3), Section 4reviews related work, Section 5 reports the experi-mental setup, details the submissions and discussesthe results, while Section 6 concludes the article.2 Baseline system2.1 Statistical machine translationIn SMT the translation problem is formulated as se-lecting the target translation t with the highest prob-ability from a set of target hypothesis sentences for413the source sentence s: t?
= argmaxt { p(t|s) } =argmaxt { p(s|t) ?
p(t) }.2.2 Phrase-based translationWhile first systems following this approach per-formed translation on the word level, modern state-of-the-art phrase-based SMT systems (Och and Ney,2002; Koehn et al, 2003) start-out from a word-aligned parallel corpus working with (in principle)arbitrarily large phrase pairs (also called blocks) ac-quired from word-aligned parallel data under a sim-ple definition of translational equivalence (Zens etal., 2002).The conditional probabilities of one phrase givenits counterpart is estimated as the relative frequencyratio of the phrases in the multiset of phrase-pairsextracted from the parallel corpus and are interpo-lated log-linearly together with a set of other modelestimates:e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}(1)where a feature function hm refer to a system model,and the corresponding ?m refers to the relativeweight given to this model.A phrase-based system employs feature func-tions for a phrase pair translation model, a lan-guage model, a reordering model, and a modelto score translation hypothesis according to length.The weights ?m are optimized for system perfor-mance (Och, 2003) as measured by BLEU (Papineniet al, 2002).Apart from the novel syntax-based reorderingmodel, we consider two reordering methods thatare widely used in phrase-based systems: a simpledistance-based reordering and a lexicalized block-oriented data-driven reordering model (Tillman,2004).3 Architecture of the reordering systemWe approach the word order challenge by includingsyntactic information in a pre-translation reorderingframework.
This section details the general idea ofour approach and details the reordering model thatwas used in English-to-German experiments.3.1 Pre-translation reordering frameworkGiven a word-aligned parallel corpus, we define thesource string permutation as the task of learningto unfold the crossing alignments between sentencepairs in the parallel corpus.
Let be given a source-target sentence pair s ?
t with word alignment seta between their words.
Unfolding the crossing in-stances in a should lead to as monotone an align-ment a?
as possible between a permutation s?
of sand the target string t. Conducting such a ?mono-tonization?
on the parallel corpus gives two par-allel corpora: (1) a source-to-permutation parallelcorpus (s ?
s?)
and (2) a source permutation-to-target parallel corpus (s?
?
t).
The latter corpus isword-aligned automatically again and used for train-ing a phrase-based translation system, while the for-mer corpus is used for training our model for pre-translation source permutation via parse tree trans-ductions.In itself, the problem of permuting the sourcestring to unfold the crossing alignments is com-putationally intractable (see (Tromble and Eisner,2009)).
However, different kinds of constraints canbe made on unfolding the crossing alignments in a.A common approach in hierarchical SMT is to as-sume that the source string has a binary parse tree,and the set of eligible permutations is defined by bi-nary ITG transductions on this tree.
This definespermutations that can be obtained only by at mostinverting pairs of children under nodes of the sourcetree.3.2 Conditional tree reordering modelGiven a parallel corpus with string pairs s ?
t withword alignment a, the source strings s are parsed,leading to a single parse tree ?s per source string.
Wecreate a source permuted parallel corpus s ?
s?
byunfolding the crossing alignments in a without/withsyntactic tree to provide constraints on the unfold-ing.Our model aims at learning from the source per-muted parallel corpus s ?
s?
a probabilistic op-timization argmaxpi(s) P (pi(s) | s, ?s).
We as-sume that the set of permutations {pi(s)} is definedthrough a finite set of local transductions over thetree ?s.
Hence, we view the permutations leadingfrom s to s?
as a sequence of local tree transduc-414tions ?s?0 ?
.
.
.
?
?s?n , where s?0 = s and s?n = s?
,and each transduction ?s?i?1 ?
?s?i is defined using atree transduction operation that at most permutes thechildren of a single node in ?s?i?1 as defined next.A local transduction ?s?i?1 ?
?s?i is modelled byan operation that applies to a single node with ad-dress x in ?s?i?1 , labeled Nx, and may permute theordered sequence of children ?x dominated by nodex.
This constitutes a direct generalization of the ITGbinary inversion transduction operation.
We assign aconditional probability to each such local transduc-tion:P (?s?i | ?s?i?1) ?
P (pi(?x) | Nx ?
?x, Cx) (2)where pi(?x) is a permutation of ?x (the orderedsequence of node labels under x) and Cx is a lo-cal tree context of node x in tree ?s?i?1 .
One wrin-kle in this definition is that the number of possiblepermutations of ?x is factorial in the length of ?x.Fortunately, the source permuted training data ex-hibits only a fraction of possible permutations evenfor longer ?x sequences.
Furthermore, by condition-ing the probability on local context, the general ap-plicability of the permutation is restrained.In principle, if we would disregard the computa-tional cost, we could define the probability of the se-quence of local tree transductions ?s?0 ?
.
.
.
?
?s?nasP (?s?0 ?
.
.
.
?
?s?n) =n?i=1P (?s?i | ?s?i?1) (3)The problem of calculating the most likely permu-tation under this kind of transduction probabilityis intractable because every local transduction con-ditions on local context of an intermediate tree1.Hence, we disregard this formulation and in practicewe take a pragmatic approach and greedily select atevery intermediate point ?s?i?1 ?
?s?i the single mostlikely local transduction that can be conducted onany node of the current intermediate tree ?s?i?1 .
The1Note that a single transduction step on the current tree?s?i?1 leads to a forest of trees ?s?i because there can be mul-tiple alternative transduction rules.
Hence, this kind of a modeldemands optimization over many possible sequences of trees,which can be packed into a sequence of parse-forests with trans-duction links between them.individual steps are made more effective by interpo-lating the term in Equation 2 with string probabilityratios:P (pi(?x) | Nx ?
?x, Cx)?
(P (s?i?1)P (s?i))(4)The rationale behind this interpolation is that oursource permutation approach aims at finding the op-timal permutation s?
of s that can serve as input fora subsequent translation model.
Hence, we aim attree transductions that are syntactically motivatedthat also lead to improved string permutations.
Inthis sense, the tree transduction definitions can beseen as an efficient and syntactically informed wayto define the space of possible permutations.We estimate the string probabilities P (s?i) using5-gram language models trained on the s?
side ofthe source permuted parallel corpus s ?
s?
.
We es-timate the conditional probability P (pi(?x) | Nx ?
?x, Cx) using a Maximum-Entropy framework,where feature functions are defined to capture thepermutation as a class, the node label Nx and itshead POS tag, the child sequence ?x together withthe corresponding sequence of head POS tags andother features corresponding to different contextualinformation.We were particularly interested in those linguisticfeatures that motivate reordering phenomena fromthe syntactic and linguistic perspective.
The featuresthat were used for training the permutation systemare extracted for every internal node of the sourcetree that has more than one child:?
Local tree topology.
Sub-tree instances that in-clude parent node and the ordered sequence ofchild node labels.?
Dependency features.
Features that determinethe POS tag of the head word of the currentnode, together with the sequence of POS tagsof the head words of its child nodes.?
Syntactic features.
Two binary features fromthis class describe: (1) whether the parent nodeis a child of the node annotated with the samesyntactic category, (2) whether the parent nodeis a descendant of a node annotated with thesame syntactic category.4154 Related workThe integration of linguistic syntax into SMT sys-tems offers a potential solution to reordering prob-lem.
For example, syntax is successfully integratedinto hierarchical SMT (Zollmann and Venugopal,2006).
In (Yamada and Knight, 2001), a set of tree-string channel operations is defined over the parsetree nodes, while reordering is modeled by permuta-tions of children nodes.
Similarly, the tree-to-stringsyntax-based transduction approach offers a com-plete translation framework (Galley et al, 2006).The idea of augmenting SMT by a reordering stepprior to translation has often been shown to improvetranslation quality.
Clause restructuring performedwith hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presentedin (Collins et al, 2005) and (Wang et al, 2007), re-spectively.
In (Xia and McCord, 2004; Khalilov,2009) word reordering is addressed by exploitingsyntactic representations of source and target texts.In (Costa-jussa` and Fonollosa, 2006) source andtarget word order harmonization is done using well-established SMT techniques and without the use ofsyntactic knowledge.
Other reordering models op-erate provide the decoder with multiple word or-ders.
For example, the MaxEnt reordering modeldescribed in (Xiong et al, 2006) provides a hierar-chical phrasal reordering system integrated withina CKY-style decoder.
In (Galley and Manning,2008) the authors present an extension of the famousMSD model (Tillman, 2004) able to handle long-distance word-block permutations.
Coming up-to-date, in (PVS, 2010) an effective application of datamining techniques to syntax-driven source reorder-ing for MT is presented.Different syntax-based reordering systems can befound in (Genzel, 2010).
In this system, reorder-ing rules capable to capture many important wordorder transformations are automatically learned andapplied in the preprocessing step.Recently, Tromble and Eisner (Tromble and Eis-ner, 2009) define source permutation as the word-ordering learning problem; the model works with apreference matrix for word pairs, expressing pref-erence for their two alternative orders, and a cor-responding weight matrix that is fit to the paralleldata.
The huge space of permutations is then struc-tured using a binary synchronous context-free gram-mar (Binary ITG) with O(n3) parsing complexity,and the permutation score is calculated recursivelyover the tree at every node as the accumulation ofthe relative differences between the word-pair scorestaken from the preference matrix.
Application toGerman-to-English translation exhibits some perfor-mance improvement.5 Experiments and submissionsDesign, architecture and configuration of the trans-lation system that we used in experimentation co-incides with the Moses-based translation system(Baseline system) described in details on theWMT 2011 web page2.This section details the experiments carried out toevaluate the proposed reordering model, experimen-tal set-up and data.5.1 DataIn our experiments we used EuroParl v6.0 German-English parallel corpus provided by the organizersof the evaluation campaign.A detailed statistics of the training, development,internal (test int.)
and official (test of.)
test datasetscan be found in Table 1.
The development corpuscoincides with the 2009 test set and for internal test-ing we used the test data proposed to the participantsof WMT 2010.?ASL?
stands for average sentence length.
All thesets were provided with one reference translation.Data Sent.
Words Voc.
ASLtrain En 1.7M 46.0M 121.3K 27.0train Ge 1.7M 43.7M 368.5K 25.7dev En 2.5K 57.6K 13.2K 22.8test int.
En 2.5K 53.2K 15.9K 21.4test of.
En 3.0K 74.8K 11.1K 24.9Table 1: German-English EuroParl corpus (version 6.0).Apart from the German portion of the EuroParlparallel corpus, two additional monolingual corporafrom news domain (the News Commentary corpus(NC) and the News Crawl Corpus 2011 (NS)) were2http://www.statmt.org/wmt11/baseline.html416used to train a language model for German.
Thecharacteristics of these datasets can be found in Ta-ble 2.
Notice that the data were not de-duplicated.Data Sent.
Words Voc.
ASLNC Ge 161.8M 3.9G 136.7M 23.9NS Ge 45.3M 799.4M 3.0M 17.7Table 2: Monolingual German corpora used for target-side language modeling.5.2 Experimental setupMoses toolkit (Koehn et al, 2007) in its standardsetting was used to build the SMT systems:?
GIZA++/mkcls (Och, 2003; Och, 1999) forword alignment.?
SRI LM (Stolcke, 2002) for language model-ing.
A 3-gram target language model was es-timated and smoothed with modified Kneser-Ney discounting.?
MOSES (Koehn et al, 2007) to build an un-factored translation system.?
the Stanford parser (Klein and Manning,2003) was used as a source-side parsing en-gine3.?
For maximum entropy modeling we used themaxent toolkit4.The discriminative syntactic reordering model isapplied to reorder training, development, and testcorpora.
A Moses-based translation system (corpusrealignment included5) is then trained using the re-ordered input.5.3 Internal results and submissionsThe outputs of two translation system were submit-ted.
First, we piled up all feature functions into a sin-gle model as described in Section 3.
It was our ?sec-ondary?
submission.
However, our experience tells3The parser was trained on the English treebank set providedwith 14 syntactic categories and 48 POS tags.4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html5Some studies show that word re-alignment of a mono-tonized corpus gives better results than unfolding of alignmentcrossings (Costa-jussa` and Fonollosa, 2006).that the system performance can increase if the setof patterns is split into partial classes conditioned onthe current node label (Khalilov and Sima?an, 2010).Hence, we trained three separate MaxEnt models forthe categories with potentially high reordering re-quirements, namely NP , SENT and SBAR(Q).It was defines as our ?primary?
submission.The ranking of submission was done according tothe results shown on internal testing, shown in Ta-ble 3.System BLEU dev BLEU test NIST testBaseline 11.03 9.78 3.78Primary 11.07 10.00 3.79Secondary 10.92 9.91 3.78Table 3: Internal testing results.5.4 Official results and discussionUnfortunately, the results of our participation thisyear were discouraging.
The primary submissionwas ranked 30th (12.6 uncased BLEU-4) and thesecondary 31th (11.2) out of 32 submitted systems.It turned out that our preliminary idea to extrapo-late the positive results of English-to-Dutch transla-tion reported in (Khalilov and Sima?an, 2010) to theWMT English-to-German translation task was notright.Analyzing the reasons of negative results duringthe post-evaluation period, we discovered that trans-lation into German differs from English-to-Dutchtask in many cases.
In contrast to English-to-Dutchtranslation, the difference in terms of automaticscores between the internal baseline system (withoutexternal reordering) and the system enhanced withthe pre-translation reordering is minimal.
It turnsout that translating into German is more complexin general and discriminative reordering is more ad-vantageous for English-to-Dutch than for English-to-German translation.A negative aspect influencing is the way how therules are extracted and applied according to our ap-proach.
Syntax-driven reordering, as described inthis paper, involves large contextual information ap-plied cumulatively.
Under conditions of scarce data,alignment and parsing errors, it introduces noise tothe reordering system and distorts the feature prob-417ability space.
At the same time, many reorderingscan be performed more efficiently based on fixed(hand-crafted) rules (as it is done in (Collins et al,2005)).
A possible remedy to this problem is tocombine automatically extracted features with fixed(hand-crafted) rules.
Our last claims are supportedby the observations described in (Visweswariah etal., 2010).During post-evaluation period we analyzed thereasons why the system performance has slightlyimproved when separate MaxEnt models are ap-plied.
The outline of reordered nodes foreach of syntactic categories considered (SENT ,SBAR(Q) and NP ) can be found in Table 4 (thesize of the corpus is 1.7 M of sentences).Category # of applicationsNP 497,186SBAR(Q) 106,243SENT 221,568Table 4: Application of reorderings for separate syntacticcategories.It is seen that the reorderings for NP nodes ishigher than for SENT and SBAR(Q) categories.While SENT and SBAR(Q) reorderings work anal-ogously for Dutch and German, our intuition is thatGerman has more features that play a role in reorder-ing of NP structures than Dutch and there is a needof more specific features to model NP permutationsin an accurate way.6 ConclusionsThis paper presents the ILLC-UvA translation sys-tem for English-to-German translation task pro-posed to the participants of the EMNLP-WMT 2011evaluation campaign.
The novel feature that wepresent this year is a source reordering model inwhich the reordering decisions are conditioned onthe features from the source parse tree.Our system has not managed to outperform themajority of the participating systems, possibly dueto its generic approach to reordering.
We plan to in-vestigate why our approach works well for English-to-Dutch and less well for the English-to-Germantranslation in order to discover more generic waysfor learning discriminative reordering rules.
Onepossible explanation of the bad results is a highsparseness of automatically extracted rules that doesnot allow for sufficient generalization of reorderinginstances.In the future, we plan (1) to perform deeper anal-ysis of the dissimilarity between English-to-Dutchand English-to-German translations from SMTperspective, and (2) to investigate linguistically-motivated ideas to extend our model such that wecan bring about some improvement to English-to-German translation.7 AcknowledgementsBoth authors are supported by a VIDI grant (nr.639.022.604) from The Netherlands Organizationfor Scientific Research (NWO).ReferencesM.
Collins, P. Koehn, and I. Kuc?erova?.
2005.
Clause re-structuring for statistical machine translation.
In Pro-ceedings of ACL?05, pages 531?540.M.
R. Costa-jussa` and J.
A. R. Fonollosa.
2006.Statistical machine reordering.
In Proceedings ofHLT/EMNLP?06, pages 70?76.M.
Galley and Ch.
D. Manning.
2008.
A simple and ef-fective hierarchical phrase reordering model.
In Pro-ceedings of EMNLP?08, pages 848?856.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thaye.
2006.
Scalable inference andtraining of context-rich syntactic translation models.In Proc.
of COLING/ACL?06, pages 961?968.D.
Genzel.
2010.
Aumotatically learning source-side re-ordering rules for large scale machine translation.
InProc.
of COLING?10, pages 376?384, Beijing, China.M.
Khalilov and K. Sima?an.
2010.
A discriminativesyntactic model for source permutation via tree trans-duction.
In Proc.
of the Fourth Workshop on Syn-tax and Structure in Statistical Translation (SSST-4) atCOLING?10, pages 92?100, Beijing (China), August.M.
Khalilov.
2009.
New statistical and syntactic mod-els for machine translation.
Ph.D. thesis, UniversitatPolite`cnica de Catalunya, October.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Annual Meeting ofthe ACL?03, pages 423?430.Ph.
Koehn, F. Och, and D. Marcu.
2003.
Statisticalphrase-based machine translation.
In Proceedings ofthe HLT-NAACL 2003, pages 48?54.Ph.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,418C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open-source toolkit forstatistical machine translation.
In Proceedings of ACL2007, pages 177?180.F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proceedings of ACL?02, pages 295?302.F.
Och.
1999.
An efficient method for determining bilin-gual word classes.
In Proceedings of ACL 1999, pages71?76.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL?03, pages160?167.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL?02, pages 311?318.A.
PVS.
2010.
A data mining approach to learn reorderrules for SMT.
In Proceedings of NAACL/HLT?10,pages 52?57.A.
Stolcke.
2002.
SRILM: an extensible language mod-eling toolkit.
In Proceedings of SLP?02, pages 901?904.C.
Tillman.
2004.
A unigram orientation model for sta-tistical machine translation.
In Proceedings of HLT-NAACL?04, pages 101?104.R.
Tromble and J. Eisner.
2009.
Learning linear order-ing problems for better translation.
In Proceedings ofEMNLP?09, pages 1007?1016.K.
Visweswariah, J. Navratil, J. Sorensen, V. Chenthama-rakshan, and N. Kambhatla.
2010.
Syntax basedreordering with automatically derived rules for im-proved statistical machine translation.
In Proc.
ofCOLING?10, pages 1119?1127, Beijing, China.C.
Wang, M. Collins, and Ph.
Koehn.
2007.
Chinesesyntactic reordering for statistical machine translation.In Proceedings of EMNLP-CoNLL?07, pages 737?745.F.
Xia and M. McCord.
2004.
Improving a statistical MTsystem with automatically learned rewrite patterns.
InProceedings of COLING?04, pages 508?514.D.
Xiong, Q. Liu, and S. Lin.
2006.
Maximum entropybased phrase reordering model for statistical machinetranslation.
In Proceedings of ACL?06, pages 521?528.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proceedings of ACL?01,pages 523?530.R.
Zens, F. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Proceedings of KI: Ad-vances in Artificial Intelligence, pages 18?32.A.
Zollmann and A. Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.
In Pro-ceedings of NAACL?06, pages 138?141.419
