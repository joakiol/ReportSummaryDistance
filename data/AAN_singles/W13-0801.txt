Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?10,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsA Semantic Evaluation of Machine Translation Lexical ChoiceMarine CarpuatNational Research Council Canada1200 Montreal Rd,Ottawa, ON K1A 0R6Marine.Carpuat@nrc.gc.caAbstractWhile automatic metrics of translation qual-ity are invaluable for machine translation re-search, deeper understanding of translationerrors require more focused evaluations de-signed to target specific aspects of translationquality.
We show that Word Sense Disam-biguation (WSD) can be used to evaluate thequality of machine translation lexical choice,by applying a standard phrase-based SMT sys-tem on the SemEval2010 Cross-Lingual WSDtask.
This case study reveals that the SMTsystem does not perform as well as a WSDsystem trained on the exact same parallel data,and that local context models based on sourcephrases and target n-grams are much weakerrepresentations of context than the simpletemplates used by the WSD system.1 IntroductionMuch research has focused on automatically eval-uating the quality of Machine Translation (MT) bycomparing automatic translations to human transla-tions on samples of a few thousand sentences.
Manymetrics (Papineni et al 2002; Banerjee and Lavie,2005; Gime?nez and Ma?rquez, 2007; Lo and Wu,2011, for instance) have been proposed to estimatethe adequacy and fluency of machine translation andevaluated based on their correlatation with humanjudgements of translation quality (Callison-Burch etal., 2010).
While these metrics have proven in-valuable in driving progress in MT research, finer-grained evaluations of translation quality are neces-sary to provide a more focused analysis of transla-tion errors.
When developing complex MT systems,comparing BLEU or TER scores is not sufficient tounderstand what improved or what went wrong.
Er-ror analysis can of course be done manually (Vilaret al 2006), but it is often too slow and expensiveto be performed as often as needed during systemdevelopment.Several metrics have been recently proposed toevaluate specific aspects of translation quality suchas word order (Birch et al 2010; Chen et al 2012).While word order is indirectly taken into account byBLEU, TER or METEOR scores, dedicated metricsprovide a direct evaluation that lets us understandwhether a given system?s reordering performanceimproved during system development.
Word ordermetrics provide a complementary tool for targetingevaluation and analysis to a specific aspect of ma-chine translation quality.There has not been as much work on evaluatingthe lexical choice performance of MT: does a MTsystem preserve the meaning of words in transla-tion?
This is of course measured indirectly by com-monly used global metrics, but a more focused eval-uation can help us gain a better understanding of thebehavior of MT systems.In this paper, we show that MT lexical choice canbe framed and evaluated as a standard Word SenseDisambiguation (WSD) task.
We leverage exist-ing WSD shared tasks in order to evaluate whetherword meaning is preserved in translation.
Let us em-phasize that, just like reordering metrics, our WSDevaluation is meant to complement global metrics oftranslation quality.
In previous work, intrinsic eval-uations of lexical choice have been performed us-ing either semi-automatically constructed data sets1based on MT reference translations (Gime?nez andMa`rquez, 2008; Carpuat and Wu, 2008), or man-ually constructed word sense disambiguation testbeds that do not exactly match MT lexical choice(Carpuat and Wu, 2005).
We will show how ex-isting Cross-Lingual Word Sense Disambiguationtasks (Lefever and Hoste, 2010; Lefever and Hoste,2013) can be directly seen as machine translationlexical choice (Section 2): their sense inventory isbased on translations in a second language ratherthan arbitrary sense representations used in otherWSD tasks (Carpuat and Wu, 2005); unlike in MTevaluation settings, human annotators can more eas-ily provide a complete representation of all correctmeanings of a word.
Second, we show how us-ing this task for evaluating the lexical choice perfor-mance of several phrase-based SMT systems (PB-SMT) gives some insights into their strengths andweaknesses (Section 5).2 Selecting a Word Sense DisambiguationTask to Evaluate MT Lexical ChoiceWord Sense Disambiguation consists in determiningthe correct sense of a word in context.
This chal-lenging problem has been studied from a rich varietyof persectives in Natural Language Processing (seeAgirre and Edmonds (2006) for an overview.)
TheSenseval and SemEval series of evaluations (Ed-monds and Cotton, 2001; Mihalcea and Edmonds,2004; Agirre et al 2007) have driven the standard-ization of methodology for evaluating WSD sys-tems.
Many shared tasks were organized over theyears, providing evaluation settings that vary alongseveral dimensions, including:?
target vocabulary: in all word tasks, systemsare expected to tag all content words in run-ning text (Palmer et al 2001), while in lexicalsample tasks, the evaluation considers a smallerpredefined set of target words (Mihalcea et al2004; Lefever and Hoste, 2010).?
language: English is by far the most studiedlanguage, but the disambiguation of words inother languages such as Chinese (Jin et al2007) has been considered.?
sense inventory: many tasks use WordNetsenses (Fellbaum, 1998), but other sense repre-sentations have been used, including alternatesemantic databases such as HowNet (Dong,1998), or lexicalizations in one or more lan-guages (Chklovski et al 2004).The Cross-Lingual Word Sense Disambiguation(CLWSD) task introduced at a recent edition of Se-mEval (Lefever and Hoste, 2010) is an English lex-ical sample task that uses translations in other Eu-ropean languages as a sense inventory.
As a result,it is particularly well suited to evaluating machinetranslation lexical choice.2.1 Translations as Word SenseRepresentationsThe CLWSD task is essentially the same task asMT lexical choice: given English target words incontext, systems are asked to predict translations inother European languages.
The gold standard con-sists of translations proposed by several bilingualhumans, as can be seen in Table 1.
MT systempredictions can be compared to human annotationsdirectly, without introducing additional sources ofambiguity and mismatches due to representation dif-ferences.
This contrasts with our previous work onevaluating MT on a WSD task (Carpuat and Wu,2005), which used text annotated with abstract sensecategories from the HowNet knowledge base (Dong,1998).
In HowNet, each word is defined using aconcept, constructed as a combination of basic unitsof meaning, called sememes.
Words that share thesame concept can be viewed as synonyms.
Evaluat-ing MT using a gold standard of HowNet categoriesrequires to map translations from the MT output tothe HowNet representation.
Some categories are an-notated with English translations, but additional ef-fort is required in order to cover all translation can-didates produced by the MT system.2.2 Controlled Learning ConditionsAnother advantage of the CLWSD task is that it pro-vides controlled learning conditions (even though itis an unsupervised task with no annotated trainingdata.)
The gold labels for CLWSD are learned fromparallel corpora.
As a result MT lexical choice mod-els can be estimated on the exact same data.
Trans-lations for English words in the lexical sample areextracted from a semi-automatic word alignment of2Target word ringEnglish context The twelve stars of the European flag are depicted on the outer ring.Gold translations anillo (3);c?
?rculo (2);corona (2);aro (1);English context The terrors which Mr Cash expresses about our future in the community have a familiar ringabout them.Gold translations sonar (3);tinte (3);connotacio?n(2);tono (1);English context The American containment ring around the Soviet bloc had been seriously breached only bythe Soviet acquisition of military facilities in Cuba.Gold translations cerco (2);c?
?rculo (2);cordo?n (2);barrera (1);blindaje (1);limitacio?n (1);Table 1: Example of annotated CLWSD instances from the SemEval 2010 test set.
For each gold Spanishtranslation, we are given the number of annotators who proposed it (out of 3 annotators.
)sentences from the Europarl parallel corpus (Koehn,2005).
These translations are then manually clus-tered into senses.
When constructing the gold an-notation, human annotators are given occurrences oftarget words in context.
For each occurrence, theyselect a sense cluster and provide all translationsfrom this cluster that are correct in this specific con-text.
Since three annotators contribute, each test oc-currence is therefore tagged with a set of translationsin another language, along with a frequency whichrepresents the number of annotators who selected it.A more detailed description of the annotation pro-cess can be found in (Lefever and Hoste, 2010).Again, this contrasts with our previous work onevaluating MT on a HowNet-based Chinese WSDtask, where Chinese sentences were manually anno-tated with HowNet senses which were completelyunrelated to the parallel corpus used for training theSMT system.
Using CLWSD as an evaluation of MTlexical choice solves this issue and provides con-trolled learning conditions.2.3 CLWSD evaluates the semantic adequacyof MT lexical choiceA key challenge in MT evaluation lies in decid-ing whether the meaning of the translation is cor-rect when it does not exactly match the referencetranslation.
METEOR uses WordNet synonyms andlearned paraphrases tables (Denkowski and Lavie,2010).
MEANT uses vector-space based lexicalsimilarity scores (Lo et al 2012).
While thesemethods lead to higher correlations with humanjudgements on average, they are not ideal for afine-grained evaluation of lexical choice: similar-ity scores are defined independently of context andmight give credit to incorrect translations (Carpuat etal., 2012).
In contrast, CLWSD solves this difficultproblem by providing all correct translation candi-dates in context according to several human anno-tators.
These multiple translations provide a morecomplete representation of the correct meaning ofeach occurrence of a word in context.The CLWSD annotation procedure is designedto easily let human annotators provide many cor-rect translation alternatives for a word.
Producingmany correct annotations for a complete sentence isa much more expensive undertaking: crowdsourc-ing can help alleviate the cost of obtaining a smallnumber of reference translation (Zbib et al 2012),but acquiring a complete representation of all pos-sible translations of a source sentence is a muchmore complex task (Dreyer and Marcu, 2012).
Ma-chine translation evaluations typically use betweenone and four reference translations, which providea very incomplete representation of the correct se-mantics of the input sentence in the output language.CLWSD provides a more complete representationthrough the multiple gold translations available.2.4 LimitationsThe main drawback of using CLWSD to evaluatelexical choice is that CLWSD is a lexical sampletask, which only evaluates disambiguation of 20 En-glish nouns.
This arbitrary sample of words does notlet us target words or phrases that might be specifi-cally interesting for MT.In addition, the data available through the sharedtask does not let us evaluate complete translationsof the CLWSD test sentences, since full referencestranslations are not available.
Instead of using3a WSD dataset for MT purposes, we could takethe converse approach andautomatically constructa WSD test set based on MT evaluation corpora(Vickrey et al 2005; Gime?nez and Ma`rquez, 2008;Carpuat and Wu, 2008; Carpuat et al 2012).
How-ever, this approach suffers from noisy automaticalignments between source and reference, as well asfrom a limited representation of the correct mean-ing of words in context due to the limited number ofreference translations.Other SemEval tasks such as the Cross-LingualLexical Substitution Task (Mihalcea et al 2010)would also provide an appropriate test bed.
Wefocused on the CLWSD task, since it uses sensesdrawn from the Europarl parallel corpus, and there-fore offers more constrained settings for comparisonbetween systems.
The lexical substitution task tar-gets verbs and adjectives in addition to nouns, andwould therefore be an interesting test case to con-sider in future work.2.5 Official and MT-centric Evaluation MetricsIn order to make comparison with other systems pos-sible, we follow the standard evaluation frameworkdefined for the task and score the output of all oursystems using four different metrics, computed us-ing the scoring tool made available by the organiz-ers.The difference between system predictions andgold standard annotations are quantified using pre-cision and recall scores1 , defined as follows.
Givena set T of test items and a set H of annotators, Hi isthe set of translation proposed by all annotators h forinstance i ?
T .
Each translation type res in Hi hasan associated frequency freqres, which representsthe number of human annotators which selected resas one of their top 3 translations.
Given a set of sys-tem answers A of items i ?
T such that the systemprovides at least one answer, ai : i ?
A is is the setof answers from the system for instance i.
For eachi, the scorer computes the intersection of the systemanswers ai and the gold standard Hi.Systems propose as many answers as deemed nec-1In this paper, we focus on evaluating translation systemswhose task is to produce a single complete translation for agiven sentence.
As a result, we only focus on the 1-best MToutput and do not report the relaxed out-of-five evaluation set-ting also considered in the official SemEval task.essary, but the scores are divided by the number ofguesses in order not to favor systems that outputmany answers per instance.Precision = 1|A|?ai:i?A?res?aifreqres|ai||Hi|Recall = 1|T |?ai:i?T?res?aifreqres|ai||Hi|We also report Mode Precision and Mode Recallscores: instead of comparing system answers to thefull set of gold standard translations Hi for an in-stance i ?
T , the Mode Precision and Recall scoresonly use a single gold translation, which is the trans-lation chosen most frequently by the human annota-tors.In addition, we compute the 1-gram precisioncomponent of the BLEU score (Papineni et al2002), denoted as BLEU1 in the result tables2.
Incontrast with the official CLWSD evaluation scoresdescribed above, BLEU1 gives equal weight to alltranslation candidates, which can be seen as multi-ple references.3 PBSMT systemWe use a typical phrase-based SMT system trainedfor English-to-Spanish translation.
Its applicationto the CLWSD task affects the selection of trainingdata and its preprocessing, but the SMT model de-sign and learning strategies are exactly the same asfor conventional translation tasks.3.1 ModelWe use the NRC?s PORTAGE phrase-based SMTsystem, which implements a standard phrasal beam-search decoder with cube pruning.
Translation hy-potheses are scored according to the following fea-tures:?
4 phrase-table scores: phrasal translation prob-abilites with Kneser-Ney smoothing and Zens-Ney lexical smoothing in both translation direc-tions (Chen et al 2011)?
6 hierarchical lexicalized reordering scores,which represent the orientation of the currentphrase with respect to the previous block thatcould have been translated as a single phrase(Galley and Manning, 2008)2even though it does not include the length penalty used inthe BLEU score.4?
a word penalty, which scores the length of theoutput sentence?
a word-displacement distortion penalty?
a Kneser-Ney smoothed 5-gram Spanish lan-guage modelWeights for these features are learned using a batchversion of the MIRA algorithm(Cherry and Foster,2012).
Phrase pairs are extracted from IBM4 align-ments obtained with GIZA++(Och and Ney, 2003).We learn phrase translation candidates for phrases oflength 1 to 7.Converting the PBSMT output for CLWSD re-quires a final straightforward mapping step.
Weuse the phrasal alignment between SMT input andoutput to isolate the translation candidates for theCLWSD target word.
When it maps to a multi-word phrase in the target language, we use the wordwithin the phrase that has the highest translationIBM1 translation probability given the CLWSD tar-get word of interest.
Note that there is no need toperform any manual mapping between SMT outputand sense inventories as in (Carpuat and Wu, 2005).3.2 DataThe core training corpus is the exact same set ofsentences from Europarl that were used to learnthe sense inventory, in order to ensure that PBSMTknows the same translations as the human annotatorswho built the gold standard.
There are about 900ksentence pairs, since only 1-to1 alignments that ex-ist in all the languages considered in CLWSD wereused (Lefever and Hoste, 2010).We exploit additional corpora from theWMT2012 translation task, using the full Eu-roparl corpus to train language models, and for oneexperiment the news-commentary parallel corpus(see Section 9.
)These parallel corpora are used to learn the trans-lation, reordering and language models.
The log-linear feature weights are learned on a developmentset of 3000 sentences sampled from the WMT2012development test sets.
They are selected based ontheir distance to the CLWSD trial and test sentences(Moore and Lewis, 2010).We tokenize and lemmatize all English andSpanish text using the FreeLing tools (Padro?
andStanilovsky, 2012).
We use lemma representationsto perform translation, since the CLWSD targets andtranslations are lemmatized.4 WSD system4.1 ModelWe also train a dedicated WSD system for this taskin order to perform a controlled comparison with theSMT system.
Many WSD systems have been eval-uated on the SemEval test bed used here, however,they differ in terms of resources used, training dataand preprocessing pipelines.
In order to control forthese parameters, we build a WSD system trainedon the exact same training corpus, preprocessing andword alignment as the SMT system described above.We cast WSD as a generic ranking problem withlinear models.
Given a word in context x, translationcandidates t are ranked according to the followingmodel: f(x, t) =?i ?i?i(x, t), where ?i(x, t) rep-resent binary features that fire when specific cluesare observed in a context x.Context clues are based on standard feature tem-plates in many supervised WSD approaches (Flo-rian et al 2002; van Gompel, 2010; Lefever et al2011):?
words in a window of 2 words around the dis-ambiguation target.?
part-of-speech tags in a window of 2 wordsaround the disambiguation target?
bag-of-word context: all nouns, verbs and ad-jectives in the context xAt training time, each example (x, t) is assigneda cost based on the translation observed in parallelcorpora: f(x, t) = 0 if t = taligned, f(x, t) = 1 oth-erwise .
Feature weights ?i can be learned in manyways.
We optimize logistic loss using stochastic gra-dient descent3.4.2 DataThe training instances for the supervised WSD sys-tem are built automatically by (1) extracting all oc-currences of English target words in context, and (2)annotating them with their aligned Spanish lemma.3we use the optimizer from http://hunch.net/?
vw v7.1.25Mode ModeSystem Prec.
Rec.
Prec.
Rec.
BLEU1WSD 25.96 25.58 55.02 54.13 76.06PBSMT 23.72 23.69 45.49 45.37 62.72MFStest 21.35 21.35 44.50 44.50 65.50MFStrain 19.14 19.14 42.00 42.00 59.70Table 2: Main CLWSD results: PBSMT yields com-petitive results, but WSD outperforms PBSMTWe obtain a total of 33139 training instances for alltargets (an average of 1656 per target, with a mini-mum of 30 and a maximum of 5414).
Note that thisprocess does not require any manual annotation.5 WSD systems can outperform PBSMTTable 2 summarizes the main results.
PBSMT out-performs the most frequent sense baseline by a widemargin, and interestingly also yields better resultsthan many of the dedicated WSD systems that par-ticipated in the SemEval task.
However, PBSMTperformance does not match that of the most fre-quent sense oracle (which uses sense frequencies ob-served in the test set rather than training set).
TheWSD system trained on the same word-aligned par-allel corpus as the PBSMT system achieves the bestperformance.
It also obtains better results than allbut the top system in the official results (Lefever andHoste, 2010).The results in Table 2 are quite different fromthose reported by Carpuat and Wu (2005) on a Chi-nese WSD task.
The Chinese-English PBSMT sys-tem performed much worse than any of the dedi-cated WSD systems on that task.
While our WSDsystem outperforms PBSMT on the CLWSD tasktoo, the difference is not as large, and the PBSMTsystem is competitive when compared to the full setof systems that were evaluated on this task.
Thisconfirms that the CLWSD task represents a more fairbenchmark for comparing PBSMT with WSD sys-tems.6 Impact of PBSMT Context ModelsWhat is the impact of PBSMT context modelson lexical choice accuracy?
Table 3 provides anoverview of experiments where we vary the contextsize available to the PBSMT system.
The main PB-Mode ModeSystem Prec.
Rec.
Prec.
Rec.
BLEU1PBSMT 23.72 23.69 45.49 45.37 62.72max source phrase length ll = 1 24.44 24.36 44.50 44.38 65.43l = 3 24.27 24.22 46.52 46.41 64.33n-gram LM ordern = 3 23.60 23.55 44.58 44.47 61.62n = 7 23.58 23.53 46.06 45.94 62.22n = 2 23.40 23.35 44.75 44.63 63.02n = 1 22.92 22.87 43.00 42.89 58.62+bilingual LM4-gram 23.89 23.84 45.49 45.37 62.62Table 3: Impact of source and target context modelson PBSMT performanceSMT system in the top row uses the default settingspresented in Section 3.In the first set of experiments, we evaluate theimpact of the source side context on CLWSD per-formance.
Phrasal translations represent the coreof PBSMT systems: they capture collocational con-text in the source language, and they are thereforeare less ambiguous than single words (Koehn andKnight, 2003; Koehn et al 2003).
The defaultPBSMT learns translations for sources phrases oflength ranging from 1 to 7 words.Limiting the PBSMT system to translate shorterphrases (Rows l = 1 and l = 3 in Table 3) surpris-ingly improves CLWSD performance, even thoughit degrades BLEU score on WMT test sets.
Thesource context captured by longer phrases thereforedoes not provide the right disambiguating informa-tion in this context.In the second set of experiments, we evaluate theimpact of the context size in the target language, byvarying the size of the n-gram language model used.The default PBSMT system used a 5-gram languagemodel.
Reducing the n-gram order to 3, 2, 1 and in-creasing it to 7 both degrade performance.
Shortern-grams do not provide enough disambiguating con-text, while longer n-grams are more sparse and per-haps do not generalize well outside of the trainingcorpus.Finally, we report a last experiment which uses abilingual language model to enrich the context rep-resentation in PBSMT (Niehues et al 2011).
Thislanguage model is estimated on word pairs formed6Mode ModeSystem Prec.
Rec.
Prec.
Rec.
BLEU1+ hier 23.72 23.69 45.49 45.37 62.72+ lex 23.69 23.64 46.66 46.54 62.22dist 23.42 23.37 45.43 45.30 62.22Table 4: Impact of reordering models: lexicalizedreodering does not hurt lexical choice only when hi-erarchical models are usedby target words augmented with their aligned sourcewords.
We use a 4-gram model, trained using Good-Turing discounting.
This only results in small im-provements (< 0.1) over the standard PBSMT sys-tem, and remains far below the performance of thededicated WSD system.These results show that source phrases are weakrepresentations of context for the purpose of lexicalchoice.
Target n?gram context is more useful thansource phrasal context, which can surprisingly harmlexical choice accuracy.7 Impact of PBSMT Reordering ModelsWhile the phrase-table is the core of PBSMT sys-tem, the reordering model used in our system isheavily lexicalized.
In this section, we evaluate itsimpact on CLWSD performance.
The standard PB-SMT system uses a hierarchical lexicalized reorder-ing model (Galley and Manning, 2008) in addition tothe distance-based distortion limit.
Unlike lexical-ized reordering(Koehn et al 2007), which modelsthe orientation of a phrase with respect to the pre-vious phrase, hierarchical reordering models definethe orientation of a phrase with respect to the previ-ous block that could have been translated as a singlephrase.In Table 4, we show that lexicalized reorderingmodel benefit CLWSD performance, and that the hi-erarchical model performs slightly better than thenon-hierarchical overall.8 Impact of phrase translation selectionIn this section, we consider the impact of variousmethods for selecting phrase translations on the lex-ical choice performance of PBSMT.First, we investigate the impact of limiting thenumber of translation candidates considered forMode ModeSystem Prec.
Rec.
Prec.
Rec.
BLEU1PBSMT 23.72 23.69 45.49 45.37 62.72Number t of translations per phraset = 20 23.68 23.63 45.66 45.54 62.32t = 100 23.65 23.60 45.65 45.53 62.52Other phrase-table pruning methodsstat sig 23.71 23.66 45.19 45.07 62.62Table 5: Impact of translation candidate selection onPBSMT performanceeach source phrase in the phrase-table.
The mainPBSMT system uses t = 50 translation candidatesper source phrase.
Limiting that number to 20 andincreasing it to 100 both have a very small impact onCLWSD.Second, we prune the phrase-table using a sta-tistical significance test to measure (Johnson et al2007).
This pruning strategy aims to drastically de-crease the size of the phrase-table without degradingtranslation performance by removing noisy phrasepairs.9 Impact of training corpusSince increasing the amount of training data is a re-liable way to improve translation performance, weevaluate the impact of training the PBSMT systemon more than the Europarl data used for controlledcomparison with WSD.
We increase the paralleltraining corpus with the WMT-12 News Commen-tary parallel data 4.
This yields an additional trainingset of roughly160k sentence pairs.
We build linearmixture models to combine translation, reorderingand language models learned on Europarl and NewsCommentary corpora (Foster and Kuhn, 2007).
Ascan be seen in Table 6, this approach improves allCLWSD scores except for 1-gram precision.
Thedecrease in 1-gram precision indicates that the addi-tion of the news corpus introduces new translationcandidates that differ from those used in the gold in-ventory.
Interestingly, the additional data is not suf-ficient to match the performance of the WSD systemlearned on Europarl only (see Table 2).
While ad-ditional data should be used when available, richercontext features are valuable to make the most ofexisting data.4http://www.statmt.org/wmt12/translation-task.html7Mode ModeSystem Prec.
Rec.
Prec.
Rec.
BLEU1Europarl 23.72 23.69 45.49 45.37 62.72+ News 24.34 24.28 47.49 47.37 61.22Table 6: Impact of training corpus on PBSMT per-formance: adding news parallel sentences helps Pre-cision and Recall, but does not match WSD on theEuroparl only.10 ConclusionWe use a SemEval Cross-Lingual WSD task toevaluate the lexical choice performance of a typi-cal phrase-based SMT system.
Unlike conventionalWSD task that rely on abstract sense inventoriesrather than translations, cross-lingual WSD providesa fair setting for comparing SMT with dedicatedWSD systems.
Unlike conventional evaluations ofmachine translation quality, the cross-lingual WSDtask lets us isolate a specific aspect of translationquality and show how it is affected by different com-ponents of the phrase-based SMT system.Unlike in previous evaluations on conventionalWSD tasks (Carpuat and Wu, 2005), phrase-basedSMT performance is on par with many dedicatedWSD systems.
However, the phrase-based SMTsystem does not perform as well as a WSD sys-tem trained on the exact same parallel data.
Anal-ysis shows that while many SMT components canpotentially have an impact on SMT lexical choice,CLWSD accuracy is most affected by the length ofsource phrases and order of target n-gram languagemodels.
Using shorter source phrases actually im-proves lexical choice accuracy.
The official resultsfor the CLWSD task at SemEval 2013 evaluationprovide further insights (Lefever and Hoste, 2013):our PBSMT system can achieve top precision asmeasured using the top prediction as in this paper,but does not perform as well as other submitted sys-tems when taking into account the top 5 predictions(Carpuat, 2013).
This suggests that local contextmodels based on source phrases and target n-gramsare much weaker representations of context than thesimple templates used by WSD systems, and thateven strong PBSMT systems can benefit from con-text models developed for WSD.New learning algorithms (Chiang et al 2009;Cherry and Foster, 2012, for instance) finally makeit possible for PBSMT to reliably learn from manymore features than the typical system used here.Evaluations such as the CLWSD task will provideuseful tools for analyzing the impact of these fea-tures on lexical choice and inform feature design inincreasingly large and complex systems.ReferencesE.
Agirre and P.G.
Edmonds.
2006.
Word Sense Dis-ambiguation: Algorithms and Applications.
Text,Speech, and Language Technology Series.
SpringerScience+Business Media B.V.Eneko Agirre, Llu?
?s Ma`rquez, and Richard Wicen-towski, editors.
2007.
Proceedings of SemEval-2007:4th International Workshop on Semantic Evaluation,Prague, July.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgement.
In Pro-ceedings of Workshop on Intrinsic and Extrinsic Eval-uation Measures for MT and/or Summarization at the43th Annual Meeting of the Association of Computa-tional Linguistics (ACL-2005), Ann Arbor, Michigan,June.Alexandra Birch, Mile Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: Evaluating reorder-ing.
Machine Translation, 24:15?26.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,WMT ?10, pages 17?53.Marine Carpuat and Dekai Wu.
2005.
Evaluating theWord Sense Disambiguation Performance of Statisti-cal Machine Translation.
In Proceedings of the SecondInternational Joint Conference on Natural LanguageProcessing (IJCNLP), pages 122?127, Jeju Island, Re-public of Korea.Marine Carpuat and Dekai Wu.
2008.
Evaluation ofContext-Dependent Phrasal Translation Lexicons forStatistical Machine Translation.
In Proceedings of thesixth conference on Language Resouces and Evalua-tion (LREC 2008), Marrakech, May.Marine Carpuat, Hal Daume?
III, Alexander Fraser, ChrisQuirk, Fabienne Braune, Ann Clifton, Ann Irvine,Jagadeesh Jagarlamudi, John Morgan, Majid Raz-mara, Ales?
Tamchyna, Katharine Henry, and Rachel8Rudinger.
2012.
Domain adaptation in machine trans-lation: Final report.
In 2012 Johns Hopkins SummerWorkshop Final Report.Marine Carpuat.
2013.
Nrc: A machine translationapproach to cross-lingual word sense disambiguation(SemEval-2013 Task 10).
In Proceedings of SemEval.Boxing Chen, Roland Kuhn, George Foster, and HowardJohnson.
2011.
Unpacking and transforming featurefunctions: New ways to smooth phrase tables.
In Pro-ceedings of Machine Translation Summit.Boxing Chen, Roland Kuhn, and Samuel Larkin.
2012.Port: a precision-order-recall mt evaluation metric fortuning.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, pages930?939.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 427?436, Montre?al, Canada, June.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In NAACL-HLT 2009: Proceedings of the 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 218?226, Boulder, Colorado.Timothy Chklovski, Rada Mihalcea, Ted Pedersen, andAmruta Purandare.
2004.
The Senseval-3 Multi-lingual English-Hindi lexical sample task.
In Pro-ceedings of Senseval-3, Third International Workshopon Evaluating Word Sense Disambiguation Systems,pages 5?8, Barcelona, Spain, July.Michael Denkowski and Alon Lavie.
2010.
METEOR-NEXT and the METEOR paraphrase tables: improvedevaluation support for five target languages.
In Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, WMT ?10, pages339?342.Zhendong Dong.
1998.
Knowledge description: what,how and who?
In Proceedings of International Sym-posium on Electronic Dictionary, Tokyo, Japan.Markus Dreyer and Daniel Marcu.
2012.
Hyter:Meaning-equivalent semantics for translation evalua-tion.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 162?171, Montre?al, Canada, June.Philip Edmonds and Scott Cotton.
2001.
Senseval-2:Overview.
In Proceedings of Senseval-2: Second In-ternational Workshop on Evaluating Word Snese Dis-ambiguation Systems, pages 1?5, Toulouse, France.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Radu Florian, Silviu Cucerzan, Charles Schafer, andDavid Yarowsky.
2002.
Combining classifiers forword sense disambiguation.
Natural Language Engi-neering, 8(4):327?241.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 848?856.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2007.
LinguisticFeatures for Automatic Evaluation of HeterogenousMT Systems.
In Proceedings of the Second Work-shop on Statistical Machine Translation, pages 256?264, PragueJesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
Discrimina-tive Phrase Selection for Statistical Machine Transla-tion.
Learning Machine Translation.
NIPS WorkshopSeries.Peng Jin, Yunfang Wu, and Shiwen Yu.
2007.
Semeval-2007 task 05: Multilingual chinese-english lexicalsample.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007),pages 19?23, Prague, Czech Republic, June.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving Translation Quality by Dis-carding Most of the Phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975.Philipp Koehn and Kevin Knight.
2003.
Feature-RichStatistical Translation of Noun Phrases.
In Proceed-ings of 41st Annual Meeting of the Association forComputational Linguistics, Sapporo, Japan, July.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.
Sta-tistical Phrase-based Translation.
In Proceedings ofHLT/NAACL-2003, Edmonton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In AnnualMeeting of the Association for Computational Linguis-tics (ACL), demonstration session, Prague, Czech Re-public, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Machine Transla-tion Summit X, Phuket, Thailand, September.9Els Lefever and Ve?ronique Hoste.
2010.
Semeval-2010task 3: Cross-lingual word sense disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 15?20, Uppsala, Sweden,July.Els Lefever and Ve?ronique Hoste.
2013.
Semeval-2013task 10: Cross-lingual word sense disambiguation.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), Atlanta, USA,May.Els Lefever, Ve?ronique Hoste, and Martine De Cock.2011.
Parasense or how to use parallel corpora forword sense disambiguation.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 317?322, Portland, Oregon, USA, June.Chi-kiu Lo and Dekai Wu.
2011.
Meant: an inexpen-sive, high-accuracy, semi-automatic metric for evalu-ating translation utility via semantic frames.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 220?229.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic MT evaluation.
InProceedings of the Seventh Workshop on StatisticalMachine Translation, pages 243?252.Rada Mihalcea and Phiip Edmonds, editors.
2004.
Pro-ceedings of Senseval-3: Third international Workshopon the Evaluation of Systems for the Semantic Analysisof Text, Barcelona, Spain.Rada Mihalcea, Timothy Chklovski, and Adam Killgar-iff.
2004.
The Senseval-3 English lexical sampletask.
In Proceedings of Senseval-3, Third Interna-tional Workshop on Evaluating Word Sense Disam-biguation Systems, pages 25?28, Barcelona, Spain,July.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010.SemEval-2010 Task 2: Cross-Lingual Lexical Substi-tution.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, pages 9?14, Uppsala,Sweden, July.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,USA.Jan Niehues, Teresa Herrmann, Stephan Vogel, and AlexWaibel.
2011.
Wider context by using bilingual lan-guage models in machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Trans-lation, WMT ?11, pages 198?206, Stroudsburg, PA,USA.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?52.Llu?
?s Padro?
and Evgeny Stanilovsky.
2012.
FreeLing3.0: Towards wider multilinguality.
In Proceedings ofthe Language Resources and Evaluation Conference(LREC 2012), Istanbul, Turkey, May.Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-ren Delfs, and Hao Trang Dang.
2001.
English tasks:All-words and verb lexical sample.
In Proceedings ofSenseval-2, Second International Workshop on Evalu-ating Word Sense Disambiguation Systems, pages 21?24, Toulouse, France, July.
SIGLEX, Association forComputational Linguistic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, Philadelphia, PA, July.Maarten van Gompel.
2010.
Uvt-wsd1: A cross-lingualword sense disambiguation system.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 238?241, Uppsala, Sweden, July.David Vickrey, Luke Biewald, Marc Teyssier, andDaphne Koller.
2005.
Word-Sense Disambigua-tion for Machine Translation.
In Joint HumanLanguage Technology conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP 2005), Vancouver.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Trans-lation Output.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC?06), pages 697?702, Genoa, Italy, May.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.2012.
Machine translation of arabic dialects.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACLHLT ?12, pages 49?59.10
