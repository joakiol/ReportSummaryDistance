First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85?89,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Disambiguation of Image CaptionsWesley May, Sanja Fidler, Afsaneh Fazly, Sven Dickinson, and Suzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, Ontario, Canada, M5S 3G4{wesley,fidler,afsaneh,sven,suzanne}@cs.toronto.eduAbstractGiven a set of images with related captions,our goal is to show how visual features canimprove the accuracy of unsupervised wordsense disambiguation when the textual con-text is very small, as this sort of data is com-mon in news and social media.
We extendprevious work in unsupervised text-only dis-ambiguation with methods that integrate textand images.
We construct a corpus by usingAmazon Mechanical Turk to caption sense-tagged images gathered from ImageNet.
Us-ing a Yarowsky-inspired algorithm, we showthat gains can be made over text-only disam-biguation, as well as multimodal approachessuch as Latent Dirichlet Allocation.1 IntroductionWe examine the problem of performing unsuper-vised word sense disambiguation (WSD) in situa-tions with little text, but where additional informa-tion is available in the form of an image.
Such situ-ations include captioned newswire photos, and pic-tures in social media where the textual context is of-ten no larger than a tweet.Unsupervised WSD has been shown to work verywell when the target word is embedded in a largeWe thank NSERC and U. Toronto for financial support.
Fi-dler and Dickinson were sponsored by the Army Research Lab-oratory and this research was accomplished in part under Co-operative Agreement Number W911NF-10-2-0060.
The viewsand conclusions contained in this document are those of the au-thors and should not be interpreted as representing the officialpolicies, either express or implied, of the Army Research Lab-oratory or the U.S. Government.Figure 1: ?The crane was so massive it blocked the sun.
?Which sense of crane?
With images the answer is clear.quantity of text (Yarowsky, 1995).
However, if theonly available text is ?The crane was so massive itblocked the sun?
(see Fig.
1), then text-only dis-ambiguation becomes much more difficult; a humancould do little more than guess.
But if an image isavailable, the intended sense is much clearer.
Wedevelop an unsupervised WSD algorithm based onYarowsky?s that uses words in a short caption alongwith ?visual words?
from the captioned image tochoose the best of two possible senses of an ambigu-ous keyword describing the content of the image.Language-vision integration is a quickly develop-ing field, and a number of researchers have exploredthe possibility of combining text and visual featuresin various multimodal tasks.
Leong and Mihal-cea (2011) explored semantic relatedness betweenwords and images to better exploit multimodal con-tent.
Jamieson et al (2009) and Feng and Lap-ata (2010) combined text and vision to perform ef-fective image annotation.
Barnard and colleagues(2003; 2005) showed that supervised WSD by couldbe improved with visual features.
Here we show thatunsupervised WSD can similarly be improved.
Lo-eff, Alm and Forsyth (2006) and Saenko and Darrell(2008) combined visual and textual information tosolve a related task, image sense disambiguation, in85an unsupervised fashion.
In Loeff et al?s work, littlegain was realized when visual features were addedto a great deal of text.
We show that these featureshave more utility with small textual contexts, andthat, when little text is available, our method is moresuitable than Saenko and Darrell?s.2 Our AlgorithmWe model our algorithm after Yarowsky?s (1995) al-gorithm for unsupervised WSD: Given a set of doc-uments that contain a certain ambiguous word, thegoal is to label each instance of that word as someparticular sense.
A seed set of collocations thatstrongly indicate one of the senses is initially used tolabel a subset of the data.
Yarowsky then finds newcollocations in the labelled data that are strongly as-sociated with one of the current labels and appliesthese to unlabelled data.
This process repeats iter-atively, building a decision list of collocations thatindicate a particular sense with a certain confidence.In our algorithm (Algorithm 1), we have a docu-ment collection D of images relevant to an ambigu-ous keyword k with senses s1 and s2 (though the al-gorithm is extensible to more than two senses).
Sucha collection might result from an internet imagesearch using an ambiguous word such as ?mouse?.Each Di is an image?caption pair repsented as abag-of-words that includes both lexical words fromthe caption, and ?visual words?
from the image.
Avisual word is simply an abstract representation thatdescribes a small portion of an image, such that sim-ilar portions in other images are represented by thesame visual word (see Section 3.2 for details).
Ourseed sets consist of the words in the definitions of s1and s2 from WordNet (Fellbaum, 1998).
Any docu-ment whose caption contains more words from onesense definition than the other is initially labelledwith that sense.
We then iterate between two stepsthat (i) find additional words associated with s1 ors2 in currently labelled data, and (ii) relabel all datausing the word sense associations discovered so far.We let V be the entire vocabulary of words acrossall documents.
We run experiements both with andwithout visual words, but when we use visual words,they are included in V .
In the first step, we com-pute a confidence Ci for each word Vi.
This con-fidence is a log-ratio of the probability of seeingVi in documents labelled as s1 as opposed to doc-uments labelled as s2.
That is, a positive Ci indi-cates greater association with s1, and vice versa.
Inthe second step we find, for each document Dj , theword Vi ?
Dj with the highest magnitude of Ci.
Ifthe magnitude of Ci is above a labelling threshold?c, then we label this document as s1 or s2 depend-ing on the sign of Ci.
Note that all old labels are dis-carded before this step, so labelled documents maybecome unlabelled, or even differently labelled, asthe algorithm progresses.Algorithm 1 Proposed AlgorithmD: set of documents D1 ... DdV : set of lexical and visual words V1 ... Vv in DCi: log-confidence Vi is sense 1 vs. sense 2S1 and S2: bag of dictionary words for each senseL1 and L2: documents labelled as sense 1 or 2for all Di do .
Initial labelling using seed setif |Di ?
S1| > |Di ?
S2| thenL1 ?
L1 ?
{Di}else if |Di ?
S1| < |Di ?
S2| thenL2 ?
L2 ?
{Di}end ifend forrepeatfor all i ?
1..v do .
Update word conf.Ci ?
log(P (Vi|L1)P (Vi|L2))end forL1 ?
?, L2 ?
?
.
Update document conf.for all Di do.
Find word with highest confidencem?
argmaxj?1..v,Vj?Di|Cj |if Cm > ?c thenL1 ?
L1 ?
{Di}else if Cm < ?
?c thenL2 ?
L2 ?
{Di}end ifend foruntil no change to L1 or L23 Creation of the DatasetWe require a collection of images with associatedcaptions.
We also require sense annotations forthe keyword for each image to use for evalua-tion.
Barnard and Johnson (2005) developed the86?Music is an importantmeans of expression formany teens.?
?Keeping your office sup-plies organized is easy, withthe right tools.?
?The internet has opened upthe world to people of allnationalities.?
?When there is no cheese Iwill take over the world.
?Figure 2: Example image-caption pairs from our dataset,for ?band?
(top) and ?mouse?
(bottom).ImCor dataset by associating images from the Coreldatabase with text from the SemCor corpus (Milleret al, 1993).
Loeff et al (2006) and Saenko andDarrell (2008) used Yahoo!
?s image search to gatherimages with their associated web pages.
While thesedatasets contain images paired with text, the textualcontexts are much larger than typical captions.3.1 Captioning ImagesTo develop a large set of sense-annotated image?caption pairs with a focus on caption-sized text, weturned to ImageNet (Deng et al, 2009).
ImageNet isa database of images that are each associated witha synset from WordNet.
Hundreds of images areavailable for each of a number of senses of a widevariety of common nouns.
To gather captions, weused Amazon Mechanical Turk to collect five sen-tences for each image.
We chose two word sensesfor each of 20 polysemous nouns and for each sensewe collected captions for 50 representative images.For each image we gathered five captions, for a to-tal of 10,000 captions.
As we have five captions foreach image, we split our data into five sets.
Each sethas the same images, but each image is paired witha different caption in each set.We specified to the Turkers that the sentencesshould be relevant to, but should not talk directlyabout, the image, as in ?In this picture there is ablue fish?, as such captions are very unnatural.
Truecaptions generally offer orthogonal information thatis not readily apparent from the image.
The key-word for each image (as specified by ImageNet) wasnot presented to the Turkers, so the captions do notnecessarily contain it.
Knowledge of the keyword ispresumed to be available to the algorithm in the formof an image tag, or filename, or the like.
We foundthat forcing a certain word to be included in the cap-tion also led to sentences that described the picturevery directly.
Sentences were required to be a leastten words long, and have acceptable grammar andspelling.
We remove stop words from the captionsand lemmatize the remaining words.
See Figure 2for some examples.3.2 Computing the Visual WordsWe compute visual words for each image with Ima-geNet?s feature extractor.
This extractor lays downa grid of overlapping squares onto the image andcomputes a SIFT descriptor (Lowe, 2004) for eachsquare.
Each descriptor is a vector that encodes theedge orientation information in a given square.
Thedescriptors are computed at three scales: 1x, 0.5xand 0.25x the original side lengths.
These vectorsare clustered with k-means into 1000 clusters, andthe labels of these clusters (arbitrary integers from 1to 1000) serve as our visual words.It is common for each image to have a ?vocab-ulary?
of over 300 distinct visual words, many ofwhich only occur once.
To denoise the visual data,we use only those visual words which account for atleast 1% of the total visual words for that image.4 Experiments and ResultsTo show that the addition of visual features improvesthe accuracy of sense disambiguation for image?caption pairs, we run our algorithm both with andwithout the visual features.
We also compare our re-sults to three different baseline methods: K-means(K-M), Latent Dirichlet Allocation (LDA) (Blei etal., 2003), and an unsupervised WSD algorithm(PBP) explained below.
We use accuracy to measureperformance as it is commonly used by the WSDcommunity (See Table 1).For K-means, we set k = 2 as we have two senses,and represent each document with a V -dimensional87Table 1: Results (Average accuracy across all five sets ofdata).
Bold indicates best performance for that word.Ours Ours K-M K-M LDA LDA PBPtext w/vis text w/vis text w/vis textband .80 .82 .66 .65 .64 .56 .73bank .77 .78 .71 .59 .52 .67 .62bass .94 .94 .90 .88 .61 .62 .49chip .90 .90 .73 .58 .57 .66 .75clip .70 .79 .65 .58 .48 .53 .65club .80 .84 .80 .81 .61 .73 .63court .79 .79 .61 .53 .62 .82 .57crane .62 .67 .76 .76 .52 .54 .66game .78 .78 .60 .66 .60 .66 .70hood .74 .73 .73 .70 .51 .45 .55jack .76 .74 .62 .53 .58 .66 .47key .81 .92 .79 .54 .57 .70 .50mold .67 .68 .59 .67 .57 .66 .54mouse .84 .84 .71 .62 .62 .69 .68plant .54 .54 .56 .53 .52 .50 .72press .60 .59 .60 .54 .58 .62 .48seal .70 .80 .61 .67 .55 .53 .62speaker .70 .69 .57 .53 .55 .62 .63squash .89 .95 .84 .92 .55 .67 .79track .78 .85 .71 .66 .51 .54 .69avg.
.76 .78 .69 .65 .56 .63 .62vector, where the ith element is the proportion ofword Vi in the document.
We run K-means both withand without visual features.For LDA, we use the dictionary sense model fromSaenko and Darrell (2008).
A topic model is learnedwhere the relatedness of a topic to a sense is basedon the probabilities of that topic generating the seedwords from its dictionary definitions.
Analogouslyto k-means, we learn a model for text alone, and amodel for text augmented with visual information.For unsupervised WSD (applied to text only),we use WordNet::SenseRelate::TargetWord, here-after PBP (Patwardhan et al, 2007), the highestscoring unsupervised lexical sample word sense dis-ambiguation algorithm at SemEval07 (Pradhan etal., 2007).
PBP treats the nearby words around thetarget word as a bag, and uses the WordNet hierar-chy to assign a similarity score between the possiblesenses of words in the context, and possible sensesof the target word.
As our captions are fairly short,we use the entire caption as context.The most important result is the gain in accuracyafter adding visual features.
While the average gainacross all words is slight, it is significant at p < 0.02(using a paired t-test).
For 12 of the 20 words, thevisual features improve performance, and in 6 ofthose, the improvement is 5?11%.For some words there is no significant improve-ment in accuracy, or even a slight decrease.
Withwords like ?bass?
or ?chip?
there is little room toimprove upon the text-only result.
For words like?plant?
or ?press?
it seems the text-only result is notstrong enough to help bootstrap the visual featuresin any useful way.
In other cases where little im-provement is seen, the problem may lie with highintra-class variation, as our visual words are not veryrobust features, or with a lack of orthogonality be-tween the lexical and visual information.Our algorithm also performs significantly betterthan the baseline measurements.
K-means performssurprisingly well compared to the other baselines,but seems unable to make much sense of the visualinformation present.
Saenko and Darrell?s (2008)LDA model makes substansial gains by using vi-sual features, but does not perform as well on thistask.
We suspect that a strict adherence to the seedwords may be to blame: while both this LDA modeland our algorithm use the same seed definitions ini-tially, our algorithm is free to change its mind aboutthe usefulness of the words in the definitions as itprogresses, whereas the LDA model has no suchcapacity.
Indeed, words that are intuitively non-discriminative, such as ?carry?, ?lack?, or ?late?, arenot uncommon in the definitions we use.5 Conclusion and Future WorkWe present an approach to unsupervised WSD thatworks jointly with the visual and textual domains.We showed that this multimodal approach makesgains over text-only disambiguation, and outper-forms previous approaches for WSD (both text-only,and multimodal), when textual contexts are limited.This project is still in progress, and there are manyavenues for further study.
We do not currently ex-ploit collocations between lexical and visual infor-mation.
Also, the bag-of-SIFT visual features thatwe use, while effective, have little semantic content.More structured representations over segmented im-age regions offer greater potential for encoding se-mantic content (Duygulu et al, 2002).88ReferencesKobus Barnard and Matthew Johnson.
2005.
Wordsense disambiguation with pictures.
In Artificial In-telligence, volume 167, pages 13?130.Kobus Barnard, Matthew Johnson, and David Forsyth.2003.
Word sense disambiguation with pictures.In Workshop on Learning Word Meaning from Non-Linguistic Data, Edmonton, Canada.David M. Blei, Andrew Ng, and Michael I. Jordan.
2003.Latent dirichlet alocation.
In JMLR, volume 3, pages993?1022.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei.
2009.
Imagenet: A large-scale hierar-chical image database.
In IEEE Conference on Com-puter Vision and Pattern Recognition.Pinar Duygulu, Kobus Barnard, Nando de Freitas, andDavid Forsyth.
2002.
Object recognition as machinetranslation: Learning a lexicon for a fixed image vo-cabulary.
In European Conference on Computer Vi-sion, Copenhagen, Denmark.Christiane Fellbaum.
1998.
Wordnet: An electronic lex-ical database.
In Bradford Books.Yansong Feng and Mirella Lapata.
2010.
Topic modelsfor image annotation and text illustration.
In AnnualConference of the North American Chapter of the ACL,pages 831?839, Los Angeles, California.Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson,Sven Dickinson, and Sven Wachsmuth.
2009.
Usinglanguage to learn structured appearance models for im-age annotation.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 32(1):148?164.Chee Wee Leong and Rada Mihalcea.
2011.
Measuringthe semantic relatedness between words and images.In International Conference on Semantic Computing,Oxford, UK.Nicolas Loeff, Cecilia Ovesdotter Alm, and DavidForsyth.
2006.
Discriminating image senses by clus-tering with multimodal features.
In Proceedings of theCOLING/ACL 2006 Main Conference Poster Sessions,pages 547?554, Sydney, Australia.David Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2):91?110.George Miller, Claudia Leacock, Randee Tengi, and RossBunker.
1993.
A semantic concordance.
In Proceed-ings of the 3rd DARPA Workshop on Human LanguageTechnology, pages 303?308.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2007.
UMND1: Unsupervised word sensedisambiguation using contextual semantic relatedness.In Proceedings of SemEval-2007, pages 390?393,Prague, Czech Republic.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Task 17: English lexical sam-ple, SRL and all words.
In Proceedings of SemEval-2007, pages 87?92, Prague, Czech Republic.Kate Saenko and Trevor Darrell.
2008.
Unsupervisedlearning of visual sense models for polysemous words.In Proceedings of Neural Information Processing Sys-tems, Vancouver, Canada.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the ACL, pages189?196, Cambridge, Massachusetts.89
