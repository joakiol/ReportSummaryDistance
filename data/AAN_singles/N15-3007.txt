Proceedings of NAACL-HLT 2015, pages 31?35,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsICE: Rapid Information Extraction Customization for NLP NovicesYifan He and Ralph GrishmanComputer Science DepartmentNew York UniversityNew York, NY 10003, USA{yhe,grishman}@cs.nyu.eduAbstractWe showcase ICE, an Integrated Customiza-tion Environment for Information Extraction.ICE is an easy tool for non-NLP experts torapidly build customized IE systems for a newdomain.1 IntroductionCreating an information extraction (IE) system for anew domain, with new vocabulary and new classesof entities and relations, remains a task requiringsubstantial time, training, and expense.
This hasbeen an obstacle to the wider use of IE technol-ogy.
The tools which have been developed for thistask typically do not take full advantage of linguis-tic analysis and available learning methods to pro-vide guidance to the user in building the IE system.They also generally require some understanding ofsystem internals and data representations.
We havecreated ICE [the Integrated Customization Environ-ment], which lowers the barriers to IE system devel-opment by providing guidance while letting the userretain control, and by allowing the user to interact interms of the words and phrases of the domain, witha minimum of formal notation.In this paper, we review related systems and ex-plain the technologies behind ICE.
The code, docu-mentation, and a demo video of ICE can be found athttp://nlp.cs.nyu.edu/ice/2 Related WorkSeveral groups have developed integrated systemsfor IE development:The extreme extraction system from BBN(Freedman et al, 2011) is similar in several regards:it is based on an extraction system initially devel-oped for ACE1, allows for the customization of enti-ties and relations, and uses bootstrapping and activelearning.
However, in contrast to our system, it isaimed at skilled computational linguists.The Language Computer Corporation has de-scribed several tools developed to rapidly extendan IE system to a new task (Lehmann et al, 2010;Surdeanu and Harabagiu, 2002).
Here too the em-phasis is on tools for use by experienced IE systemdevelopers.
Events and relations are recognized us-ing finite-state rules, with meta-rules to efficientlycapture syntactic variants and a provision for super-vised learning of rules from annotated corpora.A few groups have focused on use by NLPnovices:The WIZIE system from IBM Research(Li et al, 2012) is based on a finite-state rulelanguage.
Users prepare some sample annotatedtexts and are then guided in preparing an extractionplan (sequences of rule applications) and in writingthe individual rules.
IE development is seen as arule programming task.
This offers less in the wayof linguistic support (corpus analysis, syntacticanalysis) but can provide greater flexibility forextraction tasks where linguistic models are a poorfit.The SPIED system (Gupta and Manning, 2014)focuses on extracting lexical patterns for entityrecognition in an interactive fashion.
Our system, on1https://www.ldc.upenn.edu/collaborations/past-projects/ace31the other hand, aims at extracting both entities andrelations.
Furthermore, SPIED produces token se-quence rules, while our system helps the user to con-struct lexico-syntactic extraction rules that are basedon dependency paths.The PROPMINER system from T. U. Berlin(Akbik et al, 2013) takes an approach more similarto our own.
In particular, it is based on a depen-dency analysis of the text corpus and emphasizes ex-ploratory development of the IE system, supportedby search operations over the dependency structures.However, the responsibility for generalizing initialpatterns lies primarily with the user, whereas wesupport the generalization process through distribu-tional analysis.1.
Preprocessing2.
Key phraseextraction3.
Entity setconstruction4.
Dependencypaths extraction5.
Relation patternbootstrappingText extractionTokenizationPOS TaggingDEP ParsingNE TaggingCoref ResolutionKeyphraseIndexEntitySetsPathIndexRelationExtractorCorpusin newdomainProcessedcorpus ingeneraldomainProcessedcorpus innewdomainFigure 1: System architecture of ICE3 System Description3.1 Overall architectureThe information to be extracted by the IE systemconsists of user-specified types of entities and user-specified types of relations connecting these entities.Standard types of entities (people, organizations, lo-cations, etc.)
are built in; new entity types are de-fined extensionally as lists of terms.
Relation typesare captured in the form of sets of lexicalized depen-dency paths, discussed in more detail below.For NLP novices, it is much easier to provide ex-amples for what they want and make binary choices,than to come up with linguistic rules or compre-hensive lists.
ICE therefore guides users through aseries of linguistic processing steps, presents themwith entities and dependency relations that are po-tential seeds, and helps them to expand the seeds byanswering yes/no questions.Figure 1 illustrates the five steps of ICE process-ing: given a new corpus, preprocessing builds acache of analyzed documents to speed up furtherprocessing; key phrase extraction and entity set con-struction build new entity types; dependency pathextraction and relation pattern bootstrapping buildnew semantic relations.3.2 PreprocessingWe rely on distributional analysis to collect entitysets and relation patterns on a new domain.
Ef-fective distributional analysis requires features fromdeep linguistic analysis that are too time-consumingto perform more than once.
ICE therefore al-ways preprocesses a new corpus with the Jet NLPpipeline2when it is first added, and saves POS tags,noun chunks, dependency relations between tokens,types and extents of named-entities, and coreferencechains to a cache.
After preprocessing, each of thefollowing steps can be completed within minutes ona corpus of thousands of documents, saving the timeof the domain expert user.3.3 Key phrase extractionIn ICE, key phrases of a corpus are either nounsor multi-word terms.
We extract multi-word termsfrom noun chunks: if a noun chunk has N adjec-tives and nouns preceding the head noun, we obtainN + 1 multi-word term candidates consisting of thehead noun and its preceding i (0 ?
i ?
N ) nounsand adjectives.We count the absolute frequency of the nouns andmulti-word terms and rank them with a ratio score,which is the relative frequency compared to a gen-eral corpus.
We use the ratio score Stto measure therepresentativeness of term t with regard to the givendomain, as defined in Eq (1).St=#pos(t) ?
log?
(#pos(t))#neg(t)(1)where #pos(t) is the number of occurrences of termt in the in-domain corpus, #neg(t) is the number of2http://cs.nyu.edu/grishman/jet/jet.html32occurrences of term t in the general corpus, and ?
isa user-defined parameter to favor either common orrare words, default to 0.We present the user with a ranked list, wherewords or multi-word terms that appear more often inthe in-domain corpus than in general language willrank higher.3.4 Entity set constructionICE constructs entity sets from seeds.
Seeds are en-tities that are representative of a type: if we want toconstruct a DRUGS type, ?methamphetamine?
and?oxycodone?
can be possible seeds.
Seeds are pro-vided by the user (normally from the top scoringterms), but if the user is uncertain, ICE can recom-mend seeds automatically, using a clustering-basedheuristic.3.4.1 Entity set expansionGiven a seed set, we compute the distributionalsimilarity of all terms in the corpus with the cen-troid of the seeds, using the dependency analysis asthe basis for computing term contexts.
We repre-sent each term with a vector that encodes its syn-tactic context, which is the label of the dependencyrelation attached to the term in conjunction with theterm?s governor or dependent in that relation.Consider the entity set of DRUGS.
Drugs oftenappear in the dependency relations dobj(sell, drug)and dobj(transport, drug) (where dobj is the directobject relation), thus members in the DRUGS set willshare the features dobj sell and dobj transport.
Weuse pointwise mutual information (PMI) to weightthe feature vectors and use a cosine metric to mea-sure the similarity between two term vectors.The terms are displayed as a ranked list, and theuser can accept or reject individual members of theentity set.
At any point the user can recompute thesimilarities and rerank the list (where the rankingis based the centroids of the accepted and rejectedterms, following (Min and Grishman, 2011)).
Whenthe user is satisfied, the set of accepted terms will be-come a new semantic type for tagging further text.3.5 Dependency path extraction andlinearizationICE captures the semantic relation (if any) be-tween two entity mentions by the lexicalizedParkeroversawbusinessdistributioncrack cocainePERSONDRUGSa sophisticatednsubj dobjnnnnFigure 2: A parse tree; dotted relations ignored by LDPdependency path (LDP) and the semantic typesof the two entities.
LDP includes both the la-bels of the dependency arcs and the lemmatizedform of the lexical items along the path.
Forexample, for the sentence ?
[Parker] oversaw asophisticated [crack cocaine] distribution busi-ness.
?, consider the parse tree in Figure 2.
Thepath from ?Parker?
to ?crack cocaine?
would bensubj?1:oversee:dobj:business:nn:distribution:nn,where the?1indicates that the nsubj arc is beingtraversed from dependent to governor.
The deter-miner ?a?
and the adjective modifier ?sophisticated?are dropped in the process, making the LDP moregeneralized than token sequence patterns.We linearize LDPs before presenting them to theuser to keep the learning curve gentle for NLPnovices: given an LDP and the sentence from whichit is extracted, we only keep the word in the sentenceif it is the head word of the entity or it is on the LDP.The linearized LDP for the path in Figure 2 , ?PER-SON oversee DRUGS distribution business?, is morereadable than the LDP itself.3.6 Bootstrapping relation extractors3.6.1 Relation extractorICE builds two types of dependency-path basedrelation extractors.
Given two entities and an LDPbetween them, the exact extractor extracts a relationbetween two entities if the types of the two enti-ties match the types required by the relation, and thewords on the candidate LDP match the words on anextraction rule.
When the two nodes are linked byan arc in the dependency graph (i.e.
no word but atype label on the LDP), we require the dependencylabel to match.ICE also builds a fuzzy extractor that calculatesedit distance (normalized by the length of the rule)between the candidate LDP and the rules in the ruleset.
It extracts a relation if the minimum edit dis-33tance between the candidate LDP and the rule setfalls below a certain threshold (0.5 in ICE).
We tunethe edit costs on a development set, and use insertioncost 0.3, deletion cost 1.2, and substitution cost 0.8.Fuzzy extractors with large rule sets tend to pro-duce false positive relations.
ICE therefore boot-straps both positive and negative rules, and requiresthat the candidate LDP should be closer to (the clos-est element in) the positive rule set than to the neg-ative rule set, in order to be extracted by the fuzzyLDP matcher.3.6.2 BootstrapperThe learner follows the style of Snowball(Agichtein and Gravano, 2000), with two key dif-ferences: it bootstraps both positive and negativerules, and performs additional filtering of the top k(k = 20 in ICE) candidates to ensure diversity.Starting with a seed LDP, the learner gathers allthe pairs of arguments (endpoints) which appearwith this LDP in the corpus.
It then collects all otherLDPs which connect any of these pairs in the corpus,and presents these LDPs to the user for assessment.If the set of argument pairs connected by any of theseeds is S and the set of argument pairs of a candi-date LDP x is X , the candidate LDPs are ranked by| S ?
X | / | X |, so that LDPs most distribution-ally similar to the seed set are ranked highest.
Thelinearized LDPs which are accepted by the user asalternative expressions of the semantic relation areadded to the seed set.
At any point the user can ter-minate the bootstrapping and accept the set of LDPsas a model of the relation.Bidirectional bootstrapping.
If the user explic-itly rejects a path, but it is similar to a path in theseed set, we still bootstrap from the arg pairs of thispath.
We save all the paths rejected by the user asthe negative rule set.Diversity-based filtering.
When presenting thebootstrapped LDPs, we require paths presented inthe first ICE screen (top 20 candidates) to be distantenough from each other.4 ExperimentsWe perform end-to-end relation extraction experi-ments to evaluate the utility of ICE: we start fromSELL RESIDENT-OFP R F P R FFuzzy 0.60 0.22 0.32 0.68 0.51 0.58-neg 0.59 0.22 0.32 0.55 0.51 0.53Exact 0.92 0.10 0.18 0.72 0.47 0.57Table 1: End-to-end relation extraction using small rulesets.
Fuzzy: fuzzy match relation extractor with negativerule set; -neg: fuzzymatch extractor without negative ruleset; Exact: exact match extractor; P / R / F: Precision /Recall / F-scoreSELL RESIDENT-OFP R F P R FFuzzy 0.46 0.36 0.40 0.56 0.53 0.55-neg 0.31 0.38 0.34 0.30 0.56 0.39Exact 0.76 0.20 0.32 0.75 0.53 0.62Table 2: End-to-end relation extraction using large rulesets.
Same configurations as Table 1plain text, extract named entities, and finally ex-tract drug names and relations with models builtby ICE.
We collect approximately 5,000 web newsposts from the U.S. Drug Enforcement Administra-tion3(DEA) for our experiments.Entity Set Construction.
In our first experiment,we extracted 3,703 terms from this corpus and man-ually identified 119 DRUGS names and 97 law en-forcement agent (AGENTS) mentions, which we useas the ?gold standard?
sets.
We then ran our cus-tomizer in the following manner: 1) we providedthe entity set expansion program with two seeds(?methamphetamine?
and ?oxycodone?
for DRUGS;?special agents?
and ?law enforcement officers?
forAGENTS); 2) the program produced a ranked list ofterms; 3) in each iteration, we examined the top 20terms that had not been examined in previous iter-ations; 4) if a term is in the gold standard set, weadded it to the expander as a positive seed, other-wise, we added it as a negative seed; 5) we continuedthe expansion with the updated seed set, repeatingthe process for 10 iterations.
This process producedhigh-recall dictionary-based entity taggers (74% for3http://www.justice.gov/dea/index.shtml34drugs, 82% for agents) in just a few minutes.Relation Extraction.
With the ICE-built DRUGSdictionary, we performed end-to-end extraction oftwo relations: SELL, in which a PERSON sellsDRUGS, using ?PERSON sell DRUGS?
as seed, andRESIDENT-OF, which indicates that a PERSON re-sides in a GPE4, using ?PERSON of GPE?
as seed.We manually annotated 51 documents from theDEA collection.
There are 110 SELL relations and45 RESIDENT-OF relations in the annotated corpus.We first extracted small rule sets.
For both rela-tions, we asked a user to review the presented LDPson the first screen (20 LDPs in total) and then ranbootstrapping using the expanded seeds.
We didthis for 3 iterations, so the user evaluated 60 LDPs,which took less than half an hour.
We report theresults in Table 1.
Note that these are end-to-endscores, reflecting in part errors of entity extraction.After entity tagging and coreference resolution, therecall of entity mentions is 0.76 in our experiments.We observe that fuzzy LDP match with negativerule sets obtains best results for both relations.
Ifwe remove the negative rule set, the precision ofRESIDENT-OF is hurt more severely than the SELLrelations.
On the other hand, if we require exactmatch, the recall of SELL will decrease very signif-icantly.
This discrepancy in performance is due tothe nature of the two relations.
RESIDENT-OF is arelatively closed binary relation, with fewer lexicalvariations: the small RESIDENT-OF model coversaround 50% of the relation mentions with 7 positiveLDPs, so it is easier to rule out false positives thanto further boost recall.
SELL, in contrast, can beexpressed in many different ways, and fuzzy LDPmatch is essential for reasonable recall.We report experimental results on larger rule setsin Table 2.
The large rule sets were bootstrapped in3 iterations as well, but the user reviewed 250 LDPsin each iteration.
The best score in this setting im-proves to 0.4 F-score for SELL and 0.62 F-score forRESIDENT-OF, as we have more LDP rules.
Theexact match extractor performs better than the fuzzymatch extractor for RESIDENT-OF, as the latter ishurt by false positives.4Geo-political entity, or GPE, is an entity type defined inAce, meaning location with a government5 Conclusion and Future WorkWe described ICE, an integrated customization en-vironment for information extraction customizationand evaluated its end-to-end performance.
We planto explore more expressive models than LDP thatcan handle arbitrary number of arguments, whichwill enable ICE to build event extractors.AcknowledgementsWe thank Lisheng Fu, Angus Grieve-Smith, and ThienHuu Nguyen for discussions.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM Conference on Digi-tal Libraries, pages 85?94.Alan Akbik, Oresti Konomi, Michail Melnikov, et al2013.
Propminer: A workflow for interactive infor-mation extraction and exploration using dependencytrees.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics: Sys-tems Demonstrations, pages 157?162.Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee,Ryan Gabbard, Gary Kratkiewicz, Nicolas Ward, andRalph Weischedel.
2011.
Extreme extraction: ma-chine reading in a week.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1437?1446.Sonal Gupta and Christopher Manning.
2014.
SPIED:Stanford pattern based information extraction and di-agnostics.
In Proceedings of the Workshop on Interac-tive Language Learning, Visualization, and Interfaces,pages 38?44.John Lehmann, Sean Monahan, Luke Nezda, ArnoldJung, and Ying Shi.
2010.
LCC approaches to knowl-edge base population at tac 2010.
In Proc.
TAC 2010Workshop.Yunyao Li, Laura Chiticariu, Huahai Yang, Frederick RReiss, and Arnaldo Carreno-Fuentes.
2012.
WizIE: abest practices guided development environment for in-formation extraction.
In Proceedings of the ACL 2012System Demonstrations, pages 109?114.Bonan Min and Ralph Grishman.
2011.
Fine-grainedentity refinement with user feedback.
In Proceedingsof RANLP 2011 Workshop on Information Extractionand Knowledge Acquisition.Mihai Surdeanu and Sanda M. Harabagiu.
2002.
Infras-tructure for open-domain information extraction.
InProceedings of the second international conference onHuman Language Technology Research, pages 325?330.35
