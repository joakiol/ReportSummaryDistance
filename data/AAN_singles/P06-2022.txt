Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 168?175,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatically Extracting Nominal Mentions of Events with aBootstrapped Probabilistic Classifier?Cassandre Creswell?
and Matthew J. Beal?
and John Chen?Thomas L. Cornell?
and Lars Nilsson?
and Rohini K.
Srihari??
?Janya, Inc.1408 Sweet Home Road, Suite 1Amherst NY 14228{ccreswell,jchen,cornell,lars,rohini}@janyainc.com?Dept.
of Computer Science and EngineeringUniversity at BuffaloThe State University of New YorkAmherst NY 14260mbeal@cse.buffalo.eduAbstractMost approaches to event extraction focuson mentions anchored in verbs.
However,many mentions of events surface as nounphrases.
Detecting them can increase therecall of event extraction and provide thefoundation for detecting relations betweenevents.
This paper describes a weakly-supervised method for detecting nominalevent mentions that combines techniquesfrom word sense disambiguation (WSD)and lexical acquisition to create a classifierthat labels noun phrases as denoting eventsor non-events.
The classifier uses boot-strapped probabilistic generative modelsof the contexts of events and non-events.The contexts are the lexically-anchored se-mantic dependency relations that the NPsappear in.
Our method dramatically im-proves with bootstrapping, and comfort-ably outperforms lexical lookup methodswhich are based on very much larger hand-crafted resources.1 IntroductionThe goal of information extraction is to generatea set of abstract information objects that repre-sent the entities, events, and relations of particu-lar types mentioned in unstructured text.
For ex-ample, in a judicial domain, relevant event typesmight be ARREST, CHARGING, TRIAL, etc.Although event extraction techniques usuallyfocus on extracting mentions textually anchoredby verb phrases or clauses, e.g.
(Aone and Ramos-?
This work was supported in part by SBIR grantFA8750-05-C-0187 from the Air Force Research Laboratory(AFRL)/IFED.Santacruz, 2000), many event mentions, espe-cially subsequent mentions of events that are theprimary topic of a document, are referred to withnominals.
Because of this, detecting nominalevent mentions, like those in (1), can increase therecall of event extraction systems, in particular forthe most important events in a document.1(1) The slain journalist was a main organizer of the mas-sive demonstrations that forced Syria to withdraw itstroops from Lebanon last April, after Assad was widelyaccused of planningHariri?s assassination in a Febru-ary car bombing that was similar to today?s blast.Detecting event nominals is also an importantstep in detecting relations between event men-tions, as in the causal relation between the demon-strations and the withdrawal and the similarity re-lation between the bombing and the blast in (1).Finally, detecting nominal events can improvedetection and coreference of non-named mentionsof non-event entities (e.g.
persons, locations, andorganizations) by removing event nominals fromconsideration as mentions of entities.Current extraction techniques for verbally-anchored events rest on the assumption that mostverb phrases denote eventualities.
A system to ex-tract untyped event mentions can output all con-stituents headed by a non-auxiliary verb with afilter to remove instances of to be, to seem, etc.A statistical or rule-based classifier designed todetect event mentions of specific types can thenbe applied to filter these remaining instances.Noun phrases, in contrast, can be used to denoteanything?eventualities, entities, abstractions, andonly some are suitable for event-type filtering.1For example, in the 2005 Automatic Content Extractiontraining data, of the 5,349 event mentions, over 35% (1934)were nominals.1681.1 Challenges of nominal event detectionExtraction of nominal mentions of events encom-passes many of the fundamental challenges ofnatural language processing.
Creating a generalpurpose lexicon of all potentially event-denotingterms in a language is a labor-intensive task.
Ontop of this, even utilizing an existing lexical re-source like WordNet requires sense disambigua-tion at run-time because event nominals displaythe full spectrum of sense distinction behaviors(Copestake and Briscoe, 1995), including idiosyn-cratic polysemy, as in (2); constructional poly-semy, as in (3); coactivation, (4); and copredica-tion, as in (5).
(2) a.
On May 30 a group of Iranian mountaineers hoistedthe Iranian tricolor on the summit.b.
EU Leaders are arriving here for their two-daysummit beginning Thursday.
(3) Things are getting back to normal in the Baywood GolfClub after a chemical spill[=event].
Clean-up crewssaid the chemical spill[=result] was 99 percent waterand shouldn?t cause harm to area residents.
(4) Managing partner Naimoli said he wasn?t concernedabout recent media criticism.
(5) The construction lasted 30 years and was inauguratedin the presence of the king in June 1684.Given the breadth of lexical sense phenom-ena possible with event nominals, no existing ap-proach can address all aspects.
Lexical lookup,whether using a manually- or automatically-constructed resource, does not take context intoconsideration and so does not allow for vaguenessor unknown words.
Purely word-cooccurrence-based approaches (e.g.
(Schu?tze, 1998)) are un-suitable for cases like (3) where both senses arepossible in a single discourse.
Furthermore, mostWSD techniques, whether supervised or unsuper-vised, must be retrained for each individual lexicalitem, a computationally expensive procedure bothat training and run time.
To address these limita-tions, we have developed a technique which com-bines automatic lexical acquisition and sense dis-ambiguation into a single-pass weakly-supervisedalgorithm for detecting event nominals.The remainder of this paper is organized as fol-lows: Section 2 describes our probabilistic clas-sifier.
Section 3 presents experimental results ofthis model, assesses its performance when boot-strapped to increase its coverage, and compares itto a lexical lookup technique.
We describe relatedwork in Section 4 and present conclusions and im-plications for future work in Section 5.2 Weakly-supervised, simultaneouslexical acquisition and disambiguationIn this section we present a computational methodthat learns the distribution of context patterns thatcorrelate with event vs. non-event mentions basedon unambiguous seeds.
Using these seeds webuild two Bayesian probabilistic generative mod-els of the data, one for non-event nominals and theother for event nominals.
A classifier is then con-structed by comparing the probability of a candi-date instance under each model, with the winningmodel determining the classification.
In Section 3we show that this classifier?s coverage can be in-creased beyond the initial labeled seed set by au-tomatically selecting additional seeds from a verylarge unlabeled, parsed corpus.The technique proceeds as follows.
First, twolexicons of seed terms are created by hand.
Onelexicon includes nominal terms that are highlylikely to unambiguously denote events; the otherincludes nominal terms that are highly likely tounambiguously denote anything other than events.Then, a very large corpus (>150K documents) isparsed using a broad-coverage dependency parserto extract all instantiations of a core set of seman-tic dependency relations, including verb-logicalsubject, verb-logical object, subject-nominal pred-icate, noun phrase-appositive-modifier, etc.Format of data: Each instantiation is in theform of a dependency triple, (wa, R,wb), whereR is the relation type and where each argument isrepresented just by its syntactic head, wn.
Eachpartial instantiation of the relation?i.e.
either waor wb is treated as a wild card ?
that can be filledby any term?becomes a feature in the model.
Forevery common noun term in the corpus that ap-pears with at least one feature (including each en-try in the seed lexicons), the times it appears witheach feature are tabulated and stored in a matrixof counts.
Each column of the matrix representsa feature, e.g.
(occur,Verb-Subj, ?
); each row rep-resents an individual term,2 e.g.
murder; and eachentry is the number of times a term appeared withthe feature in the corpus, i.e.
as the instantiation of?.
For each row, if the corresponding term appearsin a lexicon it is given that designation, i.e.
EVENTor NONEVENT, or if it does not appear in eitherlexicon, it is left unlabeled.2A term is any common noun whether it is a single ormultiword expression.169Probabilistic model: Here we present the de-tails of the EVENT model?the computations forthe NONEVENT model are identical.
The probabil-ity model is built using a set of seed words labeledas EVENTs and is designed to address two desider-ata: (I) the EVENT model should assign high prob-ability to an unlabeled vector, v, if its features (asrecorded in the count matrix) are similar to thevectors of the EVENT seeds; (II) each seed terms should contribute to the model in proportion toits prevalence in the training data.3 These desider-ata can be incorporated naturally into a mixturemodel formalism, where there are as many com-ponents in the mixture model as there are EVENTseed terms.
Desideratum (I) is addressed by hav-ing each component of the mixture model assign-ing a multinomial probability to the vector, v. Forthe ith mixture component built around the ithseed, s(i), the probability isp(v|s(i)) =F?f=1(s(i)f)vf,where s(i)f is defined as the proportion of the timesthe seed was seen with feature f compared to thenumber of times the seed was seen with any fea-ture f ?
?
F .
Thus s(i)f is simply the (i, f)th entryin a row-sum normalized count matrix,s(i)f =s(i)f?Ff ?=1 s(i)f ?.Desideratum (II) is realized using a mixture den-sity by forming a weighted mixture of the abovemultinomial distributions from all the providedseeds i ?
E .
The weighting of the ith compo-nent is fixed to be the ratio of the number of oc-currences of the ith EVENT seed, denoted |s(i)|, tothe total number of all occurrences of event seedwords.
This gives more weight to more prevalentseed words:p(s(i)) =|s(i)|?i?
?E |s(i?
)|.The EVENT generative probability is then:p(v|EVENT) =?i?E[p(s(i)) ?
p(v|s(i))].An example of the calculation for a model withjust two event seeds and three features is given inFigure 1.
A second model is built from the non-3The counts used here are the number of times a term isseen with any feature in the training corpus because the in-dexing tool used to calculate counts does not keep track ofwhich instances appeared simultaneously with more than onefeature.
We do not expect this artifact to dramatically changethe relative seed frequencies in our model.f1 f2 f3event seed vector s(1) 3 1 8event seed vector s(2) 4 6 1unlabeled mention vector v 2 0 7p(v|event) =1223??312?2?
112?0?
812?7+1123??411?2?
611?0?
111?7= 0.0019Figure 1: Example of calculating the probability of unla-beled instance v under the event distribution composed oftwo event seeds s(1) and s(2).event seeds as well, and a corresponding probabil-ity p(v|NONEVENT) is computed.
The followingdifference (log odds-ratio)d(v) = log p(v|EVENT) ?
log p(v|NONEVENT)is then calculated.
An instance v encoded as thevector v is labeled as EVENT or NONEVENT byexamining the sign of d(v).
A positive differenced(v) classifies v as EVENT; a negative value ofd(v) classifies v as NONEVENT.
Should d=0 theclassifier is considered undecided and abstains.Each test instance is composed of a term andthe dependency triples it appears with in contextin the test document.
Therefore, an instance canbe classified by (i: word): Find the unlabeled fea-ture vector in the training data corresponding tothe term and apply the classifier to that vector,i.e.
classify the instance based on the term?s be-havior summed across many occurrences in thetraining corpus; (ii: context): Classify the instancebased only on its immediate test context vector; or(iii: word+context): For each model, multiply theprobability information from the word vector (=i)and the context vector (=ii).
In our experiments,all terms in the test corpus appeared at least once(80% appearing at least 500 times) in the trainingcorpus, so there were no cases of unseen terms?not suprising with a training set 1,800 times largerthan the test set.
However, the ability to labelan instance based only on its immediate contextmeans that there is a backoff method in the case ofunseen terms.3 Experimental Results3.1 Training, test, and seed word dataIn order to train and test the model, we createdtwo corpora and a lexicon of event and non-eventseeds.
The training corpus consisted of 156,000newswire documents, ?100 million words, fromthe Foreign Broadcast Information Service, Lexis170Nexis, and other online news archives.
The cor-pus was parsed using Janya?s information extrac-tion application, Semantex, which creates bothshallow, non-recursive parsing structures and de-pendency links, and all (wi, R,wj) statistics wereextracted as described in Section 2.
From the1.9 million patterns, (wi, R, ?)
and (?, R,wj) ex-tracted from the corpus, the 48,353 that appearedmore than 300 times were retained as features.The test corpus was composed of 77 additionaldocuments (?56K words), overlapping in timeand content but not included in the training set.These were annotated by hand to mark event nom-inals.
Specifically, every referential noun phraseheaded by a non-proper noun was consideredfor whether it denoted an achievement, accom-plishment, activity, or process (Parsons, 1990).Noun heads denoting any of these were markedas EVENT, and all others were left unmarked.All documents were first marked by a junior an-notator, and then a non-blind second pass was per-formed by a senior annotator (first author).
Sev-eral semantic classes were difficult to annotate be-cause they are particularly prone to coactivation,including terms denoting financial acts, legal acts,speech acts, and economic processes.
In addition,for terms like mission, plan, duty, tactic, policy,it can be unclear whether they are hyponyms ofEVENT or another abstract concept.
In every case,however, the mention was labeled as an event ornon-event depending on whether its use in thatcontext appeared to be more or less event-like,respectively.
Tests for the ?event-y?ness of thecontext included whether an unambiguous wordwould be an acceptable substitute there (e.g.
funds[=only non-event] for expenditure [either]).To create the test data, the annotated documentswere also parsed to automatically extract all com-mon noun-headed NPs and the dependency triplesthey instantiate.
Those with heads that alignedwith the offsets of an event annotation were la-beled as events; the remainder were labeled asnon-events.
Because of parsing errors, about 10%of annotated event instances were lost, that is re-mained unlabeled or were labeled as non-events.So, our results are based on the set of recover-able event nominals as a subset of all common-noun headed NPs that were extracted.
In thetest corpus there were 9,381 candidate instances,1,579 (17%) events and 7,802 (83%) non-events.There were 2,319 unique term types; of these, 167types (7%) appeared both as event tokens and non-event tokens.
Some sample ambiguous terms in-clude: behavior, attempt, settlement, deal, viola-tion, progress, sermon, expenditure.We constructed two lexicons of nominals to useas the seed terms.
For events, we created a list of95 terms, such as election, war, assassination, dis-missal, primarily based on introspection combinedwith some checks on individual terms in WordNetand other dictionaries and using Google searchesto judge how ?event-y?
the term was.To create a list of non-events, we used WordNetand the British National Corpus.
First, from theset of all lexemes that appear in only one synsetin WordNet, all nouns were extracted along withthe topmost hypernym they appear under.
Fromthese we retained those that both appeared on alemmatized frequency list of the 6,318 words withmore than 800 occurrences in the whole 100M-word BNC (Kilgarriff, 1997) and had one of thehypernyms GROUP, PSYCHOLOGICAL, ENTITY,POSSESSION.
We also retained select terms fromthe categories STATE and PHENOMENON were la-beled non-event seeds.
Examples of the 295 non-event seeds are corpse, electronics, bureaucracy,airport, cattle.Of the 9,381 test instances, 641 (6.8%) had aterm that belonged to the seed list.
With respectto types, 137 (5.9%) of the 2,319 term types in thetest data also appeared on the seed lists.3.2 ExperimentsExperiments were performed to investigate theperformance of our models, both when using orig-inal seed lists, and also when varying the contentof the seed lists using a bootstrapping techniquethat relies on the probabilistic framework of themodel.
A 1,000-instance subset of the 9,381 testdata instances was used as a validation set; the re-maining 8,381 were used as evaluation data, onwhich we report all results (with the exception ofTable 3 which is on the full test set).EXP1: Results using original seed sets Prob-abilistic models for non-events and events werebuilt from the full list of 295 non-event and 95event seeds, respectively, as described above.Table 1 (top half: original seed set) shows theresults over the 8,381 evaluation data instanceswhen using the three classification methods de-scribed above: (i) word, (ii) context, and (iii)word+context.
The first row (ALL) reports scoreswhere all undecided responses are marked as in-171BOOTSTRAPPEDORIGINALSEEDSETSEEDSETEVENT NONEVENT TOTAL AVERAGEInput Vector Correct Acc (%) Att (%) Correct Acc (%) Att (%) Correct Acc (%) Att (%) Acc (%)ALLword 1236 87.7 100.0 4217 60.5 100.0 5453 65.1 100.0 74.1context 627 44.5 100.0 2735 39.2 100.0 3362 40.1 100.0 41.9word+context 1251 88.8 100.0 4226 60.6 100.0 5477 65.4 100.0 74.7FAIRword 1236 89.3 98.3 4217 60.7 99.6 5453 65.5 99.4 75.0context 627 69.4 64.2 2735 62.5 62.8 3362 63.6 63.0 65.9word+context 1251 89.3 99.5 4226 60.7 99.9 5477 65.5 99.8 75.0ALLword 1110 78.8 100.0 5517 79.1 100.0 6627 79.1 100.0 79.0context 561 39.8 100.0 2975 42.7 100.0 3536 42.2 100.0 41.3word+context 1123 79.8 100.0 5539 79.4 100.0 6662 79.5 100.0 79.6FAIRword 1110 80.2 98.3 5517 79.4 99.6 6627 79.5 99.4 79.8context 561 62.1 64.2 2975 67.9 62.8 3536 66.9 63.0 65.0word+context 1123 80.2 99.5 5539 79.5 99.9 6662 79.7 99.8 79.9LEX 1 1114 79.1 100.0 5074 72.8 100.0 6188 73.8 100.0 75.9total counts 1408 6973 8381Table 1: (EXP1, EXP3) Accuracies of classifiers in terms of correct classifications, % correct, and % attempted (if allowed toabstain), on the evaluation test set.
(Row 1) Classifiers built from original seed set of size (295, 95); (Row 2) Classifiers builtfrom 15 iterations of bootstrapping; (Row 3) Classifier built from Lexicon 1.
Accuracies in bold are those plotted in relatedFigures 2, 3(a) and 3(b).correct.
In the second row (FAIR), undecided an-swers (d = 0) are left out of the total, so thenumber of correct answers stays the same, but thepercentage of correct answers increases.4 Scoresare measured in terms of accuracy on the EVENTinstances, accuracy on the NONEVENT instances,TOTAL accuracy across all instances, and the sim-ple AVERAGE of accuracies on non-events andevents (last column).
The AVERAGE score as-sumes that performance on non-events and eventsis equally important to us.
?From EXP1, we see that the behavior of a termacross an entire corpus is a better source of infor-mation about whether a particular instance of thatterm refers to an event than its immediate context.We can further infer that this is because the imme-diate context only provides definitive evidence forthe models in 63.0% of cases; when the contextmodel is not penalized for indecision, its accuracyimproves considerably.
Nonetheless, in combina-tion with the word model, immediate context doesnot appear to provide much additional informationover only the word.
In other words, based onlyon a term?s distribution in the past, one can makea reasonable prediction about how it will be usedwhen it is seen again.
Consequently, it seems thata well-constructed, i.e.
domain customized, lexi-con can classify nearly as well as a method thatalso takes context into account.EXP2: Results on ACE 2005 event data In ad-dition to using the data set created specifically forthis project, we also used a subset of the anno-4Note that Att(%) does not change with bootstrapping?an artifact of the sparsity of certain feature vectors in thetraining and test data, and not the model?s constituents seeds.Input Vector Acc (%) Att (%)word 96.1 97.2context 72.8 63.1word+context 95.5 98.9LEX 1 76.5 100.0Table 2: (EXP2) Results on ACE event nominals: %correct(accuracy) and %attempted, for our classifiers and LEX 1.tated training data created for the ACE 2005 EventDetection and Recognition (VDR) task.
Becauseonly event mentions of specific types are markedin the ACE data, only recall of ACE event nomi-nals can be measured rather than overall recall ofevent nominals and accuracy on non-event nom-inals.
Results on the 1,934 nominal mentions ofevents (omitting cases of d = 0) are shown in Ta-ble 2.
The performance of the hand-crafted Lex-icon 1 on the ACE data, described in Section 3.3below, is also included.The fact that our method performs somewhatbetter on the ACE data than on our own data, whilethe lexicon approach is worse (7 points highervs.
3 points lower, respectively) can likely be ex-plained by the fact that in creating our introspec-tive seed set for events, we consulted the annota-tion manual for ACE event types and attemptedto include in our list any unambiguous seed termsthat fit those types.EXP3: Increasing seed set via BootstrappingThere are over 2,300 unlabeled vectors in the train-ing data that correspond to the words that appearas lexical heads in the test data.
These unlabeledtraining vectors can be powerfully leveraged us-ing a simple bootstrapping algorithm to improvethe individual models for non-events and events,as follows: Step 1: For each vector v in the unla-beled portion of training data, row-sum normalize172100 1 5 10 15      LEX160657075808590 non?eventseventstotalaverageFigure 2: Accuracies vs. iterations of bootstrapping.
Boldsymbols on left denote classifier built from initial (295, 95)seeds; and bold (disconnected) symbols at right are LEX 1.it to produce v?
and compute a normalized mea-sure of confidence of the algorithm?s prediction,given by the magnitude of d(v?).
Step 2: Addthose vectors most confidently classified as eithernon-events or events to the seed set for non-eventsor events, according to the sign of d(v?).
Step 3:Recalculate the model based on the new seed lists.Step 4: Repeat Steps 1?3 until either no more un-labeled vectors remain or the validation accuracyno longer increases.In our experiments we added vectors to eachmodel such that the ratio of the size of the seedsets remained constant, i.e.
50 non-events and16 events were added at each iteration.
Usingour validation set, we determined that the boot-strapping should stop after 15 iterations (despitecontinuing for 21 iterations), at which point theaverage accuracy leveled out and then began todrop.
After 15 iterations the seed set is of size(295, 95)+(50, 16)?15 = (1045, 335).
Figure 2shows the change in the accuracy of the model asit is bootstrapped through 15 iterations.TOTAL accuracy improves with bootstrapping,despite EVENT accuracy decreasing, because thetest data is heavily populated with non-events,whose accuracy increases substantially.
The AV-ERAGE accuracy also increases, which proves thatbootstrapping is doing more than simply shiftingthe bias of the classifier to the majority class.
Thefigure also shows that the final bootstrapped clas-sifier comfortably outperforms Lexicon 1, impres-sive because the lexicon contains at least 13 timesmore terms than the seed lists.EXP4: Bootstrapping with a reduced numberof seeds The size of the original seed lists werechosen somewhat arbitrarily.
In order to deter-mine whether similar performance could be ob-tained using fewer seeds, i.e.
less human effort, weexperimented with reducing the size of the seedlexicons used to initialize the bootstrapping.To do this, we randomly selected a fixed frac-tion, f%, of the (295, 95) available event and non-event seeds, and built a classifier from this sub-set of seeds (and discarded the remaining seeds).We then bootstrapped the classifier?s models us-ing the 4-step procedure described above, usingcandidate seed vectors from the unlabeled train-ing corpus, and incrementing the number of seedsuntil the classifier consisted of (295, 95) seeds.We then performed 15 additional bootstrapping it-erations, each adding (50, 16) seeds.
Since theseeds making up the initial classifier are chosenstochastically, we repeated this entire process 10times and report in Figures 3(a) and 3(b) the meanof the total and average accuracies for these 10folds, respectively.
Both plots have five traces,with each trace corresponding the fraction f =(20, 40, 60, 80, 100)% of labeled seeds used tobuild the initial models.
As a point of reference,note that initializing with 100% of the seed lexiconcorresponds to the first point of the traces in Fig-ure 2 (where the x-axis is marked with f =100%).Interestingly, there is no discernible differencein accuracy (total or average) for fractions fgreater than 20%.
However, upon bootstrappingwe note the following trends.
First, Figure 3(b)shows that using a larger initial seed set increasesthe maximum achievable accuracy, but this max-imum occurs after a greater number bootstrap-ping iterations; indeed the maximum for 100% isachieved at 15 (or greater) iterations.
This reflectsthe difference in rigidity of the initial models, withsmaller initial models more easily misled by theseeds added by bootstrapping.
Second, the finalaccuracies (total and average) are correlated withthe initial seed set size, which is intuitively satisfy-ing.
Third, it appears from Figure 3(a) that the to-tal accuracy at the model size (295,95) (or 100%)is in fact anti-correlated with the size of the ini-tial seed set, with 20% performing best.
This iscorrect, but highlights the sometimes misleadinginterpretation of the total accuracy: in this casethe model is defaulting to classifying anything asa non-event (the majority class), and has a consid-erably impoverished event model.If one wants to do as well as Lexicon 1 after 15iterations of bootstrapping then one needs at least173EVENT NONEVENT TOTAL AVERAGECorr (%) Corr (%) Corr (%) (%)LEX 1 1256 79.5 5695 73.0 6951 74.1 76.3LEX 2 1502 95.1 4495 57.6 5997 63.9 76.4LEX 3 349 22.1 7220 92.5 7569 80.7 57.3Total 1579 7802 9381Table 3: Accuracy of several lexicons, showing number andpercentage of correct classifications on the full test set.an initial seed set of size 60%.
An alternative isto perform fewer iterations, but here we see thatusing 100% of the seeds comfortably achieves thehighest total and average accuracies anyway.3.3 Comparison with existing lexiconsIn order to compare our weakly-supervised proba-bilistic method with a lexical lookup method basedon very large hand-created lexical resources, wecreated three lexicons of event terms, which wereused as very simple classifiers of the test data.
Ifthe test instance term belongs to the lexicon, it islabeled EVENT; otherwise, it is labeled as NON-EVENT.
The results on the full test set using theselexicons are shown in Table 3.Lex 1 5,435 entries from NomLex (Macleod etal., 1998), FrameNet(Baker et al, 1998), CELEX(CEL, 1993), Timebank(Day et al, 2003).Lex 2 13,659 entries from WordNet 2.0 hypernymclasses EVENT, ACT, PROCESS, COGNITIVE PRO-CESS, & COMMUNICATION combined with Lex 1.Lex 3 Combination of pre-existing lexicons in theinformation extraction application from WordNet,Oxford Advanced Learner?s Dictionary, etc.As shown in Tables 1 and 3, the relativelyknowledge-poor method developed here usingaround 400 seeds performs well compared to theuse of the much larger lexicons.
For the task ofdetecting nominal events, using Lexicon 1 mightbe the quickest practical solution.
In terms of ex-tensibility to other semantic classes, domains, orlanguages lacking appropriate existing lexical re-sources, the advantage of our trainable method isclear.
The primary requirement of this method isa dependency parser and a system user-developerwho can provide a set of seeds for a class of in-terest and its complement.
It should be possi-ble in the next few years to create a dependencyparser for a language with no existing linguistic re-sources (Klein and Manning, 2002).
Rather thanhaving to spend the considerable person-years ittakes to create resources like FrameNet, CELEX,and WordNet, a better alternative will be to useweakly-supervised semantic labelers like the onedescribed here.4 Related WorkIn recent years an array of new approaches havebeen developed using weakly-supervised tech-niques to train classifiers or learn lexical classesor synonyms, e.g.
(Mihalcea, 2003; Riloff andWiebe, 2003).
Several approaches make use of de-pendency triples (Lin, 1998; Gorman and Curran,2005).
Our vector representation of the behaviorof a word type across all its instances in a corpus isbased on Lin (1998)?s DESCRIPTION OF A WORD.Yarowsky (1995) uses a conceptually similartechnique for WSD that learns from a small set ofseed examples and then increases recall by boot-strapping, evaluated on 12 idiosyncratically poly-semous words.
In that task, often a single disam-biguating feature can be found in the context of apolysemous word instance, motivating his use ofthe decision list algorithm.
In contrast, the goalhere is to learn how event-like or non-event-likea set of contextual features together are.
We donot expect that many individual features correlateunambiguously with references to events (or non-events), only that the presence of certain featuresmake an event interpretation more or less likely.This justifies our probabilistic Bayesian approach,which performs well given its simplicity.Thelen and Riloff (2002) use a bootstrapping al-gorithm to learn semantic lexicons of nouns forsix semantic categories, one of which is EVENTS.For events, only 27% of the 1,000 learned wordsare correct.
Their experiments were on a muchsmaller scale, however, using the 1,700 documentMUC-4 data as a training corpus and using only10 seeds per category.Most prior work on event nominals does not tryto classify them as events or non-events, but in-stead focuses on labeling the argument roles basedon extrapolating information about the argumentstructure of the verbal root (Dahl et al, 1987; La-pata, 2002; Pradhan et al, 2004).
Meyers, et al(1998) describe how to extend a tool for extrac-tion of verb-based events to corresponding nomi-nalizations.
Hull and Gomez (1996) design a setof rule-based algorithms to determine the sense ofa nominalization and identify its arguments.5 ConclusionsWe have developed a novel algorithm for label-ing nominals as events that combines WSD andlexical acquisition.
After automatically bootstrap-ping the seed set, it performs better than static lex-icons many times the original seed set size.
Also,174further bootstrap iterations??
initial seed setfraction (%) ?20 40 60 80 100 1 5 10 15      LEX16466687072747678808220%40%60%80%100%(a) Total Accuracyfurther bootstrap iterations?
?initial seed setfraction (%) ?20 40 60 80 100 1 5 10 15      LEX16466687072747678808220%40%60%80%100%(b) Average AccuracyFigure 3: Accuracies of classifiers built from different-sized initial seed sets, and then bootstrapped onwards to the equivalentof 15 iterations as before.
Total (a) and Average (b) accuracies highlight different aspects of the bootstrapping mechanism.Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot.
Also for reference the relevantLexicon 1 accuracy (LEX 1) is denoted with a ?
at the far right.it is more robust than lexical lookup as it can alsoclassify unknown words based on their immediatecontext and can remain agnostic in the absence ofsufficient evidence.Future directions for this work include applyingit to other semantic labeling tasks and to domainsother than general news.
An important unresolvedissue is the difficulty of formulating an appropriateseed set to give good coverage of the complementof the class to be labeled without the use of a re-source like WordNet.ReferencesC.
Aone and M. Ramos-Santacruz.
2000.
REES: Alarge-scale relation and event extraction system.
In6th ANLP, pages 79?83.C.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley FrameNet project.
In Proc.
COLING-ACL.Centre of Lexical Information, Nijmegen, 1993.CELEX English database, E25, online edition.A.
Copestake and T. Briscoe.
1995.
Semi-productivepolysemy and sense extension.
Journal of Seman-tics, 12:15?67.D.
Dahl, M. Palmer, and R. Passonneau.
1987.
Nomi-nalizations in PUNDIT.
In Proc.
of the 25th ACL.D.
Day, L. Ferro, R. Gaizauskas, P. Hanks, M. Lazo,J.
Pustejovsky, R. Sauri, A.
See, A. Setzer, andB.
Sundheim.
2003.
The TIMEBANK corpus.
InCorpus Linguistics 2003, Lancaster UK.J.
Gorman and J. Curran.
2005.
Approximate search-ing for distributional similarity.
In Proc.
of theACL-SIGLEX Workshop on Deep Lexical Acquisi-tion, pages 97?104.R.
Hull and F. Gomez.
1996.
Semantic interpretationof nominalizations.
In Proc.
of the 13th NationalConf.
on Artificial Intelligence, pages 1062?1068.A.
Kilgarriff.
1997.
Putting frequencies in the dictio-nary.
Int?l J. of Lexicography, 10(2):135?155.D.
Klein and C. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proc.
of the 40th ACL.M.
Lapata.
2002.
The disambiguation of nominalisa-tions.
Computational Linguistics, 28(3):357?388.D.
K. Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
of COLING-ACL ?98.C.
Macleod, R. Grishman, A. Meyers, L. Barrett, andR.
Reeves.
1998.
NOMLEX: A lexicon of nominal-izations.
In Proc.
of EURALEX?98.A.
Meyers, C. Macleod, R. Yangarber, R. Grishman,L.
Barrett, and R. Reeves.
1998.
Using NOMLEXto produce nominalization patterns for informationextraction.
In Proc.
of the COLING-ACL Workshopon the Computational Treatment of Nominals.R.
Mihalcea.
2003.
Unsupervised natural languagedisambiguation using non-ambiguous words.
InProc.
of Recent Advances in Natural Language Pro-cessing, pages 387?396.T.
Parsons.
1990.
Events in the Semantics of English.MIT Press, Boston.S.
Pradhan, H. Sun, W. Ward, J. Martin, and D. Juraf-sky.
2004.
Parsing arguments of nominalizations inEnglish and Chinese.
In Proc.
of HLT-NAACL.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proc.
EMNLP.H.
Schu?tze.
1998.
Automatic word sense disambigua-tion.
Computational Linguistics, 24(1):97?124.M.
Thelen and E. Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extrac-tion pattern contexts.
In Proc.
of EMNLP.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
ofthe 33rd ACL, pages 189?196.175
