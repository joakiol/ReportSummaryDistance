Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 447?457,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsEffectiveness and Efficiency of Open Relation ExtractionFilipe Mesquita Jordan Schmidek Denilson BarbosaDepartment of Computing Science, University of Alberta, Canada{mesquita,schmidek,denilson}@ualberta.caAbstractA large number of Open Relation Extrac-tion approaches have been proposed recently,covering a wide range of NLP machinery,from ?shallow?
(e.g., part-of-speech tagging)to ?deep?
(e.g., semantic role labeling?SRL).A natural question then is what is the trade-off between NLP depth (and associated com-putational cost) versus effectiveness.
This pa-per presents a fair and objective experimentalcomparison of 8 state-of-the-art approachesover 5 different datasets, and sheds some lighton the issue.
The paper also describes a novelmethod, EXEMPLAR, which adapts ideas fromSRL to less costly NLP machinery, resultingin substantial gains both in efficiency and ef-fectiveness, over binary and n-ary relation ex-traction tasks.1 IntroductionOpen Relation Extraction (ORE) (Banko and Et-zioni, 2008) has become prevalent over traditionalrelation extraction methods, especially on the Web,because of the intrinsic difficulty in training indi-vidual extractors for every single relation.
Broadlyspeaking, existing ORE approaches can be groupedaccording to the level of sophistication of the NLPtechniques they rely upon: (1) shallow parsing, (2)dependency parsing and (3) semantic role labelling(SRL).
Shallow methods annotate the sentences withpart-of-speech (POS) tags and the ORE approachesin this category, such as ReVerb (Fader et al 2011)and SONEX (Merhav et al 2012), identify rela-tions by matching patterns over such tags.
Depen-dency parsing gives unambiguous relations amongeach word in the sentence, and the ORE approachesin this category such as PATTY (Nakashole et al2012), OLLIE (Mausam et al 2012), and TreeK-ernel (Xu et al 2013) identify whole subtrees con-necting the relation predicate and its arguments.
Fi-nally, semantic annotators, such as Lund (Johans-son and Nugues, 2008) and SwiRL (Surdeanu et al2003), add roles to each node in a parse tree, en-abling ORE approaches that identify the precise con-nection between each argument and the predicate ina relation, independently.The first contribution of the paper is an objec-tive and fair experimental comparison of the state-of-the-art in ORE, on 5 datasets with varying de-gree of ?difficulty?.
Of these, 4 datasets were an-notated manually, covering both well-formed sen-tences, from the New York Times (NYT) and thePenn Treebank, as well as mixed-quality sentencesfrom a popular Web corpus.
A much larger corpuswith 12,000 sentences from NYT, automatically an-notated is also used.
Another experiment focuseson n-ary relation extractions separately.
The resultsshow, as expected, that the three broad classes aboveare separated by orders of magnitude when it comesto throughput.
Shallow methods handle ten timesmore sentences than dependency parsing methods,which in turn handle ten times more sentences thansemantic parsing methods.
Nevertheless, the cost-benefit trade-off is not as simple; and the highercomputation cost of dependency or semantic parsingdoes not always pays off with higher effectiveness.The second contribution of the paper is a newORE method, called EXEMPLAR, which applies akey idea in semantic approaches (namely, to iden-447tify the precise connection between the argumentand the predicate words in a relation) over a depen-dency parse tree (i.e., without applying SRL).
Thegoal is to achieve the higher accuracy of the seman-tic approaches at the lower computational cost of thedependency parsing approaches.
EXEMPLAR is arule-based system derived from a careful study of alldependency types identified by the Stanford parser.
(Note, however, that other parsers can be used, asshown later on.)
EXEMPLAR works for both binaryand n-ary relations, and is evaluated separately ineach case.
For binary relations, EXEMPLAR outper-forms all previous methods in terms of accuracy, los-ing to the shallow methods only in terms of through-put.
As for n-ary relations, EXEMPLAR outperformsthe methods that support this kind of extraction.2 Related WorkOthers have pointed out the importance of under-standing the trade-off between ?shallow?
versus?deep?
NLP in ORE. One side of the argument fa-vors shallow methods, claiming deep NLP costs or-ders of magnitude more and provide much less dra-matic gains in terms of effectiveness (Christensen etal., 2011).
The counterpoint, illustrated with a re-cent analysis on a industrial-scale Web crawl (Dalviet al 2012), is that the diversity with which infor-mation is encoded in text is too high.
Framing thedebate as ?shallow?
versus ?deep?
is perhaps con-venient, but nevertheless an oversimplification.
Thispaper sheds more light into the debate by compar-ing the state-of-the-art from three broad classes ofapproaches.Shallow ORE. TextRunner (Banko and Etzioni,2008) and its successor ReVerb (Fader et al 2011)are based on the idea that most relations are ex-pressed using few syntactic patterns.
ReVerb, for ex-ample, detects only three types of relations (?verb?,?verb+preposition?
and ?verb+noun+preposition?
).Following a similar approach, SONEX (Merhav etal., 2012) extends ReVerb by detecting patterns withappositions and possessives.ORE via dependency parsing.
PATTY (Nakas-hole et al 2012) extracts textual patterns from sen-tences based on paths in the dependency graph.
Forall pairs of named entities, PATTY finds the shortestDataset Source # Sentences # RelationsWEB-500 Search Snippets 500 461NYT-500 New York Times 500 150PENN-100 Penn Treebank 100 51Table 1: Binary relation datasets.path in the dependency graph that connects the twonamed entities.
They limit the search to only pathsthat start with one of these dependencies: nsubj, rc-mod and partmod.OLLIE (Mausam et al 2012) also extracts rela-tions between two entities.
It applies pattern tem-plates over the dependency subtree containing pairsof entities.
Pattern templates are learned automat-ically from a large training set that is bootstrappedfrom high confidence extractions from ReVerb.
OL-LIE merges binary relations that differ only in thepreposition and second argument to produce n-aryextractions, as in: (A, ?met with?, B) and (A, ?metin?, C) leading to (A, ?met?, [with B, in C]).The TreeKernel (Xu et al 2013) method uses adependency tree kernel to classify whether candi-date tree paths are indeed instances of relations.
Theshortest path between the two entities along with theshortest path between relational words and an entityare used as input to the tree kernel.
An expanded setof syntactic patterns based on those from ReVerb areused to generate relation candidates.ORE via semantic parsing.
Recently, a methodbased on SRL, called SRL-IE, has shown that the ef-fectiveness of ORE methods can be improved withsemantic features (Christensen et al 2011).
We im-plemented our version of SRL-IE by relying on theoutput of two SRL systems: Lund (Johansson andNugues, 2008) and SwiRL (Surdeanu et al 2003).SwiRL is trained on PropBank and expands upon thesyntactic features used in previous work.
One of itsmajor limitations is that it is only able to label ar-guments with verb predicates.
Lund, on the otherhand, is based on dependency parsing and is trainedon both PropBank and NomBank, making it able toextract relations with both verb and noun predicates.3 Experimental StudyThis section compares the effectiveness and effi-ciency of the following ORE methods: ReVerb,448MethodNYT-500 WEB-500 PENN-100Time P R F Time P R F Time P R FReVerb 0.02 0.70 0.11 0.18 0.01 0.92 0.29 0.44 0.02 0.78 0.14 0.23SONEX 0.04 0.77 0.22 0.34 0.02 0.98 0.30 0.45 0.04 0.92 0.43 0.59OLLIE 0.05 0.62 0.27 0.38 0.04 0.81 0.29 0.43 0.14 0.81 0.43 0.56EXEMPLAR[M] 0.08 0.70 0.39 0.50 0.06 0.95 0.44 0.61 0.16 0.83 0.49 0.62EXEMPLAR[S] 1.03 0.73 0.39 0.51 0.47 0.96 0.46 0.62 0.62 0.79 0.51 0.62PATTY 1.18 0.49 0.23 0.32 0.48 0.71 0.48 0.57 0.66 0.46 0.24 0.31SwiRL 2.96 0.63 0.10 0.17 1.73 0.97 0.34 0.50 2.17 0.89 0.16 0.27Lund 11.40 0.78 0.24 0.37 2.69 0.91 0.37 0.52 5.21 0.86 0.35 0.50TreeKernel ?
?
?
?
?
?
?
?
0.85 0.85 0.33 0.48Table 2: Results for the task of extracting binary relations.
Methods are ordered by computing time persentence (in seconds).
Best results for each column are underlined and marked in bold, for clarity.SONEX, OLLIE, PATTY, TreeKernel, SwiRL,Lund and our two variants of our method, EX-EMPLAR, explained in detail in Appendix A: (1)EXEMPLAR[S] uses the Stanford parser (Klein andManning, 2003) and (2) EXEMPLAR[M] uses theMalt parser (Nivre and Nilsson, 2004).3.1 Binary Relations ?
SetupWe start by evaluating the extraction of binary re-lations.
Table 1 shows our experimental datasets.WEB-500 is a commonly used dataset, developedfor the TextRunner experiments (Banko and Etzioni,2008).
These sentences are often incomplete andgrammatically unsound, representing the challengesof dealing with web text.
NYT-500 represents theother end of the spectrum with formal, well writtennew stories from the New York Times Corpus (Sand-haus, 2008).
PENN-100 contains sentences from thePenn Treebank recently used in an evaluation of theTreeKernel method (Xu et al 2013).
We manu-ally annotated the relations for WEB-500 and NYT-500 and use the PENN-100 annotations provided byTreeKernel?s authors (Xu et al 2013).We annotate each sentence manually as follows.We identify exactly two entities and a trigger (a sin-gle token indicating a relation?see Section A.3) forthe relation between them, if one exists.
In addi-tion, we specify a window of tokens allowed to bein a relation, including modifiers of the trigger andprepositions connecting triggers to their arguments.For each sentence annotated with two entities, a sys-tem must extract a string representing the relationbetween them.
Our evaluation method deems an ex-traction as correct if it contains the trigger and al-lowed tokens only.In our annotated sentences, entities are enclosedin triple square brackets, triggers and enclosed intriple curly brackets and the window of allowed to-kens is defined by arrows (?--->?
and ?<---?
),as in this example:I?ve got a media call about[[[ORG Google]]] --->?s{{{acquisition}}} of<--- [[[ORGYouTube]]] --->today<---.where ?Google?
and ?YouTube?
are entities of thetype organization, ?acquisition?
is the trigger and theallowed tokens are ?acquisition?, ??s?
and ?of?.
Weinclude time and location modifiers (e.g., ?today?,?here?)
in the list of allowed tokens since OLLIE ex-tracts them as part of the relation.
OLLIE?s extrac-tions may also include auxiliary verbs and preposi-tions that are not present in the original sentence.
Tobe fair with OLLIE, we remove auxiliary verbs andprepositions from OLLIE extractions.Our benchmarks are available upon request.Ensuring entities are recognized properly.Since every method uses a different tool to recog-nize entities, we try to ensure every method is ableto recognize the entities marked by our annotators.We replace the original entities by a single word,preventing any system from recognizing only partof an entity.
Entities are replaced by ?Europe?
and?Asia?, since we empirically found that, for 99.7%of the sentences in our experiment, all methodswere able to recognize ?Europe?
and ?Asia?
asentities (or nouns, for systems that do not use a NERtool).
In addition, we did not find any occurrence of449?Europe?
and ?Asia?
in the original sentences thatcould conflict with our entity placeholders.For methods that extract relations between nounphrases (ReVerb, OLLIE, SwiRL and Lund), thereis the additional task of identifying whether anoun phrase containing additional words surround-ing ?Europe?
and ?Asia?
is still a reference to the an-notated entity.
For example, ?the beautiful Europe?refers to the entity, while ?Europe?s enemy?
doesnot.
In our evaluation, we ignore noun phrases thatdo not reference the annotated entity.
For SwiRLand Lund, we ignore any noun phrase that do notpresent ?Europe?
or ?Asia?
as its head word.
ForReVerb and OLLIE, we ignore noun phrases that donot contain these words in the end of the phrase.Metrics.
Our evaluation focuses on sentence-levelextractions.
Therefore, we only apply the steps ofeach method that perform this task.
Additional stepsto merge relations and remove infrequent relationsare not applied.
In addition, we assume there isonly one relation between a pair of entities in a sen-tence.
The number of entity pairs with more thanone relation was insignificant in our datasets (lessthan 0.5%).The metrics used in this analysis are precision (P),recall (R) and f-measure (F), defined as usual:P =# correct# extractions, R =# correct# relations, F =2PRP + Rwhere ?# correct?
is the number of extractionsdeemed as correct.We also measure the total computing time of eachmethod, excluding initialization or loading any li-braries or models in memory.
To ensure a fair com-parison, we make sure each method runs in a single-threaded mode, thus utilizing a single computingcore at all times.3.2 Binary Relations ?
ResultsTable 2 presents the results for our experiment withbinary relations.
WEB-500 turned out to contain theeasiest sentences as evidenced by the precision ofall methods in this dataset.
This is because WEB-500 sentences were collected by querying a searchengine with known relation instances.
The othertwo datasets, on the other hand, contain randomlychosen sentences.
Although WEB-500 is a popular0.20.30.40.50.60.70.01  0.1  1  10f-measureseconds per sentenceEXEMPLAR[S]EXEMPLAR[M]REVERBSONEXOLLIE LUNDSWIRLPATTYFigure 1: Average f-measure vs average time forNYT-500, WEB-500 and PEN-100.0.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6precisionrecallEXEMPLAR[S]EXEMPLAR[M]REVERBSONEXOLLIELUNDSWIRLPATTYFigure 2: Average precision vs average recall forNYT-500, WEB-500 and PEN-100.dataset, it perhaps does not represent the challengesfound in web text.We were unable to run TreeKernel for NYT-500and WEB-500 for lack of training data.
We ranTreeKernel, as trained by its authors, on the sametest set used in their paper (Xu et al 2013).Comparing methods based on effectiveness (f-measure) or efficiency (computational cost) alonecan be misleading.
To do so, we compare methodsin terms of dominance.
We say method A dominatesmethod B if A is: (1) more effective and as efficientas B; (2) more efficient and as effective as B; or (3)both more effective and more efficient than B. Themethods that are not dominated by any other formthe state-of-the-art.Figure 1 plots the effectiveness and efficiency of450all methods, averaged over all datasets (TreeKernelwas not included due to missing results).
For effi-ciency, there is a clear separation of approximatelyone order of magnitude among methods based onshallow parsing (ReVerb and SONEX), dependencyparsing (OLLIE, EXEMPLAR[M], EXEMPLAR[S],and PATTY) and semantic parsing (SwiRL andLund).
The lines in the plot identify the state-of-the-art before (dashed) and after (solid) EXEMPLAR.
(Note: Although not clear in the figure, OLLIE andLund are dominated by SONEX as they tie in termsof effectiveness.
)In terms of efficiency, EXEMPLAR[M] and EX-EMPLAR[S] closely match OLLIE and PATTY, re-spectively, since they use the same dependencyparsers.
EXEMPLAR outperforms both systems bycovering a larger number of relational patterns.
Thisis possible because EXEMPLAR looks at each argu-ment separately, as opposed to the whole subtreeconnecting two arguments.
As it turns out, this de-sign choice greatly simplifies the task of designinggood patterns.The poor performance of PATTY is due to itsrather permissive sentence-level extraction of rela-tions, which looks at the shortest path between argu-ments.
PATTY relies on redundancy of extractionsto normalize the produced relations in order to re-cover from mistakes done in the sentence-level.Figure 2 illustrates the dominance relation dif-ferently, using precision versus recall.
Again, thedashed line shows the previous state-of-the-art, andthe solid line shows the current situation.
SONEXdominates PATTY and Lund, since they tied inrecall.
OLLIE, however, achieved greater recallthan SONEX.
Somewhat surprisingly, EXEMPLARpresents 44% more recall than the more sophisti-cated Lund, at a close level of precision.
This can beexplained by Lund?s dependency on training data,which contains only a subset of all possible pred-icates and roles.
The importance of relations trig-gered by nouns is illustrated by the higher recallof SONEX and Lund when compared, respectively,to ReVerb and SwiRL, similar methods that handleverb triggers only.3.3 Binary Relations ?
DiscussionDifferences in annotation.
It is worth notingsome differences between our annotations (WEB-500 and NYT-500) and the annotations from PEN-100.
The first difference concerns the definitionof an entity.
Consider the following sentence fromPEN-100:?.
.
.
says Leslie Quick Jr., chairman of theQuick & Reilly discount brokerage firm.
?Unlike our annotation style, the original annotationdefines that ?Leslie Quick Jr.?
is the chairman of?the Quick & Reilly discount brokerage firm?, as op-posed to ?Quick & Reilly?.
While we consider thewords surrounding ?Quick & Reilly?
as apposition,the original consider them as part of the entity.Another difference concerns the definition of theRE task.
We assume that RE methods are respon-sible for resolving co-references when necessary toidentify a relation.
For example, consider the sen-tence:?It also marks P&G?s growing concern that itsJapanese rivals, such as Kao Corp., may bringtheir superconcentrates to the U.S.?According to our annotation style, there is a rela-tion ?rivals?
between ?P&G?
and ?Kao Corp.?
inthis sentence.
On the other hand, the original an-notations for PENN-100 consider only the relationbetween ?Kap Corp.?
and the pronoun ?it?, leav-ing the task of resolving the coreference between?P&G?
and ?it?
as a posterior step.These differences in annotation illustrate the chal-lenges of producing a benchmark for open relationextraction.Differences in evaluation methodology.
Asentence-level evaluation like ours focuses on eachsentence, separately.
On the other hand, the evalua-tions of SONEX, ReVerb, PATTY, TreeKernel andOLLIE are performed at the corpus level.
Corpus-level evaluations consider an extracted relation ascorrect regardless of whether a method was ableto identify one or all sentences that describe thisrelation.Creating a ground truth for corpus-level evalu-ations is extremely hard, since one has to iden-tify and curate (e.g., merge near-duplicate relationsand co-referential entities) all relations describedin a corpus.
As a consequence, most corpus-levelevaluations perform only a manual inspection of a451MethodNYT n-aryTime P R FEXEMPLAR[M] 0.11 0.94 0.40 0.56OLLIE 0.12 0.87 0.14 0.25EXEMPLAR[S] 0.88 0.92 0.39 0.55SwiRL 2.90 0.94 0.30 0.45Lund 9.20 0.95 0.36 0.53Table 3: Results for n-ary relations.method?s extractions.
This manual inspection mea-sures a method?s precision, but is unable to measurerecall.Other differences in methodology are as follows.PATTY?s evaluation concerns relation patterns (e.g.,?wrote hits for?)
and their type signatures (e.g.Musician?Musician), as opposed to the relation it-self, which includes its two arguments.
The eval-uations of ReVerb and OLLIE consider any nounphrase as a potential argument, while the evaluationsof TreeKernel and SONEX consider named entitiesonly.Due to the lack of a ground truth and differencesin evaluation methodology, results from different pa-pers are usually not comparable.
This work tries toalleviate this problem by providing reusable annota-tions that are flexible and can be used to evaluate awide range of methods.3.4 n-ary RelationsThe goal of this experiment is to evaluate the accu-racy and performance of our method when extract-ing n-ary relations (n > 2).
For this experiment, wemanually tagged 222 sentences with n-ary relationsfrom the New York Times.
Every sentence is anno-tated with a single relation trigger and its arguments.This experiment measures precision and recallover the extracted arguments.
For each sentence, asystem can extract a number of relations of the formr(a1, a2, .
.
.
, an), where r is the relation name andai is an argument.
We only use the extracted rela-tion whose name contains the annotated trigger, ifit exists.
An argument of such relation is deemed acorrect extraction if it is annotated in the sentence;otherwise, it is deemed incorrect.Precision and recall are now defined as follows:P =# correct# extracted args, R =# correct# annotated args.MethodNYT 12KTime P R FReVerb 0.01 0.84 0.11 0.19OLLIE 0.02 0.85 0.22 0.35SONEX 0.03 0.87 0.20 0.32EXEMPLAR[M] 0.05 0.87 0.26 0.40EXEMPLAR[S] 1.20 0.86 0.29 0.43PATTY 1.29 0.86 0.18 0.30SwiRL 3.58 0.87 0.16 0.27Lund 11.28 0.86 0.21 0.33Table 4: Results for binary relations automaticallyannotated using Freebase and WordNet.where ?# correct?
is the number of argumentsdeemed as correct.
There are 765 annotated argu-ments in total.
Table 3 reports the results for ourexperiment with n-ary relations.
EXEMPLAR[M]shows a 6% increase in f-measure over Lund, thesecond best system, while being almost two ordersof magnitude faster.3.5 Automatically Annotated SentencesThe creation of datasets for open RE is an extremelytime-consuming task.
In this section we investigatewhether external data sources such as Freebase1 andWordNet2 can be used to automatically annotate adataset, leading to a useful benchmark.Our automatic annotator identifies pairs of enti-ties and a trigger of the relation between them.
Itdoes so by first trying to link all entities to Wikipedia(and consequently to Freebase, since Freebase islinked to Wikipedia) by using the method proposedby (Cucerzan, 2007).
Given two entities appearingwithin 10 tokens of each other in a sentence, our an-notator checks whether there is a relation connect-ing them in Freebase.
If such a relation exists, theannotator looks for a trigger in the sentence.
A trig-ger must be a synonym for the Freebase relation (ac-cording to WordNet) and its distance to the nearestentity cannot be more than 5 tokens.We applied this method for the New York Timesand were able to annotate over 60,000 sentenceswith over 13,000 distinct entity pairs.
For our ex-periments, we randomly selected one sentence foreach entity pair and separated a thousand for devel-opment and over 12,000 for test.1http://www.freebase.com2http://wordnet.princeton.edu/452Comparing with human annotators.
Althoughwe expect our automatic annotator to be less accu-rate than a human annotator, we are interested tomeasure the difference in accuracy between them.To do so, two authors of this paper looked at our de-velopment set and marked each sentence as corrector incorrect.
The agreement (that is, the percent-age of matching answers) between the humans was82%.
On other hand, the agreement between ourautomatic annotator and each human was 71% and72%.
This shows that our annotator?s accuracy is nottoo far below human?s level of accuracy.Table 4 shows the results for the test sentences.Both EXEMPLAR[S] and EXEMPLAR[M] outper-formed all systems in recall, while keeping the samelevel of precision.4 ConclusionThis work presents a fair and objective evaluation ofseveral ORE methods, shedding some light on thetrade-offs between f-measure and computational aswell as precision and recall.
Our evaluation is ableto assess the effectiveness of different methods byspecifying a trigger and a window of allowed tokensfor each relation.EXEMPLAR?s promising results indicate that rule-based methods may still be very competitive, espe-cially if rules are applied to each argument sepa-rately.
Looking at the recall levels of different meth-ods, we conjecture that EXEMPLAR outperformsmachine learning methods like Ollie and TreeKer-nel, because its rules apply in cases not trained bythese methods.
From a pragmatic point of view, EX-EMPLAR is also preferable because it doesn?t requiretraining data.An interesting research question is whether ma-chine learning can be used to learn more rules forEXEMPLAR in order to improve recall without lossin precision.
Rules could be learned from both de-pendency parsing and shallow parsing, or just shal-low parsing if computing time is extremely limited.The next step for our experimental study is toevaluate corpus-level extractions, where an auto-matic annotator is essential due to the massive num-ber of annotations required for even one relation, letalone thousands.AcknowledgementsWe thank the anonymous reviewers for useful sug-gestions to improve the paper, and the Natural Sci-ences and Engineering Council of Canada, throughthe NSERC Business Intelligence Network, for fi-nancial support.A EXEMPLARORE methods must recognize relations from the textalone.
To do this, each method tries to model howrelations are expressed in English.
Banko and Et-zioni claim that more than 90% of binary relationsare expressed through a few syntactic patterns, suchas ?verb?
and ?noun+preposition?
(Banko and Et-zioni, 2008).
It is unclear, however, whether n-aryrelations follow.This section presents a study focusing on how n-ary relations (n > 2) are expressed in English, basedon 100 distinct relations manually annotated from arandom sample of 514 sentences in the New YorkTimes Corpus (Sandhaus, 2008).Table 5 shows six syntactic patterns that cover86% of our n-ary relations.
These patterns areslightly different from those used for binary rela-tions.
In binary relations, the pattern implicitly de-fines the roles of the two arguments.
For instance,a relation ?met with?
indicates that the first argu-ment is the subject and the second one is the objectof ?with?.
In order to represent n-ary relations, ourpatterns do not contain prepositions, possessives orany other word connecting the relation to the argu-ment.
For instance, the sentence ?Obama met withPutin in Russia?
contains the relation ?meet?
alongthree arguments: ?Obama?
(subject), ?Putin?
(objectof preposition with) and ?Russia?
(object of prepo-sition in).Relation types.
A single relation can be repre-sented in different ways using the patterns shownin Table 5.
For instance, the relation ?donate?
canbe expressed as an active verb (?donates?
), passivevoice (?was donated by?)
and normalized verb (?do-nation?).
In addition, an apposition+noun relationcan be expressed as an copula+noun relation by re-placing apposition for the copula verb ?be?.
Bymerging these patterns, we have that most relationsfall into one of the following types: verb, verb+noun453Pattern Frequency ExampleVerb 30% Hingis beat Steffi Graf in the Italian Open two weeks ago.Apposition+noun 19% Jaden and Willow, the famous children of Will Smith, ...Passive verb 14% Jumbo the Elephant was exported from Sudan to Paris.Verb+noun 14% D-League will move its offices from Greenville to New York.Copula+noun 5% Kimball was a Fulbright scholar at the University of Heidelberg.Nominalized verb 4% Thousands died in Saddam Hussein?s attack on Halabja in 1988.Table 5: Patterns representing 86% of the relations with three or more arguments.
Frequencies collectedfrom 100 relations from the New York Times Corpus.
Relation triggers are highlighted in bold.Relation Type Freq.
Example VariationsVerb 48% beat Pass.
verb, nom.
verbCopula+noun 24% is son Apposition+nounVerb+noun 14% sign deal ?Table 6: Relation types recognized by EXEMPLAR.and copula+noun.Table 6 present the relation types found throughour analysis.
We developed EXEMPLAR to specifi-cally recognize these relation types, including theirvariations.Argument roles.
An argument role defines howan argument participates in a relation.
In ORE, theroles for each relation are not provided and must alsobe recognized from the text.We use the following roles: subject,direct object and prep object.
An argu-ment has a role prep object when its connectedto the relation by a preposition.
The roles ofprepositional objects consist of their preposition andthe suffix ?
object?, indicating that each prepositioncorresponds to a different role.
In the sentence?Obama is the president of the U.S.?, ?U.S.?
isan object of the preposition ?of?
and has the roleof object.Multiple entities can play the same role in arelation instance.
For instance, in the sentence?Obama and Putin discuss the Syria conflict?, both?Obama?
and ?Putin?
have the subject role.
Fur-thermore, some relations accept less roles than oth-ers.
Verb relations accept all three roles, while cop-ula+noun and verb+noun relations accept subjectand prep object only.Our roles are different from those used in SRL.SRL roles carry semantic information across differ-ent relations.
This information is unavailable forORE systems, and for this reason, we rely on syn-tactic roles.
An open problem is to determine whichsubject: ?NFL?relation: ?approve new stadium?of object: ?Falcons?in object: ?Atlanta?Figure 3: A relation instance extracted by EXEM-PLAR for the sentence ?NFL approves Falcons?
newstadium in Atlanta?.NFL approves Falcons' new stadium in Atlanta.nsubjamodpossdobj prep_inFigure 4: A input sentence after pre-processing.
En-tities are in bold, triggers are underline and arrowsrepresent dependencies.syntactic roles correspond to the same semantic roleacross different relations (Chambers and Jurafsky,2011).
However, this problem is out of the scopeof this work.A.1 The methodEXEMPLAR takes a stream of textual documents andextracts instances of n-ary relations as illustrated inFigure 3.A.2 PreprocessingGiven a document, EXEMPLAR extracts its syntacticstructure by applying a pipeline of NLP tools pro-vided by the Stanford Parser (Klein and Manning,2003).
Our method converts the original text intosentences, each containing a list of tokens.
Each to-ken is tagged with part of speech, lemma and de-pendencies.
EXEMPLAR also works with other de-pendency parsers based on Stanford?s dependencies,454such as the Malt parser (Nivre and Nilsson, 2004).Figure 4 illustrates our running example whereeach word is a token and arrows represent dependen-cies among tokens.
In this example, ?stadium?
de-pends on ?approves?
and the arrow connecting themcan be read as ?the direct object of approves is sta-dium?.Extracting Named Entities.
EXEMPLAR em-ploys the Stanford NER (Finkel et al 2005) to rec-ognize named entities.
We consider these types ofentities: people, organization, location, miscella-neous and date.
Figure 4 shows entities highlightedin bold.A.3 Detecting triggersAfter recognizing entities, EXEMPLAR detects re-lation triggers.
A trigger is a single token that in-dicates the presence of a relation.
A relation maypresent one or two triggers.
For instance, the rela-tion in our running example has two triggers.
EX-EMPLAR also uses triggers to determine the relationname, as discussed later.A trigger can be any noun or verb that was nottagged as being part of an entity mention.
Other re-quirements are enforced for each canonical pattern.Verb relations.
A verb relation is triggered by averb that does not include a noun as its direct object.The name of the relation is the trigger?s lemma.A noun must be a nominalized verb to be a triggerfor verb relations.
To identify nominalized verbs,EXEMPLAR checks if a noun is filed under the type?event?
in Wordnet?s Morphosemantic Database3.Doing so may generate false positives; however,EXEMPLAR has a filtering step to eliminate thesefalse positives, as discussed later.The name of a relation triggered by a nominalizedverb is the trigger?s original verb (before nominal-ization).
For instance, ?donation?
triggers the rela-tion ?donate?.Copula+noun relations.
Only noun triggers areaccepted for copula+noun relations.
The copulaused in the relation name can be a verb with cop-ula dependency to the trigger, or the verb ?be?
for3http://wordnetcode.princeton.edu/standoff-files/morphosemantic-links.xlsappositions.
The relation name is the concatena-tion of the copula?s lemma and the trigger?s lemmaalong its modifiers.
For instance, the relation in thesentence ?Jaden and Willow, the famous children ofWill Smith?
is ?be famous child?.Verb+noun relations.
EXEMPLAR recognizestwo triggers for each verb+noun relation: a verband a noun acting as its direct object.
The relationname is defined by concatenating the verb?s lemmawith the the noun and its modifiers.
In our runningexample, ?approves?
and ?stadium?
trigger therelation ?approve new stadium?.A.4 Detecting candidate argumentsAfter relation triggers are identified, EXEMPLARproceeds to detect their candidate arguments.
Forthis, we look at the dependency between each entityand a trigger separately.
EXEMPLAR relies on twoobservations: (1) an argument is often adjacent toa trigger in the dependency graph, and (2) the typeof the dependency can accurately predict whether anentity is an argument for the relation or not.Table 7 enumerates 12 types of dependencies(from a total of 53) that often connect arguments andtriggers.
EXEMPLAR identifies as a candidate argu-ment every entity that is connected to trigger, as longas their dependency type is listed in Table 7.Our observations can be seen in our running ex-ample.
The entities ?NFL?
and ?Atlanta?
dependson the trigger ?approves?
and ?Falcons?
depend onthe trigger ?stadium?.
Since their dependency typesare listed in Table 7, these entities are marked as can-didate arguments.A.5 Role DetectionEXEMPLAR determines the role of an argumentbased on the trigger type (noun or verb), the type ofdependency between the trigger and argument andthe direction of the dependency.
To take into ac-count the dependency direction, we prefix each de-pendency type with ?>?
when an entity depends onthe trigger and ?<?
when the trigger depends on theentity.Table 8 shows EXEMPLAR?s rules that assignroles to arguments for each relation type.
Rules aretriples (trigger , dependency , role) whose meaningis as follows:455Dependency Examplensubj (Subject) Romeo loves Julietdobj (Direct Object) The Prince exiles Romeonsubjpass (Pass.
Subj.)
Romeo was seen in Veronaagent (Pass.
Voice Obj.)
Juliet is loved by Romeoiobj (Indirect Object) Romeo gave Juliet a kissposs (Possessive) Romeo?s father Montagueappos (Apposition) Capulet, Juliet?s father,amod (Adj.
Modifier) The Italian city of Veronann (Noun Comp.
Mod.)
Romeo?s cousin Benvolioprep * (Prep.
Modifier) Romeo lived in Veronapartmod (Participal Mod.)
Romeo, born in Italyrcmod (Rel.
Clause Mod.)
Juliet, who loved RomeoTable 7: Dependencies connecting arguments andtriggers.
Arguments are in bold and triggers are un-derlined.If trigger type = trigger and dependencytype = dependency then assign role.For example, the first rule in Table 8a specifiesthat an argument must be assigned the role of a sub-ject if this argument depends on a verb trigger andthe dependency type is >nsubj.Rules are ordered by descending priority in Ta-ble 8.
In case several rules can be applied for an ar-gument, we apply only the rule with higher priority.If none of the rules applies to an argument, EXEM-PLAR removes this argument from the relation.Exceptions.
There are three exceptions for therules above.
The first exception concerns argu-ments of verb relations whose dependency type is<partmod or <rcmod.
EXEMPLAR chooses the roleof direct object (as oppose to subject) for these ar-guments when the verb trigger is in passive form.For instance, in the sentence ?Barbie, (which was)invented by Handler?, ?Barbie?
has the role di-rect object because ?invented?
is in passive form.The second exception is for nominalized verbsfollowed by the preposition ?by?, such as in ?Geor-gian invasion by Russia?.
Arguments of this typeof trigger with dependency types >nn, >amod or>poss are assigned the role direct object.Finallly, there is an exception for copula+nounrelations expressed with close appositions of theform: ?determiner entity noun?.
An example is ?theGreenwood Heights section of Brooklyn?.
Here,EXEMPLAR assigns the subject role to the entity be-tween the determiner and the noun.Trigger Type Dependency Type RoleVerb >nsubj subjectVerb >agent subjectVerb <partmod subjectVerb <rcmod subjectVerb >dobj direct objectVerb >subjpass direct objectVerb >iobj to objectVerb >prep * prep objectNoun >prep by subjectNoun >amod subjectNoun >nn subjectNoun >poss subjectNoun >prep of direct objectNoun >prep * prep object(a) Rules for verb relations.Trigger Type Dependency Type RoleNoun >nsubj subjectNoun >appos subjectNoun <appos subjectNoun <partmod subjectNoun <rcmod subjectNoun >prep of of objectNoun >amod of objectNoun >nn of objectNoun >poss of objectNoun >prep * prep object(b) Rules for copula+noun relations.Trigger Type Dependency Type RoleVerb >nsubj subjectVerb >agent subjectVerb <partmod subjectVerb <rcmod subjectVerb >iobj to objectVerb >prep * prep objectNoun >amod of objectNoun >nn of objectNoun >poss of objectNoun >prep * prep object(c) Rules for verb+noun relations.Table 8: Rules for assigning roles to arguments.A.6 Filtering RelationsThe final step in EXEMPLAR is to remove incom-plete relations.
EXEMPLAR removes relations withless than two arguments and relations that do notpresent subject and direct object.For EXEMPLAR, Lund and SwiRL, which extractn-ary relations, our evaluation needs to convert n-aryrelations into binary ones.
This is done by selectingall pairs of arguments from a n-ary relation and cre-ating a new (binary) relation for each of them.
Bi-nary relations containing two prepositional objects(or equivalent for SRL systems) are removed.456ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, pages 28?36, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 976?986, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2011.
An analysis of open informa-tion extraction based on semantic role labeling.
InProceedings of the Sixth International Conference onKnowledge Capture, pages 113?120.
Association forComputational Linguistics.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In Proceed-ings of Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, EMNLP-CoNLL?12, pages 708?716.Nilesh Dalvi, Ashwin Machanavajjhala, and Bo Pang.2012.
An analysis of structured data on the web.
Proc.VLDB Endow., 5(7):680?691.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open InformationExtraction.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing,EMNLP?12.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling ofpropbank.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP?08, pages 69?78, Stroudsburg, PA, USA.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL?03, pages 423?430, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Mausam, Michael Schmitz, Robert Bart, Stephen Soder-land, and Oren Etzioni.
2012.
Open language learningfor information extraction.
In Proceedings of Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, EMNLP-CoNLL?12.Yuval Merhav, Filipe de Sa?
Mesquita, Denilson Barbosa,Wai Gen Yee, and Ophir Frieder.
2012.
Extractinginformation networks from the blogosphere.
TWEB,6(3):11.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: a taxonomy of relationalpatterns with semantic types.
In Proceedings of the2012 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning, EMNLP-CoNLL?12, pages1135?1145, Stroudsburg, PA, USA.
Association forComputational Linguistics.Joakim Nivre and Jens Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of ComputationalNatural Language Learning, CoNLL?04.Evan Sandhaus.
2008.
The new york times anno-tated corpus.
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T19.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 8?15.
Associationfor Computational Linguistics.Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,and Denilson Barbosa.
2013.
Open information ex-traction with tree kernels.
In Proceedings of the 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 868?877, Atlanta, Georgia,June.
Association for Computational Linguistics.457
