Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 276?279,New York City, June 2006. c?2006 Association for Computational LinguisticsAutomatic Cluster Stopping with Criterion Functions and the Gap StatisticTed Pedersen and Anagha KulkarniDepartment of Computer ScienceUniversity of Minnesota, DuluthDuluth, MN 55812 USA{tpederse,kulka020}@d.umn.eduhttp://senseclusters.sourceforge.netAbstractSenseClusters is a freely available sys-tem that clusters similar contexts.
It canbe applied to a wide range of problems,although here we focus on word senseand name discrimination.
It supportsseveral different measures for automati-cally determining the number of clustersin which a collection of contexts shouldbe grouped.
These can be used to discoverthe number of senses in which a word isused in a large corpus of text, or the num-ber of entities that share the same name.There are three measures based on clus-tering criterion functions, and another onthe Gap Statistic.1 IntroductionWord sense and name discrimination are problemsin unsupervised learning that seek to cluster the oc-currences of a word (or name) found in multiple con-texts based on their underlying meaning (or iden-tity).
The assumption is made that each discoveredcluster will represent a different sense of a word, orthe underlying identity of a person or organizationthat has an ambiguous name.Existing approaches to this problem usually re-quire that the number of clusters to be discovered(k) be specified ahead of time.
However, in most re-alistic settings, the value of k is unknown to the user.Here we describe various cluster stopping measuresthat are now implemented in SenseClusters (Puran-dare and Pedersen, 2004) that will group N contextsinto k clusters, where the value of k will be automat-ically determined.Cluster stopping can be viewed as a problem inmodel selection, since a number of different models(i.e., clustering solutions) are created using differentvalues of k, and the one that best fits the observeddata is selected based on a criterion function.
Thisis reminiscent of earlier work on sequential modelselection for creating models of word sense disam-biguation (e.g., (O?Hara et al, 2000)), where it wasfound that forward sequential search strategies weremost effective.
These methods start with simplermodels and then add to them in a stepwise fash-ion until no further improvement in model fit is ob-served.
This is in fact very similar to what we havedone here, where we start with solutions based onone cluster, and steadily increase the number of clus-ters until we find the best fitting solution.SenseClusters supports four cluster stopping mea-sures, each of which is based on interpreting a clus-tering criterion function in some way.
The first threemeasures (PK1, PK2, PK3) look at the successivevalues of the criterion functions as k increases, andtry to identify the point at which the criterion func-tion stops improving significantly.
We have also cre-ated an adaptation of the Gap Statistic (Tibshiraniet al, 2001), which compares the criterion functionfrom the clustering of the observed data with theclustering of a null reference distribution and selectsthe value of k for which the difference between themis greatest.In order to evaluate our results, we sometimesconduct experiments with words that have beenmanually sense tagged.
We also create name con-276flations where some number of names of persons,places, or organizations are replaced with a singlename to create pseudo or false ambiguities.
For ex-ample, in this paper we refer to an example wherewe have replaced all mentions of Sonia Gandhi andLeonid Kuchma with a single ambiguous name.Clustering methods are typically either partitionalor agglomerative.
The main difference is that ag-glomerative methods start with 1 or N clusters andthen iteratively arrive at a pre?specified number (k)of clusters, while partitional methods start by ran-domly dividing the contexts into k clusters and theniteratively rearranging the members of the k clustersuntil the selected criterion function is maximized.
Inthis work we have used K-means clustering, whichis a partitional method, and the H2 criterion func-tion, which is the ratio of within?cluster similarity(I2) to between?cluster similarity (E1).2 MethodologyIn word sense or name discrimination, the num-ber of contexts (N) to cluster is usually very large,and considering all possible values of k from 1...Nwould be inefficient.
As the value of k increases,the criterion function will reach a plateau, indicat-ing that dividing the contexts into more and moreclusters does not improve the quality of the solution.Thus, we identify an upper bound to k that we referto as deltaK by finding the point at which the cri-terion function only changes to a small degree as kincreases.According to the H2 criterion function, the higherits ratio of within?cluster similarity to between?cluster similarity, the better the clustering.
A largevalue indicates that the clusters have high internalsimilarity, and are clearly separated from each other.Intuitively then, one solution to selecting k mightbe to examine the trend of H2 scores, and look forthe smallest k that results in a nearly maximum H2value.However, a graph of H2 values for a clusteringof the 2 sense name conflation Sonia Gandhi andLeonid Kuchma as shown in Figure 1 (top) revealsthe difficulties of such an approach.
There is a grad-ual curve in this graph and there is no obvious kneepoint (i.e., sharp increase) that indicates the appro-priate value of k.0.00450.00500.00550.00600.00650.00700.00750.00800 2 4 6 8 10 12 14 16H2 vs krrrrrrrrrrrrr r rr-1.5000-1.0000-0.50000.00000.50001.00000 2 4 6 8 10 12 14 16PK1 vs krrrrrrrrrrrr r rr0.95001.00001.05001.10001.15001.20001.25001.30002 4 6 8 10 12 14 16PK2 vs krrrrrrr r r rrr r r0.99001.00001.01001.02001.03001.04001.05001.06002 4 6 8 10 12 14PK3 vs krrrr r r rr rrrrrFigure 1: H2 (top) and PK1, PK2, and PK3 forthe name conflate pair Sonia Gandhi and LeonidKuchma.
The predicted number of senses is 2 forall the measures.2772.1 PK1The PK1 measure is based on (Mojena, 1977),which finds clustering solutions for all values of kfrom 1..N , and then determines the mean and stan-dard deviation of the criterion function.
Then, ascore is computed for each value of k by subtractingthe mean from the criterion function, and dividingby the standard deviation.
We adapt this techniqueby using the H2 criterion function, and limit k from1...deltaK:PK1(k) = H2(k) ?
mean(H2[1...deltaK])std(H2[1...deltaK])(1)To select a value of k, a threshold must be set.Then, as soon as PK1(k) exceeds this threshold,k-1 is selected as the appropriate number of clus-ters.
Mojena suggests values of 2.75 to 3.50, but alsostates they would need to be adjusted for differentdata sets.
We have arrived at an empirically deter-mined value of -0.70, which coincides with the pointin the standard normal distribution where 75% of theprobability mass is associated with values greaterthan this.We observe that the distribution of PK1 scorestends to change with different data sets, making ithard to apply a single threshold.
The graph of thePK1 scores shown in Figure 1 illustrates the diffi-culty : the slope of these scores is nearly linear, andas such any threshold is a somewhat arbitrary cutoff.2.2 PK2PK2 is similar to (Hartigan, 1975), in that both takethe ratio of a criterion function at k and k-1, in orderto assess the relative improvement when increasingthe number of clusters.PK2(k) = H2(k)H2(k ?
1) (2)When this ratio approaches 1, the clustering hasreached a plateau, and increasing k will have nobenefit.
If PK2 is greater than 1, then we shouldincrease k. We compute the standard deviation ofPK2 and use that to establish a boundary as to whatit means to be ?close enough?
to 1 to consider thatwe have reached a plateau.
Thus, PK2 will select kwhere PK2(k) is the closest to (but not less than) 1+ standard deviation(PK2[1...deltaK]).The graph of PK2 in Figure 1 shows an elbowthat is near the actual number of senses.
The criticalregion defined by the standard deviation is shaded,and note that PK2 selected the value of k that wasoutside of (but closest to) that region.
This is inter-preted as being the last value of k that resulted in asignificant improvement in clustering quality.
Notethat here PK2 predicts 2 senses, which correspondsto the number of underlying entities.2.3 PK3PK3 utilizes three k values, in an attempt to find apoint at which the criterion function increases andthen suddenly decreases.
Thus, for a given value ofk we compare its criterion function to the precedingand following value of k:PK3(k) = 2 ?
H2(k)H2(k ?
1) + H2(k + 1) (3)The form of this measure is identical to that of theDice Coefficient, although in set theoretic or prob-abilistic applications Dice tends to be used to com-pare two variables or sets with each other.PK3 is close to 1 if the H2 values form a line,meaning that they are either ascending, or they areon the plateau.
However, our use of deltaK elimi-nates the plateau, so in our case values of 1 show thatk is resulting in consistent improvements to clus-tering quality, and that we should continue.
WhenPK3 rises significantly above 1, we know that k+1is not climbing as quickly, and we have reached apoint where additional clustering may not be help-ful.
To select k we select the largest value ofPK3(k) that is closest to (but still greater than) thecritical region defined by the standard deviation ofPK3.PK3 is similar in spirit to (Salvador and Chan,2004), which introduces the L measure.
This tries tofind the point of maximum curvature in the criterionfunction graph, by fitting a pair of lines to the curve(where the intersection of these lines represents theselected k).2782.4 The Gap StatisticSenseClusters includes an adaptation of the GapStatistic (Tibshirani et al, 2001).
It is distinct fromthe measures PK1, PK2, and PK3 since it does notattempt to directly find a knee point in the graph ofa criterion function.
Rather, it creates a sample ofreference data that represents the observed data asif it had no meaningful clusters in it and was sim-ply made up of noise.
The criterion function of thereference data is then compared to that of the ob-served data, in order to identify the value of k in theobserved data that is least like noise, and thereforerepresents the best clustering of the data.To do this, it generates a null reference distri-bution by sampling from a distribution where themarginal totals are fixed to the observed marginalvalues.
Then some number of replicates of the ref-erence distribution are created by sampling from itwith replacement, and each of these replicates isclustered just like the observed data (for successivevalues of k using a given criterion function).The criterion function scores for the observed andreference data are compared, and the point at whichthe distance between them is greatest is taken to pro-vide the appropriate value of k. An example of thisis seen in Figure 2.
The reference distribution repre-sents the noise in the observed data, so the value ofk where the distance between the reference and ob-served data is greatest represents the most effectiveclustering of the data.Our adaption of the Gap Statistic allows us touse any clustering criterion function to make thecomparison of the observed and reference data,whereas the original formulation is based on usingthe within?cluster dispersion.3 AcknowledgmentsThis research is supported by a National ScienceFoundation Faculty Early CAREER DevelopmentAward (#0092784).ReferencesJ.
Hartigan.
1975.
Clustering Algorithms.
Wiley, NewYork.R.
Mojena.
1977.
Hierarchical grouping methods and4060801001201401601801 3 5 7 9 11 13 15 17 19 21 23 25 27 29I2(obs) vs krrrrrrrrr rrrr rrr rrr rrr rr rr rr rrrI2(ref) vs krrrrrrrrrrrrrrrr rrrrrrr rr rr rr rr1015202530354045501 3 5 7 9 11 13 15 17 19 21 23 25 27 29Gap vs krrrrrrrrrr r r rrr rrr rr r rr rrrr r r rFigure 2: I2 for observed and reference data (top)and the Gap between them (bottom) for the nameconflate pair Sonia Gandhi and Leonid Kuchma.
Thepredicted number of senses is 3.stopping rules: An evaluation.
The Computer Journal,20(4):359?363.T.
O?Hara, J. Wiebe, and R. Bruce.
2000.
Selectingdecomposable models for word-sense disambiguation:The grling-sdm system.
Computers and the Humani-ties, 34(1?2):159?164.A.
Purandare and T. Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of the Conferenceon Computational Natural Language Learning, pages41?48, Boston, MA.S.
Salvador and P. Chan.
2004.
Determining thenumber of clusters/segments in hierarchical cluster-ing/segmentation algorithms.
In Proceedings of the16th IEEE International Conference on Tools with AI,pages 576?584.R.
Tibshirani, G. Walther, and T. Hastie.
2001.
Esti-mating the number of clusters in a dataset via the Gapstatistic.
Journal of the Royal Statistics Society (SeriesB), pages 411?423.279
