Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 80?83,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Human-Computer Collaboration Approach to Improve Accuracy ofan Automated English Scoring System GJee Eun Kim Kong Joo LeeHankuk University of Foreign Studies Chungnam National UniversitySeoul, Korea Daejeon, Koreajeeeunk@hufs.ac.kr kjoolee@cnu.ac.krAbstractThis paper explores an issue of redundant errorsreported while automatically scoring Englishlearners?
sentences.
We use a human-computercollaboration approach to eliminate redundant er-rors.
The first step is to automatically select can-didate redundant errors using PMI and RFC.Since those errors are detected with different IDsalthough they represent the same error, the can-didacy cannot be confirmed automatically.
Theerrors are then handed over to human experts todetermine the candidacy.
The final candidatesare provided to the system and trained with a de-cision tree.
With those redundant errors eliminat-ed, the system accuracy has been improved.1 IntroductionAn automated English scoring system analyzes astudent sentence and provides a score and feedbackto students.
The performance of a system is eva-luated based on the accuracy of the score and therelevance of the feedback.The system described in this paper scores Eng-lish sentences composed by Korean students learn-ing English.
A detailed explanation of the systemis given in (Kim et al, 2007).
The scores are calcu-lated from three different phases including word,syntax and mapping, each of which is designed toassign 0~2 points.
Three scores are added up togenerate the final score.
A spelling error, a pluralform error, and a confusable word error are consi-dered as typical word errors.
A subject verbagreement error, a word order error and relativeclause error are typical examples of syntactic er-rors.
Even when a student sentence is perfectlycorrect in lexical and syntactic level, it may fail toconvey what is meant by the question.
Such sen-tences are evaluated as grammatical, but cannot bea correct answer for the question.
In this case, theerrors can only be recognized by comparing a stu-dent sentence with its correct answers.
The differ-ences between a student answer and one of theanswers can be considered as mapping errors.These three phases are independent from oneanother since they use different processing method,and refer different information.
Independency ofthree phases causes some problems.
(Ex1)  Correct answer: The earth is bigger than the moon.Student answer: The earth is small than the Moon.Err1: MODIFIER_COMP_ERR|4-7| syntacticErr2: LEXICAL_ERROR|4| mapping(Ex1) is an example of error reports providedto a student.
The following two lines in (Ex1) showthe error information detected from the studentanswer by the system.
Err1 in (Ex1) reports a com-parative form error of an adjective ?small?, whichcovers the 4 ~ 7th words of the student sentence.Err2 indicates that the 4th word ?small?
of the stu-dent sentence is different from that of the answersentence.
The difference was identified by compar-ing the student sentence and the answer sentence.Err1 was detected at the syntactic phase whereasErr2 was at the mapping phase.
These two errorspoints to the same word, but have been reported asdifferent errors.
(Ex2)  Correct answer: She is too weak to carry the bag.Student answer: She is too weak to carry the her bag.Err1: EXTRA_DET_ERR|7-9| syntacticErr2: UNNECESSARY_NODE_ERR|8|(her) mappingSimilarly, Err1 in (Ex2) reports an incorrectuse of an article at the 7~9th words.
The syntacticanalysis recognizes that ?the?
and ?her?
cannot oc-cur consecutively, but it is not capable of deter-mine which one to eliminate.
Err2, on the otherhand, pinpoints ?her?
as an incorrectly used wordby comparing the student sentence and the answersentence.
(Ex1) and (Ex2) have presented the errorswhich are detected at different processing phases,80but represent the same error.
Since these redundanterrors are a hindering factor to calculate accuratescores, one of the errors has to be removed.
Theproposed system deals with 70 error types; 16 forword, 46 for syntax, and 14 for mapping.
In thispaper, we have adopted a human-computer colla-boration approach by which linguistic experts as-sist the system to decide which one of theredundant errors should be removed.2 Redundant ErrorsThe system-detected errors are reported in the fol-lowing format:Error_ ID | Error_ Position | Error_Correction_InfoEach error report is composed of three fields whichare separated by ?|?.
The first field contains erroridentification.
The second includes the numbersindicating where the error is detected in a studentinput sentence.
For example, if the field has num-ber ?5-7?, it can be interpreted as the input sen-tence has an error covering from the 5th word to 7thword.
Since syntactic errors are usually detected ata phrasal level, the position of an error covers morethan one word.
The third field may or may not befilled with a value, depending on the type of anerror.
When it has a value, it is mostly a suggestion,i.e.
a corrected string which is formed by compar-ing a student sentence with its corresponding cor-rect answer.2.1 Definition of Redundant Errors(Condition 1) The errors should share an errorposition.
(Condition 2) The errors should be detectedfrom different error process phases.
(Condition 3) The errors should represent lin-guistically the same phenomenon.
(Condition 1) implies that the two errors mustdeal with one or more common words.
The posi-tion is indicated on the student sentence.
However,there are some exceptions in displaying the posi-tion.
An example of the exception is ?OBLI-GATORY_NODE_MISSING_ERR?
and ?OP-TIONAL_NODE_MISSING_ERR?
which aremapping errors.
Since these errors are detectedwhen a certain word is missing from a student in-put but included in the answer, the position is indi-cated on the answer sentence.
Err5 and Err6 from(Ex3) represent the case.
Error position ?(7)?
and?(8)?
1 means that the 7th and 8th word of the an-swer sentence, ?to?
and ?our?
are missing, respec-tively.
When an error position points to an answersentence not a student sentence, the error cannot bechecked with whether it includes the words sharedwith the errors whose positions indicate the studentsentence.
In this case, the error is assumed to haveshared words with all the other errors; Err5 andErr6 are considered containing shared words withErr 1~4 in (Ex3).
(Ex3)Correct answer: She is a teacher who came to our school last week.Student answer: She is a teacher who come school last week.Err1: CONFUSABLE_WORD_ERR|9|week wordErr2: SUBJ_VERB_AGR_ERR|3-7| syntacticErr3: VERB_SUBCAT_ERR|6-7| syntacticErr4: TENSE_UNMATCHED_ERR|6|came[past] mappingErr5: OPTIONAL_NODE_MISSING_ERR|(7)|to mappingErr6: OPTIONAL_NODE_MISSING_ERR|(8)|our mappingErr1 and Err2 from (Ex3) cannot be redundanterrors since they do not share an error position andaccordingly do not satisfy Condition 1.
Err2 andErr3 share error positions 6~7, but they are not alsoconsidered as redundant errors since both of themwere detected at the same process phase, the syn-tactic phase.
Err2 and Err4 satisfy both Condition 1and 2, but fail to meet Condition 3.
Err2 representsthe subject-predicate agreement error whereas Err4points out a tense error.
In comparison, Err3 andErr5 are legitimate candidates of ?redundant er-rors?
since they satisfy all the conditions.
Theyshare error positions, but were detected from dif-ferent error process phases, the syntactic phase andthe mapping phase, respectively.
They also dealwith the same linguistic phenomenon that a verb?come?
does not have a transitive sense but re-quires a prepositional phrase led by ?to?.2.2 Detection of Redundant ErrorsTwo errors need to satisfy all the conditions men-tioned in section 2.1 in order to be classified asredundant errors.
The system?s detecting processbegan with scoring 14,892 student answers.
Fromthe scoring result, the candidates which met Condi-tion 1 and 2 were selected.
In the following subsec-tions, we have described how to determine thefinal redundant errors using the system in collabo-ration with human?s efforts.1 Error positions in answer sentences are marked with a num-ber surrounded by a pair of parenthesis.812.2.1 Selection of the CandidatesThe system selected candidate errors which satis-fied Condition 1 and 2 among the student sen-tences.
For example, Table 1 presents 8 candidatesextracted from (Ex3).1 CONFUSABLE_WORD_ERR|9|weekOPTIONAL_NODE_MISSING_ERR|(7)|to2 CONFUSABLE_WORD_ERR|9|weekOPTIONAL_NODE_MISSING_ERR|(8)|our3 SUBJ_VERB_AGR_ERR|3-7|TENSE_UNMATCHED_ERR|6|came[past]4 SUBJ_VERB_AGR_ERR|3-7|OPTIONAL_NODE_MISSING_ERR|(7)|to5 SUBJ_VERB_AGR_ERR|3-7|OPTIONAL_NODE_MISSING_ERR|(8)|our6 VERB_SUBCAT_ERR|6-7|TENSE_UNMATCHED_ERR|6|came[past]7 VERB_SUBCAT_ERR|6-7|OPTIONAL_NODE_MISSING_ERR|(7)|to8 VERB_SUBCAT_ERR|6-7|OPTIONAL_NODE_MISSING_ERR|(8)|ourTable 1 Candidate pairs of errors extracted from (Ex3).As a result of the selection process, the total of150,419 candidate pairs was selected from 14,892scoring results of the student sentences.2.2.2 Filtering Candidate ErrorsThe candidates extracted through the process men-tioned in 2.2.1 were classified based on their erroridentifications only, without considering error po-sition and error correction information.
150,419pairs of the errors were assorted into 657 types.The frequency of each type of the candidates wasthen calculated.
These candidate errors were fil-tered by applying PMI (Pointwise Mutual Informa-tion) and RFC (Relative Frequency Count) (Su etal., 1994).
)()(),(log),(212121EPEPEEPEEPMI =      (1)freqEEfreqEERFC),(),(2121 =          (2)PMI is represented by a number indicating howfrequently two errors E1 and E2 occur simulta-neously.
RFC refers to relative frequency againstaverage frequency of the total candidates.
The fil-tering equation is as follows:kEERFCEEPMI ??
),(),( 2121     (3)Using this equation, the system filtered the candi-dates whose value was above the threshold k. Forthis experiment, 0.4 was assigned to k and 111 er-ror types were selected.2.2.3 Human Collaborated FilteringFiltered 111 error types include 29,588 candidateerrors; on the average 278 errors per type.
Theseerrors were then handed over to human experts2 toconfirm their candidacy.
They checked Condition3 against each candidate.
The manually filteredresult was categorized into three classes as shownin Table 2.Class A:(number:20)(DET_NOUN_CV_ERR,DET_UNMATCHED_ERR)(EXTRA_DET_ERR, DET_UNMATCHED_ERR)(MODIFIER_COMP_ERR, FORM_UNMATCHED_ERR)(MISSPELLING_ERR, LEXICAL_ERR)?Class B:(number:47)(SUBJ_VERB_AGR_ERR,TENSE_UNMATCHED_ERR)(AUX_MISSING_ERR, UNNECESSARY_NODE_ERR)(CONJ_MISSING_ERR, DET_UNMATCHED_ERR)?Class C:(number:44)(VERB_FORM_ERR, ASPECT_UNMATCHED_ERR)(VERB_ING_FORM_ERR, TENSE_UNMATCHED_ERR)(EXTRA_PREP_ERR, UNNECESSARY_NODE_ERR)?Table 2 Classes of Human Collaborated Filtering.Class A satisfies Condition 1 and 2 and is con-firmed as redundant errors.
When a pair of errors isa member of Class A, one of the errors can be re-moved.
Class B also meets Condition 1 and 2, butis eliminated from the candidacy because humanexperts have determined they did not deal with thesame linguistic phenomenon.
Each error of Class Bhas to be treated as unique.
With respect to Class C,the errors cannot be determined its candidacy withthe information available at this stage.
Additionalinformation is required to determine the redundan-cy.2.2.4 Final Automated Filtering Using De-cision RulesIn order to confirm the errors of Class C as redun-dant, additional information is necessary.
(Ex4)   Correct answer: I don?t know why she went there.Student answer: I don?t know why she go to their.Err1: CONFUSABLE_WORD_ERR|8|there wordErr2: SUBJ_VERB_AGR_ERR|6|went[3S] syntacticErr3: EXTRA_PREP_ERR|6-8| syntacticErr4: UNNECESSARY_NODE_ERR|7|(to) mappingErr5: TENSE_UNMATCHED_ERR|6|went[past] mapping(Ex5)   Correct answer: Would you like to come?Student answer: you go to home?Err1: FIRST_WORD_CASE_ERR|1| wordErr2: EXTRA_PREP_ERR|3-4| syntacticErr3:OBLIGATORY_NODE_MISSING_ERR|(1,3)|Would _ likemappingErr4: UNNECESSARY_NODE_ERR|4|(home) mappingErr5: LEXICAL_ERR|2|come mapping2 They are English teachers who have a linguistic backgroundand teaching experiences of 10 years or more.82EXTRA_PREP_ERR?
and ?UNNECESSARY_NODE_ERR?
were selected as a candidate fromboth (Ex4) and (Ex5) through the steps mentionedin section 2.2.1 ~ 2.2.3.
The pair from (Ex4) is aredundant error, but the one from (Ex5) is a falsealarm.
(Ex4) points out a preposition ?to?
as an un-necessary element whereas (Ex5) indicates a noun?home?
as incorrect.To determine the finalist of redundant errors,we have adopted a decision tree.
To train the deci-sion tree, we have chosen a feature set for a pair oferrors (E1, E2) as follows.
(1) The length of shared words in E1 and  E2 divided by thelength of  a shorter sentence (shared_length)(2) The length of non-shared words in E1 and E2 divided by thelength of a shorter sentence.
(non_shared_length)(3) The Error_Correction_Info of E1 (E1.Correction_Info)(4) The Error_Correction_Info of E2 (E2.Correction_Info)(5) Edit distance value between correction string of E1 and E2(edit_distance)(6) Error Position of E1 (E1.pos)(7) Error Position of E2 (E2.pos)(8) Difference of Error positions of E1 and E2 (diff_error_pos)12,178 pairs of errors for 44 types in Class C wereused to train a decision tree.
We used CART(Breiman et al, 1984) to extract decision rules.The followings show a part of the decision rules toeliminate redundant errors from Class C.E1=CONJ_MISSING_ERRE2=OPTIONAL_NODE_MISSING_ERRIf  E2.Correction_Info=?conj?
and  E2.pos=1   then redundant_errorE1=EXTRA_PREP_ERR,  E2=UNNECESSARY_NODE_ERRIf  E2.Correction_Info=?prep?
and  E2.pos=1   then redundant_errorE1=VERB_SUBCAT_ERR,E2=OPTIONAL_NODE_MISSING_ERRIf  diff_error_pos <=3 and E2.Correction_Info={?prep?
, ?adv?
}then redundant_errorE1=VERB_ING_FORM_ERR,  E2=TENSE_UNMATCHED_ERRIf  E2.Correction_Info=?verb-ing?
then redundant_error?The errors are removed according to a priority spe-cified in the rules.
The syntactic phase is assignedwith the highest priority since syntactic errors havethe most extensive coverage which is identified ata phrasal level.
On the other hand, the lowest prior-ity is given to the mapping phase because mappingerrors are detected through a simple word-to-wordcomparison of a student input with the correct an-swer.3 EvaluationWe evaluated the accuracy of determining redun-dant errors.
Table 3 presents the results.
The evalu-ation was performed on 200 sentences which werenot included in the training data.
Even though theredundancy of the pairs of errors in Class A andClass B are determined by the human expert, theaccuracies of both classes did not reach 100% be-cause the errors detected by the system were incor-rect.
The total accuracy including Class A, B, andC was 90.2%.Class A Class B Class CAccuracy 94.1% 98.0% 82.3%Table 3: The accuracyThe performance of our automated scoring sys-tem was measured using exact agreement (Attaliand Burstein, 2006) of the final scores calculatedby the system and human raters.
The overall per-formance was improved by 2.6% after redundanterrors were removed.4 ConclusionThis paper has introduced a human collaboratedfiltering method to eliminate redundant errors re-ported during automated scoring.
Since scoringprocesses are performed through three separatephases including word, syntax and mapping, someof the errors are redundantly reported with differ-ent IDs.
In addition, it is almost impossible to pre-dict every type of errors that could occur in studentanswers.
Because of these issues, it is not easy forthe system to automatically determine which errorsare reported redundantly, or to estimate all thepossible redundant errors.
As a solution to theseproblems, we have adopted a human assisted ap-proach.
The performance has been improved afterredundant errors were removed with the approachimplemented in the system.ReferencesJee Eun Kim, K. J. Lee and K. A. Jin.
2007.
Building an Au-tomated Scoring System for a Single Sentence.
Korea In-formation Processing Society Vol.4, No.3 (in Korean).Keh-Yih Su, Ming-Wen We and Jing-Shin Chang.
1994.
ACorpus-based Approach to Automatic Compound Extrac-tion, In Proceedings of the ACL 94.Leo Breiman, Jerome Friedman, Charles J.
Stone, and R.A.Olshen.
1984.
Classification and Regression Trees.
Mon-terey, Calif., U.S.A.: Wadsworth, Inc.Yigal Attali and Jill Burstein.
2006.
Automated Essay Scoringwith e-rater?
V.2.83
