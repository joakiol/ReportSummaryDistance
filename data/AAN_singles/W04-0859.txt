The University of Alicante systems at SENSEVAL-3?Sonia Va?zquez, Rafael RomeroArmando Sua?rez and Andre?s MontoyoDpt.
of Software and Computing SystemsUniversidad de Alicante, Spain{svazquez,romero}@dlsi.ua.es{armando,montoyo}@dlsi.ua.esIulia Nica and Antonia Mart??
?Dpt.
of General LinguisticsUniversidad de Barcelona, Spainiulia@clic.fil.ub.esamarti@ub.eduAbstractThe DLSI-UA team is currently working on sev-eral word sense disambiguation approaches, bothsupervised and unsupervised.
These approaches arebased on different ways to use both annotated andunannotated data, and several resources generatedfrom or exploiting WordNet (Miller et al, 1993),WordNet Domains, EuroWordNet (EWN) and addi-tional corpora.
This paper presents a view of differ-ent system results for Word Sense Disambiguationin different tasks of SENSEVAL-3.1 IntroductionWord Sense Disambiguation (WSD) is an open re-search field in Natural Language Processing (NLP).The task of WSD consists in assigning the correctsense to words in a particular context using an elec-tronic dictionary as the source of words definitions.This is a difficult problem that is receiving a greatdeal of attention from the research community.At the Second International Workshop onEvaluating Word Sense Disambiguation Systems,SENSEVAL-2, several supervised and unsupervisedsystems took part.
The more successful systems re-lay on corpus-based and supervised learning meth-ods.
At SENSEVAL-2 the average level of accu-racy achieved rounded 70%, which is insufficientfor such other NLP tasks as information retrieval,machine translation, or question answering.The DLSI-UA systems were applied to threeSENSEVAL-3 tasks: English all-words, English lex-ical sample and Spanish Lexical Sample.
Our sys-tems use both corpus-based and knowledge-basedapproaches: Maximum Entropy(ME) (Lau et al,1993; Berger et al, 1996; Ratnaparkhi, 1998) isa corpus-based and supervised method based onlinguistic features; ME is the core of a bootstrap-ping algorithm that we call re-training inspired?
This paper has been partially supported by the SpanishGovernment (CICyT) under project number TIC-2003-7180and the Valencia Government (OCyT) under project numberCTIDIB-2002-151by co-training (Blum and Mitchell, 1998); Rele-vant Domains (RD) (Montoyo et al, 2003) is aresource built from WordNet Domains (Magniniand Cavaglia, 2000) that is used in an unsuper-vised method that assigns domain and sense la-bels; Specification Marks(SP) (Montoyo and Palo-mar, 2000) exploits the relations between synsetsstored in WordNet (Miller et al, 1993) and does notneed any training corpora; Commutative Test (CT)(Nica et al, 2003), based on the Sense Discrimi-nators device derived from EWN (Vossen, 1998),disambiguates nouns inside their syntactic patterns,with the help of information extracted from raw cor-pus.A resume of which methods and how were usedin which SENSEVAL-3 tasks is shown in Table 1.DLSI-UA Method CombinedSystems ResultsALL-NOSU RD NoLS-ENG-SU Re-t NoLS-ENG-NOSURD NoLS-SPA-SU ME+Re-t NoLS-SPA-NOSU SM + ME Nouns: SMVerbs and adj.
: MELS-SPA- Pattern-Nica Nouns: SMPATTERN + ME Verbs and adj.
: METable 1: DLSI-UA Systems at SENSEVAL-3Most of these methods are relatively new and ourgoal when participating at SENSEVAL-3 is to evalu-ate for the first time such approaches.
At the mo-ment of writing this paper we can conclude thatthese are promising contributions in order to im-prove current WSD systems.In the following section each method is describedbriefly.
Then, details of how the SENSEVAL-3 trainand testing data were processed are shown.
Next,the scores obtained by each system are explained.Finally, some conclusions and future work are pre-sented.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systems2 Methods and AlgorithmsIn this section we describe the set of methods andtechniques that we used to build the four systemsthat had participated in SENSEVAL-3.2.1 Re-training and Maximum EntropyIn this section, we describe our bootstrappingmethod, which we call re-training.
Our methodis derived from the co-training method.
Our re-training system is based on two different views ofthe data (as is also the case for co-training), de-fined using several groups of features from those de-scribed in Figure 1, with several filters that ensure ahigh confidence sense labelling.?
the target word itself?
lemmas of content-words at positions ?1, ?2, ?3?
words at positions ?1, ?2,?
words at positions ?1, ?2, ?3?
content-words at positions ?1, ?2, ?3?
POS-tags of words at positions ?1, ?2, ?3?
lemmas of collocations at positions (?2,?1),(?1,+1), (+1,+2)?
collocations at positions (?2,?1), (?1,+1),(+1,+2)?
lemmas of nouns at any position in context, occur-ring at least m% times with a sense?
grammatical relation of the target word?
the word that the target word depends on?
the verb that the target word depends on?
the target word belongs to a multi-word, as identi-fied by the parser?
ANPA codes (Spanish only)?
IPTC codes (Spanish only)Figure 1: Features Used for the Supervised Learn-ingThese two views consist of two weak ME learn-ers, based on different sets of linguistic features,for every possible sense of a target word.
We de-cided to use ME as the core of our bootstrappingmethod because it has shown to be competitive inWSD when compared to other machine learning ap-proaches (Sua?rez and Palomar, 2002; Ma`rquez etal., 2003).The main difference with respect co-training isthat the two views are used in parallel in order toget a consensus of what label to assign to a particu-lar context.
Additional filters will ultimately deter-mine which contexts will then be added to the nexttraining cycle.Re-training performs several binary partial train-ings with positive and negative examples for eachsense.
These classifications must be merged in aunique label for such contexts with enough evidenceof being successfully classified.
This ?evidence?
re-lies on values of probability assigned by the MEmodule to positive and negative labels, and the factthat the unlabeled example is classified as positivefor a unique sense only.
The set of new labeled ex-amples feeds the training corpora of the next itera-tion with positive and negative examples.
The stop-ping criteria is a certain number of iterations or thefailure to obtain new examples from the unlabeledcorpus.2.2 Specification MarksSpecification Marks is an unsupervised WSDmethod over nouns.
Its context is the group of wordsthat co-occur with the word to be disambiguated inthe sentence and their relationship to the noun tobe disambiguated.
The disambiguation is resolvedwith the use of the WordNet lexical knowledge base.The underlying hypothesis of the method wepresent here is that the higher the similarity betweentwo words, the larger the amount of informationshared by two concepts.
In this case, the informa-tion commonly shared by two concepts is indicatedby the most specific concept that subsumes themboth in the taxonomy.The input for the WSD module is a group ofnouns W = {w1, w2, ..., wn} in a context.
Eachword wi is sought in WordNet, each having an asso-ciated set of possible senses Si ={Si1, Si2, ..., Sin},and each sense having a set of concepts in the IS-Ataxonomy (hypernymy/hyponymy relations).
First,the common concept to all the senses of the wordsthat form the context is gathered.
This concept ismarked by the initial specification mark (ISM).
Ifthis initial specification mark does not resolve theambiguity of the word, we then descend throughthe WordNet hierarchy, from one level to another,assigning new specification marks.
The number ofconcepts contained within the subhierarchy is thencounted for each specification mark.
The sense thatcorresponds to the specification mark with the high-est number of words is the one chosen as the sensedisambiguated within the given contextWe define six heuristics for our system: Heuris-tic of Hypernym, Heuristic of Definition, Heuristicof Common Specification Mark, Heuristic of GlossHypernym, Heuristic of Hyponym and Heuristic ofGloss Hyponym.2.3 Relevant DomainsThis is an unsupervised WSD method based on theWordNet Domains lexical resource (Magnini andCavaglia, 2000).
The underlying working hypoth-esis is that domain labels, such as ARCHITEC-TURE, SPORT and MEDICINE provide a naturalway to establish semantic relations between wordsenses, that can be used during the disambiguationprocess.
This resource has already been used onWord Sense Disambiguation (Magnini and Strappa-rava, 2000), but it has not made use of glosses infor-mation.
So our approach make use of a new lexicalresource obtained from glosses information namedRelevant Domains.First step is to obtain the Relevant Domains re-source from WordNet glosses.
For this task is nec-essary a previous part-of-speech tagging of Word-Net glosses (each gloss has associated a domain la-bel).
So we extract all nouns, verbs, adjectives andadverbs from glosses and assign them their associ-ated domain label.
With this information and usingthe Association Ratio formula (w=word,D=domainlabel), in (1), we obtain the Relevant Domains re-source.AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)The final result is for each word, a set of domainlabels sorted by Association Ratio, for example,for word plant?
its Relevant Domains are: genetics0.177515, ecology 0.050065, botany 0.038544 .
.
.
.Once obtained Relevant Domains the disam-biguation process is carried out.
We obtain fromthe text source the context words that co-occur withthe word to be disambiguated (context could bea sentence or a window of words).
We obtain acontext vector from Relevant Domains and contextwords (in case of repeated domain labels, they areweighted).
Furthermore we need a sense vector ob-tained in the same way as context vector from wordsof glosses of each word sense.
We select the cor-rect sense using the cosine measure between con-text vector and sense vectors.
So the selected senseis that for which the cosine with the context vectoris closer to one.2.4 Pattern-NicaThis is an unsupervised method only for Spanishnouns exploiting both EuroWordNet and corpus.In this method we adopt a different approach toWSD: the occurrence to be disambiguated is con-sidered not separately, but integrated into a syn-tactic pattern, and its disambiguation is carriedout in relation to this pattern.
A syntactic pat-tern is a triplet X-R-Y, formed by two lexical con-tent units X and Y and an eventual relational el-ement R, which corresponds to a syntactic rela-tion between X and Y.
Examples: [X=canal-nounR=de-preposition Y=televisio?n-noun], [X=pasaje-noun R=?
Y=ae?reo-adjective].
The strategy isbased on the hypothesis that syntactic patterns inwhich an ambiguous occurrence participates havedecisive influence on its meaning.
We also assumethat inside a syntactic pattern a word will tend tohave the same sense: the ?quasi one sense per syn-tactic pattern?
hypothesis.
The method works as fol-lows:Step 1, the identification of the syntactic patternsof the ambiguous occurrence;Step 2, the extraction of information related to it:from corpus and from the sentential context;Step 3, the application of the WSD algorithm onthe different information previously obtained;Step 4, the final sense assignment by combiningthe partial sense proposals from step 3.For step 1, we POS-tag the test sentence and ex-tract the sequences that correspond to previously de-fined combinations of POS tags.
We only kept thepatterns with frequency 5 or superior.In step 2, we use a search corpus previously POS-tagged.
For every syntactic pattern of the ambigu-ous occurrence X, we obtain from corpus two sets ofwords: the substitutes of X into the pattern (S1) andthe nouns that co-occur with the pattern in any sen-tence from the corpus (S2), In both cases, we keeponly the element with frequency 5 or superior.We perform step 3 by means of the heuristics de-fined by the Commutative Test (CT) algorithm ap-plied on each set from 2.
The algorithm is relatedto the Sense Discriminators (SD) lexical device, anadaptation of the Spanish WordNet, consisting in aset of sense discriminators for every sense of a givennoun in WordNet.
The Commutative Test algorithmlays on the hypothesis that if an ambiguous occur-rence can be substituted in a syntactic pattern by asense discriminator, then it can have the sense cor-responding to that sense discriminator.For step 4, we first obtain a sense assignment inrelation with each syntactic pattern, by intersectingthe sense proposals from the two heuristics corre-sponding to a pattern; then we choose the most fre-quent sense between those proposed by the differ-ent syntactic patterns; finally, if there are more finalproposed senses, we choose the most frequent senseon the base of sense numbers in WordNet.The method we propose for nouns requires only alarge corpus, a minimal preprocessing phase (POS-tagging) and very little grammatical knowledge, soit can easily be adapted to other languages.
Senseassignment is performed exploiting information ex-tracted from corpus, thus we make an intensive useof sense untagged corpora for the disambiguationprocess.3 Tasks ProcessingAt this point we explain for each task the systemsprocessing.
The results of each system are shown inTable2:DLSI-UA Systems Precision RecallLS-SPA-SU 84% 84%LS-ENG-SU 82% 32%ALL-NOSU 34% 28%LS-ENG-NOSU 32% 20%LS-SPA-NOSU 62% 62%LS-SPA-PATTERN 84% 47%Table 2: Results at SENSEVAL-33.1 DLSI-UA-LS-SPA-SUOur system, based on re-training and maximum en-tropy methods, processed both sense labelled andunlabelled Spanish Lexical Sample data in threeconsecutive steps:Step 1, analyzing the train corpus: words whichmost frequent sense is under 70% were selected.For each one of these words, each feature was usedin a 3-fold cross-validation in order to determine thebest set of features for re-training.Step 2, feeding training corpora: for these se-lected words, based on the results of the previousstep, each training corpus was enriched with newexamples from the unlabelled data using re-training.Step 3, classifying the test data: for the selectedwords, re-training was used again to obtain a first setof answers with, a priori, a label with a high level ofconfidence; the remaining contexts that re-trainingcould not classify were processed with the ME sys-tem using a unique set of features for all words.The lemmatization and POS information suppliedinto the SENSEVAL-3 Spanish data were the infor-mation used for defining the features of the system.0ur system obtained an accuracy of 0.84 for theSpanish lexical sample task.
Unfortunately, a shal-low analysis of the answers revealed that the UA.5system performed slightly worse than if only the ba-sic ME system were used1.
This fact means that thenew examples extracted from the unlabelled data in-troduced too much noise into the classifiers.
Be-cause this anomalous behavior was present only onsome words, a complete study of such new exam-ples must be done.
Probably, the number of itera-tions done by re-training over unlabelled data weretoo low and the enrichment of the training corporanot large enough.1The ME system, without using re-training, has not com-peted at SENSEVAL-3: our own scoring of these set of answersreported an accuracy of 0.8563.2 DLSI-UA-LS-ENG-SUIn the English Lexical Sample task our system goalwas to prove that the re-training method ensures ahigh level of precision.By means of a 3-fold cross-validation of the traindata, the features were ordered from higher to lowerprecision.
Based on this information, four execu-tions of re-training over the test data were done withdifferent selections of features for the two views ofthe method.
Each execution feed the learning cor-pora of the next one with new examples, those thatre-training considered as the most probably correct.For this system Minipar parser (Lin, 1998)wasused to properly add syntactic information to thetraining and testing data.Almost 40% of the test contexts were la-belled by our system, obtaining these scores (for?fine-grained?
and ?coarse-grained?, respectively):0.782/0.828 precision and 0.310/0.329 recall.
In ouropinion, such results must be interpreted as verypositive because the re-training method is able tosatisfy a high level of precision if the parameters ofthe system are correctly set.3.3 DLSI-UA-ALL-NOSU andDLSI-UA-LS-ENG-NOSUIn the English All Words and English Lexical Sam-ple tasks RD system was performed with informa-tion obtained from Relevant Domains resource us-ing for the disambiguation process all the 165 do-main labels.For All Words task we used as input informationall nouns, verbs, adjectives and adverbs present ina 100 words window around the word to be disam-biguated.
So our system obtained a 34% of preci-sion and a reduced recall around 28%.For Lexical Sample task we used all nouns, verbs,adjectives and adverbs present in the context of eachinstance obtaining around 32% precision.We obtained a reduced precision due to we use allthe domains label hierarchy.
In some experimentsrealized on SENSEVAL-2 data, our system obtaineda more high precision when grouping domains intothe first three levels.
Therefore we expect with re-ducing the number of domains labels, an improve-ment on precision.3.4 DLSI-UA-LS-SPA-NOSUWe used a combined system for Spanish LexicalSample task, using the SM method for disambiguat-ing nouns and the ME method for disambiguatingverbs and adjectives.
We obtained around 62% pre-cision and a 62% recall.3.5 DLSI-UA-LS-SPA-PATTERNOur goal when participating in this task was todemonstrate that the applying of syntactic patternsto WSD maintains high levels of precision.In this task we used also a combined system forSpanish Lexical Sample task, using Pattern-Nicamethod for disambiguating nouns and ME methodfor disambiguating verbs and adjectives.
We ob-tained around 84% precision and a 47% recall.4 ConclusionsThe supervised systems for the English and Span-ish lexical sample tasks are very competitive.
Al-though the processing of the train and test data wasdifferent for each task, both systems rely on re-training, a bootstrapping method, that uses a max-imum entropy-based WSD module.The results for the English task prove that re-training is capable of maintaining a high level ofprecision.
Nevertheless, for the Spanish task, al-though the scores achieved were excellent, the sys-tem must be redesigned in order to improve the clas-sifiers.The re-training method is a proposal that we aretrying to incorporate into text retrieval and ques-tion answering systems that could take advantage ofsense disambiguation of a subset of words.The unsupervised systems presented here doesnot appear to be sufficient for a stand-alone WSDsolution.
Wether these methods can be combinedwith other supervised methods to improve their re-sults requires further investigation.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Com-putational Linguistics, 22(1):39?71.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of the 11th Annual Conference onComputational Learning Theory, pages 92?100,Madison, Wisconsin, July.
ACM Press.R.
Lau, R. Rosenfeld, and S. Roukos.
1993.Adaptative statistical language modeling usingthe maximum entropy principle.
In Proceedingsof the Human Language Technology Workshop,ARPA.Dekang Lin.
1998.
Dependency-based evaluationof minipar.
In Proceedings of the Workshop onthe Evaluation of Parsing Systems, First Inter-national Conference on Language Resources andEvaluation, Granada, Spain.Bernardo Magnini and Gabriela Cavaglia.
2000.Integrating Subject Field Codes into WordNet.
InM.
Gavrilidou, G. Crayannis, S. Markantonatu,S.
Piperidis, and G. Stainhaouer, editors, Pro-ceedings of LREC-2000, Second InternationalConference on Language Resources and Evalu-ation, pages 1413?1418, Athens, Greece.Bernardo Magnini and C. Strapparava.
2000.
Ex-periments in Word Domain Disambiguation forParallel Texts.
In Proceedings of the ACL Work-shop on Word Senses and Multilinguality, HongKong, China.Llu?
?s Ma`rquez, Fco.
Javier Raya, John Car-roll, Diana McCarthy, Eneko Agirre, DavidMart?
?nez, Carlo Strapparava, and AlfioGliozzo.
2003.
Experiment A: several all-wordsWSD systems for English.
Technical ReportWP6.2, MEANING project (IST-2001-34460),http://www.lsi.upc.es/?nlp/meaning/meaning.html.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J. Miller.1993.
Five Papers on WordNet.
Special Issue ofthe International journal of lexicography, 3(4).Andre?s Montoyo and Manuel Palomar.
2000.
WordSense Disambiguation with Specification Marksin Unrestricted Texts.
In Proceedings of 11th In-ternational Workshop on Database and ExpertSystems Applications (DEXA 2000), pages 103?107, Greenwich, London, UK, September.
IEEEComputer Society.Andre?s Montoyo, Sonia Va?zquez, and GermanRigau.
2003.
Me?todo de desambiguacio?n le?xicabasada en el recurso le?xico Dominios Rele-vantes.
Procesamiento del Lenguaje Natural, 30,september.Iulia Nica, Antonia Mart?
?, and Andre?s Mon-toyo.
2003.
Colaboracio?n entre informacio?nparadigma?tica y sintagma?tica en la desam-biguacio?n sema?ntica automa?tica.
XIX Congresode la SEPLN 2003.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
Ph.D. thesis, University of Pennsylvania.Armando Sua?rez and Manuel Palomar.
2002.A maximum entropy-based word sense disam-biguation system.
In Hsin-Hsi Chen and Chin-Yew Lin, editors, Proceedings of the 19th In-ternational Conference on Computational Lin-guistics, pages 960?966, Taipei, Taiwan, August.COLING 2002.Piek Vossen.
1998.
EuroWordNet: Building a Mul-tilingual Database with WordNets for EuropeanLanguages.
The ELRA Newsletter, 3(1).
