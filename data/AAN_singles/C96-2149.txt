Learning Lineal" Precedence RulesVlad imi r  Per i c l ievMathema.
t i ca l  L ingu is t i cs  Depar tmentIns t i tu te  of Mathemat ics  ~md Computer  Sc ience,  bl.8Bu lg~r ian  Acade~ny of Sc iences,  1 113 Sofia Bu lgar iaper iQbgearn ,  acad .
bgAbst rac tA system is descril)ed which learns fl'omexamples the Linear Precedence rules inan Immediate Dominance/Linear P ece-dence grammar.
Given a particular hn-mediate Dominance grammar and hier-archies of feature values potentially relevant for linearization (=the systelu'sbias), the leanler generates appropriatenaturM language xpressions to be ewd-uated as positive or negative by a teach-er, and produces as output IAnear Prece-dence rules which can be directly used 1)ythe gralnmar.1 In t roduct ionThe lnanual cotnpilation of a. sizable grammar isa difficult and time-consuming task.
An impor-tant subtask is the construction of word orderingrules in the grammar.
'\['hough some languagesare proclaimed as having simple ordering rules,e.g.
either complete scrambling or strictly "fixed"order, most languages exhibit quite complex reg-ularities (Steele, 198l), and even the rigid wordorder languages (like 1,;nglish) and those with total scrambling (like Warlpiri; cf.
(H ale, 1983) mayshow intricate rules (Kashket, 11981); hence theneed for their automatic acquisition.
'Fhis I;askhowever, to the best of our knowledge, has notheen previously addressed.This paper describes a prograln which, givena grammar with no ordering relations, l)roducesas outpu_t a set of linearization, or Linear Prece-dence, rules which can be directly employed bythat grammar.
The learning step uses the ver-sion space algorithm, a familiar techlfique fromma.chine learning for learning from examples.
Incontrast o most previous uses of the algorithnl forvarious learning tasks, which rely on priorly giv-en classified examples, our learner generates itselftile training instances olte at a tinie, and they arethen classed as positive or negative by a teach-er.
A selective generation of training instances isemploye, d which facilitates the learning by mini-mizing the nu,nl)er of evaluations that the teacherneeds to make.The next section (h.'scribes the hmnediate Dom-inance/Linear I)recedence grammar format.
Insection 3, tlle lask of learning l,inear Precedencerules is interpreted as a task of learning from ex-amples, and section 4 introduces the version space\]nethod.
Section 5 is a system overview, and sec-tion 6 focuses on implementation.
Finally, somelimitations of the system a.re discussed as well assome directions for future research.2 hnmediate  Dominance/L inearP recedence  GrammarsA standard way of expressing tile ordering ofnodes in a grammar is I)y means of l,inearPrecedence rules in hnme(liate l)ominance/IAnearI','ocede,,ce.
(ID/LI') ,~ra.,,,,ai's.
The m/U '  for:-mat was first introduced by (Gazdar and Pullmn,1981) and (Gazdar el..
al., 1985) and is usuallyassociated with GPS(\], but is also used by I IPSG(l'ollard and Sag, 1987) and, under different guis-es, by other formMisins a.s well.In all l l ) / I ,P grammar, the two types of in-formation, constituency (or, immediate domi-nance) and linear order, are separated.
Thus,for instance, an immediate dominance rule, say,A--~I3 C D, with no l inear Precedence rules de-clared, stands for the mother node A expandedinto its siblings occurring in any order (six Con-text Free Grammar ules as result of the permuta-tions).
If l;he I,\[' rule D < C is added, the ID rulecan be expanded in the following three CFG rules:A---~ /3 D C; d--, D I/ C; d -+ D CB.
ID/LPgrammars capture an important ordering general-ization, missed by usual CFGs, by means of the socalle(/ "l'3xhaustive Partial Ordering Constraint",stating that the l)artial ordering of any two sisternodes is constant hroughout the whole grammar.That is, just one of the Mlowing three situationsis valid for the ordering of any two nodes A and 11:either A < B (A precedes B) or d > B (A followsB) or A <> 11 (A occurs in either position withrespect to B).
(The last.
<> situation is normal-ly state(l in l l ) /L I '  grammars by ~lot stating an883LP rule, but we shall use it here, as we need anexplicit ~hference to it.
)3 The Task of LP RulesAcquisit ion Viewed As Learningfrom ExamplesA program which learns from examples usual-ly reasons from very specific, low-level, instances(positive or both positive and negative) to moregeneral, high-level, rules that adequately describethose instances.
Upon a common understanding(Lea and Simon, 1974), learning from examples isa cooperative search in two spaces, the instancespace, i.e.
the space of all possible training in-stances, and the rule (=h.ypotheses) space, i.e.
thespace of all possible general rules.
Besides thesetwo spaces, two additional processes are needed,intermediating them: interpretatioT~ and instanceselection.
The interpretation process is needed, inmoving from the instance space to the rule space,to interpret the raw instances, which may be farremoved in form froln the form of the rules, so thatinstances can guide the search in the rule space.Analogously, the instance selection rules serve totransform the high-level hypotheses (rules) to arepresentation useflfl for guiding the search in theinstance space.A general description of our task is as follows:Given a specific ID grammar with no LP rules,find those LP rules.
1 In this task we also needto reason from very specific instances of LP rules(language phrases like small childreu, *children s-malt) to rnore general LP rules (adjective < noun),therefore it can be interpreted in terms of tile two-space model, described above.Our instance space will consist of all strings gen-erable by the given ID grammar (the size of thisinstance space for any non-toy grammar will bevery large).
The LP rules space will be an un-ordered set, whose elements are pairs ot' nodes,connected by one of the relations <, > or <>,e.
a LP set = \[\[A < B\], \[B < l';\], \[E > C\], ...
\].
(The size of the LP rules space will deperid uponthe size of the specific ID grammar whose LP rnlesare to be learned.
)We also need to define the interpretation andinstance-selection processes.
In the learning sys-tem to be described, for both purposes serves(basically) a mete-interpreter for l l ) /LP gram-mars, which can parse tile concrete grammar, giv-en at the outset, for both analysis and generation.In an interpretation phase, the mete-interpreterwill parse a natural language expression out-putting an LP-rules-space approriate representa-tion, whereas in the instance-selection phase the1Though indeed this is the usual way of looking atthe task, sometimes we may need to start with someLP rules already known; the program we shMl describesupports both regimes.meta-interpreter, given an LP space representa-tion as input, will generate a language xpressionto be classified as positive (i.e.
not violating wordorder rules) or negative (i.e.
violating those rules)by a teacher.4 The Version Space MethodThere are a variety of methods in the AI litera-ture for learning from exarnples.
For handling ourtask, we have chosen tile so called "version space"method (also known as the "candidate liminationalgorithm"), cf.
(Mitchell, 1982).
So we need tohave a look at this method.Tile basic idea is, that ill all representation lan-guages for the rule space, there is a partial order-ing of expressions according to their generality.This fact allows a compact representation of theset of plausible rules (=hypotheses) in the rule s-pace, since the set of points in a partially orderedset can be represented by its most general and itsmost specific elements.
Tile set of most generalrules is called the G-set, and tile set of most spe-cific rules tile S-set.Figure 1 illustrates the LP rules space of a de-terminer of some grammatical nulnber (singularor l)lura.l) and an adjective, expressed in predicatelogic.Viewed top-down, the hierarchy is in descendingorder of generality (arrows point from specific togeneral).
The topmost LP rule is most general andcovers all the other rules, since det(Num), whereNnm is a variable, covers both det(sg) and det(pl),and <> covers both < and >.
Each of the rules atlevel 2 are neither more general nor more specificthan each other, but are more general than themost specific rules at the bottorn.The learT~ing method assumes a set of positiveand negative examples, and its aim is to inducea rule which covers all the positive examples andnone of the counterexamples.
Tile basic algorithmis as follows:(1) The G-set is instantiated to the most gener-al rule, and the S-set to the first positive example(i.e.
a positive is needed to start the learning pro-cess).
(2) The next training instance is accepted.
If itis positive, frorn the G-set are removed the ruleswhich do not cover the example, and the elementsof S-set are generalized as little as possible, so thatthey cover the new instance.
If the next instance isnegative, then fl'om the S-set are removed the rulesthat cover the counterexample, and the elementsof the G-set are specialized as little as possible sothat the counterexample is no longer covered byany of the elements of the G-set.
(3) The learning process terminates when theG-set and the S-set are both singleton sets whichare identical.
If they are diflhrent singleton sets,the training instances were inconsistent.
Other-wise a new training instance is accepted.884deL(Nun,) <> a~tidet(sg) <> adj det(Num) < adj det(Nuni) > adj de.t(pl) <> adjdet(sg) < adj det(pl) < adj do.t(sg) > adj det(pl) > adjFigure 1: A Generalization hierarchyNow, let us see how this works with the l,P rulesversion space in Figure 1, asslllning further thefollowing classitied exaanplcs ((+) nteans l)ositiw~,and ( - )  negative instance):(+) det(sg) < a,ti(--) det(sg) > adj(+) dct(pl) < adjThe algorithni will instanLiate the {i-set to then:iost general rule ii i tile version space, and tim,5'-set o the first positive, obl, aining:G-set: \[\[dot(Nu,n) <> ad.i}\]s-set: \[\[det(sg) < add\]\]'/'hen the next exmnple will lie accepted, whichis negative.
The current ~g-sel does not cover it,so it relnains the sanle; the G-set is specialized aslittle as possible to exchlde tile negative, whichyields:G-sot: \[\[det(Num) < adj\]\]S-set: < adj\]\]The last example is positive.
\]'lie (;-sol reliiaill-s l, he same since it covers the positive.
The ,5'-sethowever does not, so it has to be uiilliinally gen-eralized to cover it, obtaining:G-set: \[\[de.t(Nuui) < a~ti\]\]S-sot: \ [ \ [det(Num)< acid\]\]These are singleton sets which are identical, andthe resultant (consistent) generalization is there--fore: \[det(Num) < adj\].
That is, a determiner ofany grarrniiatical lunnber niust precede a,n adjec--rive.5 Overv iew of  the  LearnerOur learning program has two basic modules: IAmversion space learner which performs the olenmn-.tary learning Step (as descril)ed in 1.he previoussection), and a nmta-hiterpretcr for l l ) / I ,P  grain-iiiars which serves the processes of interpretai.ionalld instance selection (as described in section 3).The learning proceeds in a dialog forni with tileteacher: for the learning of each individual l,Prule, the system produces natural anguage phras-es to be classitied by the teacher mttil it can con-verge to a single concept (rule).
The whole processends when all LP rules are learned.At tile outset, the prograrn is supplied with thespecific H) grallHl-lar whose l,P rules are to be ac-quired, and the user-provided bias of the.
system.The latl;er implies an explicit statement Oil thepart of tlw user of what featm'es and values arerelevant o the task, by ilqmtting the correspond-ing generalization hierarchies (the precedence gen-eralization hierarchy is taken for granted).In the particular implementation, the accept-able 11) grammar format is essentially that of alogic gt'ammar (Pereira and Warren, 1980), (l)ahland Abramson, 1990).
We only use a double arrow(to avoid mixing up with the often built-in Deft-nite Clause ()irammar notation), and besides emp-ty productions and sisters ha.ving the very samenallm are not allowed, since they interfere with1,1 > rules statmnenl.s, of.
e.g.
(Sag, 1987), (Saint-l)izier, 1988).6 T i le  imp lementat ionBelow we discuss the basic aspects of tile imple-mentation, illustrating it; with the ll) grannnarwil, h no LP restrictions, given on Figure 2.The grammar will generate simple declarativeand interrogative sentences like The Jonses readthis thick book, The 3onses read these thick books,Do the Jonses smile, etc.
as well as all their (un-granuimtical) permutations Read this thick bookthe ,lo,lscs, 7'he Jonses read thick this book do,ct, c.The progranl knows at the outset that the val-ues "sg" and "pl" are hoth more specitic than l, hevarial)lc "N ran", mal;ching any mmtber (this is tiltbias of the system).Step I.
The prograni determines the siblings885(1) s ==> name, vp.
(2) sq ~ aux, name, vp.
(3) vp ==> vtr, np.
(4) vp --> vintr.
(5) np =:=> det(Num),adj,n(Nm,,).
(6) , arno \[the-jonses\].
(7) n(sg) ~ \[book\].
(8) n(pl) =:~ \[books\].
(9) det(sg) ~ \[this\].
(10) det(p,) \[these\].
(11) act(_) \[the\].
(12) adj ==> \[thick\].
(la) vtr \[read\].
(14) vintr ~ \[smile\].
(15) aux \[do\].Figure 2: A simple i1) grammar with no LP constraints(=the right-hand sides of ID rules) that will laterhave to be linearized, by collecting them in a par-tially ordered list.
Singleton right-hand sides (rule(4) above and all dictionary rules) are thereforeleft out, and so are cuts, and "escapes to Pro-log" in curly brackets, since they are not used torepresent ree nodes, but are rather coustraintson such nodes.
Also, if some right-hand side is aset which (properly) includes another ight-handside (as in rule (2) and rule (1) abowe), the latteris not added to the sibling list, since we do notwant to learn twice the linearization of some twonodes ("name" and "vp" in our case).
The sib-ling list then, after the hierarchical sorting frolltlower-level to higher-level nodes, becomes:\[\[aux,name,vp\] \[vtr,np\],\[det (Nurn),adj ,n(Num)\]\]Now, despite the fact that the set of LP rules weneed to learn is itself unordered, the order in whichthe program learns each individual LP rule rnaybe very essential to the acquisition process.
Thus,starting Dom the first, element of the above sib-ling list, viz.
\[aux, name, vp\], we will be in troublewhen attempting to locate the misorderings in anynegative xample.
Considering just a single nega-tive instance, say The Jonses read thick this bookdo: What is(are) the misplacenmnt(s) and wheredo they occur?
In the higher-level tree nodes \[aux,name, vp\] or in the lower-level nodes \[vtr, np\] orin the still lower \[det(Num),adj,n(Num)\] ?Our program solves this problem by exploitingthe fact, peculiar to our application, that the n-odes in a grammar are hierarchically structured,therefore we may try to linearize a set of nodesA and B higher up in a tree o~lly after all lower-level nodcs dominated by both A and ft have al-ready been ordered.
Knowing these lower-lewq LPrules, our rneta-interpreter would never generateinstances like The Jonses read thick this book do,but only some repositionings of the nodes \[aux,name, vp\], their internal ordering being gua.ran-teed to be correct.
The sibling list then, after hi-erarchical sorting from lower-level to higher-levelnodes, becomes:\[\[det(Num),adj,n(Sum)\],\[vtr,np\],\[aux,,mme,vp\]\]and the lirsl; element of this list, is first passed tothe learning engine.Slep ~.
The.
program now needs to produce afirst positive example, as required by the versionsl)ace method.
Taking as input the first elelneu-t of tim sibling list, the.
I I ) /LP meta-interpret.ergenerates a phrase conforming to this descriptionand asks the teacher to re-order it correctly (ifneeded).
In our case, tile first positiw', examplewould be this thick book.
The phrase will be re-parsed in order to determine the linearization ofconstituents.A word about the I1)/I,P parser/generator.
Itsanalysis role is needed in processing the firs|; pos-itive example, and the generation role in the pro-duction of language xamples for all intermediatestages of the learning process which are then e-valuated by the teacher.
The predicate observestwo types of LP constraints: tile globally valid LPrules tha.t have been acquired by the system so far," and the "transitory" LP constraints, serving toproduce an ordering, as required by an intermedi-ate stage of the learning process.l)isposing of the ordering of constituents in thepositive example, the tra~silive closure of thesepartial orderings is computed (in our case, from\[\[det(Num) < adj\],\[adj < n(Num)\]\] we get \[\[de-t(Num) < adj\], \[ad.i < n(Num\]), \[det(Num) <n(Num)\]\]).
This result is the.
east into a rop-resentation that SUl)ports our learning process.
:320r are priorly known, in the case when the systemstarts with some LP rules declared by the user.3The concept we learn is actually a conjunction ofindividual I,P rules, when tile right-hand side of a ruleconsists of three or more constituents.886Dialog with, userthis thick book(+)(?
)(-)this book thick(-)thick this book(-)these thick books(+/LP rules space ~,a l  representation) i,,;x.: \ [dot ,~  ;a~(~j ,~, , l , _V - -(;: \[\[det,N,< >,a~0,#,ad.i,< >,,~,_,#,det,_,< >,n,j\]S: \[\[det,sg, <,adj,#,adj,<,n,_,#,det,_,<,n,_\]\]Ex.
: \] '(tet,sg,<,adj,#,adj,<,n,_,#,(tet,_,>,n,_\]---G: \[\[det,N,< >,ad.i,#,adj,< >,,l,_,#,det,_, < n,_\]\]s: \[\[det,sg,<,adj,#,a4i,<,n,_,#,det,_, <,,*,allF-EST.
: \[det,sg,<,adj,#,adj,>,n,_,},det,_,<,n,_\]G: \[\[(let, N,<> ,a dj,#,ad i,< ,n,_, # ,d0t,_, < ,.,_\]\]S: \[\[det,sg,<,adj,#,adj,<,n,_,#,det,_,<,n,_\]\]t;;x. : \[det,sg, >,adj,#,adj, <,n,_,#,det,_,< ,n,_\]G: \[\[det, N <,ad, i #,adj, <,n,_, #,det,_,<,,~,_\]\]S: \[\[det,sg, <,adi,#,adj, <,,~,_,#,det,_, <,n,_\]\]~det, l , l ,<,adj ,#,ad.
i ,<,n,_,#,do<,<,, , ,_ I - - - -G: \[\[tier,N, <,~dj,#,ad.i,<,,,,_,#,det,_,<,,l,_\]\]S: \[\[det,sg, < ,adj, # ,adj, <,n,_, # ,det,_, <,n,_\]\]-A c~, , -777adj < n(sg)det(sg) < n(sg)det(sg) < adjadj < n(sg)det(sg) > n(sg)~et(sg)  < adjadj > n(sg),let(sg) < ,,(sg)--~let(sg) > ~djadj < n(sg)det(sg) < .
(sg)adj < n(pl)det(pl) < n(pl)Consistent generalization: -~det (N)  < adj\[det,N,<,adj,#,adj,<,,,,_,#,det,_,<,n,_\] ad.i < ,I(N)det(N) < n(N)Figure 3: A sample sessionStep 3. q'he version space method is applie(Iand the individual 1,1 ) rules, resulting fron, lind-ing a consistent generalization, are asserted in theI I ) /bP granmm.r datal)ase to lie resl)eete,l by any4 further generation process.Figure 3 gives a learning cycle starting \['rein thesibling list ele,nent \[det(Nun,),adj,n(Nu,,,)\].
Thefirst column gives the dialog with the teacher, thesecond the program's internal representation ofthe l,P rules space., and the third those |'ules a.reexpressed in their nnore familiar, and final, formthat can be utilized directly by the 11) gra.nmmr.Afl,er processing the lirst 1)ositive (tirst row),tile system generalizes by varying a paraluet(:r(imnd)er or l)recedenec), verbalizes 1,he general-ization, the generated phrase is class|tied by theteacher, then another generalization is made, de-pending on the classiiication, it is verbalized, eval-uated and so on.
The 1)rocess results in the threel,P rules: det(Num) < adj; adj < ,,(Num); anddet(N.,n) < ,~(N.,,,).A remark on notation: # delimits individual ,Prules, allowing their recovery in terms of Prologstructures.
The underbars, _, are nmrely place-holders for bound variables (in our case.
thosebound to "N").
Clearly, mutually depemhmt flea-ture values need to be eonsidcre(l (i.e.
wu'ie(I I)ythe program) only once, and so they occtu" justonce in the expressions.Severa.1 additional points regarding the learningprocess need to be lnadc.4Assertions are actually made after (:he(:king forconsistency with LP's already present in the database.Though no contradictions may ~u'isc with acquiredrules, they may come from LP's declared by the userit, the case when the system is started with some suchLP's.The firsl i~ that after couw.q'ging to a. single l,prule, it is tesl.e(l wlmther 1.his rule covers allluostspecific instnnces.
For doing this, the stated gen-?~ralization hierarchies are t.aken into account a-longside with the fact that in an I I ) /LP format arule of tile type d > 13 logically implies the nega-lion of its "inverse rule" A < IL Thus, the ruledet(Num) < adj covers all potential most specif~ie instances ince the rule itself and its inverserule det(Nmn) > adj cover them, which is clearlyseen on the generalization hierarchy in Figure 1.\[\[" SO\]fie l l lOSt s l)eci \[ i ( :  i l lsta,  l | ( 'es re t t la i l l  / l l lcovere(.
l~theu they are fed again to the version space algo~ril.hlzl for a second pass.The second point is that when it is impossible\['or SOll |e s t ruc tur ( "  to  be  verbalized ue to cont, ra-dictory LP statelnellts (as ill the second row), thesystem itself evaluates this exa.ml)le as negativeand l)roceeds fln'ther.We also nee(I to emphasize that the programselectively, rather than randomly, wries the po-tentially relevant parameters (munber and prece-dence, in this particular case), ~ttempting toconverge the.
generalization process most quick-ly.
This is done in order to minilnize the nunnberof training iustanees that need to be generated,and hence (,o nfinimize the number of evMuationsthat tim teach(~r lice(Is to Ina.ke.
In other words,being generalization-driven, the generator neverproduces training instances which arc superfluousto the generalization pro('ess.
'\]'his, in particu-lar, allows the program to avoid outputting allstrings generable by the grammar whose LP rulesare being acquired {notice, for instance, in thelh'st colunm of Figure 3 that no language xpres-sion involving the dictionary rule (11) det(_)\ [the\]  from Figure 2 is displayed to the user).887In this respect our approach is in sharp contrastto a learning process whose training examples aregiven en bloc, and hence the teacher would, ofnecessity, make a great lot of assessions that thelearner would never use.Step ~.
The learning terminates uccessfullywhen all LP rules are found (i.e.
all elements ofthe sibling list are processed) and fails when noconsistent generalization may be found for somedata.
The latter fact needs to be interpreted in thesense that these data are not correctly describablewithin the ID/LP format.7 Conc lus ion  and Future  WorkWe have described a program that learns the LPrules of an ID/LP logic grammar in a form thatcan be directly utilized by that grammar.
Thistask has not been addressed in previous work.We conclude by mentioning some limitations ofthe system suggesting future directions for inves-tigation.It is known that the version space method mis-behaves on encountering noisy data: an instancemistakenly classed as negative e.g.
may lead topremature pruning of a search branch where thesolution may actually lie.
This may be a prob-lem in our task (and perhaps in many other lin-guistic tasks) since our assessments of grammati-cM/ungrammaticM word order are in some casesfar from definite yes/no's.
So haudling uncertaininput is one way our research may evolve.Another direction for filture research is address-ing the learning of word order expressed in morecomplex formalisms than I1)/LP grammars.
It hasbeen proposed in the (computational) linguisticsliterature (e.g.
(Zwicky, 1986), Ojeda, 11988, Per-icliev and Grigorov, 1994) that LP rules of thestandard format may be insufficient in some ca.s-es, and need to be augmented with other orderingrelations like "immediate precedence" <<, "fist","last", etc., and more generally, that linearizationneeds to be stated in complex logic expressionsconnected by conjunction, disjunction and nega-tion.
We can trivially add the relation << to thepresent learner, but the other parts of such pro-posals seem beyond its immediate capacity, as itstands.
From our previous work on word orderwe despose of a parser/generator that can hamdle complex expressions, however we shall needto modify (or perhaps, even replace) our learningmethod with one which is better suited to handlelogic constructions like disjunction and negation.Acknowledgements.
The research reported inthis paper was partly supported by contract 1-526/95.Referencesv.
Dahl and H. Abramson.
1990.mars, Springer.G.
Gazdar and G. Pullum.
1981.Logic Gram-Subcatego-rization, constituent order and the notion of"head".
In M. Moortgat et.
al.
(eds).
The S-cope of Lexical Rules, Dordrecht, llolland, pages107-123.G.
Gazdar, E. Klein, G. Pullum and I.
Sag.
1985.Generalized Phrase Structure Grammar, liar-yard, Cambr.
Mass.K.
Hale.
1983.
Warlpiri and the grammar of non-configurational languages.
Natural Languageand Linguistic Th.eory, v. 1, pages 5-49.M.
Kashket.
1987.
A GB-based parser forWarlpiri, a free-word order language.
MIT AILaboratory.G.
Lea and I1.
Simon.
1974.
Problem solving andrule induction: A unified view.
In L. Gregg(ed).
l(nowledge and Co;inition, Lawrence Erl-baum Associates.T.
Mitchell.
1982.
Generalization as search.
Ar-tificial Intellige,ee, 18, pages 203-226.A.
Qieda.
1988.
A linear precedence account ofcross-serial dependencies.
Linguistics aand Phi-losophy, l l ,  pages 457-492.F.C.N.
Pereira and D.H.D Warren.
1980.
DefiniteClause Grammars for Natural Language Anal-ysis.
Artificial Intelligence, 13, pages 231-278.V.
Pericliev and A. Grigorov.
1994.
Parsing aflexible word order language.
COLING'g/t, Ky-oto, pages 391-395.C.
Pollard and I.
Sag.
1987. b~formation-BasedSyntax and Semanties,vol.l: Fnndamentals.
C-SLI Lecture Notes No.
13, Stanford, CA.P.
Saiut-1)izier.
1988.
Contextual discontinuousgrammars.
In Natural Language Understand-ing and Logic Programming, lI, North-Holland,pages 29-43.I.
Sag.
1987.
Grammatical hierarchy and lin-ear precedence.
In S?lulax and Semantics, v.12.Discontinuous Constituency, Academic Press,pages 303-340.Richard Steele.
1981.
Word order variation.
InJ.
Greenberg (ed).
In Universals of Language,v./t, Stanford.A.
Zwicky.
1986. hnnaediate precedence in GPSG.OSU WPL, pages 133-138.888
