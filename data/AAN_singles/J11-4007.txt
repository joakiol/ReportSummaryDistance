Information Status Distinctions andReferring Expressions: An EmpiricalStudy of References to People inNews SummariesAdvaith Siddharthan?University of AberdeenAni Nenkova?
?University of PennsylvaniaKathleen McKeown?Columbia UniversityAlthough there has been much theoretical work on using various information status distinctionsto explain the form of references in written text, there have been few studies that attempt toautomatically learn these distinctions for generating references in the context of computer-regenerated text.
In this article, we present a model for generating references to people in newssummaries that incorporates insights from both theory and a corpus analysis of human writtensummaries.
In particular, our model captures how two properties of a person referred to in thesummary?familiarity to the reader and global salience in the news story?affect the contentand form of the initial reference to that person in a summary.
We demonstrate that these twodistinctions can be learned from a typical input for multi-document summarization and thatthey can be used to make regeneration decisions that improve the quality of extractive summaries.1.
IntroductionNews reports, and consequently news summaries, contain frequent references to thepeople who participate in the reported events.
Generating referring expressions topeople in news summaries is a complex task, however, especially in a multi-documentsummarization setting where different documents can refer to the same person indifferent ways.
One issue is that the generator has to work with textual input as opposedto closed-domain semantic representations.
More importantly, generating references topeople involves issues not generally considered in the referring expression literature.?
Department of Computing Science, University of Aberdeen, Meston Building, Meston Walk, AberdeenAB24 3UE, UK.
E-mail: advaith@abdn.ac.uk.??
University of Pennsylvania, CIS, 3330 Walnut St., Philadelphia, PA 19104, US.E-mail: nenkova@seas.upenn.edu.?
Department of Computer Science, Columbia University, 1214 Amsterdam Ave., New York, NY 10027, US.E-mail: kathy@cs.columbia.edu.Submission received: 30 May 2008; revised submission received: 18 July 2010; accepted for publication:25 March 2011.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 4People have names that usually distinguish them from others in context, and assuch, the framework of generating descriptions that rule out distractors (e.g., the bodyof research building on Dale [1992], including the recent shared tasks based on theTUNA corpus [Gatt, Belz, and Kow 2008]) is not appropriate.
Recent GREC challenges(Belz, Kow, and Viethen 2009) have, however, focused on references to named entities,and we compare that body of work to ours in Section 2.2.In our domain of summarizing news articles, the aim of generating references topeople is to introduce them to the listener and relate them to the story.
How muchdescription is needed depends on many factors, including the knowledge the speakerexpects the listener to have, the context of the discourse, and the importance of theperson to the narrative.
In this article, we explore these three information status distinc-tions in the context of generating references to people in multi-document summaries ofnewswire:1. discourse-new vs. discourse-old: whether a reference to a person is a firstor subsequent mention of that person is purely a property of the text.2.
hearer-new vs. hearer-old: whether the person being mentioned isfamiliar to the hearer is a property associated with the hearer.3.
major vs. minor character: how important or salient a person is to thenarrative depends on communicative goals and is therefore a propertyassociated with the speaker.Through our studies, we seek answers to three main research questions:1.
Is it possible to automatically infer information not explicitly stated aboutpeople in the input for summarization, such as familiarity and salience?2.
Is such information useful for the task of generating references to peoplein multi-document summaries?3.
Can summary quality be improved through an informed rewrite ofreferences to people?We report positive answers to all three questions.
Our corpus-based approachmodels the differences between first and subsequent references, provides detail on howto generate the variety of first references that occur, and shows how distinctions such asfamiliarity and salience drive generation decisions for initial references.In this article, we describe and evaluate a new algorithm for referring to peoplein multi-document news summaries that integrates results from two earlier studies(Nenkova and McKeown 2003; Nenkova, Siddharthan, and McKeown 2005).
In thefollowing sections, we first discuss related work (Section 2) and then present a simplemodel for distinguishing discourse-new and discourse-old references (Section 3, basedon Nenkova and McKeown [2003]).
A more sophisticated model based on automaticallyacquired information about familiarity and salience is presented in Section 4 and thesedistinctions are used for making generation decisions in Section 5 (based on Nenkova,Siddharthan, and McKeown [2005]).
We then present an integrated algorithm that gen-erates new references to people in news summaries based on the acquired informationstatus distinctions and report an evaluation of the effect of reference rewriting onsummary quality in Section 6, including a discussion of its scope and limitations inSection 6.2.812Siddharthan, Nenkova, and McKeown Information Status and References to People2.
Related WorkRelated research into summarization, information status distinctions, and generatingreferring expressions is reviewed here.2.1 Extractive and Abstractive SummarizationMulti-document summarization has been an active area of research over the past twodecades and yet, barring a few exceptions (Radev and McKeown 1998; Daume?
III et al2002; Barzilay and McKeown 2005), most systems still use shallow features to producean extractive summary, an age-old technique (Luhn 1958) that has well-known prob-lems.
Extractive summaries may contain phrases that the reader cannot understand outof context (Paice 1990) and irrelevant phrases that happen to occur in a relevant sentence(Knight and Marcu 2000; Barzilay 2003).
Referring expressions in extractive summariesillustrate this, as sentences compiled from different documents might contain too little,too much, or repeated information about the referent.In a study of how summary revisions could be used to improve cohesion in multi-document summarization (Otterbacher, Radev, and Luo 2002), automatic summarieswere manually revised and the revisions classified as pertaining to discourse (34% ofall revisions), entities (26%), temporal ordering (22%), and grammar (12%).
This studyfurther supports the findings from early research that unclear references in summariespose serious problems for users (Paice 1990).2.1.1 Sentence Compression and Fusion.
Research in abstractive summarization has largelyfocused on the problem of compression, developing techniques to edit sentences byremoving information that is not salient from extracted sentences.
Some approachesuse linguistic rules (e.g., Zajic et al 2007) often combined with corpus-based informa-tion (Jing and McKeown 2000), whereas other approaches use statistical compressionapplied to news (Knight and Marcu 2000; Daume?
III and Marcu 2002) and to spokendialogue (Galley and McKeown 2007).
Other researchers addressed the problem ofgenerating new sentences to include in a summary.
Information fusion, which usesbottom?up multi-sequence alignment of the parse trees of similar sentences, generatesnew summary sentences from phrases extracted from different document sentences(Barzilay and McKeown 2005; Filippova and Strube 2008).2.1.2 Summary Revision.
Research in single-document summarization on improvingsummaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work.Three types of ad hoc revision rules are defined?elimination (removing parentheticals,sentence initial prepositional phrases, and adverbial phrases), aggregation (combiningconstituents from two sentences), and smoothing.
The smoothing operators cover somereference editing operations.
They include substitution of a proper name with a namealias if the name is mentioned earlier, expansion of a pronoun with co-referential propername in a parenthetical, and replacement of a definite NP with a co-referential indefiniteif the definite occurs without a prior indefinite.
Mani et al?s (1999) work differs fromours in that it focuses primarily on subsequent mention (with the exception of pronounreplacement), is meant to work for all entities, not just mentions to people, and does notincorporate distinctions inferred from the input to the summarizer.Although the rules and the overall approach are based on reasonable intuitions, inpractice entity rewrites for summarization do introduce errors, some due to the rewriterules themselves, others due to problems with co-reference resolution and parsing.813Computational Linguistics Volume 37, Number 4Readers are very sensitive to these errors and prefer extractive summaries to summarieswhere all references have been edited (Nenkova 2008).
Automatic anaphora resolutionfor all entities mentioned in the input and summary text is also errorful, with aboutone third of all substitutions in the summary being incorrect (Steinberger et al 2007).In contrast, when editing references is restricted to references to people alone, as we doin the work presented here, there are fewer edits per summary but the overall result isperceived as better than the original by readers (Nenkova and McKeown 2003).2.1.3 Reference in Summaries.
There has been little investigation of the phenomenonof reference in news summaries.
In addition to the revision of subsequent referencesdescribed in Mani, Gates, and Bloedorn (1999), we are aware of Radev and McKeown(1997), who built a prototype system called PROFILE that extracted references to peoplefrom news, merging and recording information about people mentioned in variousnews articles.
The idea behind the system was that the rich profiles collected for peoplecould be used in summaries of later news in order to generate informative descriptions.However, the collection of information about entities from different contexts and differ-ent points in time leads to complications in description generation; for example, pastnews can refer to Bill Clinton as Clinton, an Arkansas native, the democratic presidentialcandidate Bill Clinton, U.S. President Clinton, or former president Clinton and it is not clearwhich of these descriptions are appropriate to use in a summary of a novel news item.In later work, Radev and McKeown (1998) developed an approach to learn correlationsbetween linguistic indicators and semantic constraints to address such problems, butthis line of research has not been pursued further.Next, we review related work on reference outside the field of summarization.2.2 Information Status and Generating Referring ExpressionsResearch on information status distinctions closely relates to work on generating refer-ring expressions.
We now overview the two fields and how they interact.2.2.1 Information Status Distinctions.
Information status distinctions depend on twoparameters related to the referent?s place in the discourse model maintained by thereader: (a) whether it already exists in the hearer?s model of the discourse and (b)its degree of salience.
The influence of these distinctions on the form of referringexpressions has been a focus of past research.
For example, centering theory (Grosz,Joshi, and Weinstein 1995) deals predominantly with local salience (local attentionalstatus), and the givenness hierarchy of Prince (1992) focuses on how a referent enteredthe discourse model (e.g., through a direct mention in the current discourse, throughprevious knowledge, or through inference), leading to distinctions such as discourse-old, discourse-new, hearer-old, hearer-new, inferable, and containing inferable.
Gundel,Hedberg, and Zacharski (1993) attempt to merge salience and givenness in a singlehierarchy consisting of six distinctions in cognitive status (in focus, activated, familiar,uniquely identifiable, referential, type-identifiable).
In all three theories, familiarity andsalience distinctions are shown to be associated with different preferences for syntacticform in the realization of referring expressions.2.2.2 Generating Referring Expressions (GRE).
The most developed sub-area of re-ferring expression generation deals with the problem of generating distinguishingdescriptions?descriptions that include enough attributes of the intended referent sothat it becomes uniquely identifiable among other entities (Dale 1992).
The original814Siddharthan, Nenkova, and McKeown Information Status and References to Peopleincremental algorithm (Dale and Reiter 1995) assumes that a list of attributes of the dis-course entities is readily available and that attributes that rule out the most distractorsare added to the referring expression until its interpretation contains only the intendedreferent.
Subsequent work on referring expression generation has (a) expanded thelogical framework to allow reference by negation (the dog that is not black) and referencesto multiple entities (the brown or black dogs) (van Deemter 2002; Gatt and Van Deemter2007), (b) explored different search algorithms for finding a minimal description (e.g.,Horacek 2003), and (c) offered different representation frameworks such as graph the-ory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) asalternatives for representing referring characteristics.
This body of research assumesa limited domain where the semantics of attributes and their allowed values can beformalized, though semantic representations and inference mechanisms are getting in-creasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz2008; Ren, van Deemter, and Pan 2010).
In contrast, Siddharthan and Copestake (2004)consider open-domain generation of referring expressions in a regeneration task (textsimplification); they take a different approach, approximating the hand-coded domain-knowledge of earlier systems with a measure of relatedness for attribute-values that isderived from WordNet synonym and antonym links.2.2.3 Recent Trends: Data Collection and Evaluations.
There is now increasing awarenessthat factors other than conciseness are important when planning referring expressionsand that considerable variation exists between humans generating referring expressionsin similar contexts.
Recent evaluation exercises such as the TUNA challenge (Gatt, Belz,and Kow 2008) therefore consider metrics other than length of a reference, such ashumanness and the time taken by hearers to identify the referent.
In a similar vein,Viethen and Dale (2006) examine how similar references produced by well-knownalgorithms are to human-produced references, and Dale and Viethen (2009) examinedifferences in human behavior when generating referring expressions.
There is alsogrowing collaboration between psycholinguists and computational linguists on thetopic of generating referring expressions; for instance, the PRE-CogSci workshop (vanDeemter et al 2010).Recently, several corpora marked for various information status aspects havebeen made available.
Subsequent studies concerned with predicting givenness status(Nissim 2006; Sridhar et al 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff2005) have not been used for generation or summarization tasks.
Current efforts inthe language generation community aim at providing a corpus and evaluation task(the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, andViethen 2009).
The GREC-2.0 corpus, extracted from Wikipedia articles and annotatedfor the task of referring expression generation for both first and subsequent mentionsof the main subject of the article, consists of 2,000 texts in five different domains(cities, countries, rivers, people, and mountains).
The more recent GREC-Peoplecorpus consists of 1,000 texts in just one domain (people) but references to all peoplementioned in a text have been annotated.
The GREC challenges require systems to pickthe most appropriate reference in context from a list of all references in the documentand several defaults, including pronouns, common-noun references, elided reference,and standardized versions of names.
By selecting encyclopedic articles about specificreferents, this corpus contains large numbers of subsequent references, and in general,the emphasis has been to model the form of subsequent references to named entities inlonger texts.
Discourse-new vs. discourse-old is the only information status distinction815Computational Linguistics Volume 37, Number 4that participating systems model, with other features derived from lexical and syntacticcontext; for instance, Greenbacker and McCoy (2009) consider subjecthood, parallelism,recency, and ambiguity.2.2.4 Applications of Information Status Distinctions to GRE.
The main application oftheories of information status has been in anaphora resolution.
Information statusdistinctions are not normally used in work on generating referring expressions, witha few notable exceptions.Krahmer and Theune (2002) show that the relative salience of discourse entitiescan be taken into account to produce less-informative descriptions (including fewerattributes than those necessary to uniquely identify the referent using a discourse modelthat does not incorporate salience).
In contrast, Jordan and Walker (2005) show that, intask-oriented dialogs, over-specified references (including more attributes than neededto uniquely identify the intended referent) are more likely for certain dialog statesand communicative goals.
Some participating teams in the GREC challenges use thediscourse-new vs. discourse-old distinction as a feature to help select the most likelyreference in context.
Different interpretations of Centering Theory have also been usedto generate pronominal references (McCoy and Strube 1999; Henschel, Cheng, andPoesio 2000).Our research is substantially different in that we model a much richer set of in-formation status distinctions.
Also, our choice of the news genre makes our studiescomplementary to the GREC challenges, which use Wikipedia articles about people orother named entities.
News stories tend to be about events, not people, and the choice ofinitial references to participants is particularly important to help the reader understandthe news.
Our research is thus largely focused on the generation of initial references.Due to their short length, summaries do not generally have long co-reference chainsand the issue of subsequent reference is of less interest to us.
Further, we aim to generatenew references to people by identifying semantic attributes that are appropriate giventhe context of the summary.
In contrast, the GREC challenges only require the selectionof an existing referring expression from a list.3.
Study 1: Discourse-New and Discourse-Old MentionsOur first study on information status deals with the discourse-new (first mention)versus discourse-old (subsequent mention) distinction.
This is the easiest of the threeto model, as it is explicitly given in the text.
Nevertheless, referring expressions inextractive summaries can be problematic in this respect as sentences compiled fromdifferent documents might contain too little, too much, or repeated information aboutthe referent.
The first summary reference to a person, for example, may have been thesecond reference to that person in the input article and thus might not contain enoughinformation to be comprehensible.
Conversely, if the second summary reference to aperson occurred first in the input article, it may contain more information than needed.In general, in fluent human written text, discourse-new references to entities are longerand more descriptive, whereas discourse-old references are shorter and have a purelyreferential function.
This is not always the case in automatic summaries: Figure 1 showstwo extractive summaries.
The summaries give a good indication of the problems withreference that can arise in multi-document summaries.
In the first summary, referencesto the former East German Communist leader Erich Honecker are overly repetitive andunnecessary.
The extra identification at each mention totals about 15 words, equivalentto the length of an additional informative sentence that could have been included816Siddharthan, Nenkova, and McKeown Information Status and References to PeopleFigure 1Examples of problematic extractive summaries.
The first contains too little information in thefirst reference and too much repeated detail in subsequent references.
The second summarydoes not provide any information about any of the participants in the story.instead.
In the second summary, the references to Turkish politicians are likely to remainunclear to most readers because of the reduced forms that are realized.3.1 Corpus AnalysisIn order to develop strategies for addressing these issues, we performed a corpus studyon the syntactic form of references to people in news text.
We used a corpus of newsfrom the test data used in the Document Understanding Conference (DUC) Multi-document Summarization Track (2001?2003), containing 651,000 words and comingfrom 876 news reports from six different news agencies.
The variety of sources isimportant because working with text from a single source could lead to the learningof paper-specific editorial rules.The reference characteristics we were interested in were number of pre-modifiers,presence and type of post-modifiers, and the form of name used to refer to people.The corpus was automatically annotated for person name occurrence and co-referenceusing Nominator (Wacholder, Ravin, and Choi 1997).
Syntactic form of references wasobtained using Charniak?s parser (Charniak 2000).
This automatically annotated corpuscontains references to 6,240 distinct people.The distribution of forms for discourse-new and discourse-old references are shownin Table 1.
For discourse-old references, computing the probability of a syntactic real-ization is not as straightforward as for discourse-new references, because the form ofthe reference is influenced by the form of previous references, among other factors.To capture this relationship, we used the data from discourse-old mentions to form aMarkov chain, which captures exactly the probability of transitioning from one form ofreference to another.
The stationary distributions of the chains for name and pre- andpost-modification were computed as an indication of the likelihood of each form in anydiscourse-old mentions, and are shown in the last column of Table 1.It is evident from these statistics that, in general, discourse-new references shouldcontain the full name and some form of modification, whereas discourse-old referencesshould be referred to only by surname.
At the same time, there are occasions whenthe surname only can be used for discourse-new references and, for one in every fourreferences, modifiers are also not necessary.
It is thus important to identify referent817Computational Linguistics Volume 37, Number 4Table 1Likelihood of reference forms for discourse-new and discourse-old references in DUCmulti-document track news clusters.Discourse-new Discourse-oldName Form Full name 0.97 0.08Surname only 0.02 0.87Other (e.g., Britney, JLo) 0.01 0.05Pre-Modification Any 0.51 0.21None 0.49 0.79Post-Modification None 0.60 0.89Apposition 0.25 0.04Relative clause 0.07 0.03Other 0.08 0.04Any Modification Some Modification 0.76 0.30(Either Pre- or Post-) No Modification 0.24 0.70properties that can help determine when these less-common types of references arefelicitous, and we will indeed turn to this question in Section 4.3.2 Algorithm for Generating Discourse-New and Discourse-Old Referencesto PeopleTo capture the patterns revealed by the corpus analysis, we developed a set of rewriterules for references to people shown in Algorithm 1.
These rules ensure that thediscourse-new reference is descriptive and conveys the full name of the person, andthat discourse-old references are as brief as possible, using only the surname of theperson.
For the discourse-new reference, information from all news articles in thesummarization input is used.
Applying Algorithm 1 to the extractive summaries fromFigure 1 produces the rewrite versions of the summaries in Figure 2.3.3 EvaluationTo evaluate the impact of the rewrite rules on the overall summary, Algorithm 1was used to rewrite 11 summaries chosen at random from the DUC 2001 and 2002summaries that contained at least one reference to a person.
Four human judges(graduate students at Columbia University, two in computational linguistics and twoin other areas) were then given the pairs of the original summary and its rewrittenvariant without being explicitly told which is which.
They were asked to read thesummaries and decide if they prefer one text over the other or if they are equal.
Theywere also asked to give free-form comments on what they would change themselves.The distribution of the 44 preferences is as follows: 39 (89%) preferences were forthe rewritten form, 4 (9%) were for the original, and there was no preference for theremaining 1 (2%).
This preference for the rewritten form is significant (z-test; p < 0.01).In two out of the four cases where the assessor preferred the original version, theycommented that the reason for the preference was that the original version exhibitedmore variation.
This observation indicates that the rule for strictly using surname atdiscourse-old references is too rigid and most probably will need modification in caseswhere a person is mentioned more often.818Siddharthan, Nenkova, and McKeown Information Status and References to PeopleRewrite rules for discourse-new references:1.
IF the person?s name is the head of the noun phrase THEN:(a) IF any pre-modification is found in the input THEN:i. Insert full name and the longest, in number of words, pre-modifiersequence found in the input articles.
In case of ties, thediscourse-new reference from the article from which the summarysentence is drawn is preferred.
(b) ELSE IF no pre-modification is found in the input THEN:i.
Check all discourse-new references in the input to see if any of themincludes an apposition modifier.ii.
Take the longest such modifier and include it in the discourse-newreference NP.
Pre-modification is preferred to apposition becausethey are more frequently used in human-produced texts as shown inthe statistics above (in 51% vs. 25% of cases).2.
ELSE IF The name is not the head of the noun phrase it appears in, do not rewriteit.Rewrite rules for discourse-old references:1.
Use surname only, remove all pre- and post-modifiers.Algorithm 1: Form of discourse-new vs. discourse-old references.Figure 2Rewritten versions of extractive summaries from Figure 1.These results show that even the basic distinction of discourse-new and discourse-old reference in the input and the summaries, the simplest distinction we explore, canhelp improve summaries.
Still, our distributional analysis of syntactic forms in human-generated summaries showed that a full quarter of the discourse-new references containno modification at all, and half the discourse-new references contained either no mod-ification or just a title or role modifier (e.g., Mr. or President).
There are also differencesin the forms of references to people between news reports and human summaries ofnews (Siddharthan, Nenkova, and McKeown 2004), so not all discourse-new referencesfrom the input should be reused directly.
Journalistic conventions for many mainstreamnewspapers dictate that initial mentions to people include a minimum description suchas their role or title and affiliation.
However, in news summaries written by humans,there are greater space constraints that might warrant shorter references to people.In order to capture reference variation and compression correctly, we need further819Computational Linguistics Volume 37, Number 4distinctions (familiarity and global salience) that will help us determine when it isfelicitous to leave out information.
We next describe how we learn these distinctions(Section 4) before using them to make generation decisions (Section 5).4.
Study 2: Automatic Prediction of Referent Familiarity and SalienceDeciding how much and what information to include in a reference is influenced byat least two factors: the degree of familiarity of the referent (hearer-old vs. hearer-newdistinction) and the degree of global salience of the referent (is the entity a major or minorcharacter in the news event).
These two distinctions have not previously been studiedin the context of summarization.
There is a trade-off, particularly important for a shortsummary, between what the speaker wants to convey and how much the listener needsto know.
The major/minor distinction plays a role in defining the communication goal(what the summary should be about, which characters are important enough to refer toby name, etc.).
The hearer-old/hearer-new distinction can be used to determine whethera description for a character is required from the listener?s perspective.Hearer-old vs. hearer-new.
Hearer-new entities in a summary should be described insufficient detail, whereas hearer-old entities do not require an introductory description.This distinction can have a significant impact on overall length and intelligibility of theproduced summaries.
Usually, summaries are very short?100 or 200 words for inputarticles totaling 5,000 words or more.
Several people might be involved in a story, whichmeans that if all participants are fully described, little space will be devoted to actualnews.
In addition, introducing already-familiar entities might distract the reader fromthe main story (Grice 1975).
It can therefore be useful to refer to an entity that can beassumed hearer-old by just a title + surname (e.g., President Obama) or by full name only,with no accompanying description (e.g., Michael Jackson).
Such application of the hearer-old/hearer-new distinction can explain the results from our corpus study of realizationsin which we found that 49% of discourse-new references contain only the name or thename and title with no additional descriptive information.Major vs. minor.
Another distinction that human summarizers seem to make is whethera character in a story is a central or a minor one and this distinction can be conveyed byusing different forms of referring expressions.
It is common to see references in humansummaries such as the dissident?s father.
Usually, discourse-new references made solelywith a common noun phrase, without the inclusion of the person?s name, are employedwhen the person is not the main focus of a story and is only peripherally related tothe main story focus.
By automatically inferring if a character is major or minor for thestory, we can decide whether to name the character in the summary.
Furthermore, manysummarization systems use the presence of named entities as a feature for computingthe importance of a sentence (Ge, Huang, and Wu 2003; Saggion and Gaizaukas 2004).The ability to identify and use only the major story characters for computing sentenceimportance can benefit such systems because, in the multi-document summarizationtrack of DUC, only 5% of all people mentioned in the input are also mentioned in thehuman summaries.In this section, we report our experiments on building an automatic predictor for thehearer-old/hearer-new and major/minor distinctions.
For these experiments, we usedata from DUC to approximate the distinctions of interest, without the need for manualannotation.
We then validate the performance of the predictor on an independent dataset using human judgments as gold standard, achieving high accuracy.
These two820Siddharthan, Nenkova, and McKeown Information Status and References to Peopledistinctions are then used to automatically predict the form and content of discourse-new reference in summaries (Section 5).4.1 Data PreparationWe use data from the DUC multi-document summarization collection (2001?2004),consisting of 170 pairs of document input sets (10 documents per set) and the corre-sponding human-written multi-document summaries (two or four per set).
Our aimis to identify every person mentioned in a document set (the 10 news reports andthe associated human summaries), assign labels for the hearer-old/hearer-new andmajor/minor distinctions, and generate feature sets for supervised learning.
To do this,we first pre-process the data as described next.4.1.1 Automatic Pre-Processing.
Our experiment requires an analysis of every reference toa person in the input documents.
For this purpose, we use a variety of tools to performnamed entity resolution and co-reference, and to analyze the content of references,including pre-modifiers and post-modification using relative clauses or appositives.To identify the level of analysis required, we manually inspected the first 500discourse-new references to people from human summaries in the DUC 2001?2004data.
We found that 71% of pre-modifying words were either title or role words (e.g.,Prime Minister, Physicist, or Dr.) or temporal role modifying adjectives such as former ordesignate.
Affiliation (country, state, location, or organization names) constituted 22% ofpre-modifying words.
All other kinds of pre-modifying words, such as moderate or loyalconstituted only 7%.
We concluded that there are two attributes that are particularlyrelevant for generation of discourse-new references of people: role and affiliation.1For named entity recognition and semantic analysis of references, all input doc-uments and summaries were tagged using IDENTIFINDER (Bikel, Schwartz, andWeischedel 1999) to mark up person names, organizations, and locations.
We markedup countries and American states using a list obtained from the CIA factsheet.2 Thelist consists of 230 country/nationality pairs.
To mark up roles, we used a list derivedfrom WordNet (Miller et al 1993) hyponyms of the person synset.
Our list has 2,371entries including multiword expressions such as chancellor of the exchequer, brother inlaw, senior vice president, and so forth.
The list is quite comprehensive and includes rolesfrom the fields of sports, politics, religion, military, business, and many others.
We alsoused WordNet to obtain a list of 58 temporal adjectives.
WordNet classifies these as pre-(e.g., occasional, former, incoming) or post-nominal (e.g., elect, designate, emeritus).In addition, the documents and summaries were annotated with a part-of-speechtagger and simplex noun-phrase chunker (Grover et al 2000).
Also for each named en-tity, relative clauses, appositional phrases, and copula constructs, as well as pronominalco-reference, were automatically annotated (Siddharthan 2003a, 2003b).
The goal wasto find, for each person mentioned in the input set, the list of all references to the personin all input documents.
For this purpose, all input documents were concatenatedand processed with IDENTIFINDER.
The IDENTIFINDER output was automatically1 Our findings are not specific to the news genre or the summarization task; Sekine and Artiles (2009)report that their annotators marked 123 attributes of people mentioned in 156 Web documents.
The fourmost frequent attributes in their collection were: occupation, work, affiliation, and full name; these arethe same attributes that we identify.2 https://www.cia.gov/library/publications/the-world-factbook/fields/2110.html, whichprovides a list of countries and states, abbreviations, and adjectival forms; for example, UnitedKingdom/U.K./British/Briton and California/Ca./Californian.821Computational Linguistics Volume 37, Number 4Figure 3Example information collected for Andrei Sakharov from two news reports.
?IR?
stands for?initial reference?, ?CO?
for noun co-reference, ?PR?
for pronoun reference, ?AP?
forapposition, ?RC?
for relative clause and ?IS?
for copula.post-processed to mark up co-referring names and to assign a unique canonical namefor each name co-reference chain.
For co-reference, a simple rule of matching the sur-name was used, and the canonical name was the ?FirstName LastName?
string wherethe two parts of the name could be identified.3 Concatenating all documents assures thatthe same canonical name will be assigned to all named references to the same person.The tools for pronoun co-reference and clause and apposition identification andattachment (Siddharthan 2002, 2003a) were run separately on each document.
Then foreach item in the list of canonical names derived from the IDENTIFINDER output, wematched the initial reference in the generic co-reference list for the document with thesurname from the canonical name list.
The pre-processing steps described previouslyallow us to collect co-reference information and realization forms (see Figure 3) for eachperson in each input set, for documents and summaries.The tools that we used have been evaluated separately when used in a single doc-ument setting.
In our cross-document matching processes, we could incur more errors,for example, when the co-reference chain in the merged documents is not accurate.On average, out of 27 people per input cluster of documents, 4 people are lost in thematching step for a variety of reasons such as errors in the clause identifier or theco-reference.4.1.2 Data Labeling.
We are interested in acquiring labeled data for familiarity (whether aperson is likely to be hearer-old or hearer-new) and global salience (whether a person isa major or minor character in the news story).
We now describe how we create labeleddata for each of these distinctions.Hearer-old vs. hearer-new.
Entities were automatically labeled as hearer-old or hearer-new by analyzing the syntactic form that human summarizers used for initial refer-ences to them.
The labeling rests on the assumption that the people who producedthe summaries used their own prior knowledge in choosing appropriate referencesfor the summary.
Thus, they could refer to people they regarded as familiar to thegeneral public using short forms such as (1) title or role + surname or (2) full nameonly with no pre- or post-modification.
Entities were labeled as hearer-old when themajority of human summarizers for the set referred to them using the forms (1) or(2) (we discuss the validity of this automatic labeling process in Section 4.3.1).
Thehearer-new/hearer-old distinction is dynamic; when initially unfamiliar characters (like3 Occasionally, two or more different people with the same surname are discussed in the same set and thisalgorithm would lead to errors in such cases.
We did keep a list of first names associated with the entity,so a more refined matching model could be developed, but this was not the focus of our work.822Siddharthan, Nenkova, and McKeown Information Status and References to PeopleSaddam Hussein before the first Gulf War) appear in the news over a period of time,they can become hearer-old.
Thus the classification of the same person can be differentfor different document sets dating to different years.
From the people mentioned inhuman summaries, we obtained 118 examples of hearer-old and 140 examples of hearer-new persons?258 examples in total?for supervised machine learning.Major vs. minor.
In order to label an entity as major or minor, we again used the humansummaries.
Entities that were mentioned by name in at least one summary were labeledmajor, and those not mentioned by name in any summary were labeled minor.
Theunderlying assumption is that people who are not mentioned in any human summary,or are mentioned without being named, are not central to the story.
There were 258major characters whose names made it to at least one human summary and 3,926 minorcharacters whose names were absent from all human summaries.
The small fractionof major entities in the corpus is not surprising, because many people in news articlesexpress opinions, make statements, or are in some other way indirectly related to thestory, without being central to it.4.2 Machine Learning ExperimentsHaving created labeled data for classifying people as hearer-new or hearer-old and asmajor or minor characters, we now proceed to learn these distinctions in a supervisedframework.
For our experiments, we used the WEKA (Holmes, Donkin, and Witten1994) machine learning toolkit and obtained the best results for hearer-old/hearer-newusing a support vector machine (Sequential Minimal Optimization [SMO] algorithm,with default parameters) and for major/minor, a tree-based classifier (J48, with WEKAparameters: ?J48 -U -M 4?
).We now discuss what features we used for our two classification tasks (see thelist of features in Table 2).
Our hypothesis is that features capturing the frequency andsyntactic and lexical forms of references are sufficient to infer the desired distinctions.The frequency features are likely to give a good indication of the global salience of aperson in the document set.
Pronominalization indicates that an entity was particularlysalient at a specific point of the discourse, as has been widely discussed in attentionalstatus and centering literature (Grosz and Sidner 1986; Gordon, Grosz, and Gilliom1993).
Modified noun phrases (with apposition, relative clauses, or pre-modification)can also signal different information status; for instance, we expect post-modification tobe more prevalent for characters who are less familiar.
For our lexical features, we usedtwo months worth of news articles collected over the Web (and independent of the DUCcollection) to collect unigram and bigram lexical models of discourse-new references ofpeople.
The names themselves were removed from the discourse-new reference nounphrases and the counts were collected over the pre-modifiers only.
One of the lexicalfeatures we used is whether a person?s description contains any of the 20 most frequentdescription words from our Web corpus.
We reasoned that these frequent descriptorsmay signal importance; the full list is:president, former, spokesman, sen, dr, chief, coach, attorney, minister, director, gov, rep, leader,secretary, rev, judge, US, general, manager, chairmanWe also used features based on the overall likelihood of a person?s description using thebigram model from our Web corpus.
These features can help indicate whether a personhas a role or affiliation that is important.823Computational Linguistics Volume 37, Number 4Table 2List of features used for classification.Frequency Features0,1: Number of references to the person,including pronouns (total andnormalized by feature 2)2: Total number of documentscontaining the person3: Proportion of discourse-newreferences containing full name4: Number of times the person wasreferred to by name after thediscourse-new referenceSyntactic Features5,6: Number of appositives or relativeclauses attaching to initial references(total and normalized by feature 2)7,8: Number of times apposition wasused to describe the person (totaland normalized by feature 2)9,10: Number of times a relative clausewas used to describe the person(total and normalized by feature 2)11,12: Number of apposition, relativeclause or copula descriptions (totaland normalized by feature 2)13,14: Number of copula constructionsinvolving the person (total andnormalized by feature 2)Lexical Features15,16,17: Probability of an initial referenceaccording to a bigram model (av.,max, and min of all initial references)18: Number of top 20 high frequencydescription words (from referencesto people in a large news corpus)present in initial referencesIn the experiments reported subsequently, all our features are derived exclusivelyfrom the input documents, and we do not derive any features from the summaries.
Weperformed 20-fold cross validation for both classification tasks.4.2.1 Hearer-Old vs. Hearer-New Results.
We present our results for classifying peopleas hearer-old/hearer-new in Table 3.
The 0.54 majority class prediction for the hearer-old/hearer-new classification task is that no-one is known to the reader.
Using thisprediction in a summarizer would result in excessive detail in referring expressionsand a consequent reduction in space available to summarize the news events.
The SMOprediction outperformed the baseline accuracy by 22 percentage points (significant atp = 0.01, z-test) and is more meaningful for real tasks.We performed feature selection (using the WEKA CfsSubsetEval attribute evaluatorand BestFirst -D 1 -N 5 search method) to identify which are the most important featuresfor this classification task.
The important features were: the number of appositions (fea-tures 7, 8) and relative clauses (feature 9), number of mentions within the document set(features 0,1), total number of apposition, relative clauses and copula (feature 12), num-ber of high frequency pre-modifiers (feature 18), and the minimum bigram probability(feature 17).
Thus, the lexical and syntactic features were more useful than frequencyfeatures for determining familiarity.4.2.2 Major vs. Minor Results.
For major/minor classification, the majority class predic-tion has 94% accuracy (Table 4), but is not useful for a reference generation task as it824Siddharthan, Nenkova, and McKeown Information Status and References to PeopleTable 3Cross-validation Accuracy and P/R/F results for hearer-old vs. hearer-new predictions (258 datapoints).
The improvement in accuracy of SMO over the baseline is statistically significant (z-test;p < 0.01).Classifier Accuracy Class Precision Recall FSMO (Only frequency features) 0.64 hearer-new 0.76 0.43 0.55hearer-old 0.56 0.84 0.67SMO (Only lexical features) 0.65 hearer-new 0.64 0.81 0.71hearer-old 0.68 0.47 0.55SMO (Only syntactic features) 0.72 hearer-new 0.82 0.65 0.73hearer-old 0.67 0.83 0.74SMO (frequency+lexical features) 0.66 hearer-new 0.65 0.82 0.72hearer-old 0.70 0.48 0.57SMO (all features) 0.76 hearer-new 0.84 0.68 0.75hearer-old 0.69 0.85 0.76Majority class prediction 0.54 hearer-new 0.54 1.00 0.70hearer-old 0.00 0.00 0.00Table 4Cross-validation Accuracy and P/R/F results for major vs. minor predictions (4,184 data points).The improvement in accuracy of J48 over the baseline is statistically significant (z-test; p < 0.01).Classifier Accuracy Class Precision Recall FJ48 (Only frequency features) 0.95 major-character 0.81 0.38 0.51minor-character 0.96 0.99 0.98J48 (Only lexical features) 0.94 major-character 0.70 0.16 0.26minor-character 0.95 0.99 0.97J48 (Only syntactic features) 0.95 major-character 0.72 0.36 0.48minor-character 0.96 0.99 0.98J48 (frequency+lexical features) 0.96 major-character 0.69 0.47 0.56minor-character 0.96 0.99 0.98J48 (all features) 0.96 major-character 0.70 0.53 0.60minor-character 0.97 0.99 0.98Majority class prediction 0.94 major-character 0.00 0.00 0.00minor-character 0.94 1.00 0.97predicts that no person should be mentioned by name and all are minor characters.The machine learning approach improves on the baseline accuracy by two percentagepoints, which is statistically significant (z-test; p < 0.01).
Due to the skewed nature ofour data, precision/recall measures are more useful for analyzing our results.
Table 4shows the performance of the machine learner with different combinations of frequencyand lexical and syntactic features.
The best results are obtained using all three types offeatures and it appears that all three aspects are important, yielding an F-measure of0.60 for the smaller major-character class and and 0.98 for the majority minor-characterclass.In a task where ten 400?500 word documents are summarized into 100 words,human summarizers can differ in their interpretations of what is most important toconvey.
This is a well-established and studied fact in summarization (van Halteren andTeufel 2003).
To study how human agreement on the major/minor distinction relatesto our automatic prediction results, we further analyzed the 148 persons from DUC825Computational Linguistics Volume 37, Number 4Table 5J48 Recall results and human agreement for major vs. minor classifications.Number of summaries Number of Number and %containing the person examples recalled by J481 out of 4 59 15 (20%)2 out of 4 35 20 (57%)3 out of 4 29 23 (79%)4 out of 4 25 21 (84%)?03 and DUC ?04 sets for which DUC provides four human summaries (there wereonly two summaries provided for earlier sets).
Table 5 presents the distribution ofrecall taking into account how many humans mentioned the person by name in theirsummary (in our data-labeling, people are labeled as major if any summary had areference to them, see Section 4.1.2).
It can be seen that recall is high (0.84) when allfour humans consider a character to be major, and falls to 0.2 when only one out of fourhumans does.We performed feature selection (using the WEKA CfsSubsetEval attribute evaluatorand BestFirst -D 1 -N 5 search method) to identify which are the most important featuresfor the classification task.
For the major/minor classification, the important featuresused by the classifier were the number of documents in which the person was men-tioned (feature 2); number of mentions within the document set (features 1, 4); numberof relative clauses (feature 9, 10) and copula (feature 13) constructs; total number ofapposition, relative clauses, and copula (feature 11); number of high frequency pre-modifiers (feature 18); and the maximum bigram probability (feature 16).As for the hearer-old/hearer-new classification, the syntactic forms of referenceswere a significant indicator, suggesting that the centrality of the character was signaledby journalists using specific syntactic constructs in the references.
On the other hand,unlike the case of familiarity classification, the frequency of mention within and acrossdocuments were also significant features.
This is intuitive?a frequently mentionedperson is likely to be important to the story.4.3 Validating the Results on Current NewsWe tested the classifiers on data different from that provided by DUC, and also testedhuman consensus on the hearer-new/hearer-old distinction.
For these purposes, wedownloaded 45 clusters from one day?s output from Newsblaster (McKeown et al 2002).We then automatically compiled the list of people mentioned in the automatic sum-maries for these clusters.
There were 107 unique people that appeared in the automaticsummaries and 1,075 people in the input clusters.4.3.1 Human Agreement on Hearer-Old vs. Hearer-New.
A question arises when attemptingto infer hearer-new/hearer-old status: Is it meaningful to generalize this across readers,seeing how dependent it is on the world knowledge of individual readers?To address the question, we gave four American graduate students at ColumbiaUniversity a list of the names of people in the DUC human summaries (see Section 4.1),and asked them to write down for each person, their country/state/organizationaffiliation and their role (writer/president/attorney-general, etc.).
We considered a826Siddharthan, Nenkova, and McKeown Information Status and References to PeopleTable 6Accuracy, precision, and recall for Newsblaster data.Class Precision Recall F-MeasureHearer-old 0.88 0.73 0.80Hearer-new 0.57 0.79 0.66person hearer-old to a subject if they correctly identified both role and affiliation forthat person.
For the 258 people in the DUC summaries, the four subjects demonstrated87% agreement (?
= 0.74).4Similarly, they were asked to perform the same task for the Newsblaster data, whichdeals with contemporary news,5 in contrast with the DUC data that contained newsfrom the late 1980s and early 1990s.
On these data, the human agreement was 91% (?
=0.78).
This is a high enough agreement to suggest that the classification of national andinternational figures as hearer-old/hearer-new for educated readers is a well-definedtask.4.3.2 Hearer-Old vs. Hearer-New Results on the Newsblaster Data.
We measured how wellthe models learned on DUC data perform with current news labeled using humanjudgment.
For each person who was mentioned in the automatic summaries for theNewsblaster data, we compiled one judgment from the four human subjects usingmajority vote (an example was labeled as hearer-new if two or more out of the foursubjects had marked it as hearer-new; the ties were resolved in favor of hearer-new asit is better to provide an initial description of a person when unsure about the person?sstatus).
Then we used these data as test data, to test the model trained solely on theDUC data.
These results are reported in Table 6.
The classifier for hearer-old/hearer-new distinction achieved 75% accuracy on Newsblaster data labeled by humans (sig-nificantly better than the majority class (hearer-new) baseline of 60.8%; z-test, p = 0.02).This compares well with the reported cross-validation accuracy on DUC data of 76%and indicates that the performance of the classifier is stable and does not vary betweenthe DUC and Newsblaster data.
The precision and recall for the Newsblaster data (seeTable 6) are also very similar to those for the DUC data.4.3.3 Major vs. Minor Results on Newsblaster Data.
For the Newsblaster data, no humansummaries were available, so no direct indication of whether a human summarizerwill mention a person by name in a summary was available.
In order to evaluatethe performance of the classifier, we gave a human annotator (a graduate student atColumbia University) the list of people?s names appearing in the machine summaries,together with the input cluster and the machine summary, and asked which of thenames on the list would be a suitable keyword for the set.
Our aim here was to verifythat our classifications of people as major or minor correlate with another indicator ofimportance?suitability for use as a keyword.4 ?
(kappa) is a measure of inter-annotator agreement over and above what might be expected by purechance (see Carletta [1996] for discussion of its use in NLP).
?
= 1 if there is perfect agreement betweenannotators, ?
= 0 if the annotators agree only as much as you would expect by chance, ?
< 0 if theannotators agree less than predicted by chance.5 The human judgments were made within a week of the publication of the news stories in theNewsblaster clusters.827Computational Linguistics Volume 37, Number 4Out of the 107 names on the list, the annotator chose 42 as suitable for descriptivekeyword for the set.
The major/minor classifier was run on these 107 examples; only 40were predicted to be major characters.
Of the 67 test cases that were predicted by theclassifier to be minor characters, 12 (18%) were marked by the annotator as acceptablekeywords.
In comparison, of the 40 characters that were predicted to be major charactersby the classifier, 30 (75%) were marked as possible keywords.
If the keyword selectionsof the annotator are taken as ground truth, the automatic predictions have precision andrecall of 0.75 and 0.71, respectively, for the major class.5.
Using Automatically Inferred Distinctions for Generation DecisionsHaving trained models to predict whether a person is a major or a minor character,and whether the person is likely to be hearer-old or hearer-new to the intended au-dience, we can now make informed decisions on how to generate initial referencesto people in summaries.
In this section, we demonstrate the predictive power of thedistinctions we have acquired, by showing how we can determine when to include thename attribute (Section 5.1); post-modification such as apposition or relative clauses(Section 5.2); and pre-modification using specific semantic attributes such as affiliation,role, and temporal modifiers (Section 5.3).
Then, in Section 6, we present and evaluateour full algorithm for generating referring expressions to people in multi-documentsummaries.5.1 Decision 1: Including the Name AttributeAccording to our theory, only major characters should be named in a summary.
Inaddition to using up words, naming minor characters can mark them as being importantand distract the reader from the main story by introducing Gricean implicatures (Grice1975).In our data, there were 258 people mentioned by name in at least one humansummary.
In addition to these, there were 103 people who were mentioned in at leastone human summary using only a common noun reference (these were identified byhand, as common noun co-reference cannot be performed reliably enough by automaticmeans).
This means that 29% of people mentioned in human summaries are not actuallynamed.
Examples of such references include an off duty black policeman, a Nigerian bornRoman catholic priest, and Kuwait?s US ambassador.Our WEKA machine learner for the major/minor distinction achieved a testingaccuracy of 74% on these 103 examples.
In other words, we can reproduce humanjudgment on which people to refer to by name in three quarters of cases.
This is a veryencouraging result given the novelty of the task.As mentioned before, different human summarizers can sometimes make differentdecisions on the form of reference to use.
Out of the 103 examples of people withan unnamed reference in at least one human summary, there were 63 people whowere not mentioned by name in any summary.
WEKA correctly labeled 58 (92%) asminor characters.
Out of the 40 cases where some summarizers used named referenceand others used common noun reference, 22 of these 40 (55%) were labeled as minorcharacters.
As before, we observe that when human summarizers generate referencesof the same form (reflecting consensus on conveying the perceived importance of thecharacter), the machine predictions are very accurate.828Siddharthan, Nenkova, and McKeown Information Status and References to People5.2 Decision 2: Elaborating Using Post-ModificationOne aspect of reference generation that is informed by the hearer-old/hearer-newstatus is the use of apposition or relative clauses for elaboration.
It has been observed(Siddharthan, Nenkova, and McKeown 2004) that, on average, these constructs occur2.3 times less frequently in human summaries than in machine summaries.
Post-modification tends to be more lengthy than pre-modification, and by predicting whenthis is not required, we can achieve large reductions in length.To determine when an appositive or relative clause can be used to modify a refer-ence, we considered the 151 examples out of 258 where there was at least one relativeclause or apposition describing the person in the input.
We labeled an example aspositive if at least one human summary contained an apposition or relative clause forthat person and negative otherwise.
There were 66 positive and 85 negative examples.This data is informative because although for the majority of examples (56%) all thehuman summarizers agreed not to use post-modification, there were very few examples(under 5%) where all the humans agreed to post-modify.
This reflects the high cost inword count for using these forms of modification.
Intuitively, it would appear that foraround half the cases (56%), it should be obvious that no post-modification is required,but for the other half, opinions can vary.We report that none of the hearer-old persons (as classified by the SMO algorithm)were post-modified.
Our predictions cleanly partition the examples into those wherepost-modification is not required, and those where it might be.
Because we could notthink of any simple rule that handled the remaining examples, we added the testingpredictions of hearer-old/hearer-new and major/minor as features to the list in Table 2and tried to learn this task using the tree-based learner J48.
We report a testing accuracyof 71.5%, which is significantly higher than both the 56% for the majority class baseline(z-test; p < 0.01) and 62.5% for a baseline using the original feature set without the twoinformation status features (z-test; p < 0.05).There were only three useful features?the predicted hearer-new/hearer-old status,the number of high frequency pre-modifiers for that person in the input (feature 18in Table 2), and the average number of post-modified initial references in the inputdocuments (feature 12).5.3 Decision 3: Including Pre-Modifying AttributesAs mentioned in Section 4.1.1, our analysis of pre-modification in initial referencesto people in DUC human summaries showed that 71% of pre-modifying words wereeither title or role words or temporal role modifying adjectives.
Affiliations constituted22% of pre-modifying words and all other pre-modifying words, such as moderate orloyal constituted only 7%.
We therefore only consider the inclusion of roles, temporalmodifiers, and affiliations in this section.5.3.1 Including Role and Temporal Modification Attributes.
The DUC human summarizerstended to follow journalistic conventions regarding the inclusion of a title or role ininitial references to people.
Indeed a simple rule?to always include the role/title ininitial references?reproduced the choices made by the human summarizers in 79% ofcases.
A manual analysis of cases where human summarizers omitted title/role wordsrevealed some insights.
There were a small number of historical figures (e.g., Galileo andNapoleon) and people from the entertainment industry (e.g., Robert Redford and Yoko Ono)who were always referred to only by name.
Otherwise, the main factor appears to be829Computational Linguistics Volume 37, Number 4notoriety.
People who were almost never referred to with a title or role include MoammarGadhafi, Osama bin Laden, Fidel Castro, Yasser Arafat, and Boris Yeltsin.
Others who werereferred to both with and without a title/role by different human summarizers includeGeorge Bush, Bill Clinton, Margaret Thatcher, Michael Gorbachev, and Slobodan Milosevic.As we have no insights as to how to model notoriety, we did not try to improve on this?always include?
baseline, but we can nonetheless suggest that for greater compression,the role or title can be omitted for hearer-old persons; for example, generating MargaretThatcher instead of Former Prime Minister Margaret Thatcher.5.3.2 Including Affiliation Attributes.
We now describe a procedure that uses hearer anddiscourse information to decide when to provide an affiliation in the initial referenceto a person.
This issue is ubiquitous in summarizing news; for example, the referencegenerator might need to decide between White House Press Secretary James Brady andPress Secretary James Brady, between Soviet President Gorbachev and President Gorbachev,or between Indiana Senator Dan Quayle and Senator Dan Quayle.1.
IF:(a) the person is classified as hearer-old OR(b) the person?s organization (country/ state/ affiliation) has been already mentionedAND is the most salient organization in the discourse at the point where thereference needs to be generatedTHEN the affiliation of a person can be omitted in the discourse-new reference.Algorithm 2: Omitting the affiliation in a discourse-new reference.Based on our intuitions about discourse salience and information status, we initiallypostulated the decision procedure in Algorithm 2.
We described how we make thehearer-new/hearer-old judgment in Section 4.2.
We used a salience-list (S-List) (Strube1998) to determine the salience of organizations.
This is a shallow attentional-statemodel and works as follows:1.
Within a sentence, entities are added to the salience-list from left to right.2.
Within the discourse, sentences are considered from right to left.In other words, entities in more recent sentences are more salient than those in previousones, and within a sentence, earlier references are more salient than later ones.Results.
To make the evaluation meaningful, we only considered examples where therewas an affiliation mentioned for the person in the input documents, ruling out the trivialcases where there was no choice to be made (i.e., an affiliation could never be included).There were 272 initial references to 182 persons in the human summaries that met thiscriterion (note that there were multiple human summaries for each document set).We used 139 of these 272 examples (from DUC ?01, ?02, and ?03) as training data tocheck and possibly refine our rule.
For each of these 139 initial references to people, we:1.
Obtained from the source news reports the test-set prediction from WEKA onwhether that person was hearer-new or hearer-old.830Siddharthan, Nenkova, and McKeown Information Status and References to People2.
Formed the S-List for affiliations in that human summary at the point of reference.63.
Used the decision procedure in Algorithm 2 to decide whether or not to include theaffiliation in the reference.The evaluation consisted of matching our predictions with the observed referencesin the human summaries.
Our decision procedure made the correct decision in 71%of the instances and successfully modeled variations in the initial references used bydifferent human summarizers for the same document set:1.
Brazilian President Fernando Henrique Cardoso was re-elected in the...[hearer-new and Brazil not in context]2.
Brazil?s economic woes dominated the political scene as President Cardoso...[hearer-new and Brazil most salient country in context]It also modeled variation in initial references to the same person across summaries ofdifferent document sets:1.
It appeared that Iraq?s President Saddam Hussein was determined to solve hiscountry?s financial problems and territorial ambitions...[hearer-new for this document set and Iraq not in context]2.
...A United States aircraft battle group moved into the Arabian Sea.
SaddamHussein warned the Iraqi populace that United States might attack...[hearer-old for this document set]An error analysis showed that in most of these instances the rule predicted noaffiliation in instances where the human summarizer had included it.
In many cases,the person was first mentioned in a context where a different organization/state orcountry was more salient than their own.
When we modified condition (1) of ourdecision rule (Algorithm 2) to obtain Algorithm 3, the accuracy increased to 78%.
Theimproved performance of our second decision procedure suggests that affiliation issometimes included in references to even hearer-old persons in order to aid the hearerin immediately recollecting the referent.
Both algorithms make errors on such cases,however, and there appears to be some variability in how human summarizers maketheir decisions in these contexts.1.
IF:(a) the person is hearer-old, and no country/state/org is more salient than their own OR(b) the person?s organization (country/ state/ affiliation) has been already mentionedAND is the most salient organization in the discourse at the point where thereference needs to be generatedTHEN the affiliation of a person can be omitted in the discourse-new reference.Algorithm 3: Omitting the affiliation in a discourse-new reference (version 2).Having convinced ourselves of the validity of these rules, we applied them to the133 examples in the unseen test data.
The results are shown in Table 7.
A total of 85%6 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviationsand adjectival forms; and the named entity recognition tool IDENTIFINDER marks up organizations.
Theoutput was manually cleaned to remove errors in named entity detection.831Computational Linguistics Volume 37, Number 4Table 7Test set results for the decision procedure to include affiliation in initial references.
Both rules aresignificantly better than all the baselines (z-test; p < 0.05).Algorithm AccuracyNever-Include Baseline 0.56Information-Status Baseline 0.58Salience Baseline 0.65Salience+Information Status (Algorithm 2) 0.79Salience+Information Status (Algorithm 3) 0.75of the observed human references were modeled correctly by either Algorithm 2 orAlgorithm 3, demonstrating that the hearer-old/hearer-new distinction is relevant toreference generation.
The remaining errors were largely due to misclassifications ofpeople as hearer-new by SMO, thus leading our rule to include affiliation when notrequired.
We compared the rules?
prediction accuracy to that of three baselines (seeTable 7):1.
Never-Include: This is the majority class baseline that says that affiliation is alwaysomitted.2.
Information-Status: Always include if hearer-new, never include if hearer-old(using testing predictions from automatic classification of information status).3.
Salience: Include affiliation unless that affiliation is already the most salient at thepoint of reference.Algorithm 2 performs significantly better than all baselines (z-test; p < 0.01),whereas Algorithm 3 performs significantly better than the first two baselines (z-test;p < 0.01) and for the third (z-test; p < 0.05).6.
An Algorithm for Generating References to People in SummariesHaving shown that important information status distinctions can be acquired auto-matically (Section 4) and that these distinctions predict aspects of the content andform of references (Section 5), we now update Algorithm 1 from Section 3 to obtainAlgorithm 4, our full algorithm for generating references to people in summaries thattakes into account the discourse-new vs. discourse-old, hearer-new vs. hearer-old, andmajor vs. minor distinctions.
Table 8 summarizes the accuracy of this algorithm inpredicting human generation decisions, as reported in Sections 3 and 5.
Next, we reportan evaluation of the extent to which reformulating references to people impacts on thequality of news summaries.6.1 Evaluation of Summaries Rewritten According to Algorithm 4We evaluated our algorithm using 14 news clusters from the Google News world newssection.7 We selected these clusters in the order they were presented on the GoogleNews site; we excluded three clusters that we deemed too similar to already selected7 http://news.google.com/news/section?&topic=w.832Siddharthan, Nenkova, and McKeown Information Status and References to PeopleRewrite rules for discourse-new references:1.
IF the person?s name is the head of the noun phrase THEN:(a) IF Minor Character THEN:i.
EXCLUDE name from reference and only INCLUDE role, temporalmodification, and affiliation(b) ELSE IF Major Character AND Hearer-old THEN:i.
INCLUDE nameii.
INCLUDE role and any temporal modifier, to follow journalisticconventionsiii.
EXCLUDE other modifiers including affiliationiv.
EXCLUDE any post-modification such as apposition or relativeclauses(c) ELSE IF Major Character AND Hearer-new THEN:i.
INCLUDE nameii.
INCLUDE role and any temporal modifier, to follow journalisticconventionsiii.
IF the person?s affiliation has already been mentioned AND is themost salient organization in the discourse at the point where thereference needs to be generated THEN EXCLUDE affiliation ELSEINCLUDE Affiliationiv.
Use machine learner described in Section 5.2 to decide whether toinclude post-modification2.
ELSE IF The name is not the head of the noun phrase it appears in, THEN it is notrewrittenRewrite rules for discourse-old references:1.
Use surname only, EXCLUDE all pre-modifiers and post-modifiersAlgorithm 4: Generating references to people in news summaries.clusters, however.
We used the first 10 articles in each cluster to create our 14 documentsets.
We then used the freely available extractive summarizer MEAD (Radev et al 2004)to generate 200 word summaries for each document set.
These extractive summarieswere automatically rewritten according to Algorithm 4, as described subsequently.
Ourevaluation compares the extractive and rewritten summaries.Table 8Summary of the accuracy of Algorithm 4 for specific regeneration decisions.Generation Decision Section Prediction AccuracyDiscourse-new referencesInclude Name Section 5.1 .74 (rising to .92 when there is unanimityamong human summarizers)Include Role & temporal mods Section 5.3.1 .79Include Affiliation Section 5.3.2 .75 to .79 (depending on rule)Include Post-Modification Section 5.2 .72 (rising to 1.00 when there is unanimityamong human summarizers)Discourse-old referencesInclude Only Surname Section 3 .70833Computational Linguistics Volume 37, Number 4Implementation of Algorithm 4.
Our reference rewrite module operates on parsetrees obtained using the Stanford Parser (Klein and Manning 2003).
For each personautomatically identified using the techniques described in Section 4.1.1, we matchedevery mention of their surname in the parse trees of MEAD summary sentences.
Wethen replaced the enclosing NP (includes all pre- and post-modifying constituents)with a new NP generated using Algorithm 4.
The regenerated summary was producedautomatically, without any manual correction of parses, semantic analyses, or informa-tion status classifications.
We now enumerate implementation details not covered inSection 4.1.1:1.
Although our algorithm determines when to include role and affiliation attributes,it doesn?t inform us as to which ones to include; for instance, a politician mighthave a constituency, party, or nationality affiliation.
Our implementation selectsthe most frequently mentioned role (e.g., prime minister) in references to thatperson in the document set, and then selects the affiliation associated with thatrole (e.g., British).2.
Another situation that arises is when our algorithm prescribes the inclusion ofaffiliation (or role), but our semantic tagger does not find any affiliation (or role)for that person.
In these cases, we check whether there exists any relative clause orapposition, as post-modification is often used to introduce people without usingrole or affiliation attributes.
If no post-modification is available, we include thelongest initial reference to the person from the input documents.3.
Different conventions exist regarding which name of a person to use for reference.For instance, in China it is traditional to use the first name (e.g., Chinese VicePremier Li Keqiang is commonly referred to in news articles as Li).
We do notclaim to model such naming conventions; rather we use the co-reference chainsin our analysis to pick the most frequent name used in co-reference (see [CO]tags in Figure 3).In total, there were 61 references to people modified in the 14 summaries.
Ofthose, 34 involved shortening subsequent references.
For initial references, there were6 instances of removing affiliations, 4 of adding affiliations, and 10 of adding roles.There were also 6 instances of post-modifications added to initial references and1 removed.Experimental design.
The evaluation was carried out over the Internet using aninterface that, on each slide, showed the two summaries (before and after referencerewriting) side by side with the left one labeled A and the right one B. Underneaththe summaries, there were three multiple choice questions and one free text question.
Ascreen-shot is shown in Figure 4.
The order of presentation of summaries was controlledfor (i.e., for each summary pair, equal numbers of participants saw the regeneratedsummary on the left and on the right, and for each participant, summary order on eachslide was pseudo-randomized).
In order to prevent evaluator fatigue, we split our 14summary pairs into two sets of 7 pairs, with each participant evaluating only one set.Participants were provided with the following instructions:To help our research into summarizing news stories, please compare the followingpairs of summaries, 7 in total.
On each slide, the two summaries are quite similar,but we would like you to tell us which you prefer, and which is more informativeand coherent.
In addition, we would appreciate any subjective assessment of thesummaries, particularly with regard to the amount of information provided aboutparticipants in the news story.834Siddharthan, Nenkova, and McKeown Information Status and References to PeopleFigure 4Screen shot of evaluation interface.Results.
Twenty participants (undergraduate and postgraduate students and researchfellows at Columbia University, the University of Pennsylvania, and the Universityof Aberdeen) completed the evaluation.
The results are summarized in Table 9.
Ourevaluation shows that reference rewriting makes sentences more coherent (significant atp < 0.01, z-test, sample size = 140), and that this is preferred by participants (significantat p < 0.01, z-test, sample size = 140).
The loss of informativeness (significant at p <0.01, z-test, sample size = 140) through our rewrites is not unexpected; in general we areremoving information from references, and we only rarely add information to a sum-mary.
This is evident from the summary lengths; the rewritten summary is shorter for11 out of 14 document sets and the average word lengths of the extractive and rewrittensummaries are 189 and 178, respectively.
Indeed, we found a strong correlation between835Computational Linguistics Volume 37, Number 4Table 9Results for the evaluation for rewritten summaries.
(a) Number of times any participant selected one summary type over the other(140 comparisons):More informative More coherent More preferredExtractive 46 22 37Rewritten 23 79 69No difference 71 39 34(b) Number of document sets for which participants selected one summary type more oftenthan the other (14 document sets):More informative More coherent More preferredExtractive 9 2 2Rewritten 4 12 10Equal 1 0 2differences in informativeness and differences in summary lengths (Spearman?s rho =.79; significant at p < 0.001).
We did not find any similar correlation between differencesin summary lengths and either coherence (Spearman?s rho = .05, p = 0.86) or overallpreference (Spearman?s rho = .25, p = 0.39).We also performed repeated measures ANOVAs on coherence, informativeness,and overall preference, with summary-type (Extractive or Rewritten) as a within-participant variable.
For this purpose, we converted our categorical user responses (?A?,?B?, or ?Both the same?)
into numerical ratings for each summary-type condition asfollows.
We first mapped the choice of ?A?
or ?B?
to the summary-type and then: If the user selected Extractive, we used a rating of 1 for Extractive and?1 for Rewritten. If the user selected Rewritten, we used a rating of 1 for Rewritten and?1 for Extractive. If the user selected ?Both the same?, we used a rating of 0 for bothconditions.Then, treating participants (F1) and news clusters (items; F2) as random factors,we found a main effect of summary-type on Coherence (F1(1, 19) = 34.20, p < 0.001;F2(1, 13) = 3.91, p = 0.001).
This analysis confirms that the improvement in coherenceis significant even when the variation between participants and items (news clusters)is taken into account.
We found a smaller by-participant effect of summary-type onInformativeness, and no by-item effect (F1(1, 19) = 6.12, p = 0.023; F2(1, 13) = 2.24, p =0.157).
For overall preference, we found a main by-participant effect and a smaller by-item effect of summary-type (F1(1, 19) = 10.49, p = 0.004; F2(1, 13) = 3.23, p = 0.09).6.2 Discussion of Limitations and Scope of Algorithm 4The evaluation described in the previous section provides us with some feedback aboutthe limitations of our approach, regarding the factors we consider in our algorithm as836Siddharthan, Nenkova, and McKeown Information Status and References to Peoplewell as the automatic analysis we require to fully implement it.
We had requested par-ticipants to explain their decisions, and this free text feedback proved very informative.We summarize what we learned: Sentence length is an important factor.
Introducing post-modification intoa sentence that was already long, or removing it from a sentence that wasalready short, resulted in a dispreference for the rewritten summary.
Notethat sentence length is not a feature we had given any consideration to inour algorithm. The lack of common noun co-reference in our automatic analysis cost us.In one summary, there were common noun references to the widelyrecognized winner of Ivory Coast?s election and the internationally recognizedwinner of Ivory Coast?s presidential election.
Because our analysis was unableto co-refer these to the named reference Alassane Ouattara, the referencerewrite module actually introduced more redundancy into the summary. Our strategy of selecting the most frequent role from the input was notoptimal.
In more than one instance, this strategy selected the role leader,although the original summary had a more informative role such as primeminister.
This was commented on and penalized by multiple participantsfor informativeness and often for overall preference as well.
Indeed, wefound our participants to be very sensitive to the choice of role modifier,consistently penalizing summaries that omit the more specific role (therewere 17 comments to this effect, in contrast there were only 3 commentscomplaining about lack of affiliation). Mistakes introduced during automated analysis cost us.
When the Obamaadministration slapped wide-ranging sanctions got rewritten as Obama slappedwide-ranging sanctions due to incorrect NP matching, the latter was deemedbiased and misleading.
There was one particular mistake in roleidentification that got penalized by all participants when the algorithmgenerated spin doctor David Cameron.
The phrase in the input documentthat got misanalyzed was Andy Coulson, David Cameron?s spin doctor. Participants often disagree.
Two different participants provided thefollowing comments: ...repetition of ref expressions clarify some ambiguitiesand ...introduced differently in each sentence, and that made it harder to see thatit was the same person.In addition to these limitations that are mostly concerned with implementationissues, we need to reiterate that our claims are genre-specific.
We have studied thenature of references to people in the newswire and news summary genres.
This is incontrast to many other studies on reference that make use of experimental data fromhuman reference tasks (e.g., Gatt, Belz, and Kow 2008), corpus data from dialog (e.g.,Gupta and Stent 2005), or encyclopedic texts (e.g., Belz, Kow, and Viethen 2009).
Thereare big differences in how people are referred to in different genres, arising from bothlinguistic conventions and the nature of the information in the genre.
For instance,encyclopedic or biographical texts about a person contain long co-reference chains,and information about the person is provided throughout the article.
Thus, subsequentreferences in such genres are not straightforward, and research arising from the GRECchallenges has thus justifiably focused on modeling subsequent references.837Computational Linguistics Volume 37, Number 4The nature of references to people in the news genre is quite different.
As newsarticles tend to be about events rather than people, there tends to be less informationprovided about the people involved in the story, and these descriptions are almostalways provided in the first reference to the person (see Study 1, Section 3).
Thus inthe news genre, unlike encyclopedic texts, the task of content selection for subsequentreferences to people is rather uninteresting.
This is even more so for the news summarygenre, where co-reference chains are typically short, resulting in few subsequent refer-ences to anybody.
We find that for the news summary genre, the form and content ofinitial reference is critical, and our studies have thus focused on this.
Specifically, dueto the nature of our genre, we do not attempt to model anaphoric phenomena such aspronouns and common noun co-reference.
The studies reported in this article shouldthus be seen as complementary to efforts such as GREC, and we believe that suchstudies of reference in different genres are important to get a better understanding ofthe phenomenon.7.
ConclusionsOur research both provides a characterization of references to people in the news genrethrough empirical analysis and uses that characterization to develop a model for gen-erating references to people in news summaries that is based on automatically inferredinformation status distinctions.
Because summarization takes its content from input fulltext articles, one contribution of our work is the development of a statistical model forinferring such distinctions from news reports, without requiring manual annotation.We have shown how this model can then be used to generate appropriate references,including semantic content selection (inclusion of name, role, and affiliation attributes)and realization choices (use of pre- or post-modification).References to people have very different properties from other kinds of referringexpressions (e.g., common noun references to objects).
Research on the generation ofreferring expressions from semantic representations is based on the notion of selectingattributes that distinguish the object from others; in contrast, discourse-new referencesto people often contain attributes in addition to the name, and yet the name alone wouldbe a distinguishing attribute.
In this article, we have conducted corpus-based studies toprovide answers to how and when attributes are used in references to people in thenews genre.
Our study characterizes the differences in discourse-new and discourse-old references, identifies the attributes typically used in discourse-new references topeople, and provides evidence that information status distinctions (global salience andfamiliarity) can determine when people are named in summaries and when additionalattributes such as role and affiliation are needed.These information status distinctions are important when generating summariesof news, as they help determine both what to say and how to say it.
However, usingthese distinctions for summarization requires inferring information from unrestrictednews.
We have shown that the hearer-old/hearer-new and major/minor distinctionscan be inferred reliably using features derived from the lexical and syntactic forms andfrequencies of references in the news reports.
These acquired distinctions are usefulfor determining which characters to name in summaries, which characters to furtherdescribe or elaborate on, and the forms that any descriptions should take.Finally, we have reported an evaluation of the effect of reference rewriting on extrac-tive summaries, demonstrating that our rewrites improve coherence and are generallypreferred by readers.838Siddharthan, Nenkova, and McKeown Information Status and References to PeopleReferencesAreces, Carlos, Alexander Koller, andKristina Striegnitz.
2008.
Referringexpressions as formulas of descriptionlogic.
In Proceedings of the Fifth InternationalNatural Language Generation Conference,pages 42?49, Salt Fork, OH.Barzilay, Regina.
2003.
Information Fusion forMultidocument Summarization: Paraphrasingand Generation.
Ph.D. thesis, ColumbiaUniversity, New York.Barzilay, Regina and Kathleen McKeown.2005.
Sentence fusion for multidocumentnews summarization.
ComputationalLinguistics, 31(3):297?328.Belz, Anja, Eric Kow, and Jette Viethen.2009.
The GREC named entity generationchallenge 2009: overview and evaluationresults.
In Proceedings of the 2009 Workshopon Language Generation and Summarisation,pages 88?98, Suntec, Singapore.Belz, Anja and Sebastian Varges.
2007.Generation of repeated references todiscourse entities.
In Proceedings of the11th European Workshop on NaturalLanguage Generation (ENLG?07),pages 9?16, Schloss Dagstuhl, Germany.Bikel, Daniel, Richard Schwartz, and RalphWeischedel.
1999.
An algorithm that learnswhat?s in a name.
Machine Learning,34:211?231.Calhoun, Sasha.
2007.
Predicting focusthrough prominence structure.In Proceedings of Interspeech?07,pages 622?625, Antwerp, Belgium.Carletta, Jean.
1996.
Assessing agreementon classification tasks: The kappastatistic.
Computational Linguistics,22(2):249?254.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe 1st Annual Conference of North AmericanChapter of the Association for ComputationalLinguistics, pages 132?139, Seattle, WA.Dale, Robert.
1992.
Generating ReferringExpressions: Constructing Descriptions in aDomain of Objects and Processes.
MIT Press,Cambridge, MA.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of thegricean maxims in the generation ofreferring expressions.
Cognitive Science,19(2):233?263.Dale, Robert and Jette Viethen.
2009.Referring expression generation throughattribute-based heuristics.
In Proceedingsof the 12th European Workshop on NaturalLanguage Generation, pages 58?65,Athens, Greece.Daume?
III, Hal, Arda Echihabi, DanielMarcu, Dragos Munteanu, and RaduSoricut.
2002.
GLEANS: A generator oflogical extracts and abstracts for nicesummaries.
In Proceedings of the SecondDocument Understanding Conference(DUC 2002), pages 9?14, Philadelphia, PA.Daume?
III, Hal and Daniel Marcu.
2002.A noisy-channel model for documentcompression.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL ?
2002),pages 449?456, Philadelphia, PA.Denis, Alexandre.
2010.
Generatingreferring expressions with referencedomain theory.
In Proceedings of the 6thInternational Natural Language GenerationConference-INLG, pages 27?36, Dublin,Ireland.Fellbaum, Christine.
1998.
WordNet.
Anelectronic lexical database.
Cambridge,MA: MIT Press.Filippova, Katja and Michael Strube.
2008.Sentence fusion via dependency graphcompression.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 177?185.Galley, Michel and Kathleen McKeown.2007.
Lexicalized Markov grammars forsentence compression.
In Proceedings ofHuman Language Technologies: The AnnualConference of the North American Chapter ofthe Association for Computational Linguistics(NAACL-HLT), pages 180?187.Gatt, Albert, Anja Belz, and Eric Kow.
2008.The TUNA challenge 2008: Overview andevaluation results.
In Proceedings of the FifthInternational Natural Language GenerationConference, pages 198?206.Gatt, Albert and Kees Van Deemter.
2007.Lexical choice and conceptual perspectivein the generation of plural referringexpressions.
Journal of Logic, Languageand Information, 16(4):423?443.Ge, Jiayin, Xuanjing Huang, and Lide Wu.2003.
Approaches to event-focusedsummarization based on named entitiesand query words.
In DocumentUnderstanding Conference (DUC?03).Gordon, Peter, Barbara Grosz, and LauraGilliom.
1993.
Pronouns, names, andthe centering of attention in discourse.Cognitive Science, 17:311?347.Greenbacker, Charles F. and Kathleen F.McCoy.
2009.
Feature selection forreference generation as informed bypsycholinguistic research.
In Proceedings ofthe CogSci 2009 Workshop on the Productionof Referring Expressions: Bridging the Gap839Computational Linguistics Volume 37, Number 4between Computational and EmpiricalApproaches to Reference (PRE-CogSci 2009).Grice, Paul.
1975.
Logic and conversation.In P. Cole and J. L. Morgan, editors, Syntaxand Semantics, volume 3.
Academic Press,New York, pages 43?58.Grosz, Barbara, Aravind Joshi, and ScottWeinstein.
1995.
Centering: a frameworkfor modelling the local coherence ofdiscourse.
Computational Linguistics,21(2):203?226.Grosz, Barbara and Candice Sidner.
1986.Attention, intentions, and the structureof discourse.
Computational Linguistics,3(12):175?204.Grover, Claire, Colin Matheson, AndreiMikheev, and Marc Moens.
2000.LT TTT: A flexible tokenization toolkit.In Proceedings of the Second InternationalConference on Language Resources andEvaluation, pages 1147?1154, Athens,Greece.Gundel, Jeanette, Nancy Hedberg, andRon Zacharski.
1993.
Cognitive statusand the form of referring expressionsin discourse.
Language, 69:274?307.Gupta, Surabhi and Amanda J. Stent.
2005.Automatic evaluation of referringexpression generation using corpora.In Proceedings of the 1st Workshop onUsing Corpora in NLG, pages 1?6,Birmingham.Henschel, Renate, Hua Cheng, andMassimo Poesio.
2000.
Pronominalizationrevisited.
In Proceedings of the 18thConference on Computational Linguistics(COLING?2000), pages 306?312,Saarbru?cken, Germany.Holmes, Geoffrey, Andrew Donkin, andIan H. Witten.
1994.
Weka: A machinelearning workbench.
In Proceedings ofthe 2nd Australian and New ZealandConference on Intelligent InformationSystems, pages 357?361, Brisbane,Australia.Horacek, Helmut.
2003.
A best-first searchalgorithm for generating referringexpression.
In Proceedings of the 11thConference of the European Chapter of theAssociation for Computational Linguistics(EACL?03), pages 103?106, Budapest,Hungary.Jing, Hongyan and Kathleen McKeown.2000.
Cut and paste based textsummarization.
In Proceedings of the1st Conference of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL?00), pages 178?185,Seattle, WA.Jordan, Pamela and Marilyn Walker.
2005.Learning content selection rules forgenerating object descriptions in dialogue.Journal of Artificial Intelligence Research,24:157?194.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics,pages 423?430, Sapporo, Japan.Knight, Kevin and Daniel Marcu.
2000.Statistics-based summarization?stepone: Sentence compression.
In Proceedingsof The American Association for ArtificialIntelligence Conference (AAAI-2000),pages 703?710, Austin, TX.Krahmer, Emiel and Marie?t Theune.
2002.Efficient context-sensitive generation ofreferring expressions.
In Kees van Deemterand Rodger Kibble, editors, InformationSharing: Givenness and Newness in LanguageProcessing.
CSLI Publications, Stanford,CA, pages 223?264.Krahmer, Emiel, Sebastiaan van Erk,and Andre?
Verleg.
2003.
Graph-basedgeneration of referring expressions.Computational Linguistics, 29(1):53?72.Luhn, H. P. 1958.
The automatic creation ofliterature abstracts.
IBM Journal of Researchand Development, 2(2):159?165.Mani, Inderjeet, Barbara Gates, and EricBloedorn.
1999.
Improving summaries byrevising them.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL?99), pages 558?565,College Park, MD.McCoy, Kathleen and Michael Strube.1999.
Generating anaphoric expressions:Pronoun or definite description?
InProceedings of ACL?99 Workshop on theRelationship Between Discourse/DialogueStructure and Reference, pages 63?72,College Park, MD.McKeown, Kathleen, Regina Barzilay,David Evans, Vasleios Hatzivassiloglou,Judith Klavans, Ani Nenkova, Carl Sable,Barry Schiffman, and Sergey Sigelman.2002.
Tracking and summarizing newson a daily basis with Columbia?sNewsblaster.
In Proceedings of the2nd Human Language TechnologiesConference HLT-02, pages 280?285,San Diego, CA.Nenkova, Ani.
2008.
Entity-driven rewritefor multi-document summarization.
InProceedings of the Third International JointConference on Natural Language Processing(IJCNLP08), pages 118?125, Hyderabad,India.840Siddharthan, Nenkova, and McKeown Information Status and References to PeopleNenkova, Ani and Dan Jurafsky.
2007.Automatic detection of contrastiveelements in spontaneous speech.
In IEEEWorkshop on Automatic Speech Recognitionand Understanding (ASRU), pages 201?206,Kyoto, Japan.Nenkova, Ani and Kathleen McKeown.
2003.References to named entities: a corpusstudy.
In Proceedings of the 2003 Conferenceof the North American Chapter of theAssociation for Computational Linguistics onHuman Language Technology: CompanionVolume of the Proceedings of HLT-NAACL2003?short papers-Volume 2, pages 70?72,Edmonton, Canada.Nenkova, Ani, Advaith Siddharthan,and Kathleen McKeown.
2005.Automatically learning cognitive statusfor multi-document summarization ofnewswire.
In HLT ?05: Proceedings of theconference on Human Language Technologyand Empirical Methods in Natural LanguageProcessing, pages 241?248, Vancouver,British Columbia, Canada.Nissim, Malvina.
2006.
Learning informationstatus of discourse entities.
In Proceedingsof the Conference on Emprical Methods inNatural Language Processing (EMNLP),pages 94?102, Sydney, Australia.Otterbacher, Jahna C., Dragomir R. Radev,and Airong Luo.
2002.
Revisions thatimprove cohesion in multi-documentsummaries: a preliminary study.
InProceedings of the ACL-02 Workshop onAutomatic Summarization, pages 27?36,Philadelphia, PA.Paice, Chris D. 1990.
Constructing literatureabstracts by computer: techniques andprospects.
Information ProcessingManagement, 26(1):171?186.Postolache, Oana, Ivana Kruijff-Korbayova,and Geert-Jan Kruijff.
2005.
Data-drivenapproaches for information structureidentification.
In Proceedings ofHLT/EMNLP, pages 9?16, Vancouver,Canada.Prince, Ellen.
1992.
The ZPG letter: subject,definiteness, and information status.In S. Thompson and W. Mann, editors,Discourse Description: Diverse Analysesof a Fund Raising Text.
John Benjamins,Amsterdam, The Netherlands,pages 295?325.Radev, Dragomir, Timothy Allison, SashaBlair-Goldensohn, John Blitzer, ArdaC?elebi, Stanko Dimitrov, Elliott Drabek,Ali Hakim, Wai Lam, Danyu Liu, JahnaOtterbacher, Hong Qi, Horacio Saggion,Simone Teufel, Michael Topper, AdamWinkel, and Zhu Zhang.
2004.
MEAD?Aplatform for multidocument multilingualtext summarization.
In Proceedings of the4th Conference on Language Resources andEvaluation (LREC?2004), pages 699?702,Lisbon, Portugal.Radev, Dragomir and Kathleen McKeown.1997.
Building a generation knowledgesource using internet-accessible newswire.In Proceedings of the Fifth Conference onApplied Natural Language Processing,pages 221?228, Washington, DC.Radev, Dragomir and Kathleen McKeown.1998.
Generating natural languagesummaries from multiple on-line sources.Computational Linguistics, 24(3):469?500.Ren, Yuan, Kees van Deemter, and Jeff Pan.2010.
Charting the potential of descriptionlogic for the generation of referringexpressions.
In Proceedings of the 6thInternational Natural Language GenerationConference (INLG), pages 115?124, Dublin,Ireland.Saggion, Horacio and Rob Gaizaukas.2004.
Multi-document summarization bycluster/profile relevance and redundancyremoval.
In Document UnderstandingConference (DUC04).Sekine, Satoshi and Javier Artiles.
2009.Weps2 attribute extraction task.
In 2ndWeb People Search Evaluation Workshop(WePS 2009), 18th WWW Conference,Madrid, Spain.Siddharthan, Advaith.
2002.
Resolvingattachment and clause boundaryambiguities for simplifying relativeclause constructs.
In Proceedings of theStudent Workshop, 40th Meeting of theAssociation for Computational Linguistics(ACL?02), pages 60?65, Philadelphia, PA.Siddharthan, Advaith.
2003a.
Resolvingpronouns robustly: Plumbing the depthsof shallowness.
In Proceedings of theWorkshop on Computational Treatments ofAnaphora, 11th Conference of the EuropeanChapter of the Association for ComputationalLinguistics (EACL?03), pages 7?14,Budapest, Hungary.Siddharthan, Advaith.
2003b.
SyntacticSimplification and Text Cohesion.
Ph.D.thesis, University of Cambridge, UK.Siddharthan, Advaith and Ann Copestake.2004.
Generating referring expressions inopen domains.
In Proceedings of the 42thMeeting of the Association for ComputationalLinguistics Annual Conference (ACL 2004),pages 407?414, Barcelona, Spain.Siddharthan, Advaith, Ani Nenkova, andKathleen McKeown.
2004.
Syntactic841Computational Linguistics Volume 37, Number 4simplification for improving contentselection in multi-documentsummarization.
In Proceedings ofthe 20th International Conference onComputational Linguistics (COLING 2004),pages 896?902, Geneva, Switzerland.Sridhar, Vivek Kumar Rangarajan, AniNenkova, Shrikanth Narayanan,and Dan Jurafsky.
2008.
Detectingprominence in conversational speech:pitch accent, givenness and focus.In Proceedings of the 4th Conferenceon Speech Prosody, pages 453?456,Campinas, Brazil.Steinberger, Josef, Massimo Poesio,Mijail Alexandrov Kabadjov, andKarel Jezek.
2007.
Two uses of anaphoraresolution in summarization.
InformationProcessing and Management,43(6):1663?1680.Strube, Michael.
1998.
Never look back:An alternative to centering.
In Proceedingsof the 18th International Conference onComputational Linguistics (COLING?98),pages 1251?1257, Montreal, Quebec,Canada.van Deemter, Kees.
2002.
Generatingreferring expressions: Booleanextensions of the incremental algorithm.Computational Linguistics, 28(1):37?52.van Deemter, Kees, Albert Gatt,Roger van Gompel, and Emiel Krahmer.2010.
Production of Referring Expressions(PRE-CogSci) 2009: Bridging the gapbetween computational and empiricalapproaches to reference.
Journal ofMemory and Language, 54:554?573.van Halteren, Hans and Simone Teufel.2003.
Examining the consensus betweenhuman summaries: Initial experimentswith factoid analysis.
In HLT-NAACLDUC Workshop.Viethen, Jette and Robert Dale.
2006.Algorithms for generating referringexpressions: Do they do what people do?In Proceedings of the Fourth InternationalNatural Language Generation Conference,pages 63?70, Sydney, Australia.Wacholder, Nina, Yael Ravin, andMisook Choi.
1997.
Disambigaution ofnames in text.
In Proceedings of the FifthConference on Applied NLP, pages 202?208,Washington, DC.Zajic, David, Bonnie Dorr, Jimmy Lin, andRichard Schwartz.
2007.
Multi-candidatereduction: Sentence compression as atool for document summarization tasks.Information Processing and Management,Special Issue on Summarization,43:1549?1570.842
