Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 963?970, Vancouver, October 2005. c?2005 Association for Computational LinguisticsQuery Expansion with the Minimum User Feedbackby Transductive LearningMasayuki OKABEInformation and Media CenterToyohashi University of TechnologyAichi, 441-8580, Japanokabe@imc.tut.ac.jpKyoji UMEMURAInformation and Computer SciencesToyohashi University of TechnologyAichi, 441-8580, Japanumemura@tutics.tut.ac.jpSeiji YAMADANational Institute of InformaticsTokyo,101-8430, Japanseiji@nii.ac.jpAbstractQuery expansion techniques generally se-lect new query terms from a set of topranked documents.
Although a user?smanual judgment of those documentswould much help to select good expansionterms, it is difficult to get enough feedbackfrom users in practical situations.
In thispaper we propose a query expansion tech-nique which performs well even if a usernotifies just a relevant document and anon-relevant document.
In order to tacklethis specific condition, we introduce tworefinements to a well-known query expan-sion technique.
One is application of atransductive learning technique in order toincrease relevant documents.
The other isa modified parameter estimation methodwhich laps the predictions by multiplelearning trials and try to differentiate theimportance of candidate terms for expan-sion in relevant documents.
Experimen-tal results show that our technique outper-forms some traditional query expansionmethods in several evaluation measures.1 IntroductionQuery expansion is a simple but very useful tech-nique to improve search performance by addingsome terms to an initial query.
While many queryexpansion techniques have been proposed so far, astandard method of performing is to use relevanceinformation from a user (Ruthven, 2003).
If wecan use more relevant documents in query expan-sion, the likelihood of selecting query terms achiev-ing high search improvement increases.
However itis impractical to expect enough relevance informa-tion.
Some researchers said that a user usually noti-fies few relevance feedback or nothing (Dumais andet al, 2003).In this paper we investigate the potential perfor-mance of query expansion under the condition thatwe can utilize little relevance information, espe-cially we only know a relevant document and a non-relevant document.
To overcome the lack of rele-vance information, we tentatively increase the num-ber of relevant documents by a machine learningtechnique called Transductive Learning.
Comparedwith ordinal inductive learning approach, this learn-ing technique works even if there is few training ex-amples.
In our case, we can use many documentsin a hit-list, however we know the relevancy of fewdocuments.
When applying query expansion, we usethose increased documents as if they were true rel-evant ones.
When applying the learning, there oc-curs some difficult problems of parameter settings.We also try to provide a reasonable resolution forthe problems and show the effectiveness of our pro-posed method in experiments.The point of our query expansion method is thatwe focus on the availability of relevance informationin practical situations.
There are several researcheswhich deal with this problem.
Pseudo relevancefeedback which assumes top n documents as rele-vant ones is one example.
This method is simple andrelatively effective if a search engine returns a hit-963list which contains a certain number of relative doc-uments in the upper part.
However, unless this as-sumption holds, it usually gives a worse ranking thanthe initial search.
Thus several researchers proposesome specific procedure to make pseudo feedbackbe effective (Yu and et al 2003; Lam-Adesina andJones, 2001).
In another way, Onoda (Onoda et al,2004) tried to apply one-class SVM (Support Vec-tor Machine) to relevance feedback.
Their purposeis to improve search performance by using only non-relevant documents.
Though their motivation is sim-ilar to ours in terms of applying a machine learningmethod to complement the lack of relevance infor-mation, the assumption is somewhat different.
Ourassumption is to utilizes manual but the minimumrelevance judgment.Transductive leaning has already been applied inthe field of image retrieval (He and et al, 2004).
Inthis research, they proposed a transductive methodcalled the manifold-ranking algorithm and showedits effectiveness by comparing with active learn-ing based Support Vector Machine.
However, theirsetting of relevance judgment is not different frommany other traditional researches.
They fix the totalnumber of images that are marked by a user to 20.As we have already claimed, this setting is not prac-tical because most users feel that 20 is too much forjudgment.
We think none of research has not yet an-swered the question.
For relevance judgment, mostof the researches have adopted either of the follow-ing settings.
One is the setting of ?Enough relevantdocuments are available?, and the other is ?No rele-vant document is available?.
In contrast to them, weadopt the setting of ?Only one relevant document isavailable?.
Our aim is to achieve performance im-provement with the minimum effort of judging rele-vancy of documents.The reminder of this paper is structured as fol-lows.
Section 2 describes two fundamental tech-niques for our query expansion method.
Section 3explains a technique to complement the smallnessof manual relevance judgment.
Section 4 introducesa whole procedure of our query expansion methodstep by step.
Section 5 shows empirical evidenceof the effectiveness of our method compared withtwo traditional query expansion methods.
Section 6investigates the experimental results more in detail.Finally, Section 7 summarizes our findings.2 Basic Methods2.1 Query ExpansionSo far, many query expansion techniques have beenproposed.
While some techniques focus on thedomain specific search which prepares expansionterms in advance using some domain specific train-ing documents (Flake and et al 2002; Oyama and etal, 2001), most of techniques are based on relevancefeedback which is given automatically or manually.In this technique, expansion terms are selectedfrom relevant documents by a scoring function.
TheRobertson?s wpq method (Ruthven, 2003) is oftenused as such a scoring function in many researches(Yu and et al 2003; Lam-Adesina and Jones, 2001).We also use it as our basic scoring function.
It cal-culates the score of each term by the following for-mula.wpqt =(rtR ?nt ?
rtN ?
R)?log rt/(R ?
rt)(nt ?
rt)/(N ?
nt ?
R + rt)(1)where rt is the number of seen relevant documentscontaining term t. nt is the number of documentscontaining t. R is the number of seen relevant doc-uments for a query.
N is the number of documentsin the collection.
The second term of this formulais called the Robertson/Spark Jones weight (Robert-son, 1990) which is the core of the term weightingfunction in the Okapi system (Robertson, 1997).This formula is originated in the following for-mula.wpqt = (pt ?
qt) logpt(1?
qt)qt(1?
pt)(2)where pt is the probability that a term t appears inrelevant documents.
qt is the probability that a termt appears in non-relevant documents.
We can easilynotice that it is very important how the two prob-ability of pt and qt should be estimated.
The firstformula estimates pt with rtR and qt withNt?RtN?R .
Forthe good estimation of pt and qt, plenty of relevantdocument is necessary.
Although pseudo feedbackwhich automatically assumes top n documents asrelevant is one method and is often used, its perfor-mance heavily depends on the quality of an initialsearch.
As we show later, pseudo feedback has lim-ited performance.We here consider a query expansion techniquewhich uses manual feedback.
It is no wonder964manual feedback shows excellent and stable perfor-mance if enough relevant documents are available,hence the challenge is how it keeps high perfor-mance with less amount of manual relevance judg-ment.
In particular, we restrict the manual judgmentto the minimum amount, namely only a relevantdocument and a non-relevant document.
In thisassumption, the problem is how to find more rele-vant documents based on a relevant document and anon-relevant document.
We use transductive learn-ing technique which is suitable for the learning prob-lem where there is small training examples.2.2 Transductive LearningTransductive learning is a machine learning tech-nique based on the transduction which directly de-rives the classification labels of test data withoutmaking any approximating function from trainingdata (Vapnik, 1998).
Because it does not need tomake approximating function, it works well even ifthe number of training data is small.The learning task is defined on a data set Xof n points.
X consists of training data setL = (x?1, x?2, ..., x?l) and test data set U =(x?l+1, x?l+2, ..., x?l+u); typically l ?
u.
The purposeof the learning is to assign a label to each data pointin U under the condition that the label of each datapoint in L are given.Recently, transductive learning or semi-supervised learning is becoming an attractivesubject in the machine learning field.
Severalalgorithms have been proposed so far (Joachims,1999; Zhu and et al, 2003; Blum and et al, 2004)and they show the advantage of this approach invarious learning tasks.
In order to apply transductivelearning to our query expansion, we select an algo-rithm called ?Spectral Graph Transducer (SGT)?
(Joachims, 2003), which is one of the state of the artand the best transductive learning algorithms.
SGTformalizes the problem of assigning labels to U withan optimization problem of the constrained ratiocut.By solving the relaxed problem, it produces anapproximation to the original solution.When applying SGT to query expansion, X cor-responds to a set of top n ranked documents in ahit-list.
X does not corresponds to a whole docu-ment collection because the number of documentsin a collection is too huge1 for any learning sys-tem to process.
L corresponds to two documentswith manual judgments, a relevant document anda non-relevant document.
Furthermore, U corre-sponds to the documents of X ?
L whose rele-vancy is unknown.
SGT is used to produce the rel-evancy of documents in U .
SGT actually assignsvalues around ?+ ?
?
for documents possibly be-ing relevant and ??
?
?
for documents possibly be-ing non-relevant.
?+ = +?1?fpfp , ??
= ?
?fp1?fp ,?
= 12(?+ + ??
), and fp is the fraction of relevantdocuments in X .
We cannot know the true value offp in advance, thus we have to estimate its approxi-mation value before applying SGT.According to Joachims, parameter k (the numberof k-nearest points of a data x?)
and d (the numberof eigen values to ...) give large influence to SGT?slearning performance.
Of course those two parame-ters should be set carefully.
However, besides them,fp is much more important for our task because itcontrols the learning performance.
Since extremelysmall L (actually |L| = 2 is our setting) give noinformation to estimate the true value of fp, we donot strain to estimate its single approximation valuebut propose a new method to utilize the results oflearning with some promising fp.
We describe themethod in the next section.3 Parameter Estimations based onMultiple SGT Predictions3.1 Sampling for Fraction of Positive ExamplesSGT prepares 2 estimation methods to set fp au-tomatically.
One is to estimate from the fractionof positive examples in training examples.
Thismethod is not suitable for our task because fp isalways fixed to 0.5 by this method if the numberof training examples changes despite the number ofrelevant documents is small in many practical situa-tions.
The other is to estimate with a heuristic thatthe difference between a setting of fp and the frac-tion of positive examples actually assigned by SGTshould be as small as possible.
The procedure pro-vided by SGT starts from fp = 0.5 and the next fp isset to the fraction of documents assigned as relevantin the previous SGT trial.
It repeats until fp changes1Normally it is more than ten thousand.965InputNtr // the number of training examplesOutputS // a set of sampling pointspiv = ln(Ntr); // sampling intervalnsp = 0; // the number of sampling pointsfor(i = piv; i ?
Ntr ?
1; i+ = piv){add i to ;nsp++;if(nsp == 10){ exit; }}Figure 1: Pseudo code of sampling procedure for fpfive times or the difference converges less than 0.01.This method is neither works well because the con-vergence is not guaranteed at all.Presetting of fp is primarily very difficult problemand consequently we take another approach whichlaps the predictions of multiple SGT trials with somesampled fp instead of setting a single fp.
This ap-proach leads to represent a relevant document by nota binary value but a real value between 0 and 1.
Thesampling procedure for fp is illustrated in Figure 1.In this procedure, sampling interval changes accord-ing to the number of training examples.
In our pre-liminary test, the number of sampling points shouldbe around 10.
However this number is adhoc one,thus we may need another value for another corpus.3.2 Modified estimations for pt and qtOnce we get a set of sampling points S = {f ip :i = 1 ?
10}, we run SGT with each f ip and lapseach resultant of prediction to calculate pt and qt asfollows.pt =?i rit?i Ri(3)qt =?i nt ?
rit?i N ?Ri(4)Here, Ri is the number of documents which SGTpredicts as relevant with ith value of f ip, and rit isthe number of documents in Ri where a term t ap-pears.
In each trial, SGT predicts the relevancy ofdocuments by binary value of 1 (for relevant) and 0(for non-relevant), yet by lapping multiple resultantof predictions, the binary prediction value changesto a real value which can represents the relevancy ofdocuments in more detail.
The main merit of thisapproach in comparison with fixing fp to a singlevalue, it can differentiate a value of pt if Ntr is small.4 Expansion ProceduresWe here explain a whole procedure of our query ex-pansion method step by step.1.
Initial Search: A retrieval starts by inputting aquery for a topic to an IR system.2.
Relevance Judgment for Documents in aHit-List: The IR system returns a hit-list forthe initial query.
Then the hit-list is scannedto check whether each document is relevant ornon-relevant in descending order of the rank-ing.
In our assumption, this reviewing pro-cess terminates when a relevant document anda non-relevant one are found.3.
Finding more relevant documents by trans-ductive learning: Because only two judgeddocuments are too few to estimate pt and qtcorrectly, our query expansion tries to increasethe number of relevant documents for the wpqformula using the SGT transductive learning al-gorithm.
As shown in Figure2, SGT assigns avalue of the possibility to be relevant for thetopic to each document with no relevance judg-ment (documents under the dashed line in theFig) based on two judged documents (docu-ments above the dashed line in the Figure).1.
Document     12.
Document     03.
Document     ?4.
Document     ?:i.
Document     ?
:ManuallyassignedAssigned byTransductiveLearningLabelsHit list?1?
means a positive label?0?
means a negative label???
means an unknown labelFigure 2: A method to find tentative relevant docu-ments9664.
Selecting terms to expand the initial query:Our query expansion method calculates thescore of each term appearing in relevant docu-ments (including documents judged as relevantby SGT) using wpq formula, and then selectsa certain number of expansion terms accordingto the ranking of the score.
Selected terms areadded to the initial query.
Thus an expandedquery consists of the initial terms and addedterms.5.
The Next Search with an expanded query:The expanded query is inputted to the IR sys-tem and a new hit-list will be returned.
Onecycle of query expansion finishes at this step.In the above procedures, we naturally intro-duced transductive learning into query expan-sion as the effective way in order to automati-cally find some relevant documents.
Thus wedo not need to modify a basic query expan-sion procedure and can fully utilize the poten-tial power of the basic query expansion.The computational cost of transductive learn-ing is not so much.
Actually transductive learn-ing takes a few seconds to label 100 unla-beled documents and query expansion with allthe labeled documents also takes a few sec-onds.
Thus our system can expand queries suf-ficiently quick in practical applications.5 ExperimentsThis section provides empirical evidence on howour query expansion method can improve the per-formance of information retrieval.
We compare ourmethod with other traditional methods.5.1 Environmental Settings5.1.1 Data setWe use the TREC-8 data set (Voorhees and Har-man, 1999) for our experiment.
The document cor-pus contains about 520,000 news articles.
Each doc-ument is preprocessed by removing stopwords andstemming.
We also use fifty topics (No.401-450)and relevance judgments which are prepared for ad-hoc task in the TREC-8.
Queries for an initial searchare nouns extracted from the title tag in each topic.5.1.2 Retrieval ModelsWe use two representative retrieval models whichare bases of the Okapi (Robertson, 1997) andSMART systems.
They showed highest perfor-mance in the TREC-8 competition.Okapi : The weight function in Okapi is BM25.
Itcalculates each document?s score by the follow-ing formula.score(d) =?T?Qw(1) ?
(k1 + 1)tf(k3 + 1)qtf(K + tf)(k3 + qtf)(5)w(1) = log (rt + 0.5)/(R ?
rt + 0.5)(nt ?
rt + 0.5)/(N ?
nt ?
R + rt + 0.5)(6)K = k1((1 ?
b) + b dlavdl)(7)where Q is a query containing terms T , tfis the term?s frequency in a document, qtf isthe term?s frequency in a text from which Qwas derived.
rt and nt are described in sec-tion 2.
K is calculated by (7), where dl andavdl denote the document length and the av-erage document length.
In our experiments,we set k1 = 1.2, k3 = 1000, b = 0.75, andavdl = 135.6.
Terms for query expansion areranked in decreasing order of rt ?
w(1) for thefollowing Okapi?s retrieval tests without SGT(Okapi manual and Okapi pseudo) to makeconditions the same as of TREC-8.SMART : The SMART?s weighting function is asfollows2.score(d) =?T?Q{1 + ln(1 + ln(tf))} ?
log(N + 1df ) ?
pivot (8)pivot = 10.8 + 0.2 ?
dlavdl(9)df is the term?s document frequency.
tf , dl andavdl are the same as Okapi.
When doing rele-vance feedback, a query vector is modified bythe following Rocchio?s method (with parame-ters ?
= 3, ?
= 2, ?
= 2).Q?new = ?Q?old+?|Drel|?Dreld??
?|Dnrel|?Dnreld?
(10)2In this paper, we use AT&T?s method (Singhal et al, 1999)applied in TREC-8967Table 1: Results of Initial SearchP10 P30 RPREC MAP R05POkapi ini 0.466 0.345 0.286 0.239 0.195SMART ini 0.460 0.336 0.271 0.229 0.187Drel and Dnrel are sets of seen relevant andnon-relevant documents respectively.
Termsfor query expansion are ranked in decreasingorder of the above Rocchio?s formula.Table 1 shows their initial search results of Okapi(Okapi ini) and SMART (SMART ini).
We adoptfive evaluation measures.
Their meanings are as fol-lows (Voorhees and Harman, 1999).P10 : The precision after the first 10 documents areretrieved.P30 : The precision after the first 30 documents areretrieved.R-Prec : The precision after the first R documentsare retrieved, where R is the number of relevantdocuments for the current topic.MAP : Mean average precision (MAP) is the aver-age precision for a single topic is the mean ofthe precision obtained after each relevant doc-ument is retrieved (using zero as the precisionfor relevant documents that are not retrieved).R05P : Recall at the rank where precision first dipsbelow 0.5 (after at least 10 documents havebeen retrieved).The performance of query expansion or relevancefeedback is usually evaluated on a residual collec-tion where seen documents are removed.
Howeverwe compare our method with pseudo feedback basedones, thus we do not use residual collection in thefollowing experiments.5.1.3 Settings of Manual FeedbackFor manual feedback, we set an assumption thata user tries to find relevant and non-relevant doc-uments within only top 10 documents in the resultof an initial search.
If a topic has no relevant doc-ument or no non-relevant document in the top 10documents, we do not apply manual feedback, in-stead we consider the result of the initial search forTable 2: Results of Okapi sgt (5 terms expanded)P10 P30 RPREC MAP R05P20 0.516 0.381 0.308 0.277 0.23350 0.494 0.380 0.286 0.265 0.207100 0.436 0.345 0.283 0.253 0.177Table 3: Results of Okapi sgt (10 terms expanded)P10 P30 RPREC MAP R05P20 0.508 0.383 0.301 0.271 0.21650 0.520 0.387 0.294 0.273 0.208100 0.494 0.365 0.283 0.261 0.190Table 4: Results of Okapi sgt (15 terms expanded)P10 P30 RPREC MAP R05P20 0.538 0.381 0.298 0.274 0.22350 0.528 0.387 0.298 0.283 0.222100 0.498 0.363 0.280 0.259 0.197Table 5: Results of Okapi sgt (20 terms expanded)P10 P30 RPREC MAP R05P20 0.546 0.387 0.307 0.289 0.23550 0.520 0.385 0.299 0.282 0.228100 0.498 0.369 0.272 0.255 0.188such topics.
There are 8 topics 3 which we do notapply manual feedback methods.5.2 Basic PerformanceFirstly, we evaluate the basic performance of ourquery expansion method by changing the numberof training examples.
Since our method is based onOkapi model, we represent it as Okapi sgt (with pa-rameters k = 0.5?Ntr, d = 0.8?Ntr.
k is the num-ber of nearest neighbors, d is the number of eigenvalues to use and Ntr is the number of training ex-amples).Table 2-5 shows five evaluation measures ofOkapi sgt when the number of expansion termschanges.
We test 20, 50 and 100 as the number oftraining examples and 5, 10 15 and 20 for the num-ber of expansion terms.
As for the number of train-ing examples, performance of 20 and 50 does notdiffer so much in all the number of expansion terms.However performance of 100 is clearly worse thanof 20 and 50.
The number of expansion terms doesnot effect so much in every evaluation measures.
Inthe following experiments, we compare the resultsof Okapi sgt when the number of training examplesis 50 with other query expansion methods.3Topic numbers are 409, 410, 424, 425, 431, 432, 437 and450968Table 6: Results of Manual Feedback Methods(MAP)5 10 15 20Okapi sgt 0.265 0.273 0.274 0.282Okapi man 0.210 0.189 0.172 0.169SMART man 0.209 0.222 0.220 0.219Table 7: Results of Manual Feedback Methods (10terms expanded)P10 P30 RPREC MAP R05POkapi sgt 0.520 0.387 0.294 0.273 0.208Okapi man 0.420 0.285 0.212 0.189 0.132SMART man 0.434 0.309 0.250 0.222 0.1745.3 Comparison with other Manual FeedbackMethodsWe next compare our query expansion method withthe following manual feedback methods.Okapi man : This method simply uses only onerelevant document judged by hand.
This iscalled incremental relevance feedback (Aal-bersberg, 1992; Allan, 1996; Iwayama, 2000).SMART man : This method is SMART?s manualrelevance feedback (with parameters ?
= 3,?
= 2, ?
= 0).
?
is set to 0 because the perfor-mance is terrible if ?
is set to 2.Table 6 shows the mean average precision ofthree methods when the number of expansion termschanges.
Since the number of feedback docu-ments is extremely small, two methods except forOkapi sgt get worse than their initial searches.Okapi man slightly decreases as the number of ex-pansion terms increases.
Contrary, SMART mando not change so much as the number of expansionterms increases.
Table 7 shows another evaluationmeasures with 10 terms expanded.
It is clear thatOkapi sgt outperforms the other two methods.5.4 Comparison with Pseudo FeedbackMethodsWe finally compare our query expansion methodwith the following pseudo feedback methods.Okapi pse : This is a pseudo version of Okapiwhich assumes top 10 documents in the initialsearch as relevant ones as well as TREC-8 set-tings.Table 8: Results of Pseudo Feedback Methods(MAP)5 10 15 20Okapi sgt 0.265 0.273 0.274 0.282Okapi pse 0.253 0.249 0.247 0.246SMART pse 0.236 0.243 0.242 0.242Table 9: Results of Pseudo Feedback Methods (10terms expanded)P10 P30 RPREC MAP R05POkapi sgt 0.520 0.387 0.294 0.273 0.208Okapi pse 0.478 0.369 0.279 0.249 0.206SMART pse 0.466 0.359 0.272 0.243 0.187SMART pse : This is a pseudo version of SMART.It also assumes top 10 documents as relevantones.
In addition, it assumes top 500-1000 doc-uments as non-relevant ones.In TREC-8, above two methods uses TREC1-5 disksfor query expansion and a phase extraction tech-nique.
However we do not adopt these methods inour experiments4.
Since these methods showed thehighest performance in the TREC-8 adhoc task, itis reasonable to compare our method with them ascompetitors.Table 8 shows the mean average precision ofthree methods when the number of expansion termschanges.
Performance does not differ so much if thenumber of expansion terms changes.
Okapi sgt out-performs at any number of expansion.
Table 9 showsthe results in other evaluation measures.
Okapi sgtalso outperforms except for R05P.
In particular, per-formance in P10 is quite well.
It is preferable behav-ior for the use in practical situations.6 DiscussionIn the experiments, the feedback documents forOkapi sgt is top ranked ones.
However some usersdo not select such documents.
They may chooseanother relevant and non-relevant documents whichrank in top 10.
Thus we test an another experimentwhere relevant and non-relevant documents are se-lected randomly from top 10 rank.
Table 10 showsthe result.
Compared with table 2, the performanceseems to become slightly worse.
This shows that a4Thus the performance in our experiments is a bit worse thanthe result of TREC-8969Table 10: Results of Okapi sgt with random feed-back (5 terms expanded)P10 P30 RPREC MAP R05P20 0.498 0.372 0.288 0.265 0.22250 0.456 0.359 0.294 0.268 0.200100 0.452 0.335 0.270 0.246 0.186user should select higher ranked documents for rel-evance feedback.7 ConclusionIn this paper we proposed a novel query expansionmethod which only use the minimum manual judg-ment.
To complement the lack of relevant docu-ments, this method utilizes the SGT transductivelearning algorithm to predict the relevancy of un-judged documents.
Since the performance of SGTmuch depends on an estimation of the fraction ofrelevant documents, we propose a method to sam-ple some good fraction values.
We also propose amethod to laps the predictions of multiple SGT tri-als with above sampled fraction values and try todifferentiate the importance of candidate terms forexpansion in relevant documents.
The experimentalresults showed our method outperforms other queryexpansion methods in the evaluations of several cri-teria.ReferencesI.
J. Aalbersberg.
1992.
Incremental relevance feedback.In Proceedings of SIGIR ?92, pages 11?22.J.
Allan.
1996.
Incremental relevance feedback for infor-mation filtering.
In Proceedings of SIGIR ?96, pages270?278.A.
Blum and et al 2004.
Semi-supervised learning usingrandomized mincuts.
In Proceedings of ICML 2004.S.
Dumais and et al 2003.
Sigir 2003 workshop report:Implicit measures of user interests and preferences.
InSIGIR Forum.G.
W. Flake and et al 2002.
Extracting query modifi-cation from nonlinear svms.
In Proceedings of WWW2002.J.
He and et al 2004.
Manifold-ranking based imageretrieval.
In Proceedings of Multimedia 2004, pages9?13.
ACM.M.
Iwayama.
2000.
Relevance feedback with a smallnumber of relevance judgements: Incremental rele-vance feedback vs. document clustering.
In Proceed-ings of SIGIR 2000, pages 10?16.T.
Joachims.
1999.
Transductive inference for text clas-sification using support vector machines.
In Proceed-ings of ICML ?99.T.
Joachims.
2003.
Transductive learning via spectralgraph partitioning.
In Proceedings of ICML 2003,pages 143?151.A.
M. Lam-Adesina and G. J. F. Jones.
2001.
Applyingsummarization techniques for term selection in rele-vance feedback.
In Proceedings of SIGIR 2001, pages1?9.T.
Onoda, H. Murata, and S. Yamada.
2004.
Non-relevance feedback document retrieva.
In Proceedingsof CIS 2004.
IEEE.S.
Oyama and et al 2001. keysword spices: A newmethod for building domain-specific web search en-gines.
In Proceedings of IJCAI 2001.S.
E. Robertson.
1990.
On term selection for query ex-pansion.
Journal of Documentation, 46(4):359?364.S.
E. Robertson.
1997.
Overview of the okapi projects.Journal of the American Society for Information Sci-ence, 53(1):3?7.I.
Ruthven.
2003.
Re-examining the potential effective-ness of interactive query expansion.
In Proceedings ofSIGIR 2003, pages 213?220.A.
Singhal, S. Abney, B. Bacchiani, M. Collins, D. Hin-dle, and F. Pereira.
1999.
At&t at trec-8.V Vapnik.
1998.
Statistical learning theory.
Wiley.E.
Voorhees and D. Harman.
1999.
Overview of theeighth text retrieval conference.S.
Yu and et al 2003.
Improving pseud-relevance feed-back in web information retrieval using web page seg-mentation.
In Proceedings of WWW 2003.X Zhu and et al 2003.
Semi-supervised learning usinggaussian fields and harmonic functions.
In Proceed-ings of ICML 2003, pages 912?914.970
