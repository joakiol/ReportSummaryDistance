Distributional Similarity Models: Clustering vs .
NearestNeighborsLi l l ian LeeDepar tment  of Computer  ScienceCornell  UniversityIthaca, NY 14853-7501llee@cs, cornell, eduFernando Pere i raA247, AT&T Labs - Research180 Park AvenueF lorham Park, NJ  07932-0971pereira@research, att.
comAbst ractDistributional similarity is a useful notion in es-timating the probabilities of rare joint events.It has been employed both to cluster events ac-cording to their distributions, and to directlycompute averages of estimates for distributionalneighbors of a target event.
Here, we examinethe tradeoffs between model size and predictionaccuracy for cluster-based and nearest neigh-bors distributional models of unseen events.1 I n t roduct ionIn many statistical anguage-processing prob-lems, it is necessary to estimate the joint proba-bility or cooeeurrence probability of events drawnfrom two prescribed sets.
Data sparseness canmake such estimates difficult when the eventsunder consideration are sufficiently fine-grained,for instance, when they correspond to occur-rences of specific words in given configurations.In particular, in many practical modeling tasks,a substantial fraction of the cooccurrences of in-terest have never been seen in training data.
Inmost previous work (Jelinek and Mercer, 1980;Katz, 1987; Church and Gale, 1991; Ney andEssen, 1993), this lack of information is ad-dressed by reserving some mass in the proba-bility model for unseen joint events, and thenassigning that mass to those events as a func-tion of their marginal frequencies.An intuitively appealing alternative to relyingon marginal frequencies alone is to combine es-timates of the probabilities of "similar" events.More specifically, a joint event (x, y) would beconsidered similar to another (x t, y) if the distri-butions of Y given x and Y given x' (the cooc-currence distributions of x and x ') meet an ap-propriate definition of distributional similarity.For example, one can infer that the bigram "af-ter ACL-99" is plausible - -  even if it has never33occurred before - -  from the fact that the bigram"after ACL-95" has occurred, if "ACL-99" and"ACL-95" have similar cooccurrence distribu-tions.For concreteness and experimental evalua-tion, we focus in this paper on a particular typeof cooccurrence, that of a main verb and thehead noun of its direct object in English text.Our main goal is to obtain estimates ~(vln ) ofthe conditional probability of a main verb vgiven a direct object head noun n, which canthen be used in particular prediction tasks.In previous work, we and our co-authors haveproposed two different probability estimationmethods that incorporate word similarity infor-mation: distributional clustering and nearest-neighbors averaging.
Distributional clustering(Pereira et al, 1993) assigns to each word aprobability distribution over clusters to whichit may belong, and characterizes each clusterby a centroid, which is an average of cooccur-rence distributions of words weighted accordingto cluster membership robabilities.
Cooccur-rence probabilities can then be derived from ei-ther a membership-weighted average of the clus-ters to which the words in the cooccurrence be-long, or just from the highest-probability clus-ter.In contrast, nearest-neighbors averaging 1(Dagan et al, 1999) does not explicitly clus-ter words.
Rather, a given cooccurrence prob-ability is estimated by averaging probabilitiesfor the set of cooccurrences most similar to thetarget cooccurrence.
That is, while both meth-ods involve appealing to similar "witnesses" (inthe clustering case, these witnesses are the cen-troids; for nearest-neighbors averaging, they are1In previous papers, we have used the te rm"similar ity-based",  but  this term would cause confusionin the present article.the most similar words), in nearest-neighborsaveraging the witnesses vary for different cooc-currences, whereas in distributional clusteringthe same set of witnesses i used for every cooc-currence (see Figure 1).We thus see that distributional c ustering andnearest-neighbors averaging are complementaryapproaches.
Distributional clustering gener-ally creates a compact representation of thedata, namely, the cluster membership probabil-ity tables and the cluster centroids.
Nearest-neighbors averaging, on the other hand, asso-ciates a specific set of similar words to each wordand thus typically increases the amount of stor-age required.
In a way, it is clustering taken tothe limit - each word forms its own cluster.In previous work, we have shown that bothdistributional clustering and nearest-neighborsaveraging can yield improvements of up to 40%with respect o Katz's (1987) state-of-the-artbackoffmethod in the prediction of unseen cooc-currences.
In the case of nearest-neighbors aver-aging, we have also demonstrated perplexity re-ductions of 20% and statistically significant im-provement inspeech recognition error rate.
Fur-thermore, each method has generated some dis-cussion in the literature (Hofmann et al, 1999;Baker and McCallum, 1998; Ide and Veronis,1998).
Given the relative success of these meth-ods and their complementarity, it is natural towonder how they compare in practice.Several authors (Schiitze, 1993; Dagan et al,1995; Ide and Veronis, 1998) have suggestedthat clustering methods, by reducing data toa small set of representatives, might performless well than nearest-neighbors averaging-typemethods.
For instance, Dagan et al (1995,p.
124) argue:This \[class-based\] approach, which fol-lows long traditions in semantic las-sification, is very appealing, as it at-tempts to capture "typical" propertiesof classes of words.
However .... it isnot clear that word co-occurrence pat-terns can be generalized to class co-occurrence parameters without losingtoo much information.Furthermore, early work on class-based lan-guage models was inconclusive (Brown et al,1992).34In this paper, we present a detailed com-parison of distributional c ustering and nearest-neighbors averaging on several arge datasets,exploring the tradeoff in similarity-based mod-eling between memory usage on the one handand estimation accuracy on the other.
We findthat the performances of the two methods arein general very similar: with respect o Katz'sback-off, they both provide average rror reduc-tions of up to 40% on one task and up to 7%on a related, but somewhat more difficult, task.Only in a fairly unrealistic setting did nearest-neighbors averaging clearly beat distributionalclustering, but even in this case, both meth-ods were able to achieve average rror reduc-tions of at least 18% in comparison to back-off.
Therefore, previous claims that clusteringmethods are necessarily inferior are not stronglysupported by the evidence of these experiments,although it is of course possible that the situa-tion may be different for other tasks.2 Two mode lsWe now survey the distributional clustering(section 2.1) and nearest-neighbors averaging(section 2.2) models.
Section 2.3 examines therelationships between these two methods.2.1 C luster ingThe distributional clustering model that weevaluate in this paper is a refinement ofour ear-lier model (Pereira et al, 1993).
The new modelhas important heoretical advantages over theearlier one and interesting mathematical prop-erties, which will be discussed elsewhere.
Here,we will outline the main motivation for themodel, the iterative quations that implementit, and their practical use in clustering.The model involves two discreterandom vari-ables N (nouns) and V (verbs) whose joint dis-tribution we have sampled, and a new unob-served discrete random variable C representingprobabilistic lusters of elements of N. Therole of the hidden variable C is specified bythe conditional distribution p(cln), which canbe thought of as the probability that n belongsto cluster c. We want to preserve in C as muchas possible of the information that N has aboutV, that is, maximize the mutual information 2I(V, C).
On the other hand, we would also2I( X, Y) = ~-\]~x  P(x, y) log (P(x, y)/P(x)P(y)).6" "" "o o",0 II I I ~' ~ O s / ',, O A O B .
.
_ __ .
.
.
.Figure 1: Difference between clustering and nearest neighbors.
Although A and B belong mostly tothe same cluster (dotted ellipse), the two nearest neighbors to A are not the nearest wo neighborsto B.like to control the degree of compression of Crelative to N, that is, the mutual informationI (C,N).
Furthermore, since C is intended tosummarize N in its role as a predictor of V, itshould carry no information about V that Ndoes not already have.
That is, V should beconditionally independent of C given N, whichallows us to writep(vlc ) = ~-\]p(vln)p(nlc ) .
(1)nThe distribution p(VIc ) is the centroid for clus-ter c.It can be shown that I(V, C) is maximizedsubject to fixed I(C, N) and the above condi-tional independence assumption whenp(c) p(cln ) = ~ exp \[-/3D(p(Yln)\]\]p(Ylc) ) \] , (2)where /3 is the Lagrange multiplier associatedwith fixed I(C, N), Zn is the normalizationZn = y~ p(c) exp \[-/3D(p(Y\[n)llp(Ylc ))\] ,cand D is the KuUback-Leiber (KL) divergence,which measures the distance, in an information-theoretic sense, between two distributions q andr :?
q (v )D(qllr ) = ~ q(v) lOgr(v) .vThe main behavioral difference between thismodel and our previous one is the p(c) factor in(2), which tends to sharpen cluster membershipdistributions.
In addition, our earlier experi-ments used a uniform marginal distribution forthe nouns instead of the marginal distributionin the actual data, in order to make clusteringmore sensitive to informative but relatively rare35nouns.
While neither difference leads to majorchanges in clustering results, we prefer the cur-rent model for its better theoretical foundation.For fixed /3, equations (2) and (1) togetherwith Bayes rule and marginalization can be usedin a provably convergent i erative reestimationprocess for p(glc) ,  p(YlC ) and p(C).
Thesedistributions form the model for the given/3.It is easy to see that for/3 = 0, p(nlc ) does notdepend on the cluster distribution p(VIc), so thenatural number of clusters (distinct values ofC) is one.
At the other extreme, for very large/3 the natural number of clusters is the sameas the number of nouns.
In general, a highervalue of/3 corresponds to a larger number ofclusters.
The natural number of clusters k andthe probabilistic model for different values of/3are estimated as follows.
We specify an increas-ing sequence {/3i} of/3 values (the "annealing"schedule), starting with a very low value/30 andincreasing slowly (in our experiments, /30 = 1and/3i+1 = 1-1/30.
Assuming that the naturalnumber of clusters and model for/3i have beencomputed, we set/3 =/3i+1 and split each clus-ter into two twins by taking small random per-turbations of the original cluster centroids.
Wethen apply the iterative reestimation procedureuntil convergence.
If two twins end up with sig-nificantly different centroids, we conclude thatthey are now separate clusters.
Thus, for eachi we have a number of clusters ki and a modelrelating those clusters to the data variables Nand V.A cluster model can be used to estimatep(vln ) when v and n have not occurred togetherin training.
We consider two heuristic ways ofdoing this estimation:?
all-cluster weighted average:p(vln) = ~-\]p(vlc)p(cln)c?
nearest-cluster stimate:~(vln) -- p(vlc*),where c* maximizes p(c*ln).2.2 Nearest -ne ighbors  averagingAs noted earlier, the nearest-neighbors averag-ing method is an alternative to clustering forestimating the probabilities of unseen cooccur-fences.
Given an unseen pair (n, v), we calcu-late an est imate  15(vln ) as an  appropr ia te  aver-age of p(vln I) where  n I is distributionally s im-ilar to n. Many  distributional similarity mea-sures can  be  cons idered (Lee, 1999).
In thispaper,  we  focus on  the one  that gave  the bestresults in our  earlier work  (Dagan et al, 1999),the Jensen-Shannon divergence (Rao, 1982; Lin,1991).
The Jensen-Shannon divergence of twodiscrete distributions p and q over the same do-main is defined as1 gS(p, q) = ~It is easy to see that JS(p, q) is always defined.In previous work, we used the estimate~5(vln ) = 1 ~ p(vln,)exp(_Zj(n,n,)),(In nlES(n,k)where J(n,n') = JS (p(VIn),p(Yln')), Z andk are tunable parameters, S(n, k) is the set ofk nouns with the smallest Jensen-Shannon di-vergence to n, and an is a normalization term.However, in the present work we use the simplerunweighted average1/~(vln) = -~ ~ p(vln'), (3)n'ES(n,k)and examine the effect of the choice of k onmodeling performance.
By eliminating extraparameters, this restricted formulation allows amore direct comparison of nearest-neighbors av-eraging to distributional c ustering, as discussedin the next section.
Furthermore, our earlierexperiments showed that an exponentially de-creasing weight has much the same effect on per-formance as a bound on the number of nearestneighbors participating in the estimate.2.3 D iscuss ionIn the previous two sections, we presentedtwo complementary paradigms for incorporat-ing distributional similarity information intocooccurrence probability estimates.
Now, onecannot always draw conclusions about the rel-ative fitness of two methods imply from head-to-head performance comparisons; for instance,one method might actually make use of inher-ently more informative statistics but produceworse results because the authors chose a sub-optimal weighting scheme.
In the present case,however, we are working with two models which,while representing opposite xtremes in terms ofgeneralization, share enough features to makethe comparison meaningful.First, both models use linear combinationsof cooccurrence probabilities for similar enti-ties.
Second, each has a single free param-eter k, and the two k's enjoy a natural in-verse correspondence: a large number of clus-ters in the distributional c ustering case resultsin only the closest centroids contributing sig-nificantly to the cooccurrence probability esti-mate, whereas a large number of neighbors inthe nearest-neighbors averaging case means thatrelatively distant words are consulted.
And fi-nally, the two distance functions are similar inspirit: both are based on the KL divergence tosome type of averaged istribution.
We havethus attempted to eliminate functional form,number and type of parameters, and choice ofdistance function from playing a role in the com-parison, increasing our confidence that we aretruly comparing paradigms and not implemen-tation details.What are the fundamental differences be-tween the two methods?
From the foregoingdiscussion it is clear that distributional clus-tering is theoretically more satisfying and de-pends on a single model complexity parameter.On the other hand, nearest-neighbors averagingin its most general form offers more flexibilityin defining the set of most similar words andtheir relative weights (Dagan et al, 1999).
Also,the training phase requires little computation,as opposed to the iterative re-estimation proce-dure employed to build the cluster model.
Butthe key difference is the amount of data com-pression, or equivalently the amount of general-ization, produced by the two models.
Cluster-3{}ing yields a far more  compact  representation ofthe data when k, the model  size parameter, issmaller than INf.
As  noted above, various au-thors have conjectured that this data reductionmust  inevitably result in lower performance incomparison to nearest-neighbor methods, whichstore the most specific information for each in-dividual word.
Our  experiments a im to ex-plore this hypothesized generalization-accuracytradeoff.3 Eva luat ion3.1 Methodo logyWe compared the two similarity-based esti-mation techniques at the following decisiontask, which evaluates their ability to choosethe more likely of two unseen cooccurrences.Test instances consist of noun-verb-verb triples(n, vl, v2), where both (n, Vl) and (n, v2) are un-seen cooccurrences, but (n, vl) is more likely(how this is determined is discussed below).
Foreach test instance, the language model prob-abilities 151 dej 15(vlln) and i52 dej 15(v2\]n) arecomputed; the result of the test is either cor-rect (151 > 152), incorrect (/51 < ~52,) or a tie(151 = 152).
Overall performance is measured bythe error rate on the entire test set, defined as1~(# of incorrect choices + (# of t ies)/2),where T is the number of test triples, not count-ing multiplicities.Our global experimental design was to runten-fold cross-validation experiments comparingdistributional clustering, nearest-neighbors av-eraging, and Katz's backoff (the baseline) on thedecision task just outlined.
All results we reportbelow are averages over the ten train-test splits.For each split, test triples were created from theheld-out est set.
Each model used the trainingset to calculate all basic quantities (e.g., p(vln )for each verb and noun), but not to train k.Then, the performance of each similarity-basedmodel was evaluated on the test triples for asequence of settings for k.We expected that clustering performancewith respect to the baseline would initially im-prove and then decline.
That  is, we conjec-tured that the model would overgeneralize atsmall k but overfit the training data at largek.
In contrast, for nearest-neighbors averag-ing, we hypothesized monotonical ly decreasingperformance curves: using only the very mostsimilar words would yield high performance,whereas including more distant, uninformativewords would result in lower accuracy.
From pre-vious experience, we believed that both meth-ods would do well with respect o backoff.3.2 DataIn order to implement the experimentalmethodology just described, we employed thefollow data preparation method:i.
Gather verb-object pairs using the CASSpartial parser (Abney, 1996)Partition set of pairs into ten folds .3.
For each test fold,(a) discard seen pairs and duplicates(b) discard pairs with unseen nouns or un-seen verbs(e) for each remaining (n, vl), create(n, vl, v2) such that (n, v~) is less likelyStep 3b is necessary because neither thesimilarity-based methods nor backoff handlenovel unigrams gracefully.We instantiated this schema in three ways:AP89 We retrieved 1,577,582 verb-objectpairs from 1989 Associated Press (AP)newswire, discarding singletons (pairs occurringonly once) as is commonly done in languagemodeling.
We split this set by type 3, whichdoes not realistically model how new data oc-curs in real life, but does conveniently guaran-tee that the entire test set is unseen.
In step3c all (n, v2) were found such that (n, vl) oc-curred at least twice as often as (n, v2) in thetest fold; this gives reasonable reassurance thatn is indeed more likely to cooccur with Vl, eventhough (n, v2) is plausible (since it did in factoccur) .3When a corpus is split by type, all instances of agiven type must end up in the same partition.
If thesplit is by token, then instances of the same type mayend up in different partitions.
For example, for corpus'% b a c' ,  "a b" +"a c" is a valid split by token, but notby type.37Test typeAP89AP90unseenAP90fakesplit singletons?
~ training % of test ~ test baselinepairs unseen triples errortype no 1033870 100 42795 28.3%token yes 1123686 14 4019 39.6%" " " " 14479 79.9%Table 1: Data for the three types of experiments.
All numbers are averages over the ten splits.AP90unseen 1,483,728 pairs were extractedfrom 1990 AP newswire and split by token.
Al-though splitting by token is undoubtedly a bet-ter way to generate train-test splits than split-ting by type, it had the unfortunate side effectof diminishing the average percentage ofunseencooccurrences in the test sets to 14%.
Whilethis is still a substantial fraction of the data(demonstrat ing the seriousness of the sparsedata problem), it caused difficulties in creat-ing test triples: after applying filtering step 3b,there were relatively few candidate nouns  andverbs satisfying the fairly stringent condition 3c.Therefore, singletons were retained in the AP90data.
Step 3c was  carried out as for AP89.AP90fake  The  procedure for creating theAP90unseen data resulted in much smaller testsets than in the AP89 case (see Table I).
Togenerate larger test sets, we  used the same foldsas in AP90unseen,  but imp lemented  step 3c dif-ferently.
Instead of selecting v2 f rom cooccur-rences (n, v2) in the held-out set, test tripleswere constructed using v2 that never cooccurredwith n in either the training or the test data.That  is, each test triple represented a choicebetween a plausible cooccurrence (n, Vl) and  animplausible ("fake") cooccurrence (n, v2).
Toensure a large differential between the two al-ternatives, we  further restricted (n, Vl) to occurat least twice (in the test fold).
We also chose v2f rom the set of 50 most  frequent verbs, resultingin much higher error rates for backoff.3.3  Resu l tsWe now present evaluation results ordered byrelative difficulty of the decision task.Figure 2 shows the performance of distribu-tional clustering and nearest-neighbors averag-ing on the AP90fake data (in all plots, error barsrepresent one standard eviation).
Recall thatthe task here was to distinguish between plau-sible and implausible cooccurrences, making it38a somewhat easier problem than that posed inthe AP89 and AP90unseen experiments.
Bothsimilarity-based methods improved on the base-line error (which, by construction of the testtriples, was guaranteed to be high) by as muchas 40%.
Also, the curves have the shapes pre-dicted in section 3.1.all clu'stersnearest cluster5'0 ,~0 ,~0 2~0 2;0 ~0 g0 ,~kFigure 2: Average rror reduction with respectto backoff on AP90fake test sets.We next examine our AP89 experiment re-sults, shown in Figure 3.
The similarity-basedmethods clearly outperform backoff, with thebest error reductions occurring at small k forboth types of models.
Nearest-neighbors aver-aging appears to have the advantage over dis-tributional clustering, and the nearest clustermethod yields lower error rates than the aver-aged cluster method (the differences are statisti-cally significant according to the paired t-test).We might hypothesize that nearest-neighborsaveraging is better in situations of extreme spar-sity of data.
However, these results must betaken with some caution given their unrealistictype-based train-test split.A striking feature of Figure 3 is that all thecurves have the same shape, which is not at allwhat we predicted in section 3.1.
The reason\]10all clustersnearest clusternearest neighbors25o , , , , , , 5 100 150 200 250 300 350 400kFigure 3: Average error reduction with respectto backoff on AP89 test sets.0.260.260.240.230.220.210.20.1~that the very most similar words are appar-ently not as informative as slightly more dis-tant words is due to recall errors.
Observe thatif (n, vl) and (n, v2) are unseen in the train-ing data, and if word n' has very small Jensen-Shannon divergence to n, then chances are thatn ~ also does not occur with either Vl or v2, re-sulting in an estimate of zero probability forboth test cooccurrences.
Figure 4 proves thatthis is the case: if zero-ties are ignored, then theerror rate curve for nearest-neighbors averaginghas the expected shape.
Of course, clustering isnot prone to this problem because it automati-cally smoothes its probability estimates.average error over APe9, normal vs. precision resultsnearest neighborsnearest neighbors.
Ignodng recall errors ' 0  ' ' ' ' ' ' 100 150 200 250 300 350 400kFigure 4: Average error (not error reduction)using nearest-neighbors averaging on AP89,showing the effect of ignoring recall mistakes.Finally, Figure 5 presents the results of39our AP90unseen experiments.
Again, the useof similarity information provides better-than-baseline performance, but, due to the relativedifficulty of the decision task in these exper-iments (indicated by the higher baseline er-ror rate with respect o AP89), the maximumaverage improvements are in the 6-8% range.The error rate reductions posted by weighted-average clustering, nearest-centroid clustering,and nearest-neighbors averaging are all wellwithin the standard eviations of each other.Iall clustersnearest clusternearest neighbors-20 50 100 150 200 250 300 350 400kFigure 5: Average error reduction with respectto backoff on AP90unseen test sets.
As in theAP89 case, the nonmonotonicity of the nearest-neighbors averaging curve is due to recall errors.4 Conc lus ionIn our experiments, the performances of distri-butional clustering and nearest-neighbors aver-aging proved to be in general very similar: onlyin the unorthodox AP89 setting did nearest-neighbors averaging clearly yield better errorrates.
Overall, both methods achieved peak per-formances at relatively small values of k, whichis gratifying from a computational point of view.Some questions remain.
We observe thatdistributional clustering seems to suffer highervariance.
It is not clear whether this is dueto poor estimates of the KL divergence to cen-troids, and thus cluster membership, for rarenouns, or to noise sensitivity in the search forcluster splits.
Also, weighted-average clusteringnever seems to outperform the nearest-centroidmethod, suggesting that the advantages ofprob-abilistic clustering over "hard" clustering maybe computational rather than in modeling el-fectiveness (Boolean clustering is NP-complete(Brucker, 1978)).
Last but not least, we do notyet have a principled explanation for the similarperformance of nearest-neighbors averaging anddistributional clustering.
Further experiments,especially in other tasks such as language mod-eling, might help tease apart the two methodsor better understand the reasons for their simi-larity.5 AcknowledgementsWe thank the anonymous reviewers for theirhelpful comments and Steve Abney for helpwith extracting verb-object pairs with his parserCASS.ReferencesSteven Abney.
1996.
Partial parsing via finite-statecascades.
In Proceedings of the ESSLLI '96 Ro-bust 15arsing Workshop.L.
Douglas Baker and Andrew Kachites McCallum.1998.
Distributional clustering of words for textclassification.
In Plst Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval (SIGIR '98), pages 96-103.Peter F. Brown, Vincent J. DellaPietra, Peter V.deSouza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural an-guage.
Computational Linguistics, 18(4):467-479,December.Peter Brucker.
1978.
On the complexity of clus-tering problems.
In Rudolf Henn, Bernhard H.Korte, and Werner Oettli, editors, Optimizationand Operations Research, number 157 in LectureNotes in Economics and Mathematical Systems.Springer-Verlag, Berlin.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilities of English bigrams.
Computer Speech andLanguage, 5:19-54.Ido Dagan, Shaul Marcus, and Shaul Markovitch.1995.
Contextual word similarity and estimationfrom sparse data.
Computer Speech and Lan-guage, 9:123-152.Ido Dagan, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34(1-3):43-69.Thomas Hofmann, Jan Puzicha, and Michael I. Jor-dan.
1999.
Learning from dyadic data.
In Ad-vances in Neural Information Processing Systems11.
MIT Press.
To appear.Nancy Ide and Jean Veronis.
1998.
Introduction tothe special issue on word sense disambiguation:40The state of the art.
Computational Linguistics,24(1):1-40, March.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parametersfrom sparse data.
In Proceedings of the Workshopon Pattern Recognition in Practice, Amsterdam,May.
North Holland.Slava M. Katz.
1987.
Estimation of probabilitiesfrom sparse data for the language model com-ponent of a speech recognizer.
IEEE Transac-tions on Acoustics, Speech and Signal Processing,ASSP-35(3):400-401, March.Lillian Lee.
1999.
Measures of distributional simi-larity.
In 37th Annual Meeting of the ACL, Som-erset, New Jersey.
Distributed by Morgan Kauf-mann, San Francisco.Jianhua Lin.
1991.
Divergence measures based onthe Shannon entropy.
IEEE Transactions on In-formation Theory, 37(1):145-151.Hermann Ney and Ute Essen.
1993.
Estimating'small' probabilities by leaving-one-out.
In ThirdEuropean Conference On Speech Communicationand Technology, pages 2239-2242, Berlin, Ger-many.Fernando C. N. Pereira, Naftali Tishby, and LillianLee.
1993.
Distributional clustering of Englishwords.
In 31st Annual Meeting of the ACL, pages183-190, Somerset, New Jersey.
Association forComputational Linguistics.
Distributed by Mor-gan Kaufmann, San Francisco.C.
Radhakrishna Rao.
1982.
Diversity: Its measure-ment, decomposition, apportionment and analy-sis.
SankyhS: The Indian Journal of Statistics,44(A):1-22.Hinrich Schiitze.
1993.
Word space.
In S. J. Hanson,J.
D. Cowan, and C. L. Giles, editors, Advances inNeural Information Processing Systems 5, pages895-902.
Morgan Kaufmann, San Francisco.
