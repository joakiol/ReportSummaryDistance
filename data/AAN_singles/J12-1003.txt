Learning Entailment Relations by GlobalGraph Structure OptimizationJonathan Berant?Tel Aviv UniversityIdo Dagan?
?Bar-Ilan UniversityJacob Goldberger?Bar-Ilan UniversityIdentifying entailment relations between predicates is an important part of applied semanticinference.
In this article we propose a global inference algorithm that learns such entailmentrules.
First, we define a graph structure over predicates that represents entailment relations asdirected edges.
Then, we use a global transitivity constraint on the graph to learn the optimal setof edges, formulating the optimization problem as an Integer Linear Program.
The algorithm isapplied in a setting where, given a target concept, the algorithm learns on the fly all entailmentrules between predicates that co-occur with this concept.
Results show that our global algorithmimproves performance over baseline algorithms by more than 10%.1.
IntroductionThe Textual Entailment (TE) paradigm is a generic framework for applied semanticinference.
The objective of TE is to recognize whether a target textual meaning canbe inferred from another given text.
For example, a question answering system hasto recognize that alcohol affects blood pressure is inferred from the text alcohol reducesblood pressure to answer the question What affects blood pressure?
In the TE framework,entailment is defined as a directional relationship between pairs of text expressions,denoted by T, the entailing text, and H, the entailed hypothesis.
The text T is said toentail the hypothesis H if, typically, a human reading T would infer that H is most likelytrue (Dagan et al 2009).TE systems require extensive knowledge of entailment patterns, often captured asentailment rules?rules that specify a directional inference relation between two textfragments (when the rule is bidirectional this is known as paraphrasing).
A commontype of text fragment is a proposition, which is a simple natural language expressionthat contains a predicate and arguments (such as alcohol affects blood pressure), wherethe predicate denotes some semantic relation between the concepts that are expressed?
Tel-Aviv University, P.O.
Box 39040, Tel-Aviv, 69978, Israel.
E-mail: jonatha6@post.tau.ac.il.??
Bar-Ilan University, Ramat-Gan, 52900, Israel.
E-mail: dagan@cs.biu.ac.il.?
Bar-Ilan University, Ramat-Gan, 52900, Israel.
E-mail: goldbej@eng.biu.ac.il.Submission received: 28 September 2010; revised submission received: 5 May 2011; accepted for publication:5 July 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 1by the arguments.
One important type of entailment rule specifies entailment betweenpropositional templates, that is, propositions where the arguments are possibly re-placed by variables.
A rule corresponding to the aforementioned example may be Xreduce blood pressure ?
X affect blood pressure.
Because facts and knowledge are mostlyexpressed by propositions, such entailment rules are central to the TE task.
This hasled to active research on broad-scale acquisition of entailment rules for predicates(Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009;Schoenmackers et al 2010).Previous work has focused on learning each entailment rule in isolation.
It is clear,however, that there are interactions between rules.
A prominent phenomenon is thatentailment is inherently a transitive relation, and thus the rules X ?
Y and Y ?
Z implythe rule X ?
Z.1 In this article we take advantage of these global interactions to improveentailment rule learning.After reviewing relevant background (Section 2), we describe a structure termedan entailment graph that models entailment relations between propositional templates(Section 3).
Next, we motivate and discuss a specific type of entailment graph, termed afocused entailment graph, where a target concept instantiates one of the arguments ofall propositional templates.
For example, a focused entailment graph about the targetconcept nausea might specify the entailment relations between propositional templateslike X induce nausea, X prevent nausea, and nausea is a symptom of X.In the core section of the article, we present an algorithm that uses a global approachto learn the entailment relations, which comprise the edges of focused entailmentgraphs (Section 4).
We define a global objective function and look for the graph thatmaximizes that function given scores provided by a local entailment classifier and aglobal transitivity constraint.
The optimization problem is formulated as an IntegerLinear Program (ILP) and is solved with an ILP solver, which leads to an optimalsolution with respect to the global function.
In Section 5 we demonstrate that thisalgorithm outperforms by 12?13% methods that utilize only local information as wellas methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006)rather than an ILP solver.The article also includes a comprehensive investigation of the algorithm and itscomponents.
First, we perform manual comparison between our algorithm and thebaselines and analyze the reasons for the improvement in performance (Sections 5.3.1and 5.3.2).
Then, we analyze the errors made by the algorithm against manually pre-pared gold-standard graphs and compare them to the baselines (Section 5.4).
Last, weperform a series of experiments in which we investigate the local entailment classifierand specifically experiment with various sets of features (Section 6).
We conclude andsuggest future research directions in Section 7.This article is based on previous work (Berant, Dagan, and Goldberger 2010), whilesubstantially expanding upon it.
From a theoretical point of view, we reformulate thetwo ILPs previously introduced by incorporating a prior.
We show a theoretical relationbetween the two ILPs and prove that the optimization problem tackled is NP-hard.From an empirical point of view, we conduct many new experiments that examineboth the local entailment classifier as well as the global algorithm.
Last, a rigorousanalysis of the algorithm is performed and an extensive survey of previous work isprovided.1 Assuming that Y has the same sense in both X ?
Y and Y ?
Z, as we discuss later in Section 3.74Berant et al Learning Entailment Relations by Global Graph Structure Optimization2.
BackgroundIn this section we survey methods proposed in past literature for learning entailmentrules between predicates.
First, we discuss local methods that assess entailment given apair of predicates, and then global methods that perform inference over a larger set ofpredicates.2.1 Local LearningThree types of information have primarily been utilized in the past to learn entailmentrules between predicates: lexicographic methods, distributional similarity methods, andpattern-based methods.Lexicographic methods use manually prepared knowledge bases that contain in-formation about semantic relations between lexical items.
WordNet (Fellbaum 1998b),by far the most widely used resource, specifies relations such as hyponymy, synonymy,derivation, and entailment that can be used for semantic inference (Budanitsky andHirst 2006).
For example, if WordNet specifies that reduce is a hyponym of affect, thenone can infer that X reduces Y ?
X affects Y. WordNet has also been exploited toautomatically generate a training set for a hyponym classifier (Snow, Jurafsky, and Ng2004), and we make a similar use of WordNet in Section 4.1.A drawback of WordNet is that it specifies semantic relations for words and termsbut not for more complex expressions.
For example, WordNet does not cover a complexpredicate such as X causes a reduction in Y.
Another drawback of WordNet is that it onlysupplies semantic relations between lexical items, but does not provide any informationon how to map arguments of predicates.
For example, WordNet specifies that there isan entailment relation between the predicates pay and buy, but does not describe theway in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus,using WordNet directly to derive entailment rules between predicates is possible onlyfor semantic relations such as hyponymy and synonymy, where arguments typicallypreserve their syntactic positions on both sides of the rule.Some knowledge bases try to overcome this difficulty: Nomlex (Macleod et al1998) is a dictionary that provides the mapping of arguments between verbs and theirnominalizations and has been utilized to derive predicative entailment rules (Meyerset al 2004; Szpektor and Dagan 2009).
FrameNet (Baker, Fillmore, and Lowe 1998) isa lexicographic resource that is arranged around ?frames?
: Each frame corresponds toan event and includes information on the predicates and arguments relevant for thatspecific event supplemented with annotated examples that specify argument positions.Consequently, FrameNet was also used to derive entailment rules between predicates(Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010).
Additional man-ually constructed resources for predicates include PropBank (Kingsbury, Palmer, andMarcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000).Distributional similarity methods are used to learn broad-scale resources, becauselexicographic resources tend to have limited coverage.
Distributional similarity algo-rithms employ ?the distributional hypothesis?
(Harris 1954) and predict a semanticrelation between two predicates by comparing the arguments with which they occur.Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al 2004;Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009;Schoenmackers et al 2010), which differ in terms of the specifics of the ways in whichpredicates are represented, the features that are extracted, and the function used to com-pute feature vector similarity.
Next, we elaborate on some of the prominent methods.75Computational Linguistics Volume 38, Number 1Lin and Pantel (2001) proposed an algorithm that is based on a mutual informationcriterion.
A predicate is represented by a binary template, which is a dependency pathbetween two arguments of a predicate where the arguments are replaced by variables.Note that in a dependency tree, a path between two arguments must pass through theircommon predicate.
Also note that if a predicate has more than two arguments, then itis represented by more than one binary template, where each template corresponds toa different aspect of the predicate.
For example, the proposition I bought a gift for hercontains a predicate and three arguments, and therefore is represented by the followingthree binary templates: Xsubj???
buysobj??
Y, Xobj??
buysprep???
forpcomp?n??????
Y and Xsubj???
buysprep???
forpcomp?n??????
Y.For each binary template Lin and Pantel compute two sets of features Fx and Fy,which are the words that instantiate the arguments X and Y, respectively, in a largecorpus.
Given a template t and its feature set for the X variable Ftx, every fx ?
Ftx isweighted by the pointwise mutual information between the template and the feature:wtx( fx) = logPr( fx|t)Pr( fx ), where the probabilities are computed using maximum likelihoodover the corpus.
Given two templates u and v, the Lin measure (Lin 1998a) is computedfor the variable X in the following manner:Linx(u, v) =?f?Fux?Fvx[wux ( f ) + wvx( f )]?f?Fuxwux ( f ) +?f?Fvxwvx( f )(1)The measure is computed analogously for the variable Y and the final distributionalsimilarity score, termed DIRT, is the geometric average of the scores for the twovariables:DIRT(u, v) =?Linx(u, v) ?
Liny(u, v) (2)If DIRT(u, v) is high, this means that the templates u and v share many ?informative?arguments and so it is possible that u ?
v. Note, however, that the DIRT similaritymeasure computes a symmetric score, which is appropriate for modeling synonymybut not entailment, an inherently directional relation.To remedy that, Szpektor and Dagan (2008) suggested a directional distributionalsimilarity measure.
In their work, Szpektor and Dagan chose to represent predicateswith unary templates, which are identical to binary templates, only they contain a pred-icate and a single argument, such as: Xsubj???
buys.
Szpektor and Dagan explain that unarytemplates are more expressive than binary templates, and that some predicates can onlybe encoded using unary templates.
They propose that if for two unary templates u ?
v,then relatively many of the features of u should be covered by the features of v. Thisis captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (weomit the subscript x from Fux and Fvx because in their setting there is only one argument):Cover(u, v) =?f?Fu?Fv wu( f )?f?Fu wu( f )(3)The final directional score, termed BInc (Balanced Inclusion), is the geometric averageof the Lin measure and the Cover measure:BInc(u, v) =?Lin(u, v) ?
Cover(u, v) (4)76Berant et al Learning Entailment Relations by Global Graph Structure OptimizationBoth Lin and Pantel as well as Szpektor and Dagan compute a similarity score foreach argument separately, effectively decoupling the arguments from one another.
It isclear, however, that although this alleviates sparsity problems, it disregards an impor-tant piece of information, namely, the co-occurrence of arguments.
For example, if onelooks at the following propositions: coffee increases blood pressure, coffee decreases fatigue,wine decreases blood pressure, wine increases fatigue, one can notice that the predicates occurwith similar arguments and might mistakenly infer that decrease ?
increase.
However,looking at pairs of arguments reveals that the predicates do not share a single pair ofarguments.Yates and Etzioni (2009) address this issue and propose a generative model thatestimates the probability that two predicates are synonymous (synonymy is simplybidirectional entailment) by comparing pairs of arguments.
They represent predicatesand arguments as strings and compute for every predicate a feature vector that countsthat number of times it occurs with any ordered pair of words as arguments.
Their mainmodeling decision is to assume that two predicates are synonymous if the number ofpairs of arguments they share is maximal.
An earlier work by Szpektor et al (2004)also tried to learn entailment rules between predicates by using pairs of arguments asfeatures.
They utilized an algorithm that learns new rules by searching for distributionalsimilarity information on the Web for candidate predicates.Pattern-based methods.
Although distributional similarity measures excel at iden-tifying the existence of semantic similarity between predicates, they are often unableto discern the exact type of semantic similarity and specifically determine whether it isentailment.
Pattern-based methods are used to automatically extract pairs of predicatesfor a specific semantic relation.
Pattern-based methods identify a semantic relationbetween two predicates by observing that they co-occur in specific patterns in sentences.For example, from the single proposition He scared and even startled me one might inferthat startle is semantically stronger than scare and thus startle ?
scare.
Chklovski andPantel (2004) manually constructed a few dozen patterns and learned semantic relationsbetween predicates by looking for these patterns on the Web.
For example, the patternX and even Y implies that Y is stronger than X, and the pattern to X and then Y indicatesthat Y follows X.
The main disadvantage of pattern-based methods is that they are basedon the co-occurrence of two predicates in a single sentence in a specific pattern.
Theseevents are quite rare and require working on a very large corpus, or preferably, the Web.Pattern-based methods were mainly utilized so far to extract semantic relationsbetween nouns, and there has been some work on automatically learning patterns fornouns (Snow, Jurafsky, and Ng 2004).
Although these methods can be expanded forpredicates, we are unaware of any attempt to automatically learn patterns that describesemantic relations between predicates (as opposed to the manually constructed patternssuggested by Chklovski and Pantel [2004]).2.2 Global LearningIt is natural to describe entailment relations between predicates (or language expres-sions in general) by a graph.
Nodes represent predicates, and edges represent entail-ment between nodes.
Nevertheless, using a graph for global learning of all entailmentrelations within a set of predicates, rather then between pairs of predicates, has attractedlittle attention.
Recently, Szpektor and Dagan (2009) presented the resource Argument-mapped WordNet, providing entailment relations for predicates in WordNet.
This re-source was built on top of WordNet and augments it with mapping of arguments forpredicates using NomLex (Macleod et al 1998) and a corpus-based resource (Szpektor77Computational Linguistics Volume 38, Number 1and Dagan 2008).
Their resource makes simple use of WordNet?s global graph structure:New rules are suggested by transitively chaining graph edges, and then verified usingdistributional similarity measures.
Effectively, this is equivalent to using the intersectionof the set of rules derived by this transitive chaining and the set of rules in a distribu-tional similarity knowledge base.The most similar work to ours is Snow, Jurafsky, and Ng?s (2006) algorithm fortaxonomy induction, although it involves learning the hyponymy relation betweennouns, which is a special case of entailment, rather than learning entailment betweenpredicates.
We provide here a brief review of a simplified form of this algorithm.Snow, Jurafsky, and Ng define a taxonomy T to be a set of pairs of words, expressingthe hyponymy relation between them.
The notation Huv ?
T means that the noun u is ahyponym of the noun v in T. They define D to be the set of observed data over all pairs ofwords, and define Duv ?
D to be the observed evidence we have in the data for the eventHuv ?
T. Snow, Jurafsky, and Ng assume a model exists for inferring P(Huv ?
T|Duv):the posterior probability of the event Huv ?
T, given the data.
Their goal is to find thetaxonomy that maximizes the likelihood of the data, that is, to findT?
= argmaxTP(D|T) (5)Using some independence assumptions and Bayes rule, the likelihood P(D|T) isexpressed:P(D|T) =?Huv?TP(Huv ?
T|Duv)P(Duv)P(Huv ?
T)?
?Huv/?TP(Huv /?
T|Duv)P(Duv)P(Huv /?
T)(6)Crucially, they demand that the taxonomy learned respects the constraint that hy-ponymy is a transitive relation.
To ensure that, they propose the following greedyalgorithm: At each step they go over all pairs of words (u, v) that are not in the taxonomy,and try to add the single hyponymy relation Huv.
Then, they calculate the set of relationsSuv that Huv will add to the taxonomy due to the transitivity constraint (all of therelations Huw, where w is a hypernym of v in the taxonomy).
Last, they choose toadd that set of relations Suv that maximizes P(D|T) out of all the possible candidates.This iterative process stops when P(D|T) starts dropping.
Their implementation of thealgorithm uses a hyponym classifier presented in an earlier work (Snow, Jurafsky, andNg 2004) as a model for P(Huv ?
T|Duv) and a single sparsity parameter k =P(Huv/?T)P(Huv?T).
Inthis article we tackle a similar problem of learning a transitive relation, but we use linearprogramming (Vanderbei 2008) to solve the optimization problem.2.3 Linear ProgrammingA Linear Program (LP) is an optimization problem where a linear objective function isminimized (or maximized) under linear constraints.minx?Rdcx (7)such that Ax ?
bwhere c ?
Rd is a coefficient vector, and A ?
Rn ?
Rd and b ?
Rn specify the constraints.In short, we wish to find the optimal assignment for the d variables in the vector x, such78Berant et al Learning Entailment Relations by Global Graph Structure Optimizationthat all n linear constraints specified by the matrix A and the vector b are satisfied by thisassignment.
If the variables are forced to be integers, the problem is termed an IntegerLinear Program (ILP).
ILP has attracted considerable attention recently in severalfields of NLP, such as semantic role labeling, summarization, and parsing (Althaus,Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke andLapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009).
In this article weformulate the entailment graph learning problem as an ILP, which leads to an optimalsolution with respect to the objective function (vs. a greedy optimization algorithmsuggested by Snow, Jurafsky, and Ng [2006]).
Recently, Do and Roth (2010) used ILPin a related task of learning taxonomic relations between nouns, utilizing constraintsbetween sibling nodes and ancestor?child nodes in small graphs of three nodes.3.
Entailment GraphIn this section we define a structure termed the entailment graph that describes theentailment relations between propositional templates (Section 3.1), and a specific typeof entailment graph, termed the focused entailment graph, that concentrates on entail-ment relations that are relevant for some pre-defined target concept (Section 3.2).3.1 Entailment Graph: Definition and PropertiesThe nodes of an entailment graph are propositional templates.
A propositional tem-plate is a binary template2 where at least one of the two arguments is a variable whereasthe second may be instantiated.
In addition, the sense of the predicate is specified (ac-cording to some sense inventory, such as WordNet) and so each sense of a polysemouspredicate corresponds to a separate template (and a separate graph node).
For example,Xsubj???
treats#1obj??
Y and Xsubj???
treats#2obj??
nausea are propositional templates for thefirst and second sense of the predicate treat, respectively.
An edge (u, v) represents thefact that template u entails template v. Note that the entailment relation transcendshyponymy/troponomy.
For example, the template X is diagnosed with asthma entails thetemplate X suffers from asthma, although one is not a hyponym of the other.
An exampleof an entailment graph is given in Figure 1.Because entailment is a transitive relation, an entailment graph is transitive, that is,if the edges (u, v) and (v, w) are in the graph, so is the edge (u, w).
Note that the propertyof transitivity does not hold when the senses of the predicates are not specified.
Forexample, X buys Y ?
X acquires Y and X acquires Y ?
X learns Y, but X buys Y X learnsY.
This violation occurs because the predicate acquire has two distinct senses in the twotemplates, but this distinction is lost when senses are not specified.Transitivity implies that in each strongly connected component3 of the graph allnodes entail each other.
For example, in Figure 1 the nodes X-related-to-nausea and X-associated-with-nausea form a strongly connected component.
Moreover, if we mergeevery strongly connected component to a single node, the graph becomes a DirectedAcyclic Graph (DAG), and a hierarchy of predicates can be obtained.2 We restrict our discussion to templates with two arguments, but generalization is straightforward.3 A strongly connected component is a subset of nodes in the graph where there is a path from anynode to any other node.79Computational Linguistics Volume 38, Number 1Figure 1A focused entailment graph.
For clarity, edges that can be inferred by transitivity are omitted.The single strongly connected component is surrounded by a dashed line.3.2 Focused Entailment GraphsIn this article we concentrate on learning a type of entailment graph, termed the focusedentailment graph.
Given a target concept, such as nausea, a focused entailment graphdescribes the entailment relations between propositional templates for which the targetconcept is one of the arguments (see Figure 1).
Learning such entailment rules in realtime for a target concept is useful in scenarios such as information retrieval and questionanswering, where a user specifies a query about the target concept.
The need for suchrules has been also motivated by Clark et al (2007), who investigated what typesof knowledge are needed to identify entailment in the context of the RTE challenge,and found that often rules that are specific to a certain concept are required.
Anotherexample for a semantic inference algorithm that is utilized in real time is provided byDo and Roth (2010), who recently described a system that, given two terms, determinesthe taxonomic relation between them on the fly.
Last, we have recently suggested anapplication that uses focused entailment graphs to present information about a targetconcept according to a hierarchy of entailment (Berant, Dagan, and Goldberger 2010).The benefit of learning focused entailment graphs is three-fold.
First, the targetconcept that instantiates the propositional template usually disambiguates the predicateand hence the problem of predicate ambiguity is greatly reduced.
Thus, we do notemploy any form of disambiguation in this article, but assume that every node in afocused entailment graph has a single sense (we further discuss this assumption whendescribing the experimental setting in Section 5.1), which allows us to utilize transitivityconstraints.An additional (albeit rare) reason that might also cause violations of transitivityconstraints is the notion of probabilistic entailment.
Whereas troponomy rules(Fellbaum 1998a) such as X walks ?
X moves can be perceived as being almost alwayscorrect, rules such as X coughs ?
X is sick might only be true with some probability.Consequently, chaining a few probabilistic rules such as A ?
B, B ?
C, and C ?
Dmight not guarantee the correctness of A ?
D. Because in focused entailment graphsthe number of nodes and diameter4 are quite small (for example, in the data set we4 The distance between two nodes in a graph is the number of edges in a shortest path connecting them.The diameter of a graph is the maximal distance between any two nodes in the graph.80Berant et al Learning Entailment Relations by Global Graph Structure Optimizationpresent in Section 5 the maximal number of nodes is 26, the average number of nodesis 22.04, the maximal diameter is 5, and the average diameter is 2.44), we do not findthis to be a problem in our experiments in practice.Last, the optimization problem that we formulate is NP-hard (as we show in Sec-tion 4.2).
Because the number of nodes in focused entailment graphs is rather small, astandard ILP solver is able to quickly reach the optimal solution.To conclude, the algorithm we suggest next is applied in our experiments onfocused entailment graphs.
However, we believe that it is suitable for any entailmentgraph whose properties are similar to those of focused entailment graphs.
For brevity,from now on the term entailment graph will stand for focused entailment graph.4.
Learning Entailment Graph EdgesIn this section we present an algorithm that, given the set of propositional templatesconstituting the nodes of an entailment graph, learns its edges (i.e., the entailmentrelations between all pairs of nodes).
The algorithm comprises two steps (described inSections 4.1 and 4.2): In the first step we use a large corpus and a lexicographic resource(WordNet) to train a generic entailment classifier that given any pair of propositionaltemplates estimates the likelihood that one template entails the other.
This generic stepis performed only once, and is independent of the specific nodes of the target entailmentgraph whose edges we want to learn.
In the second step we learn on the fly the edges ofa specific target graph: Given the graph nodes, we use a global optimization approachthat determines the set of edges that maximizes the probability (or score) of the entiregraph.
The global graph decision is determined by the given edge probabilities (orscores) supplied by the entailment classifier and by the graph constraints (transitivityand others).4.1 Training an Entailment ClassifierWe describe a procedure for learning a generic entailment classifier, which can be usedto estimate the entailment likelihood for any given pair of templates.
The classifieris constructed based on a corpus and a lexicographic resource (WordNet) using thefollowing four steps:(1) Extract a large set of propositional templates from the corpus.
(2) Use WordNet to automatically generate a training set of pairs oftemplates?both positive and negative examples.
(3) Represent each training set example with a feature vector of variousdistributional similarity scores.
(4) Train a classifier over the training set.
(1) Template extraction.
We parse the corpus with the Minipar dependency parser(Lin 1998b) and use the Minipar representation to extract all binary templates fromevery parse tree, employing the procedure described by Lin and Pantel (2001), whichconsiders all dependency paths between every pair of nouns in the parse tree.
Wealso apply over the extracted paths the syntactic normalization procedure describedby Szpektor and Dagan (2007), which includes transforming passive forms into activeforms and removal of conjunctions, appositions, and abbreviations.
In addition, we use81Computational Linguistics Volume 38, Number 1Table 1Positive and negative examples for entailment in the training set.
The direction of entailment isfrom the left template to the right template.Positive examples Negative examples(Xsubj???
desiresobj??
Y, Xsubj???
wantsobj??
Y) (Xsubj???
pushesobj??
Y,Xsubj???
blowsobj??
Y)(Xsubj???
causes vrel??
Y, Xsubj???
creates vrel??
Y) (Xsubj???
issues vrel??
Y,Xsubj???
signs vrel??
Y)a simple heuristic to filter out templates that probably do not include a predicate: Weomit ?uni-directional?
templates where the root of template has a single child, such astherapyprep???inp?comp????
?patient nn?
?cancer, unless one of the edges is labeled with a passiverelation, such as in the template nauseavrel???characterizedsubj??
?poisoning, which containsthe Minipar passive label vrel.5 Last, the arguments are replaced by variables, resultingin propositional templates such as Xsubj???
affectobj??
Y.
The lexical items that remain inthe template after replacing the arguments by variables are termed predicate words.
(2) Training set generation.
WordNet is used to automatically generate a trainingset of positive (entailing) and negative (non-entailing) template pairs.
Let T be the setof propositional templates extracted from the corpus.
For each ti ?
T with two variablesand a single predicate word w, we extract from WordNet the set H of direct hypernyms(distance of one in WordNet) and synonyms of w. For every h ?
H, we generate a newtemplate tj from ti by replacing w with h. If tj ?
T, we consider (ti, tj) to be a positiveexample.
Negative examples are generated analogously, only considering direct co-hyponyms of w, which are direct hyponyms of direct hypernyms of w that are notsynonymous to w. It has been shown in past work that in most cases co-hyponym termsdo not entail one another (Mirkin, Dagan, and Gefet 2006).
A few examples for positiveand negative training examples are given in Table 1.This generation method is similar to the ?distant supervision?
method proposed bySnow, Jurafsky, and Ng (2004) for training a noun hypernym classifier.
It differs in someimportant aspects, however: First, Snow, Jurafsky, and Ng consider a positive exampleto be any Wordnet hypernym, irrespective of the distance, whereas we look only atdirect hypernyms.
This is because predicates are mainly verbs and precision dropsquickly when looking at verb hypernyms in WordNet at a longer distance.
Second,Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns whereone is not the hypernym of the other.
In the spirit of ?contrastive estimation?
(Smith andEisner 2005), we prefer to generate negative examples that are ?hard,?
that is, negativeexamples that, although not entailing, are still semantically similar to positive examplesand thus focus the classifier?s attention on determining the boundary of the entailmentclass.
Last, we use a balanced number of positive and negative examples, becauseclassifiers tend to perform poorly on the minority class when trained on imbalanceddata (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008).
(3) Distributional similarity representation.
We aim to train a classifier that for aninput template pair (t1, t2) determines whether t1 entails t2.
Our approach is to representa template pair by a feature vector where each coordinate is a different distributionalsimilarity score for the pair of templates.
The different distributional similarity scores5 This passive construction is not handled by the normalization scheme employed by Szpektor and Dagan(2007).82Berant et al Learning Entailment Relations by Global Graph Structure Optimizationare obtained by utilizing various distributional similarity algorithms that differ in oneor more of their characteristics.
In this way we hope to combine the various methodsproposed in the past for measuring distributional similarity.
The distributional similar-ity algorithms we employ vary in one or more of the following dimensions: the way thepredicate is represented, the way the features are represented, and the function used tomeasure similarity between the feature representations of the two templates.Predicate representation.
As mentioned, we represent predicates over dependencytree structures.
However, some distributional similarity algorithms measure similaritybetween binary templates directly (Lin and Pantel 2001; Szpektor et al 2004; Bhagat,Pantel, and Hovy 2007; Yates and Etzioni 2009), whereas others decompose binarytemplates into two unary templates, estimate similarity between two pairs of unarytemplates, and combine the two scores into a single score (Szpektor and Dagan 2008).Feature representation.
The features of a template are some function of the terms thatinstantiated the argument variables in a corpus.
Two representations that are used inour experiments are derived from an ontology that maps natural language phrases tosemantic identifiers (see Section 5).
Another variant occurs when using binary tem-plates: a template may be represented by a pair of feature vectors, one for each variableas in the DIRT algorithm (Lin and Pantel 2001), or by a single vector, where featuresrepresent pairs of instantiations (Szpektor et al 2004; Yates and Etzioni 2009).
Theformer variant reduces sparsity problems, whereas Yates and Etzioni showed the latteris more informative and performs favorably on their data.Similarity function.
We consider two similarity functions: The symmetric Lin (Linand Pantel 2001) similarity measure, and the directional BInc (Szpektor and Dagan2008) similarity measure, reviewed in Section 2.
Thus, information about the directionof entailment is provided by the BInc measure.We compute for any pair of templates (t1, t2) 12 distributional similarity scores usingall possible combinations of the aforementioned dimensions.
These scores are then usedas 12 features representing the pair (t1, t2).
(A full description of the features is given inSection 5.)
This is reminiscent of Connor and Roth (2007), who used the output of unsu-pervised classifiers as features for a supervised classifier in a verb disambiguation task.
(4) Training a classifier Two types of classifiers may be trained in our scheme overthe training set: margin classifiers (such as SVM) and probabilistic classifiers.
Given apair of templates (u, v) and their feature vector Fuv, we denote by an indicator variableIuv the event that u entails v. A margin classifier estimates a score Suv for the eventIuv = 1, which indicates the positive or negative distance of the feature vector Fuv fromthe separating hyperplane.
A probabilistic classifier provides the posterior probabilityPuv = P(Iuv = 1|Fuv).4.2 Global Learning of EdgesIn this step we get a set of propositional templates as input, and we would like to learnall of the entailment relations between these propositional templates.
For every pair oftemplates we can compute the distributional similarity features and get a score fromthe trained entailment classifier.
Once all the scores are calculated we try to find theoptimal graph?that is, the best set of edges over the propositional templates.
Thus, inthis scenario the input is the nodes of the graph and the output are the edges.To learn edges we consider global constraints, which allow only certain graphtopologies.
Because we seek a global solution under transitivity and other constraints,ILP is a natural choice, enabling the use of state-of-the-art ILP optimization packages.Given a set of nodes V and a weighting function f : V ?
V ?
R (derived from the83Computational Linguistics Volume 38, Number 1entailment classifier in our case), we want to learn the directed graph G = (V, E), whereE = {(u, v)| Iuv = 1}, by solving the following ILP over the variables Iuv:G?
= argmaxG?u=vf (u, v) ?
Iuv (8)s.t.
?u,v,w?V Iuv + Ivw ?
Iuw ?
1 (9)?u,v?Ayes Iuv = 1 (10)?u,v?Ano Iuv = 0 (11)?u=v Iuv ?
{0, 1} (12)The objective function in Equation (8) is simply a sum over the weights of the graphedges.
The global constraint is given in Equation (9) and states that the graph mustrespect transitivity.
This constraint is equivalent to the one suggested by Finkel andManning (2008) in a coreference resolution task, except that the edges of our graphare directed.
The constraints in Equations (10) and (11) state that for a few node pairs,defined by the sets Ayes and Ano, respectively, we have prior knowledge that one nodedoes or does not entail the other node.
Note that if (u, v) ?
Ano, then due to transitivitythere must be no path in the graph from u to v, which rules out additional edge combi-nations.
We elaborate on how the sets Ayes and Ano are computed in our experiments inSection 5.
Altogether, this Integer Linear Program contains O(|V|2) variables and O(|V|3)constraints, and can be solved using state-of-the-art optimization packages.A theoretical aspect of this optimization problem is that it is NP-hard.
We can phraseit as a decision problem in the following manner: Given V, f , and a threshold k, wewish to know if there is a set of edges E that respects transitivity and?
(u,v)?Ef (u, v) ?
k.Yannakakis (1978) has shown that the simpler problem of finding in a graph G?
=(V?, E?)
a subset of edges A ?
E?
that respects transitivity and |A| ?
k is NP-hard.
Thus,we can conclude that our optimization problem is also NP-hard by the trivial poly-nomial reduction defining the function f that assigns the score 0 for node pairs (u, v) /?
E?and the score 1 for node pairs (u, v) ?
E?.
Because the decision problem is NP-hard, it isclear that the corresponding maximization problem is also NP-hard.
Thus, obtaining asolution using ILP is quite reasonable and in our experiments also proves to be efficient(Section 5).Next, we describe two ways of obtaining the weighting function f , depending onthe type of entailment classifier we prefer to train.4.2.1 Score-Based Weighting Function.
In this case, we assume that we choose to train amargin entailment classifier estimating the score Suv (a positive score if the classifierpredicts entailment, and a negative score otherwise) and define f score(u, v) = Suv ?
?.This gives rise to the following objective function:G?score = argmaxG?u=v(Suv ?
?)
?
Iuv = argmaxG??
?u=vSuv ?
Iuv???
?
?
|E| (13)The term ?
?
|E| is a regularization term reflecting the fact that edges are sparse.
Intu-itively, this means that we would like to insert into the graph only edges with a score84Berant et al Learning Entailment Relations by Global Graph Structure OptimizationSuv > ?, or in other words to ?push?
the separating hyperplane towards the positivehalf space by ?.
Note that the constant ?
is a parameter that needs to be estimated andwe discuss ways of estimating it in Section 5.2.4.2.2 Probabilistic Weighting Function.
In this case, we assume that we choose to traina probabilistic entailment classifier.
Recall that Iuv is an indicator variable denotingwhether u entails v, that Fuv is the feature vector for the pair of templates u and v, and de-fine F to be the set of feature vectors for all pairs of templates in the graph.
The classifierestimates the posterior probability of an edge given its features: Puv = P(Iuv = 1|Fuv),and we would like to look for the graph G that maximizes the posterior probabilityP(G|F).
In Appendix A we specify some simplifying independence assumptions underwhich this graph maximizes the following linear objective function:G?prob = argmaxG?u=v(logPuv1 ?
Puv+ log?)
?
Iuv = argmaxG?u=vlogPuv1 ?
Puv?
Iuv + log?
?
|E|(14)where ?
= P(Iuv=1)P(Iuv=0) is the prior odds ratio for an edge in the graph, which needs to beestimated in some manner.
Thus, the weighting function is defined by fprob(u, v) =log Puv1?Puv + log?.Both the score-based and the probabilistic objective functions obtained are quitesimilar: Both contain a weighted sum over the edges and a regularization componentreflecting the sparsity of the graph.
Next, we show that we can provide a probabilisticinterpretation for our score-based function (under certain conditions), which will allowus to use a margin classifier and interpret its output probabilistically.4.2.3 Probabilistic Interpretation of Score-Based Weighting Function.
We would like to usethe score Suv, which is bounded in (?,??
), and derive from it a probability Puv.
Tothat end we project Suv onto (0, 1) using the sigmoid function, and define Puv in thefollowing manner:Puv =11 + exp(?Suv)(15)Note that under this definition the log probability ratio is equal to the inverse of thesigmoid function:logPuv1 ?
Puv= log11+exp(?Suv )exp(?Suv )1+exp(?Suv )= log 1exp(?Suv)= Suv (16)Therefore, when we derive Puv from Suv with the sigmoid function, we can rewriteG?prob as:G?prob = argmaxG?u=vSuv ?
Iuv + log?
?
|E| = G?score (17)where we see that in this scenario the two objective functions are identical and theregularization term ?
is related to the edge prior odds ratio by: ?
= ?
log?.85Computational Linguistics Volume 38, Number 1Moreover, assume that the score Suv is computed as a linear combination over nfeatures (such as a linear-kernel SVM), that is, Suv =?ni=1 Siuv ?
?i, where Siuv denotesfeature values and ?i denotes feature weights.
In this case, the projected probabilityacquires the standard form of a logistic classifier:Puv =11 + exp(?n?i=1Siuv ?
?i)(18)Hence, we can train the weights ?i using a margin classifier and interpret the outputof the classifier probabilistically, as we do with a logistic classifier.
In our experimentsin Section 5 we indeed use a linear-kernel SVM to train the weights ?i and then wecan interchangeably interpret the resulting ILP as either score-based or probabilisticoptimization.4.2.4 Comparison to Snow, Jurafsky, and Ng (2006).
Our work resembles Snow, Jurafsky,and Ng?s work in that both try to learn graph edges given a transitivity constraint.
Thereare two key differences in the model and in the optimization algorithm, however.
First,they employ a greedy optimization algorithm that incrementally adds hyponyms to alarge taxonomy (WordNet), whereas we simultaneously learn all edges using a globaloptimization method, which is more sound and powerful theoretically, and leads tothe optimal solution.
Second, Snow, Jurafsky, and Ng?s model attempts to determinethe graph that maximizes the likelihood P(F|G) and not the posterior P(G|F).
If we casttheir objective function as an ILP we get a formulation that is almost identical to ours,only containing the inverse prior odds ratio log 1?
= ?
log?
rather than the prior oddsratio as the regularization term (cf.
Section 2):G?Snow = argmaxG?u=vlogPuv(1 ?
Puv)?
Iuv ?
log?
?
|E| (19)This difference is insignificant when ?
?
1, or when ?
is tuned empirically for optimalperformance on a development set.
If, however, ?
is statistically estimated, this mightcause unwarranted results: Their model will favor dense graphs when the prior oddsratio is low (?
< 1 or P(Iuv = 1) < 0.5), and sparse graphs when the prior odds ratio ishigh (?
> 1 or P(Iuv = 1) > 0.5), which is counterintuitive.
Our model does not sufferfrom this shortcoming because it optimizes the posterior rather than the likelihood.
InSection 5 we show that our algorithm significantly outperforms the algorithm presentedby Snow, Jurafsky, and Ng.5.
Experimental EvaluationThis section presents an evaluation and analysis of our algorithm.5.1 Experimental SettingA health-care corpus of 632MB was harvested from the Web and parsed using the Mini-par parser (Lin 1998b).
The corpus contains 2,307,585 sentences and almost 50 million86Berant et al Learning Entailment Relations by Global Graph Structure OptimizationTable 2The similarity score features used to represent pairs of templates.
The columns specify thecorpus over which the similarity score was computed, the template representation, thesimilarity measure employed, and the feature representation (as described in Section 4.1).# Corpus Template Similarity measure Feature representation1 health-care binary BInc pair of CUI tuples2 health-care binary BInc pair of CUIs3 health-care binary BInc CUI tuple4 health-care binary BInc CUI5 health-care binary Lin pair of CUI tuples6 health-care binary Lin pair of CUIs7 health-care binary Lin CUI tuple8 health-care binary Lin CUI9 health-care unary BInc CUI tuple10 health-care unary BInc CUI11 health-care unary Lin CUI tuple12 health-care unary Lin CUI13 RCV1 binary Lin lexical items14 RCV1 unary Lin lexical items15 RCV1 unary BInc lexical items16 Lin & Pantel binary Lin lexical itemsword tokens.
We used the Unified Medical Language System (UMLS)6 to annotatemedical concepts in the corpus.
The UMLS is a database that maps natural languagephrases to over one million concept identifiers in the health-care domain (termedCUIs).
We annotated all nouns and noun phrases that are in the UMLS with their(possibly multiple) CUIs.
We now provide the details of training an entailment classifieras explained in Section 4.1.We extracted all templates from the corpus where both argument instantiations aremedical concepts, that is, annotated with a CUI (?50,000 templates).
This was done toincrease the likelihood that the extracted templates are related to the health-care domainand reduce problems of ambiguity.As explained in Section 4.1, a pair of templates constitutes an input example forthe entailment classifier, and should be represented by a set of features.
The featureswe used were different distributional similarity scores for the pair of templates, assummarized in Table 2.
Twelve distributional similarity measures were computed overthe health-care corpus using the aforementioned variations (Section 4.1), where twofeature representations were considered: in the UMLS each natural language phrasemay be mapped not to a single CUI, but to a tuple of CUIs.
Therefore, in the firstrepresentation, each feature vector coordinate counts the number of times a tuple ofCUIs was mapped to the term instantiating the template argument, and in the secondrepresentation it counts the number of times each single CUI was one of the CUIsmapped to the term instantiating the template argument.
In addition, we obtained theoriginal template similarity lists learned by Lin and Pantel (2001), and had availablethree distributional similarity measures learned by Szpektor and Dagan (2008), over theRCV1 corpus,7 as detailed in Table 2.
Thus, each pair of templates is represented by atotal of 16 distributional similarity scores.6 http://www.nlm.nih.gov/research/umls.7 http://trec.nist.gov/data/reuters/reuters.html.87Computational Linguistics Volume 38, Number 1We automatically generated a balanced training set of 20,144 examples using Word-Net and the procedure described in Section 4.1, and trained the entailment classifierwith SVMperf (Joachims 2005).
We use the trained classifier to obtain estimates for Puvand Suv, given that the score-based and probabilistic scoring functions are equivalent(cf.
Section 4.2.3).To evaluate the performance of our algorithm, we manually constructed gold-standard entailment graphs.
First, 23 medical target concepts, representing typical top-ics of interest in the medical domain, were manually selected from a (longer) list ofthe most frequent concepts in the health-care corpus.
The 23 target concepts are: alcohol,asthma, biopsy, brain, cancer, CDC, chemotherapy, chest, cough, diarrhea, FDA, headache, HIV,HPV, lungs, mouth, muscle, nausea, OSHA, salmonella, seizure, smoking, and x-ray.
For eachconcept, we wish to learn a focused entailment graph (cf.
Figure 1).
Thus, the nodes ofeach graph were defined by extracting all propositional templates in which the corre-sponding target concept instantiated an argument at least K(= 3) times in the health-care corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13).Ten medical students were given the nodes of each graph (propositional templates)and constructed the gold standard of graph edges using a Web interface.
We gave anoral explanation of the annotation process to each student, and the first two graphsannotated by every student were considered part of the annotator training phase andwere discarded.
The annotators were able to select every propositional template andobserve all of the instantiations of that template in our health-care corpus.
For example,selecting the template X helps with nausea might show the propositions relaxation helpswith nausea, acupuncture helps with nausea, and Nabilone helps with nausea.
The conceptof entailment was explained under the framework of TE (Dagan et al 2009), that is, thetemplate t1 entails the template t2 if given that the instantiation of t1 with some conceptis true then the instantiation of t2 with the same concept is most likely true.As explained in Section 3.2, we did not perform any disambiguation because atarget concept disambiguates the propositional templates in focused entailment graphs.In practice, cases of ambiguity were very rare, except for a single scenario where intemplates such as X treats asthma, annotators were unclear whether X is a type of doctoror a type of drug.
The annotators were instructed in such cases to select the template,read the instantiations of the template in the corpus, and choose the sense that is mostprevalent in the corpus.
This instruction was applicable to all cases of ambiguity.Each concept graph was annotated by two students.
Following the current recog-nizing TE (RTE) practice (Bentivogli et al 2009), after initial annotation the two studentsmet for a reconciliation phase.
They worked to reach an agreement on differences andcorrected their graphs.
Inter-annotator agreement was calculated using the kappa statis-tic (Siegel and Castellan 1988) both before (?
= 0.59) and after (?
= 0.9) reconciliation.Each learned graph was evaluated against the two reconciliated graphs.Summing the number of possible edges over all 23 concept graphs we get 10,364possible edges, of which 882 on average were included by the annotators (averagingover the two gold-standard annotations for each graph).
The concept graphs wererandomly split into a development set (11 concepts) and a test set (12 concepts).We used the lpsolve8 package to learn the edges of the graphs.
This package ef-ficiently solves the model without imposing integer restrictions9 and then uses thebranch-and-bound method to find an optimal integer solution.
We note that in the8 http://lpsolve.sourceforge.net/5.5/.9 While ILP is an NP-hard problem, LP is a polynomial problem and can be solved efficiently.88Berant et al Learning Entailment Relations by Global Graph Structure Optimizationexperiments reported in this article the optimal solution without integer restrictionswas already integer.
Thus, although in general our optimization problem is NP-hard,in our experiments we were able to reach an optimal solution for the input graphsvery efficiently (we note that in some scenarios not reported in this article the optimalsolution was not integer and so an integer solution is not guaranteed a priori).As mentioned in Section 4.2, we added a few constraints in cases where there wasstrong evidence that edges are not in the graph.
This is done in the following scenarios(examples given in Table 3): (1) When two templates u and v are identical except fora pair of words wu and wv, and wu is an antonym of wv, or a hypernym of wv atdistance ?
2 in WordNet.
(2) When two nodes u and v are transitive ?opposites,?
thatis, if u = Xsubj???
wobj??
Y and v = Xobj??
wsubj???
Y, for any word w. We note that there aresome transitive verbs that express a reciprocal activity, such as X marries Y, but usuallyreciprocal events are not expressed using a transitive verb structure.In addition, in some cases we have strong evidence that edges do exist in the graph.This is done in a single scenario (see Table 3), which is specific to the output of Minipar:when two templates differ by a single edge and the first is of the type Xobj??
Y andthe other is of the type Xvrel???
Y, which expresses a passive verb modifier of nouns.Altogether, these initializations took place in less than 1% of the node pairs in thegraphs.
We note that we tried to use WordNet relations such as hypernym and synonymas ?positive?
hard constraints (using the constraint Iuv = 1), but this resulted in reducedperformance because the precision of WordNet was not high enough.The graphs learned by our algorithm were evaluated by two measures.
The firstmeasure evaluates the graph edges directly, and the second measure is motivated bysemantic inference applications that utilize the rules in the graph.
The first measure issimply the F1 of the set of learned edges compared to the set of gold-standard edges.In the second measure we take the set of learned rules and infer new propositions byapplying the rules over all propositions extracted from the health-care corpus.
We applythe rules iteratively over all propositions until no new propositions are inferred.
Forexample, given the corpus proposition relaxation reduces nausea and the edges X reducesnausea ?
X helps with nausea and X helps with nausea ?
X related to nausea, we eval-uate the set {relaxation reduces nausea, relaxation helps with nausea, relaxation related tonausea}.
For each graph we measure the F1 of the set of propositions inferred by thelearned graphs when compared to the set of propositions inferred by the gold-standardgraphs.
For both measures the final score of an algorithm is a macro-average F1 overthe 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 testconcepts).Table 3Scenarios in which we added hard constraints to the ILP.Scenario Example Initializationantonym (Xsubj???
decreaseobj??
Y,Xsubj???
increaseobj??
Y) Iuv = 0hypernym ?
2 (Xsubj???
affectobj??
Y,Xsubj???
irritateobj??
Y) Iuv = 0transitive opposite (Xsubj???
causeobj??
Y,Ysubj???
causeobj??
X) Iuv = 0syntactic variation (Xsubj???
followobj??
Y,Ysubj???
follow vrel??
X) Iuv = 189Computational Linguistics Volume 38, Number 1Learning the edges of a graph given an input concept takes about 1?2 seconds on astandard desktop.5.2 Evaluated AlgorithmsFirst, we describe some baselines that do not utilize the entailment classifier or theILP solver.
For each of the 16 distributional similarity measures (Table 2) and for eachtemplate t, we computed a list of templates most similar to t (or entailing t for directionalmeasures).
Then, for each measure we learned graphs by inserting an edge (u, v), whenu is in the top K templates most similar to v. The parameter K can be optimized either onthe automatically generated training set (from WordNet) or on the manually annotateddevelopment set.
We also learned graphs using WordNet: We inserted an edge (u, v)when u and v differ by a single word wu and wv, respectively, and wu is a direct hyponymor synonym of wv.
Next, we describe algorithms that utilize the entailment classifier.Our algorithm, named ILP-Global, utilizes global information and an ILP formula-tion to find maximum a posteriori graphs.
Therefore, we compare it to the followingthree variants: (1) ILP-Local: An algorithm that uses only local information.
This isdone by omitting the global transitivity constraints, and results in an algorithm thatinserts an edge (u, v) if and only if (Suv ?
?)
> 0.
(2) Greedy-Global: An algorithm thatlooks for the maximum a posteriori graphs but only employs the greedy optimizationprocedure as described by Snow, Jurafsky, and Ng (2006).
(3) ILP-Global-Likelihood:An ILP formulation where we look for the maximum likelihood graphs, as described bySnow, Jurafsky, and Ng (cf.
Section 4.2).We evaluate these algorithms in three settings which differ in the method by whichthe edge prior odds ratio, ?
(or ?
), is estimated: (1) ?
= 1 (?
= 0), which means thatno prior is used.
(2) Tuning ?
and using the value that maximizes performance over thedevelopment set.
(3) Estimating ?
using maximum likelihood over the development set,which results in ?
?
0.1 (?
?
2.3), corresponding to the edge density P(Iuv = 1) ?
0.09.For all local algorithms whose output does not respect transitivity constraints, weadded all edges inferred by transitivity.
This was done because we assume that the ruleslearned are to be used in the context of an inference or entailment system.
Because suchsystems usually perform chaining of entailment rules (Raina, Ng, and Manning 2005;Bar-Haim et al 2007; Harmeling 2009), we conduct this chaining as well.
Nevertheless,we also measured performance when edges inferred by transitivity are not added: Weonce again chose the edge prior value that maximizes F1 over the development setand obtained macro-average recall/precision/F1 of 51.5/34.9/38.3.
This performance iscomparable to the macro-average recall/precision/F1 of 44.5/45.3/38.1 we report nextin Table 4.5.3 Experimental Results and AnalysisIn this section we present experimental results and analysis that show that theILP-Global algorithm improves performance over baselines, specifically in terms ofprecision.Tables 4?7 and Figure 2 summarize the performance of the algorithms.
Table 4shows our main result when the parameters ?
and K are optimized to maximize per-formance over the development set.
Notice that the algorithm ILP-Global-Likelihoodis omitted, because when optimizing ?
over the development set it conflates withILP-Global.
The rows Local1 and Local2 present the best algorithms that use a singledistributional similarity resource.
Local1 and Local2 correspond to the configurations90Berant et al Learning Entailment Relations by Global Graph Structure OptimizationTable 4Results when tuning for performance over the development set.Edges PropositionsRecall Precision F1 Recall Precision F1ILP-Global (?
= 0.45) 46.0 50.1 43.8 67.3 69.6 66.2Greedy-Global (?
= 0.3) 45.7 37.1 36.6 64.2 57.2 56.3ILP-Local (?
= 1.5) 44.5 45.3 38.1 65.2 61.0 58.6Local1 (K = 10) 53.5 34.9 37.5 73.5 50.6 56.1Local2 (K = 55) 52.5 31.6 37.7 69.8 50.0 57.1Table 5Results when the development set is not used to estimate ?
and K.Edges PropositionsRecall Precision F1 Recall Precision F1ILP-Global 58.0 28.5 35.9 76.0 46.0 54.6Greedy-Global 60.8 25.6 33.5 77.8 41.3 50.9ILP-Local 69.3 19.7 26.8 82.7 33.3 42.6Local1 (K = 100) 92.6 11.3 20.0 95.3 18.9 31.1Local2 (K = 100) 63.1 25.5 34.0 77.7 39.9 50.9WordNet 10.8 44.1 13.2 39.9 72.4 47.3Table 6Results with prior estimated on the development set, that is ?
= 0.1, which is equivalent to?
= 2.3.Edges PropositionsRecall Precision F1 Recall Precision F1ILP-Global 16.8 67.1 24.4 43.9 86.8 56.3ILP-Global-Likelihood 91.8 9.8 17.5 94.0 16.7 28.0Greedy-Global 14.7 62.9 21.2 43.5 86.6 56.2Greedy-Global-Likelihood 100.0 9.3 16.8 100.0 15.5 26.5described in Table 2 by features no.
5 and no.
1, respectively (see also Table 8).
ILP-Global improves performance by at least 13%, and significantly outperforms all localmethods, as well as the greedy optimization algorithm both on the edges F1 measure(p < 0.05) and on the propositions F1 measure (p < 0.01).10Table 5 describes the results when the development set is not used to estimate theparameters ?
and K: A uniform prior (Puv = 0.5) is assumed for algorithms that usethe entailment classifier, and the automatically generated training set is employed toestimate K. Again ILP-Global-Likelihood is omitted in the absence of a prior.
ILP-Globaloutperforms all other methods in this scenario as well, although by a smaller marginfor a few of the baselines.
Comparing Table 4 to Table 5 reveals that excluding the10 We tested significance using the two-sided Wilcoxon rank test (Wilcoxon 1945).91Computational Linguistics Volume 38, Number 1Table 7Results per concept for the ILP-Global.Concept R P F1Smoking 58.1 81.8 67.9Seizure 64.7 51.2 57.1Headache 60.9 50.0 54.9Lungs 50.0 56.5 53.1Diarrhea 42.1 60.0 49.5Chemotherapy 44.7 52.5 48.3HPV 35.2 76.0 48.1Salmonella 27.3 80.0 40.7X-ray 75.0 23.1 35.3Asthma 23.1 30.6 26.3Mouth 17.7 35.5 23.7FDA 53.3 15.1 23.5sparse prior indeed increases recall at a price of a sharp decrease in precision.
Note,however, that local algorithms are more vulnerable to this phenomenon.
This makessense because in local algorithms eliminating the prior adds edges that in turn add moreedges due to the constraint of transitivity and so recall dramatically rises at the expenseof precision.
Global algorithms are not as prone to this effect because they refrain fromadding edges that eventually lead to the addition of many unwarranted edges.Table 5 also shows that WordNet, a manually constructed resource, has notablythe highest precision and lowest recall.
The low recall exemplifies how the entailmentrelations given by the gold-standard annotators transcend much beyond simple lexicalrelations that appear in WordNet: Many of the gold-standard entailment relations aremissing from WordNet or involve multi-word phrases that do not appear in WordNetat all.Note that although the precision of WordNet is the highest in Table 5, its absolutevalue (44.1%) is far from perfect.
This illustrates that hierarchies of predicates are quiteFigure 2Recall-precision curve comparing ILP-Global with Greedy-Global and ILP-Local.92Berant et al Learning Entailment Relations by Global Graph Structure OptimizationTable 8Results of all distributional similarity measures when tuning K over the development set.We encode the description of the measures presented in Table 2 in the following manner?h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similaritymeasure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIsrepresentation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similaritylists learned by Lin and Pantel.Edges PropositionsDist.
sim.
measure Recall Precision F1 Recall Precision F1h-b-B-pCt 52.5 31.6 37.7 69.8 50.0 57.1h-b-B-pC 50.5 26.5 30.7 67.1 43.5 50.1h-b-B-Ct 10.4 44.5 15.4 39.1 78.9 51.6h-b-B-C 7.6 42.9 11.1 37.9 79.8 50.7h-b-L-pCt 53.4 34.9 37.5 73.5 50.6 56.1h-b-L-pC 47.2 35.2 35.6 68.6 52.9 56.2h-b-L-Ct 47.0 26.6 30.2 64.9 47.4 49.6h-b-L-C 34.6 22.9 22.5 57.2 52.6 47.6h-u-B-Ct 5.1 37.4 8.5 35.1 91.0 49.7h-u-B-C 7.2 42.4 11.5 36.1 90.3 50.1h-u-L-Ct 22.8 22.0 18.3 49.7 49.2 44.5h-u-L-C 16.7 26.3 17.8 47.0 56.8 48.1R-b-L-l 49.4 21.8 25.2 72.4 39.0 45.5R-u-L-l 24.1 30.0 16.8 47.1 55.2 42.1R-u-B-l 9.5 57.1 14.1 37.2 84.0 49.5Lin & Pantel 37.1 32.2 25.1 58.9 54.6 48.6ambiguous and thus using WordNet directly yields relatively low precision.
WordNetis vulnerable to such ambiguity because it is a generic domain-independent resource,whereas our algorithm learns from a domain-specific corpus.
For example, the wordshave and cause are synonyms according to one of the senses in WordNet and so theerroneous rule X have asthma ?
X cause asthma is learned using WordNet.
Anotherexample is the rule X follows chemotherapy ?
X takes chemotherapy, which is incorrectlyinferred because follow is a hyponym of take according to one of WordNet?s senses (shefollowed the feminist movement).
Due to these mistakes made by WordNet, the precisionachieved by our automatically trained ILP-Global algorithm when tuning parameterson the development set (Table 4) is higher than that of WordNet.Table 6 shows the results when the prior ?
is estimated using maximum likelihoodover the development set (by computing the edge density over all the developmentset graphs), and not tuned empirically with grid search.
This allows for a comparisonbetween our algorithm that maximizes the a posteriori probability and Snow, Jurafsky,and Ng?s (2006) algorithm that maximizes the likelihood.
The gold-standard graphs arequite sparse (?
?
0.1); therefore, as explained in Section 4.2.4, the effect of the prior issubstantial.
ILP-Global and Greedy-Global learn sparse graphs with high precision andlow recall, whereas ILP-Global-Likelihood and Greedy-Global-Likelihood learn densegraphs with high recall but very low precision.
Overall, optimizing the a posterioriprobability is substantially better than optimizing likelihood, but still leads to a largedegradation in performance.
This can be explained because our algorithm is not purelyprobabilistic: The learned graphs are the product of mixing a probabilistic objectivefunction with non-probabilistic constraints.
Thus, plugging the estimated prior into thismodel results in performance that is far from optimal.
In future work, we will examine93Computational Linguistics Volume 38, Number 1a purely probabilistic approach that will allow us to reach good performance whenestimating ?
directly.
Nevertheless, currently optimal results are achieved when theprior ?
is tuned empirically.Figure 2 shows a recall?precision curve for ILP-Global, Greedy-Global, and ILP-Local, obtained by varying the prior parameter, ?.
The figure clearly demonstrates theadvantage of using global information and ILP.
ILP-Global is better than Greedy-Globaland ILP-Local in almost every point of the recall?precision curve, regardless of the exactvalue of the prior parameter.
Last, we present for completeness in Table 7 the results ofILP-Global for all concepts in the test set.In Table 8 we present the results obtained for all 16 distributional similarity mea-sures.
The main conclusion we can derive from this table is that the best distributionalsimilarity measures are those that represent templates using pairs of argument instan-tiations rather than each argument separately.
A similar result was found by Yates andEtzioni (2009), who described the RESOLVER paraphrase learning system and haveshown that it outperforms DIRT.
In their analysis, they attribute this result to theirrepresentation that utilizes pairs of arguments comparing to DIRT, which computes aseparate score for each argument.In the next two sections we perform a more thorough qualitative and quantitativecomparison trying to analyze the importance of using global information in graphlearning (Section 5.3.1), as well as the contribution of using ILP rather than a greedyoptimization procedure (Section 5.3.2).
We note that the analysis presented in both sec-tions is for the results obtained when optimizing parameters over the development set.5.3.1 Global vs. Local Information.
We looked at all edges in the test-set graphs whereILP-Global and ILP-Local disagree and checked which algorithm was correct.
Table 9presents the result.
The main advantage of using ILP-Global is that it avoids insertingwrong edges into the graph.
This is because ILP-Local adds any edge (u, v) such thatPuv crosses a certain threshold, disregarding edges that will be consequently added dueto transitivity (recall that for local algorithms we add edges inferred by transitivity, cf.Section 5.2).
ILP-Global will avoid such edges of high probability if it results in insertingmany low probability edges.
This results in an improvement in precision, as exhibitedby Table 4.Figures 3 and 4 show fragments of the graphs learned by ILP-Global and ILP-Local (prior to adding transitive edges) for the test-set concepts diarrhea and seizure,and illustrate qualitatively how global considerations improve precision.
In Figure 3,we witness that the single erroneous edge X results in diarrhea ?
X prevents diarrheainserted by the local algorithm because Puv is high, effectively bridges two stronglyconnected components and induces a total of 12 wrong edges (all edges from theupper component to the lower component), whereas ILP-Global refrains from insertingthis edge.
Figure 4 depicts an even more complex scenario.
First, ILP-Local inducesa strongly connected component of five nodes for the predicates control, treat, stop,Table 9Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs.Global=True/Local=False Global=False/Local=TrueGold standard=true 48 42Gold standard=false 78 49494Berant et al Learning Entailment Relations by Global Graph Structure OptimizationFigure 3A comparison between ILP-Global and ILP-local for two fragments of the test-set conceptdiarrhea.reduce, and prevent, whereas ILP-Global splits this strongly connected component intotwo, which although not perfect, is more compatible with the gold-standard graphs.In addition, ILP-Local inserts four erroneous edges that connect two components ofsize 4 and 5, which results in adding eventually 30 wrong edges.
On the other hand,Figure 4A comparison between ILP-Global and ILP-Local for two fragments of the test-set conceptseizure.95Computational Linguistics Volume 38, Number 1ILP-Global is aware of the consequences of adding these four seemingly good edges,and prefers to omit them from the learned graph, leading to much higher precision.Although the main contribution of ILP-Global, in terms of F1, is in an increase inprecision, we also notice an increase in recall in Table 4.
This is because the optimalprior is ?
= 0.45 in ILP-Global but ?
= 1.5 in ILP-Local.
Thus, any edge (u, v) such that0.45 < Suv < 1.5 will have positive weight in ILP-Global and might be inserted into thegraph, but will have negative weight in ILP-Local and will be rejected.
The reason is thatin a local setting, reducing false positives is handled only by applying a large penaltyfor every wrong edge, whereas in a global setting wrong edges can be rejected becausethey induce more ?bad?
edges.
Overall, this leads to an improved recall in ILP-Global.This also explains why ILP-Local is severely harmed when no prior is used at all, asshown in Table 5.Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 overthe edges in 7 graphs with an average advantage of 11.7 points, ILP-Local achievesbetter F1 over the edges in 4 graphs with an average advantage of 3.0 points, and oneperformance is equal.5.3.2 Greedy vs. Non-Greedy Optimization.
We would like to understand how using anILP solver improves performance compared with a greedy optimization procedure.Table 4 demonstrates that ILP-Global and Greedy-Global reach a similar level of re-call, although ILP-Global achieves far better precision.
Again, we investigated edgesfor which the two algorithms disagree and checked which one was correct.
Table 10demonstrates that the higher precision is because ILP-Global avoids inserting wrongedges into the graph.Figure 5 illustrates some of the reasons ILP-Global performs better than Greedy-Global.
Parts A1?A3 show the progression of Greedy-Global, which is an incrementalalgorithm, for a fragment of the headache graph.
In part A1 the learning algorithm stillseparates the nodes X prevents headache and X reduces headache from the nodes X causesheadache and X results in headache (nodes surrounded by a bold oval shape constitutea strongly connected component).
After two iterations, however, the four nodes arejoined into a single strongly connected component, which is an error in principlebut at this point seems to be the best decision to increase the posterior probabilityof the graph.
This greedy decision has two negative ramifications.
First, the stronglyconnected component can no longer be untied.
Thus, in A3 we observe that in futureiterations the strongly connected component expands further and many more wrongedges are inserted into the graph.
On the other hand, in B we see that ILP-Global takesinto consideration the global interaction between the four nodes and other nodes of thegraph, and decides to split this strongly connected component in two, which improvesthe precision of ILP-Global.
Second, note that in A3 the nodes Associate X with headacheand Associate headache with X are erroneously isolated.
This is because connecting themto the strongly connected component that contains six nodes will add many edges withTable 10Comparing disagreements between ILP-Global and Greedy-Global against the gold-standardgraphs.ILP=True/Greedy=False ILP=False/Greedy=TrueGold standard=true 66 56Gold standard=false 44 48096Berant et al Learning Entailment Relations by Global Graph Structure OptimizationFigure 5A comparison between ILP-Global and Greedy-Global.
Parts A1?A3 depict the incrementalprogress of Greedy Global for a fragment of the headache graph.
Part B depicts the correspondingfragment in ILP-Global.
Nodes surrounded by a bold oval shape are strongly connectedcomponents.low probability and so this is avoided by Greedy-Global.
Because in ILP-Global thestrongly connected component was split in two, it is possible to connect these two nodesto some of the other nodes and raise the recall of ILP-Global.
Thus, we see that greedyoptimization might get stuck in local maxima and consequently suffer in terms of bothprecision and recall.Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 overthe edges in 9 graphs with an average advantage of 10.0 points, Greedy-Global achievesbetter F1 over the edges in 2 graphs with an average advantage of 1.5 points, and in onecase performance is equal.5.4 Error AnalysisIn this section, we compare the results of ILP-Global with the gold-standard graphsand perform error analysis.
Error analysis was performed by comparing the 12 graphslearned by ILP-Global to the corresponding 12 gold-standard graphs (randomly sam-pling from the two available gold-standard graphs), and manually examining all edgesfor which the two disagree.
We found that the number of false positives and falsenegatives is almost equal: 282 edges were learned by ILP-Global but are not in the gold-standard graphs (false positive) and 287 edges were in the gold-standard graphs butwere not learned by ILP-Global (false negatives).97Computational Linguistics Volume 38, Number 1Table 11Error analysis for false positives and false negatives.False positives False negativesTotal count 282 Total count 287Classifier error 84.8% Classifier error 73.5%Co-hyponym error 18.0% Long-predicate error 36.2%Direction error 15.1% Generality error 26.8%String overlap error 20.9%Table 11 presents the results of our manual error analysis.
Most evident is the factthat the majority of mistakes are misclassifications of the entailment classifier.
For 73.5%of the false negatives the classifier?s probability was Puv < 0.5 and for 84.8% of the falsepositives the classifier?s probability was Puv > 0.5.
This shows that our current classifierstruggles to distinguish between positive and negative examples.
Figure 6 illustratessome of this difficulty by showing the distribution of the classifier?s probability, Puv,over all node pairs in the 12 test-set graphs.
Close to 80% of the scores are in the range0.45?0.5, most of which are simply node pairs for which all distributional similarityfeatures are zero.
Although in the great majority of such node pairs (t1, t2) t1 indeeddoes not entail t2, there are also some cases where t1 does entail t2.
This implies that thecurrent feature representation is not rich enough, and in the next section we explore alarger feature set.Table 11 also shows some other reasons found for false positives.
Many false posi-tives are pairs of predicates that are semantically related, that is, 18% of false positivesare templates that are hyponyms of a common predicate (co-hyponym error), and 15.1%of false positives are pairs where we err in the direction of entailment (direction error).For example ILP-Global learns that place X in mouth ?
remove X from mouth, which is aFigure 6Distribution of probabilities given by the classifier over all node pairs of the test-set graphs.98Berant et al Learning Entailment Relations by Global Graph Structure Optimizationco-hyponym error, and also that X affects lungs ?
X damages lungs, which is a directionerror because entailment holds in the other direction.
This illustrates the infamousdifficulty of distributional similarity features to discern the type of semantic relationbetween two predicates.Table 11 also shows additional reasons for false negatives.
We found that in 36.2% offalse negatives one of the two templates contained a ?long?
predicate, that is a predicatecomposed of more than one content word, such as Ingestion of X causes injury to Y. Thismight indicate that the size of the health-care corpus is too small to collect sufficientstatistics for complex predicates.
In addition, 26.8% of false negatives were manuallyanalyzed as ?generality errors.?
An example is the edge HPV strain causes X ?
associateHPV with X that is in the gold-standard graph but was missed by ILP-Global.
Indeed,this edge falls within the definition of textual entailment and is correct: For example,if some HPV strain causes cervical cancer then cervical cancer is associated with HPV.Because the entailed template is much more general than the entailing template, how-ever, they are not instantiated by similar arguments in the corpus and distributionalsimilarity features fail to capture their semantic similarity.
Last, we note that in 20.9%of the false negatives, there was some string overlap between the entailing and entailedtemplates, for example in X controls asthma symptoms ?
X controls asthma.
In the nextsection we experiment with a feature that is based on string similarity.Tables 8 and 9 show that there are cases where ILP-Global makes a mistake, whereasILP-Local or Greedy-Global are correct.
An illustrating example for such a case isshown in Figure 7.
Looking at ILP-Local we see that the entailment classifier correctlyclassifies the edges X triggers asthma ?
X causes asthma and X causes asthma ?
AssociateX with asthma, but misclassifies X triggers asthma ?
Associate X with asthma.
Because thisconfiguration violates a transitivity constraint, ILP-Global must make a global decisionwhether to add the edge X triggers asthma ?
Associate X with asthma or to omit one ofFigure 7A scenario where ILP-Global makes a mistake, but ILP-Local is correct.99Computational Linguistics Volume 38, Number 1the correct edges.
The optimal global decision in this case causes a mistake with respectto the gold standard.
More generally, a common phenomenon of ILP-Global is that itsplits components that are connected in ILP-Local, for example, in Figures 3 and 4.
ILP-Global splits the components in a way that is optimal according to the scores of the localentailment classifier, but these are not always accurate according to the gold standard.Figure 5 exemplifies a scenario where ILP-Global errs, but Greedy-Global is (partly)correct.
ILP-Global mistakenly learns entailment rules from the templates AssociateX with headache and Associate headache with X to the templates X causes headache andX results in headache, whereas Greedy-Global isolates the templates Associate X withheadache and Associate headache with X in a separate component.
This happens becauseof the greedy nature of Greedy-Global.
Notice that in step A2 the templates X causesheadache and X results in headache are already included (erroneously) in a connectedcomponent with the templates X prevents headache and X reduces headache.
Thus, addingthe rules from Associate X with headache and Associate headache with X to X causes headacheand X results in headache would also add the rules to X reduces headache and X preventsheadache and the Greedy-Global avoids that.
ILP-Global does not have that problem: Itsimply chooses the optimal choice according to the entailment classifier, which splits theconnected component presented in A2.
Thus, once again we see that mistakes made byILP-Global are often due to the inaccuracies of the scores given by the local entailmentclassifier.6.
Local Classifier ExtensionsThe error analysis in Section 5.4 exemplified that most errors are the result of misclassi-fications made by the local entailment classifier.
In this section, we investigate the localentailment classifier component, focusing on the set of features used for classification.We first present an experimental setting in which we consider a wider set of features,then we present the results of the experiment, and last we perform feature analysis anddraw conclusions.6.1 Feature Set and Experimental SettingIn previous sections we employed a distant supervision framework: We generatedtraining examples automatically with WordNet, and represented each example withdistributional similarity features.
Distant supervision comes with a price, however?itprevents us from utilizing all sources of information.
For example, looking at the pair ofgold-standard templates X manages asthma and X improves asthma management, one canexploit the fact that management is a derivation of manage to improve the estimation ofentailment.
The automatically generated training set was generated by looking at Word-Net?s hypernym, synonym, and co-hyponyms relations, however, and hence no suchexamples appear in the training set, rendering this type of feature useless.
Moreover,one cannot use WordNet?s hypernym, synonym, and co-hyponym relations as featuresbecause the generated training set is highly biased?all positive training examples areeither hypernyms or synonyms and all negative examples are co-hyponyms.In this section we would like to examine the utility of various features, while avoid-ing the biases that occur due to distant supervision.
Therefore, we use the 23 manuallyannotated gold-standard graphs for both training and testing, in a cross-validationsetting.
Although this reduces the size of the training set it allows us to estimate theutility of various features in a setting where the training set and test set are sampledfrom the same underlying distribution, without the aforementioned biases.100Berant et al Learning Entailment Relations by Global Graph Structure OptimizationWe would like to extract features that express information that is diverse andorthogonal to the one given by distributional similarity.
Therefore, we turn to existingknowledge resources that were created using both manual and automatic methods,expressing various types of linguistic and statistical information that is relevant forentailment prediction:1.
WordNet: contains manually annotated relations such as hypernymy,synonymy, antonymy, derivation, and entailment.2.
VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such asstronger-than and similar that were learned with pattern-based methods.3.
CATVAR12 (Habash and Dorr 2003): contains word derivations such asdevelop?development.4.
FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailmentrules between templates learned automatically from FrameNet.5.
NomLex14 (Macleod et al 1998): contains English nominalizationsincluding their argument mapping to the corresponding verbal form.6.
BAP15 (Kotlerman et al 2010): contains directional distributionalsimilarity scores between lexical terms (rather than propositionaltemplates) calculated with the BAP similarity scoring function.Table 12 describes the 16 new features that were generated for each of the gold-standard examples (resulting in a total of 32 features).
The first 15 features were gen-erated by the aforementioned knowledge bases.
The last feature measures the editdistance between templates: Given a pair of templates (t1, t2), we concatenate the wordsin each template and derive a pair of strings (s1, s2).
Then we compute the Levenshteinstring edit-distance (Cohen, Ravikumar, and Fienberg 2003) between s1 and s2 anddivide the score by |s1|+ |s2| for normalization.Table 12 also describes for each feature the number and percentage of examples forwhich the feature value is non-zero (out of the examples generated from the 23 gold-standard graphs).
A salient property of many of the new features is that they are sparse:The four antonymy features as well as the Derivation, Entailment, Nomlex, and FREDfeatures occur in very few examples in our data set, which might make training withthese features difficult.After generating the new features we employ a leave-one-graph-out strategy tomaximally exploit the manually annotated gold standard for training.
For each of thetest-set graphs, we train over all development and test-set graphs except for the onethat is left out,16 after tuning the algorithm?s parameters and test.
Parameter tuning isdone by cross-validation over the development set, tuning to maximize the F1 of the set11 http://demo.patrickpantel.com/demos/verbocean/.12 http://clipdemos.umiacs.umd.edu/catvar/.13 http://u.cs.biu.ac.il/?nlp/downloads/FRED.html.14 http://nlp.cs.nyu.edu/nomlex/index.html.15 http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html.16 As described in Section 5, we train with a balanced number of positive and negative examples.
Becausethe number of positive examples in the gold standard is smaller than the number of negative examples,we use all positives and randomly sample the same number of negatives, resulting in ?
1, 500 trainingexamples.101Computational Linguistics Volume 38, Number 1Table 12The set of new features.
The last two columns denote the number and percentage of examplesfor which the value of the feature is non-zero in examples generated from the 23 gold-standardgraphs.Name Type Description # %Hyper.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is a hypernym (distance ?
2) of w1in WordNet.120 1.1Syno.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is a synonym of w1 in WordNet.94 0.9Co-hypo.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is a co-hyponym of w1 in WordNet.302 2.8WN Ant.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is an antonym of w1 in WordNet.6 0.06VO Ant.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is an antonym of w1 in VerbOcean.25 0.2WN Ant.
2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)such that w2 is an antonym of w1 in WordNet.22 0.2VO Ant.
2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)such that w2 is an antonym of w1 in VerbOcean.73 0.7Derivation boolean Whether there exists in (t1, t2) a pair of words (w1, w2)such that w2 is a derivation of w1 in WordNet or CATVAR.78 0.7Entailment boolean Whether there exists in (t1, t2) a pair of words (w1, w2)such that w2 is entailed by w1 in WordNet.20 0.2FRED boolean Whether t1 entails t2 in FRED.
9 0.08Nomlex boolean Whether t1 entails t2 in Nomlex.
8 0.07VO strong boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is stronger than w1 in VerbOcean.104 1VO simil.
boolean Whether (t1, t2) are identical except for a pair of words(w1, w2) such that w2 is similar to w1 in VerbOcean.191 1.8Positive boolean Disjunction of the features Hypernym, Synonym, Nom-lex, and VO stronger.289 2.7BAP real maxw1?t1,w2?t2BAP(w1, w2).
506 4.7Edit real Normalized edit-distance.
100of learned edges (the development and test set are described in Section 5).
Graphs arealways learned with the LP-Global algorithm.Our main goal is to check whether the added features improve performance, andtherefore we run the experiment both with and without the new features.
In addi-tion, we would like to test whether using different classifiers affects performance.Therefore, we run the experiments with a linear-kernel SVM, a square-kernel SVM,a Gaussian-kernel SVM, logistic regression, and naive Bayes.
We use the SVMPerfpackage (Joachims 2005) to train the SVM classifiers and the Weka package (Hall et al2009) for logistic regression and naive Bayes.6.2 Experiment ResultsTable 13 describes the macro-average recall, precision, and F1 of all classifiers both withand without the new features on the development set and test set.
Using all features isdenoted by Xall, and using the original features is denoted by Xold.102Berant et al Learning Entailment Relations by Global Graph Structure OptimizationTable 13Macro-average recall, precision, and F1 on the development set and test set using the parametersthat maximize F1 of the learned edges over the development set.Development set Test setAlgorithm Recall Precision F1 Recall Precision F1Linearall 48.1 31.9 36.3 51.7 37.7 40.3Linearold 40.3 33.3 34.8 47.2 42.2 41.1Gaussianall 41.8 32.4 35.1 48.0 41.1 40.7Gaussianold 41.1 31.2 33.9 50.3 39.7 40.5Squareall 39.9 32.0 34.1 43.7 39.8 38.9Squareold 38.0 31.6 32.9 50.2 41.0 41.3Logisticall 34.4 27.6 29.1 39.8 41.7 37.8Logisticold 39.3 31.2 33.5 45.4 40.9 39.9Bayesall 20.8 33.2 24.5 27.4 46.0 31.7Bayesold 20.3 34.9 24.6 26.4 45.4 30.9Examining the results it does not appear that the new features improve perfor-mance.
Whereas on the development set the new features add 1.2?1.5 F1 points for allSVM classifiers, on the test set using the new features decreases performance for thelinear and square classifiers.
This shows that even if there is some slight increase inperformance when using SVM on the development set, it is masked by the varianceadded in the process of parameter tuning.
In general, including the new features doesnot yield substantial differences in performance.Secondly, the SVM classifiers perform better than the logistic and naive Bayes clas-sifiers.
Using the more complex square and Gaussian kernels does not seem justified,however, as the differences between the various kernels are negligible.
Therefore, in ouranalysis we will use a linear kernel SVM classifier.Last, we note that although we use supervised learning rather than distant super-vision, the results we get are slightly lower than those presented in Section 5.
This isprobably due to the fact that our manually annotated data set is rather small.
Nev-ertheless, this shows that the quality of the distant supervision training set generatedautomatically from WordNet is reasonable.Next, we perform analysis of the different features of the classifier to better under-stand the reasons for the negative result obtained.6.3 Feature AnalysisWe saw that the new features slightly improved performance for SVM classifiers onthe development set, although no clear improvement was witnessed on the test set.To further check whether the new features carry useful information we measured thetraining set accuracy for each of the 12 training sets (leaving out each time one test-set graph).
Using the new features improved the average training set accuracy from71.6 to 72.3.
More importantly, it improved performance consistently in all 12 trainingsets by 0.4?1.2 points.
This strengthens our belief that the new features do carry acertain amount of information, but this information is too sparse to affect the overall103Computational Linguistics Volume 38, Number 1performance of the algorithm.
In addition, notice that the absolute accuracy on thetraining set is low?72.3.
This shows that separating entailment from non-entailmentusing the current set of features is challenging.Next, we would like to perform analysis on each of the features.
First, we performan ablation test over the features by omitting each one of them and re-training theclassifier Linearall.
In Table 14, the columns ablation F1 and ?
show the F1 obtained andthe difference in performance from the Linearall classifier, which scored 40.3 F1 points.Results show that there is no ?bad?
feature that deteriorates performance.
For almost allfeatures ablation causes a decrease in performance, although this decrease is relativelysmall.
There are only four features for which ablation decreases performance by morethan one point: three distributional similarity features, but also the new hypernymfeature.The next three columns in the table describe the precision and recall of the newboolean features.
The column Feature type indicates whether we expect a feature toindicate entailment or non-entailment and the columns Prec.
and Recall specify theTable 14Results of feature analysis.
The second column denotes the proportion of manually annotatedexamples for which the feature value is non-zero.
A detailed explanation of the other columns isprovided in the body of the article.Feature name % Ablation F1 ?
Feature type Prec.
Recall Classification F1h-b-B-pCt 8.2 39.3 ?1 14.9h-b-B-pC 6.9 39.5 ?0.8 33.2h-b-B-Ct 1.6 40.3 0 15.4h-b-B-C 1.6 40.5 0.2 11.2h-b-L-pCt 23.6 38.3 ?2.0 37.0h-b-L-pC 21.4 39.4 ?0.9 35.2h-b-L-Ct 9.7 40.1 ?0.2 27.3h-b-L-C 8.1 39.7 ?0.6 14.1h-u-B-Ct 1.0 39.4 ?0.9 10.9h-u-B-C 1.1 39.8 ?0.5 12.6h-u-L-Ct 6.1 39.8 ?0.5 18.5h-u-L-C 6.3 39.2 ?1.1 19.3R-b-L-l 22.5 40.1 ?0.2 26.7R-u-L-l 8.3 39.4 ?0.9 23.2R-u-B-l 1.9 39.8 ?0.5 16.7Lin & Pantel 8.8 38.7 ?1.6 23.0Hyper.
1.1 38.7 ?1.6 + 37.1 4.9 9.7Syno.
0.9 40.3 0 + 43.1 4.5 15.8Co-hypo.
2.8 40.1 ?0.2 ?
82.0 2.5 17.9WN ant.
0.06 39.8 ?0.5 ?
75.0 0.05 1.2VO ant.
0.2 40.1 ?0.2 ?
96.0 0.2 2.2WN ant.
2.
0.2 39.4 ?0.9 ?
59.1 0.1 2.7VO ant.
2 0.7 40.2 ?0.1 ?
98.6 0.7 2.2Derivation 0.7 39.5 ?0.8 + 47.4 4.1 10.2Entailment 0.2 39.7 ?0.6 + 15.0 0.3 1.2FRED 0.08 39.7 ?0.6 + 77.8 0.8 3.2NomLex 0.07 39.8 ?0.5 + 75.0 0.7 3.3VO strong.
1 39.4 ?0.9 + 34.6 4 6.9VO simil.
1.8 39.4 ?0.9 + 28.8 6.1 12.5Positive 2.7 39.8 ?0.5 + 36.7 11.8BAP 4.7 40.1 ?0.2 13.3Edit 100 39.9 ?0.4 15.5104Berant et al Learning Entailment Relations by Global Graph Structure Optimizationprecision and recall of that feature.
For example, the feature FRED is a positive featurethat we expect to support entailment, and indeed 77.8% of the gold-standard examplesfor which it is turned on are positive examples.
It is turned on only in 0.8% of thepositive examples, however.
Similarly, VO ant.
is a negative feature that we expect tosupport non-entailment, and indeed 96% of the gold-standard examples for which it ison are negative examples, but it is turned on in only 0.2% of the negative examples.The precision results are quite reasonable: For most positive features the precision iswell over the proportion of positive examples in the gold standard, which is about10% (except for the Entailment feature whose precision is only 15%).
For the negativefeatures it seems that the precision of VerbOcean features is very high (though they aresparse), and the precision of WordNet antonyms and co-hyponyms is lower.
Lookingat the recall we can see that the coverage of the boolean features is low.The last column in the table describes results of training the classifier with a singlefeature.
For each feature we train a linear kernel SVM, tune the sparsity parameter onthe development set, and measure F1 over the test set.
Naturally, classifiers that aretrained on sparse features yield low performance.This column allows us once again (cf.
Table 8) to examine the original distributionalsimilarity features.
There are three distributional similarity features that achieve F1 ofmore than 30 points, and all three represent features using pairs of argument instan-tiations rather than treat each argument separately, as we have already witnessed inSection 5.Note also that the feature h-b-L-pCt, which uses binary templates, the Lin similaritymeasure, and features that are pairs of CUI tuples, is the best feature both in terms of theablation test and when it is used as a single feature for the classifier.
The result obtainedby this feature is only 3.3 points lower than that obtained when using the entire featureset.
We believe this is for two reasons: First, the 16 distributional similarity features arecorrelated with one another and thus using all of them does not boost performancesubstantially.
For example, the Pearson correlation coefficients between the featuresh-b-B-pCt, h-b-B-Ct, h-b-L-pCt, h-b-L-Ct, h-u-B-Ct, and h-u-L-Ct (all utilize CUI tuples) andh-b-B-pC, h-b-B-C, h-b-L-pC, h-b-L-C, h-u-B-C, and h-u-L-C (all use CUIs), respectively, areover 0.9.
The second reason for gaining only 3.3 points by the remaining features is that,as discussed, the new set of features is relatively sparse.To sum up, we suggest several hypotheses that explain our results and analysis: The new features are too sparse to substantially improve the performanceof the local entailment classifier in our data set.
This perhaps can beattributed to the nature of our domain-specific health-care corpus.
In thefuture, we would like to examine the sparsity of these features in a generaldomain. Looking at the training set accuracy, ablations, and precision of the newfeatures, it seems that the behavior of most of them is reasonable.
Thus,it is possible that in a different learning scheme that does not use theresources as features the information they provide may become beneficial.For example, in a simple ?back-off?
approach one can use rules fromprecise resources to determine entailment, and apply a classifier onlywhen no precise resource contains a relevant rule. In our corpus representing distributional similarity features withpairs of argument instantiations is better than treating each argumentindependently.105Computational Linguistics Volume 38, Number 1 Given the current training set accuracy and the sparsity of the newfeatures, it is important to develop methods that gather large-scaleinformation that is orthogonal to distributional similarity.
In our opinion,the most promising direction for acquiring such rich information is bymethods that look at co-occurrence of predicates or templates on the Web(Chklovski and Pantel 2004; Pekar 2008).7.
Conclusions and Future WorkThis article presented a global optimization algorithm for learning entailment rulesbetween predicates, represented as propositional templates.
Most previous work onlearning entailment rules between predicates focused on local learning methods, whichconsider each pair of predicates in isolation.
To the best of our knowledge, this is themost comprehensive attempt to date to exploit global interactions between predicatesfor improving the set of learned entailment rules.We modeled the problem as a graph learning problem, and searched for the bestgraph under a global transitivity constraint.
Two objective functions were defined forthe optimization procedure, one score-based and the other probabilistic, and we haveshown that under certain conditions (specified in Appendix A) the score-based functioncan be interpreted probabilistically.
This allowed us to use both margin as well asprobabilistic classifiers for the underlying entailment classifier.
We solved the optimiza-tion problem using Integer Linear Programming, which provides an optimal solution(compared to the greedy algorithm suggested by Snow, Jurafsky, and Ng [2006]), anddemonstrated empirically that this method outperforms local algorithms as well asa state-of-the-art greedy optimization algorithm on the graph learning task.
We alsoanalyzed quantitatively and qualitatively the reasons for the improved performance ofour global algorithm and performed detailed error analysis.
Last, we experimented withvarious entailment classifiers that utilize different sets of features from many knowledgebases.The experiments and analysis performed indicate that the current performance ofthe local entailment classifier needs to be improved.
We believe that the most promisingdirection for improving the local classifier is to use methods that look for co-occurrenceof predicates in sentences or documents on the Web, because these methods excel atidentifying specific semantic relations.
It is also possible to use other sources of infor-mation such as lexicographic resources, although this probably will require a learningscheme that is robust to the relatively low coverage of these resources.
Increasing thesize of the training corpus is also an important direction for improving the entailmentclassifier.Another important direction for future work is to apply our algorithm to graphsthat are larger by a few orders of magnitude than the focused entailment graphs dealtwith in this article.
This will introduce a challenge to our current optimization algorithmdue to complexity issues, as our ILP contains O(|V|3) constraints.
In addition, this willrequire careful handling of predicate ambiguity, which interferes with the transitivityof entailment and will become a pertinent issue in large graphs.
Some first steps in thisdirection have already been carried out (Berant, Dagan, and Goldberger 2011).In addition, our graphs currently contain a single type of edge, namely, the entail-ment relation.
We would like to model more types of edges in the graph, representingadditional semantic relations such as co-hyponymy, and to explicitly describe the inter-actions between the various types of edges, aiming to further improve the quality of thelearned entailment rules.106Berant et al Learning Entailment Relations by Global Graph Structure OptimizationFigure 8A hierarchical summary of propositions involving nausea as an argument, such as headache isrelated to nausea, acupuncture helps with nausea, and Lorazepam treats nausea.Last, in Section 3.1 we mentioned that by merging strongly connected componentsin entailment graphs, hierarchies of predicates can be generated (recall Figure 1).
Asproposed by Berant, Dagan, and Goldberger (2010), we believe that these hierarchies canbe useful not only in the context of semantic inference applications, but also in the fieldof faceted search and hierarchical text exploration (Stoica, Hearst, and Richardson 2007).Figure 8 exemplifies how a set of propositions can be presented to a user according tothe hierarchy of predicates shown in Figure 1.
In the field of faceted search, informationis presented using a number of hierarchies, corresponding to different facets or dimen-sions of the data.
One can easily use the hierarchy of predicates learned by our algorithmas an additional facet in the context of a text-exploration application.
In future work,we intend to implement this application and perform user experiments to test whetheradding this hierarchy facilitates exploration of textual information.Appendix A: Derivation of the Probabilistic Objective FunctionIn this section we provide a full derivation for the probabilistic objective functiongiven in Section 4.2.2.
Given two nodes u and v from a set of nodes V, we denote byIuv = 1 the event that u entails v, by Fuv the feature vector representing the orderedpair (u, v), and by F the set of feature vectors over all ordered pairs of nodes, that is,F = ?u=vFuv.
We wish to learn a set of edges E, such that the posterior probability P(G|F)is maximized, where G = (V, E).
We assume that we have a ?local?
model estimating theedge posterior probability Puv = P(Iuv = 1|Fuv).
Because this model was trained over abalanced training set, the prior for the event that u entails v under the model is uniform:P(Iuv = 1) = P(Iuv = 0) =12 .
Using Bayes?s rule we get:P(Iuv = 1|Fuv) =P(Iuv = 1)P(Fuv)?
P(Fuv|Iuv = 1) = a ?
P(Fuv|Iuv = 1) (A.1)P(Iuv = 0|Fuv) =P(Iuv = 0)P(Fuv)?
P(Fuv|Iuv = 0) = a ?
P(Fuv|Iuv = 0) (A.2)107Computational Linguistics Volume 38, Number 1where a = 12?P(Fuv ) is a constant with respect to any graph.
Thus, we conclude thatP(Iuv|Fuv) = a ?
P(Fuv|Iuv).
Next, we make three independence assumptions (the first twoare following Snow, Jurafsky, and Ng [2006]):P(F|G) =?u=vP(Fuv|G) (A.3)P(Fuv|G) = P(Fuv|Iuv) (A.4)P(G) =?u=vP(Iuv) (A.5)Assumption A.3 states that each feature vector is independent from other featurevectors given the graph.
Assumption A.4 states that the features Fuv for the pair (u, v)are generated by a distribution depending only on whether entailment holds for (u, v).Last, Assumption A.5 states that edges are independent and the prior probability of agraph is a product of the prior probabilities of the edges.
Using these assumptions andequations A.1 and A.2, we can now express the posterior P(G|F):P(G|F) ?
P(G) ?
P(F|G) (A.6)=?u=v[P(Iuv) ?
P(Fuv|Iuv)] (A.7)=?u=vP(Iuv) ?P(Iuv|Fuv)a (A.8)?
?u=vP(Iuv) ?
Puv (A.9)=?
(u,v)?EP(Iuv = 1) ?
Puv ??
(u,v)/?EP(Iuv = 0) ?
(1 ?
Puv) (A.10)Note that under the ?local model?
the prior for an edge in the graph was uniform,because the model was trained over a balanced training set.
Generally, however, this isnot the case, and thus we introduce an edge prior into the model when formulating theglobal objective function.
Now, we can formulate P(G|F) as a linear function:G?
= argmaxG?
(u,v)?EP(Iuv = 1) ?
Puv ??
(u,v)/?EP(Iuv = 0) ?
(1 ?
Puv) (A.11)= argmaxG?
(u,v)?Elog(Puv ?
P(Iuv = 1)) +?
(u,v)/?Elog[(1 ?
Puv) ?
P(Iuv = 0)] (A.12)= argmaxG?u=v(Iuv ?
log(Puv ?
P(Iuv = 1)) + (1 ?
Iuv) ?
log[(1 ?
Puv) ?
P(Iuv = 0)])(A.13)= argmaxG?u=v(logPuv ?
P(Iuv = 1)(1 ?
Puv) ?
P(Iuv = 0)?
Iuv + (1 ?
Puv) ?
P(Iuv = 0))(A.14)108Berant et al Learning Entailment Relations by Global Graph Structure Optimization= argmaxG?u=vlogPuv(1 ?
Puv)?
Iuv + log?
?
|E| (A.15)In the last transition we omit?u=v(1 ?
Puv) ?
P(Iuv = 0), which is a constant withrespect to the graph and denote the prior odds ratio by ?
= P(Iuv=1)P(Iuv=0) .
This leads to thefinal formulation described in Section 4.2.2.AcknowledgmentsWe would like to thank Roy Bar-Haim, DavidCarmel, and the anonymous reviewers fortheir useful comments.
We also thank DafnaBerant and the nine students who preparedthe gold-standard data set.
This work wasdeveloped under the collaboration ofFBK-irst/University of Haifa and waspartially supported by the Israel ScienceFoundation grant 1112/08.
The first author isgrateful to the Azrieli Foundation for theaward of an Azrieli Fellowship, and hascarried out this research in partial fulfilmentof the requirements for the Ph.D. degree.ReferencesAlthaus, Ernst, Nikiforos Karamanis, andAlexander Koller.
2004.
Computing locallycoherent discourses.
In Proceedings of theACL, pages 399?406, Barcelona.Baker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The Berkeley framenetproject.
In Proceedings of COLING-ACL,pages 86?90, Montreal.Bar-Haim, Roy, Ido Dagan, Iddo Greental,and Eyal Shnarch.
2007.
Semantic inferenceat the lexical-syntactic level.
In Proceedingsof AAAI, pages 871?876, Vancouver.Ben Aharon, Roni, Idan Szpektor, and IdoDagan.
2010.
Generating entailment rulesfrom framenet.
In Proceedings of ACL,pages 241?246, Uppsala.Bentivogli, Luisa, Ido Dagan, Hoa TrangDang, Danilo Giampiccolo, and BernardeMagnini.
2009.
The fifth Pascal recognizingtextual entailment challenge.
In Proceedingsof TAC-09, pages 14?24, Gaithersburg, MD.Berant, Jonathan, Ido Dagan, and JacobGoldberger.
2010.
Global learning offocused entailment graphs.
In Proceedingsof ACL, pages 1220?1229, Uppsala.Berant, Jonathan, Ido Dagan, and JacobGoldberger.
2011.
Global learning of typedentailment rules.
In Proceedings of ACL,pages 610?619, Portland, OR.Bhagat, Rahul, Patrick Pantel, and EduardHovy.
2007.
LEDIR: An unsupervisedalgorithm for learning directionality ofinference rules.
In Proceedings ofEMNLP-CoNLL, pages 161?170, Prague.Budanitsky, Alexander and Graeme Hirst.2006.
Evaluating Wordnet-based measuresof lexical semantic relatedness.Computational Linguistics, 32(1):13?47.Chklovski, Timothy and Patrick Pantel.2004.
VerbOcean: Mining the Web forfine-grained semantic verb relations.In Proceedings of EMNLP, pages 33?40,Barcelona.Clark, Peter, William Murray, JohnThompson, Phil Harrison, Jerry Hobbs,and Christiane Fellbaum.
2007.
On the roleof lexical and world knowledge in RTE3.In Proceedings of the Workshop on TextualEntailment and Paraphrasing, pages 54?59,Prague.Clarke, James and Mirella Lapata.
2008.Global inference for sentence compression:An integer linear programming approach.Journal of Artificial Intelligence Research,31:273?381.Cohen, William, Pradeep Ravikumar,and Stephen E. Fienberg.
2003.
Acomparison of string distance metrics forname-matching tasks.
In Proceedings ofIIWeb, pages 73?78, Acapulco.Connor, Michael and Dan Roth.
2007.Context sensitive paraphrasing witha single unsupervised classifier.In Proceedings of ECML, pages 104?115,Warsaw.Coyne, Bob and Owen Rambow.
2009.Lexpar: A freely available Englishparaphrase lexicon automatically extractedfrom Framenet.
In Proceedings of the IEEEInternational Conference on SemanticComputing, pages 53?58, Berkeley, CA.Dagan, Ido, Bill Dolan, Bernardo Magnini,and Dan Roth.
2009.
Recognizing textualentailment: Rational, evaluation andapproaches.
Natural Language Engineering,15(4):1?17.Do, Quang and Dan Roth.
2010.
Constraintsbased taxonomic relation classification.
InProceedings of EMNLP, pages 1099?1109,Cambridge, MA.Fellbaum, Christiane.
1998a.
A semanticnetwork of English: The mother of all109Computational Linguistics Volume 38, Number 1wordNets.
Natural Language Engineering,32:209?220.Fellbaum, Christiane, editor.
1998b.
WordNet:An Electronic Lexical Database (Language,Speech, and Communication).
The MIT Press,Cambridge, MA.Finkel, Jenny R. and Christopher D.Manning.
2008.
Enforcing transitivity incoreference resolution.
In Proceedings ofACL-08: HLT, Short Papers, pages 45?48,Columbus, OH.Habash, Nizar and Bonnie Dorr.
2003.A categorial variation database forEnglish.
In Proceedings of the NAACL,pages 17?23, Edmonton.Hall, Mark, Eibe Frank, Geoffrey Holmes,Bernhard Pfahringer, Peter Reutemann,and Ian H. Witten.
2009.
The WEKA datamining software: An update.
SIGKDDExplorations, 11(1):10?18.Harmeling, Stefan.
2009.
Inferring textualentailment with a probabilistically soundcalculus.
Natural Language Engineering,15(4):459?477.Harris, Zellig.
1954.
Distributional structure.Word, 10(23):146?162.Joachims, Thorsten.
2005.
A support vectormethod for multivariate performancemeasures.
In Proceedings of ICML,pages 377?384, Bonn.Kingsbury, Paul, Martha Palmer, andMitch Marcus.
2002.
Adding semanticannotation to the Penn TreeBank.In Proceedings of HLT, pages 252?256,San Diego, CA.Kipper, Karin, Hoa T. Dang, and MarthaPalmer.
2000.
Class-based constructionof a verb lexicon.
In Proceedings of AAAI,pages 691?696, Austin, TX.Kotlerman, Lili, Ido Dagan, Idan Szpektor,and Maayan Zhitomirsky-Geffet.
2010.Directional distributional similarity forlexical inference.
Natural LanguageEngineering, 16:359?389.Lin, Dekang.
1998a.
Automatic retrievaland clustering of similar words.In Proceedings of COLING-ACL,pages 768?774, Montreal.Lin, Dekang.
1998b.
Dependency-basedevaluation of Minipar.
In Proceedings of theWorkshop on Evaluation of Parsing Systems atLREC, pages 317?329, Granada.Lin, Dekang and Patrick Pantel.
2001.Discovery of inference rules for questionanswering.
Natural Language Engineering,7(4):343?360.Macleod, Catherine, Ralph Grishman,Adam Meyers, Leslie Barrett, andRuth Reeves.
1998.
Nomlex: A lexicon ofnominalizations.
In Proceedings of Euralex,pages 187?193, Lieg`e.Martins, Andre, Noah Smith, and Eric Xing.2009.
Concise integer linear programmingformulations for dependency parsing.In Proceedings of ACL, pages 342?350,Singapore.Meyers, Adam, Ruth Reeves, CatherineMacleod, Rachel Szekeley, VeronikaZielinska, and Brian Young.
2004.The cross-breeding of dictionaries.
InProceedings of LREC, pages 1095?1098,Lisbon.Mirkin, Shachar, Ido Dagan, and MaayanGefet.
2006.
Integrating pattern-basedand distributional similarity methodsfor lexical entailment acquisition.In Proceedings of COLING-ACL,pages 579?586, Sydney.Nikulin, Vladimir.
2008.
Classification ofimbalanced data with random sets andmean-variance filtering.
InternationalJournal of Data Warehousing and Mining,4(2):63?78.Pekar, Viktor.
2008.
Discovery of evententailment knowledge from text corpora.Computer Speech & Language, 22(1):1?16.Raina, Rajat, Andrew Ng, and ChristopherManning.
2005.
Robust textual inferencevia learning and abductive reasoning.In Proceedings of AAAI, pages 1099?1105,Pittsburgh, PA.Riedel, Sebastian and James Clarke.
2006.Incremental integer linear programmingfor non-projective dependency parsing.In Proceedings of EMNLP, pages 129?137,Sydney.Roth, Dan and Wen-tau Yih.
2004.
A linearprogramming formulation for globalinference in natural language tasks.In Proceedings of CoNLL, pages 1?8,Boston, MA.Schoenmackers, Stefan, Jesse Davis,Oren Etzioni, and Daniel S. Weld.2010.
Learning first-order hornclauses from Web text.
In Proceedingsof EMNLP, pages 1088?1098,Cambridge, MA.Sekine, Satoshi.
2005.
Automatic paraphrasediscovery based on context and keywordsbetween NE pairs.
In Proceedings of IWP,pages 80?87, Jeju Island.Siegel, Sidney and N. John Castellan.
1988.Non-parametric Statistics for the BehavioralSciences.
McGraw-Hill, New-York.Smith, Noah and Jason Eisner.
2005.Contrastive estimation: Training log-linearmodels on unlabeled data.
In Proceedingsof ACL, pages 354?362, Ann Arbor, MI.110Berant et al Learning Entailment Relations by Global Graph Structure OptimizationSnow, Rion, Daniel Jurafsky, and Andrew Y.Ng.
2004.
Learning syntactic patterns forautomatic hypernym discovery.
InProceedings of NIPS, pages 1297?1304,Vancouver.Snow, Rion, Daniel Jurafsky, and Andrew Y.Ng.
2006.
Semantic taxonomy inductionfrom heterogenous evidence.
InProceedings of ACL, pages 801?808, Prague.Stoica, Emilia, Marti Hearst, and MeganRichardson.
2007.
Automating creation ofhierarchical faceted metadata structures.
InProceedings of NAACL-HLT, pages 244?251,Rochester, NY.Szpektor, Idan and Ido Dagan.
2007.Learning canonical forms of entailmentrules.
In Proceedings of RANLP, pages 1?8,Borovetz.Szpektor, Idan and Ido Dagan.
2008.Learning entailment rules for unarytemplates.
In Proceedings of COLING,pages 849?856, Manchester.Szpektor, Idan and Ido Dagan.
2009.Augmenting Wordnet-based inferencewith argument mapping.
In Proceedingsof TextInfer, pages 27?35, Singapore.Szpektor, Idan, Hristo Tanev, Ido Dagan,and Bonaventura Coppola.
2004.
ScalingWeb-based acquisition of entailmentrelations.
In Proceedings of EMNLP,pages 41?48, Barcelona.Van Hulse, Jason, Taghi Khoshgoftaar, andAmri Napolitano.
2007.
Experimentalperspectives on learning from imbalanceddata.
In Proceedings of ICML,pages 935?942, Corvallis, OR.Vanderbei, Robert.
2008.
Linear Programming:Foundations and Extensions.
Springer,New-York.Weeds, Julie and David Weir.
2003.
A generalframework for distributional similarity.In Proceedings of EMNLP, pages 81?88,Sapporo.Wilcoxon, Frank.
1945.
Individualcomparisons by ranking methods.Biometrics Bulletin, 1:80?83.Yannakakis, Mihalis.
1978.
Node-andedge-deletion NP-complete problems.
InSTOC ?78: Proceedings of the Tenth AnnualACM Symposium on Theory of Computing,pages 253?264, New York, NY.Yates, Alexander and Oren Etzioni.
2009.Unsupervised methods for determiningobject and relation synonyms on the web.Journal of Artificial Intelligence Research,34:255?296.111
