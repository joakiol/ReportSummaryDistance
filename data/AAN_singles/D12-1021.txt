Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 223?232, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Bayesian Model for Learning SCFGs with Discontiguous RulesAbby LevenbergDept.
of Computer ScienceUniversity of Oxfordablev@cs.ox.ac.ukChris DyerSchool of Computer ScienceCarnegie Mellon Univeristycdyer@cs.cmu.eduPhil BlunsomDept.
of Computer ScienceUniversity of Oxfordpblunsom@cs.ox.ac.ukAbstractWe describe a nonparametric modeland corresponding inference algorithmfor learning Synchronous Context FreeGrammar derivations for parallel text.
Themodel employs a Pitman-Yor Process priorwhich uses a novel base distribution oversynchronous grammar rules.
Through bothsynthetic grammar induction and statisticalmachine translation experiments, we showthat our model learns complex translationalcorrespondences?
including discontiguous,many-to-many alignments?and producescompetitive translation results.
Further,inference is efficient and we present results onsignificantly larger corpora than prior work.1 IntroductionIn the twenty years since Brown et al1992) pio-neered the first word-based statistical machine trans-lation (SMT) models substantially more expressivemodels of translational equivalence have been devel-oped.
The prevalence of complex phrasal, discon-tiguous, and non-monotonic translation phenomenain real-world applications of machine translation hasdriven the development of hierarchical and syntac-tic models based on synchronous context-free gram-mars (SCFGs).
Such models are now widely used intranslation and represent the state-of-the-art in mostlanguage pairs (Galley et al2004; Chiang, 2007).However, while the models used for translation haveevolved, the way in which they are learnt has not:na?
?ve word-based models are still used to infer trans-lational correspondences from parallel corpora.In this work we bring the learning of the minimalunits of translation in step with the representationalpower of modern translation models.
We present anonparametric Bayesian model of translation basedon SCFGs, and we use its posterior distribution toinfer synchronous derivations for a parallel corpususing a novel Gibbs sampler.
Our model is ableto: 1) directly model many-to-many alignments,thereby capturing non-compositional and idiomatictranslations; 2) align discontiguous phrases in boththe source and target languages; 3) have no restric-tions on the length of a rule, the number of nonter-minal symbols per rule, or their configuration.Learning synchronous grammars is hard due tothe high polynomial complexity of dynamic pro-gramming and the exponential space of possiblerules.
As such most prior work for learning SCFGshas relied on inference algorithms that were heuristi-cally constrained or biased by word-based alignmentmodels and small experiments (Wu, 1997; Zhang etal., 2008; Blunsom et al2009; Neubig et al2011).In contrast to these previous attempts, our SCFGmodel scales to large datasets (over 1.3M sentencepairs) without imposing restrictions on the form ofthe grammar rules or otherwise constraining the setof learnable rules (e.g., with a word alignment).We validate our sampler by demonstrating itsability to recover grammars used to generatesynthetic datasets.
We then evaluate our model byinducing word alignments for SMT experimentsin several typologically diverse language pairs andacross a range of corpora sizes.
Our results attest toour model?s ability to learn synchronous grammarsencoding complex translation phenomena.2232 Prior WorkThe goal of directly inducing phrasal translationmodels from parallel corpora has received a lot ofattention in the NLP and SMT literature.
Marcuand Wong (2002) presented an ambitious maximumlikelihood model and EM inference algorithm forlearning phrasal translation representations.
Thefirst issue this model faced was a massive parameterspace and intractable inference.
However a moresubtle issue is that likelihood based models of thisform suffer from a degenerate solution, resultingin the model learning whole sentences as phrasesrather than minimal units of translation.
DeNeroet al2008) recognised this problem and proposeda nonparametric Bayesian prior for contiguousphrases.
This had the dual benefits of biasing themodel towards learning minimal translation units,and integrating out the parameters such that a muchsmaller set of statistics would suffice for inferencewith a Gibbs sampler.
However this work fell shortby not evaluating the model independently, insteadonly presenting results in which it was combinedwith a standard word-alignment initialisation, thusleaving open the question of its efficacy.The fact that flat phrasal models lack a structuredapproach to reordering has led many researchers topursue SCFG induction instead (Wu, 1997; Cherryand Lin, 2007; Zhang et al2008; Blunsom etal., 2009).
The asymptotic time complexity ofthe inside algorithm for even the simplest SCFGmodels is O(|s|3|t|3), too high to be practical formost real translation data.
A popular solution tothis problem is to heuristically restrict inferenceto derivations which agree with an independentalignment model (Cherry and Lin, 2007; Zhang etal., 2008).
However this may have the unintendedeffect of biasing the model back towards the initialalignments that they attempt to improve upon.More recently Neubig et al2011) reported anovel Bayesian model for phrasal alignment andextraction that was able to model phrases of multiplegranularities via a synchronous Adaptor Grammar.However this model suffered from the commonproblem of intractable inference and results werepresented for a very small number of samples froma heuristically pruned beam, making interpretingthe results difficult.Blunsom et al2009) presented an approachsimilar to ours that implemented a Gibbs samplerfor a nonparametric Bayesian model of ITG.
Whilethat work managed to scale to a non-trivially sizedcorpus, like other works it relied on a state-of-the-artword alignment model for initialisation.
Our modelgoes further by allowing discontiguous phrasaltranslation units.
Surprisingly, the freedomthat this extra power affords allows the Gibbssampler we propose to mix more quickly, allowingstate-of-the-art results from a simple initialiser.3 ModelWe use a nonparametric generative model based onthe 2-parameter Pitman-Yor process (PYP) (Pitmanand Yor, 1997), a generalisation of the Dirichlet Pro-cess, which has been used for various NLP modelingtasks with state-of-the-art results such as languagemodeling, word segmentation, text compression andpart of speech induction (Teh, 2006; Goldwater etal., 2006; Wood et al2011; Blunsom and Cohn,2011).
In this section we first provide a brief defi-nition of the SCFG formalism and then describe ourPYP prior for them.3.1 Synchronous Context-Free GrammarAn synchronous context-free grammar (SCFG) is a5-tuple ?
?,?, V, S,R?
that generalises context-freegrammar to generate strings concurrently in two lan-guages (Lewis and Stearns, 1968).
?
is a finite set ofsource language terminal symbols, ?
is a finite setof target language terminal symbols, V is a set ofnonterminal symbols, with a designated start sym-bol S, and R is a set of synchronous rewrite rules.A string pair is generated by starting with the pair?S1 | S1?
and recursively applying rewrite rules ofthe form X ?
?s, t, a?
where the left hand side(LHS) X is a nonterminal in V , s is a string in(?
?
V )?, t is a string in (?
?
V )?
and a specifiesa one-to-one mapping (bijection) between nontermi-nal symbols in s and t. The following are examples:1VP ?
?
schlage NP1 NP2 vor | suggest NP2 to NP1 ?NP ?
?
die Kommission | the commission ?1The nonterminal alignment a is indicated through sub-scripts on the nonterminals.224In a probabilistic SCFG, rules are associated withprobabilities such that the probabilities of allrewrites of a particular LHS category sum to 1.Translation with SCFGs is carried out by parsingthe source language with the monolingual sourcelanguage projection of the grammar (using standardmonolingual parsing algorithms), which inducesa parallel tree structure and translation in thetarget language (Chiang, 2007).
Alignment orsynchronous parsing is the process of concurrentlyparsing both the source and target sentences,uncovering the derivation or derivations that giverise to a string pair (Wu, 1997; Dyer, 2010).Our goal is to infer the most probable SCFGderivations that explain a corpus of parallel sen-tences, given a nonparametric prior over probabilis-tic SCFGs.
In this work we will consider grammarswith a single nonterminal category X.3.2 Pitman-Yor Process SCFGBefore training we have no way of knowing howmany rules will be needed in our grammar to ade-quately represent the data.
By using the Pitman-Yor process as a prior on the parameters of a syn-chronous grammar we can formulate a model whichprefers smaller numbers of rules that are reusedoften, thereby avoiding degenerate grammars con-sisting of large, overly specific rules.
However, asthe data being fit grows, the model can become morecomplex.
The PYP is parameterised by a discountparameter d, a strength parameter ?, and the basedistribution G0, which gives the prior probabilityof an event (in our case, events are rules) beforeany observations have occurred.
The discount issubtracted from each positive rule count and damp-ens the rich get richer effect where frequent rulesare given higher probability compared to infrequentones.
The strength parameter controls the variance,or concentration, about the base distribution.In our model, a draw from a PYP is a distributionover SCFG rules with a particular LHS (in fact, it isa distribution over all well-formed rules).
From thisdistribution we can in turn draw individual rules:GX ?
PY(d, ?,G0),X ?
?s, t, a?
?
GX .Although the PYP has no known analytical form,we can marginalise out the GX ?s and reason aboutStep 1: Generate source side length.Step 2: Generate source side configuration ofterminals (and non-terminal placeholders).Step 3: Generate target length.Step 4.
Generate target side configuration ofterminals (and non-terminal placeholders).Step 5.
Generate the words.X < _ _ _  ||| ?
>X < X1 _ X2 ||| ?
>X < X1 _ X2 ||| _ _ _  >X < X1 _ X2 ||| _ X1 X2  >X < X1 ?
X2 ||| you X1 X2  >Figure 1: Example generation of a synchronousgrammar rule in our G0.individual rules directly using the process describedby Teh (2006).
In this process, at time n a rule rnis generated by stochastically deciding whether tomake another copy of a previously generated ruleor to draw a new one from the base distribution, G0.Let ?
= (?1, ?2, .
.
.)
be the sequence of draws fromG0; thus |?| is the total number of draws from G0.
Arule rn corresponds to a selection of a ?k.
Let ckbe a counter indicating the number of times ?k hasbeen selected.
In particular, we set rn to ?k withprobabilityck ?
d?
+ n ,and increment ck, or with probability?
+ d ?
|?|?
+ n ,we draw a new rule from G0, append it to ?, and useit for rn.3.3 Base DistributionThe base distribution G0 for the PYP assigns prob-ability to a rule based our belief about what consti-tutes a good rule independent of observing any of225the data.
We describe a novel generative process forall rules X ?
?s, t, a?
that encodes these beliefs.We describe the generative process generally herein text, and readers may refer to the example in Fig-ure 1.
The process begins by generating the sourcelength (total number of terminal and nonterminalsymbols, written |s|) by drawing from a Poisson dis-tribution with mean 1:|s| ?
Poisson(1) .This assigns high probability to shorter rules,but arbitrarily long rules are possible with a lowprobability.
Then, for every position in s, we decidewhether it will contain a terminal or nonterminalsymbol by repeated, independent draws from aBernoulli distribution.
Since we believe that shorterrules should be relatively more likely to containterminal symbols than longer rules, we define theprobability of a terminal symbol to be ?|s| where0 < ?
< 1 is a hyperparameter.si ?
Bernoulli(?|s|) ?
i ?
[1, |s|] .We next generate the length of the target side ofthe rule.
Let #NT(s) denote the number of nonter-minal symbols we generated in s, i.e., the arity ofthe rule.
Our intuition here is that source and targetlengths should be similar.
However, to ensure thatthe rule is well-formed, t must contain exactly asmany nonterminal symbols as the source does.
Wetherefore draw the number of target terminal sym-bols from a Poisson whose mean is the number ofterminal symbols in the source, plus a small constant?0 to ensure that it is greater than zero:|t| ?
#NT(s) ?
Poisson (|s| ?
#NT(s) + ?0) .We then determine whether each position in t isa terminal or nonterminal symbol by drawing uni-formly from the bag of #NT(s) source nontermi-nals and |t| ?
#NT(s) terminal indicators, with-out replacement.
At this point we have created arule template which indicates how large the rule is,whether each position contains a terminal or non-terminal symbol, and the reordering of the sourcenonterminals a.
To conclude the process we mustselect the terminal types from the source and targetvocabularies.
To do so, we use the following distri-bution:Pterminals(s, t) =PM1?
(s, t) + PM1?
(s, t)2where PM1?
(s, t) (PM1?
(s, t)) first generates thesource (target) terminals from uniform draws fromthe vocabulary, then generates the string in the otherlanguage according to IBM MODEL 1, marginaliz-ing over the alignments (Brown et al1993).4 Gibbs SamplerIn this section we introduce a Gibbs sampler thatenables us to perform posterior inference given acorpus of sentence pairs.
Our innovation is to repre-sent the synchronous derivation of a sentence pair ina hierarchical 4-dimensional binary alignment grid,with elements z[s,t,u,v] ?
{0, 1}.The settings of the grid variables completelydetermine the SCFG rules in the current derivation.A setting of a binary variable z[s,t,u,v] = 1 representsa constituent linking the source span [s, t] and thetarget span [u, v] in the current derivation; variableswith a value of 0 indicate no link between spans[s, t] and [u, v].2 This relationship from our gridrepresentation is illustrated in Figure 2a.Our Gibbs sampler operates over the space of allthe random variables z[s,t,u,v], resampling one at atime.
Changes to a single variable imply that at mosttwo additional rules must be generated, as illustratedin Figure 2b.
The probability of choosing a binarysetting of 0 or 1 for a variable is proportional to theprobability of generating the two derivations underthe model described in the previous section.
Notethat for a given sentence, most of the bispan vari-ables must be set to 0 otherwise they would violatethe strict nesting constraint required for valid SCFGderivations.
We discuss below how to exploit thisfact to limit the number of binary variables that mustbe resampled for each sentence.To be valid, a Gibbs sampler must be ergodic andsatisfy detailed balance.
Ergodicity requires thatthere is non-zero probability that any state in thesampler be reachable from any other state.
Clearly2Our grid representation is the synchronous generalisationof the well-known correspondence between CFG derivationsand Boolean matrices; see Lee (2002) for an overview.226Amna will{mnasucceedkAmyAbhwgyAmnAawiilaaswucleduu(a) An example grid representation of a syn-chronous derivation.
The SCFG rules (annotatedwith their bispans) that correspond to this settingof the grid are:X[0,4,0,3] ??
X[0,1,0,1] X[1,4,1,3] | X[0,1,0,1] X[1,4,1,3] ?X[0,1,0,1] ?
?
{mna | Amna ?X[1,4,1,3] ?
?
kAmyAb hw gy | will succeed ?Amna will{mnasucceedkAmyAbhwgyAmnAawiilaaswucleduu(b) The toggle operator resamples a bispan vari-able (here, z[1,3,2,3], shown in blue) to determinewhether it should be subtracted from the immedi-ately dominating rule (bispan in red) and made intoa child rule in the derivation.
This would requirethe addition of the following two rules:X[1,4,1,3] ?
?
X[1,3,2,3] gy | will X[1,3,2,3]?X[1,3,2,3] ?
?
kAmyAb hw | succeed ?Alternatively, the active bispan variable can be setso it is not a constituent, which would require thesingle rule:X[1,4,1,3] ?
?
kAmyAb hw gy | will succeed ?Figure 2: A single operation of the Gibbs sampler for a binary alignment grid.our operator satisfies this since given any configu-ration of the alignment grid we can use the toggleoperator to flatten the derivation to a single rule andthen break it back down to reach any derivation.Detailed balance requires that the probability oftransitioning between two possible adjacent samplerstates respects their joint probabilities in the station-ary distribution.
One way to ensure this is to makethe order in which bispan variables are visited deter-ministic and independent of the variables?
currentsettings.
Then, the probability of the sampler tar-geting any bispan in the grid is equal regardless ofthe current configuration of the alignment grid.A naive instantiation of this strategy is to visit all|s|2|t|2 bispans in some order.
However, since wewish to be able to draw many samples, this is notcomputationally feasible.
A much more efficientapproach avoids resampling variables that wouldresult in violations without visiting each of themindividually.
However, to ensure detailed balancedis maintained, the order that we resample bispanshas to match the order we would sample them usingany exhaustive approach.
We achieve this by alwayschecking a derivation top-down, from largest tosmallest bispan.
Under this ordering, whether or nota smaller bispan is visited will be independent ofhow the larger ones were resampled.
Furthermore,the set of variables that may be resampled is fixedgiven this ordering.
Therefore, the probability ofsampling any possible bispan in the sentence pair isstill uniform (ensuring detailed balance), while oursampler remains fast.5 EvaluationThe preceding sections have introduced a model,and accompanying inference technique, designed toinduce a posterior distribution over SCFG deriva-tions containing discontiguous and phrasal transla-tion rules.
The evaluation that follows aims to deter-mine our models ability to meet these design goals,and to do so in a range of translation scenarios.In order to validate both the model and the sam-pler?s ability to learn an SCFG we first conduct asynthetic experiment in which the true grammar is227known.
Subsequently we conduct a series of experi-ments on real parallel corpora of increasing sizes toexplore the empirical properties of our model.5.1 Synthetic Data ExperimentsPrior work on SCFG induction for SMT has val-idated modeling claims by reporting BLEU scoreson real translation tasks.
However, the combinationof noisy data and the complexity of SMT pipelinesconspire to obscure whether models actually achievetheir design goals, normally stated in terms of anability to induce SCFGs with particular properties.Here we include a small synthetic data experimentto clearly validate our models ability to learn anSCFG that includes discontiguous and phrasal trans-lation rules with non-monotonic word order.Using the probabilistic SCFG shown in the tophalf of Table 1 we stochastically generated threethousand parallel sentence pairs as training data forour model.
We then ran the Gibbs sampler for fiftyiterations through the data.The bottom half of Table 1 lists the five ruleswith the highest marginal probability estimated bythe sampler.
Encouragingly our model was able torecover a grammar very close to the original.
Evenfor such a small grammar the space of derivationsis enormous and the task of recovering it from adata sample is non-trivial.
The divergence from thetrue probabilities is due to the effect of the priorassigning shorter rules higher probability.
With alarger data sample we would expect the influence ofthe prior in the posterior to diminish.5.2 Machine Translation EvaluationUltimately the efficacy of a model for SCFG induc-tion will be judged on its ability to underpin a state-of-the-art SMT system.
Here we evaluate our modelby applying it to learning word alignments for par-allel corpora from which SMT systems are induced.We train models across a range of corpora sizes andfor language pairs that exhibit the type of complexalignment phenomena that we are interested in mod-eling: Chinese ?
English (ZH-EN), Urdu ?
English(UR-EN) and German ?
English (DE-EN).Data and BaselinesThe UR-EN corpus is the smallest of those used inour experiments and is taken from the NIST 2009GRAMMAR RULE TRUE PROBABILITYX?
?
X1 a X2 |X1 X2 1 ?
0.2X?
?
b c d | 3 2 ?
0.2X?
?
b d | 3 ?
0.2X?
?
d | 3 ?
0.2X?
?
c d | 3 1 ?
0.2SAMPLED RULE SAMPLED PROBABILITYX?
?
d | 3 ?
0.25X?
?
b d | 3 ?
0.24X?
?
c d | 3 1 ?
0.24X?
?
b c d | 3 2 ?
0.211X?
?
X1 a X2 |X1 X2 1 ?
0.012Table 1: Manually created SCFG used to generatesynthetic data, and the five most probable inferredrules by our model.ZH-ENNISTUR-ENNISTDE-ENEUROPARLTRAIN (SRC) 8.6M 1.2M 34MTRAIN (TRG) 9.5M 1.0M 36MDEV (SRC) 22K 18K 26KDEV (TRG) 27K 16K 28KTable 2: Corpora statistics (in words).translation evaluation.3 The ZH-EN data is of amedium scale and comes from the FBIS corpus.The DE-EN pair constitutes the largest corpus andis taken from Europarl, the proceedings of the Euro-pean Parliament (Koehn, 2003).
Statistics for thedata are shown in Table 2.
We measure translationquality via the BLEU score (Papineni et al2001).All translation systems employ a Hierotranslation model during decoding.
Baselineword alignments were obtained by runningGIZA++ in both directions and symmetrizing usingthe grow-diag-final-and heuristic (Och andNey, 2003; Koehn et al2003).
Decoding wasperformed with the cdec decoder (Dyer et al2010) with the synchronous grammar extractedusing the techniques developed by Lopez (2008).All translation systems include a 5-gram languagemodel built from a five hundred million token subset3http://www.itl.nist.gov/iad/mig/tests/mt/2009/228LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFGPAIR SET BASELINE INITIALISATION WEAK M1 INIT.
STRONG HMM INIT.UR-EN MT09 23.1 18.5 23.7 24.0ZH-EN MT03-08 29.4 19.8 28.3 29.8DE-EN EUROPARL 28.4 25.5 27.8 29.2Table 3: Results for the SMT experiments in BLEU .
The baseline is produced using a full GIZA++ run.
TheMODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations.of all the English data made available for the NIST2009 shared task (Graff, 2003).Experimental SetupTo obtain the PYP-SCFG word alignments weran the sampler for five hundred iterations for eachof the language pairs and experimental conditionsdescribed below.
We used the approach of Newmanet al2007) to distribute the sampler across multi-ple threads.
The strength ?
and discount d hyper-parameters of the Pitman-Yor Processes, and the ter-minal penalty ?
(Section 3.3), were inferred usingslice sampling (Neal, 2000).The Gibbs sampler requires an initial set ofderivations from which to commence sampling.
Inour experiments we investigated both weak anda strong initialisations, the former based on wordalignments from IBM Model 1 and the latter onalignments from an HMM model (Vogel et al1996).
For decoding we used the word alignmentsimplied by the derivations in the final sample toextract a Hiero grammar with the same standard setof relative frequency, length, and language modelfeatures used for the baseline.Weak InitialisationOur first translation experiments ascertain thedegree to which our proposed Gibbs samplinginference algorithm is able to learn goodsynchronous derivations for the PYP-SCFG model.A number of prior works on alignment with Gibbssamplers have only evaluated models initialisedwith the more complex GIZA++ alignment models(Blunsom et al2009; DeNero et al2008), as aresult it can be difficult to separate the performanceof the sampler from that of the initialisation.In order to do this, we initialise the samplerPYP-SCFGLANGUAGE PAIR MODEL 1 INIT.
HMM INIT.UR-EN 1.93/2.08 1.45/1.58ZH-EN 3.47/4.28 1.69/2.37DE-EN 4.05/4.77 1.50/2.04Table 4: Average source/target rule lengths in thePYP-SCFG models after the 500th sample for thedifferent initialisations.using just the MODEL 1 distribution used in thePYP-SCFG model?s base distribution.
We denotethis a weak initialisation as no alignment modelsoutside of those included in the PYP-SCFG modelinfluence the resulting word alignments.
TheBLEU scores for translation systems built from thefive hundredth sample are show in the WEAK M1INIT.
column of Table 3.
Additionally we build atranslation system from the MODEL 1 alignmentused to initialise the sampler without using using ourPYP-SCFG model or sampling.
BLEU scores areshown in the MODEL 1 INITIALISATION columnof Table 3.
Firstly it is clear MODEL 1 is indeed aweak initialiser as the resulting translation systemsachieve uniformly low BLEU scores.
In contrast, themodels built from the output of the Gibbs samplerfor the PYP-SCFG model achieve BLEU scorescomparable to those of the MODEL 4 BASELINE.Thus the sampler has moved a good distance fromits initialisation, and done so in a direction thatresults in better synchronous derivations.Strong InitialisationGiven we have established that the sampler canproduce state-of-the-art translation results from a229weak initialisation, it is instructive to investigatewhether initialising the model with a strongalignment system, the GIZA++ HMM (Vogel etal., 1996), leads to further improvements.
ColumnHMM INIT.
of Table 3 shows the results forinitialising with the HMM word alignments andsampling for 500 iterations.
Starting with a strongerinitial sample results in both quicker mixing andbetter translation quality for the same number ofsampling iterations.Table 4 compares the average lengths of the rulesproduced by the sampler with both the strong andweak initialisers.
As the size of the training corporaincreases (UR-EN ?
ZH-EN ?
DE-EN) we see thatthe average size of the rules produced by the weaklyinitialised sampler also increases, while that of thestrongly initialised model stays relatively uniform.Initially both samplers start out with a large num-ber of long rules and as the sampling progressesthe rules are broken down into smaller, more gen-eralisable, pieces.
As such we conclude from thesemetrics that after five hundred samples the stronglyinitialised model has converged to sampling from amode of the distribution while the weakly initialisedmodel converges more slowly and on the longer cor-pora is still travelling towards a mode.
This sug-gests that longer sampling runs, and Gibbs operatorsthat make simultaneous updates to multiple partsof a derivation, would enable the weakly initialisedmodel to obtain better translation results.Grammar AnalysisThe BLEU scores are informative as a measure oftranslation quality but we also explored some of thedifferences in the grammars obtained from the PYP-SCFG model compared to the standard approach.
InFigures 3 and 4 we show some basic statistics ofthe grammars our model produces.
From Figure 3we see that the number of unique rules in the PYP-SCFG grammar decreases steadily as the sampleriterates through the data, so the model is finding anincreasingly sparser distribution with fewer but bet-ter quality rules as sampling progresses.
Note thatthe gradient of the curves appears to be a function ofthe size of the corpus and suggests that the modelbuilt from the large DE-EN corpus would benefitfrom a longer sampling run.
Figure 4 shows the dis-tribution of rules with a given arity as a percentage1401601802002202402602803003203400  20  40  60  80  100uniquegrammarrules inPYPsamplesur-en (* 1k)zh-en (* 3k)de-en (* 10k)Figure 3: Unique grammar rules for each languagepair as a function of the number of samples.
Thenumber of rule types decreases monotonically assampling continues.
Rule counts are displayed bynormalised corpus size (see Table 2).X?
??
| end of ?X?
???
| ninth ?*X?
???
X | charter X ?X?
???
| confidence in ?X?
?????
X | the chinese government X ?X?
???
| are ?X?
??????
X | beijing , X ?*X?
?????
| departments concerned ?X?
???????
X | washington , X ?*X?
????
X1?
X2 , | he X1 X2 , ?
*Table 5: The five highest ZH-EN probability rules inthe Hiero grammar built from the PYP-SCFG thatare not in the baseline Hiero grammar (top), and thetop five rules in the baseline Hiero grammar thatare not in the PYP-SCFG grammar (bottom).
An* indicates a bad translation rule.of the full grammar after the final sampling iteration.The model prior biases the results to shorter rules asthe vast majority of the model probability mass is onrules with zero, one or two nonterminals.Tables 5 and 6 show the most probable rules in theHiero translation system obtained using the PYP-SCFG alignments that are not present in the TMfrom the GIZA++ alignments and visa versa.
Forboth language pairs, four of the top five rules in230X?
?
yh | it is ?X?
?
zmyn | the earth ?X?
?
yhy X | the same X ?X?
?
X1 nhyN X2 gy | X2 not be X1 ?X?
?
X1 gY kh X2 | recommend that X2 X1 ?*X?
?
hwN gY | will ?X?
?
Gyr mlky | international ?*X?
?
X1 *rAye kY X2 | X2 to X1 sources ?*X?
?
nY X1 nhyN kyA X2 | did not X1 X2 ?*X?
?
xAtwn X1 ky X2 | woman X2 the X1?Table 6: Five of the top scoring rules in the UR-ENHiero grammar from sampled PYP-SCFG align-ments (top) versus the baseline UR-EN Hiero gram-mar rules not in the sampled grammar (bottom).
An* indicates a bad translation rule.00.10.20.30.40  1  2 3+%of rulesarityzh-enur-ende-enFigure 4: The percentage of rules with a given arityin the final grammar of the PYP-SCFG model.the PYP-SCFG grammar that are not in the heuris-tically extracted grammar are correct and minimalphrasal units of translation, whereas only two of thetop probability rules in the GIZA++ grammar are ofgood translation quality.6 Conclusion and Further WorkIn this paper we have presented a nonparametricBayesian model for learning SCFGs directlyfrom parallel corpora.
We have also introduceda novel Gibbs sampller that allows for efficientposterior inference.
We show state-of-the-artresults and learn complex translation phenomena,including discontiguous and many-to-manyphrasal alignments, without applying any heuristicrestrictions on the model to make learning tractable.Our evaluation shows that we can use a principledapproach to induce SCFGs designed specificallyto utilize the full power of grammar based SMTinstead of relying on complex word alignmentheuristics with inherent bias.Future work includes the obvious extension tolearning SCFGs that contain multiple nonterminalsinstead of a single nonterminal grammar.
We alsoexpect that expanding our sampler beyond strictbinary sampling may allow us to explore the spaceof hierarchical word alignments more quicklyallowing for faster mixing.
We expect with theseextensions our model of grammar induction mayfurther improve translation output.AcknowledgementsThis work was supported by a grant from Google,Inc.
and EPRSRC grant no.
EP/I010858/1 (Leven-berg and Blunsom), the U. S. Army Research Lab-oratory and U. S. Army Research Office under con-tract/grant no.
W911NF-10-1-0533 (Dyer).ReferencesP.
Blunsom and T. Cohn.
2011.
A hierarchical pitman-yor process hmm for unsupervised part of speechinduction.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 865?874, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.P.
Blunsom, T. Cohn, C. Dyer, and M. Osborne.
2009.A gibbs sampler for phrasal synchronous grammarinduction.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 782?790, Suntec, Singa-pore, August.
Association for Computational Linguis-tics.P.
F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra,and J. C. Lai.
1992.
An estimate of an upper boundfor the entropy of english.
Computational Linguistics,18(1):31?40.P.
F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: parameter estimation.
Computational Lin-guistics, 19(2):263?311.C.
Cherry and D. Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.231In Proceedings of SSST, NAACL-HLT 2007 / AMTAWorkshop on Syntax and Structure in Statistical Trans-lation, pages 17?24, Rochester, New York, April.Association for Computational Linguistics.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.J.
DeNero, A.
Bouchard-Co?te?, and D. Klein.
2008.
Sam-pling alignment structure under a Bayesian translationmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 314?323, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,P.
Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proceedings of the ACL 2010 SystemDemonstrations, ACLDemos ?10, pages 7?12.C.
Dyer.
2010.
Two monolingual parses are better thanone (synchronous parse).
In Proc.
of NAACL.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In D. M. Susan Dumaisand S. Roukos, editors, HLT-NAACL 2004: MainProceedings, pages 273?280, Boston, Massachusetts,USA, May 2 - May 7.
Association for ComputationalLinguistics.S.
Goldwater, T. L. Griffiths, and M. Johnson.
2006.Contextual dependencies in unsupervised word seg-mentation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for ComputationalLinguistics, Syndney, Australia.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium (LDC-2003T05).P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In NAACL ?03: Proceedingsof the 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 48?54, Morris-town, NJ, USA.
Association for Computational Lin-guistics.P.
Koehn.
2003.
Europarl: A multilingual corpus forevaluation of machine translation.L.
Lee.
2002.
Fast context-free grammar parsingrequires fast Boolean matrix multiplication.
Journalof the ACM, 49(1):1?15.P.
M. Lewis, II and R. E. Stearns.
1968.
Syntax-directedtransduction.
J. ACM, 15:465?488, July.A.
Lopez.
2008.
Machine Translation by Pattern Match-ing.
Ph.D. thesis, University of Maryland.D.
Marcu and D. Wong.
2002.
A phrase-based,jointprobability model for statistical machine translation.In Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing, pages 133?139.
Association for Computational Linguistics, July.R.
Neal.
2000.
Slice sampling.
Annals of Statistics,31:705?767.G.
Neubig, T. Watanabe, E. Sumita, S. Mori, andT.
Kawahara.
2011.
An unsupervised model for jointphrase alignment and extraction.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 632?641, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.D.
Newman, A. Asuncion, P. Smyth, and M. Welling.2007.
Distributed inference for latent dirichlet alcation.
In NIPS.
MIT Press.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51, March.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
In ACL ?02: Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, pages 311?318, Morristown, NJ, USA.Association for Computational Linguistics.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Ann.
Probab., 25:855?900.Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages985?992.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proceed-ings of the 16th conference on Computational linguis-tics, pages 836?841, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.F.
Wood, J. Gasthaus, C. Archambeau, L. James, andY.
W. Teh.
2011.
The sequence memoizer.
Commu-nications of the Association for Computing Machines,54(2):91?98.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23:377?403, September.H.
Zhang, C. Quirk, R. C. Moore, and D. Gildea.
2008.Bayesian learning of non-compositional phrases withsynchronous parsing.
In Proceedings of ACL-08:HLT, pages 97?105, Columbus, Ohio, June.
Associ-ation for Computational Linguistics.232
