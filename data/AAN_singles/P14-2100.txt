Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611?617,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsEnriching Cold Start Personalized Language ModelUsing Social Network InformationYu-Yang Huang?, Rui Yan*, Tsung-Ting Kuo?, Shou-De Lin??
?Graduate Institute of Computer Science and Information Engineering,National Taiwan University, Taipei, Taiwan?Graduate Institute of Network and Multimedia,National Taiwan University, Taipei, Taiwan*Computer and Information Science Department,University of Pennsylvania, Philadelphia, PA 19104, U.S.A.{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.eduAbstractWe introduce a generalized framework to enrichthe personalized language models for cold startusers.
The cold start problem is solved withcontent written by friends on social networkservices.
Our framework consists of a mixturelanguage model, whose mixture weights are es-timated with a factor graph.
The factor graph isused to incorporate prior knowledge and heuris-tics to identify the most appropriate weights.The intrinsic and extrinsic experiments showsignificant improvement on cold start users.1 IntroductionPersonalized language models (PLM) on socialnetwork services are useful in many aspects (Xueet al, 2009; Wen et al, 2012; Clements, 2007),For instance, if the authorship of a document isin doubt, a PLM may be used as a generativemodel to identify it.
In this sense, a PLM servesas a proxy of one?s writing style.
Furthermore,PLMs can improve the quality of informationretrieval and content-based recommendation sys-tems, where documents or topics can be recom-mended based on the generative probabilities.However, it is challenging to build a PLM forusers who just entered the system, and whosecontent is thus insufficient to characterize them.These are called ?cold start?
users.
Producingbetter recommendations is even more critical forcold start users to make them continue to use thesystem.
Therefore, this paper focuses on how toovercome the cold start problem and obtain abetter PLM for cold start users.The content written by friends on a socialnetwork service, such as Facebook or Twitter, isexploited.
It can be either a reply to an originalpost or posts by friends.
Here the hypothesis isthat friends, who usually share common interests,tend to discuss similar topics and use similarwords than non-friends.
In other words, we be-lieve that a cold start user?s language model canbe enriched and better personalized by incorpo-rating content written by friends.Intuitively, a linear combination of document-level language models can be used to incorporatecontent written by friends.
However, it should benoticed that some documents are more relevantthan others, and should be weighted higher.
Toobtain better weights, some simple heuristicscould be exploited.
For example, we can measurethe similarity or distance between a user lan-guage model and a document language model.
Inaddition, documents that are shared frequently ina social network are usually considered to bemore influential, and could contribute more tothe language model.
More complex heuristicscan also be derived.
For instance, if two docu-ments are posted by the same person, theirweights should be more similar.
The main chal-lenge lies in how such heuristics can be utilizedin a systematic manner to infer the weights ofeach document-level language model.In this paper, we exploit the information onsocial network services in two ways.
First, weimpose the social dependency assumption via afinite mixture model.
We model the true, albeitunknown, personalized language model as acombination of a biased user language model anda set of relevant document language models.
Dueto the noise inevitably contained in social mediacontent, instead of using all available documents,we argue that by properly specifying the set ofrelevant documents, a better personalized lan-guage model can be learnt.
In other words, eachuser language model is enriched by a personal-ized collection of background documents.Second, we propose a factor graph model(FGM) to incorporate prior knowledge (e.g.
theheuristics described above) into our model.
Each611mixture weight is represented by a random vari-able in the factor graph, and an efficient algo-rithm is proposed to optimize the model and inferthe marginal distribution of these variables.
Use-ful information about these variables is encodedby a set of potential functions.The main contributions of this work are sum-marized below:?
To solve the cold start problem encounteredwhen estimating PLMs, a generalized frame-work based on FGM is proposed.
We incorpo-rate social network information into user lan-guage models through the use of FGM.
An it-erative optimization procedure utilizing per-plexity is presented to learn the parameters.To our knowledge, this is the first proposal touse FGM to enrich language models.?
Perplexity is selected as an intrinsic evalua-tion, and experiment on authorship attributionis used as an extrinsic evaluation.
The resultsshow that our model yields significant im-provements for cold start users.2 Methodology2.1 Social-Driven Personalized LanguageModelThe language model of a collection of documentscan be estimated by normalizing the counts ofwords in the entire collection (Zhai, 2008).
Tobuild a user language model, one na?ve way is tofirst normalize word frequency ?
(?, ?)
withineach document, and then average over all thedocuments in a user?s document collection.
Theresulting unigram user language model is:??(?)
=1|??|??
(?, ?)|?|????=1|??|?
??(?)????
(1)where ??(?)
is the language model of a particu-lar document, and ??
is the user?s document col-lection.
This formulation is basically an equal-weighted finite mixture model.A simple yet effective way to smooth a lan-guage model is to linearly interpolate with abackground language model (Chen and Good-man, 1996; Zhai and Lafferty, 2001).
In the line-ar interpolation method, all background docu-ments are treated equally.
The entire documentcollection is added to the user language model??(?)
with the same interpolation coefficient.Our main idea is to specify a set of relevantdocuments for the target user using informationembedded in a social network, and enrich thesmoothing procedure with these documents.
Let????
denote the content from relevant persons(e.g.
social neighbors) of u1, our idea can be con-cisely expressed as:??1?
(?)
= ??1??1(?)
+ ?
??????(?)???????
(2)where ???
is the mixture weight of the languagemodel of document di, and ?
?1 + ????
= 1 .Documents posted by irrelevant users are notincluded as we believe the user language modelcan be better personalized by exploiting the so-cial relationship in a more structured way.
In ourexperiment, we choose the first degree neighbordocuments as ???
?.Also note that we have made no assumptionabout how the ?base?
user language model??1(?)
is built.
In practice, it need not be modelsfollowing maximum likelihood estimation, butany language model can be integrated into ourframework to achieve a better refined model.Furthermore, any smoothing method can be ap-plied to the language model without degradingthe effectiveness.2.2 Factor Graph Model (FGM)Now we discuss how the mixture weights can beestimated.
We introduce a factor graph model(FGM) to make use of the diverse information ona social network.
Factor graph (Kschischang etal., 2006) is a bipartite graph consisting of a setof random variables and a set of factors whichsignifies the relationships among the variables.
Itis best suited in situations where the data is clear-ly of a relational nature (Wang et al, 2012).
Thejoint distribution of the variables is factored ac-cording to the graph structure.
Using FGM, onecan incorporate the knowledge into the potentialfunction for optimization and perform joint in-ference over documents.
As shown in Figure 1,the variables included in the model are describedas follows:Candidate variables ??
= ?
?, ???
.
The ran-dom variables in the top layer stand for the de-grees of belief that a document di should be in-cluded in the PLM of the target user ?.Figure 1: A two-layered factor graph (FGM)proposed to estimate the mixture weights.612Attribute variables xi.
Local information isstored as the random variables in the bottom lay-er.
For example, x1 might represent the numberof common friends between the author of a doc-ument di and our target user.The potential functions in the FGM are:Attribute-to-candidate function.
This poten-tial function captures the local dependencies of acandidate variable to the relevant attributes.
Letthe candidate variable yi correspond to a docu-ment di, the attribute-to-candidate function of yiis defined in a log-linear form:?(??
, ?)
=1?????{???(?
?, ?)}
(3)where A is the set of attributes of either the doc-ument di or target user u; f is a vector of featurefunctions which locally model the value of yiwith attributes in A; ??
is the local partitionfunction and ?
is the weight vector to be learnt.In our experiment, we define the vector offunctions as ?
= ????
?, ????
, ???
?, ???
?, ?????
as:?
Similarity function ????
.
The similarity be-tween language models of the target user anda document should play an important role.
Weuse cosine similarity between two unigrammodels in our experiments.?
Document quality function ????.
The out-of-vocabulary (OOV) ratio is used to measure thequality of a document.
It is defined as????
= 1 ?|{?:?
?
??
?
?
?
?}||?
?|(4)where ?
is the vocabulary set of the entirecorpus, with stop words excluded.?
Document popularity function ????
.
Thisfunction is defined as the number of times di isshared to model the popularity of documents.?
Common friend function ????.
It is definedas the number of common friends between thetarget user u1 and the author of di.?
Author friendship function ???
.
Assumingthat documents posted by a user with morefriends are more influential, this function isdefined as the number of friends of di?s author.Candidate-to-candidate function.
This po-tential function defines the correlation of a can-didate variable yi with another candidate variableyj in the factor graph.
The function is defined as?(??
, ??)
=1???,????{???(?
?, ??)}
(5)where g is a vector of feature functions indicat-ing whether two variables are correlated.
If wefurther denote the set of all related variables as?(??)
, then for any candidate variable yi, wehave the following brief expression:?(??
, ?(??))
= ?
?(??
, ??)????(??
)(6)For two candidate variables, let the corre-sponding document be di and dj, respectively, wedefine the vector ?
= ?????
, ??????
as:?
User relationship function ????.
We assumethat two candidate variables have higher de-pendency if they represent documents of thesame author or the two authors are friends.The dependency should be even greater if twodocuments are similar.
Let ?(?)
denote theauthor of a document d and ?[?]
denote theclosed neighborhood of a user u, we define????
= ?{?(??)
?
?[?(??)]}
?
???(??
, ??)
(7)?
Co-category function ????.
For any two can-didate variables, it is intuitive that the two var-iables would have a higher correlation if diand dj are of the same category.
Let ?(?)
de-note the category of document d, we define????
= ?{?(??)
= ?(??)}
?
???(??
, ??)
(8)2.3 Model Inference and OptimizationLet Y and X be the set of all candidate variablesand attribute variables, respectively.
The jointdistribution encoded by the FGM is given bymultiplying all potential functions.?
(?, ?)
=??(?
?, ?)?(??
, ?(??))?
(9)The desired marginal distribution can be ob-tained by marginalizing all other variables.
Sinceunder most circumstances, however, the factorgraph is densely connected, the exact inference isintractable and approximate inference is required.After obtaining the marginal probabilities, themixture weights ???
in Eq.
2 are estimated bynormalizing the corresponding marginal proba-bilities ?(??)
over all candidate variables, whichcan be written as???
= (1 ?
??1)?(??)?
?(??)?:???????
(10)where the constraint ?
?1 + ????
= 1 leads to avalid probability distribution for our mixturemodel.A factor graph is normally optimized by gra-dient-based methods.
Unfortunately, since theground truth values of the mixture weights arenot available, we are prohibited from using su-pervised approaches.
Here we propose a two-stepiterative procedure to optimize our model.
At613first, all the model parameters (i.e.
?, ?, ??)
arerandomly initialized.
Then, we infer the marginalprobabilities of candidate variables.
Given thesemarginal probabilities, we can evaluate the per-plexity of the user language model on a held-outdataset, and search for better parameters.
Thisprocedure is repeated until convergence.
Also,notice that by using FGM, we reduce the numberof parameters from 1 + |???
?| to 1 + |?| + |?|,lowering the risk of overfitting.3 Experiments3.1 Dataset and Experiment SetupWe perform experiments on the Twitter datasetcollected by Galuba et al (2010).
Twitter datahave been used to verify models with differentpurposes (Lin et al, 2011; Tan et al, 2011).
Toemphasize on the cold start scenario, we random-ly selected 15 users with about 35 tweets and 70friends as candidates for an authorship attributiontask.
Our experiment corpus consists of 4322tweets.
All words with less than 5 occurrencesare removed.
Stop words and URLs are also re-moved and all tweets are stemmed.
We identifythe 100 most frequent terms as categories.
Thesize of the vocabulary set is 1377.We randomly partitioned the tweets of eachuser into training, validation and testing sets.
Thereported result is the average of 10 random splits.In all experiments, we vary the size of trainingdata from 1% to 15%, and hold out the samenumber of tweets from each user as validationand testing data.
The statistics of our dataset,given 15% training data, are shown in Table 1.Loopy belief propagation (LBP) is used to ob-tain the marginal probabilities of the variables(Murphy et al, 1999).
Parameters are searchedwith the pattern search algorithm (Audet andDennis, 2002).
To not lose generality, we use thedefault configuration in all experiments.# of Max.
Min.
Avg.Tweets 70 19 35.4Friends 139 24 68.9Variables 467 97 252.7Edges 9216 231 3427.1Table 1: Dataset statistics3.2 Baseline MethodsWe compare our framework with two baselinemethods.
The first (?Cosine?)
is a straightfor-ward implementation that sets all mixtureweights ???
to the cosine similarity between theprobability mass vectors of the document anduser unigram language models.
The second(?PS?)
uses the pattern search algorithm to per-form constrained optimization over the mixtureweights.
As mentioned in section 2.3, the maindifference between this method and ours(?FGM?)
is that we reduce the search space ofthe parameters by FGM.
Furthermore, socialnetwork information is exploited in our frame-work, while the PS method performs a directsearch over mixture weights, discarding valuableknowledge.Different from other smoothing methods thatare usually mutually exclusive, any othersmoothing methods can be easily merged intoour framework.
In Eq.
2, the base languagemodel ??1(?)
can be already smoothed by anytechniques before being plugged into our frame-work.
Our framework then enriches the user lan-guage model with social network information.We select four popular smoothing methods todemonstrate such effect, namely additivesmoothing, absolute smoothing (Ney et al, 1995),Jelinek-Mercer smoothing (Jelinek and Mercer,1980) and Dirichlet smoothing (MacKay andPeto, 1994).
The results of using only the basemodel (i.e.
set ???
= 0 in Eq.
2) are denoted as?Base?
in the following tables.Train %Additive AbsoluteBase Cosine PS FGM Base Cosine PS FGM1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5**5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2**10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8**15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1**Train %Jelinek-Mercer DirichletBase Cosine PS FGM Base Cosine PS FGM1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0**5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2**10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0**15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4Table 2: Testing set perplexity.
** indicates that the best score among all methods is significantly bet-ter than the next highest score, by t-test at a significance level of 0.05.6143.3 PerplexityAs an intrinsic evaluation, we first compute theperplexity of unseen sentences under each userlanguage model.
The result is shown in Table 2.Our method significantly outperforms all ofthe methods in almost all settings.
We observethat the ?PS?
method takes a long time to con-verge and is prone to overfitting, likely becauseit has to search about a few hundred parameterson average.
As expected, the advantage of ourmodel is more apparent when the data is sparse.3.4 Authorship Attribution (AA)The authorship attribution (AA) task is chosen asthe extrinsic evaluation metric.
Here the goal isnot about comparing with the state-of-the-art ap-proaches in AA, but showing that LM-based ap-proaches can benefit from our framework.To apply PLM on this task, a na?ve Bayesclassifier is implemented (Peng et al, 2004).
Themost probable author of a document d is the onewhose PLM yields the highest probability, and isdetermined by ??
= argmax?{?
??(?)???
}.The result is shown in Table 3.
Our model im-proves personalization and outperforms the base-lines under cold start settings.
When data issparse, the ?PS?
method tends to overfit thenoise, while the ?Cosine?
method contains toofew information and is severely biased.
Ourmethod strikes a balance between model com-plexity and the amount of information included,and hence performs better than the others.4 Related WorkPersonalization has long been studied in varioustextual related tasks.
Personalized search is es-tablished by modeling user behavior when usingsearch engines (Shen et al, 2005; Xue et al,2009).
Query language model could be also ex-panded based on personalized user modeling(Chirita et al, 2007).
Personalization has alsobeen modeled in many NLP tasks such as sum-marization (Yan et al, 2011) and recommenda-tion (Yan et al, 2012).
Different from our pur-pose, these models do not aim at exploiting so-cial media content to enrich a language model.Wen et al (2012) combines user-level languagemodels from a social network, but instead of fo-cusing on the cold start problem, they try to im-prove the speech recognition performance usinga mass amount of texts on social network.
On theother hand, our work explicitly models the moresophisticated document-level relationships usinga probabilistic graphical model.5 ConclusionThe advantage of our model is threefold.
First,prior knowledge and heuristics about the socialnetwork can be adapted in a structured waythrough the use of FGM.
Second, by exploiting awell-studied graphical model, mature inferencetechniques, such as LBP, can be applied in theoptimization procedure, making it much moreeffective and efficient.
Finally, different frommost smoothing methods that are mutually ex-clusive, any other smoothing method can be in-corporated into our framework to be further en-hanced.
Using only 1% of the training corpus,our model can improve the perplexity of basemodels by as much as 40% and the accuracy ofauthorship attribution by at most 15%.6 AcknowledgementThis work was sponsored by AOARD grantnumber No.
FA2386-13-1-4045 and NationalScience Council, National Taiwan Universityand Intel Corporation under Grants NSC102-2911-I-002-001 and NTU103R7501 and grant102-2923-E-002-007-MY2, 102-2221-E-002-170,101-2628-E-002-028-MY2.Train %Additive AbsoluteBase Cosine PS FGM Base Cosine PS FGM1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27**5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53**10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87**15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60**Train %Jelinek-Mercer DirichletBase Cosine PS FGM Base Cosine PS FGM1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67**5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.9310% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.5315% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40Table 3: Accuracy (%) of authorship attribution.
** indicates that the best score among all methods issignificantly better than the next highest score, by t-test at a significance level of 0.05.615ReferenceCharles Audet and J. E. Dennis, Jr. 2002.
Analysis ofgeneralized pattern searches.
SIAM J. on Optimiza-tion, 13(3):889?903, August.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-gang Nejdl.
2007.
Personalized query expansionfor the web.
In Proceedings of the 30th Annual In-ternational ACM SIGIR Conference on Researchand Development in Information Retrieval,SIGIR ?07, pages 7?14, New York, NY, USA.ACM.Maarten Clements.
2007.
Personalization of socialmedia.
In Proceedings of the 1st BCS IRSG Con-ference on Future Directions in Information Access,FDIA?07, pages 14?14, Swinton, UK, UK.
BritishComputer Society.Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty,Zoran Despotovic, and Wolfgang Kellerer.
2010.Outtweeting the twitterers - predicting informationcascades in microblogs.
In Proceedings of the 3rdConference on Online Social Networks, WOSN?10,pages 3?3, Berkeley, CA, USA.
USENIX Associa-tion.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of markov source parametersfrom sparse data.
In In Proceedings of the Work-shop on Pattern Recognition in Practice, pages381?397, Amsterdam, The Netherlands: North-Holland, May.F.
R. Kschischang, B. J. Frey, and H. A. Loeliger.2006.
Factor graphs and the sum-product algorithm.IEEE Trans.
Inf.
Theor., 47(2):498?519, Septem-ber.Jimmy Lin, Rion Snow, and William Morgan.
2011.Smoothing techniques for adaptive online languagemodels: Topic tracking in tweet streams.
In Pro-ceedings of the 17th ACM SIGKDD InternationalConference on Knowledge Discovery and DataMining, KDD ?11, pages 422?429, New York, NY,USA.
ACM.David J.C. MacKay and Linda C. Bauman Peto.
1994.A hierarchical dirichlet language model.
NaturalLanguage Engineering, 1:1?19.Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In Proceedings of theFifteenth Conference on Uncertainty in ArtificialIntelligence, UAI?99, pages 467?475, San Francis-co, CA, USA.
Morgan Kaufmann Publishers Inc.Hermann Ney, Ute Essen, and Reinhard Kneser.
1995.On the estimation of ?small?
probabilities by leav-ing-one-out.
IEEE Trans.
Pattern Anal.
Mach.
In-tell., 17(12):1202?1212, December.Fuchun Peng, Dale Schuurmans, and Shaojun Wang.2004.
Augmenting naive bayes classifiers with sta-tistical language models.
Inf.
Retr., 7(3-4):317?345,September.Xuehua Shen, Bin Tan, and ChengXiang Zhai.
2005.Implicit user modeling for personalized search.
InProceedings of the 14th ACM International Con-ference on Information and Knowledge Manage-ment, CIKM ?05, pages 824?831, New York, NY,USA.
ACM.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang,Ming Zhou, and Ping Li.
2011.
User-level senti-ment analysis incorporating social networks.
InProceedings of the 17th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, KDD ?11, pages 1397?1405, NewYork, NY, USA.
ACM.Zhichun Wang, Juanzi Li, Zhigang Wang, and JieTang.
2012.
Cross-lingual knowledge linkingacross wiki knowledge bases.
In Proceedings of the21st International Conference on World Wide Web,WWW ?12, pages 459?468, New York, NY, USA.ACM.Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, andLin-Shan Lee.
2012.
Personalized language model-ing by crowd sourcing with social network data forvoice access of cloud applications.
In Spoken Lan-guage Technology Workshop (SLT), 2012 IEEE,pages 188?193.Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang.2009.
User language model for collaborative per-sonalized search.
ACM Trans.
Inf.
Syst.,27(2):11:1?11:28, March.Rui Yan, Jian-Yun Nie, and Xiaoming Li.
2011.Summarize what you are interested in: An optimi-zation framework for interactive personalizedsummarization.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 1342?1351, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.Rui Yan, Mirella Lapata, and Xiaoming Li.
2012.Tweet recommendation with graph co-ranking.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics: Long Pa-pers - Volume 1, ACL ?12, pages 516?525,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.ChengXiang Zhai.
2008.
Statistical Language Modelsfor Information Retrieval.
Now Publishers Inc.,Hanover, MA, USA.616Chengxiang Zhai and John Lafferty.
2001.
A study ofsmoothing methods for language models applied toad hoc information retrieval.
In Proceedings of the24th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, SIGIR ?01, pages 334?342, New York, NY,USA.
ACM.617
