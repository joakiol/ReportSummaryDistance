Activity detection for information accessto oral communicationKlaus Ries and Alex Waibel?
{ries|ahw}@cs.cmu.eduInteractive Systems Labs, Carnegie Mellon University, Pittsburgh, PA, 15213, USAInteractive Systems Labs, Universita?t Karlsruhe, Fakulta?t fu?r Informatik, 76128 Karlsruhe, Germanyhttp://www.is.cs.cmu.edu/ http://werner.ira.uka.deABSTRACTOral communication is ubiquitous and carries important in-formation yet it is also time consuming to document.
Giventhe development of storage media and networks one couldjust record and store a conversation for documentation.
Thequestion is, however, how an interesting information piecewould be found in a large database.
Traditional informa-tion retrieval techniques use a histogram of keywords as thedocument representation but oral communication may offeradditional indices such as the time and place of the rejoinderand the attendance.
An alternative index could be the ac-tivity such as discussing, planning, informing, story-telling,etc.
This paper addresses the problem of the automatic de-tection of those activities in meeting situation and everydayrejoinders.
Several extensions of this basic idea are beingdiscussed and/or evaluated: Similar to activities one candefine subsets of larger database and detect those automati-cally which is shown on a large database of TV shows.
Emo-tions and other indices such as the dominance distributionof speakers might be available on the surface and could beused directly.
Despite the small size of the databases usedsome results about the effectiveness of these indices can beobtained.Keywordsactivity, dialogue processing, oral communication, speech,information access?We would like to thank our lab, especially Klaus Zechner,Alon Lavie and Lori Levin for their discussions and support.We would also like to thank our sponsors at DARPA.
Anyopinions, findings and conclusions expressed in this materialare those of the authors and may not reflect the views ofDARPA, or any other party..1.
INTRODUCTIONInformation access to oral communication is becoming aninteresting research area since recording, storing and trans-mitting large amounts of audio (and video) data is feasibletoday.
While written information is often available elec-tronically (especially since it is typically entered on com-puters) oral communication is usually only documented byconstructing a new document in written form such as a tran-script (court proceedings) or minutes (meetings).
Oral com-munications are therefore a large untapped resource, espe-cially if no corresponding written documents are availableand the cost of documentation using traditional techniquesis considered high: Tutorial introductions by a senior staffmember might be worthwhile to attend by many newcomers,office meetings may contain informations relevant for oth-ers and should be reproducable, informal and formal groupmeetings may be interesting but not fully documented.
Inessence the written form is already a reinterpretation of theoriginal rejoinder.
Such a reinterpretation are used to?
extract and condense information?
add or delete information?
change the meaning?
cite the rejoinder?
relate rejoinders to each otherReinterpretation is a time consuming, expensive and op-tional step and written documentation is combining reinter-pretation and documentation step in one 1.
If however rein-terpretation is not necessary or unwanted a system whichis producing audiovisual records is superior.
If reinterpreta-tion is wanted or needed a system using audiovisual recordsmay be used to improve the reinterpretation by adding allaudiovisual data and the option to go back to the unalteredoriginal.
Whether reinterpretation is done or not it is cru-cial to be able to navigate effectively within an audiovisualdocument and to find a specific document.1The most important exception is the literal courtroomtranscript, however one could argue that even transcriptsare reinterpretations since they do not contain a number ofinformations present in the audio channel such as emotions,hesitations, the use of slang and certain types of hetereglos-sia, accents and so forth.
This is specifically true if tran-scription machines are used which restrict the transcriberto standard orthography.Willie is sickNeed new coding schemePersonal stuffSetting up the new hard driveData collection, last WednesdayDialogue detection, with HansLanguage modeling tutorial for TimMeetingsTV showsLecturesSpeechesMeetingDatabaseSegmentFigure 1: Information access hierarchy: Oral com-munications take place in very different formats andthe first step in the search is to determine thedatabase (or sub-database) of the rejoinder.
Thenext step is to find the specific rejoinder.
Since re-joinders can be very long the rejoinder has to seg-mented and a segment has to be selected.While keywords are commonly used in information ac-cess to written information the use of other indices such asstyle is still uncommon (but see Kessler et al (1997); vanBretan et al (1998)).
Oral communication is richer thanwritten communication since it is an interactive real timeaccomplishment between participants, may involve speechgestures such as the display of emotion and is situated inspace and time.
Bahktin (1986) characterizes a conversa-tion by topic, situation and style.
Information access tooral communication can therefore make use of indices thatpertain to the oral nature of the discourse (Fig.
2).
In-dices other than topic (represented by keywords) increasein importance since browsing audio documents is cumber-some which makes the common interactive retrieval strategy?query, browse, reformulate?
less effective.
Finally the topicmay not be known at all or may not be that relevant for thequery formulation, for example if one just wants to be re-minded what was being discussed last time a person wasmet.
Activities are suggested as an alternative index andare a description of the type of interaction.
It is commonto use ?action-verbs?
such as story-telling, discussing, plan-ning, informing, etc.
to describe activities 2.
Items similarto activities have been shown to be directly retrievable fromautobiographic memory (Herrmann, 1993) and are thereforeindices that are available to participants of the conversation.Other indices may be very effective but not available: Thefrequency of the word ?I?
in the conversation, the histogramof word lengths or the histogram of pitch per participant.In Fig.
1 the information access hierarchy is being intro-duced which allows to understand the problem of informa-tion access to oral communication at different levels.
InRies (1999) we have shown that the detection of general di-2 The definition of activities such as planning may varyvastly across general dialogue genres, for example comparea military combat situation with a mother child interaction.However it is often possible to develop activities and dia-logue typologies for a specific dialogue genre.
The relatedproblem of general typologies of dialogues is still far frombeing settled and action-verbs are just one potential catego-rization (Fritz and Hundschnur, 1994).SpeakerTimeLocationRelated rejondersParts of speechEmotionOverlap between speakersSemantics / pragmaticsKeywordsSituationStyleTopicFigure 2: Bahktin?s characterization of dialogue:Bahktin (1986) describes a discourse along the threemajor properties style, situation and topic.
Currentinformation retrieval systems focus on the topical as-pect which might be crucial in written documents.Furthermore, since throughout text analysis is still ahard problem, information retrieval has mostly usedkeywords to characterize topic.
Many features thatcould be extracted are therefore ignored in a tradi-tional keyword based approach.alogue genre (database level in Fig.
1) can be done withhigh accuracy if a number of different example types havebeen annotated; in Ries et al (2000) we have shown that itis hard but not impossible to distinguish activities in per-sonal phone calls (segment level in Fig.
1) .
In this paperwe will address activities in meetings and other types of di-alogues and show that these activities can be distinguishedusing certain features and a neural network based classifier(Sec.
2, segment level in Fig.
1).
The concept of informationretrieval assessment using information theoretic measures isapplied to this task (Sec.
3).
Additionally we will introducea level somewhat below the database level in Fig.
1 that wecall ?sub-genre?
and we have collected a large database ofTV-shows that are automatically classified for their show-type (Sec.
4).
We also explore whether there are other in-dices similar to activities that could be used and we arepresenting results on emotions in meetings (Sec.
5).2.
ACTIVITY DETECTIONWe are interested in the detection of activities that aredescribed by action verbs and have annotated those in twodatabases:meetings have been collected at Interactive Systems Labs atCMU (Waibel et al, 1998) and a subset of 8 meetingshas been annotated.
Most of the meetings are by thedata annotation group itself and are fairly informal instyle.
The participants are often well acquainted andmeet each other a lot besides their meetings.Santa Barbara (SBC) is a corpus released by the LDCand 7 out of 12 rejoinders have been annotated.The annotator has been instructed to segment the rejoin-ders into units that are coherent with respect to their topicActivity SBC MeetingDiscussion 35 58Information 25 23Story-telling 24 10Planning 7 19Undetermined 5 8Advising 5 17Not meeting 3 2Interrogation 2 1Evaluation 1 0Introduction 0 1Closing 0 1Table 1: Distribution of activity types: Bothdatabases contain a lot of discussing, informing andstory-telling activities however the meeting datacontains a lot more planning and advising.and activity and annotate them with an activity which fol-lows the intuitive definition of the action-verb such as dis-cussing, planning, etc.
Additionally an activity annotationmanual containing more specific instructions has been avail-able (Ries et al, 2000; Thyme?-Gobbel et al, 2001) 3.
Thelist of tags and the distribution can be seen in Tab.
1.
Theset of activities can be clustered into ?interactive?
activitiesof equal contribution rights (discussion,planning), one per-son being active (advising, information giving, story-telling),interrogations and all others.Measure Meeting SBC CallHomeall inter all inter Spanish?
0.41 0.51 0.49 0.56 0.59Mutual inf.
0.35 0.25 0.65 0.32 0.61Table 2: Intercoder agreement for activities: Themeeting dialogues and Santa Barbara corpus havebeen annotated by a semi-naive coder and the firstauthor of the paper.
The ?-coefficient is determinedas in Carletta et al (1997) and mutual informationmeasures how much one label ?informs?
the other(see Sec.
3).
For CallHome Spanish 3 dialogues werecoded for activities by two coders and the resultseems to indicate that the task was easier.Both datasets have been annotated not only by a semi-naive annotator but also by the first author of the paper.The results for ?-statistics (Carletta et al, 1997) and mu-tual information between the coders can be seen in Tab.
2.The intercoder agreement would be considered moderate butcompares approximately to Carletta et al (1997) agreementon transactions (?
= 0.59), especially for the interactive ac-tivities and CallHome Spanish.For classification a neural network was trained that usesthe softmax function as its output and KL-divergence as3 In contrast to (Ries et al, 2000; Thyme?-Gobbel et al,2001) the ?consoling?
activity has been eliminated and an?informing?
activity has been introduced for segments whereone or more than one member of the rejoinder give informa-tion to the others.
Additionally an ?introducing?
activitywas added to account for a introduction of people or topicsat the beginning of meetings.Feature all interactiveSBC meet SBC meetbaseline 32.7 41.1 50.5 54.6dialogue acts per channel 28.1 37.6 47.7 56.7dialogue acts 28.0 36.2 46.7 65.3words 38.3 39.7 53.3 54.6dominance 32.7 44.7 64.5 58.2style 24.3 35.5 53.3 58.9style + words 42.1 38.3 52.3 57.5dominance + words 41.1 41.1 52.3 58.9dominance + style + words 42.1 39.7 53.3 60.3dialogue acts + words 42.1 37.6 57.0 61.0dialogue acts + style + words 39.3 40.4 57.9 61.0Wordnet 37.4 37.6 46.7 52.5Wordnet + words 49.5 39.0 53.3 57.5first author 59.8 57.9 73.8 72.7Table 3: Activity detection: Activities are detectedon the Santa Barbara Corpus (SBC) and the meet-ing database (meet) either without clustering theactivities (all) or clustering them according to theirinteractivity (interactive) (see Sec.
2 for details).the error function.
The network connects the input di-rectly to the output units.
Hidden units have not been usedsince they did not yield improvements on this task.
Thenetwork was trained using RPROP with momentum (Ried-miller and Braun, 1993) and corresponds to an exponen-tial model (Nigam et al, 1999).
The momentum term canbe interpreted as a Gaussian prior with zero mean on thenetwork weights.
It is the same architecture that we usedpreviously (Ries et al, 2000) for the detection of activitieson CallHome Spanish.
Although some feature sets could betrained using the iterative scaling algorithm if no hiddenunits are being used the training times weren?t high enoughto justify the use of the less flexible iterative scaling algo-rithm.
The features used for classification arewords the 50 most frequent words / part of speech pairsare used directly, all other pairs are replaced by theirpart of speech 4.stylistic features adapted from Biber (1988) and containmostly syntactic constructions and some word classes.Wordnet a total of 40 verb and noun classes (so called lex-icographers classes (Fellbaum, 1998)) are defined anda word is replaced by the most frequent class over allpossible meanings of the word.dialogue acts such as statements, questions, backchannels,.
.
.
are detected using a language model based detec-tor trained on Switchboard similar to Stolcke et al(2000) 54Klaus Zechner trained an English part of speech taggertagger on Switchboard that has been used.
The tagger usesthe code by Brill (1994).5The model was trained to be very portable and thereforethe following choices were taken: (a) the dialogue modelis context-independent and (b) only the part of speech aretaken as the input to the model plus the 50 most likelyword/part of speech types.dominance is described as the distribution of the speakerdominance in a conversation.
The distribution is rep-resented as a histogram and speaker dominance is mea-sured as the average dominance of the dialogue acts (Linellet al, 1988) of each speaker.
The dialogue acts are de-tected and the dominance is a numeric value assignedfor each dialogue act type.
Dialogue act types thatrestrict the options of the conversation partners havehigh dominance (questions), dialogue acts that signalunderstanding (backchannels) carry low dominance.First author The activities used for classification are thoseof the semi-naive coder.
The ?first author?
column de-scribes the ?accuracy?
of the first author with respectto the naive coder.The detection of interactive activities works fairly wellusing the dominance feature on SBC which is also natu-ral since the relative dominance of speakers should describewhat kind of interaction is exhibited.
The dialogue act dis-tribution on the other hand works fairly well on the morehomogeneous meeting database were there is a better chanceto see generalizations from more specific dialogue based in-formation.
Overall the combination of more than one featureis really important since word level, Wordnet and stylisticinformation, while sometimes successful, seem to be able toimprove the result while they don?t provide good features bythemselves.
The meeting data is also more difficult whichmight be due to its informal style.3.
INFORMATION ACCESS ASSESSMENTAssuming a probabilistic information retrieval model aquery r ?
in our example an activity ?
predicts a docu-ment d with the probability q(d|r) = q(r|d)q(d)q(r) .
Let p(d, r)be the real probability mass distribution of these quanti-ties.
The probability mass function q(r|d) is estimated ona separate training set by a neural network based classi-fier 6.
The quantity we are interested in is the reductionin expected coding length of the document using the neuralnetwork based detector 7:?Eplogq(D)q(D|R)?
H(R)?
Ep log1q(R|D)The two expectations correspond exactly to the measures inTab.
5, the first represents the baseline, the second the onefor the respective classifier.
In more standard informationtheoretic notation this quantity may be written as:H(R)?
(Hp(R|D) +D(p(r|d)||q(r|d)))This equivalence is not extremely useful though since thequantities in parenthesis can?t be estimated separately.
Forthe small meeting database and SBC however no entropyreductions could be obtained.
On the larger databases, onthe other hand, entropy reductions could be obtained (?0.5bit on the CallHome Spanish database Ries et al (2000),?
1bit for the sub-database detection problem in Sec.
4).6All quantities involving the neural net q(r|d) have beendetermined using a round robin approach such that networkis trained on a separate training set.7Since estimating q(d) is simple we may assume that q(d) ?
?r p(d, r).Another option is to assume that the labels of one coderare part of D. If the query by the other coder is R we are in-terested in the reduction of the document entropy given thequery.
If we furthermore assume that H(R|D) = H(R|R?
)where R?
is the activity label embedded in D:H(D)?H(D|R) = H(R)?H(R|D) = MI(R,R?)Tab.
2 shows that the labels of the semi-naive coder and thefirst author only inform each other by 0.25?0.65 bits.
How-ever, since all constraints are important to apply, it might beimportant to include manual annotations to be matched bya query or in a graphical presentation of the output results.Another interesting question to consider is whether theactivity is correlated with the rejoinder or not.
This ques-tion is important since a correlation of the activity with therejoinder would mean that the indexing performance of ac-tivities needs to be compared to other indices that apply torejoinders such as attendance, time and place (for results onthe correlation with rejoinders see Waibel et al (2001)).
Thecorrelation can be measured using the mutual informationbetween the activity and the meeting identity.
The mutualinformation is moderate for SBC (?
0.67 bit) and muchlower for the meetings (?
0.20 bit).
This also correspondsto our intuition since some of the rejoinders in SBC belongto very distinct dialogue genre while the meeting databaseis homogeneous.
The conclusion is that activities are usefulfor navigation in a rejoinder if the database is homogeneousand they might be useful for finding conversations in a moreheterogeneous database.# # #Talk 344 Edu 25 Finance 8News 217 Scifi 24 Religious 5Sitcom 97 Series 24 Series-Old 3Soap 87 Cartoon 23 Infotain 3Game 46 Movies 22 Music 2Law 32 Crafts 17 Horror 1Sports 32 Specials 15Drama 31 Comedy 9Table 4: TV show types: The distribution of showtypes in a large database of TV shows (1067 shows)that has been recorded over the period of a coupleof months until April 2000 in Pittsburgh, PA4.
DETECTION OF SUB-DATABASESWe set up an environment for TV shows that records thesubtitles with timestamps continuously from one TV chan-nel and the channel was switched every other day.
At thesame time the TV program was downloaded from http://tv.yahoo.com/ to obtain programming information in-cluding the genre of the show.
Yahoo assigns primary andsecondary show types and unless the combination of pri-mary/secondary show-type is frequent enough the primaryshowtype is used (Tab.
4).
The TV show database has theadvantage that we were able to collect a large and varieddatabase with little effort.
The same classifier as in Sec.
2has been used however dialogue acts have not been detectedsince the data contains a lot of noise, is not necessarily con-versational and speaker identities can?t be determined easily.Detection results for TV shows can be seen in Tab.
5.
It maybe noted that adding a lot of keywords does improve the de-tection result but not so much the entropy.
It may thereforebe assume that there is a limited dependence between topicand genre which isn?t really a surprise since there are manyshows with weekly sequels and there may be some true re-peats.Feature accuracy entropyWordnet stylistic wordsbaseline 32.2 3.31?
50.9 2.73?
50 62.2 2.33?
?
50 60.0 2.29?
?
61.2 2.28?
56.9 2.41?
50 61.5 2.2550 61.3 2.35250 62.7 2.17500 66.0 2.14?
?
500 64.9 2.135000 67.2 2.08Table 5: Show type detection: Using the neural net-work described in Sec.
2 the show type was detected.If there is a number in the word column the wordfeature is being used.
The number indicates howmany word/part of speech pairs are in the vocabu-lary additionally to the parts of speech.5.
EMOTION AND DOMINANCEEmotions are displayed in a variety of gestures, some ofwhich are oral and may be detected via automated methodsfrom the audio channel (Polzin, 1999).
Using only verbalinformation the emotions happy, excited and neutral canbe detected on the meeting database with 88.1% accuracywhile always picking neutral yields 83.6%.
This result can beimproved to 88.6% by adding pitch and power information.While these experiments were conducted at the utterancelevel emotions can be extended to topical segments.
Forthat purpose the emotions of the individual utterances areentered in a histogram over the segment and the vectorsare clustered automatically.
The resulting clusters roughlycorrespond to a ?neutral?, ?a little happy?
and ?somewhatexcited?
segment.
Using the classifier for emotions on theword level the segment can be classified automatically intocategories with a 83.3% accuracy while the baseline is 68.9%.The entropy reduction by automatically detected emotionalactivities is ?
0.3bit 8.
A similar attempt can be made fordominance (Linell et al, 1988) distributions: Dominance iseasy to understand for the user of an information accesssystem and it can be determined automatically with highaccuracy.8 A similar classification result for emotions on the utter-ance level has been obtained by just using the laughter vs.non-laughter tokens of the transcript as the input.
Thismay indicate that (a) the index should really be the amountof laughter in the conversational segment and that (b) emo-tions might not be displayed very overtly in meetings.
Theseresults however would require a wider sampling of meetingtypes to be generally acceptable.6.
CONCLUSION AND FUTURE WORKIt has been shown that activities can be detected and thatthey may be efficient indices for access to oral communica-tion.
Overall it is easy to make high level distinctions withautomated methods while fine-grained distinctions are evenhard to make for humans ?
on the other hand automaticmethods are still able to model some aspect of it (Fig.
3).To obtain an reduction in entropy a relatively large databasesuch as CallHome Spanish is required (120 dialogues).
Al-ternatives to activities might be emotional and dominancedistributions that are easier to detect and that may be nat-ural to understand for users.
If activities are only used forlocal navigation support within a rejoinder one could alsovisualize by displaying the dialogue act patterns for eachchannel on a time line.The author has also observed that topic clusters and activ-ities are largely independent in the meeting domain result-ing in orthogonal indices.
Since activities have intuitions fornaive users and they may be remembered it can be assumedthat users would be able to make use of these constraints.Ongoing work includes the use of speaker activity for dia-logue segmentation and further assessment of features forinformation access.
Overall the methods presented here andthe ongoing work are improving the ability to index oralcommunication.
It should be noted that some of the tech-niques presented lend themselves to implementations thatdon?t require (full) speech recognition: Speaker identifica-tion and dialogue act identification may be done withoutan LVCSR system which would allow to lower the compu-tational requirements as well as to a more robust system.Figure 3: Detection accuracy summary: The detec-tion of high-level genre as exemplified by the differ-entiation of corpora can be done with high accuracyusing simple features (Ries, 1999).
Similar it wasfairly easy to discriminate between male and femalespeakers on Switchboard (Ries, 1999).
Discrimi-nating between sub-genre such as TV-show types(Sec.
4) can be done with reasonable accuracy.
How-ever it is a lot harder to discriminate between ac-tivities within one conversation for personal phonecalls (CallHome) (Ries et al, 2000) or for generalrejoinders (Santa) and meetings (Sec.
2).ReferencesM.
M. Bahktin.
Speech Genres and other late Essays, chap-ter Speech Genres.
University of Texas Press, Austin,1986.D.
Biber.
Variation across speech and writing.
CambridgeUniversity Press, 1988.E.
Brill.
A report on recent progress in transformation basederror-driven learning.
In DARPA Workshop, 1994.J.
Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-Sneddon, and A. H. Anderson.
The reliability of a dia-logue structure coding scheme.
Computational Linguis-tics, 23(1):13?31, March 1997.C.
Fellbaum, editor.
WordNet ?
An Electronic LexicalDatabase.
MIT press, 1998.G.
Fritz and F. Hundschnur.
Handbuch der Dialoganalyse.Niemeyer, Tuebingen, 1994.D.
J. Herrmann.
Autobiographical memory and the validityof retrospective reports, chapter The validity of retrospec-tive reports as a function of the directness of retrievalprocesses, pages 21?31.
Springer, 1993.B.
Kessler, G. Nunberg, and H. Schu?tze.
Automatic detec-tion of genre.
In Proceedings of the 35th Annual Meet-ing of the Association for Computational Linguistics andthe 8th Meeting of the European Chapter of the Associ-ation for Computational Linguistics, pages 32?38.
Mor-gan Kaufmann Publishers, San Francisco CA, 1997.
URLhttp://xxx.lanl.gov/abs/cmp-lg/9707002.P.
Linell, L. Gustavsson, and P. Juvonen.
Interactionaldominance in dyadic communication: a presentation ofinitiative-response analysis.
Linguistics, 26:415?442, 1988.K.
Nigam, J. Lafferty, and A. McCallum.
Using maxi-mum entropy for text classification.
In Proceedings ofthe IJCAI-99 Workshop on Machine Learning for Infor-mation Filtering, 1999.
URL http://www.cs.cmu.edu/~lafferty/.T.
Polzin.
Detecting Verbal and Non-Verbal Cues in theCommunication of Emotion.
PhD thesis, Carnegie MellonUniversity, November 1999.M.
Riedmiller and H. Braun.
A direct adaptive method forfaster backpropagation learning: The RPROP algorithm.In Proc.
of the IEEE Int.
Conf.
on Neural Networks, pages586?591, 1993.K.
Ries.
Towards the detection and description of textualmeaning indicators in spontaneous conversations.
In Pro-ceedings of the Eurospeech, volume 3, pages 1415?1418,Budapest, Hungary, September 1999.K.
Ries, L. Levin, L. Valle, A. Lavie, and A. Waibel.Shallow discourse genre annotation in callhome spanish.In Proceecings of the International Conference on Lan-guage Ressources and Evaluation (LREC-2000), Athens,Greece, May 2000.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. V. Ess-Dykema, andM.
Meteer.
Dialogue act modeling for automatic taggingand recognition of conversational speech.
ComputationalLinguistics, 26(3), September 2000.A.
Thyme?-Gobbel, L. Levin, K. Ries, and L. Valle.
Dia-logue act, dialogue game, and activity tagging manual forspanish conversational speech.
Technical report, CarnegieMellon University, 2001. in preperation.van Bretan, J. Dewe, A. Hallberg, J. Karlgren, and N. Wolk-ert.
Genres defined for a purpose, fast clustering, and aniterative information retrieval interface.
In Eighth DE-LOS Workshop on User Interfaces in Digital LibrariesL?angholmen, pages 60?66, October 1998.A.
Waibel, M. Bett, and M. Finke.
Meeting browser: Track-ing and summarising meetings.
In Proceedings of theDARPA Broadcast News Workshop, 1998.A.
Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf, T. Schultz,H.
Soltau, H. Yu, and K. Zechner.
Advances in automaticmeeting record creation and access.
In ICASSP, Salt LakeCity, Utah, USA, 2001. to appear.
