Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 196?200, New York City, June 2006. c?2006 Association for Computational LinguisticsDependency Parsing with Reference to Slovene, Spanish and SwedishSimon Corston-OliverNatural Language ProcessingMicrosoft ResearchOne Microsoft WayRedmond WA 98052simonco@microsoft.comAnthony AueNatural Language ProcessingMicrosoft ResearchOne Microsoft WayRedmond WA 98052anthaue@microsoft.comAbstractWe describe a parser used in the CoNLL2006 Shared Task, ?Multingual Depen-dency Parsing.?
The parser first identi-fies syntactic dependencies and then labelsthose dependencies using a maximum en-tropy classifier.
We consider the impact offeature engineering and the choice of ma-chine learning algorithm, with particularfocus on Slovene, Spanish and Swedish.1 IntroductionThe system that we submitted for the CoNLL 2006Shared Task, ?Multingual Dependency Parsing,?
(Buchholz et al, 2006) is a two stage pipeline.
Thefirst stage identifies unlabeled directed dependen-cies using an extension of the parser described in(Corston-Oliver et al, 2006).
The second stage is amaximum entropy classifier that labels the directeddependencies.
The system was trained on the twelveobligatory languages, as well as the optional lan-guage, Bulgarian (Hajic?
et al, 2004; Simov et al,2005; Simov and Osenova, 2003; Chen et al, 2003;Bo?hmova?
et al, 2003; Kromann, 2003; van der Beeket al, 2002; Brants et al, 2002; Kawata and Bar-tels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006;Civit Torruella and Mart??
Anton?
?n, 2002; Nilsson etal., 2005; Oflazer et al, 2003; Atalay et al, 2003).Table 1 presents the results of the system de-scribed in the current paper on the CoNLL sharedtask, including the optional evaluation on Bulgar-ian.
For Slovene, we ranked second with a labeledLanguage Unlabeled LabeledAttachment AttachmentArabic 78.40 63.53Bulgarian 90.09 83.36Chinese 90.00 79.92Czech 83.02 74.48Danish 87.94 81.74Dutch 74.83 71.43German 87.20 83.47Japanese 92.84 89.95Portugese 88.96 84.59Slovene 81.77 72.42Spanish 84.87 80.36Swedish 89.54 79.69Turkish 73.11 61.74Table 1: Results on CoNLL 2006 shared task.dependency accuracy of 72.42%.
This was not sta-tistically significantly different from the top-rankedscore of 73.44%.
For Spanish, our labeled depen-dency accuracy of 80.36% is within 0.1% of thethird-ranked score of 80.46%.
Our unlabeled de-pendency accuracy for Swedish was the best of allthe systems at 89.54%.
Our labeled accuracy forSwedish, however, at 79.69%, fell far short of thethird-best score of 82.31%.
We therefore focus onSwedish when considering the impact of our choiceof learning algorithm on our label accuracy.2 DataWe divided the shared data into training and devel-opment test sets, using larger development test sets196for the languages supplied with more data.
The de-velopment test set consisted of 250 sentences forArabic, Slovene, Spanish and Turkish, 500 sen-tences for Danish and Portuguese, and 1,000 sen-tences for the other languages.3 The ParserThe baseline parser predicts unlabeled directed de-pendencies.
As described in (Corston-Oliver et al,2006), we reimplemented the parser described in(McDonald et al, 2005) and validated their resultsfor Czech and English.The parser finds the highest-scoring parse y?among all possible parses y ?
Y for a given sen-tence:y?
= argmaxy?Ys(y) (1)The score s of a given parse y is the sum of thescores of all the dependency links (i,j) ?
y:s(y) =?
(i,j)?yd(i, j) =?
(i,j)?yw ?
f(i, j) (2)where the link (i,j) indicates a parent-child depen-dency between the token at position i and the tokenat position j.
The score d(i, j) of each dependencylink (i,j) is further decomposed as the weighted sumof its features f(i, j).To set w, we trained twenty averaged perceptronson different shuffles of the training data, using thedevelopment test set to determine when the percep-trons had converged.
The averaged perceptrons werethen combined to make a Bayes Point Machine (Har-rington et al, 2003).
At both training and run time,edges are scored independently, and Eisner?s O(N3)decoder (Eisner, 1996) is used to find the optimalparse.
This decoder produces only projective analy-ses, although it does allow for analyses with multipleroots.The features used for scoring the edges prior toapplying Eisner?s algorithm are extracted from eachpossible parent-child dependency.
The features in-clude the case-normalized original form and lemma1of each token , the part of speech (POS) tag of eachtoken, the POS tag of each intervening token and1If no lemma was specified, we truncated the original formby taking the first two characters for Chinese words consistingof two characters or more and the first five characters for wordsconsisting of five characters or more in the other languages.of each token to the left and right of the parent andchild.
Additional features are created by combiningthese atomic features, as described in (McDonald etal., 2005).
All features are in turn combined withthe direction of attachment and the distance betweentokens.
Distance was discretized, with individualbuckets for distances 0-4, a single bucket for 5-9,and a single bucket for 10+.
In sections 3.1 and 3.2we discuss the feature engineering we performed.3.1 Part of Speech FeaturesWe experimented with using the coarse POS tag andthe fine POS tag.
In our official submission, weused fine POS tags for all languages except Dutchand Turkish.
For Dutch and Turkish, using the finePOS tag resulted in a reduction in unlabeled depen-dency accuracy of 0.12% and 0.43% respectivelyon the development test sets, apparently because ofthe sparsity of the fine POS tags.
For German andSwedish, the fine and coarse POS tags are the sameso using the fine POS tag had no effect.
For otherlanguages, using the fine POS tag showed modestimprovements in unlabeled dependency accuracy.For Swedish, we performed an additional manipu-lation on the POS tags, normalizing the distinct POStags assigned to each verbal auxiliary and modal toa single tag ?aux?.
For example, in the Swedishdata all inflected forms of the verb ?vara?
(?be?)
aretagged as AV, and all inflected forms of the modal?ma?ste?
(?must?)
are tagged as MV.
This normaliza-tion caused unlabeled dependency accuracy on theSwedish development set to improve from 89.23%to 89.45%.3.2 Features for Root IdentificationAnalysis of the baseline parser?s errors suggestedthe need for additional feature types to improve theidentification of the root of the sentence.
In particu-lar, the parser was frequently making errors in iden-tifying the root of periphrastic constructions involv-ing an auxiliary verb or modal and a participle.
InGermanic languages, for example, the auxiliary ormodal typically occurs in second position in declar-ative main clauses or in initial position in cases ofsubject-aux inversion.
We added a collection of fea-tures intended to improve the identification of theroot.
The hope was that improved root identifica-tion would have a positive cascading effect in the197identification of other dependencies, since a failureto correctly identify the root of the sentence usuallymeans that the parse will have many other errors.We extracted four feature types, the original formof the first and last tokens in the sentence and thePOS of the first and last tokens in the sentence.These features were intended to identify declarativevs.
interrogative sentences.For each child and parent token being scored, wealso noted the following four features: ?child/parentis first non-punctuation token in sentence?,?child/parent is second non-punctuation token insentence?.
The features that identify the secondtoken in the sentence were intended to improvethe identification of verb-second phenomena.
Ofcourse, this is a linguistic oversimplification.
Verb-second phenomena are actually sensitive to the orderof constituents, not words.
We therefore added fourfeature types that considered the sequence of POStags to the left of the child or parent if they occurredwithin ten tokens of the beginning of the sentenceand the sequence of POS tags to the right of thechild or parent if they occurred within ten tokens ofthe end of the sentence.We also added features intended to improve theidentification of the root in sentences without a fi-nite verb.
For example, the Dutch training datacontained many simple responses to a question-answering task, consisting of a single noun phrase.Four simple features were used ?Child/Parent is theleftmost noun in the sentence?, ?Child/Parent is anoun but not the leftmost noun in the sentence?.These features were combined with an indicator?Sentence contains/does not contain a finite verb?.Child or parent tokens that were finite verbs wereflagged as likely candidates for being the root ofthe sentence if they were the leftmost finite verb inthe sentence and not preceded by a subordinatingconjunction or relative pronoun.
Finite verbs wereidentified by POS tags and morphological features,e.g.
in Spanish, verbs without the morphologicalfeature ?mod=n?
were identified as finite, while inPortuguese the fine POS tag ?v-fin?
was used.Similarly, various sets of POS tags were used toidentify subordinating conjunctions or relative pro-nouns for different languages.
For example, in Bul-garian the fine POS tag ?pr?
(relative pronoun) and?cs?
(subordinating conjunction) were used.
ForDutch, the morphological features ?onder?, ?betr?and ?voorinf?
were used to identify subordinatingconjunctions and relative pronouns.These features wreaked havoc with Turkish, averb-final language.
For certain other languages,dependency accuracy measured on the develop-ment test set improved by a modest amount, withmore dramatic improvements in root accuracy (F1measure combining precision and recall for non-punctuation root tokens).Since the addition of these features had been mo-tivated by verb-second phenomena in Germanic lan-guages, we were surprised to discover that the onlyGermanic language to demonstrate a marked im-provement in unlabeled dependency accuracy wasDanish, whose accuracy on the development set rosefrom 87.51% to 87.72%, while root accuracy F1rose from 94.12% to 94.72%.
Spanish showed amodest improvement in unlabeled dependency accu-racy, from 85.08% to 85.13%, but root F1 rose from80.08% to 83.57%.The features described above for identifying theleftmost finite verb not preceded by a subordinat-ing conjunction or relative pronoun did not im-prove Slovene unlabeled dependency accuracy, andso were not included in the set of root-identifyingfeatures in our Slovene CoNLL submission.
Closerexamination of the Slovene corpus revealed that pe-riphrastic constructions consisting of one or moreauxiliaries followed by a participle were annotatedwith the participle as the head, whereas for otherlanguages in the shared task the consensus view ap-pears to be that the auxiliary should be annotatedas the head.
Singling out the leftmost finite verb inSlovene when a participle ought to be selected as theroot of the sentence is therefore counter-productive.The other root identification features did improveroot F1 in Slovene.
Root F1 on the development testset rose from 45.82% to 46.43%, although overallunlabeled dependency accuracy on the developmenttest set fell slightly from 80.24% to 79.94%.3.3 Morphological FeaturesAs the preceding discussion shows, morphologicalinformation was occasionally used to assist in mak-ing finer-grained POS distinctions than were madein the POS tags, e.g., for distinguishing subordi-nating vs. coordinating conjunctions.
Aside from198these surgical uses of the morphological informationpresent in the CoNLL data, morphology was not ex-plicitly used by the baseline parser.
For example,there were no features that considered subject-verbagreement nor agreement of an adjective with thenumber or lexical gender of the noun it modified.However, it is possible that morphological informa-tion influenced the training of edge weights if theinformation was implicit in the POS tags.4 The Dependency Labeler4.1 ClassifierWe used a maximum entropy classifier (Berger et al,1996) to assign labels to the unlabeled dependen-cies produced by the Bayes Point Machine.
We usedthe same training and development test split that wasused to train the dependency parser.
We chose to usemaximum entropy classifiers because they can betrained relatively quickly while still offering reason-able classification accuracy and are robust in the faceof large numbers of superfluous features, a desirableproperty given the requirement that the same parserhandle multiple languages.
Furthermore, maximumentropy classifiers provide good probability distribu-tions over class labels.
This was important to us be-cause we had initially hoped to find the optimal setof dependency labels for the children of a given nodeby modeling the probability of each set of labelsconditioned on the lemma and POS of the parent.For example, labeling each dependant of a parentnode independently might result in three OBJECTrelations dependent on a single verb; modeling setsof relations ought to prevent this.
Unfortunately, thisapproach did not outperform labeling each node in-dependently.Therefore, the system we submitted labeled eachdependency independently, using the most probablelabel from the maximum entropy classifier.
We havenoted in previous experiments that our SVM imple-mentation often gives better one-best classificationaccuracy than our maximum entropy implementa-tion, but did not have time to train SVM classifiers.To see how much the choice of classification al-gorithm affected our official results, we trained a lin-ear SVM classifier for Swedish after the competitionhad ended, tuning parameters on the developmenttest set.
As noted in section 1, our system scoredhighest for Swedish in unlabeled dependency accu-racy at 89.54% but fell well short of the third-rankedsystem when measuring labeled dependency accu-racy.
Using an SVM classifier instead of a maxi-mum entropy classifier, Swedish label accuracy rosefrom 82.33% to 86.06%, and labeled attachment ac-curacy rose from 79.69% to 82.95%, which fallsbetween the first-ranked score of 84.58% and thesecond-ranked score of 82.55%.
Similarly, Japaneselabel accuracy rose from 93.20% to 93.96%, andlabeled attachment accuracy rose from 89.95% to90.77% when we replaced a maximum entropy clas-sifier with an SVM.
This labeled attachment resultof 90.77% is comparable to the official second placeresult of 90.71% for Japanese.
We conclude that atwo stage pipeline such as ours, in which the sec-ond stage labels dependencies in isolation, is greatlyimpacted by the choice of classifier.4.2 Features Used for LabelingWe extracted features from individual nodes in thedependency tree, parent-child features and featuresthat took nodes other than the parent and child intoaccount.The features extracted from each individual par-ent and child node were the original surface form,the lemma (see footnote 1 above), the coarse and finePOS tags and each morphological feature.The parent-child features are the direction ofmodification, the combination of the parent andchild lemmata, all combinations of parent and childlemma and coarse POS tag (e.g.
child lemma com-bined with coarse POS tag of the parent) and all pair-wise combinations of parent and child morphologyfeatures (e.g.
parent is feminine and child is plural).Additional features were verb position (whetherthe parent or child is the first or last verb in the sen-tence), coarse POS and lemma of the left and rightneighbors of the parent and child, coarse POS andlemma of the grandparent, number and coarse POStag sequence of siblings to the left and to the right ofthe child, total number of siblings of the child, num-ber of tokens governed by child, whether the par-ent has a verbal ancestor, lemma and morphologicalfeatures of the verb governing the child (if any), andcoarse POS tag combined with relative offset of eachsibling (e.g., the sibling two to the left of the child isa determiner).199For Slovene, the label accuracy using all of thefeatures above was 81.91%.
We retrained our max-imum entropy classifier by removing certain classesof features in order to determine their contribu-tion.
Removing the weight features caused a notabledrop, with label accuracy on the development test setfalling 0.52% to 81.39%.
Removing the grandpar-ent features (but including weight features) causedan even greater drop of 1.03% to 80.88%.
One placewhere the grandparent features were important wasin distinguishing between Adv and Atr relations.
Itappears that the relation between a noun and its gov-erning preposition or between a verb and its govern-ing conjunction is sensitive to the part of speech ofthe grandparent.
For example, we observed a num-ber of cases where the relation between a noun andits governing preposition had been incorrectly la-beled as Adv when it should have been Atr.
Theaddition of grandparent features allowed the classi-fier to make the distinction by looking at the POS ofthe grandparent; when the POS was noun, the clas-sifier tended to correctly choose the Atr label.5 ConclusionWe have described a two stage pipeline that first pre-dicts directed unlabeled dependencies and then la-bels them.
The system performed well on Slovene,Spanish and Swedish.
Feature engineering playedan important role both in predicting dependenciesand in labeling them.
Finally, replacing the maxi-mum entropy classifier used to label dependencieswith an SVM improves upon our official results.ReferencesAdam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.S.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006.
CoNLL-X shared task on multilingual depen-dency parsing.
In Proc.
of the Tenth Conf.
on Com-putational Natural Language Learning (CoNLL-X).SIGNLL.Simon Corston-Oliver, Anthony Aue, Kevin Duh, andEric Ringger.
2006.
Multilingual dependency parsingusing bayes point machines.
In Proc.
of HLT-NAACL2006.J.
Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
ofthe 16th Intern.
Conf.
on Computational Linguistics(COLING), pages 340?345.Edward Harrington, Ralf Herbrich, Jyrki Kivinen,John C. Platt, and Robert C. Williamson.
2003.
On-line bayes point machines.
In Proceedings of SeventhPacific-Asia Conference on Knowledge Discovery andData Mining, pages 241?252.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meetingof the Assocation for Computational Linguistics.200
