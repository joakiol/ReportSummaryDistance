Proceedings of NAACL-HLT 2013, pages 63?73,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMinimally Supervised Method for Multilingual Paraphrase Extractionfrom Definition Sentences on the WebYulan Yan?
Chikara Hashimoto?
Kentaro Torisawa?Takao Kawai?
Jun?ichi Kazama?
Stijn De Saeger???
?
?
??
??
Information Analysis LaboratoryUniversal Communication Research InstituteNational Institute of Information and Communications Technology (NICT){?
yulan, ?
ch, ?
torisawa, ?
?stijn}@nict.go.jpAbstractWe propose a minimally supervised methodfor multilingual paraphrase extraction fromdefinition sentences on the Web.
Hashimotoet al(2011) extracted paraphrases fromJapanese definition sentences on the Web, as-suming that definition sentences defining thesame concept tend to contain paraphrases.However, their method requires manually an-notated data and is language dependent.
Weextend their framework and develop a mini-mally supervised method applicable to multi-ple languages.
Our experiments show that ourmethod is comparable to Hashimoto et alsfor Japanese and outperforms previous unsu-pervised methods for English, Japanese, andChinese, and that our method extracts 10,000paraphrases with 92% precision for English,82.5% precision for Japanese, and 82% preci-sion for Chinese.1 IntroductionAutomatic paraphrasing has been recognized as animportant component for NLP systems, and manymethods have been proposed to acquire paraphraseknowledge (Lin and Pantel, 2001; Barzilay andMcKeown, 2001; Shinyama et al 2002; Barzilayand Lee, 2003; Dolan et al 2004; Callison-Burch,2008; Hashimoto et al 2011; Fujita et al 2012).We propose a minimally supervised method formultilingual paraphrase extraction.
Hashimoto et al(2011) developed a method to extract paraphrasesfrom definition sentences on the Web, based ontheir observation that definition sentences definingthe same concept tend to contain many paraphrases.Their method consists of two steps; they extract def-inition sentences from the Web, and extract phrasal(1) a. Paraphrasing is the use of your own words to express the au-thor?s ideas without changing the meaning.b.
Paraphrasing is defined as a process of transforming an expres-sion into another while keeping its meaning intact.
(2) a.
?????????????????????????????????????????
(Paraphrasing refers tothe replacement of an expression into another without changingthe semantic content.)b.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?????????????????????????????????
(Paraphrasing is a process of trans-forming an expression into another of the same language whilepreserving the meaning and content as much as possible.
)(3) a.
???????????????????????????????
(Paraphrasing refers to the transformationof sentence structure by the translator without changing themeaning of original text.)b.
?????????????????????????
(Paraphrasing is a translation method of keeping the content oforiginal text but not keeping the expression.
)Figure 1: Multilingual definition pairs on ?paraphrasing.
?paraphrases from the definition sentences.
Bothsteps require supervised classifiers trained by manu-ally annotated data, and heavily depend on their tar-get language.
However, the basic idea is actuallylanguage-independent.
Figure 1 gives examples ofdefinition sentences on the Web that define the sameconcept in English, Japanese, and Chinese (with En-glish translation).
As indicated by underlines, eachdefinition pair has a phrasal paraphrase.We aim at extending Hashimoto et als methodto a minimally supervised method, thereby enablingacquisition of phrasal paraphrases within one lan-guage, but in different languages without manuallyannotated data.
The first contribution of our workis to develop a minimally supervised method formultilingual definition extraction that uses a clas-sifier distinguishing definition from non-definition.The classifier is learnt from the first sentences in63Defini?n?sentencesDefini?n?pairs?
Paraphrase?candidates?Ranked?paraphrase?candidates?ClassifierWebDefini?on?Extrac?on?(Sec?on?2.1)Paraphrase?Extrac?on?
(Sec?on?2.2)Ranking?by?ScoreAutoma?cally?constructed?training?dataWebWikipediaFigure 2: Overall picture of our method.Wikipedia articles, which can be regarded as the def-inition of the title of Wikipedia article (Kazama andTorisawa, 2007) and hence can be used as positiveexamples.
Our method relies on a POS tagger, a de-pendency parser, a NER tool, noun phrase chunkingrules, and frequency thresholds for each language,in addition to Wikipedia articles, which can be seenas a manually annotated knowledge base.
How-ever, our method needs no additional manual anno-tation particularly for this task and thus we catego-rize our method as a minimally supervised method.On the other hand, Hashimoto et als method heav-ily depends on the properties of Japanese like theassumption that characteristic expressions of defini-tion sentences tend to appear at the end of sentencein Japanese.
We show that our method is applica-ble to English, Japanese, and Chinese, and that itsperformance is comparable to state-of-the-art super-vised methods (Navigli and Velardi, 2010).
Sincethe three languages are very different we believe thatour definition extraction method is applicable to anylanguage as long as Wikipedia articles of the lan-guage exist.The second contribution of our work is to de-velop a minimally supervised method for multi-lingual paraphrase extraction from definition sen-tences.
Again, Hashimoto et als method utilizesa supervised classifier trained with annotated dataparticularly prepared for this task.
We eliminate theneed for annotation and instead introduce a methodthat uses a novel similarity measure consideringthe occurrence of phrase fragments in global con-texts.
Our paraphrase extraction method is mostlylanguage-independent and, through experiments forthe three languages, we show that it outperformsunsupervised methods (Pas?ca and Dienes, 2005;Koehn et al 2007) and is comparable to Hashimotoet als supervised method for Japanese.Previous methods for paraphrase (and entailment)extraction can be classified into a distributional sim-ilarity based approach (Lin and Pantel, 2001; Gef-fet and Dagan, 2005; Bhagat et al 2007; Szpek-tor and Dagan, 2008; Hashimoto et al 2009) and aparallel corpus based approach (Barzilay and McK-eown, 2001; Shinyama et al 2002; Barzilay andLee, 2003; Dolan et al 2004; Callison-Burch,2008).
The former can exploit large scale monolin-gual corpora, but is known to be unable to distin-guish paraphrase pairs from antonymous pairs (Linet al 2003).
The latter rarely mistakes antonymouspairs for paraphrases, but preparing parallel corporais expensive.
As with Hashimoto et al(2011), ourmethod is a kind of parallel corpus approach in that ituses definition pairs as a parallel corpus.
However,our method does not suffer from a high labor costof preparing parallel corpora, since it can automati-cally collect definition pairs from the Web on a largescale.
The difference between ours and Hashimotoet als is that our method requires no manual label-ing of data and is mostly language-independent.2 Proposed MethodOur method first extracts definition sentences fromthe Web, and then extracts paraphrases from the def-inition sentences, as illustrated in Figure 2.2.1 Definition Extraction2.1.1 Automatic Construction of Training DataOur method learns a classifier that classifies sen-tences into definition and non-definition using auto-matically constructed training data, TrDat.
TrDat?spositive examples, Pos, are the first sentences ofWikipedia articles and the negative examples, Neg,are randomly sampled Web sentences.
The formercan be seen as definition, while the chance that thesentences in the latter are definition is quite small.Our definition extraction not only distinguishesdefinition from non-definition but also identities thedefined term of a definition sentence, and in theparaphrase extraction step our method couples twodefinition sentences if their defined terms are identi-cal.
For example, the defined terms of (1a) and (1b)in Figure 1 are both ?Paraphrasing?
and thus the twodefinition sentences are coupled.
For Pos, we markup the title of Wikipedia article as the defined term.For Neg, we randomly select a noun phrase in a sen-64(A)N-gram definition pattern N-gram non-definition pattern?
[term] is the [term] may be[term] is a type of [term] is not(B)Subsequence definition pattern Subsequence non-definition pattern[term] is * which is located you may * [term][term] is a * in the was [term] * , who is(C)Subtree definition pattern Subtree non-definition pattern[term] is defined as the NP [term] will not beTable 1: Examples of English patterns.tence and mark it up as a (false) defined term.
Anymarked term is uniformly replaced with [term].2.1.2 Feature Extraction and LearningAs features, we use patterns that are characteristicof definition (definition patterns) and those that areunlikely to be a part of definition (non-definition pat-terns).
Patterns are either N-grams, subsequences, ordependency subtrees, and are mined automaticallyfrom TrDat.
Table 1 shows examples of patternsmined by our method.
In (A) of Table 1, ???
isa symbol representing the beginning of a sentence.In (B), ?*?
represents a wildcard that matches anynumber of arbitrary words.
Patterns are representedby either their words?
surface form, base form, orPOS.
(Chinese words do not inflect and thus we donot use the base form for Chinese.
)We assume that definition patterns are fre-quent in Pos but are infrequent in Neg, andnon-definition patterns are frequent in Neg butare infrequent in Pos.
To see if a given pat-tern ?
is likely to be a definition pattern, wemeasure ?
?s probability rate Rate(?).
If theprobability rate of ?
is large, ?
tends to be adefinition pattern.
The probability rate of ?
is:Rate(?)
=freq(?,Pos)/|Pos|freq(?,Neg)/|Neg|, iffreq(?,Neg) 6= 0.Here, freq(?,Pos) = |{s ?
Pos : ?
?
s}| andfreq(?,Neg) = |{s ?
Neg : ?
?
s}|.
We write ?
?
sif sentence s contains ?.
If freq(?,Neg) = 0,Rate(?)
is set to the largest value of all the patterns?Rate values.
Only patterns whose Rate is morethan or equal to a Rate threshold ?pos and whosefreq(?,Pos) is more than or equal to a frequencythreshold are regarded as definition patterns.
Simi-larly, we check if ?
is likely to be a non-definitionpattern.
Only patterns whose Rate is less or equalEnglish Japanese ChineseType Representation Pos Neg Pos Neg Pos NegN-gramSurface 120 400 30 100 20 100Base 120 400 30 100 ?
?POS 2,000 4,000 500 500 100 400SubsequenceSurface 120 400 30 100 20 40Base 120 400 30 100 ?
?POS 2,000 2,000 500 500 200 400SubtreeSurface 5 10 5 10 5 5Base 5 10 5 10 ?
?POS 25 50 25 50 25 50Table 2: Values of frequency threshold.to a Rate threshold ?neg and whose freq(?,Neg)is more than or equal to a frequency threshold areregarded as non-definition patterns.
The probabilityrate is based on the growth rate (Dong and Li,1999).
?pos and ?neg are set to 2 and 0.5, while the fre-quency threshold is set differently according to lan-guages, pattern types (N-gram, subsequence, andsubtree), representation (surface, base, and POS),and data (Pos and Neg), as in Table 2.
The thresholdsin Table 2 were determined manually, but not reallyarbitrarily.
Basically they were determined accord-ing to the frequency of each pattern in our data (e.g.how frequently the surface N-gram of English ap-pears in English positive training samples (Pos)).Below, we detail how patterns are acquired.
First,we acquire N-gram patterns.
Then, subsequencepatterns are acquired using the N-gram patterns asinput.
Finally, subtree patterns are acquired usingthe subsequence patterns as input.N-gram patterns We collect N-gram patternsfrom TrDat with N ranging from 2 to 6.
We filterout N-grams using thresholds on the Rate and fre-quency, and regard those that are kept as definitionor non-definition N-grams.Subsequence patterns We generate subsequencepatterns as ordered combinations of N-grams withthe wild card ?*?
inserted between them (we usetwo or three N-grams for a subsequence).
Then, wecheck each of the generated subsequences and keepit if there exists a sentence in TrDat that contains thesubsequence and whose root node is contained in thesubsequence.
For example, subsequence ?
[term]is a * in the?
is kept if a term-marked sentence like?
[term] is a baseball player in the Dominican Re-public.?
exists in TrDat.
Then, patterns are filtered65out using thresholds on the Rate and frequency aswe did for N-grams.Subtree patterns For each definition and non-definition subsequence, we retrieve all the term-marked sentences that contain the subsequence fromTrDat, and extract a minimal dependency subtreethat covers all the words of the subsequence fromeach retrieved sentence.
For example, assume thatwe retrieve a term-marked sentence ?
[term] isusually defined as the way of life of a group of peo-ple.?
for subsequence ?
[term] is * defined as the?.Then we extract from the sentence the minimal de-pendency subtree in the left side of (C) of Table 1.Note that all the words of the subsequence are con-tained in the subtree, and that in the subtree a node(?way?)
that is not a part of the subsequence is re-placed with its dependency label (?NP?)
assigned bythe dependency parser.
The patterns are filtered outusing thresholds on the Rate and frequency.We train a SVM classifier1 with a linear kernel,using binary features that indicate the occurrence ofthe patterns described above in a target sentence.In theory, we could feed all the features to theSVM classifier and let the classifier pick informa-tive features.
But we restricted the feature set forpractical reasons: the number of features would be-come tremendously large.
There are two reasons forthis.
First, the number of sentences in our automati-cally acquired training data is huge (2,439,257 posi-tive sentences plus 5,000,000 negative sentences forEnglish, 703,208 positive sentences plus 1,400,000negative sentences for Japanese and 310,072 posi-tive sentences plus 600,000 negative sentences forChinese).
Second, since each subsequence patternis generated as a combination of two or three N-gram patterns and one subsequence pattern can gen-erate one or more subtree patterns, using all possi-ble features leads to a combinatorial explosion offeatures.
Moreover, since the feature vector will behighly sparse with a huge number of infrequent fea-tures, SVM learning becomes very time consuming.In preliminary experiments we observed that whenusing all possible features the learning process tookmore than one week for each language.
We there-fore introduced the current feature selection method,in which the learning process finished in one day but1http://svmlight.joachims.org.Original Web sentence: Albert Pujols is a baseball player.Term-marked sentence 1: [term] is a baseball player.Term-marked sentence 2: Albert Pujols is a [term].Figure 3: Term-marked sentences from a Web sentence.still obtains good results.2.1.3 Definition Extraction from the WebWe extract a large amount of definition sen-tences by applying this classifier to sentences in ourWeb archive.
Because our classifier requires term-marked sentences (sentences in which the term be-ing defined is marked) as input, we first have to iden-tify all such defined term candidates for each sen-tence.
For example, Figure 3 shows a case where aWeb sentence has two NPs (two candidates of de-fined term).
Basically we pick up NPs in a sen-tence by simple heuristic rules.
For English, NPs areidentified using TreeTagger (Schmid, 1995) and twoNPs are merged into one when they are connected by?for?
or ?of?.
After applying this procedure recur-sively, the longest NPs are regarded as candidates ofdefined terms and term-marked sentences are gener-ated.
For Japanese, we first identify nouns that areoptionally modified by adjectives as NPs, and allowtwo NPs connected by ???
(of ), if any, to forma larger NP.
For Chinese, nouns that are optionallymodified by adjectives are considered as NPs.Then, each term-marked sentence is given a fea-ture vector and classified by the classifier.
The term-marked sentence whose SVM score (the distancefrom the hyperplane) is the largest among those fromthe same original Web sentence is chosen as the finalclassification result for the original Web sentence.2.2 Paraphrase ExtractionWe use all the Web sentences classified as defini-tion and all the sentences in Pos for paraphrase ex-traction.
First, we couple two definition sentenceswhose defined term is the same.
We filter out defini-tion sentence pairs whose cosine similarity of con-tent word vectors is less than or equal to thresholdC, which is set to 0.1.
Then, we extract phrasesfrom each definition sentence, and generate all pos-sible phrase pairs from the coupled sentences.
Inthis study, phrases are restricted to predicate phrasesthat consist of at least one dependency relation andin which all the constituents are consecutive in a66f1The ratio of the number of words shared between two can-didate phrases to the number of all of the words in the twophrases.
Words are represented by either their surface form(f1,1), base form (f1,2) or POS (f1,3).f2The identity of the leftmost word (surface form (f2,1), baseform (f2,2) or POS (f2,3)) between two candidate phrases.f3The same as f2 except that we use the rightmost word.There are three corresponding subfunctions (f3,1 to f3,3).f4The ratio of the number of words that appear in a candidatephrase segment of a definition sentence s1 and in a segmentthat is NOT a part of the candidate phrase of another def-inition sentence s2 to the number of all the words of s1?scandidate phrase.
Words are in their base form (f4,1).f5 The reversed (s1 ?
s2) version of f4,1 (f5,1).f6The ratio of the number of words (the surface form) of ashorter candidate phrase to that of a longer one (f6,1).f7Cosine similarity between two definition sentences fromwhich two candidate phrases are extracted.
Only contentwords in the base form are used (f7,1).f8The ratio of the number of parent dependency subtrees thatare shared by two candidate phrases to the number of all theparent dependency subtrees.
The parent dependency sub-trees are adjacent to the candidate phrases and representedby their surface form (f8,1), base form (f8,2), or POS (f8,3).f9The same as f8 except that we use child dependency sub-trees.
There are 3 subfunctions (f9,1 to f9,3) of f9 type.f10The ratio of the number of context N-grams that are sharedby two candidate phrases to the number of all the context N-grams of both candidate phrases.
The context N-grams areadjacent to the candidate phrases and represented by eitherthe surface form, the base form, or POS.
The N ranges from1 to 3, and the context is either left-side or right-side.
Thus,there are 18 subfunctions (3?
3?
2).Table 3: Local similarity subfunctions, f1,1 to f10,18.sentence.
Accordingly, if two definition sentencesthat are coupled have three such predicate phrasesrespectively, we get nine phrase pairs, for instance.A phrase pair extracted from a definition pair is aparaphrase candidate and is given a score that indi-cates the likelihood of being a paraphrase, Score.
Itconsists of two similarity measures, local similarityand global similarity, which are detailed below.Local similarity Following Hashimoto et al weassume that two candidate phrases (p1, p2) tend tobe a paraphrase if they are similar enough and/ortheir surrounding contexts are sufficiently similar.Then, we calculate the local similarity (localSim) of(p1, p2) as the weighted sum of 37 similarity sub-functions that are grouped into 10 types (Table 3.
)For example, the f1 type consists of three subfunc-tions, f1,1, f1,2, and f1,3.
The 37 subfunctions areinspired by Hashimoto et als features.
Then, local-Sim is defined as:localSim(p1, p2) = max(dl,dm)?DP (p1,p2)ls(p1, p2, dl, dm).Here, ls(p1, p2, dl, dm) =?10i=1?kij=1wi,j?fi,j(p1,p2,dl,dm)ki .DP (p1, p2) is the set of all definition sentence pairsthat contain (p1, p2).
(dl, dm) is a definition sen-tence pair containing (p1, p2).
ki is the numberof subfunctions of fi type.
wi,j is the weight forfi,j .
wi,j is uniformly set to 1 except for f4,1and f5,1, whose weight is set to ?1 since theyindicate the unlikelihood of (p1, p2)?s being aparaphrase.
As the formula indicates, if there ismore than one definition sentence pair that contains(p1, p2), localSim is calculated from the definitionsentence pair that gives the maximum value ofls(p1, p2, dl, dm).
localSim is local in the sense thatit is calculated based on only one definition pairfrom which (p1, p2) are extracted.Global similarity The global similarity (global-Sim) is our novel similarity function.
We decomposea candidate phrase pair (p1, p2) into Comm, the com-mon part between p1 and p2, and Diff , the differencebetween the two.
For example, Comm and Diff of(?keep the meaning intact?, ?preserve the meaning?
)is (?the meaning?)
and (?keep, intact?, ?preserve?
).globalSim measures the semantic similarity ofthe Diff of a phrase pair.
It is proposed based onthe following intuition: phrase pair (p1, p2) tendto be a paraphrase if their surface difference (i.e.Diff ) have the same meaning.
For example, if?keep, intact?
and ?preserve?
mean the same, then(?keep the meaning intact?, ?preserve the meaning?
)is a paraphrase.globalSim considers the occurrence of Diff inglobal contexts (i.e., all the paraphrase candidatesfrom all the definition pairs).
The globalSim of agiven phrase pair (p1, p2) is measured by basicallycounting how many times the Diff of (p1, p2) ap-pears in all the candidate phrase pairs from all thedefinition pairs.
The assumption is that Diff tends toshare the same meaning if it appears repeatedly inparaphrase candidates from all definition sentencepairs, i.e., our parallel corpus.
Each occurrence ofDiff is weighted by the localSim of the phrase pairin which Diff occurs.
Precisely, globalSim is definedas:67Threshold The frequency threshold of Table 2 (Section 2.1.2).NP rule Rules for identifying NPs in sentences (Section 2.1.3).POS list The list of content words?
POS (Section 2.2).Tagger/parser POS taggers, dependency parsers and NER tools.Table 4: Language-dependent components.globalSim(p1, p2) =?
(pi,pj)?PP (p1,p2)localSim(pi, pj)M.PP (p1, p2) is the set of candidate phrase pairswhose Diff is the same as (p1, p2).2 M is the num-ber of similarity subfunction types whose weight is1, i.e.
M = 8 (all the subfunction types except f4and f5).
It is used to normalize the value of eachoccurrence of Diff to [0, 1].3 globalSim is globalin the sense that it considers all the definition pairsthat have a phrase pair with the same Diff as a targetcandidate phrase pair (p1, p2).The final score for a candidate phrase pair is:Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2).The way of combining the two similarity functionshas been determined empirically after testing severalother ways of combining them.
This ranks all thecandidate phrase pairs.Finally, we summarize language-dependent com-ponents that we fix manually in Table 4.3 Experiments3.1 Experiments of Definition ExtractionWe show that our unsupervised definition extrac-tion method is competitive with state-of-the-art su-pervised methods for English (Navigli and Velardi,2010), and that it extracts a large number of defini-tions reasonably accurately for English (3,216,121definitions with 70% precision), Japanese (651,293definitions with 62.5% precision), and Chinese(682,661 definitions with 67% precision).2If there are more than one (pi, pj) in a definition pair, weuse only one of them that has the largest localSim value.3Although we claim that our idea of using globalSim is ef-fective, we do not claim that the above formula for calculatingis the optimal way to implement the idea.
Currently we are in-vestigating a more mathematically well-motivated model.3.1.1 Preparing CorporaFirst we describe Pos, Neg, and the Web corpusfrom which definition sentences are extracted.
Asthe source of Pos, we used the English Wikipediaof April 2011 (3,620,149 articles), the JapaneseWikipedia of October 2011 (830,417 articles), andthe Chinese Wikipedia of August 2011 (365,545 ar-ticles).
We removed category articles, template ar-ticles, list articles and so on from them.
Then thenumber of sentences of Pos was 2,439,257 for En-glish, 703,208 for Japanese, and 310,072 for Chi-nese.
We verified our assumption that Wikipediafirst sentences can mostly be seen as definition bymanually checking 200 random samples from Pos.96.5% of English Pos, 100% of Japanese Pos, and99.5% of Chinese Pos were definitions.As the source of Neg, we used 600 millionJapanese Web pages (Akamine et al 2010) andthe ClueWeb09 corpus for English (about 504 mil-lion pages) and Chinese (about 177 million pages).4From each Web corpus, we collected the sentencessatisfying following conditions: 1) they contain 5to 50 words and at least one verb, 2) less than halfof their words are numbers, and 3) they end with aperiod.
Then we randomly sampled sentences fromthe collected sentences as Neg so that |Neg| wasabout twice as large as |Pos|: 5,000,000 for English,1,400,000 for Japanese, and 600,000 for Chinese.In Section 3.1.3, we use 10% of the Web corpus asthe input to the definition classifier.
The number ofsentences are 294,844,141 for English, 245,537,860for Japanese, and 68,653,130 for Chinese.All the sentences were POS-tagged and parsed.We used TreeTagger and MSTParser (McDonaldet al 2006) for English, JUMAN (Kurohashi andKawahara, 2009a) and KNP (Kurohashi and Kawa-hara, 2009b) for Japanese, MMA (Kruengkrai et al2009) and CNP (Chen et al 2009) for Chinese.3.1.2 Comparison with Previous MethodsWe compared our method with the state-of-the-art supervised methods proposed by Navigli and Ve-lardi (2010), using their WCL datasets v1.0 (http://lcl.uniroma1.it/wcl/), definition and non-definition datasets for English (Navigli et al 2010).Specifically, we used its training data (TrDatwcl,hereafter), which consisted of 1,908 definition and4http://lemurproject.org/clueweb09.php/68Method Precision Recall F1 AccuracyProposeddef 86.79 86.97 86.88 89.18WCL-1 99.88 42.09 59.22 76.06WCL-3 98.81 60.74 75.23 83.48Table 5: Definition classification results on TrDatwcl.2,711 non-definition sentences, and compared thefollowing three methods.
WCL-1 and WCL-3 aremethods proposed by Navigli and Velardi (2010).They were trained and tested with 10 fold cross vali-dation using TrDatwcl.
Proposeddef is our method,which used TrDat for acquiring patterns (Section2.1.2) and training.
We tested Proposeddef on eachof TrDatwcl?s 10 folds and averaged the results.Note that, for Proposeddef , we removed sentencesin TrDatwcl from TrDat in advance for fairness.Table 5 shows the results.
The numbers for WCL-1 and WCL-3 are taken from Navigli and Velardi(2010).
Proposeddef outperformed both methods interms of recall, F1, and accuracy.
Thus, we concludethat Proposeddef is comparable to WCL-1/WCL-3.We conducted ablation tests of our method to in-vestigate the effectiveness of each type of pattern.When using only N-grams, F1 was 85.41.
Whenusing N-grams and subsequences, F1 was 86.61.When using N-grams and subtrees, F1 was 86.85.When using all the features, F1 was 86.88.
The re-sults show that each type of patterns contribute to theperformance, but the contributions of subsequencepatterns and subtree patterns do not seem very sig-nificant.3.1.3 Experiments of Definition ExtractionWe extracted definitions from 10% of the Webcorpus.
We applied Proposeddef to the cor-pus of each language, and the state-of-the-art su-pervised method for Japanese (Hashimoto et al2011) (Hashidef , hereafter) to the Japanese corpus.Hashidef was trained on their training data that con-sisted of 2,911 sentences, 61.1% of which were def-initions.
Note that we removed sentences in TrDatfrom 10% of the Web corpus in advance, while wedid not remove Hashimoto et als training data fromthe corpus.
This means that, for Hashidef , the train-ing data is included in the test data.For each method, we filtered out its positive out-puts whose defined term appeared more than 1,000times in 10% of the Web corpus, since those termstend to be too vague to be a defined term or re-fer to an entity outside the definition sentence.
Forexample, if ?the college?
appears more than 1,000times in 10% of the corpus, we filter out sen-tences like ?The college is one of three collegesin the Coast Community College District and wasfounded in 1947.?
For Proposeddef , the number ofremaining positive outputs is 3,216,121 for English,651,293 for Japanese, and 682,661 for Chinese.
ForHashidef , the number of positive outputs is 523,882.For Proposeddef of each language, we randomlysampled 200 sentences from the remaining positiveoutputs.
For Hashidef , we first sorted its output bythe SVM score in descending order and then ran-domly sampled 200 from the top 651,293, i.e., thesame number as the remaining positive outputs ofProposeddef of Japanese, out of all the remainingsentences of Hashidef .For each language, after shuffling all the samples,two human annotators evaluated each sample.
Theannotators for English and Japanese were not the au-thors, while one of the Chinese annotators was oneof the authors.
We regarded a sample as a defini-tion if it was regarded as a definition by both an-notators.
Cohen?s kappa (Cohen, 1960) was 0.55for English (moderate agreement (Landis and Koch,1977)), 0.73 for Japanese (substantial agreement),and 0.69 for Chinese (substantial agreement).For English, Proposeddef achieved 70% precisionfor the 200 samples.
For Japanese, Proposeddefachieved 62.5% precision for the 200 samples, whileHashidef achieved 70% precision for the 200 sam-ples.
For Chinese, Proposeddef achieved 67% pre-cision for the 200 samples.
From these results, weconclude that Proposeddef can extract a large num-ber of definition sentences from the Web moderatelywell for the three languages.Although the precision is not very high, our ex-periments in the next section show that we can stillextract a large number of paraphrases with high pre-cision from these definition sentences, due mainly toour similarity measures, localSim and globalSim.3.2 Experiments of Paraphrase ExtractionWe show (1) that our paraphrase extraction methodoutperforms unsupervised methods for the three lan-guages, (2) that globalSim is effective, and (3) thatour method is comparable to the state-of-the-art su-69ProposedScore: Our method.
Outputs are ranked by Score.Proposedlocal: This is the same as ProposedScore except that it ranksoutputs by localSim.
The performance drop from ProposedScoreshows globalSim?s effectiveness.Hashisup: Hashimoto et als supervised method.
Training data is thesame as Hashimoto et alOutputs are ranked by the SVM score(the distance from the hyperplane).
This is for Japanese only.Hashiuns: The unsupervised version of Hashisup.
Outputs areranked by the sum of feature values.
Japanese only.SMT: The phrase table construction method of Moses (Koehn et al2007).
We assume that Moses should extract a set of two phrasesthat are paraphrases of each other, if we input monolingual par-allel sentence pairs like our definition pairs.
We used defaultvalues for all the parameters.
Outputs are ranked by the productof two phrase translation probabilities of both directions.P&D: The distributional similarity based method by Pas?ca and Di-enes (2005) (their ?N-gram-Only?
method).
Outputs are rankedby the number of contexts two phrases share.
Following Pas?caand Dienes (2005), we used the parameters LC = 3 andMaxP = 4, while MinP , which was 1 in Pas?ca and Dienes(2005), was set to 2 since our target was phrasal paraphrases.Table 6: Evaluated paraphrase extraction methods.pervised method for Japanese.3.2.1 Experimental SettingWe extracted paraphrases from definition sen-tences in Pos and those extracted by Proposeddef inSection 3.1.3.
First we coupled two definition sen-tences whose defined term was the same.
The num-ber of definition pairs was 3,208,086 for English,742,306 for Japanese, and 457,233 for Chinese.Then we evaluated six methods in Table 6.5 Allthe methods except P&D took the same definitionpairs as input, while P&D?s input was 10% of theWeb corpus.
The input can be seen as the same forall the methods, since the definition pairs were de-rived from that 10% of the Web corpus.
In our ex-periments Exp1 and Exp2 below, all evaluation sam-ples were shuffled so that human annotators couldnot know which sample was from which method.Annotators were the same as those who conductedthe evaluation in Section 3.1.3.
Cohen?s kappa (Co-hen, 1960) was 0.83 for English, 0.88 for Japanese,5We filtered out phrase pairs in which one phrase contained anamed entity but the other did not contain the named entity fromthe output of ProposedScore, Proposedlocal, SMT , and P&D,since most of them were not paraphrases.
We used StanfordNER (Finkel et al 2005) for English named entity recognition(NER), KNP for Japanese NER, and BaseNER (Zhao and Kit,2008) for Chinese NER.
Hashisup and Hashiuns did the namedentity filtering of the same kind (footnote 3 of Hashimoto et al(2011)), and thus we did not apply the filter to them any further.and 0.85 for Chinese, all of which indicated reason-ably good (Landis and Koch, 1977).
We regarded acandidate phrase pair as a paraphrase if both annota-tors regarded it as a paraphrase.Exp1 We compared the methods that take def-inition pairs as input, i.e.
ProposedScore, Pro-posedlocal, Hashisup, Hashiuns, and SMT .
We ran-domly sampled 200 phrase pairs from the top 10,000for each method for evaluation.
The evaluation ofeach candidate phrase pair (p1, p2) was based onbidirectional checking of entailment relation, p1 ?p2 and p2 ?
p1, with p1 and p2 embedded in con-texts, as Hashimoto et al(2011) did.
Entailmentrelation of both directions hold if (p1, p2) is a para-phrase.
We used definition pairs from which candi-date phrase pairs were extracted as contexts.Exp2 We compared ProposedScore and P&D.Since P&D restricted its output to phrase pairs inwhich each phrase consists of two to four words,we restricted the output of ProposedScore to 2-to-4-words phrase pairs, too.
We randomly sampled 200from the top 3,000 phrase pairs from each methodfor evaluation, and the annotators checked entail-ment relation of both directions between two phrasesusing Web sentence pairs that contained the twophrases as contexts.3.2.2 ResultsFrom Exp1, we obtained precision curves in theupper half of Figure 4.
The curves were drawn fromthe 200 samples that were sorted in descending orderby their score, and we plotted a dot for every 5 sam-ples.
ProposedScore outperformed Proposedlocal forthe three languages, and thus globalSim was effec-tive.
ProposedScore outperformed Hashisup.
How-ever, we observed that ProposedScore acquired manycandidate phrase pairs (p1, p2) for which p1 and p2consisted of the same content words like ?send apostcard to the author?
and ?send the author a post-card,?
while the other methods tended to acquiremore content word variations like ?have a chance?and ?have an opportunity.?
Then we evaluated allthe methods in terms of how many paraphrases withcontent word variations were extracted.
We ex-tracted from the evaluation samples only candidatephrase pairs whose Diff contained a content word(content word variation pairs), to see how many7000.20.40.60.810  50  100  150  200Precision(A) Top N (#Samples)  00.20.40.60.810  50  100  150  200(B) Top N (#Samples)  00.20.40.60.810  50  100  150  200(C) Top N (#Samples)?Proposed_score??Proposed_local??SMT??Hashi_sup?
?Hashi_uns?00.20.40.60.810  50  100  150  200Precision(a) Top N (#Samples)  00.20.40.60.810  50  100  150  200(b) Top N (#Samples)  00.20.40.60.810  50  100  150  200(c) Top N (#Samples)?Proposed_score_cwv??Proposed_local_cwv??SMT_cwv??Hashi_sup_cwv?
?Hashi_uns_cwv?Figure 4: Precision curves of Exp1: English (A)(a), Chinese (B)(b), and Japanese (C)(c).00.20.40.60.810  50  100  150  200Precision(A) Top N (#Samples)  00.20.40.60.810  50  100  150  200(B) Top N (#Samples)  00.20.40.60.810  50  100  150  200(C) Top N (#Samples)?Proposed_score??Proposed_score_cwv??Pasca?
?Pasca_cwv?Figure 5: Precision curves of Exp2: English (A), Chinese (B), and Japanese (C).of them were paraphrases.
The lower half of Fig-ure 4 shows the results (curves labeled with cwv).The number of samples for ProposedScore reduceddrastically compared to the others for English andJapanese, though precision was kept at a high level.It is due mainly to the globalSim; the Diff of thenon-content word variation pairs appears frequentlyin paraphrase candidates, and thus their globalSimscores are high.From Exp2, precision curves in Figure 5 wereobtained.
P&D acquired more content word varia-tion pairs as the curves labeled by cwv indicates.However, ProposedScore?s precision outperformedP&D?s by a large margin for the three languages.From all of these results, we conclude (1) that ourparaphrase extraction method outperforms unsuper-vised methods for the three languages, (2) that glob-alSim is effective, and (3) that our method is com-parable to the state-of-the-art supervised method forJapanese, though our method tends to extract fewercontent word variation pairs than the others.Table 7 shows examples of English paraphrasesextracted by ProposedScore.is based in Halifax = is headquartered in Halifaxused for treating HIV = used to treat HIVis a rare form = is an uncommon typeis a set = is an unordered collectionhas an important role = plays a key roleTable 7: Examples of extracted English paraphrases.4 ConclusionWe proposed a minimally supervised method formultilingual paraphrase extraction.
Our experimentsshowed that our paraphrase extraction method out-performs unsupervised methods (Pas?ca and Dienes,2005; Koehn et al 2007; Hashimoto et al 2011)for English, Japanese, and Chinese, and is compara-ble to the state-of-the-art language dependent super-vised method for Japanese (Hashimoto et al 2011).71ReferencesSusumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, TakuyaKawada, Kentaro Inui, Sadao Kurohashi, and YutakaKidawara.
2010.
Organizing information on the webto support user judgments on information credibil-ity.
In Proceedings of 2010 4th International Uni-versal Communication Symposium Proceedings (IUCS2010), pages 122?129.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL2003, pages 16?23.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting of the ACL jointwith the 10th Meeting of the European Chapter of theACL (ACL/EACL 2001), pages 50?57.Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning direc-tionality of inference rules.
In Proceedings of Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP2007), pages 161?170.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 196?205.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improving dependencyparsing with subtrees from auto-parsed data.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?09,pages 570?579, Singapore.
Association for Computa-tional Linguistics.Jacob Cohen.
1960.
Coefficient of agreement for nom-inal scales.
In Educational and Psychological Mea-surement, pages 37?46.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics (COLING 2004), pages 350?356, Geneva, Switzerland, Aug 23?Aug 27.Guozhu Dong and Jinyan Li.
1999.
Efficient mining ofemerging patterns: discovering trends and differences.In Proceedings of the fifth ACM SIGKDD internationalconference on Knowledge discovery and data mining,KDD ?99, pages 43?52, San Diego, California, UnitedStates.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43nd Annual Meeting of the As-sociation for Computational Linguistics (ACL 2005),pages 363?370.Atsushi Fujita, Pierre Isabelle, and Roland Kuhn.
2012.Enlarging paraphrase collections through generaliza-tion and instantiation.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2012), pages 631?642.Maayan Geffet and Ido Dagan.
2005.
The distributionalinclusion hypotheses and lexical entailment.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2005), pages107?114.Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.2009.
Large-scale verb entailment acquisition fromthe web.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2009), pages 1172?1181.Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,Jun?ichi Kazama, and Sadao Kurohashi.
2011.
Ex-tracting paraphrases from definition sentences on theweb.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1087?1097, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named entityrecognition.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 698?707, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint chinese word segmentation and postagging.
In Proceedings of the Joint Conference of the7247th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 513?521, Suntec, Singapore,August.
Association for Computational Linguistics.Sadao Kurohashi and Daisuke Kawahara.
2009a.Japanese morphological analyzer system ju-man version 6.0 (in japanese).
Kyoto University,http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.Sadao Kurohashi and Daisuke Kawahara.
2009b.Japanese syntax and case analyzer knp version 3.0(in japanese).
Kyoto University, http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.J.
Richard Landis and Gary G. Koch.
1977.
Measure-ment of observer agreement for categorical data.
Bio-metrics, 33(1):159?174.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.2003.
Identifying synonyms among distributionallysimilar words.
In Proceedings of the 18th Inter-national Joint Conference on Artificial Intelligence(IJCAI-03), pages 1492?1493.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning, CoNLL-X ?06, pages 216?220, NewYork City, New York.Roberto Navigli and Paola Velardi.
2010.
Learningword-class lattices for definition and hypernym extrac-tion.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1318?1327, Uppsala, Sweden, July.
Association forComputational Linguistics.Roberto Navigli, Paola Velardi, and Juana Mar?
?a Ruiz-Mart??nez.
2010.
An annotated dataset for extractingdefinitions and hypernyms from the web.
In Proceed-ings of LREC 2010, pages 3716?3722.Marius Pas?ca and Pe?ter Dienes.
2005.
Aligning needlesin a haystack: paraphrase acquisition across the web.In Proceedings of the Second international joint con-ference on Natural Language Processing, IJCNLP?05,pages 119?130, Jeju Island, Korea.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to german.
In Proceedingsof the ACL SIGDAT-Workshop, pages 47?50.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proceedings of the 2nd international Con-ference on Human Language Technology Research(HLT2002), pages 313?318.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary template.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING2008), pages 849?856.Hai Zhao and Chunyu Kit.
2008.
Unsupervised seg-mentation helps supervised learning of character tag-ging for word segmentation and named entity recog-nition.
In Proceedings of the Sixth SIGHAN Workshopon Chinese Language Processing, pages 106?111, Hy-derabad, India.73
