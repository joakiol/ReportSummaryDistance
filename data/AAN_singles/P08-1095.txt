Proceedings of ACL-08: HLT, pages 834?842,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsYou talking to me?
A Corpus and Algorithm for ConversationDisentanglementMicha Elsner and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{melsner,ec}@@cs.brown.eduAbstractWhen multiple conversations occur simultane-ously, a listener must decide which conversa-tion each utterance is part of in order to inter-pret and respond to it appropriately.
We referto this task as disentanglement.
We present acorpus of Internet Relay Chat (IRC) dialoguein which the various conversations have beenmanually disentangled, and evaluate annota-tor reliability.
This is, to our knowledge, thefirst such corpus for internet chat.
We pro-pose a graph-theoretic model for disentangle-ment, using discourse-based features whichhave not been previously applied to this task.The model?s predicted disentanglements arehighly correlated with manual annotations.1 MotivationSimultaneous conversations seem to arise naturallyin both informal social interactions and multi-partytyped chat.
Aoki et al (2006)?s study of voice con-versations among 8-10 people found an average of1.76 conversations (floors) active at a time, and amaximum of four.
In our chat corpus, the average iseven higher, at 2.75.
The typical conversation, there-fore, is one which is interrupted?
frequently.Disentanglement is the clustering task of dividinga transcript into a set of distinct conversations.
It isan essential prerequisite for any kind of higher-leveldialogue analysis: for instance, consider the multi-party exchange in figure 1.Contextually, it is clear that this corresponds totwo conversations, and Felicia?s1 response ?excel-1Real user nicknames are replaced with randomly selected(Chanel) Felicia: google works :)(Gale) Arlie: you guys have never workedin a factory before have you(Gale) Arlie: there?s some real unethicalstuff that goes on(Regine) hands Chanel a trophy(Arlie) Gale, of course ... thats how theymake money(Gale) and people lose limbs or get killed(Felicia) excellentFigure 1: Some (abridged) conversation from our corpus.lent?
is intended for Chanel and Regine.
A straight-forward reading of the transcript, however, might in-terpret it as a response to Gale?s statement immedi-ately preceding.Humans are adept at disentanglement, even incomplicated environments like crowded cocktailparties or chat rooms; in order to perform this task,they must maintain a complex mental representationof the ongoing discourse.
Moreover, they adapt theirutterances to some degree to make the task easier(O?Neill and Martin, 2003), which suggests that dis-entanglement is in some sense a ?difficult?
discoursetask.Disentanglement has two practical applications.One is the analysis of pre-recorded transcripts inorder to extract some kind of information, such asquestion-answer pairs or summaries.
These tasksshould probably take as as input each separate con-versation, rather than the entire transcript.
Anotheridentifiers for ethical reasons.834application is as part of a user-interface system foractive participants in the chat, in which users target aconversation of interest which is then highlighted forthem.
Aoki et al (2003) created such a system forspeech, which users generally preferred to a conven-tional system?
when the disentanglement worked!Previous attempts to solve the problem (Aoki etal., 2006; Aoki et al, 2003; Camtepe et al, 2005;Acar et al, 2005) have several flaws.
They clus-ter speakers, not utterances, and so fail when speak-ers move from one conversation to another.
Theirfeatures are mostly time gaps between one utteranceand another, without effective use of utterance con-tent.
Moreover, there is no framework for a prin-cipled comparison of results: there are no reliableannotation schemes, no standard corpora, and noagreed-upon metrics.We attempt to remedy these problems.
We presenta new corpus of manually annotated chat room dataand evaluate annotator reliability.
We give a set ofmetrics describing structural similarity both locallyand globally.
We propose a model which uses dis-course structure and utterance contents in additionto time gaps.
It partitions a chat transcript into dis-tinct conversations, and its output is highly corre-lated with human annotations.2 Related WorkTwo threads of research are direct attempts to solvethe disentanglement problem: Aoki et al (2006),Aoki et al (2003) for speech and Camtepe et al(2005), Acar et al (2005) for chat.
We discusstheir approaches below.
However, we should em-phasize that we cannot compare our results directlywith theirs, because none of these studies publish re-sults on human-annotated data.
Although Aoki et al(2006) construct an annotated speech corpus, theygive no results for model performance, only user sat-isfaction with their conversational system.
Camtepeet al (2005) and Acar et al (2005) do give perfor-mance results, but only on synthetic data.All of the previous approaches treat the problemas one of clustering speakers, rather than utterances.That is, they assume that during the window overwhich the system operates, a particular speaker isengaging in only one conversation.
Camtepe et al(2005) assume this is true throughout the entire tran-script; real speakers, by contrast, often participatein many conversations, sequentially or sometimeseven simultaneously.
Aoki et al (2003) analyze eachthirty-second segment of the transcript separately.This makes the single-conversation restriction some-what less severe, but has the disadvantage of ignor-ing all events which occur outside the segment.Acar et al (2005) attempt to deal with this prob-lem by using a fuzzy algorithm to cluster speakers;this assigns each speaker a distribution over conver-sations rather than a hard assignment.
However, thealgorithm still deals with speakers rather than utter-ances, and cannot determine which conversation anyparticular utterance is part of.Another problem with these approaches is the in-formation used for clustering.
Aoki et al (2003) andCamtepe et al (2005) detect the arrival times of mes-sages, and use them to construct an affinity graph be-tween participants by detecting turn-taking behavioramong pairs of speakers.
(Turn-taking is typified byshort pauses between utterances; speakers aim nei-ther to interrupt nor leave long gaps.)
Aoki et al(2006) find that turn-taking on its own is inadequate.They motivate a richer feature set, which, however,does not yet appear to be implemented.
Acar etal.
(2005) adds word repetition to their feature set.However, their approach deals with all word repe-titions on an equal basis, and so degrades quicklyin the presence of noise words (their term for wordswhich shared across conversations) to almost com-plete failure when only 1/2 of the words are shared.To motivate our own approach, we examine somelinguistic studies of discourse, especially analysis ofmulti-party conversation.
O?Neill and Martin (2003)point out several ways in which multi-party text chatdiffers from typical two-party conversation.
One keydifference is the frequency with which participantsmention each others?
names.
They hypothesize thatmentioning is a strategy which participants use tomake disentanglement easier, compensating for thelack of cues normally present in face-to-face dia-logue.
Mentions (such as Gale?s comments to Ar-lie in figure 1) are very common in our corpus, oc-curring in 36% of comments, and provide a usefulfeature.Another key difference is that participants maycreate a new conversation (floor) at any time, a pro-cess which Sacks et al (1974) calls schisming.
Dur-835ing a schism, a new conversation is formed, notnecessarily because of a shift in the topic, but be-cause certain participants have refocused their atten-tion onto each other, and away from whoever heldthe floor in the parent conversation.Despite these differences, there are still strongsimilarities between chat and other conversationssuch as meetings.
Our feature set incorporates infor-mation which has proven useful in meeting segmen-tation (Galley et al, 2003) and the task of detect-ing addressees of a specific utterance in a meeting(Jovanovic et al, 2006).
These include word rep-etitions, utterance topic, and cue words which canindicate the bounds of a segment.3 DatasetOur dataset is recorded from the IRC (Internet Re-lay Chat) channel ##LINUX at freenode.net, usingthe freely-available gaim client.
##LINUX is an un-official tech support line for the Linux operating sys-tem, selected because it is one of the most active chatrooms on freenode, leading to many simultaneousconversations, and because its content is typicallyinoffensive.
Although it is notionally intended onlyfor tech support, it includes large amounts of socialchat as well, such as the conversation about factorywork in the example above (figure 1).The entire dataset contains 52:18 hours of chat,but we devote most of our attention to three anno-tated sections: development (706 utterances; 2:06hr) and test (800 utts.
; 1:39 hr) plus a short pilot sec-tion on which we tested our annotation system (359utts.
; 0:58 hr).3.1 AnnotationOur annotators were seven university students withat least some familiarity with the Linux OS, al-though in some cases very slight.
Annotation of thetest dataset typically took them about two hours.
Inall, we produced six annotations of the test set2.Our annotation scheme marks each utterance aspart of a single conversation.
Annotators are in-structed to create as many, or as few conversations asthey need to describe the data.
Our instructions statethat a conversation can be between any number of2One additional annotation was discarded because the anno-tator misunderstood the task.people, and that, ?We mean conversation in the typ-ical sense: a discussion in which the participants areall reacting and paying attention to one another.
.
.
itshould be clear that the comments inside a conver-sation fit together.?
The annotation system itself is asimple Java program with a graphical interface, in-tended to appear somewhat similar to a typical chatclient.
Each speaker?s name is displayed in a differ-ent color, and the system displays the elapsed timebetween comments, marking especially long pausesin red.
Annotators group sentences into conversa-tions by clicking and dragging them onto each other.3.2 MetricsBefore discussing the annotations themselves, wewill describe the metrics we use to compare differ-ent annotations; these measure both how much ourannotators agree with each other, and how well ourmodel and various baselines perform.
Comparingclusterings with different numbers of clusters is anon-trivial task, and metrics for agreement on su-pervised classification, such as the ?
statistic, are notapplicable.To measure global similarity between annota-tions, we use one-to-one accuracy.
This measure de-scribes how well we can extract whole conversationsintact, as required for summarization or informationextraction.
To compute it, we pair up conversationsfrom the two annotations to maximize the total over-lap3, then report the percentage of overlap found.If we intend to monitor or participate in the con-versation as it occurs, we will care more about lo-cal judgements.
The local agreement metric countsagreements and disagreements within a context k.We consider a particular utterance: the previousk utterances are each in either the same or a dif-ferent conversation.
The lock score between twoannotators is their average agreement on these ksame/different judgements, averaged over all utter-ances.
For example, loc1 counts pairs of adjacentutterances for which two annotations agree.836Mean Max MinConversations 81.33 128 50Avg.
Conv.
Length 10.6 16.0 6.2Avg.
Conv.
Density 2.75 2.92 2.53Entropy 4.83 6.18 3.001-to-1 52.98 63.50 35.63loc 3 81.09 86.53 74.75M-to-1 (by entropy) 86.70 94.13 75.50Table 1: Statistics on 6 annotations of 800 lines of chattranscript.
Inter-annotator agreement metrics (below theline) are calculated between distinct pairs of annotations.3.3 DiscussionA statistical examination of our data (table 1) showsthat that it is eminently suitable for disentanglement:the average number of conversations active at a timeis 2.75.
Our annotators have high agreement onthe local metric (average of 81.1%).
On the 1-to-1 metric, they disagree more, with a mean overlapof 53.0% and a maximum of only 63.5%.
This levelof overlap does indicate a useful degree of reliabil-ity, which cannot be achieved with naive heuristics(see section 5).
Thus measuring 1-to-1 overlap withour annotations is a reasonable evaluation for com-putational models.
However, we feel that the majorsource of disagreement is one that can be remediedin future annotation schemes: the specificity of theindividual annotations.To measure the level of detail in an annotation, weuse the information-theoretic entropy of the randomvariable which indicates which conversation an ut-terance is in.
This quantity is non-negative, increas-ing as the number of conversations grow and theirsize becomes more balanced.
It reaches its maxi-mum, 9.64 bits for this dataset, when each utteranceis placed in a separate conversation.
In our anno-tations, it ranges from 3.0 to 6.2.
This large vari-ation shows that some annotators are more specificthan others, but does not indicate how much theyagree on the general structure.
To measure this, weintroduce the many-to-one accuracy.
This measure-ment is asymmetrical, and maps each of the conver-sations of the source annotation to the single con-3This is an example of max-weight bipartite matching, andcan be computed optimally using, eg, max-flow.
The widelyused greedy algorithm is a two-approximation, although wehave not found large differences in practice.
(Lai) need money(Astrid) suggest a paypal fund or similar(Lai) Azzie [sic; typo for Astrid?
]: myshack guy here said paypal too but i haveno local bank acct(Felicia) second?s Azzie?s suggestion(Gale) we should charge the noobs $1 perquestion to [Lai?s] paypal(Felicia) bingo!
(Gale) we?d have the money in 2 days max(Azzie) Lai: hrm, Have you tried to setone up?
(Arlie) the federal reserve system conspir-acy is keeping you down man(Felicia) Gale: all ubuntu users .. pay up!
(Gale) and susers pay double(Azzie) I certainly would make suse userspay.
(Hildegard) triple.
(Lai) Azzie: not since being offline(Felicia) it doesn?t need to be ?in state?eitherFigure 2: A schism occurring in our corpus (abridged):not all annotators agree on where the thread about charg-ing for answers to techical questions diverges from theone about setting up Paypal accounts.
Either Gale?s orAzzie?s first comment seems to be the schism-inducingutterance.versation in the target with which it has the great-est overlap, then counts the total percentage of over-lap.
This is not a statistic to be optimized (indeed,optimization is trivial: simply make each utterancein the source into its own conversation), but it cangive us some intuition about specificity.
In partic-ular, if one subdivides a coarse-grained annotationto make a more specific variant, the many-to-oneaccuracy from fine to coarse remains 1.
When wemap high-entropy annotations (fine) to lower ones(coarse), we find high many-to-one accuracy, with amean of 86%, which implies that the more specificannotations have mostly the same large-scale bound-aries as the coarser ones.By examining the local metric, we can see evenmore: local correlations are good, at an average of81.1%.
This means that, in the three-sentence win-dow preceding each sentence, the annotators are of-837ten in agreement.
If they recognize subdivisions ofa large conversation, these subdivisions tend to becontiguous, not mingled together, which is why theyhave little impact on the local measure.We find reasons for the annotators?
disagreementabout appropriate levels of detail in the linguisticliterature.
As mentioned, new conversations of-ten break off from old ones in schisms.
Aoki etal.
(2006) discuss conversational features associatedwith schisming and the related process of affiliation,by which speakers attach themselves to a conversa-tion.
Schisms often branch off from asides or evennormal comments (toss-outs) within an existing con-versation.
This means that there is no clear begin-ning to the new conversation?
at the time when itbegins, it is not clear that there are two separatefloors, and this will not become clear until distinctsets of speakers and patterns of turn-taking are es-tablished.
Speakers, meanwhile, take time to ori-ent themselves to the new conversation.
An exampleschism is shown in Figure 2.Our annotation scheme requires annotators tomark each utterance as part of a single conversation,and distinct conversations are not related in any way.If a schism occurs, the annotator is faced with twooptions: if it seems short, they may view it as a meredigression and label it as part of the parent conver-sation.
If it seems to deserve a place of its own, theywill have to separate it from the parent, but this sev-ers the initial comment (an otherwise unremarkableaside) from its context.
One or two of the annota-tors actually remarked that this made the task con-fusing.
Our annotators seem to be either ?splitters?or ?lumpers??
in other words, each annotator seemsto aim for a consistent level of detail, but each onehas their own idea of what this level should be.As a final observation about the dataset, we testthe appropriateness of the assumption (used in pre-vious work) that each speaker takes part in only oneconversation.
In our data, the average speaker takespart in about 3.3 conversations (the actual numbervaries for each annotator).
The more talkative aspeaker is, the more conversations they participatein, as shown by a plot of conversations versus utter-ances (Figure 3).
The assumption is not very accu-rate, especially for speakers with more than 10 utter-ances.0 10 20 30 40 50 60Utterances012345678910ThreadsFigure 3: Utterances versus conversations participated inper speaker on development data.4 ModelOur model for disentanglement fits into the generalclass of graph partitioning algorithms (Roth and Yih,2004) which have been used for a variety of tasks inNLP, including the related task of meeting segmen-tation (Malioutov and Barzilay, 2006).
These algo-rithms operate in two stages: first, a binary classifiermarks each pair of items as alike or different, andsecond, a consistent partition is extracted.44.1 ClassificationWe use a maximum-entropy classifier (Daume?
III,2004) to decide whether a pair of utterances x andy are in same or different conversations.
The mostlikely class is different, which occurs 57% of thetime in development data.
We describe the classi-fier?s performance in terms of raw accuracy (cor-rect decisions / total), precision and recall of thesame class, and F-score, the harmonic mean of pre-cision and recall.
Our classifier uses several typesof features (table 2).
The chat-specific features yieldthe highest accuracy and precision.
Discourse andcontent-based features have poor accuracy on theirown (worse than the baseline), since they work beston nearby pairs of utterances, and tend to fail onmore distant pairs.
Paired with the time gap fea-ture, however, they boost accuracy somewhat andproduce substantial gains in recall, encouraging themodel to group related utterances together.The time gap, as discussed above, is the mostwidely used feature in previous work.
We exam-4Our first attempt at this task used a Bayesian generativemodel.
However, we could not define a sharp enough posteriorover new sentences, which made the model unstable and overlysensitive to its prior.838Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66)Time The time between x and y in sec-onds, bucketed logarithmically.Speaker x and y have the same speaker.Mention x mentions y (or vice versa),both mention the same name, ei-ther mentions any name.Discourse (Acc 52: Prec: 47 Rec: 77 F: 58)Cue words Either x or y uses a greeting(?hello?
&c), an answer (?yes?,?no?
&c), or thanks.Question Either asks a question (explicitlymarked with ???
).Long Either is long (> 10 words).Content (Acc 50: Prec: 45 Rec: 74 F: 56)Repeat(i) The number of words sharedbetween x and y which haveunigram probability i, bucketedlogarithmically.Tech Whether both x and y use tech-nical jargon, neither do, or onlyone does.Combined (Acc 75: Prec: 73 Rec: 68 F: 71)Table 2: Feature functions with performance on develop-ment data.ine the distribution of pauses between utterances inthe same conversation.
Our choice of a logarithmicbucketing scheme is intended to capture two char-acteristics of the distribution (figure 4).
The curvehas its maximum at 1-3 seconds, and pauses shorterthan a second are less common.
This reflects turn-taking behavior among participants; participants inthe same conversation prefer to wait for each others?responses before speaking again.
On the other hand,the curve is quite heavy-tailed to the right, leadingus to bucket long pauses fairly coarsely.Our discourse-based features model some pair-0 10 100 100002040secondsFrequencyFigure 4: Distribution of pause length (log-scaled) be-tween utterances in the same conversation.wise relationships: questions followed by answers,short comments reacting to longer ones, greetings atthe beginning and thanks at the end.Word repetition is a key feature in nearly everymodel for segmentation or coherence, so it is no sur-prise that it is useful here.
We bucket repeated wordsby their unigram probability5 (measured over the en-tire 52 hours of transcript).
The bucketing schemeallows us to deal with ?noise words?
which are re-peated coincidentally.The point of the repetition feature is of course todetect sentences with similar topics.
We also findthat sentences with technical content are more likelyto be related than non-technical sentences.
We labelan utterance as technical if it contains a web address,a long string of digits, or a term present in a guidefor novice Linux users 6 but not in a large news cor-pus (Graff, 1995)7.
This is a light-weight way tocapture one ?semantic dimension?
or cluster of re-lated words, in a corpus which is not amenable tofull LSA or similar techniques.
LSA in text corporayields a better relatedness measure than simple rep-etition (Foltz et al, 1998), but is ineffective in ourcorpus because of its wide variety of topics and lackof distinct document boundaries.Pairs of utterances which are widely separatedin the discourse are unlikely to be directly related?even if they are part of the same conversation, thelink between them is probably a long chain of in-tervening utterances.
Thus, if we run our classifieron a pair of very distant utterances, we expect it todefault to the majority class, which in this case willbe different, and this will damage our performancein case the two are really part of the same conver-sation.
To deal with this, we run our classifier onlyon utterances separated by 129 seconds or less.
Thisis the last of our logarithmic buckets in which theclassifier has a significant advantage over the major-ity baseline.
For 99.9% of utterances in an ongoingconversation, the previous utterance in that conver-sation is within this gap, and so the system has a5We discard the 50 most frequent words entirely.6?Introduction to Linux: A Hands-on Guide?.
MachteltGarrels.
Edition 1.25 from http://tldp.org/LDP/intro-linux/html/intro-linux.html .7Our data came from the LA times, 94-97?
helpfully, it pre-dates the current wide coverage of Linux in the mainstreampress.839chance of correctly linking the two.On test data, the classifier has a mean accuracy of68.2 (averaged over annotations).
The mean preci-sion of same conversation is 53.3 and the recall is71.3, with mean F-score of 60.
This error rate ishigh, but the partitioning procedure allows us to re-cover from some of the errors, since if nearby utter-ances are grouped correctly, the bad decisions willbe outvoted by good ones.4.2 PartitioningThe next step in the process is to cluster the utter-ances.
We wish to find a set of clusters for which theweighted accuracy of the classifier would be max-imal; this is an example of correlation clustering(Bansal et al, 2004), which is NP-complete8.
Find-ing an exact solution proves to be difficult; the prob-lem has a quadratic number of variables (one foreach pair of utterances) and a cubic number of tri-angle inequality constraints (three for each triplet).With 800 utterances in our test set, even solving thelinear program with CPLEX (Ilog, Inc., 2003) is tooexpensive to be practical.Although there are a variety of approximationsand local searches, we do not wish to investigatepartitioning methods in this paper, so we simplyuse a greedy search.
In this algorithm, we as-sign utterance j by examining all previous utter-ances i within the classifier?s window, and treat-ing the classifier?s judgement pi,j ?
.5 as a vote forcluster(i).
If the maximum vote is greater than 0,we set cluster(j) = argmaxc votec.
Otherwise jis put in a new cluster.
Greedy clustering makes atleast a reasonable starting point for further efforts,since it is a natural online algorithm?
it assigns eachutterance as it arrives, without reference to the fu-ture.At any rate, we should not take our objective func-tion too seriously.
Although it is roughly correlatedwith performance, the high error rate of the classifiermakes it unlikely that small changes in objective willmean much.
In fact, the objective value of our outputsolutions are generally higher than those for true so-8We set up the problem by taking the weight of edge i, j asthe classifier?s decision pi,j ?
.5.
Roth and Yih (2004) use logprobabilities as weights.
Bansal et al (2004) propose the logodds ratio log(p/(1 ?
p)).
We are unsure of the relative meritof these approaches.lutions, which implies we have already reached thelimits of what our classifier can tell us.5 ExperimentsWe annotate the 800 line test transcript using oursystem.
The annotation obtained has 63 conversa-tions, with mean length 12.70.
The average densityof conversations is 2.9, and the entropy is 3.79.
Thisplaces it within the bounds of our human annota-tions (see table 1), toward the more general end ofthe spectrum.As a standard of comparison for our system, weprovide results for several baselines?
trivial systemswhich any useful annotation should outperform.All different Each utterance is a separate conversa-tion.All same The whole transcript is a single conversa-tion.Blocks of k Each consecutive group of k utterancesis a conversation.Pause of k Each pause of k seconds or more sepa-rates two conversations.Speaker Each speaker?s utterances are treated as amonologue.For each particular metric, we calculate the bestbaseline result among all of these.
To find the bestblock size or pause length, we search over multiplesof 5 between 5 and 300.
This makes these baselinesappear better than they really are, since their perfor-mance is optimized with respect to the test data.Our results, in table 3, are encouraging.
On aver-age, annotators agree more with each other than withany artificial annotation, and more with our modelthan with the baselines.
For the 1-to-1 accuracy met-ric, we cannot claim much beyond these general re-sults.
The range of human variation is quite wide,and there are annotators who are closer to baselinesthan to any other human annotator.
As explainedearlier, this is because some human annotations aremuch more specific than others.
For very specificannotations, the best baselines are short blocks orpauses.
For the most general, marking all utterancesthe same does very well (although for all other an-notations, it is extremely poor).840Other Annotators Model Best Baseline All Diff All SameMean 1-to-1 52.98 40.62 34.73 (Blocks of 40) 10.16 20.93Max 1-to-1 63.50 51.12 56.00 (Pause of 65) 16.00 53.50Min 1-to-1 35.63 33.63 28.62 (Pause of 25) 6.25 7.13Mean loc 3 81.09 72.75 62.16 (Speaker) 52.93 47.07Max loc 3 86.53 75.16 69.05 (Speaker) 62.15 57.47Min loc 3 74.75 70.47 54.37 (Speaker) 42.53 37.85Table 3: Metric values between proposed annotations and human annotations.
Model scores typically fall betweeninter-annotator agreement and baseline performance.For the local metric, the results are much clearer.There is no overlap in the ranges; for every test an-notation, agreement is highest with other annota-tor, then our model and finally the baselines.
Themost competitive baseline is one conversation perspeaker, which makes sense, since if a speakermakes two comments in a four-utterance window,they are very likely to be related.The name mention features are critical for ourmodel?s performance.
Without this feature, the clas-sifier?s development F-score drops from 71 to 56.The disentanglement system?s test performance de-creases proportionally; mean 1-to-1 falls to 36.08,and mean loc 3 to 63.00, essentially baseline per-formance.
On the other hand, mentions are notsufficient; with only name mention and time gapfeatures, mean 1-to-1 is 38.54 and loc 3 is 67.14.For some utterances, of course, name mentions pro-vide the only reasonable clue to the correct deci-sion, which is why humans mention names in thefirst place.
But our system is probably overly depen-dent on them, since they are very reliable comparedto our other features.6 Future WorkAlthough our annotators are reasonably reliable, itseems clear that they think of conversations as a hi-erarchy, with digressions and schisms.
We are in-terested to see an annotation protocol which moreclosely follows human intuition and explicitly in-cludes these kinds of relationships.We are also interested to see how well this featureset performs on speech data, as in (Aoki et al, 2003).Spoken conversation is more natural than text chat,but when participants are not face-to-face, disentan-glement remains a problem.
On the other hand, spo-ken dialogue contains new sources of information,such as prosody.
Turn-taking behavior is also moredistinct, which makes the task easier, but accordingto (Aoki et al, 2006), it is certainly not sufficient.Improving the current model will definitely re-quire better features for the classifier.
However, wealso left the issue of partitioning nearly completelyunexplored.
If the classifier can indeed be improved,we expect the impact of search errors to increase.Another issue is that human users may prefer moreor less specific annotations than our model provides.We have observed that we can produce lower orhigher-entropy annotations by changing the classi-fier?s bias to label more edges same or different.
Butwe do not yet know whether this corresponds withhuman judgements, or merely introduces errors.7 ConclusionThis work provides a corpus of annotated data forchat disentanglement, which, along with our pro-posed metrics, should allow future researchers toevaluate and compare their results quantitatively9.Our annotations are consistent with one another, es-pecially with respect to local agreement.
We showthat features based on discourse patterns and thecontent of utterances are helpful in disentanglement.The model we present can outperform a variety ofbaselines.AcknowledgementsOur thanks to Suman Karumuri, Steve Sloman, MattLease, David McClosky, 7 test annotators, 3 pilotannotators, 3 anonymous reviewers and the NSFPIRE grant.9Code and data for this project will be available athttp://cs.brown.edu/people/melsner.841ReferencesEvrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-ishnamoorthy, and Blent Yener.
2005.
Model-ing and multiway analysis of chatroom tensors.
InPaul B. Kantor, Gheorghe Muresan, Fred Roberts,Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,and Ralph C. Merkle, editors, ISI, volume 3495 ofLecture Notes in Computer Science, pages 256?268.Springer.Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-ski, James D. Thornton, Daniel Wilson, and AllisonWoodruff.
2003.
The mad hatter?s cocktail party: asocial mobile audio space supporting multiple simul-taneous conversations.
In CHI ?03: Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 425?432, New York, NY, USA.
ACMPress.Paul M. Aoki, Margaret H. Szymanski, Luke D.Plurkowski, James D. Thornton, Allison Woodruff,and Weilie Yi.
2006.
Where?s the ?party?
in ?multi-party??
: analyzing the structure of small-group socia-ble talk.
In CSCW ?06: Proceedings of the 2006 20thanniversary conference on Computer supported coop-erative work, pages 393?402, New York, NY, USA.ACM Press.Nikhil Bansal, Avrim Blum, and Shuchi Chawla.
2004.Correlation clustering.
Machine Learning, 56(1-3):89?113.Seyit Ahmet Camtepe, Mark K. Goldberg, MalikMagdon-Ismail, and Mukkai Krishnamoorty.
2005.Detecting conversing groups of chatters: a model, al-gorithms, and tests.
In IADIS AC, pages 89?96.Hal Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper availableat http://pub.hal3.name#daume04cg-bfgs, implemen-tation available at http://hal3.name/megam/, August.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence withlatent semantic analysis.
Discourse Processes,25(2&3):285?307.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
In ACL ?03: Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics, pages 562?569, Morristown, NJ,USA.
Association for Computational Linguistics.David Graff.
1995.
North American News Text Corpus.Linguistic Data Consortium.
LDC95T21.Ilog, Inc. 2003.
Cplex solver.Natasa Jovanovic, Rieks op den Akker, and Anton Ni-jholt.
2006.
Addressee identification in face-to-facemeetings.
In EACL.
The Association for ComputerLinguistics.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In ACL.The Association for Computer Linguistics.Jacki O?Neill and David Martin.
2003.
Text chat in ac-tion.
In GROUP ?03: Proceedings of the 2003 inter-national ACM SIGGROUP conference on Supportinggroup work, pages 40?49, New York, NY, USA.
ACMPress.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of CoNLL-2004, pages1?8.
Boston, MA, USA.Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.1974.
A simplest systematics for the organization ofturn-taking for conversation.
Language, 50(4):696?735.842
