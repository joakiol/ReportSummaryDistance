Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1505?1515,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLearning to Interpret and Describe Abstract ScenesLuis Gilberto Mateos Ortiz, Clemens Wolff and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB{clemens.wolff,luismattor}@gmail.com, mlap@inf.ed.ac.ukAbstractGiven a (static) scene, a human can effort-lessly describe what is going on (who is do-ing what to whom, how, and why).
The pro-cess requires knowledge about the world, howit is perceived, and described.
In this paper westudy the problem of interpreting and verbal-izing visual information using abstract scenescreated from collections of clip art images.
Wepropose a model inspired by machine trans-lation operating over a large parallel corpusof visual relations and linguistic descriptions.We demonstrate that this approach produceshuman-like scene descriptions which are bothfluent and relevant, outperforming a num-ber of competitive alternatives based on tem-plates, sentence-based retrieval, and a multi-modal neural language model.1 IntroductionWhat is going on in the scene in Figure 1?
Is theboy trying to feed the dog or play with it?
Why isthe girl upset?
Is it because the dog is wearing herglasses?
Or perhaps she is just scared of the dog?Scene interpretation is effortless for humans, almosteveryone can summarize Figure 1 in a few words,without probably paying too much attention to thefact the girl is wearing a pink dress, the sun is yellowor that there is a plane in the sky.Discovering what an image means and relaying itin words is of theoretical importance raising ques-tions about language and its grounding in the per-ceptual world but also has practical applications.Examples include sentence-based image search andtools that enhance the accessibility of the web forvisually impaired (blind and partially sighted) in-dividuals.
Indeed, there has been a recent surgeof interest in the development of models that au-tomatically describe image content in natural lan-Figure 1: Given an image, humans do not simply see anarrangement of objects, they understand how they relateto each other as well as their attributes and the activitiesthey are involved in.guage (see references in Section 2).
Due to the com-plex nature of the problem, existing approaches re-sort to modeling simplifications, on the generationside (e.g., through the use of templates and sentence-based retrieval methods), or the image processingside (e.g., by avoiding object-detection), or both.In this paper we study the problem of interpretingvisual scenes and rendering their content using nat-ural language.
We approach this problem within themethodology of Zitnick and Parikh (2013), who pro-posed the use of abstract scenes generated from clipart to model scene understanding (see Figure 1).
Theuse of abstract scenes offers several advantages overreal images.
Firstly, it allows us to study the scenedescription problem in isolation, without the noiseintroduced by automatic object and attribute detec-tors in real images.
Secondly, it is relatively easy togather large amounts of data, allowing us to comparemultiple models on an equal footing, study in moredetail the problem of language grounding, and howto identify what is important in an image.
Thirdly,information learned from abstract scenes will leadto better understanding of the challenges and datarequirements arising when using real images.We propose a model inspired by machine trans-1505lation, where the task is to transform a source sen-tence E into its target translation F .
We argue thatgenerating descriptions for scenes is quite similar,but with a twist: the translation process is very looseand selective; there will always be objects in a scenenot worth mentioning, and words in a descriptionthat will have no visual counterpart.
Our key in-sight is to represent scenes via visual dependencyrelations (Elliott and Keller, 2013) corresponding tosentential descriptions.
This allows us to create alarge parallel corpus for training a statistical ma-chine translation system, which we interface witha content selection component guiding the transla-tion toward interesting or important scene content.Advantageously, our model can be used in the re-verse direction, i.e., to generate scenes, without ad-ditional engineering effort.
Our approach outper-forms a number of competitive alternatives, whenevaluated both automatically and by humans.2 Related WorkThe task of image description generation has re-cently gained popularity in the natural language pro-cessing and computer vision communities.
Severalmethods leverage recent advances in computer vi-sion and generate novel sentences relying on ob-ject detectors, attribute predictors, action detectors,and pose estimators.
Generation is performed usingtemplates or syntactic rules which piece the descrip-tion together while leveraging word-co-occurrencestatistics (Kulkarni et al, 2011; Yang et al, 2011;Elliott and Keller, 2013; Mitchell et al, 2012).
Re-cent advances in neural language models have led toapproaches which generate captions by conditioningon feature vectors from the output of a deep convo-lutional neural network without the use of templatesor syntactic trees (Kiros et al, 2014; Vinyals et al,2014).
Most methods assume no structural infor-mation on the image side either (images are repre-sented as unstructured bags of regions or as featurevectors).
A notable exception are Elliott and Keller(2013) who introduce visual dependency relationsbetween objects and argue that such structured rep-resentations are beneficial for image description.A large body of work has focused on the comple-mentary problem of matching sentences (Ordonez etal., 2011; Farhadi et al, 2010; Hodosh et al, 2013;Feng and Lapata, 2013; Mason and Charniak, 2014)or phrases (Kuznetsova et al, 2012; Kuznetsovaet al, 2014) to an image from existing human au-thored descriptions.
Sentence-based approaches em-bed images and descriptions into the same multi-dimensional space, and retrieve descriptions fromimages most similar to a query image.
Phrase-basedapproaches are more involved in that phrases needto be composed into a description and extraneousinformation optionally removed.
A common model-ing choice is the use of Integer Linear Programming(ILP) which naturally allows to encode various well-formedness constraints (e.g., grammaticality).We are not aware of any previous work generatingdescriptions for abstract scenes, although the samedataset has been used to model sentence-to-scenegeneration (Zitnick et al, 2013) and predict objectdynamics in scenes (Fouhey and Zitnick, 2014).
Us-ing the visual relations put forward in Elliott andKeller (2013), we convert the abstract scenes datasetinto a parallel corpus of visual and linguistic de-scriptions, which allows us to train a statistical ma-chine translation (SMT) model.
In contrast to ear-lier work (Kuznetsova et al, 2014; Kuznetsova etal., 2012), which models the task as an optimiza-tion problem end-to-end, we employ ILP for contentselection only, deferring the surface realization pro-cess entirely to an SMT engine.3 The Abstract Scenes DatasetThe abstract scenes dataset1was created with the in-tent to represent real-world scenes that depict a di-verse set of subtle relations.
It contains 10,020 im-ages of children playing outside and 60,396 descrip-tions (on average six per image).
The data was col-lected in three stages.
First, Amazon MechanicalTurk (AMT) workers were asked to created scenesfor a collection of 80 pieces of clip art depicting aboy and a girl (in different poses and with differ-ent facial expressions), and several objects includingtrees, toys, hats, animals, and so on.
Next, a new setof subjects were asked to describe the scenes usinga one or two sentence description, finally, semanti-cally similar scenes were generated by asking multi-ple subjects to create scenes depicting the same writ-1http://research.microsoft.com/en-us/um/people/larryz/clipart/abstract_scenes.html15060 6s 3s.png 0 3 467 24 2 1hb0 10s.png 2 10 145 182 0 1hb1 19s png 3 19 323 188 0 1c 9s.png 5 9 161 116 0 1c 5s.png 7 4 43 172 0 0t 4s.png 7 4 43 173 0 0?Jenny is upset because Mike isn?t sharing the soccer ball.
?, ?Mike iswearing sunglasses.
?, ?Jenny is wearing a silly hat.
?, ?Mike is kickingthe soccer ball away from Jenny.
?, ?Jenny is chasing Mike to get theball.
?, ?Jenny is wearing a silly hat.
?Figure 2: Example of a scene, its rendering information(right), and human-written descriptions (bottom).ten description.
By construction, the dataset encodesthe objects in each scene, and their position.An example is shown in Figure 2.
The tableon the right-hand side specifies how the image wasrendered.
The top row contains the scene identi-fier (i.e., 0) and the number of pieces of clip artin the image (i.e., 6).
The remaining rows encoderendering information for each individual piece ofclipart, i.e., its name (column 1), type (column 2),attribute (column 3), position (columns 4?6), andwhether or not it is horizontally flipped (column 7).Six human authored descriptions are shown the bot-tom.
AMT participants were instructed to write sim-ple descriptions using basic words that would ap-pear in a book for young children ages 4?6.
Par-ticipants who wished to use proper names in theirdescriptions were provided with names ?Mike?
and?Jenny?
for the boy and girl.
The vocabulary con-sists of 2,705 words, and the average sentence lengthis 6.3 words.
As can be seen in Figure 2, subjectschoose to focus on different aspects of the image(e.g., Mike and his sunglasses, the fact that Jenny ischasing Mike).
Also notice that even though by de-sign we know which visual objects are present in theimage and their spatial relationships (see the righthand-side in Figure 2), the alignment between piecesof clipart and linguistic expressions is hidden.
Inother words, we do not know which actions the ob-jects depict (e.g., playing, holding) and which wordscan be used to describe them (e.g., that t 4s.png iscalled a ball).4 Problem FormulationWe formulate scene description generation as atranslation problem from the visual to the linguis-tic modality.
Our approach follows the generalparadigm of SMT with two important differences.Firstly, the source side (i.e., scene) is fundamentallydifferent from the target (i.e., linguistic descriptions)both in terms of representation and structure.
Sec-ondly, the scene and its corresponding descriptionsconstitute a very loose parallel corpus: not all visualobjects are verbalized (note that no participant choseto mention the sun in Figure 2) and there are multi-ple valid descriptions for a single scene focusing ondifferent objects and their relations.
In the follow-ing we first describe how we create a parallel corpusrepresenting the arrangement of objects in a sceneand their linguistic realization and then we move onto present our generation model.4.1 Parallel Corpus CreationAs mentioned earlier, each scene in the dataset hassix descriptions (on average).
For each linguisticdescription we create its corresponding visual en-coding.
We initially ground words and phrases byaligning them to pieces of clipart.
We parse the de-scriptions using a dependency parser, and identifyexpressions that function as arguments (e.g., subject,object).
In our experiments we used the Stanfordparser (Klein and Manning, 2003) but any parserwith similar output could have been used instead.Next, we compute the mutual information (MI) be-tween arguments and clip-art objects defined as:MI(X ,Y ) =?x?X?y?Yp(x,y) logp(x,y)p(x)p(y)(1)where X is the set of clip-art objects and Y the set ofarguments found in the dataset.
We assume that thevisual rendering of an argument is the clip-art objectwith which its MI value is highest.
Figure 3 showsargument-clipart pairs with high MI values.Having identified which objects in the scene aretalked about, we move on to encode their spatial re-lations.
We adopt the relations outlined in VisualDependency Grammar (VDG; Elliott and Keller(2013)).
The latter are defined for pairs of imageregions but can also directly apply to clip-art ob-jects.
VDR Relations are specified according to1507X on Y More than 50% of X overlapswith YX surrounds Y X overlaps entirely with YX above Y The angle between X and Y isbetween 225?and 315?X below Y The angle between X and Y isbetween 45?and 135?.X opposite Y The angle between X and Y isbetween 315?and 45?or 135?and225?.
The Euclidean distancebetween X and Y is greater thanw ?0.72.X near Y Similar to opposite but theEuclidean distance betweenX and Y is greater than w ?0.36.X close Y Similar to opposite but theEuclidean distance betweenX and Y is less or equalto w ?0.36.X infront Y X is in front Y in the Z-planeX behind Y X is behind Y in the Z-planeX same Y X and Y are at the same depthTable 1: VDG relations between pairs of clip art objects.All relations are considered with respect to the centroid ofan object and the angle between those centroids.
We fol-low the definition of the unit circle, in which 0?lies to theright and a turn around the circle is counter-clockwise.All regions are mutually exclusive.
Parameter w refers tothe width of the scene.three geometric properties: pixel overlap, the anglebetween regions, and the distance between regions.Table 1 summarizes the relations used in our experi-ments most of which encode spatial object relationsin the x-y space; the last three encode relative objectposition along the z axis.
Our relations are broadlysimilar to those proposed in Elliott and Keller (2013)with the exception of beside which we break downinto more fine-grained relations (namely near andclose).
We also add the same relation in the z axis.
Incases where object X is facing object Y we subscriptrelations opposite, near, and close with the letter F .The procedure described above will generate a vi-sual description for each linguistic description.
Italso assumes that visual relations hold between pairsof objects.
The assumption is not unwarranted,73.87% of the descriptions in the dataset involveherjennyno onepink dressshebaseball batbatbat and ballhomerunone baseball batapple piebaked pieberry piedelicious piepieblue collarbrown doghappy dogsmall dogsmiling dogappleapple treebig apple treecherriesfruitbaseball capbaseball hatblue capblue hatstar hatFigure 3: Examples of argument-clipart object pairs withhigh MI values (shown in descending order).only two arguments.
The parallel sentences cor-responding to Figure 2 are illustrated in Table 2.In cases where the the original description involvesthree objects, ternary relations are decomposed intobinary ones.
We create as many visual represen-tations as there are linguistic descriptions.
If twohumans generate identical descriptions, we produceidentical visual encodings.
In total, we were able tocreate 46,053 parallel descriptions2accounting for79.5% of the sentences in the dataset.4.2 Generation ModelIt is straightforward to train a phrase-based SMTmodel on the parallel corpus outlined above.
Themodel would learn to translate a visual description(see the source side in Table 2) into natural lan-guage.
However, when generating linguistic de-scriptions for a scene at test time, we must firstdecide ?what to say?
(content selection) and then?how to say?
it (surface realization).
What is themost important content in the scene worth describ-ing?
Given that visual relations between objects areassumed to be binary (see the VDG grammar in Ta-ble 1), there are n(n?
1) combinations of pairs ofobjects in a scene (where n is the number of clipartpieces available) and as many corresponding visualexpressions.
However, many of these visual expres-sions will capture unimportant aspects of the scene,or even express atypical relations unattested in thetraining data.
We develop below a content selectioncomponent based on the intuition that frequently2Our parallel corpus can be downloaded fromhttp://homepages.inf.ed.ac.uk/mlap/index.php?page=resources.1508Image description1.
hb0.10s.png closeFsame t.4s.png Mike isn?t sharing the soccer ball2.
hb0.10s.png surrounds same c.9s.png Mike is wearing sunglasses3.
hb1.19s.png below same c.5s.png Jenny is wearing a silly hat4.
hb0.10s.png closeFsame t.4s.png Mike is kicking the soccer ball5.
hb1.19s.png closeFsame hb0.10s.png Jenny is chasing Mike6.
hb1.19s.png below same c.5s.png Jenny is wearing a silly hatTable 2: Parallel corpus of visual expressions and linguistic descriptions corresponding to Figure 2.mentioned object pairs probably express importantscene content.
In addition, it is reasonable to assumethat the selected objects will be in close proximity,especially when actions are involved.
One wouldexpect the agent of the action to be near the objector person receiving it (e.g., Mike is kicking the ball,Jenny is holding Mike?s hand ).
The same is true forinstruments, which are typically held by the personsusing them (e.g., Jenny is digging with a shovel ).Content Selection We cast the problem of findingsuitable objects to talk about as an integer linear pro-gram (ILP).
Our model selects clip art object pairsthat best describe the content of a scene and ranksthem based on their relevance.
Indicator variable ystkdenotes whether two objects are being selected andhow they are ranked (e.g, whether they should bementioned first or last):ystk=??
?1 if objects s and t are selected for rank kand s is before t0 otherwise(2)where s and t index two clip art objects andk = 1, ..,S encodes their rank (based on relevance).Our objective function is given below:Z =?s?S,t?SFst?Dst?
?k?S((card(S)+1)?k) ?ystk(3)where Fstquantifies the normalized co-occurrencefrequency of objects s and t (in the training set)and Dstspecifies their relative distance.
The term((card(S)+1)?k) accounts for the ranking of pairsso that most relevant ones are ranked first.
Here,card(S) represents the cardinality of the set S de-noting the number of clip art objects in the scene;k ranges over all available ranks (which is limited bythe number of clip art objects available).
The termFigure 4: Example of three clip art objects and the mostfrequent objects they co-occur with.
((card(S)+1)?k) is inversely proportional to k, soits highest value is when k is 1.
In other words, thevalue of the term is maximum when the selected ob-jects are ranked first.
This way, we ensure that mostrelevant object pairs are given high ranks.We compute Fstfrom our parallel corpus (see left-hand side in Table 2), simply by counting the num-ber of times objects s and t co-occur.
Figure 4 showsthree clip art objects (Mike, a snake, and a bear)and their most frequent co-occurrences.
We estimateterm Dst, the distance between objects s and t, usingfunction??x2+?y2+?z2.
Coordinate z has onlythree possible values (see Table 1).
To increase theeffect of ?z, we use a scaling factor.
We normalizeand invert Dstso that it ranges from 0 to 1.
In ad-dition, we transform it with a sigmoid function soas to maximize the effect of near and distant objects(distances of relatively close objects are set to 1 anddistances of distant objects are set to 0).The objective function in Equation (3) is too per-missive, allowing repetitions of the same objectwithin a pair and of the object pairs themselves.Constraints (4)?
(10) avoid repetitions and ensure1509that the selected objects are varied with the aim ofgenerating logically consistent descriptions.
Con-straint (4) avoids empty descriptions, by enforcingthat at least one clip art object pair is selected.
Con-straint (5) ensures that an object cannot appear inthe same pair twice, whereas constraint (6) requiresthat at most one pair can be selected for a givenrank k. We also enforce the selection of differentpairs of objects (constraint (7)) at contiguous ranks(constraint (8)).
?s?S,t?Syst1= 1 (4)?stk,s==t, ystk= 0 (5)?k,?s?S,t?Systk?
1; (6)?st,?k?S(ystk+ ytsk)?
1 (7)?k=1,..,S?1,?s?S,t?Systk+1?
?s?S,t?Systk(8)Finally, to instill some coherence in the descrip-tions, while avoiding overly repetitive discourse, wedisallow objects to be selected more than four times:?s, sums=?t?S,k?Systk(9)?t, sumt=?s?S,k?Systk(10)?st,s==t, sums+ sumt?
4 (11)Auxiliary variables sumsand sumtrepresent thenumber of times objects s and t are selected to bethe first and second object of a pair.Given a new unseen scene, we obtain the Fstval-ues for all pair-wise combinations of the objects in itand compute their distance Dst.
We solve the ILPproblem defined above and read the value of thevariable ystkwhich contains the selected clip art ob-ject pairs ranked by relevance.
So, our model can inprinciple produce multiple descriptions for a givenscene, highlighting potentially different aspects ofthe visually encoded information.
We used GLPK3to maximize the objective function subject to theconstraints introduced above.3https://www.gnu.org/software/glpk/Surface realization The ILP selects alldescription-worthy pairs of clip art objects fora scene.
Using the rules presented in Table 1 wecreate visual encodings for them (see Table 2,source side), and finally translate them into naturallanguage using a Phrase-based SMT engine (Koehnet al, 2003).
Specifically, given a source visualexpression f, our aim is to find an equivalent targetnatural language description?e that maximizes theposterior probability:?e= argmaxeP(e|f) (12)Most recent SMT work models the posterior P(e|f)directly (Och and Ney, 2002) using a log-linear com-bination of several models where:P(e|f) =exp?Kk=1?khk(f,e)?e?
exp?Kk=1?khk(f,e?
)(13)and the decision decision rule is given by:?e= argmaxeK?k=1?khk(f,e) (14)where hk(f,e) is a scoring function representing im-portant features for the translation of f into e. Exam-ples include the language model of the target lan-guage, a reordering model, or several translationmodels.
K is the number of models (or features)and ?kare the weights of the log-linear combina-tion.
Typically, the weights ?
= [?1, .
.
.
,?K]Tareoptimized on a development set, by means of Mini-mum Error Rate Training (MERT; Och (2003)).One of the most popular instantiations of loglinearmodels in SMT are phrase-based (PB) models (Zenset al, 2002; Koehn et al, 2003).
PB models allow tolearn translations for entire phrases instead of indi-vidual words.
The basic idea behind PB translationis to segment the source sentence into phrases, thento translate each source phrase into a target phrase,and finally reorder the translated target phrases in or-der to compose the target sentence.
For this purpose,phrase-tables are produced, in which a source phraseis listed together with several target phrases and theprobability of translating the former into the latter.Throughout our experiments, we obtained transla-tion models using the PB SMT framework imple-mented in MOSES (Koehn et al, 2007).1510Mike is kicking the ball The plane is flying in the skynsubj,aux,verb,det,dobj det,nsubj,aux,verb,prep,det,pobjTable 3: Sample scenes with human-written descriptionsand corresponding templates.5 Model ComparisonWe evaluated the model described above throughcomparison to four alternatives, representing differ-ent modeling paradigms in the literature.
Our firstcomparison model is based on templates, which arecommonly used to produce descriptions for images(Elliott and Keller, 2013; Kulkarni et al, 2011).Rather than manually creating template rules we in-duce them from dependency-parsed scene descrip-tions.
We represent each description in the data asa sequence of typed dependencies.
The scene de-scriptions are relatively simple, and many sentenceshave similar structure.
Overall, 20 templates repre-sent the syntactic structure of more than 44% of allscene descriptions.
Examples of scenes, their de-scriptions, and corresponding templates are shownin Table 3 (template nsubj,aux,verb,det,dobj isthe most frequent in the data).We train a logistic regression classifier (Yu et al,2011) on scene-template pairs, and learn to assign atemplate for a new unseen scene.
The ?template-predictor?
uses variety of features based on thealignment between clip-art objects and POS-tags aswell as objects and dependency roles.
The align-ments were computed using MI as described in Sec-tion 4.1.
We also used visual features based on theabsolute and relative distance between objects, theirco-occurrence, spatial location, depth ordering, fa-cial expression and poses (see Zitnick et al (2013)for details).
In order to transform the templatesinto natural language sentences we train a ?word-predictor?
which fills the most likely word for everygrammatical function slot in a given template (againusing logistic regression and the same feature spaceas for the template predictor).
The word predictoruses a vocabulary of 70 frequently occurring words(attested no less than 150 times in the corpus).
Fora new scene, candidate templates are predicted andsubsequently expanded to descriptions by predictingwords for every function slot in the templates.
Can-didate descriptions are ranked using a trigram lan-guage model to ensure grammatical coherence.We also implemented two sentence-based re-trieval approaches.
Our first system is conceptu-ally similar to the model proposed by Farhadi et al(2010).
In their work, images and descriptions seenat training time are mapped into a shared meaningspace M using a function f .
Given an unseen im-age ?, the description closest to f (?)
in M is re-trieved and returned by the model.
We used theword-predictor described above as a simple way ofannotating an unseen scene ?
with the words thatmost saliently describe it.
These keywords then usedas a TFIDF search query against the set H of humanscene descriptions seen during training:TFIDF(q,d) =?w?qTF(w,d)IDF(w),TF(w,q) =??w?
?q1w=w?,IDF(w) = 1+ log?H?1+?d?H?w??d1w=w?.
(15)where H is the set of all human descriptions seenat training time, ???
is the set-norm, q is a searchquery, d is any description in H and 1w=w?an indica-tor variable set to 1 if w and w?are the same word and0 otherwise.
The human description maximizing theTFIDF similarity with the predicted keywords is re-turned as the description for the new scene.Our third baseline exploits image similarity (Or-donez et al, 2011).
Given an unseen scene ?, we re-trieve from the training set ?
?, the scene most similarto it, and return one of ??
?s human descriptions se-lected at random.
We used locality sensitivity hash-ing to find the subset of candidate scenes similarto ?.
Scenes were represented with the same visualfeatures used for the word and template predictorsand their similarity was computed with the cosinemetric.Finally, we also trained a multimodal log-bilinearmodel (Kiros et al, 2014) on the abstract scenes1511System BLEU METEORLBL 7.33 17.76MLBL 12.30 20.40Image 12.80 21.77Keyword 14.70 26.60Template 40.30 30.40SMT 43.70 35.60Table 4: Model comparison on scene description task us-ing automatic measures.System 1st2nd3rd4thAvgRankKeyword 0.24 0.13 0.22 0.41 2.22Template 0.25 0.16 0.13 0.46 2.21SMT 0.53 0.24 0.12 0.11 3.19Human 0.57 0.27 0.12 0.04 3.36Table 5: Rankings (shown as proportions) and mean rat-ings given to systems by human participants.dataset.
The model essentially implements a feed-forward neural network to predict the next wordgiven the image and previous words.4Imageswere associated with feature representations ob-tained from the output of a convolutional network,following the feature learning procedure outlined inKiros et al (2014).6 ResultsWe evaluated system output automatically using(smoothed) BLUE and METEOR as calculated byNIST?s MultEval software5using the human-writtendescriptions as reference.
Elliott and Keller (2014)find that both metrics correlate well with humanjudgments.
For a fair comparison, we force ourmodel to output one description, i.e., the most rel-evant one.Our results are summarized in Table 4.
Ascan be seen, our model (SMT) performs best bothin terms of BLEU and METEOR.
The template-based generator (Template) obtains competitive per-formance which is not surprising, it incorporatessome of the ingredients of the SMT system such as4We used the implementation at http://www.cs.toronto.edu/?rkiros/multimodal.html.5ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gzResp 1st2nd3rd4th5th6thYES 75.5 65.8 53.0 44.7 44.0 37.5NO 18.0 24.8 31.2 31.3 37.5 58.0MAYBE 6.5 9.50 15.8 15.8 18.5 4.50Table 6: Proportion of SMT descriptions deemed accu-rate and relevant.
System output evaluated for rank place-ments 1. .
.6.word-to-clipart alignments, a language model, andis guaranteed to produce grammatical output.
Theperformance of the multimodal log-bilinear model(MLBL) keyword- and image-based retrieval sys-tems is inferior.
We conjecture that the image fea-tures, and similarity functions used in these mod-els are not fine-grained enough to capture the subtledifferences in scenes which humans detect and ex-press in the descriptions.
Finally, notice that visualinformation is critical in doing well on the descrip-tion generation task.
A log-bilinear language model(LBL) trained solely on the descriptions performspoorly (see the top row in Table 4).We further evaluated system output eliciting hu-man judgments for 100 randomly selected testscenes.
Participants were presented with a scene anddescriptions generated by our system, the template-based model, the best-performing sentence retrievalmodel, and a randomly selected human description.Subjects were asked to rank the four descriptionsfrom best to worst (ties are allowed) in order of in-formativeness (does the description capture what isshown in the scene?)
and fluency (is the descrip-tion written in well-formed English?).
We elicitedrankings using Amazon?s Mechanical Turk crowd-sourcing platform.
Participants (self-reported nativeEnglish speakers) saw 10 scenes per session.
Wecollected 5 responses per item.The results of our human evaluation study areshown in Table 5.
Specifically, we show, propor-tionally, how often our participants ranked each sys-tem 1st, 2nd and so on.
Perhaps unsurprisingly,the human-written descriptions were considered best(and ranked 1st 57% of the time).
Our model isranked best 0.53% of the time, followed by the tem-plate and keyword-based retrieval systems which areonly ranked first 25% of the time.6We further6Percentages do not sum to 100% because ties are allowed.1512Jenny is holding a hot dog.
Jenny is sitting in the sandbox.Jenny is wearing a witch hat.
Jenny is wearing purple sunglasses.Jenny is scared of the snake.
The cat is sitting in the sandbox.The snake is under the pine tree.
The cat is watching Jenny.Figure 5: Examples of descriptions generated by theSMT model for two scenes.converted the ranks to ratings on a scale of 1 to 4(assigning ratings 4. .
.1 to rank placements 1. .
.4).This allowed us to perform Analysis of Variance(ANOVA) which revealed a reliable effect of systemtype.
Specifically, post-hoc Tukey tests showed thatour SMT model is significantly (p < 0.01) betterthan the other two comparison systems but does notdiffer significantly from the human goldstandard.We also evaluated more thoroughly our contentselection mechanism.
Since our system can in prin-ciple generate multiple descriptions for a scene, wewere interested to see how many of these are indeedrelevant.
We let the system generate the six bestdescriptions per scene and asked AMT participantsto assess whether they were accurate (are the peo-ple, objects and actions mentioned in the descrip-tion shown in the scene?)
and appropriate (is thedescription relevant for the scene?).
Participants an-swered with ?yes?, ?no?, or ?maybe?.
Again weused 100 items from the test set, and elicited 5 re-sponses per item.
Table 6 shows the outcome ofthis study.
The majority of first-best descriptions(75.5%) returned by our system are perceived as rel-evant and scene appropriate.
The same is true for2nd and 3rd best descriptions, whereas the qualityof descriptions deteriorates with lower ranks.
Thissuggests that we could generate short discourses de-scribing different viewpoints in a scene.Figure 5 illustrates the descriptions produced byour model for two scenes, whereas Figure 6 showsexample output when the system is run in reverse,i.e., it takes descriptions as input and generates ascene.
This can be done straightforwardly, withoutany additional effort, however note that the model is?Mike and Jenny decide to make hot dogs on the grill.
?, ?It?s arainstorm and Jenny runs away to stay dry.
?, ?Mike stays besidethe fire.
?, ?Jenny is standing next to the tree.
?, ?Mike is sitting nextto the fire.
?, ?The hot dog is on the pit.
?Figure 6: Right scene is generated by SMT model (leftscene is the original) given descriptions (bottom) as input.unaware of the absolute position of objects, it placesthe cloud next to Jenny.7 ConclusionsIn this paper we presented proof of concept thatan SMT-based approach is successful at generat-ing human-like scene descriptions provided that(a) there is a large enough parallel corpus to learnfrom and (b) a content selection component iden-tifies important scene content.
Our results furtherindicate that instilling some degree of structural in-formation in visual scenes (via the VDG) is benefi-cial.
It allows to describe visual content more ac-curately and facilitates its rendering in natural lan-guage (since the two modalities are structurally sim-ilar).
The template-based, retrieval, and languagemodeling systems do not use this structural informa-tion, and even though their descriptions are largelygrammatical, they are not as felicitous.
Our resultsalso point to difficulty of the task.
Even when com-puter vision is taken out of the equation, and thedescription language is simple, human-written textis still preferable (see Table 5).
In the future, wewould like to develop better content selection mod-els (e.g., identify surprising aspects in a scene) andmore accurate grounding strategies (e.g., via dis-criminative alignment).Acknowledgments We are grateful to LukasDirzys for his help with the LBL and MLBL mod-els.
Special thanks to Frank Keller for his commentson an earlier version of this paper and Larry Zitnickwhose talk at the UW MSR Summer Institute 2013insipred this work.1513ReferencesDesmond Elliott and Frank Keller.
2013.
Image descrip-tion using visual dependency representations.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1292?1302, Seattle, Washington.Desmond Elliott and Frank Keller.
2014.
Comparing au-tomatic evaluation measures for image description.
InProceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Volume 2: ShortPapers), pages 452?457, Baltimore, Maryland.Ali Farhadi, Mohsen Hejrati, Amin Sadeghi, Peter Yong,Cyrus Rashtchian, Julia Hockenmaier, and DavidForsyth.
2010.
Every picture tells a story: Generat-ing sentences from images.
In Proceedings of the 11thEuropean Conference on Computer Vision, pages 25?29, Heraklion, Greece.Yansong Feng and Mirella Lapata.
2013.
Automaticcaption generation for news images.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,35(4):797?812.David F. Fouhey and C. Lawrence Zitnick.
2014.
Pre-dicting object dynamics in scenes.
In Proceedingsof the 2014 IEEE Conference on Computer Visionand Pattern Reconition, pages 2027?2034, Columbus,Ohio.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
Journal of Arti-ficial Intelligence Research, 47:853?899.Ryan Kiros, Ruslan Slakhutdinov, and Richard Zemel.2014.
Multimodal neural language models.
In Pro-ceedings of the 31st International Conference on Ma-chine Learning, Beijing, China.
volume 32.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In AnnualMeeting of the North American Chapter of the Associ-ation for Computational Linguistics, pages 48?54, Ed-monton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and generatingimage descriptions.
In Proceedings of the 2011 IEEEConference on Computer Vision and Pattern Recogni-tion, pages 1601?1608, Colorado Springs, Colorado.Polina Kuznetsova, Vicente Ordonez, Alexander Berg,Tamara Berg, and Yejin Choi.
2012.
Collective gener-ation of natural image descriptions.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages359?368, Jeju Island, Korea.Polina Kuznetsova, Vicente Ordonez, Tamara L. Berg,and Yejin Choi.
2014.
Treetalk: Composition andcompression of trees for image descriptions.
Trans-actions of the Association for Computational Luin-gusitics, 2:351?362.Rebecca Mason and Eugene Charniak.
2014.
Nonpara-metric method for data-driven image captioning.
InProceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Volume 2: ShortPapers), pages 592?598, Baltimore, Maryland.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,Alex Berg, Tamara Berg, and Hal Daume III.
2012.Midge: Generating image descriptions from computervision detections.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 747?756, Avignon,France.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, Pennsylva-nia.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 160?167, Sapporo, Japan.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2text: Describing images using 1 millioncaptioned photographs.
In J. Shawe-Taylor, R.S.Zemel, P. Bartlett, F.C.N.
Pereira, and K.Q.
Wein-berger, editors, Advances in Neural Information Pro-cessing Systems 24, pages 1143?1151.
Curran Asso-ciates, Inc.Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-mitru Erhan.
2014.
Show and tell: A neural imagecaption generator.
arXiv:1411.4555.Yezhou Yang, Ching Teo, Hal Daume III, and YiannisAloimonos.
2011.
Corpus-guided sentence genera-tion of natural images.
In Proceedings of the 20111514Conference on Empirical Methods in Natural Lan-guage Processing, pages 444?454, Edinburgh, Scot-land.Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.2011.
Dual coordinate descent methods for logisticregression and maximum entropy models.
MachineLearning, 85(1-2):41?75.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Springer Verlag, editor,German Conference on Artificial Intelligence, pages18?32.C.
Lawrence Zitnick and Devi Parikh.
2013.
Bring-ing semantics into focus using visual abstraction.
InProceedings of the 2013 IEEE Conference on Com-puter Vision and Pattern Recognition, pages 3009?3016, Portland, Oregon.C.
Lawrewnce Zitnick, Devi Parikh, and Lucy Vander-wende.
2013.
Learning the visual interpretation ofsentences.
In Proceedings of the 2013 IEEE Interna-tional Conference on Computer Vision, pages 1681?1688, Sydney, Australia.1515
