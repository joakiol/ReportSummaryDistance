An Empirical Study of Smoothing Techniques for LanguageModelingStanley F .
ChenHarvard  Un ivers i tyA iken  Computat ion  Laboratory33 Oxford  St.Cambr idge ,  MA 02138sfc?eecs, harvard, eduJoshua GoodmanHarvard  Un ivers i tyA iken Computat ion  Laboratory33 Oxford  St.Cambr idge ,  MA 02138goodma.n~eecs, harvard, eduAbst ractWe present an extensive empirical com-parison of several smoothing techniques inthe domain of language modeling, includ-ing those described by Jelinek and Mer-cer (1980), Katz (1987), and Church andGale (1991).
We investigate for the firsttime how factors such as training datasize, corpus (e.g., Brown versus Wall StreetJournal), and n-gram order (bigram versustrigram) affect the relative performance ofthese methods, which we measure throughthe cross-entropy of test data.
In addition,we introduce two novel smoothing tech-niques, one a variation of Jelinek-Mercersmoothing and one a very simple linear in-terpolation technique, both of which out-perform existing methods.1 In t roduct ionSmoothing is a technique ssential in the construc-tion of n-gram language models, a staple in speechrecognition (Bahl, Jelinek, and Mercer, 1983) as wellas many other domains (Church, 1988; Brown et al,1990; Kernighan, Church, and Gale, 1990).
A lan-guage model is a probability distribution over stringsP(s) that attempts to reflect the frequency withwhich each string s occurs as a sentence in natu-ral text.
Language models are used in speech recog-nition to resolve acoustically ambiguous utterances.For example, if we have that P(it takes two) >>P(it takes too), then we know ceteris paribus to pre-fer the former transcription over the latter.While smoothing is a central issue in languagemodeling, the literature lacks a definitive compar-ison between the many existing techniques.
Previ-ous studies (Nadas, 1984; Katz, 1987; Church andGale, 1991; MacKay and Peto, 1995) only comparea small number of methods (typically two) on a sin-gle corpus and using a single training data size.
Asa result, it is currently difficult for a researcher tointelligently choose between smoothing schemes.In this work, we carry out an extensiveempirical comparison of the most widely usedsmoothing techniques, including those describedby 3elinek and Mercer (1980), Katz (1987), andChurch and Gale (1991).
We carry out experimentsover many training data sizes on varied corpora us-ing both bigram and trigram models.
We demon-strate that the relative performance of techniquesdepends greatly on training data size and n-gramorder.
For example, for bigram models producedfrom large training sets Church-Gale smoothing hassuperior performance, while Katz smoothing per-forms best on bigram models produced from smallerdata.
For the methods with parameters that canbe tuned to improve performance, we perform anautomated search for optimal values and show thatsub-optimal parameter selection can significantly de-crease performance.
To our knowledge, this is thefirst smoothing work that systematically investigatesany of these issues.In addition, we introduce two novel smooth-ing techniques: the first belonging to the class ofsmoothing models described by 3elinek and Mer-cer, the second a very simple linear interpolationmethod.
While being relatively simple to imple-ment, we show that these methods yield good perfor-mance in bigram models and superior performancein trigram models.We take the performance of a method m to be itscross-entropy on test data1 ITIvT - log  Pro(t,)i=1where Pm(ti) denotes the language model producedwith method m and where the test data T is com-posed of sentences ( t l , .
.
.
, t z r )  and contains a totalof NT words.
The entropy is inversely related tothe average probability a model assigns to sentencesin the test data, and it is generally assumed thatlower entropy correlates with better performance inapplications.3101.1 Smooth ing  n -gram Mode lsIn n-gram language modeling, the probability of astring P(s) is expressed as the product of the prob-abilities of the words that compose the string, witheach word probability conditional on the identity ofthe last n - 1 words, i.e., i f s  = w l - .
.wt  we havel 1P(s) = H P(wi\[w{-1) ~ 1-~ P i-1 (1)i=1 i=1where w i j denotes the words wi ?
?.
wj.
Typically, n istaken to be two or three, corresponding to a bigramor trigram model, respectively.
1Consider the case n = 2.
To estimate the proba-bilities P(wi lwi - , )  in equation (1), one can acquirea large corpus of text, which we refer to as trainingdata, and takeP(Wi-lWi)PML(Wil i-1) -- P(wi-1)c(wi-lWi)/Nse(wi-1)/Nsc(wi_ w )where c(c 0 denotes the number of times the stringc~ occurs in the text and Ns denotes the total num-ber of words.
This is called the maximum likelihood(ML) estimate for P(wilwi_l) .While intuitive, the maximum likelihood estimateis a poor one when the amount of training data issmall compared to the size of the model being built,as is generally the case in language modeling.
For ex-ample, consider the situation where a pair of words,or bigram, say burnish the, doesn't occur in thetraining data.
Then, we have PML(the Iburnish) = O,which is clearly inaccurate as this probability shouldbe larger than zero.
A zero bigram probability canlead to errors in speech recognition, as it disallowsthe bigram regardless of how informative the acous-tic signal is.
The term smoothing describes tech-niques for adjusting the maximum likelihood esti-mate to hopefully produce more accurate probabili-ties.As an example, one simple smoothing technique isto pretend each bigram occurs once more than it ac-tually did (Lidstone, 1920; Johnson, 1932; Jeffreys,1948), yieldingC(Wi-lWi) "\[- 1= + IVlwhere V is the vocabulary, the set of all words be-ing considered.
This has the desirable quality of1To make  the  te rm P(wdw\[Z~,,+~) meaningfu l  fori < n, one can pad  the  beg inn ing  of the  s t r ing  wi tha d i s t ingu ished  token.
In th is  work, we assume there  aren - 1 such  d i s t ingu ished  tokens  preced ing each sentence.preventing zero bigram probabilities.
However, thisscheme has the flaw of assigning the same probabil-ity to say, burnish the and burnish thou (assumingneither occurred in the training data), even thoughintuitively the former seems more likely because theword the is much more common than thou.To address this, another smoothing technique is tointerpolate the bigram model with a unigram modelPML(Wi) = c(wi)/Ns, a model that reflects how of-ten each word occurs in the training data.
For ex-ample, we can takePinto p( i J i-1) = APM (w  pW _l) + (1 -getting the behavior that bigrams involving commonwords are assigned higher probabilities (Jelinek andMercer, 1980).2 Prev ious  WorkThe simplest type of smoothing used in practice isadditive smoothing (Lidstone, 1920; Johnson, 1932;aeffreys, 1948), where we takei w i-1 e(wi_, ,+l) + = + elVl (2)and where Lidstone and Jeffreys advocate /i = 1.Gale and Church (1990; 1994) have argued that thismethod generally performs poorly.The Good-Turing estimate (Good, 1953) is cen-tral to many smoothing techniques.
It is not useddirectly for n-gram smoothing because, like additivesmoothing, it does not perform the interpolation oflower- and higher-order models essential for goodperformance.
Good-Turing states that an n-gramthat occurs r times should be treated as if it hadoccurred r* times, wherer* = (r + 1)n~+land where n~ is the number of n-grams that.
occurexactly r times in the training data.Katz smoothing (1987) extends the intuitions ofGood-Turing by adding the interpolation of higher-order models with lower-order models.
It is perhapsthe most widely used smoothing technique in speechrecognition.Church and Gale (1991) describe a smoothingmethod that combines the Good-Turing estimatewith bucketing, the technique of partitioning a set,of n-grams into disjoint groups, where each groupis characterized independently through a set of pa-rameters.
Like Katz, models are defined recursivelyin terms of lower-order models.
Each n-gram is as-signed to one of several buckets based on its fre-quency predicted from lower-order models.
Eachbucket is treated as a separate distribution andGood-Turing estimation is performed within each,giving corrected counts that are normalized to yieldprobabilities.311Nd bucket ing2?
* ~ ?
%  ?o ?$ o ?
.?
.~  ?e o *?*?
* ??
** o~,~L.s  ?o .
?
ooO o ~ o  ?
*b; .
?
*~a- : .
.
?
.
??
% a t...,~;e.T?
: ?
.
.
.
: ??
% o% **?
~ - ?~?~ ?
o o?
?
?
?~ ?o*?
ooo, , , i  , , , i  , , , i  , " .
.
.
.
0l o  100  1000 10000 100000 0 .o01r~rn~?
of  counts  i n  d i s tN~t \ ]onnew bucket ing.
,  .
.
.
,oeW~ o.
6'V,*?Na,o* * I * , , I , , * I , , * I , *0 .01  0 .1  1 10average  r~n-zem count  in  d i s~but ion  r~nus  OneFigure 1: )~ values for old and new bucketing schemes for Jelinek-Mercer smoothing; each point represents asingle bucketThe other smoothing technique besides Katzsmoothing widely used in speech recognition is dueto Jelinek and Mercer (1980).
They present a classof smoothing models that involve linear interpola-tion, e.g., Brown et al (1992) takei - -1PML(Wi IWi-n+l) "Iv ~Wi__  1 i - -1i - -  n- \ ] - IP~ /W i -1  , (1 - -  )~to~-~ ) inte~pt i wi_n+2) (3)i - -  u-I-1That is, the maximum likelihood estimate is inter-polated with the smoothed lower-order distribution,which is defined analogously.
Training a distinctI ~-1 for each wi_,~+li-1 is not generally felicitous;Wi- -n -{ -1Bahl, Jelinek, and Mercer (1983) suggest partition-i -1  ing the 1~,~-~ into buckets according to c(wi_~+l),i - -  n-l-1where all )~w~-~ in the same bucket are constrainedi - -  n-l-1to have the same value.To yield meaningful results, the data used to esti-mate the A~!-, need to be disjoint from the data~-- n"l-1used to calculate PML .2 In held-out interpolation,one reserves a section of the training data for thispurpose.
Alternatively, aelinek and Mercer describea technique called deleted interpolation where differ-ent parts of the training data rotate in training eitherPML or the A,o!-' ; the results are then averaged.z-- n- \ [ - ISeveral smoothing techniques are motivatedwithin a Bayesian framework, including work byNadas (1984) and MacKay and Peto (1995).3 Novel Smoothing TechniquesOf the great many novel methods that we have tried,two techniques have performed especially well.2When the same data is used to estimate both, settingall )~ ~-~ to one yields the optimal result.Wl- -  n-l-13.1 Method  average-countThis scheme is an instance of Jelinek-Mercersmoothing.
Referring to equation (3), recall thatBahl et al suggest bucketing the A~!-I accordingi - -1 to c(Wi_n+l).
We have found that partitioning the~!
-~ according to the average number of counts* - -~+1per non-zero element ~(~--~"+1) yields better Iw i :~(~:_ .+~)>01results.Intuitively, the less sparse the data for estimat-ing i-1 PML(WilWi_n+l), the larger A~,~-~ should be.
*-- ~-t-1While larger i-1 c(wi_n+l) generally correspond to lesssparse distributions, this quantity ignores the allo-cation of counts between words.
For example, wewould consider a distribution with ten counts dis-tributed evenly among ten words to be much moresparse than a distribution with ten counts all on asingle word.
The average number of counts per wordseems to more directly express the concept of sparse-ness,In Figure 1, we graph the value of ~ assigned toeach bucket under the original and new bucketingschemes on identical data.
Notice that the new buck-eting scheme results in a much tighter plot, indicat-ing that it is better at grouping together distribu-tions with similar behavior.3.2 Method  one-countThis technique combines two intuitions.
First,MacKay and Peto (1995) argue that a reasonableform for a smoothed istribution is?
i -1Pone(W i i-1 c(wL, +l) + Po,,e(wilw _ +9IWi - -nq-1)  = i - -1 c(wi_n+l) +The parameter a can be thought of as the num-ber of counts being added to the given distribution,312where the new counts are distributed as in the lower-order distribution.
Secondly, the Good-Turing esti-mate can be interpreted as stating that the numberof these extra counts should be proportional to thenumber of words with exactly one count in the givendistribution.
We have found that takingi -1O~ = "y \ [n l (Wi_n+l )  -~- ~\] (4)works well, wherei - i  iis the number of words with one count, and where/3and 7 are constants.4 Exper imenta l  Methodo logy4.1 DataWe used the Penn treebauk and T IPSTER cor-pora distributed by the Linguistic Data Consor-tium.
From the treebank, we extracted text fromthe tagged Brown corpus, yielding about one mil-lion words.
From TIPSTER,  we used the Associ-ated Press (AP), Wall Street Journal (WSJ), andSan Jose Mercury News (SJM) data, yielding 123,84, and 43 million words respectively.
We createdtwo distinct vocabularies, one for the Brown corpusand one for the T IPSTER data.
The former vocab-ulary contains all 53,850 words occurring in Brown;the latter vocabulary consists of the 65,173 wordsoccurring at least 70 times in T IPSTER.For each experiment, we selected three segmentsof held-out data along with the segment of train-ing data.
One held-out segment was used as thetest data for performance evaluation, and the othertwo were used as development test data for opti-mizing the parameters of each smoothing method.Each piece of held-out data was chosen to be roughly50,000 words.
This decision does not reflect practicevery well, as when the training data size is less than50,000 words it is not realistic to have so much devel-opment test data available.
However, we made thisdecision to prevent us having to optimize the train-ing versus held-out data tradeoff or each data size.In addition, the development test data is used to op-timize typically very few parameters, o in practicesmall held-out sets are generally adequate, and per-haps can be avoided altogether with techniques uchas deleted estimation.4.2 Smooth ing  Imp lementat ionsIn this section, we discuss the details of our imple-mentations of various smoothing techniques.
Dueto space limitations, these descriptions are not com-prehensive; a more complete discussion is presentedin Chen (1996).
The titles of the following sectionsinclude the mnemonic we use to refer to the imple-mentations in later sections.
Unless otherwise speci-fied, for those smoothing models defined recursivelyin terms of lower-order models, we end the recursionby taking the n = 0 distribution to be the uniformdistribution Punif(wi) = l/ IV\[.
For each method, wehighlight the parameters (e.g., Am and 5 below) thatcan be tuned to optimize performance.
Parametervalues are determined through training on held-outdata.4.2.1 Base l ine  Smooth ing  ( in terp -base l ine)For our baseline smoothing method, we use aninstance of Jelinek-Mercer smoothing where we con-strain all A,~!-I to be equal to a single value A,~ for, -  n-hieach n, i.e.,i--1 i -1  Pb so(wilw _ +i) = A,, +(I Am) -- Pbase(WilWi_n+2)4.2.2 Add i t ive  Smooth ing  (p lus -one  andplus-delta)We consider two versions of additive smoothing.Referring to equation (2), we fix 5 = 1 in p lus -onesmoothing.
In p lus -de l ta ,  we consider any 6.4.2.3 Katz  Smooth ing  (katz)While the original paper (Katz, 1987) uses a singleparameter k, we instead use a different k for eachn > 1, k,~.
We smooth the unigram distributionusing additive smoothing with parameter 5.4.2.4 Church-Ga le  Smooth ing(church-gale)To smooth the counts n~ needed for the Good-Turing estimate, we use the technique described byGale and Sampson (1995).
We smooth the unigramdistribution using Good-tiering without any bucket-ing.Instead of the bucketing scheme described in theoriginal paper, we use a scheme analogous to theone described by Bahl, Jelinek, and Mercer (1983).We make the assumption that whether a bucket islarge enough for accurate Good-Turing estimationdepends on how many n-grams with non-zero countsoccur in it.
Thus, instead of partitioning the spaceof P(wi - JP(wi)  values in some uniform way as wasdone by Church and Gale, we partition the spaceso that at least Cmi n non-zero n-grams fall in eachbucket.Finally, the original paper describes only bigramsmoothing in detail; extending this method to tri-gram smoothing is ambiguous.
In particular, it isunclear whether to bucket trigrams according toi -1  i--1 P(wi_ JP(w d or P(wi_JP(wilwi-1).
We chose theformer; while the latter may yield better perfor-mance, our belief is that it is much more difficultto implement and that it requires a great deal morecomputation.4.2.5 Je l inek -Mercer  Smooth ing(interp-held-out and interp-del-int)We implemented two versions of Jelinek-Mercersmoothing differing only in what data is used to313train the A's.
We bucket the A ~-1 according to Wi--n-bl i-1 C(Wi_~+I) as suggested by Bahl et al Similar to ourChurch-Gale implementation, we choose buckets toensure that at least Cmi n words in the data used totrain the A's fall in each bucket.In in terp -he ld -out ,  the A's are trained usingheld-out interpolation on one of the developmenttest sets.
In in terp -de l - in t ,  the A's are trainedusing the relaxed deleted interpolation technique de-scribed by Jelinek and Mercer, where one word isdeleted at a time.
In in terp -de l - in t ,  we bucketan n-gram according to its count before deletion, asthis turned out to significantly improve performance.4.2.6 Novel Smoothing Methods(new-avg-count and new-one-count)The implementation new-avg-count,  correspond-ing to smoothing method average-count, is identicalto in terp -he ld -out  except that we use the novelbucketing scheme described in section 3.1.
In theimplementation new-one-count,  we have differentparameters j3~ and 7~ in equation (4) for each n.5 Resu l t sIn Figure 2, we display the performance of thein terp -base l ine  method for bigram and trigrammodels on TIPSTER, Brown, and the WSJ subsetof TIPSTER.
In Figures 3-6, we display the relativeperformance of various smoothing techniques withrespect o the baseline method on these corpora, asmeasured by difference in entropy.
In the graphson the left of Figures 2-4, each point represents anaverage over ten runs; the error bars represent heempirical standard deviation over these runs.
Dueto resource limitations, we only performed multipleruns for data sets of 50,000 sentences or less.
Eachpoint on the graphs on the right represents a sin-gle run, but we consider sizes up to the amount ofdata available.
The graphs on the bottom of Fig-ures 3-4 are close-ups of the graphs above, focusingon those algorithms that perform better than thebaseline.
To give an idea of how these cross-entropydifferences translate to perplexity, each 0.014 bitscorrespond roughly to a 1% change in perplexity.In each run except as noted below, optimal val-ues for the parameters of the given technique weresearched for using Powell's search algorithm as real-ized in Numerical Recipes in C (Press et al, 1988,pp.
309-317).
Parameters were chosen to optimizethe cross-entropy of one of the development test setsassociated with the given training set.
To constrainthe search, we searched only those parameters thatwere found to affect performance significantly, asverified through preliminary experiments over sev-eral data sizes.
For katz  and church-gale ,  we didnot perform the parameter search for training setsover 50,000 sentences due to resource constraints,and instead manually extrapolated parameter val-Method Linesinterp-baseline ~ 400plus-one 40p lus -de l ta  40katz  300church-gale i000?nterp-held-out 400interp-del-int 400new-avg-count 400new-one-count 50Table 1: Implementation difficulty of various meth-ods in terms of lines of C++ codeues from optimal values found on smaller data sizes.We ran in terp -de l - in t  only on sizes up to 50,000sentences due to time constraints.From these graphs, we see that additive smooth-ing performs poorly and that methods katz  andin terp -he ld -out  consistently perform well.
Ourimplementation church-ga le  performs poorly ex-cept on large bigram training sets, where it performsthe best.
The novel methods new-avg-count andnew-one-count perform well uniformly across train-ing data sizes, and are superior for trigram models.Notice that while performance is relatively consis-tent across corpora, it varies widely with respect otraining set size and n-gram order.The method interp-del-int performs signifi-cantly worse than in terp -he ld -out ,  though theydiffer only in the data used to train the A's.
However,we delete one word at a time in in terp -de l - in t ;  wehypothesize that deleting larger chunks would leadto more similar performance.In Figure 7, we show how the values of the pa-rameters 6 and Cmin affect the performance of meth-ods katz  and new-avg-count,  respectively, over sev-eral training data sizes.
Notice that poor parametersetting can lead to very significant losses in perfor-mance, and that optimal parameter settings dependon training set size.To give an informal estimate of the difficulty ofimplementation f each method, in Table 1 we dis-play the number of lines of C++ code in each imple-mentation excluding the core code common acrosstechniques.6 D iscuss ionTo our knowledge, this is the first empirical compari-son of smoothing techniques in language modeling ofsuch scope: no other study has used multiple train-ing data sizes, corpora, or has performed parameteroptimization.
We show that in order to completely3To implement the baseline method, we just used theinterp-held-out code as it is a special case.
Writtenanew, it probably would have been about 50 lines.31411.510.5109.50a.5average over ten runs at each size, up to 50,0OO sentences" -~: :  : .
TIPSTER bigram"-.
"'~:-WS.J bigrarn1000 10000sentences of training data (-25 words~sentence)t l .51110.5tO9.508.587.576.5tOOsingle run at each size",..~io~n t rigrarn-.~ ~..
."
' , .
,  " ' "~ ' .
: " -~ .
TIPSTER bigram.
.
.
.
.
.
, .
.
:= : : : : ;  .
.
.
.
.
.
.
.
.
.V~SJ b~gram.
TIPSTER tdgra~tO00 1O000 100000 le+06 )e+07sentences of training data (-25 words/sentence)Figure 2: Baseline cross-entropy on test data; graph on left displays averages over ten runs for training setsup to 50,000 sentences, graph on right displays single runs for training sets up to 10,000,000 sentencesaverage over ten runs at each size, up to 50,000 sentences7 .
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.  )
?
.
.l~Us-one ........... ~ ............ ~ ............ = ................6 ....... ~ .....c~ ..... plu s=dsita ..........
I ....4 ........... .. !._c- t  .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~1000 10000sentences of training data (-25 wordS/sentence)single run at each size, up to 10,000,000 sentences.
.
,  .
.
,  .
.
.
,  .
.
.
,  .
.+ ......... +....--~ plus~ne..... ..~...y~ " ' " '~"  -..+.,..,,.,.,-" .... o.-..-~'""~.......~ ' - .
.
.--..
~,2=,j .
.......1 J .church-gata " "*ks/z, interp-held-out, ~nterpdel-int, new-avg-count, new-one-count (see below)-1  , ?
.
.
.
.
.
.
.
.  '
' ' " '  "100 1000 10000 100000 le+06 le+07sentences of training data (-25 words~sentence)average over ten runs at each size, up to 50,000 sentences single run at each size, Up to 10,000,000 sentences0.04 .
.
, ?
- - , ?
- , - - , ?0 ...........................................................................................................................................................-0,02 "~" - .
.
i n t e r p - d e l q n t-0.00~.1 n ........... ~t  1 ............ ~ ........... ~ ......................... ~ ................ ::::::::::::::::::::::::::::::::::::::::::::::-0 .16  .
.
.
.
.
.
.
.
J .
.
.
.
.
.
.
.
.
.
.too 1000 10000sentences of training data (-25 words~sentence}o.02 o .,';'~o~o,-~nt t-0.02-0.04 ~.-..._Z .~ .
.
.
.
.
.
~ .
- .
katz dI" / :*"  - ' -"(" '" " 'm.. .
.~ .
, \ ] \ [F  ,.a ........ a " "~ ' ,JO.O8 " inteq)qle)d-out .
.o ' " "~3.1 .~" .~- - - -~ .
new-one-count c / "  x .
.
.
-~-0.12 ..... " t~"""  " .
.
.
.
.
.
.
.
.
~- .
.
.
.
.
- "  .
/  new-svg-count-0 .14  k .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x _~_ .
.
_~.
.
.
i "  "~"~"0.1610 o lO0O 1OOOO 10oooo le+06 le+07sentences of training data (-25 words/sentonce)Figure 3: Trigram model on T IPSTER data; relative performance of various methods with respect to baseline;graphs on left display averages over ten runs for training sets up to 50,000 sentences, graphs on right displaysingle runs for training sets up to 10,000,000 sentences; top graphs show all algorithms, bottom graphs zoomin on those methods that perform better than the baseline method315E =oaverage over ten runs at each size, up to 50,000 senlences5 .
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
, ?
?
-4.6 .... ~- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
"~'" plus-o'n~ ... .
.
.
.
.
.
.
.
.
.
~ ....4 ...........3.S "3 ....... t~.,, " " -~ ........... ~ p lus4e l~2.5 .
.
.
.
.
.
.
.
.
.
.
*1.
Ichurch*gale '0.50 .
.
.
.
.-05100 1000 10000sentences of training data ( -26  words/sentence)average over t~ runs at each size, up to 50,000 sentences0.02 .
.
.
.
.
.
.
, .
.
.
.
.
.
.
, .
.
?-0.02 " "  "~... humh*gale~- .. .
.
.
......~ ............ {.
..............-0.00. .
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
- '~- '~"  :=L::~" T n :w:n2ount  / ~  .
.
.
.
.
.
.
.
1.0.14100 1000 10000sentences of training data ( -26  words/sentence)single run at each size, up to 10,000,000 sentences5 , ?
.
,  .
?
.
.
.
,  .
?
.
,  .
?4 " ..3.5 "~"'~".. .
I~US*one2.5 t ' ' ? '
" " '~ ' " " 'o .
.
.
.
.
.
.
.
.
. "
" "o .
"* .1 f - - , ~  " "~" " p us-de ta church-gale " '~.. .O.5 "~- .
.- - .
_o .~3.6 T , ,k~tz, taterp-he~-out., interp~, el~tat, ,~ew,~zvg~ou ' .
.
.
.
.
.
~ne.~ount  !
.
.
.
.
.
.
,ow), l100 1000 10000 100000 le+06 le+07sentences of training data ( -26  words/sentence)stagle r~n at each size, up to 10,OCO,O00 sentences0.02 ?
?
?
,  .
, ?
.
,  .
,  .
.
.o I -0.02 church*gale ...~:,'~-0.04 ~ " " ,? "
.." .
n erp-he d-out . "
~,~ -~  Interprdel-mt .
.
-  .
* ~ ' "  .
.
.
.
.
.
.
.
~ .
.~  .~.
/ .
-~0~-0.1 new-one-count ..~D -...B'" .~..?
.
- .
.~ j .~ .~.
.
: .
: : ;$ .
,  ~Om 12ew-avg-count.0.14 ' , , I , , , i , , , m , .
, I , ,10o 10oo 1ooo0 1oo000 le+06 le+07sentences of training data ( -25 wo~ds/sentence)Figure 4: Bigram model on TIPSTER data; relative performance of various methods with respect to baseline;graphs on left display averages over ten runs for training sets up to 50,000 sentences, graphs on right displaysingle runs for training sets up to 10,000,000 sentences; top graphs show all algorithms, bottom graphs zoomin on those methods that perform better than the baseline methodbigram model0.02 .
.
.
.
.
.
, .
.
.
.
.
.
,0i -0.02 church-galeinterprdel-int.0,04 ..-~, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~..0.00.0.o8 .
.
.
z "  ......... ~*..-0.12 "= .
~inte~p~held-outlew-a-~n~t .
.
.~- -~272~: : ' z - - .
.
"n~:orpe-~t  a ...... .
.
.
.
D .
.
.
.
.
.
.
.
.
.-0.16-0.18 .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
i100 1000 IOQO0sentences of training data (-21 words~sentence)tzigram model0 ........................................................................................................................................................................0.02-0.06 katz .
- -~"  " '~ ' " ' " " ' "?
.
.
- : : : .
.
.
.
.
.
.
.
.
.
.
.,,<.
:..-"-0.06 .-" .
.
.
.
.
.
.
.
.
.
.
i  r ~ te t p..<1 el-~ip_t .
.
.
.
.
.
.
.
.
..0.12 : : : .
- .
.  "
'~=.
.
.
.
.. .
.
.
.
.
.
.
~ ... .
.
.
.
.
.
.
.
Q.. interp*held-out.
.
.
.
.
.
.
.
.
.
.
.
7~.=: =-P~::.... " ......... e .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o .
.
.
.
.
.
.
.
.
.
.
e ... .
.
.
.
.
.
.
.
~ .. .
.
.
.
.
.
.
.
.
.
.
.
..0.14 - ~ - = ' : : :=*~-~_ .___  ~ new-one-count-0 .1600 1000 10000sentences of traJelng data (-21 words/sentence)Figure 5: Bigram and trigram models on Brown corpus; relative performance of various methods with respectto baseline316bigram model  tdgram model"(, ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o 0 i ~ 0.02-0.02 hurch-gale~ '--nt erp~J el-int "~ -0.04 .
.
.
.
.
.
~ -0.02inte rp-d el-int " .
.
~ inte rpheld~out .
.
.
~""  ~ " ' -~E -0.06 ? "
.
,  ~,~.
\] ~ - .
-  ~,. "
'A .-- -0.06 .
- ' - ' ka tz  " " -~ " - .=  .
.-0?3 ' : i :  , .
.
........ ::.>~,.- .~.
..... .
.
~ " ' -  - : : : : .
.
,y -oo  i -.
.
.
.
.
.
.
.
.
.
.
.
.
.
.-0.14 ? "
" "  "~ " ? "
-k-atz-0.13 ~ -018 .
.
.
.
.
.
.
.
= ' ' ,1oo 1000 10oo0 100000 le+06 10o 1000 10000 100000 le+0osentences of training data ( -25  words/sentence) sentences of t relelr~g data ( -25  words/sentence)Figure 6: Bigram and trigram models on Wall Street Journal corpus; relative performance of various methodswith respect to baselinez~C==.=_performance el katz with respect to delta1.6 .
.
.
.
, ?
.
.
,  .
.
,  .
.
.
,  ?
.
.
,  .
.10O senl1.41.2110,0O0 sent0.8 1,0O0 sent ..a0.6 / '  .,.~/"0.4 / .-" .ED,O00 sent)<0.2" ' " 'd : .
/ .." ~ ' "e .
.
.
.
I , , , i  , , , r  , , , I  , , , i  , , ,0.0Ol o.01 0.1 1 lO 10o 1000delta-0.0O-0.07==--O.082 -0.O3-0.1-0.11-0.12-0.13performance of new-avg-c~nt with respect to c-min.
.
.
,  .
.
.
,  .
.x\\ /~'\ lO.000,000 sent / "  // "x \ ,  , .o" , \  / /  ,,,".... /'"6.
..'"' 2 l" " " 'u ,  1 OO3,0OO sent  " /j /10 ,0O0 sent10 100 tO00 10(00 100000minimum number of counts per bucketFigure 7: Performance of katz and new-avg-count with respect to parameters ~ and Cmin, respectivelycharacterize the relative performance of two tech-niques, it is necessary to consider multiple trainingset sizes and to try both bigram and trigram mod-els.
Multiple runs should be performed wheneverpossible to discover whether any calculated differ-ences are statistically significant.
Furthermore, weshow that sub-optimM parameter selection can alsosignificantly affect relative performance.We find that the two most widely used techniques,Katz smoothing and Jelinek-Mercer smoothing, per-form consistently well across training set sizes forboth bigram and trigram models, with Katz smooth-ing performing better on trigram models producedfrom large training sets and on bigram models ingeneral.
These results question the generality of theprevious reference result concerning Katz smooth-ing: Katz (1987) reported that his method slightlyoutperforms an unspecified version of Jelinek-Mercersmoothing on a single training set of 750,000 words.Furthermore, we show that Church-Gale smooth-ing, which previously had not been compared withcommon smoothing techniques, outperforms all ex-isting methods on bigram models produced fromlarge training sets.
Finally, we find that our novelmethods average-count and one-count are superiorto existing methods for trigram models and performwell on bigram models; method one-count yieldsmarginally worse performance but is extremely easyto implement.In this study, we measure performance solelythrough the cross-entropy of test data; it wouldbe interesting to see how these cross-entropy differ-ences correlate with performance in end applicationssuch as speech recognition.
In addition, it would beinteresting to see whether these results extend tofields other than language modeling where smooth-ing is used, such as prepositional phrase attachment(Collins and Brooks, 1995), part-of-speech tagging(Church, 1988), and stochastic parsing (Magerman,1994).317AcknowledgementsThe authors would like to thank Stuart Shieber andthe anonymous reviewers for their comments on pre-vious versions of this paper.
We would also like tothank William Gale and Geoffrey Sampson for sup-plying us with code for "Good-Turing frequency esti-mation without ears."
This research was supportedby the National Science Foundation under Grant No.IRI-93-50192 and Grant No.
CDA-94-01024.
Thesecond author was also supported by a National Sci-ence Foundation Graduate Student Fellowship.ReferencesBahl, Lalit R., Frederick Jelinek, and Robert L.Mercer.
1983.
A maximum likelihood approachto continuous peech recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, PAMI-5(2):179-190, March.Brown, Peter F., John Cocke, Stephen A. DellaPi-etra, Vincent J. DellaPietra, Frederick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79-85, June.Brown, Peter F., Stephen A. DellaPietra, Vincent J.DellaPietra, Jennifer C. Lai, and Robert L. Mer-cer.
1992.
An estimate of an upper bound forthe entropy of English.
Computational Linguis-tics, 18(1):31-40, March.Chen, Stanley F. 1996.
Building Probabilistic Mod-els for Natural Language.
Ph.D. thesis, HarvardUniversity.
In preparation.Church, Kenneth.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProceedings of the Second Conference on AppliedNatural Language Processing, pages 136-143.Church, Kenneth W. and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilities of English bigrams.
Computer Speech andLanguage, 5:19-54.Collins, Michael and James Brooks.
1995.
Prepo-sitional phrase attachment through a backed-offmodel.
In David Yarowsky and Kenneth Church,editors, Proceedings of the Third Workshop onVery Large Corpora, pages 27-38, Cambridge,MA, June.Gale, William A. and Kenneth W. Church.
1990.Estimation procedures for language context: poorestimates are worse than none.
In COMP-STAT, Proceedings in Computational Statistics,9th Symposium, pages 69-74, Dubrovnik, Yu-goslavia, September.Gale, William A. and Kenneth W. Church.
1994.What's wrong with adding one?
In N. Oostdijkand P. de Haan, editors, Corpus-Based Researchinto Language.
Rodolpi, Amsterdam.Gale, William A. and Geoffrey Sampson.
1995.Good-Turing frequency estimation without ears.Journal of Quantitative Linguistics, 2(3).
To ap-pear.Good, I.J.
1953.
The population frequencies ofspecies and the estimation of population parame-ters.
Biometrika, 40(3 and 4):237-264.Jeffreys, H. 1948.
Theory of Probability.
ClarendonPress, Oxford, second edition.Jelinek, Frederick and Robert L. Mercer.
1980.
In-terpolated estimation of Markov source parame-ters from sparse data.
In Proceedings of the Work-shop on Pattern Recognition in Practice, Amster-dam, The Netherlands: North-Holland, May.Johnson, W.E.
1932.
Probability: deductive andinductive problems.
Mind, 41:421-423.Katz, Slava M. 1987.
Estimation of probabilitiesfrom sparse data for the language model com-ponent of a speech recognizer.
IEEE Transac-tions on Acoustics, Speech and Signal Processing,ASSP-35(3):400-401, March.Kernighan, M.D., K.W.
Church, and W.A.
Gale.1990.
A spelling correction program based ona noisy channel model.
In Proceedings of theThirteenth International Conference on Compu-tational Linguistics, pages 205-210.Lidstone, G.J.
1920.
Note on the general case of theBayes-Laplace formula for inductive or a posteri-ori probabilities.
Transactions of the Faculty ofActuaries, 8:182-192.MacKay, David J. C. and Linda C. Peto.
1995.
A hi-erarchical Dirichlet language model.
Natural Lan-guage Engineering, 1(3):1-19.Magerman, David M. 1994.
Natural Language Pars-ing as Statistical Pattern Recognition.
Ph.D. the-sis, Stanford University, February.Nadas, Arthur.
1984.
Estimation of probabilities inthe language model of the IBM speech recognitionsystem.
IEEE Transactions on Acoustics, Speechand Signal Processing, ASSP-32(4):859-861, Au-gust.Press, W.H., B.P.
Flannery, S.A. Teukolsky, andW.T.
Vetterling.
1988.
Numerical Recipes in C.Cambridge University Press, Cambridge.318
