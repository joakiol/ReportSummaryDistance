Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 342?347,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsExploring Consensus in Machine Translation for Quality EstimationCarolina Scarton and Lucia SpeciaDepartment of Computer Science, University of SheffieldRegent Court, 211 Portobello, Sheffield, S1 4DP, UK{c.scarton,l.specia}@sheffield.ac.ukAbstractThis paper presents the use of consensusamong Machine Translation (MT) systemsfor the WMT14 Quality Estimation sharedtask.
Consensus is explored here by com-paring the MT system output against sev-eral alternative machine translations usingstandard evaluation metrics.
Figures ex-tracted from such metrics are used as fea-tures to complement baseline predictionmodels.
The hypothesis is that knowingwhether the translation of interest is simi-lar or dissimilar to translations from multi-ple different MT systems can provide use-ful information regarding the quality ofsuch a translation.1 IntroductionWhile Machine Translation (MT) evaluation met-rics can rely on the similarity of the MT systemoutput to reference (human) translations as a proxyto quality assessment, this is not possible for MTsystems in use, translating unseen texts.
QualityEstimation (QE) metrics are used in such settingsas a way of predicting translation quality.
Whilereference translations are not available for QE,previous work has explored the so called pseudo-references (Soricut and Echihabi, 2010; Soricut etal., 2012; Soricut and Narsale, 2012; Shah et al.,2013).
Pseudo-references are alternative transla-tions produced by MT systems different from thesystem that we intend to predict quality for (Al-brecht and Hwa, 2008).
These can be used to pro-vide additional features to train QE models.
Suchfeatures are normally figures resulting from au-tomatic metrics (such as BLEU, Papineni et al.
(2002)) computed between pseudo-references andthe output of the given MT system.Soricut and Echihabi (2010) explore pseudo-references for document-level QE prediction torank outputs from an MT system.
The pseudo-references-based features are BLEU scores ex-tracted by comparing the output of the MT sys-tem under investigation and the output of an off-the-shelf MT system, for both the target and thesource texts.
The statistical MT system trainingdata is also used as pseudo-references to computetraining data-based features.
The use of pseudo-references has been shown to outperform strongbaseline results.
Soricut and Narsale (2012) pro-pose a method that uses sentence-level predictionmodels for document-level QE.
They also use apseudo-references-based feature (based in BLEU)and claim that this feature is one of the most pow-erful in the framework.For QE at sentence-level, Soricut et al.
(2012)use BLEU based on pseudo-references combinedwith other features to build the best QE system ofthe WMT12 QE shared task.1Shah et al.
(2013)use pseudo-references in the same way to ex-tract a BLEU feature for sentence-level prediction.Feature analysis on a number of datasets showedthat this feature contributed the most across alldatasets.Louis and Nenkova (2013) apply pseudo-references for summary evaluation.
They use sixsystems classified as ?best systems?, ?mediocresystems?
or ?worst systems?
to make the compar-ison, with ROUGE (Lin and Och, 2004) as qualityscore.
They also experiment with a combination ofthe ?best systems?
and the ?worst systems?.
Theuse of only ?best systems?
led to the best results.Examples of ?bad summaries?
are said not to bevery useful because a summary close to the worstsystems outputs can mean that either it is bad orit is too different from the best systems outputs interms of content.
Albrecht and Hwa (2008) usepseudo-references to improve MT evaluation bycombining them with a single human reference.They show that the use of pseudo-references im-1http://www.statmt.org/wmt12/342proves the correlation with human judgements.Soricut and Echihabi (2010) claim that pseudo-references should be produced by systems as dif-ferent as possible from the MT system of interest.This ensures that the similarities found among thesystems?
translations are not related to the similar-ities of the systems themselves.
Therefore, the as-sumption that a translation from system X sharessome characteristics with a translation from sys-tem Y is not a mere coincidence.
Another way tomake the most of pseudo-references is to use anMT system known as generally better (or worse)than the MT system of interest.
In that case, thecomparison will lead to whether the MT system ofinterest is similar to a good (or bad) MT system.However, in most scenarios it is difficult to relyon the average translation quality of a given sys-tem as an absolute indicator of its quality.
Thisis particularly true for sentence-level QE, wherethe quality of a given system can vary signifi-cantly across sentences.
Finding translations fromMT systems that are considerably different canalso be a challenge.
In this paper we exploitpseudo-references in a different way: measuringthe consensus among different MT systems in thetranslations they produce.
As sources of pseudo-references, we use translations given in a multi-translation dataset or those produced by the par-ticipants in the WMT translation task for the samedata.
While some MT systems can be similarto each other, for some language pairs, such asEnglish-Spanish, a wide range of MT systemswith different average qualities are available.
Ourhypothesis is that by using translations from sev-eral MT systems we can find consensual infor-mation (even if some of the systems are similarto the one of interest).
The use of more than oneMT system is expected to smooth out the effectof ?coincidences?
in the similarities between sys-tems?
translations.This paper describes the use of consensualinformation for the WMT14 QE shared task(USHEFF-consensus system), simulating a sce-nario where we do not know the quality of thepseudo-references, nor the characteristics of anyMT systems (the system of interest or the systemswhich generated the pseudo-references).
We par-ticipated in all variants of Task 1, sentence-levelQE, for both for scoring and ranking.
Section 2explains how we extracted consensual informationfor all tasks.
Section 3 shows our official resultscompared to the baselines provided.
Section 4presents some conclusions.2 Consensual information extractionThe consensual information is exploited in twodifferent ways in Task 1.
Task 1.1 used?perceived?post-editing effort labels as quality scores for scor-ing and ranking in four languages pairs.
These la-bels vary within [1-3], where:?
1 = perfect translation?
2 = near miss translation (sentences with 2-3errors that are easy to fix)?
3 = very low quality sentence.The training and test sets for each languagepair in Task 1.1 contain 3-4 translations of thesame source sentences.
The language pairs areGerman-English (DE-EN) with 150 source sen-tences for test and 350 source sentences for train-ing, English-German (EN-DE) with 150 sourcesentences for test and 350 source sentencesfor training, English-Spanish (EN-ES) with 150source sentences for test and 954 source sentencesfor training, and Spanish-English (ES-EN) with150 source sentences for test and 350 source sen-tences for training.
The translations for each lan-guage pair include a human translation and trans-lations produced by a statistical MT (SMT) sys-tem, a rule-based MT (RBMT) system, and a hy-brid system (for the EN-DE and EN-ES languagepairs only).By inspecting the source side of the training set,we noticed that the translations were ordered persystems, since the source file had sentences re-peated in batches.
For example, the EN-ES lan-guage pair had 954 English sentences and 3,816Spanish sentences.
In the source file, the Englishsentences were repeated in batches of 954 sen-tences.
Based on that, we assumed that in the tar-get file each set of 954 translations in sequencecorresponded to a given MT system (or human).For each system (human translation is consid-ered as a system, since we do not know the or-der of the translations), we calculate the consen-sual information considering the other 2-3 systemsavailable as pseudo-references.The quality scores for Task 1.2 and Task 1.3were computed as HTER (Human Translation Er-ror Rate (Snover et al., 2006)) and post-editingtime, respectively, for both scoring and ranking.343The datasets were a mixture of test sets from theWMT13 and WMT12 translation shared tasks forthe EN-ES language pair only.
In this case, theconsensual information was extracted by usingsystems submitted to the WMT translation sharedtasks of both years.
Therefore, for each sourcesentence in the WMT12/13 data, all translationsproduced by the participating MT systems of thatyear were used as pseudo-references.
The uedinsystem outputs for both WMT13 and WMT12were not considered, since the datasets in Tasks1.2 and 1.3 were created from translations gener-ated by this system.2The Asyia Toolkit3(Gim?enez and M`arquez,2010) was used to extract the automatic metricsconsidered as features.
BLEU, TER (Snover etal., 2006), METEOR (Banerjee and Lavie, 2005)and ROUGE (Lin and Och, 2004) are used inall task variants.
For Tasks 1.2 and 1.3 we alsouse metrics based on syntactic similarities fromshallow and dependency parser information (met-rics SPOc(*) and DPmHWCM c1, respectively, inAsyia).
BLEU is a precision-oriented metric thatcompares n-grams (n=1-4 in our case) from refer-ence documents against n-grams of the MT out-put, measuring how close the output of a systemis to one or more references.
TER (TranslationError Rate) measures the minimum number of ed-its required to transform the MT output into theclosest reference document.
METEOR (Metricfor Evaluation of Translation with Explicit OR-dering) scores MT outputs by aligning them withgiven references.
This alignment can be done byexact, stem, synonym and paraphrases matching(here, exact matching was used).
ROUGE is arecall-oriented metric that measures similarity be-tween sentences by considering the longest com-mon n-gram statistics between a translation sen-tence and the corresponding reference sentence.SPOc(*) measures the lexical overlap according tothe chunk types of the syntactic realisation.
The?*?
means that an average of all chunk types iscomputed.
DPmHWCM c1 is based on the match-ing of head-word chains.
We considered the matchof grammatical categories of only one head-word.These consensual features are combined withthe 17 QuEst baseline features provided by theshared task organisers.2WMT14 QE shared task organisers, personal communi-cation, March 2014.3http://asiya.lsi.upc.edu/3 Experiments and ResultsThe results reported herein are the official sharedtask results, that is, they were computed using thetrue scores of the test set made available by theorganisers after our submission.For training the QE models, we used Sup-port Vector Machines (SVM) regression algorithmwith a radial basis function (RBF) kernel withthe hyperparameters optimised via grid search.The scikit-learn algorithm available in the QuEstFramework4(Specia et al., 2013) was used forthat.We compared the results obtained against usingonly the QuEst baseline (BL) features, which isthe same system used as the official baseline forthe shared task.
For the scoring variant we alsocompare our results against a baseline that ?pre-dicts?
the average of the true scores of the train-ing set as scores for each sentence of the test set(Mean ?
each sentence has the same predictedscore).For all language pairs in Task 1.1, Table 1 showsthe average results for the scoring variant usingMAE (Mean Absolute Error) as evaluation met-ric, while Table 2 shows the results for the rankingvariant using DeltaAvg.The results for scoring improved over the base-lines with the use of consensual information forlanguage pairs DE-EN and EN-ES.
For EN-DEand ES-EN the consensual features achieved simi-lar results to BL.
The best result for consensual in-formation features was achieved with EN-ES (0.03of MAE difference from BL).For the ranking variant, the consensual informa-tion improved the results for all language pairs.The largest improvement from consensual-basedfeatures was achieved for ES-EN, with a differ-ence of 0.11 from the baseline.
It is worth men-tioning that for ES-EN our system achieved thebest ranking result in Task 1.1.Since the results varied for different languagespairs, we further inspected them for each languagepair.
First, we looked at the true scores distributionand realised that the first batch of translations foreach language pair was probably the human refer-ence since the percentage of 1s ?
the best qualityscore ?
was much higher for this system (see Fig-ure 1 for EN-DE as an example).
By using thishuman translation as a reference for the other MTsystems, we computed BLEU for each sentence4http://www.quest.dcs.shef.ac.uk/344DE-EN EN-DE EN-ES ES-ENMean 0.67 0.68 0.46 0.58BL 0.65 0.64 0.52 0.57BL+Consensus 0.63 0.64 0.49 0.57Table 1: Scoring results for Task 1.1 in terms of MAEDE-EN EN-DE EN-ES ES-ENBL 0.21 0.23 0.14 0.12BL+Consensus 0.28 0.26 0.21 0.23Table 2: Ranking results for Task 1.1 in terms of DeltaAvgand averaged these values.
The results are shownin Table 3.For DE-EN, EN-DE and EN-ES, the varioussystems appeared to be less dissimilar in termsof BLEU, when compared to ES-EN.
For ES-EN,the difference between the two MT systems washigher than for other language pairs (0.12 for thetest set and 0.11 for the training set).
Moreover,for DE-EN, EN-DE and EN-ES, the difference be-tween the averaged BLEU score of the training setand the average BLEU score of the test set is verysmall (smaller than 0.01).
For ES-EN, however,the difference between the scores for the trainingand test sets was also higher (0.04 for System1 and0.03 for System2).
This can be one reason why theconsensual features did not show improvementsfor this language pair.
Since the systems are con-siderably different and also there is a considerabledifference between training and test sets, the datacan be too noisy to be used as pseudo-references.For EN-DE, the reasons for the bad perfor-mance of consensual features are not clear.
Thislanguage pair showed the worst average qualityscores for all systems.
Reasons for this can includecharacteristics of German language, such as com-pound words which are not well treated in MT, andcomplex grammar.
One hypothesis is that theselow BLEU scores (as Table 3 shows) introducenoise instead of useful information for QE.
An-other difference that appeared only in EN-DE wasthe distributions of the scores across the differentsystems.
As Figure 1 shows, System1 has a dis-tribution considerably different from the other twosystems.
For the other language pairs, the distribu-tions across different systems were more uniform.This difference can be another factor influencingthe results for this language pair.Table 4 shows the results for scoring (MAE) andTable 5 shows the results for ranking (DeltaAvg)for Tasks 1.2 and 1.3.Task 1.2 Task 1.3Mean 16.93 23.34BL 15.23 21.49BL+Consensus 13.61 21.48Table 4: Scoring results of Tasks 1.2 and 1.3 interms of MAETask 1.2 Task 1.3BL 5.08 14.71BL+Consensus 7.93 14.98Table 5: Ranking results of Tasks 1.2 and 1.3 interms of DeltaAvgFor Tasks 1.2 and 1.3 the use of consensualinformation only slightly improved the baselineresults for scoring.
For the ranking variant,BL+Consensus achieved better results, but onlysignificantly so for Task 1.2.
Therefore, consen-sual information seems useful to rank sentencesaccording to predicted HTER, its contribution topredicting actual HTER is not noticeable.
Forpost-editing time as quality labels, the improve-ment achieved with the use of consensual infor-mation was marginal.4 ConclusionsThe use of consensual information of MT systemscan be useful to improve state-of-the-art results forQE.
For some scenarios, it is possible to acquireseveral translations for a given source segment,but with no additional information on the qual-ity or type of MT systems used to produce them.Therefore, these translations could not be used aspseudo-references in the same way as in (Soricutand Echihabi, 2010).345DE-EN EN-DE EN-ES ES-ENSys1 Sys2 Sys1 Sys2 Sys3 Sys1 Sys2 Sys3 Sys1 Sys2Average BLEU(test) 0.31 0.25 0.20 0.19 0.21 0.36 0.29 0.32 0.44 0.32Average BLEU(training) 0.31 0.26 0.21 0.18 0.22 0.35 0.29 0.31 0.40 0.29Table 3: Average BLEU of systems in Task 1.1Figure 1: Distribution of true quality scores for the EN-DE language pairThe use of several references with the hypoth-esis that they share consensual information hasbeen shown useful in some settings, particularlyin Task 1.1.
In others, the results were inconclu-sive.
In particular, the approach does not seem ap-propriate for scenarios where the MT systems areconsiderably different (as shown in Table 3).
Inthose cases, better ways to exploit consensual in-formation need to be investigated further.Acknowledgements: This work was supportedby the EXPERT (EU Marie Curie ITN No.317471) project.ReferencesJoshua S. Albrecht and Rebecca Hwa.
2008.
Therole of pseudo references in mt evaluation.
In Pro-ceedings of WMT 2008, pages 187?190, Columbus,Ohio, USA.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
In Proceedingsof the ACL 2005Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.Jes?us Gim?enez and Llu?
?s M`arquez.
2010.
Asiya: AnOpen Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, (94):77?86.Chin-Yew Lin and Franz J. Och.
2004.
AutomaticEvaluation of Machine Translation Quality UsingLongest Common Subsequence and Skip-BigramStatics.
In Proceedings of ACL 2004, Barcelona,Spain.Annie Louis and Ani Nenkova.
2013.
Automaticallyassessing machine summary content without a goldstandard.
Computational Linguistics, 39(2):267?300, June.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL2002, pages 311?318, Philadelphia, USA.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An Investigation on the Effectiveness of Features forTranslation Quality Estimation.
In Proceedings ofthe XIV MT Summit, pages 167?174, Nice, France.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA 2006, pages 223?231.Radu Soricut and Abdessamad Echihabi.
2010.TrustRank: Inducing Trust in Automatic Transla-346tions via Ranking.
In Proceedings of the ACL 2010,pages 612?621, Uppsala, Sweden.Radu Soricut and Sushant Narsale.
2012.
Combin-ing Quality Prediction and System Selection for Im-proved Automatic Translation Output.
In Proceed-ings of WMT 2012, Montreal, Canada.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL Language Weaver Systems in the WMT12Quality Estimation Shared Task.
In Proceedings ofWMT 2012, Montreal, Canada.Lucia Specia, Kashif Shah, Jose G.C.
de Souza, andTrevor Cohn.
2013.
Quest - a translation quality es-timation framework.
In Proceedings of WMT 2013:System Demonstrations, ACL-2013, pages 79?84,Sofia, Bulgaria.347
