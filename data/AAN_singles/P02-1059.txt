Supervised Ranking in Open-Domain Text SummarizationTadashi NomotoNational Institute of Japanese Literature1-16-10 Yutaka ShinagawaTokyo 142-8585, Japannomoto@nijl.ac.jpYuji MatsumotoNara Institute of Science and Technology8916-5 Takayama IkomaNara 630-0101, Japanmatsu@is.aist-nara.ac.jpAbstractThe paper proposes and empirically moti-vates an integration of supervised learningwith unsupervised learning to deal withhuman biases in summarization.
In par-ticular, we explore the use of probabilisticdecision tree within the clustering frame-work to account for the variation as wellas regularity in human created summaries.The corpus of human created extracts iscreated from a newspaper corpus and usedas a test set.
We build probabilistic de-cision trees of different flavors and in-tegrate each of them with the clusteringframework.
Experiments with the cor-pus demonstrate that the mixture of thetwo paradigms generally gives a signif-icant boost in performance compared tocases where either of the two is consideredalone.1 IntroductionNomoto and Matsumoto (2001b) have recentlymade an interesting observation that an unsu-pervised method based on clustering sometimesbetter approximates human created extracts than asupervised approach.
That appears somewhat con-tradictory given that a supervised approach shouldbe able to exploit human supplied information aboutwhich sentence to include in an extract and whichnot to, whereas an unsupervised approach blindlychooses sentences according to some selectionscheme.
An interesting question is, why this shouldbe the case.The reason may have to do with the variation inhuman judgments on sentence selection for a sum-mary.
In a study to be described later, we asked stu-dents to select 10% of a text which they find mostimportant for making a summary.
If they agree per-fectly on their judgments, then we will have only10% of a text selected as most important.
However,what we found was that about half of a text weremarked as important, indicating that judgments canvary widely among humans.Curiously, however, Nomoto and Matsumoto(2001a) also found that a supervised system faresmuch better when tested on data exhibiting highagreement among humans than an unsupervised sys-tem.
Their finding suggests that there are indeedsome regularities (or biases) to be found.So we might conclude that there are two aspects tohuman judgments in summarization; they can varybut may exhibit some biases which could be usefullyexploited.
The issue is then how we might modelthem in some coherent framework.The goal of the paper is to explore a possible in-tegration of supervised and unsupervised paradigmsas a way of responding to the issue.
Taking a de-cision tree and clustering as representing the respec-tive paradigm, we will show how coupling them pro-vides a summarizer that better approximates humanjudgments than either of the two considered alone.To our knowledge, none of the prior work on sum-marization (e.g., Kupiec et al (1995)) explicitly ad-dressed the issue of the variability inherent in humanjudgments in summarization tasks.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
465-472.Proceedings of the 40th Annual Meeting of the Association forX10||zzzzzz 1?
?BBBBBBY1(?1y, ?1n) X20????????
1?
?BBBBBBY2(?2y, ?2n)Y3(?3y, ?3n)Figure 1: Probabilistic Decision Tree2 Supervised Ranking with ProbabilisticDecision TreeOne technical problem associated with the use of adecision tree as a summarizer is that it is not able torank sentences, which it must be able do, to allow forthe generation of a variable-length summary.
In re-sponse to the problem, we explore the use of a prob-abilistic decision tree as a ranking model.
First, letus review some general features of probabilistic de-cision tree (ProbDT, henceforth) (Yamanishi, 1997;Rissanen, 1997).ProbDT works like a usual decision tree exceptthat rather than assigning each instance to a singleclass, it distributes each instance among classes.
Foreach instance xi, the strength of its membership toeach of the classes is determined by P (ck | xi) foreach class ck.Consider a binary decision tree in Fig 1.
Let X1and X2 represent non-terminal nodes, and Y1 andY2 leaf nodes.
?1?
and ?0?
on arcs denote valuesof some attribute at X1 and X2.
?iy and ?in repre-sent the probability that a given instance assignedto the node i is labeled as yes and no, repectively.Abusing the terms slightly, let us assume that X1 andX2 represent splitting attributes as well at respectivenodes.
Then the probability that a given instancewith X1 = 1 and X2 = 0 is labeled as yes (no) is?2y (?2n).
Note that?c ?jc = 1 for a given node j.Now to rank sentences with ProbDT simply in-volves finding the probability that each sentence isassigned to a particular class designating sentencesworthy of inclusion in a summary (call it ?Select?class) and ranking them accordingly.
(Hereafter andthroughout the rest of the paper, we say that a sen-tence is wis if it is worthy of inclusion in a summary:thus a wis sentence is a sentence worthy of inclusionin a summary.)
The probabiliy that a sentence u islabeled as wis is expressed as in Table 1, where ~uis a vector representation of u, consisting of a set ofvalues for features of u; ?
is a smoothing function,e.g., Laplace?s law; t(~u) is some leaf node assignedto ~u; and DT represents some decision tree used toclassify ~u.3 Diversity Based SummarizationAs an unsupervised summarizer, we use diversitybased summarization (DBS) (Nomoto and Mat-sumoto, 2001c).
It takes a cluster-and-rank approachto generating summaries.
The idea is to form a sum-mary by collecting sentences representative of di-verse topics discussed in the text.
A nice featureabout their approach is that by creating a summarycovering potential topics, which could be marginalto the main thread of the text, they are in fact able toaccommodate the variability in sentence selection:some people may pick up subjects (sentences) asimportant which others consider irrelevant or onlymarginal for summarization.
DBS accomodates thissituation by picking them all, however marginal theymight be.More specifically, DBS is a tripartite process con-sisting of the following:1.
Find-Diversity: find clusters of lexically sim-ilar sentences in text.
(In particular, we repre-sent a sentence here a vector of tfidf weights ofindex terms it contains.)2.
Reduce-Redundancy: for each cluster found,choose a sentence that best represents that clus-ter.3.
Generate-Summary: collect the representa-tive sentences, put them in some order, and re-turn them to the user.Find-Diversity is based on the K-means clusteringalgorithm, which they extended with Minimum De-scription Length Principle (MDL) (Li, 1998; Ya-manishi, 1997; Rissanen, 1997) as a way of optimiz-ing K-means.
Reduce-Redundancy is a tfidf basedranking model, which assigns weights to sentencesin the cluster and returns a sentence that ranks high-est.
The weight of a sentence is given as the sum oftfidf scores of terms in the sentence.Table 1: Probabilistic Classification with DT.
~u is a vector representation of sentence u. ?
is a smoothingfunction.
t(~u) is some leaf node assigned to ~u by DT.P (Select | ~u,DT) = ?
(the number of ?Select?
sentences at t(~u)the total number of sentences at t(~u))4 Combining ProbDT and DBSCombining ProbDT and DBS is done quite straight-forwardly by replacing Reduce-Redundacy withProbDT.
Thus instead of picking up a sentence withthe highest tfdif based weight, DBS/ProbDT at-tempts to find a sentences with the highest score forP (Select | ~u,DT).4.1 FeaturesThe following lists a set of features used for encod-ing a sentence in ProbDT.
Most of them are eitherlength- or location-related features.1<LocSen> The location of a sentence X definedby:#S(X)?
1#S(Last Sentence)?#S(X)?
denotes an ordinal number indicating theposition of X in a text, i.e.
#S(kth sentence) = k.?Last Sentence?
refers to the last sentence in a text.LocSen takes values between 0 and N?1N .
N is thenumber of sentences in the text.<LocPar> The location of a paragraph in whicha sentence X occurs given by:#Par(X)?
1#Last Paragraph?#Par(X)?
denotes an ordinal number indicat-ing the position of a paragraph containing X .
?#Last Paragraph?
is the position of the last para-graph in a text, represented by the ordinal number.<LocWithinPar> The location of a sentenceX within a paragraph in which it appears.#S(X)?#S(Par Init Sen)Length(Par(X))1Note that one may want to add tfidf to a set of features fora decision tree or, for that matter, to use features other than tfidffor representing sentences in clustering.
The idea is worthy ofconsideration, but not pursued here.Table 2: Linguistic cuescode category1 non-past2 past /-ta/3 copula /-da/4 noun5 symbols, e.g., parentheses6 sentence-ending particles, e.g., /-ka/0 none of the above?Par Init Sen?
refers to the initial sentence of a para-graph in which X occurs, ?Length(Par(X))?
denotesthe number of sentences that occur in that paragraph.LocWithinPar takes continuous values rangingfrom 0 to l?1l , where l is the length of a paragraph:a paragraph initial sentence would have 0 and a para-graph final sentence l?1l .<LenText> The text length in Japanese charac-ter i.e.
kana, kanji.<LenSen> The sentence length in kana/kanji.Some work in Japanese linguistics found that aparticular grammatical class a sentence final ele-ment belongs to could serve as a cue to identifyingsummary sentences.
These include categories likePAST/NON-PAST, INTERROGATIVE, and NOUN andQUESTION-MARKER.
Along with Ichikawa (1990),we identified a set of sentence-ending cues andmarked a sentence as to whether it contains a cuefrom the set.2 Included in the set are inflectionalclasses PAST/NON-PAST (for the verb and verbaladjective), COPULA, and NOUN, parentheses, andQUESTION-MARKER -ka.
We use the following at-tribute to encode a sentence-ending form.<EndCue> The feature encodes one of sentence-2Word tokens are extracted by using CHASEN, a Japanesemorphological analyzer which is reported to achieve the accu-racy rate of over 98% (Matsumoto et al, 1999).ending forms described above.
It is a discrete valuedfeature.
The value ranges from 0 to 6.
(See Table 2for details.
)Finally, one of two class labels, ?Select?
and?Don?t Select?, is assigned to a sentence, depend-ing on whether it is wis or not.
The ?Select?
labelis for wis sentences, and the ?Don?t Select?
label fornon-wis sentences.5 Decision Tree AlgorithmsTo examine the generality of our approach, we con-sider, in addition to C4.5 (Quinlan, 1993), the fol-lowing decision tree algorithms.
C4.5 is used withdefault options, e.g., CF=25%.5.1 MDL-DTMDL-DT stands for a decision tree with MDL basedpruning.
It strives to optimize the decision treeby pruning the tree in such a way as to producethe shortest (minimum) description length for thetree.
The description length refers to the num-ber of bits required for encoding information aboutthe decision tree.
MDL ranks, along with AkaikeInformation Criterion (AIC) and Bayes Informa-tion Criterion (BIC), as a standard criterion in ma-chine learning and statistics for choosing amongpossible (statistical) models.
As shown empiricallyin Nomoto and Matsumoto (2000) for discourse do-main, pruning DT with MDL significantly reducesthe size of tree, while not compromising perfor-mance.5.2 SSDTSSDT or Subspace Splitting Decision Tree repre-sents another form of decision tree algorithm.
(Wangand Yu, 2001) The goal of SSDT is to discover pat-terns in highly biased data, where a target class, i.e.,the class one likes to discover something about, ac-counts for a tiny fraction of the whole data.
Note thatthe issue of biased data distribution is particularlyrelevant for summarization, as a set of sentences tobe identified as wis usually account for a very smallportion of the data.SSDT begins by searching the entire data spacefor a cluster of positive cases and grows the clusterby adding points that fall within some distance tothe center of the cluster.
If the splitting based on thecluster offers a better Gini index than simply usingFigure 2: SSDT in action.
Filled circles representpositive class, white circles represent negative class.SSDT starts with a small spherical cluster of pos-itive points (solid circle) and grows the cluster by?absorbing?
positive points around it (dashed circle).one of the attributes to split the data, SSDT splits thedata space based on the cluster, that is, forms one re-gion outside of the cluster and one inside.3 It repeatsthe process recursively on each subregions spawneduntil termination conditions are met.
Figure 2 givesa snapshot of SSDT at work.
SSDT locates someclusters of positive points, develops spherical clus-ters around them.With its particular focus on positive cases, SSDTis able to provide a more precise characterization ofthem, compared, for instance, to C4.5.6 Test Data and ProcedureWe asked 112 Japanese subjects (students at grad-uate and undergraduate level) to extract 10% sen-tences in a text which they consider most importantin making a summary.
The number of sentences toextract varied from two to four, depending on thelength of a text.
The age of subjects varied from 18to 45.
We used 75 texts from three different cate-gories (25 for each category); column, editorial andnews report.
Texts were of about the same size interms of character counts and the number of para-graphs, and were selected randomly from articlesthat appeared in a Japanese financial daily (Nihon-Keizai-Shimbun-Sha, 1995).
There were, on aver-age, 19.98 sentences per text.3For a set S of data with k classes, its Gini index is givenas: Gini(S) = 1?Pki p2i , where pi denotes the probability ofobserving class i in S.Table 3: Test Data.
N denotes the total number ofsentences in the test data.
K ?
n means that a wis(positive) sentence gets at least n votes.K N positive negative?
1 1424 707 717?
2 1424 392 1032?
3 1424 236 1188?
4 1424 150 1274?
5 1424 72 1352The kappa agreement among subjects was0.25.
The result is in a way consistent withSalton et al (1999), who report a low inter-subjectagreement on paragraph extracts from encyclope-dias and also with Gong and Liu (2001) on a sen-tence selection task in the cable news domain.
Whilethere are some work (Marcu, 1999; Jing et al, 1998)which do report high agreement rates, their successmay be attributed to particularities of texts used, assuggested by Jing et al (1998).
Thus, the questionof whether it is possible to establish an ideal sum-mary based on agreement is far from settled, if ever.In the face of this, it would be interesting and per-haps more fruitful to explore another view on sum-mary, that the variability of a summary is the normrather than the exception.In the experiments that follow, we decided notto rely on a particular level of inter-coder agree-ment to determine whether or not a given sentenceis wis.
Instead, we used agreement threshold to dis-tinguish between wis and non-wis sentences: for agiven threshold K, a sentence is considered wis (orpositive) if it has at least K votes in favor of its in-clusion in a summary, and non-wis (negative) if not.Thus if a sentence is labeled as positive at K ?
1,it means that there are one or more judges takingthat sentence as wis. We examined K from 1 to 5.
(On average, seven people are assigned to one arti-cle.
However, one would rarely see all of them unan-imously agree on their judgments.
)Table 3 shows how many positive/negative in-stances one would get at a given agreement thresh-old.
At K ?
1, out of 1424 instances, i.e., sen-tences, 707 of them are marked positive and 717 aremarked negative, so positive and negative instancesare evenly spread across the data.
On the other hand,at K ?
5, there are only 72 positive instances.
Thismeans that there is less than one occurrence of wiscase per article.In the experiments below, each probabilistic ren-dering of the DTs, namely, C4.5, MDL-DT, andSSDT is trained on the corpus, and tested withand without the diversity extension (Find-Diversity).When used without the diversity component, eachProbDT works on a test article in its entirety, pro-ducing the ranked list of sentences.
A summarywith compression rate ?
is obtained by selectingtop ?
percent of the list.
When coupled with Find-Diversity, on the other hand, each ProbDT is setto work on each cluster discovered by the diversitycomponent, producing multiple lists of sentences,each corresponding to one of the clusters identified.A summary is formed by collecting top ranking sen-tences from each list.Evaluation was done by 10-fold cross vali-dation.
For the purpose of comparison, wealso ran the diversity based model as given inNomoto and Matsumoto (2001c) and a tfidf basedranking model (Zechner, 1996) (call it Z model),which simply ranks sentences according to the tfidfscore and selects those which rank highest.
Recallthat the diversity based model (DBS) (Nomoto andMatsumoto, 2001c) consists in Find-Diversity andthe ranking model by Zechner (1996), which theycall Reduce-Redundancy.7 Results and DiscussionTables 4-8 show performance of each ProbDT andits combination with the diversity (clustering) com-ponent.
It also shows performance of Z model andDBS.
In the tables, the slashed ?V?
after the nameof a classifier indicates that the relevant classifier isdiversity-enabled, meaning that it is coupled withthe diversity extension.
Notice that each decisiontree here is a ProbDT and should not be confusedwith its non-probabilistic counterpart.
Also worthnoting is that DBS is in fact Z/V, that is, diversity-enabled Z model.Returning to the tables, we find that for mostof the times, the diversity component has clear ef-fects on ProbDTs, significantly improving their per-formance.
All the figures are in F-measure, i.e.,F = 2?P?RP+R .
In fact this happens regardless of a par-ticular choice of ranking model, as performance ofZ is also boosted with the diversity component.
Notsurprisingly, effects of supervised learning are alsoevident: diversity-enabled ProbDTs generally out-perform DBS (Z/V) by a large margin.
What is sur-prising, moreover, is that diversity-enabled ProbDTsare superior in performance to their non-diversitycounterparts (with a notable exception for SSDT atK ?
1), which suggests that selecting marginal sen-tences is an important part of generating a summary.Another observation about the results is that asone goes along with a larger K, differences in per-formance among the systems become ever smaller:at K ?
5, Z performs comparably to C4.5, MDL,and SSDT either with or without the diversity com-ponent.
The decline of performance of the DTs maybe caused by either the absence of recurring patternsin data with a higher K or simply the paucity ofpositive instances.
At the moment, we do not knowwhich is the case here.It is curious to note, moreover, that MDL-DT isnot performing as well as C4.5 and SSDT at K ?
1,K ?
2, and K ?
3.
The reason may well haveto do with the general properties of MDL-DT.
Re-call that MDL-DT is designed to produce as smalla decision tree as possible.
Therefore, the resultingtree would have a very small number of nodes cov-ering the entire data space.
Consider, for instance,a hypothetical data space in Figure 3.
Assume thatMDL-DT bisects the space into region A and B, pro-ducing a two-node decision tree.
The problem withthe tree is, of course, that point x and y in region Bwill be assigned to the same probability under theprobabilistic tree model, despite the fact that point xis very close to region A and point y is far out.
Thisproblem could happen with C4.5, but in MDL-DT,which covers a large space with a few nodes, pointsin a region could be far apart, making the problemmore acute.
Thus the poor performance of MDL-DTmay be attributable to its extensive use of pruning.8 ConclusionAs a way of exploiting human biases towards an in-creased performance of the summarizer, we have ex-plored approaches to embedding supervised learn-ing within a general unsupervised framework.
In theAyBxFigure 3: Hypothetical Data Spacepaper, we focused on the use of decision tree as aplug-in learner.
We have shown empirically that theidea works for a number of decision trees, includingC4.5, MDL-DT and SSDT.
Coupled with the learn-ing component, the unsupervised summarizer basedon clustering significantly improved its performanceon the corpus of human created summaries.
Moreimportantly, we found that supervised learners per-form better when coupled with the clustering thanwhen working alone.
We argued that that has to dowith the high variation in human created summaries:the clustering component forces a decision tree topay more attention to sentences marginally relevantto the main thread of the text.While ProbDTs appear to work well with rank-ing, it is also possible to take a different approach:for instance, we may use some distance metric in in-stead of probability to distinguish among sentences.It would be interesting to invoke the notion like pro-totype modeler (Kalton et al, 2001) and see how itmight fare when used as a ranking model.Moreover, it may be worthwhile to exploresome non-clustering approaches to representingthe diversity of contents of a text, such asGong and Liu (2001)?s summarizer 1 (GLS1, forshort), where a sentence is selected on the basis ofits similarity to the text it belongs to, but which ex-cludes terms that appear in previously selected sen-tences.
While our preliminary study indicates thatGLS1 produces performance comparable and evensuperior to DBS on some tasks in the document re-trieval domain, we have no results available at themoment on the efficacy of combining GLS1 andProbDT on sentence extraction tasks.Finally, we note that the test corpus used forTable 4: Performance at varying compression rates for K ?
1.
MDL-DT denotes a summarizer basedon C4.5 with the MDL extension.
DBS (=Z/V) denotes the diversity based summarizer.
Z represents theZ-model summarizer.
Performance figures are in F-measure.
?V?
indicates that the relevant classifier isdiversity-enabled.
Note that DBS =Z/V.cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z0.2 0.371 0.459 0.353 0.418 0.437 0.454 0.429 0.2310.3 0.478 0.507 0.453 0.491 0.527 0.517 0.491 0.3400.4 0.549 0.554 0.535 0.545 0.605 0.553 0.529 0.4350.5 0.614 0.600 0.585 0.593 0.639 0.606 0.582 0.510Table 5: K ?
2cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z0.2 0.381 0.441 0.343 0.391 0.395 0.412 0.386 0.2160.3 0.420 0.441 0.366 0.418 0.404 0.431 0.421 0.2900.4 0.434 0.444 0.398 0.430 0.415 0.444 0.444 0.3440.5 0.427 0.447 0.409 0.437 0.423 0.439 0.443 0.381Table 6: K ?
3cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z0.2 0.320 0.354 0.297 0.345 0.328 0.330 0.314 0.3140.3 0.300 0.371 0.278 0.350 0.321 0.338 0.342 0.3490.4 0.297 0.357 0.298 0.348 0.325 0.340 0.339 0.3370.5 0.297 0.337 0.301 0.329 0.307 0.327 0.322 0.322Table 7: K ?
4cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z0.2 0.272 0.283 0.285 0.301 0.254 0.261 0.245 0.2450.3 0.229 0.280 0.234 0.284 0.249 0.267 0.269 0.2690.4 0.238 0.270 0.243 0.267 0.236 0.248 0.247 0.2470.5 0.235 0.240 0.245 0.246 0.227 0.233 0.232 0.232Table 8: K ?
5cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z0.2 0.242 0.226 0.252 0.240 0.188 0.189 0.191 0.1910.3 0.194 0.220 0.197 0.231 0.171 0.206 0.194 0.1940.4 0.184 0.189 0.189 0.208 0.175 0.173 0.173 0.1730.5 0.174 0.175 0.176 0.191 0.145 0.178 0.167 0.167evaluation is somewhat artificial in the sense thatwe elicit judgments from people on the summary-worthiness of a particular sentence in the text.
Per-haps, we should look at naturally occurring ab-stracts or extracts as a potential source for train-ing/evaluation data for summarization research.
Be-sides being natural, they usually come in large num-ber, which may alleviate some concern about thelack of sufficient resources for training learning al-gorithms in summarization.ReferencesYihong Gong and Xin Liu.
2001.
Generic text summa-rization using relevance measure and latent semanticanalysis.
In Proceedings of the 24th Annual Interna-tional ACM/SIGIR Conference on Research and De-velopment, New Orleans.
ACM-Press.Takashi Ichikawa.
1990.
Bunsho?ron-gaisetsu.
Kyo?iku-Shuppan, Tokyo.Hongyan Jing, Regina Barzilay, Kathleen McKeown, andMachael Elhadad.
1998.
Summarization evaluationmethods: Experiments and analysis.
In AAAI Sym-posium on Intelligent Summarization, Stanford Uni-vesisty, CA, March.Annaka Kalton, Pat Langely, Kiri Wagstaff, and Jung-soon Yoo.
2001.
Generalized clustering, supervisedlearning, and data assignment.
In Proceedings of theSeventh International Conference on Knowledge Dis-covery and Data Mining (KDD2001), San Francisco,August.
ACM.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedings ofthe Fourteenth Annual International ACM/SIGIR Con-ference on Research and Developmnet in InformationRetrieval, pages 68?73, Seattle.Hang Li.
1998.
A Probabilistic Approach to Lexical Se-mantic Knowledge Acquistion and Structural Disam-biguation.
Ph.D. thesis, University of Tokyo, Tokyo.Daniel Marcu.
1999.
Discourse trees are good indicatorsof importance in text.
In Indejeet Mani and Mark T.Maybury, editors, Advances in Automatic Text Summa-rization, pages 123?136.
The MIT Press.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, andYoshitaka Hirano.
1999.
Japanese morphologicalanalysis system chasen version 2.0 manual.
Technicalreport, NAIST, Ikoma, April.
NAIST-IS-TR99008.Nihon-Keizai-Shimbun-Sha.
1995.
Nihon keizai shim-bun 95 nen cd-rom ban.
CD-ROM.
Tokyo, NihonKeizai Shimbun, Inc.Tadashi Nomoto and Yuji Matsumoto.
2000.
Comparingthe minimum description length principle and boostingin the automatic analysis of discourse.
In Proceedingsof the Seventeenth International Conference on Ma-chine Learning, pages 687?694, Stanford University,June-July.
Morgan Kaufmann.Tadashi Nomoto and Yuji Matsumoto.
2001a.
The diver-sity based approach to open-domain text summariza-tion.
Unpublished Manuscript.Tadashi Nomoto and Yuji Matsumoto.
2001b.
An exper-imental comparison of supervised and unsupervisedapproaches to text summarization.
In Proceedings of2001 IEEE International Conference on Data Mining,pages 630?632, San Jose.
IEEE Computer Society.Tadashi Nomoto and Yuji Matsumoto.
2001c.
A newapproach to unsupervised text summarization.
In Pro-ceedings of the 24th International ACM/SIGIR Confer-ence on Research and Development in InformationalRetrieval, New Orleans, September.
ACM.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Jorma Rissanen.
1997.
Stochastic complexity in learn-ing.
Journal of Computer and System Sciences, 55:89?95.Gerald Salton, Amit Singhal, Mandara Mitra, and ChrisBuckley.
1999.
Automatic text structuring and sum-marization.
In Inderjeet Mani and Mark T. Maybury,editors, Advances in Automatic Text Summarization,pages 342?355.
The MIT Press.
Reprint.Haixun Wang and Philip Yu.
2001.
SSDT: A scalablesubspace-splitting classifier for biased data.
In Pro-ceedings of 2001 IEEE International Conference onData Mining, pages 542?549, San Jose, December.IEEE Computer Society.Kenji Yamanishi.
1997.
Data compression and learning.Journal of Japanese Society for Artificial Intelligence,12(2):204?215.
in Japanese.Klaus Zechner.
1996.
Fast generation of abstracts fromgeneral domain text corpora by extracting relevant sen-tences.
In Proceedings of the 16th International Con-ference on Computational Linguistics, pages 986?989,Copenhagen.
