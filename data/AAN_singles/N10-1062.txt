Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394?402,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsStream-based Translation Models for Statistical Machine TranslationAbby LevenbergSchool of InformaticsUniversity of Edinburgha.levenberg@ed.ac.ukChris Callison-BurchComputer Science DepartmentJohns Hopkins Universityccb@cs.jhu.eduMiles OsborneSchool of InformaticsUniversity of Edinburghmiles@inf.ed.ac.ukAbstractTypical statistical machine translation sys-tems are trained with static parallel corpora.Here we account for scenarios with a continu-ous incoming stream of parallel training data.Such scenarios include daily governmentalproceedings, sustained output from transla-tion agencies, or crowd-sourced translations.We show incorporating recent sentence pairsfrom the stream improves performance com-pared with a static baseline.
Since frequentbatch retraining is computationally demand-ing we introduce a fast incremental alternativeusing an online version of the EM algorithm.To bound our memory requirements we usea novel data-structure and associated trainingregime.
When compared to frequent batch re-training, our online time and space-boundedmodel achieves the same performance withsignificantly less computational overhead.1 IntroductionThere is more parallel training data available to-day than there has ever been and it keeps increas-ing.
For example, the European Parliament1 releasesnew parallel data in 22 languages on a regular basis.Project Syndicate2 translates editorials into sevenlanguages (including Arabic, Chinese and Russian)every day.
Existing translation systems often get?crowd-sourced?
improvements such as the optionto contribute a better translation to GoogleTrans-late3.
In these and many other instances, the data canbe viewed as an incoming unbounded stream since1http://www.europarl.europa.eu2http://www.project-syndicate.org3http://www.translate.google.comthe corpus grows continually with time.
Dealingwith such unbounded streams of parallel sentencespresents two challenges: making retraining efficientand operating within a bounded amount of space.Statistical Machine Translation (SMT) systemsare typically batch trained, often taking many CPU-days of computation when using large volumes oftraining material.
Incorporating new data into thesemodels forces us to retrain from scratch.
Clearly,this makes rapidly adding newly translated sen-tences into our models a daunting engineering chal-lenge.
We introduce an adaptive training regime us-ing an online variant of EM that is capable of in-crementally adding new parallel sentences withoutincurring the burdens of full retraining.For situations with large volumes of incomingparallel sentences we are also forced to considerplacing space-bounds on our SMT system.
We in-troduce a dynamic suffix array which allows us toadd and delete parallel sentences, thereby maintain-ing bounded space despite processing a potentiallyhigh-rate input stream of unbounded length.Taken as a whole we show that online translationmodels operating within bounded space can performas well as systems which are batch-based and haveno space constraints thereby making our approachsuitable for stream-based translation.2 Stepwise Online EMThe EM algorithm is a common way of inducinglatent structure from unlabeled data in an unsuper-vised manner (Dempster et al, 1977).
Given a setof unlabeled examples and an initial, often uniformguess at a probability distribution over the latentvariables, the EM algorithm maximizes the marginal394log-likelihood of the examples by repeatedly com-puting the expectation of the conditional probabilityof the latent data with respect to the current distri-bution, and then maximizing the expectations overthe observations into a new distribution used in thenext iteration.
EM (and related variants such as vari-ational or sampling approaches) form the basis ofhow SMT systems learn their translation models.2.1 Batch vs. Online EMComputing an expectation for the conditional prob-abilities requires collecting the sufficient statistics Sover the set of n unlabeled examples.
In the caseof a multinomial distribution, S is comprised of thecounts over each conditional observation occurringin the n examples.
In traditional batch EM, we col-lect the counts over the entire dataset of n unlabeledtraining examples via the current ?best-guess?
proba-bility model ?
?t at iteration t (E-step) before normal-izing the counts into probabilities ??
(S) (M-step)4.After each iteration all the counts in the sufficientstatistics vector S are cleared and the count collec-tion begins anew using the new distribution ?
?t+1.When we move to processing an incoming datastream, however, the batch EM algorithm?s require-ment that all data be available for each iteration be-comes impractical since we do not have access to alln examples at once.
Instead we receive examplesfrom the input stream incrementally.
For this reasononline EM algorithms have been developed to up-date the probability model ??
incrementally withoutneeding to store and iterate through all the unlabeledtraining data repeatedly.Various online EM algorithms have been investi-gated (see Liang and Klein (2009) for an overview)but our focus is on the stepwise online EM (sOEM)algorithm (Cappe and Moulines, 2009).
Insteadof iterating over the full set of training examples,sOEM stochastically approximates the batch E-stepand incorporates the information from the newlyavailable streaming observations in steps.
Each stepis called a mini-batch and is comprised of one ormore new examples encountered in the stream.Unlike in batch EM, in sOEM the expected countsare retained between EM iterations and not cleared.4As the M-step can be computed in closed form we desig-nate it in this work as ??
(S).Algorithm 1: Batch EM for Word AlignmentsInput: {F (source),E (target)} sentence-pairsOutput: MLE ?
?T over alignments a?
?0 ?MLE initialization;for iteration k = 0, .
.
.
, T doS ?
0; // reset countsforeach (f, e) ?
{F,E} do // E-stepS ?
S +?a?
?aPr(f, a?|e; ??t);end?
?t+1 ?
?
?t(S) ; // M-stependThat is, for each new example we interpolate its ex-pected count with the existing set of sufficient statis-tics.
For each step we use a stepsize parameter ?which mixes the information from the current ex-ample with information gathered from all previousexamples.
Over time the sOEM model probabilitiesbegin to stabilize and are guaranteed to converge toa local maximum (Cappe and Moulines, 2009).Note that the stepsize ?
has a dependence on thecurrent mini-batch.
As we observe more incomingdata the model?s current probability distribution iscloser to the true distribution so the new observa-tions receive less weight.
From Liang and Klein(2009), if we set the stepsize as ?t = (t + 2)?
?,with 0.5 < ?
?
1, we can guarantee convergence inthe limit as n ?
?.
If we set ?
low, ?
weighs thenewly observed statistics heavily whereas if ?
is lownew observations are down-weighted.2.2 Batch EM for Word AlignmentsBatch EM is used in statistical machine translationto estimate word alignment probabilities betweenparallel sentences.
From these alignments, bilingualrules or phrase pairs can be extracted.
Given a setof parallel sentence examples, {F,E}, with F theset of source sentences and E the corresponding tar-get sentences, we want to find the latent alignmentsa for a sentence pair (f , e) ?
{F,E} that definesthe most probable correspondence between words fjand ei such that aj = i.
We can induce these align-ments using an HMM-based alignment model wherethe probability of alignment aj is dependent only onthe previous alignment at aj?1 (Vogel et al, 1996).395We can writePr(f ,a | e) =?a?
?a|f |?j=1p(aj | aj?1, |e|) ?
p(fj | eaj )where we assume a first-order dependence on previ-ously aligned positions.To find the most likely parameter weights forthe translation and alignment probabilities for theHMM-based alignments, we employ the EM algo-rithm via dynamic programming.
Since HMMs havemultiple local minima, we seed the HMM-basedmodel probabilities with a better than random guessusing IBM Model 1 (Brown et al, 1993) as is stan-dard.
IBM Model 1 is of the same form as theHMM-based model except it uses a uniform distri-bution instead of a first-order dependency.
Althougha series of more complex models are defined, IBMModels 2 to Model 6 (Brown et al, 1993; Och andNey, 2003), researchers typically find that extract-ing phrase pairs or translation grammar rules usingModel 1 and the HMM-based alignments results inequivalently high translation quality.
Nevertheless,there is nothing in our approach which limits us tousing just Model 1 and the HMM model.A high-level overview of the standard, batch EMalgorithm applied to HMM-based word alignmentmodel is shown in Algorithm 1.2.3 Stepwise EM for Word AlignmentsApplication of sOEM to HMM and Model 1 basedword aligning is straightforward.
The process ofcollecting the counts over the expected conditionalprobabilities inside each iteration loop remains thesame as in the batch case.
However, instead of clear-ing the sufficient statistics between the iterations weretain them and interpolate them with the batch ofcounts gathered in the next iteration.Algorithm 2 shows high level pseudocode of oursOEM framework as applied to HMM-based wordalignments.
Here we have an unbounded inputstream of source and target sentences {F,E} whichwe do not have access to in its entirety at once.Instead we observe mini-batches {M} comprisedof chronologically ordered strict subsets of the fullstream.
To word align the sentences for each mini-batch m ?
M, we use the probability assigned bythe current model parameters and then interpolateAlgorithm 2: sOEM Algorithm for Word Align-mentsInput: mini-batches of sentence pairs{M : M ?
{F (source), E(target)}}Input: stepsize weight ?Output: MLE ?
?T over alignments a?
?0 ?MLE initialization;S ?
0; k = 0;foreach mini-batch {m : m ?M} dofor iteration t = 0, .
.
.
, T doforeach (f, e) ?
{m} do // E-steps???a?
?aPr(f, a?|e; ??t);end?
= (k + 2)??
; k = k + 1; // stepsizeS ?
?s?
+ (1?
?
)S; // interpolate?
?t+1 ?
?
?t(S) ; // M-stependendthe newest sufficient statistics s?
with our full countvector S using an interpolation parameter ?.
The in-terpolation parameter ?
has a dependency on howfar along the input stream we are processing.3 Dynamic Suffix ArraysSo far we have shown how to incrementally retraintranslation models.
We now consider how we mightbound the space we use for them when processing(potentially) unbounded streams of parallel data.Suffix arrays are space-efficient data structures forfast searching over large text strings (Manber andMyers, 1990).
Treating the entire corpus as a sin-gle string, a suffix array holds in lexicographical or-der (only) the starting index of each suffix of thestring.
After construction, since the corpus is nowordered, we can query the suffix array quickly us-ing binary search to efficiently find all occurrencesof a particular token or sequence of tokens.
Then wecan easily compute, on-the-fly, the statistics requiredsuch as translation probabilities for a given sourcephrase.
Suffix arrays can also be compressed, whichmake them highly attractive structures for represent-ing massive translation models (Callison-Burch etal., 2005; Lopez, 2008).We need to delete items if we wish to maintain396epoch 2epoch 1 epoch 2 model coveragemodel coverageinput streamTest Pointsinput streamTest PointsStaticUnboundedinput streamTest PointsBoundedmodel coveragesliding windowsFigure 1: Streaming coverage conditions.
In traditionalbatch based modeling the coverage of a trained modelnever changes.
Unbounded coverage operates withoutany memory constraints so the model is able to contin-ually add data from the input stream.
Bounded coverageuses just a fixed window.constant space when processing unbounded streams.Standard suffix arrays are static, store a fixed corpusand do not support deletions.
Nevertheless, a dy-namic variant of the suffix array does support dele-tions as well as insertions and therefore can be usedin our stream-based approach (Salson et al, 2009).Using a dynamic suffix array, we can compactlyrepresent the set of parallel sentences from whichwe eventually extract grammar rules.
Furthermore,when incorporating new parallel sentences, we sim-ply insert them into the array and, to maintain con-stant space usage, we delete an equivalent number.4 ExperimentsIn this section we describe the experiments con-ducted comparing various batch trained translationmodels (TMs) versus online incrementally retrainedTMs in a full SMT setting with different conditionsset on model coverage.
We used publicly availableresources for all our tests.
We start by showing thatrecency motivates incremental retraining.4.1 Effects of Recency on SMTFor language modeling, it is known that perfor-mance can be improved using the criterion of re-cency where training data is drawn from timeschronologically closer to the test data (Rosenfeld,00.511.522.55  10  15  20  25  30  35DeltainBLEUscoresepochsFigure 2: Recency effects to SMT performance.
De-picted are the differences in BLEU scores for multipletest points decoded by a static baseline system and a sys-tem batched retrained on a fixed sized window prior tothe test point in question.
The results are accentuated atthe end of the timeline when more time has passed con-firming that recent data impacts translation performance.1995).
Given an incoming stream of parallel text,we gauged the extent to which incorporating recentdata into a TM affects translation quality.We used the Europarl corpus5 with the Fr-En lan-guage pair using French as source and English as tar-get.
Europarl is released in the format of a daily par-liamentary session per time-stamped file.
The actualdates of the full corpus are interspersed unevenly(they do not convene daily) over a continuous time-line corresponding to the parliament sessions fromApril,1996 through October, 2006, but for concep-tual simplicity we treated the corpus as a continualinput stream over consecutive days.As a baseline we aligned the first 500k sentencepairs from the beginning of the corpus timeline.
Weextracted a grammar for and translated 36 held outtest documents that were evenly spaced along the re-mainder of the Europarl timeline.
These test docu-ments effectively divided the remaining training datainto epochs and we used a sliding window over thetimeline to build 36 distinct, overlapping trainingsets of 500k sentences each.We then translated all 36 test points again usinga new grammar for each document extracted fromonly the sentences contained in the epoch that wasbefore it.
To explicitly test the effect of recency5Available at http://www.statmt.org/europarl397on the TM all other factors of the SMT pipeline re-mained constant including the language model andthe feature weights.
Hence, the only change fromthe static baseline to the epochs performance was theTM data which was based on recency.
Note that atthis stage we did not use any incremental retraining.Results are shown in Figure 2 as the differencesin BLEU score (Papineni et al, 2001) between thebaseline TM versus the translation models trainedon material chronologically closer to the given testpoint.
The consistently positive deltas in BLEUscores between the model that is never retrained andthe models that are retrained show that we achieve ahigher translation performance when using more up-to-date TMs that incorporate recent sentence pairs.As the chronological distance between the initial,static model and the retrained models increases, wesee ever-increasing differences in translation perfor-mance.
This underlines the need to retrain transla-tion models with timely material.4.2 Unbounded and Bounded TranslationModel RetrainingHere we consider how to process a stream along twomain axes: by bounding time (batch versus incre-mental retraining) and by bounding space (either us-ing all the stream seen so far, or only using a fixedsized sample of it).To ensure the recency results reported above werenot limited to French-English, this time our paral-lel input stream was generated from the German-English language pair of Europarl with German assource and English again as target.
For testing weheld out a total of 22k sentences from 10 evenlyspaced intervals in the input stream which dividedthe input stream into 10 epochs.
Stream statistics forthree example epochs are shown in Table 1.
We heldout 4.5k sentence pairs as development data to opti-mize the feature function weights using minimumerror rate training (Och, 2003) and these weightswere used by all models.
We used Joshua (Li etal., 2009), a syntax-based decoder with a suffix arrayimplementation, and rule induction via the standardHiero grammar extraction heuristics (Chiang, 2007)for the TMs.
Note that nothing hinges on whetherwe used a syntax or a phrase-based system.We used a 5-gram, Kneser-Ney smoothed lan-guage model (LM) trained on the initial segment ofEp From?To Sent Pairs Source/Target00 04/1996?12/2000 600k 15.0M/16.0M03 02/2002?09/2002 70k 1.9M/2.0M06 10/2003?03/2004 60k 1.6M/1.7M10 03/2006?09/2006 73k 1.9M/2.0MTable 1: Date ranges, total sentence pairs, and source andtarget word counts encountered in the input stream forexample epochs.
Epoch 00 is baseline data that is alsoused as a seed corpus for the online models.the target side parallel data used in the first base-line as described further in the next subsection.
Asour initial experiments aim to isolate the effect ofchanges to the TM on overall translation system per-formance, our in-domain LM remains static for ev-ery decoding run reported below until indicated.We used the open-source toolkit GIZA++ (Ochand Ney, 2003) for all word alignments.
For theonline adaptation experiments we modified Model1 and the HMM model in GIZA++ to use the sOEMalgorithm.
Batch baselines were aligned using thestandard version of GIZA++.
We ran the batch andincremental versions of Model 1 and HMM for thesame number of iterations each in both directions.4.3 Time and Space BoundsFor both batch and sOEM we ran a number of ex-periments listed below corresponding to the differ-ent training scenarios diagrammed in Figure 1.1.
Static: We used the first half of the in-put stream, approximately 600k sentences and15/16 million source/target words, as paralleltraining data.
We then translated each of the 10test sets using the static model.
This is the tradi-tional approach and the coverage of the modelnever changes.2.
Unbounded Space: Batch or incremental re-training with no memory constraint.
For eachepoch in the stream, we retrained the TM us-ing all the data from the beginning of the in-put stream until just before the present with re-spect to a given test point.
As more time passesour training data set grows so each batch runof GIZA++ takes more time.
Overall this is themost computationally expensive approach.398Baseline Unbounded BoundedEpoch Test Date Test Sent.
Train Sent.
Rules Train Sent.
Rules Train Sent.
Rules03 09/23/2002 1.0k 580k 4.0M 800k 5.0M 580k 4.2M06 03/29/2004 1.5k 580k 5.0M 1.0M 7.0M 580k 5.5M10 09/26/2006 3.5k 580k 8.5M 1.3M 14.0M 580k 10.0MTable 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.Test and Train Sent.
is the number of sentence pairs in test and training data respectively.
Rules is the count of uniqueHiero grammar rules extracted for the corresponding test set.0.20.40.60.811.21.41.61  2  3  4  5  6  7  8  9  10DeltainBLEUscoresepochsunboundedboundedFigure 3: Static vs. online TM performance.
Gains intranslation performance measured by BLEU are achievedwhen recent German-English sentence pairs are auto-matically incorporated into the TM.
Shown are relativeBLEU improvements for the online models against thestatic baseline.3.
Bounded Space: Batch and incremental re-training with an enforced memory constraint.Here we batch or incrementally retrain usinga sliding window approach where the trainingset size (the number of sentence pairs) remainsconstant.
In particular, we ensured that weused the same number of sentences as the base-line.
Each batch run of GIZA++ takes approxi-mately the same time.The time for aligning in the sOEM model is unaf-fected by the bounded/unbounded conditions sincewe always only align the mini-batch of sentencesencountered in the last epoch.
In contrast, for batchEM we must realign all the sentences in our trainingset from scratch to incorporate the new training data.Similarly space usage for the batch training growswith the training set size.
For sOEM, in theory mem-ory used is with respect to vocabulary size (whichgrows slowly with the stream size) since we retaincount history for the entire stream.
To make spaceusage truly constant, we filter for just the neededword pairs in the current epoch being aligned.
Thiseffectively means that online EM is more mem-ory efficient than the batch version.
As our exper-iments will show, the sufficient statistics kept be-tween epochs by sOEM benefits performance com-pared to the batch models which can only use infor-mation present within the batch itself.4.4 Incremental Retraining ProcedureOur incremental adaptation procedure was as fol-lows: after the latest mini-batch of sentences hadbeen aligned using sOEM we added all newlyaligned sentence pairs to the dynamic suffix ar-rays.
For the experiments where our memory wasbounded, we also deleted an equal number of sen-tences from the suffix arrays before extracting theHiero grammar for the next test point.
For the un-bounded coverage experiments we deleted nothingprior to grammar extraction.
Table 2 presents statis-tics for the number of training sentence pairs andgrammar rules extracted for each coverage conditionfor various test points.4.5 ResultsFigure 3 shows the results of the static baselineagainst both the unbounded and bounded online EMmodels.
We can see that both the online modelsoutperform the static baseline.
On average the un-constrained model that contains more sentence pairsfor rule extraction slightly outperforms the boundedcondition which uses less data per epoch.
However,the static baseline and the bounded models both usethe same number of sentence-pairs for TM training.We see there is a clear gain by incorporating recentsentence-pairs made available by the stream.399Static Baseline Retrained (Unbounded) Retrained (Bounded)Test Date Batch Batch Online Batch Online09/23/2002 26.10 26.60 26.43 26.19 26.4003/29/2004 27.40 28.33 28.42 28.06 28.3809/26/2006 28.56 29.74 29.75 29.73 29.80Table 3: Sample BLEU results for all baseline and online EM model conditions.
The static baseline is a traditionalmodel that is never retrained.
The batch unbounded and batch bounded models incorporate new data from the streambut retraining is slow and computationally expensive (best results are bolded).
In contrast both unbounded and boundedonline models incrementally retrain only the mini-batch of new sentences collected from the incoming stream soquickly adopt the new data (best results are italicized).Table 3 gives results of the online models com-pared to the batch retrained models.
For presentationclarity we show only a sample of the full set of tentest points though all results follow the pattern thatusing more aligned sentences to derive our gram-mar set resulted in slightly better performance ver-sus a restricted training set.
However, for the samecoverage constraints not only do we achieve com-parable performance to batch retrained models us-ing the sOEM method of incremental adaptation, weare able to align and adopt new data from the inputstream orders of magnitude quicker since we onlyalign the mini-batch of sentences collected from thelast epoch.
In the bounded condition, not only dowe benefit from quicker adaptation, we also see thatsOEM models slightly outperform the batch basedmodels due to the online algorithm employing alonger history of count-based evidence to draw onwhen aligning new sentence pairs.Figure 4 shows two example test sentences thatbenefited from the online TM adaptation.
Trans-lations from the online model produce more andlonger matching phrases for both sentences (e.g.,?creation of such a?, ?of the occupying forces?
)leading to more fluent output as well as the improve-ments achieved in BLEU scores.We experimented with a variety of interpolationparameters (see Algorithm 2) but found no signifi-cant difference between them (the biggest improve-ment gained over all test points for all parameter set-tings was less than 0.1% BLEU).4.6 Increasing LM CoverageA natural and interesting extension to the experi-ments above is to use the target side of the incomingstream to extend the LM coverage alongside the TM.Test Date Static Unbounded Bounded09/23/2002 26.46 27.11 26.9603/29/2004 28.11 29.53 29.2009/26/2006 29.53 30.94 30.88Table 4: Unbounded LM coverage improvements.
Shownare the BLEU scores for each experimental conditionalwhen we allow the LM coverage to increase.It is well known that more LM coverage (via largertraining data sets) is beneficial to SMT performance(Brants et al, 2007) so we investigated whether re-cency gains for the TM were additive with recencygains afforded by a LM.To test this we added all the target side data fromthe beginning of the stream to the most recent epochinto the LM training set before each test point.
Wethen batch retrained6 and used the new LM withgreater coverage for the next decoding run.
Experi-ments were for the static baseline and online models.Results are reported in Table 4.
We can see thatincreasing LM coverage is complimentary to adapt-ing the TM with recent data.
Comparing Tables3 and 4, for the bounded condition, adapting onlythe TM achieved an absolute improvement of +1.24BLEU over the static baseline for the final test point.We get another absolute gain of +1.08 BLEU by al-lowing the LM coverage to adapt as well.
Using anonline, adaptive model gives a total gain of +2.32BLEU over a static baseline that does not adapt.6Although we batch retrain the LMs we could use an onlineLM that incorporates new vocabulary from the input stream asin Levenberg and Osborne (2009).400Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them.Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles.Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles.Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichenPrinzipien mitzuwirken.Static:  Our position is clear and we all know: we are against the war and the occupation of Iraq by the United States and the UnitedKingdom, and we are calling for the immediate withdrawal of the besatzungsm?chte from this country.Online: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the UnitedKingdom, and we demand the immediate withdrawal of the occupying forces from this country .Reference: Our position is clear and well known: we are against the war and the US-British occupation in Iraq and we demand theimmediate withdrawal of the occupying forces from that country.Source: Unser Standpunkt ist klar und allseits bekannt: Wir sind gegen den Krieg und die Besetzung des Irak durch die USA und dasVereinigte K?nigreich, und wir verlangen den unverz?glichen Abzug der Besatzungsm?chte aus diesem Land.Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recentsentences.
In both examples we get longer matching phrases in the online translation compared to the static one.5 Related Work5.1 Translation Model Domain AdaptationOur work is related to domain adaptation for transla-tion models.
See, for example, Koehn and Schroeder(2007) or Bertoldi and Federico (2009).
Most tech-niques center around using mixtures of translationmodels.
Once trained, these models generally neverchange.
They therefore fall under the batch trainingregime.
The focus of this work instead is on incre-mental retraining and also on supporting boundedmemory consumption.
Our experiments examineupdating model parameters in a single domain overdifferent periods in time.
Naturally, we could alsouse domain adaptation techniques to further improvehow we incorporate new samples.5.2 Online EM for SMTFor stepwise online EM for SMT models, the onlyprior work we are aware of is Liang and Klein(2009), where variations of online EM were exper-imented with on various NLP tasks including wordalignments.
They showed application of sOEM canproduce quicker convergence compared to the batchEM algorithm.
However, the model presented doesnot incorporate any unseen data, instead iteratingover a static data set multiple times using sOEM.For Liang and Klein (2009) incremental retrainingis simply an alternative way to use a fixed trainingset.5.3 Streaming Language ModelsRecent work in Levenberg and Osborne (2009) pre-sented a streaming LM that was capable of adapt-ing to an unbounded monolingual input stream inconstant space and time.
The LM has the ability toadd or delete n-grams (and their counts) based onfeedback from the decoder after translation points.The model was tested in an SMT setting and resultsshowed recent data benefited performance.
How-ever, adaptation was only to the LM and no testswere conducted on the TM.6 Conclusion and Future WorkWe have presented an online EM approach for wordalignments.
We have shown that, for a SMT system,incorporating recent parallel data into a TM from aninput stream is beneficial to translation performancecompared to a traditional, static baseline.Our strategy for populating the suffix array wassimply to use a first-in, first-out stack.
For futurework we will investigate whether information pro-vided by the incoming stream coupled with the feed-back from the decoder allows for more sophisti-cated adaptation strategies that reinforce useful wordalignments and delete bad or unused ones.In the near future we also hope to test the onlineEM setup in an application setting such as a com-puter aided translation or crowdsourced generatedstreams via Amazon?s Mechanical Turk.401AcknowledgementsResearch supported by EuroMatrixPlus funded bythe European Commission, by the DARPA GALEprogram under Contract Nos.
HR0011-06-2-0001and HR0011-06-C-0022, and the NSF under grantIIS-0713448.ReferencesNicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In WMT09: Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, pages 182?189, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 858?867.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics(ACL?05), pages 255?262, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Olivier Cappe and Eric Moulines.
2009.
Online EM al-gorithm for latent data models.
Journal Of The RoyalStatistical Society Series B, 71:593.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39:1?38.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for SMT.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: an open source toolkit for parsing-based machine translation.
In WMT09: Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation, pages 135?139, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Percy Liang and Dan Klein.
2009.
Online EM for unsu-pervised models.
In North American Association forComputational Linguistics (NAACL).Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 505?512, Manchester, UK, August.Coling 2008 Organizing Committee.Udi Manber and Gene Myers.
1990.
Suffix arrays:A new method for on-line string searches.
In TheFirst Annual ACM-SIAM Symposium on Dicrete Algo-rithms, pages 319?327.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Ronald Rosenfeld.
1995.
Optimizing lexical and n-gramcoverage via judicious use of linguistic data.
In InProc.
European Conf.
on Speech Technology, pages1763?1766.Mikae?l Salson, Thierry Lecroq, Martine Le?onard, andLaurent Mouchard.
2009.
Dynamic extended suffixarrays.
Journal of Discrete Algorithms, March.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational linguistics, pages 836?841, Morristown,NJ, USA.
Association for Computational Linguistics.402
