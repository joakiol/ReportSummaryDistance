Improved Estimation of Entropy forEvaluation of Word Sense InductionLinlin Li?Microsoft Development Center NorwayIvan Titov?
?University of AmsterdamCaroline Sporleder?Trier UniversityInformation-theoretic measures are among the most standard techniques for evaluation ofclustering methods including word sense induction (WSI) systems.
Such measures rely onsample-based estimates of the entropy.
However, the standard maximum likelihood estimatesof the entropy are heavily biased with the bias dependent on, among other things, the number ofclusters and the sample size.
This makes the measures unreliable and unfair when the numberof clusters produced by different systems vary and the sample size is not exceedingly large.
Thiscorresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense numberarguably does not exist and the standard evaluation scenarios use a small number of instances ofeach word to compute the score.
We describe more accurate entropy estimators and analyze theirperformance both in simulations and on evaluation of WSI systems.1.
IntroductionThe task of word sense induction (WSI) has grown in popularity recently.
WSI has theadvantage of not assuming a predefined inventory of senses.
Rather, senses are inducedin an unsupervised fashion on the basis of corpus evidence (Schu?tze 1998; Purandareand Pedersen 2004).
WSI systems can therefore better adapt to different target domainsthat may require sense inventories of different granularities.
However, the fact that WSIsystems do not rely on fixed inventories also makes it notoriously difficult to evaluateand compare their performance.
WSI evaluation is a type of cluster evaluation problem.Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehland Gosh 2002; Meila 2007), it is still not a solved problem.
Finding a good way toscore partially incorrect clusters is particularly difficult.
Several solutions have been?
Microsoft Development Center Norway.
E-mail: linlin@coli.uni-saarland.de.??
Institute for Logic, Language and Computation.
E-mail: titov@uva.nl.?
Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany.E-mail: sporledc@uni-trier.de.Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication:20 November 2013doi:10.1162/COLI a 00196?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 3proposed but information theoretic measures have been among the most successfuland widely used techniques.
One example is the normalized mutual information, alsoknown as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), whichhas, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al.
2010).All information theoretic measures of cluster quality essentially rely on sample-based estimates of entropy.
For instance, the mutual information I(c, k) between a goldstandard class c and an output cluster k can be written H(c) + H(k) ?
H(k, c), whereH(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their jointentropy.
The most standard estimator is the maximum-likelihood (ML) estimator, whichsubstitutes the probability of each event (cluster, classes, or cluster-class pair occurrence)with its normalized empirical frequency.Entropy estimators, even though consistent, are biased.
This means that the expectedestimate of the entropy on a finite sample set is different from the true value.
It isalso different from an expected estimate on a larger test set generated from the samedistribution, as the bias depends on the size of the sample.
This discrepancy negativelyaffects entropy-based evaluation measures, such as the V-measure.
This is different fromsupervised classification evaluation, where the classification accuracy on a finite test setis expected to be equal to the error rate (for the independent and identically distributed,i.i.d.)
case, though it can be different due to variance (due to choice of the test set).
Aslong as the number of samples is large with respect to the number of classes and clusters,the estimate is sufficiently close to the true entropy.
Otherwise, the quality of entropyestimators matters and the bias of the estimator can be large.
This problem is especiallyprominent for the ML estimator (Miller 1955).In WSI, we are faced with exactly those conditions that negatively affect the entropyestimators.
In a typical setting, the number of examples per word is small?for example,less than 100 on average for the SemEval 2010 WSI task.
The number of clusters, onthe other hand, can be fairly high, with some systems outputting more than 10 senseclusters per word on average.
Because the bias of an entropy estimator is dependenton, among other things, the number of clusters, the ranking of different WSI systems ispartly affected by the number of clusters they produce.
Even worse, the ranking is alsoaffected by the size of the test set.
The problem is exacerbated when computing the jointentropy between clusters and classes, H(k, c), because this requires estimating the jointprobability of cluster-class pairs for which the statistics are even more sparse.The bias problem of entropy estimators has long been known in the informationtheory community and many studies have addressed this issue (e.g., Miller 1955; Batuet al.
2002; Grasberger and Schu?rmann 1996).
In this article, we compare differentestimators and their influence on the computed evaluation scores.
We run simulationsusing a Zipfian distribution where we know the true entropy.
We also compare differentestimators against the SemEval 2010 WSI benchmark.
Our results strongly suggest thatthere are estimators, namely, the best-upper-bound (BUB) estimator (Paninski 2003)and jackknifed (Tukey 1958; Quenouille 1956) estimators, which are clearly preferableto the commonly used ML estimators.2.
Clustering Evaluation2.1 Information-Theoretic MeasuresThe main challenge in evaluating clustering methods is that successful measures shouldbe able to compare solutions found at different levels of granularity.
In other words,one cannot assume that there exists one-to-one mapping between the predicted clusters672Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Inductionand the gold-standard classes.
A natural approach would be to consider arbitrarystatistical dependencies between the cluster assignment k and the class assignment c.The standard measure of statistical dependence of two random variables is the Shannonmutual information I(k, c) (MI).
MI is 0 if two variables are independent, and it is equalto the entropy of a variable if another variable is deterministically dependent on it.Clearly, such measure would favor clusterings with higher entropy, and, consequently,normalized versions of MI are normally used to evaluate clusterings.
One instance ofnormalized MI actively used in the context of WSI evaluation is the V-measure, orsymmetric uncertainty (Witte and Frank 2005; Rosenberg and Hirschberg 2007):V(k, c) =2I(k, c)H(k) + H(c)=2(H(k) + H(c) ?
H(k, c))H(k) + H(c)though other forms of MI normalization have also been explored (Strehl and Gosh 2002).Because the true marginal and the joint entropies are not known, the standardmaximum likelihood estimators (also called plug-in estimators of entropy) are normallyused instead.
The ML estimates H?
have the analytical form of an entropy with thenormalized empirical frequency substituted instead of the unknown true membershipprobabilities, for example:H?
(c) =m?i=1?niN logniN (1)where ni is the number of times cluster i appears in the set, m is the number of clusters,and N is the size of the set (i.e., the sample size).The ML estimators of entropy are consistent but heavily negatively biased (seeSection 3 for details).
In other words, the expectation of H?
is lower than the true entropy,and this discrepancy increases with the number of clusters m and decreases with thesample size N. When m is comparable to N, the ML estimator is known to be veryinaccurate (Paninski 2004).Note that for V-measure estimation the main source of the estimation error is thejoint entropy H(k, c),1 as the number of possible pairs (c, k) for most systems would belarge whereas the total number of occurrences will remain the same as for the estimationof H(c) and H(k).
Therefore, the absolute value of the bias for H?
(c, k) will exceed theaggregate bias of the estimators of marginal entropy, H?
(c) and H?(k).
As a result, theV-measure will be positively biased, and this bias would be especially high for systemspredicting a large number of clusters.This phenomenon has been previously noticed (Manandhar et al.
2010) but no satis-factory explanation has been given.
The shortcomings of the ML estimator are especiallyeasy to see on the example of a baseline system that assigns every instance in the testingset to an individual cluster.
This baseline, when averaged over the 100 target words, out-performs all the participants?
systems of the SemEval-2010 task on the standard testingset (Manandhar and Klapaftis 2009).
Though we cannot compute the true bias for anyreal system, the computation is trivial for this baseline.
The true V-measure is equal to 0,1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimationthey are all equivalent.
For some more complex estimators, including some of the ones considered here,the resulting estimates will be somewhat different depending on the decomposition.
We will focus on thesymmetric form presented here.673Computational Linguistics Volume 40, Number 3as the baseline can be regarded as a limiting case of a stochastic system that picks up oneof the m clusters under the uniform distribution with m ?
?
; the mutual informationbetween any class labels and clustering produced by such model equals 0 for every m.However, the ML estimate for the V-measure is V?
(k, c) = 2H?
(c)/(log N + H?(c)).
For thetesting set of SemEval 2010, this estimate, averaged over all the words, yields 31.7%,which by far exceeds the best result of any system (16.2%).
On an infinite (or sufficientlylarge) set, however, its performance would change to the worst.
This is a problem notonly for the baseline but for any system which outputs a large number of classes: Theerror measures computed on the small test set are far from their expectations on thenew data.
We will see in our quantitative analyses (Section 5) that using more accurateestimators will have the most significant effect on both the V-measure and on the ranksof systems which output richer clustering, agreeing with this argument.Though in this analysis we focused on the V-measure, other information theoreticmeasures have also been proposed.
Examples of such measures include the variationof information measure (Meila 2007) VI(c, k) = H(c|k) + H(k|c) and Q0 measure (Dom2001) H(c|k).
This argument applies to these evaluation measures as well, and they canall be potentially improved by using more accurate estimators.2.2 Alternative MeasuresNot only information-theoretic measures have been proposed for clustering evaluation.An alternative evaluation strategy is to attempt to find the best possible mappingbetween the predicted clusters and the gold-standard classes and then apply standardmeasures like precision, recall, and F-score.
However, if the best mapping is selected onthe test set the result can be overoptimistic, especially for rich clusterings.
Consequently,such methods constrain the set of permissible mappings to a restricted family.
For exam-ple, for the F-score, one considers only mappings from each class to a single predictedcluster (Zhao and Karypis 2005; Agirre and Soroa 2007).
This restriction is generally toostrong for many clustering problems (Meila 2007; Rosenberg and Hirschberg 2007), andespecially inappropriate for the WSI evaluation setting, as it penalizes sense inductionsystems that induce more fine-grained senses than the ones present in the gold-standardsense set.The Paired F-score (Manandhar et al.
2010) is somewhat less restrictive than theF-score measures in that it defines precision and recall in terms of pairs of instances (i.e.,effectively evaluating systems based on the proportion of correct links).
However, thePaired F-score has the undesirable property that it ranks those systems highest whichput all instances in one cluster, thereby obtaining perfect recall.As an alternative, the supervised evaluation measure has been proposed (Agirreet al.
2006).
This approach in addition to the testing set uses an auxiliary mapping set.First the mapping is induced on the mapping set, then the quality of the mapping isevaluated on the testing set.
One problem with this evaluation scenario is that the sizeof the mapping set has an effect on the results and there is no obvious criterion forselecting the right size of the mapping set.
For the WSI task, the importance of the setsize was empirically confirmed when the evaluation set was split in proportions 80:20(80% for the mapping sets, and 20% for testing) instead of the original 60:40 split: Thescores of all top 10 systems improved and the ranking changed as well (Manandharet al.
2010) (see also Table 1 later in this article).Further cluster evaluation measures have been proposed for other language pro-cessing tasks, such as B3 (Bagga and Baldwin 1998) or CEAF (Luo 2005) for coreferenceresolution evaluation.
In this article, we are concerned with entropy-based measures.674Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense InductionFor a more general assessment of measures for clustering evaluation see Amigo et al.
(2009) and Klapaftis and Manandhar (2013).3.
Entropy EstimationGiven the influence that information theory has had on many fields, including signalprocessing, neurophysiology, and psychology, to name a few, it is not surprising that thetopic of entropy estimation has received considerable attention over the last 50 years.2However, much of the work has focused on settings where the number of classes issignificantly lower than the size of the sample.
More recently a set-up where the samplesize N is comparable to the number of classes m has started to receive attention (Paninski2003, 2004).In this section, we start by discussing the intuition for why the ML estimator isheavily biased in this setting.
Though unbiased estimators of entropy do not exist,3various techniques have been proposed to reduce the bias while controlling the vari-ance (Grasberger and Schu?rmann 1996; Batu et al.
2002).
We will discuss widely usedbias-corrected estimators, the ML estimator with Miller-Madow bias correction (Miller1955) and the jackknifed estimator (Strong et al.
1998).
Then we turn to the morerecent technique proposed specifically for the N ?
m setting, the best-upper-bound(BUB) estimator (Paninski 2003).
We will conclude this section by explaining how theseestimators can be computed using stochastic (weighted) output of WSI systems.3.1 Standard Estimators of EntropyAs we discussed earlier, the ML estimator (1) is negatively biased.
For a fixed distri-bution p, a little algebra can be used to show that the bias of the maximum likelihoodestimator can be written asH ?
Ep(H?)
= Ep(D(p?
?
p))where Ep denotes an expectation under p, D is the Kullback-Leibler (KL) divergence,and p?
is the empirical distribution in the sample of N elements drawn i.i.d.
from p.Because the KL-divergence is always non-negative, it follows that the bias is alwaysnon-positive.
It also follows that the expected divergence is larger if the size of thesample is small.
In fact, this expression can be used to obtain the asymptotic bias rate(N ?
?)
(Miller 1955).
The bias rate derived in this way would suggest a form ofcorrection to the ML estimator, called Miller-Madow bias correction H?MM = H?
+m?
?1N ,where m?
is an estimate of m, as the true size of support m may not be known.
In ourexperiments, we use a basic estimator m?
which is just the number of different clusters(classes or cluster-class pairs depending on the considered entropy) appearing in thesample.
We will call the estimator H?MM the Miller-Madow (MM) estimator.
As the MMestimator is motivated by the asymptotic behavior of the bias, it is not very appropriatefor N ?
m.2 For a relatively recent overview of progress in entropy estimation research see, for example, theproceedings of the NIPS 2003 workshop on entropy estimation.3 The expectation of any estimate from i.i.d.
samples is a polynomial function of class probabilities.The entropy is non-polynomial and therefore unbiased estimators do not exist.675Computational Linguistics Volume 40, Number 3The bias of the ML estimator decreases with the size of the sample.
Intuitively, anestimate of the discrepancy in estimates produced from samples of different sizes canbe used to correct the ML estimator: If an estimate based on N ?
1 samples significantlyexceeds the estimate from N samples, then the bias of the estimator is still large.Roughly, this intuition is encoded in the jackknifed (JK) estimator (Strong et al.
1998):H?JK = NH?
?N ?
1NN?j=1H?
?jwhere H?
?j is the ML estimator based on the original sample excluding the example j.3.2 BUB EstimatorWe can observe that all the previous estimators can be expressed in the form of a linearfunction of the ordered histogram statisticsH?
(a) =N?j=0aj,Nhj (2)where hj is the number of classes which appear j times in the sample:hj =m?i=1[[ni = j]] (3)where [[ ]] denotes the indicator function.
The coefficients aj,N for the ML, MM, and JKestimators are equal to:aML,j,N = ?jN logjNaMM,j,N = ?jN logjN +1 ?
jNNaJK,j,N = NaML,j,N ?N ?
1N ((N ?
j)aML,j,N?1 + jaML,j?1,N?1)This observation suggests that it makes sense to study an estimator of the generalform H?
(a) as defined in Equation (2).
Upper bounds on the bias and variance of suchestimators4 have been stated in Paninski (2003).
These bounds imply an upper boundon the standard measure of estimator performance, mean squared error (MSE, thesum of the variance and squared bias).
The worst-case estimator is then obtainedby selecting a to minimize the upper bound on MSE, and, therefore, it is called thebest-upper-bound estimator.
This optimization problem5 corresponds to a regularized4 We argued that variance is not particularly important for the ML estimator with N ?
m. However,for an arbitrary estimator of the form of Equation (2) this may not be true, as the coefficients aj,Nmay be oscillating, resulting in an estimator with a large variance (Antos and Kontoyiannis 2001).5 More formally, its modification where the L2 norm is optimized instead of the original L?optimization set-up.676Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Inductionleast-squares problem and can be solved analytically (see Appendix A and Paninski[2003] for technical details).This technique is fairly general, and can potentially be used to minimize the boundfor a particular type of distribution.
This direction can be promising, as the types ofdistributions observed in WSI are normally fairly skewed (arguably Zipfian) and tighterbounds on MSE may be possible.
In this work, we use the universal worst-case boundsadvocated in Paninski (2003).3.3 Estimation with Stochastic PredictionsAs many WSI systems maintain a distribution over predicted clusters, in SemEval 2010the participants were encouraged to provide a weighted prediction (i.e., a distributionover potential clusters for each example) instead of predicting just a single most-likelycluster.We interpret the weighted output of a system on an example l as a categoricaltrial with the probabilities of outcomes p?
(l)i provided by the model where i is an indexof the cluster.
Therefore a stochastic output of a system on the test set representsa distribution over samples generated from these trials; the estimator can be com-puted as an expectation under this distribution.
For estimators of the form of Equa-tion (2), we can exploit the linearity of expectations and write the expected value of theestimator asEp?[H?(a)]=N?j=0aj,NEp?
[hj]where Ep?
[hj]is the expected number of classes with j counts.
We can rewrite it using thelinearity property again, this time for expression (3):Ep?[H?(a)]=N?j=0aj,Nm?i=1P?
(ni = j, N) (4)where P?i(ni = j, N) is the distribution over the number of counts for non-identicalBernoulli trials p?
(l)i and 1 ?
p?
(l)i (l = 1, .
.
.
, N), known as the Poisson binomial distribu-tion, a generalization of the standard binomial distribution.
The probabilities can beefficiently computed using one of alternative recursive formulas (Wang 1993).
Oneof the simplest schemes, with good numerical stability properties, is the recursivecomputation:P?i(ni = j, t) = P?i(ni = j ?
1, t ?
1)p?
(j)i + P?i(ni = j, t ?
1)(1 ?
p?
(j)i )where j = 1, .
.
.
, N.4.
SimulationsBecause the true entropy (and V-measure) is not known on the WSI task, we start withsimulations where we generated samples from a known distribution and can compare677Computational Linguistics Volume 40, Number 30 5 10 15 20 25 30 35 40 45 5000.511.522.5Sample Size (N)EntropyHBUBJKMMMLFigure 1The estimated and true entropy for uniform distribution.the estimates (and their biases) with the true entropy.
In all our experiments, we set thenumber of clusters m to 10 and varied the sample size N (Figure 1).
Each point on thegraph is the result of averaging over 1,000 sampling experiments.6The distribution of senses for a given word is normally skewed: For most wordsthe vast majority of occurrences correspond to one or two most common senses eventhough the total number of senses can be quite large (Kilgarriff 2004).
This type of long-tail distribution can be modeled with Zipf?s law.
Consequently, most of our experimentsconsider Zipfian distributions.
For Zipf?s law, the probability of choosing an elementwith rank k is proportional to 1ks , where s is a shape parameter.
Small values of theparameter s correspond to flatter distributions; the distributions with a larger s areincreasingly more skewed.
The estimators?
prediction for Zipfian distributions witha different s are shown in Figure 2.
For s = 4, over 90% of the probability mass isconcentrated on a single class.
For every distribution we plot the true entropy (H)and the estimated values; compare results with a uniform distribution as seen inFigure 1.In all figures, we observe that over the entire range of sample sizes, the biasfor the bias-corrected estimates is indeed reduced substantially with respect to thatof the ML estimator.
This difference is particularly large for smaller N?the realisticsetting for the computation of H(c, k) for the WSI task.
For the uniform distributionand flatter Zipf distributions (s = 1 and 2), the JK estimator seems preferable for allbut the smallest sample sizes (N > 3).
The BUB estimator outperforms the JK estimatorwith very skewed distributions (s = 3 and s = 4) and in most cases provides the leastbiased estimates with very small N. However, these results with very small sample sizes(N ?
2) may not have much practical relevance as any estimator is highly inaccurate inthis mode.
The MM bias correction, as expected, is not sufficient for small N. Althoughit outperforms the ML estimates, its error is consistently larger than those of other bias-correction strategies.Overall, the simulations suggest that the ML estimators are not very appropriate forentropy estimation with the types of distributions which are likely to be observed in the6 In this way we study only the bias of estimators.678Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction0 5 10 15 20 25 30 35 40 45 5000.511.522.5Sample Size (N)EntropyHBUBJKMMML0 5 10 15 20 25 30 35 40 45 5000.20.40.60.811.21.4Sample Size (N)EntropyHBUBJKMMML(a) s = 1 (b) s = 20 5 10 15 20 25 30 35 40 45 5000.10.20.30.40.50.60.70.80.9Sample Size (N)EntropyHBUBJKMMML0 5 10 15 20 25 30 35 40 45 5000.10.20.30.40.50.60.70.80.9Sample Size (N)EntropyHBUBJKMMML(c) s = 3 (d) s = 4Figure 2The estimated and true entropy of Zipf?s law.WSI tasks.
Both the JK and BUB estimators are considerably less biased alternatives tothe ML estimations.5.
Effects on WSI EvaluationTo gauge the effect of the bias problem on WSI evaluation, we computed how theranking of the SemEval 2010 systems (Manandhar et al.
2010) were affected by differentestimators.
The SemEval 2010 organizers supplied a test set containing 8,915 manuallyannotated examples covering 100 polysemous lemmas.The average number of gold standard senses per lemma was 3.79.
Overall,27 systems participated and were ranked according to their performance on the testset, applying the V-measure evaluation as well as paired F-score and a supervisedevaluation scheme.
The systems were also compared against three baselines.
For theMost Frequent Sense (MFS) baseline all test instances of a given target lemma aregrouped into one cluster, that is, there is exactly one cluster per lemma.
The secondbaseline, Random, assigns each instance randomly to one of four clusters.
The lastbaseline, proposed in Manandhar and Klapaftis (2009), 1-cluster-per-instance (1ClI),produces as many clusters as there are instances in the test set.Table 1 gives an overview of the different systems and the three baselines (shown initalics).
The systems are presented in the order in which they were given in the official679Computational Linguistics Volume 40, Number 3Table 1V-measure computed with different estimators.
Supervised recall is shown for comparison(80:20 and 60:40 splits for mapping/evaluation, numbers as provided by Manandhar et al.
2010).The corresponding ranks are shown in parentheses.System C# ML MM JK BUB Supervised Recall80:20 60:401ClI 89.1 31.6 (1) 29.5 (1) 27.4 (1) ?3.6 (29) ?
?Hermit 10.8 16.2 (2) 13.1 (4) 10.7 (4) 11.0 (2) 58.3 (17) 57.3 (18)UoY 11.5 15.7 (4) 14.3 (2) 13.1 (2) 11.4 (1) 62.4 (1) 62.0 (1)KSU KDD 17.5 15.7 (3) 13.2 (3) 11.0 (3) 7.6 (3) 52.2 (24) 50.4 (25)Duluth-WSI 4.1 9.0 (5) 6.9 (5) 5.7 (5) 5.6 (5) 60.5 (2) 59.5 (5)Duluth-WSI-SVD 4.1 9.0 (6) 6.9 (6) 5.7 (6) 5.6 (6) 60.5 (3) 59.5 (4)Duluth-R-110 9.7 8.6 (7) 4.7 (16) 1.9 (20) 3 (17) 54.8 (23) 53.6 (23)Duluth-WSI-Co 2.5 7.9 (8) 6.4 (7) 5.7 (7) 5.7 (4) 60.8 (4) 60.1 (2)KCDC-PCGD 2.9 7.8 (9) 6.3 (8) 5.5 (8) 5.2 (7) 59.5 (9) 59.1 (7)KCDC-PC 2.9 7.5 (10) 6.2 (9) 5.4 (9) 5.0 (8) 59.7 (8) 58.9 (9)KCDC-PC-2 2.9 7.1 (11) 5.7 (12) 4.9 (12) 4.5 (13) 59.8 (7) 58.9 (8)Duluth-Mix-Narrow-Gap 2.4 6.9 (15) 5.5 (14) 4.8 (14) 4.8 (9) 56.6 (21) 56.2 (21)KCDC-GD-2 2.8 6.9 (14) 5.7 (11) 4.9 (11) 4.6 (12) 58.7 (13) 57.9 (15)KCDC-GD 2.8 6.9 (12) 5.8 (10) 5.0 (10) 4.6 (11) 59.0 (11) 58.3 (11)Duluth-Mix-Narrow-PK2 2.7 6.8 (16) 5.4 (15) 4.6 (15) 4.6 (10) 56.1 (22) 55.7 (22)Duluth-MIX-PK2 2.7 5.6 (17) 4.3 (17) 3.5 (17) 3.5 (16) 51.6 (25) 50.5 (24)Duluth-R-15 5.0 5.3 (18) 2.4 (20) 0.7 (24) 1.3 (22) 56.8 (20) 56.5 (19)Duluth-WSI-Co-Gap 1.6 4.8 (19) 4.1 (18) 3.8 (16) 4.1 (15) 60.3 (5) 59.5 (3)Random 4.0 4.4 (20) 1.9 (22) 0.5 (25) 0.8 (24) 57.3 (19) 56.5 (20)Duluth-R-13 3.0 3.6 (21) 1.5 (25) 0.5 (26) 0.7 (25) 58.0 (18) 57.6 (17)Duluth-WSI-Gap 1.4 3.1 (22) 2.6 (19) 2.5 (18) 2.7 (18) 59.8 (6) 59.3 (6)Duluth-Mix-Gap 1.6 3.0 (23) 2.3 (21) 1.9 (19) 2.0 (19) 50.6 (26) 49.8 (26)Duluth-Mix-Uni-PK2 2.0 2.4 (24) 1.8 (23) 1.5 (21) 1.4 (21) 19.3 (27) 19.1 (27)Duluth-R-12 2.0 2.3 (25) 0.8 (27) 0.2 (27) 0.3 (28) 58.5 (16) 57.7 (16)KCDC-PT 1.5 1.9 (26) 1.6 (24) 1.4 (22) 1.5 (20) 58.9 (12) 58.3 (13)Duluth-Mix-Uni-Gap 1.4 1.4 (27) 1.0 (26) 0.8 (23) 1.0 (23) 18.7 (28) 18.9 (28)KCDC-GDC 2.8 6.9 (13) 5.7 (13) 4.8 (13) 4.5 (14) 59.1 (10) 58.3 (10)MFS 1.0 0 (29) 0.0 (29) 0.0 (28) 0.5 (27) 58.7 (15) 58.3 (12)Duluth-WSI-SVD-Gap 1.0 0.0 (28) 0.0 (28) 0.0 (29) 0.5 (26) 58.7 (14) 58.2 (14)KCDC-PC-2* 2.9 5.7 7.2 2.3 2.2 ?
?UoY* 11.5 25.1 22.8 17.8 5.0 ?
?SemEval 2010 results table (Table 4 in Manandhar et al.
(2010), p. 66).
Table 1 showsthe average number of clusters per word (C#), the V-measure computed with differentestimators (ML, MM, JK, and BUB), and the rankings it produces (in brackets).7 Forcomparison, the results of a supervised evaluation are also shown.
The bottom two rows(KCDC-PC-2?
and UoY?)
show the scores computed from the stochastic (weighted)output (Section 3.3) for systems KCDC-PC-2 and UoY, respectively.
Other systems didnot produce weighted output.7 The ranking produced by the ML estimator should mirror that of the official results.
In some cases itdoes not?for example, system UoY was placed before KSU in the official results, whereas the MLestimator would predict the reverse order.
As the difference in V-measure is small, we attribute thisdiscrepancy to rounding errors.
The system KCDC-GDC seems to be misplaced in the official resultslist; according to V-measure it should be ranked higher.
Our ranking was computed before rounding,and there were no ties.680Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction0 2 4 6 8 10 12 14 16 1800.010.020.030.040.050.060.07Average Cluster NumberML?JK0 2 4 6 8 10 12 14 16 18?0.0100.010.020.030.040.050.060.070.080.09Average Cluster NumberML?BUB(a) H?
- H?JK (b) H?
- H?BUBFigure 3Discrepancy in estimates as a function of the predicted number of classes.The 27 systems vary widely in the number of average clusters they outputper lemma, ranging from 1.02 (Duluth-WSI-SVD) to 17.5 (KSU-KDD).
To assess theinfluence of the cluster granularity on the entropy estimates, we compared the estimatesgiven by the ML estimator against those given by JK and BUB for different numbers ofclusters.
Figure 3 plots the cluster numbers output by the systems against the estimatedifference for ML vs. JK (Figure 3a) and ML vs. BUB (Figure 3b).
If two estimatorsagree perfectly (i.e., produce the same estimate), their difference should always bezero, independent of the number of clusters.
As can be seen, this is not the case.
Asexpected, the difference is larger for systems with larger numbers of clusters, such asKSU-KDD.
This trend will result in unfair preference towards systems producing richerclusterings.Figure 4 shows the effect that the discrepancy in estimates has on the rankingsproduced by using either of the three estimators.
Figure 4a plots the ranking of theML estimator against JK, and Figure 4b plots the ranking of ML against BUB.
Dots thatlie on the diagonal line indicate systems whose rank has not changed.
It can be seen thatthis only applies to a minority of the systems.
In general, there are significant differencesbetween the rankings produced by ML and those by JK or BUB.
We have seen that theML estimator can lead to counterintuitive and undesirable results, such as ranking the0 5 10 15 20 25 30051015202530ML Ranking (SemEval10)JKRanking0 5 10 15 20 25 30051015202530ML Ranking (SemEval10)BUBRanking(a) H?
vs. H?JK (b) H?
vs. H?BUBFigure 4Discrepancy in rankings as a function of the predicted number of classes.681Computational Linguistics Volume 40, Number 31-cluster-per-instance baseline highest.
The BUB estimator corrects this and assigns thelast rank to this baseline.8The estimate for the V-measure is based on the estimates of the marginal and jointentropies.
To confirm our intuition that joint entropies are more significantly corrected,we looked into the differences between estimates of each entropy for five systems withthe largest number of clusters (excluding the 1C1I baseline).
The average differencesin estimation of H(k) and H(k, c) between JK and ML estimators are 0.08 and 0.16,respectively, confirming our hypothesis.
Analogous discrepancies for the pair BUB vs.ML are 0.15 and 0.06, respectively.
The differences in the entropy of the gold standardclustering, H(c), is less significant (< 0.02 for both methods) as the gold standard is lessfine-grained than the clusters proposed by these five systems.For the stochastic version evaluation, we can observe that the score for the KCDC-PC-2 system is mostly decreased with respect to the ?deterministic?
evaluation (exceptfor the MM estimator).
Conversely, the score of UoY is mostly improved except tothe prediction of the BUB estimator.
These differences are somewhat surprising: Thestochastic version resulted in significantly larger disagreement between the estimatorsthan the deterministic version.
We do not yet have a satisfactory explanation for thisphenomenon.It is important to notice that for the vast majority of the systems there is agreementbetween the scores of the JK and BUB estimator, wheres the ML estimator significantlyoverestimates the V-measure for most of the systems.
This observation, coupled withthe observed behavior of the JK and BUB estimators in the simulations, suggest thattheir predictions are considerably more reliable than predictions of the plug-in MLestimator.Comparing the V-measure (BUB) rankings to those obtained by supervised evalu-ation (last two columns in Table 1) shows noticeable differences.
Several systems thatrank highly according to the V-measure occupy the lower end of the scale when evalu-ated according to supervised recall (Hermit, KSU KDD, Duluth-Mix-Narrow-Gap).6.
ConclusionsIn this work, we analyzed the shortcomings of information-theoretic measures in thecontext of WSI evaluation and argued that main drawbacks of these approaches, suchas the preference for the systems predicting richer clusterings or assigning the top scoreto the 1-cluster-per-instance baseline, are caused by the bias of the underlying sample-based estimates of entropy.
We studied alternative estimators, including one specificallydesigned to deal with cases where the number of examples is comparable with the num-ber of clusters.
Two of the considered estimators, the jackknifed estimator and the best-upper-bound estimator, achieve consistently and significantly less biased results thanthe standard ML estimator when evaluated in simulations with Zipfian distributions.The corresponding estimates in the WSI evaluation context can result in significantchanges in scores and relative rankings, with systems producing richer clusterings moreseverely affected.
We believe that these results strongly suggest that more accurateestimates of entropy should be used in future evaluations of sense induction systems.Other unsupervised tasks in natural language processing, such as word clustering or8 Note that the V-measure is actually negative here.
Though this is not possible for the true V-measure,the estimated V-measure expresses a difference between the estimated joint entropy and the marginalentropies and can be negative.682Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Inductionnamed entity disambiguation, may also benefit from using information-theoretic scoresbased on more accurate estimators.Appendix A: Derivation for the BUB EstimatorWe provide a brief derivation for the BUB estimator and refer the reader to Paninski(2003) for details and discussion.
The BUB estimator is obtained by minimizing an upperbound on MSE for estimators H(a) (see Equation (2)).
First, MSE is bounded from aboveby maximizing the variance and the bias independently:maxpBp(H?
(a))2+ Vp(H?
(a)) ?
maxpBp(H?
(a))2+ maxpVp(H?
(a)) (A.1)where p = (p1, .
.
.
, pm) is an underlying discrete measure; Bp and Vp are the bias and thevariance of the estimator given p. Then individual bounds both for the squared bias andthe variance can be constructed.We start by deriving a bound for the bias.
Using linearity of expectation, the expec-tation of H?
(a) can be written asEp(H?
(a)) =N?j=0aj,NEp(hj) =N?j=0aj,Nm?i=1Bj,N(pi)where Bj,N(x) is the binomial polynomial(Nj)xj(1 ?
x)N?j.
Then, with simple algebra,we haveBp(H?
(a)) =m?i=1?
?N?j=0aj,NBj,N(pi) ?
H(pi)?
?where H(x) = ?x log x, the entropy function.
A uniform upper bound can be obtainedby bounding each term in the sum:|Bp(H?
(a))| ?
m supx|N?j=0aj,NBj,N(x) ?
H(x)|However, this bound is not too tight as it would overemphasize importance of theapproximation quality for components i with pi close to 1.
Intuitively, the behavior near0 is more important, as there can be more components pi close to 0.
Paninski (2003)generalizes this bound by considering a weighted version|Bp(H?
(a))| ?
2 supxf (x)|N?j=0aj,NBj,N(x) ?
H(x)| (A.2)with the function f chosen to emphasize smaller componentsf (x) ={m, x < 1m1/x, x ?
1m683Computational Linguistics Volume 40, Number 3As shown in Antos and Kontoyiannis (2001) and in Paninski (2003), bounds onthe variance of the estimator H?
(a) can be derived using either McDiarmid or Steelebounds (Steele 1986).
For the Steele bound, it has the formVp(H?
(a)) < N max0?j<N(aj+1 ?
aj)2 (A.3)Finally, MSE can be bounded by substituting Equations (A.2) and (A.3) into theinequality (A.1).
For computational reasons, instead of choosing a to minimize thebound, the L2 relaxation of the L?
loss is used, resulting in a regularized least-squaresproblem.AcknowledgmentsThe research was carried out when theauthors were at Saarland University.
It wasfunded by the German Research FoundationDFG (Cluster of Excellence on MultimodalComputing and Interaction, SaarlandUniversity, Germany).
We would like tothank the anonymous reviewers for theirvaluable feedback.ReferencesAgirre, Eneko, David Mart?
?nez, Oier Lo?pezde Lacalle, and Aitor Soroa.
2006.
Evaluatingand optimizing the parameters of anunsupervised graph-based WSD algorithm.In Workshop on TextGraphs, at HLT-NAACL2006, pages 89?96, New York, NY.Agirre, Eneko and Aitor Soroa.
2007.Semeval-2007 task 02: Evaluating wordsense induction and discriminationsystems.
In Proceedings of the 4thInternational Workshop on SemanticEvaluation, pages 7?12, Prague.Amigo, Enrique, Julio Gonzalo, JavierArtiles, and Felisa Verdejo.
2009.
Acomparison of extrinsic clusteringevaluation metrics based on formalconstraints.
Information Retrieval,12(4):461?486.Antos, A. and I. Kontoyiannis.
2001.Convergence properties of functionalestimates of discrete distributions.
RandomStructures and Algorithms, 19:163?193.Bagga, Amit and Breck Baldwin.
1998.Algorithms for scoring coreference chains.In The First International Conference onLanguage Resources and Evaluation Workshopon Linguistics Coreference, pages 563?566,Granada.Batu, Tugkan, Sanjoy Dasgupta, Ravi Kumar,and Ronitt Rubinfeld.
2002.
The complexityof approximating entropy.
In Symposiumon the Theory of Computing (STOC),pages 678?687, Montreal.Dom, Byron E. 2001.
An information-theoretic external cluster-validity measure.Technical Report No.
RJ10219, IBM.Grasberger, P. and T. Schu?rmann.
1996.Entropy estimation of symbol sequences.CHAOS, 6(3):414?427.Kilgarriff, Adam.
2004.
How dominantis the commonest sense of a word?
InSojka, Kopecek, and Pala, editors, Text,Speech, Dialogue, volume 3206 of LectureNotes in Artificial Intelligence.
Springer,pages 103?112.Klapaftis, Ioannis P. and Suresh Manandhar.2013.
Evaluating word sense inductionand disambiguation methods.
LanguageResources and Evaluation, 47(3):1?27.Luo, Xiaoqiang.
2005.
On coreferenceresolution performance metrics.
InProceedings of the Conference on HumanLanguage Technology and Empirical Methodsin Natural Language Processing (HLT-05),pages 25?32, Vancouver.Manandhar, Suresh and Ioannis Klapaftis.2009.
Semeval-2010 task 14: Evaluationsetting for word sense induction &disambiguation systems.
In Proceedings ofthe Workshop on Semantic Evaluations:Recent Achievements and Future Directions(SEW-2009), pages 117?122, Boulder, CO.Manandhar, Suresh, Ioannis Klapaftis,Dmitriy Dligach, and Sameer Pradhan.2010.
Semeval-2010 task 14: Word senseinduction & disambiguation.
In Proceedingsof the 5th International Workshop on SemanticEvaluation, pages 63?68, Uppsala.Meila, Marina.
2007.
Comparingclusterings?an information baseddistance.
Journal of Multivariate Analysis,98:873?895.Miller, G. 1955.
Note on the bias ofinformation estimates.
Information Theoryin Psychology II-B, pages 95?100.Paninski, Liam.
2003.
Estimation of entropyand mutual information.
NeuralComputation, 15:1,191?1,253.684Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense InductionPaninski, Liam.
2004.
Estimating entropyof m bins given fewer than m samples.IEEE Transactions on Information Theory,50(9):2,200?2,203.Purandare, Amruta and Ted Pedersen.
2004.Word sense discrimination by clusteringcontexts in vector and similarity spaces.In Proceedings of the CoNLL, pages 41?48,Boston, MA.Quenouille, M. 1956.
Notes on bias andestimation.
Biometrika, 43:353?360.Rosenberg, Andrew and Julia Hirschberg.2007.
V-measure: A conditional entropy-based external cluster evaluationmeasure.
In Proceedings of the 2007EMNLP-CoNll Joint Conference,pages 410?420, Prague.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?123.Steele, J. Michael.
1986.
An Efron-Steininequality for nonsymmetric statistics.Annals of Statistics, 14:753?758.Strehl, Alexander and Joydeep Gosh.
2002.Cluster ensembles: A knowledge reuseframework for combining multiplepartitions.
Journal of Machine LearningResearch, 3:583?617.Strong, S., R. Koberle, S. R. van de Ruyter,and W. Bialek.
1998.
Entropy andinformation in neural spike trains.Physical Review Letters, 80:197?202.Tukey, J.
1958.
Bias and confidence in notquite large samples.
Annals of MathematicalStatistics, 29:614.Wang, Y. H. 1993.
On the number ofsuccesses in independent trials.
StatisticaSinica 3, 2:295?312.Witte, Ian and Eibe Frank.
2005.
DataMining: Practical Machine Learning Toolsand Techniques.
Morgan Kaufmann,Amsterdam.Zhao, Y. and G. Karypis.
2005.
Hierarchicalclustering algorithms for documentdatasets.
Data Mining and KnowledgeDiscovery, 10(2):141?168.685
