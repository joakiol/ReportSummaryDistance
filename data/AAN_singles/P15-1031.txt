Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 313?322,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAn Effective Neural Network Model for Graph-based Dependency ParsingWenzhe Pei Tao Ge Baobao Chang?Key Laboratory of Computational Linguistics, Ministry of Education,School of Electronics Engineering and Computer Science, Peking University,No.5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinaCollaborative Innovation Center for Language Ability, Xuzhou, 221009, China.
{peiwenzhe,getao,chbb}@pku.edu.cnAbstractMost existing graph-based parsing modelsrely on millions of hand-crafted features,which limits their generalization abilityand slows down the parsing speed.
In thispaper, we propose a general and effectiveNeural Network model for graph-baseddependency parsing.
Our model can auto-matically learn high-order feature combi-nations using only atomic features by ex-ploiting a novel activation function tanh-cube.
Moreover, we propose a simple yeteffective way to utilize phrase-level infor-mation that is expensive to use in conven-tional graph-based parsers.
Experimentson the English Penn Treebank show thatparsers based on our model perform betterthan conventional graph-based parsers.1 IntroductionDependency parsing is essential for computers tounderstand natural languages, whose performancemay have a direct effect on many NLP applica-tion.
Due to its importance, dependency parsing,has been studied for tens of years.
Among a vari-ety of dependency parsing approaches (McDonaldet al, 2005; McDonald and Pereira, 2006; Car-reras, 2007; Koo and Collins, 2010; Zhang andNivre, 2011), graph-based models seem to be oneof the most successful solutions to the challengedue to its ability of scoring the parsing decisionson whole-tree basis.
Typical graph-based modelsfactor the dependency tree into subgraphs, rang-ing from the smallest edge (first-order) to a con-trollable bigger subgraph consisting of more thanone single edge (second-order and third order),and score the whole tree by summing scores of thesubgraphs.
In these models, subgraphs are usuallyrepresented as a high-dimensional feature vectors?Corresponding authorwhich are fed into a linear model to learn the fea-ture weight for scoring the subgraphs.In spite of their advantages, conventional graph-based models rely heavily on an enormous num-ber of hand-crafted features, which brings aboutserious problems.
First, a mass of features couldput the models in the risk of overfitting and slowdown the parsing speed, especially in the high-order models where combinational features cap-turing interactions between head, modifier, sib-lings and (or) grandparent could easily explodethe feature space.
In addition, feature design re-quires domain expertise, which means useful fea-tures are likely to be neglected due to a lack ofdomain knowledge.
As a matter of fact, these twoproblems exist in most graph-based models, whichhave stuck the development of dependency parsingfor a few years.To ease the problem of feature engineering, wepropose a general and effective Neural Networkmodel for graph-based dependency parsing in thispaper.
The main advantages of our model are asfollows:?
Instead of using large number of hand-craftedfeatures, our model only uses atomic fea-tures (Chen et al, 2014) such as word uni-grams and POS-tag unigrams.
Feature com-binations and high-order features are auto-matically learned with our novel activationfunction tanh-cube, thus alleviating the heavyburden of feature engineering in conven-tional graph-based models (McDonald et al,2005; McDonald and Pereira, 2006; Koo andCollins, 2010).
Not only does it avoid the riskof overfitting but also it discovers useful newfeatures that have never been used in conven-tional parsers.?
We propose to exploit phrase-level informa-tion through distributed representation forphrases (phrase embeddings).
It not only en-313Figure 1: First-order and Second-order factoriza-tion strategy.
Here h stands for head word, mstands for modifier word and s stands for the sib-ling of m.ables our model to exploit richer context in-formation that previous work did not considerdue to the curse of dimension but also cap-tures inherent correlations between phrases.?
Unlike other neural network based models(Chen et al, 2014; Le and Zuidema, 2014)where an additional parser is needed for ei-ther extracting features (Chen et al, 2014) orgenerating k-best list for reranking (Le andZuidema, 2014), both training and decodingin our model are performed based on our neu-ral network architecture in an effective way.?
Our model does not impose any change tothe decoding process of conventional graph-based parsing model.
First-order, second-order and higher order models can be easilyimplemented using our model.We implement three effective models with in-creasing expressive capabilities.
The first modelis a simple first-order model that uses only atomicfeatures and does not use any combinational fea-tures.
Despite its simpleness, it outperformsconventional first-order model (McDonald et al,2005) and has a faster parsing speed.
To fur-ther strengthen our parsing model, we incorpo-rate phrase embeddings into the model, whichsignificantly improves the parsing accuracy.
Fi-nally, we extend our first-order model to a second-order model that exploits interactions between twoadjacent dependency edges as in McDonald andPereira (2006) thus further improves the modelperformance.We evaluate our models on the English PennTreebank.
Experiment results show that both ourfirst-order and second-order models outperformthe corresponding conventional models.2 Neural Network ModelA dependency tree is a rooted, directed tree span-ning the whole sentence.
Given a sentence x,graph-based models formulates the parsing pro-cess as a searching problem:y?
(x) = argmaxy?
?Y (x)Score(x, y?
(x); ?)
(1)where y?
(x) is tree with highest score, Y (x) isthe set of all trees compatible with x, ?
are modelparameters and Score(x, y?
(x); ?)
represents howlikely that a particular tree y?
(x) is the correct anal-ysis for x.
However, the size of Y (x) is expo-nential large, which makes it impractical to solveequation (1) directly.
Previous work (McDonald etal., 2005; McDonald and Pereira, 2006; Koo andCollins, 2010) assumes that the score of y?
(x) fac-tors through the scores of subgraphs c of y?
(x) sothat efficient algorithms can be designed for de-coding:Score(x, y?
(x); ?)
=?c?y?
(x)ScoreF (x, c; ?)
(2)Figure 1 gives two examples of commonly usedfactorization strategy proposed by Mcdonald et.al(2005) and Mcdonald and Pereira (2006).
Thesimplest subgraph uses a first-order factorization(McDonald et al, 2005) which decomposes a de-pendency tree into single dependency arcs (Fig-ure 1(a)).
Based on the first-order model, second-order factorization (McDonald and Pereira, 2006)(Figure 1(b)) brings sibling information into de-coding.
Specifically, a sibling part consists of atriple of indices (h,m, s) where (h,m) and (h, s)are dependencies and s andm are successive mod-ifiers to the same side of h.The most common choice for ScoreF (x, c; ?
),which is the score function for subgraph c in thetree, is a simple linear function:ScoreF (x, c; ?)
= w ?
f(x, c) (3)where f(x, c) is the feature representation of sub-graph c and w is the corresponding weight vector.However, the effectiveness of this function reliesheavily on the design of feature vector f(x, c).
Inprevious work (McDonald et al, 2005; McDonaldand Pereira, 2006), millions of hand-crafted fea-tures were used to capture context and structureinformation in the subgraph which not only lim-its the model?s ability to generalize well but onlyslows down the parsing speed.314Figure 2: Architecture of the Neural NetworkIn our work, we propose a neural networkmodel for scoring subgraph c in the tree:ScoreF (x, c; ?)
= NN(x, c) (4)where NN is our scoring function based on neu-ral network (Figure 2).
As we will show in the fol-lowing sections, it alleviates the heavy burden offeature engineering in conventional graph-basedmodels and achieves better performance by auto-matically learning useful information in the data.The effectiveness of our neural network de-pends on five key components: Feature Em-beddings, Phrase Embeddings, Direction-specifictransformation, Learning Feature Combinationsand Max-Margin Training.2.1 Feature EmbeddingsAs shown in Figure 2, part of the input to the neu-ral network is feature representation of the sub-graph.
Instead of using millions of features as inconventional models, we only use use atomic fea-tures (Chen et al, 2014) such as word unigramsand POS-tag unigrams, which are less likely to besparse.
The detailed atomic features we use willbe described in Section 3.
Unlike conventionalmodels, the atomic features in our model are trans-formed into their corresponding distributed repre-sentations (feature embeddings).The idea of distributed representation for sym-bolic data is one of the most important reasonswhy neural network works in NLP tasks.
It isshown that similar features will have similar em-beddings which capture the syntactic and seman-tic information behind features (Bengio et al,Figure 3: Illustration for phrase embeddings.
h,mand x0to x6are words in the sentence.2003; Collobert et al, 2011; Schwenk et al, 2012;Mikolov et al, 2013; Socher et al, 2013; Pei et al,2014).Formally, we have a feature dictionaryD of size|D|.
Each feature f ?
D is represented as a real-valued vector (feature embedding) Embed(f) ?Rdwhere d is the dimensionality of the vectorspace.
All feature embeddings stacking togetherforms the embedding matrix M ?
Rd?|D|.
Theembedding matrix M is initialized randomly andtrained by our model (Section 2.6).2.2 Phrase EmbeddingsContext information of word pairs1such as the de-pendency pair (h,m) has been widely believed tobe useful in graph-based models (McDonald et al,2005; McDonald and Pereira, 2006).
Given a sen-tence x, the context for h and m includes threecontext parts: prefix, infix and suffix, as illustratedin Figure 3.
We call these parts phrases in ourwork.Context representation in conventional mod-els are limited: First, phrases cannot be used asfeatures directly because of the data sparsenessproblem.
Therefore, phrases are backed off tolow-order representation such as bigrams and tri-grams.
For example, Mcdonald et.al (2005) usedtri-gram features of infix between head-modifierpair (h,m).
Sometimes even tri-grams are expen-sive to use, which is the reason why Mcdonald andPereira (2006) chose to ignore features over triplesof words in their second-order model to preventfrom exploding the size of the feature space.
Sec-1A word pair is not limited to the dependency pair (h,m).It could be any pair with particular relation (e.g., sibling pair(s,m) in Figure 1).
Figure 3 only uses (h,m) as an example.315ond, bigrams or tri-grams are lexical features thuscannot capture syntactic and semantic informationbehind phrases.
For instance, ?hit the ball?
and?kick the football?
should have similar represen-tations because they share similar syntactic struc-tures, but lexical tri-grams will fail to capture theirsimilarity.Unlike previous work, we propose to usedistributed representation (phrase embedding) ofphrases to capture phrase-level information.
Weuse a simple yet effective way to calculate phraseembeddings from word (POS-tag) embeddings.As shown in Figure 3, we average the word em-beddings in prefix, infix and suffix respectively andget three global word-phrase embeddings.
Forpairs where no prefix or suffix exists, the corre-sponding embedding is set to zero.
We also getthree global POS-phrase embeddings which arecalculated in the same way as words.
These em-beddings are then concatenated with feature em-beddings and fed to the following hidden layer.Phrase embeddings provide panorama represen-tation of the context, allowing our model to cap-ture richer context information compared with theback-off tri-gram representation.
Moreover, asa distributed representation, phrase embeddingsperform generalization over specific phrases, thusbetter capture the syntactic and semantic informa-tion than back-off tri-grams.2.3 Direction-specific TransformationIn dependency representation of sentence, theedge direction indicates which one of the words isthe head h and which one is the modifier m. Un-like previous work (McDonald et al, 2005; Mc-Donald and Pereira, 2006) that models the edgedirection as feature to be conjoined with other fea-tures, we model the edge direction with direction-specific transformation.As shown in Figure 2, the parameters in hiddenlayer (Wdh, bdh) and the output layer (Wdo, bdo) arebound with index d ?
{0, 1} which indicates thedirection between head and modifier (0 for left arcand 1 for right arc).
In this way, the model canlearn direction-specific parameters and automati-cally capture the interactions between edge direc-tion and other features.2.4 Learning Feature CombinationThe key to the success of graph-based dependencyparsing is the design of features, especially com-binational features.
Effective as these features are,as we have said in Section 1, they are prone tooverfitting and hard to design.
In our work, weintroduce a new activation function that can auto-matically learn these feature combinations.As shown in Figure 2, we first concatenate theembeddings into a single vector a.
Then a is fedinto the next layer which performs linear trans-formation followed by an element-wise activationfunction g:h = g(Wdha+ bdh) (5)Our new activation function g is defined as fol-lows:g(l) = tanh(l3+ l) (6)where l is the result of linear transformation andtanh is the hyperbolic tangent activation functionwidely used in neural networks.
We call this newactivation function tanh-cube.As we can see, without the cube term, tanh-cubewould be just the same as the conventional non-linear transformation in most neural networks.The cube extension is added to enhance the abil-ity to capture complex interactions between inputfeatures.
Intuitively, the cube term in each hid-den unit directly models feature combinations in amultiplicative way:(w1a1+ w2a2+ ...+ wnan+ b)3=?i,j,k(wiwjwk)aiajak+?i,jb(wiwj)aiaj...These feature combinations are hand-designed inconventional graph-based models but our modellearns these combinations automatically and en-codes them in the model parameters.Similar ideas were also proposed in previousworks (Socher et al, 2013; Pei et al, 2014; Chenand Manning, 2014).
Socher et.al (2013) andPei et.al (2014) used a tensor-based activationfunction to learn feature combinations.
However,tensor-based transformation is quite slow evenwith tensor factorization (Pei et al, 2014).
Chenand Manning (2014) proposed to use cube func-tion g(l) = l3which inspires our tanh-cube func-tion.
Compared with cube function, tanh-cube hasthree advantages:?
The cube function is unbounded, making theactivation output either too small or too big ifthe norm of input l is not properly controlled,especially in deep neural network.
On the316contrary, tanh-cube is bounded by the tanhfunction thus safer to use in deep neural net-work.?
Intuitively, the behavior of cube function re-sembles the ?polynomial kernel?
in SVM.In fact, SVM can be seen as a special one-hidden-layer neural network where the ker-nel function that performs non-linear trans-formation is seen as a hidden layer and sup-port vectors as hidden units.
Compared withcube function, tanh-cube combines the powerof ?kernel function?
with the tanh non-lineartransformation in neural network.?
Last but not least, as we will show in Section4, tanh-cube converges faster than the cubefunction although the rigorous proof is stillopen to investigate.2.5 Model OutputAfter the non-linear transformation of hiddenlayer, the score of the subgraph c is calculated inthe output layer using a simple linear function:ScoreF (x, c) = Wdoh+ bdo(7)The output score ScoreF (x, c) ?
R|L|is a scorevector where |L| is the number of dependencytypes and each dimension of ScoreF (x, c) is thescore for each kind of dependency type of head-modifier pair (i.e.
(h,m) in Figure 1).2.6 Max-Margin TrainingThe parameters of our model are ?
={Wdh, bdh,Wdo, bdo,M}.
All parameters areinitialized with uniform distribution within (-0.01,0.01).For model training, we use the Max-Margin cri-terion.
Given a training instance (x, y), we searchfor the dependency tree with the highest scorecomputed as equation (1) in Section 2.
The objectof Max-Margin training is that the highest scor-ing tree is the correct one: y?= y and its scorewill be larger up to a margin to other possible treey?
?
Y (x):Score(x, y; ?)
?
Score(x, y?
; ?)
+4(y, y?
)The structured margin loss4(y, y?)
is defined as:4(y, y?)
=n?j?1{h(y, xj) 6= h(y?, xj)}1-order-atomich?2.w, h?1.w, h.w, h1.w, h2.wh?2.p, h?1.p, h.p, h1.p, h2.pm?2.w, m?1.w, m.w, m1.w, m2.wm?2.p, m?1.p, m.p, m1.p, m2.pdis(h, m)1-order-phrase+ hm prefix.w, hm infix.w, hm suffix.w+ hm prefix.p, hm infix.p, hm suffix.p2-order-phrase+ s?2.w, s?1.w, s.w, s1.w, s2.w+ s?2.p, s?1.p, s.p, s1.p, s2.p+ sm infix.w, sm infix.pTable 1: Features in our three models.
w isshort for word and p for POS-tag.
h indicateshead and m indicates modifier.
The subscript rep-resents the relative position to the center word.dis(h,m) is the distance between head and modi-fier.
hm prefix, hm infix and hm suffix are phrasesfor head-modifier pair (h,m).
s indicates the sib-ling in second-order model.
sm infix is the infixphrase between sibling pair (s,m)where n is the length of sentence x, h(y, xj) is thehead (with type) for the j-th word of x in tree y and?
is a discount parameter.
The loss is proportionalto the number of word with an incorrect head andedge type in the proposed tree.
This leads to theregularized objective function for m training ex-amples:J(?)
=1mm?i=1li(?)
+?2||?||2li(?)
= maxy?
?Y (xi)(Score(xi, y?
; ?)
+4(yi, y?
))?Score(xi, yi; ?))
(8)We use the diagonal variant of AdaGrad (Duchiet al, 2011) with minibatchs (batch size = 20)to minimize the object function.
We also applydropout (Hinton et al, 2012) with 0.5 rate to thehidden layer.3 Model ImplementationBase on our Neural Network model, we presentthree model implementations with increasing ex-pressive capabilities in this section.3.1 First-order modelsWe first implement two first-order models: 1-order-atomic and 1-order-phrase.
We use theEisner (2000) algorithm for decoding.
The firsttwo rows of Table 1 list the features we use in thesetwo models.1-order-atomic only uses atomic features asshown in the first row of Table 1.
Specifically, the317ModelsDev TestSpeed (sent/s)UAS LAS UAS LASFirst-orderMSTParser-1-order 92.01 90.77 91.60 90.39 201-order-atomic-rand 92.00 90.71 91.62 90.41 551-order-atomic 92.19 90.94 92.14 90.92 551-order-phrase-rand 92.47 91.19 92.25 91.05 261-order-phrase 92.82 91.48 92.59 91.37 26Second-orderMSTParser-2-order 92.70 91.48 92.30 91.06 142-order-phrase-rand 93.39 92.10 92.99 91.79 102-order-phrase 93.57 92.29 93.29 92.13 10Third-order (Koo and Collins, 2010) 93.49 N/A 93.04 N/A N/ATable 2: Comparison with conventional graph-based models.head word and its local neighbor words that arewithin the distance of 2 are selected as the head?sword unigram features.
The modifier?s word un-igram features is extracted in the same way.
Wealso use the POS-tags of the corresponding wordfeatures and the distance between head and modi-fier as additional atomic features.We then improved 1-order-atomic to 1-order-phrase by incorporating additional phrase embed-dings.
The three phrase embeddings of head-modifier pair (h,m): hm prefix, hm infix andhm suffix are calculated as in Section 2.2.3.2 Second-order modelOur model can be easily extended to a second-order model using the second-order decoding al-gorithm (Eisner, 1996; McDonald and Pereira,2006).
The third row of Table 1 shows the addi-tional features we use in our second-order model.Sibling node and its local context are used asadditional atomic features.
We also used the in-fix embedding for the infix between sibling pair(s,m), which we call sm infix.
It is calculated inthe same way as infix between head-modifier pair(h,m) (i.e., hm infix) in Section 2.2 except thatthe word pair is now s and m. For cases where nosibling information is available, the correspondingsibling-related embeddings are set to zero vector.4 Experiments4.1 Experiment SetupWe use the English Penn Treebank (PTB) to eval-uate our model implementations and Yamada andMatsumoto (2003) head rules are used to extractdependency trees.
We follow the standard splits ofPTB3, using section 2-21 for training, section 22as development set and 23 as test set.
The StanfordPOS Tagger (Toutanova et al, 2003) with ten-wayjackknifing of the training data is used for assign-ing POS tags (accuracy ?
97.2%).Hyper-parameters of our models are tuned onthe development set and their final settings areas follows: embedding size d = 50, hidden layer(Layer 2) size = 200, regularization parameter ?
=10?4, discount parameter for margin loss ?
= 0.3,initial learning rate of AdaGrad alpha = 0.1.4.2 Experiment ResultsTable 2 compares our models with several conven-tional graph-based parsers.
We use MSTParser2for conventional first-order model (McDonald etal., 2005) and second-order model (McDonald andPereira, 2006).
We also include the result of athird-order model of Koo and Collins (2010) forcomparison3.
For our models, we report the resultswith and without unsupervised pre-training.
Pre-training only trains the word-based feature embed-dings on Gigaword corpus (Graff et al, 2003) us-ing word2vec4and all other parameters are stillinitialized randomly.
In all experiments, we re-port unlabeled attachment scores (UAS) and la-beled attachment scores (LAS) and punctuation5is excluded in all evaluation metrics.
The parsingspeeds are measured on a workstation with IntelXeon 3.4GHz CPU and 32GB RAM.As we can see, even with random initialization,1-order-atomic-rand performs as well as conven-tional first-order model and both 1-order-phrase-2http://sourceforge.net/projects/mstparser3Note that Koo and Collins (2010)?s third-order modeland our models are not strict comparable since their modelis an unlabeled model.4https://code.google.com/p/word2vec/5Following previous work, a token is a punctuation if itsPOS tag is {?
?
: , .
}318Figure 4: Convergence curve for tanh-cube andcube activation function.rand and 2-order-phrase-rand perform betterthan conventional models in MSTParser.
Pre-training further improves the performance of allthree models, which is consistent with the conclu-sion of previous work (Pei et al, 2014; Chen andManning, 2014).
Moreover, 1-order-phrase per-forms better than 1-order-atomic, which showsthat phrase embeddings do improve the model.
2-order-phrase further improves the performancebecause of the more expressive second-order fac-torization.
All three models perform significantlybetter than their counterparts in MSTParser wheremillions of features are used and 1-order-phraseworks surprisingly well that it even beats the con-ventional second-order model.With regard to parsing speed, 1-order-atomicis the fastest while other two models have similarspeeds as MSTParser.
Further speed up could beachieved by using pre-computing strategy as men-tioned in Chen and Manning (2014).
We did nottry this strategy since parsing speed is not the mainfocus of this paper.Model tanh-cube cube tanh1-order-atomic 92.19 91.97 91.731-order-phrase 92.82 92.25 92.132-order-phrase 93.57 92.95 92.91Table 3: Model Performance of different activa-tion functions.We also investigated the effect of different acti-vation functions.
We trained our models with thesame configuration except for the activation func-tion.
Table 3 lists the UAS of three models on de-velopment set.Feature Type Instance NeighboorsWords(word2vec)inthe, of, and,for, fromhishimself, her, he,him, fatherwhichits, essentially,similar, that, alsoWords(Our model)inon, at, behind,among, duringhisher, my, their,its, hewhichwhere, who, whom,whose, thoughPOS-tagsNNNNPS, NNS, EX,NNP, POSJJJJR, JJS, PDT,RBR, RBSTable 4: Examples of similar words and POS-tagsaccording to feature embeddings.As we can see, tanh-cube function outperformscube function because of advantages we men-tioned in Section 2.4.
Moreover, both tanh-cubefunction and cube function performs better thantanh function.
The reason is that the cube term cancapture more interactions between input features.We also plot the UAS of 2-order-phrase dur-ing each iteration of training.
As shown in Figure4, tanh-cube function converges faster than cubefunction.4.3 Qualitative AnalysisIn order to see why our models work, we madequalitative analysis on different aspects of ourmodel.Ability of Feature AbstractionFeature embeddings give our model the ability offeature abstraction.
They capture the inherent cor-relations between features so that syntactic similarfeatures will have similar representations, whichmakes our model generalizes well on unseen data.Table 4 shows the effect of different featureembeddings which are obtained from 2-order-phrase after training.
For each kind of featuretype, we list several features as well as top 5 fea-tures that are nearest (measured by Euclidean dis-tance) to the corresponding feature according totheir embeddings.We first analysis the effect of word embeddingsafter training.
For comparison, we also list theinitial word embeddings in word2vec.
As wecan see, in word2vec word embeddings, wordsthat are similar to in and which tends to be those319Phrase NeighboorOn a Saturday morningOn Monday night footballOn SundayOn SaturdayOn Tuesday afternoonOn recent Saturday morningmost of itof itof it allsome of it alsomost of these areonly some ofbig investment bankgreat investment bankbank investmententire equity investmentanother cash equity investorreal estate lending divisionTable 5: Examples of similar phrases according tophrase embeddings.co-occuring with them and for word his, similarwords are morphologies of he.
On the contrary,similar words measured by our embeddings havesimilar syntactic functions.
This is helpful for de-pendency parsing since parsing models care moreabout the syntactic functions of words rather thantheir collocations or morphologies.POS-tag embeddings also show similar behav-ior with word embeddings.
As shown in Table 4,our model captures similarities between POS-tagseven though their embeddings are initialized ran-domly.We also investigated the effect of phrase embed-dings in the same way as feature embeddings.
Ta-ble 5 lists the examples of similar phrases.
Ourphrase embeddings work pretty well given thatonly a simple averaging strategy is used.
Phrasesthat are close to each other tend to share simi-lar syntactic and semantic information.
By usingphrase embeddings, our model sees panorama ofthe context rather than limited word tri-grams andthus captures richer context information, which isthe reason why phrase embeddings significantlyimprove the performance.Ability of Feature LearningFinally, we try to unveil the mysterious hiddenlayer and investigate what features it learns.
Foreach hidden unit of 2-order-phrase, we get itsconnections with embeddings (i.e., Wdhin Figure2) and pick the connections whose weights haveabsolute value > 0.1.
We sampled several hiddenunits and invenstigated which features their highlyweighted connections belong to:?
Hidden 1: h.w, m.w, h?1.w, m1.w?
Hidden 2: h.p, m.p, s.p?
Hidden 3: hm infix.p, hm infix.w, hm prefix.w?
Hidden 4: hm infix.w, hm prefix.w, sm infix.w?
Hidden 5: hm infix.p, hm infix.w, hm suffix.wThe samples above give qualitative results of whatfeatures the hidden layer learns:?
Hidden unit 1 and 2 show that atomic featuresof head, modifier, sibling and their local con-text words are useful in our model, which isconsistent with our expectations since thesefeatures are also very important features inconventional graph-based models (McDon-ald and Pereira, 2006).?
Features in the same hidden unit will ?com-bine?
with each other through our tanh-cubeactivation function.
As we can see, featurecombination in hidden unit 2 were also usedin Mcdonald and Pereira (2006).
However,these feature combinations are automaticallycaptured by our model without the labor-intensive feature engineering.?
Hidden unit 3 to 5 show that phrase-levelinformation like hm prefix, hm suffix andsm infix are effective in our model.
Thesefeatures are not used in conventional second-order model (McDonald and Pereira, 2006)because they could explode the feature space.Through our tanh-cube activation function,our model further captures the interactionsbetween phrases and other features withoutthe concern of overfitting.5 Related WorkModels for dependency parsing have been stud-ied with considerable effort in the NLP commu-nity.
Among them, we only focus on the graph-based models here.
Most previous systems ad-dress this task by using linear statistical modelswith carefully designed context and structure fea-tures.
The types of features available rely on treefactorization and decoding algorithm.
Mcdonaldet.al (2005) proposed the first-order model whichis also know as arc-factored model.
Efficient de-coding can be performed with Eisner (2000) algo-rithm in O(n3) time and O(n2) space.
Mcdonaldand Pereira (2006) further extend the first-ordermodel to second-order model where sibling infor-mation is available during decoding.
Eisner (2000)320algorithm can be modified trivially for second-order decoding.
Carreras (2007) proposed a morepowerful second-order model that can score bothsibling and grandchild parts with the cost ofO(n4)time and O(n3) space.
To exploit more struc-ture information, Koo and Collins (2010) pro-posed three third-order models with computationalrequirements of O(n4) time and O(n3) space.Recently, neural network models have been in-creasingly focused on for their ability to minimizethe effort in feature engineering.
Chen et.al (2014)proposed an approach to automatically learningfeature embeddings for graph-based dependencyparsing.
The learned feature embeddings are usedas additional features in conventional graph-basedmodel.
Le and Zuidema (2014) proprosed aninfinite-order model based on recursive neural net-work.
However, their model can only be used asan reranking model since decoding is intractable.Compared with these work, our model is ageneral and standalone neural network model.Both training and decoding in our model are per-formed based on our neural network architecturein an effective way.
Although only first-orderand second-order models are implemented in ourwork, higher-order graph-based models can beeasily implemented using our model.6 ConclusionIn this paper, we propose a general and effec-tive neural network model that can automaticallylearn feature combinations with our novel acti-vation function.
Moreover, we introduce a sim-ple yet effect way to utilize phrase-level informa-tion, which greatly improves the model perfor-mance.
Experiments on the benchmark datasetshow that our model achieves better results thanconventional models.AcknowledgmentsThis work is supported by National NaturalScience Foundation of China under Grant No.61273318 and National Key Basic Research Pro-gram of China 2014CB340504.
We want to thankMiaohong Chen and Pingping Huang for theirvaluable comments on the initial idea and helpingpre-process the data.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In EMNLP-CoNLL, pages 957?961.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 740?750, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Wenliang Chen, Yue Zhang, and Min Zhang.
2014.Feature embedding for dependency parsing.
In Pro-ceedings of COLING 2014, the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 816?826, Dublin, Ireland, August.Dublin City University and Association for Compu-tational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 999999:2121?2159.Jason M Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Pro-ceedings of the 16th conference on Computationallinguistics-Volume 1, pages 340?345.
Associationfor Computational Linguistics.Jason Eisner.
2000.
Bilexical grammars and theircubic-time parsing algorithms.
In Advances in prob-abilistic and other parsing technologies, pages 29?61.
Springer.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2003.
English gigaword.
Linguistic DataConsortium, Philadelphia.Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1?11.
Association forComputational Linguistics.321Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 729?739, Doha, Qatar,October.
Association for Computational Linguistics.Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In EACL.
Citeseer.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd an-nual meeting on association for computational lin-guistics, pages 91?98.
Association for Computa-tional Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 293?303, Bal-timore, Maryland, June.
Association for Computa-tional Linguistics.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous spacelanguage models on a gpu for statistical machinetranslation.
In Proceedings of the NAACL-HLT 2012Workshop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling forHLT, pages 11?19.
Association for ComputationalLinguistics.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1631?1642, Seattle, Washington, USA,October.
Association for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, volume 3, pages195?206.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.322
