CUCWeb: a Catalan corpus built from the WebG.
Boleda1 S. Bott1 R. Meza2 C. Castillo2 T. Badia1 V. Lo?pez21Grup de Lingu??
?stica Computacional2Ca?tedra Telefo?nica de Produccio?n MultimediaFundacio?
Barcelona MediaUniversitat Pompeu FabraBarcelona, Spain{gemma.boleda,stefan.bott,rodrigo.meza}@upf.edu{carlos.castillo,toni.badia,vicente.lopez}@upf.eduAbstractThis paper presents CUCWeb, a 166 mil-lion word corpus for Catalan built bycrawling the Web.
The corpus has beenannotated with NLP tools and made avail-able to language users through a flexibleweb interface.
The developed architectureis quite general, so that it can be used tocreate corpora for other languages.1 IntroductionCUCWeb is the outcome of the common interestof two groups, a Computational Linguistics groupand a Computer Science group interested on Webstudies.
It fits into a larger project, The Span-ish Web Project, aimed at empirically studying theproperties of the Spanish Web (Baeza-Yates et al,2005).
The project set up an architecture to re-trieve a portion of the Web roughly correspond-ing to the Web in Spain, in order to study its for-mal properties (analysing its link distribution as agraph) and its characteristics in terms of pages,sites, and domains (size, kind of software used,language, among other aspects).One of the by-products of the project is a 166million word corpus for Catalan.1 The biggestannotated Catalan corpus before CUCWeb is theCTILC corpus (Rafel, 1994), consisting of about50 million words.In recent years, the Web has been increasinglyused as a source of linguistic data (Kilgarriff andGrefenstette, 2003).
The most straightforward ap-proach to using the Web as corpus is to gather dataonline (Grefenstette, 1998), or estimate counts1Catalan is a relatively minor language.
There arecurrently about 10.8 million Catalan speakers, similarto Serbian (12), Greek (10.2), or Swedish (9.3).
Seehttp://www.upc.es/slt/alatac/cat/dades/catala-04.html(Keller and Lapata, 2003) using available searchengines.
This approach has a number of draw-backs, e.g.
the data one looks for has to be knownbeforehand, and the queries have to consist of lex-ical material.
In other words, it is not possibleto perform structural searches or proper languagemodeling.Current technology makes it feasible and rela-tively cheap to crawl and store terabytes of data.In addition, crawling the data and processing itoff-line provides more potential for its exploita-tion, as well as more control over the data se-lection and pruning processes.
However, this ap-proach is more challenging from a technologicalviewpoint.
2 For a comprehensive discussion ofthe pros and cons of the different approaches tousing Web data for linguistic purposes, see e.g.Thelwall (2005) and Lu?deling et al (To appear).We chose the second approach because of the ad-vantages discussed in this section, and because itallowed us to make the data available for a largenumber of non-specialised users, through a webinterface to the corpus.
We built a general-purposecorpus by crawling the Spanish Web, processingand filtering them with language-intensive tools,filtering duplicates and ranking them according topopularity.The paper has the following structure: Sec-tion 2 details the process that lead to the consti-tution of the corpus, Section 3 explores some ofthe exploitation possibilities that are foreseen forCUCWeb, and Section 4 discusses the current ar-chitecture.
Finally, Section 5 contains some con-clusions and future work.2The WaCky project (http://wacky.sslmit.unibo.it/) aimsat overcoming this challenge, by developing ?a set of tools(and interfaces to existing tools) that will allow a linguist tocrawl a section of the web, process the data, index them andsearch them?.192 Corpus Constitution2.1 Data collectionOur goal was to crawl the portion of the Web re-lated to Spain.
Initially, we crawled the set ofpages with the suffix .es.
However, this domainis not very popular, because it is more expensivethan other domains (e.g.
the cost of a .com do-main is about 15% of that of an .es domain), andbecause its use is restricted to company names orregistered trade marks.3 In a second phase a dif-ferent heuristic was used, and we considered thata Web site was in Spain if either its IP address wasassigned to a network located in Spanish land, or ifthe Web site?s suffix was .es.
We found that only16% of the domains with pages in Spain were un-der .es.The final collection of the data was carriedout in September and October 2004, using acommercial piece of software by Akwan (da Silvaet al, 1999).
4 The actual collection was startedby the crawler using as a seed the list of URLs in aSpanish search engine ?which was a commercialsearch engine back in 2000?
under the name ofBuscopio.
That list covered the major part of theexisting Web in Spain at that time.
5.
New URLswere extracted from the downloaded pages, andthe process continued recursively while the pageswere in Spain ?see above.
The crawler down-loaded all pages, except those that had an identicalURL (http://www.web.es/main/ andhttp://www.web.es/main/index.htmlwere considered different URLs).
We retrievedover 16 million Web pages (corresponding toover 300,000 web sites and 118,000 domains),and processed them to extract links and text.
Theuncompressed text of the pages amounts to 46GB, and the metadata generated during the crawlto 3 GB.In an initial collection process, a number of dif-ficulties in the characterisation of the Web of Spainwere identified, which lead to redundancy in thecontents of the collection:Parameters to a program inside URL addresses.This makes it impossible to adequately sep-3In the case of Catalan, additionally, there is a politicaland cultural opposition to the .es domain.4We used a PC with two Intel-4 processors running at 3GHz and with 1.6 GB of RAM under Red-Hat Linux.
Forthe information storage we used a RAID of disks with 1.8 TBof total capacity, although the space used by the collection isabout 50 GB.5http://www.buscopio.netarate static and dynamic pages, and maylead to repeatedly crawl pages with the samecontent.Mirrors (geographically distributed copies of thesame contents to ensure network efficiency).Normally, these replicas are entire collectionswith a large volume, so that there are manysites with the same contents, and these areusually large sites.
The replicated informa-tion is estimated between 20% and 40% ofthe total Web contents ((Baeza-Yates et al,2005)).Spam on the Web (actions oriented to deceivesearch engines and to give to some pages ahigher ranking than they deserve in search re-sults).
Recognizing spam pages is an activeresearch area, and it is estimated that over 8%of what is indexed by search engines is spam(Fetterly et al, 2004).
One of the strategiesthat induces redundancy is to automaticallygenerate pages to improve the score they ob-tain in link-based rankings algorithms.DNS wildcarding (domain name spamming).Some link analysis ranking functions assignless importance to links between pages inthe same Web site.
Unfortunately, this hasmotivated spammers to use several differentWeb sites for the same contents, usuallythrough configuring DNS servers to assignhundreds or thousands of site names tothe same IP address.
Spain?s Web seemsto be quite populated with domain namespammers: 24 out of the 30 domains with thehighest number of Web sites are configuredwith DNS wildcarding (Baeza-Yates et al,2005).Most of the spam pages were under the .comtop-level domain.
We manually checked the do-mains with the largest number of sites and pages toban a list of them, mostly sites containing pornog-raphy or collections of links without informationcontent.
This is not a perfect solution againstspam, but generates significant savings in termsof bandwidth and storage, and allows us to spendmore resources in content-rich Web sites.
We alsorestricted the crawler to download a maximum of400 pages per site, except for the Web sites within.es, that had no pre-established limit.20Documents (%) Words (%)Language classifier 491,850 100 375,469,518 100Dictionary filter 277,577 56.5 222,363,299 59Duplicate detector 204,238 41.5 166,040,067 44Table 1: Size of the Catalan corpus2.2 Data processingThe processing of the data to obtain the Catalancorpus consisted of the following steps: languageclassification, linguistic filtering and processing,duplicate filtering and corpus indexing.
This sec-tion details each of these aspects.We built a language classifier with the NaiveBayes classifier of the Bow system (Mccallum,1996).
The system was trained with corpora cor-responding to the 4 official languages in Spain(Spanish, Catalan, Galician and Basque), as wellas to the other 6 most frequent languages inthe Web (Anonymous, 2000): English, German,French, Italian, Portuguese, and Dutch.38% of the collection could not be reliably clas-sified, mostly because of the presence of pageswithout enough text, for instance, pages contain-ing only images or only lists of proper nouns.Within the classified pages, Catalan was the thirdmost used language (8% of the collection).
Asexpected, most of the collection was in Spanish(52%), but English had a large part (31%).
Thecontents in Galician and Basque only compriseabout 2% of the pages.We wanted to use the Catalan portion as a cor-pus for NLP and linguistic studies.
We were notinterested in full coverage of Web data, but inquality.
Therefore, we filtered it using a compu-tational dictionary and some heuristics in order toexclude documents with little linguistic relevance(e.g.
address lists) or with a lot of noise (program-ming code, multilingual documents).
In addition,we performed a simple duplicate filter: web pageswith a very similar content (determined by a hashof the processed text) were considered duplicates.The sizes of the corpus (in documents andwords6) after each of the processes are depicted inTable 1.
Note that the two filtering processes dis-card almost 60% of the original documents.
Thefinal corpus consists of 166 million words from204 thousand documents.Its distribution in terms of top-level domains isshown in Table 2, and the 10 biggest sites in Ta-6Word counts do not include punctuation marks.ble 3.
Note that the .es domain covers almosthalf of the pages and com a quarter, but .org and.net alo have a quite large share of the pages.As for the biggest sites, they give an idea of thecontent of CUCWeb: they mainly correspond touniversity and institutional sites.
A similar dis-tribution can be observed for the 50 biggest sites,which will determine the kind of language foundin CUCWeb.Documents (%)es 89,541 44.6com 49,146 24.5org 35,528 17.7net 18,819 9.4info 5,005 2.5edu 688 0.3others 2,042 1.4Table 2: Domain distribution in CUCWebThe corpus was further processed with CatCG( `Alex Alsina et al, 2002), a POS-tagger and shal-low parser for Catalan built with the ConnexorConstraint Grammar formalism and tools.7 CatCGprovides part of speech, morphological features(gender, number, tense, etc.)
and syntactic infor-mation.
The syntactic information is a functionaltag (e.g.
subject, object, main verb) annotated atword level.Since we wanted the corpus not only to be anin-house resource for NLP purposes, but also tobe accessible to a large number of users.
To thatend, we indexed it using the IMS Corpus Work-bench tools8 and we built a web interface to it (seeSection 3.1).
The CWB includes facilities for in-dexing and searching corpora, as well as a specialmodule for web interfaces.
However, the size ofthe corpus is above the advisable limit for thesetools.
9 Therefore, we divided it into 4 subcorpora7http://www.connexor.com/8http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/9According to Stefan Evert ?personal communication?, ifa corpus has to be split into several parts, a good rule of thumbis to split it in 100M word parts.
In his words ?depending onvarious factors such as language, complexity of annotations21Site Description Documentsupc.es University 1574gencat.es Institution 1372publicacions.bcn.es Institution 1282uab.es University 1190revista.consumer.es Company 1132upf.es University 1076nil.fut.es Distribution lists 1045conc.es Insitution 1033uib.es University 977ajtarragona.es Institution 956Table 3: 10 biggest sites in CUCWeband indexed each of them separately.
The searchengine for the corpus is the CQP (Corpus QueryProcessor, one of the modules of the CWB).Since CQP provides sequential access to doc-uments we ordered the corpus documents byPageRank so that they are retrieved according totheir popularity on the Internet.3 Corpus ExploitationCUCWeb is being exploited in two ways: on theone hand, data can be accessed through a webinterface (Section 3.1).
On the other hand, theannotated data can be exploited by theoretical orcomputational linguists, lexicographers, transla-tors, etc.
(Section 3.2).3.1 Corpus interfaceDespite the wide use of corpora in NLP, few in-terfaces have been built, and still fewer are flex-ible enough to be of interest to linguistic re-searchers.
As for Web data, some initiatives ex-ist (WebCorp 10, the Linguist?s Search Engine 11,KWiCFinder 12), but they are meta-interfaces tosearch engines.
For Catalan, there is a web inter-face for the CTILC corpus13, but it only allows forone word searches, of which a maximum of 50 hitsare viewed.
It is not possible either to downloadsearch results.From the beginning of the project our aim wasto create a corpus which could be useful for boththe NLP community and for a more general au-dience with an interest in the Catalan language.and how much RAM you have, a larger or smaller size maygive better overall performance.
?.10http://www.webcorp.org.uk/11http://lse.umiacs.umd.edu12http://miniappolis.com/KWiCFinder13http://pdl.iec.esThis includes linguists, lexicographers and lan-guage teachers.We expected the latter kind of user not to be fa-miliar with corpus searching strategies and corpusinterfaces, at least not to a large extent.
Therefore,we aimed at creating a user-friendly web interfacewhich should be useful for both non-trained andexperienced users.14 Further on, we wanted theinterface to support not only example searches butalso statistical information, such as co-occurrencefrequency, of use in lexicographical work and po-tentially also in language teaching or learning.There are two web interfaces to the corpus:an example search interface and a statistics inter-face.
Furthermore, since the flexibility and expres-siveness of the searches potentially conflicts withuser-friendliness, we decided to divide the exam-ple search interface into two modalities: a simplesearch mode and an expert search mode.The simple mode allows for searches of words,lemmata or word strings.
The search can be re-stricted to specific parts of speech or syntacticfunctions.
For instance, a user can search foran ambiguous word like Catalan ?la?
(masculinenoun, or feminine determiner or personal pro-noun) and restrict the search to pronouns.
Or lookfor word ?traduccions?
(?translations?)
function-ing as subject.
The advantage of the simple modeis that an untrained person can use the corpus al-most without the need to read instructions.
If newusers find it useful to use CUCWeb, we expect thatthe motivation to learn how to create advanced cor-pus queries will arise.The expert mode is somewhat more complexbut very flexible.
A string of up to 5 word unitscan be searched, where each unit may be a word14http://www.catedratelefonica.upf.es/cucweb22form, lemma, part of speech, syntactic function orcombination of any of those.
If a part of speechis specified, further morphological information isdisplayed, which can also be queried.Each word unit can be marked as optional orrepeated, which corresponds to the Boolean op-erators of repetition and optionality.
Within eachword unit each information field may be negated,allowing for exclusions in searches, e.g.
requiringa unit not to be a noun or not corresponding to acertain lemma.
This use of operators gives the ex-pert mode an expressiveness close to regular gram-mars, and exploits almost all querying functional-ities of CQP ?the search engine.In both modes, the user can retrieve up to 1000examples, which can be viewed online or down-loaded as a text file, and with different contextsizes.
In addition, a link to a cache copy of thedocument and to its original location is provided.As for the statistics interface, it searches forfrequency information regarding the query of theuser.
The frequency can be related to any of the4 annotation levels (word, lemma, POS, function).For example, it is possible to search for a givenverb lemma and get the frequencies of each verbform, or to look for adjectives modifying the worddona (?woman?)
and obtain the list of lemmatawith their associated frequency.
The results areoffered as a table with absolute and relative fre-quency, and they can be viewed online or retrievedas a CSV file.
In addition, each of the results hasan associated link to the actual examples in thecorpus.The interface is technically quite complex, andthe corpus quite large.
There are still aspects tobe solved both in the implementation and the doc-umentation of the interface.
Even restricting thesearches to 1000 hits, efficiency remains often aproblem in the example search mode, and moreso in the statistics interface.
Two partial solutionshave been adopted so far: first, to divide the cor-pus into 4 subcorpora, as explained in Section 2.2,so that parallel searches can be performed and thusthe search engine is not as often overloaded.
Sec-ond, to limit the amount of memory and time for agiven query.
In the statistics interface, a status barshows the progress of the query in percentage andthe time left.The interface does not offer the full range ofCWB/CQP functionalities, mainly because it wasnot demanded by our ?known?
users (most of themlinguists and translators from the Department ofTranslation and Philology at Universitat PompeuFabra).
However it is planned to increasingly addnew features and functionalities.
Up to now we didnot detect any incompatibility between splittingthe corpora and the implementation of CWB/CQPdeployment or querying functionalities.3.2 Whole datasetThe annotated corpus can be used as a source ofdata for NLP purposes.
A previous version of theCUCWeb corpus ?obtained with the methodologydescribed in this paper, but crawling only the .esdomain, consisting of 180 million words?
has al-ready been exploited in a lexical acquisition task,aimed at classifying Catalan verbs into syntacticclasses (Mayol et al, 2006).Cluster analysis was applied to a 200 verb set,modeled in terms of 10 linguistically defined fea-tures.
The data for the clustering were first ex-tracted from a fragment of CTILC (14 millionword).
Using the manual tagging of the corpus, anaverage 0.84 f-score was obtained.
Using CatCG,the performance decreased only 2 points (0.82 f-score).In a subsequent experiment, the data were ex-tracted from the CUCWeb corpus.
Given that itis 12 times larger than the traditional corpus, thequestion was whether ?more data is better data?
(Church and Mercer, 1993, 18-19).
Banko andBrill (2001) present a case study on confusion setdisambiguation that supports this slogan.
Surpris-ingly enough, results using CUCWeb were sig-nificantly worse than those using the traditionalcorpus, even with automatic linguistic processing:CUCWeb lead to an average 0.71 f-score, so an 11point difference resulted.
These results somewhatquestion the quality of the CUCWeb corpus, par-ticularly so as the authors attribute the differenceto noise in the CUCWeb and difficulties in linguis-tic processing (see Section 4).
However, 0.71 isstill well beyond the 0.33 f-score baseline, so thatour analysis is that CUCWeb can be successfullyused in lexical acquisition tasks.
Improvement inboth filtering and linguistic processing is still amust, though.4 Discussion of the architectureThe initial motivation for the CUCWeb projectwas to obtain a large annotated corpus for Catalan.However, we set up an architecture that enables23Figure 1: Architecture for building Web corporathe construction of web corpora in general, pro-vided the language-dependent modules are avail-able.
Figure 1 shows the current architecture forCUCWeb.The language-dependent modules are the lan-guage classifier (our classifier now covers 10 lan-guages, as explained in Section 2.2) and the lin-guistic processing tools.
In addition, the web inter-face has to be adapted for each new tagset, pieceof information and linguistic level.
For instance,the interface currently does not support searchesfor chunks or phrases.Most of the problems we have encountered inprocessing Web documents are not new (Baroniand Ueyama, To appear), but they are much morefrequent in that kind of documents than in standardrunning text.15 We now review the main problemswe came across:Textual layout In general, they are problemsthat arise due to the layout of Web documents,which is very different to that of standard text.
Pre-processing tools have to be adapted to deal withthese elements.
These include headers or footers(Last modified...), copyright statements or frameelements, the so-called boilerplates.
Currently,due to the fact that we process the text extracted bythe crawler, no boilerplate detection is performed,which increases the amount of noise in the cor-pus.
Moreover, the pre-processing module doesnot even handle e-mail addresses or phone num-bers (they are not frequently found in the kind of15By ?standard text?, we mean edited pieces of text, suchas newspapers, novels, encyclopedia, or technical manuals.24text it was designed to process); as a result, forexample, one of the most frequent determiners inthe corpus is 93, the phone prefix for Barcelona.Another problem for the pre-processing module,again due to the fact that we process the text ex-tracted from the HTML markup, is that most of thestructural information is lost and many segmenta-tion errors occur, errors that carry over to subse-quent modules.Spelling mistakes Most of the texts publishedon the Web are only edited once, by their au-thor, and are neither reviewed nor corrected, as isusually the case in traditional textual collections(Baeza-Yates et al, 2005).
It could be arguedthat this makes the language on the Web closerto the ?actual language?, or at least representativeof other varieties in contrast to traditional corpora.However, this feature makes Web documents diffi-cult to process for NLP purposes, due to the largequantity of spelling mistakes of all kinds.
TheHTML support itself causes some of the difficul-ties that are not exactly spelling mistakes: A par-ticularly frequent kind of problem we have foundis that the first letter of a word gets segmentedfrom the rest of the word, mainly due to formattingeffects.
Automatic spelling correction is a morenecessary module in the case of Web data.Multilinguality Multilinguality is also not anew issue (there are indeed multilingual books orjournals), but is one that becomes much more ev-ident when handling Web documents.
Our cur-rent approach, given that we are not interested infull coverage, but in quality, is to discard multi-lingual documents (through the language classifierand the linguistic filter).
This causes two prob-lems.
On the one hand, potentially useful textsare lost, if they are inserted in multilingual doc-uments (note that the linguistic filter reduces theinitial collection to almost a half; see Table 1).
Onthe other hand, many multilingual documents re-main in the corpus, because the amount of textin another language does not reach the specifiedthreshold.
Due to the sociological context of Cata-lan, Spanish-Catalan documents are particularlyfrequent, and this can cause trouble in e.g.
lexicalacquisition tasks, because both are Romance lan-guages and some word forms coincide.
Currently,both the language classifier and the dictionary fil-ter are document-based, not sentence-based.
Abetter approach would be to do sentence-basedlanguage classification.
However, this would in-crease the complexity of corpus construction andmanagement: If we want to maintain the notionof document, pieces in other languages have to bemarked but not removed.
Ideally, they should alsobe tagged and subsequently made searchable.Duplicates Finally, a problem which is indeedparticular to the Web is redundancy.
Despite allefforts in avoiding duplicates during the crawl-ing and in detecting them in the collection (seeSection 2), there is still quite a lot of dupli-cates or near-duplicates in the corpus.
This is aproblem both for NLP purposes and for corpusquerying.
More sophisticated algorithms, as inBroder (2000), are needed to improve duplicatedetection.5 Conclusions and future workWe have presented CUCWeb, a project aimed atobtaining a large Catalan corpus from the Web andmaking it available for all language users.
As anexisting resource, it is possible to enhance it andmodify it, with e.g.
better filters, better duplicatedetectors, or better NLP tools.
Having an actualcorpus stored and annotated also makes it possibleto explore it, be it through the web interface or asa dataset.The first CUCWeb version (from data gatheringto linguistic processing and web interface imple-mentation) was developed in only 6 months, withpartial dedication of a a team of 6 people.
Sincethen, many improvements have taken place, andmany more remain as a challenge, but it confirmsthat creating a 166 million word annotated corpus,given the current technological state of the art, is arelatively easy and cheap issue.Resources such as CUCWeb facilitate the tech-nological development of non-major languagesand quantitative linguistic research, particularly soif flexible web interfaces are implemented.
In ad-dition, they make it possible for NLP and Webstudies to converge, opening new fields of research(e.g.
sociolinguistic studies of the Web).We have argued that the developed architectureallows for the creation of Web corpora in general.In fact, in the near future we plan to build a Span-ish Web corpus and integrate it into the same webinterface, using the data already gathered.
TheSpanish corpus, however, will be much larger thanthe Catalan one (a conservative estimate is 60025million words), so that new challenges in process-ing and searching it will arise.We have also reviewed some of the challengesthat Web data pose to existing NLP tools, and ar-gued that most are not new (textual layout, mis-spellings, multilinguality), but more frequent onthe Web.
To address some of them, we plan to de-velop a more sophisticated pre-processing moduleand a sentence-based language classifier and filter.A more general challenge of Web corpora is thecontrol over its contents.
Unlike traditional cor-pora, where the origin of each text is clear anddeliberate, in CUCWeb the strategy is to gatheras much text as possible, provided it meets somequality heuristics.
The notion of balance is notpresent anymore, although this needs not be adrawback (Web corpora are at least representa-tive of the language on the Web).
However, whatis arguably a drawback is the black box effectof the corpus, because the impact of text genre,topic, and so on cannot be taken into account.It would require a text classification procedure toknow what the collected corpus contains, and thisis again a meeting point for Web studies and NLP.AcknowledgementsMar?
?a Eugenia Fuenmayor and Paulo Golgher managed theWeb crawler during the downloading process.
The languageclassifier was developed by Ba?rbara Poblete.
The corporaused to train the language detection module were kindlyprovided by Universita?t Gesamthochschule, Paderborn (Ger-man), by the Institut d?Estudis Catalans, Barcelona (Catalan),by the TALP group, Universitat Polite`cnica de Catalunya(Spanish), by the IXA Group, Euskal Herriko Unibertsitatea(Basque), by the Centre de Traitement Automatique du Lan-gage de l?UCL, Leuven (French, Dutch and Portuguese),by the Seminario de Lingu??
?stica Informa?tica, Universidadede Vigo (Galician) and by the Istituto di Linguistica Com-putazionale, Pisa (Italian).
We thank Mart??
Quixal for hisrevision of a previous version of this paper and three anony-mous reviewers for useful criticism.This project has been partially funded by Ca?tedraTelefo?nica de Produccio?n Multimedia.References`Alex Alsina, Toni Badia, Gemma Boleda, Stefan Bott,`Angel Gil, Mart??
Quixal, and Oriol Valent??n.
2002.CATCG: a general purpose parsing tool applied.
InProceedings of Third International Conference onLanguage Resources and Evaluation, Las Palmas,Spain.Anonymous.
2000.
1.6 billion served: the Web ac-cording to Google.
Wired, 8(12):18?19.Ricardo Baeza-Yates, Carlos Castillo, and VicenteLo?pez.
2005.
Characteristics of the Web of Spain.Cybermetrics, 9(1).Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Association for Computational Linguistics,pages 26?33.Marco Baroni and Motoko Ueyama.
To appear.
Build-ing general- and special-purpose corpora by webcrawling.
In Proceedings of the NIJL InternationalWorkshop on Language Corpora.Andrei Z. Broder.
2000.
Identifying and filteringnear-duplicate documents.
In Combinatorial Pat-tern Matching, 11th Annual Symposium, pages 1?10, Montreal, Canada.Kenneth W. Church and Robert L. Mercer.
1993.
In-troduction to the special issue on computational lin-guistics using large corpora.
Computational Lin-guistics, 19(1):1?24.Altigran da Silva, Eveline Veloso, Paulo Golgher, Al-berto Laender, and Nivio Ziviani.
1999.
Cobweb -a crawler for the brazilian web.
In String Processingand Information Retrieval (SPIRE), pages 184?191,Cancun, Mexico.
IEEE CS Press.Dennis Fetterly, Mark Manasse, and Marc Najork.2004.
Spam, damn spam, and statistics: Using sta-tistical analysis to locate spam web pages.
In Sev-enth workshop on the Web and databases (WebDB),Paris, France.Gregory Grefenstette.
1998.
The World Wide Webas a resource for example-based machine translationtasks.
In ASLIB Conference on Translating and theComputer, volume 21, London, England.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29:459?484.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the Web as corpus.Computational Linguistics, 29(3):333?347.Anke Lu?deling, Stefan Evert, and Marco Baroni.
Toappear.
Using web data for linguistic purposes.
InMarianne Hundt, Caroline Biewer, and Nadja Nes-selhauf, editors, Corpus Linguistics and the Web.Rodopi, Amsterdam.Laia Mayol, Gemma Boleda, and Toni Badia.
2006.Automatic acquisition of syntactic verb classes withbasic resources.
Submitted.Andrew K. Mccallum.
1996.
Bow: Atoolkit for statistical language modeling,text retrieval, classification and clustering.<http://www.cs.cmu.edu/?mccallum/bow/>.Joaquim Rafel.
1994.
Un corpus general de refere`nciade la llengua catalana.
Caplletra, 17:219?250.Mike Thelwall.
2005.
Creating and using web cor-pora.
International Journal of Corpus Linguistics,10(4):517?541.26
