Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 38?46,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsConsistency Checking for Treebank AlignmentMarkus DickinsonIndiana Universitymd7@indiana.eduYvonne SamuelssonStockholm Universityyvonne.samuelsson@ling.su.seAbstractThis paper explores ways to detect errorsin aligned corpora, using very little tech-nology.
In the first method, applicableto any aligned corpus, we consider align-ment as a string-to-string mapping.
Treat-ing the target string as a label, we ex-amine each source string to find incon-sistencies in alignment.
Despite settingup the problem on a par with grammat-ical annotation, we demonstrate crucialdifferences in sorting errors from legiti-mate variations.
The second method ex-amines phrase nodes which are predictedto be aligned, based on the alignment oftheir yields.
Both methods are effective incomplementary ways.1 IntroductionParallel corpora?texts and their translations?have become essential in the development ofmachine translation (MT) systems.
Alignmentquality is crucial to these corpora; as Tiede-mann (2003) states, ?
[t]he most important fea-ture of texts and their translations is the corre-spondence between source and target segments?(p.
2).
While being useful for translation studiesand foreign language pedagogy (see, e.g., Botleyet al, 2000; McEnery and Wilson, 1996), PARAL-LEL TREEBANKS?syntactically-annotated paral-lel corpora?offer additional useful informationfor machine translation, cross-language infor-mation retrieval, and word-sense disambiguation(see, e.g., Tiedemann, 2003),While high-quality alignments are desirable,even gold standard annotation can contain anno-tation errors.
For other forms of linguistic an-notation, the presence of errors has been shownto create various problems, from unreliable train-ing and evaluation of NLP technology (e.g., Padroand Marquez, 1998) to low precision and recallof queries for already rare linguistic phenomena(e.g., Meurers and Mu?ller, 2008).
Even a smallnumber of errors can have a significant impacton the uses of linguistic annotation, e.g., chang-ing the assessment of parsers (e.g., Habash et al,2007).
One could remove potentially unfavorablesentence pairs when training a statistical MT sys-tem, to avoid incorrect word alignments (Okita,2009), but this removes all relevant data fromthose sentences and does not help evaluation.We thus focus on detecting errors in the anno-tation of alignments.
Annotation error detectionhas been explored for part-of-speech (POS) anno-tation (e.g., Loftsson, 2009) and syntactic anno-tation (e.g., Ule and Simov, 2004; Dickinson andMeurers, 2005), but there have been few, if any, at-tempts to develop general approaches to error de-tection for aligned corpora.
Alignments are differ-ent in nature, as the annotation does not introduceabstract categories such as POS, but relies upondefining translation units with equivalent mean-ings.We use the idea that variation in annotation canindicate errors (section 2), for consistency check-ing of alignments, as detailed in section 3.
In sec-tion 4, we outline language-independent heuristicsto sort true ambiguities from errors, and evaluatethem on a parallel treebank in section 5.
In sec-tion 6 we turn to a complementary method, ex-ploiting compositional properties of aligned tree-banks, to align more nodes.
The methods are sim-ple, effective, and applicable to any aligned tree-bank.
As far as we know, this is the first attempt tothoroughly investigate and empirically verify er-ror detection methods for aligned corpora.382 Background2.1 Variation N -gram MethodAs a starting point for an error detection methodfor aligned corpora, we use the variation n-gramapproach for syntactic annotation (Dickinson andMeurers, 2003, 2005).
The approach is based ondetecting strings which occur multiple times inthe corpus with varying annotation, the so-calledVARIATION NUCLEI.
The nucleus with repeatedsurrounding context is referred to as a VARIATIONn-GRAM.
The basic heuristic for detecting anno-tation errors requires one word of recurring con-text on each side of the nucleus, which is suffi-cient for detecting errors in grammatical annota-tion with high precision (Dickinson, 2008).The approach detects bracketing and labelingerrors in constituency annotation.
For example,the variation nucleus last month occurs once inthe Penn Treebank (Taylor et al, 2003) with thelabel NP and once as a non-constituent, handledthrough a special label NIL.
As a labeling errorexample, next Tuesday occurs three times, twiceas NP and once as PP (Dickinson and Meur-ers, 2003).
The method works for discontinuousconstituency annotation (Dickinson and Meurers,2005), allowing one to apply it to alignments,which may span over several words.2.2 Parallel Treebank Consistency CheckingFor the experiments in this paper we will usethe SMULTRON parallel treebank of Swedish,German, and English (Gustafson-C?apkova?
et al,2007), containing syntactic annotation and align-ment on both word and phrase levels.1 Addition-ally, alignments are marked as showing either anEXACT or a FUZZY (approximate) equivalence.Corpora with alignments often have under-gone some error-checking.
Previous consistencychecks for SMULTRON, for example, consistedof running one script for comparing differencesin length between the source and target languageitems, and one script for comparing alignmentlabels, to detect variation between EXACT andFUZZY links.
For example, the pair and (English)and samt (German, ?together with?)
had 20 FUZZYmatches and 1 (erroneous) EXACT match.
Such1SMULTRON is freely available for research purposes, seehttp://www.cl.uzh.ch/kitt/smultron/.methods are limited, in that they do not, e.g., han-dle missing alignments.The TreeAligner2 tool for annotating andquerying aligned parallel treebanks (Volk et al,2007) employs its own consistency checking, re-cently developed by Torsten Marek.
One methoduses 2 ?
2 contingency tables over words, look-ing, e.g., at the word-word or POS-POS combina-tions, pinpointing anomalous translation equiva-lents.
While potentially effective, this does not ad-dress the use of alignments in context, i.e., whenwe might expect to see a rare translation.A second, more treebank-specific methodchecks for so-called branch link locality: if twonodes are aligned, any node dominating one ofthem can only be aligned to a node dominating theother one.
While this constraint can flag erroneouslinks, it too does not address missing alignments.The two methods we propose in this paper addressthese limitations and can be used to complementthis work.
Furthermore, these methods have notbeen evaluated, whereas we evaluate our methods.3 Consistency of AlignmentTo adapt the variation n-gram method and deter-mine whether strings in a corpus are consistentlyaligned, we must: 1) define the units of data weexpect to be consistently annotated (this section),and 2) define which information effectively iden-tifies the erroneous cases (section 4).3.1 Units of DataAlignment relates words in a source language andwords in a target language, potentially mediatedby phrase nodes.
Following the variation n-grammethod, we define the units of data, i.e., the vari-ation nuclei, as strings.
Then, we break the prob-lem into two different source-to-target mappings,mapping a source variation nucleus to a target lan-guage label.
With a German-English aligned cor-pus, for example, we look for the consistency ofaligning German words to their English counter-parts and separately examine the consistency ofaligning English words with their German ?la-bels.?
Because a translated word can be used indifferent parts of a sentence, we also normalize alltarget labels into lower-case, preventing variationbetween, e.g., the and The.2http://www.cl.uzh.ch/kitt/treealigner39.$.bl?htenVVFINInAPPR einigenPIAT G?rtenNN unterAPPR denART Obstb?umenNN Kr?nzeNNdichteADJA vonAPPR OsterglockenNN..theDT fruitNN treesNNSInIN someDT ofIN theDT gardensNNS wereVBD encircledVBN ---NONE- withIN denseJJ clustersNNS ofIN daffodilsNNS*HDHDNK HDNPNKPPMOHDNK HDNPNKPPMOHDHDAPNKHDHDNPNKPPMNRNPSBSVROOTNPSBJNPPPNPPPLOCNP NP NPPPNPPPCLRVPVPSVROOT.$.bl?hteennVFI NAbPRRi PhegTGeII ?trPiu dGtONbbrIIsArthberPmKPhtl?eRuzIcd rthONAPmnmNbPiu DJ?l?rhII tOPRRiPiu votrkrAII..u?rmu ktGAII thFI e?rmu OtGGTGII ?t?nfm hTeif GrNlenf gte?FI Nbif OSl?if NbFI Nmu egtel?IIamamamNPIzPPDwIz amamAPIzId amASPDw NPwPIz amamamNPIzPPDIiNPvf VSROOTNP NPPPBw-NPvfKA	SP NPPPPP-BiSPSPVSROOTFigure 1: Word and phrase alignments span thesame string on the left, but not on the right.Although alignment maps strings to strings forthis method, complications arise when mediatedby phrase nodes: if a phrase node spans over onlyone word, it could have two distinct mappings,one as a word and one as a phrase, which mayor may not result in the same yield.
Figure 1 il-lustrates this.
On the left side, Osterglocken isaligned to daffodils at the word level, and the samestring is aligned on the phrase level (NP to NP).In contrast, on the right side, the word Spiegel isaligned to the word mirror, while at the phraselevel, Spiegel (NP) is aligned to the mirror (NP).As word and phrase level strings can behave dif-ferently, we split error detection into word-leveland phrase-level methods, to avoid unnecessaryvariation.
By splitting the problem first into differ-ent source-to-target mappings and then into wordsand phrases, we do not have to change the under-lying way of finding consistency.Multiple Alignment The mapping betweensource strings and target labels handles n-to-malignments.
For example, if Ga?rten maps to thegardens, the and gardens is considered one string.Likewise, in the opposite direction, the gardensmaps as a unit to Ga?rten, even if discontinuous.Unary Branches With syntactic annotation,unary branches present a potential difficulty, inthat a single string could have more than one la-bel, violating the assumption that the string-to-.$.b$ll$l?hteneVVFINAPPnRNNignTG?TRerudeNNihRRTOVsnRRmKzcDdhJnRVVFINrgdvvAGonkhRinRvIcgRTvvGngRnkTG?fhRiNNkgeTvvGStRJndPRueNN.. lleanO?
hRPrndNN gPVwB DuuiNNIDIN rnvGv hP-Vwv PuknuRnNN ?gEgRJVw* uRIN eanO?
niJnNN uDIN PehdEheguRNNSOSONPvONm SONPcwvSHv SOSONPcwSONPKTSONm SOSOSONPNmPPMNGNPNmPPMKAGASVPMKAVROOTNPcwLNPvGOVPNPcwLNPcwLNP NPPPNPPPHfGVPAVPAA	SRTOV AVROOT.$bl?htenVFINAtPPRigTGGn?eTrudn?etAGtniOTGgthtPFslmrKgz?
.bNdtPiP cdtGiD dTJteD vttGeDI NmotmGtII tKNtiDk?k?NPsDk?ASPzfk?ASPzfIS ISNPP?VSROOTNPsDaA	SP A	SPNPPi?SPSPVSROOTFigure 2: The word someone aligned as a phraseon the left, but not a phrase by itself on the right.label mapping is a function.
For example, inPenn Treebank-style annotation, an NP node candominate a QP (quantifier phrase) node via aunary branch.
Thus, an annotator could (likelyerroneously) assign different alignments to eachphrasal node, one for the NP and one for the QP,resulting in different target labels.We handle all the (source) unary branch align-ments as a conjunction of possibilities, orderedfrom top to bottom.
Just as the syntactic struc-ture can be relabeled as NP/QP (Dickinson andMeurers, 2003), we can relabel a string as, e.g.,the man/man.
If different unary nodes result in thesame string (the man/the man), we combine them(the man).
Note that unary branches are unprob-lematic in the target language since they alwaysyield the same string, i.e., are still one label.3.2 Consistency and CompletenessError detection for syntactic annotation finds in-consistencies in constituent labeling (e.g., NP vs.QP) and inconsistencies in bracketing (e.g., NP vs.NIL).
Likewise, we can distinguish inconsistencyin labeling (different translations) from inconsis-tency in alignment (aligned/unaligned).
Detectinginconsistency in alignment deals with the com-pleteness of the annotation, by using the label NILfor unaligned strings.We use the method from Dickinson and Meur-ers (2005) to generate NILs, but using NIL for un-aligned strings is too coarse-grained for phrase-level alignment.
A string mapping to NIL mightbe a phrase which has no alignment, or it might40not be a phrase and thus could not possibly havean alignment.
Thus, we create NIL-C as a newlabel, indicating a constituent with no alignment,differing from NIL strings which do not even forma phrase.
For example, on the left side of Fig-ure 2, the string someone aligns to jemanden onthe phrase level.
On the right side of Figure 2,the string someone by itself does not constitute aphrase (even though the alignment in this instanceis correct) and is labeled NIL.
If there were in-stances of someone as an NP with no alignment,this would be NIL-C. NIL-C cases seem to be use-ful for inconsistency detection, as we expect con-sistency for items annotated as a phrase.3.3 Alignment TypesAligned corpora often specify additional informa-tion about each alignment, e.g., a ?sure?
or ?pos-sible?
alignment (Och and Ney, 2003).
In SMUL-TRON, for instance, an EXACT alignment meansthat the strings are considered direct translationequivalents outside the current sentence context,whereas a FUZZY one is not as strict an equiva-lent.
For example, something in English EXACT-aligns with etwas in German.
However, if some-thing and irgend etwas (?something or other?)
areconstituents on the phrase level, <something, ir-gend etwas> is an acceptable alignment (since thecorpus aligns as much as possible), but is FUZZY.Since EXACT alignments are the ones we expectto consistently align with the same string acrossthe corpus, we attach information about the align-ment type to each corpus position.
This can beused to filter out variations involving, e.g., FUZZYalignments (see section 4.4).
When multiplealignments form a single variation nucleus, therecould be different types of alignment for each link,e.g., dog EXACT-aligning and the FUZZY-aligningwith Hund.
We did not observe this, but one caneasily allow for a mixed type (EXACT-FUZZY).3.4 AlgorithmThe algorithm first splits the data into appropriateunits (SL=source language, TL=target language):1.
Divide the alignments into two SL-to-TL mappings.2.
Divide each SL-to-TL alignment set into word-leveland phrase-level alignments.For each of the four sets of alignments:1.
Map each string in SL with an alignment to a label?
Label = <(lower-cased) TL translation, EX-ACT|FUZZY|EXACT-FUZZY>?
(For phrases) Constituent phrases with no align-ment are given the special label, NIL-C.?
(For phrases) Constituent phrases which areunary branches are given a single, normalized la-bel representing all target strings.2.
Generate NIL alignments for string tokens which occurin SL, but have no alignment to TL, using the methoddescribed in Dickinson and Meurers (2005).3.
Find SL strings which have variation in labeling.4.
Filter the variations from step 3, based on likelihood ofbeing an error (see section 4).4 Identifying Inconsistent AlignmentsAs words and phrases have acceptable variants fortranslation, the method in section 3 will lead todetecting acceptable variations.
We use severalheuristics to filter the set of variations.4.1 NIL-only VariationAs discussed in section 3.2, we use the label NIL-C to refer to syntactic constituents which do notreceive an alignment, while NIL refers to non-constituent strings without an alignment.
A stringwhich varies between NIL and NIL-C, then, is notreally varying in its alignment?i.e., it is alwaysunaligned.
We thus remove cases varying only be-tween NIL and NIL-C.4.2 Context-based FilteringThe variation n-gram method has generally reliedupon immediate lexical context around the vari-ation nucleus, in order to sort errors from ambi-guities (Dickinson, 2008).
However, while use-ful for grammatical annotation, it is not clear howuseful the surrounding context is for translationtasks, given the wide range of possible translationsfor the same context.
Further, requiring identicalcontext around source words is very strict, leadingto sparse data problems, and it ignores alignment-specific information (see sections 4.3 and 4.4).We test three different notions of context.Matching the variation n-gram method, we firstemploy a filter identifying those nuclei whichshare the ?shortest?
identical context, i.e., oneword of context on every side of a nucleus.
Sec-ondly, we relax this to require only one word of41context, on either the left or right side.
Finally, werequire no identical context in the source languageand rely only on other filters.
For example, withthe nucleus come in the context Where does theworld come from, the first notion requires worldcome from to recur, the second either world comeor come from, and the third only requires that thenucleus itself recur (come).4.3 Target Language FilteringBecause translation is open-ended, there can bedifferent translations in a corpus.
We want tofilter out cases where there is variation in align-ment stemming from multiple translation possibil-ities.
We implement a TARGET LANGUAGE FIL-TER, which keeps only the variations where thetarget words are present in the same sentence.
Ifword x is sometimes aligned to y1 and sometimesto y2 , and word y2 occurs in at least one sentencewhere y1 is the chosen target, then we keep thevariation.
If y1 and y2 do not occur in any of thesame sentences, we remove the variation: giventhe translations, there is no possibility of havingthe same alignment.This also works for NIL labels, given sentencealignments.3 For NILs, the check is in only onedirection: the aligned sentence must contain thetarget string used as the label elsewhere in the cor-pus.
For instance, the word All aligns once withalle and twice with NIL.
We check the two NILcases to see whether one of them contains alle.Sentences which are completely unaligned leadto NILs for every word and phrase, and we alwayskeep the variation.
In practice, the issue of havingno alignment should be handled separately.4.4 Alignment Type FilteringA final filter relies on alignment type informa-tion.
Namely, the FUZZY label already indicatesthat the alignment is not perfect, i.e., not nec-essarily applicable in other contexts.
For exam-ple, the English word dead FUZZY-aligns with theGerman verschwunden (?gone, missing?
), the besttranslation in its context.
In another part of thecorpus, dead EXACT-aligns with leblosen (?life-less?).
While this is variation between verschwun-den and leblosen, the presence of the FUZZY label3In SMULTRON, sentence alignments are not given di-rectly, but can be deduced from the set of word alignments.word phraseall 540 251oneword 340 182shortest 96 21all-TL 194 140oneword-TL 130 94shortest-TL 30 16Table 1: Number of variations across contextsalerts us to the fact that it should vary with anotherword.
The ALIGNMENT TYPE FILTER removescases varying between one EXACT label and oneor more FUZZY labels.5 EvaluationEvaluation was done for English to German onhalf of SMULTRON (the part taken from the novelSophie?s World), with approximately 7500 wordsfrom each language and 7600 alignments (roughly4800 word-level and 2800 phrase-level).
Basicstatistics are in Table 1.
We filter based on thetarget language (TL) and provide three differentcontextual definitions: no context, i.e., all varia-tions (all); one word of context on the left or right(oneword); and one word of context on the left andright, i.e., the shortest surrounding context (short-est).
The filters reduce the number of variations,with a dramatic loss for the shortest contexts.A main question concerns the impact of the fil-tering conditions on error detection.
To gauge this,we randomly selected 50 (all) variations for theword level and 50 for the phrase level, each corre-sponding to just under 400 corpus instances.
Thevariations were checked manually to see whichwere true variations and which were errors.We report the effect of different filters on preci-sion and recall in Table 2, where recall is with re-spect to the all condition.4 Adding too much lexi-cal context in the source language (i.e., the short-est conditions) results in too low a recall to bepractically effective.
Using one word of contexton either side has higher recall, but the precisionis no better than using no source language con-text at all.
What seems to be most effective is toonly use the target language filter (all-TL).
Here,we find higher precision?higher than any sourcelanguage filter?and the recall is respectable.4Future work should test for recall of all alignment errors,by first manually checking a small section of the corpus.42Word PhraseCases Errors P R Cases Errors P Rall 50 17 34% 100% 50 15 30% 100%oneword 33 12 36% 71% 33 8 24% 53%shortest 8 2 25% 12% 4 1 25% 7%all-TL 20 11 55% 65% 27 12 44% 80%oneword-TL 15 6 40% 35% 14 7 50% 47%shortest-TL 2 1 50% 6% 3 1 33% 7%Table 2: Error precision and recallTL filter An advantage of the target languagefilter is its ability to handle lexical (e.g., case) vari-ations.
One example of this is the English phrasea dog, which varies between German einem Hund(dative singular), einen Hund (accusative singu-lar) and Hunde (accusative plural).
Similar to us-ing lower-case labels, one could map strings tocanonical forms.
However, the target languagefilter naturally eliminates such unwanted varia-tion, without any language-specific information,because the other forms do not appear across sen-tences.Several of the variations which the target lan-guage filter incorrectly removes would, once theerror is fixed, still have variation.
As an example,consider cat, which varies between Katze (5 to-kens) and NIL (2 tokens).
In one of the NIL cases,the word needs to be FUZZY-aligned with the Ger-man Tigerkatze.
The variation points out the error,but there would still be variation (between Katze,Tigerkatze, and NIL) after correction.
This showsthe limitation of the heuristic in identifying the re-quired non-exact alignments.Another case the filter misses is the variationnucleus heard, which varies between geho?rt (2 to-kens) and ho?ren (1 token).
In this case, one of theinstances of <heard, geho?rt> should be <heard,geho?rt hatte>.
Note that here the erroneous caseis not variation-based at all; it is a problem withthe label geho?rt.
What is needed is a method todetect more translation possibilities.As an example of a problem for phrases, con-sider the variation for the nucleus end with 5 in-stances of NIL and 1 of ein Ende.
In one NILinstance, the proper alignment should be <theend, Ende>, with a longer source string.
Sincethe target label is Ende and not ein Ende, the fil-ter removes this variation.
One might exploremore fuzzily matching NIL strings, so that Endematches with ein Ende.
We explore a differentmethod for phrases next, which deals with someof these NIL cases.6 A Complementary MethodAlthough it works for any type of aligned corpus,the string-based variation method of detecting er-rors is limited in the types of errors it can de-tect.
There might be ways to generalize the vari-ation n-gram method (cf.
Dickinson, 2008), butthis does not exploit properties inherent to alignedtreebanks.
We pursue a complementary approach,as this can fill in some gaps a string-based methodcannot deal with (cf.
Loftsson, 2009).6.1 Phrase Alignment Based on Word LinksUsing the existing word alignments, we can searchfor missing or erroneous phrase alignments.
Ifthe words dominated by a phrase are aligned, thephrases generally should be, too (cf.
Lavie et al,2008).
We take the yield of a constituent in oneside of a corpus, find the word alignments of thisyield, and use these alignments to predict a phrasalalignment for the constituent.
If the predictedalignment is not annotated, it is flagged as a possi-ble error.
This is similar to the branch link localityof the TreeAligner (see section 2.2), but here as aprediction, rather than a restriction, of alignment.For example, consider the English VP chooseher own friends in (1).
Most of the words arealigned to words within Ihre Freunde vielleichtwa?hlen (?possibly choose her friends?
), with noalignment to words outside of this German VP.
Wewant to predict that the phrases be aligned.
(1) a.
[VP choose1 her2 own friends3 ]b.
[VP Ihre2 Freunde3 vielleicht wa?hlen1 ]The algorithm works as follows:1.
For every phrasal node s in the source treebank:(a) Predict a target phrase node t to align with,where t could be non-alignment (NIL):43i.
Obtain the yield (i.e., child nodes) of thephrase node s: s1 , ... sn .ii.
Obtain the alignments for each child nodesi , resulting in a set of child nodes in thetarget language (t1 , ... tm ).iii.
Store every mother node t?
covering all thetarget child nodes, i.e., all <s, t?> pairs.
(b) If a predicted alignment (<s, t?>) is not in theset of actual alignments (<s, t>), add it to theset of potential alignments, AS 7?T .i.
For nodes which are predicted to have non-alignment (but are actually aligned), outputthem to a separate file.2.
Perform step 1 with the source and target reversed,thereby generating both AS 7?T and AT 7?S .3.
Intersect AS 7?T and AT 7?S , to obtain the set of pre-dicted phrasal alignments not currently aligned.The main idea in 1a is to find the children of asource node and their alignments and then obtainthe target nodes which have all of these alignednodes as children.
A node covering all these targetchildren is a plausible candidate for alignment.Consider example (2).
Within the 8-word En-glish ADVP (almost twice .
.
.
), there are six wordswhich align to words in the corresponding Ger-man sentence, all under the same NP.5 It does notmatter that some words are unaligned; the factthat the English ADVP and the German NP coverbasically the same set of words suggests that thephrases should be aligned, as is the case here.
(2) a. Sophie lived on2 [NP1 the2 outskirts3 of a4sprawling5?
suburb6?]
and had [ADVP almost7twice8 as9 far10 to school as11 Joanna12?]
.b.
Sophie wohnte am2 [NP1 Ende3 eines4ausgedehnten5?
Viertels6?
mit Einfam-ilienha?usern] und hatte [NP einen fast7doppelt8 so9 langen10 Schulweg wie11Jorunn12?]
.The prediction of an aligned node in 1a allowsfor multiple possibilities: in 1aiii, we only checkthat a mother node t?
covers all the target children,disregarding extra children, since translations cancontain extra words.
In general, many such dom-inating nodes exist, and most are poor candidatesfor alignment of the node in question.
This is thereason for the bidirectional check in steps 2 and 3.For example, in (3), we correctly predict align-ment between the NP dominating you in Englishand the NP dominating man in German.
Fromthe word alignment, we generate a list of mother5FUZZY labels are marked by an asterisk, but are not used.nodes of man as potential alignments for the youNP.
Two of these (six) nodes are shown in (3b).In the other direction, there are eight nodes con-taining you; two are shown in (3a).
These are thepredicted alignment nodes for the NP dominatingman.
In either direction, this overgenerates; theintersection, however, only contains alignment be-tween the lowest NPs.
(3) a.
But it ?s just as impossible to realize [S [NPyou1 ] have to die without thinking how incred-ibly amazing it is to be alive ] .b.
[S Und es ist genauso unmo?glich , daru?bernachzudenken , dass [NP man1 ] sterben muss, ohne zugleich daran zu denken , wie phan-tastisch das Leben ist .
]While generally effective, certain predictionsare less likely to be errors.
In figure 3, for ex-ample, the sentence pair is an entire rephrasing;<her, ihr> is the only word alignment.
For eachphrasal node in the SL, the method only requiresthat all its words be aligned with the words underthe TL node.
Thus, the English PP on her, the VPhad just been dumped on her, and the two VPs inbetween are predicted as possible alignments withthe German VP ihr einfach in die Wiege gelegtworden or its immediate VP daughter: they allhave her and ihr aligned, and no contradictingalignments.
Sparse word alignments lead to mul-tiple possible phrase alignments.
After intersect-ing, we mark cases with more than one predictedsource or target phrase and do not evaluate them.If in step 1aiii, no target mother (t?)
exists, butthere is alignment in the corpus, then in step 1bi,we output predicted non-alignment.
In Example(2), for instance, the English NP the outskirts ofa sprawling suburb is (incorrectly) predicted tohave no alignment, although most words align towords within the same German NP.
This predic-tion arises because the aligns to a word (am) out-side of the German NP, due to am being a contrac-tion of the preposition an and the article dem, (cf.on and the, respectively).
The method for predict-ing phrase alignments, however, relies upon wordsbeing within the constituent.
We thus concludethat: 1) the cases in step 1bi are unlikely to be er-rors, and 2) there are types of alignments whichwe simply will not find, a problem also for au-tomatic alignment based on similar assumptions(e.g., Zhechev and Way, 2008).
In (2), for in-stance, were there not already alignment between44.$.bl?htenVFlINFA bP?RightNNTiGiT?hhNNru?NNdO irgslmutFh rgtNNO RritOK zriTiVV..ci?NON$ GPPDIVVA ulRhJF voI?OJ kiighJV RofSiRhJV aaaVwVda PgnV ui?NONBcFcFNPAJcFcFcFNPFtcFASP-wcFVE cFNPVEPP-wSPw* SPw*VNPAJHARSP NP NPPP*MOSPSPSPVFigure 3: A sentence with minimal alignmentthe NPs, we would not predict it.6.2 EvaluationThe method returns 318 cases, in addition to 135cases with multiple source/target phrases and 104predicted non-alignments.
To evaluate, we sam-pled 55 of the 318 flagged phrases and found that25 should have been aligned as suggested.
21of the phrases have zero difference in length be-tween source and target, while 34 have differencesof up to 9 tokens.
Of the phrases with zero-length difference, 18 should have been aligned(precision=85.7%), while only 7 with length dif-ferences should have been aligned.
This is in linewith previous findings that length difference canhelp predict alignment (cf., e.g., Gale and Church,1993).
About half of all phrase pairs that shouldbe aligned should be EXACT, regardless of thelength difference.The method is good at predicting the alignmentof one-word phrases, e.g., pronouns, as in (3).
Ofthe 11 suggested alignments where both sourceand target have a length of 1, all were correct sug-gestions.
This is not surprising, since all wordsunder the phrases are (trivially) aligned.
Althoughshorter phrases with short length differences gen-erally means a higher rate of correct suggestions,we do not want to filter out items based on phraselength, since there are outliers that are correct sug-gestions, e.g., phrase pairs with lengths of 15 and13 (difference=2) or 31 and 36 (difference=5).
Itis worth noting that checking the suggestions tookvery little time.7 Summary and OutlookThis paper explores two simple, language-independent ways to detect errors in aligned cor-pora.
In the first method, applicable to any alignedcorpus, we consider alignment as a string-to-stringmapping, where a string could be the yield of aphrase.
Treating the target string as a label, wefind inconsistencies in the labeling of each sourcestring.
Despite setting the problem up in a similarway to grammatical annotation, we also demon-strated that new heuristics are needed to sort er-rors.
The second method examines phrase nodeswhich are predicted to be aligned, based on thealignment of their yields.
Both methods are ef-fective, in complementary ways, and can be usedto suggest alignments for annotators or to suggestrevisions for incorrect alignments.The wide range of possible translations and thelinguistic information which goes into them indi-cate that there should be other ways of finding er-rors.
One possibility is to use more abstract sourceor target language representations, such as POS,to overcome the limitations of string-based meth-ods.
This will likely also be a useful avenue toexplore for language pairs more dissimilar thanEnglish and German.
By investigating differentways to ensure alignment consistency, one can be-gin to provide insights into automatic alignment(Zhechev and Way, 2008).
Additionally, by cor-recting the errors, one can determine the effect onmachine translation evaluation.AcknowledgmentsWe would like to thank Martin Volk and ThorstenMarek for useful discussion and feedback of ear-lier versions of this paper and three anonymousreviewers for their comments.45ReferencesBotley, S. P., McEnery, A. M., and Wilson, A.,editors (2000).
Multilingual Corpora in Teach-ing and Research.
Rodopi, Amsterdam, AtlantaGA.Dickinson, M. (2008).
Representations for cat-egory disambiguation.
In Proceedings ofCOLING-08, pages 201?208, Manchester.Dickinson, M. and Meurers, W. D. (2003).
Detect-ing inconsistencies in treebanks.
In Proceed-ings of TLT-03, pages 45?56, Va?xjo?, Sweden.Dickinson, M. and Meurers, W. D. (2005).
De-tecting errors in discontinuous structural anno-tation.
In Proceedings of ACL-05, pages 322?329.Gale, W. A. and Church, K. W. (1993).
A pro-gram for aligning sentences in bilingual cor-pora.
Computational Linguistics, 19(1):75?102.Gustafson-C?apkova?, S., Samuelsson,Y., and Volk, M. (2007).
SMUL-TRON (version 1.0) - The Stock-holm MULtilingual parallel TReebank.www.ling.su.se/dali/research/smultron/index.htm.Habash, N., Gabbard, R., Rambow, O., Kulick, S.,and Marcus, M. (2007).
Determining case inArabic: Learning complex linguistic behaviorrequires complex linguistic features.
In Pro-ceedings of EMNLP-CoNLL-07, pages 1084?1092.Lavie, A., Parlikar, A., and Ambati, V. (2008).Syntax-driven learning of sub-sentential trans-lation equivalents and translation rules fromparsed parallel corpora.
In Proceedings of theACL-08: HLT Second Workshop on Syntax andStructure in Statistical Translation (SSST-2),pages 87?95, Columbus, OH.Loftsson, H. (2009).
Correcting a POS-taggedcorpus using three complementary methods.In Proceedings of EACL-09, pages 523?531,Athens, Greece.McEnery, T. and Wilson, A.
(1996).
Corpus Lin-guistics.
Edinburgh University Press, Edin-burgh.Meurers, D. and Mu?ller, S. (2008).
Corporaand syntax (article 44).
In Lu?deling, A.
andKyto?, M., editors, Corpus Linguistics.
An In-ternational Handbook, Handbooks of Linguis-tics and Communication Science.
Mouton deGruyter, Berlin.Och, F. J. and Ney, H. (2003).
A systematic com-parison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Okita, T. (2009).
Data cleaning for word align-ment.
In Proceedings of the ACL-IJCNLP 2009Student Research Workshop, pages 72?80, Sun-tec, Singapore.Padro, L. and Marquez, L. (1998).
On the eval-uation and comparison of taggers: the effectof noise in testing corpora.
In Proceedings ofACL-COLING-98, pages 997?1002, San Fran-cisco, California.Taylor, A., Marcus, M., and Santorini, B.
(2003).The penn treebank: An overview.
In Abeille?,A., editor, Treebanks: Building and using syn-tactically annotated corpora, chapter 1, pages5?22.
Kluwer, Dordrecht.Tiedemann, J.
(2003).
Recycling Translations -Extraction of Lexical Data from Parallel Cor-pora and their Application in Natural Lan-guage Processing.
PhD thesis, Uppsala univer-sity.Ule, T. and Simov, K. (2004).
Unexpected pro-ductions may well be errors.
In Proceedings ofLREC-04, Lisbon, Portugal.Volk, M., Lundborg, J., and Mettler, M. (2007).A search tool for parallel treebanks.
In Pro-ceedings of the Linguistic Annotation Workshop(LAW) at ACL, pages 85?92, Prague, Czech Re-public.
Association for Computational Linguis-tics.Zhechev, V. and Way, A.
(2008).
Automatic gen-eration of parallel treebanks.
In Proceedingsof Coling 2008, pages 1105?1112, Manchester,UK.46
