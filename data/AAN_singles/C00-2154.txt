WebDIPLOMAT: A Web-Based Interactive Machine TranslationSystemChristol)her Hogan and Robert  FrederkingI,anguagc Techliologies \]institutePittsburgh, \]~ennsylva.nia, USAchogan~e-l?ngo, c m, ref@cs, cmu.
eduAbst ractWe have implenlented a.n interactive, Wel)-based,chat-style machine translation system, SUpl)ort;ingspeech recognition and synthesis, local- or third-party correction of speech recognition and machinetra.nslation output, a.nd online learning.
The un-derlying client-server architecture, implemented in.la.va TM, pl:ovides remote, distributed computationfor the translation and speech sut)systems.
We fur-ther describe our Web-based user interthces, whMican easily produce different uscflfl eonfigllrartions.1.
Introduct ionThe World Wide Web (Berners-l,ee, 11989) seemsto be all ideal environment for machine transla-tion: it is easily accessible around the world usingfreely-available, easy-to-use tools which are ava.ilableto persons speaking a. nlyriad of langua.ges, all ofwhom would like to I)e able to communicate withone another without language barriers.
IlL. is there-fore not too surl)rising that a few companies haveattempted to make machine translation availablein this medium (AltaVista, 1999; FreeTranslation,1.q99; hlt.erTran, 1999).
'.l'he l)riinary use identifiedfor these tra.nslators has been that of translatingWeb pages or amusing oneself with the inadequa.-cies of ma.dfine translation (Yang and l,ange, 1998).What these systems cannot be used for is real-time,speech-to-speech ommunication with translation.l{eal-time communication over the hiternet hasmore properly been the (lomain of '<chat" l)roto-eels: primarily Interact Relay Chat (11{(3) (Oikari-nen and Reed, 1993), and similar instant messagingprotocols developed commercially (America OnlineInc., 2000; Microsoft Corp., 2000; ICQ Inc., 1999).While some portals have been developed to permitaccess to chat using the Web (iTRiBE lnc., 1996),the primary point of access eems to be chat-specificclient software.
Although chat defines protocols andprovides infrastructure, it is limited ill the kind ofdata that it can transl)ort, and client software istightly focussed oil the text domain.
Such limita-tions have not, however, prevented researchers fi'omexl)erilnenting with the possibilities of incorporat-ing machine translation or speech into tile chat ex-perience (1,enzo, 1998; Seligma.n et al, 1998).
Theoutcome of these experiments has been to show thatcomn-mrcial machine translation systems may 1)e rea-sonably integrated into the chat room, and that com-mercial speech software ca.n be connected to existingchat software to provide the desired experience.We have taken a difl~rent road.
It has been noted(Seligman, 19.(.
)7; l"rederking et al, 2000) that broad-coverage machine translation and speech recognitioncannot now be usefld mdess users can interact withthe system to improve results.
While Seligman etal.
(1998) were able to etDct user editing of speechrecognition by editing text before submitting it fortranslation, they were unable to do the same for tiletranslation system, prilnarily due to limitations ofcommercial software.
Additional imitations are en-countered in the communication medium: chat isnot amenable to non-text interaction with transla-tion agents, and commercial chat software does not,in any case, support such interaction.To deal with these limitations, we have developeda fully interactive, Web-based, chat-style tra.nslationsystem, supporting sl)eech recognition and synthesis,local-or third-1)arty correction of speech reeognitioi,and machine translation, and online learning, whichca.n be used with nothing lllore than a Well browserand some simple add-ons.
All intensive processing,including translation and speech recognition is per-formed a.t central servers, permitting access for thosewith limited computational resources.
In a.ddition,tile modular design of t.he system and interface per-mit computa.tional tasks to be easily distributed anddifferent dialog configurations to be explored.2 In ter face  Des ignThe design of the Webl)IPLOMAT system is in-tended to facilitate the following kind of interaction:(numbers correspond to Figure 1)1.
Speech fl'om the user is recognized and dis-played in an editing window, where it may beedited by respeaking or using the keyboard.2.
When text is acceptable to the user, it is sub-mitted tbr translation and transfer to the other1041' ,  .
.
.
.
.
.
.
.
.
I  ; 5 .
.
.
.........
I '-- -v  )Figure 1: User-level perspective on information flow.See text for explanation of labels.l)arty.3.
Text to be translated is optionally presented toa human expert, who is able to translate, cor-rect and teach the system a correct translation.4.
Upon machine translation of tlLe text, or accep-tance by the expert, a translation is deliveredto the other pa.rty and synthesized.5.
13oth sides of the conversation are tracked a.u-tomatically for all users, and displayed on theirinterfaces.Although the above is the original vision for tihesystem, other configurations are easily imagined.Configurations with more than two participants, orwhere one of the users is also simultaneously all ex-pert are stra.ightforwardly handled.
International-ization of the interfaces, for use in different locales, isalso easily handled.
Many changes of this nature arehandled by easy modifications to the HTMI, code forgiven \?eb pages.
More COml)licated tasks may beaccomplished by modifications of underlying code.In order to produce the above configuration, thecurrent system implements two user interthces (UIs):the Client UI, which provides peech and text inputcapabilities to the primary end-users of the system;and the Editor UI, which provides translation edit-ing capabilities to a human translation expert, inthe rest of this section, we describe in detail certainunique aspects of each interface.2.1 Cl ient User  InterfaceIn addition to speech-input and editing capabilities,the Client UI is able to track the entire dialog asit progresses.
Because the Central CommunicationsServer (@ ~a.l) forwards every message to all con-nected clients, every component of the system can beaware of how the dialog turn is proceeding.
Ill tileClient UI, this capability is used to l)rovide a run-ning transcript of the conversation as it occurs.
Bynoting the identifiers on messages (cf.
~,3.4), the U1can assign appropriate labels to each of the follow-ing: our original utterance, translation of our utter-ance, other person's utterance, translation of theirutterance.
In ~ddition, we use knowledge about thestatus of the dialog to prevent the user from send-ing several utterances belbre the other party has re-sponded.2.2 Ed i to r  User  In ter faceThe F, ditor UI provides tools which make it possiblefor a human expert to edit translations producedby the machine translator betbre they are sent tothe users.
As mentioned earlier, the editing step isoptional, and is intended to improve the quality oftransla.tions.
The Editor UI may be configured sothat either of the two users, or a remote third partycan act as editor.
Onr motivations for providing anediting capability are twofold:?
Although our MT system (@ ~3.2) dots notalways produce the correct answer, the correctanswer is usually available a.mong the possibili-ties it.
considers.t .a l  Q ?
,H~ MT system provides for online updates ofits knowledge base which a.llows tbr translationsto improve over time.In order to take advantage of' these capabilities, wehave designed two editing tools, the chart editor anda.lways-active l arning, that enable a human expertto rapidly produce an accurate tlJaillslatioll aud tostore tha.t translation in the MT knowledge base forfuture use.As discussed in ~a.2, our MT system ma.y producemore than one translation for each par t  of tile input,from which it attempts to se\]ect the best translation.The entire set of translations i available to the Web-I ) IPLOMAT system, and ix used in the cha.rt editor.By double-clicking on words in the translation, theOriginal EnglishMy name is John .
.
.
.
.
.
.Edited Frenclll inen nora estJehnFigure 2: Popup Chart Editor1042human edit()\]: is l)resented a. pol)Ul)-menu of alterna.-tire tra.nslations beginning a.t a particular locationin the sentence (see l?igure 2).
When one o\[' the al-ternatives is sek;cted, it replaces the original word orwords.
In this way, a. sentence may be rapidly editedto an acceptable sta.te.In order to reduce develolmmnt \]line, our MT sys-tem can be used in a ra.pid-del)loylnent style: afl;er a.minimal knowledge base is constructed, the systemis put into use with a huma.n expert supervising, sothat domain-rel(:va.nt data ma.y be elicited (lui(:ldy.In order to supl)ort this, all uttera.nces a.re consid-ered for learning.
When the editor presses the 'Ac-ccitt/Learn' l)utton, the original utterance and itstra.nslatiotl are exa.ntined to determine if they aresuital)le for learning.
(Turrently all utterances forwhich the forward tra.nslation has 1teen edited aresu brat\]ted \['or learning, a.lthough other criteria ma.yalso be entertained.
More detail about online lea.r|>ing may 1)e found ill ~3.2.Although the editor UI is primarily i\]lte\]l(led tbruse by a. tra.nslation expert, it, will sometimes also 1)eu,qed 1)y tllose who are not as expert.
For this situa-ti:)n, we ha.re introduced it lta('ktra.lisla.l.ion capalJil-ity which retra.nsla.tos the edited forward trai/sla.tiollinto the language of the input.
Although i,~iperl'ect,baektranslatio\]l can often give the user an idea ofwhether the forward transla.tion was suits\]ant\]ally(:O\]:l:eot,.3 System_ Designh, this section, we describe 1.he eOml)uta,l, io\]|al archi-t()etu r(" \[lllderlyin,,g the W(;b I) 11) I,O M A'I' sys| ,e l l l .3.1.
Ar(:hite('t;m'( ~.The underlyil\]g arel\]itecture of the \?obl)II)I,OMAT 'system is shown in Figure 3.
The system is organizedarotllld three servel:s:The We.It Serv<'.r serves I1T\]Vll, l)ages to <:lients.We used an unmodified version of th<; Apachell'l"l'l) Server (Apache Softwa.re l:oundation,1999).Tim SI)eech Recogniz( : r (s)  l)erform speechrecognition for clients.The Cent ra l  Commmf icat ions  Server  allowscomrmmica.tion between clients, l,hicapsulatedoh.jeers sent to this server are forwarded toall connected clients.
With the exception ofspeech and HTTP,  all communications betweenclients use this server.The servers are designed to be small, and a.re in~tended to coexist on one lnachine.
1 Currently, how-ever, the speech server inchides a full speech recog-l This is necessary due to security restrictions on .\]~twt 'I'MApplets.nizer, a.nd therefore consunies a greater amount o1'resources than the other servers.Most processing is intended Co be perforumd byclients, which haw~' no loca.lity requirements, andmay therefore I)e distributed across nm.chi\]les andnetworks as necessary.
The User and Editor Clientswere described in {i?2.1 and 2.2.
We will now ex-amine the most important l~rocessing mechanisms,ilmluding machine translation and speech recogni-tion/synthesis.3.2 Mach ine  Trans la t ionl"or Machine Transla.tion, we rely on the l)anliteM|dti-lgl\]gine Machine Translation (MEMT) Server(l:rederking a.nd lh:own, 1996).
This system, whichis outlined in Figure 4, makes use of several trans-lation engines at once, combining their output witha.
sta.tistica\] language model (Brown and l:rederk-ing, 1995).
Each traiisla.tion engine makes use of adill'ere|tt transla.tion technok)gy, and produ(:es multi-t)1% possibly overlal)ping , l.ra\]mlations for every partof tit(; inl)ut that it can translate.
All of the trans-lations I)roduced 1)3: the various engines a,re pla.cedin a chart data struci;ure (Kay, 1967; Winograd,1983), indexed by the'Jr position i\]\] the input utter-a.nce.
A statistical huiguage model is used, togetherwith scores provided I)y the tra.nslation engines, todetermine the optima.l path through the set of trans-lated segments, which informa,tion is also stored i\]\]the chart.
Upon completion of tra.nslation, the chartdata struct||re is made a.vailable For use by the resto\[7 the WeM)II)I,OMA:I ' system.
(;urrently, we enq)loy l,exica.l Transfer and Ex-Source TargetLanguage LanguageMorphological I Analyzer i~\ [  User Interthce- i~  ii'ransfer-Based MT- i~ Example-Based MT Statistical ModellerKnowledge-Based MTExpansion slotFigure d: Multi-Engine Machine 3h:ansla.tion Archi-tecture1043MEMTServerMTInterfacei Speech Synthesizer ', Central Web'Recognizer(s) Server i Interface Server..... i-'"3"r":q~'-'7"~"~i' --\]'; 7 7~'-'- -" .
.
.
.  "
...................... "':':'i;:": \] :):')}:'i}}i"':":":':i:i :: ";" :':\]'17'ii'i ...........
I l l \[el 'net.,.'"
/ ...... ",, ................ ....." .
.
.
.
.
.
.,.1Speech User l Speech Speech User2 Speech /Plugin Client Synth.
Plugin Client Synth.
Editor Client 1/ Editor Client 2Figure 3: Serverample Based Machine Translation (EBMT) engines(Na.gao, 1984; Brown, 1996).
Lexical Transfer usesbilingual dictionaries and phrasal glossaries to pro-vide phrase-for-phrase translations, while EBMTuses a fllzzy matching step to produce translationsfroln a bilingual corpus of matched sentence pairs.Because the knowledge bases for these techniques aresimple, they both suI)port online augmentation.
Asmentioned in ?2.2, the Editor UI attempts to learnfrom utterances that have been edited.
Pairs of ut-terances ubmitted for learning to the translator areplaced in a Lexical Transfer glossary if less than sixwords long, and in an EBMT corpus if two wordsor longer.
Higher scores are given to these newlycreated resources, so that they are preferred.The MT server is interfa.ced to the Central Serverthrough MT interfa.ce clients, which handle, interalia, character set conversions, support for learningand conversion of MT output into an internal ob-ject representation usable by other clients.
It alsoensures that outgoing translations are staml)ed withcorrect identifiers (cf.
~3.4), relative to the incomingtext, to ensure that translations are directed to theappropriate clients.a.a Speech Recognition and SynthesisIn the current system, speech recognition is handledas a private communication between a browser plug-in, running on the user's machine, and a speechrecognition server, and is not routed through thecentral server.
Speech is streamed over the networkto the server, which performs the recognition, andreturns the results as a text string.
This configura-tion permits most of the computational resources tobe offloaded from the client machine onto powerfulremote servers.
The speech may be streamed overthe network as-is, or it may be lightly preprocessedinto a feature stream for use over lower-bandwidthconnections.
The recognized text is returned di-Architecturerectly to tile user client for editing and validationby the user belbre heing sent for translation.
Ourspeech server is a previously implemented esign(Issar, 1997) based on the Sphinx II speech recog-nizer (Huang et a l., 1992).
As mentioned earlier,the speech server and recognizer are not currentlydesigned to run in a distributed fashion.Unlike speech recognition, which is handled bythe User Client, speech synthesis does not requirehuman interaction, and can therefore be connecteddirectly to the central server.
Currently, Synthe-sizer Interfaces unpackage internal representationsand send utterances to be synthesized on a speechsynthesizer unning locally on the user's machine.Future plans call for speech to be synthesized at acentral ocation and transported across the net.workin standard andio formats.3.4 Imp lementat ionAll components of the Webl)IPLOMA'\]' except thespeech components and Web Server were imple-mented in Java TM (Gosling et el., 1996), inclndingthe Central Server.
Messages between clients areimplemented as a Java class Capsule, containing aS t r ing  identifier and any number of data.
Objects.Object serialization permits simple implementationof message streams.
User Interface clients are de-veloped as Applets, which are embedded in HTMLpages served by the Web Server.4 Future Work and ConclusionThe most significant change we would like to maketo the current system is the way that speech is han-dled.
We firmly believe that the best speech inputdevice is the one people are already familiar with,namely the telephone.
A revised system would al-low users to call specific phone numbers (connectedto the central server) in order to access the system,which would then recognize and synthesize speech1044over tile telephone line while still using web-based in-terfaces.
This, of COtlrse, takes us closer to the grandAI Challenge of the translating telephone (OAIAE,1996; Kurzweil, 1999; Frederking et al, 1999).
Wecontend that by using interactive machine transla-tion, the goal of a broad-domain translating tele-phone Call be more easily brought o fruition.ReferencesAltaVista.
1999.
Babel Fish: A SYSTI{AN transla-tion system, http://babelfish.altavista.com/.America Ojflit\e Inc. 2000.
AOI, InstantMessengertSm).
http://www.aol.com/aim/home.html.The Apache Software Foundation.
1999.
TheApache H' I 'T I  ) Server Project.
ht tp : / /www.apache.org.Tim Berners-l,ee.
1989.
Informa.tion manage-ment: A proposal, http://www.w3.org/History/1989/proposal.html, March.
CI!
;RN.l~.alf Brown and Robert Frederking.
1995.
Ap-plying statistical English language modeling tosymholic machine translation.
\]n Proceedings ofthe ,5'ixlh International Uo~dbrence on 7'heorcticaland Methodological Issues in Machine Trcmslation(TMI-95), pages 221-239.Ralf Brown.
1996.
Example-based inachine transla-tion in the Pangloss ystem.
In Proccedirtg.s of thel(ith International Co~@rencc o1~ ComputationalLingttistics (COIJNG-96).Robert l,'rcderking and l{alf Ih'own.
1996.
ThePangloss-lAte machine translation system.
In Pro-ceedings of the Col~ference of the Association forMachine 7)'anslation in the Americas (AMTA).Robert Frederking, Christol)hel: logan, and Alexan-der l{udnicky.
1999.
A new approach to the trans-lating telephone.
In l)wcccdiltfls of the Mac\ira:7)'anslalion 5'ummit VII: 1147' i~t i, hc G'tvat 7)'ans-lation I';ra, Singapore, September.lb)bert Frederking, Alexander Rudnicky, Christo-pher Hogan, and Kevin Lenzo.
2000.
Interactivespeech translation i  the DIPLOMAT project.
MTJournal.
To appear.l"ree'lS:anslation.
1999.
Free'l'rmMation: A Trans-parent l,anguage translation system, http://www.freetranslation.com/.James Gosling, Bill .loy, and (luy L. Steele, Jr. 1996.7'he Java "pM La,~(luage ,5'pcciJication.
Addison-Wesley Publishing Co.Xuedong Ihmng, Fileno Alleva, Hsiao-Wuen Hen,Mei-Yuh Hwang, and Ronald l{osenfeld.
1992.The SPHINX-II speech recognition system: Anoverview.
'l'echnicM l{el)ort CMU-CS-92-112,Carnegie Mellon University School of ComputerScience.ICQ Inc. 1999.
ICQ IRC Services.
http://www.icq.COrn/.Inter'Dan.
1999.
An lnterTran translation system.http://www.airsho.com/transLator3.htm.Snnil Issar.
1997.
A speech interface for forms onWWW.
In Proceedings of the 5th European Con-ferencc on ,5'peech Communication and 7'echnol-ogy, September.iTRiBE Inc. 1996..lil{C. http://virtual.itribe.net/jirc/.Martin Kay.
1967.
Experiments with a powerfi|lparser.
In Proceedings of the 2~M lnternatio~mlCOLING, Angust.Ray Kurzweil.
1999.
The Age of ,5'piritual Ma-chines: I~Tten Computers Exceed tluman h~telli-flence.
Viking Press.Kevin Lenzo.
1998. personal conmmnication.Microsoft Corp. 2000.
MSN TM Messenger Service.http://messenger.msn.com/.M.
Nagao.
1984.
A \['ramework of a nlechanicaltranslation between Japanese and English by anal-ogy principle.
In A. Elithorn and l{.
13aneI:ii, ed-itors, Artificial and \]\]'uma~ Intelligc~cc.
NN.I'Olhlblications.
()\[\[ice of Arti\[icial Intelligence Analysis and F, vahm-lion OAIAE.
1996.
Artificial intelligence -An ex-ecutive overview, http://www.ai.usma.edu:8080/overview/cover.html.Jarkko Oikarinen and l)arren l/.eed.
1993.
Internetrelay chat protocol, ftp://ftp.demon.co.uk/pub/doc/rfc/rfc1738.txt, l{equcst for Comments 1459,Network Worldng (-lroup.Mark Seligman, Mary Flanagan, and Sophie Toole.1998.
Dictated input {'or broad-coverage speechtra.nslation.
In Clare Voss and Fie Reeder, edi-tors, Workshop on Embedded MT' ,h'flstems: De-sign, Consh'uclion, and l'Jvahtatiol~ of 5'9slcms'with an 1147' Component, l,anghorne, Pennsylva-nia, October.
AMTA.Mark Seligman.
1997.
Six issues in speech trans-lation.
In Steven Kra.uwer et al, editors, Spo-ken Language Translation Workshop, pages 83--89,Madrid, July.Terry Winograd.
1.983.
Langua9e as a Co.qnitiveProcess.
Volume 1: Syntax.
Addison-Wesley.Jin Yang and Elke 1).
Lange.
1998.
SYSTllANon AltaVista.
: A user study on real-time ma-chine translation on tile Intcrnet.
In l)avid Far-well et al, editors, Proceedings of the Third Con-ference of the Association for Machine 7;r(msla-lion in lhe Americas (AMTA '98), pages 275-285, Langhorne, Pennsylvania, October.
Springer-Verlag.1045
