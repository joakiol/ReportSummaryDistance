Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 128?131,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPMaximum N-gram HMM-based Name Transliteration: Experiment inNEWS 2009 on English-Chinese CorpusYilu ZhouGeorge Washington Universityyzhou@gwu.eduAbstractWe propose an English-Chinese name transli-teration system using a maximum N-gramHidden Markov Model.
To handle specialchallenges with alphabet-based and character-based language pair, we apply a two-phasetransliteration model by building two HMMmodels, one between English and Chinese Pi-nyin and another between Chinese Pinyin andChinese characters.
Our model improves tradi-tional HMM by assigning the longest priortranslation sequence of syllables the largestweight.
In our non-standard runs, we use aWeb-mining module to boost the performanceby adding online popularity information ofcandidate translations.
The entire model doesnot rely on any dictionaries and the probabilitytables are derived merely from training corpus.In participation of NEWS 2009 experiment,our model achieved 0.462 Top-1 accuracy and0.764 Mean F-score.1 IntroductionIt is in general difficult for human to translateunfamiliar personal names, place names andnames of organizations (Lee et al, 2006).
Onereason is the variability in name translation.
Inmany situations, there is more than one correcttranslation for the same name.
In some languag-es, such as Arabic, it can go up to as many asforty (Arbabi et al, 1994).
Even professionaltranslators find it difficult to identify all varia-tions.
For example, when translating ?Phelps?into Chinese, there are at least 5 different ways totranslate this name: ?????,?
?????,??????,?
?????,?
and ?????,?
withsome more popular than others.The variability in translation implies thecomplexity in name translation that can hardly beaddressed in typical machine translation systems.Machine translation systems are often black box-es where only one translation is provided, whichdo not offer a solution to variability issue.
Theaccuracy of a machine translation system,whether statistical or example-based, largely de-pends on sentence context information.
This con-text information is often not available with nametranslation.
Furthermore, emerging names aredifficult to capture in regular machine translationsystems if they have not been included in train-ing corpus or translation dictionary.
Thus, beingable to translate proper names not only has itsown application area, it will also enhance theperformance of current machine translation sys-tems.In our previous English-Arabic name trans-literation work (Zhou et al, 2008), we proposeda framework for name transliteration using a 2-gram and a 3-gram Hidden Markov Model(HMM).
In this research, we extend our 2-gramand 3-gram HMM to an N-gram HMM where Nis the maximum number of prior translationmapping sequence that can be identified in thetraining corpus.
In our non-standard runs, wealso integrated a Web mining module.
The restof the paper is structured as follows.
Section 2reviews related work; Section 3 describes ouralgorithm; Section 4 discusses implementationand evaluation results are provided in Section 5.Section 6 concludes our work.2 Related WorkResearch in translating proper names has focusedon two strategies: One is to mine translation pairsfrom bilingual online resources or corpora (Leeet.
al, 2006).
The second approach is a directtranslation approach (Chen and Zong, 2008).The first approach is based on the assumptionthat the two name equivalents should share simi-lar relevant context words in their languages.Correct transliteration is then extracted from theclosest matching proper nouns.
The second ap-proach, direct translation, is often done by trans-literation.
Transliteration is the representation ofa word or phrase in the closest correspondingletters or characters of a language with differentalphabet so that the pronunciation is as close aspossible to the original word or phrase (AbdulJa-leel and Larkey, 2003).
Unlike mining-based ap-proach, transliteration can deal with low-frequency proper names, but may generate ill-formed translations.128Transliteration models can be categorized intorule-based approach and statistical approach.
Arule-based approach maps each letter or a set ofletters in the source language to the closestsounding letter or letters in the target languageaccording to pre-defined rules or mapping tables.It relies on manual identification of all translite-ration rules and heuristics, which can be verycomplex and time consuming to build (Darwishet al, 2001).
A statistical approach obtains trans-lation probabilities from a training corpus: pairsof transliterated words.
When new words come,the statistical approach picks the transliterationcandidate with the highest transliteration proba-bilities generated as the correct transliteration.Most statistical-based research used phoneme-based transliteration, relying on a pronunciationdictionary.
Al-Onaizan and Knight showed that agrapheme-based approach out-performed a pho-neme-based approach in Arabic-English transli-teration (Al-Onaizan and Knight, 2002).3 Challenges with Chinese LanguageThere are several challenges in transliteratingEnglish names into Chinese.
First, written Chi-nese is a logogram language.
Thus, a phoneticrepresentation of Chinese characters, Pinyin, isused as an intermediate Romanization.
Ourprocess of translating an English name into Chi-nese consists of two steps: translating Englishword into Pinyin and then mapping Pinyin intoChinese characters.Second, Chinese is not only monosyllabic, butthe pronunciation of each Chinese character isalways composed of one (or none) Consonantunit and one Vowel unit with the Consonant al-ways appears at the beginning.
For example,/EKS/ is one syllable in English but is three syl-lables in Chinese (/E/ + /KE/ + /SI/).
English syl-lables need to be processed in a way that can bemapped to Chinese Pinyin.4 Proposed Maximum N-gram HMMFigure 1 illustrates our name translationframework.
The framework consists of three ma-jor components: 1) Training, 2) Hidden MarkovModel-based Transliteration, and 3) Web Min-ing-enhanced ranking.4.1 TrainingThe training process (Figure 1 Module 1) gene-rates two transliteration probability tables basedon a training corpus of English-Pinyin pair andPinyin-Chinese name pairs.
Pinyin is not pro-vided in the training corpus, but is easy to obtainfrom a Chinese Pinyin table.In order to perform mapping from Englishnames to Chinese Pinyin, an English name isdivided into sub-syllables and this process iscalled Syllabification.
Although many Englishsyllabification algorithms have been proposed,they need to be adjusted.
During syllabification,light vowels are inserted between two continuousconsonants and silent letters are deleted.
We usea finite state machine to implement the syllabifi-cation process.
For example, ?Phelps?
becomes{/ph/ /e/ /l/ /@/ /p/ /@/ /s/ /@/} with ?@?
beinginserted light vowels.Alignment process maps each sub-syllable inan English name to target Pinyin.
The accuracyof Alignment process largely depends on theaccuracy of Syllabification.
Pinyin to Chinesecharacter alignment is more straightforwardwhere each Pinyin syllable (consonant + vowel)is mapped to the corresponding Chinese charac-ter.
Once the alignment is done, occurrence ofeach translation pair can be calculated.
Using thisoccurrence information, we can derive probabili-ties under various situations to support probabili-ty models.We use the Hidden Markov Model which isone of the most popular probability models andhas been used in speech recognition, the humangenome project, consumer decision modeling,etc.
(Rabiner, 1989).
In transliteration, traditionalHMM can be viewed as a 2-gram model wherethe current mapping selection depends on theprevious mapping pair.
We expand it to an N-gram model and use the combination of 1-gram,2-gram, ... , (N-1)-gram and N-gram HMMwhere N is the maximum number of mappingsequence that can be found in training corpus.The goal of our model is to find the candi-date transliteration with the highest translitera-tion probabilities:(1)Where s is the source name to be transliterated,which contains letter string s1s2?
si; t is the tar-get name, which contains letter string t1t2?
ti.In a simple statistical model, or a 1-grammodel, transliteration probability is estimated as:(2)Where)|()......|()|(),......,,,|,......,,,(2211321321nnnnstPstPstPssssttttP=corpusinappearsstimesofcorpusinttotranslatesstimesofstPiiiii ##)|( =)...|..(maxarg)|(maxarg 2121 nn ssstttPstP =129The bigram HMM improves the simple sta-tistical model in that it incorporates context in-formation into a probability calculation.
Thetransliteration of the current letter is dependenton the transliteration of ONE previous letter (oneprevious state in HMM).
Transliteration proba-bility is estimated as:(3)WhereandThe trigram HMM intends to capture evenmore context information by translating the cur-rent letter dependent on the TWO previous let-ters.
Transliteration probability is estimated as:(4)WhereandThis process is continued until the maximummapping sequence is found in the transliterationcorpus.
The final probability estimation is aweighted combination of all N-grams:In our submitted results, we applied ?1=1, ?2=2,?., ?n=N such that longer matched sequence hasa larger contribution in the final probability.
Therationale is that the longer the prior sequenceidentified in training data, the higher probabilitythat the translation sequence is the correct tone.These ?
parameters can be tuned in the future.We call this approach Maximum N-gramHMM.
The same process is conducted for Pinyinto Chinese character translation as shown in thelower part of Figure 1 Module 1.4.2 Translation and RankingOnce the two Maximum N-gram HMM Modelare obtained, new incoming names are translatedby obtaining a letter sequence that maximizes theoverall probability through the HMM (Figure 1Module 2).
This step uses a modified Viterbi?ssearch algorithm (Viterbi 1967).
The originalViterbi?s algorithm only keeps the most optimalpath.
To cope with name translation variations,we keep the top-20 optimal paths for furtheranalysis.4.3 Web Mining ComponentTo boost the transliteration performance we pro-pose to use the Web mining approach, whichanalyzes candidates?
occurrence on the Web),|()......,|)(,|()|(),......,,,|,......,,,(123312211321321?= nnnnntstptsttstPstPssssttttPoccursstimesofttotranslatesstimesofstPiiiii ##)|( =11111 ##),|(????
?>?=iiiiiiiii ttotranslatesstimesoftsgiventtotranslatesstimesoftstP),,|()......,,|(),|()|(),......,,,|,......,,,(21123312211321321?
?= nnnnnnttstpttstPtstpstPssssttttPoccursstimesofttotranslatesstimesofstPiiiii ##)|( =22112211321##),,|(?????????
?>?>?=iiiiiiiiiiiiittotranslatessandttotranslatesstimesoftsandtsgiventtotranslatesstimesofttstP11111 ##),|(????
?>?=iiiiiiiii ttotranslatesstimesoftsgiventtotranslatesstimesoftstP)(......)2()1( 21 HMMgramNHMMgramHMMgramScoreationTransliterFinaln ?++?+?=??
?130(Figure 1 Module 3).
Each one of the top-20transliterations obtained from the previous step issent to a Web search engine using a meta-searchprogram which records the number of documentsretrieved, referred to as Web frequency.
By ex-amining the popularity of all possible translitera-tions on the Internet, bad transliterations can befiltered and their online popularity can serve asan indicator of transliteration correctness.
Thepopularity is estimated by acquiring the numberof documents returned from a search engine us-ing the translation candidate as query.
The finalrank of transliterations is derived from aweighted score of the normalized Web frequencyand the probability score.5 EvaluationNamed Entity Workshop (NEWS) 2009 MachineTransliteration Shared Task provided a trainingcorpus with 31,961 pairs of English and Chinesename translations and 2,896 testing cases.
Wesubmitted one standard run with Maximum N-gram HMM (N-HMM) setting, and two non-standard runs with 3-gram HMM (3-HMM), andMaximum N-gram HMM + Web mining (N-HMM+W).
There are two other runs that wesubmitted which contains error in the results andthey are not discussed here.
We present our eval-uation results in Table 1.Top-1AccF-scoreMRR MAP(Ref)MAP(10)N-HMM 0.456 0.763 0.587 0.456 0.185N-HMM+W0.462 0.764 0.564 0.462 0.1753-HMM 0.458 0.763 0.602 0.458 0.191Table 1: Evaluation Results with Top-10 CandidatesIt is confirmed that Web-mining moduleboosted the performance of N-gram HMM in allmeasure except for MAP(10).
However, the boost-ing effect is small (1.3%).
To our surprise, 3-gram HMM outperformed Maximum N-gramHMM slightly (3% in MAP(10)).
Our best Top-1accuracy is 0.462, and best Mean F-score is0.764 both achieved by N-gram HMM with Webmining module.
We believe this slightly lowerperformance of Maximum N-gram HMM can beimproved with some tuning of weight parame-ters.6 ConclusionsWe propose an English-Chinese name translite-ration system using a maximum N-gram HiddenMarkov Model.
To handle special challengeswith alphabet-based and character-based lan-guage pair, we apply a two-phase transliterationmodel by building two HMM models, one be-tween English and Chinese Pinyin and anotherbetween Chinese Pinyin and Chinese characters.In participation of NEWS 2009 experiment, ourmodel achieved 0.462 Top-1 accuracy and 0.764Mean F-score.
We plan to conduct further studythe impact of Web mining component and findoptimal set of parameters.
Our model does notrely on any existing dictionary and the transla-tion results are entirely based on learning thecorpus data.
In the future, this framework can beextended to other language pairs.AcknowledgmentWe thank the data source provider of this sharedtask fromEnglish-Chinese (EnCh): Haizhou Li, Min Zhang,Jian Su, "A joint source channel model for machinetransliteration", Proc.
of the 42nd ACL, 2004ReferencesAbdulJaleel, N., and Larkey, L. S., Statistical transli-teration for English-Arabic Cross Language In-formation Retrieval, in Proceedings of (CIKM)New Orleans, LA, pp.
139 (2003).Al-Onaizan, Y., and Knight, K., Machine Translitera-tion of Names in Arabic Text, in Proceedingsof the ACL-02 Workshop on ComputationalApproaches to Semitic Languages Philadel-phia, Pennsylvania pp.
1 (2002).Arbabi, M., Fischthal, S. M., Cheng, V. C., and Bart,E., Algorithms for Arabic Name Translitera-tion, IBM Journal of Research and Develop-ment, 38, 183 (1994).Chen, Y., and Zong, C., A Structure-based Model forChinese Organization Name Translation, ACMTransactions on ACL, 7, 1 (2008).Darwish, K., Doermann, D., Jones, R., Oard, D., andRautiainen, M., TREC-10 Experiments at Uni-versity of Maryland CLIR and Video in TREC,Gaithersburg, Maryland (2001).Lee, C.J., Chang, J. S., Jang, J.S.R, Extraction oftransliteration pairs from parallel corpora usinga statistical transliteration model, InformationSciences, 176(1), 67-90 (2006).Rabiner, L. R., A Tutorial on Hidden Markov Modelsand Selected Applications in Speech Recogni-tion, Proceedings of the IEEE, 77, 257?286(1989).Viterbi, A. J., Error Bounds for Convolutional Codesand an Asymptotically Optimum Decoding Al-gorithm, IEEE Transactions on InformationTheory, 13, 260 (1967).Zhou, Y., Huang, F., and Chen, H., Combining prob-ability Models and Web Mining Models: AFramework for Proper Name transliteration, In-formation Technology and Management, 9, 91(2008).131
