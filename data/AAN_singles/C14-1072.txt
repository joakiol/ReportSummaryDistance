Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 762?771, Dublin, Ireland, August 23-29 2014.docrep: A lightweight and efficient document representation frameworkTim Dawborn and James R. Currane-lab, School of Information TechnologiesUniversity of SydneyNSW 2006, Australia{tim.dawborn,james.r.curran}@sydney.edu.auAbstractModelling linguistic phenomena requires highly structured and complex data representations.Document representation frameworks (DRFs) provide an interface to store and retrieve multipleannotation layers over a document.
Researchers face a difficult choice: using a heavy-weightDRF or implement a custom DRF.
The cost is substantial, either learning a new complex system,or continually adding features to a home-grown system that risks overrunning its original scope.We introduce DOCREP, a lightweight and efficient DRF, and compare it against existing DRFs.We discuss our design goals and implementations in C++, Python, and Java.
We transform theOntoNotes 5 corpus using DOCREP and UIMA, providing a quantitative comparison, as well asdiscussing modelling trade-offs.
We conclude with qualitative feedback from researchers whohave used DOCREP for their own projects.
Ultimately, we hope DOCREP is useful for the busyresearcher who wants the benefits of a DRF, but has better things to do than to write one.1 IntroductionComputational Linguistics (CL) is increasingly a data-driven research discipline with researchers us-ing diverse collections of large-scale corpora (Parker et al., 2011).
Representing linguistic phenomenacan require modelling intricate data structures, both flat and hierarchical, layered over the original text;e.g.
tokens, sentences, parts-of-speech, named entities, coreference relations, and trees.
The scale andcomplexity of the data demands efficient representations.
A document representation framework (DRF)should support the creation, storage, and retrieval of different annotation layers over collections of hetero-geneous documents.
DRFs typically store their annotations as stand-off annotations, treating the sourcedocument as immutable and annotations ?stand-off?
with offsets back into the document.Researchers may choose to use a heavy-weight DRF, for example GATE (Cunningham et al., 2002)or UIMA (G?otz and Suhre, 2004), but this can require substantial investment to learn and apply theframework.
Alternatively, researchers may ?roll-their-own?
framework for a particular project.
Whilethis is not inherently bad, our experience is that the scope of such smaller DRFs often creeps, without thebenefits of the features and stability present in mature DRFs.
Moreover, some DRFs are based on objectserialisation, restricting the user to a specific language.
In sum, while DRFs provide substantial benefits,they can come at an opportunity cost to valuable research time.DOCREP aims to solve this problem by proving a light-weight DRF that does not get in the way.
Usinga language-agnostic storage layer enables reuse across different tasks in whatever tools and programminglanguages are most appropriate.
Efficiency is our primary goal, and we emphasise compact serialisationand lazy loading.
Our streaming design is informed by the pipeline operation of UNIX commands.Section 2 compares existing DRFs and annotation schemes.
We describe and introduce DOCREP inSection 3, outlining the design goals and the problems it aims to solve.
We compare DOCREP to UIMAthrough a case study in Section 4, converting OntoNotes to both DRFs.
Section 5 discusses real world usesof DOCREP within our research group and outlines experiences of its use by NLP researchers.
DOCREPThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/762will be useful for any researcher who wants rapid development with multi-layered annotation that per-forms well at scale, but at minimal technical cost.2 BackgroundEasily and efficiently storing and retrieving linguistic annotations over corpora is a core issue for data-driven linguistics.
A number of attempts to formalise linguistic annotation formats have emerged over theyears, including Annotation Graphs (AG) (Bird and Liberman, 1999), the Linguistic Annotation Format(LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF)(Ide and Suderman, 2007).
GRAF is a serialisation of the LAF model, using XML stand-off annotationsto store layers of annotation.
The GRAF representation is sufficiently abstract as to be used as a pivotformat between other annotation schemes.
Ide and Suderman (2009) use GRAF as an intermediate formatto convert annotations between GATE and UIMA.
The MASC corpus (Ide et al., 2010) has multiple layersof annotation which are distributed in GRAF.
Neumann et al.
(2013) provide insight into the effectivenessof GRAF as a format for corpus distribution when they import MASC into an annotation database.
Theselinguistic annotation formalisations provide a useful set of requirements for DRFs.
While these abstractformalisations are constructive from a theoretical perspective, they do not take into account the runtimeperformance of abstract representations, nor their ease of use for programmers.Several DRFs have been developed and used within the CL community.
GATE (Cunningham et al.,2002; Cunningham, 2002) has a focus on the human annotation of textual documents.
While it has alarge collection of extensions and plugins, it was not designed in a matter than suits web-scale corpusprocessing.
Additionally, GATE is limited to Java, making integration with CL tools written in otherlanguages difficult.
UIMA (G?otz and Suhre, 2004; Lally et al., 2008) is a Java framework for providingannotations over the abstract definition of documents, providing functionality to link between differentviews of the same document (e.g.
translations of a document).
UIMA calls these different views different?subjects of analysis?
(SOFA).
When UIMA was adopted into the Apache Software Foundation, a C++version of the UIMA API was developed.
However, it appears to lag behind behind the Java API in devel-opment effort and usefulness, with many undocumented components, numerous external dependencies,and with substantial missing functionality provided by the Java API.
Additionally, the C++API is writtenin an non-idiomatic manner, making it harder for developers to use.Publicly available CL pipelining tools have emerged in recent years, providing a way to perform a widerange of CL processes over documents.
The Stanford NLP pipeline1is one such example, but is Java onlyand must be run on a single machine.
CURATOR (Clarke et al., 2012) provides a cross-language NLPpipeline using Thrift to provide cross-language communication and RPC.
CURATOR requires a server tocoordinate the components within the pipeline.
Using pipelining functionality within a framework oftenthe inspection of per-component contributions more difficult.
We are not aware of any DRFs which use astreaming model to utilise UNIX pipelines, a paradigm CL researchers are already familiar with.3 The docrep document representation frameworkDOCREP (/d6krEp/), a portmanteau of document representation, is a lightweight, efficient, and moderndocument representation framework for NLP systems that is designed to be simple to use and intuitive towork with.
We use the term lightweight to compare it to the existing document representation systemsused within the CL community, the main one being UIMA.
The overhead of using DOCREP instead of aflat-file format is minimal, especially in comparison to large bulky frameworks.Our research group has used DOCREP as its primary data storage format in both research projects andcommercial projects since mid-2012.
DOCREP has undergone an iterative design process during this timeas limitations and issues arose, allowing modelling issues to be ironed out and a set of best practices to beestablished.
These two years of solid use by CL researchers has resulted in a easy to use DRF we believeis suitable for most CL applications and researchers.DOCREP was designed with streaming in mind, facilitating from the data storage layer upwards theability for CL applications to utilise parallel processing.
This streaming model is a model that many1http://nlp.stanford.edu/software/corenlp.shtml763CL researchers are already familiar with from writing UNIX pipelines (Church, 1994; Brew and Moens,2002), again reducing the overhead required to use DOCREP.DOCREP is not a new language that researchers need to learn.
Instead, it is a serialisation protocoland set of APIs to interact with annotations and documents.
Using DOCREP is as simple as importing thepackage in ones favourite programming language and annotating class definitions appropriately.
Neithera separate compilation step nor an external annotation definition file are required.3.1 Idiomatic APIsOne of the motivations for constructing DOCREP was the lack of a good document representation frame-work in programming languages other than Java.
We have implemented DOCREP APIs in three com-monly used programming languages in the CL community: C++, Python, and Java.
All of these APIs areopen source and publicly available on GitHub,2released under the MIT licence.
The C++API is writtenin C++11, the Python API supports version 2.7 as well as versions ?
3.3, and the Java API supportsversions ?
6.
All three APIs are setup to use the standard build tools for the language.When implementing these APIs, we aimed to make the interface as similar as possible between thethree languages, while still feeling idiomatic within that language.
Using the API should feel naturalfor that language.
Figure 1 shows an example set of identical model definitions in C++, Python, andJava.
This example defines a Token type, a Sent type spanning over a series of sequential Tokenannotations, and a Doc type.
The Token and Sent types include some annotation attributes.
Annota-tion instances are stored on the document in Stores.
Apart from the missing implementations of theSchema constructors in the C++example, these are complete and runnable definitions of annotationtypes in DOCREP.
The Schema classes in the C++example are automatically induced via runtime classintrospection in the Python and Java APIs; functionality which C++does not possess.3.2 Serialisation protocolWe chose to reuse an existing serialisation format for DOCREP.
This allows developers to use existingserialisation libraries for processing DOCREP streams in languages we do not provide a DOCREP API for.One of our design considerations when creating DOCREP was a desire for the protocol to be self-describing.
With a self-describing protocol, no external files need to be associated with a serialisedstream in order to know how to interpret the serialised data.
This requires an efficient serialisationprotocol because including the definition of the type system with each document comes at a cost.
This isdifferent to UIMA which requires its XML type definition files in order to deserialise the serialised data.The four main competitors in the web-scale binary serialisation format space are BSON,3Mes-sagePack,4Protocol Buffers,5and Thrift.6BSON and MessagePack are similar in their design.
They bothaim to provide a general purpose data serialisation format for common data types and data structures.BSON is used as the primary data representation within the MongoDB database.
Protocol Buffers andThrift work in a similar manner to one another.
Their serialisation protocols are not self describing andrequire an external file which defines how to interpret the messages on the stream.
In this external file,users define the structure of the messages they wish to serialise and deserialise, and use a provided toolto convert this external file into source code for their programming language of choice.
Protocol Buffersand Thrift also provide RPC functionality, however this was not needed for our situation.
Thrift is usedby the CURATOR NLP pipeline (Clarke et al., 2012) to provide both serialisation and RPC functionalitybetween cross-language disjoint components in the pipeline.After designing the serialisation protocol for DOCREP, we implemented it on top of these binary se-rialisation formats in order to compare the size of the serialised data and the speed at which it could becompressed.
As a simple stand-off annotation task, we chose to use the CoNLL 2003 NER shared task2https://github.com/schwa-lab/libschwa3http://bsonspec.org/4http://msgpack.org/5http://code.google.com/p/protobuf/6http://thrift.apache.org/764struct Token : public dr::Ann {dr::Slice<uint64_t> span;std::string raw;std::string norm;class Schema;};struct Sent : public dr::Ann {dr::Slice<Token*> span;bool is_headline;class Schema;};struct Doc : public dr::Doc {dr::Store<Token> tokens;dr::Store<Sent> sents;class Schema;};struct Token::Schema : public dr::Ann::Schema<Token> {DR_FIELD(&Token::span) span;DR_FIELD(&Token::raw) raw;DR_FIELD(&Token::norm) norm;Schema(void);};struct Sent::Schema : public dr::Ann::Schema<Sent> {DR_POINTER(&Sent::span, &Doc::tokens) tokens;DR_FIELD(&Sent::is_headline) is_headline;Schema(void);};struct Doc::Schema : public dr::Doc::Schema<Doc> {DR_STORE(&Doc::tokens) tokens;DR_STORE(&Doc::sents) sents;Schema(void);};(a) C++exampleclass Token(dr.Ann):span = dr.Slice()raw = dr.Text()norm = dr.Text()class Sent(dr.Ann):span = dr.Slice(Token)is_headline = dr.Field()class Doc(dr.Doc):tokens = dr.Store(Token)sents = dr.Store(Sent)(b) Python example@dr.Annpublic class Token extends AbstractAnn {@dr.Field public ByteSlice span;@dr.Field public String raw;@dr.Field public String norm;}@dr.Annpublic class Sent extends AbstractAnn {@dr.Pointer public Slice<Token> span;@dr.Field public bool isHeadline;}@dr.Docpublic class Doc extends AbstractDoc {@dr.Store public Store<Token> tokens;@dr.Store public Store<Sent> sents;}(c) Java exampleFigure 1: Examples of identical type definitions using the DOCREP API in C++, Python, and Java.Self- Uncompressed DEFLATE Snappy LZMAdescribing Time Size Time Size Time Size Time SizeOriginal data ?
?
31.30 1.0 5.95 0.1 9.81 39 0.39BSON X 2.5 188.42 5.3 30.32 0.6 56.36 441 16.22MessagePack X 1.6 52.15 3.2 16.61 0.3 24.82 61 4.36Protocol Buffers ?
1.4 51.51 3.5 18.52 0.3 29.31 67 5.13Thrift ?
1.0 126.12 3.5 20.64 0.4 33.69 224 10.99Table 1: A comparison of binary serialisation libraries being used as the DOCREP serialisation format.Times are reported in seconds and sizes in MB.
MessagePack and BSON include the full type systemdefinition on the stream for each document whereas Protocol Buffers and Thrift do not.765data, randomly sampling around 50 MB worth of sentences from the English training data.
The seriali-sation stores the documents, sentences, and tokens, along with the POS and NER tags for the tokens.
Theappropriate message specification files were written for Protocol Buffers and Thrift, and the type systemwas serialised as a header for BSON and MessagePack.Table 1 shows the results of this experiment.
The reported size of the original data is smaller than thesample size as we chose to output it in a more concise textual representation than the data was originallydistributed in.
BSON performs noticeably worse than the others, in terms of both size and speed.
Whileserialising slightly faster, the size of the serialised data produced by Thrift is more then double the size ofboth MessagePack and Protocol Buffers, and does not compress quite as well.
MessagePack compressedslightly better than Protocol Buffers and was on par in terms of speed, while being self-describing on thestream.
The result of this experiment and some similar others lead us to conclude that MessagePack wasthe best serialisation format for DOCREP to use.At the time of writing, the Python and Java DOCREP APIs use the official MessagePack libraries forthose languages.
We implemented our own C++MessagePack library to facilitate laziness.3.3 LazinessThe serialisation protocol was designed such that we could make the streaming aspect of DOCREP as effi-cient as possible.
Before each collection of annotation objects appears in the serialised data, the numberof bytes used to store the serialised annotations is stored.
If the current application is not interested inthe particular annotation types that are about to be read in, it can simply skip over the correct number ofbytes without having to deserialise the internal MessagePack structure.All three of our APIs implement this laziness.
Only the types of annotations that the applicationspecifies interest in will be deserialised at runtime.
The other types of annotations will simply be kept intheir serialised format and written back out to the output stream unmodified.
This is also true for attributeson annotations that the current application is not interested in.
The Python API provides an option tofully instantiate each of the types at runtime, even if you have not defined classes for them.
Unknownannotation types will have classes created at runtime based on the schema of the types described in theserialisation protocol.3.4 Processing toolsWe trade-off performance against easy inspection of files.
We provide a set of command-line tools formanipulating, filtering, and distributing DOCREP streams.
The command-line tools mimic the standardset of UNIX tools used to process textual files as well as some other stream introspection and statisticsgathering tools.
All of these tools and their uses are documented on the DOCREP website.7Our providedtoolbox for processing DOCREP streams contains tools for counting, visualising, filtering, ordering, par-titioning, and exporting DOCREP streams.
Due to space limitations in this paper, we are unable to go intothese tools in detail.Below are two examples of some of the tools in action.
The first example filters the documents by aregular expression comparison against their ID attribute, and then outputs the ID of the document withthe most number of tokens.
The second randomly chooses 10 documents from a stream, passing them toanother tool, and then opens the first returned document in the stream visualiser.$ dr grep 'doc.id ?
/x-\d+/' corpus.dr | dr count -s tokens | sort -rn | head -n 1$ dr sample -n 10 corpus.dr | ./my-tool | dr head -n 1 | dr less3.5 Streaming modelEmphasising the fact that the DOCREP protocol is a streaming protocol, combining multiple DOCREPfiles together is as simple as concatenating the files together.
The DOCREP deserialisers expect an inputstream to contain zero or more serialised documents.
Being able to easily distribute all documents in acorpus along with their annotation layers as a single file is very attractive.7https://github.com/schwa-lab/libschwa766This kind of streaming model makes distributed processing very easy using a typical work queuemodel.
A distributed pipeline ?source?
can serve the documents from the DOCREP stream by readingthem off the input stream without having to deserialise them (subsection 3.3) and a ?sink?
can simplyconcatenate the received documents together to the output stream, again without having to deserialisethem.
We provide a DOCREP source and sink distributed processing tool along with APIs for easilywriting worker clients.
The distribution is achieved through ?MQ8which allows for both scale-up andscale-out distributed processing out of the box without the need for a separate controller process tomanage communication between client processes.4 Case study: OntoNotes 5The OntoNotes 5 corpus (Pradhan et al., 2013) is a large corpus of linguistically annotated documentsfrom multiple genres in three different languages.
This 5th release covers newswire, broadcast news,broadcast conversation, and web data in English and Chinese, a pivot corpus in English, and newswiredata in Arabic.
Roughly half of the broadcast conversation data is parallel data, with some of the docu-ments providing tree-to-tree alignments.
Of the 15 710 documents in the corpus, 13 109 are in English,2002 are in Chinese, and 599 are in Arabic.Each of the documents in the OntoNotes 5 corpus contain multiple layers of syntactic and semanticannotations.
It builds upon the Penn Treebank for syntax and PropBank for predicate-argument structure,adding named entities, coreference, and word sense disambiguation layers to some documents.The annotations in the OntoNotes 5 corpus are provided in two different formats: as a series of flatfiles (340 MB) per document with each file containing one annotation layer, and as a relational databasein the form of a SQL file (5812 MB).
Both of these data formats have usability issues.
Working withthe flat files requires parsing each of the different file formats and aligning the data between the files forthe same document.
Working with the database requires working out how the tables are related to oneanother, as well as knowledge of SQL, or having access to an efficient API for querying the database.To outline the effectiveness of document representation frameworks, and in particular the efficiencyof DOCREP, we provide code to convert the OntoNotes 5 corpus into both DOCREP and UIMA represen-tations, comparing the conversion time, resultant size on disk, and ease of doing this conversion.
Weprovide conversion scripts in all three languages for DOCREP and in Java and C++for UIMA.
Addi-tionally, we also provide a verification script, reproducing the original OntoNotes 5 flat files from thedocument representation form, ensuring that no data was lost in the conversion.4.1 Modelling decisionsThe choices made on how to model the different annotation layers were almost identical in UIMA andDOCREP.
The main difference occurs when you have an annotation over a sequential span of otherannotations.
UIMA has no way to model this directly.
The most common way users choose to modelthis is as a normal Annotation subtype with its begin offset set to the begin offset of the firstcovered annotation and its end offset set to the end offset of the last covered annotation.
An example ofthis situation is named entity annotations.
In OntoNotes, named entities are represented as annotationsover a sequence of token annotations.
How this is represented in UIMA is shown in the XML snippet inFigure 2.
The main disadvantage in this modelling approach is that there is then no direct representationthat the named entity annotation is an annotation over a sequence of token annotations.
In DOCREP,named entity annotation is directly modelled as a sequence of token annotations.
The DOCREP definitionfor the named entity type is shown on the right hand side of Figure 2.DOCREP does not allow for the direct modelling of cross-document information.
This occurs in theOntoNotes 5 corpus in the form of the parallel document and parallel tree information.
Because DOCREPis a streaming protocol, the documents are thought of as independent from one another and as such, noformal relationships between the documents can be made at the framework level.
This parallel documentinformation can still be be stored as metadata on the documents.
This situation is dealt with in UIMA bythe SOFA.8http://www.zeromq.org/767<typeDescription><name>ontonotes5.to_uima.types.NamedEntity</name><description/><supertypeName>uima.tcas.Annotation</supertypeName><features><featureDescription><name>tag</name><description>The NE tag.</description><rangeTypeName>uima.cas.String</rangeTypeName></featureDescription><featureDescription><name>startOffset</name><description>Character offset into the start token.</description><rangeTypeName>uima.cas.Integer</rangeTypeName></featureDescription><featureDescription><name>endOffset</name><description>Character offset into the end token.</description><rangeTypeName>uima.cas.Integer</rangeTypeName></featureDescription></features></typeDescription>@dr.Annpublic class NamedEntity extends AbstractAnn {@dr.Pointer public Slice<Token> span;@dr.Field public String tag;@dr.Field public int startOffset;@dr.Field public int endOffset;}Figure 2: Defining the named entity annotation type in UIMA (left) and the DOCREP Java API (top-right).UIMA DOCREPJava Java Java Java C++C++C++Java C++PythonXMI XCAS bin cbin XMI XCAS bin ?
?
?Conversion time 25 25 25 25 77 77 77 12 12 27Serialisation time 131 122 2103 76 630 611 695 61 23 32Size on disk 1894 3252 1257 99 2141 3252 2135 371 371 371Table 2: A comparison of the resources required to represent the OntoNotes 5 corpus in UIMA andDOCREP.
Times are reported in seconds and sizes are reported in MB.4.2 Empirical resultsIn these experiments, we first load all of the data into memory from the database for the current documentwe are processing.
This data is stored in an object structure which knows nothing about documentrepresentation frameworks.
We then convert this object representation into the appropriate UIMA andDOCREP annotations, recording how long the conversion took.
The UIMA and DOCREP versions of thedocuments are then serialised to disk, recording how long the serialisation took and the resultant sizeon disk.
All of these performance experiments were run on the same isolated machine, running 64-bitUbuntu 12.04, using OpenJDK 1.7, CPython 2.7, and gcc 4.8.In order to provide a fair comparison between UIMA and DOCREP, we perform the conversion usingboth the Java and C++UIMA APIs, as well as using all three DOCREP APIs (Java, C++, and Python).The code to load the data from the database and construct the in-memory object structure was commonbetween the UIMA and DOCREP conversions.
For UIMA, we serialise in all available output formats: boththe XMI and XCAS XML formats, the binary format (bin), and the compressed binary (cbin) format.
TheUIMA C++API does not appear to support output in the compressed binary format.The result of this conversion process can be seen in Table 2.
The first row shows the accumulatedtime taken to convert all of the documents from their in-memory representation into UIMA and DOCREPannotations.
As visible in the table, DOCREP performs this conversion twice as fast as UIMA in Javaand six times as fast as UIMA in C++?The second row shows the accumulated time taken to serialise768Flat DOCREP UIMA UIMA UIMA UIMA SQL MySQL MySQLfiles XMI XCAS bin cbin -indices +indicesUncompressed 340 371 1894 3252 1257 99 4560 4303 5812gzip (DEFLATE) 52 115 268 330 375 66 646 ?
?xz (LZMA) 30 69 144 185 150 65 262 ?
?Table 3: A comparison of the how well each of the annotation serialisation formats compress usingstandard compression libraries.
All sizes are reported in MB.all of the documents to disk.
DOCREP serialises up to 34 times faster than UIMA in Java, dependingon the UIMA output format, and up to 30 times faster in C++?The third row in this table shows theaccumulated serialisation size on disk.
Apart from the compressed binary output format in UIMA (cbin),DOCREP serialisation requires up to nine times less space than UIMA?We are unsure why the sizes for thedifferent output formats in UIMA do not match up between the Java and C++APIs?We are also unsurewhy the UIMA Java binary serialisation is so slow, especially in comparison to the compressed binaryserialisation.Table 3 shows how well each of the serialisation formats compress using three standard compressionlibraries.
Each of these compression libraries were run with their default settings.
The files generatedby UIMA as well as the ?flat file?
files were first placed into a tarball so that the compression algorithmscould be run over the whole corpus instead of per document.
The ?flat files?
used were the originalOntoNotes 5 flat files containing the annotation layers that were converted.
The SQL numbers are usingthe original OntoNotes 5 SQL file.
The MySQL numbers are obtained after loading the original SQL intoa MySQL database and obtaining table and index sizes from the information_schema.tablestable.
The MySQL database was not altered from the initial import.
Unsurprisingly, the DOCREP binaryrepresentation does not compress as well as textual serialisation formats with lots of repetition, such asXML or the original stand-off annotation files.
However, under all of these reported situations, apartfrom the UIMA compressed binary format, our DOCREP representation is two to five times smaller thanits UIMA counterpart, and 15 times smaller than the representation in MySQL.
The UIMA compressedbinary (cbinary) format has already been compressed so it is unsurprising that compressing it furthermakes little difference.5 UsabilityWe have primarily evaluated the usefulness of DOCREP from an efficiency perspective, reporting timeand space requirements for a complex corpus conversion.
In this section, we provide feedback from NLPresearchers in our lab who have been using DOCREP over the past two years for a variety of NLP tasks.As researchers ourselves, we are aware of how valuable research time is.
We provide these real-worldexamples of DOCREP?s use to solidifying that DOCREP is a valuable tool for researchers.Coreference DOCREP is a great tool for this project as all we want to do is develop a good coreferencesystem; we do not want to have to worry about the storage of data.
Having an API in Python issuper convenient, allowing us to write code that changes frequently as we try new ideas.
Relatedpublication: Webster and Curran (2014)Event Linking Some work on Event Linking sought to work with gold annotations on one hand, andknowledge from web-based hyperlinks on the other.
For some processes these data sources wereto be treated identically, and for some differently.
DOCREP?s extensibility easily supported thisuse-case, while providing a consistent polymorphic abstraction that made development straightfor-ward, while incorporating many other layers of annotation such as extracted temporal relations.Separately, describing the relationship between a pair of documents in DOCREP was a challenginguse-case that required more engineering and fore-thought than most DOCREP applications so far.Related publication: Nothman et al.
(2012).769Named Entity Linking Our approach to NEL uses a pipeline of components and we initially wroteour own DRF using Python?s object serialisation.
While this worked well initially, we accruedtechnical debt as we added features with minimal refactoring.
Before too long, a substantial partof our experiment runtime was devoted to dataset loading and storage.
DOCREP made this easierand using UNIX pipelines over structured document objects is a productive workflow.
Relatedpublications: Radford et al.
(2012); Pink et al.
(2013).Quote Extraction and Attribution For this task we performed experiments over four corpora, all withdistinct data formats and assumptions.
Our early software loaded each format into memory, whichwas a slow, error-prone, and hard-to-debug process.
This approach became completely unusablewhen we decided to experiment with coreference systems, as it introduced even more unique dataformats.
Converting everything to DOCREP greatly simplified the task, as we could represent ev-erything we needed efficiently, and within one representation system.
We also gained a nice speedboost, and were able to write a simple set of tests that examined a given DOCREP file for validity,which greatly improved our code quality.
Related publication: O?Keefe et al.
(2013).Slot Filling Being one of the last stages in an NLP pipeline, slot filling utilises all of the documentinformation it can get its hands on.
Being able to easily accept annotation layers from prior NLPcomponents allows us to focus on slot filling instead of component integration engineering.
Havingaccess to a multi-language API means we are able to write efficiency-critical code in C++and themore experimental and dynamic components in Python.6 ConclusionWe present a light-weight and easy-to-use document representation framework for the busy NLP re-searcher who wants to model document structure, but does not want to use a heavy-weight DRF.
Weprovide empirical evidence of the efficiency of DOCREP, and provide insights into its use within ourresearch group over the past two years.
We believe NLP other researchers will benefit from DOCREP asthey are now able to utilise the usefulness of a DRF without it getting in the way of their research time.AcknowledgmentsWe would like to thank the anonymous reviewers for their useful feedback.
We would also like to thankWill Radford and Joel Nothman for their contributions to this paper as well as to DOCREP itself over thepast years.
This work was supported by ARC Discovery grant DP1097291 and the Capital Markets CRCComputable News project.ReferencesSteven Bird and Mark Liberman.
1999.
A formal framework for linguistic annotation.
Speech Commu-nication, 33:23?60.Chris Brew and Marc Moens.
2002.
Data-intensive linguistics.
HCRC Language Technology Group,University of Edinburgh.Kenneth Ward Church.
1994.
Unix?
for poets.
Notes of a course from the European Summer School onLanguage and Speech Communication, Corpus Based Methods.James Clarke, Vivek Srikumar, Mark Sammons, and Dan Roth.
2012.
An NLP curator (or: How I learnedto stop worrying and love NLP pipelines).
In Proceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12).
Istanbul, Turkey.Hamish Cunningham.
2002.
GATE, a general architecture for text engineering.
Computers and theHumanities, 36:223?254.Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan.
2002.
GATE: an archi-tecture for development of robust HLT applications.
In Proceedings of 40th Annual Meeting of theAssociation for Computational Linguistics, pages 168?175.
Association for Computational Linguis-tics, Philadelphia, Pennsylvania, USA.770T.
G?otz and O. Suhre.
2004.
Design and implementation of the UIMA common analysis system.
IBMSystems Journal, 43(3):476?489.Nancy Ide, Collin Baker, Christiane Fellbaum, and Rebecca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by the people.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 68?73.
Uppsala, Sweden.Nancy Ide and Laurent Romary.
2004. International standard for a linguistic annotation framework.Natural Language Engineering, 10(3-4):211?225.Nancy Ide and Laurent Romary.
2006.
Representing linguistic corpora and their annotations.
In Pro-ceedings of the Fifth Language Resources and Evaluation Conference LREC.Nancy Ide and Keith Suderman.
2007.
GrAF: A graph-based format for linguistic annotations.
InProceedings of the Linguistic Annotation Workshop, pages 1?8.
Association for Computational Lin-guistics, Prague, Czech Republic.Nancy Ide and Keith Suderman.
2009.
Bridging the Gaps: Interoperability for GrAF, GATE, and UIMA.In Proceedings of the Third Linguistic Annotation Workshop, pages 27?34.
Association for Computa-tional Linguistics, Suntec, Singapore.Adam Lally, Karin Verspoor, and Eoric Nyberg.
2008.
Unstructured Information Management Architec-ture (UIMA) Version 1.0.
Standards Specification 5, OASIS.Arne Neumann, Nancy Ide, and Manfred Stede.
2013.
Importing MASC into the ANNIS linguisticdatabase: A case study of mapping GrAF.
In Proceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 98?102.
Sofia, Bulgaria.Joel Nothman, Matthew Honnibal, Ben Hachey, and James R. Curran.
2012.
Event linking: groundingevent reference in a news archive.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers), pages 228?232.
Jeju, Korea.Tim O?Keefe, James R. Curran, Peter Ashwell, and Irena Koprinska.
2013.
An annotated corpus ofquoted opinions in news articles.
In Proceedings of the 51st Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers), pages 516?520.
Association for ComputationalLinguistics, Sofia, Bulgaria.Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2011.
English Gigaword FifthEdition.
Technical report, Linguistic Data Consortium, Philadelphia.Glen Pink, Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Daniel Tse, and James R.Curran.
2013.
SYDNEY CMCRC at TAC 2013.
In Proceedings of the Text Analysis Conference.National Institute of Standards and Technology, Gaithersburg, MD USA.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj?orkelund, OlgaUryupina, Yuchen Zhang, and Zhi Zhong.
2013.
Towards Robust Linguistic Analysis usingOntoNotes.
In Proceedings of the Seventeenth Conference on Computational Natural LanguageLearning, pages 143?152.
Sofia, Bulgaria.Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Glen Pink, Daniel Tse, and James R.Curran.
2012.
(Almost) Total Recall ?
SYDNEY CMCRC at TAC 2012.
In Proceedings of the TextAnalysis Conference.
National Institute of Standards and Technology, Gaithersburg, MD USA.Kellie Webster and James R. Curran.
2014.
Low memory incremental coreference resolution.
In Pro-ceedings of COLING 2014.
The COLING 2014 Organizing Committee, Dublin, Ireland.
To appear.771
