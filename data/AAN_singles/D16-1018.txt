Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183?191,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsContext-Dependent Sense Embedding?Lin Qiu?
and Kewei Tu?
and Yong Yu??
Shanghai Jiao Tong University, Shanghai, China, {lqiu,yyu}@apex.sjtu.edu.cn?
ShanghaiTech University, Shanghai, China, tukw@shanghaitech.edu.cnAbstractWord embedding has been widely studied andproven helpful in solving many natural lan-guage processing tasks.
However, the ambi-guity of natural language is always a prob-lem on learning high quality word embed-dings.
A possible solution is sense embed-ding which trains embedding for each senseof words instead of each word.
Some re-cent work on sense embedding uses contextclustering methods to determine the senses ofwords, which is heuristic in nature.
Otherwork creates a probabilistic model and per-forms word sense disambiguation and senseembedding iteratively.
However, most of theprevious work has the problems of learningsense embeddings based on imperfect wordembeddings as well as ignoring the depen-dency between sense choices of neighboringwords.
In this paper, we propose a novelprobabilistic model for sense embedding thatis not based on problematic word embeddingof polysemous words and takes into accountthe dependency between sense choices.
Basedon our model, we derive a dynamic program-ming inference algorithm and an Expectation-Maximization style unsupervised learning al-gorithm.
The empirical studies show that ourmodel outperforms the state-of-the-art modelon a word sense induction task by a 13% rela-tive gain.1 IntroductionDistributed representation of words (aka word em-bedding) aims to learn continuous-valued vectors to?The second author was supported by the National NaturalScience Foundation of China (61503248).represent words based on their context in a large cor-pus.
They can serve as input features for algorithmsof natural language processing (NLP) tasks.
Highquality word embeddings have been proven helpfulin many NLP tasks (Collobert and Weston, 2008;Turian et al, 2010; Collobert et al, 2011; Maaset al, 2011; Chen and Manning, 2014).
Recently,with the development of deep learning, many novelneural network architectures are proposed for train-ing high quality word embeddings (Mikolov et al,2013a; Mikolov et al, 2013b).However, since natural language is intrinsicallyambiguous, learning one vector for each word maynot cover all the senses of the word.
In the case of amulti-sense word, the learned vector will be aroundthe average of all the senses of the word in the em-bedding space, and therefore may not be a good rep-resentation of any of the senses.
A possible solutionis sense embedding which trains a vector for eachsense of a word.
There are two key steps in trainingsense embeddings.
First, we need to perform wordsense disambiguation (WSD) or word sense induc-tion (WSI) to determine the senses of words in thetraining corpus.
Then, we need to train embeddingvectors for word senses according to their contexts.Early work on sense embedding (Reisinger andMooney, 2010; Huang et al, 2012; Chen et al,2014; Neelakantan et al, 2014; Kageback et al,2015; Li and Jurafsky, 2015) proposes context clus-tering methods which determine the sense of a wordby clustering aggregated embeddings of words in itscontext.
This kind of methods is heuristic in natureand relies on external knowledge from lexicon likeWordNet (Miller, 1995).183Recently, sense embedding methods based oncomplete probabilistic models and well-definedlearning objective functions (Tian et al, 2014; Bar-tunov et al, 2016; Jauhar et al, 2015) becomemore popular.
These methods regard the choice ofsenses of the words in a sentence as hidden vari-ables.
Learning is therefore done with expectation-maximization style algorithms, which alternate be-tween inferring word sense choices in the trainingcorpus and learning sense embeddings.A common problem with these methods is thatthey model the sense embedding of each center worddependent on the word embeddings of its contextwords.
As we previously explained, word embed-ding of a polysemous word is not a good repre-sentation and may negatively influence the qual-ity of inference and learning.
Furthermore, thesemethods choose the sense of each word in a sen-tence independently, ignoring the dependency thatmay exist between the sense choices of neighbor-ing words.
We argue that such dependency is im-portant in word sense disambiguation and thereforehelpful in learning sense embeddings.
For exam-ple, consider the sentence ?He cashed a check atthe bank?.
Both ?check?
and ?bank?
are ambiguoushere.
Although the two words hint at banking relatedsenses, the hint is not decisive (as an alternative in-terpretation, they may represent a check mark at ariver bank).
Fortunately, ?cashed?
is not ambiguousand it can help disambiguate ?check?.
However, ifwe consider a small context window in sense em-bedding, then ?cashed?
cannot directly help disam-biguate ?bank?.
We need to rely on the dependencybetween the sense choices of ?check?
and ?bank?
todisambiguate ?bank?.In this paper, we propose a novel probabilisticmodel for sense embedding that takes into accountthe dependency between sense choices of neighbor-ing words.
We do not learn any word embeddings inour model and hence avoid the problem with em-bedding polysemous words discussed above.
Ourmodel has a similar structure to a high-order hiddenMarkov model.
It contains a sequence of observablewords and latent senses and models the dependencybetween each word-sense pair and between neigh-boring senses in the sequence.
The energy of neigh-boring senses can be modeled using existing wordembedding approaches such as CBOW and Skip-gram (Mikolov et al, 2013a; Mikolov et al, 2013b).Given the model and a sentence, we can perform ex-act inference using dynamic programming and getthe optimal sense sequence of the sentence.
Ourmodel can be learned from an unannotated corpusby optimizing a max-margin objective using an al-gorithm similar to hard-EM.Our main contributions are the following:1.
We propose a complete probabilistic model forsense embedding.
Unlike previous work, wemodel the dependency between sense choicesof neighboring words and do not learn senseembeddings dependent on problematic wordembeddings of polysemous words.2.
Based on our proposed model, we derive anexact inference algorithm and a max-marginlearning algorithm which do not rely on ex-ternal knowledge from any knowledge base orlexicon (except that we determine the numbersof senses of polysemous words according to anexisting sense inventory).3.
The performance of our model on contex-tual word similarity task is competitive withprevious work and we obtain a 13% relativegain compared with previous state-of-the-artmethods on the word sense induction task ofSemEval-2013.The rest of this paper is organized as follows.
Weintroduce related work in section 2.
Section 3 de-scribes our models and algorithms in detail.
Wepresent our experiments and results in section 4.
Insection 5, a conclusion is given.2 Related WorkDistributed representation of words (aka word em-bedding) was proposed in 1986 (Hinton, 1986;Rumelhart et al, 1986).
In 2003, Bengio et al(2003) proposed a neural network architecture totrain language models which produced word em-beddings in the neural network.
Mnih and Hin-ton (2007) replaced the global normalization layerof Bengio?s model with a tree-structure to accel-erate the training process.
Collobert and Weston(2008) introduced a max-margin objective function184to replace the most computationally expensive max-likelihood objective function.
Recently proposedSkip-gram model, CBOW model and GloVe model(Mikolov et al, 2013a; Mikolov et al, 2013b; Pen-nington et al, 2014) were more efficient than tradi-tional models by introducing a log-linear layer andmaking it possible to train word embeddings witha large scale corpus.
With the development of neu-ral network and deep learning techniques, there havebeen a lot of work based on neural network mod-els to obtain word embedding (Turian et al, 2010;Collobert et al, 2011; Maas et al, 2011; Chen andManning, 2014).
All of them have proven that wordembedding is helpful in NLP tasks.However, the models above assumed that oneword has only one vector as its representation whichis problematic for polysemous words.
Reisingerand Mooney (2010) proposed a method for con-structing multiple sense-specific representation vec-tors for one word by performing word sense dis-ambiguation with context clustering.
Huang etal.
(2012) further extended this context cluster-ing method and incorporated global context to learnmulti-prototype representation vectors.
Chen et al(2014) extended the context clustering method andperformed word sense disambiguation according tosense glosses from WordNet (Miller, 1995).
Nee-lakantan et al (2014) proposed an extension of theSkip-gram model combined with context clusteringto estimate the number of senses for each word aswell as learn sense embedding vectors.
Instead ofperforming word sense disambiguation tasks, Kage-back et al (2015) proposed the instance-context em-bedding method based on context clustering to per-form word sense induction tasks.
Li and Jurafsky(2015) introduced a multi-sense embedding modelbased on the Chinese Restaurant Process and appliedit to several natural language understanding tasks.Since the context clustering based models areheuristic in nature and rely on external knowledge,recent work tends to create probabilistic models forlearning sense embeddings.
Tian et al (2014)proposed a multi-prototype Skip-gram model anddesigned an Expectation-Maximization (EM) algo-rithm to do word sense disambiguation and learnsense embedding vectors iteratively.
Jauhar et al(2015) extended the EM training framework andretrofitted embedding vectors to the ontology ofWordNet.
Bartunov et al (2016) proposed a non-parametric Bayesian extension of Skip-gram to au-tomatically learn the required numbers of represen-tations for all words and perform word sense induc-tion tasks.3 Context-Dependent Sense EmbeddingModelWe propose the context-dependent sense embeddingmodel for training high quality sense embeddingswhich takes into account the dependency betweensense choices of neighboring words.
Unlike pervi-ous work, we do not learn any word embeddings inour model and hence avoid the problem with embed-ding polysemous words discussed previously.
In thissection, we will introduce our model and describeour inference and learning algorithms.3.1 ModelWe begin with the notation in our model.
In a sen-tence, let wi be the ith word of the sentence and sibe the sense of the ith word.
S(w) denotes the set ofall the senses of word w. We assume that the sets ofsenses of different words do not overlap.
Therefore,in this paper a word sense can be seen as a lexemeof the word (Rothe and Schutze, 2015).Our model can be represented as a Markov net-work shown in Figure 1.
It is similar to a high-order hidden Markov model.
The model containsa sequence of observable words (w1, w2, .
.
.)
and la-tent senses (s1, s2, .
.
.).
It models the dependencybetween each word-sense pair and between neigh-boring senses in the sequence.
The energy functionis formulated as follows:E(w, s) = ?i(E1(wi, si) + E2(si?k, .
.
.
, si+k))(1)Here w = {wi|1 ?
i ?
l} is the set of words ina sentence with length l and s = {si|1 ?
i ?
l} isthe set of senses.
The function E1 models the de-pendency between a word-sense pair.
As we assumethat the sets of senses of different words do not over-lap, we can formulate E1 as follows:185s1w1s2w2s3w3s4w4s5w5SensesWords?????
?Figure 1: Context-Dependent Sense Embedding Model with window size k = 1E1(wi, si) ={0 si ?
S(wi)+?
si /?
S(wi)(2)Here we assume that all the matched word-sensepairs have the same energy, but it would also beinteresting to model the degrees of matching withdifferent energy values in E1.
In Equation 1, thefunctionE2 models the compatibility of neighboringsenses in a context window with fixed size k. Ex-isting embedding approaches like CBOW and Skip-gram (Mikolov et al, 2013a; Mikolov et al, 2013b)can be used here to define E2.
The formulation us-ing CBOW is as follows:E2(si?k, .
.
.
, si+k) =?
?
( ?i?k?j?i+k,j 6=iV T (sj)V ?
(si)) (3)Here V (s) and V ?
(s) are the input and output em-bedding vectors of sense s. The function ?
is anactivation function and we use the sigmoid functionhere in our model.
The formulation using Skip-gramcan be defined in a similar way:E2(si?k, .
.
.
, si+k) =?
?i?k?j?i+k,j 6=i?
(V T (sj)V ?
(si)) (4)3.2 InferenceIn this section, we introduce our inference algo-rithm.
Given the model and a sentence w, we wantto infer the most likely values of the hidden variables(i.e.
the optimal sense sequence of the sentence) thatminimize the energy function in Equation 1:s?
= arg mins E(w, s) (5)We use dynamic programming to do inferencewhich is similar to the Viterbi algorithm of thehidden Markov model.
Specifically, for everyvalid assignment Ai?2k, .
.
.
, Ai?1 of every sub-sequence of senses si?2k, .
.
.
, si?1, we definem(Ai?2k, .
.
.
, Ai?1) as the energy of the best sensesequence up to position i ?
1 that is consistentwith the assignment Ai?2k, .
.
.
, Ai?1.
We start withm(A1, .
.
.
, A2k) = 0 and then recursively computem in a left-to-right forward process based on the up-date formula:m(Ai?2k+1, .
.
.
, Ai) = minAi?2k(m(Ai?2k, .
.
.
, Ai?1)+ E1(wi, Ai) + E2(Ai?2k, .
.
.
, Ai))(6)Once we finish the forward process, we can retrievethe best sense sequence with a backward process.The time complexity of the algorithm is O(n4kl)where n is the maximal number of senses of a word.Because most words in a typical sentence have eithera single sense or far less than n senses, the actualrunning time of the algorithm is very fast.3.3 LearningIn this section, we introduce our unsupervised learn-ing algorithm.
In learning, we want to learn all the186input and output sense embedding vectors that opti-mize the following max-margin objective function:??
= arg min??w?Cmins?w?
?i=1?sneg?Sneg(wi)max(1 + E1(wi, si) + E2(si?k, .
.
.
, si+k)?E2(si?k, .
.
.
, si?1, sneg, si+1, .
.
.
, si+k), 0)(7)Here ?
is the set of all the parameters includ-ing V and V ?
for all the senses.
C is the set oftraining sentences.
Our learning objective is similarto the negative sampling and max-margin objectiveproposed for word embedding (Collobert and We-ston, 2008).
Sneg(wi) denotes the set of negativesamples of senses of word wi which is defined withthe following strategy.
For a polysemous word wi,Sneg(wi) = S(wi)\{si}.
For the other words with asingle sense, Sneg(wi) is a set of randomly selectedsenses of a fixed size.The objective in Equation 7 can be optimized bycoordinate descent which in our case is equivalentto the hard Expectation-Maximization algorithm.
Inthe hard E step, we run the inference algorithm us-ing the current model parameters to get the optimalsense sequences of the training sentences.
In the Mstep, with the sense sequences s of all the sentencesfixed, we learn sense embedding vectors.
Assumewe use the CBOW model for E2 (Equation 3), thenthe M-step objective function is as follows:??
= arg min??w?C?w??i=1?sneg?Sneg(wi)max(1?
?
(?i?k?j?i+k,j 6=iV (sj)TV ?
(si))+ ?
(?i?k?j?i+k,j 6=iV (sj)TV ?
(sneg)), 0)(8)Here E1 is omitted because the sense sequencesproduced from the E-step always have zero E1value.
Similarly, if we use the Skip-gram model forE2 (Equation 4), then the M-step objective functionis:??
= arg min??w?C?w?
?i=1?i?k?j?i+k,j 6=i?sneg?Sneg(wi)max(1?
?
(V (sj)TV ?
(si))+ ?
(V (sj)TV ?
(sneg)), 0)(9)We optimize the M-step objective function usingstochastic gradient descent.We use a mini batch version of the hard EM al-gorithm.
For each sentence in the training corpus,we run E-step to infer its sense sequence and thenimmediately run M-step (for 1 iteration of stochas-tic gradient descent) to update the model parametersbased on the senses in the sentence.
Therefore, thebatch size of our algorithm depends on the length ofeach sentence.The advantage of using mini batch is twofold.First, while our learning objective is highly non-convex (Tian et al, 2014), the randomness in minibatch hard EM may help us avoid trapping into localoptima.
Second, the model parameters are updatedmore frequently in mini batch hard EM, resulting infaster convergence.Note that before running hard-EM, we need todetermine, for each word w, the size of S(w).
Inour experiments, we used the sense inventory pro-vided by Coarse-Grained English All-Words Task ofSemEval-2007 Task 07 (Navigli et al, 2007) to de-termine the number of senses for each word.
Thesense inventory is a coarse version of WordNet senseinventory.
We do not use the WordNet sense in-ventory because the senses in WordNet are too fine-grained and are difficult to recognize even for humanannotators (Edmonds and Kilgarriff, 2002).
Sincewe do not link our learned senses with external senseinventories, our approach can be seen as performingWSI instead of WSD.4 ExperimentsThis section presents our experiments and results.First, we describe our experimental setup includ-ing the training corpus and the model configuration.187Word Nearest Neigborsbank 1 banking, lender, loanbank 2 river, canal, basinbank 3 slope, tilted, slantapple 1 macintosh, imac, blackberryapple 2 peach, cherry, piedate 1 birthdate, birth, daydate 2 appointment, meet, dinnerfox 1 cbs, abc, nbcfox 2 wolf, deer, rabbitTable 1: The nearest neighbors of senses of polysemous wordsThen, we perform a qualitative evaluation on ourmodel by presenting the nearest neighbors of sensesof some polysemous words.
Finally, we introducetwo different tasks and show the experimental re-sults on these tasks respectively.4.1 Experimental Setup4.1.1 Training CorpusOur training corpus is the commonly usedWikipedia corpus.
We dumped the October 2015snapshot of the Wikipedia corpus which contains 3.6million articles.
In our experiments, we removed theinfrequent words with less than 20 occurrences andthe training corpus contains 1.3 billion tokens.4.1.2 ConfigurationIn our experiments, we set the context windowsize to 5 (5 words before and after the center word).The embedding vector size is set to 300.
The sizeof negative sample sets of single-sense words is setto 5.
We trained our model using AdaGrad stochas-tic gradient decent (Duchi et al, 2010) with initiallearning rate set to 0.025.
Our configuration is simi-lar to that of previous work.Similar to Word2vec, we initialized our modelby randomizing the sense embedding vectors.
Thenumber of senses of all the words is determined withthe sense inventory provided by Coarse-Grained En-glish All-Words Task of SemEval-2007 Task 07(Navigli et al, 2007) as we explained in section 3.3.4.2 Case StudyIn this section, we give a qualitative evaluation ofour model by presenting the nearest neighbors of thesenses of some polysemous words.
Table 1 showsthe results of our qualitative evaluation.
We list sev-eral polysemous words in the table, and for eachword, some typical senses of the word are picked.The nearest neighbors of each sense are listed aside.We used the cosine distance to calculate the distancebetween sense embedding vectors and find the near-est neighbors.In Table 1, we can observe that our model pro-duces good senses for polysemous words.
For exam-ple, the word ?bank?
can be seen to have three dif-ferent sense embedding vectors.
The first one meansthe financial institution.
The second one means thesloping land beside water.
The third one means theaction of tipping laterally.4.3 Word Similarity in ContextThis section gives a quantitative evaluation of ourmodel on word similarity tasks.
Word similar-ity tasks evaluate a model?s performance with theSpearman?s rank correlation between the similarityscores of pairs of words given by the model and themanual labels.
However, traditional word similaritytasks like Wordsim-353 (Finkelstein et al, 2001) arenot suitable for evaluating sense embedding modelsbecause these datasets do not include enough am-biguous words and there is no context informationfor the models to infer and disambiguate the sensesof the words.
To overcome this issue, Huang etal.
(2012) released a new dataset named Stanford?sContextual Word Similarities (SCWS) dataset.
Thedataset consists of 2003 pairs of words along withhuman labelled similarity scores and the sentencescontaining these words.Given a pair of words and their contexts, wecan perform inference using our model to disam-biguate the questioned words.
A similarity score canbe calculated with the cosine distance between thetwo embedding vectors of the inferred senses of thequestioned words.
We also propose another methodfor calculating similarity scores.
In the inferenceprocess, we compute the energy of each sense choiceof the questioned word and consider the negative en-ergy as the confidence of the sense choice.
Then wecalculate the cosine similarity between all pairs ofsenses of the questioned words and compute the av-erage of similarity weighted by the confidence of thesenses.
The first method is named HardSim and the188Model SimilarityMetrics ??
100Huang AvgSim 62.8Huang AvgSimC 65.7Chen AvgSim 66.2Chen AvgSimC 68.9Neelakantan AvgSim 67.2Neelakantan AvgSimC 69.2Li 69.7Tian Model M 63.6Tian Model W 65.4Bartunov AvgSimC 61.2Ours + CBOW HardSim 64.3Ours + CBOW SoftSim 65.6Ours + Skip-gram HardSim 64.9Ours + Skip-gram SoftSim 66.1Table 2: Spearman?s rank correlation results on the SCWSdatasetsecond method is named SoftSim.Table 2 shows the results of our context-dependent sense embedding models on the SCWSdataset.
In this table, ?
refers to the Spearman?s rankcorrelation and a higher value of ?
indicates betterperformance.
The baseline performances are fromHuang et al (2012), Chen et al (2014), Neelakan-tan et al (2014), Li and Jurafsky (2015), Tian etal.
(2014) and Bartunov et al (2016).
Here Ours+ CBOW denotes our model with a CBOW basedenergy function and Ours + Skip-gram denotes ourmodel with a Skip-gram based energy function.
Theresults above the thick line are the models basedon context clustering methods and the results belowthe thick line are the probabilistic models includingours.
The similarity metrics of context clusteringbased models are AvgSim and AvgSimC proposedby Reisinger and Mooney (2010).
Tian et al (2014)propose two metrics Model M and Model W whichare similar to our HardSim and SoftSim metrics.From Table 2, we can observe that our model out-performs the other probabilistic models and is notas good as the best context clustering based model.The context clustering based models are overall bet-ter than the probabilistic models on this task.
Apossible reason is that most context clustering basedmethods make use of more external knowledge thanprobabilistic models.
However, note that Faruquiet al (2016) presented several problems associatedwith the evaluation of word vectors on word simi-larity datasets and pointed out that the use of wordsimilarity tasks for evaluation of word vectors is notsustainable.
Bartunov et al (2016) also suggest thatSCWS should be of limited use for evaluating wordrepresentation models.
Therefore, the results on thistask shall be taken with caution.
We consider thatmore realistic natural language processing tasks likeword sense induction are better for evaluating senseembedding models.4.4 Word Sense InductionIn this section, we present an evaluation of ourmodel on the word sense induction (WSI) tasks.
TheWSI task aims to discover the different meaningsfor words used in sentences.
Unlike a word sensedisambiguation (WSD) system, a WSI system doesnot link the sense annotation results to an existingsense inventory.
Instead, it produces its own senseinventory and links the sense annotation results tothis sense inventory.
Our model can be seen as aWSI system, so we can evaluate our model with WSItasks.We used the dataset from task 13 of SemEval-2013 as our evaluation set (Jurgens and Klapaftis,2013).
The dataset contains 4664 instances inflectedfrom one of the 50 lemmas.
Both single-senseinstances and instances with a graded mixture ofsenses are included in the dataset.
In this paper, weonly consider the single sense instances.
Jurgens andKlapaftis (2013) propose two fuzzy measures namedFuzzy B-Cubed (FBC) and Fuzzy Normalized Mu-tual Information (FNMI) for comparing fuzzy senseassignments from WSI systems.
the FBC measuresummarizes the performance per instance while theFNMI measure is based on sense clusters rather thaninstances.Table 3 shows the results of our context-dependent sense embedding models on this dataset.Here HM is the harmonic mean of FBC and FNMI.The result of AI-KU is from Baskaya et al (2013),MSSG is from Neelakantan et al (2014), ICE-online and ICE-kmeans are from Kageback et al(2015).
Our models are denoted in the same wayas in the previous section.From Table 3, we can observe that our models189Model FBC(%) FNMI(%) HMAI-KU 35.1 4.5 8.0MSSG 45.9 3.7 6.8ICE-online 48.7 5.5 9.9ICE-kmeans 51.1 5.9 10.6Ours + CBOW 53.8 6.3 11.3Ours + Skip-gram 56.9 6.7 12.0Table 3: Results of single-sense instances on task 13 ofSemEval-2013outperform the previous state-of-the-art models andachieve a 13% relative gain.
It shows that our mod-els can beat context clustering based models on re-alistic natural language processing tasks.5 ConclusionIn this paper we propose a novel probabilistic modelfor learning sense embeddings.
Unlike previouswork, we do not learn sense embeddings dependenton word embeddings and hence avoid the problemwith inaccurate embeddings of polysemous words.Furthermore, we model the dependency betweensense choices of neighboring words which can helpus disambiguate multiple ambiguous words in a sen-tence.
Based on our model, we derive a dynamicprogramming inference algorithm and an EM-styleunsupervised learning algorithm which do not relyon external knowledge from any knowledge baseor lexicon except that we determine the number ofsenses of polysemous words according to an existingsense inventory.
We evaluate our model both quali-tatively by case studying and quantitatively with theword similarity task and the word sense inductiontask.
Our model is competitive with previous workon the word similarity task.
On the word sense in-duction task, our model outperforms the state-of-the-art model and achieves a 13% relative gain.For the future work, we plan to try learning ourmodel with soft EM.
Besides, we plan to use sharedsenses instead of lexemes in our model to improvethe generality of our model.
Also, we will studyunsupervised methods to link the learned senses toexisting inventories and to automatically determinethe numbers of senses.
Finally, we plan to evaluateour model with more NLP tasks.ReferencesSergey Bartunov, Dmitry Kondrashkin, Anton Osokin,and Dmitry Vetrov.
2016.
Breaking sticks and am-biguities with adaptive skip-gram.Osman Baskaya, Enis Sert, Volkan Cirik, and DenizYuret.
2013.
Ai-ku: Using substitute vectors andco-occurrence modeling for word sense induction anddisambiguation.
In Second Joint Conference on Lexi-cal and Computational Semantics (*SEM), Volume 2:Seventh International Workshop on Semantic Evalua-tion (SemEval 2013), pages 300?306.Yoshua Bengio, Holger Schwenk, Jean Sbastien Sencal,Frderic Morin, and Jean Luc Gauvain.
2003.
A neu-ral probabilistic language model.
Journal of MachineLearning Research, 3(6):1137?1155.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014.A unified model for word sense representation and dis-ambiguation.
In EMNLP, pages 1025?1035.
Associa-tion for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference onMachine learn-ing, pages 160?167.
ACM.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch, 12(7):257?269.Philip Edmonds and Adam Kilgarriff.
2002.
Introduc-tion to the special issue on evaluating word sense dis-ambiguation systems.
Natural Language Engineering,8(4):279?291.Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,and Chris Dyer.
2016.
Problems with evaluation ofword embeddings using word similarity tasks.
arXivpreprint arXiv:1605.02276.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the conceptrevisited.
In Proceedings of international conferenceon World Wide Web, pages 406?414.G.
E. Hinton.
1986.
Learning distributed representationsof concepts.
In Proceedings of the eighth annual con-ference of the cognitive science society.190Eric H Huang, Richard Socher, Christopher D Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Association for Computa-tional Linguistics.Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.2015.
Ontologically grounded multi-sense represen-tation learning for semantic vector space models.
InProc.
NAACL, pages 683?693.David Jurgens and Ioannis Klapaftis.
2013.
Semeval-2013 task 13: Word sense induction for graded andnon-graded senses.
In Second Joint Conference onLexical and Computational Semantics (*SEM), Vol-ume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 290?299.Mikael Kageback, Fredrik Johansson, Richard Johans-son, and Devdatt Dubhashi.
2015.
Neural contextembeddings for automatic discovery of word senses.In Proceedings of NAACL-HLT, pages 25?32.Jiwei Li and Dan Jurafsky.
2015.
Do multi-sense em-beddings improve natural language understanding?
InEMNLP, pages 1722?1732.
Association for Computa-tional Linguistics.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 142?150.
Associationfor Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
In Workshop at ICLR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InProceedings of the Twenty-Fourth International Con-ference on Machine Learning, pages 641?648.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: coarse-grainedenglish all-words task.
In International Workshop onSemantic Evaluations, pages 30?35.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In EMNLP, pages 1059?1069.Association for Computational Linguistics.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1532?1543.Joseph Reisinger and Raymond J Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 109?117.
Association for Computational Linguistics.Sascha Rothe and Hinrich Schutze.
2015.
Autoex-tend: Extending word embeddings to embeddings forsynsets and lexemes.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics, pages 1793?1803.
Association for Com-putational Linguistics.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning representation by back-propagating errors.
Nature, 323(6088):533?536.Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,Enhong Chen, and Tie-Yan Liu.
2014.
A probabilisticmodel for learning multi-prototype word embeddings.In COLING, pages 151?160.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thannual meeting of the association for computationallinguistics, pages 384?394.
Association for Computa-tional Linguistics.191
