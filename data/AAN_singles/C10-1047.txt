Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 412?420,Beijing, August 2010Finding the Storyteller:Automatic Spoiler Tagging using Linguistic CuesSheng GuoDepartment of Computer ScienceVirginia Techguos@cs.vt.eduNaren RamakrishnanDepartment of Computer ScienceVirginia Technaren@cs.vt.eduAbstractGiven a movie comment, does it containa spoiler?
A spoiler is a comment that,when disclosed, would ruin a surprise orreveal an important plot detail.
We studyautomatic methods to detect commentsand reviews that contain spoilers and ap-ply them to reviews from the IMDB (Inter-net Movie Database) website.
We developtopic models, based on Latent DirichletAllocation (LDA), but using linguistic de-pendency information in place of simplefeatures from bag of words (BOW) repre-sentations.
Experimental results demon-strate the effectiveness of our techniqueover four movie-comment datasets of dif-ferent scales.1 IntroductionIn everyday parlance, the notion of ?spoilers?refers to information, such as a movie plot, whoseadvance revelation destroys the enjoyment of theconsumer.
For instance, consider the movie De-railed which features Clive Owen and JenniferAniston.
In the script, Owen is married and meetsAniston on a train during his daily commute towork.
The two of them begin an affair.
The adul-tery is noticed by some inscrupulous people whoproceed to blackmail Owen and Aniston.
To ex-perience a spoiler, consider this comment fromimdb.com:I can understand why Aniston wanted to do thisrole, since she gets to play majorly against type(as the supposedly ?nice?
girl who?s really - ohno!
- part of the scam), but I?m at a loss to fig-ure out what Clive Owen is doing in this sub-par,unoriginal, ugly and overly violent excuse for athriller.i.e., we learn that Aniston?s character is actuallya not-so-nice person who woos married men forlater blackmail, and thus a very suspenseful pieceof information is revealed.
Automatic ways to de-tect spoilers are crucial in large sites that host re-views and opinions.Arguably, what constitutes a spoiler isinherently a subjective assessment and, formovies/books with intricate storylines, somecomments are likely to contain more spoilers thanothers.
We therefore cast the spoiler detectionproblem as a ranking problem so that commentsthat are more likely to be spoilers are to beranked higher than others.
In particular, we rankuser comments w.r.t.
(i.e., given) the movie?ssynopsis which, according to imdb, is ?
[a detaileddescription of the movie, including spoilers, sothat users who haven?t seen a movie can readanything about the title]?.Our contributions are three fold.
(i) We for-mulate the novel task of spoiler detection in re-views and cast it as ranking user comments againsta synopsis.
We demonstrate how simple bag-of-words (BOW) representations need to be aug-mented with linguistic cues in order to satisfac-torily detect spoilers.
(ii) We showcase the abil-ity of dependency parses to extract discrimina-tory linguistic cues that can distinguish spoil-ers from non-spoilers.
We utilize an LDA-basedmodel (Wei and Croft, 2006) to probabilisticallyrank spoilers.
Our approach does not require man-ual tagging of positive and negative examples ?
anadvantage that is crucial to large scale implemen-tation.
(iii) We conduct a detailed experimentalevaluation with imdb to assess the effectivenessof our framework.
Using manually tagged com-412ments for four diverse movies and suitably con-figured design choices, we evaluate a total of 12ranking strategies.2 LDAProbabilistic topic modeling has attracted signifi-cant attention with techniques such as probabilis-tic latent semantic analysis (PLSA) (Hofmann,1999) and LDA (Blei et al, 2003; Griffiths andSteyvers, 2004; Heinrich, 2008; Steyvers andGriffiths, 2007).
We discuss LDA in detail dueto its centrality to our proposed techniques.
As agenerative model, LDA describes how text couldbe generated from a latent set of variables denot-ing topics.
Each document is modeled as a mix-ture of topics, and topics are modeled as multino-mial distributions on words.An unlabeled training corpus can be usedto estimate an LDA model.
Many infer-ence methods have been proposed, e.g., vari-ational methods (Blei et al, 2003), expecta-tion propagation (Griffiths and Steyvers, 2004),Gibbs sampling (Griffiths and Steyvers, 2004),and a collapsed variational Bayesian inferencemethod (Teh et al, 2007).
Gibbs sampling, asa specific form of Markov chain Monte Carlo(MCMC), is a popular method for estimatingLDA models.
After an LDA model is estimated,it can be used in a very versatile manner: toanalyze new documents, for inference tasks, orfor retrieval/comparison functions.
For instance,we can calculate the probability that a givenword appears in a document conditioned on otherwords.
Furthermore, two kinds of similaritiescan be assessed: between documents and betweenwords (Steyvers and Griffiths, 2007).
The sim-ilarity between two documents can also be usedto retrieve documents relevant to a query docu-ment (Heinrich, 2008).
Yet another application isto use LDA as a dimensionality reduction tool fortext classification (Blei et al, 2003).To improve LDA?s expressiveness, we can re-lax the bag-of-words assumption and plug in moresophisticated topic models (Griffiths et al, 2005;Griffiths et al, 2007; Wallach, 2006; Wallach,2008; Wang and Mccallum, 2005; Wang et al,2007).
sLDA (supervised LDA), as a statisti-cal model of labeled collections, focuses on theprediction problem (Blei and Mcauliffe, 2007).The correlated topic model (CTM) (Blei and Laf-ferty, 2007) addresses plain LDA?s inability tomodel topic correlation.
The author-topic model(AT) (Steyvers et al, 2004) considers not onlytopics but also authors of the documents, andmodels documents as if they were generated bya two-stage stochastic process.3 LDA-based spoiler ranking3.1 MethodsBased on the fact that a spoiler should be topicallyclose to the synopsis, we propose three methodsto solve the spoiler ranking problem.
The firsttwo use LDA as a preprocessing stage, whereasthe third requires positive training data.Predictive perplexity: Our first method is moti-vated by the use of LDA-based predictive per-plexity (PP) for collaborative filtering (Blei et al,2003).
Here, the PP metric is evaluated over afixed test dataset in order to empirically compareLDA with other models (pLSI, mixture of uni-grams).
In our work, we view documents as anal-ogous to users, and words inside documents asanalogous to movies.
Given a group of knownwords, we predict the other group of unkownwords.
We can either calculate the predictive per-plexity value from each movie comment Com tothe unique synopsis (PP1), or from the synopsisSyn to each comment (PP2).PP1(Syn,wcom) = exp{?PMsynd=1 log p(wd|wcom)Msyn }PP2(Com,wsyn) = exp{?PMcomd=1 log p(wd|wsyn)Mcom }In the equations above, p(wd|wcom) andp(wd|wsyn) are the probabilities to generate theword (wd) from a group of observed words wobs(either a comment wcom or a synopsis wsyn).p(w|wobs) =?
?z p(w|z)p(z|?
)p(?|wobs)d?Mcom or Msyn is the length of a comment ora synopsis.
Notice that p(?|wobs) can be easilycalculated after estimating LDA model by Gibbssampling.
It is also discussed as ?predictivelikelihood ranking?
in (Heinrich, 2008).Symmetrized KL-divergence: Since docu-ments are modeled as mixtures of topics inLDA, we can calculate the similarity betweensynopsis and comment by measuring their413topic distributions?
similarity.
We adopt thewidely-used symmetrized Kullback Leibler(KL) divergence (Heinrich, 2008; Steyversand Griffiths, 2007) to measure the differencebetween the two documents?
topic distributions,sKL(Syn,Com) = 12 [DKL(Syn?Com) + DKL(Com?Syn)]where DKL(p?q) =?Tj=1 pj log2pjqjLPU: Viewing the spoiler ranking problem as aretrieval task given the (long) query synopsis, wealso consider the LPU (Learning from Positiveand Unlabeled Data) method (Liu et al, 2003).We apply LPU as if the comment collection wasthe unlabeled dataset, and the synopsis togetherwith few obvious spoiler comments as the posi-tive training data.3.2 Dependency ParsingLDA, as a topic model, is widely used as a clus-tering method and dimensionality reduction tool.It models text as a mixture of topics.
However,topics extracted by LDA are not necessarily thesame topics as judged by humans since the def-inition of topic is very subjective.
For instance,when conducting sentimental polarity analysis,we hope that topics are clusters concerning onecertain kind of subjective sentiment.
But for otherpurposes, we may desire topics focusing on broad?plots.?
Since LDA merely processes a collectionaccording to the statistical distribution of words,its results might not fit either of these two casesmentioned above.In a basic topic model (section 3.1), neither theorder of a sequence of words nor the semanticconnections between two words affect the prob-abilistic modeling.
Documents are generated onlybased on a BOW assumption.
However, word or-der information is very important for most text-related tasks, and simply discarding the order in-formation is inappropriate.
Significant work hasgone in to address this problem.
Griffiths et aluse order information by incorporating colloca-tions (Griffiths et al, 2005; Griffiths et al, 2007).They give an example of the collocation ?unitedkingdom?, which is ideally treated as a singlechunk than two independent words.
However,this model can only be used to capture colloca-tions involving sequential terms.
Their extendedmodel (Griffiths et al, 2007) integrates topics andsyntax, and identifies syntactic classes of wordsbased on their distribution.
More sophisticatedmodels exist (Wallach, 2006; Wang and Mccal-lum, 2005; Wang et al, 2007; Wallach, 2008) butall of them are focused on solving linguistic anal-ysis tasks using topic models.
In this paper, how-ever, our focus is on utilizing dependency infor-mation as a preprocessing step to help improve theaccuracy of LDA models.In more detail, we utilize dependency parsing tobreakup sentences and treat parses as independent?virtual words,?
to be added to the original BOW-based LDA model.
In our experiments we employthe Stanford typed dependency parser 1 (Marneffeet al, 2006) as our parsing tool.
We use collapsedtyped dependencies (a.k.a.
grammatical relations)to form the virtual words.
However, we do not in-corporate all the dependencies.
We only retain de-pendencies whose terms have the part-of-speechtags such as ?NN", ?VB?, ?JJ?, ?PRP?
and ?RB?2,since these terms have strong plot meaning, andare close to the movie topic.
Fig.
2 shows a typi-cal parsing result from one sample sentence.
Thissentence is taken from a review of Unbreakable.Figure 2: Dependency parse of ?David Dunn isthe sole survivor of this terrible disaster?.Consider Fig.
1, which depicts five sample sen-tences all containing two words: ?Dunn?
and?survivor?.
Although these sentences appear dif-ferent, these two words above refer to the sameindividual.
By treating dependencies as virtualwords, we can easily integrate these plot-relatedrelations into an LDA model.
Notice that amongthese five sentences, the grammatical relations be-tween these two words are different: in the fourthsentence, ?survivor?
serves as an appositionalmodifier of the term ?Dunn?
(appos), whereas in1http://nlp.stanford.edu/software, V1.62In the implementation, we actually considered all thePOS tags with these five tags as prefix, such as ?NNS?,?VBN?, etc.414David Dunn is the sole survivor of this terrible disaster.David Dunn (Bruce Willis) is the only survivor in a horrific train trash.David Dunn, a man caught in what appears to be a loveless, deteriorating marriage, is the sole survivor of a Philadelphia train wreck.In this Bruce Willis plays David Dunn, the sole survivor of a passenger train accident.Then the story moves to security guard David Dunn (Bruce Willis) miraculously being the lone survivor of a mile-long train crash (thatyou find out later was not accidental), and with no injuries what-so-ever.nsubjnsubjnsubjapposnsubjFigure 1: Four sentences with the same topical connection between ?Dunn?
and ?survivor?.
We inte-grate this relation into LDA by treating it as a virtual word ?Dunn-survivor.
?other sentences, ?Dunn?
serves as the nominalsubject of ?survivor?(nsubj).
What is importantto note is that the surface distances between thesegiven words in different sentences vary a lot.
Byutilizing dependency parsing, we can capture thesemantic connection which is physically sepa-rated by even as much as 15 words, as in the thirdsentence.We evaluate topic drift among the results fromplain LDA.
We mainly check whether plain LDAwill assign the same topic to those terms that havespecific linguistic dependency relations.
We onlyconsider the following four types of dependenciesfor evaluation3:?
Relations with two noun terms: <NN, NN>,such as ?appos?, ?nn?, ?abbrev?
etc.;?
Relations with one noun and one adjective:<NN, JJ>, like ?amod?;?
Relations with one noun and one verb: <NN,VB>, such as ?agent?, ?dobj?, etc.;?
Relations with only one noun: <NN, *>,which is the relaxed version of <NN, NN>;We experimented with different pre-set topicnumbers (500, 50, and 2) and conducted exper-iments on four different movie comment collec-tions with LDA analysis.
Table 1 shows that<NN, NN> dependency has the highest chance3Here we use <NN, JJ> to express relations having NNand JJ terms, but not necessarily in that order.
Also, NNrepresents all tags related with nouns in the Penn TreebankTagset, such as NNS.
This applies to all the four expressionshere.to be topic-matched4 than other relations.
How-ever, all dependencies have very low percentageto be topic-matched, and with a topic number of 2,there remained a significant amount of unmatched<NN, NN> dependencies, demonstrating that sim-ply doing plain LDA may not capture the plot?topic?
as we desire.Observing the results above, each method fromsection 3.1 (PP1, PP2, sKL and LPU) can be ex-tended by: (1) using BOW-based words, (2) usingonly dependency-based words, or (3) using a mixof BOW and dependency (dependencies as virtualwords).
This induces 12 different ranking strate-gies.Table 1: Topic match analysis for plain LDA(Each entry is the ratio of topic-matched depen-dencies to all dependencies)topic number = 500Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>Unbreakable 772/3024 412/4411 870/19498 5672/61251Blood Diamond 441/1775 83/553 80/1012 609/3496Shooter 242/1846 42/1098 114/2150 1237/15793Role Models 409/2978 60/1396 76/2529 559/7276topic number = 50Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>Unbreakable 1326/3024 953/4411 3354/19498 14067/61251Blood Diamond 806/1775 151/553 210/1012 1194/3496Shooter 584/1846 204/1098 392/2150 3435/15793Role Models 1156/2978 190/1396 309/2529 1702/7276topic number = 2Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>Unbreakable 2379/3024 3106/4411 13606/19498 43876/61251Blood Diamond 1391/1775 404/553 761/1012 2668/3496Shooter 1403/1846 768/1098 1485/2150 11008/15793Role Models 2185/2978 908/1396 1573/2529 4920/72764When both the left term and the right term of a depen-dency share the same topic, the relation is topic-matched.415Table 2: Some examples of incorrect spoiler tagging in IMDb (italicized sentences are spoilers).No.
Tag by IMDb Comment in IMDb1 SpoilerThe whole film is somewhat slow and it would?ve been possible to add more action scenes.
Even though I liked it very much (6.8/10) I think it is lessimpressive than "The Sixth Sense" (8.0/10).
I would like to be more specific with each scene but it will turn this comment into a spoiler so I will leaveit there.
I recommend you to see the movie if you come from the basic Sci-Fi generation, otherwise you may feel uncomfortable with it.
Anyway onceupon a time you were a kid in wonderland and everything was possible.
[tt0217869]2SpoilerThis is one of the rare masterpiece that never got the respect it deserved because people were expecting sixth sense part 2.Sixth sense was a great filmbut this is M.N.
Shyamalan?s best work till date.
This is easily one of my top 10 films of all time.
Excellent acting, direction, score, cinematography andmood.
This movie will hold you in awe from start to finish and any student of cinema would tell what a piece of art this film is.
The cast is phenomenal,right from bruce willis to sam jackson and penn , everyone is spectacular in their roles and they make u realise that you do not need loud dramatic momentsto create an impact, going slow and subtle is the trick here.
This is not a thriller, it?s a realistic superhero film.
[tt0217869]3SpoilerI can?t believe this movie gets a higher rating than the village.
OK, after thinking about it, i get the story of unbreakable and i understand what it?s tryingto say.
I do think the plot and the idea is captivating and interesting.
Having said that, i don?t think the director did anything to make this movie captivatingnor interesting.
It seemed to try too hard to make this movie a riddle for the audience to solve.
The pace was slow at the beginning and ended just as itwas getting faster.
I remember going out of the cinema, feeling frustrated and confused.
it?s not until i thoroughly thought about it that i understood theplot.
I believe a good movie should engaged the audience and be cleverly suspenseful without confusing the audience too much.
Unbreakable tried to bethat but failed miserably.
2 out of 10, see the village instead.
[tt0217869]4SpoilerThis movie touched me in ways I have trouble expressing, and brings forth a message one truly need to take seriously!
I was moved, and the endingbrought a tear to my eye, as well as a constant two-minute shiver down my spine.
It shows how our western way of life influence the lives of thousands ofinnocents, in a not-so-positive way.
Conflict diamonds, as theme this movie debates, are just one of them.
Think of Nike, oil, and so on.
We continuallyexploit "lesser developed" nations for our own benefit, leaving a trail of destruction, sorrow, and broken backs in our trail.
I, for one, will be more attentiveas to what products I purchase in the future, that?s for sure.
[tt0450259]5Non-spoiler...
But the movie takes a while to get to the point.
"Mr. Glass" has caused lots of mass tragedies in order to find the UNBREAKABLE person.
Thus,he is both a mentor and a MONSTER.
... [tt0217869]6Non-spoiler...
This film is about a sniper who loses his best friend while on a shooting mission.
A few years later, he is now retired and living in a woodland with hisdo.
Then he is visited by the military to plan an assassination of the president.
The shot is fired.
Unfortunately he is set up to being the shooter and ishunted by cops everywhere.
He must find out why he has been set up and also try and stop the real killers.
... [tt0822854]4 Experimental Results4.1 Data preparationIMDb boasts a collection of more than 203,000movies (from 1999 to 2009), and the number ofcomments and reviews for these movies num-ber nearly 970,000.
For those movies with syn-opsis provided by IMDb, the average length oftheir synopses is about 2422 characters5.
Ourexperimental setup, for evaluation purposes, re-quires some amount of labeled data.
We choosefour movies from IMDb, together with 2148 com-ments.
As we can see in Table 3, these fourmovies have different sizes of comment sets: themovie ?Unbreakable?
(2000) has more than 1000comments, whereas the movie ?Role Models?
(2008) has only 123 comments.Table 3: Evaluation dataset about four movieswith different numbers of comments.Movie Name IMDB ID #Comments #SpoilersUnbreakable tt0217869 1219 205Blood Diamond tt0450259 538 147Shooter tt0822854 268 73Role Models tt0430922 123 39We labeled all the 2148 comments for thesefour movies manually, and as Table 3 shows,5Those movies without synopsis are not included.about 20% of each movie?s comments are spoil-ers.
Our labeling result is a little different from thecurrent labeling in IMDb: among the 2148 com-ments, although 1659 comments have the same la-bels with IMDb, the other 489 are different (205are treated as spoilers by IMDb but non-spoilersby us; vice versa with 284) The current labelingsystem in IMDb is very coarse: as shown in Ta-ble 2, the first four rows of comments are labeledas spoilers by IMDb, but actually they are not.The last two rows of comments are ignored byIMDb; however, they do expose the plots aboutthe twisting ends.After crawling all the comments of these fourmovies, we performed sentence chunking usingthe LingPipe toolkit and obtained 356 sentencesfor the four movies?
synopses, and 26964 sen-tences for all the comments of these four movies.These sentences were parsed to extract depen-dency information: we obtained 5655 dependen-cies for all synopsis sentences and 448170 depen-dencies for all comment sentences.
From these,we only retain those dependencies that have atleast one noun term in either left side or the rightside.
For measures which require the dependencyinformation, the dependencies are re-organizedand treated as a new term planted in the text.4164.2 Experiments4.2.1 Topic number analysisOne of the shortcomings of LDA-based meth-ods is that they require setting a number of topicsin advance.
Numerous ways have been proposedto handle this problem (Blei et al, 2004; Blei etal., 2003; Griffiths and Steyvers, 2004; Griffiths etal., 2007; Heinrich, 2008; Steyvers and Griffiths,2007; Teh et al, 2006).
Perplexity, which iswidely used in the language modeling commu-nity, is also used to predict the best number oftopics.
It is a measure of how well the modelfits the unseen documents, and is calculated asaverage per-word held-out likelihood.
The lowerthe perplexity is, the better the model is, andtherefore, the number of topic is specified as theone leading to the best performance.
Griffithsand Steyvers (Griffiths and Steyvers, 2004) alsodiscuss the standard Bayesian method whichcomputes the posterior probability of differentmodels given the observed data.
Another methodfrom non-parametric Bayesian statistics auto-matically helps choose the appropriate numberof topics, with flexibility to still choose hyper-parameters (Blei et al, 2004; Teh et al, 2006).Although the debate of choosing an appropriatenumber of topics continues (Boyd-Graber etal., 2009), we utilized the classic perplexitymethod in our work.
Heinrich (Heinrich, 2008)demonstrated that perplexity can be calculated by:P (W?|M) = ?Mm=1 p( ~?wm|M)?1N = exp{?PMm=1 log p( ~?wm|M)PMm=1 Nm}We chose different topic numbers and calculatedthe perplexity value for the 20% held-out com-ments.
A good number of topics was found tobe between 200 and 600 for both Bow-basedstrategy and Bow+Dependency strategy, andis also affected by the size of movie commentcollections.
(We used 0.1 as the document topicprior, and 0.01 as the topic word prior.
)4.2.2 LDA analysis processAs discussed earlier, our task is to rank all thecomments according to their possibilities of beinga spoiler.
We primarily used four methods to dothe ranking: PP1, PP2, sKL, and the LPU method.For each method, we tried the basic model using?bag-of-words?, and the model using dependencyparse information (only), and also with both BOWand dependency information mixed.
We utilizeLingPipe LDA clustering component which usesGibbs sampling.Among the four methods studied here, PP1,PP2 and sKL are based on LDA preprocessing.After obtaining the topic-word distribution andthe posterior distributions for topics in each doc-ument, the PP1 and PP2 metrics can be easilycalculated.
The symmetrized KL divergence be-tween each pair of synopsis and comment is calcu-lated by comparing their topic distributions.
LPUmethod, as a text classifier, requires a set of pos-itive training data.
We selected those commentswhich contain terms or phrases as strong hint ofspoiler (using a list of 20 phrases as the filter, suchas ?spoiler alert?, ?spoiler ahead?, etc).
Thesespoiler comments together with the synopsis, aretreated as the positive training data.
We then uti-lized LPU to label each comment with a real num-ber for ranking.4.3 EvaluationTo evaluate the ranking effects of the 12 strate-gies, we plot n-best precision and recall graphs,which are widely used for assessing colloca-tion measures (Evert and Krenn, 2001; Pecinaand Schlesinger, 2006).
Fig.
3 visualizes theprecision-recall graphs from 12 different mea-sures for the four movie comment collections.The x-axis represents the proportion of the rank-ing list, while the y-axis depicts the correspond-ing precision or recall value.
The upper part ofthe figure is the result for the movie which con-tains more than 1000 comments, while the bot-tom part demonstrates the result for the relativelysmall comment collection.
The n-best evaluationshows that for all the four movie comment col-lections, PP1_mix and PP1 perform significantlybetter than the other methods, and the dependencyinformation helps to increase the accuracy sig-nificantly, especially for the larger size collec-tion.
The LPU method, though using part of thepositive training data, did not perform very well.The reason could be that although some of theusers put the warning phrases (like ?spoiler alert?
)ahead of their comments, the comment might con-tain only indirect plot-revealing information.
Thisalso reflects that a spoiler tagging method by us-4170 50 100 150 2000102030405060708090100N?best list (top n)Precision(%)Precision0 200 400 600 800 10000102030405060708090100N?best list (top n)Recall (%)RecallPP1PP1_mixPP2PP2_mixLPULPU_mixPP2_mixPP2PP2_mixPP1_mixPP1PP1_mix0 50 100 150 2000102030405060708090100N?best list (top n)Precision(%)Precision0 100 200 300 4000102030405060708090100N?best list (top n)Recall (%)RecallPP1PP1_mixPP2PP2_mixLPULPU_mixPP2_mixPP2 PP1_mixPP2_mixPP1PP1_mix0 50 100 150 2000102030405060708090100N?best list (top n)Precision(%)Precision0 50 100 150 200 250 3000102030405060708090100N?best list (top n)Recall (%)RecallPP1PP1_mixPP2PP2_mixLPULPU_mixPP1_mixPP1PP2PP1_mix0 20 40 60 80 1000102030405060708090100N?best list (top n)Precision(%)Precision0 20 40 60 80 1000102030405060708090100N?best list (top n)Recall (%)RecallPP1PP1_mixPP2PP2_mixLPULPU_mixPP1_mixPP1PP1_mixFigure 3: N-best(top nth) evaluation (Burnin period = 100): comparison of precision-recall for differentmethods on four movie comment collections.
The PP1 method with BOW and dependency informationmixed performs the best among all the measures.
Other six methods such as dependency only andKL-based which do not give good performance are ignored in this figure to make it readable.
Fullcomparison is available at: http://sites.google.com/site/ldaspoiler/418ing only keywords typically will not work.
Fi-nally, the approach to directly calculating the sym-metrized KL divergence seems to be not suitable,either.4.4 LDA iteration analysisWe also compared the average precision val-ues and normalized discounted cumulative gain(nDCG) values (Croft et al, 2009; J?rvelin andKek?l?inen, 2002) of the ranking results with dif-ferent parameters for Gibbs sampling, such asburnin period and sample size.
Average precisionis calculated by averaging the precision valuesfrom the ranking positions where a valid spoileris found, and the nDCG value for the top-p list iscalculated as nDCGp = DCGpIDCG ?DCGp is defined as:DCGp = rel1 +?pi=2relilog2 i where reli is 1 whenthe i-th comment in the list is judged as a realspoiler, and 0, otherwise.
IDCG denotes the max-imum possible DCG value when all the real spoil-ers are ranked at the top (perfect ranking) (J?rvelinand Kek?l?inen, 2002).Table 4: Comparison of ranking by PP_mix us-ing different parameters for Gibbs sampling (ana-lyzed on the top 150 ranking lists, and the valuesin the table are the mean of the accuracy from fourmovie comment collections).<S=100; Lag=2> <S=10; Lag=2> <S=1; Lag=2>Burnin AvgP (%) nDCG AvgP (%) nDCG AvgP (%) nDCG400 80.85 0.951 78.2 0.938 78.1 0.94200 80.95 0.951 80.5 0.948 79.1 0.94100 87.25 0.974 80.2 0.943 82.4 0.9650 81.5 0.958 79.5 0.942 80.0 0.9410 78.9 0.944 79.5 0.949 75.9 0.921 79.4 0.940 79.2 0.952 58.0 0.86As we can see from Table 4, the accuracy isnot affected too much as long as the burin periodfor the MCMC process is longer than 50 and thesample size retained is larger than 10.
In our ex-periments, we use 100 as the burin parameter, andbeyond that, 100 samples were retained with sam-ple lag of 2.4.5 Representative resultsAs shown in Table 5, we find that the basic BOWstrategy prefers the longer comments whereas thestrategy that uses dependency information prefersthe shorter ones.
Although it is reasonable thata longer comment would have a higher probabil-ity of revealing the plot, methods which prefersthe longer comments usually leave out the shortspoiler comments.
By incorporating the depen-dency information together with the basic BOW,the new method reduces this shortcoming.
For in-stance, consider one short comment for the movie?Unbreakable (2000)?
:This is the same formula as Sixth Sense ?
fromthe ability to see things other people don?t, tothe shocking ending.
Only this movie is just notplausible ?
I mean Elijah goes around causingdisasters, trying to see if anyone is ?Unbreak-able?
?
it?s gonna take a lot of disasters becauseits a big world.whcih is ranked as the 27th result in the PP1_mixmethod, whereas the BOW based PP1 methodplaces it at the 398th result in the list.
Obviously,this comment reveals the twisting end that it is Eli-jah who caused the disasters.Table 5: Comparison of average length of the top-50 comments of 4 movies from 2 strategies.Role Models Shooter Blood Diamond UnbreakableBOW 2162.14 2259.36 2829.86 1389.18Dependency 1596.14 1232.12 2435.58 1295.725 Conclusions and future workWe have introduced the spoiler detection problemand proposed using topic models to rank moviecomments according to the extent they reveal themovie?s plot.
In particular, integrating linguisticcues from dependency information into our topicmodel significantly improves the ranking accu-racy.In future work, we seek to study schemes whichcan segment comments to potentially identify therelevant spoiler portion automatically.
The auto-matic labeling idea of (Mei et al, 2007) can alsobe studied in our framework.
Deeper linguisticanalysis, such as named entity recognition and se-mantic role labeling, can also be conducted.
Inaddition, evaluating topic models or choosing theright number of topics using dependency informa-tion can be further studied.
Finally, integratingthe dependency relationships more directly intothe probabilistic graphical model is also worthyof study.419ReferencesBlei, David M. and John D. Lafferty.
2007.
A cor-related topic model of science.
Annals of AppliedStatistics, 1(1):17?35.Blei, David M. and Jon D. Mcauliffe.
2007.
Super-vised topic models.
In Proceedings of the 21st An-nual Conference on Neural Information ProcessingSystems.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of ma-chine learning research, 3:993?1022.Blei, David M., T. Gri, M. Jordan, and J. Tenenbaum.2004.
Hierarchical topic models and the nested chi-nese restaurant process.
In Proceedings of the 18thAnnual Conference on Neural Information Process-ing Systems.Boyd-Graber, Jordan, Jonathan Chang, Sean Gerrish,Chong Wang, and David Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Pro-ceedings of the 23rd Annual Conference on NeuralInformation Processing Systems.Croft, Bruce, Donald Metzler, and Trevor Strohman.2009.
Search Engines: Information Retrieval inPractice.
Addison Wesley, 1 edition.Evert, Stefan and Brigitte Krenn.
2001.
Methods forthe qualitative evaluation of lexical association mea-sures.
In Proceedings of 39th Annual Meeting of theAssociation for Computational Linguistics.Griffiths, Thomas L. and M. Steyvers.
2004.
Find-ing scientific topics.
In Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101 Suppl 1:5228?5235, April.Griffiths, Thomas L., Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2005.
Integrating topicsand syntax.
In Proceedings of the 19th Annual Con-ference on Neural Information Processing Systems.Griffiths, Thomas L., Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114(2):211?244, April.Heinrich, Gregor.
2008.
Parameter estimation for textanalysis.
Technical report, University of Leipzig.Hofmann, Thomas.
1999.
Probabilistic latent seman-tic analysis.
In Proceedings of 15th Conference onUncertainty in Artificial Intelligence.J?rvelin, Kalervo and Jaana Kek?l?inen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTransactions on Information Systems, 20(4):422?446.Liu, Bing, Yang Dai, Xiaoli Li, Wee Lee, and Philip S.Yu.
2003.
Building text classifiers using positiveand unlabeled examples.
In Proceedings of the 3rdIEEE International Conference on Data Mining.Marneffe, M., B. Maccartney, and C. Manning.
2006.Generating typed dependency parses from phrasestructure parses.
In Proceedings of the 5th Inter-national Conference on Language Resources andEvaluation.Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
In Proceedings of the 13th ACM SIGKDDconference.Pecina, Pavel and Pavel Schlesinger.
2006.
Com-bining association measures for collocation extrac-tion.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th An-nual Meeting of the Association for ComputationalLinguistics.Steyvers, Mark and Tom Griffiths, 2007.
ProbabilisticTopic Models.
Lawrence Erlbaum Associates.Steyvers, Mark, Padhraic Smyth, Michal R. Zvi, andThomas Griffiths.
2004.
Probabilistic author-topicmodels for information discovery.
In Proceedingsof the 10th ACM SIGKDD conference.Teh, Yee Whye, Jordan, I. Michael, Beal, J. Matthew,Blei, and M. David.
2006.
Hierarchical dirichletprocesses.
Journal of the American Statistical As-sociation, 101(476):1566?1581, December.Teh, Yee W., David Newman, and Max Welling.
2007.A collapsed variational bayesian inference algo-rithm for latent dirichlet alocation.
In Proceedingsof the 21st Annual Conference on Neural Informa-tion Processing Systems.Wallach, Hanna M. 2006.
Topic modeling: beyondbag-of-words.
In Proceedings of the 23rd Interna-tional Conference on Machine Learning.Wallach, Hanna M. 2008.
Structured topic models forlanguage.
Ph.D. thesis, University of Cambridge.Wang, Xuerui and Andrew Mccallum.
2005.
A noteon topical n-grams.
Technical report, University ofMassachusetts Amherst.Wang, Xuerui, Andrew McCallum, and Xing Wei.2007.
Topical n-grams: Phrase and topic discovery,with an application to information retrieval.
In Pro-ceedings of the 7th IEEE International Conferenceon Data Mining.Wei, Xing and Bruce W. Croft.
2006.
Lda-based doc-ument models for ad-hoc retrieval.
In Proceedingsof the 29th Annual International ACM SIGIR Con-ference.420
