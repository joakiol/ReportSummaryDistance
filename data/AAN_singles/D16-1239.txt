Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2215?2224,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAnchoring and Agreement in Syntactic AnnotationsYevgeni BerzakCSAIL MITberzak@mit.eduYan HuangLanguage Technology LabDTAL Cambridge Universityyh358@cam.ac.ukAndrei BarbuCSAIL MITandrei@0xab.comAnna KorhonenLanguage Technology LabDTAL Cambridge Universityalk23@cam.ac.ukBoris KatzCSAIL MITboris@mit.eduAbstractWe present a study on two key character-istics of human syntactic annotations: an-choring and agreement.
Anchoring is a wellknown cognitive bias in human decision mak-ing, where judgments are drawn towards pre-existing values.
We study the influence ofanchoring on a standard approach to creationof syntactic resources where syntactic annota-tions are obtained via human editing of taggerand parser output.
Our experiments demon-strate a clear anchoring effect and reveal un-wanted consequences, including overestima-tion of parsing performance and lower qual-ity of annotations in comparison with human-based annotations.
Using sentences from thePenn Treebank WSJ, we also report systemat-ically obtained inter-annotator agreement es-timates for English dependency parsing.
Ouragreement results control for parser bias, andare consequential in that they are on par withstate of the art parsing performance for En-glish newswire.
We discuss the impact of ourfindings on strategies for future annotation ef-forts and parser evaluations.11 IntroductionResearch in NLP relies heavily on the availability ofhuman annotations for various linguistic predictiontasks.
Such resources are commonly treated as defacto ?gold standards?
and are used for both training1The experimental data in this study will be made publiclyavailable.and evaluation of algorithms for automatic annota-tion.
At the same time, human agreement on theseannotations provides an indicator for the difficultyof the task, and can be instrumental for estimatingupper limits for the performance obtainable by com-putational methods.Linguistic gold standards are often constructedusing pre-existing annotations, generated by auto-matic tools.
The output of such tools is then man-ually corrected by human annotators to produce thegold standard.
The justification for this annotationmethodology was first introduced in a set of exper-iments on POS tag annotation conducted as part ofthe Penn Treebank project (Marcus et al, 1993).
Inthis study, the authors concluded that tagger-basedannotations are not only much faster to obtain, butalso more consistent and of higher quality comparedto annotations from scratch.
Following the PennTreebank, syntactic annotation projects for variouslanguages, including German (Brants et al, 2002),French (Abeille?
et al, 2003), Arabic (Maamouriet al, 2004) and many others, were annotated us-ing automatic tools as a starting point.
Despite thewidespread use of this annotation pipeline, there is,to our knowledge, little prior work on syntactic an-notation quality and on the reliability of system eval-uations on such data.In this work, we present a systematic study of theinfluence of automatic tool output on characteristicsof annotations created for NLP purposes.
Our in-vestigation is motivated by the hypothesis that anno-tations obtained using such methodologies may be2215subject to the problem of anchoring, a well estab-lished and robust cognitive bias in which human de-cisions are affected by pre-existing values (Tverskyand Kahneman, 1974).
In the presence of anchors,participants reason relative to the existing values,and as a result may provide different solutions fromthose they would have reported otherwise.
Mostcommonly, anchoring is manifested as an alignmenttowards the given values.Focusing on the key NLP tasks of POS taggingand dependency parsing, we demonstrate that thestandard approach of obtaining annotations via hu-man correction of automatically generated POS tagsand dependencies exhibits a clear anchoring effect ?a phenomenon we refer to as parser bias.
Given thisevidence, we examine two potential adverse impli-cations of this effect on parser-based gold standards.First, we show that parser bias entails substantialoverestimation of parser performance.
In particu-lar, we demonstrate that bias towards the output ofa specific tagger-parser pair leads to over-estimationof the performance of these tools relative to othertools.
Moreover, we observe general performancegains for automatic tools relative to their perfor-mance on human-based gold standards.
Second, westudy whether parser bias affects the quality of theresulting gold standards.
Extending the experimen-tal setup of Marcus et al (1993), we demonstratethat parser bias may lead to lower annotation qual-ity for parser-based annotations compared to human-based annotations.Furthermore, we conduct an experiment on inter-annotator agreement for POS tagging and depen-dency parsing which controls for parser bias.
Ourexperiment on a subset of section 23 of the WSJPenn Treebank yields agreement rates of 95.65 forPOS tagging and 94.17 for dependency parsing.This result is significant in light of the state of theart tagging and parsing performance for Englishnewswire.
With parsing reaching the level of humanagreement, and tagging surpassing it, a more thor-ough examination of evaluation resources and eval-uation methodologies for these tasks is called for.To summarize, we present the first study to mea-sure and analyze anchoring in the standard parser-based approach to creation of gold standards forPOS tagging and dependency parsing in NLP.
Weconclude that gold standard annotations that arebased on editing output of automatic tools can leadto inaccurate figures in system evaluations and lowerannotation quality.
Our human agreement experi-ment, which controls for parser bias, yields agree-ment rates that are comparable to state of the artautomatic tagging and dependency parsing perfor-mance, highlighting the need for a more extensiveinvestigation of tagger and parser evaluation in NLP.2 Experimental Setup2.1 Annotation TasksWe examine two standard annotation tasks in NLP,POS tagging and dependency parsing.
In the POStagging task, each word in a sentence has to be cate-gorized with a Penn Treebank POS tag (Santorini,1990) (henceforth POS).
The dependency parsingtask consists of providing a sentence with a labeleddependency tree using the Universal Dependencies(UD) formalism (De Marneffe et al, 2014), accord-ing to version 1 of the UD English guidelines2.
Toperform this task, the annotator is required to specifythe head word index (henceforth HIND) and relationlabel (henceforth REL) of each word in the sentence.We distinguish between three variants of thesetasks, annotation, reviewing and ranking.
In the an-notation variant, participants are asked to conductannotation from scratch.
In the reviewing variant,they are asked to provide alternative annotations forall annotation tokens with which they disagree.
Theparticipants are not informed about the source of thegiven annotation, which, depending on the experi-mental condition can be either parser output or hu-man annotation.
In the ranking task, the participantsrank several annotation options with respect to theirquality.
Similarly to the review task, the participantsare not given the sources of the different annotationoptions.
Participants performing the annotation, re-viewing and ranking tasks are referred to as annota-tors, reviewers and judges, respectively.2.2 Annotation FormatAll annotation tasks are performed using a CoNLLstyle text-based template, in which each word ap-pears in a separate line.
The first two columns ofeach line contain the word index and the word, re-2http://universaldependencies.org/#en2216spectively.
The next three columns are designatedfor annotation of POS, HIND and REL.In the annotation task, these values have to bespecified by the annotator from scratch.
In thereview task, participants are required to edit pre-annotated values for a given sentence.
The sixth col-umn in the review template contains an additional# sign, whose goal is to prevent reviewers fromoverlooking and passively approving existing anno-tations.
Corrections are specified following this signin a space separated format, where each of the exist-ing three annotation tokens is either corrected withan alternative annotation value or approved using a* sign.
Approval of all three annotation tokens ismarked by removing the # sign.
The example be-low presents a fragment from a sentence used for thereviewing task, in which the reviewer approves theannotations of all the words, with the exception of?help?, where the POS is corrected from VB to NNand the relation label xcomp is replaced with dobj....5 you PRP 6 nsubj6 need VBP 3 ccomp7 help VB 6 xcomp # NN * dobj...The format of the ranking task is exemplified be-low.
The annotation options are presented to the par-ticipants in a random order.
Participants specify therank of each annotation token following the verticalbar.
In this sentence, the label cop is preferred overaux for the word ?be?
and xcomp is preferred overadvcl for the word ?Common?....8 it PRP 10 nsubjpass9 is VBZ 10 auxpass10 planed VBN 0 root11 to TO 15 mark12 be VB 15 aux-cop | 2-113 in IN 15 case14 Wimbledon NNP 15 compound15 Common NNP 10 advcl-xcomp | 2-1...The participants used basic validation scriptswhich checked for typos and proper formatting ofthe annotations, reviews and rankings.2.3 Evaluation MetricsWe measure both parsing performance and inter-annotator agreement using tagging and parsing eval-uation metrics.
This choice allows for a direct com-parison between parsing and agreement results.
Inthis context, POS refers to tagging accuracy.
Weutilize the standard metrics Unlabeled AttachmentScore (UAS) and Label Accuracy (LA) to measureaccuracy of head attachment and dependency labels.We also utilize the standard parsing metric LabeledAttachment Score (LAS), which takes into accountboth dependency arcs and dependency labels.
In allour parsing and agreement experiments, we excludepunctuation tokens from the evaluation.2.4 CorporaWe use sentences from two publicly availabledatasets, covering two different genres.
The firstcorpus, used in the experiments in sections 3 and4, is the First Certificate in English (FCE) Cam-bridge Learner Corpus (Yannakoudakis et al, 2011).This dataset contains essays authored by upper-intermediate level English learners3.The second corpus is the WSJ part of the PennTreebank (WSJ PTB) (Marcus et al, 1993).
Sinceits release, this dataset has been the most commonlyused resource for training and evaluation of Englishparsers.
Our experiment on inter-annotator agree-ment in section 5 uses a random subset of the sen-tences in section 23 of the WSJ PTB, which is tradi-tionally reserved for tagging and parsing evaluation.2.5 AnnotatorsWe recruited five students at MIT as annotators.Three of the students are linguistics majors andtwo are engineering majors with linguistics minors.Prior to participating in this study, the annotatorscompleted two months of training.
During training,the students attended tutorials, and learned the an-notation guidelines for PTB POS tags, UD guide-lines, as well as guidelines for annotating challeng-ing syntactic structures arising from grammatical er-rors.
The students also annotated individually six3The annotation bias and quality results reported in sections3 and 4 use the original learner sentences, which contain gram-matical errors.
These results were replicated on the error cor-rected versions of the sentences.2217practice batches of 20-30 sentences from the En-glish Web Treebank (EWT) (Silveira et al, 2014)and FCE corpora, and resolved annotation disagree-ments during group meetings.Following the training period, the students anno-tated a treebank of learner English (Berzak et al,2016) over a period of five months, three of whichas a full time job.
During this time, the studentscontinued attending weekly meetings in which fur-ther annotation challenges were discussed and re-solved.
The annotation was carried out for sentencesfrom the FCE dataset, where both the original anderror corrected versions of each sentence were an-notated and reviewed.
In the course of the anno-tation project, each annotator completed approxi-mately 800 sentence annotations, and a similar num-ber of sentence reviews.
The annotations and re-views were done in the same format used in thisstudy.
With respect to our experiments, the exten-sive experience of our participants and their priorwork as a group strengthen our results, as these char-acteristics reduce the effect of anchoring biases andincrease inter-annotator agreement.3 Parser BiasOur first experiment is designed to test whether ex-pert human annotators are biased towards POS tagsand dependencies generated by automatic tools.
Weexamine the common out-of-domain annotation sce-nario, where automatic tools are often trained on anexisting treebank in one domain, and used to gener-ate initial annotations to speed-up the creation of agold standard for a new domain.
We use the EWTUD corpus as the existing gold standard, and a sam-ple of the FCE dataset as the new corpus.ProcedureOur experimental procedure, illustrated in figure1(a) contains a set of 360 sentences (6,979 tokens)from the FCE, for which we generate three goldstandards: one based on human annotations and twobased on parser outputs.
To this end, for each sen-tence, we assign at random four of the participants tothe following annotation and review tasks.
The fifthparticipant is left out to perform the quality rankingtask described in section 4.The first participant annotates the sentence fromscratch, and a second participant reviews this an-Turbo RBGSentenceAnnotatorsJudgeReviewersHuman Gold Turbo Gold RBG Gold(b) Quality(a) BiasFigure 1: Experimental setup for parser bias (a) and annotationquality (b) on 360 sentences (6,979 tokens) from the FCE.
Foreach sentence, five human annotators are assigned at randomto one of three roles: annotation, review or quality assessment.In the bias experiment, presented in section 3, every sentenceis annotated by a human, Turbo parser (based on Turbo tag-ger output) and RBG parser (based on Stanford tagger output).Each annotation is reviewed by a different human participant toproduce three gold standards of each sentence: ?Human Gold?,?Turbo Gold?
and ?RBG Gold?.
The fifth annotator performsa quality assessment task described in section 4, which requiresto rank the three gold standards in cases of disagreement.notation.
The overall agreement of the reviewerswith the annotators is 98.24 POS, 97.16 UAS, 96.3LA and 94.81 LAS.
The next two participants re-view parser outputs.
One participant reviews an an-notation generated by the Turbo tagger and parser(Martins et al, 2013).
The other participant reviewsthe output of the Stanford tagger (Toutanova et al,2003) and RBG parser (Lei et al, 2014).
The taggersand parsers were trained on the gold annotations ofthe EWT UD treebank, version 1.1.
Both parsers usepredicted POS tags for the FCE sentences.Assigning the reviews to the human annotationsyields a human based gold standard for each sen-tence called ?Human Gold?.
Assigning the reviewsto the tagger and parser outputs yields two parser-based gold standards, ?Turbo Gold?
and ?RBGGold?.
We chose the Turbo-Turbo and Stanford-RBG tagger-parser pairs as these tools obtain com-parable performance on standard evaluation bench-2218Turbo RBGPOS UAS LA LAS POS UAS LA LASHuman Gold 95.32 87.29 88.35 82.29 95.59 87.19 88.03 82.05Turbo Gold 97.62 91.86 92.54 89.16 96.64 89.16 89.75 84.86Error Reduction % 49.15 35.96 35.97 38.79 23.81 15.38 14.37 15.65RBG Gold 96.43 88.65 89.95 84.42 97.76 91.22 91.84 87.87Error Reduction % 23.72 10.7 13.73 12.03 49.21 31.46 31.83 32.42Table 1: Annotator bias towards taggers and parsers on 360 sentences (6,979 tokens) from the FCE.
Tagging and parsing resultsare reported for the Turbo parser (based on the output of the turbo Tagger) and RBG parser (based on the output of the Stanfordtagger) on three gold standards.
Human Gold are manual corrections of human annotations.
Turbo Gold are manual correctionsof the output of Turbo tagger and Turbo parser.
RBG Gold are manual corrections of the Stanford tagger and RBG parser.
Errorreduction rates are reported relative to the results obtained by the two tagger-parser pairs on the Human Gold annotations.
Note that(1) The parsers perform equally well on Human Gold.
(2) Each parser performs better than the other parser on its own reviews.
(3)Each parser performs better on the reviews of the other parser compared to its performance on Human Gold.
The differences in (2)and (3) are statistically significant with p 0.001 using McNemar?s test.marks, while yielding substantially different anno-tations due to different training algorithms and fea-ture sets.
For our sentences, the agreement be-tween the Turbo tagger and Stanford tagger is 96.97POS.
The agreement between the Turbo parser andRBG parser based on the respective tagger outputsis 90.76 UAS, 91.6 LA and 87.34 LAS.Parser Specific and Parser Shared BiasIn order to test for parser bias, in table 1 wecompare the performance of the Turbo-Turbo andStanford-RBG tagger-parser pairs on our three goldstandards.
First, we observe that while these toolsperform equally well on Human Gold, each tagger-parser pair performs better than the other on its ownreviews.
These parser specific performance gaps aresubstantial, with an average of 1.15 POS, 2.63 UAS,2.34 LA and 3.88 LAS between the two conditions.This result suggests the presence of a bias towardsthe output of specific tagger-parser combinations.The practical implication of this outcome is that agold standard created by editing an output of a parseris likely to boost the performance of that parser inevaluations and over-estimate its performance rela-tive to other parsers.Second, we note that the performance of each ofthe parsers on the gold standard of the other parser isstill higher than its performance on the human goldstandard.
The average performance gap betweenthese conditions is 1.08 POS, 1.66 UAS, 1.66 LAand 2.47 LAS.
This difference suggests an annota-tion bias towards shared aspects in the predictionsof taggers and parsers, which differ from the humanbased annotations.
The consequence of this obser-vation is that irrespective of the specific tool thatwas used to pre-annotate the data, parser-based goldstandards are likely to result in higher parsing per-formance relative to human-based gold standards.Taken together, the parser specific and parsershared effects lead to a dramatic overall average er-ror reduction of 49.18% POS, 33.71% UAS, 34.9%LA and 35.61% LAS on the parser-based gold stan-dards compared to the human-based gold standard.To the best of our knowledge, these results are thefirst systematic demonstration of the tendency of thecommon approach of parser-based creation of goldstandards to yield biased annotations and lead tooverestimation of tagging and parsing performance.4 Annotation QualityIn this section we extend our investigation to ex-amine the impact of parser bias on the quality ofparser-based gold standards.
To this end, we per-form a manual comparison between human-basedand parser-based gold standards.Our quality assessment experiment, depictedschematically in figure 1(b), is a ranking task.
Foreach sentence, a randomly chosen judge, who didnot annotate or review the given sentence, ranks dis-agreements between the three gold standards HumanGold, Turbo Gold and RBG Gold, generated in theparser bias experiment in section 3.Table 2 presents the preference rates of judges2219Human Gold Preference % POS HIND RELTurbo Gold 64.32* 63.96* 61.5*# disagreements 199 444 439RBG Gold 56.72 61.38* 57.73*# disagreements 201 435 440Table 2: Human preference rates for a human-based gold stan-dard Human Gold over the two parser-based gold standardsTurbo Gold and RBG Gold.
# disagreements denotes the num-ber of tokens that differ between Human Gold and the respec-tive parser-based gold standard.
Statistically significant valuesfor a two-tailed Z test with p < 0.01 are marked with *.
Notethat for both tagger-parser pairs, human judges tend to preferhuman-based over parser-based annotations.for the human-based gold standard over each of thetwo parser-based gold standards.
In all three eval-uation categories, human judges tend to prefer thehuman-based gold standard over both parser-basedgold standards.
This result demonstrates that the ini-tial reduced quality of the parser outputs comparedto human annotations indeed percolates via anchor-ing to the resulting gold standards.The analysis of the quality assessment experi-ment thus far did not distinguish between caseswhere the two parsers agree and where they dis-agree.
In order to gain further insight into the rela-tion between parser bias and annotation quality, webreak down the results reported in table 2 into twocases which relate directly to the parser specific andparser shared components of the tagging and pars-ing performance gaps observed in the parser bias re-sults reported in section 3.
In the first case, called?parser specific approval?, a reviewer approves aparser annotation which disagrees both with the out-put of the other parser and the Human Gold anno-tation.
In the second case, called ?parser shared ap-proval?, a reviewer approves a parser output whichis shared by both parsers but differs with respect toHuman Gold.Table 3 presents the judge preference rates for theHuman-Gold annotations in these two scenarios.
Weobserve that cases in which the parsers disagree areof substantially worse quality compared to human-based annotations.
However, in cases of agreementbetween the parsers, the resulting gold standards donot exhibit a clear disadvantage relative to the Hu-man Gold annotations.This result highlights the crucial role of parserHuman Gold Preference % POS HIND RELTurbo specific approval 85.42* 78.69* 80.73*# disagreements 48 122 109RBG specific approval 73.81* 77.98* 77.78*# disagreements 42 109 108Parser shared approval 51.85 58.49* 51.57# disagreements 243 424 415Table 3: Breakdown of the Human preference rates for thehuman-based gold standard over the parser-based gold stan-dards in table 2, into cases of agreement and disagreement be-tween the two parsers.
Parser specific approval are cases inwhich a parser output approved by the reviewer differs fromboth the output of the other parser and the Human Gold anno-tation.
Parser shared approval denotes cases where an approvedparser output is identical to the output of the other parser but dif-fers from the Human Gold annotation.
Statistically significantvalues for a two-tailed Z test with p < 0.01 are marked with*.
Note that parser specific approval is substantially more detri-mental to the resulting annotation quality compared to parsershared approval.specific approval in the overall preference of judgestowards human-based annotations in table 2.
Fur-thermore, it suggests that annotations on which mul-tiple state of the art parsers agree are of sufficientlyhigh accuracy to be used to save annotation timewithout substantial impact on the quality of the re-sulting resource.
In section 7 we propose an annota-tion scheme which leverages this insight.5 Inter-annotator AgreementAgreement estimates in NLP are often obtained inannotation setups where both annotators edit thesame automatically generated input.
However, insuch experimental conditions, anchoring can intro-duce cases of spurious disagreement as well as spu-rious agreement between annotators due to align-ment of one or both participants towards the giveninput.
The initial quality of the provided annotationsin combination with the parser bias effect observedin section 3 may influence the resulting agreementestimates.
For example, in Marcus et al (1993) an-notators were shown to produce POS tagging agree-ment of 92.8 on annotation from scratch, comparedto 96.5 on reviews of tagger output.Our goal in this section is to obtain estimates forinter-annotator agreement on POS tagging and de-pendency parsing that control for parser bias, and2220as a result, reflect more accurately human agree-ment on these tasks.
We thus introduce a novelpipeline based on human annotation only, whicheliminates parser bias from the agreement measure-ments.
Our experiment extends the human-based an-notation study of Marcus et al (1993) to includealso syntactic trees.
Importantly, we include an ad-ditional review step for the initial annotations, de-signed to increase the precision of the agreementmeasurements by reducing the number of errors inthe original annotations.SentenceScratchScratch reviewedFigure 2: Experimental setup for the inter-annotator agreementexperiment.
300 sentences (7,227 tokens) from section 23 ofthe PTB-WSJ are annotated and reviewed by four participants.The participants are assigned to the following tasks at randomfor each sentence.
Two participants annotate the sentence fromscratch, and the remaining two participants review one of theseannotations each.
Agreement is measured on the annotations(?scratch?)
as well after assigning the review edits (?scratch re-viewed?
).For this experiment, we use 300 sentences (7,227tokens) from section 23 of the PTB-WSJ, the stan-dard test set for English parsing in NLP.
The exper-imental setup, depicted graphically in figure 2, in-cludes four participants randomly assigned for eachsentence to annotation and review tasks.
Two of theparticipants provide the sentence with annotationsfrom scratch, while the remaining two participantsprovide reviews.
Each reviewer edits one of theannotations independently, allowing for correctionof annotation errors while maintaining the indepen-dence of the annotation sources.
We measure agree-ment between the initial annotations (?scratch?
), aswell as the agreement between the reviewed versionsof our sentences (?scratch reviewed?
).The agreement results for the annotations and thereviews are presented in table 4.
The initial agree-ment rate on POS annotation from scratch is higherthan in (Marcus et al, 1993).
This difference islikely to arise, at least in part, due to the fact thattheir experiment was conducted at the beginningof the annotation project, when the annotators hada more limited annotation experience compared toour participants.
Overall, we note that the agree-ment rates from scratch are relatively low.
The re-view round raises the agreement on all the evalua-tion categories due to elimination of annotation er-rors present the original annotations.POS UAS LA LASscratch 94.78 93.07 92.3 88.32scratch reviewed 95.65 94.17 94.04 90.33Table 4: Inter-annotator agreement on 300 sentences (7,227 to-kens) from the PTB-WSJ section 23.
?scratch?
is agreementon independent annotations from scratch.
?scratch reviewed?
isagreement on the same sentences after an additional indepen-dent review round of the annotations.Our post-review agreement results are consequen-tial in light of the current state of the art performanceon tagging and parsing in NLP.
For more than adecade, POS taggers have been achieving over 97%accuracy with the PTB POS tag set on the PTB-WSJtest set.
For example, the best model of the Stanfordtagger reported in Toutanova et al (2003) producesan accuracy of 97.24 POS on sections 22-24 of thePTB-WSJ.
These accuracies are above the humanagreement in our experiment.With respect to dependency parsing, recentparsers obtain results which are on par or higher thanour inter-annotator agreement estimates.
For exam-ple, Weiss et al (2015) report 94.26 UAS and An-dor et al (2016) report 94.61 UAS on section 23of the PTB-WSJ using an automatic conversion ofthe PTB phrase structure trees to Stanford depen-dencies (De Marneffe et al, 2006).
These resultsare not fully comparable to ours due to differencesin the utilized dependency formalism and the auto-matic conversion of the annotations.
Nonetheless,we believe that the similarities in the tasks and eval-uation data are sufficiently strong to indicate thatdependency parsing for standard English newswiremay be reaching human agreement levels.22216 Related WorkThe term ?anchoring?
was coined in a seminal paperby Tversky and Kahneman (1974), which demon-strated that numerical estimation can be biased byuninformative prior information.
Subsequent workacross various domains of decision making con-firmed the robustness of anchoring using both in-formative and uninformative anchors (Furnham andBoo, 2011).
Pertinent to our study, anchoring bi-ases were also demonstrated when the participantswere domain experts, although to a lesser degreethan in the early anchoring experiments (Wilson etal., 1996; Mussweiler and Strack, 2000).Prior work in NLP examined the influence ofpre-tagging (Fort and Sagot, 2010) and pre-parsing(Skj?rholt, 2013) on human annotations.
Our workintroduces a systematic study of this topic using anovel experimental framework as well as substan-tially more sentences and annotators.
Differentlyfrom these studies, our methodology enables charac-terizing annotation bias as anchoring and measuringits effect on tagger and parser evaluations.Our study also extends the POS tagging exper-iments of Marcus et al (1993), which comparedinter-annotator agreement and annotation quality onmanual POS tagging in annotation from scratch andtagger-based review conditions.
The first result re-ported in that study was that tagger-based editing in-creases inter-annotator agreement compared to an-notation from scratch.
Our work provides a novelagreement benchmark for POS tagging which re-duces annotation errors through a review processwhile controlling for tagger bias, and obtains agree-ment measurements for dependency parsing.
Thesecond result reported in Marcus et al (1993) wasthat tagger-based edits are of higher quality com-pared to annotations from scratch when evaluatedagainst an additional independent annotation.
Wemodify this experiment by introducing ranking as analternative mechanism for quality assessment, andadding a review round for human annotations fromscratch.
Our experiment demonstrates that in thisconfiguration, parser-based annotations are of lowerquality compared to human-based annotations.Several estimates of expert inter-annotator agree-ment for English parsing were previously reported.However, most such evaluations were conducted us-ing annotation setups that can be affected by ananchoring bias (Carroll et al, 1999; Rambow etal., 2002; Silveira et al, 2014).
A notable excep-tion is the study of Sampson and Babarczy (2008)who measure agreement on annotation from scratchfor English parsing in the SUSANNE framework(Sampson, 1995).
The reported results, however,are not directly comparable to ours, due to the useof a substantially different syntactic representation,as well as a different agreement metric.
Their studyfurther suggests that despite the high expertise of theannotators, the main source of annotation disagree-ments was annotation errors.
Our work alleviatesthis issue by using annotation reviews, which reducethe number of erroneous annotations while main-taining the independence of the annotation sources.Experiments on non-expert dependency annotationfrom scratch were previously reported for French,suggesting low agreement rates (79%) with an ex-pert annotation benchmark (Gerdes, 2013).7 DiscussionWe present a systematic study of the impact of an-choring on POS and dependency annotations usedin NLP, demonstrating that annotators exhibit an an-choring bias effect towards the output of automaticannotation tools.
This bias leads to an artificial boostof performance figures for the parsers in questionand results in lower annotation quality as comparedwith human-based annotations.Our analysis demonstrates that despite the adverseeffects of parser bias, predictions that are sharedacross different parsers do not significantly lower thequality of the annotations.
This finding gives riseto the following hybrid annotation strategy as a po-tential future alternative to human-based as well asparser-based annotation pipelines.
In a hybrid anno-tation setup, human annotators review annotationson which several parsers agree, and complete the re-maining annotations from scratch.
Such a strategywould largely maintain the annotation speed-ups ofparser-based annotation schemes.
At the same time,it is expected to achieve annotation quality compa-rable to human-based annotation by avoiding parserspecific bias, which plays a pivotal role in the re-duced quality of single-parser reviewing pipelines.Further on, we obtain, to the best of our knowl-2222edge for the first time, syntactic inter-annotatoragreement measurements on WSJ-PTB sentences.Our experimental procedure reduces annotation er-rors and controls for parser bias.
Despite the de-tailed annotation guidelines, the extensive experi-ence of our annotators, and their prior work as agroup, our experiment indicates rather low agree-ment rates, which are below state of the art taggingperformance and on par with state of the art parsingresults on this dataset.
We note that our results donot necessarily reflect an upper bound on the achiev-able syntactic inter-annotator agreement for Englishnewswire.
Higher agreement rates could in princi-ple be obtained through further annotator training,refinement and revision of annotation guidelines, aswell as additional automatic validation tests for theannotations.
Nonetheless, we believe that our esti-mates reliably reflect a realistic scenario of expertsyntactic annotation.The obtained agreement rates call for a more ex-tensive examination of annotator disagreements onparsing and tagging.
Recent work in this area hasalready proposed an analysis of expert annotator dis-agreements for POS tagging in the absence of anno-tation guidelines (Plank et al, 2014).
Our annota-tions will enable conducting such studies for annota-tion with guidelines, and support extending this lineof investigation to annotations of syntactic depen-dencies.
As a first step towards this goal, we planto carry out an in-depth analysis of disagreementin the collected data, characterize the main sourcesof inconsistent annotation and subsequently formu-late further strategies for improving annotation ac-curacy.
We believe that better understanding of hu-man disagreements and their relation to disagree-ments between humans and parsers will also con-tribute to advancing evaluation methodologies forPOS tagging and syntactic parsing in NLP, an im-portant topic that has received only limited attentionthus far (Schwartz et al, 2011; Plank et al, 2015).Finally, since the release of the Penn Treebank in1992, it has been serving as the standard benchmarkfor English parsing evaluation.
Over the past fewyears, improvements in parsing performance on thisdataset were obtained in small increments, and arecommonly reported without a linguistic analysis ofthe improved predictions.
As dependency parsingperformance on English newswire may be reachinghuman expert agreement, not only new evaluationpractices, but also more attention to noisier domainsand other languages may be in place.AcknowledgmentsWe thank our terrific annotators Sebastian Garza,Jessica Kenney, Lucia Lam, Keiko Sophie Mori andJing Xian Wang.
We are also grateful to KarthikNarasimhan and the anonymous reviewers for valu-able feedback on this work.
This material is basedupon work supported by the Center for Brains,Minds, and Machines (CBMM) funded by NSF STCaward CCF-1231216.
This work was also supportedby AFRL contract No.
FA8750-15-C-0010 and byERC Consolidator Grant LEXICAL (648909).ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a treebank for french.
In Treebanks,pages 165?187.
Springer.Daniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally normal-ized transition-based neural networks.
In Proceedingsof ACL, pages 2442?2452.Yevgeni Berzak, Jessica Kenney, Carolyn Spadine,Jing Xian Wang, Lucia Lam, Keiko Sophie Mori, Se-bastian Garza, and Boris Katz.
2016.
Universal de-pendencies for learner english.
In Proceedings of ACL,pages 737?746.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The tiger treebank.In Proceedings of the workshop on treebanks and lin-guistic theories, volume 168.John Carroll, Guido Minnen, and Ted Briscoe.
1999.Corpus annotation for parser evaluation.
arXivpreprint cs/9907013.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, volume 6, pages 449?454.Marie-Catherine De Marneffe, Timothy Dozat, NataliaSilveira, Katri Haverinen, Filip Ginter, Joakim Nivre,and Christopher D Manning.
2014.
Universal stan-ford dependencies: A cross-linguistic typology.
InProceedings of LREC, pages 4585?4592.Kare?n Fort and Beno?
?t Sagot.
2010.
Influence of pre-annotation on pos-tagged corpus development.
In Pro-ceedings of the fourth linguistic annotation workshop,pages 56?63.2223Adrian Furnham and Hua Chu Boo.
2011.
A literaturereview of the anchoring effect.
The Journal of Socio-Economics, 40(1):35?42.Kim Gerdes.
2013.
Collaborative dependency annota-tion.
DepLing 2013, 88.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scoringdependency structures.
In Proceedings of ACL, vol-ume 1, pages 1381?1391.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The penn arabic treebank:Building a large-scale annotated arabic corpus.
InNEMLAR conference on Arabic language resourcesand tools, volume 27, pages 466?467.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The penn treebank.
Computational lin-guistics, 19(2):313?330.Andre?
FT Martins, Miguel Almeida, and Noah A Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of ACL, pages617?622.Thomas Mussweiler and Fritz Strack.
2000.
Numericjudgments under uncertainty: The role of knowledgein anchoring.
Journal of Experimental Social Psychol-ogy, 36(5):495?518.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.Linguistically debatable or just plain wrong?
In Pro-ceedings of ACL: Short Papers, pages 507?511.Barbara Plank, He?ctor Mart?
?nez Alonso, Z?eljko Agic?,Danijela Merkler, and Anders S?gaard.
2015.
Do de-pendency parsing metrics correlate with human judg-ments?
In Proceedings of CoNLL.Owen Rambow, Cassandre Creswell, Rachel Szekely,Harriet Taber, and Marilyn A Walker.
2002.
A depen-dency treebank for english.
In Proceedings of LREC.Geoffrey Sampson and Anna Babarczy.
2008.
Def-initional and human constraints on structural anno-tation of english.
Natural Language Engineering,14(04):471?494.Geoffrey Sampson.
1995.
English for the computer: Su-sanne corpus and analytic scheme.Beatrice Santorini.
1990.
Part-of-speech tagging guide-lines for the penn treebank project (3rd revision).Technical Reports (CIS).Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing eval-uation.
In Proceedings of ACL, pages 663?672.Natalia Silveira, Timothy Dozat, Marie-CatherineDe Marneffe, Samuel R Bowman, Miriam Connor,John Bauer, and Christopher D Manning.
2014.
Agold standard dependency corpus for english.
In Pro-ceedings of LREC, pages 2897?2904.Arne Skj?rholt.
2013.
Influence of preprocessing ondependency syntax annotation: speed and agreement.LAW VII & ID.Kristina Toutanova, Dan Klein, Christopher D Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL, pages 173?180.Amos Tversky and Daniel Kahneman.
1974.
Judgmentunder uncertainty: Heuristics and biases.
Science,185(4157):1124?1131.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings of ACL,pages 323?333.Timothy D Wilson, Christopher E Houston, Kathryn MEtling, and Nancy Brekke.
1996.
A new lookat anchoring effects: basic anchoring and its an-tecedents.
Journal of Experimental Psychology: Gen-eral, 125(4):387.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of ACL, pages180?189.2224
