Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 1: Coreference Resolution in Multiple LanguagesMarta Recasens?Llu?
?s M`arquez?Emili Sapena?M.
Ant`onia Mart??
?Mariona Taul?e?V?eronique Hoste?Massimo PoesioYannick Versley???
: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu?
: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu?
: University College Ghent, veronique.hoste@hogent.be: University of Essex/University of Trento, poesio@essex.ac.uk??
: University of T?ubingen, versley@sfs.uni-tuebingen.deAbstractThis paper presents the SemEval-2010task on Coreference Resolution in Multi-ple Languages.
The goal was to evaluateand compare automatic coreference reso-lution systems for six different languages(Catalan, Dutch, English, German, Italian,and Spanish) in four evaluation settingsand using four different metrics.
Such arich scenario had the potential to provideinsight into key issues concerning corefer-ence resolution: (i) the portability of sys-tems across languages, (ii) the relevance ofdifferent levels of linguistic information,and (iii) the behavior of scoring metrics.1 IntroductionThe task of coreference resolution, defined as theidentification of the expressions in a text that re-fer to the same discourse entity (1), has attractedconsiderable attention within the NLP community.
(1) Major League Baseball sent its head of se-curity to Chicago to review the second in-cident of an on-field fan attack in the lastseven months.
The league is reviewing se-curity at all ballparks to crack down onspectator violence.Using coreference information has been shown tobe beneficial in a number of NLP applicationsincluding Information Extraction (McCarthy andLehnert, 1995), Text Summarization (Steinbergeret al, 2007), Question Answering (Morton, 1999),and Machine Translation.
There have been a fewevaluation campaigns on coreference resolution inthe past, namely MUC (Hirschman and Chinchor,1997), ACE (Doddington et al, 2004), and ARE(Orasan et al, 2008), yet many questions remainopen:?
To what extent is it possible to imple-ment a general coreference resolution systemportable to different languages?
How muchlanguage-specific tuning is necessary??
How helpful are morphology, syntax and se-mantics for solving coreference relations?How much preprocessing is needed?
Does itsquality (perfect linguistic input versus noisyautomatic input) really matter??
How (dis)similar are different coreferenceevaluation metrics?MUC, B-CUBED,CEAF and BLANC?
Do they all provide thesame ranking?
Are they correlated?Our goal was to address these questions in ashared task.
Given six datasets in Catalan, Dutch,English, German, Italian, and Spanish, the taskwe present involved automatically detecting fullcoreference chains?composed of named entities(NEs), pronouns, and full noun phrases?in fourdifferent scenarios.
For more information, thereader is referred to the task website.1The rest of the paper is organized as follows.Section 2 presents the corpora from which the taskdatasets were extracted, and the automatic toolsused to preprocess them.
In Section 3, we describethe task by providing information about the dataformat, evaluation settings, and evaluation met-rics.
Participating systems are described in Sec-tion 4, and their results are analyzed and comparedin Section 5.
Finally, Section 6 concludes.2 Linguistic ResourcesIn this section, we first present the sources of thedata used in the task.
We then describe the auto-matic tools that predicted input annotations for thecoreference resolution systems.1http://stel.ub.edu/semeval2010-coref1Training Development Test#docs #sents #tokens #docs #sents #tokens #docs #sents #tokensCatalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007English 229 3,648 79,060 39 741 17,044 85 1,141 24,206German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040Table 1: Size of the task datasets.2.1 Source CorporaCatalan and Spanish The AnCora corpora (Re-casens and Mart?
?, 2009) consist of a Catalan anda Spanish treebank of 500k words each, mainlyfrom newspapers and news agencies (El Peri?odico,EFE, ACN).
Manual annotation exists for ar-guments and thematic roles, predicate semanticclasses, NEs, WordNet nominal senses, and coref-erence relations.
AnCora are freely available forresearch purposes.Dutch The KNACK-2002 corpus (Hoste and DePauw, 2006) contains 267 documents from theFlemish weekly magazine Knack.
They weremanually annotated with coreference informationon top of semi-automatically annotated PoS tags,phrase chunks, and NEs.English The OntoNotes Release 2.0 corpus(Pradhan et al, 2007) covers newswire and broad-cast news data: 300k words from The Wall StreetJournal, and 200k words from the TDT-4 col-lection, respectively.
OntoNotes builds on thePenn Treebank for syntactic annotation and on thePenn PropBank for predicate argument structures.Semantic annotations include NEs, words senses(linked to an ontology), and coreference informa-tion.
The OntoNotes corpus is distributed by theLinguistic Data Consortium.2German The T?uBa-D/Z corpus (Hinrichs et al,2005) is a newspaper treebank based on data takenfrom the daily issues of ?die tageszeitung?
(taz).
Itcurrently comprises 794k words manually anno-tated with semantic and coreference information.Due to licensing restrictions of the original texts, ataz-DVD must be purchased to obtain a license.2Italian The LiveMemories corpus (Rodr?
?guezet al, 2010) will include texts from the ItalianWikipedia, blogs, news articles, and dialogues2Free user license agreements for the English and Germantask datasets were issued to the task participants.(MapTask).
They are being annotated accordingto the ARRAU annotation scheme with coref-erence, agreement, and NE information on topof automatically parsed data.
The task datasetincluded Wikipedia texts already annotated.The datasets that were used in the task were ex-tracted from the above-mentioned corpora.
Ta-ble 1 summarizes the number of documents(docs), sentences (sents), and tokens in the train-ing, development and test sets.32.2 Preprocessing SystemsCatalan, Spanish, English Predicted lemmasand PoS were generated using FreeLing4forCatalan/Spanish and SVMTagger5for English.Dependency information and predicate semanticroles were generated with JointParser, a syntactic-semantic parser.6Dutch Lemmas, PoS and NEs were automat-ically provided by the memory-based shallowparser for Dutch (Daelemans et al, 1999), and de-pendency information by the Alpino parser (vanNoord et al, 2006).German Lemmas were predicted by TreeTagger(Schmid, 1995), PoS and morphology by RFTag-ger (Schmid and Laws, 2008), and dependency in-formation by MaltParser (Hall and Nivre, 2008).Italian Lemmas and PoS were provided byTextPro,7and dependency information by Malt-Parser.83The German and Dutch training datasets were not com-pletely stable during the competition period due to a few er-rors.
Revised versions were released on March 2 and 20, re-spectively.
As to the test datasets, the Dutch and Italian doc-uments with formatting errors were corrected after the eval-uation period, with no variations in the ranking order of sys-tems.4http://www.lsi.upc.es/ nlp/freeling5http://www.lsi.upc.edu/ nlp/SVMTool6http://www.lsi.upc.edu// xlluis/?x=cat:57http://textpro.fbk.eu8http://maltparser.org23 Task DescriptionParticipants were asked to develop an automaticsystem capable of assigning a discourse entity toevery mention,9thus identifying all the NP men-tions of every discourse entity.
As there is nostandard annotation scheme for coreference andthe source corpora differed in certain aspects, thecoreference information of the task datasets wasproduced according to three criteria:?
Only NP constituents and possessive deter-miners can be mentions.?
Mentions must be referential expressions,thus ruling out nominal predicates, appos-itives, expletive NPs, attributive NPs, NPswithin idioms, etc.?
Singletons are also considered as entities(i.e., entities with a single mention).To help participants build their systems, thetask datasets also contained both gold-standardand automatically predicted linguistic annotationsat the morphological, syntactic and semantic lev-els.
Considerable effort was devoted to provideparticipants with a common and relatively simpledata representation for the six languages.3.1 Data FormatThe task datasets as well as the participants?answers were displayed in a uniform column-based format, similar to the style used in previousCoNLL shared tasks on syntactic and semantic de-pendencies (2008/2009).10Each dataset was pro-vided as a single file per language.
Since corefer-ence is a linguistic relation at the discourse level,documents constitute the basic unit, and are de-limited by ?#begin document ID?
and ?#end doc-ument ID?
comment lines.
Within a document, theinformation of each sentence is organized verti-cally with one token per line, and a blank line afterthe last token of each sentence.
The informationassociated with each token is described in severalcolumns (separated by ?\t?
characters) represent-ing the following layers of linguistic annotation.ID (column 1).
Token identifiers in the sentence.Token (column 2).
Word forms.9Following the terminology of the ACE program, a men-tion is defined as an instance of reference to an object, andan entity is the collection of mentions referring to the sameobject in a document.10http://www.cnts.ua.ac.be/conll2008ID Token Intermediate columns Coref1 Major .
.
.
(12 League .
.
.3 Baseball .
.
.
1)4 sent .
.
.5 its .
.
.
(1)|(26 head .
.
.7 of .
.
.8 security .
.
.
(3)|2)9 to .
.
.. .
.
.
.
.
.
.
.
.
.
.27 The .
.
.
(128 league .
.
.
1)29 is .
.
.Table 2: Format of the coreference annotations(corresponding to example (1) in Section 1).Lemma (column 3).
Token lemmas.PoS (column 5).
Coarse PoS.Feat (column 7).
Morphological features (PoStype, number, gender, case, tense, aspect,etc.)
separated by a pipe character.Head (column 9).
ID of the syntactic head (?0?
ifthe token is the tree root).DepRel (column 11).
Dependency relations cor-responding to the dependencies described inthe Head column (?sentence?
if the token isthe tree root).NE (column 13).
NE types in open-close notation.Pred (column 15).
Predicate semantic class.APreds (column 17 and subsequent ones).
Foreach predicate in the Pred column, its seman-tic roles/dependencies.Coref (last column).
Coreference relations inopen-close notation.The above-mentioned columns are ?gold-standard columns,?
whereas columns 4, 6, 8, 10,12, 14, 16 and the penultimate contain the sameinformation as the respective previous column butautomatically predicted?using the preprocessingsystems listed in Section 2.2.
Neither all layersof linguistic annotation nor all gold-standard andpredicted columns were available for all six lan-guages (underscore characters indicate missing in-formation).The coreference column follows an open-closenotation with an entity number in parentheses (seeTable 2).
Every entity has an ID number, and ev-ery mention is marked with the ID of the entityit refers to: an opening parenthesis shows the be-ginning of the mention (first token), while a clos-ing parenthesis shows the end of the mention (last3token).
For tokens belonging to more than onemention, a pipe character is used to separate mul-tiple entity IDs.
The resulting annotation is a well-formed nested structure (CF language).3.2 Evaluation SettingsIn order to address our goal of studying the effectof different levels of linguistic information (pre-processing) on solving coreference relations, thetest was divided into four evaluation settings thatdiffered along two dimensions.Gold-standard versus Regular setting.
Onlyin the gold-standard setting were participants al-lowed to use the gold-standard columns, includ-ing the last one (of the test dataset) with truemention boundaries.
In the regular setting, theywere allowed to use only the automatically pre-dicted columns.
Obtaining better results in thegold setting would provide evidence for the rel-evance of using high-quality preprocessing infor-mation.
Since not all columns were available forall six languages, the gold setting was only possi-ble for Catalan, English, German, and Spanish.Closed versus Open setting.
In the closed set-ting, systems had to be built strictly with the in-formation provided in the task datasets.
In con-trast, there was no restriction on the resources thatparticipants could utilize in the open setting: sys-tems could be developed using any external toolsand resources to predict the preprocessing infor-mation, e.g., WordNet, Wikipedia, etc.
The onlyrequirement was to use tools that had not been de-veloped with the annotations of the test set.
Thissetting provided an open door into tools or re-sources that improve performance.3.3 Evaluation MetricsSince there is no agreement at present on a stan-dard measure for coreference resolution evalua-tion, one of our goals was to compare the rank-ings produced by four different measures.
Thetask scorer provides results in the two mention-based metrics B3(Bagga and Baldwin, 1998) andCEAF-?3(Luo, 2005), and the two link-basedmetrics MUC (Vilain et al, 1995) and BLANC(Recasens and Hovy, in prep).
The first three mea-sures have been widely used, while BLANC is aproposal of a new measure interesting to test.The mention detection subtask is measured withrecall, precision, and F1.
Mentions are rewardedwith 1 point if their boundaries coincide with thoseof the gold NP, with 0.5 points if their boundariesare within the gold NP including its head, andwith 0 otherwise.4 Participating SystemsA total of twenty-two participants registered forthe task and downloaded the training materials.From these, sixteen downloaded the test set butonly six (out of which two task organizers) sub-mitted valid results (corresponding to nine systemruns or variants).
These numbers show that thetask raised considerable interest but that the finalparticipation rate was comparatively low (slightlybelow 30%).The participating systems differed in terms ofarchitecture, machine learning method, etc.
Ta-ble 3 summarizes their main properties.
Systemslike BART and Corry support several machinelearners, but Table 3 indicates the one used for theSemEval run.
The last column indicates the exter-nal resources that were employed in the open set-ting, thus it is empty for systems that participatedonly in the closed setting.
For more specific detailswe address the reader to the system description pa-pers in Erk and Strapparava (2010).5 Results and EvaluationTable 4 shows the results obtained by two naivebaseline systems: (i) SINGLETONS considers eachmention as a separate entity, and (ii) ALL-IN-ONEgroups all the mentions in a document into a sin-gle entity.
These simple baselines reveal limita-tions of the evaluation metrics, like the high scoresof CEAF and B3for SINGLETONS.
Interestinglyenough, the naive baseline scores turn out to behard to beat by the participating systems, as Ta-ble 5 shows.
Similarly, ALL-IN-ONE obtains highscores in terms of MUC.
Table 4 also reveals dif-ferences between the distribution of entities in thedatasets.
Dutch is clearly the most divergent cor-pus mainly due to the fact that it only contains sin-gletons for NEs.Table 5 displays the results of all systems for alllanguages and settings in the four evaluation met-rics (the best scores in each setting are highlightedin bold).
Results are presented sequentially by lan-guage and setting, and participating systems areordered alphabetically.
The participation of sys-tems across languages and settings is rather irreg-ular,11thus making it difficult to draw firm conclu-11Only 45 entries in Table 5 from 192 potential cases.4System Architecture ML Methods External ResourcesBART(Broscheit et al, 2010) Closest-first with entity-mention model (English),Closest-first model (German,Italian)MaxEnt (English, Ger-man), Decision trees(Italian)GermaNet & gazetteers (Ger-man), I-Cab gazetteers (Italian),Berkeley parser, Stanford NER,WordNet, Wikipedia name list,U.S.
census data (English)Corry(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-Net, U.S. census dataRelaxCor(Sapena et al, 2010) Graph partitioning (solved byrelaxation labeling)Decision trees, Rules WordNetSUCRE(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-tional database model, Regularfeature definition languageDecision trees, NaiveBayes, SVM, MaxEnt?TANL-1(Attardi et al, 2010) Highest entity-mention simi-larityMaxEnt PoS tagger (Italian)UBIU(Zhekova and K?ubler, 2010) Pairwise model MBL ?Table 3: Main characteristics of the participating systems.sions about the aims initially pursued by the task.In the following, we summarize the most relevantoutcomes of the evaluation.Regarding languages, English concentrates themost participants (fifteen entries), followed byGerman (eight), Catalan and Spanish (seven each),Italian (five), and Dutch (three).
The number oflanguages addressed by each system ranges fromone (Corry) to six (UBIU and SUCRE); BART andRelaxCor addressed three languages, and TANL-1five.
The best overall results are obtained for En-glish followed by German, then Catalan, Spanishand Italian, and finally Dutch.
Apart from differ-ences between corpora, there are other factors thatmight explain this ranking: (i) the fact that most ofthe systems were originally developed for English,and (ii) differences in corpus size (German havingthe largest corpus, and Dutch the smallest).Regarding systems, there are no clear ?win-ners.?
Note that no language-setting was ad-dressed by all six systems.
The BART system,for instance, is either on its own or competingagainst a single system.
It emerges from par-tial comparisons that SUCRE performs the best inclosed?regular for English, German, and Italian,although it never outperforms the CEAF or B3sin-gleton baseline.
While SUCRE always obtains thebest scores according to MUC and BLANC, Re-laxCor and TANL-1 usually win based on CEAFand B3.
The Corry system presents three variantsoptimized for CEAF (Corry-C), MUC (Corry-M),and BLANC (Corry-B).
Their results are consis-tent with the bias introduced in the optimization(see English:open?gold).Depending on the evaluation metric then, therankings of systems vary with considerable scoredifferences.
There is a significant positive corre-lation between CEAF and B3(Pearson?s r = 0.91,p< 0.01), and a significant lack of correlation be-tween CEAF and MUC in terms of recall (Pear-son?s r = 0.44, p< 0.01).
This fact stresses theimportance of defining appropriate metrics (or acombination of them) for coreference evaluation.Finally, regarding evaluation settings, the re-sults in the gold setting are significantly better thanthose in the regular.
However, this might be a di-rect effect of the mention recognition task.
Men-tion recognition in the regular setting falls morethan 20 F1points with respect to the gold setting(where correct mention boundaries were given).As for the open versus closed setting, there is onlyone system, RelaxCor for English, that addressedthe two.
As expected, results show a slight im-provement from closed?gold to open?gold.6 ConclusionsThis paper has introduced the main features ofthe SemEval-2010 task on coreference resolution.5CEAF MUC B3BLANCR P F1R P F1R P F1R P BlancSINGLETONS: Each mention forms a separate entity.Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4ALL-IN-ONE: All mentions are grouped into a single entity.Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4Table 4: Baseline scores.The goal of the task was to evaluate and compareautomatic coreference resolution systems for sixdifferent languages in four evaluation settings andusing four different metrics.
This complex sce-nario aimed at providing insight into several as-pects of coreference resolution, including portabil-ity across languages, relevance of linguistic infor-mation at different levels, and behavior of alterna-tive scoring metrics.The task attracted considerable attention from anumber of researchers, but only six teams submit-ted their final results.
Participating systems did notrun their systems for all the languages and evalu-ation settings, thus making direct comparisons be-tween them very difficult.
Nonetheless, we wereable to observe some interesting aspects from theempirical evaluation.An important conclusion was the confirmationthat different evaluation metrics provide differentsystem rankings and the scores are not commen-surate.
Attention thus needs to be paid to corefer-ence evaluation.
The behavior and applicability ofthe scoring metrics requires further investigationin order to guarantee a fair evaluation when com-paring systems in the future.
We hope to have theopportunity to thoroughly discuss this and the restof interesting questions raised by the task duringthe SemEval workshop at ACL 2010.An additional valuable benefit is the set of re-sources developed throughout the task.
As taskorganizers, we intend to facilitate the sharing ofdatasets, scorers, and documentation by keepingthem available for future research use.
We believethat these resources will help to set future bench-marks for the research community and will con-tribute positively to the progress of the state of theart in coreference resolution.
We will maintain andupdate the task website with post-SemEval contri-butions.AcknowledgmentsWe would like to thank the following peo-ple who contributed to the preparation of thetask datasets: Manuel Bertran (UB), OriolBorrega (UB), Orph?ee De Clercq (U. Ghent),Francesca Delogu (U. Trento), Jes?us Gim?enez(UPC), Eduard Hovy (ISI-USC), Richard Johans-son (U. Trento), Xavier Llu?
?s (UPC), MontseNofre (UB), Llu?
?s Padr?o (UPC), Kepa JosebaRodr?
?guez (U. Trento), Mihai Surdeanu (Stan-ford), Olga Uryupina (U. Trento), Lente Van Leu-ven (UB), and Rita Zaragoza (UB).
We would alsolike to thank LDC and die tageszeitung for dis-tributing freely the English and German datasets.This work was funded in part by the Span-ish Ministry of Science and Innovation throughthe projects TEXT-MESS 2.0 (TIN2009-13391-C04-04), OpenMT-2 (TIN2009-14675-C03), andKNOW2 (TIN2009-14715-C04-04), and an FPUdoctoral scholarship (AP2006-00994) held byM.
Recasens.
It also received financial sup-port from the Seventh Framework Programmeof the EU (FP7/2007-2013) under GA 247762(FAUST), from the STEVIN program of the Ned-erlandse Taalunie through the COREA and SoNaRprojects, and from the Provincia Autonoma diTrento through the LiveMemories project.6Mention detection CEAF MUC B3BLANCR P F1R P F1R P F1R P F1R P BlancCatalanclosed?goldRelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2closed?regularSUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8open?goldopen?regularDutchclosed?goldSUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3closed?regularSUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3open?goldopen?regularEnglishclosed?goldRelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0closed?regularSUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0open?goldCorry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7open?regularBART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1Germanclosed?goldSUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5closed?regularSUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0open?goldBART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8open?regularBART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3Italianclosed?goldSUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9closed?regularSUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2open?goldopen?regularBART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5Spanishclosed?goldRelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3closed?regularSUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4open?goldopen?regularTable 5: Official results of the participating systems for all languages, settings, and metrics.7ReferencesGiuseppe Attardi, Stefano Dei Rossi, and Maria Simi.2010.
TANL-1: coreference resolution by parseanalysis and similarity clustering.
In Proceedingsof SemEval-2.Amit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of theLREC Workshop on Linguistic Coreference, pages563?566.Samuel Broscheit, Massimo Poesio, Simone PaoloPonzetto, Kepa Joseba Rodr?
?guez, Lorenza Ro-mano, Olga Uryupina, Yannick Versley, and RobertoZanoli.
2010.
BART: A multilingual anaphora res-olution system.
In Proceedings of SemEval-2.Walter Daelemans, Sabine Buchholz, and Jorn Veen-stra.
1999.
Memory-based shallow parsing.
In Pro-ceedings of CoNLL 1999.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The Automatic Content Extrac-tion (ACE) program ?
Tasks, data, and evaluation.In Proceedings of LREC 2004, pages 837?840.Katrin Erk and Carlo Strapparava, editors.
2010.
Pro-ceedings of SemEval-2.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for German dependency and con-stituency representations.
In Proceedings of the ACLWorkshop on Parsing German (PaGe 2008), pages47?54.Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-mann.
2005.
A unified representation for morpho-logical, syntactic, semantic, and referential annota-tions.
In Proceedings of the ACL Workshop on Fron-tiers in Corpus Annotation II: Pie in the Sky, pages13?20.Lynette Hirschman and Nancy Chinchor.
1997.MUC-7 Coreference Task Definition ?
Version 3.0.In Proceedings of MUC-7.V?eronique Hoste and Guy De Pauw.
2006.
KNACK-2002: A richly annotated corpus of Dutch writtentext.
In Proceedings of LREC 2006, pages 1432?1437.Hamidreza Kobdani and Hinrich Sch?utze.
2010.
SU-CRE: A modular system for coreference resolution.In Proceedings of SemEval-2.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of HLT-EMNLP 2005, pages 25?32.Joseph F. McCarthy and Wendy G. Lehnert.
1995.
Us-ing decision trees for coreference resolution.
In Pro-ceedings of IJCAI 1995, pages 1050?1055.Thomas S. Morton.
1999.
Using coreference in ques-tion answering.
In Proceedings of TREC-8, pages85?89.Constantin Orasan, Dan Cristea, Ruslan Mitkov, andAnt?onio Branco.
2008.
Anaphora Resolution Exer-cise: An overview.
In Proceedings of LREC 2008.Sameer S. Pradhan, Eduard Hovy, Mitch Mar-cus, Martha Palmer, Lance Ramshaw, and RalphWeischedel.
2007.
Ontonotes: A unified rela-tional semantic representation.
In Proceedings ofthe International Conference on Semantic Comput-ing (ICSC 2007), pages 517?526.Marta Recasens and Eduard Hovy.
in prep.
BLANC:Implementing the Rand Index for Coreference Eval-uation.Marta Recasens and M. Ant`onia Mart??.
2009.
AnCora-CO: Coreferentially annotated corpora for Spanishand Catalan.
Language Resources and Evaluation,DOI:10.1007/s10579-009-9108-x.Kepa Joseba Rodr?
?guez, Francesca Delogu, YannickVersley, Egon Stemle, and Massimo Poesio.
2010.Anaphoric annotation of Wikipedia and blogs inthe Live Memories Corpus.
In Proceedings ofLREC 2010, pages 157?163.Emili Sapena, Llu?
?s Padr?o, and Jordi Turmo.
2010.RelaxCor: A global relaxation labeling approach tocoreference resolution for the SemEval-2 Corefer-ence Task.
In Proceedings of SemEval-2.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-grained POS tagging.
In Pro-ceedings of COLING 2008, pages 777?784.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to German.
InProceedings of the ACL SIGDAT Workshop, pages47?50.Josef Steinberger, Massimo Poesio, Mijail A. Kabad-jov, and Karel Jeek.
2007.
Two uses of anaphoraresolution in summarization.
Information Process-ing and Management: an International Journal,43(6):1663?1680.Olga Uryupina.
2010.
Corry: A system for corefer-ence resolution.
In Proceedings of SemEval-2.Gertjan van Noord, Ineke Schuurman, and VincentVandeghinste.
2006.
Syntactic annotation of largecorpora in STEVIN.
In Proceedings of LREC 2006.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of MUC-6, pages 45?52.Desislava Zhekova and Sandra K?ubler.
2010.
UBIU:A language-independent system for coreference res-olution.
In Proceedings of SemEval-2.8
