Active Learning and the Total Cost of AnnotationJason Baldridge and Miles OsborneSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9LW, UK{jbaldrid,miles}@inf.ed.ac.ukAbstractActive learning (AL) promises to reducethe cost of annotating labeled datasets fortrainable human language technologies.Contrary to expectations, when creatinglabeled training material for HPSG parseselection and later reusing it with othermodels, gains from AL may be negligibleor even negative.
This has serious impli-cations for using AL, showing that addi-tional cost-saving strategies may need tobe adopted.
We explore one such strategy:using a model during annotation to auto-mate some of the decisions.
Our best re-sults show an 80% reduction in annotationcost compared with labeling randomly se-lected data with a single model.1 IntroductionAL methods such as uncertainty sampling (Cohnet al, 1995) or query by committee (Seung et al,1992) have all been shown to dramatically reducethe cost of creating highly informative labeled setsfor speech and language technologies.
However, ex-periments using AL assume a model that is fixedahead of time: the model used in AL is the sameone we are currently developing training materialfor.
For many complex tasks, we are unlikely to havea clear idea how best to model the task at the time ofannotation; thus, in practice, we will need to reusethe labeled training material with other models.In this paper, we show that AL can be brittle: un-der a variety of natural reuse scenarios (for example,allowing the later model to improve in quality, orelse reusing the labeled training material using a dif-ferent machine learning algorithm) performance oflater models can be significantly undermined whentraining upon material created using AL.
The keyto knowing how well one model will be able to usematerial selected by another is their relatedness ?
yetthere may be no means to determine this prior to an-notation, leading to a chicken-and-egg problem.Our reusability results thus demonstrate that, ad-ditionally, other strategies must be adopted to en-sure we reduce the total cost of annotation.
In Os-borne and Baldridge (2004), we showed that ensem-ble models can increase model performance and alsoproduce annotation savings when incorporated intothe AL process.
An obvious next step is automatingsome decisions.
Here, we consider a simple automa-tion strategy that reduces annotation costs indepen-dently of AL and examine its effect on reusability.We find that using both semi-automation and ALwith high-quality models can eliminate the perfor-mance gap found in many reuse scenarios.
However,for weak models, we show that semi-automationwith random sampling is more effective for improv-ing reusability than using it with AL ?
demonstrat-ing further cause for caution with AL.Finally, we show that under the standard assump-tion of reuse by the selecting model, using a strat-egy which combines AL, ensembling, and semi-automated annotation, we are able to achieve ourhighest annotation savings to date on the complextask of parse selection for HPSG: an 80% reductionin annotation cost compared with labeling randomlyselected data with our best single model.2 Parse selection for RedwoodsWe now briefly describe the Redwoods treebankingenvironment (Oepen et al, 2002), our parse selec-tion models and their performance.2.1 The Redwoods TreebankThe Redwoods treebank project provides tools andannotated training material for creating parse se-lection models for the English Resource Grammar(ERG, Flickinger (2000)).
The ERG is a hand-builtbroad-coverage HPSG grammar that provides an ex-plicit grammar for the treebank.
Using this approachhas the advantage that analyses for within-coveragesentences convey more information than just phrasestructure: they also contain derivations, semantic in-terpretations, and basic dependencies.For each sentence, Redwoods records all analyseslicensed by the ERG and indicates which of them,if any, the annotators selected as being contextuallycorrect.
When selecting such distinguished parses,rather than simply enumerating all parses and pre-senting them to the annotator, annotators make useof discriminants which disambiguate the parse for-est more rapidly, as described in section 3.In this paper, we report results using the thirdgrowth of Redwoods, which contains English sen-tences from appointment scheduling and travel plan-ning domains of Verbmobil.
In all, there are 5302sentences for which there are at least two parses anda unique preferred parse is identified.
These sen-tences have 9.3 words and 58.0 parses on average.2.2 Modeling parse selectionAs is now standard for feature-based grammars, wemainly use log-linear models for parse selection(Johnson et al, 1999).
For log-linear models, theconditional probability of an analysis ti given a sen-tence with a set of analyses ?
= {t .
.
.}
is given as:P (ti|s,Mk) =exp(?mj=1 fj(ti)wj)Z(s)(1)where fj(ti) returns the number of times featurej occurs in analysis t, wj is a weight from modelMk, and Z(s) is a normalization factor for the sen-tence.
The parse with the highest probability is takenas the preferred parse for the model.
We use thelimited memory variable metric algorithm to deter-mine the weights.
We do not regularize our log-linear models since labeled data -necessary to sethyperparameters- is in short supply in AL.We also make use of simpler perceptron modelsfor parse selection, which assign scores rather thanprobabilities.
Scores are computed by taking the in-ner product of the analysis?
feature vector with theparameter vector:score(ti, s,Mk) =m?j=1fj(ti)wj (2)The preferred parse is that with the highest score outof all analyses.
We do not use voted perceptronshere (which indeed have better performance) as forthe reuse experiments described later in section 6 wereally do wish to use a model that is (potentially)worse than a log-linear model.Later for AL , it will be useful to map perceptronscores into probabilities, which we do by exponenti-ating and renormalizing the score:Pp(ti | s,Mk) =exp(score(ti, s,Mk))Z(s)(3)Z(s) is again a normalizing constant.The previous parse selection models (equations1 and 3) use a single model (feature set).
It ispossible to improve performance using an ensem-ble parse selection model.
We create our ensemblemodel (called a product model) using the product-of-experts formulation (Hinton, 1999):P (ti|s,M1 .
.
.Mn) =?nj=1 P (ti|s,Mj)Z(s)(4)Note that each individual model Mi is a well-defineddistribution usually taken from a fixed set of mod-els.
Z(s) is a constant to ensure the product distri-bution sums to one over the set of possible parses.
Aproduct model effectively averages the contributionsmade by each of the individual models.
Though sim-ple, this model is sufficient to show enhanced perfor-mance when using multiple models.2.3 Parse selection performanceOsborne and Baldridge (2004) describe three dis-tinct feature sets ?
configurational, ngram, andconglomerate ?
which utilize the various struc-tures made available in Redwoods: derivation trees,phrase structures, semantic interpretations, and ele-mentary dependency graphs.
They incorporate dif-ferent aspects of the parse selection task; this iscrucial for creating diverse models for use in prod-uct parse selection models as well as for ensemble-based AL methods.
Here, we also use models cre-ated from a subset of the conglomerate feature set:the mrs feature set.
This only has features from thesemantic interpretations.The three main feature sets are used to train threelog-linear models ?
LL-CONFIG, LL-NGRAM, andLL-CONGLOM?
and a product ensemble of thosethree feature sets, LL-PROD, using equation 4.
Addi-tionally, we use a perceptron with the conglomeratefeature set, P-CONGLOM.
Finally, we include a log-linear model that uses the mrs feature set, LL-MRS,and a perceptron, P-MRS.Parse selection accuracy is measured using exactmatch.
A model is awarded a point if it picks someparse for a sentence and that parse is the correct anal-ysis indicated by the corpus.
To deal with ties, theaccuracy is given as 1/m when a model ranks mparses highest and the best parse is one of them.The results for a chance baseline (selecting aparse at random), the base models and the productmodel are given in Table 1.
These are 10-fold cross-validation results, using all the training data for esti-mation and the test split for evaluation.
See section5 for more details.Model Perf.
Model Perf.LL-CONFIG 75.05 LL-PROD 77.78LL-NGRAM 74.01 LL-MRS 64.98LL-CONGLOM 74.85 P-CONGLOM 73.00Chance 22.70 P-MRS 62.11Table 1: Parse selection accuracy.3 Measuring annotation costTo aid identification of the best parse out of all thoselicensed by the ERG, the Redwoods annotation envi-ronment provides local discriminants which the an-notator can mark as true or false properties for theanalysis of a sentence in order to disambiguate largeportions of the parse forest.
As such, the annotatordoes not need to inspect all parses and so parses arenarrowed down quickly (usually exponentially so)even for sentences with a large number of parses.More interestingly, it means that the labeling burdenis relative to the number of possible parses (ratherthan the number of constituents in a parse).Data about how many discriminants were neededto annotate each sentence is recorded in Redwoods.Typically, more ambiguous sentences require morediscriminant values to be set, reflecting the extra ef-fort put into identifying the best parse.
We showedin Osborne and Baldridge (2004) that discriminantcost does provide a more accurate approximation ofannotation cost than assigning a fixed unit cost foreach sentence.
We thus use discriminants as the ba-sis of calculating annotation cost to evaluate the ef-fectiveness of different experiment AL conditions.Specifically, we set the cost of annotating a givensentence as the number of discriminants whosevalue were set by the human annotator plus one toindicate a final ?eyeball?
step where the annotator se-lects the best parse of the few remaining ones.1 Thediscriminant cost of the examples we use averages3.34 and ranges from 1 to 14.4 Active learningSuppose we have a set of examples and labels Dn ={?x1, y1?, ?x2, y2?, .
.
.}
which is to be extended witha new labeled example {?xi, yi?}.
The informationgain for some model is maximized after selecting,labeling, and adding a new example xi to Dn suchthat the noise level of xi is low and both the bias andvariance of some model using Dn ?
{?xi, yi?}
areminimized (Cohn et al, 1995).In practice, selecting data points for labeling suchthat a model?s variance and/or bias is maximallyminimized is computationally intractable, so ap-proximations are typically used instead.
One suchapproximation is uncertainty sampling.
Uncertaintysampling (also called tree entropy by Hwa (2000)),measures the uncertainty of a model over the set ofparses of a given sentence, based on the conditional1This eyeball step is not always taken, but Redwoods doesnot contain information about when this occurred, so we applythe cost for the step uniformly for all examples.distribution it assigns to them.
Following Hwa, weuse the following measure to quantify uncertainty:fus(s, ?,Mk) = ??t?
?P (t|s,Mk) logP (t|s,Mk) (5)?
denotes the set of analyses produced by the ERGfor the sentence and Mk is some model.
Higher val-ues of fus(s, ?,Mk) indicate examples on which thelearner is most uncertain .
Calculating fus is triv-ial with the conditional log-linear and perceptronsmodels described in section 2.2.Uncertainty sampling as defined above is a single-model approach.
It can be improved by simply re-placing the probability of a single log-linear (or per-ceptron) model with a product probability:fenus (s, ?,M) = ??t?
?P (t|s,M) logP (t|s,M) (6)M is the set of models M1 .
.
.Mn.
As we men-tioned earlier, AL for parse selection is potentiallyproblematic as sentences vary both in length and thenumber of parses they have.
Nonetheless, the abovemeasures do not use any extra normalization as wehave found no major differences after experimentingwith a variety of normalization strategies.We use random sampling as a baseline and un-certainty sampling for AL.
Osborne and Baldridge(2004) show that uncertainty sampling producesgood results compared with other AL methods.5 Experimental frameworkFor all experiments, we used a 20-fold cross-validation strategy by randomly selecting 10%(roughly 500 sentences) for the test set and select-ing samples from the remaining 90% (roughly 4500sentences) as training material.
Each run of AL be-gins with a single randomly chosen annotated seedsentence.
At each round, new examples are selectedfor annotation from a randomly chosen, fixed sized500 sentence subset according to random selectionor uncertainty sampling until models reach certaindesired accuracies.
We select 20 examples for anno-tation at each round, and exclude all examples thathave more than 500 parses.22Other parameter settings (such as how many examples tolabel at each stage) did not produce substantially different re-sults to those reported here.AL results are usually presented in terms of theamount of labeling necessary to achieve given per-formance levels.
We say that one method is bet-ter than another method if, for a given performancelevel, less annotation is required.
The performancemetric used here is parse selection accuracy as de-scribed in section 2.3.6 Reusing training materialAL can be considered as selecting some labeledtraining set which is ?tuned?
to the needs of a particu-lar model.
Typically, we might wish to reuse labeledtraining material, so a natural question to ask is howgeneral are training sets created using AL.
So, if welater improved upon our feature set, or else improvedupon our learner, would the previously created train-ing set still be useful?
If AL selects highly idiosyn-cratic datasets then we would not be able to reuse ourdatasets and thus it might, for example, actually bebetter to label datasets using random sampling.
Thisis a realistic situation since models typically changeand evolve over time ?
it would be very problem-atic if the training set itself inherently limits the ben-efit of later attempts to improve the model.We use two baselines to evaluate how well amodel is able to reuse data selected for labeling byanother model: (1) Selecting the data randomly.This provides the essential baseline; if AL in reusesituations is going to be useful, it ought to outper-form this model-free approach.
(2) Reuse by theAL model itself.
This is the standard AL scenario;against this, we can determine if reused data can beas good as when a model selects data for itself.We evaluate a variety of reuse scenarios.
We re-fer to the model used with AL as the selector andthe model that is reusing that labeled data as thereuser.
Models can differ in the machine learning al-gorithm and/or the feature set they use.
To measurerelatedness, we use Spearman?s rank correlation onthe rankings that two models assign to the parses ofa sentence.
The overall relatedness of two modelsis calculated as the average rank correlation on allexamples tested in a 10-fold parse selection experi-ment using all available training material.Figure 1 shows complete learning curves for LL-CONFIG when it reuses material selected by itself,LL-CONGLOM, P-MRS, and random sampling.
Thegraph shows that self-reuse is the most effective ofall strategies ?
this is the idealized situation com-monly assumed in active learning studies.
However,the graph reveals that random sampling is actuallymore effective than selection both by LL-CONGLOMuntil nearly 70% accuracy is reached and by P-MRSuntil about 73%.
Finally, we see that the materialselected by LL-CONGLOM is always more effectivefor LL-CONFIG than that selected by P-MRS. Thereason for this can be explained by the relatednessof each of these selector models to LL-CONFIG: LL-CONGLOM and LL-CONFIG have an average rankcorrelation of 0.84 whereas P-MRS and LL-CONFIGhave a correlation of 0.65.505560657075800  1000  2000  3000  4000  5000  6000  7000  8000AccuracyAnnotation costSelector: LL-CONFIGSelector: LL-CONGLOMSelector: RANDSelector: P-MRSFigure 1: Learning curves for LL-CONFIG whenreusing material by different selectors.Table 2 fleshes out the relationship between relat-edness and reusability more fully.
It shows the anno-tation cost incurred by various reusers to reach 65%,70%, and 73% accuracy when material is selectedby various models.
The list is ordered from top tobottom according to the rank correlation of the twomodels.
The first three lines provide the baselines ofwhen LL-PROD, LL-CONGLOM, and LL-CONFIG se-lect material for themselves.
The last three show theamount of material needed by these models whenrandom sampling is used.
The rest gives the resultsfor when the selector differs from the reuser.For each performance level, the percent increasein annotation cost over self-reuse is given.
Forexample, a cost of 2300 discriminants is requiredfor LL-PROD to reach the 73% performance levelwhen it reuses material selected by LL-CONGLOM;this is a 10% increase over the 2100 discriminantsneeded when LL-PROD selects for itself.
Similarly,the 5500 discriminants needed by LL-CONGLOM toreach 73% when reusing material selected by LL-CONFIG is a 31% increase over the 4200 discrimi-nants LL-CONGLOM needs with its own selection.As can be seen from Table 2, reuse always leadsto an increase in cost over self-reuse to reach a givenlevel of performance.
How much that increase willbe is in general inversely related to the rank corre-lation of the two models.
Furthermore, consideringeach reusing model individually, this relationship isalmost entirely inversely related at all performancelevels, with the exception of P-CONGLOM and LL-MRS selecting for LL-CONFIG at the 73% level.The reason for some models being more relatedto others is generally easy to see.
For example, LL-CONFIG and LL-CONGLOM are highly related to LL-PROD, of which they are both components.
In bothof these cases, using AL for use by LL-PROD beatsrandom sampling by a large amount.That LL-MRS is more related to LL-CONGLOMthan to LL-CONFIG is explained by the fact the mrsfeature set is actually a subset of the conglom set.The former contains 15% of the latter?s features.Accordingly, material selected by LL-MRS is alsogenerally more reusable by LL-CONGLOM than toLL-CONFIG.
This is encouraging since the case ofLL-CONGLOM reusing material selected by LL-MRSrepresents the common situation in which an initialmodel ?
that was used to develop the corpus ?
iscontinually improved upon.A particularly striking aspect revealed by Figure 1and Table 2 is that random sampling is overwhelm-ingly a better strategy when there is still little la-beled material.
AL tends to select examples whichare more ambiguous and hence have a higher dis-criminant cost.
So, while these examples may behighly informative for the selector model, they arenot cheap ?
and are far less effective when reusedby another model.Considering unit cost (i.e., each sentence costs thesame) instead of discriminant cost (which assigns avariable cost per sentence), AL is generally moreeffective than random sampling for reuse through-out all accuracy levels ?
but not always.
For exam-ple, even using unit cost, random sampling is bet-ter than selection by LL-MRS or P-MRS for reuse byRank 65% 70% 73%Selector Reuser Corr.
DC Incr DC Incr DC IncrLL-PROD LL-PROD 1.00 690 0.0% 1200 0.0% 2050 0.0%LL-CONGLOM LL-CONGLOM 1.00 1190 0.0% 2330 0.0% 4160 0.0%LL-CONFIG LL-CONFIG 1.00 1160 0.0% 2530 0.0% 4780 0.0%LL-CONFIG LL-PROD .92 850 23.2% 1470 22.5% 2430 18.5%LL-CONGLOM LL-PROD .92 840 21.7% 1560 30.0% 2630 28.3%LL-CONFIG LL-CONGLOM .84 1340 12.6% 2610 12.0% 4720 13.5%LL-CONGLOM LL-CONFIG .84 1660 43.1% 3760 48.6% 6840 43.1%P-CONGLOM LL-CONFIG .79 1960 69.0% 3910 54.5% 7940 66.1%LL-MRS LL-CONGLOM .77 1600 34.5% 3400 45.9% 6420 54.3%LL-MRS LL-PROD .76 1080 56.5% 2040 70.0% 3700 80.5%LL-MRS LL-CONFIG .71 2100 81.0% 4270 68.8% 6870 43.7%P-MRS LL-CONFIG .65 2650 128.4% 4870 92.5% 8260 72.8%RAND LL-PROD - 820 18.8% 1950 62.5% 3680 79.5%RAND LL-CONGLOM - 1400 17.6% 3470 48.9% 7150 71.9%RAND LL-CONFIG - 1160 0.0% 3890 53.8% 8560 79.1%Table 2: Comparison of various selection and reuse conditions.
Values are given for discriminant cost (DC)and the percent increase (Incr) in cost over use of material selected by the reuser.LL-CONFIG until 67% accuracy.
Thus, LL-MRS andP-MRS are so divergent from LL-CONFIG that theirselections are truly sub-optimal for LL-CONFIG, par-ticularly in the initial stages.Together, these results shows that AL cannot beused blindly and always be expected to reduce thetotal cost of annotation.
The data is tuned to themodels used during AL and how useful that datawill be for other models depends on the degree ofrelatedness of the models under consideration.Given that AL may or may not provide cost reduc-tions, we consider the effect that semi-automatingannotation has on reducing the total cost of annota-tion when used with and without AL.7 Semi-automated labelingCorpus building, with or without AL, is generallyviewed as selecting examples and then from scratchlabeling such examples.
This can be inefficient, es-pecially when dealing with labels that have complexinternal structures, as a model may be able to rule-out some of the labeling possibilities.For our domain, we exploit the fact that we mayalready have partial information about an example?slabel by presenting only the top n-best parses tothe annotator, who then navigates to the best parsewithin that set using those discriminants relevant tothat set of parses.
Rather than using a value for nthat is fixed or proportional to the ambiguity of thesentence, we simply select all parses for which themodel assigns a probability higher than chance.
Thishas the advantage of reducing the number of parsespresented to the annotator as the model uses moretraining material and reduces its uncertainty.When the true best parse is within the top n pre-sented to the annotator, the cost we record is thenumber of discriminants needed to identify it fromthat subset, plus one ?
the same calculation as whenall parses are presented, with the advantage thatfewer discriminants and parses need to be inspected.When the best parse is not present in the n-bestsubset, there is a question as to how to record theannotation cost.
The discriminant decisions madein reducing the subset are still valid and useful inidentifying the best parse from the entire set, but wemust incur some penalty for the fact that the anno-tator must confirm that this is the case.
To deter-mine the cost for such situations, we add one to theusual full cost of annotating the sentence.
This en-codes what we feel is a reasonable reflection of thepenalty since decisions taken in the n-best phase arestill valid in the context of all parses.3Performance level65% 70% 73%1.
RAND 820 1950 36802.
LL-PROD 690 1200 20503.
RAND (NB) 670 1350 24304.
LL-PROD (NB) 680 1120 1760Table 3: Cost for LL-PROD to reach given perfor-mance levels when using n-best automation (NB).Table 3 shows the effects of using semi-automatedlabeling with LL-PROD.
As can be seen, randomselection costs reduce dramatically with n-best au-tomation (compare rows 1 and 3).
It is also an earlywinner over basic uncertainty sampling (row 2),though the latter eventually reaches the higher ac-curacies more quickly.
Nonetheless, the mixture ofAL and semi-automation provides the biggest over-all gains: to reach 73% accuracy, n-best uncertaintysampling (row 4) reduces the cost by 17% over n-best random sampling (row 3) and by 15% over ba-sic uncertainty sampling (row 2).
Similar patternshold for n-best automation with LL-CONFIG.Figure 2 provides an overall view on the accumu-lative effects of ensembling, n-best automation, anduncertainty sampling in the ideal situation of reuseby the AL model itself.
Ensemble models and n-bestautomation show that massive improvements can bemade without AL.
Nonetheless, we see the largestreductions by using AL, n-best automation, and en-semble models together: LL-PROD using uncertaintysampling and n-best automation (row 4 of Table 3)reaches 73% accuracy with a cost of 1760 comparedto 8560 needed by LL-CONFIG using random sam-pling without automation.
This is our best annota-tion saving: a cost reduction of 80%.8 Closing the reuse gapThe previous section?s semi-automated labeling ex-periments did not involve reuse.
If models are ex-pected to evolve, could n-best automation fill in thecost gap created by reuse?
To test this, we con-sidered reusing examples with our best model (LL-3When we do not allow ourselves to benefit from such la-beling decisions, our annotation savings naturally decrease, butnot below when we do not use n-best labeling.505560657075800  500  1000  1500  2000  2500  3000  3500  4000  4500  5000AccuracyAnnotation costLL-PRODUCT, N-best, Uncertainty samplingLL-PRODUCT, N-best, Random samplingLL-PRODUCT, Random samplingLL-CONFIG, Random samplingFigure 2: Learning curves for accumulative im-provements to the annotation scenario starting fromrandom sampling with LL-CONFIG: ensembling, n-best automation, and uncertainty sampling.PROD), as selected by different models using bothAL and n-best automation as a combined strategy.For LL-CONFIG and LL-CONGLOM as selectors, thegap is entirely closed: costs for reuse were virtuallyequal to when LL-PROD selects examples for itselfwithout n-best (Table 3, row 2).The gap also closes when n-best automation andAL are used with the weaker LL-MRS model.
Per-formance (Table 4, row 1) still falls far short of LL-PROD selecting for itself without n-best (Table 3,row 2).
However, the gap closes even more when n-best automation and random sampling are used withLL-MRS (Table 4, line 2).Performance level65% 70% 73%1.
NB & US 1040 1920 33202.
NB & RAND 680 1450 2890Table 4: Cost for LL-PROD to reach given perfor-mance levels in reuse situations where n-best au-tomation (NB) was used with LL-MRS with uncer-tainty sampling (US) or random sampling (RAND).Interestingly, when using a weak selector (LL-MRS), n-best automation combined with randomsampling was more effective than when combinedwith uncertainty sampling.
The reason for this isclear.
Since AL typically selects more ambiguousexamples, a weak model has more difficulty gettingthe best parse within the n-best when AL is used.Thus, the gains from the more informative examplesselected by AL are surpassed by the gains that comewith the easier labeling with random sampling.For most situations, n-best automation is benefi-cial: the gap introduced by reuse can be reduced.
n-best automation never results in an increase in cost.This is still true even if we do not allow ourselves toreuse those discriminants which were used to selectthe best parse from the n-best subset and the bestparse was not actually present in that subset.9 Related workThere is a large body of AL work in the machinelearning literature, but less so within natural lan-guage processing (NLP).
Most work in NLP hasprimarily focused upon uncertainty sampling (Hwa,2000; Tang et al, 2002).
Hwa (2001) consideredreuse of examples selected for one parser by an-other with uncertainty sampling.
This performedbetter than sequential sampling but was only half aseffective as self-selection.
Here, we have consid-ered reuse with respect to many models and theirco-relatedness.
Also, we compare reuse perfor-mance against against random sampling, which weshowed previously to be a much stronger baselinethan sequential sampling for the Redwoods corpus(Osborne and Baldridge, 2004).
Hwa et al (2003)showed that for parsers, AL outperforms the closelyrelated co-training, and that some of the labelingcould be automated.
However, their approach re-quires strict independence assumptions.10 DiscussionAL should only be considered for creating labeleddata when the the task is either well-understood orelse the model is unlikely to substantially change.Otherwise, it would be prudent to consider improv-ing either the model itself (using, for example, en-semble techniques) or else semi-automating the la-beling task.
Naturally, there is a cost associated withcreating the model itself, and this in turn will needto be factored into the total cost.
When there is gen-uine uncertainty about the model, or else how thelabeled data is going to be eventually used, then thebest strategy may well be to use random selectionrather than AL ?
especially when using some formof automated annotation.AcknowledgmentsWe would like to thank Markus Becker, JeremiahCrim, Dan Flickinger, Alex Lascarides, StephanOepen, and Andrew Smith.
We?d also like tothank pc-jbaldrid and pc-rosie for theirhard work and 24/7 dedication.
This work was sup-ported by Edinburgh-Stanford Link R36763, ROSIEproject.ReferencesDavid A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.1995.
Active learning with statistical models.
In G. Tesauro,D.
Touretzky, and T. Leen, editors, Advances in Neural Infor-mation Processing Systems, volume 7, pages 705?712.
TheMIT Press.Dan Flickinger.
2000.
On building a more efficient grammar byexploiting types.
Natural Language Engineering, 6(1):15?28.
Special Issue on Efficient Processing with HPSG.G.
E. Hinton.
1999.
Products of experts.
In Proc.
of the 9th Int.Conf.
on Artificial Neural Networks, pages 1?6.Rebecca Hwa, Miles Osborne, Anoop Sarkar, and Mark Steed-man.
2003.
Corrected Co-training for Statistical Parsers.
InProceedings of the ICML Workshop ?The Continuum fromLabeled to Unlabeled Data?, pages 95?102.
ICML-03.Rebecca Hwa.
2000.
Sample selection for statistical gram-mar induction.
In Proc.
of the 2000 Joint SIGDAT Conf.
onEMNLP and VLC, pages 45?52, Hong Kong, China.Rebecca Hwa.
2001.
On minimizing training corpus for parseracquisition.
In Proc.
of the 5th Conference on Natural Lan-guage Learning, Toulouse.Mark Johnson, Stuart Geman, Stephen Cannon, Zhiyi Chi,and Stephan Riezler.
1999.
Estimators for Stochastic?Unification-Based?
Grammars.
In 37th Annual Meeting ofthe ACL.Stephan Oepen, Kristina Toutanova, Stuart Shieber, ChristopherManning, Dan Flickinger, and Thorsten Brants.
2002.
TheLinGO Redwoods Treebank: Motivation and preliminary ap-plications.
In Proc.
of the 19th International Conference onComputational Linguistics, Taipei, Taiwan.Miles Osborne and Jason Baldridge.
2004.
Ensemble-basedactive learning for parse selection.
In Proc.
of HLT-NAACL,Boston.H.
S. Seung, Manfred Opper, and Haim Sompolinsky.
1992.Query by committee.
In Computational Learning Theory,pages 287?294.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.
Ac-tive Learning for Statistical Natural Language Parsing.
InProc.
of the 40th Annual Meeting of the ACL, pages 120?127, Philadelphia, Pennsylvania, USA, July.
