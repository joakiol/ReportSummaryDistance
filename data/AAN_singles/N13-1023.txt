Proceedings of NAACL-HLT 2013, pages 230?238,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSegmentation Strategies for Streaming Speech TranslationVivek Kumar Rangarajan Sridhar, John Chen, Srinivas BangaloreAndrej Ljolje, Rathinavelu ChengalvarayanAT&T Labs - Research180 Park Avenue, Florham Park, NJ 07932vkumar,jchen,srini,alj,rathi@research.att.comAbstractThe study presented in this work is a first ef-fort at real-time speech translation of TEDtalks, a compendium of public talks with dif-ferent speakers addressing a variety of top-ics.
We address the goal of achieving a sys-tem that balances translation accuracy and la-tency.
In order to improve ASR performancefor our diverse data set, adaptation techniquessuch as constrained model adaptation and vo-cal tract length normalization are found to beuseful.
In order to improve machine transla-tion (MT) performance, techniques that couldbe employed in real-time such as monotonicand partial translation retention are found tobe of use.
We also experiment with insertingtext segmenters of various types between ASRand MT in a series of real-time translation ex-periments.
Among other results, our experi-ments demonstrate that a good segmentationis useful, and a novel conjunction-based seg-mentation strategy improves translation qual-ity nearly as much as other strategies suchas comma-based segmentation.
It was alsofound to be important to synchronize variouspipeline components in order to minimize la-tency.1 IntroductionThe quality of automatic speech-to-text and speech-to-speech (S2S) translation has improved so signifi-cantly over the last several decades that such systemsare now widely deployed and used by an increasingnumber of consumers.
Under the hood, the individ-ual components such as automatic speech recogni-tion (ASR), machine translation (MT) and text-to-speech synthesis (TTS) that constitute a S2S sys-tem are still loosely coupled and typically trainedon disparate data and domains.
Nevertheless, themodels as well as the pipeline have been optimizedin several ways to achieve tasks such as high qual-ity offline speech translation (Cohen, 2007; Kings-bury et al 2011; Federico et al 2011), on-demandweb based speech and text translation, low-latencyreal-time translation (Wahlster, 2000; Hamon et al2009; Bangalore et al 2012), etc.
The design of aS2S translation system is highly dependent on thenature of the audio stimuli.
For example, talks, lec-tures and audio broadcasts are typically long and re-quire appropriate segmentation strategies to chunkthe input signal to ensure high quality translation.In contrast, single utterance translation in severalconsumer applications (apps) are typically short andcan be processed without the need for additionalchunking.
Another key parameter in designing aS2S translation system for any task is latency.
Inoffline scenarios where high latencies are permit-ted, several adaptation strategies (speaker, languagemodel, translation model), denser data structures (N-best lists, word sausages, lattices) and rescoring pro-cedures can be utilized to improve the quality ofend-to-end translation.
On the other hand, real-time speech-to-text or speech-to-speech translationdemand the best possible accuracy at low latenciessuch that communication is not hindered due to po-tential delay in processing.In this work, we focus on the speech translationof talks.
We investigate the tradeoff between accu-racy and latency for both offline and real-time trans-lation of talks.
In both these scenarios, appropriatesegmentation of the audio signal as well as the ASRhypothesis that is fed into machine translation is crit-ical for maximizing the overall translation quality ofthe talk.
Ideally, one would like to train the modelson entire talks.
However, such corpora are not avail-able in large amounts.
Hence, it is necessary to con-230form to appropriately sized segments that are similarto the sentence units used in training the languageand translation models.
We propose several non-linguistic and linguistic segmentation strategies forthe segmentation of text (reference or ASR hypothe-ses) for machine translation.
We address the prob-lem of latency in real-time translation as a functionof the segmentation strategy; i.e., we ask the ques-tion ?what is the segmentation strategy that maxi-mizes the number of segments while still maximiz-ing translation accuracy?
?.2 Related WorkSpeech translation of European Parliamentaryspeeches has been addressed as part of the TC-STAR project (Vilar et al 2005; Fu?gen et al 2006).The project focused primarily on offline translationof speeches.
Simultaneous translation of lecturesand speeches has been addressed in (Hamon et al2009; Fu?gen et al 2007).
However, the work fo-cused on a single speaker in a limited domain.
Of-fline speech translation of TED1 talks has been ad-dressed through the IWSLT 2011 and 2012 evalua-tion tracks.
The talks are from a variety of speakerswith varying dialects and cover a range of topics.The study presented in this work is the first effort onreal-time speech translation of TED talks.
In com-parison with previous work, we also present a sys-tematic study of the accuracy versus latency tradeofffor both offline and real-time translation on the samedataset.Various utterance segmentation strategies for of-fline machine translation of text and ASR outputhave been presented in (Cettolo and Federico, 2006;Rao et al 2007; Matusov et al 2007).
The workin (Fu?gen et al 2007; Fu?gen and Kolss, 2007)also examines the impact of segmentation on of-fline speech translation of talks.
However, the real-time analysis in that work is presented only forspeech recognition.
In contrast with previous work,we tackle the latency issue in simultaneous transla-tion of talks as a function of segmentation strategyand present some new linguistic and non-linguisticmethodologies.
We investigate the accuracy versuslatency tradeoff across translation of reference text,utterance segmented speech recognition output and1http://www.ted.compartial speech recognition hypotheses.3 Problem FormulationThe basic problem of text translation can be formu-lated as follows.
Given a source (French) sentencef = fJ1 = f1, ?
?
?
, fJ , we aim to translate it intotarget (English) sentence e?
= e?I1 = e?1, ?
?
?
, e?I .e?
(f) = arg maxePr(e|f) (1)If, as in talks, the source text (reference or ASR hy-pothesis) is very long, i.e., J is large, we attemptto break down the source string into shorter se-quences, S = s1 ?
?
?
sk ?
?
?
sQs , where each sequencesk = [fjkfjk+1 ?
?
?
fj(k+1)?1], j1 = 1, jQs+1 =J + 1.
Let the translation of each foreign sequencesk be denoted by tk = [eikeik+1 ?
?
?
ei(k+1)?1], i1 =1, iQs+1 = I?+ 12.
The segmented sequences canbe translated using a variety of techniques such asindependent chunk-wise translation or chunk-wisetranslation conditioned on history as shown in Eqs.
2and 3, respectively.
In Eq.
3, t?i denotes the besttranslation for source sequence si.e?
(f) = arg maxt1Pr(t1|s1) ?
?
?
arg maxtkPr(tk|sk)(2)e?
(f) = arg maxt1Pr(t1|s1) arg maxt2Pr(t2|s2, s1, t?1)?
?
?
arg maxtkPr(tk|s1, ?
?
?
, sk, t?1, ?
?
?
, t?k?1)(3)Typically, the hypothesis e?
will be more accuratethan e?
for long texts as the models approximatingPr(e|f) are conventionally trained on short text seg-ments.
In Eqs.
2 and 3, the number of sequences Qsis inversely proportional to the time it takes to gen-erate partial target hypotheses.
Our main focus inthis work is to obtain a segmentation S such that thequality of translation is maximized with minimal la-tency.
The above formulation for automatic speechrecognition is very similar except that the foreignstring f?
= f?J1 = f?1, ?
?
?
, f?J?
is obtained by decodingthe input speech signal.2The segmented and unsegmented talk may not be equal inlength, i.e., I 6= I?231Model Language Vocabulary #words #sents CorporaAcoustic Model en 46899 2611144 148460 1119 TED talksASR Language Model en 378915 3398460155 151923101 Europarl, WMT11 Gigaword, WMT11 News crawlWMT11 News-commentary, WMT11 UN, IWSLT11 TED trainingParallel text en 503765 76886659 7464857 IWSLT11 TED training talks, Europarl, JRC-ACQUISOpensubtitles, Web dataMT es 519354 83717810 7464857Language Model es 519354 83717810 7464857 Spanish side of parallel textTable 1: Statistics of the data used for training the speech translation models.4 DataIn this work, we focus on the speech translationof TED talks, a compendium of public talks fromseveral speakers covering a variety of topics.
Overthe past couple of years, the International Work-shop on Spoken Language Translation (IWSLT) hasbeen conducting the evaluation of speech translationon TED talks for English-French.
We leverage theIWSLT TED campaign by using identical develop-ment (dev2010) and test data (tst2010).
However,English-Spanish is our target language pair as ourinternal projects are cater mostly to this pair.
As aresult, we created parallel text for English-Spanishbased on the reference English segments released aspart of the evaluation (Cettolo et al 2012).We also harvested the audio data from the TEDwebsite for building an acoustic model.
A totalof 1308 talks in English were downloaded, out ofwhich we used 1119 talks recorded prior to Decem-ber 2011.
We split the stereo audio file and dupli-cated the data to account for any variations in thechannels.
The data for the language models was alsorestricted to that permitted in the IWSLT 2011 eval-uation.
The parallel text for building the English-Spanish translation model was obtained from sev-eral corpora: Europarl (Koehn, 2005), JRC-Acquiscorpus (Steinberger et al 2006), Opensubtitle cor-pus (Tiedemann and Lars Nygaard, 2004), Webcrawling (Rangarajan Sridhar et al 2011) as well ashuman translation of proprietary data.
Table 1 sum-marizes the data used in building the models.
It isimportant to note that the IWSLT evaluation on TEDtalks is completely offline.
In this work, we performthe first investigation into the real-time translation ofthese talks.5 Speech Translation ModelsIn this section, we describe the acoustic, languageand translation models used in our experiments.5.1 Acoustic and Language ModelWe use the AT&T WATSONSM speech recog-nizer (Goffin et al 2004).
The speech recogni-tion component consisted of a three-pass decodingapproach utilizing two acoustic models.
The mod-els used three-state left-to-right HMMs representingjust over 100 phonemes.
The phonemes representedgeneral English, spelled letters and head-body-tailrepresentation for the eleven digits (with ?zero?
and?oh?).
The pronunciation dictionary used the appro-priate phoneme subset, depending on the type of theword.
The models had 10.5k states and 27k HMMs,trained on just over 300k utterances, using both ofthe stereo channels.
The baseline model training wasinitialized with several iterations of ML training, in-cluding two builds of context dependency trees, fol-lowed by three iterations of Minimum Phone Error(MPE) training.The Vocal Tract Length Normalization (VTLN)was applied in two different ways.
One was esti-mated on an utterance level, and the other at the talklevel.
No speaker clustering was attempted in train-ing.
The performance at test time was comparablefor both approaches on the development set.
Oncethe warps were estimated, after five iterations, theML trained model was updated using MPE training.Constrained model adaptation (CMA) was appliedto the warped features and the adapted features wererecognized in the final pass with the VTLN model.All the passes used the same LM.
For offline recog-nition the warps, and the CMA adaptation, are per-formed at the talk level.
For the real-time speechtranslation experiments, we used the VTLN model.232The English language model was built using thepermissible data in the IWSLT 2011 evaluation.
Thetexts were normalized using a variety of cleanup,number and spelling normalization techniques andfiltered by restricting the vocabulary to the top375000 types; i.e., any sentence containing a to-ken outside the vocabulary was discarded.
First, weremoved extraneous characters beyond the ASCIIrange followed by removal of punctuations.
Sub-sequently, we normalized hyphenated words and re-moved words with more than 25 characters.
The re-sultant text was normalized using a variety of num-ber conversion routines and each corpus was fil-tered by restricting the vocabulary to the top 150000types; i.e., any sentence containing a token outsidethe vocabulary was discarded.
The vocabulary fromall the corpora was then consolidated and anotherround of filtering to the top 375000 most frequenttypes was performed.
The OOV rate on the TEDdev2010 set is 1.1%.
We used the AT&T FSMtoolkit (Mohri et al 1997) to train a trigram lan-guage model (LM) for each component (corpus).
Fi-nally, the component language models were interpo-lated by minimizing the perplexity on the dev2010set.
The results are shown in Table 2.Accuracy (%)Model dev2010 test2010Baseline MPE 75.5 73.8VTLN 78.8 77.4CMA 80.5 80.0Table 2: ASR word accuracies on the IWSLT datasets.35.2 Translation ModelWe used the Moses toolkit (Koehn et al 2007) forperforming statistical machine translation.
Mini-mum error rate training (MERT) was performed onthe development set (dev2010) to optimize the fea-ture weights of the log-linear model used in trans-lation.
During decoding, the unknown words werepreserved in the hypotheses.
The data used to trainthe model is summarized in Table 1.3We used the standard NIST scoring package as we did nothave access to the IWSLT evaluation server that may normalizeand score differentlyWe also used a finite-state implementation oftranslation without reordering.
Reordering can posea challenge in real-time S2S translation as the text-to-speech synthesis is monotonic and cannot retractalready synthesized speech.
While we do not ad-dress the text-to-speech synthesis of target text inthis work, we perform this analysis as a precursorto future work.
We represent the phrase transla-tion table as a weighted finite state transducer (FST)and the language model as a finite state acceptor(FSA).
The weight on the arcs of the FST is thedot product of the MERT weights with the transla-tion scores.
In addition, a word insertion penaltywas also applied to each word to penalize short hy-potheses.
The decoding process consists of compos-ing all possible segmentations of an input sentencewith the phrase table FST and language model, fol-lowed by searching for the best path.
Our FST-basedtranslation is the equivalent of phrase-based transla-tion in Moses without reordering.
We present re-sults using the independent chunk-wise strategy andchunk-wise translation conditioned on history in Ta-ble 3.
The chunk-wise translation conditioned onhistory was performed using the continue-partial-translation option in Moses.6 Segmentation StrategiesThe output of ASR for talks is a long string ofwords with no punctuation, capitalization or seg-mentation markers.
In most offline ASR systems,the talk is first segmented into short utterance-likeaudio segments before passing them to the decoder.Prior work has shown that additional segmentationof ASR hypotheses of these segments may be nec-essary to improve translation quality (Rao et al2007; Matusov et al 2007).
In a simultaneousspeech translation system, one can neither find theoptimal segmentation of the entire talk nor toleratehigh latencies associated with long segments.
Con-sequently, it is necessary to decode the incoming au-dio incrementally as well as segment the ASR hy-potheses appropriately to maximize MT quality.
Wepresent a variety of linguistic and non-linguistic seg-mentation strategies for segmenting the source textinput into MT.
In our experiments, they are appliedto different inputs including reference text, ASR 1-best hypothesis for manually segmented audio and233incremental ASR hypotheses from entire talks.6.1 Non-linguistic segmentationThe simplest method is to segment the incoming textaccording to length in number of words.
Such a pro-cedure can destroy semantic context but has little tono overhead in additional processing.
We experi-ment with segmenting the text according to wordwindow sizes of length 4, 8, 11, and 15 (denotedas data sets win4, win8, win11, win15, respectivelyin Table 3).
We also experiment with concatenatingall of the text from one TED talk into a single chunk(complete talk).A novel hold-output model was also developed inorder to segment the input text.
Given a pair of par-allel sentences, the model segments the source sen-tence into minimally sized chunks such that crossinglinks and links of one target word to many sourcewords in an optimal GIZA++ alignment (Och andNey, 2003) occur only within individual chunks.The motivation behind this model is that if a segments0 is input at time t0 to an incremental MT system,it can be translated right away without waiting for asegment si that is input at a later time ti, ti > 0.
Thehold-output model detects these kinds of segmentsgiven a sequence of English words that are inputfrom left to right.
A kernel-based SVM was used todevelop this model.
It tags a token t in the input witheither the label HOLD, meaning to chunk it with thenext token, or the label OUTPUT, meaning to outputthe chunk constructed from the maximal consecutivesequence of tokens preceding t that were all taggedas HOLD.
The model considers a five word and POSwindow around the target token t. Unigram, bigram,and trigram word and POS features based upon thiswindow are used for classification.
Training and de-velopment data for the model was derived from theEnglish-Spanish TED data (see Table 1) after run-ning it through GIZA++.
Accuracy of the model onthe development set was 66.62% F-measure for theHOLD label and 82.75% for the OUTPUT label.6.2 Linguistic segmentationSince MT models are trained on parallel text sen-tences, we investigate segmenting the source textinto sentences.
We also investigate segmenting thetext further by predicting comma separated chunkswithin sentences.
These tasks are performed bytraining a kernel-based SVM (Haffner et al 2003)on a subset of English TED data.
This dataset con-tained 1029 human-transcribed talks consisting ofabout 103,000 sentences containing about 1.6 mil-lion words.
Punctuation in this dataset was normal-ized as follows.
Different kinds of sentence endingpunctuations were transformed into a uniform end ofsentence marker.
Double-hyphens were transformedinto commas.
Commas already existing in the inputwere kept while all other kinds of punctuation sym-bols were deleted.
A part of speech (POS) taggerwas applied to this input.
For speed, a unigram POStagger was implemented which was trained on thePenn Treebank (Marcus et al 1993) and used or-thographic features to predict the POS of unknownwords.
The SVM-based punctuation classifier relieson a five word and POS window in order to classifythe target word.
Specifically, token t0 is classifiedgiven as input the window t?2t?1tot1t2.
Unigram,bigram, and trigram word and POS features based onthis window were used for classification.
Accuracyof the classifier on the development set was 60.51%F-measure for sentence end detection and 43.43%F-measure for comma detection.
Subsequently, datasets pred-sent (sentences) and pred-punct (comma-separated chunks) were obtained.
Corresponding tothese, two other data sets ref-sent and ref-punct wereobtained based upon gold-standard punctuations inthe reference.Besides investigating the use of comma-separatedsegments, we investigated other linguistically moti-vated segments.
These included conjunction-wordbased segments.
These segments are separated ateither conjunction (e.g.
?and,?
?or?)
or sentence-ending word boundaries.
Conjunctions were iden-tified using the unigram POS tagger.
F-measureperformance for detecting conjunctions by the tag-ger on the development set was quite high, 99.35%.As an alternative, text chunking was performedwithin each sentence, with each chunk correspond-ing to one segment.
Text chunks are non-recursivesyntactic phrases in the input text.
We investi-gated segmenting the source into text chunks us-ing TreeTagger, a decision-tree based text chun-ker (Schmid, 1994).
Initial sets of text chunkswere created by using either gold-standard sentenceboundaries or boundaries detected using the punc-tuation classifier, yielding the data sets chunk-ref-234Reference text ASR 1-bestBLEU Mean BLEU MeanSegmentation Segmentation Independent chunk-wise chunk-wise #words Independent chunk-wise chunk-wise #wordstype strategy FST Moses with history per segment FST Moses with history per segmentwin4 22.6 21.0 25.5 3.9?0.1 17.7 17.1 20.0 3.9?0.1win8 26.6 26.2 28.2 7.9?0.3 20.6 20.9 22.3 7.9?0.2Non-linguistic win11 27.2 27.4 29.2 10.9?
0.3 21.5 21.8 23.1 10.9?0.4win15 28.5 28.5 29.4 14.9?0.6 22.3 22.8 23.3 14.9?0.7ref-hold 13.3 14.0 17.1 1.6?1.9 12.7 13.1 17.5 1.5?1.0pred-hold 15.9 15.7 16.3 2.2?1.9 12.6 12.9 17.4 1.5?1.0complete talk 23.8 23.9 ?
2504 18.8 19.2 ?
2515ref-sent 30.6 31.5 30.5 16.7?11.8 24.3 25.1 24.4 17.0?11.6ref-punct 30.4 31.5 30.3 7.1?5.3 24.2 25.1 24.1 8.7?6.1pred-punct 30.6 31.5 30.4 8.7?8.8 24.1 25.0 24.0 8.8?6.8conj-ref-eos 30.5 31.5 30.2 11.2?7.5 24.1 24.9 24.0 11.5?7.7conj-pred-eos 30.3 31.2 30.3 10.9?7.9 24.0 24.8 24.0 11.4?8.5chunk-ref-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.2 16.9 1.4?0.7Linguistic lgchunk1-ref-punct 21.0 21.8 25.1 1.7?1.0 16.9 17.4 19.6 1.8?1.0lgchunk2-ref-punct 22.4 23.1 26.0 2.1?1.1 17.9 18.4 20.4 2.1?1.1lgchunk3-ref-punct 24.3 25.1 27.4 2.5?1.7 19.2 19.9 21.3 2.5?1.7chunk-pred-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.1 16.9 1.4?0.7lgchunk1-pred-punct 21.2 21.9 25.2 1.8?1.0 16.7 17.2 19.7 1.8?1.0lgchunk2-pred-punct 22.6 23.1 26.0 2.1?1.2 17.7 18.3 20.5 2.1?1.2lgchunk3-pred-punct 24.5 25.3 27.4 2.6?1.8 19.1 20.0 21.3 2.5?1.7Table 3: BLEU scores at the talk level for reference text and ASR 1-best for various segmentation strategies.The ASR 1-best was performed on manually segmented audio chunks provided in tst2010 set.punct and chunk-pred-punct.
Chunk types includedNC (noun chunk), VC (verb chunk), PRT (particle),and ADVC (adverbial chunk).Because these chunks may not provide sufficientcontext for translation, we also experimented withconcatenating neighboring chunks of certain typesto form larger chunks.
Data sets lgchunk1 concate-nate together neighboring chunk sequences of theform NC, VC or NC, ADVC, VC, intended to cap-ture as single chunks instances of subject and verb.In addition to this, data sets lgchunk2 capture chunkssuch as PC (prepositional phrase) and VC followedby VC (control and raising verbs).
Finally, data setslgchunk3 capture as single chunks VC followed byNC and optionally followed by PRT (verb and its di-rect object).Applying the conjunction segmenter after theaforementioned punctuation classifier in order to de-tect the ends of sentences yields the data set conj-pred-eos.
Applying it on sentences derived from thegold-standard punctuations yields the data set conj-ref-eos.
Finally, applying the hold-output model tosentences derived using the punctuation classifierproduces the data set pred-hold.
Obtaining Englishsentences tagged with HOLD and OUTPUT directlyfrom the output of GIZA++ on English-Spanish sen-tences in the reference produces the data set ref-hold.The strategies containing the keyword ref for ASRsimply means that the ASR hypotheses are used inplace of the gold reference text.0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.80100020003000400050006000processing time per token (sec)ASRtimeout (ms)ASR+MT(BLEU)10.011.712.613.313.714.014.114.314.614.714.714.8ASR+PunctSeg+MT(BLEU)15.115.115.115.115.115.115.115.115.115.115.115.1Figure 1: Latencies and BLEU scores for tst2010 setusing incremental ASR decoding and translationWe also performed real-time speech translation byusing incremental speech recognition, i.e., the de-coder returns partial hypotheses that, independent of235the pruning during search, will not change in thefuture.
Figure 1 shows the plot for two scenarios:one in which the partial hypotheses are sent directlyto machine translation and another where the bestsegmentation strategy pred-punct is used to segmentthe partial output before sending it to MT.
The plotshows the BLEU scores as a function of ASR time-outs used to generate the partial hypotheses.
Fig-ure 1 also shows the average latency involved in in-cremental speech translation.7 DiscussionThe BLEU scores for the segmentation strategiesover ASR hypotheses was computed at the talk level.Since the ASR hypotheses do not align with thereference source text, it is not feasible to evalu-ate the translation performance using the gold refer-ence.
While other studies have used an approximateedit distance algorithm for resegmentation of the hy-potheses (Matusov et al 2005), we simply concate-nate all the segments and perform the evaluation atthe talk level.The hold segmentation strategy yields the poor-est translation performance.
The significant drop inBLEU score can be attributed to relatively short seg-ments (2-4 words) that was generated by the model.The scheme oversegments the text and since thetranslation and language models are trained on sen-tence like chunks, the performance is poor.
For ex-ample, the input text the sea should be translatedas el mar, but instead the hold segmenter chunks itas the?sea which MT?s chunk translation renders asel?el mar.
It will be interesting to increase the spanof the hold strategy to subsume more contiguous se-quences and we plan to investigate this as part offuture work.The chunk segmentation strategy yields quite poortranslation performance.
In general, it does notmake the same kinds of errors that the hold strat-egy makes; for example, the input text the sea willbe treated as one NC chunk by the chunk seg-mentation strategy, leading MT to translate it cor-rectly as el mar.
The short chunk sizes of chunklead to other kinds of errors.
For example, the in-put text we use will be chunked into the NC weand the VC use, which will be translated incor-rectly as nosotros?usar; the infinitive usar is se-lected rather than the properly conjugated form us-amos.
However, there is a marked improvement intranslation accuracy with increasingly larger chunksizes (lgchunk1, lgchunk2, and lgchunk3).
Notably,lgchunk3 yields performance that approaches that ofwin8 with a chunk size that is one third of win8?s.The conj-pred-eos and pred-punct strategies workthe best, and it can be seen that the average seg-ment length (8-12 words) generated in both theseschemes is very similar to that used for training themodels.
It is also about the average latency (4-5seconds) that can be tolerated in cross-lingual com-munication, also known as ear-voice span (Lederer,1978).
The non-linguistic segmentation using fixedword length windows also performs well, especiallyfor the longer length windows.
However, longerwindows (win15) increase the latency and any fixedlength window typically destroys the semantic con-text.
It can also be seen from Table 3 that translat-ing the complete talk is suboptimal in comparisonwith segmenting the text.
This is primarily due tobias on sentence length distributions in the trainingdata.
Training models on complete talks is likely toresolve this issue.
Contrasting the use of referencesegments as input to MT (ref-sent, ref-punct, conj-ref-eos) versus the use of predicted segments (pred-sent, pred-punct, conj-pred-eos, respectively), it isinteresting to note that the MT accuracies never dif-fered greatly between the two, despite the noise inthe set of predicted segments.The performance of the real-time speech transla-tion of TED talks is much lower than the offline sce-nario.
First, we use only a VTLN model as perform-ing CMA adaptation in a real-time scenario typicallyincreases latency.
Second, the ASR language modelis trained on sentence-like units and decoding the en-tire talk with this LM is not optimal.
A languagemodel trained on complete talks will be more appro-priate for such a framework and we are investigatingthis as part of current work.Comparing the accuracies of different speechtranslation strategies, Table 3 shows that pred-punctperforms the best.
When embedded in an incremen-tal MT speech recognition system, Figure 1 showsthat it is more accurate than the system that sendspartial ASR hypotheses directly to MT.
This advan-tage decreases, however, when the ASR timeout pa-rameter is increased to more than five or six sec-236onds.
In terms of latency, Figure 1 shows that theaddition of the pred-punct segmenter into the incre-mental system introduces a significant delay.
Aboutone third of the increase in delay can be attributedto merely maintaining the two word lookahead win-dow that the segmenter?s classifier needs to makedecisions.
This is significant because this kind ofwindow has been used quite frequently in previouswork on simultaneous translation (cf.
(Fu?gen et al2007)), and yet to our knowledge this penalty asso-ciated with this configuration was never mentioned.The remaining delay can be attributed to the longchunk sizes that the segmenter produces.
An inter-esting aspect of the latency curve associated with thesegmenter in Figure 1 is that there are two peaks atASR timeouts of 2,500 and 4,500 ms, and that thelowest latency is achieved at 3,000 ms rather than ata smaller value.
This may be attributed to the factthat the system is a pipeline consisting of ASR, seg-menter, and MT, and that 3,000 ms is roughly thelength of time to recite comma-separated chunks.Consequently, the two latency peaks appear to cor-respond with ASR producing segments that are mostdivergent with segments that the segmenter pro-duces, leading to the most pipeline ?stalls.?
Con-versely, the lowest latency occurs when the timeoutis set so that ASR?s segments most resemble the seg-menter?s output to MT.8 ConclusionWe investigated various approaches for incremen-tal speech translation of TED talks, with the aimof producing a system with high MT accuracy andlow latency.
For acoustic modeling, we found thatVTLN and CMA adaptation were useful for increas-ing the accuracy of ASR, leading to a word accuracyof 80% on TED talks used in the IWSLT evalua-tion track.
In our offline MT experiments retentionof partial translations was found useful for increas-ing MT accuracy, with the latter being slightly morehelpful.
We experimented with several linguisticand non-linguistic strategies for text segmentationbefore translation.
Our experiments indicate that anovel segmentation into conjunction-separated sen-tence chunks resulted in accuracies almost as highand latencies almost as short as comma-separatedsentence chunks.
They also indicated that signifi-cant noise in the detection of sentences and punc-tuation did not seriously impact the resulting MTaccuracy.
Experiments on real-time simultaneousspeech translation using partial recognition hypothe-ses demonstrate that introduction of a segmenter in-creases MT accuracy.
They also showed that in or-der to reduce latency it is important for buffers in dif-ferent pipeline components to be synchronized so asto minimize pipeline stalls.
As part of future work,we plan to extend the framework presented in thiswork for performing speech-to-speech translation.We also plan to address the challenges involved inS2S translation across languages with very differentword order.AcknowledgmentsWe would like to thank Simon Byers for his helpwith organizing the TED talks data.ReferencesS.
Bangalore, V. K. Rangarajan Sridhar, P. Kolan,L.
Golipour, and A. Jimenez.
2012.
Real-time in-cremental speech-to-speech translation of dialogs.
InProceedings of NAACL:HLT, June.M.
Cettolo and M. Federico.
2006.
Text segmentationcriteria for statistical machine translation.
In Proceed-ings of the 5th international conference on Advancesin Natural Language Processing.M.
Cettolo, C. Girardi, and M. Federico.
2012.
WIT3:Web Inventory of Transcribed and Translated Talks.
InProceedings of EAMT.J.
Cohen.
2007.
The GALE project: A description andan update.
In Proceedings of ASRU Workshop.M.
Federico, L. Bentivogli, M. Paul, and S. Stu?ker.
2011.Overview of the IWSLT 2011 evaluation campaign.
InProceedings of IWSLT.C.
Fu?gen and M. Kolss.
2007.
The influence of utterancechunking on machine translation performance.
In Pro-ceedings of Interspeech.C.
Fu?gen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker,S.
Vogel, and A. Waibel.
2006.
Open domain speechrecognition & translation: Lectures and speeches.
InProceedings of ICASSP.C.
Fu?gen, A. Waibel, and M. Kolss.
2007.
Simultaneoustranslation of lectures and speeches.
Machine Trans-lation, 21:209?252.V.
Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tu?r,A.
Ljolje, and S. Parthasarathy.
2004.
The AT&TWatson Speech Recognizer.
Technical report, Septem-ber.237P.
Haffner, G. Tu?r, and J. Wright.
2003.
Optimizingsvms for complex call classification.
In Proceedingsof ICASSP?03.O.
Hamon, C. Fu?gen, D. Mostefa, V. Arranz, M. Kolss,A.
Waibel, and K. Choukri.
2009.
End-to-end evalua-tion in simultaneous translation.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), March.B.
Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-KwangKuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin.2011.
The IBM 2009 GALE Arabic speech translationsystem.
In Proceedings of ICASSP.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, Shen W.,C.
Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT Summit.M.
Lederer.
1978.
Simultaneous interpretation: units ofmeaning and other features.
In D. Gerver and H. W.Sinaiko, editors, Language interpretation and commu-nication, pages 323?332.
Plenum Press, New York.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn treebank.
Computational Linguistics,19(2):313?330.E.
Matusov, G. Leusch, O. Bender, and H. Ney.
2005.Evaluating machine translation output with automaticsentence segmentation.
In Proceedings of IWSLT.E.
Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-Tu?r, M. Ostendorf, and H. Ney.
2007.
Improvingspeech translation with automatic boundary predic-tion.
In Proceedings of Interspeech.M.
Mohri, F. Pereira, and M. Riley.
1997.
At&tgeneral-purpose finite-state machine software tools,http://www.research.att.com/sw/tools/fsm/.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.V.
K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.2011.
A scalable approach to building a parallel cor-pus from the Web.
In Proceedings of Interspeech.S.
Rao, I.
Lane, and T. Schultz.
2007.
Optimizing sen-tence segmentation for spoken language translation.
InProceedings of Interspeech.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of the Interna-tional Conference on New Methods in Language Pro-cessing.R.
Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-javec, and D. Tufis.
2006.
The JRC-Acquis: A multi-lingual aligned parallel corpus with 20+ languages.
InProceedings of LREC.J.
Tiedemann and L. Lars Nygaard.
2004.
The OPUScorpus - parallel & free.
In Proceedings of LREC.D.
Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.2005.
Statistical machine translation of European par-liamentary speeches.
In Proceedings of MT Summit.W.
Wahlster, editor.
2000.
Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer.238
