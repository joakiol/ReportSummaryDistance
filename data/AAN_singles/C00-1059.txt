Corpus-dependent Association Thesauri for Information RetrievalHiroyuki Kaji "~, Yasutsugu Morimoto "~, Toshiko Aizono "l, and Noriyuki Yamasaki "2"~ Central Research Laboratory, Hitachi, Ltd. ,2 Software Division, Hitachi, Ltd.1-280 Higashi-Koigakubo, Kokubunji-shi 549-6 Shinano-cho, Totsuka-kt,, Yokohanla-shiTokyo 185-8601, Japan Kanagawa 244-0801, Japan{kaji, morimoto, aizono}@crl.hitachi.co.jp, yamasa n@soft.hitachi.co.jpAbstractThis paper presents a method for automati-cally generating an association thesaurusfrom a text corpus, and demonstrates it ap-plication to information retrieval.
The the-saurus generation method .consists of ex-tracting tenns and co-occurrence data from acorpus and analyzing the correlation betweenterms statistically.
A new method for dis-ambiguating the structure of compoundnouns, which is a key component for termextraction, is also proposed.
The automati-cally generated thesaurus i  effectively usedas a tool for exploring infonnation.
A the-saurus navigator having novel functions uchas term clustering, thesaurus overview, andzooming-in is proposed.1 IntroductionA thesaurus plays essential roles in informationretrieval systems.
In particular, a domain-specific thesaurus greatly improves the effective-ness of information retrieval.
However, we areconfronted with the difficult problem of how toconstruct and maintain a domain-specific thesau-rus.
The goal of our present research is to es-tablish a method for autolnatically generating athesaurus from a text corpus of a domain anddemonstrate its application to information re-trieval.Thesauri are classifiedinto taxonomy-type thesauriand association thesauri.There has been variousresearch on the extraction oftaxonomic information |'io111a corpus, including extrac-tion of hyponyms by usinglinguistic patterns (Hearst1992) and extraction of synonyms based on thesimilarity of sets of co-occurring words (Ruge1991; Grefenstette 1992).
However, the perfor-mance of these methods is limited, and theyshould be considered as aids to augment hand-made thesauri.
In contrast, an association the-saurus, that is a collection of pairs of semanti-cally associated terms, can be possibly generatedfrom a corpus entirely automatically.
Wordassociation orms based on co-occurrence infor-mation have been proposed by (Church andHanks 1990).
Here we focus on the automaticgeneration of an association thesanrus.Association thesauri are as useful as taxon?omy-type thesauri in information retrieval.
Theimprovement of retrieval effectiveness by usingan association thesaurus has been reported by anumber of papers (Jing and Croft 1994; Schutzeand Pedersen 1994).
We propose to use a coropus-dependent association thesaurus interac-tively.2 Automat ic  Generat ion of an Associa-tion Thesaurus  f rom a Corpus2.1 Outline of the thesaurus generationmethodThe proposed thcsaurus generation method con-sists of term extraction, co-occurrence data ex-traction, and correlation analysis, as shown in Fig.1.~?
Term FU Co-occ.rrence\[ data extraction r l pmrs and their223___C o r r e l a t !
m ~Fig.
1.
Automatic generation of a thesaurus from a corpus.4042.1.1 Ternt extractionA thesaurus hould consist of terms, each repre-senting a domain-specific concept.
Most of tileterms representing important concepts are nol, lllS,simple or compound, that frequently occur in thecorpus.
ThereR)re, we extract both simplenonns and compound nouns whose occurrencefrequencies exceed a predetermined threshold.We also use a list of stop words since frequentlyoccurring nouns are not always terms.Compound nouns are identified by a patternmatching method using a part-of-speech sequen-ce pattern.
Naturally, the pattern is languagespecific.
The following is a pattern for Japanesecompound nouns:COMP NOUN := { PREFIX } NOUN +SUFFIX } { PREFIX } NOUN +A problem in extracting compound nouns isthat a word sequence matched to the above pat-tern, which actually defines just the type of nounphrase, is not always a term.
We filter out somekind of non-term noun phrases by using a list ofstop words for the first and last elements of com-pound nouns.
Stop words for the first elementof compound nouns include referential nouns (e.g.jouki (above-mentioned)) and determiner nouns(e.g.
kaku (each)).
Stop words for the last ele-ment of compound nouns include time/placenouns (e.g.
nai (inside)) and relational nouns (e.g.koyuu (peculiar)).Another importmat problem we are confrontedwith in term extraction is the structural ambiguityof compound nouns.
For our purpose, we needto extract non-maximal compound nouns as wellsts lnaxin la l  comp()und nouns.
Here a non-maximal compound noun means one that occursas a part o f  a larger con lpound noun, and an lax imal  compound nonn means  one thstt occursnot as a part of a larger compound noun.
Wemust disambiguate tile structure of compoundnouns to correctly extract non-maximal com-pound nouns.
We have developed a statisticaldisambiguation method, the detail and evaluationof which are described in 2.2.2.1.2 Co-occurrence data evtractionOur purpose is to collect pairs of semantically orcontextually associated terms, no matter whatkind of association.
So we extract co-occurrence in a window.
That is, every pair ofterms occurring together within a window isextracted as the window is moved through a text.The window size can be specified rather arbitrar-ily.
Considering our purpose, the windowshould accommodate a few sentences.
At tilesame time, the window size should not be toolarge from the viewpoint of computational load.Therefore, 20 to 50 words, excluding functionwords,  seems to be an appropriate value.Note that we filter out any pair of words co-occurring within a compound noun.
If suchpairs were included in co-occurrence data, theywould show high correlation.
However, theywould be redundant because compound nouns aretreated as entities in our thesaurus.2.1.3 Correlation analysisAs a correlation measure between terms, we usemutual information (Church and Hanks 1990).The mutual inlbrmation between terms t~ and t i isdefined by the following formula:g(ti, tj)/~' g(t,, t,)Ml(ti, tj) = log_, /i.i ,{t'(t0/ i ~ f(t0}' { f(tj)//j~ f(ti,where f(t~) is the occurrence frequency of term t~,and g(ti,ti ) is the co-occurrence frequency ofterms t~ and tj.
A rnaxinmm nunrber of associat-.ed terms for each term is predetermined as wellas a threshold for tile mutual information, andassociated terms are selected based on the de-oscending order of mutual information.Mutual infornaation involves a problem inthat it is overestimated for low-frequency terms(I)unning 1993).
Therefore, we determinewhether two terms are related to each other by alog-likelihood ratio test, and we filter out pairs ofterms that do not pass the test.2.2 Disambiguation of compound nounstructure2.2.1 Disantbiguation based on coitus statisticsOur disanabiguation method is described belowfor tile case of a compound noun consisting ofthree elelnents.
A compound noun W~W2W 3has two possible structures: WI(W~W3) and(W~W,)W 3.
We deternfine its structure basedon the occurrence t}equencies of maxilnal com-pound nouns as follows: If tile maximal com-pound noun W~W3 occurs more frequently thantile inaxinml compound noun W~W,, then tile405Table 1(a) ExamplesGlobal-statistics-based disambiguation vs. local-statistics-based disambiguation.Maximal compound nounDeta shori shisutemu(data processing system)Deta tsuskin seiL:vo souchi(Data communicationcontroller)Kaisen seigyo purosessa(Line control processor)F req .
Global-statistics-basedStructure Freq.478 (Deta shori) shisutemu 47894 Deta (tsuskin seigvo 94souchi)54 Kaisen (seign'o 54purosessa)Local-statistics-basedStructure Freq.
(Deta shori) shisutemu 368Deta (skori shisutemu) 110(Deta tsushin) seigg'o souchi 40Deta (tsuskin seig:vo souchi) 54(Kaisen seign.,o) purosessa 54(b) Summary of disambiguation resultsGlobal-statistics-basedCorrect structure 12,565 words (62.0%)Incorrect structure 7,688 words (38.0%)Total 20,253 words (100%)(Note: Numbers of words are occurrence-based.
)structure Wl(W2W3) is preferred.
On the con?trary, if the maximal COlnpound noun W~W2occurs more frequently than the maximal com-pound noun W2W3, then the structure (W~W2) W3is preferred.The generalized isambiguation rnle is asfollows: If a compound noun CN includes twocompound noun candidates CN~ and CN2, whichare incompatible with each other, and the maxi-mal compound noun CN~ occurs more frequentlythan the maximal compound noun CN> then astructure of CN including CN~ is preferred to astructure of CN including CN,.We have two alternatives regarding the rangewhere we count occurrence frequencies of maxi-mal compound nouns.
One is global-statisticswhich means that frequencies are counted in thewhole corpus and they are used to disambiguateall compound nouns in the corpus.
The other islocal-statistics which means that frequencies arecounted in each document in the corpus and theyare used to disambiguate compound nouns in thecorresponding document.2.2.2 Evaluation" Global-statistics vs. local-statisticsWe evaluated both the global-statistics-baseddisambiguation and the local-statistics-baseddisambiguation by using a 23.7-M Byte corpusconsisting of 800 patent documents.
Table l(a)shows comparative xamples of these methods.Evah, ation results for the 200 highest-frequencymaximal COlnpound nouns consisting of three orLocal-statistics-basedmore  words are summa ~rized in Table l(b).14,921 words (73.7%) They prove that thelocal-statistics-based 5,332 words (26.3%)disambiguation method20,253 words (100%) is superior to the global-statistics-based disam-biguation method.Note that in the local-statistics-based disam-biguation method, we resorted to the global-statistics when local-statistics were not available.The percentage of cases the local-statistics werenot available was 25.1 percent.
(Kobayasi et al 1994) proposed a disam-biguation method using collocation informationand semantic categories, and reported that thestructure of compound nouns was disambiguatedat 83% accuracy.
Note that their accuracy wascalculated for compound nouns including unam-biguous compound nouns, i.e.
those consisting ofonly two words.
If it were calculated for com-.pound nouns consisting three or more words, itwould be less than that of our method.
Thus, wecan conclude that our local-statistics-basedmethod compares quite well with rather sophisti-cated previous methods.2.3 Prototype and an experimentWe implemented a prototype thesaurus generatorin which the local-statistics-based method wasused to disambiguate the structure of compoundnouns.
Using this thesaurus generator, we got athesaurus consisting of 38,995 terms froln a 61-M Byte corpus consisting of almost 48,000 arti-cles in the financial pages of a Japanese newspa-per.
In this experiment, the threshold for occur-fence frequencies of terms in the term extractionstep was set to 10, and the window size in the co-occurrence data extraction step was set to 25.406The abow+" rtm took 5.4 hours on a HP9000C200 workstation.
The tlarouglaput is tolerablefrom a practical point of view.
We should alsonote that a thesaurus can be updated as efficientlyas it can be initially generated.
Because \ve canrun the first two steps (extraction of terms andextraction of co-occurrence data) in accumulativefashion, and we only need to run the third stepover again when a considerable amount of termsand co-occurrence data are accunmlated.3 Navigation in an Association Thesau-FUS3.1 Purpose and outline of the proposedthesaurus navigatorA big problem with toclafs information re-trieval systems based on search techniques i thatthey require users, who may not know exactlywhat thcy arc looking for, to explicitly describetheir information needs.
Another problem isthat mismatched vocabularies between users andthe corpus would bring poor retrieval results.To solve these problems, we propose a corptts-~dependent association-thesaurus navigator en-abling users to efficiently explore informationthrough a corpus.Users' requirements are summarized as foblows:They want to grasp the overall informationstructure of a domain.They want to know what topics or sub?.domains arc contained in the corpus.- They want to know terms that appropriatelydescribe their w~gue information eeds.To meet the above requirements, our pro,-posed thesaurus navigator has novel functionssuch as clustering of related terms, generation ofa thesaurus overview, and zoom-in on a sub--domain of interest.
A conceptual image ofthesaurus navigation using these ftmctions isshown in Fig.
2.
A typical informatio,>exploration session proceeds as follows.At the beginning, the system displays anoverview of a corpus-dependent thesaurus o thatusers can easily enter the information space ofthe corpus.
The overview is a kind of summaryof the corpus, it consists of clusters of genericterms of the domain, and makes it easy to under-stand what topics or sub-domains are containedin the corpus.
Looking at the thesaurus over-Thesaurttsoverview' .
.
.
.
.
.O Generic termZoom-in Zoom-in.
.
.
.//?
Specific termFig.
2.
Conceptual image of thesaurusnavigation.view, the users can select one or a few termclusters they have interest in, and the screen willzoom in on the cluster(s).
The zoomed viewconsists of a nulnber of clusters, each includingmore specific terms than those in the overview.Users can repeat his zoom-in operation until theyreach term clusters representing sufficientlyspecific topics.3.2 Functions of the thesaurus navigator3.2.1 Clustering of  related termsWe made a preliminary experiment o evaluatestandard agglomerative clustering algorithmsincluding the single-linkage method, the con>pletedinkage method, and the group-average-linkage method (Eldqamdouchi and Willett 1989)~Among them, the group--average-linkage m thodresulted in the best results, ttowever, severalpotential clusters tended to merge into a large onewhen we repeated the merge operation until apredetermined number of clusters were obtained.Accordingly, we use the group-average-linkagemethod with an upper limit on the size of a clus-ter.3.2.2 Generation era thesaurus overviewOur method tot generating a thesaurus overviewconsists of major-term extraction and term clus-tering.
The m~0or-term extracting algorithm,which is carried out beforehand in batch mode, isdescribed below.
See 3.2.1 for the term clus-tering algorithnl.An overview of the thesaurus hould consistof generic terms included in the corpus, flow-ever, we do not have a definite criterion for get>eric terms.
So we collect m~oor terms from thecorpus as follows.
The number of m~!jor terms,407denoted by M below, was set to 300 in the pro-totype.i) Determine a characteristic term set for eachdoculnent.Calculate the weight w~j of term tj fordocument 4 according to the tf-idf (term fre-quency - inverse document frequency) for-mula.
Then select the first re(i) terms in thedescending order of u, u for each document d,,where re(i), the number of characteristic termsfor document 4, is set to 20% of the totalnumber of distinct erms in 4.
It is also lim-ited to between 5 and 50.ii) Select major terms in the corpus.Select the first M terms in the descendingorder of the frequency of being contained inthe characteristic term sets?3.2.3 Zoom-in on a term cluster of  interestOur method for zooming in on a term clusterconsists of term-set expansion and term cluster~ing.
The term-set expanding algorithm is de~scribed below.
See 3.2.1 for the term clusteringalgorithm.A user-specified term set To = {t~, 6. .
.
.
.
t,,,} isexpanded into a term set T,.
consisting of M termsas follows.
M was set to 300 in the prototype.i) Set the initial value of 7",.
to 7",,.ii) Whi le  IT,.I< M for i = 1, 2 .
.
.
.
do;While IE, I < Mfor j  = 1,2, ..., m do;Add the tenn having the i-th highestcorrelation with tj to T,,;end;end;The reason why the above-described proce-dure implements the zoom-in is that genericterms tend to have higher correlation with semi-generic terms than with specific terms.
As-suming that high-frequency terms are generic andlow-frequency terms are specific, we examinedthe distribution of terms by the distance from themajor terms and the average occurrence frequen-cy of terms for each distance.
Here the distanceis the length of the shortest path in a graph that isobtained by connecting every pair of associatedterms with an edge.
Table 2 shows the resultsfor the example thesaurus mentioned in 2.3.According to it, the average occurrence frequen-cy decreases with the distance from the majorterms.
Therefore, starting from an overview,our method is likely to produce more and morespecific views.3.3 Prototype and an exper imentWe developed a prototype as a client/server sys-tem.
The thesaurus navigator is available onWWW browsers.
It also has an interface totext-retrieval engines, through which a termcluster is transferred as a query.Test use was made with the example thesau-rus mentioned in 2.3.
The response time for thezoom-in operation during the navigation sessionswas about 8 seconds.
This is acceptable giventhe rich information provided by the clusteredview.
Note that the response time is almostindependent of the size of the thesaurus or corpus,because the number of temls to be clustered isalways constant, as described in 3.2.2 and 3.2.3~An example from navigation sessions isshown in Fig.
3.
It demonstrates the useflflnessof the corpus-dependent thesaurus navigation as afront-end for text retrieval.
The effectiveness ofour thesaurus navigator is summarized as fololows.- Improved accessibility to text retrieval systems:Users are not necessarily required to inputterms to describe their information need.They need only select from among terms pre~sented on the screen.
This makes text re-.trieval systems accessible ven for those hav-ing vague information eeds, or those unfa-miliar with the domain.- Improved navigation efficiency:The unit of users' cognition is a topic ratherthan a term.
That is, they can recognize atopic from a cluster of terms at a glance.Therefi)re, they can efficiently navigatethrough an information space.Table 2 Distribution of terms by distance from major terms.Distance flom major terms 0 1 2Number of terms 245 4278 23832Average occurrence frequency 2642.1 318.6 61.93 4 510408 149 l 8210.6 7.7 9.0 -408I :~#-k ,  99 Y,# -"'" " '=' / I \ [ 'C~?
:::Z ::::::::::::::::::::::::::: : /  ' : " : '(a) Thesaurus overview1"~ 9~_/.x 9D 27,9 -"2'~I~\]J!c?x 72-'_ fAY \ ] "Z : -E .~ : iLtF'kM?ZJ?,ff ,'ib'..
9,ft.)
v'~t/t!.
7,5_Ir a~,:?
: :,5 ,&5;: i!17.ii8 :~!~.
I~f i  ~< ~qT.
>g .
'JJ/g i~;?
E '~3:i~ / ' l ' .
, ' i tR ;C,'i:: i" "_Z tA :ki~':\[ ' :  ~LL.rJ.=\] 2>EU i::,.
;~:'\]:::~: ;?~-:,':::,: i~L i~.
.+ <?..
~ /: .:.
r- ~;" +.
't :f" Z ?0  ~t ?
?~le'~ ' ' ~ I ' *~a ,-m/I':" ~ "~I o .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: ' " "~7:  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(b) Zoom-inF ~-)Ea?
':~A rivaL752 ?~ o E o ~  ~ aLW.eEtlffa ~ Y_22_rD j tm~~f~ 7ai~.NL~ ;r_u'2-?.,- in 97,_J(c) Further zoom-inAn overview of the thesaurus was di.sTJko,ed.Then the user selected the.fi/ih and seventhcluste#w hich he was interested in: {China,col!\[L'rence, Asia, cooperation, meeting,Vietnam, region, development, technology,environment}, and/economy export, aid,toward, summit, Soviet Union, debt, Russia,reconstruction}.
This means that the userwas interested in "development assistanceto developing countries or areas ".The Jil?h aml seventh cluste~w J)'om (a) wereshown close up, and clustelw indicatingmore .Vwc~/ic domains were presented.The user couM undelwtand which topics therespective clustelw suggested: "Economicassis'tance for the development ofthe Asia-Pat(lie region ", "Global environmentalproblems ", "bTternational debt problems ","'Mattetw on China ", "Energy resourcedevelopment", and so on.
Since he waxe.vwcially interested in "International debtproblems ", he selected the third cluster{debt, Egypt, Paris Club, creditor nation,q\[licial debt, de/'erred, Poland, .fin'eign debt,reimbursement, Paris, club, pro,merit,.fi, reign}.The third cluster fi'om (I 0 was shown closeup.
The resulting screen gave the user achoice el'many sT~ecific terms relevant o"htternational debt problems ", althoughnot all oJ'the clustel:v indicated spec!/ictopics.
The user was able to retrievedocuments by simply selectin?
terms o/interest./i'om those displayed on the screen.Fig.
3.
Example of thesaurus navigation.4094 Comparison with related workLet us make a briefcolnparison with related work.Both scatter/gather document clustering (Cuttinget al 1992; Hearst and Pedersen 1996) and Ko-honen's self-organizing map (Lin et al 1991;Lagus et al 1996; Kohonen 1998) enable explo-ration through a corpus.
While they treat acorpus as a collection of documents, we treat it asa collection of terms.
Therefore our method canelicit finer information structure than these previ-ous methods, and moreover, it can be applied to acorpus that includes multi-topic doculnents.Our method compares quite well with the previ-otis methods for throughput and response time.5 ConclusionWe demonstrated the feasibility of automaticgeneration of an association thesaurus from acorpus.
The proposed thesaurus generationmethod consists of extracting terms and co-occurrence data from a corpus and analyzing thecorrelation between terms statistically.
As acomponent technology for thesaurus generation,a method for disambiguating the structure ofcompound nouns based on corpus statistics wasdeveloped and evaluated.We also demonstrated the information re-trieval application of an automatically generatedassociation thesaurus.
A thesaurus navigatorhaving novel functions such as term clustering,thesaurus overview, and zooming-in was devel-oped.
An experiment with an association the-saurus generated from a newspaper article corpusproved that the thesaurus navigator allows us toefficiently explore information through a textcorpus even when our information needs arevague.Acknowledgements: This research was st, p-ported in part by the Next-generation DigitalLibrary System R&D Project of MITI (Ministryof International Trade and Industry), IPA (Inlbr-mation-technology Promotion Agency), andJIPDEC (Japan Information Processing Devel-opment Center).
We thank Mainichi Newspa-pers, Ltd. for permitting us to use the CD-ROMsof the '91, '92, '93, '94 and '95 Mainichi Shim-bun tbr the experiment.ReferencesChurch, K. W., and P. ttanks.
1990.
Word associationnorms, mutual information, and lexicography.
Con>putational Linguistics, 16( 1 ): 22-29.Cutting, D. R., D. R. Karger, J. O. Pedersen, and J. W.Tukey.
1992.
Scatter/gather: A cluster-based ap-proach to browsing large doctnnent collections.
Proc.ACM SIG1R '92, pp.
318-329.Dunning, T. 1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19( 1 ): 61-74.El-Hamdouchi, A., and P. Willett.
1989.
Comparisonof hierarchical agglomerative clustering methods fordocument retrieval.
The Computer Journal, 32(3):220-227.Grefenstette, G. 1992.
Use of syntactic context toproduce term association lists for text retrieval.
Proc.ACM SIGIR '92, pp.
89-97.Hearst, M. A.
1992.
Automatic acquisition ofhypo-nyms from large text corpora.
Proc.
COLING '92,pp.
539-545.Hearst, M. A., and J. O. Pedersen.
1996.
Reexaminingthe cluster hypothesis: Scatter/gather on retrievalresults.
Proc.
ACM S1GIR '96, pp.
76-84.Jing, Y., and W. B. Croft.
1994.. An association the-.saurus for information retrieval.
Proc.
R1AO '94,Cont.
on Intelligent Text and linage Handling, pp.146-160.Kobayasi, Y., T. Tokunaga, and tf.
Tanaka.
1994.Analysis of Japanese compound nouns using colloocational information.
Proc.
COLING '94, pp.
865-869.Kohonen, T. 1998.
Self-organization of very largedocument collections: State of the art.
Proc.
8th lnt'lCone on Artificial Neural Networks, vol.
1, pp.
65-74.Lagus, K., T. Honkela, S. Kaski, and T. Kohonen.1996.
Self organizing maps of document collections:a new approach to interactive xploration.
Proc.
2ndlnt'l Cone on Knowledge Discovery and Data Min-ing, pp.
238-243.Lin, X., D. Soergel, and G. Marchionini.
1991.
A self-organizing semantic map for information retrieval.Proc.
ACM SIGIR '91, pp.
262-269.Ruge, G. 1991.
Experiments on linguistically basedterm associations.
Proc.
RIAO '91, Conf.
on Intelli-gent Text and hnage Handling, pp.
528-545.Schutze, tt., and J. O. Pedersen.
1994.
A cooccur-rence-based thesaurus and two applications to in-formation retriewd.
Proc.
RIAO '94, Conf.
on Intel-ligent Text and hnage ttandling, pp.
266-274.410
