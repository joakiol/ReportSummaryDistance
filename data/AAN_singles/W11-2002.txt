Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsSpoken Dialog Challenge 2010:Comparison of Live and Control Test ResultsAlan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  OliverLemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, JasonD.
Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi11Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK3Engineering Department, Cambridge University, Cambridge, UK4AT&T Labs ?
Research, Florham Park, NJ, USAawb@cs.cmu.eduAbstractThe Spoken Dialog Challenge 2010 was anexercise to investigate how different spo-ken dialog systems perform on the sametask.
The existing Let?s Go Pittsburgh BusInformation System was used as a task andfour teams provided systems that were firsttested in controlled conditions with speechresearchers as users.
The three most stablesystems were then deployed to real callers.This paper presents the results of the livetests, and compares them with the controltest results.
Results show considerablevariation both between systems and be-tween the control and live tests.
Interest-ingly, relatively high task completion forcontrolled tests did not always predictrelatively high task completion for livetests.
Moreover, even though the systemswere quite different in their designs, wesaw very similar correlations between worderror rate and task completion for all thesystems.
The dialog data collected isavailable to the research community.1 BackgroundThe goal of the Spoken Dialog Challenge (SDC) isto investigate how different dialog systems per-form on a similar task.
It is designed as a regularlyrecurring challenge.
The first one took place in2010.
SDC participants were to provide one ormore of three things: a system; a simulated user,and/or an evaluation metric.
The task chosen forthe first SDC was one that already had a largenumber of real callers.
This had several advan-tages.
First, there was a system that had been usedby many callers.
Second, there was a substantialdataset that participants could use to train their sys-tems.
Finally, there were real callers, rather thanonly lab testers.
Past work has found systemswhich appear to perform well in lab tests do notalways perform well when deployed to real callers,in part because real callers behave differently thanlab testers, and usage conditions can be considera-bly different [Raux et al2005, Ai et al2008].
De-ploying systems to real users is an important traitof the Spoken Dialog Challenge.The CMU Let?s Go Bus Information system[Raux et al2006] provides bus schedule informa-tion for the general population of Pittsburgh.
It isdirectly connected to the local Port Authority,whose evening calls for bus information are redi-rected to the automated system.
The system hasbeen running since March 2005 and has servedover 130K calls.The software and the previous years of dialogdata were released to participants of the challengeto allow them to construct their own systems.
Anumber of sites started the challenge, and four sitessuccessfully built systems, including the originalCMU system.An important aspect of the challenge is thatthe quality of service to the end users (people inPittsburgh) had to be maintained and thus an initialrobustness and quality test was carried out on con-tributed systems.
This control test provided sce-narios over a web interface and requiredresearchers from the participating sites to call eachof the systems.
The results of this control test werepublished in [Black et al 2010] and by the individ-ual participants [Williams et al 2010, Thomson etal.
2010, Hastie et al 2010] and they are repro-2duced below to give the reader a comparison withthe later live tests.Important distinctions between the controltest callers and the live test callers were that thecontrol test callers were primarily spoken dialogresearchers from around the world.
Although theywere usually calling from more controlled acousticconditions, most were not knowledgeable aboutPittsburgh geography.As mentioned above, four systems took partin the SDC.
Following the practice of other chal-lenges, we will not explicitly identify the siteswhere these systems were developed.
We simplyrefer to them as SYS1-4 in the results.
We will,however, state that one of the systems is the systemthat has been running for this task for severalyears.
The architectures of the systems cover anumber of different techniques for building spokendialog systems, including agenda based systems,VoiceXML and statistical techniques.2 Conditions of Control and Live testsFor this task, the caller needs to provide the depar-ture stop, the arrival stop and the time of departureor arrival in order for the system to be able to per-form a lookup in the schedule database.
The routenumber can also be provided and used in thelookup, but it is not necessary.
The present livesystem covers the East End of Pittsburgh.
Al-though the Port Authority message states that otherareas are not covered, callers may still ask forroutes that are not in the East End; in this case, thelive system must say it doesn?t have informationavailable.
Some events that affect the length of thedialog include whether the system uses implicit orexplicit confirmation or some combination of both,whether the system has an open-ended first turn ora directed one, and whether it deals with requestsfor the previous and/or following bus (this lattershould have been present in all of the systems).Just before the SDC started, the Port Author-ity had removed some of its bus routes.
The sys-tems were required to be capable of informing thecaller that the route had been canceled, and thengiving them a suitable alternative.SDC systems answer live calls when the PortAuthority call center is closed in the evening andearly morning.
There are quite different types andvolumes of calls over the different days of theweek.
Weekend days typically have more calls, inpart because the call center is open fewer hours onweekends.
Figure 1 shows a histogram of averagecalls per hour for the evening and the early morn-ing of each day of the week.calls per weekday / ave per hour012345678910Fr-19-0Sa-0-8Sa-16-0Su-0-8Su-16-0Mo-0-7Mo-19-0Tu-0-7Tu-19-0We-0-7We-19-0Th-0-7Th-19-0Fr-0-7Figure 1: average number of calls per hour on weekends(dark bars) and weekdays.
Listed are names of days andtimes before and after midnight when callers called thesystem.The control tests were set up through a simpleweb interface that presented 8 different scenariosto callers.
Callers were given a phone number tocall; each caller spoke to each of the 4 differentsystems twice.
A typical scenario was presentedwith few words, mainly relying on graphics in or-der to avoid influencing the caller?s choice of vo-cabulary.
An example is shown in Figure 2.Figure 2: Typical scenario for the control tests.
Thisexample requests that the user find a bus from the cor-ner of Forbes and Morewood (near CMU) to the airport,using bus route 28X, arriving by 10:45 AM.33 Control Test ResultsThe logs from the four systems were labeled fortask success by hand.
A call is successful if any ofthe following outputs are correctly issued:?
Bus schedule for the requested departure andarrival stops for the stated bus number (if giv-en).?
A statement that there is no bus available forthat route.?
A statement that there is no scheduled bus atthat time.We additionally allowed the following boundarycases:?
A departure/arrival stop within 15 minuteswalk.?
Departure/arrival times within one hour of re-quested time.?
An alternate bus number that serves the re-quested route.In the control tests, SYS2 had system connectionissues that caused a number of calls to fail to con-nect, as well as a poorer task completion.
It wasnot included in the live tests.
It should be pointedout that SYS2 was developed by a single graduatestudent as a class project while the other systemswere developed by teams of researchers.
The re-sults of the Control Tests are shown in Table 1 andare discussed further below.Table 1.
Results of hand analysis of the four systems inthe control testThe three major classes of system responseare as follows.
no_info: this occurs when the sys-tem gives neither a specific time nor a valid excuse(bus not covered, or none at that time).
no_infocalls can be treated as errors (even though theremaybe be valid reasons such as the caller hangs upbecause the bus they are waiting for arrives).donthave: identifies calls that state the requestedbus is not covered by the system or that there is nobus at the requested time.
pos_out: identifies callswhere a specific time schedule is given.
Bothdonthave and pos_out calls may be correct or er-roneous (e.g the given information is not for therequested bus,  the departure stop is wrong, etc).4 Live Tests ResultsIn the live tests the actual Pittsburgh callers hadaccess to three systems: SYS1, SYS3, and SYS4.Although engineering issues may not always beseen to be as relevant as scientific results, it is im-portant to acknowledge several issues that had tobe overcome in order to run the live tests.Since the Pittsburgh Bus Information Systemis a real system, it is regularly updated with newschedules from the Port Authority.
This happensabout every three months and sometimes includeschanges in bus routes as well as times and stops.The SDC participants were given these updatesand were allowed the time to make the changes totheir systems.
Making things more difficult is thefact that the Port Authority often only releases theschedules a few days ahead of the change.
Anotherconcern was that the live tests be run within oneschedule period so that the change in schedulewould not affect the results.The second engineering issue concernedtelephony connectivity.
There had to be a way totransfer calls from the Port Authority to the par-ticipating systems (that were run at the participat-ing sites, not at CMU) without slowing down orperturbing service to the callers.
This wasachieved by an elaborate set of call-forwardingmechanisms that performed very reliably.
How-ever, since one system was in Europe, connectionsto it were sometimes not as reliable as to the US-based systems.SYS1 SYS3 SYS4Total Calls 678 451 742Non-empty calls 633 430 670no_ info 18.5% 14.0% 11.0%donthave 26.4% 30.0% 17.6%donthave_corr 47.3% 40.3% 37.3%donthave_incorr 52.7% 59.7% 62.7%pos_out 55.1% 56.0% 71.3%pos_out_corr 86.8% 93.8% 91.6%pos_out_incorr 13.2% 6.2% 8.4%Table 2.
Results of hand analysis of the three systems inthe live tests.
Row labels are the same as in Table 1.SYS1 SYS2 SYS3 SYS4Total Calls 91 61 75 83no_ info 3.3% 37.7% 1.3% 9.6%donthave 17.6% 24.6% 14.7% 9.6%donthave_corr 68.8% 33.3% 100.0% 100.0%donthave_incorr 31.3% 66.7% 0.0% 0.0%pos_out 79.1% 37.7% 84.0% 80.7%pos_out_corr 66.7% 78.3% 88.9% 80.6%pos_out_incorr 33.3% 21.7% 11.1% 19.4%4We ran each of the three systems for multiple twoday periods over July and August 2010.
This de-sign gave each system an equal distribution ofweekdays and weekends, and also ensured thatrepeat-callers within the same day experienced thesame system.One of the participating systems (SYS4)could support simultaneous calls, but the other twocould not and the caller would receive a busy sig-nal if the system was already in use.
This, how-ever, did not happen very often.Results of hand analysis of real calls areshown in Table 4 alongside the results for the Con-trol Test for easy comparison.
In the live tests wehad an additional category of call types ?
emptycalls (0-turn calls) ?
which are calls where thereare no user turns, for example because the callerhung up or was disconnected before saying any-thing.
Each system had 14 days of calls and exter-nal daily factors may change the number of calls.We do suspect that telephony issues may have pre-vented some calls from getting through to SYS3 onsome occasions.Table 3 provides call duration information foreach of the systems in both the control and livetests.Length (s) Turns/call Words/turnSYS1 control 155 18.29 2.87 (2.84)SYS1 live 111 16.24 2.15 (1.03)SYS2 control 147 17.57 1.63 (1.62)SYS3 control 96 10.28 2.73 (1.94)SYS3 live 80 9.56 2.22 (1.14)SYS4 control 154 14.70 2.25 (1.78)SYS4 live 126 11.00 1.63 (0.77)Table 3: For live tests, average length of each call, aver-age number of turns per call, and average number ofwords per turn (numbers in brackets are standard devia-tions).Each of the systems used a different speechrecognizer.
In order to understand the impact ofword error rate on the results, all the data werehand transcribed to provide orthographic transcrip-tions of each user turn.
Summary word error sta-tistics are shown in Table 4.
However, summarystatistics do not show the correlation between worderror rate and dialogue success.
To achieve this,following Thomson et al(2010), we computed alogistic regression of success against word errorrate (WER) for each of the systems.
Figure 3shows the regressions for the Control Tests andFigure 4 for the Live Tests.SYS1 SYS3 SYS4Control 38.4 27.9 27.5Live 43.8 42.5 35.7Table 4: Average dialogue word error rate (WER).0 20 40 60 80 1000.00.20.40.60.81.0WERSuccessRateSys4Sys3Sys1Figure 3: Logistic regression of control test success vsWER for the three fully tested systems0 20 40 60 80 1000.00.20.40.60.81.0WERSuccessSys1Sys3Sys4Figure 4: Logistic regression of live success vs WER forthe three fully tested systems5In order to compare the control and live tests,we can calculate task completion as the percentageof calls that gave a correct result.
We include onlynon-empty calls (excluding 0-turn calls), and treatall no_info calls as being incorrect, even thoughsome may be due to extraneous reasons such as thebus turning up (Table 5).SYS1 SYS3 SYS4Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%)Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%)Table 5: Live and control test task completion (std.
err).5 DiscussionAll systems had lower WER and higher task com-pletion in the controlled test vs. the live test.
Thisagrees with past work [Raux et al2005, Ai et al2008], and underscores the challenges of deployingreal-world systems.For all systems, dialogs with controlled sub-jects were longer than with live callers ?
both interms of length and number of turns.
In addition,for all systems, live callers used shorter utterancesthan controlled subjects.
Controlled subjects maybe more patient than live callers, or perhaps livecallers were more likely to abandon calls in theface of higher recognition error rates.Some interesting differences between the sys-tems are evident in the live tests.
Looking at dia-log durations, SYS3 used confirmations least often,and yielded the fastest dialogs (80s/call).
SYS1made extensive use of confirmations, yielding themost turns of any system and slightly longer dia-logs (111s/call).
SYS4 was the most system-directed, always collecting information one ele-ment at a time.
As a result it was the slowest of thesystems (126s/call), but because it often used im-plicit confirmation instead of explicit confirmation,it had fewer turns/call than SYS1.For task completion, SYS3 performed best inthe controlled trials, with SYS1 worst and SYS4 inbetween.
However in the live test, SYS4 per-formed best, with SYS3 and SYS1 similar andworse.
It was surprising that task completion forSYS3 was the highest for the controlled tests yetamong the lowest for the live tests.
Investigatingthis, we found that much of the variability in taskcompletion for the live tests appears to be due toWER.
In the control tests SYS3 and SYS4 hadsimilar error rates but the success rate of SYS3 washigher.
The regression in Figure 3 shows thisclearly.
In the live tests SYS3 had a significantlyhigher word error rate and average success ratewas much lower than in SYS4.It is interesting to speculate on why the rec-ognition rates for SYS3 and SYS4 were differentin the live tests, but were comparable in the controltests.
In a spoken dialogue system the architecturehas a considerable impact on the measured worderror rate.
Not only will the language model anduse of dialogue context be different, but the dia-logue design and form of system prompts will in-fluence the form and content of user inputs.
Thus,word error rates do not just depend on the qualityof the acoustic models ?
they depend on the wholesystem design.
As noted above, SYS4 was moresystem-directed than SYS3 and this probably con-tributed to the comparatively better ASR perform-ance with live users.
In the control tests, thebehavior of users (research lab workers) may havebeen less dependent on the manner in which userswere prompted for information by the system.Overall, of course, it is user satisfaction and tasksuccess which matter.6 Corpus Availability and EvaluationThe SDC2010 database of all logs from all systemsincluding audio plus hand transcribed utterances,and hand defined success values is releasedthrough CMU?s Dialog Research Center(http://dialrc.org).One of the core goals of the Spoken DialogChallenge is to not only create an opportunity forresearchers to test their systems on a common plat-form with real users, but also create common datasets for testing evaluation metrics.
Although somework has been done on this for the control test data(e.g.
[Zhu et al2010]), we expect further evalua-tion techniques will be applied to these data.One particular issue which arose during thisevaluation concerned the difficulty of defining pre-cisely what constitutes task success.
A precise de-finition is important to developers, especially ifreinforcement style learning is being used to opti-mize the success.
In an information seeking taskof the type described here, task success is straight-forward when the user?s requirements can be satis-fied but more difficult if some form of constraintrelaxation is required.
For example, if the user6asks if there is a bus from the current location tothe airport ?
the answer ?No.?
may be strictly cor-rect but not necessarily helpful.
Should this dia-logue be scored as successful or not?
The answer?No, but there is a stop two blocks away whereyou can take the number 28X bus direct to the air-port.?
is clearly more useful to the user.
Shouldsuccess therefore be a numeric measure rather thana binary decision?
And if a measure, how can it beprecisely defined?
A second and related issue isthe need for evaluation algorithms which deter-mine task success automatically.
Without these,system optimization will remain an art rather thana science.7 ConclusionsThis paper has described the first attempt at an ex-ercise to investigate how different spoken dialogsystems perform on the same task.
The existingLet?s Go Pittsburgh Bus Information System wasused as a task and four teams provided systemsthat were first tested in controlled conditions withspeech researchers as users.
The three most stablesystems were then deployed ?live?
with real call-ers.
Results show considerable variation both be-tween systems and between the control and livetests.
Interestingly, relatively high task completionfor controlled tests did not always predict rela-tively high task completion for live tests.
Thisconfirms the importance of testing on live callers,not just usability subjects.The general organization and frameworkof the evaluation worked well.
The ability to routeaudio telephone calls to anywhere in the world us-ing voice over IP protocols was critical to the suc-cess of the challenge since it provides a way forindividual research labs to test their in-house sys-tems without the need to port them to a central co-ordinating site.Finally, the critical role of precise evalua-tion metrics was noted and the need for automatictools to compute them.
Developers need these atan early stage in the cycle to ensure that when sys-tems are subsequently evaluated, the results andsystem behaviors can be properly compared.AcknowledgmentsThanks to AT&T Research for providing telephonysupport for transporting telephone calls during thelive tests.
This work was in part supported by theUS National Science foundation under the project?Dialogue Research Center?.ReferencesAi, H., Raux, A., Bohus, D., Eskenzai, M., and Litman,D.
(2008)  ?Comparing spoken dialog corpora col-lected with recruited subjects versus real users?, ProcSIGDial, Columbus, Ohio, USA.Black, A., Burger, S., Langner, B., Parent, G., and Es-kenazi, M. (2010) ?Spoken Dialog Challenge 2010?,SLT 2010, Berkeley, CA.Hastie, H., Merigaud, N., Liu, X and Oliver Lemon.
(2010) ?
?Let?s Go Dude?, Using The Spoken Dia-logue Challenge to Teach Spoken Dialogue Devel-opment?, SLT 2010, Berkeley, CA.Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi,M.
(2005)  ?Let?s go public!
Taking a spoken dialogsystem to the real world?, Interspeech 2005, Lisbon,Portugal.Raux, A., Bohus, D., Langner, B., Black, A., and Eske-nazi, M. (2006) ?Doing Research on a DeployedSpoken Dialogue System: One Year of Let's Go!
Ex-perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-tem for the Let?s Go Spoken Dialogue Challenge?,SLT 2010, Berkeley, CA.Williams, J., Arizmendi, I., and Conkie, A.
?Demonstra-tion of AT&T ?Let?s Go?
: A Production-Grade Statis-tical Spoken Dialog System.?
SLT 2010, Berkeley,CA.Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., andKing, I.
(2010) ?Using Finite State Machines forEvaluating Spoken Dialog Systems?, SLT 2010,Berkeley, CA.7
