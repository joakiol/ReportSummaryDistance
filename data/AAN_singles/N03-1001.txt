Effective Utterance Classification with Unsupervised Phonotactic ModelsHiyan AlshawiAT&T Labs - ResearchFlorham Park, NJ 07932, USAhiyan@research.att.comAbstractThis paper describes a method for utteranceclassification that does not require manualtranscription of training data.
The methodcombines domain independent acoustic modelswith off-the-shelf classifiers to give utteranceclassification performance that is surprisinglyclose to what can be achieved using conven-tional word-trigram recognition requiring man-ual transcription.
In our method, unsupervisedtraining is first used to train a phone n-grammodel for a particular domain; the output ofrecognition with this model is then passed toa phone-string classifier.
The classification ac-curacy of the method is evaluated on three dif-ferent spoken language system domains.1 IntroductionA major bottleneck in building data-driven speech pro-cessing applications is the need to manually transcribetraining utterances into words.
The resulting corpus oftranscribed word strings is then used to train application-specific language models for speech recognition, and insome cases also to train the natural language componentsof the application.
Some of these speech processing ap-plications make use of utterance classification, for exam-ple when assigning a call destination to naturally spokenuser utterances (Gorin et al, 1997; Carpenter and Chu-Carroll, 1998), or as an initial step in converting speechto actions in spoken interfaces (Alshawi and Douglas,2001).In this paper we present an approach to utterance clas-sification that avoids the manual effort of transcribingtraining utterances into word strings.
Instead, only thedesired utterance class needs to be associated with eachsample utterance.
The method combines automatic train-ing of application-specific phonotactic models togetherwith token sequence classifiers.
The accuracy of thisphone-string utterance classification method turns out tobe surprisingly close to what can be achieved by conven-tional methods involving word-trigram language mod-els that require manual transcription.
To quantify this,we present empirical accuracy results from three differ-ent call-routing applications comparing our method withconventional utterance classification using word-trigramrecognition.Previous work at AT&T on utterance classificationwithout words used information theoretic metrics to dis-cover ?acoustic morphemes?
from untranscribed utter-ances paired with routing destinations (Gorin et al, 1999;Levit et al, 2001; Petrovska-Delacretaz et al, 2000).However, that approach has so far proved impractical:the major obstacle to practical utility was the low run-time detection rate of acoustic morphemes discoveredduring training.
This led to a high false rejection rate (be-tween 40% and 50% for 1-best recognition output) whena word-based classification algorithm (the one describedby Wright et.
al (1997)) was applied to the detected se-quence of acoustic morphemes.More generally, previous work using phone string (orphone-lattice) recognition has concentrated on tasks in-volving retrieval of audio or video (Jones et al, 1996;Foote et al, 1997; Ng and Zue, 1998; Choi et al, 1999).In those tasks, performance of phone-based systems wasnot comparable to the accuracy obtainable from word-based systems, but rather the rationale was avoiding thedifficulty of building wide coverage statistical languagemodels for handling the wide range of subject matter thata typical retrieval system, such as a system for retrievingnews clips, needs to cover.
In the work presented here, thetask is somewhat different: the system can automaticallylearn to identify and act on relatively short phone subse-quences that are specific to the speech in a limited domainof discourse, resulting in task accuracy that is comparableto word-based methods.Edmonton, May-June 2003Main Papers , pp.
1-7Proceedings of HLT-NAACL 2003In section 2 we describe the utterance classificationmethod.
Section 3 describes the experimental setup andthe data sets used in the experiments.
Section 4 presentsthe main comparison of the performance of the methodagainst a ?conventional?
approach using manual tran-scription and word-based models.
Section 5 gives someconcluding remarks.2 Utterance Classification Method2.1 Runtime OperationThe runtime operation of our utterance classificationmethod is simple.
It involves applying two models(which are trained as described in the next subsection): Astatistical n-gram phonotactic model and a phone stringclassification model.
At runtime, the phonotactic modelis used by an automatic speech recognition system to con-vert a new input utterance into a phone string which ismapped to an output class by applying the classificationmodel.
(We will often refer to an output class as an ?ac-tion?, for example transfer to a specific call-routing des-tination).
The configuration at runtime is as shown inFigure 1.
More details about the specific recognizer andclassifier components used in our experiments are givenin the Section 3.NewutterancePhonen -gramrecognizerPredictedaction,ConfidencescoreClassifierruntimepredictionPhonestringPromptPhonestringclassificationmodelPhonen -grammodel NmaxFigure 1: Utterance classifier runtime operationThe classifier can optionally make use of more infor-mation about the context of an utterance to improve theaccuracy of mapping to actions.
As noted in Figure 1,in the experiments presented here, we use a single addi-tional feature as a proxy for the utterance context, specif-ically, the identity of the spoken prompt that elicited theutterance.
It should be noted, however, that inclusion ofsuch additional information is not central to the method:Whether, and how much, context information to includeto improve classification accuracy will depend on the ap-plication.
Other candidate aspects of context may includethe dialog state, the day of week, the role of the speaker,and so on.2.2 Training ProcedureTraining is divided into two phases.
First, train a phonen-gram model using only the training utterance speechfiles and a domain-independent acoustic model.
Second,train a classification model mapping phone strings andprompts (the classifier inputs) to actions (the classifieroutputs).The recognition training phase is an iterative proce-dure in which a phone n-gram model is refined succes-sively: The phone strings resulting from the current passover the speech files are used to construct the phone n-gram model for the next iteration.
In other words, thisis a ?Viterbi re-estimation?
or ?1-best re-estimation?
pro-cess.
We currently only re-estimate the n-gram model, sothe same general-purpose HMM acoustic model is usedfor ASR decoding in all iterations.
Other more expen-sive n-gram re-estimation methods can be used instead,including ones in which successive n-gram models arere-estimated from n-best or lattice ASR output.
Candi-dates for the initial model used in this procedure are anunweighted phone loop or a general purpose phonotacticmodel for the language being recognized.The steps of the training process are as follows.
(Theprocedure is depicted in Figure 2.
)TrainingutteranceaudiofilesPhone-looprecognizer Phonestrings0Phonen -grammodel1Actions PhonestringclassificationmodelsequenceclassifiertrainingalgorithmPhonen -gramrecognizer PhonestringsNmaxPhonen -grammodel Nmax?
?PromptsFigure 2: Utterance classifier training procedure1.
Set the phone string model G to an initial phonestring model.
Initialize the n-gram order N to 1.
(Here ?order?
means the size of the n-grams, so forexample 2 means bi-grams.)2.
Set S to the set of phone strings resulting from rec-ognizing the training speech files with G (after pos-sibly adjusting the insertion penalty, as explainedbelow).3.
Estimate an n-gram model G?
of order N from theset of strings S.4.
If N < Nmax, set N ?
N + 1 and G?
G?
and goto step 2, otherwise continue with step 5.5.
For each recognized string s ?
S, construct a clas-sifier input pair (s, r) where r is the prompt thatelicited the utterance recognized as s.6.
Train a classification model M to generalize thetraining function f : (s, r) ?
a, where a is theaction associated with the utterance recognized as s.7.
Return the classifier model M and the final n-grammodel G?
as the results of the training procedure.Instead of increasing the order N of the phone n-grammodel during re-estimation, an alternative would be toiterate Nmax times with a fixed n-gram order, possiblywith successively increased weight being given to the lan-guage model vs. the acoustic model in ASR decoding.One issue that arises in the context of unsupervisedrecognition without transcription is how to adjust recog-nition parameters that affect the length of recognizedstrings.
In conventional training of recognizers fromword transcriptions, a ?word insertion penalty?
is typ-ically tuned after comparing recognizer output againsttranscriptions.
To address this issue, we estimate the ex-pected speaking rate (in phones per second) for the rele-vant type of speech (human-computer interaction in theseexperiments).
The token insertion penalty of the recog-nizer is then adjusted so that the speaking rate for auto-matically detected speech in a small sample of trainingdata approximates the expected speaking rate.3 Experimental Setup3.1 DataThree collections of utterances from different domainswere used in the experiments.
Domain A is the one stud-ied in previously cited experiments (Gorin et al, 1999;Levit et al, 2001; Petrovska-Delacretaz et al, 2000).
Ut-terances for domains B and C are from similar interactivespoken natural language systems.Domain A.
The utterances being classified are the cus-tomer side of live English conversations between AT&Tresidential customers and an automated customer caresystem.
This system is open to the public so the num-ber of speakers is large (several thousand).
There were40106 training utterances and 9724 test utterances.
Theaverage length of an utterance was 11.29 words.
The splitbetween training and test utterances was such that the ut-terances from a particular call were either all in the train-ing set or all in the test set.
There were 56 actions inthis domain.
Some utterances had more than one actionassociated with them, the average number of actions as-sociated with an utterance being 1.09.Domain B.
This is a database of utterances from an in-teractive spoken language application relating to productline information.
There were 10470 training utterancesand 5005 test utterances.
The average length of an utter-ance was 3.95 words.
There were 54 actions in this do-main.
Some utterances had more than one action associ-ated with them, the average number of actions associatedwith an utterance being 1.23.Domain C. This is a database of utterances from aninteractive spoken language application relating to con-sumer order transactions (reviewing order status, etc.)
ina limited domain.
There were 14355 training utterancesand 5000 test utterances.
The average length of an utter-ance was 8.88 words.
There were 93 actions in this do-main.
Some utterances had more than one action associ-ated with them, the average number of actions associatedwith an utterance being 1.07.3.2 RecognizerThe same acoustic model was used in all the experimentsreported here, i.e.
for experiments with both the phone-based and word-based utterance classifiers.
This modelhas 42 phones and uses discriminatively trained 3-stateHMMs with 10 Gaussians per state.
It uses feature spacetransformations to reduce the feature space to 60 fea-tures prior to discriminative maximum mutual informa-tion training.
This acoustic model was trained by AndrejLjolje and is similar to the baseline acoustic model usedfor experiments with the Switchboard corpus, an earlierversion of which is described by Ljolje et al (2000).
(Like the model used here, the baseline model in thoseexperiments does not involve speaker and environmentnormalizations.
)The n-gram phonotactic models used were representedas weighted finite state automata.
These automata (withthe exception of the initial unweighted phone loop) wereconstructed using the stochastic language modeling tech-nique described by Riccardi et al (1996).
This modelingtechnique, which includes a scheme for backing off toprobability estimates for shorter n-grams, was originallydesigned for language modeling at the word level.3.3 ClassifierDifferent possible classification algorithms can be used inour utterance classification method.
For the experimentsreported here we use the BoosTexter classifier (Schapireand Singer, 2000).
Among the alternatives are decisiontrees (Quinlan, 1993) and support vector machines (Vap-nik, 1995).
BoosTexter was originally designed for textcategorization.
It uses the AdaBoost algorithm (Freundand Schapire, 1997; Schapire, 1999), a wide margin ma-chine learning algorithm.
At training time, AdaBoostselects features from a specified space of possible fea-tures and associates weights with them.
A distinguishingcharacteristic of the AdaBoost algorithm is that it placesmore emphasis on training examples that are difficult toclassify.
The algorithm does this by iterating through anumber of rounds: at each round, it imposes a distribu-tion on the training data that gives more probability massto examples that were difficult to classify in the previ-ous round.
In our experiments, 500 rounds of boostingwere used; each round allows the selection of a new fea-ture and the adjustment of weights associated with exist-ing features.
In the experiments, the possible features areidentifiers corresponding to prompts, and phone n-gramsor word n-grams (for the phone and word-based methodsrespectively) up to length 4.3.4 Experimental ConditionsThree experimental conditions are considered.
The suf-fixes (M and H) in the condition names refer to whetherthe two training phases (i.e.
training for recognition andclassification respectively) use inputs produced by ma-chine (M) or human (H) processing.PhonesMM This experimental condition is the methoddescribed in this paper, so no human transcriptionsare used.
Unsupervised training from the trainingspeech files is used to build a phone recognitionmodel.
The classifier is trained on the phone stringsresulting from recognizing the training speech fileswith this model.
At runtime, the classifier is ap-plied to the results of recognizing the test files withthis model.
The initial recogition model for the un-supervised recognition training process was an un-weighted phone loop.
The final n-gram order usedin the recognition training procedure (Nmax in sec-tion 2) was 5.WordsHM Human transcriptions of the training speechfiles are used to build a word trigram model.
Theclassifier is trained on the word strings resultingfrom recognizing the training speech files with thisword trigram model.
At runtime, the classifier is ap-plied to the results of recognizing the test files withthe word trigram model.Learned phone Correspondingsequence wordsb ih l ih billingk ao l z callsn ah m b numberf aa n phoner ey t ratek ae n s cancelaa p ax r operatoraw t m ay what mych eh k checkm ay b my billp ae n ih companys w ih ch switcher n ae sh internationalv ax k w have a questionl ih ng p billing planr ey t s ratesk t uw p like to payae l ax n balancem er s er customer servicer jh f ao charge forTable 1: Example phone sequences learned by the train-ing procedure from domain A training speech files.WordsHH Human transcriptions of the training speechfiles are used to build a word trigram model.
Theclassifier is trained on the human transcriptions ofthe speech training files.
At runtime, the classifieris applied to the results of recognizing the test fileswith the word trigram model.For all three conditions, median recognition and classi-fication time for test data was less than real time (i.e.
theduration of test speech files) on current micro-processors.As noted earlier, the acoustic model, the number of boost-ing rounds, and the use of prompts as an additional clas-sification feature, are the same for all experimental con-ditions.3.5 Example learned phone sequencesTo give an impression of the kind of phone sequencesresulting from the automatic training procedure and ap-plied by the classifier at runtime, see Table 1.
The tablelists some examples of such phone strings learned fromdomain A training speech files, together with Englishwords, or parts of words (shown in bold type), they maycorrespond to.
(Of course, the words play no part in themethod and are only included for expository purposes.
)The phone strings are shown in the DARPA phone alpha-bet.Rejection PhoneMM WordHM WordHHrate (%) accuracy accuracy accuracy0 74.6 76.2 77.010 79.5 81.1 81.520 84.4 85.8 86.230 89.4 90.5 90.940 94.1 94.7 94.450 97.2 97.3 96.7Table 2: Phone-based and word-based utterance classifi-cation accuracy for domain A4 Classification AccuracyIn this section we compare the accuracy of our phone-string utterance classification method (PhonesMM) withmethods (WordsHM and WordsHH) using manual tran-scription and word string models.Accuracy MetricThe results are presented as utterance classification rates,specifically the percentage of utterances in the test set forwhich the predicted action is valid.
Here a valid predic-tion means that the predicted action is the same as one ofthe actions associated with the test utterance by a humanlabeler.
(As noted in section 3, the average number ofactions associated with an utterance was 1.09, 1.23, and1.07 for domains A, B, and C, respectively.)
In this met-ric we only take into account a single action predictedby the classifier, i.e.
this is ?rank 1?
classification ac-curacy, rather than the laxer ?rank 2?
classification ac-curacy (where the classifier is allowed to make two pre-dictions) reported by Gorin et.
al (1999) and Petrovskaet.
al (2000).In practical applications of utterance classification,user inputs are rejected if the confidence of the classifierin making a prediction falls below a threshold appropri-ate to the application.
After rejection, the system may,for example, route the call to a human or reprompt theuser.
We therefore show the accuracy of classifying ac-cepted utterances at different rejection rates, specifically0% (all utterances accepted), 10%, 20%, 30%, 40%, and50%.
Following Schapire and Singer (2000), the con-fidence level, for rejection purposes, assigned to a pre-diction is taken to be the difference between the scoresassigned by BoosTexter to the highest ranked action (thepredicted action) and the next highest ranked action.Accuracy ResultsUtterance classification accuracy rates, at various rejec-tion rates, for domain A are shown in Table 2 for thethree experimental conditions described in section 3.4.The corresponding results for domains B and C are shownin Tables 3 and 4.Rejection PhoneMM WordHM WordHHrate (%) accuracy accuracy accuracy0 80.8 81.6 81.010 86.0 86.7 85.320 90.0 90.6 89.530 93.9 93.7 92.340 96.3 96.8 94.750 97.5 97.7 96.4Table 3: Phone-based and word-based utterance classifi-cation accuracy for domain BRejection PhoneMM WordHM WordHHrate (%) accuracy accuracy accuracy0 68.2 68.9 69.910 73.3 73.7 74.920 78.9 79.2 80.230 84.8 84.7 85.540 89.7 89.3 90.250 94.1 93.3 94.5Table 4: Phone-based and word-based utterance classifi-cation accuracy for domain CThe utterances in domain A are on average longer andmore complex than in domain B; this may partly explainthe higher classification rates for domain B.
The gener-ally lower classification accuracy rates for domain C mayreflect the larger set of actions for this domain (92 ac-tions, compared with 56 and 54 actions for domains Aand B).
Another difference between the domains was thatthe recording quality for domain B was not as high asfor domains A and C. Despite these differences betweenthe domains, there is a consistent pattern for the compar-ison of most interest to this paper, i.e.
the relative per-formance of utterance classification methods requiring ornot requiring transcription.Perhaps the most surprising outcome of these ex-periments is that the phone-based method with short?phrasal?
contexts (up to four phones) has classifica-tion accuracy that is so close to that provided by thelonger phrasal contexts of trigram word recognition andword-string classification.
Of course, the re-estimationof phone n-grams employed in the phone-based methodmeans that two-word units are implicitly modeled sincethe phone 5-grams modeled in recognition, and 4-gramsin classification, can straddle word boundaries.The experiments suggest that if transcriptions areavailable (i.e.
the effort to produce them has alreadybeen expended), then they can be used to slightly improveperformance over the phone-based method (PhonesMM)not requiring transcriptions.
For domains A and C, thiswould give an absolute performance difference of about2%, while for domain B the difference is around 1%.Nmax Recog.
Classif.accuracy accuracy0 54.2 70.01 56.6 70.62 59.1 71.23 59.5 71.54 60.0 73.25 62.3 74.6Table 5: Phone recognition accuracy and phone stringclassification accuracy (PhoneMM with no rejection) forincreasing values of Nmax for domain A.Nmax Recog.
Classif.accuracy accuracy0 27.9 69.21 38.3 70.72 48.6 74.73 53.3 77.64 55.1 79.25 55.7 80.8Table 6: Phone recognition accuracy and phone stringclassification accuracy (PhoneMM with no rejection) forincreasing values of Nmax for domain B.Whether it is optimal to train the word-based classifier onthe transcriptions (WordsHH) or the output of the recog-nizer (WordsHM) seems to depend on the particular dataset.When the operational setting of utterance classifica-tion demands very high confidence, and a high degreeof rejection is acceptable (e.g.
if sufficient human backupoperators are available), then the small advantage of theword-based methods is reduced further to less than 1%.This can be seen from the high rejection rate rows of theaccuracy tables.Effectiveness of Unsupervised TrainingTables 5, 6, and 7, show the effect of increasing Nmax(the final iteration number in the unsupervised phonerecognition model) for domains A, B and C, respectively.The row with Nmax = 0 corresponds to the initial un-weighted phone loop recognition.
The classification ac-curacies shown in this table are all at 0% rejection.
Phonerecognition accuracy is the standard ASR error rate ac-curacy in terms of the percentage of phone insertions,deletions, and substitutions, determined by aligning theASR output against reference phone transcriptions pro-duced by the pronounciation component of our speechsynthesizer.
(Since these reference phone transcriptionsare not perfect, the actual phone recognition accuracy isprobably slightly higher.)
Clearly, for all three domains,unsupervised recognition model training improves bothNmax Recog.
Classif.accuracy accuracy0 55.4 61.11 59.8 61.82 65.3 64.33 68.1 66.34 69.1 67.45 69.3 68.2Table 7: Phone recognition accuracy and phone stringclassification accuracy (PhoneMM with no rejection) forincreasing values of Nmax for domain C.recognition and classification accuracy compared with asimple phone loop.
Unsupervised training of the recogni-tion model is particularly important for domain B wherethe quality of recordings is not as high as for domainsA and C, so the system needs to depend more on the re-estimated n-gram models to achieve the final classifica-tion accuracy.5 Concluding RemarksIn this paper we have presented an utterance classifica-tion method that does not require manual transcriptionof training data.
The method combines unsupervised re-estimation of phone n-ngram recognition models togetherwith a phone-string classifier.
The utterance classifica-tion accuracy of the method is surprisingly close to amore traditional method involving manual transcriptionof training utterances into word strings and recognitionwith word trigrams.
The measured absolute differencein classification accuracy (with no rejection) between ourmethod and the word-based method was only 1% for onetest domain and 2% for two other test domains.
The per-formance difference is even smaller (less than 1%) if highrejection thresholds are acceptable.
This performancelevel was achieved despite the large reduction in effortrequired to develop new applications with the presentedutterance classification method.ReferencesH.
Alshawi and S. Douglas.
2001.
Variant transduction:A method for rapid development of interactive spokeninterfaces.
In Proceedings of the SIGDial Workshop onDiscourse and Dialogue, Aalborg, Denmark, Septem-ber.R.
Carpenter and J. Chu-Carroll.
1998.
Natural languagecall routing: a robust, self-organizing approach.
InProceedings of the International Conference on Speechand Language Processing, Sydney, Australia.J.
Choi, D. Hindle, J. Hirschberg, F. Pereira, A. Singhal,and S. Whittaker.
1999.
Spoken content-based audionavigation (scan).
In Proceedings of ICPhS-99 (In-ternational Congress of Phonetics Sciences, San Fran-cisco, California, August.J.
T. Foote, S. J.
Young, G. J. F Jones, and K. SparckJones.
1997.
Unconstrained keyword spotting usingphone lattices with application to spoken document re-trieval.
Computer Speech and Language, 11(2):207?224.Y.
Freund and R. E. Schapire.
1997.
A decision-theoreticgeneralization of on-line learning and an application toboosting.
Journal of Computer and System Sciences,55(1):119?139.A.
L. Gorin, G. Riccardi, and J. H. Wright.
1997.How may I help you?
Speech Communication, 23(1-2):113?127.A.
L. Gorin, D. Petrovska-Delacretaz, G. Riccardi, andJ.
H. Wright.
1999.
Learning Spoken Language with-out Transcription.
In Proceedings of the ASRU Work-shop, Keystone, Colorado, December.K.
Sparck Jones, G. J. F. Jones, J. T. Foote, and S. J.Young.
1996.
Experiments in spoken documentretrieval.
Information Processing and Management,32(4):399?417.M.
Levit, A. L. Gorin, and J. H. Wright.
2001.
Mul-tipass Algorithm for Acquisition of Salient AcousticMorphemes.
In Proceedings of Eurospeech 2001, Aal-borg, Denmark, September.A.
Ljolje, D. M. Hindle, M. D. Riley, and R. W. Sproat.2000.
The AT&T LVCSR-2000 System.
In SpeechTranscription Workshop, Univ.
of Maryland, May.K.
Ng and V. Zue.
1998.
Phonetic recognition for spo-ken document retrieval.
In Proceedings of ICASSP 98,Seattle, Washington, May.D.
Petrovska-Delacretaz, A. L. Gorin, J. H. Wright, andG.
Riccardi.
2000.
Detecting Acoustic Morphemes inLattices for Spoken Language Understanding.
In Pro-ceedings of the Interanational Conference on SpokenLanguage Processing, Beijing, China, October.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.G.
Riccardi, R. Pieraccini, and E. Bocchieri.
1996.Stochastic automata for language modeling.
ComputerSpeech and Language, 10:265?293.R.
E. Schapire and Y.
Singer.
2000.
BoosTexter: Aboosting-based system for text categorization.
Ma-chine Learning, 39(2/3):135?168.R.
E. Schapire.
1999.
A brief introduction to boost-ing.
In Proceedings of the Sixteenth International JointConference on Artificial Intelligence.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer, New York.J.
H. Wright, A. L. Gorin, and G. Riccardi.
1997.
Au-tomatic acquisition of salient grammar fragments forcall-type classification.
In Proceedings of EuropeanConference on Speech Communication and Technol-ogy, pages 1419?1422, Rhodes, Greece, September.
