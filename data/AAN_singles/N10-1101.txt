Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689?692,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomatic Generation of Personalized Annotation Tags for Twitter UsersWei Wu, Bin Zhang, Mari OstendorfElectrical EngineeringUniversity of Washington, Seattle, WA{weiwu, binz, ostendor}@uw.eduAbstractThis paper introduces a system designed forautomatically generating personalized annota-tion tags to label Twitter user?s interests andconcerns.
We applied TFIDF ranking andTextRank to extract keywords from Twittermessages to tag the user.
The user tagging pre-cision we obtained is comparable to the preci-sion of keyword extraction fromweb pages forcontent-targeted advertising.1 IntroductionTwitter is a communication platform which com-bines SMS, instant messages and social networks.
Itenables users to share information with their friendsor the public by updating their Twitter messages.A large majority of the Twitter users are individ-ual subscribers, who use Twitter to share informa-tion on ?what am I doing?
or ?what?s happeningright now?.
Most of them update their Twitter mes-sages very frequently, in which case the Twitter mes-sages compose a detailed log of the user?s everydaylife.
These Twitter messages contain rich informa-tion about an individual user, including what s/he isinterested in and concerned about.
Identifying anindividual user?s interests and concerns can help po-tential commercial applications.
For instance, thisinformation can be employed to produce ?follow-ing?
suggestions, either a person who shares simi-lar interests (for expanding their social network) ora company providing products or services the user isinterested in (for personalized advertisement).In this work, we focus on automatically gener-ating personalized annotation tags to label Twitteruser?s interests and concerns.
We formulate thisproblem as a keyword extraction task, by selectingwords from each individual user?s Twitter messagesas his/her tags.
Due to the lack of human generatedannotations, we employ an unsupervised strategy.Specifically, we apply TFIDF ranking and TextRank(Mihalcea and Tarau, 2004) keyword extraction onTwitter messages after a series of text preprocess-ing steps.
Experiments on randomly selected usersshowed good results with TextRank, but high vari-ability among users.2 Related WorkResearch work related to Twitter message analysisincludes a user sentiment study (Jansen et al, 2009)and information retrieval indexing.
To our knowl-edge, no previously published research has yet ad-dressed problems on tagging user?s personal inter-ests from Twitter messages via keyword extraction,though several studies have looked at keyword ex-traction using other genres.For supervised keyword extraction, (Turney,2000; Turney, 2003; Hulth, 2003; Yih et al, 2006;Liu et al, 2008) employed TFIDF or its variantswith Part-of-Speech (POS), capitalization, phraseand sentence length, etc., as features to train key-word extraction models, and discriminative trainingis usually adopted.
Yih et al (2006) use logis-tic regression to extract keywords from web pagesfor content-targeted advertising, which has the mostsimilar application to our work.
However, due to thelack of human annotation on Twitter messages, wehave to adopt an unsupervised strategy.For unsupervised keyword extraction, TFIDFranking is a popular method, and its effective-ness has been shown in (Hulth, 2003; Yih et al,2006).
TextRank and its variants (Mihalcea and Ta-rau, 2004; Wan et al, 2007; Liu et al, 2009) aregraph-based text ranking models, which are derivedfrom Google?s PageRank algorithm (Brin and Page,1998).
It outperforms TFIDF ranking on traditionalkeyword extraction tasks.
However, previous workon both TFIDF ranking and TextRank has been donemainly on academic papers, spoken documents or689web pages, whose language style is more formal (or,less ?conversational?)
than that of Twitter messages.Twitter messages contain large amounts of ?noise?like emoticons, internet slang words, abbreviations,and misspelled words.
In addition, Twitter messagesare a casual log of a user?s everyday life, which oftenlacks of a coherent topic sequence compared to aca-demic papers and most spoken documents.
Hence,it remains to see whether TFIDF ranking and Tex-tRank are effective for identifying user?s interestsfrom Twitter messages.3 System ArchitectureFigure 1 shows the framework of our system fortagging Twitter user?s interests.
A preprocessingpipeline is designed to deal with various types of?noise?
in Twitter messages and produce candidatewords for user tags.
Then the TFIDF ranking or Tex-tRank algorithm is applied to select user tags fromthe candidate words.Removing replying messagesRemoving emoticonsSubstituting/removing internetslang words and abbreviationsPart-of-Speech tagging andfilteringTFIDF ranking / TextRankPersonalized annotation tags forthe Twitter userMessages fromone Twitter userPreprocessingStemming and stopword removingFigure 1: Framework of the personalized annotation taggeneration system for Twitter users3.1 PreprocessingIn addition to messages describing ?What am I do-ing?
or ?what?s happening right now?, Twitter usersalso write replying messages to comment on otherusers?
messages.
This kind of message generallycontains more information about the users they re-ply to than about themselves, and therefore they areremoved in the preprocessing pipeline.Emoticons frequently appear in Twitter messages.Although some of them help express user?s senti-ment on certain topics, they are not directly helpfulfor keyword analysis and may interfere with POStagging in the preprocessing pipeline.
Therefore, wedesigned a set of regular expressions to detect andremove them.Internet slang words and abbreviations are widelyused in Twitter messages.
Most of them are out-of-vocabulary words in the POS tagging model usedin the next step, and thus will deteriorate the POStagging accuracy.
Hence, we build a lookup tablebased on the list of abbreviations in the NoSlang on-line dictionary,1 which we divide by hand into threesets for different processing.
The first set includes422 content words and phrases, such as ?bff?
(bestfriend forever) and ?fone?
(phone), with valid can-didate words for user tags.
The second set includes67 abbreviations of function words that usually formgrammatical parts in a sentence, such as ?im?
(i?m),?abt?
(about).
Simply removing them will affect thePOS tagging.
Thus, the abbreviations in both thesesets are replaced with the corresponding completewords or phrases.
The third set includes 4576 phraseabbreviations that are usually separable parts of asentence that do not directly indicate discussion top-ics, such as ?lol?
(laugh out loud), ?clm?
(cool likeme), which are removed in this step.We apply the Stanford POS tagger (Toutanovaand Manning, 2000) on Twitter messages, and onlyselect nouns and adjectives as valid candidates foruser tags.
At the end of the preprocessing pipeline,the candidate words are processed with the rule-based Porter stemmer2 and stopwords are filtered us-ing a publicly available list.31www.noslang.com/dictionary2tartarus.org/ martin/PorterStemmer/3armandbrahaj.blog.al/2009/04/14/list-of-english-stop-words/6903.2 User Tag Extraction3.2.1 TFIDF rankingIn the TFIDF ranking algorithm, messages fromuser u are put together as one document.
The TFIDFvalue of word i from this user?s messages is com-puted astfidfi,u =ni,u?j nj,ulog(UUi)where ni,u is the count of word i in user u?s mes-sages, Ui is the number of users whose messagescontain word i, and U is the total number of users inthe Twitter corpus.
For each user, words with top NTFIDF values are selected as his/her tags.3.2.2 TextRankAccording to the TextRank algorithm (Mihalceaand Tarau, 2004), each candidate word is repre-sented by a vertex in the graph; edges are addedbetween two candidate words according to their co-occurrence.
In the context of user tag extraction, webuild a TextRank graph with undirected edges foreach Twitter user.
One edge is added between twocandidate words if they co-exist within at least onemessage; the edge weight is set to be the total countof within-message co-occurrence of the two wordsthroughout all messages of this user.Starting with an arbitrarily assigned value (e.g.1.0), the rank value R(Vi) of the candidate word atvertex Vi is updated iteratively according to the fol-lowing equation,R(Vi) = (1?
d)+ d?Vj?E(Vi)wji?Vk?E(Vj) wjkR(Vj)where wji is the weight of the edge that links Vjand Vi, E(Vi) is the set of vertices which Vi is con-nected to, and d is a damping factor that is usuallyset to 0.85 (Brin and Page, 1998).
The rank updateiteration continues until convergence.
The candidatewords are then sorted according to their rank values.Words with top-N rank values are selected as tagsfor this user.4 Experiment4.1 Experimental SetupWe employed the Twitter API to download Twittermessages.
A unigram English language model wasPrecision (%) TFIDF TextRanktop-1 59.6 67.3top-3 61.5 66.0top-5 61.2 63.0top-10 59.0 58.3Table 1: Tagging precision on all users in the test setused to filter out non-English users.
We obtainedmessages from 11,376 Twitter users, each of themhad 180 to 200 messages.
The word IDF for TFIDFranking was computed over these users.We adopted an evaluation measure similar to theone proposed in (Yih et al, 2006) for identifyingadvertising keywords on web pages, which empha-sizes precision.
We randomly selected 156 Twit-ter users to evaluate the top-N precision of TFIDFranking and TextRank.
After we obtained the top-N outputs from the system, three human evaluatorswere asked to judge whether the output tags from thetwo systems (unidentified) reflected the correspond-ing Twitter user?s interests or concerns according tothe full set of his/her messages.4 We adopted a con-servative standard in the evaluation: when a person?sname is extracted as a user tag, which is frequentamong Twitter users, we judge it as a correct tagonly when it is a name of a famous person (pop star,football player, etc).
The percentage of the correcttags among the top-N selected tags corresponds tothe top-N precision of the system.4.2 Experimental ResultsTable 1 gives the top-N precision for TFIDF andTextRank for different values of N, showing thatTextRank leads to higher precision for small N. Al-though Twitter messages are much ?noisier?
thanregular web pages, the top-N precision we obtainedfor Twitter user tagging is comparable to the webpage advertising keyword extraction result reportedin (Yih et al, 2006).Figure 2 shows an example of the candidate wordranking result of a Twitter user by TextRank (thefont size is set to be proportional to each word?sTextRank value).
By examining the Twitter mes-sages, we found that this user is an information tech-4The pairwise Kappa value for inter-evaluator agreementranged from 0.77-0.83, showing good agreement.691Figure 2: Example of a Twitter user?s word ranks (thefont size is proportional to each word?s TextRank value)Precision (%)top-N ?
>0.6 ?
?0.6 H>5.4 H?5.4top-1 71.6 60.7 78.5 50.8top-3 71.9 56.8 74.2 54.0top-5 69.3 53.1 69.2 53.7top-10 65.1 47.7 63.8 50.2Table 2: TextRank tagging precision on users with dif-ferent Top-10 TextRank value standard deviation (?)
anduser message text entropy (H).nology ?geek?, who is very interested in writing Ap-ple?s iPhone applications, and also a user of GoogleWave.
In this work, we use only isolated words asuser tags, however, ?google?, ?wave?, and ?palo?,?alto?
extracted in this example indicate that phraselevel tagging can bring us more information aboutthe user, which is typical of many users.Although most Twitter users express their inter-ests to some extent in their messages, there are someusers whose message content is not rich enough toextract reliable information.
We investigated twomeasures for identifying such users: standard devi-ation of the top-10 TextRank values and the user?smessage text entropy.
Table 2 shows a compari-son of tagging precision where the users are dividedinto two groups with a threshold on each of the twomeasures.
It is shown that users with larger Tex-tRank value standard deviation or message text en-tropy tend to have higher tagging precision, and themessage text entropy has better correlation with thetop-10 tagging precision than TextRank value stan-dard deviation (0.33 v.s.
0.20 absolute).5 SummaryIn this paper, we designed a system to automat-ically extract keywords from Twitter messages totag user interests and concerns.
We evaluated twotagging algorithms, finding that TextRank outper-formed TFIDF ranking, but both gave a tagging pre-cision that was comparable to that reported for webpage advertizing keyword extraction.
We noticedsubstantial variation in performance across users,with low entropy indicative of users with fewer key-words, and a need for extracting key phrases (in ad-dition to words).
Other follow-on work might con-sider temporal characteristics of messages in termsof the amount of data needed for reliable tags vs.their time-varying nature, as well as sentiment asso-ciated with the identified tags.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual web search engine.
Computer Networksand ISDN Systems, 30(1-7):107?117.A.
Hulth.
2003.
Improved automatic keyword extractiongiven more linguistic knowledge.
In Proc.
EMNLP,pages 216?223.B.
J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.2009.
Twitter power: Tweets as electronic word ofmouth.
Journal of the American Society for Informa-tion Science and Technology, 60(11):2169?2188.F.
Liu, F. Liu, and Y. Liu.
2008.
Automatic keywordextraction for the meeting corpus using supervised ap-proach and bigram expansion.
In Proc.
IEEE SLT,pages 181?184.F.
Liu, D. Pennell, F. Liu, and Y. Liu.
2009.
Unsuper-vised approaches for automatic keyword extraction us-ing meeting transcripts.
In Proc.
HLT/NAACL, pages620?628.R.
Mihalcea and P. Tarau.
2004.
Textrank: Bringingorder into texts.
In Proc.
EMNLP.K.
Toutanova and C. D. Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proc.
EMNLP, pages 63?77.P.
D. Turney.
2000.
Learning algorithms for keyphrase.Information Retrieval, 2(4):303?336.P.
D. Turney.
2003.
Coherent keyphrase extraction viaweb mining.
In Proc.
IJCAI, pages 434?439.X.
Wan, J. Yang, and J. Xiao.
2007.
Towards an iter-ative reinforcement approach for simultaneous docu-ment summarization and keyword extraction.
In Proc.ACL, pages 552?559.W.-T. Yih, J. Goodman, and V. R. Carvalho.
2006.
Find-ing advertising keywords on web pages.
In Proc.
15thInternational Conference on World Wide Web, pages213?222.692
