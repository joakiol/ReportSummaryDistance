Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320?330,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJoint Bilingual Sentiment Classification with Unlabeled Parallel CorporaBin Lu1,3*, Chenhao Tan2, Claire Cardie2 and Benjamin K. Tsou3,11 Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong2 Department of Computer Science, Cornell University, Ithaca, NY, USA3 Research Centre on Linguistics and Language Information Sciences,Hong Kong Institute of Education, Hong Konglubin2010@gmail.com, {chenhao, cardie}@cs.cornell.edu, btsou99@gmail.comAbstractMost previous work on multilingual sentimentanalysis has focused on methods to adaptsentiment resources from resource-richlanguages to resource-poor languages.
Wepresent a novel approach for joint bilingualsentiment classification at the sentence levelthat augments available labeled data in eachlanguage with unlabeled parallel data.
We relyon the intuition that the sentiment labels forparallel sentences should be similar and presenta model that jointly learns improved mono-lingual sentiment classifiers for each language.Experiments on multiple data sets show that theproposed approach (1) outperforms the mono-lingual baselines, significantly improving theaccuracy for both languages by 3.44%-8.12%;(2) outperforms two standard approaches forleveraging unlabeled data; and (3) produces(albeit smaller) performance gains whenemploying pseudo-parallel data from machinetranslation engines.1 IntroductionThe field of sentiment analysis has quicklyattracted the attention of researchers andpractitioners alike (e.g.
Pang et al, 2002; Turney,2002; Hu and Liu, 2004; Wiebe et al, 2005; Brecket al, 2007; Pang and Lee, 2008).
1 Indeed,sentiment analysis systems, which mine opinionsfrom textual sources (e.g.
news, blogs, andreviews), can be used in a wide variety of*The work was conducted when the first author was visitingCornell University.applications, including interpreting productreviews, opinion retrieval and political polling.Not surprisingly, most methods for sentimentclassification are supervised learning techniques,which require training data annotated with theappropriate sentiment labels (e.g.
document-levelor sentence-level positive vs. negative polarity).This data is difficult and costly to obtain, and mustbe acquired separately for each language underconsideration.Previous work in multilingual sentiment analysishas therefore focused on methods to adaptsentiment resources (e.g.
lexicons) from resource-rich languages (typically English) to otherlanguages, with the goal of transferring sentimentor subjectivity analysis capabilities from English toother languages (e.g.
Mihalcea et al (2007); Baneaet al (2008; 2010); Wan (2008; 2009);Prettenhofer and Stein (2010)).
In recent years,however, sentiment-labeled data is graduallybecoming available for languages other thanEnglish (e.g.
Seki et al (2007; 2008); Nakagawa etal.
(2010); Schulz et al (2010)).
In addition, thereis still much room for improvement in existingmonolingual (including English) sentimentclassifiers, especially at the sentence level (Pangand Lee, 2008).This paper tackles the task of bilingualsentiment analysis.
In contrast to previous work,we (1) assume that some amount of sentiment-labeled data is available for the language pairunder study, and (2) investigate methods tosimultaneously improve sentiment classificationfor both languages.
Given the labeled data in eachlanguage, we propose an approach that exploits anunlabeled parallel corpus with the following320intuition: two sentences or documents that areparallel (i.e.
translations of one another) shouldexhibit the same sentiment ?
their sentimentlabels (e.g.
polarity, subjectivity, intensity) shouldbe similar.
The proposed maximum entropy-basedEM approach jointly learns two monolingualsentiment classifiers by treating the sentimentlabels in the unlabeled parallel text as unobservedlatent variables, and maximizes the regularizedjoint likelihood of the language-specific labeleddata together with the inferred sentiment labels ofthe parallel text.
Although our approach should beapplicable at the document-level and for additionalsentiment tasks, we focus on sentence-levelpolarity classification in this work.We evaluate our approach for English andChinese on two dataset combinations (see Section4) and find that the proposed approach outperformsthe monolingual baselines (i.e.
maximum entropyand SVM classifiers) as well as two alternativemethods for leveraging unlabeled data(transductive SVMs (Joachims, 1999b) and co-training (Blum and Mitchell, 1998)).
Accuracy issignificantly improved for both languages, by3.44%-8.12%.
We furthermore find thatimprovements, albeit smaller, are obtained whenthe parallel data is replaced with a pseudo-parallel(i.e.
automatically translated) corpus.
To ourknowledge, this is the first multilingual sentimentanalysis study to focus on methods forsimultaneously improving sentiment classificationfor a pair of languages based on unlabeled datarather than resource adaptation from one languageto another.The rest of the paper is organized as follows.Section 2 introduces related work.
In Section 3, theproposed joint model is described.
Sections 4 and5, respectively, provide the experimental setup andresults; the conclusion (Section 6) follows.2 Related WorkMultilingual Sentiment Analysis.
There is agrowing body of work on multilingual sentimentanalysis.
Most approaches focus on resourceadaptation from one language (usually English) toother languages with few sentiment resources.Mihalcea et al (2007), for example, generatesubjectivity analysis resources in a new languagefrom English sentiment resources by leveraging abilingual dictionary or a parallel corpus.
Banea etal.
(2008; 2010) instead automatically translate theEnglish resources using automatic machinetranslation engines for subjectivity classification.Prettenhofer and Stein (2010) investigate cross-lingual sentiment classification from theperspective of domain adaptation based onstructural correspondence learning (Blitzer et al,2006).Approaches that do not explicitly involveresource adaptation include Wan (2009), whichuses co-training (Blum and Mitchell, 1998) withEnglish vs. Chinese features comprising the twoindependent ?views?
to exploit unlabeled Chinesedata and a labeled English corpus and therebyimproves Chinese sentiment classification.Another notable approach is the work of Boyd-Graber and Resnik (2010), which presents agenerative model --- supervised multilingual latentDirichlet alocation --- that jointly models topicsthat are consistent across languages, and employsthem to better predict sentiment ratings.Unlike the methods described above, we focuson simultaneously improving the performance ofsentiment classification in a pair of languages bydeveloping a model that relies on sentiment-labeled data in each language as well as unlabeledparallel text for the language pair.Semi-supervised Learning.
Another line ofrelated work is semi-supervised learning, whichcombines labeled and unlabeled data to improvethe performance of the task of interest (Zhu andGoldberg, 2009).
Among the popular semi-supervised methods (e.g.
EM on Na?ve Bayes(Nigam et al, 2000), co-training (Blum andMitchell, 1998), transductive SVMs (Joachims,1999b), and co-regularization (Sindhwani et al,2005; Amini et al, 2010)), our approach employsthe EM algorithm, extending it to the bilingualcase based on maximum entropy.
We compare toco-training and transductive SVMs in Section 5.Multilingual NLP for Other Tasks.
Finally,there exists related work using bilingual resourcesto help other NLP tasks, such as word sensedisambiguation (e.g.
Ido and Itai (1994)), parsing(e.g.
Burkett and Klein (2008); Zhao et al (2009);Burkett et al (2010)), information retrieval (Gao etal., 2009), named entity detection (Burkett et al,2010); topic extraction (e.g.
Zhang et al, 2010),text classification (e.g.
Amini et al, 2010), andhyponym-relation acquisition (e.g.
Oh et al, 2009).321In these cases, multilingual models increaseperformance because different languages containdifferent ambiguities and therefore presentcomplementary views on the shared underlyinglabels.
Our work shares a similar motivation.3 A Joint Model with Unlabeled ParallelTextWe propose a maximum entropy-based statisticalmodel.
Maximum entropy (MaxEnt) models1 havebeen widely used in many NLP tasks (Berger et al,1996; Ratnaparkhi, 1997; Smith, 2006).
Themodels assign the conditional probability of thelabel   given the observation   as follows:(1)where    is a real-valued vector of feature weightsand    is a feature function that maps pairs       toa nonnegative real-valued feature vector.
Eachfeature has an associated parameter,   , which iscalled its weight; and   is the correspondingnormalization factor.Maximum likelihood parameter estimation(training) for such a model, with a set of labeledexamples, amounts to solving thefollowing optimization problem:(2)3.1 Problem DefinitionGiven two languages    and   , suppose we havetwo distinct (i.e.
not parallel) sets of sentiment-labeled data,    and     written in    andrespectively.
In addition, we have unlabeled (w.r.t.sentiment) bilingual (in    and   ) parallel datathat are defined as follows.where               denotes the polarity ofthe  -th instance    (positive or negative);    andare respectively the numbers of labeled instancesin    and   ;andare parallel instances inand   , respectively (i.e.
they are supposed to be1They are sometimes referred to as log-linear models, but alsoknown as exponential models, generalized linear models, orlogistic regression.translations of one another), whose labelsandare unobserved, but according to the intuitionoutlined in Section 1, should be similar.Given the input data        and  , our task is tojointly learn two monolingual sentiment classifiers?
one for    and one for   .
With MaxEnt, welearn from the input data:whereandare the vectors of feature weightsfor    and   , respectively (for brevity we denotethem as    and    in the remaining sections).
In thisstudy, we focus on sentence-level sentimentclassification, i.e.
each    is a sentence, andandare parallel sentences.3.2 The Joint ModelGiven the problem definition above, we nowpresent a novel model to exploit thecorrespondence of parallel sentences in unlabeledbilingual text.
The model maximizes the followingjoint likelihood with respect to    and   :(3)where          denotes    or   ; the first term onthe right-hand side is the likelihood of labeled datafor both    and   ; and the second term is thelikelihood of the unlabeled parallel data  .If we assume that parallel sentences are perfecttranslations, the two sentences in each pair shouldhave the same polarity label, which gives us:(4)whereis the unobserved class label for the  -thinstance in the unlabeled data.
This probabilitydirectly models the sentiment label agreementbetweenand.However, there could be considerable noise inreal-world parallel data, i.e.
the sentence pairs maybe noisily parallel (or even comparable) instead offully parallel (Munteanu and Marcu, 2005).
In suchnoisy cases, the labels (positive or negative) couldbe different for the two monolingual sentences in asentence pair.
Although we do not know the exactprobability that a sentence pair exhibits the samelabel, we can approximate it using their translation322probabilities, which can be computed using wordalignment toolkits such as Giza++ (Och and Ney,2003) or the Berkeley word aligner (Liang et al,2006).
The intuition here is that if the translationprobability of two sentences is high, the probabilitythat they have the same sentiment label should behigh as well.
Therefore, by considering the noise inparallel data, we get:(5)where       is the translation probability of the  -thsentence pair in  ;2    is the opposite of   ; the firstterm models the probability thatandhavethe same label; and the second term models theprobability that they have different labels.By further considering the weight to ascribe tothe unlabeled data vs. the labeled data (and theweight for the L2-norm regularization), we get thefollowing regularized joint log likelihood to bemaximized:(6)where the first term on the right-hand side is thelog likelihood of the labeled data from both    andthe second is the log likelihood of theunlabeled parallel data  , multiplied by     , aconstant that controls the contribution of theunlabeled data; and      is a regularizationconstant that penalizes model complexity or largefeature weights.
When    is 0, the algorithmignores the unlabeled data and degenerates to twoMaxEnt models trained on only the labeled data.3.3 The EM Algorithm on MaxEntTo solve the optimization problem for the model,we need to jointly estimate the optimal parametersfor the two monolingual classifiers by finding:(7)This can be done with an EM algorithm, whosesteps are summarized in Algorithm 1.
First, theMaxEnt parameters,    and   , are estimated from2The probability should be rescaled within the range of [0, 1],where 0.5 means that we are completely unsure if thesentences are translations of each other or not, and only thosetranslation pairs with a probability larger than 0.5 aremeaningful for our purpose.just the labeled data.
Then, in the E-step, theclassifiers, based on current values of     and   ,compute          for each labeled example andassign probabilistically-weighted class labels toeach unlabeled example.
Next, in the M-step, theparameters,    and   , are updated using both theoriginal labeled data (   and   ) and the newlylabeled data  .
These last two steps are iterateduntil convergence or a predefined iteration limit  .Algorithm 1.
The MaxEnt-based EM Algorithm forMultilingual Sentiment ClassificationInput: Labeled data    andUnlabeled parallel dataOutput:Two monolingual MaxEnt classifiers withparametersand, respectively1.
Train two initial monolingual modelsTrain and initializeandon the labeled data2.
Jointly optimize two monolingual modelsfor     to   do // T: number of iterationsE-Step:Compute         for each example in    ,    andbased onand;Compute the expectation of the log likelihood withrespect to       ;M-Step:Findandby maximizing the regularizedjoint log likelihood;Convergence:If the increase of the joint log likelihood issufficiently small, break;end for3.
Outputass, andasIn the M-step, we can optimize the regularizedjoint log likelihood using any gradient-basedoptimization technique (Malouf, 2002).
Thegradient for Equation 3 based on Equation 4 isshown in Appendix A; those for Equations 5 and 6can be derived similarly.
In our experiments, weuse the L-BFGS algorithm (Liu et al, 1989) andrun EM until the change in regularized joint loglikelihood is less than 1e-5 or we reach 100iterations.33Since the EM-based algorithm may find a local maximum ofthe objective function, the initialization of the parameters isimportant.
Our experiments show that an effective maximumcan usually be found by initializing the parameters with thoselearned from the labeled data; performance would be muchworse if we initialize all the parameters to 0 or 1.3233.4 Pseudo-Parallel Labeled and UnlabeledDataWe also consider the case where a parallel corpusis not available: to obtain a pseudo-parallel corpus(i.e.
sentences in one language with theircorresponding automatic translations), we use anautomatic machine translation system (e.g.
Googlemachine translation 4 ) to translate unlabeled in-domain data from    to    or vice versa.Since previous work (Banea et al, 2008; 2010;Wan, 2009) has shown that it could be useful toautomatically translate the labeled data from thesource language into the target language, we canfurther incorporate such translated labeled data intothe joint model by adding the following componentinto Equation 6:(8)where    is the alternative class of  ,is theautomatically translated example from; andis a constant that controls the weight of thetranslated labeled data.4 Experimental Setup4.1 Data Sets and PreprocessingThe following labeled datasets are used in ourexperiments.MPQA (Labeled English Data): The Multi-Perspective Question Answering (MPQA) corpus(Wiebe et al, 2005) consists of newswiredocuments manually annotated with phrase-levelsubjectivity information.
We extract all sentencescontaining strong (i.e.
intensity is medium orhigher), sentiment-bearing (i.e.
polarity is positiveor negative) expressions following Choi andCardie (2008).
Sentences with both positive andnegative strong expressions are then discarded, andthe polarity of each remaining sentence is set tothat of its sentiment-bearing expression(s).NTCIR-EN (Labeled English Data) andNTCIR-CH (Labeled Chinese Data): TheNTCIR Opinion Analysis task (Seki et al, 2007;2008) provides sentiment-labeled news data inChinese, Japanese and English.
Only thosesentences with a polarity label (positive ornegative) agreed to by at least two annotators areextracted.
We use the Chinese data from NTCIR-64http://translate.google.com/as our Chinese labeled data.
Since far fewersentences in the English data pass the annotatoragreement filter, we combine the English data fromNTCIR-6 and NTCIR-7.
The Chinese sentencesare segmented using the Stanford Chinese wordsegmenter (Tseng et al, 2005).The number of sentences in each of thesedatasets is shown in Table 1.
In our experiments,we evaluate two settings of the data: (1)MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIR-CH.
In each setting, the English labeled dataconstitutes    and the Chinese labeled data,   .MPQA NTCIR-EN NTCIR-CHPositive 1,471 (30%) 528 (30%) 2,378 (55%)Negative 3,487 (70%) 1,209 (70%) 1,916 (45%)Total 4,958 1,737 4,294Table 1: Sentence Counts for the Labeled DataUnlabeled Parallel Text and its Preprocessing.For the unlabeled parallel text, we use the ISIChinese-English parallel corpus (Munteanu andMarcu, 2005), which was extracted automaticallyfrom news articles published by Xinhua NewsAgency in the Chinese Gigaword (2nd Edition) andEnglish Gigaword (2nd Edition) collections.Because sentence pairs in the ISI corpus are quitenoisy, we rely on Giza++ (Och and Ney, 2003) toobtain a new translation probability for eachsentence pair, and select the 100,000 pairs with thehighest translation probabilities.5We also try to remove neutral sentences fromthe parallel data since they can introduce noise intoour model, which deals only with positive andnegative examples.
To do this, we train a singleclassifier from the combined Chinese and Englishlabeled data for each data setting above byconcatenating the original English and Chinesefeature sets.
We then classify each unlabeledsentence pair by combining the two sentences ineach pair into one.
We choose the most confidentlypredicted 10,000 positive and 10,000 negativepairs to constitute the unlabeled parallel corpusfor each data setting.5We removed sentence pairs with an original confidence score(given in the corpus) smaller than 0.98, and also removed thepairs that are too long (more than 60 characters in onesentence) to facilitate Giza++.
We first obtain translationprobabilities for both directions (i.e.
Chinese to English andEnglish to Chinese) with Giza++, take the log of the productof those two probabilities, and then divide it by the sum oflengths of the two sentences in each pair.3244.2 Baseline MethodsIn our experiments, the proposed joint model iscompared with the following baseline methods.MaxEnt: This method learns a MaxEntclassifier for each language given the monolinguallabeled data; the unlabeled data is not used.SVM: This method learns an SVM classifier foreach language given the monolingual labeled data;the unlabeled data is not used.
SVM-light(Joachims, 1999a) is used for all the SVM-relatedexperiments.Monolingual TSVM (TSVM-M): This methodlearns two transductive SVM (TSVM) classifiersgiven the monolingual labeled data and themonolingual unlabeled data for each language.Bilingual TSVM (TSVM-B): This methodlearns one TSVM classifier given the labeledtraining data in two languages together with theunlabeled sentences by combining the twosentences in each unlabeled pair into one.
Weexpect this method to perform better than TSVM-M since the combined (bilingual) unlabeledsentences could be more helpful than the unlabeledmonolingual sentences.Co-Training with SVMs (Co-SVM): Thismethod applies SVM-based co-training given boththe labeled training data and the unlabeled paralleldata following Wan (2009).
First, two monolingualSVM classifiers are built based on only thecorresponding labeled data, and then they arebootstrapped by adding the most confidentpredicted examples from the unlabeled data intothe training set.
We run bootstrapping for 100iterations.
In each iteration, we select the mostconfidently predicted 50 positive and 50 negativesentences from each of the two classifiers, and takethe union of the resulting 200 sentence pairs as thenewly labeled training data.
(Examples withconflicting labels within the pair are not included.
)5 Results and AnalysisIn our experiments, the methods are tested in thetwo data settings with the corresponding unlabeledparallel corpus as mentioned in Section 4.6 We use6 The results reported in this section employ Equation 4.Preliminary experiments showed that Equation 5 does notsignificantly improve the performance in our case, which isreasonable since we choose only sentence pairs with thehighest translation probabilities to be our unlabeled data (seeSection 4.1).5-fold cross-validation and report average accuracy(also MicroF1 in this case) and MacroF1 scores.Unigrams are used as binary features for allmodels, as Pang et al (2002) showed that binaryfeatures perform better than frequency features forsentiment classification.
The weights for unlabeleddata and regularization,    and   , are set to 1unless otherwise stated.
Later, we will show thatthe proposed approach performs well with a widerange of parameter values.75.1 Method ComparisonWe first compare the proposed joint model (Joint)with the baselines in Table 2.
As seen from thetable, the proposed approach outperforms all fivebaseline methods in terms of both accuracy andMacroF1 for both English and Chinese and in bothof the data settings.
8  By making use of theunlabeled parallel data, our proposed approachimproves the accuracy, compared to MaxEnt, by8.12% (or 33.27% error reduction) on English and3.44% (or 16.92% error reduction) on Chinese inthe first setting, and by 5.07% (or 19.67% errorreduction) on English and 3.87% (or 19.4% errorreduction) on Chinese in the second setting.Among the baselines, the best is Co-SVM;TSVMs do not always improve performance usingthe unlabeled data compared to the standaloneSVM; and TSVM-B outperforms TSVM-M exceptfor Chinese in the second setting.
The MPQA datais more difficult in general compared to the NTCIRdata.
Without unlabeled parallel data, theperformance on the Chinese data is better than onthe English data, which is consistent with resultsreported in NTCIR-6 (Seki et al, 2007).Overall, the unlabeled parallel data improvesclassification accuracy for both languages whenusing our proposed joint model and Co-SVM.
Thejoint model makes better use of the unlabeledparallel data than Co-SVM or TSVMs presumablybecause of its attempt to jointly optimize the twomonolingual models via soft (probabilistic)assignments of the unlabeled instances to classes ineach iteration, instead of the hard assignments inCo-SVM and TSVMs.
Although English sentiment7The code is at http://sites.google.com/site/lubin2010.8 Significance is tested using paired t-tests with  <0.05: ?denotes statistical significance compared to the correspondingperformance of MaxEnt; * denotes statistical significancecompared to SVM; and?denotes statistical significancecompared to Co-SVM.325classification alone is more difficult than Chinesefor our datasets, we obtain greater performancegains for English by exploiting unlabeled paralleldata as well as the Chinese labeled data.5.2 Varying the Weight and Amount ofUnlabeled DataFigure 1 shows the accuracy curve of the proposedapproach for the two data settings when varyingthe weight for the unlabeled data,   , from 0 to 1.When    is set to 0, the joint model degenerates totwo MaxEnt models trained with only the labeleddata.We can see that the performance gains for theproposed approach are quite remarkable even whenis set to 0.1; performance is largely stable afterreaches 0.4.
Although MPQA is more difficultin general compared to the NTCIR data, we stillsee steady improvements in performance withunlabeled parallel data.
Overall, the proposedapproach performs quite well for a wide range ofparameter values of   .Figure 2 shows the accuracy curve of theproposed approach for the two data settings whenvarying the amount of unlabeled data from 0 to20,000 instances.
We see that the performance ofthe proposed approach improves steadily by addingmore and more unlabeled data.
However, evenwith only 2,000 unlabeled sentence pairs, theproposed approach still produces largeperformance gains.5.3 Results on Pseudo-Parallel UnlabeledDataAs discussed in Section 3.4, we generate pseudo-parallel data by translating the monolingualsentences in each setting using Google?s machinetranslation system.
Figures 3 and 4 show theperformance of our model using the pseudo-parallel data versus the real parallel data, in the twosettings, respectively.
The EN->CH pseudo-parallel data consists of the English unlabeled dataand its automatic Chinese translation, and viceversa.Although not as significant as those with paralleldata, we can still obtain improvements using thepseudo-parallel data, especially in the first setting.The difference between using parallel versuspseudo-parallel data is around 2-4% in Figures 3and 4, which is reasonable since the quality of thepseudo-parallel data is not as good as that of theparallel data.
Therefore, the performance usingpseudo-parallel data is better with a small weight(e.g.
= 0.1) in some cases.Setting 1: NTCIR-EN+NTCIR-CH Setting 2: MPQA+NTCIR-CHAccuracy MacroF1 Accuracy MacroF1English Chinese English Chinese English Chinese English ChineseMaxEnt 75.59 79.67 66.61* 79.34 74.22 79.67 65.09* 79.34SVM 76.34 81.02 61.12 80.75?
76.74?
81.02 61.35 80.75?TSVM-M 73.46 80.21 55.33 79.99 72.89 81.14 52.82 79.99TSVM-B 78.36 81.60?
65.53 81.42 76.42?
78.51 61.66 78.32Co-SVM 82.44?
* 82.79?
72.61?
* 82.67?
* 78.18?
* 82.63?
* 68.03?
* 82.51?
*Joint 83.71?
* 83.11?
* 75.89?*?
82.97?
* 79.29?*?
83.54?
* 72.58?*?
83.37?
*Table 2: Comparison of ResultsFigure 1.
Accuracy vs.
Weight of Unlabeled Data                Figure 2.
Accuracy vs.
Amount of Unlabeled Data0 0.2 0.4 0.6 0.8 17274767880828486Weight of Unlabeled DataAccuracy(%)English on NTCIR-EN+NTCIR-CHChinese on NTCIR-EN+NTCIR-CHEnglish  on MPQA+NTCIR-CHChinese on MPQA+NTCIR-CH0 0.5 1 1.5 27274767880828486Size of Unlabeled DataAccuracy(%)English on NTCIR-EN+NTCIR-CHChinese on NTCIR-EN+NTCIR-CHEnglish  on MPQA+NTCIR-CHChinese on MPQA+NTCIR-CH3265.4 Adding Pseudo-Parallel Labeled DataIn this section, we investigate how addingautomatically translated labeled data mightinfluence the performance as mentioned in Section3.4.
We use only the translated labeled data to trainclassifiers, and then directly classify the test data.The average accuracies in setting 1 are 66.61% and63.11% on English and Chinese, respectively;while the accuracies in setting 2 are 58.43% and54.07% on English and Chinese, respectively.
Thisresult is reasonable because of the language gapbetween the original language and the translatedlanguage.
In addition, the class distributions of theEnglish labeled data and the Chinese are quitedifferent (30% vs. 55% for positive as shown inTable 1).Figures 5 and 6 show the accuracies whenvarying the weight of the translated labeled data vs.the labeled data, with and without the unlabeledparallel data.
From Figure 5 for setting 1, we cansee that the translated data can be helpful given thelabeled data and even the unlabeled data, as long asis small; while in Figure 6, the translated datadecreases the performance in most cases for setting2.
One possible reason is that in the first datasetting, the NTCIR English data covers the sametopics as the NTCIR Chinese data and thus directtranslation is helpful, while the English andChinese topics are quite different in the seconddata setting, and thus direct translation hurts theperformance given the existing labeled data in eachlanguage.5.5 DiscussionTo further understand what contributions ourproposed approach makes to the performance gain,we look inside the parameters in the MaxEntmodels learned before and after adding the parallelunlabeled data.
Table 3 shows the features in themodel learned from the labeled data that have thelargest weight change after adding the parallel data;Figure 3.
Accuracy with Pseudo-Parallel Unlabeled           Figure 4.
Accuracy with Pseudo-Parallel UnlabeledData in Setting 1                                                         Data in Setting 2Figure 5.
Accuracy with Pseudo-Parallel Labeled              Figure 6.
Accuracy with Pseudo-Parallel LabeledData in Setting 1                                                      Data in Setting 20 0.2 0.4 0.6 0.8 174767880828486Weight of Unlabeled DataAccuracy(%)English on Parallel DataChinese on Parallel DataEnglish on EN->CH Pseudo-Parallel DataChinese on EN->CH Pseudo-Parallel DataEnglish on CH->EN Pseudo-Parallel DataChinese on CH->EN Pseudo-Parallel Data0 0.2 0.4 0.6 0.8 16570758085Weight of Unlabeled DataAccuracy(%)English on Parallel DataChinese on Parallel DataEnglish on EN->CH Pseudo-Parallel DataChinese on EN->CH Pseudo-Parallel DataEnglish on CH->EN Pseudo-Parallel DataChinese on CH->EN Pseudo-Parallel Data0 0.2 0.4 0.6 0.8 170727476788082846Weight of Translated Labeled DataAccuracy(%)English w/o Unlabeled DataChinese w/o Unlabeled DataEnglish with Unlabeled DataChinese with Unlabeled Data0 0.2 0.4 0.6 0.8 16870727476788828486Weight of Translated Labeled DataAccuracy(%)English w/o Unlabeled DataChinese w/o Unlabeled DataEnglish with Unlabeled DataChinese with Unlabeled Data327Positive NegativeWord Weight Word Weightfriendly 0.701 german 0.783principles 0.684 arduous 0.531hopes 0.630 oppose 0.511hoped 0.553 administrations 0.431cooperative 0.552 oau9 0.408Table 4.
New Features Learned from Unlabeled Dataand Table 4 shows the newly learned features fromthe unlabeled data with the largest weights.From Table 3 10  we can see that the weightchanges of the original features are quitereasonable, e.g.
the top words in the positive classare obviously positive and the proposed approachgives them higher weights.
The new features alsoseem reasonable given the knowledge that thelabeled and unlabeled data includes negative newsabout for specific topics (e.g.
Germany, Taiwan),.We also examine the process of joint training bychecking the performance on test data and theagreement of the two monolingual models on theunlabeled parallel data in both settings.
Theaverage agreement across 5 folds is 85.06% and73.87% in settings 1 and 2, respectively, before thejoint training, and increases to 100% and 99.89%,respectively, after 100 iterations of joint training.Although the average agreement has alreadyincreased to 99.50% and 99.02% in settings 1 and2, respectively, after 30 iterations, the performanceon the test set steadily improves in both settingsuntil around 50-60 iterations, and then becomesrelatively stable after that.Examination of those sentence pairs in setting 2for which the two monolingual models still9This is an abbreviation for the Organization of African Unity.10The features and weights in Tables 3 and 4 are extractedfrom the English model in the first fold of setting 1.disagree after 100 iterations of joint training oftenproduces sentences that are not quite parallel, e.g.
:English: The two sides attach great importance tointernational cooperation on protection and promotion ofhuman rights.Chinese: ????,????????????????,?????????????????
(Both sides agree that doublestandards on the issue of human rights are to be avoided, andare opposed to using pressure on human rights issues ininternational relations.
)Since the two sentences discuss human rightsfrom very different perspectives, it is reasonablethat the two monolingual models will classify themwith different polarities (i.e.
positive for theEnglish sentence and negative for the Chinesesentence) even after joint training.6 ConclusionIn this paper, we study bilingual sentimentclassification and propose a joint model tosimultaneously learn better monolingual sentimentclassifiers for each language by exploiting anunlabeled parallel corpus together with the labeleddata available for each language.
Our experimentsshow that the proposed approach can significantlyimprove sentiment classification for bothlanguages.
Moreover, the proposed approachcontinues to produce (albeit smaller) performancegains when employing pseudo-parallel data frommachine translation engines.In future work, we would like to apply the jointlearning idea to other learning frameworks (e.g.SVMs), and to extend the proposed model tohandle word-level parallel information, e.g.bilingual dictionaries or word alignmentinformation.
Another issue is to investigate how toimprove multilingual sentiment analysis byexploiting comparable corpora.AcknowledgmentsWe thank Shuo Chen, Long Jiang, ThorstenJoachims, Lillian Lee, Myle Ott, Yan Song,Xiaojun Wan, Ainur Yessenalina, Jingbo Zhu andthe anonymous reviewers for many usefulcomments and discussion.
This work wassupported in part by National Science FoundationGrants BCS-0904822, BCS-0624277, IIS-0968450; and by a gift from Google.
Chenhao Tanis supported by NSF (DMS-0808864), ONR (YIP-N000140910911), and a grant from Microsoft.WordWeightBefore After ChangePositiveimportant 0.452 1.659 1.207cooperation 0.325 1.492 1.167support 0.533 1.483 0.950importance 0.450 1.193 0.742agreed 0.347 1.061 0.714Negativedifficulties 0.018 0.663 0.645not 0.202 0.844 0.641never 0.245 0.879 0.634germany 0.035 0.664 0.629taiwan 0.590 1.216 0.626Table 3.
Original Features with Largest Weight Change328ReferencesMassih-Reza Amini, Cyril Goutte, and Nicolas Usunier.2010.
Combining coregularization and consensus-based self-training for multilingual textcategorization.
In Proceeding of SIGIR?10.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2010.
Multilingual subjectivity: Are more languagesbetter?
In Proceedings of COLING?10.Carmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivityanalysis using machine translation.
In Proceedings ofEMNLP?08.Adam L. Berger, Stephen A. Della Pietra and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
ComputationalLinguistics, 22(1).John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspond-dence learning.
In Proceedings of EMNLP?06.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of COLT?98.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingualsupervised Latent Dirichlet Allocation.
InProceedings of EMNLP?10.Eric Breck, Yejin Choi, and Claire Cardie.
2007.Identifying expressions of opinion in context.
InProceedings of IJCAI?07.David Burkett, Slav Petrov, John Blitzer, and DanKlein.
2010.
Learning better monolingual modelswith unannotated bilingual text.
In Proceedings ofCoNLL?10.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
InProceedings of EMNLP?08.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofEMNLP?08.Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.2009.
Exploiting bilingual information to improveweb search.
In Proceedings of ACL/IJCNLP?09.Minqing Hu and Bing Liu.
2004.
Mining opinionfeatures in customer reviews.
In Proceedings ofAAAI?04.Ido Dagan, and Alon Itai.
1994.
Word sensedisambiguation using a second language monolingualcorpus, Computational Linguistics, 20(4): 563-596.Thorsten Joachims.
1999a.
Making Large-Scale SVMLearning Practical.
In: Advances in Kernel Methods -Support Vector Learning, B. Sch?lkopf, C. Burges,and A. Smola (ed.
), MIT Press.Thorsten Joachims.
1999b.
Transductive inference fortext classification using support vector machines.
InProceedings of ICML?99.Percy Liang, Ben Taskar, and Dan Klein.
2006.Alignment by agreement.
In Proceedings ofNAACL?06.Dong C. Liu and  Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, (45): 503?528.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
InProceedings of CoNLL?02.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective language viacross-lingual projections.
In Proceedings of ACL?07.Dragos S. Munteanu and Daniel Marcu.
2005.Improving machine translation performance byexploiting non-parallel corpora.
ComputationalLinguistics, 31(4): 477?504.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing CRFs with hidden variables.
In Proceedings ofNAACL/HLT ?10.Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun,and Tom Mitchell.
2000.
Text classification fromlabeled and unlabeled documents using EM.
MachineLearning, 39(2): 103?134.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1): 19-51.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis, Foundations and Trends inInformation Retrieval, Now Publishers.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings ofEMNLP?02.Peter  Prettenhofer and  Benno Stein.
2010.
Cross-language text classification using structuralcorrespondence learning.
In Proceedings of ACL?10.Adwait Ratnaparkhi.
1997.
A simple introduction tomaximum entropy models for natural languageprocessing.
Technical Report 97-08, University ofPennsylvania.329Julia M. Schulz, Christa Womser-Hacker, and ThomasMandl.
2010.
Multilingual corpus development foropinion mining.
In Proceedings of LREC?10.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-His Chen, and Noriko Kando.
2008.
Overviewof multilingual opinion analysis task at NTCIR-7.
InProceedings of the NTCIR-7 Workshop.Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun,Hsin-His Chen, Noriko Kando, and Chin-Yew Lin.2007.
Overview of opinion analysis pilot task atNTCIR-6.
In Proceedings of the NTCIR-6 Workshop.Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.2005.
A co-regularization approach to semi-supervised learning with multiple views.
InProceedings of ICML?05.Noah A. Smith.
2006.
Novel estimation methods forunsupervised discovery of latent structure in naturallanguage text.
Ph.D. thesis, Department of ComputerScience, Johns Hopkins University.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky and Christopher Manning.
2005.
Aconditional random field word segmenter.
InProeedings of the 4th SIGHAN Workshop.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervisedclassification of reviews, In Proceedings of ACL?02.Xiaojun Wan.
2008.
Using Bilingual Knowledge andEnsemble Techniques for Unsupervised ChineseSentiment Analysis.
In Proceedings of  EMNLP?08.Xiaojun Wan.
2009.
Co-training for cross-lingualsentiment classification.
In Proceedings ofACL/AFNLP?09.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions andemotions in language.
Language Resources andEvaluation, 39(2- 3): 165-210.Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.
2010.Cross-lingual latent topic extraction, In Proceedingsof ACL?10.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009.
Cross language dependency parsing using abilingual lexicon.
In Proceedings ofACL/IJCNLP?09.Xiaojin Zhu and Andrew B. Goldberg.
2009.Introduction to Semi-Supervised Learning.
Morgan& Claypool Publishers.Appendix A.
Equation DeductionIn this appendix, we derive the gradient for the objectivefunction in Equation 3, which is used in parameterestimation.
As mentioned in Section 3.3, the parameterscan be learned by finding:Since the first term on the right-hand side is just theexpression for the standard MaxEnt problem, we willfocus on the gradient for the second term, and denoteas ( ).Let         denote    or   , andbe the  th weightin the vector   .
For brevity, we drop the   in the abovenotation, and writeto denote.
Then the partialderivative of (*) based on Equation 4 with respect tois as follows:(1)Further, we obtain:(2)Merge (2) into (1), we get:330
