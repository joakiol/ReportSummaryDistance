Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254?262,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLearning Bilingual Linguistic Reordering Model for StatisticalMachine TranslationHan-Bin Chen, Jian-Cheng Wu and Jason S. ChangDepartment of Computer ScienceNational Tsing Hua University101, Guangfu Road, Hsinchu, Taiwan{hanbin,d928322,jschang}@cs.nthu.edu.twAbstractIn this paper, we propose a method for learn-ing reordering model for BTG-based statisti-cal machine translation (SMT).
The modelfocuses on linguistic features from bilingualphrases.
Our method involves extracting reor-dering examples as well as features such aspart-of-speech and word class from alignedparallel sentences.
The features are classifiedwith special considerations of phrase lengths.We then use these features to train the maxi-mum entropy (ME) reordering model.
Withthe model, we performed Chinese-to-Englishtranslation tasks.
Experimental results showthat our bilingual linguistic model outper-forms the state-of-the-art phrase-based andBTG-based SMT systems by improvements of2.41 and 1.31 BLEU points respectively.1 IntroductionBracketing Transduction Grammar (BTG) is a spe-cial case of Synchronous Context Free Grammar(SCFG), with binary branching rules that are eitherstraight or inverted.
BTG is widely adopted inSMT systems, because of its good trade-off be-tween efficiency and expressiveness (Wu, 1996).In BTG, the ratio of legal alignments and all possi-ble alignment in a translation pair drops drasticallyespecially for long sentences, yet it still coversmost of the syntactic diversities between two lan-guages.It is common to utilize phrase translation inBTG systems.
For example in (Xiong et al, 2006),source sentences are segmented into phrases.
Eachsequences of consecutive phrases, mapping to cellsin a CKY matrix, are then translated through a bi-lingual phrase table and scored as implemented in(Koehn et al, 2005; Chiang, 2005).
In other words,their system shares the same phrase table withstandard phrase-based SMT systems.3 ?
?
3 ?
?three   after  2Ayears 1A   threeago  2A  years 1A(a) (b)Figure 1: Two reordering examples, with straightrule applied in (a), and inverted rule in (b).On the other hand, there are various proposedBTG reordering models to predict correct orienta-tions between neighboring blocks (bilingualphrases).
In Figure 1, for example, the role of reor-dering model is to predict correct orientations ofneighboring blocks A1 and A2.
In flat model (Wu,1996; Zens et al, 2004; Kumar and Byrne, 2005),reordering probabilities are assigned uniformlyduring decoding, and can be tuned depending ondifferent language pairs.
It is clear, however, thatthis kind of model would suffer when the dominantrule is wrongly applied.Predicting orientations in BTG depending oncontext information can be achieved with lexicalfeatures.
For example, Xiong et al (2006) pro-posed MEBTG, based on maximum entropy (ME)classification with words as features.
In MEBTG,first words of blocks are considered as the fea-tures, which are then used to train a ME model254for predicting orientations of neighboring blocks.Xiong et al (2008b) proposed a linguistically an-notated BTG (LABTG), in which linguistic fea-tures such as POS and syntactic labels fromsource-side parse trees are used.
Both MEBTGand LABTG achieved significant improvementsover phrase-based Pharaoh (Koehn, 2004) andMoses (Koehn et al, 2007) respectively, on Chi-nese-to-English translation tasks.?
?
?
?Nes  Nf    Nv?
?
?DE   Nathe  details  of14      49     502Athe  plan14    18 1AFigure 2: An inversion reordering example, withPOS below source words, and class numbers belowtarget words.However, current BTG-based reordering meth-ods have been limited by the features used.
Infor-mation might not be sufficient or representative, ifonly the first (or tail) words are used as features.For example, in Figure 2, consider target first-wordfeatures extracted from an inverted reordering ex-ample (Xiong et al, 2006) in MEBTG, in whichfirst words on two blocks are both "the".
This kindof feature set is too common and not representativeenough to predict the correct orientation.
Intui-tively, one solution is to extend the feature set byconsidering both boundary words, forming a morecomplete boundary description.
However, thismethod is still based on lexicalized features, whichcauses data sparseness problem and fails to gener-alize.
In Figure 2, for example, the orientationshould basically be the same, when thesource/target words "?
?/plan" from block A1 isreplaced by other similar nouns and translations(e.g.
"plans", "events" or "meetings").
However,such features would be treated as unseen by thecurrent ME model, since the training data can notpossibly cover all such similar cases.In this paper we present an improved reorder-ing model based on BTG, with bilingual linguisticfeatures from neighboring blocks.
To avoid datasparseness problem, both source and target wordsare classified; we perform part-of-speech (POS)tagging on source language, and word classifica-tion on target one, as shown in Figure 2.
Addition-ally, features are extracted and classifieddepending on lengths of blocks in order to obtain amore informed model.The rest of this paper is organized as follows.Section 2 reviews the related work.
Section 3 de-scribes the model used in our BTG-based SMTsystems.
Section 4 formally describes our bilinguallinguistic reordering model.
Section 5 and Section6 explain the implementation of our systems.
Weshow the experimental results in Section 7 andmake the conclusion in Section 8.2 Related WorkIn statistical machine translation, reordering modelis concerned with predicting correct orders of tar-get language sentence given a source language oneand translation pairs.
For example, in phrase-basedSMT systems (Koehn et al, 2003; Koehn, 2004),distortion model is used, in which reordering prob-abilities depend on relative positions of target sidephrases between adjacent blocks.
However, distor-tion model can not model long-distance reordering,due to the lack of context information, thus is diffi-cult to predict correct orders under different cir-cumstances.
Therefore, while phrase-based SMTmoves from words to phrases as the basic unit oftranslation, implying effective local reorderingwithin phrases, it suffers when determining phrasereordering, especially when phrases are longer thanthree words (Koehn et al, 2003).There have been much effort made to improvereordering model in SMT.
For example, research-ers have been studying CKY parsing over the lastdecade, which considers translations and orienta-tions of two neighboring block according togrammar rules or context information.
In hierar-chical phrase-based systems (Chiang, 2005), forexample, SCFG rules are automatically learnedfrom aligned bilingual corpus, and are applied inCKY style decoding.As an another application of CKY parsing tech-nique is BTG-based SMT.
Xiong et al (2006) andXiong et al (2008a) developed MEBTG systems,in which first or tail words from reordering exam-ples are used as features to train ME-based reorder-ing models.Similarly, Zhang et al (2007) proposed a modelsimilar to BTG, which uses first/tail words ofphrases, and syntactic labels (e.g.
NP and VP)255from source parse trees as features.
In their work,however, inverted rules are allowed to apply onlywhen source phrases are syntactic; for non-syntactic ones, blocks are combined straight with aconstant score.More recently, Xiong et al (2008b) proposedLABTG, which incorporates linguistic knowledgeby adding features such as syntactic labels andPOS from source trees to improve their MEBTG.Different from Zhang's work, their model do notrestrict non-syntactic phrases, and applies invertedrules on any pair of neighboring blocks.Although POS information is used in LABTGand Zhang's work, their models are syntax-oriented,since they focus on syntactic labels.
Boundary POSis considered in LABTG only when source phrasesare not syntactic phrases.In contrast to the previous works, we present areordering model for BTG that uses bilingual in-formation including class-level features of POSand word classes.
Moreover, our model is dedi-cated to boundary features and considers differentcombinations of phrase lengths, rather than onlyfirst/tail words.
In addition, current state-of-the-artChinese parsers, including the one used in LABTG(Xiong et al, 2005), lag beyond in inaccuracy,compared with English parsers (Klein and Man-ning, 2003; Petrov and Klein 2007).
In our work,we only use more reliable information such asChinese word segmentation and POS tagging (Maand Chen, 2003).3 The ModelFollowing Wu (1996) and Xiong et al (2006), weimplement BTG-based SMT as our system, inwhich three rules are applied during decoding:?
?21 AAA ?
(1)21 AAA ?
(2)yxA /?
(3)where A1 and A2 are blocks in source order.
Straightrule (1) and inverted rule (2) are reordering rules.They are applied for predicting target-side orderwhen combining two blocks, and form the reorder-ing model with the distributionsreoorderAA ?
)(P ,,reo 21where order ?
{straight, inverted}.In MEBTG, a ME reordering model is trainedusing features extracted from reordering examplesof aligned parallel corpus.
First words on neighbor-ing blocks are used as features.
In reordering ex-ample (a), for example, the feature set is{"S1L=three", "S2L=ago", "T1L=3", "T2L=?
"}where "S1" and "T1" denote source and targetphrases from the block A1.Rule (3) is lexical translation rule, which trans-lates source phrase x into target phrase y.
We usethe same feature functions as typical phrase-basedSMT systems (Koehn et al, 2005):654321ee)|()|()|()|()|(Ptrans??????ylwlwxypyxpxypyxpyx?????
?where 43 )|()|( ??
xypyxp lwlw ?
, 5e?
and 6e ?yare lexical translation probabilities in both direc-tions, phrase penalty and word penalty.During decoding, the blocks are produced byapplying either one of two reordering rules on twosmaller blocks, or applying lexical rule (3) onsome source phrase.
Therefore, the score of a blockA is defined asreolm orderAAAAAAA??
),,(P),(P)P()P()P(reo21lm2121????
?or)|(P)(P)P( translm yxAA lm ??
?where lmA ?
)(Plm  and lmAA ?
),(P 21lm?
are respec-tively the usual and incremental score of languagemodel.To tune all lambda weights above, we performminimum error rate training (Och, 2003) on thedevelopment set described in Section 7.Let B be the set of all blocks with source sidesentence C. Then the best translation of C is thetarget side of the block A , where256)P(argmaxA ABA?
?4 Bilingual Linguistic ModelIn this section, we formally describe the problemwe want to address and the proposed method.4.1 Problem StatementWe focus on extracting features representative ofthe two neighboring blocks being considered forreordering by the decoder, as described in Section3.
We define S(A) and T(A) as the information onsource and target side of a block A.
For twoneighboring blocks A1 and A2, the set of featuresextracted from information of them is denoted asfeature set function F(S(A1), S(A2), T(A1), S(A2)).
InFigure 1 (b), for example, S(A1) and T(A1) are sim-ply the both sides sentences "3 ? "
and "threeyears", and F(S(A1), S(A2), T(A1), S(A2)) is{"S1L=three", "S2L=after", "T1L=3", "T2L=?
"}where "S1L" denotes the first source word on theblock A1, and "T2L" denotes the first target wordon the block A2.Given the adjacent blocks A1 and A2, our goalincludes (1) adding more linguistic and representa-tive information to A1 and A2 and (2) finding a fea-ture set function F' based on added linguisticinformation in order to train a more linguisticallymotivated and effective model.4.2 Word ClassificationAs described in Section 1, designing a more com-plete feature set causes data sparseness problem, ifwe use lexical features.
One natural solution is us-ing POS and word class features.In our model, we perform Chinese POS taggingon source language.
In Xiong et al (2008b) andZhang et al (2007), Chinese parsers with PennChinese Treebank (Xue et al, 2005) style are usedto derive source parse trees, from which source-side features such as POS are extracted.
However,due to the relatively low accuracy of current Chi-nese parsers compared with English ones, we in-stead use CKIP Chinese word segmentation system(Ma and Chen, 2003) in order to derive Chinesetags with high accuracy.
Moreover, compared withthe Treebank Chinese tagset, the CKIP tagset pro-vides more fine-grained tags, including many tagswith semantic information (e.g., Nc for placenouns, Nd for time nouns), and verb transitivityand subcategorization (e.g., VA for intransitiveverbs, VC for transitive verbs, VK for verbs thattake a clause as object).On the other hand, using the POS features incombination with the lexical features in target lan-guage will cause another sparseness problem in thephrase table, since one source phrase would map tomultiple target ones with different POS sequences.As an alternative, we use mkcls toolkit (Och,1999), which uses maximum-likelihood principleto perform classification on target side.
After clas-sification, the toolkit produces a many-to-onemapping between English tokens and class num-bers.
Therefore, there is no ambiguity of wordclass in target phrases and word class features canbe used independently to avoid data sparsenessproblem and the phrase table remains unchanged.As mentioned in Section 1, features based onwords are not representative enough in some cases,and tend to cause sparseness problem.
By classify-ing words we are able to linguistically generalizethe features, and hence predict the rules morerobustly.
In Figure 2, for example, the target wordsare converted to corresponding classes, and formthe more complete boundary feature set{"T1L=14", "T1R=18", "T2L=14", "T2R=50"}  (4)In the feature set (4), #14 is the class containing"the", #18 is the class containing "plans", and #50is the class containing "of."
Note that we add last-word features "T1R=18" and "T2R=50".
As men-tioned in Section 1, the word "plan" from block A1is replaceable with similar nouns.
This extends toother nominal word classes to realize the generalrule of inverting "the ... NOUN" and "the ... of".It is hard to achieve this kind of generality usingonly lexicalized feature.
With word classification,we gather feature sets with similar concepts fromthe training data.
Table 1 shows the word classescan be used effectively to cope with data sparse-ness.
For example, the feature set (4) occurs 309times in our training data, and only 2 of them arestraight, with the remaining 307 inverted examples,implying that similar features based on wordclasses lead to similar orientation.
Additional ex-amples of similar feature sets with different wordclasses are shown in Table 1.257class X T1R = X    straight/inverted9 graph, government 2/48818 plans, events 2/30720 bikes, motors 0/69448 day, month, year 4/510Table 1: List of feature sets in the form of{"T1L=14", "T1R=X", "T2L=14", "T2R=50"}.4.3 Feature with Length ConsiderationBoundary features using both the first and lastwords provide more detailed descriptions ofneighboring blocks.
However, we should take thespecial case blocks with length 1 into consideration.For example, consider two features sets fromstraight and inverted reordering examples (a) and(b) in Figure 3.
There are two identical source fea-tures in both feature set, since first words on blockA1 and last words on block A2 are the same:{"S1L=P","S2R=Na"}?F(S(A1),S(A2),T(A1), S(A2))Therefore, without distinguishing the special case,the features would represent quite different caseswith the same feature, possibly leading to failure topredict orientations of two blocks.We propose a method to alleviate the problem offeatures with considerations of lengths of two ad-jacent phrases by classifying both the both sourceand target phrase pairs into one of four classes: M,L, R and B, corresponding to different combina-tions of phrase lengths.Suppose we are given two neighboring blocksA1 and A2, with source phrases P1 and P2 respec-tively.
Then the feature set from source side isclassified into one of the classes as follows.
Wegive examples of feature set for each class accord-ing to Figure 4.??
P??
?
?Neqa  Na?
?
?P    Nc??
?
?VC    Nahold meeting  2A  for 1Athesereasons  2Ainjordan 1A(a) (b)Figure 3: Two reordering examples with ambigu-ous features on source side.A1 A2  A1  A2?Nh??VE??P??
?
?Neqa    NaI think  for  these  reasons(a)                                     (b)M class                             L classA1 A2  A1  A2??
?Na  Caa??Na?
?
?P     Nc??
?
?VC      Natechnology and equipment in  Jordan  hold  meeting(c)                                        (d)R class                                 B classFigure 4:   Examples of different length combina-tions, mapping to four classes.1.
M class.
The lengths of P1 and P2 are both 1.
InFigure 4 (a), for example, the feature set is{"M1=Nh", "M2=VE"}2.
L class.
The length of P1 is 1, and the length ofP2 is greater than 1.
In Figure 4 (b), for exam-ple, the feature set is{"L1=P", "L2=Neqa", "L3=Na"}3.
R class.
The length of P1 is greater than 1, andthe length of P2 is 1.
In Figure 4 (c), for exam-ple, the feature set is{"R1=Na", "R2=Caa", "R3=Na"}4.
B class.
The lengths of P1 and P2 are bothgreater than 1.
In Figure 4 (d), for example, thefeature set is{"B1=P", "B2=Nc", "B3=VC", "B4=Na"}We use the same scheme to classify the two tar-get phrases.
Since both source and target words areclassified as described in Section 4.2, the featuresets are more representative and tend to lead toconsistent prediction of orientation.
Additionally,the length-based features are easy to fit into mem-ory, in contrast to lexical features in MEBTG.To summarize, we extract features based onword lengths, target-language word classes, andfine-grained, semantic oriented parts of speech.
Toillustrate, we use the neighboring blocks from Fig-258ure 2 to show an example of complete bilinguallinguistic feature set:{"S.B1=Nes", "S.B2=Nv", "S.B3=DE","S.B4=Na", "T.B1=14", "T.B2=18", "T.B3=14","T.B4=50"}where "S." and "T." denote source and target sides.In the next section, we describe the process ofpreparing the feature data and training an MEmodel.
In Section 7, we perform evaluations of thisME-based reordering model against standardphrase-based SMT and previous work based onME and BTG.5 TrainingIn order to train the translation and reorderingmodel, we first set up Moses SMT system (Koehnet al, 2007).
We obtain aligned parallel sentencesand the phrase table after the training of Moses,which includes running GIZA++ (Och and Ney,2003), grow-diagonal-final symmetrization andphrase extraction (Koehn et al, 2005).
Our systemshares the same translation model with Moses,since we directly use the phrase table to applytranslation rules (3).On the other side, we use the aligned parallelsentences to train our reordering model, which in-cludes classifying words, extracting bilingualphrase samples with orientation information, andtraining an ME model for predicting orientation.To perform word classification, the source sen-tences are tagged and segmented before the Mosestraining.
As for target side, we ran the Mosesscripts to classify target language words using themkcls toolkit before running GIZA++.
Therefore,we directly use its classification result, which gen-erate 50 classes with 2 optimization runs on thetarget sentences.To extract the reordering examples, we choosesentence pairs with top 50% alignment scores pro-vided by GIZA++, in order to fit into memory.Then the extraction is performed on these alignedsentence pairs, together with POS tags and wordclasses, using basically the algorithm presented inXiong et al (2006).
However, we enumerate allreordering examples, rather than only extract thesmallest straight and largest inverted examples.Finally, we use the toolkit by Zhang (2004) to trainthe ME model with extracted reordering examples.6 DecodingWe develop a bottom-up CKY style decoder in oursystem, similar to Chiang (2005).
For a Chinesesentence C, the decoder finds its best translation onthe block with entire C on source side.
The decoderfirst applies translation rules (3) on cells in a CKYmatrix.
Each cell denotes a sequence of sourcephrases, and contains all of the blocks with possi-ble translations.
The longest length of sourcephrase to be applied translations rules is restrictedto 7 words, in accordance with the default settingsof Moses training scripts.To reduce the search space, we apply thresholdpruning and histogram pruning, in which the blockscoring worse than 10-2 times the best block in thesame cell or scoring worse than top 40 highestscores would be pruned.
These pruning techniquesare common in SMT systems.
We also apply re-combination, which distinguish blocks in a cellonly by 3 leftmost and rightmost target words, assuggested in (Xiong et al, 2006).7 Experiments and ResultsWe perform Chinese-to-English translation taskon NIST MT-06 test set, and use Moses andMEBTG as our competitors.The bilingual training data containing 2.2M sen-tences pairs from Hong Kong Parallel Text(LDC2004T08) and Xinhua News Agency(LDC2007T09), with length shorter than 60, isused to train the translation and reordering model.The source sentences are tagged and segmentedwith CKIP Chinese word segmentation system (Maand Chen, 2003).About 35M reordering examples are extractedfrom top 1.1M sentence pairs with higher align-ment scores.
We generate 171K features for lexi-calized model used in MEBTG system, and 1.41Kfeatures for our proposed reordering model.For our language model, we use Xinhua newsfrom English Gigaword Third Edition(LDC2007T07) to build a trigram model withSRILM toolkit (Stolcke, 2002).Our development set for running minimum errorrate training is NIST MT-08 test set, with sentencelengths no more than 20.
We report the experimen-tal results on NIST MT-06 test set.
Our evaluationmetric is BLEU (Papineni et al, 2002) with case-insensitive matching from unigram to four-gram.259System BLEU-4Moses(distortion) 22.55Moses(lexicalized) 23.42MEBTG 23.65WC+LC 24.96Table 2: Performances of various systems.The overall result of our experiment is shown inTable 2.
The lexicalized MEBTG system proposedby Xiong et al (2006) uses first words on adjacentblocks as lexical features, and outperforms phrase-based Moses with default distortion model and en-hanced lexicalized model, by 1.1 and 0.23 BLEUpoints respectively.
This suggests lexicalizedMoses and MEBTG with context information out-performs distance-based distortion model.
Besides,MEBTG with structure constraints has betterglobal reordering estimation than unstructuredMoses, while incorporating their local reorderingability by using phrase tables.The proposed reordering model trained withword classification (WC) and length consideration(LC) described in Section 4 outperforms MEBTGby 1.31 point.
This suggests our proposed modelnot only reduces the model size by using 1% fewerfeatures than MEBTG, but also improves the trans-lation quality.We also evaluate the impacts of WC and LCseparately and show the results in Table 3-5.
Table3 shows the result of MEBTG with word classifiedfeatures.
While classified MEBTG only improves0.14 points over original lexicalized one, it drasti-cally reduces the feature size.
This implies WCalleviates data sparseness by generalizing the ob-served features.Table 4 compares different length considerations,including boundary model demonstrated in Section4.2, and the proposed LC in Section 4.3.
Althoughboundary model describes features better than us-ing only first words, which we will show later, itsuffers from data sparseness with twice feature sizeof MEBTG.
The LC model has the largest featuresize but performs best among three systems, sug-gesting the effectiveness of our LC.In Table 5 we show the impacts of WC and LCtogether.
Note that all the systems with WC sig-nificantly reduce the size of features compared tolexicalized ones.System Feature size BLEU-4MEBTG 171K 23.65WC+MEBTG 0.24K 23.79Table 3: Performances of lexicalized and wordclassified MEBTG.System Feature size BLEU-4MEBTG 171K 23.65Boundary 349K 23.42LC 780K 23.86Table 4: Performances of BTG systems with dif-ferent representativeness.System Feature size BLEU-4MEBTG 171K 23.65WC+MEBTG 0.24K 23.79WC+Bounary 0.48K 24.29WC+LC 1.41K 24.96Table 5: Different representativeness with wordclassification.While boundary model is worse than first-wordMEBTG in Table 4, it outperforms the latter whenboth are performed WC.
We obtain the best resultthat outperforms the baseline MEBTG by morethan 1 point when we apply WC and LC together.Our experimental results show that we are ableto ameliorate the sparseness problem by classifyingwords, and produce more representative featuresby considering phrase length.
Moreover, they areboth important, in that we are unable to outperformour competitors by a large margin unless we com-bine both WC and LC.
In conclusion, while de-signing more representative features of reorderingmodel in SMT, we have to find solutions to gener-alize them.8 Conclusion and Future WorksWe have proposed a bilingual linguistic reorderingmodel to improve current BTG-based SMT sys-tems, based on two drawbacks of previously pro-posed reordering model, which are sparseness andrepresentative problem.First, to solve the sparseness problem in previ-ously proposed lexicalized model, we performword classification on both sides.260Secondly, we present a more representative fea-ture extraction method.
This involves consideringlength combinations of adjacent phrases.The experimental results of Chinese-to-Englishtask show that our model outperforms baselinephrase-based and BTG systems.We will investigate more linguistic ways to clas-sify words in future work, especially on target lan-guage.
For example, using word hierarchicalstructures in WordNet (Fellbaum, 1998) systemprovides more linguistic and semantic informationthan statistically-motivated classification tools.AcknowledgementsThis work was supported by National ScienceCouncil of Taiwan grant NSC 95-2221-E-007-182-MY3.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL 2005, pp.
263-270.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Massachusetts.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT/NAACL 2003.Philipp Koehn.
2004.
Pharaoh: a Beam Search  Decoderfor Phrased-Based Statistical Machine TranslationModels.
In Proceedings of AMTA 2004.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In International Workshop on Spoken LanguageTranslation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan,Wade Shen, Christine Moran, Rich-ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-strantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL 2007, Demonstration Session.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of ACL 2003.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of HLT-EMNLP 2005.Wei-Yun Ma and Keh-Jiann Chen.
2003.
Introductionto CKIP Chinese Word Segmentation System for theFirst International Chinese Word SegmentationBakeoff.
In Proceedings of ACL, Second SIGHANWorkshop on Chinese Language Processing, pp168-171.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In EACL ?99: NinthConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 71?76,Bergen, Norway, June.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29:19-51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL 2003, pages 160-167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the ACL, pages 311?318.Slav Petrov and Dan Klein.
2007.
Improved Inference-for Unlexicalized Parsing.
In Proceedings of HLT-NAACL 2007.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904.Dekai Wu.
1996.
A Polynomial-Time Algorithm forStatistical Machine Translation.
In Proceedings ofACL 1996.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, andYueliang Qian.
2005.
Parsing the Penn Chinese tree-bank with semantic knowledge.
In Proceedings ofIJCNLP 2005, pages 70-81.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofACL-COLING 2006.Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,and Shouxun Liu.
2008a.
Refinements in BTG-basedstatistical machine translation.
In Proceedings ofIJCNLP 2008, pp.
505-512.Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li.2008b.
Linguistically Annotated BTG for StatisticalMachine Translation.
In Proceedings of COLING2008.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: Phrase261structure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering Constraints for Phrase-Based StatisticalMachine Translation.
In Proceedings of CoLing 2004,Geneva, Switzerland, pp.
205-211.Le Zhang.
2004.
Maximum Entropy Modeling Toolkitfor Python and C++.
Available at http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou.2007.
Phrase Reordering Model Integrating SyntacticKnowledge for SMT.
In Proceedings of EMNLP-CoNLL 2007.262
