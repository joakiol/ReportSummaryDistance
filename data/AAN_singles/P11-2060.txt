Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 346?350,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Detection and Correction of  Errors in Dependency Tree-banksAlexander VolokhDFKIStuhlsatzenhausweg 366123 Saarbr?cken, Germanyalexander.volokh@dfki.deG?nter NeumannDFKIStuhlsatzenhausweg 366123 Saarbr?cken, Germanyneumann@dfki.deAbstractAnnotated corpora are essential for almost allNLP applications.
Whereas they are expectedto be of a very high quality because of theirimportance  for  the  followup  developments,they still contain a considerable number of er-rors.
With this work we want to draw attentionto this fact.
Additionally, we try to estimatethe amount of errors and propose a method fortheir  automatic  correction.
Whereas  our  ap-proach is able to find only a portion of the er-rors that we suppose are contained in almostany annotated corpus due to the nature of theprocess of its creation, it has a very high pre-cision, and thus is in any case beneficial forthe quality of the corpus it  is  applied to.
Atlast, we compare it to a different method forerror detection in treebanks and find out thatthe errors that we are able to detect are mostlydifferent and that our approaches are comple-mentary.1 IntroductionTreebanks and other annotated corpora  have be-come essential for almost all NLP applications.
Pa-pers about corpora like the Penn Treebank [1] havethousands of citations, since most of the algorithmsprofit from annotated data during the developmentand testing and thus are widely used in the field.Treebanks are therefore expected to be of a veryhigh  quality  in  order  to  guarantee  reliability  fortheir theoretical and practical uses.
The construc-tion of an annotated corpus involves a lot of workperformed by large groups.
However, despite thefact that a lot of human post-editing and automaticquality  assurance  is  done,  errors  can  not  beavoided completely [5].In this paper we propose an approach for find-ing and correcting errors in dependency treebanks.We apply our method to the English dependencycorpus ?
conversion of the Penn Treebank to thedependency format done by Richard Johansson andMihai  Surdeanu [2]  for  the  CoNLL shared tasks[3].
This  is  probably  the  most  used  dependencycorpus, since English is the most popular languageamong the researchers.
Still we are able to find aconsiderable amount of errors in it.
Additionally,we  compare  our  method  with  an  interesting  ap-proach developed by a different group of research-ers (see section 2).
They are able to find a similarnumber of errors in different corpora, however, asour investigation shows, the overlap between ourresults is quite small and the approaches are rathercomplementary.2 Related WorkSurprisingly, we were not able to find a lot of workon the topic of error detection in treebanks.
Someorganisers of shared tasks usually try to guaranteea certain quality of the used data, but the qualitycontrol is usually performed manually.
E.g.
in thealready mentioned CoNLL task the organisers ana-lysed a large amount of dependency treebanks fordifferent  languages  [4],  described  problems  theyhave encountered and forwarded them to the de-velopers  of  the  corresponding corpora.
The  onlywork,  that  we were able to find,  which involvedautomatic quality control, was done by the alreadymentioned  group  around  Detmar  Meurers.
Thiswork  includes  numerous  publications  concerningfinding errors in phrase structures [5] as well as independency treebanks [6].
The approach is basedon the concept of ?variation detection?, first intro-duced  in  [7].
Additionally,  [5]  presents  a  good346method  for  evaluating the automatic  error  detec-tion.
We will perform a similar evaluation for theprecision of our approach.3 Variation DetectionWe will  compare  our  outcomes  with  the  resultsthat can be found with the approach of ?variationdetection?
proposed by Meurers  et  al.
For  spacereasons, we will not be able to elaborately presentthis method and advise to read the referred work,However, we think that we should at least brieflyexplain its idea.The idea behind ?variation detection?
is to findstrings, which occur multiple times in the corpus,but which have varying annotations.
This can obvi-ously have only two reasons: either the strings areambiguous and can have different  structures,  de-pending on the meaning, or the annotation is erro-neous in at least one of the cases.
The idea can beadapted to dependency structures as well, by ana-lysing the possible dependency relations betweensame words.
Again different dependencies can beeither the result of ambiguity or errors.4 Automatic Detection of ErrorsWe propose a different approach.
We take the Eng-lish  dependency  treebank  and  train  models  withtwo different  state  of  the  art  parsers:  the  graph-based  MSTParser  [9]  and  the  transition-basedMaltParser [10].
We then parse the data, which wehave used for training, with both parsers.
The ideabehind this step is that we basically try to repro-duce the gold standard, since parsing the data seenduring the training is very easy (a similar idea inthe area of POS tagging is very broadly describedin  [8]).
Indeed  both  parsers  achieve  accuraciesbetween 98% and 99% UAS (Unlabeled Attach-ment Score), which is defined as the proportion ofcorrectly identified dependency relations.
The reas-on why the parsers are not able to achieve 100% ison the one hand the fact that some of the phenom-ena are too rare and are not captured by their mod-els.
On the other hand, in many other cases parsersdo make correct predictions, but the gold standardthey are evaluated against is wrong.We  have  investigated  the  latter  case,  namelywhen both parsers  predict  dependencies  differentfrom the gold standard (we do not consider the cor-rectness of the dependency label).
Since MSTPars-er and MaltParser are based on completely differ-ent parsing approaches they also tend to make dif-ferent mistakes [11].
Additionally, considering theaccuracies of 98-99% the chance that both parsers,which  have  different  foundations,  make  an  erro-neous  decision  simultaneously is  very small  andtherefore these cases are the most likely candidateswhen looking for errors.5 Automatic Correction of ErrorsIn this section we propose our algorithm for auto-matic  correction of  errors,  which consists  out  ofthe following steps:1.
Automatic  detection  of  error  candidates,i.e.
cases where two parsers deliver resultsdifferent to gold-standard.2.
Substitution of the annotation of the errorcandidates by the annotation proposed byone  of  the  parsers  (in  our  caseMSTParser).3.
Parse of the modified corpus with a thirdparser (MDParser).4.
Evaluation of the results.5.
The modifications are only kept for thosecases  when  the  modified  annotation  isidentical  with  the  one  predicted  by  thethird parser and undone in other cases.For the English dependency treebank we haveidentified  6743  error  candidates,  which  is  about0.7% of all tokens in the corpus.The third dependency parser, which is used isMDParser1 - a fast transition-based parser.
We sub-situte  the  gold  standard  by  MSTParser  and  notMaltParser in order not to give an advantage to aparser  with  similar  basics  (both  MDParser  andMDParser are transition-based).During this experiment we have found out thatthe result of MDParser significantly improves: it isable to correctly recgonize 3535 more dependen-cies than before the substitution of the gold stand-ard.
2077 annotations remain wrong independentlyof the changes in the gold standard.
1131 of the re-lations  become  wrong  with  the  changed  goldstandard,  whereas they were correct  with the oldunchanged version.
We then undo the changes tothe gold standard when the wrong cases remainedwrong and when the correct cases became wrong.We suggest that the 3535 dependencies which be-came correct after the change in gold standard are1 http://mdparser.sb.dfki.de/347errors, since a) two state of the art parsers deliver aresult which differs from the gold standard and b) athird parser confirms that by delivering exactly thesame result as the proposed change.
However, theexact  precision of  the  approach can probably becomputed only by manual investigation of all cor-rected dependencies.6 Estimating the Overall Number Of Er-rorsThe previous section tries to evaluate the precisionof the approach for the identified error candidates.However, it remains unclear how many of the er-rors are found and how many errors can be still ex-pected in the corpus.
Therefore in this section wewill describe our attempt to evaluate the recall ofthe proposed method.In  order  to  estimate  the  percentage  of  errors,which can be found with our method, we have de-signed the following experiment.
We have takensentences of different lengths from the corpus andprovided them with a ?gold standard?
annotationwhich  was  completely  (=100%)  erroneous.
Wehave achieved that by substituting the original an-notation by the annotation of a different sentenceof the same length from the corpus, which did notcontain  dependency  edges  which  would  overlapwith the original annotation.
E.g consider the fol-lowing sentence in the (slightly simplified) CoNLLformat:1 Not RB 6 SBJ2 all PDT 1 NMOD3 those DT 1 NMOD4 who WP 5 SBJ5 wrote VBD 1 NMOD6 oppose VBP 0 ROOT7 the DT 8 NMOD8 changes NNS 6 OBJ9 .
.
6 PWe would substitute its annotation by an annota-tion chosen from a different sentence of the samelength:1 Not RB 3 SBJ2 all PDT 3 NMOD3 those DT 0 NMOD4 who WP 3 SBJ5 wrote VBD 4 NMOD6 oppose VBP 5 ROOT7 the DT 6 NMOD8 changes NNS 7 OBJ9 .
.
3 PThis way we know that we have introduced awell-formed dependency tree (since its annotationbelonged to a different tree before) to the corpusand  the  exact  number  of  errors  (since  randomlycorrect  dependencies  are  impossible).
In  case  ofour example 9 errors are introduced to the corpus.In  our  experiment  we  have  introduced  sen-tences  of  different  lengths  with  overall  1350tokens.
We  have  then  retrained  the  models  forMSTParser and MaltParser and have applied ourmethodology  to  the  data  with  these  errors.
Wehave then counted how many of these 1350 errorscould  be  found.
Our  result  is  that  619  tokens(45.9%)  were different  from the  erroneous gold-standard.
That means that despite the fact that thetraining data contained some incorrectly annotatedtokens, the parsers were able to annotate them dif-ferently.
Therefore we suggest that the recall of ourmethod is close to the value of 0.459.
However, ofcourse we do not know whether the randomly in-troduced errors  in  our  experiment  are  similar  tothose which occur in real treebanks.7 Comparison with Variation DetectionThe interesting question which naturally arises atthis  point  is  whether  the  errors  we  find  are  thesame as those found by the method of variation de-tection.
Therefore we have performed the follow-ing experiment: We have counted the numbers ofoccurrences  for   the  dependencies  B?
A (theword B is the head of the word A) and C?
A(the  word  C is  the  head  of  the  word  A),  whereB?
A is the dependency proposed by the pars-ers and  C?
A is the dependency proposed bythe gold standard.
In order for variation detectionto be applicable the frequency counts for both rela-tions must be available and the counts for the de-pendency proposed by the parsers should ideallygreatly outweigh the frequency of the gold stand-ard, which would be a great indication of an error.For the 3535 dependencies that we classify as er-rors the variation detection method works only 934times (39.5%).
These are the cases when the goldstandard is obviously wrong and occurs only fewtimes, most often - once, whereas the parsers pro-348pose much more frequent dependencies.
In all oth-er cases the counts suggest that the variation detec-tion would not work, since both dependencies havefrequent counts or the correct dependency is evenoutweighed by the incorrect one.8 ExamplesWe will provide some of the example errors, whichwe are able to find with our approach.
Thereforewe  will  provide  the  sentence  strings  and brieflycompare the gold standard dependency annotationof a certain dependency within these sentences.Together, the two stocks wreaked havoc amongtakeover stock traders, and caused a 7.3% drop inthe DOW Jones Transportation Average, second insize  only  to  the  stock-market  crash of  Oct.  191987.In this sentence the gold standard suggests thedependency  relation  market?
the ,  whereasthe  parsers  correctly  recognise  the  dependencycrash?
the .
Both  dependencies  have  veryhigh counts  and therefore  the  variation detectionwould not work well in this scenario.Actually, it  was down only a few points at thetime.In  this  sentence  the  gold  standard  suggestspoints?at ,  whereas  the  parsers  predictwas?
at .
The gold standard suggestion occursonly  once  whereas  the  temporal  dependencywas?
at occurs 11 times in the corpus.
This isan example of an error which could be found withthe variation detection as well.Last October, Mr. Paul paid out $12 million ofCenTrust's cash ?
plus a $1.2 million commission?
for ?Portrait of a Man as Mars?.In this sentence the gold standard suggests thedependency relation $?
a , whereas the parserscorrectly  recognise  the  dependencycommission?a .
The  interesting  fact  is  thatthe  relation  $?
a is  actually  much  more  fre-quent than commission?a , e.g.
as in the sen-tence he cought up an additional $1 billion or so.
( $?
an )  So  the  variation  detection  alonewould not suffice in this case.9 ConclusionThe quality of treebanks is of an extreme import-ance for the community.
Nevertheless, errors canbe found even in the most popular and widely-usedresources.
In this paper we have presented an ap-proach for  automatic  detection and correction  oferrors and compared it to the only other work wehave found in this field.
Our results show that bothapproaches are rather complementary and find dif-ferent types of errors.We have only analysed the errors in the head-modifier annotation of the dependency relations inthe  English  dependency  treebank.
However,  thesame methodology can easily be applied to detectirregularities in any kind of annotations, e.g.
labels,POS tags etc.
In fact, in the area of POS tagging asimilar strategy of using the same data for trainingand testing in order to detect  inconsistencies hasproven to be very efficient [8].
However, the meth-od lacked means  for  automatic  correction of  thepossibly inconsistent annotations.
Additionally, themethod off course can as well be applied to differ-ent corpora in different languages.Our  method  has  a  very  high  precision,  eventhough  we  could  not  compute  the  exact  value,since it  would require an expert  to go through alarge number of cases.
It is even more difficult toestimate the recall of our method, since the overallnumber of errors in a corpus is unknown.
We havedescribed an experiment  which to  our  mind is  agood  attempt  to  evaluate  the  recall  of  our  ap-proach.
On  the  one  hand  the  recall  we  haveachieved in this experiment is rather low (0.459),which means that our method would definitely notguarantee to find all errors in a corpus.
On the oth-er hand it has a very high precision and thus is inany case beneficial,  since the quality of the tree-banks increases with the removal of errors.
Addi-tionally, the low recall suggests that treebanks con-tain an even larger number of errors, which couldnot  be found.
The overall  number  of errors  thusseems to be over 1% of the total size of a corpus,which is expected to be of a very high quality.
Afact that one has to be aware of when working withannotated resources and which we would like toemphasize with our paper.10 AcknowledgementsThe presented work was partially supported by agrant from the German Federal  Ministry of Eco-nomics  and  Technology  (BMWi)  to  the  DFKITheseus  project  TechWatch?Ordo  (FKZ:  01M-Q07016).349References[1]  Mitchell  P.  Marcus,  Beatrice  Santorini  and  MaryAnn Marcinkiewicz , 1993.
Building a Large Annot-ated Corpus of English: The Penn Treebank.
In Com-putational Lingustics, vol.
19, pp.
313-330.
[2] Mihai Surdeanu, Richard Johansson, Adam Meyers,Lluis Marquez and Joakim Nivre.
The CoNLL-2008Shared Task on Joint  Parsing of Syntactic and Se-mantic  Dependencies.
In  Proceedings  of  the  12thConference  on  Computational  Natural  LanguageLearning (CoNLL-2008), 2008[3] Sabine Buchholz and Erwin Marsi, 2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings  of  CONLL-X,  pages  149?164,  NewYork.
[4] Sabine Buchholz and Darren Green, 2006.
Qualitycontrol  of  treebanks:  documenting,  converting,patching.
In LREC 2006 workshop on Quality assur-ance  and  quality  measurement  for  language  andspeech resources.
[5] Markus Dickinson and W. Detmar Meurers,  2005.Prune  Diseased  Branches  to  Get  Healthy  Trees!How to Find Erroneous Local Trees in a Treebankand Why  It  Matters.
In  Proceedings  of  the  FourthWorkshop on Treebanks and Linguistic Theories, pp.41?52[6] Adriane Boyd, Markus Dickinson and Detmar Meur-ers, 2008.
On Detecting Errors in Dependency Tree-banks.
In  Research on Language and Computation,vol.
6, pp.
113-137.
[7] Markus Dickinson and Detmar Meurers, 2003.
De-tecting inconsistencies in treebanks.
In  Proceedingsof TLT 2003[8] van Halteren, H. (2000).
The detection of inconsist-ency  in  manually  tagged  text.
In  A.
Abeill?,  T.Brants, and H. Uszkoreit (Eds.
), Proceedings of theSecond Workshop on Linguistically Interpreted Cor-pora (LINC-00), Luxembourg.
[9 R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c .2005.
Non-projective  Dependency  Parsing  usingSpanning Tree Algorithms.
In Proc.
of HLT/EMNLP2005.
[10]  Joakim Nivre,  Johan  Hall,  Jens  Nilsson,  AtanasChanev,  Gulsen  Eryigit,  Sandra  Kubler,  SvetoslavMarinov  and  Erwin  Marsi.
2007.
MaltParser:  ALanguage-Independent  System for Data-Driven De-pendency  Parsing,  Natural  Language  EngineeringJournal, 13, pp.
99-135.
[11] Joakim Nivre and Ryan McDonald, 2008.
Integrat-ing GraphBased and Transition-Based DependencyParsers.
In Proceedings of the 46th Annual Meetingof  the  Association  for  Computational  Linguistics:Human Language Technologies.350
