Proceedings of the ACL Student Research Workshop, pages 61?66,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsTowards an Optimal Lexicalization in a Natural-Sounding PortableNatural Language Generator for Dialog SystemsInge M. R. De BleeckerDepartment of LinguisticsThe University of Texas at AustinAustin, TX 78712, USAimrdb@mail.utexas.eduAbstractIn contrast to the latest progress in speechrecognition, the state-of-the-art in naturallanguage generation for spoken languagedialog systems is lagging behind.
Thecore dialog managers are now more so-phisticated; and natural-sounding andflexible output is expected, but notachieved with current simple techniquessuch as template-based systems.
Portabil-ity of systems across subject domains andlanguages is another increasingly impor-tant requirement in dialog systems.
Thispaper presents an outline of LEGEND, asystem that is both portable and generatesnatural-sounding output.
This goal isachieved through the novel use of existinglexical resources such as FrameNet andWordNet.1 IntroductionMost of the natural language generation (NLG)components in current dialog systems are imple-mented through the use of simple techniques suchas a library of hand-crafted and pre-recorded utter-ances, or a template-based system where the tem-plates contain slots in which different values canbe inserted.
These techniques are unmanageable ifthe dialog system aims to provide variable, natural-sounding output, because the number of pre-recorded strings or different templates becomesvery large (Theune, 2003).
These techniques alsomake it difficult to port the system into anothersubject domain or language.In order to be widely successful, natural lan-guage generation components of future dialog sys-tems need to provide natural-sounding outputwhile being relatively easy to port.
This can beachieved by developing more sophisticated tech-niques based on concepts from deep linguistically-based NLG and text generation, and through theuse of existing resources that facilitate both thenatural-sounding and the portability requirement.We might wonder what exactly it means for acomputer to generate ?natural-sounding?
output.Computer-generated natural-sounding outputshould not mimic the output a human would con-struct, because spontaneous human dialog tends tobe teeming with disfluencies, interruptions, syntac-tically incorrect and incomplete sentences amongothers (Zue, 1997).
Furthermore, Oberlander(1998) points out that humans do not always takethe most efficient route in their reasoning andcommunication.
These observations lead us todefine natural-sounding computer-generated outputto consist of utterances that are free of disfluenciesand interruptions, and where complete andsyntactically correct sentences convey the meaningin a concise yet clear manner.Secondly we can define the portabilityrequirement to include both domain and languageindependence.
Domain-independence suggests thatthe system must be easily portable betweendifferent domains, while language-independencerequires that the system must be able toaccommodate a new natural language without anychanges to the core components.Section 2 of this paper explains some prerequi-sites, such as the NLG pipeline architecture oursystem is based on, and the FrameNet and Word-Net resources.
Next an overview of the system ar-61chitecture and implementation, as well as an in-depth analysis of the lexicalization component arepresented.
Section 3 presents related work.
Section4 outlines a preliminary conclusion and lists someoutstanding issues.2  System Architecture2.1 Three-Stage Pipeline ArchitectureOur natural language generator architecturefollows the three-stage pipeline architecture, asdescribed in Reiter & Dale (2000).
In thisarchitecture, the generation component of a textgeneration system consists of the followingsubcomponents:?
The document planner determines what theactual content of the output will be on anabstract level and decides how pieces ofcontent should be grouped together.?
The microplanner includes lexicalization,aggregation, and referring expressiongeneration tasks.?
The surface realizer takes the informationconstructed by the microplanner andgenerates a syntactically correct sentence ina natural language.2.2 Lexical ResourcesThe use of FrameNet and WordNet in our systemis critical to its success.
The FrameNet database(Baker et al, 1998) is a machine-readable lexico-graphic database which can be found athttp://framenet.icsi.berkeley.edu/.
It is based on theprinciples of Frame Semantics (Fillmore, 1985).The following quote explains the idea behindFrame Semantics: ?The central idea of Frame Se-mantics is that word meanings must be describedin relation to semantic frames ?
schematic repre-sentations of the conceptual structures and patternsof beliefs, practices, institutions, images, etc.
thatprovide a foundation for meaningful interaction ina given speech community.?
(Fillmore et al, 2003,p.
235).
In FrameNet, lexical units are grouped inframes; frame hierarchy information is providedfor each frame, in combination with a list of se-mantically annotated corpus sentences and syntac-tic valence patterns.WordNet is a lexical database that uses conceptual-semantic and lexical relations in order to grouplexical items and link them to other groups(Fellbaum, 1998).2.3 System OverviewOur system, called LEGEND (LExicalization innatural language GENeration for Dialog systems)adapts the pipeline architecture presented insection 2.1 by replacing the document planner withthe dialog manager.
This makes it more suitablefor use in dialog systems, since the dialog managerdecides on the actual content of the output indialog systems.
Figure 1 below shows an overviewof our system architecture.Figure 1.
System ArchitectureAs figure 1 shows, the dialog manager providesthe generator with a dialog manager meaningrepresentation (DM MR), which contains thecontent information for the answer.Our research focuses on the lexicalization sub-component of the microplanner (number 1 in fig-ure 1).
Lexicalization is further divided into twoprocesses: lexical choice and lexical search.
Basedon the DM MR, the lexical choice process (number2 in figure 1) constructs a set of all potential outputcandidates.
Section 2.5 describes the lexical choiceprocess in detail.
Lexical search (number 3 in fig-ure 1) consists of the decision algorithm that de-62cides which one of the set of possible candidates ismost appropriate in any situation.
Lexical search isalso responsible for packaging up the most appro-priate candidate information in an adapted F-structure, which is subsequently processed throughaggregation and referring expression generation,and finally sent to the surface realizer.
Section 2.6describes the details of the lexical search process.2.4 Implementation DetailsGiven time and resource constraints, our imple-mentation will consist of a prototype (written inPython) of the lexical choice and lexical searchprocesses only of the microplanner.
We take a DMMR as our input.
Aggregation and referring ex-pression generation requirements are hard-codedfor each example;  algorithm development, identi-fication and implementation for these modules isbeyond the scope of this research.Our system uses the LFG-based XLE system?sgenerator component as a surface realizer.
Formore information, refer to Shemtov (1997) andKaplan & Wedekind (2000).2.5 Lexical ChoiceThe task of the lexical choice process is to take themeaning representation presented by the dialogmanager (refer to figure 1), and to construct a setof output candidates.
We will illustrate this by tak-ing a simple example through the entire dialog sys-tem.
The example question and answer aredeliberately kept simple in order to focus on theworkings of the system, rather than the specifics ofthe example.Assume this is a dialog system that helps theconsumer in buying camping equipment.
The usersays to the dialog system: ?Where can I buy atent??
The speech recognizer recognizes the utter-ance, and feeds this information to the parser.
Thesemantic parser parses the input and builds themeaning representation shown in figure 2.
Themain event (main verb) is identified as the lexicalitem buy.
The parser looks up this lexical item inFrameNet, and identifies it as belonging to thecommerce_buy frame.
This frame is defined inFrameNet as: ??
describing a basic commercialtransaction involving a buyer and a seller exchang-ing money and goods, taking the perspective of thebuyer.?
(http://framenet.icsi.berkeley.edu/).
Allother elements in the meaning representation areextracted from the input utterance.Figure 2.
Parser Meaning RepresentationThis meaning representation is then sent to thedialog manager.
The dialog manager consults thedomain model for help in the query resolution, andsubsequently composes a meaning representationconsisting of the answer to the user?s question(figure 3).
For our example, the domain model pre-sents the query resolution as ?Camping World?,the name of a (fictitious) store selling tents.
TheDM MR also shows that the Agent and the Patienthave been identified by their frame element names.This DM MR serves as the input to themicroplanner, where the first task is that of lexicalchoice.Figure 3.
Dialog Mgr Meaning RepresentationIn order to construct the set of output candidates,the lexical choice process mines the FrameNet andWordNet databases in order to find acceptablegeneration possibilities.
This is done in severalsteps:?
In step 1, lexicalization variations of themain Event within the same frame are iden-tified.?
Step 2 consists of the investigation of lexicalvariation in the frames that are one linkaway in the hierarchy, namely the frame thecurrent frame inherits from, and the sub-frames, if any exist.?
Step 3 is concerned with special relationswithin FrameNet, such as the ?use?-relationThe lexical variation within these frames isinvestigated.We return to our example in figure 3 to clarifythese 3 steps.In step 1, appropriate lexical variation within thesame frame is identified.
This is done by listing allEvent: buyFrame: commerce_buyQuery Resolution: place ?Camping World?Agent: buyer (1st p.s.
=> 2nd p.s.
)Object: goods (?tent?
)Event: buyFrame: commerce_buyQuery: locationAgent: 1st pers singPatient: tent63lexical units of same syntactic category as theoriginal word.
The following verbs are lexical unitsin commerce_buy: buy, lease, purchase, rent.These verbs are not necessarily synonyms or near-synonyms of each other, but do belong to the sameframe.
In order to determine which of these lexicalitems are synonyms or near-synonyms, we turn toWordNet, and look at the entry for buy.
The onlylexical item that is also listed in one of the sensesof buy is purchase.
We thus conclude that buy andpurchase are both good verb candidates.Step 2 investigates the lexical items in the framesthat are one link away from the commerce_buyframe.
Commerce_buy inherits from getting, andhas no subframes.
The lexical items of the gettingframe are listed.
The lexical items of the gettingframe are: acquire, gain, get, obtain, secure.
Foreach entry, WordNet is consulted as a first pruningmechanism.
This results in the following:?
Acquire: get?
Gain: acquire, win?
Get: acquire?
Obtain: get, find, receive, incur?
Secure: no items on the listHow exactly lexical choice determines that getand acquire are possible candidates, while the oth-ers are not (because they aren?t suitable in the con-text in which we use them) is as of yet an openissue.
It is also an open issue whether WordNet isthe most appropriate resource to use for this goal;we must consider other options, such as Thesaurus,etc?In step 3 we investigate the other relations thatFrameNet presents.
To date, we have only investi-gated the ?use relation?.
Other relations availableare the inchoative and causative relations.
At thispoint, it is not entirely clear how those relationswill prove to be of any value to our task.
Thecommerce_buy  frame uses com-merce_goods_transfer, which is also used bycommerce_sell.
We find our frame elements goodsand buyer in the commerce_sell frame as well.Lexical choice concludes that the use of the lexicalitems in this frame might be valuable and repeatsstep 1 on these lexical items.After all 3 steps are completed, we assume ourset of output candidates to be complete.
The set ofoutput candidates is presented to the lexical searchprocess, whose task it is to choose the most appro-priate candidate.
For the example we have beenusing throughout this section, the set of outputcandidates is as follows:?
You can buy a tent at Camping World.?
You can purchase a tent at Camping World.?
You can get a tent at Camping World.?
You can acquire a tent at Camping World.?
Camping World sells tents.As mentioned at the beginning of this section,this example is very simple.
For this reason, onecan definitely argue that the first 4 output possibili-ties could be constructed in much simpler waysthan the method used here, e.g.
by simply takingthe question and making it an affirmative sentencethrough a simple rule.
However, it should bepointed out that the last possibility on the listwould not be covered by this simple method.While user studies would need to provide backupfor this assumption, we feel that possibility 5 is avery good example of natural-sounding output, andthus proves our method to be valuable, even forsimple examples.2.6 Lexical SearchThe set of output candidates for the example abovecontains 5 possibilities.
The main task of the lexi-cal search process is to choose the most optimalcandidate, thus the most natural-sounding candi-date (or at least one of the most natural-soundingcandidates, if more than one candidate fits that cri-terion).
There are a number of directions we cantake for this implementation.One option is to implement a rule-based system.Every output candidate is matched against therules, and the most appropriate one comes out atthe top.
Problems with rule-based systems arewell-known: they must be handcrafted, which isvery time-consuming, constructing the rule basesuch that the desired rules fire in the desired cir-cumstances is somewhat of a ?black?
art, and ofcourse a rule base is highly domain-dependent.Extending and maintaining it is also a laboriouseffort.Next we can look at a corpus-based technique.One suggestion is to construct a language model ofthe corpus data, and use this model to statistically64determine the most suitable candidate.
Langkilde(2000) uses this approach.
However, the mainproblem here is that one needs a large corpus in thedomain of the application.
Rambow (2001) agreesthat most often, no suitable corpora are availablefor dialog system development.Another possibility is to use machine learning totrain the microplanner.
Walker et al (2002) usethis approach in the SPOT sentence planner.
Theirranker?s main purpose is to choose between differ-ent aggregation possibilities.
The authors suggestthat many generation problems can successfully betreated as ranking problems.
The advantage of thisapproach is that no domain-dependent hand-craftedrules need to be constructed, and no existence of acorpus is needed.Our current research idea is somewhat related tooption two.
A relatively small domain-independentcorpus of spoken dialogue is semi-automaticallylabeled with frames and semantic roles.
For eachframe, all the occurrences in the corpus are orderedaccording to their frequency for each separate va-lence pattern.
This model is then used as a com-parator for all output candidates, and the mostoptimal one (most frequent one) will be selected.This approach is currently not implemented; fur-ther work needs to determine the viability of theapproach.Independent of the method used to find the mostsuitable candidate, the output must be packaged upto be sent to the surface realizer.
The XLE systemexpects a fairly detailed syntactic description of theutterance?s argument structure.
We construct thisthrough the use of FrameNet and its valence pat-tern information.
In returning to our example, let?sassume the selected candidate is ?Camping Worldsells tents.?
Its meaning representation is as fol-lows:Figure 4.
?Camping World sells tents.
?FrameNet provides an overview of the frameelements a given frame requires (?core elements?
)and those that are optional (?peripheral elements?
).For the commerce_sell frame, the two coreelements are Goods and Seller.
It also provides anoverview of the valence patterns that were found inthe annotated sentences for this frame.
FrameNetdoes not include frequency information for eachannotation.
We thus need to pick a valence patternat random.
One way of doing this is to find apattern that includes all (both) frame elements inour utterance, and then use the (non-statistical)frequency information.
Figure 5 shows that, for ourexample above, this results in:FE_Seller sell FE_goodsWith the following syntactic pattern:NP.Ext sell NP.ObjNo.
Annotated PatternsGoods                 Seller3 -- NP.Ext2 NP.Comp NP.Ext27 NP --4 NP.Ext PP[by].Comp27 NP.Obj NP.ExtFigure 5.
Valence Patterns ?commerce_sell?Thus our output to the surface realizer indicatesthat the seller frame element fills the subject roleand consists of an NP, while the goods frameelement fills the object role and consists of an NP.Given this syntactic pattern information that wegather from FrameNet, we are able to construct anF-structure that is suitable as the input to thesurface realizer.3 Related WorkTo date, only a limited amount of research hasdealt with deep linguistically-based natural lan-guage generation for dialog systems.
Theune(2003) presents an extensive overview of differentNLG methods and systems.
A number of stochas-tic-based generation efforts have been undertakenin recent years.
These generators generally consistof an architecture similar to ours, in which first aset of possible candidates is constructed, followedby a decision process to choose the most appropri-ate output.
Some examples are the Nitrogen system(Langkilde and Knight, 1998) and the SPoT train-able sentence planner (Walker et al, 2002).4 Outlook and Future WorkWe propose a novel approach to lexicalization inNLG in order to generate natural-sounding speechin a portable environment.
The use of existingEvent: sellFrame: commerce_sellSeller: Camping WorldGoods: tents65lexical resources allows a system to be more port-able across subject domains and languages, as longas those resources are available for the targeteddomains and languages.
FrameNet in particularallows us to generate multiple possibilities of natu-ral-sounding output while WordNet helps in a firststep to prune this set.
FrameNet is further appliedon an existing corpus to help with the final deci-sion on choosing the most optimal candidateamong the presented possibilities.
The valence pat-tern information in FrameNet helps constructingthe detailed syntactic pattern required by the sur-face realizer.A number of issues need further consideration,including the following:?
lexical choice: investigation of semantic dis-tances (step 2 of algorithm), use of WordNetand/or other resources for first-step pruning.?
lexical search: develop initial research ideasfurther and implement?
a user study to assess whether the goals ofnatural-sounding output and portability havesuccessfully been fulfilled.Furthermore, for this generator to be used in areal-life environment, the entire dialog systemmust be developed; for our research purposes, wehave left out the construction of a semantic parser,the dialog manager, and an appropriate domainmodel.
We have also not focused on the develop-ment of the aggregation and referring expressiongeneration subtasks in the microplanner.ReferencesBaker, Collin F. and Charles J. Fillmore and John B.Lowe.
1998.
The Berkeley FrameNet project.
In Pro-ceedings of the COLING-ACL, Montreal, Canada.Dale, Robert and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the genera-tion of referring expressions.
Cognitive Science18:233-263.Fellbaum, Christiane.
1998.
A Semantic Network ofEnglish: The Mother of All WordNets.
In Computersand the Humanities, Kluwer, The Netherlands, 32:209-220.Fillmore, Charles J. and Christopher R. Johnson andMiriam R.L.
Petruck.
2003.
Background to Frame-Net.
In International Journal of Lexicography.
Vol.16 No.
3.
2003.
Oxford University Press.
Oxford,UK.Fillmore, Charles J.
1985.
Frames and the semantics ofunderstanding.
In Quaderni di Semantica, Vol.
6.2:222-254.Oberlander, Jon.
1998.
Do the Right Thing?
but Ex-pect the Unexpected.
Computational Linguistics.Volume 24, Number 3.
September 1998, pp.
501-507.
The MIT Press, Cambridge, MA.Shemtov, Hadar.
1997.
Ambiguity Management inNatural Language Generation, PhD Thesis, Stanford.Kaplan, R. M. and J. Wedekind.
2000.
LFG generationproduces context-free languages.
In Proceedings ofCOLING-2000, Saarbruecken, pp.
297-302.Langkilde, Irene.
2000.
Forest-based Statistical Sen-tence Generation.
In Proceedings of the NorthAmerican Meeting of the Association for Computa-tional Linguistics (NAACL), 2000.Langkilde, Irene and Kevin Knight.
1998.
Generationthat Exploits Corpus-Based Statistical Knowledge.
InProceedings of Coling-ACL 1998.
Montr?al, Canada.Rambow, Owen, 2001.
Corpus-based Methods in Natu-ral Language Generation: Friend or Foe?
Invited talkat the European Workshop for Natural LanguageGeneration, Toulouse, France.Reiter, Ehud and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.
Cambridge, UK.Theune, Mari?t.
2000.
From data to speech: languagegeneration in context.
Ph.D. thesis, Eindhoven Uni-versity of Technology.Theune, Mari?t.
2003.
Natural Language Generation forDialogue: System Survey.
University of Twente.Twente, the Netherlands.Walker, Marilyn and Owen Rambow and Monica Ro-gati.
2002.
Training a Sentence Planner for SpokenDialogue Using Boosting.
Computer Speech andLanguage, Special Issue on Spoken Language Gen-eration, July 2002.Zue, Victor.
1997.
Conversational Interfaces: Advancesand Challenges.
Keynote in Proceedings of Eu-rospeech 1997.
Rhodes, Greece.66
