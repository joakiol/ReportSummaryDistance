First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335?339,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUWashington: Negation Resolution using Machine Learning MethodsJames Paul WhiteUniversity of WashingtonDepartment of Linguistics, Box 354340Seattle, WA 98195, USAjimwhite@uw.eduAbstractThis  paper  reports  on  a  simple  system  forresolving the scope of negation in the closedtrack of  the *SEM 2012 Shared Task.
Cuedetection  is  performed  using  regularexpression  rules  extracted  from  the  trainingdata.
Both  scope  tokens  and  negated  eventtokens  are  resolved  using  a  ConditionalRandom  Field  (CRF)  sequence  tagger  ?namely  the  SimpleTagger  library  in  theMALLET machine learning toolkit.
The fullnegation  F1 score  obtained  for  the  taskevaluation is 48.09% (P=74.02%, R=35.61%)which ranks this system fourth among the sixsubmitted for the closed track.1 IntroductionResolving the scope of negation is an interestingarea of research for Natural Language Processing(NLP) systems because many such systems haveused methods that are insensitive to polarity.
As aresult  it  is  fairly  common to have  a system thattreats ?X does Y?
and ?X does not Y?
as havingthe same, or very nearly the same, meaning1.
Afew  application  areas  that  have  been  addressingthis issue recently are in sentiment analysis,  bio?medical  NLP,  and  recognition  of  textual  entail?ment.
Sentiment analysis systems are frequentlyused in corporate and product marketing, call cen?ter quality control, and within ?recommender?
sys?tems which are all contexts where it is important torecognize that ?X does like Y?
is contrary to ?Xdoes not like Y?.
Similarly in biomedical text such1A one token difference between the strings surely indicatingat least an inexact match.as research papers and abstracts, diagnostic proce?dure reports, and medical records it is important todifferentiate between statements about what is thecase and what is not the case.The *SEM 2012 Shared Task is actually two re?lated tasks run in parallel.
The one this system wasdeveloped for is the identification of three featuresof  negation:  the  cue,  the  scope,  and  the  factualnegated event (if any).
The other task is concernedwith the focus of negation.
Detailed description ofboth subtasks, including definition of the relevantconcepts  and  terminology  (negation,  cue,  scope,event, and focus) appears in this volume (Moranteand Blanco, 2012).
Roser Morante  and EduardoBlanco  describe  the  corpora  provided  to  partici?pants with numbers and examples,  methods usedused to process the data, and briefly describes eachparticipant and analyzes the overall results.Annotation of the corpus was undertaken at theUniversity of Antwerp and was performed on sev?eral Sherlock Holmes works of fiction written bySir Arthur Conan Doyle.
The corpus includes allsentences from the original text, not just those em?ploying  negation.
Roser  Morante  and  WalterDaelemans  provide  a  thorough  explanation  ofthose gold annotations of negation cue, scope, andnegated  event  (if  any)  (Morante  and Daelemans,2012).
Their paper explains the motivations for theparticular annotation decisions and describes in de?tail the guidelines, including many examples.2 Related WorkRecognition of phrases containing negation, partic?ularly in the medical domain, using regular expres?sions has been described using several different ap?proaches.
Systems such as Negfinder  (Mutalik et335al,  2001) and NegEx  (Chapman et  al,  2001) usemanually constructed rules to extract phrases fromtext and classify them as to whether they containan expression of negation.
Rokach et alevaluateseveral  methods  and  show their  highest  level  ofperformance (an F1 of 95.9 ?
1.9%) by using cas?caded decision trees of regular expressions learnedfrom labelled narrative medical reports (Rokach etal, 2008).Those systems perform a different function thanthat  required for this task though.
They classifyphrases  extracted  from  plain  text  as  to  whetherthey contain negation or not, while the requirementof this shared task for negation cue detection is toidentify the particular token(s) or part of a tokenthat signals the presence of negation.
Furthermore,those systems only identify the scope of negationat the level of phrasal constituents, which is differ?ent than what is required for this task in which thescopes are not necessarily contiguous.Conditional Random Field (CRF) sequence tag?gers have been successfully applied to many scoperesolution problems,  including those of negation.The  NegScope  system  (Agarwal  and  Yu,  2010)trains a CRF sequence tagger on labelled data toidentify both the cue and scope of negation.
How?ever, that system only recognizes a whole word asa cue and does not recognize nor generalize nega?tion cues which are affixes.
There are also systemsthat  use  CRF  sequence  taggers  for  detection  ofhedge scopes (Tang et al 2010, Zhao et al 2010).Morante and Daelemans describe a method for im?proving  resolution  of  the  scope  of  negation  bycombining IGTREE, CRF, and Support Vector Ma?chines (SVM) (Morante and Daelemans, 2009).3 System DescriptionThis system is implemented as a three stage cas?cade  with  the  output  from each  of  the  first  twostages  included as  input  to the subsequent  stage.The stages are ordered as cue detection, scope de?tection,  and  finally  negated  event  detection.
Theformat of the inputs and outputs for each stage usethe shared task?s  CoNLL?style file  format.
Thatsimplifies  the  use  of  the  supplied  gold?standarddata for training of each stage separately.Because  this  system  was  designed  for  theclosed track of the shared task, it makes minimallanguage?specific assumptions and learns (nearly)all  language?specific  rules from the gold?labelledtraining data (which includes the development setfor the final system).The CRF sequence tagger used by the system isthat implemented in the SimpleTagger class of theMALLET toolkit, which is a Java library distrib?uted under the Common Public License2.The system is implemented in the Groovy pro?gramming  language,  an  agile  and  dynamic  lan?guage for the Java Virtual Machine3.
The sourcecode is available under the GNU Public License onGitHub4.3.1 Cue DetectionCues are recognized by four different regular ex?pression rule patterns: affixes (partial token), single(whole)  token,  contiguous  multiple  token,  andgappy  (discontiguous)  multiple  token.
The  rulesare learned by a two pass process.
In the first pass,for each positive example of a negation cue in thetraining data, a rule that matches that example isadded to the prospective rule set.
Then, in the sec?ond pass, the rules are applied to the training dataand the counts of correct and incorrect matches areaccumulated.
Rules that are wrong more often thanthey are right are removed from the set used by thesystem.A further  filtering  of  the  prospective  rules  isdone  in  which  gappy  multiple  token  rules  thatmatch the same word type more than once are re?moved.
Those  prospective  rules  are  created  tomatch cases in the supplied training data where thea repetition has occurred and then encoded by theannotators as a single cue (and thus scope) of nega?tion5.The single token and multiple token rules matchboth the word string feature (ignoring case) and thepart?of?speech (POS) feature of each token.
Andbecause a single token rule might also match a cuethat belongs to a multiple token rule, multiple to?ken rules are checked first.Affix rules are of two types: prefix cues andnon?prefix cues.
The distinction is that while pre?fix cues must match starting at the beginning of theword string, the non?prefix cues may have a suffixfollowing them in the word string that is not part ofthe cue.
Affix rules only match against the word2http://mallet.cs.umass.edu/3http://groovy.codehaus.org/4https://github.com/jimwhite/SEMST20125Such as baskervilles12 174: ?Not a whisper, not a rustle,rose...?
which has a cue annotation of ?Not?
gap ?not?.336string feature of the tokens and are insensitive tothe POS feature.In order to generalize the affix rules, sets are ac?cumulated of both base word strings (the substringfollowing  a  prefix  cue  or  substring  preceding  anon?prefix cue) and suffixes (the substring follow?ing non?prefix cues, if any).
In addition, all otherword strings and lemma strings in the training cor?pus that are at least four characters long are addedto the set of possible base word strings6.
A set ofnegative word  strings is  also  accumulated  in thesecond pass of the rule training to condition againstfalse positive matches for each affix rule.A prefix  cue  rule  will  match  a  token  with  aword string that  starts  with the cue string and isfollowed by any of the strings in the base word set.Similarly  a  suffix  cue  rule  will  match  a  tokenwhose word string contains the cue string precededby a string in the base word set and is either at theend  of  the  string  or  is  followed  by  one  of  thestrings in the suffix string set.
Affix rules, unlikethe other cue?matching rules, also output the stringfor matched base word as the value of the scope forthe matched token.
In any case, if the token?s wordstring is in the negative word string set for the rulethen it will not be matched.Following submission of the system outputs forthe shared tasked I discovered that a hand writtenregular expression rule that filters out the (poten?tial)  cues  detected  for  ?
(be|have)  no  doubt?
and?none the (worse|less)?
was inadvertently includedin  the  system.
Although  those  rules  could  belearned automatically from the training data (andsuch  was  my  intention),  the  system  as  reportedhere does not currently do so.3.2 Negation Scope ResolutionFor  each  cue  detected,  scope  resolution  is  per?formed as a ternary classification of each token inthe sentence as to whether it is part of a cue, part ofa scope, or neither.
The classifier is the CRF se?quence  tagger  implemented  in  the  SimpleTaggerclass of the MALLET toolkit  (McCallum, 2002).Training is performed using the gold?standard dataincluding the gold cues.
The output of the tagger isnot used to determine the scope value of a token in6This ?longer than four character?
rule was manually createdto correct for over?generalization observed in the training data.If the affix rule learner selected this value using the correct/in?correct counts as it does with the other rule parameters thenthis bit of language?specific tweaking would be unnecessary.those cases where an affix rule in the cue detectorhas matched a token and therefore has supplied thematched base word string as the value of the scopefor the token.For features that are computed in terms of thecue  token,  the  first  (lowest  numbered)  tokenmarked as a cue is used when there is more thanone cue token for the scope.Features used by the scope CRF sequence tag?ger are:?
Of the per?token data: word string in low?ercase, lemma string in lowercase, part?of?speech  (POS)  tag,  binary  flag  indicatingwhether the token is a cue, a binary flag in?dicating whether the token is at the edge ofits parent non?terminal node or an internalsibling,  a  binary  flag  indicating  whetherthe token is a cue token, and relative posi?tion to the cue token in number of tokens.?
Of the cue token data:  word string in low?ercase,  lemma  string  in  lowercase,   andPOS tag.?
Of the path through the syntax tree fromthe cue token: an ordered list of the non?terminal labels of each node up the tree tothe lowest common parent, an ordered listof  the  non?terminal  labels  of  each  nodedown the tree  from that  lowest  commonparent, a path relation value consisting ofthe  label  of  the  lowest  common  parentnode  concatenated  with  an  indication  ofthe relative position of the paths to the cueand token in terms of sibling order.3.3 Negated Event ResolutionDetection of the negated event or property is per?formed using the same CRF sequence tagger andfeatures used for scope detection.
The only differ?ence is that the token classification is in terms ofwhether each token in the sentence is part of a fac?tual negated event for each negation cue.3.4 Feature Set SelectionA comparison  of  the  end?to?end  performance  ofthis system using several different sets of per tokenfeature  choices  for  the  scope  and  negated  eventclassifiers is shown in Table 1.
In each case thetraining data is the entire training data and the devdata is the entire dev data supplied by the organiz?ers for this shared task.
The scores are computed337by the evaluation program also supplied by the or?ganizers.
The baseline features are those providedin the data, with the exception of the syntactic treefragment: word string in lowercase, lemma in low?ercase, and POS tag.
The ?set 1?
features are theremainder of the features described in section 3.2,with the exception of those of the path through thesyntax tree from the cue token.
The ?set 2?
fea?tures are the three baseline features plus the threefeatures of the path through the syntax tree fromthe cue token: list of non?terminal labels from cueup to the lowest common parent, lowest commonparent label concatenated with the relative distancein nodes between the siblings, list of non?terminalsfrom the lowest common parent down to the token.The ?system?
feature set is the union of set 1 andset 2, and is the one used by the submitted system.The baseline score is an F1 of 31.5% (P=79.1%,R=19.7%) on the dev data.
Using either feature set1 or 2 results in substantially better performance.They achieve nearly the same score on the dev setwith an F1 of 50?0.5% (P=87?0.2%, R=35?0.3%)in which the difference is that between one case oftrue positive  vs.  false  negative out  of  173.
Thecombination  of  those  feature  sets  is  better  stillthough with an F1 of 54.4% (P=88.3%, R=39.3%).4 ResultsTable 2 presents the scores computed for the sys?tem output on the held?out evaluation data.
The F1for  full  negation  is  48.1%  (P=74%,  R=35.6%),which  is  noticeably  lower  than  that  seen  for  thedev data (54.4%).
That reduction is to be expectedbecause the dev data was used for system tuning.There was also evidence of significant over?fittingto  the  training  data  because  the  F1 for  that  was76.5% (P=92%,  R=65.5%).
The  largest  compo?nent of the fall off in performance is in the recall.The worst performing component of the systemis the negated event detection which has an F1 of54.3% (P=58%,  R=51%) on  the evaluation  data.One contributor to low precision for the negatedevent detector is that the root word of an affix cueis always output as a negated event, bypassing thenegated  event  CRF  sequence  classifier.
In  thecombined training and dev data there is a total of1157 gold cues (and scopes) of which 738 (63.8%)are annotated as having a negated event.
Of the1198  cues  the  system outputs  for  that  data,  188(15.7%) are affix cues, each of which will also beoutput as a negated event.
Therefore it would bereasonable  to  expect  that  approximately  16(27.7%) of the false positives for the negated eventin the evaluation (60) are due to that behavior.Table 1: Comparison of full negation scores for various feature sets.Gold  System  TP FP FN Precision (%) Recall (%) F1 (%)Baseline  (train) 984 1034 382 56 602 87.21 38.82 53.73(dev)  173 164 34 9 139 79.07 19.65 31.48Set 1        (train) 984 1034 524 56 460 90.34 53.25 67.00(dev) 173 164 60 9 113 86.96 34.68 49.59Set 2        (train) 984 1034 666 56 318 92.24 67.68 78.07(dev) 173 164 61 9 112 87.14 35.26 50.21System    (train) 984 1034 644 56 340 92.00 65.45 76.49(dev) 173 164 68 9 105 88.31 39.31 54.40Table 2: System evaluation on held?out data.Gold  System  TP FP FN Precision (%) Recall (%) F1 (%)Cues 264 285 243 33 21 88.04 92.05 90.00Scopes (no cue match) 249 270 158 33 89 82.90 64.26 72.40Scope tokens (no cue match) 1805 1816 1512 304 293 83.26 83.77 83.51Negated (no cue match) 173 154 83 60 80 58.04 50.92 54.25Full negation 264 285 94 33 170 74.02 35.61 48.09Cues B 264 285 243 33 21 85.26 92.05 88.52Scopes B (no cue match) 249 270 158 33 89 59.26 64.26 61.66Negated B (no cue match) 173 154 83 60 80 53.9 50.92 52.37Full negation B 264 285 94 33 170 32.98 35.61 34.243385 ConclusionThis paper describes the system I implemented forthe closed track of the *SEM 2012 Shared Task fornegation cue, scope, and event resolution.
The sys?tem?s performance on the held?out evaluation data,an F1 of  48.09% (P=74.02%, R=35.61%) for thefull  negation,  relative to the other entries for thetask  is  fourth  among  the  six  teams  that  partici?pated.The strongest part of this system is the scope re?solver which performs at a level near that of thebest?performing  systems  in  this  shared  task.
Ithink it is likely that the performance on scope res?olution would be equivalent to them with a betternegation cue detector.
That is supported by the ?nocue match?
version of the scope resolution evalua?tion  for  which  this  system  has  the  highest  F1(72.4%).Clearly the weakest  link is  the  negated  eventdetector.
Since one obvious source of error is thatthe root word extracted when an affix cue is de?tected  is  always  output  as  a  negated  event,  apromising approach for improvement would be toinstead  utilize  that  as  a  feature  for  the  negatedevent?s CRF sequence tagger so that they have achance to be filtered out in non?factual contexts.AcknowledgementsI  want  to  thank  Roser  Morante  and  EduardoBlanco for organizing this task, the reviewers fortheir  thorough and very  helpful  suggestions,  andEmily Bender for her guidance.ReferencesShashank Agarwal and Hong Yu.
2010.
Biomedicalnegation scope detection with conditional randomfields.
Journal of the American Medical Informatics As?sociation, 17(6), 696?701.doi:10.1136/jamia.2010.003228Chapman, W. W., Bridewell, W., Hanbury, P., Cooper,G.
F., & Buchanan, B. G..  2001.
A simple algorithmfor identifying negated findings and diseases in dis?charge summaries.
Journal of Biomedical Informatics,34(5), 301?310.
doi:10.1006/jbin.2001.1029Andrew McCallum.
2002.
MALLET: A MachineLearning for Language Toolkit.
Retrieved fromhttp://mallet.cs.umass.eduRoser Morante and Eduardo Blanco.
2012.
*SEM2012 Shared Task: Resolving the Scope and Focus ofNegation.
Proceedings of the First Joint Conference onLexical and Computational Semantics.
Presented at the*SEM 2012, Montreal, Canada.Roser Morante and Walter Daelemans.
2009.
A Met?alearning Approach to Processing the Scope of Nega?tion.
Proceedings of the Thirteenth Conference on Com?putational Natural Language Learning (CoNLL?2009)(pp.
21?29).
Boulder, Colorado: Association for Com?putational Linguistics.Roser Morante and Walter Daelemans.
2012.
Conan?Doyle?neg: Annotation of negation in Conan Doyle sto?ries.
Proceedings of the Eighth International Confer?ence on Language Resources and Evaluation (LREC).Pradeep G. Mutalik, Aniruddha Deshpande, and PrakashM.
Nadkarni.
2001.
Use of general?purpose negationdetection to augment concept indexing of medical docu?ments: a quantitative study using the UMLS.
Journal ofthe American Medical Informatics Association: JAMIA,8(6), 598?609.Lior Rokach, Roni Romano, and Oded Maimon.
2008.Negation recognition in medical narrative reports.Information Retrieval, 11(6), 499?538.doi:10.1007/s10791?008?9061?0Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,and Shixi Fan.
2010.
A Cascade Method for DetectingHedges and their Scope in Natural Language Text.
Pro?ceedings of the Fourteenth Conference on Computa?tional Natural Language Learning (pp.
13?17).
Upp?sala, Sweden: Association for Computational Linguis?tics.Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong Cheng.2010.
Learning to Detect Hedges and their Scope UsingCRF.
Proceedings of the Fourteenth Conference onComputational Natural Language Learning (pp.
100?105).
Uppsala, Sweden: Association for ComputationalLinguistics.339
