Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 296?305, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsChoosing the Right Words:Characterizing and Reducing Error of the Word Count ApproachH.
Andrew Schwartz,1 Johannes Eichstaedt,1 Lukasz Dziurzynski,1 Eduardo Blanco,2Margaret L. Kern,1 Stephanie Ramones,1 Martin Seligman,1 and Lyle Ungar11University of Pennsylvania2Lymba Corporationhansens@seas.upenn.eduAbstractSocial scientists are increasingly using thevast amount of text available on social me-dia to measure variation in happiness andother psychological states.
Such studies countwords deemed to be indicators of happinessand track how the word frequencies changeacross locations or time.
This word count ap-proach is simple and scalable, yet often picksup false signals, as words can appear in differ-ent contexts and take on different meanings.We characterize the types of errors that occurusing the word count approach, and find lex-ical ambiguity to be the most prevalent.
Wethen show that one can reduce error with asimple refinement to such lexica by automat-ically eliminating highly ambiguous words.The resulting refined lexica improve precisionas measured by human judgments of word oc-currences in Facebook posts.1 IntroductionMassive social media corpora, such as blogs, tweets,and Facebook statuses have recently peaked the in-terest of social scientists.
Compared to traditionalsamples in tens or hundreds, social media samplesizes are orders of magnitude larger, often contain-ing millions or billions of posts or queries.
Such textprovides potential for unobtrusive, inexpensive, andreal-time measurement of psychological states (suchas positive or negative affect) and aspects of sub-jective well-being (such as happiness and engage-ment).
Social scientists have recently begun to usesocial media text in a variety of studies (Cohn etal., 2004; Kramer, 2010; Tausczik and Pennebaker,2010; Kamvar and Harris, 2011; Dodds et al 2011;Golder and Macy, 2011).One of the most popular approaches to estimatepsychological states is by using the word countmethod (Pennebaker et al 2007), where one tracksthe frequency of words that have been judged to beassociated with a given state.
Greater use of suchwords is taken to index the prevalence of the cor-responding state.
For example, the use of the word?happy?
is taken to index positive emotion, and ?an-gry?
to index negative emotion.
The most widelyused tool to carry out such analysis, and the one weinvestigate in this paper, is Pennebaker?s LinguisticInquiry and Word Count, (LIWC) (Pennebaker et al2001; Pennebaker et al 2007).
LIWC, originally de-veloped to analyze writing samples for emotion andcontrol, has grown to include a variety of lexica forlinguistic and psychosocial topics including positiveand negative emotions, pronouns, money, work, andreligion.
The word count approach has high appealto social scientists in need of a tool to approach so-cial media, and although others have been used (see,for example (Gottschalk and Bechtel, 1998; Bollenet al 2010), LIWC?s lexica are generally perceivedas a ?tried-and-tested?
list of words (Miller, 2011).Unfortunately, the word count approach has somedrawbacks when used as indicators for psycholog-ical states.
Words are the unit of measurement, butwords can carry many different meanings dependingon context.
Consider the Facebook posts below con-taining instances of ?play?, a word associated withpositive emotion in LIWC.2961.
so everyone should come to the play tomor-row...2.
Does anyone what type of file i need to convertyoutube videos to play on PS3???3.
Time to go play with Chalk from the EasterBunny!Out of the three instances, only (3) seems to com-municate positive emotion.
In (1), ?play?
is used asa noun rather than the expected verb, while in (2),?play?
is a verb but it is used in a sense that is notdirectly associated with positive emotion.
(1) and(2) demonstrate how lexical ambiguities (i.e.
multi-ple parts-of-speech or word senses) can affect accu-racy of words in a lexicon.
Additionally, even whenappearing as the expected part of speech and wordsense, signal from a word may change due to its con-text, such as being within the scope of a negation asin (4), or describing something desired as in (5).4.
...all work no play :-(5. i sure wish i had about 50 hours a day to playcodOur goal is to characterize the errors of the widelyused word count approach, and show that such lex-ica can be significantly improved by employing anambiguity metric to refine such lexica.
Rather thanwork on a new method of measuring psychologicalstates, we work within the bounds of word count andask how accurate it is and whether we can improveit without sacrificing its simplicity and scalability.We attempt to reduce the erroneous signal ofthe word count approach while maintaining legiti-mate signal simply by refining the lexicon.
In otherwords, we would like to move closer to the goal inFigure 1, by eliminating words that often carry er-roneous signal such as ?play?, and keeping wordswhich often carry the sought-after signal, such as?cheerful?.
The difficulty in doing this is that we donot have the data to tell us which words are mostlikely to carry signal (even if we had such data wewould like to develop a method that could be appliedto any newly created lexica).
Instead we leveragepart-of-speech and word sense data to help us deter-mine which words are lexically ambiguous.Figure 1: The relationship between text expressing posi-tive emotion (POSEMO) and text containing LIWC termsfor POSEMO.Our approach of eliminating ambiguous wordsincreases the precision at the expense of recall, areasonable trade-off in social media where we areworking with millions or even billions of word in-stances.
Additionally, it is minimally-supervised, inthat we do not require training data on human-state;instead we use existing hand-labeled corpora, suchas SemCor (Miller et al 1993), for word sense in-formation.
Not requiring training data also meansour refinement is flexible; it can be applied to mul-tiple domains and lexica, it makes few assumptionsthat might introduce problems of over-fitting, and itis parsimonious in that it merely improves an estab-lished approach.This paper makes two primary contributions: (1)an analysis of the types of errors common for theword count approach (Section 3), and (2) a generalmethod for refining psychosocial lexica based on theambiguity of words (Section 4).
Before describingthese contributions, we discuss related work, mak-ing the case for using social media in social scienceand surveying some work in computational linguis-tics.
We then evaluate both the original LIWC lex-icon and our refinement of it against human judg-ments of expression of positive and negative emo-tions on hand-annotated Facebook posts, and showthe benefit of lexicon refinement for estimating well-being over time for large aggregates of posts.
Fi-nally, we discuss the implications of our work andpossible future directions.2972 BackgroundCompared to traditional approaches in the social sci-ences, large scale analysis of social media is cheap,near real-time, unobtrusive, and gives high cover-age.
We outline these advantages below.Inexpensive Extracting information from sourcessuch Facebook and Twitter is vastly cheaper than themore conventional polling done by companies suchas Gallup ?
and by many social science researchers.Social media data does not require phone calls to bemade or doors to be knocked on.
For example, a rep-resentative survey asking 1,000 people by a leadingpolling company costs to the order of $10,0001.
Incontrast, once the software exists, social media datafrom tens of millions of users can be obtained andanalyzed at a fraction of the cost.Temporal Resolution Much of the attraction ofsocial media stems from the fact that it capturesa written live stream of collective thought.
WhenGoogle relied on search queries to monitor health-seeking behavior to predict influenza epidemics, thereporting lag was a mere day, whereas traditionalCDC surveillance systems take 1-2 weeks to pub-lish their data (Ginsberg et al 2009).
Infrastructurebased on social media and Internet use data allowsreporting and analysis systems with little to no re-porting lag.
Additionally, traditional survey designsare typically only designed to assess psychologicalstates at a given point in time.Unobtrusive Estimation Traditional self-reportsurvey approaches, even those implemented on theweb, suffer from social desirability, priming, andother biases.
For example, Kahneman et al(Kah-neman et al 2006) found that the order in whichquestions are asked on questionnaires can determinehow they are answered.
By looking directly into thesocial worlds, many of these self-report biases canbe avoided.
The traces of human interactions in so-cial media represent the goings-on in their originalecologies of meaning and signification.
This ap-proach diminishes the inferential distance betweenthe context of the phenomena and the context ofmeasurement ?
and thus decreases the room for sys-tematic distortion of signal.1Gallup, Personal correspondence.2.1 The Word Count ApproachAs previously noted, the word count approach ismost often used by social scientists through the toolknown as Linguistic Inquiry and Word Count orLIWC (Pennebaker et al 2007).
The LIWC2007dictionary is composed of almost 4,500 words andword stems organized across one or more word cat-egories, including 406 positive emotion words and499 negative emotion words.
When long form textsare analyzed with LIWC, the program simply re-turns the percentages of words belonging to the dif-ferent analytical categories ?
the simplicity of thisapproach makes it popular with non-technical socialscientists.LIWC?s positive and negative emotion lexica haverecently begun to be used on ?short form?
writing insocial media.
For example, Golder and Macy (2011)used LIWC to study diurnal and seasonal variationin mood in a collection of 400 million Twitter mes-sages.
Kramer (2010) proposed the ?Gross NationalHappiness?
index and Kivran-Swaine and Naaman(2011) examined associations between user expres-sions of positive and negative emotions and the sizeand density of social networks.
A comprehensivereview can be found in Tausczik and Pennebaker(2010).To our knowledge there is only one work whichhas evaluated LIWC?s accuracy over social media.Bantum and Owen (2009) evaluated LIWC on a setof posts to an Internet-based breast cancer supportgroup.
By annotating expression of emotion withinthis text, they were able to produce accuracy figuresof sensitivity (much like recall) and predictive va-lidity (precision).
Sensitivity measured how oftena word (in context) expressing positive or negativeemotion was captured by LIWC.
Predictive validitymeasured how often a word (in context) capturedby LIWC as measuring positive or negative emotionwas indeed expressing positive or negative emotion.While they found a recall of 0.88, the precision wasonly 0.31 ?
that is, only 31% of instances contain-ing words indexed by LIWC actually conveyed theassociated emotion.
We contend that this is a majordrawback for applying LIWC to social media, be-cause while it is not important to catch every expres-sion of emotion out of a million Tweets, it is impor-tant that when something is captured it is an accurate298estimate of the true state.2.2 Related Work in ComputationalLinguisticsResearchers have been exploring the use of lexicathat define the subjective orientation of words fortasks such as sentiment or subjectivity analysis.
Acommon weakly-supervised approach starts with asmall set of sentiment knowledge (seed words as-sociated with a given sentiment) and expands thewords into a large lexicon (Hatzivassiloglou andMcKeown, 1997; Kamps and Marx, 2002; Kim andHovy, 2004; Kanayama and Nasukawa, 2006; Bac-cianella et al 2010).
We take a different approach.Rather than expanding lexica, we start with a largeset of words and refine the set.
The refinement in-creases precision at the cost of recall, which is areasonable exchange when we are looking at mil-lions or even billions of word instances.
Standardapplications of sentiment analysis, such as annotat-ing movie reviews, may not be as inclined to skipinstances, since they want to make predictions foritems which have very few reviews.Another line of work in sentiment analysis hascreated lexicons using supervised learning.
One ofthe first works to do so was by Pang and colleagues(2002), who used data including author ratings ofreviews, such as IMDB movie reviews.
The authorratings become training data for sentiment classifi-cation.
Pang et alshowed that human-created lexi-cons did not perform as well as lexicons based onsimple word statistics over the training data.
In-terestingly, they found that words like ?still?
weremost predictive of positive movie reviews, and thatpunctuation marks of ?!?
and ???
were strong signsof negative movie reviews.
Unfortunately, trainingdata for subjective well-being or happiness is notyet available, preventing the use of such supervisedlearning methods.
Additionally, this work seeks toexperiment within the bounds of what social sci-entists are in fact using (with publications in high-impact venues such as Science).
We thus take a dif-ferent approach, and automatically improve humancreated lexicons.Wiebe and Cardie (2005) generalized the task ofsentiment analysis to that of discovering subjectiv-ity such as ?opinions, emotions, sentiments, specu-lations, evaluations, etc.?.
More recently, Wilson etPOSEMO NEGEMOterm frequency term frequencylike 774,663 hate 167,109love 797,833 miss 158,274good 571,687 bad 151,496friend* 406,568 bore* 140,684happy 384,797 shit* 114,923LOL 370,613 hurt* 98,291well* 284,002 craz* 94,518great 263,113 lost 94,059haha* 240,587 damn* 93,666best 227,381 fuck 90,212better 212,547 stupid* 85,587fun 216,432 kill* 83,593please* 174,597 hell 80,046hope 170,998 fuckin* 79,959thank 161,827 wrong* 70,714Table 1: Most frequent POSEMO and NEGEMO terms inLIWC in the 12.7 million Facebook posts.
?*?
indicates awildcard, so that ?well*?
matches ?wellness?.al.
(2009) contended that the context may neutralizeor change the polarity of the subjective orientationof a word.
It is difficult to determine where conceptsof happiness such as quality of relationships or de-gree of achievement in life fit in with subjectivity.Thus, we do not claim to be measuring subjectivityand instead we use the general term of ?psychologi-cal state?, referring to ?the way something [a person]is with respect to its main attributes?
(Miller, 1993).To the best of our knowledge, while part-of-speech tagging and word sense disambiguation arestaple tasks in the computational linguistics commu-nity, the utility of a lexical ambiguity metric has yetto be explored.3 Annotation and Analysis of Errors fromthe Word Count MethodOne objective of our work is to document and de-scribe how often different types of errors occur whenusing the word count approach on social media.
Todo this, we first judged a sample of 1,000 instancesof LIWC terms occurring in Facebook posts to indi-cate whether they contribute signal towards the as-sociated LIWC category (i.e.
positive emotion).
Wethen took instances that were deemed to carry erro-neous signal and annotated them with a label for the299category agreement instances base ratePOSEMO 0.742 500 .654NEGEMO 0.746 500 .697TOTAL 0.744 1,000 .676random 0.343 - -Table 2: Inter-annotator agreement over 1,000 instancesof LIWC terms in Facebook posts.
Base rate is the aver-age of how often an annotator answered true.type of signal error.
This section describes the pro-cess we used in generating these annotations and theresults we found.3.1 Annotation ProcessAnnotating social media instances of lexica termsprovides insight into how well the word count ap-proach works, and also yields a ?ground truth?
forevaluating our lexicon refinement methods.
We ran-domly selected for labeling a sample of 1,000 sta-tus updates containing words from a given lexicondrawn from a collection of 12.7 million Facebookstatus updates provided by the Cambridge myPer-sonality project (Kosinski and Stillwell, 2012).We used terms from the LIWC positive emotion(POSEMO) and negative emotion (NEGEMO) lex-ica, which are the same lexica used by the works ofKramer (2010), Kivran-Swaine and Naaman (2011),and Golder and Macy (2011).
Table 1 lists themost frequent POSEMO and NEGEMO terms in ourFacebook sample.As mentioned above, we did two types of annota-tions.
First, we judged whether each given instanceof a word conveyed the correct associated type ofemotion.
The second task took a sample of instancesjudged to have incorrect signal and labeled themwith a reason for the error; We refer to this as signalerror type.For the first task, we had three human judges inde-pendently evaluate the 1,000 status update instancesas to whether they were indeed correct signal.
Thequestion the judges were told to answer was ?Doesthe word contribute to the associated psychological-state (POSEMO or NEGEMO) within the sentenceit appears??.
In other words, ?would the sentenceconvey less [positive emotion or negative emotion]without this word??.
Subjective feedback from thejudges indicated that it was often difficult to makea decision, so we used three judges per instance.
Inthe case of conflict between judges, the ?correct?
la-bel for validation of the refined lexicon was definedto be the majority vote.
A sampling of Facebook sta-tuses demonstrates a mixed picture of relevance forthe unrefined LIWC dictionaries:1. has had a very good day (?good?
- POSEMO)2. is so very bored.
(?bore*?
- NEGEMO)3. damn, that octopus is good, lol (?damn?
-NEGEMO)4. thank you for his number (?numb*?
-NEGEMO)5.
I got pranked sooooo bad (?bad?
- NEGEMO)6. don?t be afraid to fail (?afraid?
- NEGEMO)7.
I wish I could .
.
.
and we could all just be happy(?happy?
- POSEMO)Some posts clearly use positive or negative lexiconwords such as (1) and (2).
Curse words can signifynegative emotion or emphasize the opposite state asin (3), which is clearly emphasizing positive emo-tion here.
Example (5) demonstrates the word senseissue we discussed previously.
Words with wild-cards that expand into other words with differentmeanings can be particularly problematic, as the ex-panded word can be far more frequent ?
and verydifferent in meaning ?
from the original word.
Forexample, ?numb*?
matches ?number?
in 4.A different problem occurs when the contextof the word changes its implication for the emo-tional state of the writer.
This can either occurthrough negation such as in (6) where ?afraid?
sig-nals NEGEMO, but is negated with ?don?t?
or thesignal can be changed indirectly through a variety ofwords indicating that the writer desires (and hencelacks) the state, as in (7) where someone is wishingto be ?happy?.Table 2 shows the agreement between an-notators calculated as?i agree(A(i)1 ,A(i)2 ,A(i)3 )1,000 , whereagree(A1, A2, A3) was 1 when all three annota-tions matched and 0 otherwise.
Given the aver-age positive base rate across annotators was 0.676the chance that all three reviewers agree accord-ing to chance (random agreement) is calculated as300category precision instancesPOSEMO 67.9% 500NEGEMO 72.8% 500both 70.4% 1,000Table 4: Accuracy of LIWC POSEMO and NEGEMOlexica over Facebook posts.0.6763+(1?0.676)3 = 0.343, the probability of allthree answering yes plus the probability of all threeanswering no.For the second task, we selected 100 instancesjudged to be incorrect signal from the first task, andlabeled them according to the best reason for themistake.
This task required more linguistic exper-tise and was performed by a single annotator.
La-bels and descriptions are given in Table 3, whichbreaks down the cases into lexical ambiguity, director indirect negation, and other reasons such as thestemming issue (stem plus wildcard expanding intowords indicating a different (or no) emotional state).3.2 Analysis of ErrorsBefore discussing the types of errors we found whenusing the word count approach, we examine LIWC?soverall accuracy on our dataset.
Table 4 shows theprecision broken down for both the positive emotion(POSEMO) and the negative emotion (NEGEMO)lexica.
We see that the precision for NEGEMO isslightly higher than POSEMO, indicating the termsin that category may be more likely to indicate theirassociated state.Although the overall accuracy seems decent, oneshould keep in mind our subjective judgement crite-ria were quite tolerant, allowing any amount of con-tribution of the corresponding signal to be consid-ered accurate.
For example, a salutation like ?HappyNew Year?
was judged to be a correct use of ?happy?to signal POSEMO, even though it clearly does nothave as strong a signal as someone saying ?I feeldeliriously happy?.Frequencies of signal errors are given in Table5.
The most common signal error was wrong wordsense, where the word did not signal emotionalstate and some other sense or definition of the wordwas intended (e.g.
?u feel like ur living in a mu-sic video?
; corresponding to the sense ?to inhabit?rather than the intended sense, ?to have life; becategory label frequencyLexical AmbiguityWrong POS 15Wrong WS 38Signal NegationStrict Negation 16Desiring 6OtherStem Issue 5Other 24Table 5: Frequency of the signal error types.alive?
(Miller, 1993)).
Other common signal errorsinclude strict negation where the word is canceledout by a clear negative quantifier (e.g.
?Don?t beafraid to fail?)
and wrong part of speech where theword is signaling a different part of speech than theemotion (e.g.
?well, we cant afford to go to NYC?
).There were also various other signal error types thatinclude stem issues where the stem matched clearlyunintended words, desiring statuses where the statusis commenting on wanting the emotion instead ofexperiencing it and other less prevalent issues suchas non-English language post, memes, or clear sar-casm.4 Method for Refining LexicaThe idea behind our refinement method is to removewords that are likely to carry erroneous signal aboutthe underlying state or emotion of the person writ-ing the tweet or Facebook post.2 We do so in anindirect fashion, without actually using training dataof which posts are, in fact indicative of positive ornegative emotion.
Instead, we focus on reducing er-rors that are due to lexical ambiguity.
By remov-ing words that are often used with multiple parts ofspeech or multiple senses, we can tilt the balance to-ward precision at some cost in recall (losing somesignal from the ambiguous words).
This makes theword count approach more suitable for use in themassive corpora afforded by social media.4.1 Lexical AmbiguityWe address lexical ambiguity at the levels of bothpart of speech (POS) and word sense.
As a metricof inverse-ambiguity, we determine the probabilitythat a random instance is the most frequent sense(mfs) of the most frequent part of speech (mfp) of2Refinement tool is available at wwbp.org.301category label description examplesLexical AmbiguityWrong POS Not a valid signal because it isthe wrong POSso everyone should come to theplay tomorrow...Wrong WS Not a valid signal because it isthe wrong word sense (includesmetaphorical senses)Does anyone what type of file ineed to convert youtube videosto play on PS3??
?Signal NegationStrict Negation Within the scope of a negation,where there is a clear negativequantifier...all work no play :-(Desiring Within the scope of a desire /wishing for somethingi sure wish i had about 50 hoursa day to play codOtherStem Issue Clearly not intended to bematched with the given stemnumb* for NEGEMO match-ing numberOther Any other issue or difficult toclassifyTable 3: Signal error types.the word, denoted TSP (for top sense probability).Given a wordw, we consider all parts of speech ofw(POS(w)) and all senses for the most frequent partof speech (senses(mfp(w))):pmfp(w) =max[wpos?POS(w)]fp(wpos)?wpos?POS(w)fp(wpos)pmfs(w) =max[wsense?senses(mfp(w))]fs(wsense)?wsense?senses(mfp(w))fs(wsense)TSP (w) = (pmfp(w) ?
pmfs(w))2 (1)Here, fp and fs represent the frequencies of a cer-tain part-of-speech and a certain sense of a word,respectively.
This is the squared-probability that aninstance of w is the top sense ?
the most-frequentpart-of-speech and the most-frequency sense of thatpart-of-speech.
The probability is squared becauseboth the word in the lexicon and the word occurringin context should be the top sense (two independentprobabilities: given an instance of a word in a cor-pus, and another instance of the word in the lexicon,what is the probability that both are the top POSand sense?).
Frequency data is provided for parts-of-speech from the Google N-Grams 2.0 (Lin et al2010) and for word senses from SemCor (Miller etal., 1993).
This aspect of the refinement is inspiredby the most frequent sense heuristic for word sensedisambiguation (McCarthy et al 2004; Yarowsky,1993), in which the sense of a word is chosen with-out regard to the context, but rather is simply basedon the frequencies of senses in corpora.
In our case,we restrict ourselves this way in order for the appli-cation of the lexicon to remain unchanged.For some words, we were unable to find sense fre-quency data.
We decided to keep such terms, onthe assumption that a lack in available frequency in-formation implies that the word is not very ambigu-ous.
Many of these terms include Web speak such as?haha?
or ?lol?, which we believe can carry a strongsignal for positive and negative emotion.Lastly, since TSP is only a metric for the in-verse ambiguity of a word, we must apply a thresh-old to determine which words to keep.
We denotethis threshold as ?, and the description of the refinedlexicon for a category, cat, is below.lex?
(cat) = {w|w ?
cat ?
TSP (w) > ?
}4.2 Handling StemsSome lexica, such as the LIWC dictionary, includeword stems that are intended to match multipleforms of a word.
Stems are marked by the suffix?*?.
LIWC describes the application of stems as fol-lows ?the asterisk, then, denotes the acceptance ofall letters, hyphens, or numbers following its ap-302lex cat prec sizefullPOSEMO 67.9% 500NEGEMO 72.8% 500both 70.4% 1,000lex0.10POSEMO 70.9% 392NEGEMO 71.6% 423both 71.3% 815lex0.50POSEMO 75.7% 239NEGEMO 78.9% 232both 77.3% 471lex0.90POSEMO 72.5% 109NEGEMO 78.1% 128both 75.5% 237Table 6: Precision (prec) and instance subset size (size)of refinements to the LIWC POSEMO and NEGEMO lex-ica with various ?
thresholds (0.10, 0.50, 0.90)pearance.
?3 This presents a problem because, whilethe creators of such lexica obviously intended stemsto match multiple forms of a word, stems also oftenmatch completely different words, such as ?numb*?matching ?number?
or ?won*?
matching ?won?t?.We identified how often unintended matches hap-pen in Section 3.
Finding that the stemming issueswere not the biggest problem, here, we just describehow they fit into our lexical ambiguity metric, ratherthan describe a technique to rid the lexicon of stem-ming problems.
One approach might be to deter-mine how ambiguous a stem is ?
i.e.
determinehow many words, parts-of-speech, and senses a stemcould be expanded into, but this ignores the fact thatthe dictionary creators obviously intended the stemto match multiple words.
Instead, we expand stemsinto all words that they match and replace them intothe lexica.We base our expansion on the actual terms usedin social media.
We find all words matching stemsamong 1 million randomly selected Twitter mes-sages posted over a 6-month period (August 2009- February 2010), and restrict to those occurring atleast 20 times.
Then, each word stem in the lexiconis replaced with the expanded set of matching words.Figure 2: The relationship between precision and sizewhen increasing the TSP threshold (?
).5 EvaluationWe evaluate our refinement by comparing againsthuman judgements of the emotion conveyed bywords in individual posts.
In the case of hu-man judgements, we find that the subset of human-annotated instances matching the refined lexica aremore accurate than the complete set.In section 3 we discussed the method we used tojudge instances of LIWC POSEMO and NEGEMOwords as to whether they contributed the associatedaffect.
Each of the 1,000 instances in our evaluationcorpus were judged three times such that the major-ity was taken as truth.
In order to validate our refinedlexica, we find the accuracy (precision) of the subsetof instances which contain the refined lexica terms.Table 6 shows the change in precision when us-ing the refined lexica.
size represents the number ofinstances from the full evaluation corpus matchingwords in the refined lexica.
One can see that ini-tially precision increase as the size becomes smaller.This is more clearly seen in Figure 2.
As discussedin the method section, our goal with the refine-ment is improving precision, making lexica moresuitable to applications over massive social mediawhere one can more readily afford to skip instances(i.e.
smaller size) in order to achieve more accu-racy.
Still, removing more ambiguous words does3?How it works?
: http://www.liwc.net/howliwcworks.php303not guarantee improved precision at capturing theintended psychological state; it is possible that thatall senses of an ambiguous word do in fact carry in-tended signal or that the intended sense a low ambi-guity word is not the most frequent.Our maximum precision occurs with a thresholdof 0.50, where things somewhat level-out.
This rep-resents approximately a 23% reduction in error, andverifies that we can increase precision through theautomatic lexicon refinement based on lexical ambi-guity.6 ConclusionsSocial scientists and other researchers are startingto measure psychological states such as happinessthrough text in Facebook and Twitter.
We haveshown that the widely used word count method,where one simply counts occurrences of positive ornegative words, can often produce noisy and inaccu-rate estimates of expressions of psychological states.We characterized and measured the frequency ofdifferent types of errors that occur using this ap-proach, and found that when counting words withoutconsidering context, it is lexical ambiguities (unin-tended parts-of-speech or word senses) which causethe most errors.
We proposed a method for refin-ing lexica by removing those words most likely tobe ambiguous, and showed that we can significantlyreduce error as measured by human judgements.AcknowledgmentsSupport for this research was provided by theRobert Wood Johnson Foundation?s Pioneer Portfo-lio, through a grant to Martin Seligman, ?ExploringConcepts of Positive Health?.
We thank the review-ers for their constructive and insightful comments.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10),Valletta, Malta, may.
European Language ResourcesAssociation (ELRA).Erin O.C.
Bantum and J.E.
Owen.
2009.
Evaluating thevalidity of computerized content analysis programs foridentification of emotional expression in cancer narra-tives.
Psychological assessment, 21(1):79.Johan Bollen, Huina Mao, and Xiao-Jun Zeng.
2010.Twitter mood predicts the stock market.
Computer andInformation Science, 1010:1?8.Michael A. Cohn, M.R.
Mehl, and J.W.
Pennebaker.2004.
Linguistic markers of psychological change sur-rounding september 11, 2001.
Psychological Science,15(10):687.Peter Sheridan Dodds, Kameron Decker Harris, Isabel MKloumann, Catherine A Bliss, and Christopher MDanforth.
2011.
Temporal patterns of happiness andinformation in a global social network: Hedonomet-rics and twitter.
Diversity, page 26.Jeremy Ginsberg, M.H.
Mohebbi, R.S.
Patel, L. Bram-mer, M.S.
Smolinski, L. Brilliant, et al2009.
De-tecting influenza epidemics using search engine querydata.
Nature, 457(7232):1012?4.Scott A. Golder and M.W.
Macy.
2011.
Diurnal andseasonal mood vary with work, sleep, and daylengthacross diverse cultures.
Science, 333(6051):1878?1881.Louis A. Gottschalk and RJ Bechtel.
1998.
Psychiatriccontent analysis and diagnosis (pcad2000).Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Annual Meeting of the Association for Com-putational Linguistics, pages 174?181.Daniel Kahneman, A.B.
Krueger, D. Schkade,N.
Schwarz, and A.A. Stone.
2006.
Would yoube happier if you were richer?
a focusing illusion.Science, 312(5782):1908.Jaap Kamps and Maarten Marx.
2002.
Words with atti-tude.
In 1st International WordNet Conference, pages332?341, Mysore, India.Sepandar D. Kamvar and J. Harris.
2011.
We feel fineand searching the emotional web.
In Proceedingsof the fourth ACM international conference on Websearch and data mining, pages 117?126.
ACM.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.
Fullyautomatic lexicon expansion for domain-oriented sen-timent analysis.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?06, pages 355?363, Stroudsburg,PA, USA.
Association for Computational Linguistics.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Funda Kivran-Swaine and M. Naaman.
2011.
Networkproperties and social sharing of emotions in socialawareness streams.
In Proceedings of the ACM 2011304conference on Computer supported cooperative work,pages 379?382.
ACM.Michal.
Kosinski and David J. Stillwell.
2012.mypersonality research wiki.
mypersonality project.http://www.mypersonality.org/wiki/.Adam D.I.
Kramer.
2010.
An unobtrusive behavioralmodel of gross national happiness.
In Proceedings ofthe 28th international conference on Human factors incomputing systems, pages 287?290.
ACM.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word sensesin untagged text.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguistics(ACL?04), Main Volume, pages 279?286, Barcelona,Spain, July.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T Bunker.
1993.
A semantic concordance.
InProceedings of the ARPA Workshop on Human Lan-guage Technology, pages 303?308.
Morgan Kaufman.George A. Miller.
1993.
Five papers on wordnet.
Tech-nical Report, Princeton University.Greg Miller.
2011.
Social scientists wade into the tweetstream.
Science, 333(6051):1814?1815.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 79?86.James W Pennebaker, Martha E Francis, and Roger JBooth.
2001.
Linguistic inquiry and word count:Liwc 2001.
Word Journal Of The International Lin-guistic Association.James W. Pennebaker, C.K.
Chung, M. Ireland, A. Gon-zales, and R.J. Booth.
2007.
The development andpsychometric properties of liwc2007.
Austin, TX,LIWC.
Net.Yla R. Tausczik and J.W.
Pennebaker.
2010.
The psy-chological meaning of words: Liwc and computerizedtext analysis methods.
Journal of Language and So-cial Psychology, 29(1):24.Janyce Wiebe and Claire Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
lan-guage resources and evaluation.
In Language Re-sources and Evaluation.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational Linguistics, 35:399?433, September.David Yarowsky.
1993.
One sense per collocation.
InProceedings of the workshop on Human LanguageTechnology, HLT ?93, pages 266?271, Stroudsburg,PA, USA.
Association for Computational Linguistics.305
