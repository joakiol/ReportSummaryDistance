Proceedings of the ACL 2010 Conference Short Papers, pages 342?347,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSimultaneous Tokenization and Part-of-Speech Tagging for Arabicwithout a Morphological AnalyzerSeth KulickLinguistic Data ConsortiumUniversity of Pennsylvaniaskulick@seas.upenn.eduAbstractWe describe an approach to simultaneoustokenization and part-of-speech taggingthat is based on separating the closed andopen-class items, and focusing on the like-lihood of the possible stems of the open-class words.
By encoding some basic lin-guistic information, the machine learningtask is simplified, while achieving state-of-the-art tokenization results and compet-itive POS results, although with a reducedtag set and some evaluation difficulties.1 IntroductionResearch on the problem of morphological disam-biguation of Arabic has noted that techniques de-veloped for lexical disambiguation in English donot easily transfer over, since the affixation presentin Arabic creates a very different tag set than forEnglish, in terms of the number and complexityof tags.
In additional to inflectional morphology,the POS tags encode more complex tokenizationsequences, such as preposition + noun or noun +possessive pronoun.One approach taken to this problem is to usea morphological analyzer such as BAMA-v2.0(Buckwalter, 2004) or SAMA-v3.1 (Maamouri etal., 2009c)1, which generates a list of all possi-ble morphological analyses for a given token.
Ma-chine learning approaches can model separate as-pects of a solution (e.g., ?has a pronominal clitic?
)and then combine them to select the most appro-priate solution from among this list.
A benefitof this approach is that by picking a single solu-tion from the morphological analyzer, the part-of-speech and tokenization comes as a unit (Habashand Rambow, 2005; Roth et al, 2008).1SAMA-v3.1 is an updated version of BAMA, with manysignificant differences in analysis.In contrast, other approaches have used apipelined approach, with separate models to firstdo tokenization and then part-of-speech tagging(Diab et al, 2007; Diab, 2009).
While these ap-proaches have somewhat lower performance thanthe joint approach, they have the advantage thatthey do not rely on the presence of a full-blownmorphological analyzer, which may not always beavailable or appropriate as the data shifts to differ-ent genres or Arabic dialects.In this work we present a novel approach tothis problem that allows us to do simultaneous to-kenization and core part-of-speech tagging witha simple classifier, without using a full-blownmorphological analyzer.
We distinguish betweenclosed-class and open-class categories of words,and encode regular expressions that express themorphological patterns for the former, and simpleregular expressions for the latter that provide onlythe generic templates for affixation.
We find that asimple baseline for the closed-class words alreadyworks very well, and for the open-class words weclassify only the possible stems for all such ex-pressions.
This is however sufficient for tokeniza-tion and core POS tagging, since the stem identi-fies the appropriate regular expression, which thenin turn makes explicit, simultaneously, the tok-enization and part-of-speech information.2 BackgroundThe Arabic Treebank (ATB) contains a full mor-phological analysis of each ?source token?, awhitespace/punctuation-delimited string from thesource text.
The SAMA analysis includes fourfields, as shown in the first part of Table 1.2 TEXTis the actual source token text, to be analyzed.
VOCis the vocalized form, including diacritics.
EachVOC segment has associated with it a POS tag and2This is the analysis for one particular instance of ktbh.The same source token may receive another analysis else-where in the treebank.342ATB analysis for one source token:TEXT: ktbhVOC: kutub u huPOS: NOUN CASE NOM POSS PRON 3MSGLOSS: books [def.nom.]
its/hisResults in two ATB tree tokens:TEXT: ktb hVOC: kutub+u huPOS: NOUN+CASE NOM POSS PRON 3MSCurrent work recovers:TEXT: ktb hPOS: NOA POSS PRONTable 1: Example analysis of one source tokenNOUN, ADJ, NOUN.VN, NOAADJ.VN, NOUN NUM, ADJ NUM, (Noun orNOUN QUANT, ADJ COMP, ABBREV Adjective)IV, IV PASS IVPV, PV PASS PVIVSUFF DO, PVSUFF DO OBJ PRONTable 2: Collapsing of ATB core tags into reducedcore tagsGLOSS.
While ?tokenization?
can be done in dif-ferent ways on top of this analysis, the ATB splitsthe VOC/POS/GLOSS segments up based on thePOS tags to form the ?tree tokens?
necessary fortreebanking.
As shown in the second part of Table1, the first two segments remain together as onetree token, and the pronoun is separated as a sep-arate tree token.
In addition, the input TEXT isseparated among the two tree tokens.3Each tree token?s POS tag therefore consistsof what can be considered an ?ATB core tag?,together with inflectional material (case, gender,number).
For example, in Table 1, the ?core tag?of the first tree token is NOUN.
In this work, we aimto recover the separation of a source token TEXTinto the corresponding separate tree token TEXTs,together with a ?reduced core tag?
for each tree to-ken.
By ?reduced core tag?, we mean an ATB coretag that has been reduced in two ways:(1) All inflectional material [infl] is strippedoff six ATB core tags: PRON[infl], POSS PRON[infl],DEM[infl], [IV|PV|CV]SUFF DO[infl](2) Collapsing of some ATB core tags, as listedin Table 2.These two steps result in a total of 40 reducedcore tags, and each tree token has exactly one suchreduced core tag.
We work with the ATB3-v3.2 re-lease of the ATB (Maamouri et al, 2009b), which3See (Kulick et al, 2010) for a detailed discussion ofhow this splitting is done and how the tree token TEXT field(called INPUT STRING in the ATB releases) is created.NOA 173938 PART 288PREP 49894 RESTRIC PART 237PUNC 41398 DET 215NOUN PROP 29423 RC PART 192CONJ 28257 FOCUS PART 191PV 16669 TYPO 188IV 15361 INTERROG PART 187POSS PRON 9830 INTERROG ADV 169SUB CONJ 8200 INTERROG PRON 112PRON 6995 CV 106REL PRON 5647 VOC PART 74DEM 3673 VERB 62OBJ PRON 2812 JUS PART 56NEG PART 2649 FOREIGN 46PSEUDO VERB 1505 DIALECT 41FUT PART 1099 INTERJ 37ADV 1058 EMPHATIC PART 19VERB PART 824 CVSUFF DO 15REL ADV 414 GRAMMAR PROB 4CONNEC PART 405 LATIN 1Table 3: The 40 ?reduced core tags?, and their fre-quencies in ATB3-v3.2.
The total count is 402291,which is the number of tree tokens in ATB3-v3.2.has 339710 source tokens and 402291 tree tokens,where the latter are derived from the former as dis-cussed above.
Table 3 lists the 40 reduced tagswe use, and their frequency among the ATB3-v3.2tree tokens.3 Description of ApproachGiven a source token, we wish to recover (1) thetree tokens (which amounts to recovering the ATBtokenization), and (2) the reduced core POS tag foreach tree token.
For example, in Table 1, given theinput source token TEXT ktbh, we wish to recoverthe tree tokens ktb/NOA and h/POSS PRON.As mentioned in the introduction, we use reg-ular expressions that encode all the tokenizationand POS tag possibilities.
Each ?group?
(substringunit) in a regular expression (regex) is assigned aninternal name, and a list is maintained of the pos-sible reduced core POS tags that can occur withthat regex group.
It is possible, and indeed usu-ally the case for groups representing affixes, thatmore than one such POS tag is possible.
How-ever, it is crucial for our approach that while somegiven source token TEXT may match many regu-lar expressions (regexes), when the POS tag is alsotaken into account, there can be only one matchamong all the (open or closed-class) regexes.
Wesay a source token ?pos-matches?
a regex if theTEXT matches and POS tags match, and ?text-matches?
if the TEXT matches the regex regard-less of the POS.
During training, the pos-matching343(REGEX #1) [w|f]lmw: [PART, CONJ, SUB CONJ, PREP]f: [CONJ, SUB CONJ, CONNEC PART, RC PART]lm: [NEG PART](REGEX #2) [w|f]lmw: and f: same as abovelm: [REL ADV,INTERROG ADV]Figure 1: Two sample closed-class regexesregex for a source token TEXT is stored as thegold solution for closed-class patterns, or used tocreate the gold label for the open-class classifier.We consider the open-class tags in Table 3 to be:NOUN PROP, NOA, IV, CV, PV, VERB.
A sourcetoken is considered to have an open-class solutionif any of the tree tokens in that solution have anopen-class tag.
For example, ktbh in Table 1 hasan open-class solution because one of the tree to-kens has an open-class tag (NOA), even though theother is closed-class (POSS PRON).We encode the possible solutions for closed-class source tokens using the lists in the ATB mor-phological guidelines (Maamouri et al, 2009a).For example, Figure 1 shows two of the closed-class regexes.
The text wlm can text-match eitherREGEX #1 or #2, but when the POS tag for lm istaken into account, only one can pos-match.
Wereturn to the closed-class regexes in Section 4.We also encode regular expression for the open-class source tokens, but these are simply generictemplates expressing the usual affix possibilities,such as:[wf] [blk] stem NOA poss pronounwhere there is no list of possible strings forstem_NOA, but which instead can match any-thing.
While all parts except for the stem are op-tional, we do not make such parts optional in asingle expression.
Instead, we multiple out thepossibilities into different expressions with differ-ent parts (e.g., [wf]) being obligatory).
The reasonfor this is that we give different names to the stemin each case, and this is the basis of the featuresfor the classifier.
As with the closed-class regexes,we associate a list of possible POS tags for eachnamed group within a regular expression.
Herethe stem NOA group can only have the tag NOA.We create features for a classifier for the open-class words as follows.
Each word is run throughall of the open-class regular expressions.
For eachexpression that text-matches, we make a featurewhich is the name of the stem part of the regularexpression, along with the characters that matchthe stem.
The stem name encodes whether thereis a prefix or suffix, but does not include a POStag.
However, the source token pos-matches ex-actly one of the regular expressions, and the postag for the stem is appended to the named stem forthat expression to form the gold label for trainingand the target for testing.For example, Table 4 lists the matching regularexpression for three words.
The first, yjry, text-matches the generic regular expressions for anystring/NOA, any string/IV, etc.
These are sum-marized in one listing, yjry/all.
The name of thestem for all these expressions is the same, juststem, and so they all give rise to the same feature,stem=yjry.
It also matches the expression fora NOA with a possessive pronoun4, and in thiscase the stem name in the regular expression isstem_spp (which stands for ?stem with a pos-sessive pronoun suffix?
), and this gives rise to thefeature stem_spp=yjr.
Similarly, for wAfAdtthe stem of the second expression has the namep_stem, for a prefix.
The third example showsthe different stem names that occur when there areboth prefix and suffix possibilities.
For each exam-ple, there is exactly one regex that not only text-matches, but also pos-matches.
The combinationof the stem name in these cases together with thegold tag forms the gold label, as indicated in col-umn 3.Therefore, for each source token TEXT, thefeatures include the ones arising from the namedstems of all the regexes that text-match that TEXT,as shown in column 4, and the gold label is theappropriate stem name together with the POStag, as shown in column 3.
We also includesome typical features for each stem, such as firstand last two letters of each stem, etc.
For ex-ample, wAfAdt would also have the featuresstem_fl=w, p_stem_fl=A, indicating that thefirst letter of stem is w and the first letter ofp_stem is A.
We also extract a list of propernouns from SAMA-v3.1 as a temporary proxy fora named entity list, and include a feature for astem if that stem is in the list (stem_in_list,p_stem_in_list, etc.
)We do not model separate classifiers for prefixpossibilities.
There is a dependency between the4The regex listed is slightly simplified.
It actually con-tains a reference to the list of all possessive pronouns, notjust y.344source TEXT text-matching regular expressions gold label featureyjry yjry/all (happens) stem:IV stem=yjryyjr/NOA+y/POSS PRON stem spp=yjrwAfAdt wAfAdt/all stem=wAfAdtw + AfAdt/all (and+reported) p stem:PV p stem=AfAdtlAstyDAHhm lAstyDAHhm/all stem=lAstyDAHhml/PREP + AstyDAHhm/NOA p stem=AstyDAHhml/PREP + AstyDAH/NOA + hm/POSS PRON p stem spp:NOA p stem spp=AstyDAHfor + request for clarification + theirlAstyDAH/NOA + hm/POSS PRON stem spp=lAstyDAHlAstyDAH/IV,PV,CV + hm/OBJ PRON stem svop=lAstyDAHl/PREP,JUS PART + AstyDAH/IV,PV,CV + p stem svop=AstyDAHhm/OBJ PRONTable 4: Example features and gold labels for three words.
Each text-matching regex gives rise to onefeature shown in column 4, based on the stem of that regular expression.
A p before a stem means thatit has a prefix, spp after means that it has a possessive pronouns suffix, and svop means that it hasa (verbal) object pronoun suffix.
?all?
in the matching regular expression is shorthand for text-matchingall the corresponding regular expressions with NOA, IV, etc.
For each word, exactly one regex alsopos-matches, which results in the gold label, shown in column 3.possibility of a prefix and the likelihood of the re-maining stem, and so we focus on the likelihood ofthe possible stems, where the open-class regexesenumerate the possible stems.
A gold label to-gether with the source token TEXT maps back toa single regex, and so for a given label, the TEXTis parsed by that regular expression, resulting in atokenization along with list of possible POS tagsfor each affix group in the regex.5During training and testing, we run each wordthrough all the open and closed regexes.
Text-matches for an open-class regex give rise to fea-tures as just described.
Also, if the word matchesany closed-class regex, it receives the featureMATCHES CLOSED.
During training, if the cor-rect match for the word is one of the closed-classexpressions, then the gold label is CLOSED.
Theclassifier is used only to get solutions for the open-class words, although we wish to give the classifierall the words for the sentence.
The cross-productof the stem name and (open-class) reduced corePOS tags, plus the CLOSED tag, yields 24 labelsfor a CRF classifier in Mallet (McCallum, 2002).4 Experiments and EvaluationWe worked with ATB3-v3.2, following the train-ing/devtest split in (Roth et al, 2008) on a pre-vious release of the same data.
We keep a list-ing (List #1) of all (source token TEXT, solution)pairs seen during training.
For an open-class so-lution, ?solution?
is the gold label as described in5In Section 4 we discuss how these are narrowed down toone POS tag.Section 3.
For a closed-class solution, ?solution?is the name of the single pos-matching regex.
Inaddition, for every regex seen during training thatpos-matches some source token TEXT, we keep alisting (List #2) of all ((regex-group-name, text),POS-tag) tuples.
We use the information in List#1 to choose a solution for all words seen in train-ing in the Baseline and Run 2 below, and in Run3, for words text-matching a closed-class expres-sion.
We use List #2 to disambiguate all remain-ing cases of POS ambiguity, wherever a solutioncomes from.For example, if wlm is seen during testing, List#1 will be consulted to find the most common so-lution (REGEX #1 or #2), and in either case, List#2 will be consulted to determine the most fre-quent tag for w as a prefix.
While there is certainlyroom for improvement here, this works quite wellsince the tags for the affixes do not vary much.We score the solution for a source token in-stance as correct for tokenization if it exactlymatches the TEXT split for the tree tokens derivedfrom that source token instance in the ATB.
It iscorrect for POS if correct for tokenization and ifeach tree token has the same POS tag as the re-duced core tag for that tree token in the ATB.For a simple baseline, if a source token TEXTis in List #1 then we simply use the most fre-quent stored solution.
Otherwise we run the TEXTthrough all the regexes.
If it text-matches anyclosed-class expression, we pick a random choicefrom among those regexes and otherwise from theopen-class regexes that it text-matches.
Any POSambiguities for a regex group are disambiguated345Solution Baseline Run 2 Run 3Origin # tokens Tok POS # tokens Tok POS # tokens Tok POSAll 51664 96.0% 87.4% 51664 99.4% 95.1% 51664 99.3% 95.1%Stored 46072 99.8% 96.6% 46072 99.8% 96.6% 16145 99.6% 96.4%Open 5565 64.6% 11.6% 10 10.0% 0.0% 11 54.5% 0.0%Closed 27 81.5% 59.3% 27 81.5% 63.0% 27 81.5% 63.0%Mallet 0 5555 96.0% 83.8% 35481 99.1% 94.5%Table 5: Results for Baseline and two runs.
Origin ?stored?
means that the appropriate regex came fromthe list stored during training.
Origins ?open?
and ?closed?
are random choices from the open or closedregexes for the source token.
?Mallet?
means that it comes from the label output by the CRF classifier.using List #2, as discussed above.
The resultsare shown in Table 5.
The score is very high forthe words seen during training, but much lowerfor open-class words that were not.
As expected,almost all (except 27) instances of closed-classwords were seen during training.For run 2, we continue to use the stored solutionif the token was seen in training.
If not, then if theTEXT matches one or more closed-class regexes,we randomly choose one.
Otherwise, if the CRFclassifier has produced an open-class match forthat token, we use that (and otherwise, in only 10cases, use a random open-class match).
There is asignificant improvement in the score for the open-class items, and therefore in the overall results.For run 3, we put more of a burden on the clas-sifier.
If a word matches any closed-class expres-sion, we either use the most frequent occurenceduring training (if it was seen), or use a randommaching closed-class expression (if not).
If theword doesn?t match a closed-class expression, weuse the mallet result.
The mallet score goes up, al-most certainly because the score is now includingresults on words that were seen during training.The overall POS result for run 3 is slightly lessthan run 2.
(95.099% compared to 95.147%).It is not a simple matter to compare results withprevious work, due to differing evaluation tech-niques, data sets, and POS tag sets.
With differ-ent data sets and training sizes, Habash and Ram-bow (2005) report 99.3% word accuracy on tok-enization, and Diab et al (2007) reports a scoreof 99.1%.
Habash and Rambow (2005) reported97.6% on the LDC-supplied reduced tag set, andDiab et al (2007) reported 96.6%.
The LDC-supplied tag set used is smaller than the one inthis paper (24 tags), but does distinguish betweenNOUN and ADJ.
However, both (Habash andRambow, 2005; Diab et al, 2007) assume goldtokenization for evaluation of POS results, whichwe do not.
The ?MorphPOS?
task in (Roth et al,2008), 96.4%, is somewhat similar to ours in thatit scores on a ?core tag?, but unlike for us there isonly one such tag for a source token (easier) but itdistinguishes between NOUN and ADJ (harder).We would like to do a direct comparison by sim-ply runing the above systems on the exact samedata and evaluating them the same way.
However,this unfortunately has to wait until new versionsare released that work with the current version ofthe SAMA morphological analyzer and ATB.5 Future WorkObvious future work starts with the need to in-clude determiner information in the POS tags andthe important NOUN/ADJ distinction.
There arevarious possibilities for recovering this informa-tion, such as (1) using a different module combin-ing NOUN/ADJ disambiguation together with NPchunking, or (2) simply including NOUN/ADJ inthe current classifier instead of NOA.
We will beimplementing and comparing these alternatives.We also will be using this system as a preprocess-ing step for a parser, as part of a complete ArabicNLP pipeline.AcknowledgementsWe thank Ann Bies, David Graff, Nizar Habash,Mohamed Maamouri, and Mitch Marcus forhelpful discussions and comments.
This workwas supported by the Defense Advanced Re-search Projects Agency, GALE Program GrantNo.
HR0011-06-1-0003 and by the GALE pro-gram, DARPA/CMO Contract No.
HR0011-06-C-0022.
The content of this paper does not nec-essarily reflect the position or the policy of theGovernment, and no official endorsement shouldbe inferred.346ReferencesTim Buckwalter.
2004.
Buckwalter Arabic morpho-logical analyzer version 2.0.
Linguistic Data Con-sortium LDC2004L02.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.2007.
Automatic processing of Modern StandardArabic text.
In Abdelhadi Soudi, Antal van denBosch, and Gunter Neumann, editors, Arabic Com-putational Morphology, pages 159?179.
Springer.Mona Diab.
2009.
Second generation tools (AMIRA2.0): Fast and robust tokenization, pos tagging, andbase phrase chunking.
In Proceedings of 2nd Inter-national Conference on Arabic Language Resourcesand Tools (MEDAR), Cairo, Egypt, April.Nizar Habash and Owen Rambow.
2005.
Arabic to-kenization, part-of-speech tagging and morphologi-cal disambiguation in one fell swoop.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05), pages 573?580, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Seth Kulick, Ann Bies, and Mohamed Maamouri.2010.
Consistent and flexible integration of mor-phological annotation in the Arabic Treebank.
InLanguage Resources and Evaluation (LREC).Mohamed Maamouri, Ann Bies, Sondos Krouna,Fatma Gaddeche, Basma Bouziri, Seth Kulick, Wig-dane Mekki, and Tim Buckwalter.
2009a.
ArabicTreebank Morphological and Syntactic guidelines,July.
http://projects.ldc.upenn.edu/ArabicTreebank.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Fatma Gaddeche, and Wajdi Zaghouani.2009b.
Arabic treebank part 3 - v3.2.
LinguisticData Consortium LDC2010T08, April.Mohammed Maamouri, Basma Bouziri, SondosKrouna, David Graff, Seth Kulick, and Tim Buck-walter.
2009c.
Standard Arabic morphological ana-lyzer (SAMA) version 3.1.
Linguistic Data Consor-tium LDC2009E73.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic morphologi-cal tagging, diacritization, and lemmatization usinglexeme models and feature ranking.
In Proceed-ings of ACL-08: HLT, Short Papers, pages 117?120,Columbus, Ohio, June.
Association for Computa-tional Linguistics.347
