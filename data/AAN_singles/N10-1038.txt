Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293?296,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMovie Reviews and Revenues: An Experiment in Text Regression?Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.eduAbstractWe consider the problem of predicting amovie?s opening weekend revenue.
Previouswork on this problem has used metadata abouta movie?e.g., its genre, MPAA rating, andcast?with very limited work making use oftext about the movie.
In this paper, we usethe text of film critics?
reviews from severalsources to predict opening weekend revenue.We describe a new dataset pairing movie re-views with metadata and revenue data, andshow that review text can substitute for meta-data, and even improve over it, for prediction.1 IntroductionPredicting gross revenue for movies is a problemthat has been studied in economics, marketing,statistics, and forecasting.
Apart from the economicvalue of such predictions, we view the forecastingproblem as an application of NLP.
In this paper, weuse the text of critics?
reviews to predict openingweekend revenue.
We also consider metadata foreach movie that has been shown to be successful forsimilar prediction tasks in previous work.There is a large body of prior work aimed at pre-dicting gross revenue of movies (Simonoff and Spar-row, 2000; Sharda and Delen, 2006; inter alia).
Cer-tain information is used in nearly all prior work onthese tasks, such as the movie?s genre, MPAA rating,running time, release date, the number of screens onwhich the movie debuted, and the presence of partic-ular actors or actresses in the cast.
Most prior text-based work has used automatic text analysis tools,deriving a small number of aggregate statistics.
Forexample, Mishne and Glance (2006) applied sen-timent analysis techniques to pre-release and post-release blog posts about movies and showed higher?We appreciate reviewer feedback and technical advicefrom Brendan O?Connor.
This work was supported by NSF IIS-0803482, NSF IIS-0844507, and DARPA NBCH-1080004.correlation between actual revenue and sentiment-based metrics, as compared to mention counts of themovie.
(They did not frame the task as a revenueprediction problem.)
Zhang and Skiena (2009) useda news aggregation system to identify entities andobtain domain-specific sentiment for each entity inseveral domains.
They used the aggregate sentimentscores and mention counts of each movie in newsarticles as predictors.While there has been substantial prior work onusing critics?
reviews, to our knowledge all of thiswork has used polarity of the review or the numberof stars given to it by a critic, rather than the reviewtext directly (Terry et al, 2005).Our task is related to sentiment analysis (Pang etal., 2002) on movie reviews.
The key difference isthat our goal is to predict a future real-valued quan-tity, restricting us from using any post-release textdata such as user reviews.
Further, the most im-portant clues about revenue may have little to dowith whether the reviewer liked the movie, but ratherwhat the reviewer found worth mentioning.
This pa-per is more in the tradition of Ghose et al (2007) andKogan et al (2009), who used text regression to di-rectly quantify review ?value?
and make predictionsabout future financial variables, respectively.Our aim in using the full text is to identify partic-ular words and phrases that predict the movie-goingtendencies of the public.
We can also perform syn-tactic and semantic analysis on the text to identifyricher constructions that are good predictors.
Fur-thermore, since we consider multiple reviews foreach movie, we can compare these features acrossreviews to observe how they differ both in frequencyand predictive performance across different mediaoutlets and individual critics.In this paper, we use linear regression from textand non-text (meta) features to directly predict grossrevenue aggregated over the opening weekend, andthe same averaged per screen.293Domain train dev test totalAustin Chronicle 306 94 62 462Boston Globe 461 154 116 731LA Times 610 2 13 625Entertainment Weekly 644 208 187 1039New York Times 878 273 224 1375Variety 927 297 230 1454Village Voice 953 245 198 1396# movies 1147 317 254 1718Table 1: Total number of reviews from each domain forthe training, development and test sets.2 DataWe gathered data for movies released in 2005?2009.For these movies, we obtained metadata and a listof hyperlinks to movie reviews by crawling Meta-Critic (www.metacritic.com).
The metadatainclude the name of the movie, its production house,the set of genres it belongs to, the scriptwriter(s),the director(s), the country of origin, the primaryactors and actresses starring in the movie, the re-lease date, its MPAA rating, and its running time.From The Numbers (www.the-numbers.com),we retrieved each movie?s production budget, open-ing weekend gross revenue, and the number ofscreens on which it played during its opening week-end.
Only movies found on both MetaCritic and TheNumbers were included.Next we chose seven review websites that mostfrequently appeared in the review lists for movies atMetacritic, and obtained the text of the reviews byscraping the raw HTML.
The sites chosen were theAustin Chronicle, the Boston Globe, the LA Times,Entertainment Weekly, the New York Times, Vari-ety, and the Village Voice.
We only chose thosereviews that appeared on or before the release dateof the movie (to ensure that revenue information isnot present in the review), arriving at a set of 1718movies with at least one review.
We partitioned thisset of movies temporally into training (2005?2007),development (2008) and test (2009) sets.
Not allmovies had reviews at all sites (see Table 1).3 Predictive TaskWe consider two response variables, both inU.S.
dollars: the total revenue generated by a movieduring its release weekend, and the per screen rev-enue during the release weekend.
We evaluate thesepredictions using (1) mean absolute error (MAE) inU.S.
dollars and (2) Pearson?s correlation betweenthe actual and predicted revenue.We use linear regression to directly predict theopening weekend gross earnings, denoted y, basedon features x extracted from the movie metadataand/or the text of the reviews.
That is, given an inputfeature vector x ?
Rp, we predict an output y?
?
Rusing a linear model: y?
= ?0 + x>?.
To learn val-ues for the parameters ?
= ??0,?
?, the standardapproach is to minimize the sum of squared errorsfor a training set containing n pairs ?xi, yi?
wherexi ?
Rp and yi ?
R for 1 ?
i ?
n:??
= argmin?=(?0,?
)12nn?i=1(yi ?
(?0 + x>i ?
))2+?P (?
)A penalty term P (?)
is included in the objective forregularization.
Classical solutions use an `2 or `1norm, known respectively as ridge and lasso regres-sion.
Introduced recently is a mixture of the two,called the elastic net (Zou and Hastie, 2005):P (?)
=?pj=1(12(1?
?
)?2j + ?|?j |)where ?
?
(0, 1) determines the trade-off be-tween `1 and `2 regularization.
For our experi-ments we used the elastic net and specifically theglmnet package which contains an implementa-tion of an efficient coordinate ascent procedure fortraining (Friedman et al, 2008).We tune the ?
and ?
parameters on our develop-ment set and select the model with the ?
?, ??
com-bination that yields minimum MAE on the develop-ment set.4 ExperimentsWe compare predictors based on metadata, predic-tors based on text, and predictors that use both kindsof information.
Results for two simple baselines ofpredicting the training set mean and median are re-ported in Table 2 (Pearson?s correlation is undefinedsince the standard deviation is zero).4.1 Metadata FeaturesWe considered seven types of metadata features, andevaluated their performance by adding them to ourpool of features in the following order: whether the294film is of U.S. origin, running time (in minutes), thelogarithm of its budget, # opening screens, genre(e.g., Action, Comedy) and MPAA rating (e.g., G,PG, PG-13), whether the movie opened on a holidayweekend or in summer months, total count as well asof presence of individual Oscar-winning actors anddirectors and high-grossing actors.
For the first taskof predicting the total opening weekend revenue ofa movie, the best-performing feature set in terms ofMAE turned out to be all the features.
However, forthe second task of predicting the per screen revenue,addition of the last feature subset consisting of infor-mation related to the actors and directors hurt perfor-mance (MAE increased).
Therefore, for the secondtask, the best performing set contained only the firstsix types of metadata features.4.2 Text FeaturesWe extract three types of text features (described be-low).
We only included feature instances that oc-curred in at least five different movies?
reviews.
Westem and downcase individual word components inall our features.I.
n-grams.
We considered unigrams, bigrams, andtrigrams.
A 25-word stoplist was used; bigramsand trigrams were only filtered if all words werestopwords.II.
Part-of-speech n-grams.
As with words, weadded unigrams, bigrams, and trigrams.
Tagswere obtained from the Stanford part-of-speechtagger (Toutanova and Manning, 2000).III.
Dependency relations.
We used the Stanfordparser (Klein and Manning, 2003) to parse thecritic reviews and extract syntactic dependen-cies.
The dependency relation features consistof just the relation part of a dependency triple?relation, head word, modifier word?.We consider three ways to combine the collec-tion of reviews for a given movie.
The first (???
)simply concatenates all of a movie?s reviews intoa single document before extracting features.
Thesecond (?+?)
conjoins each feature with the sourcesite (e.g., New York Times) from whose review it wasextracted.
A third version (denoted ?B?)
combinesboth the site-agnostic and site-specific features.Features SiteTotal Per ScreenMAE MAE($M) r ($K) rPredict mean 11.672 ?
6.862 ?Predict median 10.521 ?
6.642 ?metaBest 5.983 0.722 6.540 0.272textI?
8.013 0.743 6.509 0.222+ 7.722 0.781 6.071 0.466see Tab.
3 B 7.627 0.793 6.060 0.411I ?
II?
8.060 0.743 6.542 0.233+ 7.420 0.761 6.240 0.398B 7.447 0.778 6.299 0.363I ?
III?
8.005 0.744 6.505 0.223+ 7.721 0.785 6.013 0.473B 7.595 0.796 ?6.010 0.421meta?textI?
5.921 0.819 6.509 0.222+ 5.757 0.810 6.063 0.470B 5.750 0.819 6.052 0.414I ?
II?
5.952 0.818 6.542 0.233+ 5.752 0.800 6.230 0.400B 5.740 0.819 6.276 0.358I ?
III?
5.921 0.819 6.505 0.223+ 5.738 0.812 6.003 0.477B 5.750 0.819 ?5.998 0.423Table 2: Test-set performance for various models, mea-sured using mean absolute error (MAE) and Pearson?scorrelation (r), for two prediction tasks.
Within a column,boldface shows the best result among ?text?
and ?meta ?text?
settings.
?Significantly better than the meta baselinewith p < 0.01, using the Wilcoxon signed rank test.4.3 ResultsTable 2 shows our results for both prediction tasks.For the total first-weekend revenue prediction task,metadata features baseline result (r2 = 0.521) iscomparable to that reported by Simonoff and Spar-row (2000) on a similar task of movie gross predic-tion (r2 = 0.446).
Features from critics?
reviewsby themselves improve correlation on both predic-tion tasks, however improvement in MAE is onlyobserved for the per screen revenue prediction task.A combination of the meta and text featuresachieves the best performance both in terms of MAEand r. While the text-only models have some highnegative weight features, the combined models donot have any negatively weighted features and onlya very few metadata features.
That is, the text is ableto substitute for the other metadata features.Among the different types of text-based featuresthat we tried, lexical n-grams proved to be a strongbaseline to beat.
None of the ?I ?
??
feature sets aresignificantly better than n-grams alone, but adding295the dependency relation features (set III) to the n-grams does improve the performance enough tomake it significantly better than the metadata-onlybaseline for per screen revenue prediction.Salient Text Features: Table 3 lists some of thehighly weighted features, which we have catego-rized manually.
The features are from the text-onlymodel annotated in Table 2 (total, not per screen).The feature weights can be directly interpreted asU.S.
dollars contributed to the predicted value y?
byeach occurrence of the feature.
Sentiment-relatedfeatures are not as prominent as might be expected,and their overall proportion in the set of featureswith non-zero weights is quite small (estimated inpreliminary trials at less than 15%).
Phrases thatrefer to metadata are the more highly weightedand frequent ones.
Consistent with previous re-search, we found some positively-oriented sentimentfeatures to be predictive.
Some other prominentfeatures not listed in the table correspond to spe-cial effects (?Boston Globe: of the art?, ?and cgi?
),particular movie franchises (?shrek movies?, ?Vari-ety: chronicle of?, ?voldemort?
), hype/expectations(?blockbuster?, ?anticipation?
), film festival (?Vari-ety: canne?
with negative weight) and time of re-lease (?summer movie?
).5 ConclusionWe conclude that text features from pre-release re-views can substitute for and improve over a strongmetadata-based first-weekend movie revenue pre-diction.
The dataset used in this paper has beenmade available for research at http://www.ark.cs.cmu.edu/movie$-data.ReferencesJ.
Friedman, T. Hastie, and R. Tibshirani.
2008.
Regular-ized paths for generalized linear models via coordinatedescent.
Technical report, Stanford University.A.
Ghose, P. G. Ipeirotis, and A. Sundararajan.
2007.Opinion mining using econometrics: A case study onreputation systems.
In Proc.
of ACL.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InAdvances in NIPS 15.S.
Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.Smith.
2009.
Predicting risk from financial reportswith regression.
In Proc.
of NAACL, pages 272?280.Feature Weight ($M)ratingpg +0.085New York Times: adult -0.236New York Times: rate r -0.364sequels this series +13.925LA Times: the franchise +5.112Variety: the sequel +4.224people Boston Globe: will smith +2.560Variety: brittany +1.128?
producer brian +0.486genreVariety: testosterone +1.945Ent.
Weekly: comedy for +1.143Variety: a horror +0.595documentary -0.037independent -0.127sentiment Boston Globe: best parts of +1.462Boston Globe: smart enough +1.449LA Times: a good thing +1.117shame $ -0.098bogeyman -0.689plotVariety: torso +9.054vehicle in +5.827superhero $ +2.020Table 3: Highly weighted features categorized manu-ally.
?
and $ denote sentence boundaries.
?brittany?frequently refers to Brittany Snow and Brittany Murphy.??
producer brian?
refers to producer Brian Grazer (TheDa Vinci Code, among others).G.
Mishne and N. Glance.
2006.
Predicting movie salesfrom blogger sentiment.
In AAAI Spring Symposiumon Computational Approaches to Analysing Weblogs.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proc.
of EMNLP, pages 79?86.R.
Sharda and D. Delen.
2006.
Predicting box office suc-cess of motion pictures with neural networks.
ExpertSystems with Applications, 30(2):243?254.J.
S. Simonoff and I. R. Sparrow.
2000.
Predicting moviegrosses: Winners and losers, blockbusters and sleep-ers.
Chance, 13(3):15?24.N.
Terry, M. Butler, and D. De?Armond.
2005.
The de-terminants of domestic box office performance in themotion picture industry.
Southwestern Economic Re-view, 32:137?148.K.
Toutanova and C. D. Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proc.
of EMNLP, pages 63?70.W.
Zhang and S. Skiena.
2009.
Improving movie grossprediction through news analysis.
In Web Intelligence,pages 301?304.H.
Zou and T. Hastie.
2005.
Regularization and variableselection via the elastic net.
Journal Of The Royal Sta-tistical Society Series B, 67(5):768?768.296
