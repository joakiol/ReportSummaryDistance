Nonlocal Language Modelingbased on Context Co-occurrence VectorsSadao Kurohash i  and  Manabu Or iGraduate School of InformaticsKyoto UniversityYoshida-honmachi, Sakyo; Kyoto, 606-8501 Japankuro@?, ky0to -u ,  ac.
j p, or iOpine,  kuee.
kyoto -u ,  ac.
j pAbstractThis paper presents a novel nonlocal lmlguagemodel which utilizes contextual information.A reduced vector space model calculated fromco-occurrences of word pairs provides wordco-occurrence v ctors.
The sum of word co-occurrence vectors represents ile context of adocument, and the cosine similarity betweenthe context vector and the word co-occurrencevectors represents he \]ong-distmlce exical de-pendencies.
Experiments on the MainichiNewspaper corpus show significant improve-ment in perplexity (5.070 overall and 27.2%on target vocabulary)1 I n t roduct ionHuman pattern recognition rarely handles iso-lated or independent objects.
We recog-nize objects in various patiotemporal circum-stances uch as an object in a scene, a wordin an uttermlce.
These circumstances workas conditions, eliminating ambiguities and en-abling robust recognition.
The most challeng-ing topics in machine pattern recognition arein what representation a d to what extentthose circumstances are utilized.In laalguage processing, a context--that is;a portion of the utterance or the text beforethe object--is ml important circumstmlce.One way of representing a context is statis-tical language nmdels which provide a wordsequence probability, P(w~), where w~ de-notes the sequence wi .
.
.w j .
In other words,they provide the conditional probability of aword given with the previous word sequence,P( wilw~-l ), which shows the prediction of aword in a given context.The most conmmn laalguage models usednowadays are N-granl models based on a(N-  1)-th order Markov process: event pre-dictions depend on at most (N-  1) previousevents.
Therefore, they offer the following ap-proximation:P(w.ilw  -1)   wiJwi_N+l) (I)A common value for N is 2 (bigram languagemodel) or 3 (trigram language model); onlya short local context of one or two words isconsidered.Even such a local context is effective insome cases.
For example, in Japanese, afterthe word kokumu 'state affairs', words such asdaijin 'minister' mad shou 'department' likelyfollow; kaijin 'monster' and shou 'priZe' donot.
After dake de 'only at', you cml oftenfind wa (topic-marker), but you hardly findga (nominative-marker) or wo (accusative-marker).
These examples how behaviors ofcompound nouns and function word sequencesare well handled by bigram mad trigraan mod-els.
These models are exploited in several ap-plications uch as speech recognition, opticalcharacter recognition and nmrphological nal-ysis.Local language models, however, cannotpredict nmch in some cases.
For instance, theword probability distribution after de wa 'at(topic-marker)' is very flat.
However, even ifthe probability distribution isflat in local lan-guage models, the probability of daijin 'min-ister' and kaijin 'monster' must be very differ-ent in documents concenfing politics.
Bigramand trigram models are obviously powerlessto such kind of nonlocal, long-distmlce l xicaldependencies.This paper presents a nonlocal languagemodel.
The important information concern-ing long-distance l xical dependencies is theword co-occurrence information.
For example,words such as politics, govermnent, admin-istration, department, tend to co-occur withdaijin 'minister'.
It is easy to measure co-occurrences ofword pairs from a training cor-pus, but utilizing them as a representation fcontext is the problem.
We present a vector80WlW2w3w4w5w6D1 D2 D3 D4 D~ D6 D7 Ds1 0 1 0 1 0 1 01 O 1 1 0 0 0 00 1 0 0 1 1 0 11 1 1 0 0 0 0 00 0 0 O 1 0 1 00 0 0 0 1 0 0 1Wlw2w3w4w5w6Wl W2 w3 W4 w5 w 64 2 1 2 2 13 0 2 0 04 1 1 23 0 02 12Figure 1: V~rord-document co-occurrence ma-trix.representation f word co-occurrence informa-tion; and show that the context can be repre-sented as a sum of word co-occurrence v ctorsin a docmnent and it is incorporated in a non-local language model.2 Word  Co-occur rence  Vector2.1 Word-Document  Co-occur renceMat r ixWord co-occurrences are directly representedin a matrix whose rows correspond to wordsand whose columns correspond to documents(e.g.
a newspaper article).
The element ofthe matrix is 1 if the word of the row ap-pears in the document of the colunm (Figure1).
Wre call such a matrix a word-documentco-occurrence matrix.The row-vectors of a word-document co-occurrence matrix represent the co-occurrenceinformation of words.
If two words tend to ap-pear in the same documents, that is: tend toco-occur, their row-vectors are similar, that is,they point in sinfilar directions.The more document is considered, the morereliable and realistic the co-occurrence infor-mation will be.
Then, the row size of a word-document co-occurrence matrix may becomevery large.
Since enormous amounts of onlinetext are available these days, row size can be-come more than a million documents.
Then,it is not practical to use a word-docmnent co-occurrence matrix as it is.
It is necessary toreduce row size and to simulate the tendencyin the original matrix by a reduced matrix.2.2 Reduct ion  o f  Word-DocumentCo-occur rence  Matr ixThe aim of a word-document co-occurrencematrix is to measure co-occurrence of twowords by the angle of the two row-vectors.In the reduction of a matrix, angles of tworow-vectors in the original matrLx should bemaintained in the reduced matrLx.Figure 2: ~Vord-word co-occurrence matrix.As such a matrix reduction, we utilized alearning method developed by HNC Software(Ilgen and Rushall, 1996).
11.
Not the word-docmnent co-occurrencematrix is constructed from tile learningcorpus, but a word-word co-occurrencematrix.
In this matrix: the rows andcolunms correspond to words and the i-th diagonal element denotes the numberof documents in which the word wl ap-pears, F(wi).
The i:j-th element denotesthe number of documents in which bothwords w,: and wj appear, F(wi, wj) (Fig-ure 2).The importmlt information in a word-document co-occurrence matrix is the co-sine of the angle of the row-vector of wiand that of wj, which can be calculatedby the word-word co-occurrence matrixas follows:F(w,:, wj) (2)This is because x/F(wi) corresponds tothe magnitude of the row-vector of wl,and F(wl, wi) corresponds to the dotproduct of the row-vector of wl andthat of wj in the word-docmnent co-occurrence matrix.2.
Given a reduced row size, a matrix is ini-tialized as follows: matrix elements arechosen from a normal distribution ran-domly, then each row-vector is normal-ized to magnitude 1.0.
The random refitrow-vector of the word wl is denoted as,WCi Rand.Random unit row-vectors in high di-mensional floating point spaces have a1The goal of HNC was the enhancement of textretrieval.
The reduced word vectors were regarded assemantic representation f words and used to representdocuments and queries.81sori wa kakugi de' kankyo mondai(Prime Minister) (Cabinet meeting) (environment) (issue)I wc I \] wc I I  wc\ni tuitew (cc ?
wc) 2 Pckaigi (conference) 0.237962 0.002702senkyo (election) 0.150773 0.001712yosan (budget) 0.128907 0.001463daijin (minister) 0.018549 0.000211yakyu (baseball) 0.004556 0.000052kaijin (monster) 0.000002 0.000000sugaku (mathematics) 0.000001 0.000000TOTAL 88.079230 1.000000Figure 3: An example of context co-occurrence probabilities.property that is referred to a "qnasi-orthogonality'.
That is; the expected~?alue of the dot product between an3"pair of random row-vectors, wci  Rand andwet  and, is approximately equal to zero(i.e.
all vectors are approximately or-thogonal).3.
The trained row-vector, wai is calculatedas follows:WCi -~ ~13C~ and + "q ~ O'ij'T.ll4 andJ (3)wc - (4)The procedure iterates the following calcu-lation:OJwen e~' = wc l  - q Owe/= + rl (a j - we~.
wcj)wc(6)new -- W C7 e~:ilwcF wl I (7)The learning method by HNC is a rathersimple approximation of the procedure, doingjust one step of it.
Note that wci .wc j  isapproximately zero for the initialized randomvectors.ai j  corresponds to the degree of the co-occurrence of two words.
By addingwc~ and to wet  a'd depending on ai j ,  th.elearning formula (3) achieves that twowords that, tend to co-occur will havetrained vectors that point in shnilar di-rections, r/is a design parameter chosento optimize performance.
The formula(4) is to normalize vectors to magnitude1.0.We call the trained row-vector we/o f  theword wi a word co-occurrence vector.The background of the above method is astochastic gradient descent procedure for min-imizing the cost function:1 J = ~ .~(a i j  -- we/"  wcj )  2 (5)%3subject to the constraints \[\[we/I\[ = 1.3 Context  Co-occur rence  VectorThe next question is how to represent hecontext of a document based on word co-occurrence vectors.
We propose a simplemodel which represents the context as the sumof the word co-occurrence vectors associatedwith content words ill a document so far.
Itshould be noted that the vector is normalizedto unit length.
V~re call the resulting vector acontext co-occurrence vector.W'ord co-occurrence vectors have the prop-erty that words which tend to co-occur havevectors that.
point in similar directions.
Con-text co-occurrence vectors are expected tohave the sinfilar property.
That is, if a wordtends to appear in a given context, the wordco-occurrence vector of the word and the con-text co-occurrence vector of the context willpoint in similar directions .
.
.
.
.
.Such a context co-occurrence vector can beseen to predict the occurrence of words in a82wherep(.wdwi_,) = ( P(C~lwi-' ) x P(wdw~-'Cc)P(Cflwj-') x P(wdw~-lc/) (if wl E C~if wi E C/P(C~Iw~ -1)P(wilw~-:C~)P(wi\[w -lc/)= A:P(Cc) + A2P(C~lwi_l ) + A3P(C~\[wi-2wi-1)= AclP(wiICc) + A~2P(wi\[wi-lC~) + A~3P(wi\[wi-2Wi-lCc)= 1-  P(C~lwj - : )= a/ :P (wdc / )  + a/2P(wd,  -:ci) +withFigure 4: Context language model.given context, mad is utilized as a componentof statistical language modeling, as shown inthe next section.4 Language Model ing usingContext  Co-occur rence  Vector4.1 Context Co-occurrenceProbab i l i tyThe dot product of a context co-occurrencevector and a word co-occurrence vector showsthe degree of affinity of the context m:d theword.
The probability of a content word basedon such dot products, called a context co-occurrence probability, can be calculated asfollows:Pc(wilw~_lcc) = f(cc~ -1 "~cl)~wjEcc f(cc~ -1" ~vcj)(S)where cc~ -1 denotes the context co-occurrencevector of the left context, Wl .
.
.
wi-1, and Ccdenotes a content word class.
Pc(wilw~-lcc)metals the conditional probability of wi giventhat a content word follows wj- : .One choice for the function .f(x) is the iden-tity.
However, a linear contribution of dotproducts to the probability results in poorerestimates, since the differences of dot prod-ucts of related words (tend to co-occur) andunrelated words are not so large.
Experimentsshowed that x 2 or x 3 is a better estimate.An example of context co-occurrence prob-abilities is shown in Figure 3.4.2 Language Modeling using ContextCo-occurrence Probab i l i tyContext co-occurrence probabilities can hamdle long-distance l xical dependencies while astandard trigram model can handle local con-texts more clearly: in this way they comple-ment each other.
Therefore, language model-ing of their linear interpolation is employed.Note that tile linear interpolation of unigram,bigram and trigram models is simply referredto 'trigxan: model' in this paper.The proposed language model, called a con-text language model, computes probabilitiesas shown in Figure 4.
Since context co-occurrence probabilities are considered onlyfor content words (Cc), probabilities are cal-culated separately for content words (Co) andfunction words (C/).P(Cc\[w~ -1) denotes the probability that acontent word follows w~-:, which is approx-imated by a trigrmn nmdel.
P(.wi\[w~-lcc)denotes the probability that wi follows w~-:given that a content word follows w~-:, whichis a linear interpolation of a standard trigrammodel and the context co-occurrence proba-bilities.In the case of a function word, since thecontext co-occurrence probability is not con-sidered, P(wdw~-lCi) is just a standard tri-granl model.X's adapt using an EM re-estimation proce-dure on the held-out data.83Table 1: Perplexity results for the stmldard trigrazn model and the context language nmdel.Perplexity on Perplexity onLanguage Model the entire the targetvocabulary vocabularyStandard Trigram Model 107.7 1930.2Context Language ModelVector size 0 f (x)500 0.5 x ~1000 0.3 x ~1000 0.5 x* 1000 0.5 x 21000 0.5 x 31000 1.0 x 22000 0.5 x 2106.3 (-1.3%)~o 102.7 (-4., %)103.6 (-3.9%)102.4 (-5.0%)102.4 (-5.0%)102.5 (-4.8%)102.4 (-5.0%)1663.8 (-13.8%)1495.9 (-22.5%)1496.1 (-22.5%)1406.2 (-27.2%)1416.8 (-26.9%)1430.3 (-25.9%)1408.1 (-27.1%)Standard Bigram Model 130.28 2719.67Context Language Model125.06 (-4.0%)122.85 (-5.7%)1000 0.5 x1000 0.5 x 22075.10 (-23.7%)1933.68 (-28.9%)shijyo no ~ wo ~ ni Wall-gai ga kakkyou wo teishi, bei kabushiki'US' 'stock' 'market' 'sudden rise' 'background' %Vall Street' 'activity' 'show'wagayonoharu wo ~a~ shire iru.
\[shoukenl kaisha, ~h~ ginkou wa 1996 nen ni'prosperity' 'enjoy' 'do' 'stock' 'company' 'investment' 'bank' 'year'halite ka o saiko  l ko shi  \] '96 ne,  I k b shiki l so.ha '95'enter' 'past' maximum' 'profit' 'renew' 'year'ni I .tsuzuki\] kyushin .
mata \] kab.uka\] kyushin wo'continue' 'rapid increase' 'stock price' 'rapidly increase'I shinkabul hakkou ga ~ saikou to natta.
'new stock' 'issue' 'past' 'maximum' 'become''stock' 'market' 'year'ni ~u~ no'background' 'corporation'Figure 5: Comparison of probabilities of content words by the trigraan model and the contextmodel.
(Note that wa, ga, wo, ni; to and no are Japanese postpositions.
)4.3 Test Set Perp lex i tyBy using the Mainichi Newspaper corpus(from 1991 to 1997, 440,000 articles), testset perplexities of a standard trigrmn/bigrammodel and the proposed context languagemodel are compared.
The articles of sixyears were used for the leanfing of word co-occurrence vectors, unigrams, bigrmns andtrigrams; the articles of half a year were usedas a held-out data for EM re-estimation f A's;the remaining articles (half a year) for com-puting test set perplexities.Word co-occurrence v ctors were computedfor the top 50,000 frequent content words (ex-cluding pronouns, numerals, temporal nouns,mad light verbs) in the corpus, and unigrmn:bigrmn and trigrmn were computed for the top60,000 frequent words.The upper part of Table 1 shows thecom-parison results of the stmldard trigram modeland the context language model.
For the bestparameters (marked by *), the overall per-plexity decreased 5.0% and the perplexity ontarget vocabulary (50,000 content words) de-creased 27.270 relative to the standard trigrammodel.
For the best parameters, A's wereadapted as follows:A1 = 0.08, A2 = 0.50, A3 = 0.42Acl = 0.03, ~c2 = 0.50, Xc3 = 0.30, Xcc = 0.17Afl = 0.06, ~f2 = 0.57, A f3 = 0.37As for parazneter settings, note that per-formance is decreased by using shorter wordco-occurrence vector size.
The vaxiation of~/does not change the performance so much.84f (x )  = x 2 and f (x )  = x 3 are alnmst the same;better thaaa f (x )  = x.The lower part of Table 1 shows the compar-ison results of the standard bigram model andthe context language model.
Here, the contextlanguage model is based on the bigrana model,that is, the terms concerning trigrmn in Fig-ure 4 were eliminated.
The result was similar,but the perplexity decreased a bit more; 5.7%overall and 28.9% on target vocabulary.Figure 5 shows a test article in which theprobabilities of content words by the trigramlnodel aald the context model are compared.
Ifthat by the context model is bigger (i.e.
thecontext model predicts better), the word isboxed; if not, the word is underlined.The figure shows that the context modelusually performs better after a function word,where the trigram model usually has little pre-diction.
On the other hand, the trigram modelperforms better after a content word (i.e.
ina compound noun) because a clear predictionby the trigram model is reduced by payingattention to the relatively vague context co-occurrence probability (Acc is 0.17).The proposed model is a constant interpo-lation of a trigram model and the context co-.0ccurrence probabilities.
More adaptive inter-polation depending on the N-gram probabil-ity distribution may improve the performance.5 Re la ted  WorkCache language models (Kuhn mad de Mori,1990) boost the probability of the words al-ready seen in the history.Trigger models (Lau et al, 1993), even moregeneral, try to capture the co-occurrences be-tween words.
While the basic idea of ourmodel is similar to trigger models, they handleco-occurrences of word pairs independentlyand do not use a representation of the wholecontext.
This omission is also done in ap-plications such as word sense dismnbiguation(Yarowsky: 1994; FUNG et al, 1999).Our model is the most related to Coccaromad Jurafsky (1998), in that a reduced vec-tor space approach was taken and context isrepresented by the accumulation of word co-occurrence vectors.
Their model was reportedto decrease the test set perplexity by 12%,compared to the bigram nmdel.
The majordifferences are:1.
SVD (Singular Value Decomposition)was used to reduce the matrix which iscommon in the Latent Semaaltic Analysis(Deerwester et ai.
; 1990), and2.
context co-occurrence probabilities werecomputed for all words, and the degreeof combination of context co-occurrenceprobabilities and N-gram probabilitieswas computed for each word, dependingon its distribution over the set of docu-l nents .As for the first point, we utilized thecomputationally-light, i eration-based proce-dure.
One reason for this is that the com-putational cost of SVD is very high whenmillions or more documents are processed.Furthermore, considering an extension of ournmdel with a cognitive viewpoint, we believean iteration-based model seems more reason-able than an algebraic model such as SVD.As for the second point, we doubt the ap-propriateness to use the word's distributionas a measure of combination of two models.What we need to do is to distinguish wordsto which semantics hould be considered andother words.
We judged the distinction of con-tent words and function words is good enoughfor that purpose, and developed their trigram-based distinction as shown in Figure 4.Several topic-based models have been pro-posed based on the observation that certainwords tend to have different probability dis-tributions in different topics.
For example,Florian and Yarowsky (1999) proposed the fol-lowing model:t(9)where t denotes a topic id.
Topics areobtained by hierarchical clustering from atraining corpus, and a topic-specific languagemodel, Pt, is learned from the clustered ocu-ments.
Reductions in perplexity relative to abigrmn model were 10.5% for the entire textand 33.5% for the target vocabulary.Topic-based models capture long-distancelexical dependencies via intermediate topics.In other words, the estimated istribution oftopics, P(t\]w~), is the representation f a con-text.
Our model does not use such interme-diate topics, but accesses word cg-occurrenceinformation directly aald represents a contextas the accumulation of this information.856 ConclusionIn this paper we described a novel languagemodel of incorporating long-distance lexicaldependencies based on context co-occurrencevectors.
Reduced vector representation ofword co-occurrences nables rather simple buteffective representation of the context.
Sig-nificant reductions in perplexity are obtainedrelative to a staaldard trigram model: both onthe entire text.
(5.0~) and on the target vo-cabulary (27.2%).AcknowledgmentsThe research described in this paper was sup-ported in part.
by JSPS-RFTF96P00502 (TheJapan Society for the Promotion of Science,Research for the Future Program).ReferencesNoah Coccaxo and Daniel Jurafsky.
1998.
To-wards better integration of semantic predictorsin statistical language modeling.
In Proceedingsof ICSLP-98, volume 6, pages 2403-2406.Scott Deem, ester, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and RichardHarshmaa~.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society forInformation Science, 41(6):391-407.Radu Florian and David Yarowsky.
1999.
Dy-namic nonlocal anguage modefing via hierar-chical topic-based adaptation.
In Proceedings offthe 37rd Annual Meeting of ACL, pages 167-174.Pascale FUNG, LIU Xiaohu, mad CHEUNG ChiShun.
1999.
Mixed language query disambigua-tion.
In Proceedings of the 37rd Annual Meetingof A CL, pages 333-340.Maa'd R. Ilgen and David A. Rushall.
1996.
Re-cent advances in HNC's context vector informa-tion retrieval technology.
In TIPSTER PRO-GRAM PHASE II, pages 149--158.R.
Kuhn and IL de Mori.
1990.
A cache-basednatural anguage model for speech recognition.IEEE Transactions on Pattern Analysis andMachine Intelligence, 12(6):570-583.R.
Lau, Ronald Rosenfeld, and Safim Roukos.1993.
Trigger based language models: a max-imum entropy approach.
In Proceedings ofICASSP, pages 45-48.David Yarowsky.
1994.
Decision fists for lexicalambiguity resolution : Application to accentrestoration in Spanish and French.
In Proceed-ings o/the 32nd Annual Meeting of A CL, pages88-995.86
