///I///Do Not Forget:Full Memory in Memory-Based Learning of Word Pronunciation *Antal van den Bosch and Walter DaelemansTilburg University, ILKP.O.
Box 90153, NL-5000 LE TilburgThe Netherlands{ant alb, ealt er}@kub, nlAbstractMemory-based learning, keeping full memoryofleaxning material, appeaxs a viable approachto learning N-~ tasks, and is often superiorin genera~sation accuracy to eager learningapproaches that abstract from learning mate-riaL Here we investigate three pa~'tial memory-based learning approaches which remove frommemory specific task instance types estimatedto be exceptional.
The three approaches achimplement one heuristic function for estimat-ing exceptiona\]ity of instance types: (i) typi-catty, (ii) class prediction strength, and (fii)friencfly-neighbourhood size.
Experiments areperformed with the memory-based l arning al-gorithm IBI-IG trained on English word pro-nunciatlon.
We find that removing instancetypes with low prediction strength (il) is theonly tested method which does not seriouslyharm generallsation accuracy.
We concludethat keeping full memory of types rather thantokens, and excluding minority ambiguities ap-pear to be the only performance-preserving op-timi~tions of memory-based leaxning.1 IntroductionMemory-based learning of classification tasks is abranch of supervised machine learning in which thelearning phase consists simply of storing all en-countered instances from a training set in mem-ory (Aha, 1997).
Memory-based learning algorithmsdo not invest effort during learning in abstract-ing from the tr-lnlng data, such as eager-learning(e.g., decision-tree algorithms, rule-induction, orconnectionist-learning al orithms, (Qululan, 1993;Mitchell, 1997)) do.
Rather, they defer investingeffort until new instances axe presented.
On be-ing presented with an instance, a memory-based*This research was done in the context of the "Induc-tion of Linguistic Knowledge" research programme, par-tially supported by the Foundation for Language Speechand Logic (TSL), which is funded by the NetherlandsOrganization for Scientific Research (NWO).
Part of thefirst author's work was performed at the Department ofComputer Science of the Unlversiteit Maastricht.learning algorithm searches for a best-matching in-stance, or, more generically, a set of the k best-matching instances in memory.
Having found sucha set of h best-matching instances, the algorithmtakes the (majority) class with which the instancesin the set axe labeled to be the class of the newinstance.
Pure memory-based learning algorithmsimplement he classic k-nearest neighbour algo-rithm (Cover and Hart, 1967; Devijver and Kittler,1982; Aha, Kibler, and Albert, 1991); in differentcontexts, memory-based learning algorithms havealso been named lazy, instance-based, exemplar-based, memory-based, case-based learning or reason-ing (Stanfdl and Waltz, 1986; Kolodner, 1993; Aha,Kibler, and Albert, 1991; Aha, 1997))Memory-based learning has been demonstratedto yield accurate models of various natural lan-guage tasks such as grapheme-phoneme conver-sion, word stress assignment, part-of-speech tagging,and PP-attachment (Daelemans, Van den Bosch,and Weijters, 1997a).
For example, the memory-based learning algorithm ml-IG (Daelemans andVan den Bosch, 1992; Daclemans, Van den Bosch,and We~jters, 1997b), which extends the well-knownml algorithm (Aha, Kibler, and Albert, 1991)with an information-gain weighted similaxity met-tic, has been demonstrated to perform adequatelyand, moreover, consistently and significantly betterthan eager-lea~'ning algorithms which do invest ef-fort in abstraction during learning (e.g., decision-tree learning (Daelemans, Van den Bosch, andWeijters, 1997b; Quinlan, 1993), and connectionistlearning (Rumelhart, Hinton, and Williams, 1986))when trained and tested on a range of morpho-phonological tasks (e.g., morphological segmenta-tion, grapheme-phoneme conversion, syllabitlcation,and word stress assignment) (Daelemans, Gillis, andDurieux, 1994; Van den Bosch, Daelemans, andWe~jters, 1996; Van den Bosch, 1997).
Thus, whenlearning NLP tasks, the abstraction oeeurnng in de-cision trees (i.e., the explicit forgetting of informa-tion considered to be redundant) and in connee-tionist networks (i.e., a non-symbolic encoding anddecoding in relatively small numbers of connectionvan den Bosch and Daelemans 195 Memory-Based Learning of Word PronunciationAntal van den Bosch and Walter Daelemans (1998) Do Not Forget: Full Memory in Memory-Based Learning of WordPronunciation.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and ComputationalNatural Language Learning, ACL, pp 195-204.weights) both hamper accurate generalisation f thelearned knowledge to new material.These findings appear to contrast with the generalassumption behind eager learning, that data repre-senting real-world classification tasks tends to con-tains (i) redundancy and (ii) exceptions: redundantdata can be compressed, yielding smaller descrip-tions of the original data; some exceptions (e.g., low-frequency exceptions) can (or should) be discardedsince they are expected to be bad predictors for clas-shying new (test) material.
However, both redun-dancy and exeeptionality cannot be computed triv-ially; heuristic functions are generally used to esti-mate them (e.g., functions from ixLformation theory(Qnlnl~m, 1993)).
The lower generalization accura-cies of both decision-tree and eonnectionist learning,compared to memory-based learning, on the above-mentioned NLP tasks, suggest that these heuristic es-timates may not be the best choice for learning NLPtasks.
It appears that in order to learn such taskssuccessfully, a learning algorithm should not forget(i.e., explicitly remove from memory) any informa-tion contained in the learning material: it should notabstract from the individual instances.An obvious type of abstraction that is not harm-ful for generalisation accuracy (but that is not al-ways acknowledged in implementations of memory-based learning) is the straightforward abstractionfrom tokens to types with frequency information.In general, data sets representing natural anguagetasks, when large enough, tend to contain consider-able numbers of duplicate sequences mapping to thesame output or class.
For example, in data repre-senting word pronunciations, some sequences of let-ters, such as ing at the end of English words, occurhundreds of times, while each of the sequences ipronounced i entically, viz.
/llJ/.
Instead of storingall individual sequence tokens in memory, each setof identical tokens can be safely stored in memoryas a single sequence type with frequency informa-tion, without loss of generalisation accuracy (Daele-roans and Van den Bosch, 1992; Daelemans, Van denBosch, and Weijters, 1997b).
Thus, forgetting in-stance tokens and replacing them by instance typesmay lead to considerable computational optlmi~a-tions of memory-based learning, since the memorythat needs to be searched may become considerablysmal le r  ?Given the safe, performance-preserving optlmi-e~-tion of replacing sets of instance tokens by instancetypes with frequency information, a next step of in-vestigation into optlmlsing memory-based learningis to measure the effects offorge~ing instance typeson grounds of their exceptionality, the underlyingidea being that the more exceptional  task instancetype is, the more likely it is that it is a bad predic-tor for new instances.
Thus, exceptionality should insome way express the unsuitability of a task instancetype to be a best match (nearest neighbour) to newinstances: it would be unwise to copy its associatedclassification to best-matching ew instances.
In thispaper, we investigate three criteria for estimatingan instance type's exceptionality, and removing in-stance types estimated to be the most exceptionalby each of these criteria.
The criteria investigatedare1.
typicality of instance types;2. class prediction strength of instance types;3. fi-iendly-neighbourhood sizeof instance types;4. random (to provide a baseline xperiment).We base our experiments on a large data set ofEnglish word pronunciation.
We briefly describethis data set, and the way it is converted into aninstance base fit for memotT-based learning, in Sec-tion 2.
In Section 3 we describe the settings of ourexperiments and the memory-based learning algo-rithm IBI-Io with which the experiments are per-formed.
We then turn to describing the notionsof typicality, class-prediction strength, and friendly-neighbourhood size, and the functions to estimatethem, in Section 4.
Section 5 provides the experi-mental results.
In Section 6, we discuss the obtainedresults and formulate our conclusions.2 The  word-pronunc ia t ion  dataConverting written words to stressed phonemic tran-scription, i.e., word pronunciation, is a well-knownbenchmark task in machine learning (Stanfill andWaltz, 1986; Sejnowski and Rosenberg, 1987; Shav-lik, Mooney, and Towell, 1991; Dietterich, Hild, andBaklri, 1990; Wolpert, 1990).
We define the task asthe conversion of fixed-sized instances representingparts of words to a class representing the phonemeand the stress marker of the instance's middle let-ter.
To genexate the instances, windowing is used(Sejnowski and Rosenberg, 1987).
Table I displaysexample instances and their classifications generatedon the basis of the sample word booking.
Classifica-tious, i.e., phonemes with stress markers (henceforthPSs), are denoted by composite labels.
For exam-pie, the first instance in Table 1, -_book, maps todass labd /b/ l ,  denoting a /b /  which is the firstphoneme of a syllable receiving primary stress.
Inthis study, we chose a fixed window width of sevenletters, which offers ufficient context information foradequate performance, though extension of the win-dow decreases ambiguity within the data set (Vanden Bosch, 1997).The task, henceforth referred to as Qs (Grapheme-phoneme conversion and stress assignment) is sim-ilar to the NBTTALK task presented by Sejnowskiand Rosenberg (1986), but is performed on a laxgercorpus, of 77,565 English word-pronunciation pairs,extracted from the cBr.Bx lexical data base (Bur-nage, 1990).
Converted into fixed-sized instance, thevan den Bosch and Daelemans 196 Memory-Based L arning of Word PronunciationIiIIIiIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIinstance leftnumber context12 b3 o4 b o o5 o o k6 o k i7 k i nfocusletterb00kingfightcontexto o ko k ik i ni n gn g _gTable 1: Example of instances generated fox the word-pronunciationclassification/b/1/u/0/-/0/k/0/olO/-/otask from the word booking.full instance base representing the as task contains675,745 instances.
The task features 159 classes(combined phonemes and stress markers).
The cod-ing of the output as 159 atomic ('local') classes com-bining grapheme-phoneme conversion and stress as-signment is one out of many types of output cod-ing (Shavlik, Mooney, and Towel\], 1991), e.g., dis-tributed bit coding using articulatory features (Se-jnowski and Rosenberg, 1987), error-correcting out-put coding (Diettefich, Hild, and Bakid, 1990), orsplit discrete coding of gmpheme-phoneme conver-sion and stress assignment (Van den Bosch, 1997).While these studies point at back-propagation learn-ing (Rumelhart, Hinton, and Williams, 1986), us-ing distributed output code, as the better pet-former as compared to ID3 (Quinlan, 1986), a sym-bolic inductive-learning decision tree algorithm (Di-etterich, Hild, and Bakid, 1990; Shavllk, Mooney,and Towel\], 1991), unless IV3 was equipped witherror-correcting output codes and additional man-ual tweaks (Dietterich, Hild, and Bakiri, 1990).
Sys-tematic experiments with the data also used in thispaper have indicated that both back-propagationand decision-tree l arning (using either distributedor atomic output coding) ate consistently and sig-nificantly outperformed by memory-based learningof gmpheme-phoneme conversion, stress assignment,and the combination of the two (Van den Bosch,1997), using atomic output coding.
Our choice foratomic output classes in the present study is moti-vated by the latte~ results.3 Algorithm and experimental setup3.1  Memory-based learning in IB I - IGIn the experiments reported here, we employ IBI-IG(Daelemaus and Van den Bosch, 1992; Daelemans,Van den Bosch, and Weijters, 1997b), which hasbeen demonstrated to perform adequately, and sig-nitleant\]y better than eager-learning algorithms onthe os task (Van den Bosch, 1997).
ZBI-IG con-structs an instance base daring learning.
An in-stance in the instance base consists of a fixed-lengthvector of n feature-value pairs (here, n = 7), an in-formation field containing the classification of thatparticular feature-value v ctor, and an informationfield containing the occurrences of the instance withits classification in the full training set.
The lat-ter information field thus enables the storage of in-stance types rather than the more extensive storageof identical instance tokens.
After the instance baseis built, new (test) instances are classified by match-ing them to all instance types in the instance base,and by calculating with each match the distance be-tween the new instance X and the memory instancetype Y, A(X, Y), using the function given in Eq.
1:f tA(X, Y) = E W(\[i)6(Xi, Yi), (1)i= lwhere W(fi) is the weight of the ith feature, and6(zl, yi) is the distance between the values of theith fcature in the instances X and Y.
When thevalues of the instance features are symbolic, as withthe Gs task (i.e., feature values are letters), a simpledistance function is used (Eq.
2):6(Xi, Y/) = 0 i f  Xi = Yi else 1.
(2)The classification of the memory instance type Ywith the smallest A(X,Y)  is then taken as the clas-sification of X.
This procedure is also known as1-NN, i.e., a search for the single nearest neighbour,the simplest variant of k-NN (Devijver and Kittler,1982).The weighting function of IBI-IG, W(fi), repre-sents the information gain of feature fi.
Weight-ing features in k-NN ~ezs  such as IB 1-IG is anactive field of research (cf.
(Wettschereck, 1995;Wettschereck, Aha, and Mohrl, 1997), for compre-hensive overviews and discussion).
Information gainis a function from information theory also used inzv3 (Qnlnlan, 1986) and c4.5 (Quinlan, 1993).
Theinformation gain of a feature expresses its relativerelevance compared to the other features when per-forming the mapping from input to classification.The idea behind computing the information gainof features is to interpret he training set as an in-formation source capable of generating a number ofmessages (i.e., classifications) with a certain proba-bility.
The information entropy/it of such an infor-mation source can be compared in turn for each ofvan den Bosch and Daelemans 197 Memory-Based Learning of Word Pronunciationthe features characterising the instances (let n equalthe number of features), to the average informationentropy of the information source when the value ofthose features are known.Data-base information entropy H(D) is equal tothe number of bits of information eeded to knowthe classification given an instance.
It is computedby equation 3, where p/ (the probability of classifi-cation i) is estimated by its relative frequency in thetraini~,g set.H(D) = - pjog p  (3)iTo determine the information gain of each of the nfeatures f l - .
.
f,~, we compute the average informa-tion entropy for each feature and subtract it f~omthe information entropy of the data base.
To com-pute the average information entropy for a featurefi, given in equation 4, we take the average informa-tion entropy of the data base restricted to each pos-sible value for the feature.
The expression D\[y~=~\]refers to those patterns in the data base that havevalue vj for feature fi, j is the number of possiblevalues of f~, and V is the set of possible values forfeature f~.
Finally, IDI is the number of patterns inthe (sub) data base.IDLt'="J\]I (4)IDl ~j6VInformation gain of feature f~ is then obtained byequation 5.G(I,) = H(D) - H(D , 1) (5)Using the weighting function W(fi) acknowledgesthe fact that for some tasks, such as the current GStask, some features axe fax more relevant (impor-tant) than other features.
Using it, instances thatmatch on a feature with a relatively high informa-tion gain axe regarded as less distant (more alike)than instances that match on a feature with a lowerinformation gain.Finding a nearest neighbour to a test instance mayresult in two or more candidate ne~aest-neighbourinstance types at an identical distance to the test in-stance, yet associated with different classes.
The im-plementation flBl-IG used here handles uch easesin the following way.
First, IBI-IG selects the classwith the highest occurrence within the merged set ofclasses of the best-mateblng instance types.
In caseof occurrence ties, the classification is selected thathas the highest overall occurrence in the training set.
(Daehmans, Van den Bosch, and Weijters, 1997b).3.2 SetupWe performed a series of experiments in which m 1-IG is applied to the Gs data set, systematically editedaccording to each of the three tested criteria (plusthe baseline random criterion) described in the nextsection.
We performed the following global proce-dure:1.
We partioned the full Gs data set into a trainingset of 608,228 instances (90% of the full dataset) and a test set of 67,517 instances (10%).For use with IB 1-IG, which stores instance typesrather than instance tokens, the data set was re-duced to contain 222,601 instance types (i.e.,unique combinations of feature-value vectorsand their classifications), with frequency infor-mation.2.
For each exceptionality criterion (i.e., typ-icality, class prediction strength, friendly-neighbourhood size, and random selection),(a) we created four edited instance bases byremoving 1%, 2%, 5%, and 10% of themost exceptional instance types (accordingto the criterion) from the training set, re-spectively.
(b) For each of these increasingly edited train-ing sets, we performed one experiment inwhich IBI-IG was trained on the editedtraining set, and tested on the originalunedited test set.4 Three  es t imat ions  o fexcept iona l i tyWe investigate three methods for estimating the(degree of) exceptionality of instance types: typ-icality, class prediction strength, and f~iendly-neighbouthood size.4.1 Typical i tyIn its common meaning, "typicality" denotesroughly the opposite of exeeptionality; atypicalitycan be said to be s synonym of exceptionality.
Weadopt a definition from (Zhang, 1992), who proposesa typicality function.
Zhang computes typiealitiesofiustance types by taking both their feature valuesand their classifications into account (Zhang, 1992).He adopts the notions of Jaffa.concept similarity/andinter-concept similarity (Rosch and Mervis, 1975) todo this.
First, Zhang introduces a distance func-tion slmilsr to Equation 1, in which W(fi) = 1.0for all features (i.e., fiat Euclidean distance ratherthan information-gain weighted istance), in whichthe distance between two instances X and Y is nor-malised by dividing the summed squared istance byn, the number of features, and in which 6(zi, 9i) isgiven as Equation 2.
The normalised istance func-tion used by Zhang is given in Equation 6.A(x ,y )  = _1n i=1van den Bosch and Daelemans 198 Memory-Based Learning of Word PronunciationIIIIlIII//Ii/I/////IThe intra-concept similarity of instance X withclassification C is its similarity (i.e., 1-distance)with all instances in the data set with the same clas-sification C: this subset is referred to as X's family,Fara(X).
Equation 7 gives the intra-concept simi-laxity function In~ra(X) (\]Fam(X)\[being the num-ber of instances in X's family, and Faro(X) ~ the ithinstance in that family).1 I~'am(X)lIn t ra(X)_ lFam(X)} ~ 1.0-Z~(X, Fa,~(X)')i= l  (7)All remaining instances belong to the subset of un-related instances, Unr(X).
The inter-concept simi-larity of an instance X, Inter(X), is given in Equa-tion 8 (with \[Unr(X)\[ being the number of instancesunrelated to X, and Unr(X)" the ith instance inthat subset).1 IV'~,(x)lI,~e~(X) = i~rnrCX) I ~ 1 .0 -a (X ,  U,r(X)')i----1(s)The typicality of an instance X, Typ(X), is the quo-tient of X's intra-concept similarity and X's inter-concept similarity, as given in Equation 9.~nt~a(X) (9)Typ( X ) = Inter(X)An instance type is typical when its intra-conceptsimilarity is laxger than its inter-concept similar-ity, which results in a typicality larger than 1.An instance type is atypical when its intra-conceptsimilarity is smaller than its inter-concept similar-ity, which results in a typicality between 0 and 1.Around typicality value 1, instances cannot be sen-sibly called typical or atypical; (Zhang, 1992) refersto such instances as boundary instances.In our experiments, we compute the typicality ofall instance types in the training set, order themon their typicality, and remove 1%, 2%, 5%, and10% of the instance types with the lowest ypicality,i.e., the most atypical instance types.
In addition tothese four experiments, we performed an additionaleight experiments using the same percentages, andediting on the basis of (i) instance types' typicality(by ordering them in reverse order) and (il) their in-difference towards typicality or atypicality (i.e., thecloseness of their typicality to 1.0, by ordering themin order of the absolute value of their typicality sub-tracted by 1.0).
The experiments with removing typ-ical and boundary instance types provide interestingcomparisons with the more intuitive diting of atyp-ical instance types.Table 2 provides examples of four atypical, bound-ary, and typical instance types found in the train-ing set.
Globally speaking, (i) the set of atypicalinstances tend to contain foreign spellings of loanvan den Bosch and Daelemans 199words; (ii) there is no clear characteristic of bound-ary instances; ~and (iii) 'certain' pronunciations, i.e.,instance types with high typicality values often in-volve instance types of which the middle letters areat the beginning of words or immediately followinga hyphen, or high-frequency instance types, or in-stance types mapping to a low-frequency class thatalways occurs with a certain spelling (dass frequencyis not accounted for in Zhang's metric).4.2 Class-predictlon strengthA second estimate of exceptionality is to measurehow well an instance type predicts the class ofall instance types within the training set (includ-ing itself).
Several functions for computing class-prediction strength ave been proposed, e.g., as acriterion for removing instances in memory-based(k-nn) learning algorithms, such as m3 (Aha, Ki-bier, and Albert, 1991) (cf.
earlier work on editedk-nn (Wilson, 1972; Voisin and Devijver, 1987));or for weighting instances in the Each\[ algorithm(Salzberg, 1990; Cost and Salzberg, 1993).
We choseto implement he straightforward class-predictionstrength function as proposed in (Salzberg, 1990)in two steps.
First, we count (a) the number oftimes that the instance type is the nearest neigh-bour of another instance type, and (b) the numberof occurrences that when the instance type is a near-eat neighbour of another instance type, the classesof the two instances match.
Second, the instance'sclass-prediction strength is computed by taking theratio of (b) over (a).
An instance type with class-prediction strength 1.0 is a perfect predictor of itsown class; a class-prediction strength of 0.0 indicatesthat the instance type is a bad predictor of classesof other instances, presumably indicating that theinstance type is exceptional.We computed the class-prediction strength of allinstance types in the training set, ordered the in-stance types according to their strengths, and cre-ated edited training sets with 1%, 2%, 5%, and10% of the instance types with the lowest classprediction strength removed, respectively.
In Ta-ble 3, four sample instance types axe displayedwhich have elass-prediction strength 0.0, i.e., thelowest possible strength.
They are never a correctnearest-ncighbour match, since they all have higher-frequency counterpart types with the same featurevalues.
For example, the letter sequence _ algo oc-curs in two types, one associated with the pronun-ciation / '~ /  (via., primary-stressed /re/, or lm inour labelling), as in algorithm and algorithms; theother associated with the pronunciation / '~/ (v iz .secondary-stressed /~/  or 2se), as in algorithmic.The latter instance type occurs less frequently thanthe former, which is the reason that the class of theformer is preferred over the latter.
Thus, an am-biguous type with a minority class (a minority am-biguity) can never be a correct predictor, not evenMemory-Based Learning of Word Pronunciationatypicalfeature values class I t.ypicalityureaucr OOU 0.428freudia Oar 0.442_tissue Of 0.458_czech O- 0.542instance typesII boundary feature values class typicalitycheques Oks 1.000elgium_ O- 1.000laby__ Ova 1.000manna__ O- 1.000I typicalfeature values class typicality__oiff l:)z 7.338etectio 0kf 8.452ow-by-b 0b 9.130ng-iron 2van 12.882Table 2: Examples of atypical (left), boundary (middle), and typical (left) instance types in the training set.For each instance (seven letters and a class mapping to the middle letter), its typicality value is given.feature values class cps__algo 2re 0.0ck-benc lb 0.0erby__ Om 0.0reface_ Oez 0.0Table 3: Examples of instance types with the lowestpossible class prediction strength (cps) 0.0.for itself, when using ml-iG as a classifier, whichalways prefers high f~equency over low f~equency incase of ties.4.3 Pr lendly-nelghbourhood sizeA third estimate for the exceptiona\]ity of instancetypes is counting by how many nearest neighbours ofthe same class an instance type is surrounded in in-stance space.
Given a training set of instance types,for each instance type a ranking can be made oral\] ofits nearest neighbours, ordered by their distance tothe instance type.
The number of neaxest-neighbouzinstance types in this ranking with the same class,henceforth refe~ed to as the frendly-neighbourhoodsize, may range between 0 and the total number ofinstance types of the same class.
When the friendlyneighbourhood is empty, the instance type only hasneaxest neighbouts of different classes.
The argu-mentation to regard a small friendly neighbourhoodas an indication of an instance type's exceptionality,follows f~om the same argumentation asused withe!~s-prediction strength: when an instance type hasnearest neighbours of different classes, it is vice versaa bad predictor for those classes.
Thus, the smalleran instance type's friendly neighboaxhood, the moreit could be regarded exceptional.To illustrate the  computation of frend\]y-neighbou~hood size, Table 4 lists fou~ examples ofpossible neaxest-neighbou~ zankings (truncated atten nearest neighbours) with their respective num-ber of friendly neighbours.
The Table shows thatthe number of friendly neighboaxs i the number ofslmilaxly-labeled instances counted from left to rightin the ranking, until a disslmilaxly-labeled instanceoccurs .feature values class fns_-edib 2E: 0__edib 1E: 0echnocr In 0soiree_ Or 0Table 5: Examples of instance types with the lowestpossible f~iendly-neighbourhood size (fns) 0, i.e., nofriendly neighbours.Friendly-neighbouthood sizeand class-predictionstrength a~e related functions, but differ in thei~treatment of class ambiguity.
As stated above, in-stance types may receive a class-prediction strengthof 0.0 when they axe minority ambiguities.
Countinga friendly neighbouzhood does not take class ambi-guity into account; each of a set of ambiguous typesnecessarily has no friendly neighbouzs, ince they axeeachothez's nearest neighbouts with different classes.Thus, friendiy-neighbourhood sizedoes not discrim-inate between minority and majority ambiguities.
InTable 5, four sample instance types axe listed withfrendly-neighbouthood size0.
While some of theseinstance types without friendly neighbours in thetraining set (perhaps with friendly neighbours in thetest set) are minority ambiguities (e.g., __edib 2~),others are majority ambiguities (e.g., __edib 1~),while others are not ambiguous at all but simplyhave a nearest neighbouz at some distance with adifferent class (e.g., soiree_ 0z).5 ResultsFigure 1 displays the generalisatiou acc~acies interms of incorrectly classified test instances obtainedwith all performed experiments.
The leftmost pointin the Figure, f~om which all lines originate, indi-cates the performance of IBI?IG when trained onthe full data set of 222,601 types, viz.
6.42% in-correctly classified test instances (when computed interms of incorrectly pronounced test words, IBI-IGpronounces 64.61 of all test words flawlessly).The line graph representing the fou~ expemnentsin which instance types are removed randomly canbe seen as the baseline graph.
It can be expectedII||mkvan den Bosch and Daelemans 200 Memory-Based L arning of Word PronunciationIIIIIIIIIIIIIIIIIIIIIIIIIIIInearest neighbour rank #1 2 3 4 5 6 7 8 9 10 ?~o l  x2  03 03 03 04 x4  ?5 x5  ?6 1o l  o l  o l  o l  o1 o l  o2 02 03 x4  9x2  02 02 02 o2 02 x3  ?3 x3  x4  0o l  o l  o l  x3  x4  x4  x4  x4  x 5 o6 3Table 4: Four examples of nearest-neighbour rankings and their respective numbers of friendly neighbours(fa).
Each ranked nearest neighbour is identified by its match (o) or mismatch (?)
with the target instancethe ranking is computed for, and a number denoting its distance to the target instance.that removing instances randomly leads to a degra-dation of generalisation performance.
The upwardcurve of the line graph denoting the experimentswith random selection indeed shows degrading per-formanee with increasing numbers of left-out in-stance types.
The relative decrease in generalisationaccuracy is 2.0% when 1% of the training material isremoved randomly, 3.8% with 2% random removal,10.7% with 5% random removal, and 20.7% with10% random removal.Surprisingly, the only experiments showing lowerperformance degradation than removal by randomselection are those with class-prediction strength;the other criteria for removing exceptional instanceslead to worse degradations.
It does not matterwhether instance types are removed on grounds oftheir typicality: apparently, a markedly low, neutral,or high typicality value indicates that the instancetype is (on average) important, rather than remov-able.
The same applies to friendly-neighbourhoodsize: instances with small neighbourhood sizes ap-pear to contribute significantly to performance ontest material.
It is remarkable that the largest er-rors with 1% and 2% removal are obtained withthe friendly-neighbourhood sizecriterion: it appearsthat on average, the instances with few or no nearestneighbours are important in the classification of testmaterial.When using class-prediction strength as removalcriterion, performance does not degrade until about5% of the instance types with the lowest strengthare removed from memory.
The reason is that c|_~ss-prediction strength is the only criterion that detectsminority ambiguities, i.e., instance types with pre-diction strength 0.0, that cannot contribute to classi-fication since they are always overshadowed by theircounterpart instance types with majority classes,even for their own classification.
In the tralni~g set,9,443 instance types are minority ambiguities, i.e.,4.2% of the instance types (accounting for 3.8% ofthe instance tokens in the original token set).Thus, among the tested methods for reducingthe memory needed for storing an instance base inmemory-based learning, only two relatively trivialmethods are performance-preserving while account-ing for a substantial reduction in the amount ofmemory needed by IB 1-IG:1.
Replacing instance tokens by instance types ac-counts for a reduction of about 63% of mem-ory needed to store instances, excluding thememory needed to store frequency information.When frequency information is stored in twobytes per instance type, the memory reductionis about 54%..
Removing instance types that are minority am-bigulties on top of the type/token-reduction ac-counts only for an additional memory reduc-tion of 2%, i.e., for a total memory reductionof 65%; 56% with two-byte frequency informa-tion stored per instance.6 D iscuss ion  and  fu ture  researchAs previous research has suggested (Daelemans,1996; Daelemans, Van den Bosch, and Weijters,1997a; Van den Bosch, 1997), keeping full mem-ory in memory-based l arning of word pronunciationstrongly appears to yield optimal generalisation ac-curacy.
The experiments in this paper show that op-timi~tion of memory use in memory-based l arningwhile preserving eneralisation accuracy can only beperformed by (i) replacing instance tokens by in-stance types with frequency information, and (ii)removing minority ambiguities.
Both optimi~tionscan be performed straightforwardly; minority ambi-guities can be traced with less effort than by usingclass-prediction strength.
Our implementation fIB1-I6 described in (Daelemans and Van den Bosch,1992; Daelemans, Van den Bosch, and Weijters,1997b) already makes use of this knowledge, albeitpartially (it stores class distributions with letter-window types).Our results also show that atypicality, non-typic-ality, and typicality (Zhang, 1992), and friendly-neighbourhood size are all estimates of exception-ality that indicate the importance of instance typesfor classification, rather than their removability.
Asfar as these estimates of exeeptionality are viable,our results suggest hat exceptions hould be keptin memory and not be thrown away.van den Bosch and Daelemans 201 Memory-Based Learning of Word Pronunciation12.011.0o~v10.0(Dt -O 9.0._~v 8.0,t -  (D7.06.0atypical o?
typical ......non-typical --~ ....small neighbourhood .
*  ......low prediction strength .
.
.
.
.
.
.  "
/ x?
random -.. .... .
_ .
~  ........ :::::::::::::::::::::::::........................... : .
~:::::~-.
............ .
.
.  "
I .
.
.0 .
.
.o"  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 00 10000 15000 20000number of removed instances typesFigure 1: Generallsation errors (percentages of incorrectly classified test instances of TRIBL-IG, with increasednumbers of edited instances, according to the tested exeeptionality criteria atypical, typical, boundary,small neighbourhood, low prediction strength, and random selection.
Performances, denoted by points, aremeasured when 1%, 2%, 5%, and 10% of the most exceptional instance types ate edited.Lazy vs. eager; not stable vs. unstableF~om the results in this paper and those reportedeatlier (Daelemans, Van den Bosch, and Weijters,1997a; Van den Bosch, 1997), it appeats that nocompromise can be made on memory-base learningin terms of abstraction by forgetting without los-ing generalisation accuracy.
Consistently ower per-formances axe obtained with algorithms that forgetby constructing decision trees or connectionist net-works, or by editing instance types.
Generalisationaccuracy appears to be related to the dimension lazy-eager leaxning; for the Gs task (and for many otherlanguage tasks, (Daelemans, Van den Bosch, andWeijtezs, 1997a)), it is demonstrated that memory-based lazy leatning leads to the best generalisationaccuracies,Another explanation for the difference in per-formance between decision-tree, connectionist, andediting methods versus pure memory-based leaxn-ing is that the former generally display high ~ar/-ance, which is the portion of the generalisation errorcaused by the u~tabili~/of the learning algorithm(Breiman, 1996a).
An algorithm is unstable whensmall perturbations in the learning material lead tolarge differences in induced models, and stable oth-ezwise; pure memory-based learning algorithms axesaid to be very stable, and decision-tree algorithmsand conneetionist learning to be unstable (Breiman,1996a).
High variance is usually coupled with lowbias, i.e., unstable leaxning algorithms with highvaziance tend to have few limitations in the fxeedomto approximate the task or function to be leaxned)(Bzeiman, 1996b).
Breiman points out that oftenthe opposite also holds: a stable classitiez with alow variance can display a high bias when it can-not represent data adequately in its available set ofmodels, but it is not cleat whether or how this ap-plies to pure memory-based leatning as in ml-IG;its success in representing the Gs data and otherlanguage tasks quite adequately would rather sug-gest that IB 1-I6 has both low vatiance and low bias.Apatt fzom the possibility that the lazy and eagerleatning algorithms investigated here and in eatllezwork do not have a strongly contrasting bias, we con-jecture that the editing methods discussed here, andsome specific decision-tree l axning algorithms inves-tigated eaxlier (i.e., IGTItEE (Daclemuns, Van denBosch, and Weijters, 1997b), a decision tree learn-ing algorithm that is an approximate optimisationof IBI-IG) have a slmilat vatia~lce to that of IB1-IG; they axe virtually as stable as ~I-IQ.
We basethis conjecture on the fact that the standard evi-ations of both decision-tree l arning and memory-based learning trained and tested on the GS data axenot only very small (in the order of 1/10 percents),but also hatdiy different (cf.
(Van den Bosch, 1997)for details and examples).
Only counectionist net-works trained with back-propagation a d decision-tree leaxning with pruning display latger standarddeviations when accuracies ate averaged over exper-van den Bosch and Daelemans 202 Memory-Based Learning of Word PronunciationIIIIIIIIIIlIIIIIIl//////l////Aiments (Van den Bosch, 1997); the stable-unstabledimension might play a role there, but not in thedifference between pure memory-based learning andedited memory-based learning.Future researchThe results of the present study suggest thatthe following questions be investigated in future re-search:, The tested criteria for editing can be employedas instance weights as in EACH (Salzberg,1990) and PEI3LS (Cost and Salzberg, 1993),rather than as criteria for instance removal.Instance weighting, preserving pure memory-based learning, may add relevant informationto similarity matching, and may improve IB1-IG~s performance..
Different data sets of different sizes may con-tain different portions of atypical instances orminority ambiguities.
Moreover, data sets maycontain pure noise.
While atypical or excep-tional instances may (and do) return in testmaterial, the chances of noise to return is rel-ativdy minute.
Our results generalise to datasets with approximately the characteristics ofthe Gs dataset.
Although there are indica-tions that data sets representing other languagetasks indeed share some essential characteristics(e.g., memory-based learning is consistently thebest-performlng algorithm), more investigationis needed to make these characteristics explicit.AcknowledgementsWe thank the members of the ILK group, Ton Weij-ters, and Eric Postma for fruitful discussions, andthe anonymous reviewers for relevant comments andsuggestions.Re ferencesAha, D. W., editor.
1997.
Lazy learning.
Dordrecht:Kluwet Academic Publishers.
reprinted from: Ar-tificial Intelligence Review, 11:1-5.Aha, D. W., D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 7:37-66.Breiman, L. 1996a.
Bagging predictors.
MachineLearning, 24(2).Breiman, L. 1996b.
Bias, variance and arcing elas-sifters.
Technical Report 460, University of Cali-fornia, Statistics Department, Berkeley, CA.Burnage, G., 1990.
CELEX: A guide for users.
Cen-tre for Lexical Information, Nijmegen.Cost, S. and S. Salzberg.
1993.
A weighted near-est neighbor algorithm for learning with symbolicfeatures.
Machine Learning, 10:57-78.van den Bosch and Daelemans 203Cover, T. M. and P. E. Hart.
1967.
Nearest neigh-bor pattern classification.
Institute of Eledricaland Electronics Engineers Transactions on Infor-mation Theory, 13:21-27.Daelemans, W. 1996.
Abstraction considered harm-ful: lazy learning of language processing.
In H. J.Van den Herik and A. Weijters, editors, Proceed-ings of the Sizth Belgian-Dutch Conference onMachine Learning, pages 3-12, Maastricht, TheNetherlands.
MXTRIK$.Daelemans, W., S. Gillis, and G. Durieux.
1994.The acquisition of stress: a data-oriented ap-proach.
Coraputational Linguistics, 20(3):421-451.Dadema~, W. and A.
Van den Bosch.
1992.
Gen-eralisation performance ofbackpropagation learn-ing on s syllabification task.
In M. F. L Drossaersand A. Nijholt, editors, TWLT3: Connectionismand Natural Language Processing, pages 27-37,Enschede.
Twente University.Daelemans, W., A.
Van den Bosch, and A. Weij-ters.
1997a.
Empirical earning of natural an-guage processing tasks.
Lecture Notes in Artifi-cial Intelligence, , number 1224, pages 337-344.Berlin: Springer-Verlag.Daelemans, W., A.
Van den Bosch, and A. Weij-ters.
1997b.
IGTree: using trees for classificationin lazy learning algorithms.
Artificial IntelligenceReview, 11:407--423.Devijver, P .
.A .
and J. Kittler.
1982.
Patternrecognition.
A statistical approach.
London, UK:Prentice-HalLDietterich, T. G., H. Hild, and G. Bakiri.
1990.
Acomparison of ID3 and backpropagation for En-glish text-to-speech mapping.
Technical Report90-20-4, Oregon State University.Kolodner, J.
1993.
Case-based reasoning.
San Ma-teo, CA: Morgan Kanfmann.Mitchell, T. 1997.
Machine learning.
New York,NY: McGraw Hill.Quinlan, J. R. 1986.
Induction of decision trees.Machine LeaNing, 1:81-206.Quinlsn, J. R. 1993. c4.5: Programs for Machinelearning.
San Mateo, CA: Morgan Kaufi~mm.Roach, E. and C. B. Mervis.
1975.
Fam~y resem-blances: studies in the internal structure of cate-gories.
Cognitive Psychology, 7:??-?
?Rumelhart, D. E., G. E. Hinton, and R. J. Williams.1986.
Learning internal representations by errorpropagation.
In D. E. Rumelhart and J. L. Mc-Clelland, editors, Parallel Distributed Processing:gzplorations in the Microstructure of Cognition.Cambridge, MA: The MIT Press, pages 318-362.Memory-Based Learning of Word PronunciationSalzberg, S. 1990.
Learning with nested generalisedezemplars.
Norwell, MA: Klawer Academic Pub-lishers.Sejnowski, T. J. and C. S. Rosenberg.
1987.
Paral-lel networks that learn to pronounce English text.gomplez Systems, 1:145-168.Shavlik, J. W., R. J. Mooney, and G. G. Towell.1991.
Symbolic and neural earning algorithms:An experimental comparison.
Machine Learning,6:111-143.Stanfdl, C. and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29(12):1213-1228.Van den Bosch, A.
1997.
Learning to pronouncewritten words, a study in inductive language learn-ing.
Ph.D. thesis, Universiteit Manstricht.Van den Bosch, A., W. Daelemans, and A. Weijters.1996.
Morphological nalysis as classification: aninductive-learning approach.
In K. Oflazer andH.
Somers, editors, Proceedings of the Second In-ternational Conference on New Methods in Nat-ural Language Processing, NeMLaP-~, Ankara,Turkey, pages 79-89.Voisin, J. and P. A. Devijver.
1987.
An applica-tion of the Multiedit-Condensing technique to thereference selection problem in a print recognitionsystem.
Pattern Recognition, 5:465-474.Wettschereck, D. 1995.
A study of distance-basedmachine.learning algorithms.
Ph.D. thesis, Ore-gon State University.Wettschereck, D., D. W. Aha, and T. Mohri.
1997.A review and empirical evaluation of featureweighting methods for a class of lazy learning algo-rithms.
Artificial Intelligence Revietv, 11:273-314.Wilson, D. 1972.
Asymptotic propexties of near-eat neighbor ules using edited data.
Instit~zte ofElectrical and Electronic Engineers Transactionson Systems, Man and Cybernetics, 2:408-421.Wolpert, D. H. 1990.
Constructing a generalizersuperior to NETtalk via a mathematical theory ofgeneralization.
Neural Networks, 3:445--452.Zhang, J.
1992.
Selecting typical instances ininstance-based learning.
In Proceedings of the In-ternational Machine Learning Conference 199~,pages 470-479.IIIIIIIIIIIIIIIIIIIIIImvan den Bosch and Daelemans 204 Memory-Based Learning of Word Pronunciation
