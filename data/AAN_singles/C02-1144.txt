Concept Discovery from TextDekang Lin and Patrick PantelDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{lindek,ppantel}@cs.ualberta.caAbstractBroad-coverage lexical resources such asWordNet are extremely useful.
However,they often include many rare senses whilemissing domain-specific senses.
We presenta clustering algorithm called CBC (Cluster-ing By Committee) that automaticallydiscovers concepts from text.
It initiallydiscovers a set of tight clusters calledcommittees that are well scattered in thesimilarity space.
The centroid of themembers of a committee is used as thefeature vector of the cluster.
We proceed byassigning elements to their most similarcluster.
Evaluating cluster quality hasalways been a difficult task.
We present anew evaluation methodology that is basedon the editing distance between outputclusters and classes extracted from WordNet(the answer key).
Our experiments show thatCBC outperforms several well-knownclustering algorithms in cluster quality.1 IntroductionBroad-coverage lexical resources such asWordNet are extremely useful in applicationssuch as Word Sense Disambiguation (Leacock,Chodorow, Miller 1998) and Question-Answering (Pasca and Harabagiu 2001).However, they often include many rare senseswhile missing domain-specific senses.
Forexample, in WordNet, the words dog, computerand company all have a sense that is a hyponymof person.
Such rare senses make it difficult fora coreference resolution system to use WordNetto enforce the constraint that personal pronouns(e.g.
he or she) must refer to a person.
On theother hand, WordNet misses the user-interface-object sense of the word dialog (as often used insoftware manuals).
One way to deal with theseproblems is to use a clustering algorithm toautomatically induce semantic classes (Lin andPantel 2001).Many clustering algorithms represent a clusterby the centroid of all of its members (e.g., K-means) (McQueen 1967) or by a representativeelement (e.g., K-medoids) (Kaufmann andRousseeuw 1987).
When averaging over allelements in a cluster, the centroid of a clustermay be unduly influenced by elements that onlymarginally belong to the cluster or by elementsthat also belong to other clusters.
For example,when clustering words, we can use the contextsof the words as features and group together thewords that tend to appear in similar contexts.
Forinstance, U.S. state names can be clustered thisway because they tend to appear in the followingcontexts:(List A) ___ appellate court campaign in ______ capital governor of ______ driver's license illegal in ______ outlaws sth.
primary in ______'s sales tax senator for ___If we create a centroid of all the state names, thecentroid will also contain features such as:(List B) ___'s airport archbishop of ______'s business district fly to ______'s mayor mayor of ______'s subway outskirts of ___because some of the state names (like New Yorkand Washington) are also names of cities.Using a single representative from a clustermay be problematic too because each individualelement has its own idiosyncrasies that may notbe shared by other members of the cluster.In this paper, we propose a clustering algo-rithm, CBC (Clustering By Committee), inwhich the centroid of a cluster is constructed byaveraging the feature vectors of a subset of thecluster members.
The subset is viewed as acommittee that determines which other elementsbelong to the cluster.
By carefully choosingcommittee members, the features of the centroidtend to be the more typical features of the targetclass.
For example, our system chose thefollowing committee members to compute thecentroid of the state cluster: Illinois, Michigan,Minnesota, Iowa, Wisconsin, Indiana, Nebraskaand Vermont.
As a result, the centroid containsonly features like those in List A.Evaluating clustering results is a very difficulttask.
We introduce a new evaluation methodol-ogy that is based on the editing distance betweenoutput clusters and classes extracted fromWordNet (the answer key).2 Previous WorkClustering algorithms are generally categorizedas hierarchical and partitional.
In hierarchicalagglomerative algorithms, clusters areconstructed by iteratively merging the mostsimilar clusters.
These algorithms differ in howthey compute cluster similarity.
In single-linkclustering, the similarity between two clusters isthe similarity between their most similarmembers while complete-link clustering uses thesimilarity between their least similar members.Average-link clustering computes this similarityas the average similarity between all pairs ofelements across clusters.
The complexity ofthese algorithms is O(n2logn), where n is thenumber of elements to be clustered (Jain, Murty,Flynn 1999).Chameleon is a hierarchical algorithm thatemploys dynamic modeling to improveclustering quality (Karypis, Han, Kumar 1999).When merging two clusters, one might considerthe sum of the similarities between pairs ofelements across the clusters (e.g.
average-linkclustering).
A drawback of this approach is thatthe existence of a single pair of very similarelements might unduly cause the merger of twoclusters.
An alternative considers the number ofpairs of elements whose similarity exceeds acertain threshold (Guha, Rastogi, Kyuseok1998).
However, this may cause undesirablemergers when there are a large number of pairswhose similarities barely exceed the threshold.Chameleon clustering combines the twoapproaches.K-means clustering is often used on large datasets since its complexity is linear in n, thenumber of elements to be clustered.
K-means isa family of partitional clustering algorithms thatiteratively assigns each element to one of Kclusters according to the centroid closest to itand recomputes the centroid of each cluster asthe average of the cluster?s elements.
K-meanshas complexity O(K?T?n) and is efficient formany clustering tasks.
Because the initialcentroids are randomly selected, the resultingclusters vary in quality.
Some sets of initialcentroids lead to poor convergence rates or poorcluster quality.Bisecting K-means (Steinbach, Karypis,Kumar 2000), a variation of K-means, beginswith a set containing one large cluster consistingof every element and iteratively picks the largestcluster in the set, splits it into two clusters andreplaces it by the split clusters.
Splitting a clusterconsists of applying the basic K-meansalgorithm ?
times with K=2 and keeping thesplit that has the highest average element-centroid similarity.Hybrid clustering algorithms combinehierarchical and partitional algorithms in anattempt to have the high quality of hierarchicalalgorithms with the efficiency of partitionalalgorithms.
Buckshot (Cutting, Karger,Pedersen, Tukey 1992) addresses the problem ofrandomly selecting initial centroids in K-meansby combining it with average-link clustering.Buckshot first applies average-link to a randomsample of n  elements to generate K clusters.
Itthen uses the centroids of the clusters as theinitial K centroids of K-means clustering.
Thesample size counterbalances the quadraticrunning time of average-link to make Buckshotefficient: O(K?T?n + nlogn).
The parameters Kand T are usually considered to be smallnumbers.3 Word SimilarityFollowing (Lin 1998), we represent each wordby a feature vector.
Each feature corresponds toa context in which the word occurs.
Forexample, ?threaten with __?
is a context.
If theword handgun occurred in this context, thecontext is a feature of handgun.
The value of thefeature is the pointwise mutual information(Manning and Sch?tze 1999 p.178) between thefeature and the word.
Let c be a context andFc(w) be the frequency count of a word woccurring in context c. The pointwise mutualinformation between c and w is defined as:( )( ) ( )NjFNwFNwFcwjciicmi ??
?=,where N = ( )?
?i ji jF  is the total frequencycounts of all words and their contexts.
A well-known problem with mutual information is thatit is biased towards infrequent words/features.We therefore multiplied miw,c with a discountingfactor:( )( )( ) ( )( ) ( ) 11 +????????????????
?+ ?
??
?i jcii jciccjF,wFminjF,wFminwFwFWe compute the similarity between two wordswi and wj using the cosine coefficient (Salton andMcGill 1983) of their mutual informationvectors:( ) ????
?=ccwccwccwcwjijijimimimimiw,wsim224 CBC AlgorithmCBC consists of three phases.
In Phase I, wecompute each element?s top-k similar elements.In our experiments, we used k = 20.
In Phase II,we construct a collection of tight clusters, wherethe elements of each cluster form a committee.The algorithm tries to form as many committeesas possible on the condition that each newlyformed committee is not very similar to anyexisting committee.
If the condition is violated,the committee is simply discarded.
In the finalphase of the algorithm, each element is assignedto its most similar cluster.4.1.
Phase I: Find top-similar elementsComputing the complete similarity matrixbetween pairs of elements is obviouslyquadratic.
However, one can dramatically reducethe running time by taking advantage of the factthat the feature vector is sparse.
By indexing thefeatures, one can retrieve the set of elements thathave a given feature.
To compute the top similarwords of a word w, we first sort w?s featuresaccording to their mutual information with w.We only compute pairwise similarities betweenw and the words that share a high mutualinformation feature with w.4.2.
Phase II: Find committeesThe second phase of the clustering algorithmrecursively finds tight clusters scattered in thesimilarity space.
In each recursive step, thealgorithm finds a set of tight clusters, calledcommittees, and identifies residue elements thatare not covered by any committee.
We say acommittee covers an element if the element?ssimilarity to the centroid of the committeeexceeds some high similarity threshold.
Thealgorithm then recursively attempts to find morecommittees among the residue elements.
Theoutput of the algorithm is the union of allcommittees found in each recursive step.
Thedetails of Phase II are presented in Figure 1.In Step 1, the score reflects a preference forbigger and tighter clusters.
Step 2 givespreference to higher quality clusters in Step 3,where a cluster is only kept if its similarity to allpreviously kept clusters is below a fixedthreshold.
In our experiments, we set ?1 = 0.35.Input: A list of elements E to be clustered, asimilarity database S from Phase I, thresh-olds ?1 and ?2.Step 1: For each element e ?
ECluster the top similar elements of e from Susing average-link clustering.For each cluster discovered c compute thefollowing score: |c| ?
avgsim(c), where|c| is the number of elements in c andavgsim(c) is the average pairwise simi-larity between elements in c.Store the highest-scoring cluster in a list L.Step 2: Sort the clusters in L in descending order oftheir scores.Step 3: Let C be a list of committees, initiallyempty.For each cluster c ?
L in sorted orderCompute the centroid of c by averaging thefrequency vectors of its elements andcomputing the mutual information vectorof the centroid in the same way as we didfor individual elements.If c?s similarity to the centroid of eachcommittee previously added to C is be-low a threshold ?1, add c to C.Step 4: If C is empty, we are done and return C.Step 5: For each element e ?
EIf e?s similarity to every committee in C isbelow threshold ?2, add e to a list of resi-dues R.Step 6: If R is empty, we are done and return C.Otherwise, return the union of C and theoutput of a recursive call to Phase II us-ing the same input except replacing Ewith R.Output: A list of committees.Figure 1.
Phase II of CBC.Step 4 terminates the recursion if no committeeis found in the previous step.
The residueelements are identified in Step 5 and if noresidues are found, the algorithm terminates;otherwise, we recursively apply the algorithm tothe residue elements.Each committee that is discovered in thisphase defines one of the final output clusters ofthe algorithm.4.3.
Phase III: Assign elements to clustersIn Phase III, every element is assigned to thecluster containing the committee to which it ismost similar.
This phase resembles K-means inthat every element is assigned to its closestcentroid.
Unlike K-means, the number ofclusters is not fixed and the centroids do notchange (i.e.
when an element is added to acluster, it is not added to the committee of thecluster).5 Evaluation MethodologyMany cluster evaluation schemes have beenproposed.
They generally fall under twocategories:?
comparing cluster outputs with manuallygenerated answer keys (hereon referred toas classes); or?
embedding the clusters in an applicationand using its evaluation measure.An example of the first approach considers theaverage entropy of the clusters, which measuresthe purity of the clusters (Steinbach, Karypis,and Kumar 2000).
However, maximum purity istrivially achieved when each element forms itsown cluster.
An example of the second approachevaluates the clusters by using them to smoothprobability distributions (Lee and Pereira 1999).Like the entropy scheme, we assume that thereis an answer key that defines how the elementsare supposed to be clustered.
Let C be a set ofclusters and A be the answer key.
We define theediting distance, dist(C, A), as the number ofoperations required to make C consistent with A.We say that C is consistent with A if there is aone to one mapping between clusters in C andthe classes in A such that for each cluster c in C,all elements of c belong to the same class in A.We allow two editing operations:?
merge two clusters; and?
move an element from one cluster toanother.Let B be the baseline clustering where eachelement is its own cluster.
We define the qualityof a set of clusters C as follows:( )( )ABdistACdist,,1?Suppose the goal is to construct a clusteringconsistent with the answer key.
This measurecan be interpreted as the percentage ofoperations saved by starting from C versusstarting from the baseline.We aim to construct a clustering consistentwith A as opposed to a clustering identical to Abecause some senses in A may not exist in thecorpus used to generate C. In our experiments,we extract answer classes from WordNet.
Theword dog belongs to both the Person and Animalclasses.
However, in the newspaper corpus, thePerson sense of dog is at best extremely rare.There is no reason to expect a clusteringalgorithm to discover this sense of dog.
Thebaseline distance dist(B, A) is exactly thenumber of elements to be clustered.We made the assumption that each elementbelongs to exactly one cluster.
The transforma-tion procedure is as follows:1.
Suppose there are m classes in the answerkey.
We start with a list of m empty sets,each of which is labeled with a class in theanswer key.2.
For each cluster, merge it with the setwhose class has the largest number ofelements in the cluster (a tie is brokenarbitrarily).3.
If an element is in a set whose class is notthe same as one of the element?s classes,move the element to a set where it be-longs.dist(C, A) is the number of operations performedusing the above transformation rules on C.abecdeacdb ebacdeabcdeA) B)C) D) E)Figure 2.
An example of applying the transformation rulesto three clusters.
A) The classes in the answer key; B) theclusters to be transformed; C) the sets used to reconstructthe classes (Rule 1); D) the sets after three mergeoperations (Step 2); E) the sets after one move operation(Step 3).Figure 2 shows an example.
In D) the clustercontaining e could have been merged with eitherset (we arbitrarily chose the second).
The totalnumber of operations is 4.6 Experimental ResultsWe generated clusters from a news corpus usingCBC and compared them with classes extractedfrom WordNet (Miller 1990).6.1.
Test DataTo extract classes from WordNet, we firstestimate the probability of a random wordbelonging to a subhierarchy (a synset and itshyponyms).
We use the frequency counts ofsynsets in the SemCor corpus (Landes, Leacock,Tengi 1998) to estimate the probability of asubhierarchy.
Since SemCor is a fairly smallcorpus, the frequency counts of the synsets inthe lower part of the WordNet hierarchy are verysparse.
We smooth the probabilities by assumingthat all siblings are equally likely given theparent.
A class is then defined as the maximalsubhierarchy with probability less than athreshold (we used e-2).We used Minipar 1  (Lin 1994), a broad-coverage English parser, to parse about 1GB(144M words) of newspaper text from the TRECcollection (1988 AP Newswire, 1989-90 LATimes, and 1991 San Jose Mercury) at a speedof about 500 words/second on a PIII-750 with512MB memory.
We collected the frequencycounts of the grammatical relationships(contexts) output by Minipar and used them tocompute the pointwise mutual informationvalues from Section 3.
The test set is constructedby intersecting the words in WordNet with thenouns in the corpus whose total mutualinformation with all of its contexts exceeds athreshold m. Since WordNet has a low coverageof proper names, we removed all capitalizednouns.
We constructed two test sets: S13403consisting of 13403 words (m = 250) and S3566consisting of 3566 words (m = 3500).
We thenremoved from the answer classes the words thatdid not occur in the test sets.
Table 1 summa-rizes the test sets.
The sizes of the WordNetclasses vary a lot.
For S13403 there are 99 classesthat contain three words or less and the largestclass contains 3246 words.
For S3566, 78 classeshave three or less words and the largest classcontains 1181 words.1Available at www.cs.ualberta.ca/~lindek/minipar.htm.6.2.
Cluster EvaluationWe clustered the test sets using CBC and theclustering algorithms of Section 2 and appliedthe evaluation methodology from the previoussection.
Table 2 shows the results.
The columnsare our editing distance based evaluationmeasure.
Test set S3566 has a higher score for allalgorithms because it has a higher number ofaverage features per word than S13403.For the K-means and Buckshot algorithms, weset the number of clusters to 250 and themaximum number of iterations to 8.
We used asample size of 2000 for Buckshot.
For theBisecting K-means algorithm, we applied thebasic K-means algorithm twice (?
= 2 in Section2) with a maximum of 8 iterations per split.
Ourimplementation of Chameleon was unable tocomplete clustering S13403 in reasonable time dueto its time complexity.Table 2 shows that K-means, Buckshot andAverage-link have very similar performance.CBC outperforms all other algorithms on bothdata sets.6.3.
Manual InspectionLet c be a cluster and wn(c) be the WordNetclass that has the largest intersection with c. Theprecision of c is defined as:Table 1.
A description of the test sets in our experiments.DATASETTOTALWORDSm Average #of FeaturesTOTALCLASSESS13403 13403 250 740.8 202S3566 3566 3500 2218.3 150DATASETTOTALWORDSM Avg.
Featuresper Word13403 250 740.83566 3500 2218.3Table 2.
Cluster quality (%) of several clusteringalgorithms on the test sets.ALGORITHM S13403 S3566CBC 60.95 65.82K-means (K=250) 56.70 62.48Buckshot 56.26 63.15Bisecting K-means  43.44 61.10Chameleon n/a 60.82Average-link 56.26 62.62Complete-link 49.80 60.29Single-link 20.00 31.74( ) ( )ccwnccprecision?=CBC discovered 943 clusters.
We sorted themaccording to their precision.
Table 3 shows fiveof the clusters evenly distributed according totheir precision ranking along with their Top-15features with highest mutual-information.
Thewords in the clusters are listed in descendingorder of their similarity to the cluster centroid.For each cluster c, we also include wn(c).
Theunderlined words are in wn(c).
The first clusteris clearly a cluster of firearms and the secondone is of pests.
In WordNet, the word pest iscuriously only under the person hierarchy.
Thewords stopwatch and houseplant do not belongto the clusters but they have low similarity totheir cluster centroid.
The third clusterrepresents some kind of control.
In WordNet, thelegal power sense of jurisdiction is not ahyponym of social control as are supervision,oversight and governance.
The fourth cluster isabout mixtures.
The words blend and mix as theevent of mixing are present in WordNet but notas the result of mixing.
The last cluster is aboutconsumers.
Here is the consumer class inWordNet 1.5:addict, alcoholic, big spender, buyer, client,concert-goer, consumer, customer, cutter, diner,drinker, drug addict, drug user, drunk, eater,feeder, fungi, head, heroin addict, home buyer,junkie, junky, lush, nonsmoker, patron, policy-holder, purchaser, reader, regular, shopper,smoker, spender, subscriber, sucker, taker, user,vegetarian, wearerIn our cluster, only the word client belongs toWordNet?s consumer class.
The cluster is rankedvery low because WordNet failed to considerwords like patient, tenant and renter asconsumers.Table 3 shows that even the lowest rankingCBC clusters are fairly coherent.
The featuresassociated with each cluster can be used toclassify previously unseen words into one ormore existing clusters.Table 4 shows the clusters containing the wordcell that are discovered by various clusteringalgorithms from S13403.
The underlined wordsrepresent the words that belong to the cell classin WordNet.
The CBC cluster correspondsalmost exactly to WordNet?s cell class.
K-meansand Buckshot produced fairly coherent clusters.The cluster constructed by Bisecting K-means isobviously of inferior quality.
This is consistentwith the fact that Bisecting K-means has a muchlower score on S13403 compared to CBC, K-means and Buckshot.Table 3.
Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutualinformation and the WordNet classes that have the largest intersection with each cluster.RANK MEMBERS TOP-15 FEATURES wn(c)1 handgun, revolver, shotgun, pistol, rifle,machine gun, sawed-off shotgun,submachine gun, gun, automatic pistol,automatic rifle, firearm, carbine,ammunition, magnum, cartridge,automatic, stopwatch__ blast, barrel of __ , brandish __, fire __, point __,pull out __, __ discharge, __ fire, __ go off, arm with__, fire with __, kill with __, open fire with __, shootwith __, threaten with __artifact / artifact236 whitefly, pest, aphid, fruit fly, termite,mosquito, cockroach, flea, beetle, killerbee, maggot, predator, mite, houseplant,cricket__ control, __ infestation, __ larvae, __ population,infestation of __, specie of __, swarm of __ , attract__, breed __, eat __, eradicate __, feed on __, get ridof __, repel __, ward off __animal / animate being /beast / brute / creature /fauna471 supervision, discipline, oversight,control, governance, decision making,jurisdictionbreakdown in __, lack of __ , loss of __, assume __,exercise __, exert __, maintain __, retain __, seize __,tighten __, bring under __, operate under __, placeunder __, put under __, remain under __act / human action /human activity706 blend, mix, mixture, combination,juxtaposition, combine, amalgam,sprinkle, synthesis, hybrid, melangedip in __, marinate in __, pour in __, stir in __, use in__, add to __, pour __, stir __, curious __, eclectic __,ethnic __, odd __, potent __, unique __, unusual __group / grouping941 employee, client, patient, applicant,tenant, individual, participant, renter,volunteer, recipient, caller, internee,enrollee, giverbenefit for __, care for __, housing for __, benefit to__, service to __, filed by __, paid by __, use by __,provide for __, require for --, give to __, offer to __,provide to __, disgruntled __, indigent __worker7 ConclusionWe presented a clustering algorithm, CBC, forautomatically discovering concepts from text.
Itcan handle a large number of elements, a largenumber of output clusters, and a large sparsefeature space.
It discovers clusters using well-scattered tight clusters called committees.
In ourexperiments, we showed that CBC outperformsseveral well known hierarchical, partitional, andhybrid clustering algorithms in cluster quality.For example, in one experiment, CBCoutperforms K-means by 4.25%.By comparing the CBC clusters with WordNetclasses, we not only find errors in CBC, but alsooversights in WordNet.Evaluating cluster quality has always been adifficult task.
We presented a new evaluationmethodology that is based on the editingdistance between output clusters and classesextracted from WordNet (the answer key).AcknowledgementsThe authors wish to thank the reviewers for theirhelpful comments.
This research was partlysupported by Natural Sciences and EngineeringResearch Council of Canada grant OGP121338and scholarship PGSB207797.ReferencesCutting, D. R.; Karger, D.; Pedersen, J.; and Tukey, J. W. 1992.Scatter/Gather: A cluster-based approach to browsing largedocument collections.
In Proceedings of SIGIR-92.
pp.
318?329.Copenhagen, Denmark.Guha, S.; Rastogi, R.; and Kyuseok, S. 1999.
ROCK: A robustclustering algorithm for categorical attributes.
In Proceedings ofICDE?99.
pp.
512?521.
Sydney, Australia.Jain, A. K.; Murty, M. N.; and Flynn, P. J.
1999.
Data Clustering: AReview.
ACM Computing Surveys 31(3):264?323.Kaufmann, L. and Rousseeuw, P. J.
1987.
Clustering by means ofmedoids.
In Dodge, Y.
(Ed.)
Statistical Data Analysis based on theL1 Norm.
pp.
405?416.
Elsevier/North Holland, Amsterdam.Karypis, G.; Han, E.-H.; and Kumar, V. 1999.
Chameleon: Ahierarchical  clustering algorithm using dynamic modeling.
IEEEComputer: Special Issue on Data Analysis and Mining 32(8):68?75.Landes, S.; Leacock, C.; and Tengi, R. I.
1998.
Building SemanticConcordances.
In WordNet: An Electronic Lexical Database, editedby C. Fellbaum.
pp.
199-216.
MIT Press.Leacock, C.; Chodorow, M.; and Miller; G. A.
1998.
Using corpusstatistics and WordNet relations for sense identification.Computational Linguistics, 24(1):147-165.Lee, L. and Pereira, F. 1999.
Distributional similarity models:Clustering vs. nearest neighbors.
In Proceedings of ACL-99.
pp.
33-40.
College Park, MD.Lin, D. 1994.
Principar - an Efficient, Broad-Coverage, Principle-BasedParser.
In Proceedings of COLING-94.
pp.
42-48.
Kyoto, Japan.Lin, D. 1998.
Automatic retrieval and  clustering of similar words.
InProceedings of COLING/ACL-98.
pp.
768-774.
Montreal, Canada.Lin, D. and Pantel, P. 2001.
Induction of semantic classes from naturallanguage text.
In Proceedings of SIGKDD-01.
pp.
317-322.
SanFrancisco, CA.Manning, C. D. and Sch?tze, H. 1999.
Foundations of StatisticalNatural Language Processing.
MIT Press.McQueen, J.
1967.
Some methods for classification and analysis ofmultivariate observations.
In Proceedings of 5th Berkeley Symposiumon Mathematics, Statistics and Probability, 1:281-298.Miller, G. 1990.
WordNet: An Online Lexical Database.
InternationalJournal of Lexicography, 1990.Pasca, M. and Harabagiu, S. 2001.
The informative role of WordNet inOpen-Domain Question Answering.
In Proceedings of NAACL-01Workshop on WordNet and Other Lexical Resources.
pp.
138-143.Pittsburgh, PA.Salton, G. and McGill, M. J.
1983.
Introduction to Modern InformationRetrieval.
McGraw Hill.Steinbach, M.; Karypis, G.; and Kumar, V. 2000.
A comparison ofdocument clustering techniques.
Technical Report #00-034.Department of Computer Science and Engineering, University ofMinnesota.sTable 4.
The clusters representing the cell concept for several clustering algorithms using S13403.ALGORITHMS CLUSTERS THAT HAVE THE LARGEST INTERSECTION WITH THE WORDNET CELL CLASS.CBC white blood cell, red blood cell, brain cell, cell, blood cell, cancer cell, nerve cell, embryo, neuronK-means cadaver, meteorite, secretion, receptor, serum, handwriting, cancer cell, thyroid, body part, hemoglobin, red bloodcell, nerve cell, urine, gene, chromosome, embryo, plasma, heart valve, saliva, ovary, white blood cell, intestine,lymph node, sperm, heart, colon, cell, blood, bowel, brain cell, central nervous system, spinal cord, blood cell,cornea, bladder, prostate, semen, brain, spleen, organ, nervous system, pancreas, tissue, marrow, liver, lung,marrow, kidneyBuckshot cadaver, vagina, meteorite, human body, secretion, lining, handwriting, cancer cell, womb, vein, bloodstream,body part, eyesight, polyp, coronary artery, thyroid, membrane, red blood cell, plasma, gene, gland, embryo,saliva, nerve cell, chromosome, skin, white blood cell, ovary, sperm, uterus, blood, intestine, heart, spinal cord,cell, bowel, colon, blood vessel, lymph node, brain cell, central nervous system, blood cell, semen, cornea,prostate, organ, brain, bladder, spleen, nervous system, tissue, pancreas, marrow, liver, lung, bone marrow, kidneyBisecting K-means picket line, police academy, sphere of influence, bloodstream, trance, sandbox, downtown, mountain, camera,boutique, kitchen sink, kiln, embassy, cellblock, voting booth, drawer, cell, skylight, bookcase, cupboard,ballpark, roof, stadium, clubhouse, tub, bathtub, classroom, toilet, kitchen, bathroom,WordNet Class blood cell, brain cell, cancer cell, cell, cone, egg, nerve cell, neuron, red blood cell, rod, sperm, white blood cell
