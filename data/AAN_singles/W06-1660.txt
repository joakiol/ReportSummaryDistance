Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 509?516,Sydney, July 2006. c?2006 Association for Computational LinguisticsEmpirical Study on the Performance Stability of Named EntityRecognition Model across DomainsHong Lei Guo Li Zhang and Zhong SuIBM China Research LaboratoryBuilding 19, Zhongguancun Software Park8 Dongbeiwang WestRoad, Haidian District, Beijing, 100094, P.R.C.
{guohl, lizhang , suzhong }@cn.ibm.comAbstractWhen a machine learning-based namedentity recognition system is employed ina new domain, its performance usually de-grades.
In this paper, we provide an em-pirical study on the impact of training datasize and domain information on the per-formance stability of named entity recog-nition models.
We present an informativesample selection method for building highquality and stable named entity recogni-tion models across domains.
Experimen-tal results show that the performance ofthe named entity recognition model is en-hanced significantly after being trainedwith these informative samples.1 IntroductionNamed entities (NE) are phrases that containnames of persons, organizations, locations, etc.Named entity recognition (NER) is an importanttask in many natural language processing appli-cations, such as information extraction and ma-chine translation.
There have been a number ofconferences aimed at evaluating NER systems,for example, MUC6, MUC7, CoNLL2002 andCoNLL2003, and ACE (automatic content extrac-tion) evaluations.Machine learning approaches are becomingmore attractive for NER in recent years since theyare trainable and adaptable.
Recent research onEnglish NER has focused on the machine learningapproach (Sang and Meulder, 2003).
The relevantalgorithms include Maximum Entropy (Borth-wick, 1999; Klein et al, 2003), Hidden MarkovModel (HMM) (Bikel et al, 1999; Klein et al,2003), AdaBoost (Carreras et al, 2003), Memory-based learning (Meulder and Daelemans, 2003),Support Vector Machine (Isozaki and Kazawa,2002), Robust Risk Minimization (RRM) Classi-fication method (Florian et al, 2003), etc.For Chinese NER, most of the existing ap-proaches use hand-crafted rules with word (orcharacter) frequency statistics.
Some machinelearning algorithms also have been investigated inChinese NER, including HMM (Yu et al, 1998;Jing et al, 2003), class-based language model(Gao et al, 2005; Wu et al, 2005), RRM (Guoet al, 2005; Jing et al, 2003), etc.However, when a machine learning-based NERsystem is directly employed in a new domain, itsperformance usually degrades.
In order to avoidthe performance degrading, the NER model is of-ten retrained with domain-specific annotated cor-pus.
This retraining process usually needs moreefforts and costs.
In order to enhance the perfor-mance stability of NER models with less efforts,some issues have to be considered in practice.
Forexample, how much training data is enough forbuilding a stable and applicable NER model?
Howdoes the domain information and training data sizeimpact the NER performance?This paper provides an empirical study on theimpact of training data size and domain informa-tion on NER performance.
Some useful observa-tions are obtained from the experimental resultson a large-scale annotated corpus.
Experimentalresults show that it is difficult to significantly en-hance the performance when the training data sizeis above a certain threshold.
The threshold of thetraining data size varies with domains.
The perfor-mance stability of each NE type recognition alsovaries with domains.
Corpus statistical data showthat NE types have different distribution across do-mains.
Based on the empirical investigations, wepresent an informative sample selection method509for building high quality and stable NER models.Experimental results show that the performance ofthe NER model is enhanced significantly acrossdomains after being trained with these informativesamples.
In spite of our focus on Chinese, we be-lieve that some of our observations can be poten-tially useful to other languages including English.This paper is organized as follows.
Section 2describes a Chinese NER system using multi-levellinguistic features.
Section 3 discusses the impactof domain information and training data size onthe NER performance.
Section 4 presents an in-formative sample selection method to enhance theperformance of the NER model across domains.Finally the conclusion is given in Section 5.2 Chinese NER Based on MultilevelLinguistic FeaturesIn this paper, we focus on recognizing four typesof NEs: Persons (PER), Locations (LOC), Orga-nizations (ORG) and miscellaneous named enti-ties (MISC) which do not belong to the previousthree groups (e.g.
products, conferences, events,brands, etc.).
All the NER models in the follow-ing experiments are trained with a Chinese NERsystem.
In this section, we simply describe thisChinese NER system.
The Robust Risk Minimiza-tion (RRM) Classification method and multi-levellinguistic features are used in this system (Guo etal., 2005).2.1 Robust Risk Minimization ClassifierWe can view the NER task as a sequential classi-fication problem.
If toki (i = 0, 1, ..., n) denotesthe sequence of tokenized text which is the inputto the system, then every token toki should be as-signed a class-label ti.The class label value ti associated with each to-ken toki is predicted by estimating the conditionalprobability P (ti = c|xi) for every possible class-label value c, where xi is a feature vector associ-ated with token toki.We assume that P (ti = c|xi) = P (ti =c|toki, {tj}j?i).
The feature vector xi can dependon previously predicted class labels {tj}j?i, butthe dependency is typically assumed to be local.In the RRM method, the above conditional proba-bility model has the following parametric form:P (ti = c|xi, ti?l, ..., ti?1) = T (wTc xi + bc),where T (y) = min(1,max(0, y)) is the truncationof y into the interval [0, 1].
wc is a linear weightvector and bc is a constant.
Parameters wc and bccan be estimated from the training data.
Giventraining data (xi, ti) for i = 1, ..., n, the modelis estimated by solving the following optimizationproblem for each c (Zhang et al, 2002):infw,b1nn?i=1f(wTc xi + bc, yic),where yic = 1 when ti = c, and yic = ?1 other-wise.
The function f is defined as:f(p, y) =?????
?2py py < 112(py ?
1)2 py ?
[?1, 1]0 py > 1Given the above conditional probability model,the best possible sequence of ti?s can be estimatedby dynamic programming in the decoding stage(Zhang et al, 2002).2.2 Multilevel Linguistic FeaturesThis Chinese NER system uses Chinese charac-ters (not Chinese words) as the basic token units,and then maps word-based features that are as-sociated with each word into corresponding fea-tures of those characters that are contained in theword.
This approach can effectively incorporateboth character-based features and word-based fea-tures.
In general, we may regard this approachas information integration from linguistic views atdifferent abstraction levels.We integrate a diverse set of local linguistic fea-tures, including word segmentation information,Chinese word patterns, complex lexical linguis-tic features (e.g.
part of speech and semantic fea-tures), aligned at the character level.
In additional,we also use external NE hints and gazetteers, in-cluding surnames, location suffixes, organizationsuffixes, titles, high-frequency Chinese charactersin Chinese names and translation names, and listsof locations and organizations.
In this system, lo-cal linguistic features of a token unit are derivedfrom the sentence containing this token unit.
Allspecial linguistic patterns (i.e.
date, time, numeralexpression) are encoded into pattern-specific classlabels aligned with the tokens.3 Impact of Training Data Size AndDomain Information on the NERPerformanceIt is very important to keep the performance sta-bility of NER models across domains in practice.510However, the performance usually becomes unsta-ble when NER models are applied in different do-mains.
We focus on the impact of the training datasize and domain information on the NER perfor-mance in this section.3.1 DataWe built a large-scale high-quality Chinese NE an-notated corpus.
The corpus size is 114.25M Chi-nese characters.
All the data are news articles se-lected from several Chinese newspapers in 2001and 2002.
All the NEs in the corpus are manuallytagged.
Documents in the corpus are also man-ually classified into eight domain categories, in-cluding politics, sports, science, economics, enter-tainment, life, society and others.
Cross-validationis employed to ensure the tagging quality.All the training data and test data in the exper-iments are selected from this Chinese annotatedcorpus.
The general training data are randomly se-lected from the corpus without distinguishing theirdomain categories.
All the domain-specific train-ing data are selected from the corpus according totheir domain categories.
One general test data setand seven domain-specific test data sets are usedin our experiments (see Table 1).
The size of thegeneral test data set is 1.34M Chinese characters.Seven domain-specific test sets are extracted fromthe general test data set according to the documentdomain categories.Domain NE distribution in the domain-oriented test data set Test setPER ORG LOC MISC Total SizeGeneral 11,991 9,820 12,353 1,820 35,984 1.34MPolitics 2,470 1,528 2,540 480 7,018 0.2MEconomics 1,098 2,971 2,362 493 6,924 0.26MSports 1,802 1,323 1,246 478 4,849 0.10MEntertainment 2,458 526 738 542 4,264 0.10MSociety 916 418 823 349 2,506 0.08MLife 2,331 1,690 3,634 763 8,418 0.39MScience 1,802 1,323 1,246 478 4,849 0.10MTable 1: NE distribution in the general anddomain-specific test data setsIn our evaluation, only NEs with correct bound-aries and correct class labels are considered as thecorrect recognition.
We use the standard P (i.e.Precision), R (i.e.
Recall), and F-measure (de-fined as 2PR/(P+R)) to measure the performanceof NER models.3.2 Impact of Training Data Size on the NERPerformance across DomainsThe amount of annotated data is always a bottle-neck for supervised learning methods in practice.Figure 1: Performance curves of the general andspecific domain NER modelsThus, we evaluate the impact of training data sizeon the NER performance across domains.In this baseline experiment, an initial generalNER model is trained with 0.1M general data atfirst.
Then the NER model is incrementally re-trained by adding 0.1M new general training dataeach time till the performance isn?t enhanced sig-nificantly.
The NER performance curve (labelledwith the tag ?General? )
in the whole retrainingprocess is shown in Figure 1.
Experimental resultsshow that the performance of the general NERmodel is significantly enhanced in the first severalretraining cycles since more training data are used.However, when the general training data set size ismore than 2.4M, the performance enhancement isvery slight.In order to analyze how the training data sizeimpacting the performance of NER models in spe-cific domains, seven domain-specific NER mod-els are built using the similar retraining process.Each domain-specific NER model is also trainedwith 0.1M domain-specific data at first.
Then,each initial domain-specific NER model is incre-mentally retrained by adding 0.1M new domain-specific data each time.NER F(%) Size NE distribution in the training setModel thre-shold(M) PER ORG LOC MISC TotalGeneral 80.38 2.4 24,960 27,231 21,098 7,439 80,728Politics 83.09 0.9 11,388 6,618 14,350 1,974 34,330Econ-omics 85.46 1.7 7,197 21,113 15,582 3,466 47,358Sports 90.78 0.6 11,647 8,105 7,468 3,070 30,290Entert-ainment 83.31 0.6 12,954 2,823 4,665 3,518 32,860Society 76.55 0.6 7,099 3,279 6,946 1,909 19,233Life 81.06 1.7 10,502 5,675 18,980 2,420 37,577Science 70.02 0.4 1,625 3,010 2,083 902 7,620Table 2: Performance of NER models, size thresh-old and NE distribution in the corresponding train-ing data sets511The performance curves of these domain-specific NER models are also shown in Figure 1(see the curves labelled with the domain tags).
Al-though the initial performance of each domain-specific NER model varies with domains, the per-formance is also significantly enhanced in the firstseveral retraining cycles.
When the size of thedomain-specific training data set is above a certainthreshold, the performance enhancement is veryslight as well.The final performance of the trained NER mod-els, and the corresponding training data sets areshown in Table 2.From these NER performance curves, we obtainthe following observations.1.
More training data are used, higher NER per-formance can be achieved.
However, it isdifficult to significantly enhance the perfor-mance when the training data size is above acertain threshold.2.
The threshold of the training data size andthe final achieved performance vary with do-mains (see Table 2).
For example, in enter-tainment domain, the threshold is 0.6M andthe final F-measure achieves 83.31%.
In eco-nomic domain, the threshold is 1.7M, and thecorresponding F-measure is 85.46%.3.3 The Performance Stability of Each NEType Recognition across DomainsStatistic data on our large-scale annotated corpus(shown in Table 3) show that the distribution of NEtypes varies with domains.
We define ?
NE density?
to quantitatively measure the NE distribution inan annotated data set.
NE density is defined as ?thecount of NE instances in one thousand Chinesecharacters?.
Higher NE density usually indicatesthat more NEs are contained in the data set.
Wemay easily measure the distribution of each NEtype across domains using NE density.
In this an-notated corpus, PER, LOC, and ORG have similarNE density while MISC has the smallest NE den-sity.
All the NE types also have different NE den-sity in each domain.
For example, the NE densityof ORG and LOC is much higher than that of PERin economic domain.
PER and LOC have higherNE density than ORG in politics domain.
PERhas the highest NE density among these NE typesin both sports and entertainment domains.
Theunbalanced NE distribution across domains showsthat news articles on different domains usually fo-cus on different specific NE types.
These NE dis-tribution features imply that each NE type has dif-ferent domain dependency feature.
The perfor-mance stability of domain-focused NE type recog-nition becomes more important in domain-specificapplications.
For example, since economic newsarticles usually focus on ORG and LOC NEs, thehigh-quality LOC and ORG recognition modelswill be more valuable in economic domain.
In ad-dition, these distribution features also can be usedto guide training and test data selection.Domain NE distribution in the specific domainPER LOC ORG MISC ALL Ratio(%)Politics 167,989 180,193 105,936 30,830 484,948 16.43Econ-omics 117,459 200,261 352,323 76,320 746,363 25.29Sports 129,137 73,435 98,618 33,304 334,494 11.33Entert- 154,193 50,408 40,444 52,460 297,505 10.08ainmentLife 200,222 234,150 145,138 65,733 645,243 21.86Society 63,793 53,724 43,657 21,162 182,336 6.18Science 27,878 30,737 72,413 16,824 147,852 5.00Others 31,723 40,730 26,666 13,926 113,045 3.83All 892,394 863,638 885,195 310,559 2,951,786 ?Domain NE density in the Chinese annotated corpus SizePER LOC ORG MISC ALL (M)Politics 10.70 11.48 6.75 1.96 31.21 15.70Econ-omics 4.18 7.13 12.55 2.72 26.58 28.08Sports 16.43 9.34 12.55 4.24 42.57 7.86Entert-ainment 16.81 5.05 4.14 5.72 32.44 9.17Life 5.64 6.59 4.09 1.85 18.17 35.52Society 8.57 7.22 5.87 2.84 24.51 7.44Science 4.30 4.74 11.17 2.60 22.82 6.48Others 7.9 10.18 6.67 3.48 28.26 4.00All 7.81 7.56 7.75 2.72 25.89 114.25Table 3: NE distribution in the Chinese annotatedcorpusIn this experiment, the performance stabilityof NER models across domains is evaluated, es-pecially the performance stability of each NEtype recognition.
The general NER model istrained with 2.4M general data.
Seven domain-specific models are trained with the correspondingdomain-specific training sets (see Table 2 in Sec-tion 3.2).The performance stability of the general NERmodel is firstly evaluated on the general anddomain-specific test data sets (see Table 1 in Sec-tion 3.1 ).
The experimental results are shown inTable 4.
The performance curves of the generalmodel are shown in Figure 2, including the totalF-measure curve of the NER model (labelled withthe tag ?All?)
and F-measure curves of each NEtype recognition in the specific domains (labelledwith the NE tags respectively).The performance stability of the seven domain-specific NER models are also evaluated.
Eachdomain-specific NER model is tested on the gen-512Domain F(%) of general NER modelPER LOC ORG MISC ALLGeneral 86.69 85.55 73.59 56.00 80.38Economic 85.11 88.22 75.91 49.53 80.50Politics 86.26 87.00 71.31 61.50 81.90Sports 91.87 89.03 81.67 67.41 86.10Entertainment 84.24 85.85 68.65 60.96 79.31Life 86.62 83.54 70.30 58.49 79.73Society 84.53 76.16 68.89 41.14 74.50Science 87.74 86.42 65.85 24.10 69.55Table 4: Performance of the general NER modelin specific domainsFigure 2: Performance curves of the general NERmodel in specific domainseral test data and the other six different domain-specific test data sets.
The experimental results areshown in Table 5.
The performance curves of threedomain-specific NER models are shown in Figure3, Figure 4 and Figure 5 respectively.From these experimental results, we have thefollowing conclusions.1.
The performance stability of all the NERmodels is limited across domains.
When aNER model is employed in a new domain, itsperformance usually decreases.
Moreover, itsperformance is usually much lower than theperformance of the corresponding domain-specific model.2.
The general NER model has better per-Figure 3: Performance curves of economic do-main NER model in the other specific domainsNER F(%) in specific domainModel Gen- Eco- Poli- Spo- Enter- Life Soc- Sci-eral nomic tics rts tainment iety enceGeneral 80.38 80.50 81.90 86.10 79.31 79.73 74.50 69.55Econ-omic 75.30 85.46 74.32 72.89 68.46 76.23 65.75 68.97Politics 73.37 66.39 83.09 76.37 71.51 74.83 67.31 53.76Sports 71.23 62.56 68.99 90.78 73.48 71.18 64.82 53.85Entert-ainment 70.82 61.52 72.04 75.34 83.31 71.80 69.10 52.50Life 73.53 66.92 75.07 73.86 72.68 81.06 69.61 57.36Society 70.29 62.55 72.70 70.69 72.24 74.10 76.55 53.42Science 67.26 67.57 69.00 64.32 63.84 69.05 64.85 70.02Table 5: Performance of NER models in specificdomainsFigure 4: Performance curves of sports domainNER model in the other specific domainsformance stability than the domain-specificNER model when they are applied in new do-mains (see Table 5).
Domain-specific mod-els usually could achieve a higher perfor-mance in its corresponding domain after be-ing trained with a smaller amount of domain-specific annotated data (see Table 2 in Sec-tion 3.2).
However, the performance stabilityof domain-specific NER model is poor acrossdifferent domains.
Thus, it is very popular tobuild a general NER model for the generalapplications in practice.3.
The performance of PER, LOC and ORGrecognition is better than that of MISC recog-Figure 5: Performance curves of politics domainNER model in the other specific domains513nition in NER (see Figure 2 ?
Figure 5).The main reason for the poor performance ofMISC recognition is that there are less com-mon indicative features among various MISCNEs which we do not distinguish.
In addi-tion, NE density of MISC is much less thanthat of PER, LOC, and ORG.
There are arelatively small number of positive trainingsamples for MISC recognition.4.
NE types have different domain dependencyattribute.
The performance stability of eachNE type recognition varies with domains (seeFigure 2 ?
Figure 5).
The performance ofPER and LOC recognition are more stableacross domains.
Thus, few efforts are neededto adapt the existing high-quality generalPER and LOC recognition models in domain-specific applications.
Since ORG and MISCNEs usually contain more domain-specificsemantic information, ORG and MISC aremore domain-dependent than PER and LOC.Thus, more domain-specific features shouldbe mined for ORG and MISC recognition.4 Use Informative Training Samples toEnhance the Performance of NERModels across DomainsA higher performance system usually requiresmore features and a larger number of training data.This requires larger system memory and more effi-cient training method, which may not be available.Within the limitation of available training data andcomputational resources, it is necessary for us toeither limit the number of features or select moreinformative data which can be efficiently handledby the training algorithm.
Active learning methodis usually employed in text classification (McCal-lum and Nigam et al, 1998).
It is only recentlyemployed in NER (Shen et al, 2004).In order to enhance the performance and over-come the limitation of available training data andcomputational resources, we present an informa-tive sample selection method using a variant ofuncertainty-sampling (Lewis and Catlett, 1994).The main steps are described as follows.1.
Build an initial NER model (F-measure=76.24%) using an initial dataset.
The initial data set (about 1M Chinesecharacters) is randomly selected from thelarge-scale candidate data set (about 9M ).Figure 6: Performance curves of general NERmodels after being trained with informative sam-ples and random samples respectively2.
Refine the training set by adding more infor-mative samples and removing those redun-dant samples.
In this refinement phase, all ofthe data are annotated by the current recogni-tion model (e.g.
the initial model built in Step1).
Each annotation has a confidence scoreassociated with the prediction.
In general, anannotation with lower confidence score usu-ally indicates a wrong prediction.
The con-fidence score of the whole sample sentenceis defined as the average of the confidencescores of all the annotations contained in thesentence.
Thus, we add those sample sen-tences with lower confidence scores into thetraining set.
Meanwhile, in order to keep areasonable size of the training set, those oldtraining sample sentences with higher confi-dence scores are removed from the currenttraining set.
In each retraining phase, all ofthe sample sentences are sorted by the con-fidence score.
The top 1000 new samplesentences with lowest confidence scores areadded into the current training set.
The top500 old training sample sentences with high-est confidence scores are removed from thecurrent training set.3.
Retrain a new Chinese NER model with thenewly refined training set4.
Repeat Step 2 and Step 3, until the perfor-mance doesn?t improve any more.We apply this informative sample selectionmethod to incrementally build the general domainNER model.
The size of the final informativetraining sample set is 1.05M Chinese characters.This informative training sample set has higherNE density than the random training data set (seeTable 6).514We denote this general NER model trained withthe informative sample set as ?general informa-tive model?, and denote the general-domain modelwhich is trained with 2.4M random general train-ing data as ?general random model?.
The perfor-mance curves of the general NER models after be-ing trained with informative samples and randomdata respectively are shown in Figure 6.
Experi-ment results (see Table 6) show that there is a sig-nificant enhancement in F-measure if using infor-mative training samples.
Compared with the ran-dom model, the informative model can increase F-measure by 4.21 percent points.Type Using informative sample set Using random training set(1.05M) (2.4M)F(%) NEs NE density F(%) NEs NE densityPER 89.87 18,898 18.00 86.69 24,960 10.38LOC 89.68 24,862 23.68 85.55 21,089 11.33ORG 79.22 22,173 21.12 73.59 27,231 8.78MISC 64.27 8,067 7.68 56.00 7,439 3.10Total 84.59 74,000 70.48 80.38 80,728 33.58Table 6: Performance of informative model andrandom model in the general domainDomain F(%) of general informative modelPER LOC ORG MISC ALLEconomic 89.26 90.66 81.24 61.14 84.63Politics 89.36 89.37 74.76 65.95 84.70Sports 93.65 90.66 86.00 72.05 88.71Entertainment 88.38 87.54 73.88 58.32 82.74Life 89.15 88.35 75.68 72.01 84.66Society 86.61 82.15 72.99 58.55 79.49Science 90.91 88.35 71.69 25.16 72.71Table 7: Performance of the general informativemodel in specific domainsThis informative model is also evaluated on thedomain-specific test sets.
Experimental results areshown in Table 7.
We view the performance of thedomain-specific NER model as the baseline per-formance in its corresponding domain (see Table8), denoted as Fbaseline.
The performance of in-formative model in specific domains is very closeto the corresponding Fbaseline (see Figure 7).
Wedefine the domain-specific average F-measure asthe average of all the F-measure of the NER modelin seven specific domains, denote as F .
The av-erage of all the Fbaseline in specific domains isdenoted as F baseline.
The average F-measure ofthe informative model and the random model inspecific domains is denoted as F informative andF random respectively.
Compared with F baseline(F =81.47%), the informative model increases Fby 1.05 percent points.
However, F decreases by2.67 percent points if using the random model.
Es-pecially, the performance of the informative modelis better than the corresponding baseline perfor-Figure 7: Performance comparison of informa-tive model, random model, and the correspondingdomain-specific modelsmance in politics, life, society and science do-mains.
Moreover, the size of the informative sam-ple set is much less than the life domain trainingset (1.7M).NER F(%) in specific domainsmodel Eco- Poli- Spo- Entert- Life So- Sci- Fnomic tics rts ainment ciety encedomain-specific 85.46 83.09 90.78 83.31 81.06 76.55 70.02 81.47(baseline)Infor-mative 84.63 84.70 88.71 82.74 84.66 79.49 72.71 82.52Random 80.50 81.90 86.10 79.31 79.73 74.50 69.55 78.80NER ?
(F ) in specific domainmodel ?
(F ) = (F ?
F ) ?Eco- Poli- Spo- Entert- Life So- Sci-nomic tics rts ainment ciety enceInfor-mative 2.11 2.18 6.19 0.22 2.14 -3.03 -9.81 4.74Random 1.7 3.1 7.3 0.51 0.93 -4.3 -9.25 4.94Table 8: Performance comparison of informa-tive model, random model and the correspondingdomain-specific model in each specific domainThe informative model has much better perfor-mance than the random model in specific domains(see Table 8 and Figure 7).
F informative is 82.52%while F random is 78.80%.
The informative modelcan increase F by 3.72 percent points.
The infor-mative model is also more stable than the randommodel in specific domains (see Table 8).
Standarddeviation of F-measure for the informative modelis 4.74 while that for the random model is 4.94.Our experience with the incremental sample se-lection provides the following hints.1.
The performance of the NER model acrossdomains can be significantly enhanced afterbeing trained with informative samples.
In515order to obtain a high-quality and stable NERmodel, it is only necessary to keep the infor-mative samples.
Informative sample selec-tion can alleviate the problem of obtaining alarge amount of annotated data.
It is also aneffective method for overcoming the poten-tial limitation of computational resources.2.
In learning NER models, annotated resultswith lower confidence scores are more use-ful than those samples with higher confidencescores.
This is consistent with other studieson active learning.5 ConclusionEfficient and robust NER model is very impor-tant in practice.
This paper provides an empiricalstudy on the impact of training data size and do-main information on the performance stability ofNER.
Experimental results show that it is difficultto significantly enhance the performance when thetraining data size is above a certain threshold.
Thethreshold of the training data size varies with do-mains.
The performance stability of each NE typerecognition also varies with domains.
The large-scale corpus statistic data also show that NE typeshave different distribution across domains.
Theseempirical investigations provide useful hints forenhancing the performance stability of NER mod-els across domains with less efforts.
In order to en-hance the NER performance across domains, wepresent an informative training sample selectionmethod.
Experimental results show that the per-formance is significantly enhanced by using infor-mative training samples.In the future, we?d like to focus on furtherexploring more effective methods to adapt NERmodel to a new domain with much less efforts,time and performance degrading.ReferencesDaniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Machine Learning, 34(1-3):211?231.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. thesis,New York University.Xavier Carreras, Llu?
?s Ma`rquez, and Llu?
?s Padro?.2003.
A simple named entity extractor using ad-aboost.
In Proceedings of CoNLL-2003, pages 152?155.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named entity recogintionthrough classifier combination.
In ProceedingsCoNLL-2003, pages 168?171.Jian F. Gao, Mu Li, Anndy Wu, and Chang N., Huang.2005.
Chinese Word Segmentation and Named En-tity Recognition: A Pragmatic Approach.
Computa-tional Linguisitc,31(4):531-574.Hong L. Guo, Jian M. Jiang, Gang Hu, and TongZhang.
2005.
Chinese Named Entity RecognitionBased on Multilevel Linguistic Features.
LectureNotes in Artificial Intelligence,3248:90-99,Springer.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recogni-tion.
In Proceedings of Coling-2002, pages 1-7.Hongyan Jing, Radu Florian, Xiaoqiang Luo, TongZhang, and Abraham Ittycheriah.
2003.
Howtoge-tachinesename (entity) : Segmentation and combi-nation issues.
In EMNLP 2003, pages 200-207.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recogni-tion with character-level models.
In Proceedings ofCoNLL-2003, pages 180?183.David D. Lewis and Jason Catlett.
1994.
Heteroge-neous uncertainty sampling for supervised learning.In Proceedings of the Eleventh International Con-ference on Machine Learning, pages 148?156.Andrew Kamal McCallum and K. Nigam.
1998.
Em-ploying EM in pool-based active learning for textclassification.
Proceedings of 15th InternationalConference on Machine Learning, pages 350-358.Fien De Meulder and Walter Daelemans.
2003.Memory-based named entity recognition usingunannotated data.
In Proceedings of CoNLL-2003,pages 208?211.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Lan-guage independent named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 142?147.Dan Shen, Jie Zhang, Jian Su, Gou D. Zhou, and ChewL.Tan, 2004.
Multi-Criteria-based Active Learn-ing for Named Entity Recognition.
Proceedings ofACL04, pages 589-596.Yu Z. Wu, Jun Zhao, Bo Xu, and Hao Yu.
2005.
Chi-nese Named Entity Recognition Based on MultipleFeatures.
Proceedings of EMNLP05, pages 427-434Shi H. Yu, Shuan H. Bai, and Paul Wu.
1998.
De-scription of the kent ridge digital labs system usedfor muc-7.
In Proceedings of the Seventh MessageUnderstanding Conference (MUC-7).Tong Zhang, Fred Damerau, and David E. Johnson.2002.
Text chunking based on a generalization ofWinnow.
Journal of Machine Learning Research,2:615?637.516
