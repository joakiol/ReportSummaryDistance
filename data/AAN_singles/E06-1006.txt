Phrase-Based Backoff Models for Machine Translation of Highly InflectedLanguagesMei YangDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAyangmei@ee.washington.eduKatrin KirchhoffDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAkatrin@ee.washington.eduAbstractWe propose a backoff model for phrase-based machine translation that translatesunseen word forms in foreign-languagetext by hierarchical morphological ab-stractions at the word and the phrase level.The model is evaluated on the Europarlcorpus for German-English and Finnish-English translation and shows improve-ments over state-of-the-art phrase-basedmodels.1 IntroductionCurrent statistical machine translation (SMT) usu-ally works well in cases where the domain isfixed, the training and test data match, and a largeamount of training data is available.
Nevertheless,standard SMT models tend to perform much bet-ter on languages that are morphologically simple,whereas highly inflected languages with a largenumber of potential word forms are more prob-lematic, particularly when training data is sparse.SMT attempts to find a sentence e?
in the desiredoutput language given the corresponding sentencef in the source language, according toe?
= argmaxeP (f |e)P (e) (1)Most state-of-the-art SMT adopt a phrase-basedapproach such that e is chunked into I phrasese?1, ..., e?I and the translation model is definedover mappings between phrases in e and in f .i.e.
P (f?
|e?).
Typically, phrases are extracted froma word-aligned training corpus.
Different inflectedforms of the same lemma are treated as differentwords, and there is no provision for unseen forms,i.e.
unknown words encountered in the test dataare not translated at all but appear verbatim in theoutput.
Although the percentage of such unseenword forms may be negligible when the trainingset is large and matches the test set well, it may risedrastically when training data is limited or froma different domain.
Many current and future ap-plications of machine translation require the rapidporting of existing systems to new languages anddomains without being able to collect appropri-ate training data; this problem can therefore beexpected to become increasingly more important.Furthermore, untranslated words can be one of themain factors contributing to low user satisfactionin practical applications.Several previous studies (see Section 2 below)have addressed issues of morphology in SMT, butmost of these have focused on the problem of wordalignment and vocabulary size reduction.
Princi-pled ways of incorporating different levels of mor-phological abstraction into phrase-based modelshave mostly been ignored so far.
In this paper wepropose a hierarchical backoff model for phrase-based translation that integrates several layers ofmorphological operations, such that more specificmodels are preferred over more general models.We experimentally evaluate the model on transla-tion from two highly-inflected languages, Germanand Finnish, into English and present improve-ments over a state-of-the-art system.
The rest ofthe paper is structured as follows: The followingsection discusses related background work.
Sec-tion 4 describes the proposed model; Sections 5and 6 provide details about the data and baselinesystem used in this study.
Section 7 provides ex-perimental results and discussion.
Section 8 con-cludes.412 Morphology in SMT SystemsPrevious approaches have used morpho-syntacticknowledge mainly at the low-level stages of a ma-chine translation system, i.e.
for preprocessing.
(Niessen and Ney, 2001a) use morpho-syntacticknowledge for reordering certain syntactic con-structions that differ in word order in the sourcevs.
target language (German and English).
Re-ordering is applied before training and after gener-ating the output in the target language.
Normaliza-tion of English/German inflectional morphologyto base forms for the purpose of word alignment isperformed in (Corston-Oliver and Gamon, 2004)and (Koehn, 2005), demonstrating that the vocab-ulary size can be reduced significantly without af-fecting performance.Similar morphological simplifications havebeen applied to other languages such as Roma-nian (Fraser and Marcu, 2005) in order to de-crease word alignment error rate.
In (Niessenand Ney, 2001b), a hierarchical lexicon model isused that represents words as combinations of fullforms, base forms, and part-of-speech tags, andthat allows the word alignment training procedureto interpolate counts based on the different lev-els of representation.
(Goldwater and McCloskey,2005) investigate various morphological modifi-cations for Czech-English translations: a subsetof the vocabulary was converted to stems, pseu-dowords consisting of morphological tags were in-troduced, and combinations of stems and morpho-logical tags were used as new word forms.
Smallimprovements were found in combination with aword-to-word translation model.
Most of thesetechniques have focused on improving word align-ment or reducing vocabulary size; however, it isoften the case that better word alignment does notimprove the overall translation performance of astandard phrase-based SMT system.Phrase-based models themselves have not ben-efited much from additional morpho-syntacticknowledge; e.g.
(Lioma and Ounis, 2005) do notreport any improvement from integrating part-of-speech information at the phrase level.
One suc-cessful application of morphological knowledge is(de Gispert et al, 2005), where knowledge-basedmorphological techniques are used to identify un-seen verb forms in the test text and to generateinflected forms in the target language based onannotated POS tags and lemmas.
Phrase predic-tion in the target language is conditioned on thephrase in the source language as well the corre-sponding tuple of lemmatized phrases.
This tech-nique worked well for translating from a morpho-logically poor language (English) to a more highlyinflected language (Spanish) when applied to un-seen verb forms.
Treating both known and un-known verbs in this way, however, did not resultin additional improvements.
Here we extend thenotion of treating known and unknown words dif-ferently and propose a backoff model for phrase-based translation.3 Backoff ModelsGenerally speaking, backoff models exploit rela-tionships between more general and more spe-cific probability distributions.
They specify underwhich conditions the more specific model is usedand when the model ?backs off?
to the more gen-eral distribution.
Backoff models have been usedin a variety of ways in natural language process-ing, most notably in statistical language modeling.In language modeling, a higher-order n-gram dis-tribution is used when it is deemed reliable (deter-mined by the number of occurrences in the train-ing data); otherwise, the model backs off to thenext lower-order n-gram distribution.
For the caseof trigrams, this can be expressed as:pBO(wt|wt?1, wt?2) (2)={dcpML(wt|wt?1, wt?2) if c > ??
(wt?1, wt?2)pBO(wt|wt?1) otherwisewhere pML denotes the maximum-likelihoodestimate, c denotes the count of the triple(wi, wi?1, wi?2) in the training data, ?
is the countthreshold above which the maximum-likelihoodestimate is retained, and dN(wi,wi?1,wi?2) is a dis-counting factor (generally between 0 and 1) that isapplied to the higher-order distribution.
The nor-malization factor ?
(wi?1, wi?2) ensures that thedistribution sums to one.
In (Bilmes and Kirch-hoff, 2003) this method was generalized to a back-off model with multiple paths, allowing the com-bination of different backed-off probability esti-mates.
Hierarchical backoff schemes have alsobeen used by (Zitouni et al, 2003) for languagemodeling and by (Gildea, 2001) for semantic rolelabeling.
(Resnik et al, 2001) used backoff trans-lation lexicons for cross-language information re-trieval.
More recently, (Xi and Hwa, 2005) haveused backoff models for combining in-domain and42out-of-domain data for the purpose of bootstrap-ping a part-of-speech tagger for Chinese, outper-forming standard methods such as EM.4 Backoff Models in MTIn order to handle unseen words in the test datawe propose a hierarchical backoff model that usesmorphological information.
Several morphologi-cal operations, in particular stemming and com-pound splitting, are interleaved such that a morespecific form (i.e.
a form closer to the full wordform) is chosen before a more general form (i.e.
aform that has undergone morphological process-ing).
The procedure is shown in Figure 1 and canbe described as follows: First, a standard phrasetable based on full word forms is trained.
If anunknown word fi is encountered in the test datawith context cfi = fi?n, ..., fi?1, fi+1, ..., fi+m,the word is first stemmed, i.e.
f ?i = stem(fi).The phrase table entries for words sharing thesame stem are then modified by replacing therespective words with their stems.
If an en-try can be found among these such that thesource language side of the phrase pair consists offi?n, ..., fi?1, stem(fi), fi+1, ..., fi+m, the corre-sponding translation is used (or, if several pos-sible translations occur, the one with the high-est probability is chosen).
Note that the con-text may be empty, in which case a single-wordphrase is used.
If this step fails, the model backsoff to the next level and applies compound split-ting to the unknown word (further described be-low), i.e.
(f ?
?i1, f ?
?i2) = split(fi).
The match withthe original word-based phrase table is then per-formed again.
If this step fails for either of thetwo parts of f ?
?, stemming is applied again: f ??
?i1 =stem(f ?
?i1) and f ??
?i2 = stem(f ?
?i2), and a match withthe stemmed phrase table entries is carried out.Only if the attempted match fails at this level is theinput passed on verbatim in the translation output.The backoff procedure could in principle beperformed on demand by a specialized decoder;however, since we use an off-the-shelf decoder(Pharaoh (Koehn, 2004)), backoff is implicitly en-forced by providing a phrase-table that includesall required backoff levels and by preprocessingthe test data accordingly.
The phrase table willthus include entries for phrases based on full wordforms as well as for their stemmed and/or splitcounterparts.For each entry with decomposed morphologicalii ii1 i2 ii1 i1i2 i2i1 i2Figure 1: Backoff procedure.forms, four probabilities need to be provided: twophrasal translation scores for both translation di-rections, p(e?|f?)
and p(f?
|e?
), and two correspond-ing lexical scores, which are computed as a prod-uct of the word-by-word translation probabilitiesunder the given alignment a:plex(e?|f?)
=J?j=11|j|a(i) = j|I?a(i)=jp(fj |ei) (3)where j ranges of words in phrase f?
and i rangesof words in phrase e?.
In the case of unknownwords in the foreign language, we need the prob-abilities p(e?|stem(f?
)), p(stem(f?)|e?)
(where thestemming operation stem(f?)
applies to the un-known words in the phrase), and their lexicalequivalents.
These are computed by relative fre-quency estimation, e.g.p(e?|stem(f?))
= count(e?, stem(f?))count(stem(f?))
(4)The other translation probabilities are computedanalogously.
Since normalization is performedover the entire phrase table, this procedure hasthe effect of discounting the original probabilityporig(e?|f?)
since e?
may now have been generatedby either f?
or by stem(f?).
In the standard formu-lation of backoff models shown in Equation 3, thisamounts to:pBO(e?|f?)
(5)={de?,f?porig(e?|f?)
if c(e?, f?)
> 0p(e?|stem(f?))
otherwise43wherede?,f?
=1 ?
p(e?, stem(f?
))p(e?, f?)
(6)is the amount by which the word-based phrasetranslation probability is discounted.
Equiva-lent probability computations are carried out forthe lexical translation probabilities.
Similar tothe backoff level that uses stemming, the trans-lation probabilities need to be recomputed forthe levels that use splitting and combined split-ting/stemming.In order to derive the morphological decompo-sition we use existing tools.
For stemming weuse the TreeTagger (Schmid, 1994) for Germanand the Snowball stemmer1 for Finnish.
A vari-ety of ways for compound splitting have been in-vestigated in machine translation (Koehn, 2003).Here we use a simple technique that considers allpossible ways of segmenting a word into two sub-parts (with a minimum-length constraint of threecharacters on each subpart).
A segmentation is ac-cepted if the subparts appear as individual itemsin the training data vocabulary.
The only linguis-tic knowledge used in the segmentation process isthe removal of final <s> from the first part of thecompound before trying to match it to an existingword.
This character (Fugen-s) is often inserted as?glue?
when forming German compounds.
Otherglue characters were not considered for simplic-ity (but could be added in the future).
The seg-mentation method is clearly not linguistically ad-equate: first, words may be split into more thantwo parts.
Second, the method may generate mul-tiple possible segmentations without a principledway of choosing among them; third, it may gener-ate invalid splits.
However, a manual analysis of300 unknown compounds in the German develop-ment set (see next section) showed that 95.3% ofthem were decomposed correctly: for the domainat hand, most compounds need not be split intomore than two parts; if one part is itself a com-pound it is usually frequent enough in the train-ing data to have a translation.
Furthermore, lexi-calized compounds, whose decomposition wouldlead to wrong translations, are also typically fre-quent words and have an appropriate translation inthe training data.1http://snowball.tartarus.org5 DataOur data consists of the Europarl training, devel-opment and test definitions for German-Englishand Finnish-English of the 2005 ACL shared datatask (Koehn and Monz, 2005).
Both Germanand Finnish are morphologically rich languages:German has four cases and three genders andshows number, gender and case distinctions notonly on verbs, nouns, and adjectives, but alsoon determiners.
In addition, it has notoriouslymany compounds.
Finnish is a highly agglutina-tive language with a large number of inflectionalparadigms (e.g.
one for each of its 15 cases).
Nouncompounds are also frequent.
On the 2005 ACLshared MT data task, Finnish to English trans-lation showed the lowest average performance(17.9% BLEU) and German had the second low-est (21.9%), while the average BLEU scores forFrench-to-English and Spanish-to-English weremuch higher (27.1% and 27.8%, respectively).The data was preprocessed by lowercasing andfiltering out sentence pairs whose length ratio(number of words in the source language dividedby the number of words in the target language,or vice versa) was > 9.
The development andtest sets consist of 2000 sentences each.
In orderto study the effect of varying amounts of trainingdata we created several training partitions consist-ing of random selections of a subset of the fulltraining set.
The sizes of the partitions are shownin Table 1, together with the resulting percentageof out-of-vocabulary (OOV) words in the develop-ment and test sets (?type?
refers to a unique wordin the vocabulary, ?token?
to an instance in the ac-tual text).6 SystemWe use a two-pass phrase-based statistical MTsystem using GIZA++ (Och and Ney, 2000) forword alignment and Pharaoh (Koehn, 2004) forphrase extraction and decoding.
Word alignmentis performed in both directions using the IBM-4 model.
Phrases are then extracted from theword alignments using the method described in(Och and Ney, 2003).
For first-pass decoding weuse Pharaoh in n-best mode.
The decoder uses aweighted combination of seven scores: 4 transla-tion model scores (phrase-based and lexical scoresfor both directions), a trigram language modelscore, a distortion score, and a word penalty.
Non-monotonic decoding is used, with no limit on the44German-EnglishSet # sent # words oov dev oov testtrain1 5K 101K 7.9/42.6 7.9/42.7train2 25K 505K 3.8/22.1 3.7/21.9train3 50K 1013K 2.7/16.1 2.7/16.1train4 250K 5082K 1.3/8.1 1.2/7.5train5 751K 15258K 0.8/4.9 0.7/4.4Finnish-EnglishSet # sent # words oov dev oov testtrain1 5K 78K 16.6/50.6 16.4/50.6train2 25K 395K 8.6/28.2 8.4/27.8train3 50K 790K 6.3/21.0 6.2/20.8train4 250K 3945K 3.1/10.4 3.0/10.2train5 717K 11319K 1.8/6.2 1.8/6.1Table 1: Training set sizes and percentages ofOOV words (types/tokens) on the developmentand test sets.dev testFinnish-English 22.2 22.0German-English 24.6 24.8Table 2: Baseline system BLEU scores (%) on devand test sets.number of moves.
The score combination weightsare trained by a minimum error rate training pro-cedure similar to (Och and Ney, 2003).
The tri-gram language model uses modified Kneser-Neysmoothing and interpolation of trigram and bigramestimates and was trained on the English side ofthe bitext.
In the first pass, 2000 hypotheses aregenerated per sentence.
In the second pass, theseven scores described above are combined with4-gram language model scores.
The performanceof the baseline system on the development and testsets is shown in Table 2.
The BLEU scores ob-tained are state-of-the-art for this task.7 Experiments and ResultsWe first investigated to what extent the OOV rateon the development data could be reduced by ourbackoff procedure.
Table 3 shows the percentageof words that are still untranslatable after back-off.
A comparison with Table 1 shows that thebackoff model reduces the OOV rate, with a largerreduction effect observed when the training setis smaller.
We next performed translation withbackoff systems trained on each data partition.
Ineach case, the combination weights for the indi-German-Englishdev set test settrain1 5.2/27.7 5.1/27.3train2 2.0/11.7 2.0/11.6train3 1.4/8.1 1.3/7.6train4 0.5/3.1 0.5/2.9train5 0.3/1.7 0.2/1.3Finnish-Englishdev set test settrain1 9.1/28.5 9.2/28.9train2 3.8/12.4 3.7/12.3train3 2.5/8.2 2.4/8.0train4 0.9/3.2 0.9/3.0train5 0.4/1.4 0.4/1.5Table 3: OOV rates (%) on the developmentand test sets under the backoff model (wordtypes/tokens).vidual model scores were re-optimized.
Table 4shows the evaluation results on the dev set.
Sincethe BLEU score alone is often not a good indi-cator of successful translations of unknown words(the unigram or bigram precision may be increasedbut may not have a strong effect on the over-all BLEU score), position-independent word errorrate (PER) rate was measured as well.
We see im-provements in BLEU score and PERs in almostall cases.
Statistical significance was measured onPER using a difference of proportions significancetest and on BLEU using a segment-level pairedt-test.
PER improvements are significant almostall training conditions for both languages; BLEUimprovements are significant in all conditions forFinnish and for the two smallest training sets forGerman.
The effect on the overall development set(consisting of both sentences with known wordsonly and sentences with unknown words) is shownin Table 5.
As expected, the impact on overall per-formance is smaller, especially for larger trainingdata sets, due to the relatively small percentage ofOOV tokens (see Table 1).
The evaluation resultsfor the test set are shown in Tables 6 (for the sub-set of sentences with OOVs) and 7 (for the entiretest set), with similar conclusions.The examples A and B in Figure 2 demon-strate higher-scoring translations produced by thebackoff system as opposed to the baseline sys-tem.
An analysis of the backoff system outputshowed that in some cases (e.g.
examples C and45German-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 14.2 56.9 15.4 55.5train2 16.3 55.2 17.3 51.8train3 17.8 51.1 18.4 49.7train4 19.6 51.1 19.9 47.6train5 21.9 46.6 22.6 46.0Finnish-Englishbaseline backoffSet BLEU PER BLEU PERSet BLEU PER BLEU PERtrain1 12.4 59.9 13.6 57.8train2 13.0 61.2 13.9 59.1train3 14.0 58.0 14.7 57.8train4 17.4 52.7 18.4 50.8train5 16.8 52.7 18.7 50.2Table 4: BLEU (%) and position-independentword error rate (PER) on the subset of the devel-opment data containing unknown words (second-pass output).
Here and in the following tables,statistically significant differences to the baselinemodel are shown in boldface (p < 0.05).German-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 15.3 56.4 16.3 55.1train2 19.0 53.0 19.5 51.6train3 20.0 49.9 20.5 49.3train4 22.2 49.0 22.4 48.1train5 24.6 46.5 24.7 45.6Finnish-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 13.1 59.3 14.4 57.4train2 14.5 59.7 15.4 58.3train3 16.0 56.5 16.5 56.5train4 21.0 50.0 21.4 49.2train5 22.2 50.5 22.5 49.7Table 5: BLEU (%) and position-independentword error rate (PER) for the entire developmentset.German-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 14.3 56.2 15.5 55.1train2 17.1 54.3 17.6 50.7train3 17.4 50.8 18.1 49.7train4 18.9 49.8 18.8 48.2train5 19.1 46.3 19.4 46.2Finnish-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 12.4 59.5 13.5 57.5train2 13.3 60.7 14.2 59.0train3 14.1 58.2 15.1 57.3train4 17.2 54.0 18.4 50.2train5 16.6 51.8 19.0 49.4Table 6: BLEU (%) and position-independentword error rate (PER) for the test set (subset withOOV words).D in Figure 2), the backoff model produced agood translation, but the translation was a para-phrase rather than an identical match to the ref-erence translation.
Since only a single referencetranslation is available for the Europarl data (pre-venting the computation of a BLEU score basedon multiple hand-annotated references), good butnon-matching translations are not taken into ac-count by our evaluation method.
In other casesthe unknown word was translated correctly, butsince it was translated as single-word phrase thesegmentation of the entire sentence was affected.This may cause greater distortion effects since thesentence is segmented into a larger number ofsmaller phrases, each of which can be reordered.We therefore added the possibility of translatingan unknown word in its phrasal context by stem-ming up to m words to the left and right in theoriginal sentence and finding translations for theentire stemmed phrase (i.e.
the function stem()is now applied to the entire phrase).
This stepis inserted before the stemming of a single wordf in the backoff model described above.
How-ever, since translations for entire stemmed phraseswere found only in about 1% of all cases, therewas no significant effect on the BLEU score.
An-other possibility of limiting reordering effects re-sulting from single-word translations of OOVs isto restrict the distortion limit of the decoder.
Our46German-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 15.3 55.8 16.3 54.8train2 19.4 52.3 19.6 50.9train3 20.3 49.6 20.7 49.2train4 22.5 48.1 22.5 47.9train5 24.8 46.3 25.1 45.5Finnish-Englishbaseline backoffSet BLEU PER BLEU PERtrain1 12.9 58.7 14.0 57.0train2 14.5 59.5 15.3 58.4train3 15.6 56.6 16.4 56.2train4 20.6 50.3 21.0 49.6train5 22.0 50.0 22.3 49.5Table 7: BLEU (%) and position-independentword error rate (PER) for the test set (entire testset).experiments showed that this improves the BLEUscore slightly for both the baseline and the backoffsystem; the relative difference, however, remainedthe same.8 ConclusionsWe have presented a backoff model for phrase-based SMT that uses morphological abstractionsto translate unseen word forms in the foreign lan-guage input.
When a match for an unknown wordin the test set cannot be found in the trained phrasetable, the model relies instead on translation prob-abilities derived from stemmed or split versionsof the word in its phrasal context.
An evalua-tion of the model on German-English and Finnish-English translations of parliamentary proceedingsshowed statistically significant improvements inPER for almost all training conditions and signifi-cant improvements in BLEU when the training setis small (100K words), with larger improvementsfor Finnish than for German.
This demonstratesthat our method is mainly relevant for highly in-flected languages and sparse training data condi-tions.
It is also designed to improve human accep-tance of machine translation output, which is par-ticularly adversely affected by untranslated words.AcknowledgmentsThis work was funded by NSF grant no.
IIS-0308297.
We thank Ilona Pitka?nen for help withExample A: (German-English):SRC: wir sind berzeugt davon, dass ein europa des friedensnicht durch milita?rbu?ndnisse geschaffen wird.BASE: we are convinced that a europe of peace, not bymilita?rbu?ndnisse is created.BACKOFF: we are convinced that a europe of peace, notby military alliance is created.REF: we are convinced that a europe of peace will not becreated through military alliances.Example B.
(Finnish-English):SRC: arvoisa puhemies, puhuimme ta?a?lla?
eilisiltanaserviasta ja siella?
tapahtuvista vallankumouksellisistamuutoksista.BASE: mr president, we talked about here last night, onthe subject of serbia and there, of vallankumouksellisistachanges.BACKOFF: mr president, we talked about here lastnight, on the subject of serbia and there, of revolutionarychanges.REF: mr. president, last night we discussed the topic ofserbia and the revolutionary changes that are taking placethere.Example C. (Finnish-English):SRC: toivon ta?lta?
osin, etta?
yhdistyneiden kansakuntienalaisuudessa ka?yta?vissa?
neuvotteluissa pa?a?sta?isiin sell-aiseen lopputulokseen, etta?
kyproksen kreikkalainen jaturkkilainen va?esto?nosa voisivat yhdessa?
nauttia liittymisenmukanaan tuomista eduista yhdistetyssa?
tasavallassa.BASE: i hope that the united nations in the negotiationsto reach a conclusion that the greek and turkish accessionto the benefi t of the benefi ts of the republic of ydistetyssa?brings together va?esto?nosa could, in this respect, under theauspices.BACKOFF: i hope that the united nations in the nego-tiations to reach a conclusion that the greek and turkishcommunities can work together to bring the benefi ts of theaccession of the republic of ydistetyssa?.
in this respect,under theREF: in this connection, i would hope that the talksconducted under the auspices of the united nations will beable to come to a successful conclusion enabling the greekand turkish cypriot populations to enjoy the advantagesof membership of the european union in the context of areunifi ed republic.Example D. (German-English):SRC:so sind wir beim durcharbeiten des textes verfahren,wobei wir bei einer reihe von punkten versucht haben, nocheinige straffungen vorzunehmen.BASE: we are in the durcharbeiten procedures of the text,although we have tried to make a few straffungen to carryout on a number of issues.BACKOFF: we are in the durcharbeiten procedures, andwe have tried to make a few streamlining of the text in anumber of points.REF: this is how we came to go through the text, andattempted to cut down on certain items in the process.Figure 2: Translation examples (SRC = source,BASE = baseline system, BACKOFF = backoffsystem, REF = reference).
OOVs and their trans-lation are marked in boldface.47the Finnish language.ReferencesJ.A.
Bilmes and K. Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
InProceedings of the 2003 Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics,pages 4?6, Edmonton, Canada.S.
Corston-Oliver and M. Gamon.
2004.
Normaliz-ing German and English inflectional morphology toimprove statistical word alignment.
In Robert E.Frederking and Kathryn Taylor, editors, Proceedingsof the Conference of the Association for MachineTranslation in the Americas, pages 48?57, Washing-ton, DC.A.
de Gispert, J.B. Marin?o, and J.M.
Crego.
2005.
Im-proving statistical machine translation by classifyingand generalizing inflected verb forms.
In Proceed-ings of 9th European Conference on Speech Commu-nication and Technology, pages 3193?3196, Lisboa,Portugal.A.
Fraser and D. Marcu.
2005.
ISI?s participation inthe Romanian-English alignment task.
In Proceed-ings of the 2005 ACL Workshop on Building and Us-ing Parallel Texts: Data-Driven Machine Transla-tion and Beyond, pages 91?94, Ann Arbor, Michi-gan.D.
Gildea.
2001.
Statistical Language UnderstandingUsing Frame Semantics.
Ph.D. thesis, University ofCalifornia, Berkeley, California.S.
Goldwater and D. McCloskey.
2005.
Improving sta-tistical MT through morphological analysis.
In Pro-ceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Nat-ural Language Processing, pages 676?683, Vancou-ver, British Columbia, Canada.P.
Koehn and C. Monz.
2005.
Shared task: statisticalmachine translation between European languages.In Proceedings of the 2005 ACL Workshop on Build-ing and Using Parallel Texts: Data-Driven MachineTranslation and Beyond, pages 119?124, Ann Ar-bor, Michigan.P.
Koehn.
2003.
Noun Phrase Translation.
Ph.D. the-sis, Information Sciences Institute, USC, Los Ange-les, California.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Robert E. Frederking and Kathryn Taylor, editors,Proceedings of the Conference of the Association forMachine Translation in the Americas, pages 115?124, Washington, DC.P.
Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Proceedings of MTSummit X, Phuket, Thailand.C.
Lioma and I. Ounis.
2005.
Deploying part-of-speech patterns to enhance statistical phrase-basedmachine translation resources.
In Proceedings of the2005 ACL Workshop on Building and Using Paral-lel Texts: Data-Driven Machine Translation and Be-yond, pages 163?166, Ann Arbor, Michigan.S.
Niessen and H. Ney.
2001a.
Morpho-syntacticanalysis for reordering in statistical machine trans-lation.
In Proceedings of MT Summit VIII, Santiagode Compostela, Galicia, Spain.S.
Niessen and H. Ney.
2001b.
Toward hierar-chical models for statistical machine translation ofinflected languages.
In Proceedings of the ACL2001 Workshop on Data-Driven Methods in Ma-chine Translation, pages 47?54, Toulouse, France.F.J.
Och and H. Ney.
2000.
Giza++:Training of statistical translation mod-els.
http://www-i6.informatik.rwth-aachen.de/ och/software/GIZA++.html.F.J.
Och and H. Ney.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan.P.
Resnik, D. Oard, and G.A.
Levow.
2001.
Improvedcross-language retrieval using backoff translation.In Proceedings of the First International Conferenceon Human Language Technology Research, pages153?155, San Diego, California.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of the Inter-national Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, UK.C.
Xi and R. Hwa.
2005.
A backoff model for boot-strapping resources for non-English languages.
InProceedings of Human Language Technology Con-ference and Conference on Empirical Methods inNatural Language Processing, pages 851?858, Van-couver, British Columbia, Canada.I.
Zitouni, O. Siohan, and C.-H. Lee.
2003.
Hierar-chical class n-gram language models: towards bet-ter estimation of unseen events in speech recogni-tion.
In Proceedings of 8th European Conference onSpeech Communication and Technology, pages 237?240, Geneva, Switzerland.48
