Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107?116,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsSemi-Supervised Recognition of Sarcastic Sentencesin Twitter and AmazonDmitry DavidovICNCThe Hebrew UniversityJerusalem, Israeldmitry@alice.nc.huji.ac.ilOren TsurInstitute of Computer ScienceThe Hebrew UniversityJerusalem, Israeloren@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew UniversityJerusalem, Israelarir@cs.huji.ac.ilAbstractSarcasm is a form of speech act in whichthe speakers convey their message in animplicit way.
The inherently ambiguousnature of sarcasm sometimes makes it hardeven for humans to decide whether an ut-terance is sarcastic or not.
Recognition ofsarcasm can benefit many sentiment analy-sis NLP applications, such as review sum-marization, dialogue systems and reviewranking systems.In this paper we experiment with semi-supervised sarcasm identification on twovery different data sets: a collection of5.9 million tweets collected from Twit-ter, and a collection of 66000 product re-views from Amazon.
Using the Mechani-cal Turk we created a gold standard sam-ple in which each sentence was tagged by3 annotators, obtaining F-scores of 0.78 onthe product reviews dataset and 0.83 onthe Twitter dataset.
We discuss the dif-ferences between the datasets and how thealgorithm uses them (e.g., for the Amazondataset the algorithm makes use of struc-tured information).
We also discuss theutility of Twitter #sarcasm hashtags for thetask.1 IntroductionSarcasm (also known as verbal irony) is a sophis-ticated form of speech act in which the speakersconvey their message in an implicit way.
One in-herent characteristic of the sarcastic speech act isthat it is sometimes hard to recognize.
The dif-ficulty in recognition of sarcasm causes misun-derstanding in everyday communication and posesproblems to many NLP systems such as onlinereview summarization systems, dialogue systemsor brand monitoring systems due to the failure ofstate of the art sentiment analysis systems to detectsarcastic comments.
In this paper we experimentwith a semi-supervised framework for automaticidentification of sarcastic sentences.One definition for sarcasm is: the activity ofsaying or writing the opposite of what you mean,or of speaking in a way intended to make someoneelse feel stupid or show them that you are angry(Macmillan English Dictionary (2007)).
Using theformer definition, sarcastic utterances appear inmany forms (Brown, 1980; Gibbs and O?Brien,1991).
It is best to present a number of exampleswhich show different facets of the phenomenon,followed by a brief review of different aspects ofthe sarcastic use.
The sentences are all taken fromour experimental data sets:1.
?thank you Janet Jackson for yet anotheryear of Super Bowl classic rock!?
(Twitter)2.
?He?s with his other woman: XBox 360.
It?s4:30 fool.
Sure I can sleep through the gun-fire?
(Twitter)3.
?Wow GPRS data speeds are blazing fast.?(Twitter)4.
?
[I] Love The Cover?
(book, amazon)5.
?Defective by design?
(music player, ama-zon)Example (1) refers to the supposedly lame mu-sic performance in super bowl 2010 and attributesit to the aftermath of the scandalous performanceof Janet Jackson in the previous year.
Note that theprevious year is not mentioned and the reader hasto guess the context (use universal knowledge).The words yet and another might hint at sarcasm.107Example (2) is composed of three short sentences,each of them sarcastic on its own.
However, com-bining them in one tweet brings the sarcasm toits extreme.
Example (3) is a factual statementwithout explicit opinion.
However, having a fastconnection is a positive thing.
A possible sar-casm emerges from the over exaggeration (?wow?,?blazing-fast?
).Example (4) from Amazon, might be a genuinecompliment if it appears in the body of the review.However, recalling the expression ?don?t judge abook by its cover?, choosing it as the title of thereview reveals its sarcastic nature.
Although thenegative sentiment is very explicit in the iPod re-view (5), the sarcastic effect emerges from the punthat assumes the knowledge that the design is oneof the most celebrated features of Apple?s prod-ucts.
(None of the above reasoning was directlyintroduced to our algorithm.
)Modeling the underlying patterns of sarcasticutterances is interesting from the psychologicaland cognitive perspectives and can benefit var-ious NLP systems such as review summariza-tion (Popescu and Etzioni, 2005; Pang and Lee,2004; Wiebe et al, 2004; Hu and Liu, 2004) anddialogue systems.
Following the ?brilliant-but-cruel?
hypothesis (Danescu-Niculescu-Mizil et al,2009), it can help improve ranking and recommen-dation systems (Tsur and Rappoport, 2009).
Allsystems currently fail to correctly classify the sen-timent of sarcastic sentences.In this paper we utilize the semi-supervised sar-casm identification algorithm (SASI) of (Tsur etal., 2010).
The algorithm employs two modules:semi supervised pattern acquisition for identify-ing sarcastic patterns that serve as features for aclassifier, and a classification stage that classifieseach sentence to a sarcastic class.
We experimentwith two radically different datasets: 5.9 milliontweets collected from Twitter, and 66000 Amazonproduct reviews.
Although for the Amazon datasetthe algorithm utilizes structured information, re-sults for the Twitter dataset are higher.
We discussthe possible reasons for this, and also the utilityof Twitter #sarcasm hashtags for the task.
Our al-gorithm performed well in both domains, substan-tially outperforming a strong baseline based on se-mantic gap and user annotations.
To further test itsrobustness we also trained the algorithm in a crossdomain manner, achieving good results.2 DataThe datasets we used are interesting in their ownright for many applications.
In addition, our algo-rithm utilizes some aspects that are unique to thesedatasets.
Hence, before describing the algorithm,we describe the datasets in detail.Twitter Dataset.
Since Twitter is a relativelynew service, a somewhat lengthy description ofthe medium and the data is appropriate.Twitter is a very popular microblogging service.It allows users to publish and read short messagescalled tweets (also used as a verb: to tweet: the actof publishing on Twitter).
The tweet length is re-stricted to 140 characters.
A user who publishes atweet is referred to as a tweeter and the readers arecasual readers or followers if they are registered toget al tweets by this tweeter.Apart from simple text, tweets may contain ref-erences to url addresses, references to other Twit-ter users (these appear as @<user>) or a con-tent tag (called hashtags) assigned by the tweeter(#<tag>).
An example of a tweet is: ?listen-ing to Andrew Ridgley by Black Box Recorder on@Grooveshark: http://tinysong.com/cO6i #good-music?, where ?grooveshark?
is a Twitter username and #goodmusic is a tag that allows tosearch for tweets with the same tag.
Though fre-quently used, these types of meta tags are optional.In order to ignore specific references we substi-tuted such occurrences with special tags: [LINK],[USER] and [HASHTAG] thus we have ?listen-ing to Andrew Ridgley by Black Box Recorder on[USER]: [LINK] [HASHTAG]?.
It is importantto mention that hashtags are not formal and eachtweeter can define and use new tags as s/he likes.The number of special tags in a tweet is onlysubject to the 140 characters constraint.
There isno specific grammar that enforces the location ofspecial tags within a tweet.The informal nature of the medium and the 140characters length constraint encourages massiveuse of slang, shortened lingo, ascii emoticons andother tokens absent from formal lexicons.These characteristics make Twitter a fascinat-ing domain for NLP applications, although posinggreat challenges due to the length constraint, thecomplete freedom of style and the out of discoursenature of tweets.We used 5.9 million unique tweets in ourdataset: the average number of words is 14.2108words per tweet, 18.7% contain a url, 35.3% con-tain reference to another tweeter and 6.9% containat least one hashtag1.The #sarcasm hashtag One of the hashtagsused by Twitter users is dedicated to indicate sar-castic tweets.
An example of the use of the tagis: ?I guess you should expect a WONDERFULvideo tomorrow.
#sarcasm?.
The sarcastic hashtagis added by the tweeter.
This hashtag is used in-frequently as most users are not aware of it, hence,the majority of sarcastic tweets are not explicitlytagged by the tweeters.
We use tagged tweets asa secondary gold standard.
We discuss the use ofthis tag in Section 5.Amazon dataset.
We used the same datasetused by (Tsur et al, 2010), containing 66000 re-views for 120 products from Amazon.com.
Thecorpus contained reviews for books from differ-ent genres and various electronic products.
Ama-zon reviews are much longer than tweets (somereach 2000 words, average length is 953 charac-ters), they are more structured and grammatical(good reviews are very structured) and they comein a known context of a specific product.
Reviewsare semi-structured as besides the body of the re-view they all have the following fields: writer,date, star rating (the overall satisfaction of the re-view writer) and a one line summary.Reviews refer to a specific product and rarelyaddress each other.
Each review sentence is, there-fore, part of a context ?
the specific product, thestar rating, the summary and other sentences inthat review.
In that sense, sentences in the Ama-zon dataset differ radically from the contextlesstweets.
It is worth mentioning that the majorityof reviews are on the very positive side (star ratingaverage of 4.2 stars).3 Classification AlgorithmOur algorithm is semi-supervised.
The input isa relatively small seed of labeled sentences.
Theseed is annotated in a discrete range of 1 .
.
.
5where 5 indicates a clearly sarcastic sentence and1 indicates a clear absence of sarcasm.
A 1 .
.
.
5scale was used in order to allow some subjectiv-ity and since some instances of sarcasm are moreexplicit than others.1The Twitter data was generously provided to us by Bren-dan O?Connor.Given the labeled sentences, we extracted a setof features to be used in feature vectors.
Two basicfeature types are utilized: syntactic and pattern-based features.
We constructed feature vectors foreach of the labeled examples in the training set andused them to build a classifier model and assignscores to unlabeled examples.
We next provide adescription of the algorithmic framework of (Tsuret al, 2010).Data preprocessing A sarcastic utterance usu-ally has a target.
In the Amazon dataset thesetargets can be exploited by a computational al-gorithm, since each review targets a product, itsmanufacturer or one of its features, and these areexplicitly represented or easily recognized.
TheTwitter dataset is totally unstructured and lackstextual context, so we did not attempt to identifytargets.Our algorithmic methodology is based onpatterns.
We could use patterns that includethe targets identified in the Amazon dataset.However, in order to use less specific patterns,we automatically replace each appearanceof a product, author, company, book name(Amazon) and user, url and hashtag (Twitter)with the corresponding generalized meta tags?[PRODUCT]?,?[COMPANY]?,?[TITLE]?
and?[AUTHOR]?
tags2 and ?[USER]?,?[LINK]?
and?[HASHTAG]?.
We also removed all HTML tagsand special symbols from the review text.Pattern extraction Our main feature type isbased on surface patterns.
In order to extract suchpatterns automatically, we followed the algorithmgiven in (Davidov and Rappoport, 2006).
We clas-sified words into high-frequency words (HFWs)and content words (CWs).
A word whose cor-pus frequency is more (less) than FH (FC) is con-sidered to be a HFW (CW).
Unlike in (Davidovand Rappoport, 2006), we consider all punctuationcharacters as HFWs.
We also consider [product],[company], [title], [author] tags as HFWs for pat-tern extraction.
We define a pattern as an orderedsequence of high frequency words and slots forcontent words.
The FH and FC thresholds wereset to 1000 words per million (upper bound forFC) and 100 words per million (lower bound forFH )3.2Appropriate names are provided with each review so thisreplacement can be done automatically.3Note that FH and FC set bounds that allow overlap be-tween some HFWs and CWs.109The patterns allow 2-6 HFWs and 1-6 slots forCWs.
For each sentence it is possible to gener-ate dozens of patterns that may overlap.
For ex-ample, given a sentence ?Garmin apparently doesnot care much about product quality or customersupport?, we have generated several patterns in-cluding ?
[COMPANY] CW does not CW much?,?does not CW much about CW CW or?, ?not CWmuch?
and ?about CW CW or CW CW.?.
Notethat ?[COMPANY]?
and ?.?
are treated as highfrequency words.Pattern selection The pattern extraction stageprovides us with hundreds of patterns.
However,some of them are either too general or too specific.In order to reduce the feature space, we have usedtwo criteria to select useful patterns.First, we removed all patterns which appearonly in sentences originating from a single prod-uct/book (Amazon).
Such patterns are usuallyproduct-specific.
Next we removed all patternswhich appear in the seed both in some example la-beled 5 (clearly sarcastic) and in some other exam-ple labeled 1 (obviously not sarcastic).
This filtersout frequent generic and uninformative patterns.Pattern selection was performed only on the Ama-zon dataset as it exploits review?s meta data.Pattern matching Once patterns are selected,we have used each pattern to construct a single en-try in the feature vectors.
For each sentence wecalculated a feature value for each pattern as fol-lows:??????????????????????????????????????????
?1 : Exact match ?
all the pattern componentsappear in the sentence in correctorder without any additional words.?
: Sparse match ?
same as exact matchbut additional non-matching words can beinserted between pattern components.?
?
n/N : Incomplete match ?
only n > 1 of N patterncomponents appear in the sentence,while some non-matching words canbe inserted in-between.
At least one of theappearing components should be a HFW.0 : No match ?
nothing or only a singlepattern component appears in the sentence.0 ?
?
?
1 and 0 ?
?
?
1 are parameters we useto assign reduced scores for imperfect matches.Since the patterns we use are relatively long, ex-act matches are uncommon, and taking advantageof partial matches allows us to significantly re-duce the sparsity of the feature vectors.
We used?\?
0.05 0.1 0.20.05 0.48 0.45 0.390.1 0.50 0.51 0.400.2 0.40 0.42 0.33Table 1: Results (F-Score for ?no enrichment?
mode) ofcross validation with various values for ?
and ?
on Twit-ter+Amazon data?
= ?
= 0.1 in all experiments.
Table 1 demon-strates the results obtained with different valuesfor ?
and ?.Thus, for the sentence ?Garmin apparently doesnot care much about product quality or customersupport?, the value for ?
[company] CW does not?would be 1 (exact match); for ?
[company] CWnot?
would be 0.1 (sparse match due to insertionof ?does?
); and for ?
[company] CW CW does not?would be 0.1 ?
4/5 = 0.08 (incomplete matchsince the second CW is missing).Punctuation-based features In addition topattern-based features we used the followinggeneric features: (1) Sentence length in words,(2) Number of ?!?
characters in the sentence, (3)Number of ???
characters in the sentence, (4)Number of quotes in the sentence, and (5) Num-ber of capitalized/all capitals words in the sen-tence.
All these features were normalized by di-viding them by the (maximal observed value ?
av-eraged maximal value of the other feature groups),thus the maximal weight of each of these fea-tures is equal to the averaged weight of a singlepattern/word/n-gram feature.Data enrichment Since we start with only asmall annotated seed for training (particularly, thenumber of clearly sarcastic sentences in the seed ismodest) and since annotation is noisy and expen-sive, we would like to find more training exampleswithout requiring additional annotation effort.To achieve this, we posited that sarcastic sen-tences frequently co-appear in texts with other sar-castic sentences (i.e.
example (2) in Section 1).We performed an automated web search using theYahoo!
BOSS API4, where for each sentence s inthe training set (seed), we composed a search en-gine query qs containing this sentence5.
We col-lected up to 50 search engine snippets for eachexample and added the sentences found in thesesnippets to the training set.
The label (level of sar-4http://developer.yahoo.com/search/boss.5If the sentence contained more than 6 words, only thefirst 6 words were included in the search engine query.110casm) Label(sq) of a newly extracted sentence sqis similar to the label Label(s) of the seed sen-tence s that was used for the query that acquired it.The seed sentences together with newly acquiredsentences constitute the (enriched) training set.Data enrichment was performed only for theAmazon dataset where we have a manually taggedseed and the sentence structure is closer to stan-dard English grammar.
We refer the reader to(Tsur et al, 2010) for more details about the en-richment process and for a short discussion aboutthe usefulness of web-based data enrichment in thescope of sarcasm recognition.Classification In order to assign a score to newexamples in the test set we use a k-nearest neigh-bors (kNN)-like strategy.
We construct featurevectors for each example in the training and testsets.
We would like to calculate the score for eachexample in the test set.
For each feature vector v inthe test set, we compute the Euclidean distance toeach of the matching vectors in the extended train-ing set, where matching vectors share at least onepattern feature with v.Let ti, i = 1..k be the k vectors with lowestEuclidean distance to v6.
Then v is classified witha label l as follows:Count(l) = Fraction of training vectors with label lLabel(v) =[1k?iCount(Label(ti)) ?
Label(ti)?j Count(label(tj))]Thus the score is a weighted average of the k clos-est training set vectors.
If there are less than kmatching vectors for the given example then fewervectors are used in the computation.
If there areno matching vectors found for v, we assigned thedefault value Label(v) = 1, since sarcastic sen-tences are fewer in number than non-sarcastic ones(this is a ?most common tag?
strategy).4 Evaluation SetupSeed and extended training sets (Amazon).
Asdescribed in the previous section, SASI is semi su-pervised, hence requires a small seed of annotateddata.
We used the same seed of 80 positive (sar-castic) examples and 505 negative examples de-scribed at (Tsur et al, 2010).After automatically expanding the training set,our training data now contains 471 positive exam-ples and 5020 negative examples.
These ratios are6We used k = 5 for all experiments.to be expected, since non-sarcastic sentences out-number sarcastic ones, definitely when most on-line reviews are positive (Liu et al, 2007).
Thisgenerally positive tendency is also reflected in ourdata ?
the average number of stars is 4.12.Seed training set with #sarcasm (Twitter).
Weused a sample of 1500 tweets marked with the#sarcasm hashtag as a positive set that representssarcasm styles special to Twitter.
However, this setis very noisy (see discussion in Section 5).Seed training set (cross domain).
Results ob-tained by training on the 1500 #sarcasm hash-tagged tweets were not promising.
Examination ofthe #sarcasm tagged tweets shows that the annota-tion is biased and noisy as we discuss in lengthin Section 5.
A better annotated set was neededin order to properly train the algorithm.
Sarcas-tic tweets are sparse and hard to find and annotatemanually.
In order to overcome sparsity we usedthe positive seed annotated on the Amazon dataset.The training set was completed by manually se-lected negative example from the Twitter dataset.Note that in this setting our training set is thus ofmixed domains.4.1 Star-sentiment baselineMany studies on sarcasm suggest that sarcasmemerges from the gap between the expected utter-ance and the actual utterance (see echoic mention,allusion and pretense theories in Related WorkSection( 6)).
We implemented a baseline designedto capture the notion of sarcasm as reflected bythese models, trying to meet the definition ?sayingthe opposite of what you mean in a way intendedto make someone else feel stupid or show you areangry?.We exploit the meta-data provided by Amazon,namely the star rating each reviewer is obligedto provide, in order to identify unhappy review-ers.
From this set of negative reviews, our base-line classifies as sarcastic those sentences that ex-hibit strong positive sentiment.
The list of positivesentiment words is predefined and captures wordstypically found in reviews (for example, ?great?,?excellent?, ?best?, ?top?, ?exciting?, etc).4.2 Evaluation procedureWe used two experimental frameworks to testSASI?s accuracy.
In the first experiment we eval-uated the pattern acquisition process, how consis-tent it is and to what extent it contributes to correct111classification.
We did that by 5-fold cross valida-tion over the seed data.In the second experiment we evaluated SASI ona test set of unseen sentences, comparing its out-put to a gold standard annotated by a large numberof human annotators (using the Mechanical Turk).This way we verify that there is no over-fitting andthat the algorithm is not biased by the notion ofsarcasm of a single seed annotator.5-fold cross validation (Amazon).
In this ex-perimental setting, the seed data was divided to 5parts and a 5-fold cross validation test is executed.Each time, we use 4 parts of the seed as the train-ing data and only this part is used for the featureselection and data enrichment.
This 5-fold pro-cess was repeated ten times.
This procedure wasrepeated with different sets of optional features.We used 5-fold cross validation and not thestandard 10-fold since the number of seed exam-ples (especially positive) is relatively small hence10-fold is too sensitive to the broad range of possi-ble sarcastic patterns (see the examples in Section1).Classifying new sentences (Amazon & Twitter).Evaluation of sarcasm is a hard task due to theelusive nature of sarcasm, as discussed in Sec-tion 1.
In order to evaluate the quality of our al-gorithm, we used SASI to classify all sentencesin both corpora (besides the small seed that waspre-annotated and was used for the evaluation inthe 5-fold cross validation experiment).
Since itis impossible to created a gold standard classifica-tion of each and every sentence in the corpus, wecreated a small test set by sampling 90 sentenceswhich were classified as sarcastic (labels 3-5) and90 sentences classified as not sarcastic (labels 1,2).The sampling was performed on the whole corpusleaving out only the seed data.Again, the meta data available in the Amazondataset alows us a stricter evaluation.
In orderto make the evaluation harder for our algorithmand more relevant, we introduced two constraintsto the sampling process: i) we sampled only sen-tences containing a named-entity or a reference toa named entity.
This constraint was introduced inorder to keep the evaluation set relevant, since sen-tences that refer to the named entity (the target ofthe review) are more likely to contain an explicitor implicit sentiment.
ii) we restricted the non-sarcastic sentences to belong to negative reviews(1-3 stars) so that all sentences in the evaluationset are drawn from the same population, increas-ing the chances they convey various levels of di-rect or indirect negative sentiment7.Experimenting with the Twitter dataset, we sim-ply classified each tweet into one of 5 classes(class 1: not sarcastic, class 5: clearly sarcastic)according to the label given by the algorithm.
Justlike the evaluation of the algorithm on the Amazondataset, we created a small evaluation set by sam-pling 90 sentences which were classified as sarcas-tic (labels 3-5) and 90 sentences classified as notsarcastic (labels 1,2).Procedure Each evaluation set was randomlydivided to 5 batches.
Each batch contained 36 sen-tences from the evaluation set and 4 anchor sen-tences: two with sarcasm and two sheer neutral.The anchor sentences were not part of the test setand were the same in all five batches.
The purposeof the anchor sentences is to control the evaluationprocedure and verify that annotation is reasonable.We ignored the anchor sentences when assessingthe algorithm?s accuracy.We used Amazon?s Mechanical Turk8 servicein order to create a gold standard for the evalua-tion.
We employed 15 annotators for each eval-uation set.
We used a relatively large number ofannotators in order to overcome the possible biasinduced by subjectivity (Muecke, 1982).
Each an-notator was asked to assess the level of sarcasm ofeach sentence of a set of 40 sentences on a scale of1-5.
In total, each sentence was annotated by threedifferent annotators.Inter Annotator Agreement.
To simplify theassessment of inter-annotator agreement, the scal-ing was reduced to a binary classification where 1and 2 were marked as non-sarcastic and 3-5 as sar-castic (recall that 3 indicates a hint of sarcasm and5 indicates ?clearly sarcastic?).
We checked theFleiss?
?
statistic to measure agreement betweenmultiple annotators.
The inter-annotator agree-ment statistic was ?
= 0.34 on the Amazon datasetand ?
= 0.41 on the Twitter dataset.These agreement statistics indicates a fairagreement.
Given the fuzzy nature of the task at7Note that the second constraint makes the problem lesseasy.
If taken from all reviews, many of the sentences wouldbe positive sentences which are clearly non-sarcastic.
Doingthis would bias selection to positive vs. negative samples in-stead of sarcastic-nonsarcastic samples.8https://www.mturk.com/mturk/welcome112Prec.
Recall Accuracy F-scorepunctuation 0.256 0.312 0.821 0.281patterns 0.743 0.788 0.943 0.765pat+punct 0.868 0.763 0.945 0.812enrich punct 0.4 0.390 0.832 0.395enrich pat 0.762 0.777 0.937 0.769all: SASI 0.912 0.756 0.947 0.827Table 2: 5-fold cross validation results on the Amazon goldstandard using various feature types.
punctuation: punctua-tion mark;, patterns: patterns; enrich: after data enrichment;enrich punct: data enrichment based on punctuation only; en-rich pat: data enrichment based on patterns only; SASI: allfeatures combined.hand, this ?
value is certainly satisfactory.
We at-tribute the better agreement on the twitter data tothe fact that in twitter each sentence (tweet) is con-text free, hence the sentiment in the sentence is ex-pressed in a way that can be perceived more easily.Sentences from product reviews come as part of afull review, hence the the sarcasm sometimes re-lies on other sentences in the review.
In our evalu-ation scheme, our annotators were presented withindividual sentences, making the agreement lowerfor those sentences taken out of their original con-text.
The agreement on the control set (anchor sen-tences) had ?
= 0.53.Using Twitter #sarcasm hashtag.
In addition tothe gold standard annotated using the MechanicalTurk, we collected 1500 tweets that were tagged#sarcastic by their tweeters.
We call this samplethe hash-gold standard.
It was used to further eval-uate recall.
This set (along with the negative sam-ple) was used for a 5-fold cross validation in thesame manner describe for Amazon.5 Results and discussion5-fold cross validation (Amazon).
Results areanalyzed and discussed in detail in (Tsur et al,2010), however, we summarize it here (Table 2)in order to facilitate comparison with the resultsobtained on the Twitter dataset.
SASI, includingall components, exhibits the best overall perfor-mances with 91.2% precision and with F-Scoreof 0.827.
Interestingly, although data enrichmentbrings SASI to the best performance in both preci-sion and F-score, patterns+punctuations achievesalmost comparable results.Newly introduced sentences (Amazon).
In thesecond experiment we evaluated SASI based on agold standard annotation created by 15 annotators.Table 3 presents the results of our algorithm aswell as results of the heuristic baseline that makesPrec.
Recall FalsePos FalseNeg F ScoreStar-sent.
0.5 0.16 0.05 0.44 0.242SASI (AM) 0.766 0.813 0.11 0.12 0.788SASI (TW) 0.794 0.863 0.094 0.15 0.827Table 3: Evaluation on the Amazon (AM) and the Twitter(TW) evaluation sets obtained by averaging on 3 human an-notations per sentence.
TW results were obtained with cross-domain training.Prec.
Recall Accuracy F-scorepunctuation 0.259 0.26 0.788 0.259patterns 0.765 0.326 0.889 0.457enrich punct 0.18 0.316 0.76 0.236enrich pat 0.685 0.356 0.885 0.47all no enrich 0.798 0.37 0.906 0.505all SASI: 0.727 0.436 0.896 0.545Table 4: 5-fold cross validation results on the Twitter hash-gold standard using various feature types.
punctuation: punc-tuation marks; patterns: patterns; enrich: after data enrich-ment; enrich punct: data enrichment based on punctuationonly; enrich pat: data enrichment based on patterns only;SASI: all features combined.use of meta-data, designed to capture the gap be-tween an explicit negative sentiment (reflected bythe review?s star rating) and explicit positive senti-ment words used in the review.
Precision of SASIis 0.766, a significant improvement over the base-line with precision of 0.5.The F-score shows more impressive improve-ment as the baseline shows decent precision but avery limited recall since it is incapable of recog-nizing subtle sarcastic sentences.
These results fitthe works of (Brown, 1980; Gibbs and O?Brien,1991) claiming many sarcastic utterances do notconform to the popular definition of ?saying orwriting the opposite of what you mean?.
Table 3also presents the false positive and false negativeratios.
The low false negative ratio of the baselineconfirms that while recognizing a common typeof sarcasm, the naive definition of sarcasm cannotcapture many other types sarcasm.Newly introduced sentences (Twitter).
Resultson the Twitter dataset are even better than thoseobtained on the Amazon dataset, with accuracy of0.947 (see Table 3 for precision and recall).Tweets are less structured and are context free,hence one would expect SASI to perform poorlyon tweets.
Moreover, the positive part of the seedis taken from the Amazon corpus hence mightseem tailored to sarcasm type targeted at prod-ucts and part of a harsh review.
On top of that,the positive seed introduces some patterns withtags that never occur in the Twitter test set ([prod-uct/company/title/author]).113Our explanation of the excellent results is three-fold: i) SASI?s robustness is achieved by the sparsematch (?)
and incomplete match (?)
that toler-ate imperfect pattern matching and enable the useof variations of the patterns in the learned featurevector.
?
and ?
allow the introduction of patternswith components that are absent from the posi-tive seed, and can perform even with patterns thatcontain special tags that are not part of the testset.
ii) SASI learns a model which spans a featurespace with more than 300 dimensions.
Only partof the patterns consist of meta tags that are spe-cial to product reviews, the rest are strong enoughto capture the structure of general sarcastic sen-tences and not product-specific sarcastic sentencesonly.
iii) Finally, in many cases, it might be thatthe contextless nature of Twitter forces tweeters toexpress sarcasm in a way that is easy to understandfrom individual sentence.
Amazon sentences co-appear with other sentences (in the same review)thus the sarcastic meaning emerges from the con-text.
Our evaluation scheme presents the annota-tors with single sentences therefore Amazon sen-tences might be harder to agree on.hash gold standard (Twitter).
In order to fur-ther test out algorithm we built a model consist-ing of the positive sample of the Amazon training,the #sarcasm hash-tagged tweets and a sample ofnon sarcastic tweets as the negative training set.We evaluated it in a 5-fold cross validation man-ner (only against the hash-gold standard).
Whileprecision is still high with 0.727, recall drops to0.436 and the F-Score is 0.545.Looking at the hash-gold standard set, we ob-served three main uses for the #sarcasm hashtag.Differences between the various uses can explainthe relatively low recall.
i) The tag is used as asearch anchor.
Tweeters add the hashtag to tweetsin order to make them retrievable when searchingfor the tag.
ii) The tag is often abused and addedto non sarcastic tweets, typically to clarify that aprevious tweet should have been read sarcastically,e.g.
: ?
@wrightfan05 it was #Sarcasm ?.
iii) Thetag serves as a sarcasm marker in cases of a verysubtle sarcasm where the lack of context, the 140length constraint and the sentence structure makeit impossible to get the sarcasm without the ex-plicit marker.
Typical examples are: ?#sarcasmnot at all.?
or ?can?t wait to get home tonite #sar-casm.
?, which cannot be decided sarcastic withoutthe full context or the #sarcasm marker.These three observations suggest that the hash-gold standard is noisy (containing non-sarcastictweets) and is biased toward the hardest (insepa-rable) forms of sarcasm where even humans getit wrong without an explicit indication.
Giventhe noise and the bias, the recall is not as bad asthe raw numbers suggest and is actually in synchwith the results obtained on the Mechanical Turkhuman-annotated gold standard.
Table 4 presentsdetailed results and the contribution of each typeof feature to the classification.We note that the relative sparseness of sarcas-tic utterances in everyday communication as wellas in these two datasets make it hard to accuratelyestimate the recall value over these huge unanno-tated data sets.
Our experiment, however, indi-cates that we achieve reasonable recall rates.Punctuation Surprisingly, punctuation marksserve as the weakest predictors, in contrast to Tep-permann et al (2006).
An exception is three con-secutive dots, which when combine with other fea-tures constitute a strong predictor.
Interestinglythough, while in the cross validation experimentsSASI performance varies greatly (due to the prob-lematic use of the #sarcasm hashtag, describedpreviously), performance based only on punctua-tion are similar (Table 2 and Table 4).Tsur et al (2010) presents some additional ex-amples for the contribution of each type of featureand their combinations.6 Related WorkWhile the use of irony and sarcasm is well stud-ied from its linguistic and psychologic aspects(Muecke, 1982; Stingfellow, 1994; Gibbs and Col-ston, 2007), automatic recognition of sarcasm is anovel task, addressed only by few works.
In thecontext of opinion mining, sarcasm is mentionedbriefly as a hard nut that is yet to be cracked, seecomprehensive overview by (Pang and Lee, 2008).Tepperman et al (2006) identify sarcasm inspoken dialogue systems, their work is restrictedto sarcastic utterances that contain the expres-sion ?yeah-right?
and it depends heavily on cuesin the spoken dialogue such as laughter, pauseswithin the speech stream, the gender (recognizedby voice) of the speaker and prosodic features.Burfoot and Baldwin (2009) use SVM to deter-mine whether newswire articles are true or satir-ical.
They introduce the notion of validity whichmodels absurdity via a measure somewhat close to114PMI.
Validity is relatively lower when a sentenceincludes a made-up entity or when a sentence con-tains unusual combinations of named entities suchas, for example, those in the satirical article be-ginning ?Missing Brazilian balloonist Padre spot-ted straddling Pink Floyd flying pig?.
We notethat while sarcasm can be based on exaggerationor unusual collocations, this model covers only alimited subset of the sarcastic utterances.Tsur et al (2010) propose a semi supervisedframework for recognition of sarcasm.
The pro-posed algorithm utilizes some features specific to(Amazon) product reviews.
This paper continuesthis line, proposing SASI a robust algorithm thatsuccessfully captures sarcastic sentences in other,radically different, domains such as twitter.Utsumi (1996; 2000) introduces the implicit dis-play theory, a cognitive computational frameworkthat models the ironic environment.
The complexaxiomatic system depends heavily on complex for-malism representing world knowledge.
Whilecomprehensive, it is currently impractical to im-plement on a large scale or for an open domain.Mihalcea and Strapparava (2005) and Mihalceaand Pulman (2007) present a system that identi-fies humorous one-liners.
They classify sentencesusing naive Bayes and SVM.
They conclude thatthe most frequently observed semantic features arenegative polarity and human-centeredness.
Thesefeatures are also observed in some sarcastic utter-ances.Some philosophical, psychological and linguis-tic theories of irony and sarcasm are worth refer-encing as a theoretical framework: the constraintssatisfaction theory (Utsumi, 1996; Katz, 2005),the role playing theory (Clark and Gerrig, 1984),the echoic mention framework (Wilson and Sper-ber, 1992) and the pretence framework (Gibbs,1986).
These are all based on violation of the max-ims proposed by Grice (1975).7 ConclusionWe used SASI, the first robust algorithm for recog-nition of sarcasm, to experiment with a novelTwitter dataset and compare performance with anAmazon product reviews dataset.
Evaluating invarious ways and with different parameters con-figurations, we achieved high precision, recall andF-Score on both datasets even for cross-domaintraining and with no need for domain adaptation.In the future we will test the contribution ofsarcasm recognition for review ranking and sum-marization systems and for brand monitoring sys-tems.ReferencesR.
L. Brown.
1980.
The pragmatics of verbal irony.In R. W. Shuy and A. Snukal, editors, Language useand the uses of language, pages 111?127.
George-town University Press.Clint Burfoot and Timothy Baldwin.
2009.
Automaticsatire detection: Are you having a laugh?
In Pro-ceedings of the ACL-IJCNLP 2009 Conference ShortPapers, pages 161?164, Suntec, Singapore, August.Association for Computational Linguistics.H.
Clark and R. Gerrig.
1984.
On the pretence the-ory of irony.
Journal of Experimental Psychology:General, 113:121?126.Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinberg, and Lillian Lee.
2009.
How opinionsare received by online communities: A case study onamazon.com helpfulness votes.
Jun.D.
Davidov and A. Rappoport.
2006.
Efficientunsupervised discovery of word categories usingsymmetric patterns and high frequency words.
InCOLING-ACL.Macmillan English Dictionary.
2007.
Macmillan En-glish Dictionary.
Macmillan Education, 2 edition.Raymond W Gibbs and Herbert L. Colston, editors.2007.
Irony in Language and Thought.
Routledge(Taylor and Francis), New York.R.
W. Gibbs and J. E. O?Brien.
1991.
Psychologicalaspects of irony understanding.
Journal of Pragmat-ics, 16:523?530.R.
Gibbs.
1986.
On the psycholinguistics of sar-casm.
Journal of Experimental Psychology: Gen-eral, 105:3?15.H.
P. Grice.
1975.
Logic and conversation.
In PeterCole and Jerry L. Morgan, editors, Syntax and se-mantics, volume 3.
New York: Academic Press.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In KDD ?04: Proceed-ings of the tenth ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 168?177, New York, NY, USA.
ACM.A.
Katz.
2005.
Discourse and social-cultural factorsin understanding non literal language.
In Colston H.and Katz A., editors, Figurative language compre-hension: Social and cultural influences, pages 183?208.
Lawrence Erlbaum Associates.115Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang,and Ming Zhou.
2007.
Low-quality product re-view detection in opinion summarization.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 334?342.Rada Mihalcea and Stephen G. Pulman.
2007.
Char-acterizing humour: An exploration of features in hu-morous texts.
In CICLing, pages 337?347.Rada Mihalcea and Carlo Strapparava.
2005.
Makingcomputers laugh: Investigations in automatic humorrecognition.
pages 531?538, Vancouver, Canada.D.C.
Muecke.
1982.
Irony and the ironic.
Methuen,London, New York.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe ACL, pages 271?278.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Now Publishers Inc, July.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InHLT ?05: Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 339?346,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Frank Jr. Stingfellow.
1994.
The Meaning of Irony.State University of NY, New York.J.
Tepperman, D. Traum, and S. Narayanan.
2006.Yeah right: Sarcasm recognition for spoken dialoguesystems.
In InterSpeech ICSLP, Pittsburgh, PA.Oren Tsur and Ari Rappoport.
2009.
Revrank: A fullyunsupervised algorithm for selecting the most help-ful book reviews.
In International AAAI Conferenceon Weblogs and Social Media.Oren Tsur, Dmitry Davidiv, and Ari Rappoport.
2010.Icwsm ?
a great catchy name: Semi-supervisedrecognition of sarcastic sentences in product re-views.
In International AAAI Conference on We-blogs and Social Media.Akira Utsumi.
1996.
A unified theory of irony andits computational formalization.
In COLING, pages962?967.Akira Utsumi.
2000.
Verbal irony as implicit dis-play of ironic environment: Distinguishing ironicutterances from nonirony.
Journal of Pragmatics,32(12):1777?1806.Janyce Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277?
308, January.D.
Wilson and D. Sperber.
1992.
On verbal irony.
Lin-gua, 87:53?76.116
