Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152?1161,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPSimple Coreference Resolution with Rich Syntactic and Semantic FeaturesAria Haghighi and Dan KleinComputer Science DivisionUC Berkeley{aria42, klein}@cs.berkeley.eduAbstractCoreference systems are driven by syntactic, se-mantic, and discourse constraints.
We presenta simple approach which completely modularizesthese three aspects.
In contrast to much currentwork, which focuses on learning and on the dis-course component, our system is deterministic andis driven entirely by syntactic and semantic com-patibility as learned from a large, unlabeled corpus.Despite its simplicity and discourse naivete, oursystem substantially outperforms all unsupervisedsystems and most supervised ones.
Primary con-tributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) thedemonstration that most remaining errors can be at-tributed to syntactic and semantic factors externalto the coreference phenomenon (and perhaps bestaddressed by non-coreference systems).1 IntroductionThe resolution of entity reference is influenced bya variety of constraints.
Syntactic constraints likethe binding theory, the i-within-i filter, and appos-itive constructions restrict reference by configura-tion.
Semantic constraints like selectional compat-ibility (e.g.
a spokesperson can announce things)and subsumption (e.g.
Microsoft is a company)rule out many possible referents.
Finally, dis-course phenomena such as salience and centeringtheory are assumed to heavily influence referencepreferences.
As these varied factors have givenrise to a multitude of weak features, recent workhas focused on how best to learn to combine themusing models over reference structures (Culotta etal., 2007; Denis and Baldridge, 2007; Klenner andAilloud, 2007).In this work, we break from the standard view.Instead, we consider a vastly more modular systemin which coreference is predicted from a determin-istic function of a few rich features.
In particu-lar, we assume a three-step process.
First, a self-contained syntactic module carefully representssyntactic structures using an augmented parser andextracts syntactic paths from mentions to potentialantecedents.
Some of these paths can be ruled inor out by deterministic but conservative syntacticconstraints.
Importantly, the bulk of the work inthe syntactic module is in making sure the parsesare correctly constructed and used, and this mod-ule?s most important training data is a treebank.Second, a self-contained semantic module evalu-ates the semantic compatibility of headwords andindividual names.
These decisions are made fromcompatibility lists extracted from unlabeled datasources such as newswire and web data.
Finally,of the antecedents which remain after rich syntac-tic and semantic filtering, reference is chosen tominimize tree distance.This procedure is trivial where most systems arerich, and so does not need any supervised corefer-ence data.
However, it is rich in important wayswhich we argue are marginalized in recent coref-erence work.
Interestingly, error analysis from ourfinal system shows that its failures are far moreoften due to syntactic failures (e.g.
parsing mis-takes) and semantic failures (e.g.
missing knowl-edge) than failure to model discourse phenomenaor appropriately weigh conflicting evidence.One contribution of this paper is the explorationof strong modularity, including the result that oursystem beats all unsupervised systems and ap-proaches the state of the art in supervised ones.Another contribution is the error analysis resultthat, even with substantial syntactic and semanticrichness, the path to greatest improvement appearsto be to further improve the syntactic and semanticmodules.
Finally, we offer our approach as a verystrong, yet easy to implement, baseline.
We makeno claim that learning to reconcile disparate fea-tures in a joint model offers no benefit, only that itmust not be pursued to the exclusion of rich, non-reference analysis.2 Coreference ResolutionIn coreference resolution, we are given a docu-ment which consists of a set of mentions; each1152mention is a phrase in the document (typicallyan NP) and we are asked to cluster mentions ac-cording to the underlying referent entity.
Thereare three basic mention types: proper (BarackObama), nominal (president), and pronominal(he).1For comparison to previous work, we eval-uate in the setting where mention boundaries aregiven at test time; however our system can easilyannotate reference on all noun phrase nodes in aparse tree (see Section 3.1.1).2.1 Data SetsIn this work we use the following data sets:Development: (see Section 3)?
ACE2004-ROTH-DEV: Dev set split of the ACE2004 training set utilized in Bengston andRoth (2008).
The ACE data also annotatespre-nominal mentions which we map ontonominals.
68 documents and 4,536 mentions.Testing: (see Section 4)?
ACE2004-CULOTTA-TEST: Test set split of theACE 2004 training set utilized in Culotta etal.
(2007) and Bengston and Roth (2008).Consists of 107 documents.2?
ACE2004-NWIRE: ACE 2004 Newswire set tocompare against Poon and Domingos (2008).Consists of 128 documents and 11,413 men-tions; intersects with the other ACE data sets.?
MUC-6-TEST: MUC6 formal evaluation setconsisting of 30 documents and 2,068 men-tions.Unlabeled: (see Section 3.2)?
BLIPP: 1.8 million sentences of newswireparsed with the Charniak (2000) parser.
Nolabeled coreference data; used for mining se-mantic information.?
WIKI: 25k articles of English Wikipedia ab-stracts parsed by the Klein and Manning(2003) parser.3No labeled coreference data;used for mining semantic information.1Other mention types exist and are annotated (such as pre-nominal), which are treated as nominals in this work.2The evaluation set was not made available to non-participants.3Wikipedia abstracts consist of roughly the first paragraphof the corresponding article2.2 EvaluationWe will present evaluations on multiple corefer-ence resolution metrics, as no single one is clearlysuperior:?
Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity clus-ter.
Note that this over-penalizes the mergeror separation of clusters quadratically in thesize of the cluster.?
b3(Amit and Baldwin, 1998): For each men-tion, form the intersection between the pre-dicted cluster and the true cluster for thatmention.
The precision is the ratio of the in-tersection and the true cluster sizes and recallthe ratio of the intersection to the predictedsizes; F1 is given by the harmonic mean overprecision and recall from all mentions.?
MUC (Vilain et al, 1995): For each true clus-ter, compute the number of predicted clusterswhich need to be merged to cover the truecluster.
Divide this quantity by true clustersize minus one.
Recall is given by the sameprocedure with predicated and true clustersreversed.4?
CEAF (Luo, 2005): For a similarity functionbetween predicted and true clusters, CEAFscores the best match between true and pre-dicted clusters using this function.
We usethe ?3similarity function from Luo (2005).3 System DescriptionIn this section we develop our system and re-port developmental results on ACE2004-ROTH-DEV (see Section 2.1); we report pairwise F1 fig-ures here, but report on many more evaluationmetrics in Section 4.
At a high level, our systemresembles a pairwise coreference model (Soon etal., 1999; Ng and Cardie, 2002; Bengston andRoth, 2008); for each mention mi, we select ei-ther a single-best antecedent amongst the previ-ous mentions m1, .
.
.
,mi?1, or the NULL men-tion to indicate the underlying entity has not yetbeen evoked.
Mentions are linearly ordered ac-cording to the position of the mention head withties being broken by the larger node coming first.4The MUC measure is problematic when the system pre-dicts many more clusters than actually exist (Luo, 2005;Finkel and Manning, 2008); also, singleton clusters do notcontribute to evaluation.1153While much research (Ng and Cardie, 2002; Cu-lotta et al, 2007; Haghighi and Klein, 2007; Poonand Domingos, 2008; Finkel and Manning, 2008)has explored how to reconcile pairwise decisionsto form coherent clusters, we simply take the tran-sitive closure of our pairwise decision (as in Ngand Cardie (2002) and Bengston and Roth (2008))which can and does cause system errors.In contrast to most recent research, our pair-wise decisions are not made with a learned modelwhich outputs a probability or confidence, but in-stead for each mentionmi, we select an antecedentamongst m1, .
.
.
,mi?1or the NULL mention asfollows:?
Syntactic Constraint: Based on syntac-tic configurations, either force or disallowcoreference between the mention and an an-tecedent.
Propagate this constraint (see Fig-ure 4).?
Semantic/Syntactic Filter: Filter the re-maining possible antecedents based uponcompatibility with the mention (see Fig-ure 2).?
Selection: Select the ?closest?
mention fromthe set of remaining possible antecedents (seeFigure 1) or the NULL antecedent if empty.Initially, there is no syntactic constraint (im-proved in Section 3.1.3), the antecedent com-patibility filter allows proper and nominal men-tions to corefer only with mentions that have thesame head (improved in Section 3.2), and pro-nouns have no compatibility constraints (improvedin Section 3.1.2).
Mention heads are determinedby parsing the given mention span with the Stan-ford parser (Klein and Manning, 2003) and us-ing the Collins head rules (Collins, 1999); Poonand Domingos (2008) showed that using syntacticheads strongly outperformed a simple rightmostheadword rule.
The mention type is determinedby the head POS tag: proper if the head tag is NNPor NNPS, pronoun if the head tag is PRP, PRP$, WP,or WP$, and nominal otherwise.For the selection phase, we order mentionsm1, .
.
.
,mi?1according to the position of thehead word and select the closest mention that re-mains after constraint and filtering are applied.This choice reflects the intuition of Grosz et al(1995) that speakers only use pronominal men-tions when there are not intervening compatibleS!!!!!!!
"""""""NP#1###$$$NPNNPNintendoPP%%&&INofNP#2NNPAmericaVP''''((((VBDannouncedNP#3)))***NP#1PRP$itsNP%%&&JJnewNNconsoleFigure 1: Example sentence where closest tree dis-tance between mentions outperforms raw distance.For clarity, each mention NP is labeled with theunderlying entity id.mentions.
This system yields a rather low 48.9pairwise F1 (see BASE-FLAT in Table 2).
Thereare many, primarily recall, errors made choos-ing antecedents for all mention types which wewill address by adding syntactic and semantic con-straints.3.1 Adding Syntactic InformationIn this section, we enrich the syntactic represen-tation and information in our system to improveresults.3.1.1 Syntactic SalienceWe first focus on fixing the pronoun antecedentchoices.
A common error arose from the use ofmention head distance as a poor proxy for dis-course salience.
For instance consider the exam-ple in Figure 1, the mention America is closestto its in flat mention distance, but syntacticallyNintendo of America holds a more prominent syn-tactic position relative to the pronoun which, asHobbs (1977) argues, is key to discourse salience.MappingMentions to Parse Nodes: In order touse the syntactic position of mentions to determineanaphoricity, we must associate each mention inthe document with a parse tree node.
We parseall document sentences with the Stanford parser,and then for each evaluation mention, we find thelargest-span NP which has the previously deter-mined mention head as its head.5Often, this re-sults in a different, typically larger, mention spanthan annotated in the data.Now that each mention is situated in a parsetree, we utilize the length of the shortest tree pathbetween mentions as our notion of distance.
In5If there is no NP headed by a given mention head, weadd an NP over just that word.1154S!!!!!!!!
""""""""NP-ORG#1###$$$The IsraelisVP!!!!!!!!!!
!%%%% """""""""""VBPregardNP#2###$$$NPthe sitePP&&''INasNP#2&&''a shrineSBAR(((((()******INbecausePP++,,TOtoNP#1PRPthemS###$$$it is sacredFigure 2: Example of a coreference decision fixedby agreement constraints (see Section 3.1.2).
Thepronoun them is closest to the sitemention, but hasan incompatible number feature with it.
The clos-est (in tree distance, see Section 3.1.1) compatiblemention is The Israelis, which is correctparticular, this fixes examples such as those inFigure 1 where the true antecedent has many em-bedded mentions between itself and the pronoun.This change by itself yields 51.7 pairwise F1 (seeBASE-TREE in Table 2), which is small overall, butreduces pairwise pronoun antecedent selection er-ror from 51.3% to 42.5%.3.1.2 Agreement ConstraintsWe now refine our compatibility filtering to in-corporate simple agreement constraints betweencoreferent mentions.
Since we currently allowproper and nominal mentions to corefer only withmatching head mentions, agreement is only a con-cern for pronouns.
Traditional linguistic theorystipulates that coreferent mentions must agree innumber, person, gender, and entity type (e.g.
an-imacy).
Here, we implement person, number andentity type agreement.6A number feature is assigned to each mentiondeterministically based on the head and its POStag.
For entity type, we use NER labels.
Ideally,we would like to have information about the en-tity type of each referential NP, however this in-formation is not easily obtainable.
Instead, we optto utilize the Stanford NER tagger (Finkel et al,2005) over the sentences in a document and anno-tate each NP with the NER label assigned to thatmention head.
For each mention, when its NP isassigned an NER label we allow it to only be com-patible with that NER label.7For pronouns, wedeterministically assign a set of compatible NERvalues (e.g.
personal pronouns can only be a PER-6Gender agreement, while important for general corefer-ence resolution, did not contribute to the errors in our largelynewswire data sets.7Or allow it to be compatible with all NER labels if theNER tagger doesn?t predict a label.gore president florida statebush governor lebanese territorynation people arafat leaderinc.
company aol companynation country assad presidentTable 1: Most common recall (missed-link) errorsamongst non-pronoun mention heads on our de-velopment set.
Detecting compatibility requiressemantic knowledge which we obtain from a largecorpus (see Section 3.2).S```NP#1NNPWal-MartVPhhhh((((VBZsaysShhhh((((NP#2XXNPNNPGitano,,NP-APPOS#2PPNP#1PRPitsJJtopNNSbrandVPPPis undersellingFigure 4: Example of interaction between the ap-positive and i-within-i constraint.
The i-within-i constraint disallows coreference between parentand child NPs unless the child is an appositive.Hashed numbers indicate ground truth but are notin the actual trees.SON, but its can be an ORGANIZATION or LOCA-TION).
Since the NER tagger typically does notlabel non-proper NP heads, we have no NER com-patibility information for nominals.We incorporate agreement constraints by filter-ing the set of possible antecedents to those whichhave compatible number and NER types with thetarget mention.
This yields 53.4 pairwise F1, andreduces pronoun antecedent errors to 42.5% from34.4%.
An example of the type of error fixed bythese agreement constraints is given by Figure 2.3.1.3 Syntactic Configuration ConstraintsOur system has so far focused only on improvingpronoun anaphora resolution.
However, a pluralityof the errors made by our system are amongst non-pronominal mentions.8We take the approach thatin order to align a non-pronominal mention to anantecedent without an identical head, we requireevidence that the mentions are compatible.Judging compatibility of mentions generally re-quires semantic knowledge, to which we returnlater.
However, some syntactic configurations8There are over twice as many nominal mentions in ourdevelopment data as pronouns.1155NP#1!!!!!!!
"""""""NP####$$$$NN#1painterNNPPabloNNPPicasso,,NP#1%%%%%%&&&&&&subject of the [exhibition]2NP-PERS#1!!!!!!!!
""########NP$$$$%%%%NP-APPOS#1NNpainterNP-PERS&&''NNPPabloNNPPicasso,,NP-APPOS#1(((((())))))subject of the [exhibition]2(a) (b)Figure 3: NP structure annotation: In (a) we have the raw parse from the Klein and Manning (2003)parser with the mentions annotated by entity.
In (b), we demonstrate the annotation we have added.
NERlabels are added to all NP according to the NER label given to the head (see Section 3.1.1).
AppositiveNPs are also annotated.
Hashes indicate forced coreferent nodesguarantee coreference.
The one exploited mostin coreference work (Soon et al, 1999; Ng andCardie, 2002; Luo et al, 2004; Culotta et al, 2007;Poon and Domingos, 2008; Bengston and Roth,2008) is the appositive construction.
Here, we rep-resent apposition as a syntactic feature of an NPindicating that it is coreferent with its parent NP(e.g.
it is an exception to the i-within-i constraintthat parent and child NPs cannot be coreferent).We deterministically mark a node as NP-APPOS(see Figure 3) when it is the third child in of a par-ent NP whose expansion begins with (NP , NP),and there is not a conjunction in the expansion (toavoid marking elements in a list as appositive).Role Appositives: During development, we dis-covered many errors which involved a variant ofappositives which we call ?role appositives?
(seepainter in Figure 3), where an NP modifying thehead NP describes the role of that entity (typi-cally a person entity).
There are several challengesto correctly labeling these role NPs as being ap-positives.
First, the NPs produced by Treebankparsers are flat and do not have the required inter-nal structure (see Figure 3(a)).
While fully solvingthis problem is difficult, we can heuristically fixmany instances of the problem by placing an NParound maximum length sequences of NNP tagsor NN (and JJ) tags within an NP; note that thiswill fail for many constructions such as U.S. Pres-ident Barack Obama, which is analyzed as a flatsequence of proper nouns.
Once this internal NPstructure has been added, whether the NP immedi-ately to the left of the head NP is an appositive de-pends on the entity type.
For instance, Rabbi Ashiis an apposition but Iranian army is not.
Again, afull solution would require its own model, here wemark as appositions any NPs immediately to theleft of a head child NP where the head child NP isidentified as a person by the NER tagger.9We incorporate NP appositive annotation as aconstraint during filtering.
Any mention whichcorresponds to an appositive node has its set ofpossible antecedents limited to its parent.
Alongwith the appositive constraint, we implement thei-within-i constraint that any non-appositive NPcannot be be coreferent with its parent; this con-straint is then propagated to any node its parentis forced to agree with.
The order in which theseconstraints are applied is important, as illustratedby the example in Figure 4: First the list of pos-sible antecedents for the appositive NP is con-strained to only its parent.
Now that all apposi-tives have been constrained, we apply the i-within-i constraint, which prevents its from having the NPheaded by brand in the set of possible antecedents,and by propagation, also removes the NP headedby Gitano.
This leaves the NP Wal-Mart as theclosest compatible mention.Adding these syntactic constraints to our systemyields 55.4 F1, a fairly substantial improvement,but many recall errors remain between mentionswith differing heads.
Resolving such cases willrequire external semantic information, which wewill automatically acquire (see Section 3.2).Predicate Nominatives: Another syntactic con-straint exploited in Poon and Domingos (2008) isthe predicate nominative construction, where theobject of a copular verb (forms of the verb be) isconstrained to corefer with its subject (e.g.
Mi-crosoft is a company in Redmond).
While muchless frequent than appositive configurations (thereare only 17 predicate nominatives in our devel-9Arguably, we could also consider right modifying NPs(e.g., [Microsoft [Company]1]1) to be role appositive, but wedo not do so here.1156Path ExampleNP!!!
"""NP-NNP PRN-NNPNP#####$$%%%%%NP-president CC NP-NNPAmerica Online Inc. (AOL)NPNP-NNP PRN-NNPNP$$NP-president CC NP-NNP[President and C.E.O] Bill GatesFigure 5: Example paths extracted via semantic compatibility mining (see Section 3.2) along with exam-ple instantiations.
In both examples the left child NP is coreferent with the rightmost NP.
Each categoryin the interior of the tree path is annotated with the head word as well as its subcategorization.
Theexamples given here collapse multiple instances of extracted paths.opment set), predicate nominatives are anotherhighly reliable coreference pattern which we willleverage in Section 3.2 to mine semantic knowl-edge.
As with appositives, we annotate objectpredicate-nominative NPs and constrain corefer-ence as before.
This yields a minor improvementto 55.5 F1.3.2 Semantic KnowledgeWhile appositives and related syntactic construc-tions can resolve some cases of non-pronominalreference, most cases require semantic knowledgeabout the various entities as well as the verbs usedin conjunction with those entities to disambiguatereferences (Kehler et al, 2008).However, given a semantically compatible men-tion head pair, say AOL and company, onemight expect to observe a reliable appositiveor predicative-nominative construction involvingthese mentions somewhere in a large corpus.In fact, the Wikipedia page for AOL10has apredicate-nominative construction which supportsthe compatibility of this head pair: AOL LLC (for-merly America Online) is an American global In-ternet services and media company operated byTime Warner.In order to harvest compatible head pairs, weutilize our BLIPP and WIKI data sets (see Sec-tion 2), and for each noun (proper or common) andpronoun, we assign a maximal NP mention nodefor each nominal head as in Section 3.1.1; we thenannotate appositive and predicate-nominative NPsas in Section 3.1.3.
For any NP which is annotatedas an appositive or predicate-nominative, we ex-tract the head pair of that node and its constrainedantecedent.10http://en.wikipedia.org/wiki/AOLThe resulting set of compatible head words,while large, covers a little more than half of theexamples given in Table 1.
The problem is thatthese highly-reliable syntactic configurations aretoo sparse and cannot capture all the entity infor-mation present.
For instance, the first sentence ofWikipedia abstract for Al Gore is:Albert Arnold ?Al?
Gore, Jr. is anAmerican environmental activist whoserved as the 45th Vice President of theUnited States from 1993 to 2001 underPresident Bill Clinton.The required lexical pattern X who served as Y isa general appositive-like pattern that almost surelyindicates coreference.
Rather than opt to manu-ally create a set of these coreference patterns as inHearst (1992), we instead opt to automatically ex-tract these patterns from large corpora as in Snowet al (2004) and Phillips and Riloff (2007).
Wetake a simple bootstrapping technique: given aset of mention pairs extracted from appositivesand predicate-nominative configurations, we ex-tract counts over tree fragments between nodeswhich have occurred in this set of head pairs (seeFigure 5); the tree fragments are formed by an-notating the internal nodes in the tree path withthe head word and POS along with the subcatego-rization.
We limit the paths extracted in this wayin several ways: paths are only allowed to go be-tween adjacent sentences and have a length of atmost 10.
We then filter the set of paths to thosewhich occur more than a hundred times and withat least 10 distinct seed head word pairs.The vast majority of the extracted fragments arevariants of traditional appositives and predicate-nominatives with some of the structure of the NPs1157MUC b3Pairwise CEAFSystem P R F1 P R F1 P R F1 P R F1ACE2004-ROTH-DEVBASIC-FLAT 73.5 66.8 70.0 80.6 68.6 74.1 63.6 39.7 48.9 68.4 68.4 68.4BASIC-TREE 75.8 68.9 72.2 81.9 69.9 75.4 65.6 42.7 51.7 69.8 69.8 69.8+SYN-COMPAT 77.8 68.5 72.9 84.1 69.7 76.2 71.0 43.1 53.4 69.8 69.8 69.8+SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8+SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5ACE2004-CULOTTA-TESTBASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9+SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2+SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6+SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3Supervised ResultsCulotta et al (2007) - - - 86.7 73.2 79.3 - - - - - -Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - -MUC6-TEST+SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0Unsupervised ResultsPoon and Domingos (2008) 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - -Supervised ResultsFinkel and Manning (2008) 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - -ACE2004-NWIRE+SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5Unsupervised ResultsPoon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - -Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, thelargest result is bolded.
The CEAF measure has equal values for precision, recall, and F1.specified.
However there are some tree fragmentswhich correspond to the novel coreference pat-terns (see Figure 5) of parenthetical alias as wellas conjunctions of roles in NPs.We apply our extracted tree fragments to ourBLIPP and WIKI data sets and extract a set of com-patible word pairs which match these fragments;these words pairs will be used to relax the seman-tic compatibility filter (see the start of the section);mentions are compatible with prior mentions withthe same head or with a semantically compatiblehead word.
This yields 58.5 pairwise F1 (see SEM-COMPAT in Table 2) as well as similar improve-ments across other metrics.By and large the word pairs extracted in thisway are correct (in particular we now have cov-erage for over two-thirds of the head pair recallerrors from Table 1.)
There are however word-pairs which introduce errors.
In particular city-state constructions (e.g.
Los Angeles, California)appears to be an appositive and incorrectly allowsour system to have angeles as an antecedent forcalifornia.
Another common error is that the %symbol is made compatible with a wide variety ofcommon nouns in the financial domain.4 Experimental ResultsWe present formal experimental results here(see Table 2).
We first evaluate our modelon the ACE2004-CULOTTA-TEST dataset used inthe state-of-the-art systems from Culotta et al(2007) and Bengston and Roth (2008).
Both ofthese systems were supervised systems discrimi-natively trained to maximize b3and used featuresfrom many different structured resources includ-ing WordNet, as well as domain-specific features(Culotta et al, 2007).
Our best b3result of 79.0is broadly in the range of these results.
We shouldnote that in our work we use neither the gold men-tion types (we do not model pre-nominals sepa-rately) nor do we use the gold NER tags whichBengston and Roth (2008) does.
Across metrics,the syntactic constraints and semantic compatibil-ity components contribute most to the overall finalresult.On the MUC6-TEST dataset, our system outper-1158PROPERNOMINALPRONOUNNULLTOTALPROPER 21/451 8/20 - 72/288 101/759NOMINAL 16/150 99/432 - 158/351 323/933PRONOUN 29/149 60/128 15/97 1/2 105/376Table 3: Errors for each type of antecedent deci-sion made by the system.
Each row is a mentiontype and the column the predicted mention typeantecedent.
The majority of errors are made in theNOMINAL category.forms both Poon and Domingos (2008) (an un-supervised Markov Logic Network system whichuses explicit constraints) and Finkel and Manning(2008) (a supervised system which uses ILP in-ference to reconcile the predictions of a pairwiseclassifier) on all comparable measures.11Simi-larly, on the ACE2004-NWIRE dataset, we also out-perform the state-of-the-art unsupervised systemof Poon and Domingos (2008).Overall, we conclude that our system outper-forms state-of-the-art unsupervised systems12andis in the range of the state-of-the art systems of Cu-lotta et al (2007) and Bengston and Roth (2008).5 Error AnalysisThere are several general trends to the errors madeby our system.
Table 3 shows the number ofpairwise errors made on MUC6-TEST dataset bymention type; note these errors are not equallyweighted in the final evaluations because of thetransitive closure taken at the end.
The most er-rors are made on nominal mentions with pronounscoming in a distant second.
In particular, we mostfrequently say a nominal is NULL when it has anantecedent; this is typically due to not having thenecessary semantic knowledge to link a nominalto a prior expression.In order to get a more thorough view of thecause of pairwise errors, we examined 20 randomerrors made in aligning each mention type to anantecedent.
We categorized the errors as follows:?
SEM.
COMPAT: Missing information aboutthe compatibility of two words e.g.
pay andwage.
For pronouns, this is used to mean that11Klenner and Ailloud (2007) took essentially the same ap-proach but did so on non-comparable data.12Poon and Domingos (2008) outperformed Haghighi andKlein (2007).
Unfortunately, we cannot compare against Ng(2008) since we do not have access to the version of the ACEdata used in their evaluation.we incorrectly aligned a pronoun to a men-tion with which it is not semantically com-patible (e.g.
he aligned to board).?
SYN.
COMPAT: Error in assigning linguisticfeatures of nouns for compatibility with pro-nouns (e.g.
disallowing they to refer to team).?
HEAD: Errors involving the assumption thatmentions with the same head are always com-patible.
Includes modifier and specificity er-rors such as allowing Lebanon and SouthernLebanon to corefer.
This also includes errorsof definiteness in nominals (e.g.
the peoplein the room and Chinese people).
Typically,these errors involve a combination of missingsyntactic and semantic information.?
INTERNAL NP: Errors involving lack of inter-nal NP structure to mark role appositives (seeSection 3.1.3).?
PRAG.
/ DISC.
: Errors where discourse salienceor pragmatics are needed to disambiguatemention antecedents.?
PROCESS ERROR: Errors which involved a tok-enization, parse, or NER error.The result of this error analysis is given in Ta-ble 4; note that a single error may be attributed tomore than one cause.
Despite our efforts in Sec-tion 3 to add syntactic and semantic informationto our system, the largest source of error is stilla combination of missing semantic information orannotated syntactic structure rather than the lackof discourse or salience modeling.Our error analysis suggests that in order to im-prove the state-of-the-art in coreference resolu-tion, future research should consider richer syntac-tic and semantic information than typically used incurrent systems.6 ConclusionOur approach is not intended as an argumentagainst the more complex, discourse-focused ap-proaches that typify recent work.
Instead, we notethat rich syntactic and semantic processing vastlyreduces the need to rely on discourse effects or ev-idence reconciliation for reference resolution.
In-deed, we suspect that further improving the syn-tactic and semantic modules in our system mayproduce greater error reductions than any other1159Mention Type SEM.
COMPAT SYN.
COMPAT HEAD INTENAL NP PRAG / DISC.
PROCESS ERROR OTHER CommentNOMINAL 7 - 5 6 2 2 1 2 general appos.
patternsPRONOUN 6 3 - 6 3 3 3 2 cataphoraPROPER 6 - 3 4 4 4 1Table 4: Error analysis on ACE2004-CULOTTA-TEST data by mention type.
The dominant errors are ineither semantic or syntactic compatibility of mentions rather than discourse phenomena.
See Section 5.route forward.
Of course, a system which is richin all axes will find some advantage over any sim-plified approach.Nonetheless, our coreference system, despitebeing relatively simple and having no tunable pa-rameters or complexity beyond the non-referencecomplexity of its component modules, managesto outperform state-of-the-art unsupervised coref-erence resolution and be broadly comparable tostate-of-the-art supervised systems.ReferencesB.
Amit and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC7.Eric Bengston and Dan Roth.
2008.
Understanding thevalue of features for corefernce resolution.
In Em-pirical Methods in Natural Language Processing.E.
Charniak.
2000.
Maximum entropy inspired parser.In North American Chapter of the Association ofComputational Linguistics (NAACL).Mike Collins.
1999.
Head-driven statistical models fornatural language parsing.A Culotta, M Wick, R Hall, and A McCallum.
2007.First-order probabilistic models for coreference res-olution.
In NAACL-HLT.Pascal Denis and Jason Baldridge.
2007.
Global,Joint Determination of Anaphoricity and Corefer-ence Resolution using Integer Programming.
InHLT-NAACL.Jenny Finkel and Christopher Manning.
2008.
Enforc-ing transitivity in coreference resolution.
In Associ-ation of Computational Linguists (ACL).Jenny Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sam-pling.
In ACL.Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-stein.
1995.
Centering: A framework for modellingthe local coherence of discourse.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics.
As-sociation for Computational Linguistics.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Conference onNatural Language Learning (COLING).J.
R. Hobbs.
1977.
Resolving pronoun references.Lingua.Andrew Kehler, Laura Kertz, Hannah Rohde, and Jef-frey Elman.
2008.
Coherence and coreference re-visited.D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Association of Computational Lin-guists (ACL).Manfred Klenner and Etienne Ailloud.
2007.
Op-timization in coreference resolution is not needed:A nearly-optimal algorithm with intensional con-straints.
In Recent Advances in Natural LanguageProcessing.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
AMention-Synchronous Coreference Resolution Al-gorithm Based on the Bell Tree.
In Association ofComputational Linguists.X Luo.
2005.
On coreference resolution performancemetrics.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing.Vincent Ng and Claire Cardie.
2002.
Improving Ma-chine Learning Approaches to Coreference Resolu-tion.
In Association of Computational Linguists.Vincent Ng.
2008.
Unsupervised models of corefer-ence resolution.
In EMNLP.W.
Phillips and E. Riloff.
2007.
Exploiting role-identifying nouns and expressions for informationextraction.
In Recent Advances in Natural LanguageProcessing (RANLP).Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing.R.
Snow, D. Jurafsky, and A. Ng.
2004.
Learning syn-tactic patterns for automatic hypernym discovery.
InNeural Information Processing Systems (NIPS).W.H.
Soon, H. T. Ng, and D. C. Y. Lim.
1999.
Amachine learning approach to coreference resolutionof noun phrases.1160Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In MUC-6.1161
