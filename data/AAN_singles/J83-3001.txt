Recovery Strategies forParsing Extragrammatical Language 1Jaime G. Carbonell and Philip J. HayesComputer  Sc ience  Depar tmentCarneg ie -Me l lon  Un ivers i tyP i t t sburgh ,  PA  15213Practical natural language interfaces must exhibit robust behaviour in the presence ofextragrammatical user input.
This paper classifies different types of grammatical deviationsand related phenomena at the lexical, sentential and dialogue levels and presents recoverystrategies tailored to specific phenomena in the classification.
Such strategies constitute atool chest of computationally tractable methods for coping with extragrammatieality inrestricted domain natural language.
Some of the strategies have been tested and provenviable in existing parsers.1.
IntroductionAny robust natural language interface must be capableof processing input utterances that deviate from itsgrammatical and semantic expectations.
Many re-searchers have made this observation and have takeninitial steps towards coverage of certain classes ofextragrammatical constructions.
Since robust parsersmust deal primarily with input that does meet theirexpectations, the various efforts at coping with extra-grammaticality have been generally structured as ex-tensions to existing parsing methods.
Probably themost popular approach has been to extendsyntactical ly-oriented parsing techniques employingAugmented Transition Networks (ATNs) (Kwasny andSondheimer 1981, Weischedel and Sondheimer 1984,Weischedel and Black 1980, Woods et al 1976).
Oth-er researchers have attempted to deal with ungrammat-ical input through network-based semantic grammartechniques (Hendrix 1977), through extensions topattern matching parsing in which partial patternmatching is allowed (Hayes and Mouradian 1981),through conceptual case frame instantiation (Dejong1979, Schank, Lebowitz,  and Birnbaum 1980), andthrough approaches involving multiple cooperatingparsing strategies (Carbonell and Hayes 1984, Carbo-nell et al 1983, Hayes and Carbonell 1981).1 This research was sponsored in part by the Air Force Officeof Scientific Research under Contract AFOSR-82-0219 and in partby Digital Equipment Corporation as part of the XCALIBURproject.Given the background of existing work, this paperfocuses on three major objectives:1. to create a taxonomy of possible grammatical devi-ations covering a broad range of extragrammaticali-ties, including some lexical and discourse phenom-ena (for example, novel words and dialogue levelellipsis) that can be handled by the same mecha-nisms that detect and process true grammaticalerrors;2. to outline strategies for processing many of thesedeviations - some of these strategies have beenpresented in our earlier work, some are similar tostrategies proposed by other researchers, and somehave never been analyzed before;3. to assess how easily these strategies can be em-ployed in conjunction with several of the existingapproaches to parsing ungrammatical input, and toexamine why mismatches arise.The overall result should be a synthesis of differentparse-recovery strategies organized by the grammaticalphenomena they address (or violate), an evaluation ofhow well the strategies integrate with existing ap-proaches to parsing extragrammatical input, and a setof characteristics desirable in any parsing process deal-ing with extragrammatieal input.
We hope this will aidresearchers designing robust natural language interfac-es in two ways:1. by providing a tool chest of computationally ef-fective approaches to cope with extragrammatical-ity;Copyright 1984 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial dvantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/83/030123-24503.00American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 123Ja ime G. Carbonell  and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language2.
by assisting in the selection of a basic parsingmethodology in which to embed these recoverytechniques.In assessing the degree of compatibi l ity betweenrecovery techniques and various approaches to parsing,we avoid the issue of whether a given recovery tech-nique can be used with a specific approach to parsing.The answer to such a question is almost always affirm-ative.
Instead, we are concerned with how naturallythe recovery strategies fit with the various parsingapproaches.
In particular, we consider the computa-tional tractability of the recovery strategies and howeasily they can obtain the information they need tooperate in the context of different parsing approaches.The need for robust parsing is greatest for interac-tive natural language interfaces that have to cope withlanguage produced spontaneously by their users.
Suchinterfaces typically operate in the context of a well-defined, but restricted, domain in which strong seman-tic constraints are available.
In contrast, text process-ing often operates in domains that are semanticallymuch more open-ended.
However,  the need to dealwith extragrammaticality is much less pronounced intext processing, since texts are normally carefully pre-pared and edited, eliminating most grammatical errorsand suppressing many dialogue phenomena that pro-duce fragmentary utterances.
Consequently, we shallemphasize recovery techniques that exploit and dependon strong semantic constraints.
In some cases, it isunclear whether the techniques we discuss will scaleup properly to unrestricted text or discourse, but evenwhere they may not, we anticipate that their use in therestricted situation will provide insights into the moregeneral problem.Before proceeding with our discussion, the termextragrammaticality requires clarification.
Extra-grammaticalities include patently ungrammatical  con-structions, which may nevertheless be semanticallycomprehensible, as well as lexical difficulties (for ex-ample, misspellings), violations of semantic con-straints, utterances that may be grammatically accept-able but are beyond the syntactic coverage of the sys-tem, ellipsed fragments and other dialogue phenomena,and any other difficulties that may arise in parsingindividual utterances.
An extragrammaticality s thusdefined with respect to the capabilities of a particularsystem, rather than with respect to an absolute exter-nal competence model of the ideal speaker.Extragrammatical i ty may arise at various levels:lexical, sentential, and dialogue.
The following sec-tions examine each of these levels in turn, classifyingthe extragrammaticalities that can occur, and discuss-ing recovery strategies.
At the end of each section, weconsider how well the various recovery strategieswould fit with or be supported by various approachesto parsing.
A final section discusses ome experimen-tal robust parsers that we have implemented.
Ourexperience with these parsers forms the basis for manyof the observations we offer throughout the paper.We also discuss some more recent work on integratingmany of the recovery strategies considered earlier intoa single robust multi-strategy parser for restricted do-main natural language interpretation.2.
Lexical Level ExtragrammaticalitiesOne of the most frequent parsing problems is findingan unrecognizable word in the input stream.
The fol-lowing subsections discuss the underlying reasons forthe presence of unrecognizable words and developapplicable recovery strategies.2.1.
The unknown word problemThe word is a legitimate lexeme but is not in thesystem's dictionary.
There are three reasons for this:?
The word is outside the intended coverage of theinterface (For example, there is no reason why anatural language interface to an electronic mailsystem should know words like "chair"  or "sky" ,which cannot be defined in terms of concepts in itssemantic domain).?
The word refers to a legitimate domain concept orcombination of domain concepts, but was not in-cluded in the dictionary.
(For example, a word like" forward"  \[a message\] can be defined as a com-mand verb, its action can be clearly specified, andthe objects upon which it operates - an old mes-sage and a new recipient - are already well-formeddomain concepts.)?
The word is a proper name or a unique identifier,such as a catalogue part name/number ,  not hereto-fore encountered by the system, but recognizableby a combination of contextual expectations andmorphological or orthographic features (for exam-ple, capitalization).In the first situation, there is no meaningful re-covery strategy other than focused interaction (Hayes1981) to inform the user of the precise difficulty.
Inthe third, little action is required beyond recognizingthe proper name and recording it appropriately forfuture reference.
The second situation is more compli-cated; three basic recovery strategies are possible:1.
Follow the KLAUS (Haas and Hendrix 1983) ap-proach, where the system temporari ly wrests initia-tive from the user and plays a well designed"twenty questions" game, classifying the unknownterm syntactically, and relating it semantical ly toexisting concepts encoded in an inheritance hier-archy.
This method has proven successful for verbs,nouns and adjectives, but only when they turn outto be instances of predefined general classes ofobjects and actions in the domain model.2.
Apply the project and integrate method (Carbonell1979) to infer the meaning and syntactic categoryof the word from context.
This method has proven124 American Journal of Computat ional  Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983Jaime G. Carbonel| and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageuseful for nouns and adjectives whose meaning canbe viewed as a recombination of features presentelsewhere in the input.
Unlike the KLAUS method,it operates in the background, placing no majorrun-time burden on the user.
However, it remainshighly experimental and may not prove practicalwithout user confirmation.3.
Interact with the user in a focused manner to pro-vide a paraphrase of the segment of input contain-ing the unknown word.
If this paraphrase results inthe desired action, it is stored and becomes themeaning of the new word in the immediate contextin which it appeared.
The L IFER system (Hendrix1977) had a rudimentary capacity for defining syn-onymous phrases.
A more general method wouldgeneralize synonyms to classify the new word orphrase in different semantic ontexts.2.2.2 M isspe l l ingsMisspellings arise when an otherwise recognizablelexeme has letters omitted, substituted, transposed, orspuriously inserted.
Misspellings are the most commonform of extragrammatical ity encountered by naturallanguage interfaces.
Usually, a word is misspelt intoan unrecognizable character string.
But, occasionally aword is misspelt into another word in the dictionarythat violates semantic or syntactic expectations.
Forinstance:"Copy the flies from the accounts directory tomy directory"Although "fl ies" may be a legitimate word in the do-main of a particular interface (for example, the filescould consist of statistics on med-f ly infestation inCalifornia), it is obvious to the human reader thatthere is a misspelling in the sentence above.There are well-known algorithms for matching amisspelt word against a set of possible corrections(Durham, Lamb, and Saxe 1983), and the simplestrecovery strategy is to match unknown words againstthe set of all words in an interface's dictionary.
How-ever, this obviously produces incorrect results when aword is misspelt into a word already in the dictionary,and can produce unnecessary ambiguities in other cas-es.Superior results are obtained by making the spellingcorrection sensitive to the parser's syntactic and se-mantic expectations.
In the following example:Add two fixed haed dual prot disks to the order"haed" can be corrected to: "had",  "head",  "hand","heed",  and "hated".
Syntactic expectations rule twoof these out, and domain semantics rule out two oth-ers, leaving "fixed head disk" as the appropriate cor-rection.
Computationally, there are two ways to or-ganize this.
One can either match parser expectationsagainst all possible corrections in the parser's currentvocabulary, and rule out spurious corrections, or onecan use the parse expectations to generate a set ofpossible words that can be recognized at the presentpoint and use this as input to the spelling correctionalgorithm.
The latter, when it can be done, is clearlythe preferable choice on efficiency criteria.
Generatingall possible corrections with a 10,000 word dictionary,only to rule out all but one or two, is acomputational ly- intensive process, whereas exploitingfully-indexed parser expectations is far more con-strained and less likely to generate ambiguity.
For theexample above, "prot"  has 16 possible corrections in asmall on-line dictionary.
However,  domain semanticsallow only one word in the same position as "prot" ,  socorrection is most effective if the list of possible wordsis generated first.2.3.
In teract ion  of  morpho logy  and misspe l l ingTroublesome side-effects of spelling correction canarise with parsers that have an initial morphologicalanalysis phase to reduce words to their root form.
Forinstance, a parser might just store the root form of'directory' and reduce 'directories' to 'directory' plus aplural marker as part of its initial morphological phase.This process is normally driven by first failing to rec-ognize the inflected form as a word that is present inthe dictionary, and then applying standard morphologi-cal rules (for example, - ies =)  +y) to derive a rootfrom the inflected form.
If any root thus derived is inthe dictionary, the input word is assumed to be theappropriate inflected form.There are several ways in which this procedure caninteract with spelling correction:1.
The same test, viz.
not finding the word in thedictionary, is used to trigger both morphologicalanalysis and spelling correction, so there is a ques-tion of which to do first.2.
The root of the word may be misspelt (e.g.
dircto-ties), even though the inflexion is correct, so thatafter the inflexion is removed, there is still nomatching dictionary entry.3.
The inflexion itself may be misspelt (e.g.
director-ise), so that the standard morphological transfor-mations do not apply.The first kind of interaction is not usually a majorproblem.
On the assumption that inflexion is morecommon than misspelling, the most straightforwardand probably best strategy is to try inflexion first onunknown words and then if that does not produce aword in the dictionary, try spelling correction.
Match-ing only against contextually appropriate words shouldavoid cases in which a misspelling produces an inflect-ed form of a different word.If the root of an inflected word is misspelt, it willbe necessary to spelling correct all of the (possiblyseveral) uninflected forms, which might be inefficient.Again, contextual sensitivity can help.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 125Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageThe third kind of interaction is most troublesome.Most inflexions are too short for spelling correction tobe effective - letter substitution or omission on twoletter sequences is hard to identify.
Moreover, inflex-ion processing does not normally use an explicit list ofinflexions, but instead is organized as a discriminationnet, containing the inflexions implicitly.
One solutionmay be to have a list of all misspellings of inflectedforms, but even utilizing hash coding schemes, search-ing this set would be inefficient.A simpler solution to the entire problem of interac-tion between spelling correction and morphologicalanalysis is to eliminate the morphological analysis, andjust store all inflected forms in the dictionary.
Thishas the disadvantages of being unaesthetic and beingunable to deal with novel inflexions, but neither ofthese are major problems for restricted domain naturallanguage interfaces.
There is also a second orderproblem in that more than one inflected form of thesame word could be found as candidate correctionsthrough spelling correction, but this can be overcomeby explicitly grouping the various inflexions of a givenroot together in the lexicon.2.4.
Incorrect segmentationInput typed to a natural language interface is segment-ed into words by spaces and punctuation marks.
Bothkinds of segmenting markers, especially the second,can be omitted or inserted speciously.
Incorrect seg-mentation at the lexical level results in two or morewords being run together, as in " runtogether" ,  or asingle word being split up into two or more segments,as in "tog ether" or (inconveniently) "to get her", orcombinations of these effects as in "runto geth er".In all these cases, it is possible to deal with sucherrors by extending the spelling correction mechanismto be able to recognize target words as initial segmentsof unknown words, and vice-versa.
For instance, byspelling correcting "portdisks" against what is accepta-ble in the position it occupies in:Add two dual portdisks to the orderit should be possible to recognize the initial segment"por t "  as the intended word, with "disks" as a leftover segment o be inserted into the input string afterthe corrected word for further parsing, resulting in thiscase in the correct parse.
Again, in:Add two dual port disks to the orderan unrecognized (and uncorrectable) word "er "  fol-lowing a word "ord"  which has been recognized as aninitial segment abbreviation should trigger an attemptto attach the unknown word to the end of the abbrevi-ation to see if it completes it.
Correction ofAdd two du alport disks to the orderwould be somewhat harder.
After failing in the aboverecovery methods, one letter at a time would be strip-ped off the beginning of the second unrecognizableword ("alport")  and added at the end of the first un-recognizable word ("du") .
This process succeeds onlyif at some step both words are recognizable and enablethe parse to continue.
Migrating the delimiter (thespace) backwards as well as forwards should also beattempted between a pair of unknown words, stoppingif both words become recognizable.
Of course, thecompounding of multiple lexical deviations (for exam-ple, misspellings, run-on words and split words in thesame segment) requires combinatorial ly inefficientrecovery strategies.
Strong parser expectations amelio-rate this problem partially, but trade-offs must bemade between resilience and efficiency for compounderror recovery.2.5.
Support for recovery strategies by variousparsing approachesIn general, lexical level recovery strategies operate in asufficiently localized manner that the variations inglobal behaviour of different approaches to parsing donot come into play.
However,  most of the strategiesare capable of using contextual restrictions on whatincorrect lexical item might be, and therefore are mosteffective when the constraints on the unknown wordare strongest.
This suggests that they will be mostsuccessful when used with an approach to parsing inwhich it is easy to bring semantic constraints to bear.So, for instance, such techniques are likely to be moreeffective using a semantic grammar (Hendrix 1977,Brown and Burton 1975) or case frame instantiation(Dejong 1979, Hayes and Carbonell  1981) approach,than in an approach using a syntactic ATN (Woods,Kaplan and Nash-Webber 1972), where the expecta-tions are never more specific than membership in oneor more general syntactic ategories.3.
Sentential Level ExtragrammaticalitiesRecovery from extragrammatical i ty at the sententiallevel is much more dependent on the particular kind ofparsing techniques that are employed.
Some tech-niques lend themselves to straightforward recoverymethods, while others make recovery difficult.
Aninitial examination of the requirements for recoveryfrom various kinds of sentential level ungrammaticalitywill allow us to draw some general conclusions aboutthe most suitable basic parsing approaches to build on.We examine ungrammatical it ies in five categories:missing words, spurious words or phrases, out of orderconstituents, agreement violations, and semantic con-straint violations.3.1.
Missing constituentsIt is not uncommon for the user of a natural languageinterface to omit words from his input, either by mis-126 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagetake or in an attempt to be cryptic.
The degree ofrecovery possible from such ungrammaticalities is, ofcourse, dependent on which words were left out.
Forinstance in:Add two fixed head dual ported disks to myorderomitting "dual"  would be unrecoverable since all disksare ported and the discriminating information aboutthe number of ports would not be there.
On the otherhand, if "ported" is omitted, all vital information isstill there (the only thing dual about disks is the num-ber of ports) and it should be possible to recover.Also the omission of function words like prepositionsor determiners i usually (though not always) recover-able.
In practice, most omissions are of words whosecontribution to the sentence is redundant, and aredone consciously in an attempt to be cryptic or"computer- l ike"  (as in "Copy  new files mydirectory").
This suggests that techniques that fill inthe gaps on semantic grounds are more likely to besuccessful than strategies that do not facilitate theapplication of domain semantics.In general, coping with missing words requires aparsing process to determine the parse structure thatwould have been obtained if those words had beenthere.
If the information provided by the missingwords is not redundant (as in the case of "dual"above),  then this structure will have gaps, but thestructure will convey the broad sense of the user'sintention, and the gaps can be filled in by inference or(more practically and safely) by interaction with theuser, focusing on the precise gaps in the context of theglobal parse structure (see Section 4.2 for further dis-cussion of focused interaction techniques.
)A parsing process postulates a missing word errorwhen its expectations (syntactic or semantic) of whatshould go at a certain place in the input utterance areviolated.
To discover that the problem is in fact amissing word, and to find the parse structure corre-sponding to the user's intention, the parsing processmust "step back"  and examine the context of theparse as a whole.
It needs to ignore temporari ly theunfulfilled expectations and their contribution to theoverall structure while it tries to fulfil some of its oth-er expectations through parsing other parts of the in-put and integrating them with already parsed constitu-ents.
In terms of a left-to-right parse of the aboveexample (minus "dual") ,  this would mean that whenthe parser encountered "ported",  it should note thateven though it was expecting the start of a modifiersuitable for a computer component (assuming its ex-pectations are semantic), it had in fact found the latterpart of a modifier for a disk and so could proceed asthough the whole of the modifier was there.
A parserwith greater directional f reedom might find "d isk"first and then look more specifically for qualifiers suit-able for disks.
Again, the existence of a complete diskqualifier in the user's intended utterance could be as-sumed from finding part of the qualifier in a placewhere a whole one should go.Another way of looking at this is as an attempt odelimit the gap in the input utterance, correlate it witha gap in the parse structure (filling in that gap if it isuniquely determined), and realign the parsing mecha-nism as though the gap did not exist.
Such a realign-ment can be done top-down by hypothesizing the oth-er expected constituents from the parse structure al-ready obtained and attempting to find them in theinput stream.
Alternatively, realignment can be donebottom-up by recognizing as yet unparsed elements ofthe input, and either fitting them into an existing parsestructure, or finding a larger structure to subsume boththem and the existing structure.
This latter approachis essential when the structuring words are missing orgarbled.Whether a top-down or a bottom-up method is bestin any given instance will depend on how much struc-ture the parser can recognize before having to dealwith the missing word.
If the parser is left-to-rightand the gap appears early in the input, there is likelyto be little structure already built up, so a bottom-upapproach will probably produce better results.
Similar-ly, if the missing word itself provides the highest levelof structure (for example, "add"  in the exampleabove), a bottom-up approach is essential.
On theother hand, if the missing word corresponds to a spotlow-down in the parse structure, and the gap is late inthe utterance, or the parser is not b~und to a strictleft-to-right directionality, a top-down approach islikely to be much more efficient.
In general, bothmethods should be available.3.2.
Spurious constituentsWords in an input utterance that are spurious to aparse can arise from a variety of sources:legitimate phrases that the parser cannot deal with: Itis not uncommon for the user of a restricted do-main interface to say things that the interface can-not understand because of either conceptual orgrammatical limitations.
Sometimes, spurious ver-bosity or politeness is involved:Add if you would be so kind two fixed headand if possible dual ported disks to my order.Or the user may offer irrelevant (to the system)explanations or justifications, as observed in pre-paratory experiments for the GUS system (Bobrowet al 1977), for example,I think I need more storage capacity, so addtwo fixed head dual ported disks to my or-der.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 127Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageSome common phrases of politeness can be recog-nized explicitly, but in most cases, the only reason-able response is to ignore the unknown phrases,realign the parse on the recognizable input, and if asemantically and syntactically complete structureresults, postulate that the ignored segment was in-deed redundant.
In most such cases, the usershould be informed that part of the input was ig-nored.?
b roken-o f f  and restarted utterances: These occurwhen people start to say one thing, change theirmind, and say another:Add I mean remove a disk from my orderUtterances in this form are more likely to occur inspoken input, but a similar effect can arise in typedinput when a user forgets to hit the erase line orerase character key:Add remove a disk from my orderAdd a single ported dual ported disk frommy orderAgain the best tactic is to discard the broken-offfragment, but identifying and delineating the super-seded fragment requires strategies uch as the onediscussed below.?
unknown words filling a known grammatical role:Sometimes the user will generate an incomprehensi-ble phrase synonymous with a constituent he sys-tem is perfectly capable of understanding:Add a dual ported rotating mass storage de-vice to my orderHere the system might not know that "rotat ingmass storage device" is synonymous with "disk".This phenomenon will result in missing words aswell as spurious words.
If the system has a uniqueexpectation for what should go in the gap, it should(with appropriate confirmation from the user) re-cord the unknown words as synonymous with whatit expected.
If the system has a limited set of ex-pectations for what might go in the gap, it couldask the user which one (if any) he meant and againrecord the synonym for future reference.
In caseswhere there are no strong expectations, the systemwould ask for a paraphrase of the incomprehensiblefragment.
If this proved comprehensible, it wouldthen postulate the synonymy relation, ask the userfor confirmation, and again store the results forfuture reference.The kind of recovery strategies required here aresurprisingly similar to those required for missingwords.
Essentially, the parser must recognize that theinput contains recognizable segments as well as unex-pected and unrecognizable words and phrases inter-spersed among them.
The way that a parser (at least aleft-to-r ight parser) would encounter the problem isidentical to the way that missing words are manifested,viz.
the next word in sequence does not fulfil theparser's expectations.
Overcoming this problem in-volves the same notion of "stepping back" and seeinghow subsequent elements of the input fit with theparsing structure built up so far.
A major difference isthat the word that violated the expectations, and pos-sibly other subsequent words, may not be incorporatedinto the resulting structure.
Moreover,  in the case ofpurely spurious phrases, that structure may not haveany gaps.
For a parser with more directional freedom,the process of finding spurious phrases may be simplerin that it could parse all the words that fit into thestructure before concluding that the unrecognizablewords and phrases were indeed spurious.
When gapsin the parse structure remain after parsing all the rec-ognizable input, the unrecognizable segment may notbe spurious after all.
It can be aligned with the gap inthe parse and the possible synonymy relations dis-cussed above can be presented to the user for approv-al.In the case of broken-of f  utterances, there aresome more specific methods that allow the spuriouspart of the input to be detected:?
If a sequence of two constituents of identical syn-tactic and semantic type is found where only one ispermissible, simply ignore the first constituent.Two main command verbs in sequence (for exam-ple, as in "Add remove ..." above), instantiate theidentical sentential case header role in a case frameparser, enabling the former to be ignored.
Similar-ly, two instantiations of the same prenominal casefor the "disk" case frame would be recognized asmutually incompatible and the former again ig-nored.
Other parsing strategies can be extended torecognize equivalent constituent repetition, but caseframe instantiation seems uniquely well suited to it.?
Recognize explicit corrective phrases and if theconstituent o the right is of equivalent syntacticand semantic type as the constituent o the left,substitute the right constituent for the left constitu-ent and continue the parse.
This strategy recoversfrom utterances uch as "Add I mean remove ...",if " I  mean" is recognized as a corrective phrase.?
Select the minimal constituent for all substitutions.For instance inAdd a high speed tape drive, that's diskdrive, to the orderone desires "disk drive" to substitute for " tapedrive", not for the larger phrase "high speed tapedrive", which also forms a legitimate constituent oflike semantic and syntactic type.
This preference isbased solely on pragmatic grounds and empiricalevidence.In addition to identifying and ignoring spuriousinput, a robust interface must tell the user what it has128 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageignored and should paraphrase the part of the inputthat it did recognize.
The unrecognized input mayexpress vital information, and if that information is notcaptured by the paraphrase, the user may wish to tryagain.
Exceptions to this rule arise when the spuriousinput can be recognized explicitly as such.
Expres-sions of politeness, for instance, might be treated thisway.
The ability to recognize such "noise" phrasesmakes them in some sense part of the expectations ofthe parser, and thus not truly spurious.
However,isolating them in the same way as spurious input pro-vides the advantage that they can then be recognizedat any point in the input without having to clutter theparser's normal processing with expectations aboutwhere they might occur.3.3.
Out  of order  const i tuents  and f ragmentaryinputSometimes, a user will use non-standard word order.There are a variety of reasons why users violate ex-pected constituent ordering relations, including unwill-ingness to change what has already been typed, espe-cially when extensive retyping would be required.Two fixed head dual ported disk drives add tothe orderor a belief that a computer will understand a clippedpseudo-military style more easily than standard usage:two disk drives fixed head dual ported to myorder addSimilar myths about what computers understand bestcan lead to a very fragmented and cryptic style inwhich all function words are eliminated:Add disk drive orderinstead of "add a disk drive to my order".These two phenomena, out of order constituentsand fragmentary input, are grouped together becausethey are similar from the parsing point of view.
Theparser's problem in each case is to put together agroup of recognizable sentence fragments without thenormal syntactic glue of function words or positioncues to indicate how the fragments should be com-bined.
Since this syntactic information is not present,semantic considerations have to shoulder the burdenalone.
Hence, parsers that make it easy for semanticinformation to be brought to bear are at a considera-ble advantage.Both bottom-up and top-down recovery strategiesare possible for detecting and recovering from missingand spurious constituents.
In the bottom-up approach,all the fragments are recognized independently, andpurely semantic constraints are used to assemble theminto a single f ramework meaningful in terms of thedomain of discourse.
When the domain is restrictedenough, the semantic onstraints can be such that theyalways produce a unique result.
This characteristicwas exploited to good effect in the PLANES system(Waltz 1978) in which an input utterance was recog-nized as a sequence of fragments which were thenassembled into a meaningful whole on the basis ofsemantic considerations alone.
A top-down approachto fragment recognition requires that the top-level ororganizing concept in the utterance ( "add"  in theabove examples) be located first and the predictionsobtainable from it about what else might appear in theutterance be used to guide and constrain the recogni-tion of the other fragments.As a final point, note that in the case of out oforder constituents, a parser relying on a strict left-to-right scan will have much greater difficulty than onewith more directional freedom.
In out of order input,there may be no meaningful set of left-to-right expec-tations, even allowing for gaps or extra constituents,that will fit the input.
For instance, a case frameparser that scans for the head of a case frame, andsubsequently attempts to instantiate the individualcases from surrounding input, is far more amenable tothis type of recovery than one dependent upon rigidword order constraints.3.4.
Syntact i c  and semant ic  const ra intv io lat ionsInput to a natural language system can violate bothsyntactic and semantic onstraints.
The most commonform of syntactic constraint violation is agreementfailure between subject and verb or determiner andhead noun:Do the order include a disk drives?Semantic constraint violations can occur because theuser has conceptual problems:Add a floating head tape drive to the orderor because he is imprecise in his language, using arelated object in place of the object he really means.For instance, if he is trying to decide on the amount ofmemory to include in an order he might sayCan you connect a video disk drive to the twomegabytes.when what he really means is "... to the computer withtwo megabytes of memory".These different kinds of constraint violation requirequite different kinds of treatment.
In general, thesyntactic agreement violations can be ignored; cases inwhich agreement or lack of it distinguishes betweentwo otherwise valid readings of an input are rare.However,  one problem that sometimes arises is know-ing whether a noun phrase is singular or plural whenthe determiner or quantifier disagrees with the headnoun.
It is typically best to let quantifiers dominatewhen they are used; for example, " two disk" reallyAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 129Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagemeans "two disks".
And with determiner disagree-ment, it is often unimportant which reading is taken.In the example of disagreement above, it does notmatter whether the user meant "a disk drive" or "anydisk drives".
The answer will be the same in eithercase, viz.
a listing of all the disk drives that the ordercontains.
In cases where the action of the systemwould be different depending on whether the nounphrase was singular or plural (e.g.
"delete a disks fromthe order") ,  the system should interact with the user ina focused way to determine what he really meant.Semantic constraint violations due to a user's con-ceptual problems are harder to deal with.
Once de-tected, the only solution is to inform the user of hismisconception and let him take it from there.
Theactual detection of the problem, however, can causesome difficulty for a parser relying heavily on semanticconstraints to guide its parse.
The constraint violationmight cause it to assume there was some other prob-lem such as out of order or spurious constituents, andlook for (and perhaps even find) some alternative andunintended way of putting all the pieces together.This is one case where syntactic considerations shouldcome to the fore.Semantic constraint violations based on the men-tion of a related object instead of the entity actuallyintended by the user will manifest themselves in thesame way as the semantic constraint violations basedon misconceptions, but their processing needs to bequite different.
The violation can be resolved if thesystem can look at objects related to the one the usermentioned and find one that satisfies the constraints.In the example above, this means going from the mem-ory size to the machine that has that amount of memo-ry.
Clearly, the distance of the relationship overwhich this kind of substitution is allowed needs to becontrolled fairly carefully - in a restricted domaineverything is eventually related to everything else.But there may well be rules that control the kind ofsubstitutions that are allowed.
In the above example,it suffices to allow a part to substitute for a whole(metonymy),  especially if, as we assumed, it had beenused earlier in the dialogue to distinguish betweendifferent instances of the whole.3.5.
Support for recovery strategies by variousparsing approachesWe now turn the question of incorporating the senten-tial level recovery strategies we have been discussinginto the various approaches to parsing mentioned inthe introduction.
As we shall see, there are considera-ble differences in the underlying suitability of the vari-ous approaches as bases for the recovery strategies.To address this issue, we classify parsing approachesinto three general groups: transition network ap-proaches (including syntactic ATNs and network-based semantic grammars),  pattern matching ap-proaches, and approaches based on case frame instan-tiation.3.5.1.
Recovery strategies using a transitionnetwork approachAlthough attempts have been made to incorporatesentential evel recovery strategies into network-basedparsers including both syntactical ly-based ATNs(Kwasny and Sondheimer 1981, Weischedel and Son-dheimer 1984, Weischedel and Black 1980, Woods etal.
1976) and semantic grammar networks (Hendrix1977), the network paradigm itself is not well suitedto the kinds of recovery strategies discussed in thepreceding sections.
These strategies generally requirean interpretive ability to "step back" and take a broadview of the situation when a parser's expectations areviolated, and this is very hard to do when using net-works.
The underlying problem is that a significantamount of state information during the parse is implic-itly encoded by the position in the network; in thecase of ATNs, other aspects of the state are containedin the settings of scattered registers.
As demonstratedby the meta-rule approach to diagnosing parse failuresdescribed by Weischedel and Sondheimer (1983) else-where in this journal issue, these and other difficultieselaborated below do not preclude recovery from extra-grammatical input.
However,  they do make it difficultand often impractical, since much of the procedurallyencoded state must be made declarative and explicit tothe recovery strategies.Often an ATN parse will continue beyond the pointwhere the grammatical deviation, say an omitted word,occurred, and reach a node in the network from whichit can make no further progress (that is, no arcs can betraversed).
At this point, the parser cannot ascertainthe source of the error by examining its internal stateeven if the state is accessible - the parser may havepopped from embedded subnets, or followed a totallyspurious sequence of arcs before realizing it was get-ting in trouble.
If these problems can be overcomeand the source of the error determined precisely, amajor problem remains: in order to recover, and parseinput that does not accord with the grammar, whileremaining true to the network formalism, the parsermust modify the network dynamically and temporari ly,using the modif ied network to proceed through thepresent difficulties.
Needless to say, this is at best avery complex process, one whose computational tract-ability is open to question.
It is perhaps not surprisingthat in one of the most effective recovery mechanismsdeveloped for network-based parsing, the LIFERsystem's ellipsis handling routine (Hendrix 1977), thekey step operates completely outside the network for-malism.As we have seen, semantic constraints are veryimportant in recovering from many types of ungram-matical input, and these are by definition unavailable130 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagein a purely syntactic ATN parser.
However, semanticinformation can be brought to bear on network basedparsing, either through the semantic grammar approachin which joint semantic and syntactic categories areused directly in the ATN, or by allowing the tests onATN arcs to depend on semantic criteria (Bobrow1978, Bobrow and Webber 1980).
In the former tech-nique, the appropriate semantic information for re-covery can be applied only if the correct network nodecan be located - a sometimes difficult task as we haveseen.
In the latter technique, sometimes known ascascaded ATNs (Woods 1980), the syntactic and se-mantic parts of the grammar are kept separate, thusgiving the potential for a higher degree of interpretive-ness in using the semantic information.
However, thenatural way to use this technique is to employ thesemantic information only to confirm or disconfirmparses arrived at on syntactic grounds.
So the rigidityof the network formalism makes it very difficult tobring the available semantic information to bear effec-tively on extragrammatical input.A further disadvantage of the network approach forimplementing flexible recovery strategies is that net-works naturally operate in a top-down left-to-rightmode.
As we have seen, a bottom-up capability isessential for many recovery strategies, and directionalflexibility often enables easier and more efficient oper-ation of the strategies.
Of course, the top-down left-to-right mode of operation is a characteristic of thenetwork interpreter, not of the network formalismitself, and an attempt (Woods et al 1976) has beenmade to operate an ATN in an "island" mode, that is,bottom-up, center-out.
This experiment was done inthe context of a speech parser where the low-levelrecognition of many of the input words was uncertain,though the input as a whole was assumed to be gram-matical.
In that situation, there were clear advantagesto starting with islands of relative lexical certainty, andworking out from there.
Problems, however, ariseduring leftward expansion from an island when it isnecessary to run the network backwards.
The admissi-bility of ATN transitions can depend on tests that ac-cess the values of registers which would have been setearlier when traversing the network forwards, butwhich cannot have been set when traversing back-wards.
This leads at best to an increase in non-determinism, and at worse to blocking the traversalcompletely.3.5.2.
Recovery  s t rateg ies  using a patternmatch ing approachA pattern matching approach to parsing provides abetter f ramework to recover from some sentential-level deviations than a network-based approach.
Inparticular, the definition of what constitutes a patternmatch can be relaxed to allow for missing or spuriousconstituents.
For missing constituents, patterns whichmatch some, but not all, of their components can becounted temporari ly as complete matches, and spuriousconstituents can be ignored so long as they are embed-ded in a pattern whose other components do match.In these cases, the patterns taken as a whole provide abasis on which to perform the kind of "stepping back"discussed above as being vital for flexible recovery.
Inaddition, when pattern elements are defined semanti-cally instead of lexically, as with Wilks's (1975) ma-chine translation system, semantic constraints caneasily be brought to bear on the recognition.
Howev-er, dealing with out of order constituents i not so easyfor a pattern-based approach since constituent order isbuilt into a pattern in a rigid way, similarly to a net-work.
It is possible to accept any permutation of ele-ments of a pattern as a match, but this provides somuch flexibility that many spurious recognitions arelikely to be obtained as well as the correct ones (seeHayes and Mouradian 1981).An underlying problem here is that there is no nat-ural way to make the distinctions about the relativeimportance or difference in role between one wordand another.
For instance, parsing many of the exam-ples we have used might involve use of a pattern like:(<determiner> <disk-drive-attribute>* <disk-drive>)which specifies a pattern of a determiner, followed byzero or more attributes of a disk drive, followed by aphrase synonymous with "disk drive".
So this patternwould recognize phrases like "a dual ported disk" or"the disk drive".
Using the method of dealing withmissing constituents mentioned above, " the"  wouldconstitute just as good a partial match for this patternas "disk drive", a clearly undesirable result.
Theproblem is that there is no way to tell the flexiblematcher which components of the pattern are discrimi-nating from the point of view of recognition and whichare not.
Another manifestation of the same problem isthat different words and constituents may be easier orharder to recognize (for example, preposit ions areeasier to recognize than the noun phrases they intro-duce), and thus may be more or less worthwhile tolook for in an attempt to recover from a grammaticaldeviation.The underlying problem then is the uniformity ofthe grammar epresentation and the method of apply-ing it to the input.
Any uniformly represented gram-mar, whether based on patterns or networks, will havetrouble representing and using the kinds of distinctionsjust outlined, and thus will be less well equipped todeal with many grammatical deviations in an efficientand discriminating manner.
See Hayes and Carbonell(1981) for a fuller discussion of this point.3.5.3.
Recovery  s t rateg ies  in a case f rameparad igmRecursive case frame instantiation appears to provideAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 131Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagea better framework for recovery from missing wordsthan approaches based on either network traversal orpattern matching.
There are several reasons:?
Case frame instantiation is inherently a highly inter-pretive process.
Case frames provide a high-levelset of syntactic and semantic expectations that canbe applied to the input in a variety of ways.
Theyalso provide an overall f ramework that can be usedto realize the notion of "stepping back" to obtain abroad view of a parser's expectations.
As we haveemphasised, this ability to "step back" is importantwhen input deviates from the standard expecta-tions.?
Case frame instantiation is a good vehicle for bring-ing semantic and pragmatic information to bear inorder to help determine the appropriate parse in theabsence of expected syntactic constituents.
If apreposition is omitted (as commonly happens whendealing with cryptic input from hunt-and-peck typ-ists), the resulting sentence is syntactically anoma-lous.
However, semantic case constraints can besufficiently strong to attach each noun phrase tothe correct structure.
Consider, for instance, thefollowing sentence typed to an electronic mail sys-tem natural language interface:"Send message John Smith"The missing determiner presents few problems, butthe missing preposition can be more serious.
Do wemean to send a message "to John Smith", "aboutJohn Smith", "with John Smith", " for  JohnSmith", " f rom John Smith", "in John Smith", "ofJohn Smith", etc.?
The domain semantics of thecase frame rule out the latter three possibilities andothers like them as nonsensical.
However, prag-matic knowledge is required to select "to JohnSmith" as the preferred reading (possibly subject touser confirmation) - the destination case of theverb is required for the command to be effective,whereas the other cases, if present, are optional.This knowledge of the underlying action must bebrought to bear at parse time to disambiguate thecryptic command.
In the XCALIBUR system caseframe encoding (Carbonell ,  Boggs, Mauldin, andAnick 1983), we apply precisely such pragmaticknowledge represented as preference constraints(cf.
Wilks 1975) on case fillers at parse time.Thus, problems created by the absence of expectedcase markers can be overcome by the application ofdomain knowledge.?
The propagation of semantic knowledge through acase frame (via attached procedures uch as thoseof KRL (Bobrow and Winograd 1977) or SRL(Wright and Fox 1983)) can fill in parser defaultsand allow the internal completion of phrases suchas "dual disks" to mean "dual ported disks".
Thisprocess is also responsible for noticing when infor-mation is either missing or ambiguously determined,thereby initiating a focused clarificational dialogue(Hayes 1981).?
The representat ion of case frames is inherentlynon-uniform.
Case fillers, case markers, and caseheaders are all represented separately, and this dis-tinction can be used by the parser interpretivelyinstantiating the case frame.
For instance, if a caseframe accounts for the non-spurious part of aninput containing spurious constituents, a recoverystrategy can skip over the unrecognizable words byscanning for case markers as opposed to case fillerswhich typically are much harder to find and parse.This ability to exploit non-uniformity goes a longway to overcoming the problems with uniform pars-ing methods outlined in the previous section onpattern matching.4.
Dialogue Level ExtragrammaticalityThe underlying causes of many extragrammaticalit iesdetected at the sentential evel are rooted in dialoguephenomena.
For instance, ellipses and other fragmen-tary inputs are patently ungrammatical t the senten-tial level, but can be understood in the context of adialogue.
Viewed at this more global level, ellipsis isnot an "ungrammatical i ty" .
Nevertheless, the samecomputat ional  mechanisms required to recover fromlexical and (especially) sentential problems are neces-sary to detect ellipsis and parse the fragments correct-ly for incorporation into a larger structure.
In thesame way, many dialogue phenomena are classifiedpragmatically as extragrammaticalities.In addition to addressing dialogue level extragram-maticalities, any robust parsing system must engagethe user in dialogue for cooperative resolution of pars-ing problems too difficult for automatic recovery.
In-teraction with the user is also necessary for a coopera-tive parser to confirm any assumptions it makes ininterpreting extragrammatical input and to resolve anyambiguities it cannot overcome on its own.
We havereferred several times in our discussions to the princi-ple of focused interaction, and stated that practicalrecovery dialogues should be focused as tightly aspossible on the specific problem at hand.
Section 4.2discusses some considerations for structuring focusedinteraction dialogues - in particular, why they need tobe so tightly focused, and what mechanisms are need-ed to achieve tight focusing in a natural manner.4.1.
EllipsisEllipsis is a many-faceted phenomenon.
Its manifesta-tions are varied and wide ranging, and recovery strate-gies for many types of ellipsis remain to be discovered.Nevertheless, it is also a very common phenomenonand must be addressed by any interface intended forserious use by real users.
Empirical observations haveshown that users of natural language interfaces employ132 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageellipsis and other abbreviating devices (for example,anaphora, short definite noun phrases, cryptic lan-guage omitting semantically superfluous words, andlexical abbreviations) with alarming frequency(Carbonell 1983).
The results of our empirical obser-vations can be summarized as follows:Terseness principle: Users of natural languageinterfaces insist on being as terse as possible,independent of task, communication media, typ-ing ability, or instructions to the contrary, with-out sacrificing the flexibility of expression inher-ent in natural language communication.Broadly speaking, one can classify ellipsis into in-trasentential and intersentential e lipsis, with the lattercategory being far more prevalent in practical naturallanguage interfaces.
Intrasentential  ellipsis occursmost frequently in coordinate clauses such as:John likes oranges and Mary apples.Often, this type of ellipsis is detectable only on se-mantic grounds (there is no meaningful noun-noununit called "Mary  apples").
The following sentencewith identical syntax has a preferred reading that con-tains no ellipsis:John likes oranges and Macintosh apples.We know of no proven general strategies for interpret-ing this class of intrasentential e lipsis.
An interesting,but untried, approach might be an application of thestrategies described below with each coordinate clausein an intrasentential e lipsis being considered as a sep-arate utterance and with extensions to exploit the syn-tactic and semantic parallelism between correspondingconstituents of coordinate clauses.There are several forms of intersentential e lipsis:?
Elaboration - An ellipsed fragment by either speak-er can be an elaboration of a previous utterance.Either speaker can make the elaboration, but thesecond speaker usually does so, as in the followingexample:User: Give me a large capacity disk.System: With dual ports?User: Yes, and a universal frequency adap-ter.?
Echo - A fragment of the first speaker's utterance isechoed by the second speaker.
As described morefully in Hayes and Reddy (1983), this allows thesecond speaker to confirm his understanding of thefirst speaker's utterance without requiring an ex-plicit confirmation.User: Add a dual disk to the order.System: A dual ported disk.
What storage ca-pacity?If, on the other hand, the system had explicitlyasked "Do you mean a dual ported disk?
", the userwould have been conversationally obliged to reply.However, in either case, the user is free to correctany misapprehension the system displays.
Some-times, as in the example in the next bullet below,an echo may also be an expression of bewilder-ment.
In general, this form of ellipsis is far moreprevalent in spoken interactions than in typed com-munication, but the need for a robust parsing sys-tem to confirm assumptions it is making withoutbeing too disruptive of the flow of conversationmakes it very useful for natural language interfacesin general (see Section 4.2).?
Correction - An ellipsed fragment substitutes for aportion of an earlier utterance that was in error.The correction occurs in three typical modes:?
The first speaker can correct himself immediate-ly (much like the repeated segment problem dis-cussed in Section 3.2).?
The second speaker can offer a correction(marked as such, or simply an ellipsed fragmentin the interrogative).?
Or, the first speaker can correct himself in re-sponse to a clarificational query from the secondspeaker.
The form of the clarificational querycan be a direct question, a statement of confu-sion, or echoing the troublesome fragment ofthe input, thereby combining two forms of ellip-sis as illustrated below.User: Give me a dual port tape drive.System: A dual port tape drive?User: Sorry, a dual port disk drive.?
Reformulation - Part of an old utterance is reformu-lated and meant to be interpreted in place Of thecorresponding old constituent.
This is perhaps themost common form of ellipsis and the only one forwhich tractable computational strategies have beenimplemented.
All the examples below are of thistype.The LIFER/LADDER system (Hendrix 1977, Sacer-doti 1977) handled a restricted form of reformulationellipsis.
LIFER's ellipsis algorithm accepted a frag-mentary input if it matched a partial parse tree derivedfrom the previous complete parse tree by (a) selectinga subtree that accounted for a contiguous segment ofthe previous input, and (b) possibly pruning back oneor more of its branches.
If a f ragmentary inputmatched such a partial parse tree, it was assumed to bea reformulation ellipsis and the missing parts of thepartial parse tree were filled out from the previouscomplete parse tree.
In particular, if a single grammarcategory accounted for the entire fragment, and thiscategory was present in the last query parsed by thesystem, the ellipsis algorithm substituted the fragmentAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 133Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagedirectly for whatever filled the category in the lastparse.
An example of this is:User: What is the length of the Kennedy?System: 200 metersUser: The fastest aircraft carrier?Since both "the Kennedy" and the "the fastest aircraftcarrier" match the semantic category <sh ip>,  the lat-ter phrase is allowed to substitute for the former.
Notethat a purely syntactic parse would not be sufficientlyselective to make the proper substitution.
"The fastestaircraft carrier" is a noun phrase, and there are threenoun phrases in the original sentence: "the length","the length of the Kennedy" and "the Kennedy".However,  the rigid structure of semantic grammarsproves insufficient to handle some common forms ofreformulation ellipsis.
The semantic grammar formal-ism is too restrictive for a simple substitution strategyto apply effectively if there is more than one fragment,if there is a bridging fragment (such as "the smallestwith two ports"  in the example below that bridgesover "d isk") ,  or if the fragment does not preservelinear ordering.
In contrast, case frame substitutionprovides the freedom to handle such ellipsed frag-ments.The following examples are illustrative of the kindof sentence fragments the case frame method handles.We assume that each sentence fragment occurs imme-diately following the initial query below.
Note alsothat we are using case frame here to refer to nominalas well as sentential case frames - the case frame be-ing instantiated in these examples is the one for a diskwith cases such as storage capacity, number of ports,etc..INITIAL QUERY:"What is the price of the three largest singleport fixed media disks?
"SUBSEQUENT QUERIES:"Speed?
""Two smallest?
""How about the price of the two smallest?
""Also the smallest with dual ports?
""Speed with two ports?
""Disk with two ports?
"In these representative examples, punctuation is of nohelp, and pure syntax is of very limited utility.
Forinstance, the last three phrases are syntactically similar(indeed, the last two are indistinguishable), but eachrequires that a different substitution be made on thepreceding query.The DYPAR-II system (discussed in Section 5.2)handles ellipsis at the case frame level.
Here we pre-sent the basic case frame ellipsis resolution method itemploys.
Its coverage appears to be a superset of theLIFER/LADDER system (Hendrix 1977, Sacerdoti1977) and the PLANES ellipsis module (Waltz andGoodman 1977).
Although it handles most of thereformulation ellipsis we encountered, it is not meantto be a general linguistic solution to the ellipsis phe-nomenon.Consider the following example:>What  is the size of the 3 largest single port fixedmedia disks?>disks with two ports?Note that it is impossible to resolve this kind of ellipsisin a general manner if the previous query is storedverbatim or as a semantic grammar parse tree.
"Diskswith two ports"  would at best correspond to some<disk-descriptor> non-terminal, and hence, accordingto the LIFER algorithm, would replace the entirephrase "single port fixed media disks" that corre-sponded to <disk-descriptor> in the parse of the origi-nal query.
However,  an informal poll of potentialusers suggests that the preferred interpretation of theellipsis retains the MEDIA specifier of the originalquery.
The ellipsis resolution process, therefore, re-quires a finer grain substitution method than simplyinserting the highest level non-terminals in the ellipsedinput in place of the matching non-terminals in theparse tree of the previous utterance.Taking advantage of the fact that a case frameanalysis of a sentence or object description capturesthe relevant semantic relations among its constituentsin a canonical manner, a partially instantiated nominalcase frame can be merged with the previous caseframe as follows:?
If a case is instantiated both in the original queryand in the ellipsis, use the filler from the ellipsis.For instance "with two ports"  overrides "singleport"  in our example, as both entail different val-ues of the same case descriptor, regardless of theirdifferent syntactic roles.
("Single por t"  in theoriginal query is an adjectival construction, whereas"with two ports" is a post-nominal modifier in theellipsed fragment.)?
Retain any cases in the original parse that are notexplicitly contradicted by new information in theellipsed fragment.
For instance, "f ixed media" isretained as part of the disk description, as are allthe sentential-level cases in the original query, suchas the quantity specifier and the projection attri-bute of the query ("size").If a case is specified in the ellipsed fragment, butnot in the original query, use the filler from theellipsis.
For instance, the "f ixed head" descriptor isadded as the media case of the disk nominal caseframe in resolving the ellipsed fragment in the fol-lowing example:>Which disks are configurable on a VAX11-7807134 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983Ja ime G. Carbonell  and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language>Any configurable fixed head disks?
4.2.
Focused interaction?
In the event that a new case frame is mentioned inthe ellipsed fragment, wholesale substitution occurs,much as in the semantic grammar approach.
Forinstance, if after the last example one were to ask"How about tape drives?
", the substitution wouldreplace "fixed head disks" with "tape drives", rath-er than replacing only "disks" and producing thephrase "fixed head tape drives", which is semanti-cally anomalous.
In these instances, the semanticrelations captured in a case frame representationand not in a semantic grammar parse tree provecritical.The key advantage case frame instantiation pro-vides for ellipsis resolution is the ability to match cor-responding cases, rather than surface strings, syntacticstructures, or non-canonical representations.
Imple-menting an ellipsis resolution mechanism of equalpower for a semantic grammar approach would, there-fore, be very difficult.
The essential problem is thatsemantic grammars inextricably combine syntax withsemantics in a manner that requires multiple represen-tations for the same semantic entity.
For instance, theordering of marked cases in the input does not reflectany difference in meaning, 2 while the surface orderingof unmarked cases does.
With a semantic grammar,the parse trees produced by different marked caseorderings can differ, so the knowledge that surfacepositioning of unmarked cases is meaningful, but posi-tioning of marked ones is not, must be contained with-in the ellipsis resolution process.
This is a very unnat-ural repository for such basic information.
Moreover,in order to attain the functionality described above forcase frames, an ellipsis resolution based on semanticgrammar parse trees would also have to keep track ofsemantically equivalent adjectival and post nominalforms (corresponding to different non-terminals anddifferent relative positions in the parse trees).
This isnecessary to allow ellipsed structures uch as "a diskwith 1 port"  to replace the "dual -port"  part of thephrase "...dual-port f ixed-media disk ..." in an earlierutterance.
One way to achieve this effect would be tocollect together specific nonterminals that can substi-tute for each other in certain contexts, in essencegrouping non-canonical representations into context-sensitive semantic equivalence classes.
However, thisprocess would require hand-crafting large associativetables or similar data structures, a high price to pay foreach domain-specific semantic grammar.
In brief, theencoding of domain semantics and canonical structurefor multiple surface manifestations makes case frameinstantiation a much better basis for robust ellipsisresolution than semantic grammars.2 leaving aside the differential emphasis and other pragmaticconsiderations reflected in surface orderingIn addition to dealing with ellipsis and other extra-grammatical phenomena that arise naturally for aninteractive interface, a truly robust parsing systemmust initiate subdialogues of its own.
Such dialoguesare needed?
when a robust parser makes assumptions that maynot be justified and needs confirmation from theuser that it has guessed correctly;?
when a parser comes up against ambiguities that itcannot resolve on its own, either because of extra-grammaticality on the part of the user or becauseof some essential ambiguity in perfectly grammati-cal input;?
when the more automated strategies may prove toocostly or uncertain (e.g., when recovering fromcompound lexical errors);?
or when the required information is simply not pres-ent.When an interactive system moves from the passiverole of answering questions or awaiting individual usercommands to a more active information-seeking role inclarificational dialogues, it must address the questionof how to organize its communication so that it willbehave in a way that fits with the conversational ex-pectations and conventions of its human user.
Issuesof when explicit replies are required, how to conveyinformation in such a way as to require the minimalresponse from the user, how to keep the conversationwithin the domain of discourse of the system, etc.,must all be addressed by a natural language interfacecapable of mixed-initiative dialogue.
Examining allthese topics here would take us too far afield from theissue of robust parsing, so we will confine ourselves toissues specific to the kind of recovery interaction de-scribed above.
See Carbonell  (1982) and Hayes andReddy (1983) for a fuller discussion of the issues in-volved in organizing the dialogue of an interactivenatural language system.We offer four guidelines for organizing recoverydialogues:?
the interaction should be as focused as possible;?
the required user response should be as terse aspossible;?
the interaction should be in terms of the system'sdomain of discourse rather than the linguistic con-cepts it uses internally;?
there should be as few such interactions as possible.To see the need for focused interaction, considerthe input:Add two fixed head ported disks to my orderThe problem is that the user has omitted "dual"  be-tween "head"  and "ported".
Assuming that disks canonly be single or dual ported, and using the sententiallevel recovery strategies described earlier, a parserAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983 135Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageshould be able to come up with an interpretation ofthe input that is two ways ambiguous.
Interactionwith the user is required to resolve this ambiguity, butthe degree to which the system's initial question isfocused on the problem can make a big difference inhow easy it is for the user to respond, and how muchwork is required of the system to interpret the userresponse.
An unfocused way of asking the question is:Do yon mean:Add two f ixed head single ported disks to my or-der, orAdd two f ixed head dual ported disks to my orderHere the user is forced to compare two very similarlooking possibilities to ascertain the system's interpre-tation problem.
Comparisons of this kind to isolatepossible interpretation problems place an unnecessarycognitive load on the user.
Furthermore, it is unclearhow the user should reply.
Other than saying "thesecond one",  he has little option but to repeat thewhole input.
Since the system's query is not focusedon the source of the ambiguity, it is conversationallyawkward for the user to give the single word reply,"dual".
This response is highly elliptical, but from thepoint of view of required information, it is complete.It also satisfies our second guideline that the requiredresponse be as terse as possible.A much better way of asking the user to resolve theambiguity is:Do you mean 'single' or 'dual' ported disks?This question focuses precisely on the ambiguity, andtherefore requires no effort from the user besides thatof giving the information the system desires.
Moreo-ver, it invites the highly desirable reply "dual".
Sincethe system is focused on the precise ambiguity, it canalso generate a discourse expectation for this and oth-er appropriate elliptical fragments in the user's re-sponse, and thereby recognize them with little difficul-ty.The ability to generate focused queries to resolveambiguities places certain requirements on how a par-ser represents the ambiguous tructure internally.
Un-less the ambiguity is represented as locally as possible,it will be very hard to generate focused queries.
If aparser finds the ambiguity in the above example bydiscovering it has two independent parse structures atthe end of the parsing process, then generating a fo-cused query involves a computationally taxing intracta-ble comparison process.
However, if the ambiguity isrepresented as locally as possible, for instance as twoalternative fillers for a single instantiation of a diskframe nested within the "add to order" frame, thengenerating the focused query is easy - just output aparaphrase of the case frame (the one for disk) at thelevel immediately above the ambiguity with a disjunc-tion taking the place of the single filler of the ambigu-ous case (the portedness case).
Moreover,  such arepresentation forms an excellent basis for interpretingthe natural elliptical reply.
As Hayes and Carbonell(1981) show, parsers based on case frame instantia-tion are particularly well suited to generating ambigui-ty representations of this kind.Another tactic related to focused interaction thatparsing systems can employ to smooth recovery dia-logues is to couch their questions in terms that make itmore likely that the user's reply will be something theycan understand.
Thus in:Please add two 300 megabyte rotating mass storagedevices to my order.if "rotating mass storage device" is not in the system'svocabulary, it is unwise for it to reply "what is a rotat-ing mass storage device?
",  since the terms the userchooses to clarify his input may be equally unintelligi-ble to the system.
It is much better to give the user achoice between the things that the system could recog-nize in the place where the unrecognizable phraseoccurred.
In this example, this would mean giving theuser a choice between all the computer  componentsthat can admit 300 megabytes as a possible case filler.If this list was unmanageably ong, the system shouldat least confirm explicitly that the unknown phraserefers to a computer component by something like:By 'rotating mass storage device' are you referringto a computer component?This at least establishes whether the user is trying todo something that the system can help him with orwhether the user has misconceptions about the abilitiesof the system.Upon confirmation that the user meant 'disk', thesystem could add the new phrase as a synonym fordisk, perhaps after engaging the user in further clarifi-cational dialogue to ascertain that 'disk' is not merelyone kind of 'rotating mass storage device', or viceversa.
If it were the case that one was more generalthan the other, the new entry could be placed in asemantic hierarchy and used in future recognition(perhaps after determining on what key features thetwo differ).Our third guideline stated that the interactionshould be in terms of the domain of discourse ratherthan the internal linguistic conventions of the system.Breaking this rule might involve requiring the user to,for instance, compare the parse trees representing twoambiguous interpretations of his input or telling himthe name of the internal state where the parse failed inan ATN parser.
Such interaction requires a linguisti-cally and computationally sophisticated user.
Moreo-ver, it is highly non-focused from the user's point ofview since it requires him to translate the parser's viewof the problem into one that has meaning within thetask domain, thereby switching contexts from perform-136 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly -December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageance of the task to linguistic issues.
This enforceddigression places an undue cognitive load on the userand should be avoided.The final guideline is to minimize the amount ofcorrective interactions that occur.
It is very tediousfor a user to be confronted with questions about whathe meant after almost every input, or as Codd (1974)has suggested, to approve a paraphrase of each inputbefore the system does anything.
Clearly, there aresituations when the user must be asked a direct ques-tion, to wit, when information is missing or in thepresence of real ambiguity.
However,  a technique notrequiring a reply is preferable when the system makesassumptions that are very likely to be correct, or whenthere are strong preferences for one alternative amongseveral in ambiguity, anaphora, or ellipsis resolution.The echoing technique mentioned in Section 4.1 isvery useful in keeping required user replies to a mini-mum while still allowing the user to overrule any un-warranted assumptions on the part of the system.
Thetrick is for the system to incorporate any assumptionsit makes into its next output, so the user can see whatit has understood, correct it if it is wrong, and ignoreit if it is correct:User: Add two dual ported rotating mass storagedevices to my orderSystem: What storage capacity should the two dualported disks have?Here the system informs the user of its assumptionabout the meaning of "rotating mass storage device"(possible because only disks have dual ports) withoutasking him directly if he means "disk".This section has given a brief glimpse of some ofthe dialogue issues that arise in a robust parsing sys-tem.
The overriding point here is that robust parsingtechniques do not stop at the single sentence level.Instead, they must be integrated with dialogue tech-niques that allow for active user cooperation as a re-covery strategy of last resort.5.
Exper iments  in Robust  ParsingHaving examined various kinds of extragrammaticalityand the kinds of recovery strategies required to handlethem, we turn finally to a series of experiments wehave conducted or plan to conduct in robust parsing.Before describing some of the parsers involved inthese experiments, we summarize some of the broadlessons that can be drawn from our earlier discussion.These observations have had a major role in guidingthe design of our experimental systems.?
The parsing process should be as interpretive aspossible.
We have seen several times the need fora parsing process to "stand back" and look at abroad picture of the set of expectations (or gram-mar) it is applying to the input when an ungram-maticality arises.
The more interpretive a parser is,the better able it is to do this.
A highly interpre-tive parser is also better able to apply its expecta-tions to the input in more than one way, which maybe crucial if the standard way does not work in theface of an ungrammaticality.?
The parsing process should make it easy to applysemantic information.
As we have seen, semanticinformation is often very important in resolvingungrammaticality.1, The parsing process should be able to take advan-tage of non-uniformity in language like that identi-fied in Section 3.5.2.
As we have seen, recoverycan be much more efficient and reliable if a parseris able to make use of variations in ease of recogni-tion or discriminating power between different con-stituents.
This kind of "opportunism" can be builtinto recovery strategies.I, The parsing process should be capable of operatingtop-down as well as bottom-up.
We have seenexamples where both of these modes are essential.Our earliest experiments in robust parsing wereconducted through the FlexP parsing system (Hayesand Mouradian 1981).
This system was based onpartial pattern matching, and while it had the first andlast of the characteristics listed above, it did not meas-ure up well to the other two.
Indeed, many of ourideas on the importance of those characteristics weredeveloped though observation of FlexP's shortcomingsas described in 3.5.2, and more fully in Hayes andCarbonel l  (1981).
With these lessons in mind, weconstructed two additional experimental parsers:CASPAR to explore the utility of case frame instantia-tion in robust parsing, and DYPAR to explore the no-tion of combining several different parsing strategiesin a single parser.
Both experiments proved fruitful,as the next two sections show, and DYPAR has nowbeen developed into a complete parsing system, theDYPAR-II parser, as part of the XCALIBUR expertsystem interface (Carbonell et al 1983).
After that,we describe an approach to parsing we are currentlydeveloping that we believe to be based on the bestfeatures of both systems.
A final section discussesother methods and approaches that we consider prom-ising avenues for future research.5.1.
The CASPAR parserAs our earlier discussion on sentential-level ungram-maticality pointed out, case frame instantiation ap-pears to have many advantages as a f ramework forrobust parsing.
Our initial experiments in realizingthese advantages were conducted through the CASPARparser (Hayes and Carbonel l  1981).
CASPAR wasrestricted in coverage, but could deal with simple im-perative verb phrases (that is, imperative verbs fol-lowed by a sequence of noun phrases possibly markedby prepositions) in a very robust way.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 137Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageExamples of grammatical input for CASPAR (drawnfrom an interface to a data base keeping track ofcourse registration at a university) include:Cancel math 247Enrol Jim Campbell  in English 324Transfer student 5518 from Economics 101 toBusiness Administration 111Such constructions are classic examples of case con-structions; the verb or command is the central con-cept, and the noun phrases or arguments are its cases.Considered as surface cases, the command argumentsare either marked by prepositions, or unmarked andidentified by position, such as the position of directobject in the examples above.The types of grammatical deviation that CASPARcould deal with include:?
Unexpected and unrecognizable (to the system)interjections as in:?S?Q?
S3 Enrol if you don't  mind student2476 I think in Economics 247.?
missing case markers:Enrol Jim Campbell  Economics 247.?
out of order cases:In Economics 247 Jim Campbell enrol.?
ambiguous cases:Transfer Jim Campbell  Economics 247 English332.Combinat ions of these ungrammaticalit ies could alsobe dealt with.CASPAR used a parsing strategy specifically de-signed to exploit the recognition characteristics ofimperative case frames, viz.
that the prepositions usedto mark cases are much easier to recognize than theircorresponding case fillers.
Below the clause level,CASPAR used linear pattern matching to recognizelower level constituents, which were defined in seman-tic terms appropriate to the restricted omain in whichCASPAR was used.
The algorithm used by CASPARwas as follows:1.
Starting from the left of the input string, applythe linear pattern matcher in scanning mode 4 us-ing all the patterns which correspond to impera-tive verbs (commands).
If this succeeds, the3 The reason for including these particular extraneous charac-ters will be easily guessed by users of certain computers.4 The linear pattern matcher may be operated in anchoredmode, where it tries to match one of a number of linear patternsstarting at a fixed word in the input, or in scanning mode, where ittries to match the patterns it is given at successive points in theinput string until one of the patterns matches, or it reaches the endof the string.command corresponding to the pattern thatmatched becomes the current command, and theremainder of the input string is parsed relative toits domain-specific case frame.
If it fails,CASPAR cannot parse the input.2.
If the current command has an unmarked irectobject case, apply the linear pattern matcher inanchored mode at the next 5 word using the set ofpatterns appropriate to the type of object thatshould fill the case.
If this succeeds, record thefiller thus obtained as the filler for the case.3.
Starting from the next word, apply the patternmatcher in scanning mode using the patterns cor-responding to the surface markers of all the mark-ed cases that have not yet been filled.
If thisfails, terminate.4.
If the last step succeeds, CASPAR selects a mark-ed case - the one from which the successful pat-tern came.
Apply the matcher in anchored modeat the next word using the set of patterns appro-priate to the type of object that should fill thecase selected.
If this succeeds record the fillerthus obtained as the filler for the case.5.
Go to step 3.Unless the input turns out to be completely unparsa-ble, this algorithm will produce a command and a(possibly incomplete) set of arguments.
It is also in-sensitive to spurious input immediately preceding acase marker.
However, it is not able to deal with anyof the other ungrammatical it ies mentioned above.Dealing with them involves going back over any partsof the input that were skipped by the pattern matcherin scanning mode.
If, after the above algorithm hasterminated, there are any such skipped substrings, andthere are also arguments to the command that havenot been filled, the pattern matcher is applied in scan-ning mode to each of the skipped substrings using thepatterns corresponding to the filler types of the un-filled arguments.
This will pick up any argumentswhich were misplaced, or had garbled or missing casemarkers.This algorithm would deal with, for instance, theconvoluted example:To Economics 247 Jim Campbell  transfer pleasefrom Mathematics 121as follows:?
The initial scan for a command verb would find"transfer",  and thus cause all further parsing to bein terms of the case frame for that command.5 The word after the last one the pattern matcher matchedthe last time it was applied.
If some input was skipped in findingthe verb in step l, this is tacked onto the end of the sequence usedby the next operation.138 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983Ja ime G. Carbonell  and Philip J. Hayes Recovery Strategies for Parsing Extrammatica l  Language?
The direct object required by "transfer" would notbe found its expected place, after the verb, soCASPAR would skip to look for a case marker.?
The case marker " f rom" would be found, andCASPAR would subsequently recognize the casemarked by " f rom" and put it in the source courseslot of the transfer case frame.?
The end of the input is then reached, but some cas-es remain unfilled, so CASPAR goes into skippingmode looking for case markers on the missed initialsegment and finds the destination course case.?
Now only the 'Jim Campbell' and 'please' segmentsare left and the student case is left unfilled, soCASPAR can fill the student case correctly, and has'please' left over as spurious input.While limited in its scope of coverage, CASPARprovides a practical demonstration of how well caseframe instantiation fulfills our list of desiderata forrobust parsing.?
CASPAR uses its case frames in a highly interpretivemanner.
It can, for instance, search directly afterthe verb for the filler of a case which is expected tobe the direct object, but if that does not work, it isprepared to recognize the same case elsewhere inthe input.
Also, when it deals with out of orderinput, it "steps back" and takes a broad view byonly considering unparsed input segments as poten-tial fillers of cases that have not yet been filled.?
The case frame representation makes it easy tobring semantic information to bear, e.g.
restrictionson what can fill each case, considerations of whichcases are optional or mandatory, and whether anycases can have fillers that impose pragmatic onst-raints.?
CASPAR also shows the ability of case frame instan-tiation to exploit variations in importance and easeof recognition among different constituents.
Thepower of exploiting such variations is shown bothby the range of grammatical deviations CASPARcan handle, and by the efficiency it displays instraightforward parsing of grammatical input.
Thisefficiency is derived from the limited number ofpatterns that the pattern matcher has to deal withat any one time.
On its first application, thematcher only deals with command patterns; on sub-sequent applications, it alternates between the pat-terns for the markers of the unfilled cases of thecurrent command, and the patterns for a specificobject type.
Also, except in post-processing ofskipped input, only case marker and command pat-terns are employed when the pattern matcher is inits less efficient scanning mode.
The constituentsthat are more difficult to recognize (e.g., objectdescriptions) are processed in the more efficientanchored mode.Only in its predominance of top-down versusbottom-up processing does CASPAR fail to meetour desiderata.
The only bottom-up component toCASPAR is the initial verb recognition phrase.
Ifthe verb were not there, it would be completelyunable to parse.
An extension to CASPAR to ame-liorate this problem would be to start parsing casefillers bottom-up, and hypothesize the existence ofthe verb whose case frame most closely matchedthe set of case fillers found (or ask the user if therewas no clear choice).
This is obviously a much lessefficient mode of operation than the one presentedabove, but it illustrates a way in which the basiccase frame information could be interpreted to dealwith the lack of a recognizable case header.5.2.
The DYPAR parserDYPAR originated as an experimental vehicle to testthe feasibility and potential benefits of combiningmultiple parsing strategies into a uniform framework.Initially, three parsing strategies (pattern matching,semantic grammar interpretation, and syntactic trans-formations) were combined.
Transformations wereused to reduce variant sentential structures to canoni-cal form.
In addition to a large set of operators, 6 thepatterns could contain recursive non-terminal sub-constituents corresponding to semantic grammar cate-gories or other subconstituents.
Each grammar non-terminal could expand to a full pattern containingadditional non-terminals.The experiment proved successful in that DYPARallowed one to write grammars at least an order ofmagnitude more concise than pure semantic grammarsof equivalent coverage.
This version of the system iscalled DYPAR-I (Boggs, Carbonell, and Monarch1983) and has been made available for general use.Subsequently, case frame instantiation was introducedas the new dominant strategy, and the new system,DYPAR-II, is currently used as the experimental parserfor XCALIBUR, a natural language interface to expertsystems (Carbonell et al 1983).The multi-strategy approach to parsing grammaticalinput in DYPAR-II facilitated the introduction of sever-al additional strategies to recover from different kindsof extragrammaticality:?
Spelling correction combined with morphologicaldecomposition of inflected words.?
Bridging garbled or spurious phrases in otherwisecomprehensible input.?
Recognizing constituents when they occur in unex-pected order in the input.?
Generalized case frame ellipsis resolution, exploit-ing strong domain semantics.6 Operators in DYPAR-I include: matching arbitrary subcon-stituent repetition, optional constituents, free permutation matches,register assignment and reference, forbidden constituents, andanchored and scanning modes.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983 139Jaime G. Carbonell  and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageThe two sentential-level recovery strategies (secondand third on the list above) were inspired by, andlargely patterned after, the corresponding strategies inCASPAR, therefore little additional commentary isrequired.
However, an additional complication inDYPAR-II is that the case frame instantiation processrecognizes recursively embedded case frames, and inthe presence of ill-formed input must deal with multi-ple levels of expectations.
Were it not for strong do-main semantics, this additional source of complexitywould have introduced some ambiguity in the correc-tion process requiring additional interaction with theuser.5.2.1.
Spelling correction and morphology inDYPARDYPAR combines expectation-based spelling correctionand morphological decomposition of inflected words.Since the DYPAR grammars are compiled into a cross-referenced form that indexes dictionary entries frompatterns, it proved simple to generate lists of expectedwords when encountering an unrecognizable term.Although often the lists were short (highly constrainedby expectations), on occasion a substantial fraction ofthe dictionary was generated.Since spelling correction interacts with morphologi-cal decomposition, the two were combined into a sin-gle recovery algorithm.
Here we present a somewhatsimplified form of the algorithm in which the onlymorphological operations allowed are on word endings(e.g., singularization and other suffix stripping opera-tions).1.
Define the reduced dictionary to be the set of ex-pected words at the point in the parse where theunrecognized word was found.
This set may con-tain expected or allowed morphological inflexionsand variants, as well as root forms of words.2.
Morphological decomposition phase - If the word(plus any accompanying morphological informa-tion) is a member of the reduced dictionary, returnit and exit.3.
Attempt to perform a one level morphological oper-ation on the current word (e.g., stripping a legalsuffix)a.
If successful, set the word to the decomposedform (e.g.
root and suffix), save the potentialdecomposition on a list, and go to step 2.b.
If no morphological operation is possible, go tothe spelling correction phase (step 4).
Only legalsequences of suffixes are allowed.4.
Spelling correction phase - For each element in thelist of possible decompositions (starting with theoriginal unrecognized word), apply the spellingcorrection algorithm to the root word using thereduced dictionary as the candidate correction set.a.
If successful, return the corrected word (alongwith any morphological information) and exit.b.
If no spelling correction is possible, go on to thenext proposed ecomposition.5.
If no proposed morphological decomposition yieldsa recognizable root, either by direct match or spell-ing correction, exit the algorithm with a failurecondition.Clearly this strategy incorporates a best-match orminimal-correction criterion, rather than generatingthe set of all possible corrections.
Moreover, wordsare only looked up in the reduced dictionary.
Thismeans that misspellings into words that are in the fulldictionary but violate expectations (and are thereforenot members of the reduced dictionary) are handled inthe same manner as ordinary misspellings.Let us trace this correction strategy on the word"intrestingness'.
Since that word is not recognized, weenter the algorithm above and generate a reduced dic-tionary.
Assume that the reduced dictionary containsthe word "interest", but none of its morphologicalvariants.
First we strip the "ness" suffix, but the re-suiting character string remains unrecognizable.
Thenwe strip the "ing" suffix with similar results.
Finallywe strip off the coincidental "est"  as a suffix and stillfind no recognizable root.
At this point, morphologycan do no more and the algorithm enters the spellingcorrection phase with the following candidate((root: (intrestingness) uffixes: 0 )(root: (intresting) suffixes: (ness))(root: (intrest) suffixes: (ing ness))(root: (intr) suffixes: (est ing ness))Next, we attempt to spelling correct "intrestingness"using the reduced dictionary and fail.
We also fail with"intresting", but succeed with " intrest" and exit thealgorithm with the value(root: (interest) suffixes: (ing ness))and without considering the spurious "est"  stripping.Had the word been correctly spelt, or had any of thecompound morphological forms been inserted into thedictionary explicitly, the algorithm would have suc-ceeded and exited sooner.5.2.2.
Ellipsis resolutionDYPAR-II utilizes a variant of the case frame ellipsisresolution method discussed in Section 4.1.
In addi-tion to the general algorithm, it incorporates a methodfor dealing with ellipsis when another component ofthe XCALIBUR system has generated strong discourseexpectations.
The ellipsed fragment is parsed in thecontext of these expectations, as illustrated by therecovery strategy below:Exemplary discourse expectation rule:IF: The system generated a query for confirmationor disconfirmation of a proposed value of a140 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languagefiller of a case in a case frame in focus,THEN: EXPECT one or more of the following:1) A confirmation or disconfirmation patternappropriate to the query in focus.2) A different but semantically permissiblefiller of the case frame in question(optionally naming the attribute or provid-ing the case marker).3) A comparative or evaluative pattern appro-priate to the proposed value of the case infocus.4) A query about possible fillers or constraintson possible fillers of the case in question.\[If this expectation is confirmed, a sub-dialogue is entered, where previously fo-cused entities remain in focus.\]The following dialogue fragment illustrates howthese expectations come into play in a focused dia-logue:>Add a line printer with graphics capabilities.Is 150 lines per minute acceptable?>No,  320 is better Expectations 1, 2 & 3(or) other options for the speed?
Expectation 4(or) Too slow, try 300 or faster Expectations 2 & 3The utterance "try 300 or faster" is syntactically acomplete sentence, but semantically it is just as frag-mentary as the previous utterances.
The strong dis-course expectations suggest hat it be processed in thesame manner as syntactically incomplete utterances,since it satisfies the dialogue expectations listed above.Thus, the terseness principle operates at all levels:syntactic, semantic and pragmatic.Additionally, DYPAR-II contains rules to ensuresemantic completeness of utterances even in the ab-sence of specific discourse expectations.
As we havejust seen, not all sentence fragments are fragmentaryin the syntactic sense.
But not all such purely seman-tic ellipsis can be predicted through dialogue generatedexpectations.>Which fixed media disks are configurable on aVAX780?The RP07-aa, the RP07-ab, ...>Add the largestIn this example, there is no good basis for predictingwhat the user will do in response to the information inthe answer to his question.
His response turns out tobe semantically elliptical - we need to answer thequestion "largest what?"
before proceeding.
One cancall this problem a special case of definite noun phraseresolution, rather than semantic ellipsis, but terminol-ogy is immaterial.
Such phrases occur with regularityin our corpus of examples and must be resolved by afairly general process.
The following rule answers thequestion from context, regardless of the syntactic om-pleteness of the new utterance.Contextual substitution ruleIF: A command or query case frame lacks one ormore required case fillers, and the last caseframe in focus has an instantiated case thatmeets all the semantic tests for the case miss-ing the filler,THEN" I) Copy the filler onto the new case frame,and2) Attempt to copy uninstantiated case fillersas well (if they meet semantic tests).3) Echo the action being performed for im-plicit confirmation by the user.For the example above, the case frame with a missingcomponent is the selection case frame introduced by" largest"  that requires a set of components fromwhich to select.
The previous (and therefore still fo-cused) input has a set of disks in its only case slot andthis meets the semantic criteria for the selection slot;hence it is copied over and used.Rules such as the one above are fairly general incoverage, and the statement of the rule is independentof any specific case grammar or domain semantics.The rules, however, rely on the presence of the samespecific case frames and the semantic constraints asused in the normal parsing of isolated grammaticalconstructions.5.3.
Multi-strategy parsingIn addition to underscoring the importance of our fourdesiderata for robust parsers listed at the beginning ofthis section, our experiments with CASPAR andDYPAR demonstrated that robustness can be achievedby the use of several different parsing strategies on thesame input.
These strategies operate both on gram-matical input and as a means of recovery from un-grammatical input.
The notion of multiple strategiesfits very well with the four desiderata.
In particular:?
The required high degree of interpretiveness can beobtained by having several different strategies ap-ply the same grammatical information to the inputin several different ways.?
Different strategies can be written to take advan-tage of different aspects of non-uniformity for dif-ferent construction types.?
Some strategies can operate top-down and othersbottom up.Nor, as we have seen in DYPAR, is a multiple strat-egy approach inconsistent with our previous emphasison case frame instantiation as a suitable vehicle forrobust parsing.
Indeed, many of the strategies re-quired by a robust parser will be based on case frameinstantiation with all the flexibility that that entails.However,  case frame instantiation cannot carry theAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 141Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageentire burden of robustness alone, and so must besupplemented by other strategies such as the onespresent in DYPAR.
In fact, even the method of caseframe instantiation presented for CASPAR can be seenas two strategies: one an initial pass using standardexpectations, and the other a recovery strategy forwhen the first fails.
The bottom-up strategy discussedat the end of the section on CASPAR would make athird.5.3.1.
Coordinat ing mult iple strategies throughan ent i ty-or iented approachA major problem that arises in using multiple parsingstrategies is coordination between the strategies.Questions of interaction and order of application areinvolved.
In CASPAR and DYPAR, the problem wassolved simply by "hard-wir ing" the interactions, butthis is not satisfactory in general, especially if we wishto extend the set of strategies available in a smoothway.
One alternative we have begun to explore in-volves the idea of entity-oriented parsing (Hayes1984).The central notion behind entity-oriented parsing isthat the primary task of a natural anguage interface isto recognize entities - objects, actions, states, com-mands, etc.
- from the domain of discourse of theinterface.
This recognition may be recursive in thesense that descriptions of entities may contain descrip-tions of subsidiary entities (for example, commandsrefer to objects).In entity-oriented parsing, all the entities that aparticular interface system needs to recognize are de-fined separately.
These definitions contain informa-tion both about the way the entities will be manifestedin the natural anguage input (this information can alsobe used to generate output), and about the internalsemantic structure of the entities.
This arrangementhas the following advantages for parsing robustness:?
The individual entity definitions form an idealframework around which to organize multiple pars-ing strategies.
In particular, each definition canspecify which strategies are applicable to recogniz-ing it.
Of course, this only provides a frameworkfor robust recognition, the robustness achieved willstill depend on the quality of the various recogni-tion strategies used.?
The individual definition of all recognizable domainentities allows them to be recognized independent-ly.
Assuming there is appropriate indexing of enti-ties through lexical items that might appear in asurface description of them, this recognition can bedone bottom-up, thus allowing for recognition ofelliptical, fragmentary, or partially incomprehensi-ble input.
The same definitions can also be used ina more efficient top-down manner when the inputconforms to the system's expectations.?
This style of organization is particularly well suitedto case frame instantiation.
The appropriate caseframes can be associated with each entity definitionfor use by case-oriented strategies.
Of course, thisdoes not prevent other strategies from being usedto recognize the entity, so long as suitable informa-tion for the other strategies to interpret is providedin the entity definition.These arguments can be made more concrete by exam-ple.5.3.2.
Example ent i ty  def in i t ionsFirst we examine some example entity and languagedefinitions suitable for use in entity-oriented parsing.The examples are drawn from the domain of an inter-face to a data base of college courses.
Here is the(partial) definition of a course.
Square brackets de-note attr ibute/value lists, and round brackets ordinarylists.\[EntityName: CollegeCourseType: StructuredComponents: (\[ComponentName: CourseNumberType: IntegerGreaterThan: 99LessThan: 1000\]\[ComponentName: CourseDepartmentType: CollegeDepartment\]\[ComponentName: CourseClassType: CollegeClass\]\[ComponentName: CourselnstructorType: CollegeProfessor\])SurfaceRepresentation: (\[SyntaxType: PatternPattern: ($CourseDepartment $CourseNumber)\]\[SyntaxType: NounPhraseHead: (course I seminar I ...)AdjectivalComponents: (CourseDepartment ...)Adjectives: (\[AdjectivalPhrase: (new I most recent)Component: CourseSemesterValue: CurrentSemester\])PostNominalCases: (\[Case-marker: (?intended for I directed to I ...)Component: CourseClass142 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Jaime G. Carbonel| and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language\[)\]\]\[Case-marker: (?taught by I ...)Component: Courselnstructor\])Precise details of this language are not relevant here.Important features to note include the definition of acourse as a structured object with components: num-ber, department,  instructor, etc..
This definition isseparate from the surface representation of a coursewhich is defined to take one of two forms: a simpleword pattern of the course department followed by thecourse number (dollar signs refer back to the compo-nents), or a full noun phrase with adjectives, post-nominal cases, etc.
Since we are assuming a multi-strategy approach to parsing, the two quite differentkinds of surface language definition do not cause anyproblem - they can both be applied to the input inde-pendently by different construction-specific strategies,and the one which accounts for the input best will beused.Subsidiary objects like Col legeDepartment are de-fined in similar fashion.\[EntityName: CollegeDepartmentType: EnumerationEnumeratedValues: (ComputerScienceDepartmentMathematicsDepartmentHistoryDepartment)SurfaceRepresentation: (\[SyntaxType: PatternPattern: (CS I Computer Science I CompSci I ...)Value: ComputerScienceDepartment\])\]CollegeCourse itself will be a subsidiary entity inother higher-level entities of our restricted domain,such as a command to the data base system to enrol astudent in a course.\[EntityName: EnrolCommandType: StructuredComponents: (\[ComponentName: EnrolleeType: CollegeStudent\]\[ComponentName: EnrolInType: CollegeCourse\])SurfaceRepresentation: (\[SyntaxType: ImperativeCaseFrameHead: (enroll register I include I ...)DirectObject: ($Enrollee)Cases: (\[Case-marker: (in I into I ...)Component: EnrolIn\[)\])\[5.3.3.
Pars ing  w i th  an ent i ty -or iented approachNow we turn to the question of how language defini-tions like those in the examples just given can be usedto drive a parser.
Let us examine first how a simpledata base command likeEnrol Susan Smith in CS 101might be parsed using the above language definitions.The first job is to recognize that we are parsing anEnrolCommand.
In a purely top-down system, wewould establish this by having a list of all the entitiesthat we are prepared to recognize as complete inputsand trying each one of these to see if they could berecognized, a rather inefficient process.
A more natu-ral strategy in an entity-oriented approach is to try toindex bottom-up from words in the input to thoseentities that they might appear in.
In this case, thebest indexer for Enro lCommand is the first word,'enrol'.
In general, the best indexer need not be thefirst word of the input and we need to consider allwords, thus raising the potential of indexing more thanone entity.
Hence we might also index CollegeStu-dent, CollegeCourse, and CollegeDepartment.
A sim-ple method of cutting down the number of index-generated possibilities to investigate top-down is toeliminate all those that are subsidiary to others thathave been indexed.
For our example, this would elim-inate everything except Enro lCommand,  the desiredresult.
One final point about indexing: it is clearlyundesirable to index from every word that could ap-pear in the surface representation of an entity; onlyhighly discriminating words like 'enrol '  or 'CS'  shouldbe used.
Whether a word is sufficiently discriminatingcan be determined either manually, which is unreliable,or automatically by keeping a count of the number ofentities indexed by a given word and removing it fromthe index if it indexes more than a certain thresholdnumber.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 143Ja ime G. Carbonell  and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageOnce EnrolCommand has been established as theentity to recognize in the above example, the remain-der of the recognition can be accomplished straightfor-wardly in a top-down manner.
The definition of thesurface representation of EnrolCommand is an impera-tive case frame with a CollegeStudent as direct objectand with a CollegeCourse as a second case indicatedby 'in'.
This information can be used directly by asimple case frame recognition strategy of the type usedin CASPAR.
No translation into a structurally differ-ent representation is necessary.
The most natural wayto represent the resulting parse would be:\[InstanceOf: EnrolCommandEnrollee: \[InstanceOf: CollegeStudentFirstNames: (Susan)Surname: Smith\]Enrolln: \[InstanceOf: CollegeCourseCourseDepartment: ComputerScienceDepartmentCourseNumber: 101\]\]Note how this parse result is expressed in terms of theunderlying structural representation used in the entitydefinitions without the need for a separate semanticinterpretation step.To see the possibilities for robustness with theentity-oriented approach, consider the input:Place Susan Smith in computer science for fresh-menThere are two problems here: we assume that the userintended 'place' as a synonym for 'enrol' ,  but that ithappens not to be in the system's vocabulary; the userhas also shortened the grammatical ly acceptablephrase, 'the computer science course for freshmen',  toan equivalent phrase not covered by the surface repre-sentation for CollegeCourse as defined above.
Since'place' is not a synonym for 'enrol '  in the language aspresently defined, we cannot index Enro lCommandfrom it and hence cannot get the same kind of top-down recognition as before.
So we are forced to rec-ognize smaller fragments bottom-up.
Let 's  assume wehave a complete listing of students and so can recog-nize 'Susan Smith' as a student.
That leaves 'computerscience for freshmen'.
We can recognize 'computerscience' as a Col legeDepartment and ' f reshmen' as aCollegeClass, so since they are both components ofCollegeCourse, we can attempt o unify our currentlyf ragmentary recognition by trying to recognize acourse description from the segment of the input thatthey span, viz.
'computer science for freshmen'.There are two possible surface representations giv-en for CollegeCourse.
The first, a pattern, is partiallymatched by 'computer science', but does not unify thetwo fragments.
The second, a noun phrase accountsfor both of the fragments (one is adjectival, the otherpart of a post-nominal case), but would not normallymatch them because the head noun is missing.
Infragment recognition mode, however, this kind of gapis acceptable, and the phrase can be accepted as adescription of a Col legeCourse with ComputerScien-ceDepartment  as CourseDepartment,  and Freshman-Class as CourseClass.The input still consists of two fragments, however,a CollegeStudent and a CollegeCourse, and since wedo not have any information about the word 'place',we are forced to consider all the entities that havethose two sub-entities as components.
We will sup-pose there are three: Enro lCommand,  WithdrawCom-mand, and TransferCommand (with the obvious inter-pretations).
Trying to recognize each of these, we canrule out TransferCommand in favour of the first twobecause it requires two courses and we only have one.Also, Enro lCommand is preferred to Wi thdrawCom-mand since the preposition ' in' indicates the Enrol lncase of EnrolCommand,  but does not indicate With-drawFrom, the course-containing case of Withdraw-Command.
Thus we can conclude that the user in-tended an EnrolCommand.In following this bottom-up fragment combinationprocedure, we have ignored other combination possi-bilities that did not lead to the correct answer - forinstance, taking 'Computer  Science' as the StudentDe-partment case of the CollegeStudent, 'Susan Smith'.In practice, an algorithm for bottom-up fragment com-bination would have to consider all such possibilities.However, if, as in this case, the combination did notturn out to fit into a higher-level combinat ion thataccounted for all of the input, it could be discarded infavour of combinations that did lead to a completeparse.
More than one complete parse would be han-dled, just like any other ambiguity, through focusedinteraction.Even assuming that the above example had a uni-que result, since it involved several significant assump-tions, we would need to use focused interaction tech-niques (Hayes 1981) to present a paraphrase of ourinterpretation to the user for approval before acting onit.
Note that if the user does approve it, we should beable (perhaps with further approval) to add 'place' tothe vocabulary as a synonym for 'enrol '  since 'place'was an unrecognized word in the surface positionwhere 'enrol '  should have been.A pilot implementat ion of a parser constructedaccording to the entity-or iented principles outlinedabove has been completed and preliminary evaluationis promising.
We are hoping to build a more completeparser along these lines.6.
Concluding RemarksAny practical natural language interface must be capa-ble of dealing with a wide range of extragrammatical144 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly -December 1983Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Languageinput.
This paper has proposed a taxonomy of theprevalent forms of extragrammaticality in real lan-guage use and presented recovery strategies for manyof them.
We also discussed how well various ap-proaches to parsing could support the recovery strate-gies, and concluded that case frame instantiation pro-vided the best framework among the commonly usedparsing methodologies.At a more general level, we argued that the superi-ority of case frame instantiation over other parsingmethodologies for robust parsing is due to how well itsatisfies four parsing characteristics that are importantfor many of the recovery strategies that we described:?
The parsing process should be as interpretive aspossible.?
The parsing process should make it easy to applysemantic information.?
The parsing process should be able to take advan-tage of non-uniformity in language.?
The parsing process should be capable of operatingtop-down as well as bottom-up.We claimed that while case frame instantiation satis-fies these desiderata better than any other commonlyused parsing methodology,  it was possible to do evenbetter by using a multi-strategy approach in whichcase frame instantiation was just one member (albeit avery important one) of a whole array of parsing andrecovery strategies.
We described some experimentsthat led us to this view and outlined a parsing metho-dology, entity-oriented parsing, that we believe willsupport a multi-strategy approach.It is our hope that by pursuing lines of researchleading to parsers that maximize the characteristicslisted above, we can approach, in semantically limiteddomains, the extraordinary degree of robustness inlanguage recognition exhibited by human beings, andgain some insights into how robustness might beachieved in more general language settings.7.
ReferencesBobrow, D.G.
and Winograd, T. 1977 An Overview of KRL, aKnowledge Representation Language.
Cognitive Science 1 (1):3-46.Bobrow, R.J. 1978 The RUS System.
BBN Report 3878.
BoltBeranek and Newman, Cambridge, Massachuseets.Bobrow, R.J. and Webber, B.
1980 Knowledge Representation forSyntactic/Semantic Processing.
Proc.
National Conference of theAmerican Association for Artificial Intelligence.
Stanford Univer-sity, Stanford, California (August).Bobrow, D.G.
; Kaplan, R.M.
; Kay, M.; Norman D.A.
; Thompson,H.
; and Winograd, T. 1977 GUS: a Frame-Driven DialogueSystem.
Artificial lntelligence 8: 155-173.Boggs, W.M.
and Carbonell, J.G.
and Monarch, I.
1983 TheDYPAR-I Tutorial and Reference Manual.
Technical report.Computer Science Department, Carnegie-Mellon University,Pittsburgh, Pennsylvania.Brown, J.S.
and Burton, R.R.
1975 Multiple Representations ofKnowledge for Tutorial Reasoning.
In Bobrow, D.G.
and Col-lins, A., eds.
Representation and Understanding.
Academic Press,New York, New York: 311-349.Carbonell, J.G.
1979 Towards a Self-Extending Parser.
Proceed-ings of the 17th Meeting of the Association for ComputationalLinguistics: 3-7.Carbonell, J.G.
and Hayes, P.J.
1984 Robust Parsing Using Multi-ple Construction-Specific Strategies.
In Bole, L., ed., NaturalLanguage Parsing Systems.
Springer-Verlag, New York, NewYork.Carbonell, J.G.
; Boggs, W.M.
; Mauldin, M.L.
; and Anick, P.G.1983 The XCALIBUR Project, A Natural Language Interfaceto Expert Systems.
Proceedings of the Eighth International JointConference on Artificial Intelligence.Carbonell, J.G.
; Boggs, W.M.
; Mauldin, M.L.
; and Anick, P.G.1983 XCALIBUR Progress Report #l:  First Steps Towards anIntegrated Natural Language Interface.
Technical report.Computer Science Department, Carnegie-Mellon University,Pittsburgh, Pennsylvania.Carbonell, J.G.
1982 Meta-Language Utterances in PurposiveDiscourse.
Technical report.
Computer Science Department,Carnegie-Mellon University, Pittsburgh, Pennsylvania.Carbonell, J.G.
1983 Discourse Pragmatics in Task-OrientedNatural Language Interfaces.
Proceedings of the 21st AnnualMeeting of the Association for Computational Linguistics.Codd, E.F. 1974 Seven Steps to RENDEZVOUS with the CasualUser In Klimbie, J.W.
and Koffeman, K.L., eds., Proceedings ofthe 1FIP TC-2 Working Conference on Data Base ManagementSystems.
North Holland, Amsterdam: 179-200.Dejong, G. 1979 Skimming Stories in Real-Time.
Ph.D. disserta-tion.
Computer Science Department, Yale University, NewHaven, Connecticut.Durham, i.; Lamb, D.D.
; and Saxe, J.B. 1983 Spelling Correctionin User Interfaces.
Communications of the ACM 26.Haas, N. and Hendrix, G.G.
1983 Learning by Being Told: Ac-quiring Knowledge for Information Management.
In Michalski,R.S.
; Carbonell, J.G.
; and Mitchell, T.M., eds., Machine Learn-ing, An Artificial Intelligence Approach.
Tioga Press, Palo Alto,California.Hayes, P.J.
1981 A Construction Specific Approach to FocusedInteraction in Flexible Parsing.
Proceedings of 19th AnnualMeeting of the Association for Computational Linguistics (June):149-152.Hayes, P.J.
1984 Entity-Oriented Parsing.
COLING84, StanfordUniversity, Stanford, California (July).Hayes, P.J.
and Mouradian, G.V.
1981 Flexible Parsing.
Ameri-can Journal of Computational Linguistics 7(4); 232-241.Hayes, P.J.
and Carbonell, J.G.
1981 Multi-strategy Construction-Specific Parsing for Flexible Data Base Query and Update.Proceedings of the Seventh International Joint Conference on Arti-ficial Intelligence.
Vancouver (August): 432-439.Hayes, P.J.
and Reddy, D.R.
1983 Steps Toward Graceful Interac-tion.
Spoken and Written Man-Machine Communication, Interna-tional Journal of Man-Machine Studies.
19(3): 211-284.Hayes, P.J.
and Carbonell, J.G.
1981 Multi-Strategy Parsing andits Role in Robust Man-Machine Communication.
Technicalreport CMU-CS-81-118.
Computer Science Department,Carnegie-Mellon University, Pittsburgh, Pennsylvania (May).Hendrix, G.G.
1977 Human Engineering for Applied NaturalLanguage Processing.
Proceedings of the Fifth International JointConference on A rtificial Intelligence: 183-191.Kwasny, S.C. and Sondheimer, N.K.
1981 Relaxation Techniquesfor Parsing Grammatically Ill-Formed Input in Natural Lan-guage Understanding Systems.
American Journal of Computa-tional Linguistics 7(2): 99-108.Sacerdoti, E.D.
1977 Language Access to Distributed Data withError Recovery.
Proceedings of the Fifth International JointConference on Artificial Intelligence: 196-202.Schank, R.C.
; Lebowitz, M.; and Birnbaum, L. 1980 An Integrat-ed Understander.
American Journal of Computational Linguistics6(1): 13-30.Waltz, D.L.
1978 An English Language Question AnsweringSystem for a Large Relational Data Base, Communications of theACM 21(7): 526-539.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 145Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical LanguageWaltz, D.L.
and Goodman, A.B.
1977 Writing a Natural LanguageData Base System.
Proceedings of the Fifth International JointConference on Artificial Intelligence: 144-150.Weischedel, R.M.
and Sondheimer, N.K.
1983 Meta-Rules as aBasis for Processing Ill-formed Input.
American Journal ofComputational Linguistics 9(3-4): 161-177.Weischedel, R.M.
and Black, J.
1980 Responding-to PotentiallyUnparseable Sentences.
American Journal of ComputationalLinguistics 6: 97-109.Wilks, Y.
A.
1975 Preference Semantics.
In Keenan, ed., FormalSemantics of Natural Lanbuage.
Cambridge University Press,Cambridge, England.Woods, W. A.
1980 Cascaded ATN Grammars.
American Journalof Computational Linguistics 6: 1-12.Woods, W.A.
; Kaplan, R.M.
; and Nash-Webber, B.
1972 TheLunar Sciences Language System: Final Report.
Technicalreport 2378.
Bolt Beranek and Newman, Cambridge, Massa-chusetts.Woods, W.A.
; Bates, M.; Brown, G.; Bruce, B.; Cook, C.; Klovst-ad, J.; Makhoul, J.; Nash-Webber, B.; Schwartz, R.; Wolf, J.;and Zue, V. 1976 Speech Understanding Systems - FinalTechnical Report.
Technical report 3438.
Bolt Beranek andNewman, Cambridge, Massachusetts.Wright, K. and Fox, M 1983 The SRL Users Manual.
Technicalreport.
Robotics Institute, Carnegie-Mellon University, Pitts-burgh, Pennsylvania.146 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
