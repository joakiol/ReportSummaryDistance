Inheritance and Constraint-BasedGrammar FormalismsR6mi  Zajac*tProject POLYGLOSSWe describe an approach to unification grammars that integrates two paradigms: the object-oriented approach, which offers multiple inheritance, complex objects with role-value restrictionsand role-values equality, querying as subsumption; the relational programming approach, whichoffers declarativity, logical variables, nondeterminism with backtracking, and existential queries.This approach isembodied ina constraint-based object-oriented formalism.
The interpreter of theformalism is described as a term rewriting system based on unification of typed feature structures.The grammar writer organizes unification grammars as inheritance networks of typed featurestructures.
Complex linguistic structures are described by means of recursive type constraints.
Weillustrate the use of inheritance networks with two examples: an HPSG example where implication(as used for "principles") is modeled using inheritance and an example of bilingual transfer wherethe minimal amount of information eeded for the translation is specified at different levels ofgeneralization.1.
Int roduct ionIdeally, a linguistic formalism combining the best of the object-oriented approach andthe unification-based approach would be realized in a constraint-based architecture foran object-oriented language based on inheritance, feature structures, and unification.The Typed Feature Structure language (TFS) is an attempt o provide a synthesisof several key concepts temming from unification-based grammar formalisms (fea-ture structure: Kay 1984) knowledge representation languages (inheritance), and logicprogramming (narrowing).
The formalism supports an object-oriented style based onabstraction and generalization through inheritance; it is a fully declarative formalismbased on unification of typed feature structures.
It is flexible and has enough expres-sive power to support various kinds of linguistic theories, not necessarily based onconstituencyLThe use of an object-oriented methodology for natural language processing isvery attractive, and the use of inheritance offers a number of advantages such as ab-straction and generalization, information sharing and default reasoning, and modular-ity and reusability (Daelemans 1990).
Inheritance-based descriptions are already usedin computational linguistics: linguistic theories such as Systemic Functional Gram-mar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard* IMS-CL/IfI-AIS, University of Stuttgart, Azenbergstrat~e 12, D-W-7000 Stuttgart 1.
E-marl:zajac@informatik.uni-stuttgart.de,t Research reported in this paper is partly supported by the German Ministry of Research andTechnology (BMFT, Bundesminister ffi Forschung und Technologie), under grant No.
08 B3116 3.
Theviews and conclusions contained herein are those of the author and should not be interpreted asrepresenting official policies.1 In particular, it allows a direct implementation f HPSG-style grammars (Emele 1988): HPSG is so farthe only linguistic theory based on both inheritance and feature structures.
(~ 1992 Association for Computational LinguisticsComputational Linguistics Volume 18, Number 2and Sag 1987) make use of inheritance to describe linguistic structures at the lexical,morphological, syntactic, or semantic (conceptual) evels.
These theories are usuallydirectly implemented in object-oriented programming languages (e.g., LOOM in thecase of the PENMAN system \[Mann and Matthiessen 1985\]), but there is a growingnumber of linguistic formalisms used for specific purposes, e.g., DATR (Evans andGazdar 1989) for the lexicon.On the other hand, current linguistic theories uch as LFG, UCG, HPSG, and someformalisms for linguistic description such as FUG or PATR-II are based on the notionof partial information: linguistic structures are described using feature structures thatgive partial information about the object being modeled, a linguistic structure beingdescribed by a set of feature structures that mutually constrain the description.
Featurestructures are partially ordered according to a subsumption ordering interpreted as anordering on the amount of conveyed information; the combination of information isdefined as the unification of feature structures.
Formalisms based on feature structureand unification are declarative, and they can be given a sound formal semantics.Combining object-oriented approaches to linguistic description with unification-based grammar formalisms, as in HPSG, is very attractive.
On one hand, we gain theadvantages of the object-oriented approach: abstraction and generalization throughthe use of inheritance.
On the other hand, we gain a fully declarative framework,with all the advantages of logical formalisms: expressive power, simplicity, and soundformal semantics.
To arrive at such a result, we have to enrich the formalism of featurestructures with the notion of inheritance and abandon some of the procedural featuresof object-oriented languages in order to gain referential transparency.Referential transparency is one of the characteristic properties of declarative lan-guages (Stoy 1977), where the meaning of each language construct is given by a fewsimple and general rules.
For example, the value of a variable should be independentfrom its position within the scope of its declaration.
This is true for PROLOG variablesinside a clause, but not for PASCAL or LIsP variables that make use of assignment.
Ahigher level example is the meaning of a procedure: it is not transparent if the pro-cedure makes use of global variables that are set by some other procedure.
Similarly,the meaning of a PROLOG predicate should be transparent because there is no globalvariable, but a predicate definition might be modified during execution by imperativepredicates uch as assert  and ret ract ,  thus destroying the referential transparencyof pure PROLOG.Clearly, most of the object-oriented languages lack referential transparency in sev-eral ways, using for example procedural attachments for object methods.
Anotherexample is the use of nonmonotonic inheritance, which is advocated in computa-tional linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt andde Graaf 1990; and Fraser and Hudson 1990.
Nonmonotonic inheritance is seen asa practical device designed to deal with exceptions, but such a feature goes againstgenerality and referential transparency.
Furthermore, as expressed by Etherington etal.
1989, a still unresolved issue in nonmonotonic reasoning is the issue of... scaling formal non-monotonic theories up to real problems (merelya formality?).
Most existant heories are intractable some don't haveeven a proof theory--and it is often difficult to tell how large bodiesof information will (or even should) interact.Given the complexity of the state of the art in nonmonotonic reasoning and the lack of160R6mi Zajac Inheritance and Constraint-Based Grammar Formalismsa basic commonly agreed formalization, a the issue of nonmonotonicity is not addressedin the work described in this article.Knowledge representation languages are evolving toward more declarativity, asexemplified by the evolution from KL-ONE (Brachman and Schmolze 1985) to lan-guages such as GLASSIC (Borgida et al 1989) or LOOM (MacGregor 1988, 1990).
Theterminological component describing the objects (the data model of object-orienteddatabase systems) has always been more declarative than the assertional component(procedural attachment or methods), and the current trend is to integrate those twocomponents more closely, where the assertional component is some kind of rule-basedsystem, as in LOOM (Yen, Neches, and MacGregor 1988).Typed feature structures are very similar to structured objects of object-orientedlanguages and to conceptual structures of knowledge representation languages.
Thus,typed feature structures have the potential to act as a lingua franca for both compu-tational inguistics and artificial intelligence, and this should ease the communicationbetween those two worlds.
Since conceptual structures are used for example in textgeneration (Bourbeau et al 1990) or knowledge-based machine translation (Nirenburget al 1992), typed feature structures provide an attractive alternative to current proce-dural implementations.In Section 2, we present a language that combines the notions of partial informationand inheritance in a fully declarative framework.
It is based on feature structuresaugmented with the notion of types, which are organized into an inheritance network.Using types, it is possible to define structured omains of feature structures and toclassify feature structures.
Logical conditions are attached to types, akin to methodattachment, but in a fully declarative framework.
Recursivity is an integral part ofthe language, giving the necessary expressive power for describing complex recursivelinguistic structures.
We end the section by an overview of the TFS abstract rewritemachine used for computing descriptions of the meaning of typed feature structures.
3Section 3 describes the use of inheritance in two examples of unification grammarsusing the TFS formalism: an HPSG grammar for a fragment of English and an LFG-style transfer grammar for a small machine translation problem between English andFrench.2.
Inheritance Networks of Typed Feature StructuresAssume the existence of an (abstract) informational domain U, for example, the setof linguistic objects.
Feature structures describe objects of this universe by specifyingvalues for attributes of objects and equality constraints between some values.
Moreprecisely, as feature structures can provide only partial information about the objectsthey describe, a feature structure denotes a set of objects in this universe.
This setcould be a singleton set, for example, in the case of atomic feature structures.
Featurestructures are ordered by a subsumption relation: a feature structure fl subsumesanother feature structure f2 iff fl provides the same or less information than f2: fl _> f2.
Inour universe, this means that the set described by fl is a superset of the set describedby f2.
Note that there can be feature structures that cannot consistently describe the2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of featurestructures are derived, plays this role in formal accounts of feature structures.3 This formalism isfully implemented.
An interpreter for the TFS rewrite machine has beenimplemented at the University of Stuttgart by Martin Emele and the author and has been used to testseveral linguistic models uch as DCG, LFG, HPSG, and SFG \[Emele and Zajac 1990b; Emele t al.1990; Zajac 1990a; Bateman and Momma 1991\].161Computational Linguistics Volume 18, Number 2same objects.
For example, a feature structure describing verb phrases and a featurestructure describing noun phrases are not consistent: he intersection of the sets theydenote is usually empty.As different sets of attribute-value pairs make sense for different kinds of objects,we also divide our feature structures into different ypes.
These types are ordered bya subtype relation: a type t2 is a subtype of another type tl if t2 provides at least as muchinformation as tl.
For example, assuming that a verb phrase is a phrase, then the setof verb phrases is included in the set of phrases.
Using types to model this taxonomichierarchy, the type symbol VP denotes the set of verb phrases, the symbol PH denotesthe set of phrases, and we define VP as a subtype of I'H.This description implies that, if we know that a linguistic object is a verb phrase,we can deduce that it is a phrase.
This deduction mechanism is expressed in ourtype system as type inheritance.
Furthermore, with each type we associate constraintsexpressed as feature structures, thereby defining an inheritance network of typed fea-ture structures: if a feature structure is of type t and tlhere exist supertypes of t, then tinherits all the attribute-value pairs and equality constraints of the feature structuresassociated with all the supertypes of t.Computation is performed by a typed feature structure machine capable of check-ing a set of type constraints defined as an inheritance network of typed feature struc-tures.
Given a typed feature structure inheritance network, we query the machine byasking if some feature structure satisfies the constraints defined by the network.
Toproduce the answer, the system proceeds by gradually adding the constraints thatshould be satisfied by the query: an answer will be a set of feature structures whereeach feature structure is subsumed by the query and where all the type constraints ofthe network hold on all substructures of the elements of the answer.
The answer is theempty set when the query does not satisfy the constraints defined by the network.Related work.The basic approach described in this section is based on original work by Ait-Kaci(1984, 1986) on the KBL language and has also been influenced by the work on HPSGby Pollard and Sag (1987) and Pollard and Moshier (1990).
Among the growing lit-erature on the semantics of feature structures, many relevant results and techniqueshave been published by Smolka (1988, 1989), Smolka and A'/t-Kaci (1988), and Ait-Kaci and Podelski (1991).
Based on Pollard and Sag (1987), Pollard (1990), and Pollardand Moshier (1990), a computational formalism, very close to the TFS formalism, iscurrently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990).The early work presented in Emele and Zajac (1989a) was an attempt to directlyimplement the ideas presented in Ait-Kaci (1984, 1986); Emele and Zajac (1989b) wasalready a departure from Ait-Kaci's KBL language, implementing a different evalua-tion strategy that allowed the use of cyclic feature structures.
Compared with the KBLlanguage, the current TFS language?
allows the use of cyclic feature structures,?
has a simple operational semantics implementing an inheritance rule anda specialization rule,?
uses a lazy evaluation scheme for the evaluation of constraints,?
implements static coherence checks,?
has a simple syntax distinguishing the definition of the partial order ontype symbols, and the definition of constraints associated to types.162R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms2.1 TypesIn the following presentation, we adopt an algebraic approach based on lattice theory(Birkoff 1984; Ait-Kaci 1984).
Alternative presentations could be developed as well; forexample, a proof-theoretical approach using an adaptation of a feature logic (Roundsand Kasper 1986; Smolka 1988).
It is possible to prove formally the equivalence ofthese different models, as this is done for the LIFE language in Ait-Kaci and Podelski(1991).
The presentation is nevertheless rather informal, and a more technical accountcan be found in Emele and Zajac (1990a) and Zajac (1990b).The universe of feature structures i  structured in an inheritance network thatdefines a partial ordering on kinds of available information.
The backbone of thenetwork is defined by a finite set of type symbols T together with a partial ordering< on T: the partially ordered set (poset) (T, -</- The ordering _ defines the subtyperelation: for A, B E T we read A < B as "A is a subtype of B."
We call the smallesttypes of T the minimal types.To have a well-behaved type hierarchy, we require that (T, _<) be such that:?
T contains the symbols T and _1_, where T is the greatest element and _1_is the least element of 7-.4any two type symbols A and B of T have a greatest common lowerbound written glb(A, B}.
A poset where greatest common lower boundsexist is a meet semi-lattice: we introduce a new operationA A B = glb(A, B}, where A A B is called the meet of A and B.Since we allow the user to specify any finite poset, a technicality arises whentwo types do not have a unique greatest common lower bound: in that case, the setof greatest common lower bounds is interpreted isjunctively using the followingpowerlattice construction, which preserves the ordering and the existing meets.The poset (T, <) is embedded in (crowns(T), C-H).
The set crowns(T) is the setof all nonempty subsets of incomparable elements of T (the "crowns" of T).
Thesesubsets are partially ordered by the Hoare ordering GH: VX, Y E crowns(T), X GH Yiff Vx E X, 3y C Y such that x < y.The canonical injection of T in crowns(T), which takes any element x of T intothe singleton {x} trivially preserves the ordering: {x} GH {y} iff x ~ y.The meet between two elements X and Y of crowns(T), X ~ Y, is defined as theunion of the intersection ofeach of the principal ideals generated by the elements of Xwith each of the principal ideals generated by the elements of Y and then extracting themaximal elements.
It is easy to see that existing meets are preserved: {z} -- {x} ~ {y}i f f z=xAy.Some meets are added, as in the following example.
Let ({a, b, c, d}, _<) be a posetwhere d < b, d < a, c < b, and c < a (this poset is represented using a Hasse diagramin Figure 1).
The meet a A b does not exist, but {a} R {b} -~ {c, d}, and this meet isinterpreted disjunctively.The join between two elements X and Y of crowns(T), X U Y, is defined as the setof maximal elements of the union of X and Y.
It can be shown that, equipped withR and U, (crowns(T), ~-H/is a distributive lattice.
This construction is carried over totyped feature structures, with the property that this definition of the join does not loseinformation, contrary to the strict generalization f feature structures: this definition of4 T represents underspecified nformation, and ?
represents inconsistent i formation.163Computational Linguistics Volume 18, Number 2b {a, c, dc d {el\[b, c, d}\[d}Figure 1A poset and the set of the principal ideals generated by the elements of the poset ordered byset inclusion.the join is appropriate to represent disjunctive information as, for example, generatedby a nondeterministic computation (see Section 2.4).This powerlattice construction is completely transparent to the user, and to sim-plify the presentation, we will assume in the following that (T, ~) is a meet semi-lattice.2.2 Feature StructuresWe use the attribute-value matrix (AVM) notation for feature structures, and we writethe type symbol for each feature structure in front of the opening square bracket of theAVM.
In the remainder of this section, we shall implicitly refer to some given signature(T, <, .T / where (T, ~, ) is a type hierarchy and ~" is a set of feature symbols, and weshall also assume a set of variables V.A typed feature structure t is then an expression of the form tl\]\[\]A ...: tnwhere \[\] is a variable in a set of variables V, A is a type symbol in T, f l , .
.
.
,  fn (withn _> 0) are features in ~-, and h , .
- .
,  tn are typed feature structures.We have to add some restrictions that capture properties commonly associatedwith feature structures:1.
A feature is a selector that gives access to a substructure: it has to beunique for a given feature structure.2.
3_ represents inconsistent information: it is not allowed in a featurestructure.3.
A variable is used to capture equality constraints ("reentrancy") in thefeature structure, and the shared value is represented only once: there isat most one occurrence of a variable \[\] that is the root of a structuredifferent from \[\]T.Given a signature (T, _<,/T), feature structures are partially ordered by a subsump-tion relation.
This captures the intuitive notion that a feature structure t ~ containing164R6mi Zajac Inheritance and Constraint-Based Grammar Formalismsmore information than a feature structure t is more specific than t. A feature structuret subsumes a structure {, t > t' iff:1. all paths in t are in t~;2. all equality constraints in t hold in t'.3.
for a given path in t, its type isgreater or equal than the correspondingtype in tqSince we have a partial order on feature structures, the meet operation betweentwo feature structures t and t r is defined in the usual way as the greatest commonlower bound of t and tq It is computed using a typed unification algorithm.
A featurestructure is represented as a graph where each node has a type, an equivalence classused to represent equational constraints ("co-references"), and a set of outgoing arcs.The unification algorithm uses the union/f ind procedure on an inverted set represen-tation of the equivalence classes adapted by Ait-Kaci (1984) after Huet (1976).
Theactual algorithm used in the system is optimized using several different echniques tominimize copying and to behave as efficiently as a pattern-matcher in cases when oneof the feature structures subsumes the other (Emele 1991).2.3 Inheritance Network of Feature StructuresThe template mechanism (as, for example, in PATR-II \[Shieber 1986\]) already providesa simple inheritance mechanism used to organize lexical descriptions.
In comparison,networks of typed feature structures are more expressive and provide a more generaland more powerful inheritance mechanism, which allows the use of recursive typedefinitions, whereas recursivity is forbidden in templates ince they are expandedstatically using a macro-expansion mechanism.
Furthermore, typing provides a no-tion of well-formedness that is used to implement a type-discipline and consistencychecks, giving the user the means of checking statically the coherence of a set of typedefinitions.2.3.1 Type Discipline.
As different combinations of attribute value pairs make sensefor different kinds of objects, we divide our feature structures into different classesby associating with a type a certain class of feature structures.
Each type defines aspecific collection of features that are appropriate for it, restrictions on their possiblevalues, and equality constraints between values.
The definition of a type A is def(A),expressed as a feature structure (of type A).
A type symbol that does not have anydefinition is called an atomic type.
A type that has a definition is called a complextype.The association of types and feature structures allows the definition of well-formedness conditions for feature structures using the following two typing rules:TypingTypingRule 1:A feature appearing in a feature structure has always to be declared asappropriate for some type.
The user cannot introduce arbitrary features:one must declare all the features that one will use.
All features that arenot explicitly declared as being appropriate for some type are by defaultdefined as being appropriate for _L.rule 2:A feature structure cannot have a feature that is not appropriate for its165Computational Linguistics Volume 18, Number 2type or for one of the supertypes.
Thus, any feature structure with a fea-ture f has to belong to some type for which f is appropriate.These well-formedness conditions are enforced at compile-time using a type in-ference procedure which infers for each feature structure its possible minimal types.
Ifthe inferred type is _l_, an error is reported indicating that the respective feature struc-ture does not obey the typing rules.
The internal representation built by the compileruses these inferred minimal types to ensure that it is not possible to add an arbitraryfeature to a feature structure during computation, but only those declared for the typeof the structure, thus preserving well-formedness.2.3.2 Inheritance and Generalization.
Since types are organized in an inheritancenetwork, a type inherits all the features, value restrictions, and equality constraintsfrom all its super-types monotonically: the constraints expressed as feature structuresare conjoined using typed unification.
The compiler makes sure that the user hasspecified an inheritance network, building an internal representation where for everytwo types such that A < B we have def(A) < def(B).
5 If there is a type A such thatA # _1_ and def(A) = 3_, the network is inconsistent and an error is reported.
Thecompiler also has a generalization step where all constraints common to all subtypesof a given type are also defined for that type.2.3.3 Interpreting an Inheritance Network.
The constraints expressed as an inher-itance network are interpreted as follows.
For a given typed feature structure t =GIA\[...\], the feature structure t belongs to the domain of A (i.e., it satisfies the con-straints associated with A) if and only if:Inheritance Rule:t satisfies the constraints specified by the definition of A and by the defi-nitions of all the supertypes of A;Specialization Rule:t satisfies the constraints specified by the definitions of at least one ofthe subtypes of A.The inheritance rule states the necessary conditions for a feature structure of type Ato satisfy the constraints associated with A.
The specialization rule states the sufficientconditions and implements a kind of closed-world assumption: a type is exhaustivelycovered by its subtypes.
For example, a feature structure of type LIST can be anempty list (type NIL) or a nonempty list (CONS), but nothing else.
These two rules areimplemented by the TFS interpreter described in Section 2.4.Example.
A simple example of an inheritance network of feature structures i dis-played in Figure 2 using Hasse diagrams.
The subnetwork on the right defines adomain of lists expressed as feature structures: the set of all possible lists is definedby the type LIST, which has no associated constraints.
This type has two subtypes:?
NIL is an atomic type and represents he empty list;?
CONS is a complex type and represents he set of all possible nonemptylists and defines the following constraints, ik feature structure of type5 Inheritance is pre-computed statically: A G B =~ def(A) = def(A) A def(B).166R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms1: LIST 1 LIST, \[,o1 APPEND0\[2: \[\]\]LIST I APPENDli2: ~\]LIST :- APPEND 2: \ [ \ ]t3:  J /3:Figure 2Type hierarchy for LIST and APPEND (T and 3_ omitted).CONS has only two features (first typing rule): first, whose value is thefirst element of the list and can be anything, and rest, whose value isconstrained to be a list.
6 Note that this latter constraint is recursive.The subnetwork on the left defines the domain of the APPEND relation, encodedusing feature structures.
The constraints associated with the supertype APPEND saythat it has three arguments, identified by the features 1, 2, and 3, and that the valuesof all arguments should be in the LIST domain.
The subtypes APPEND0 and APPEND1encode the two cases where the first argument is the empty list (APPEND0), and thenonempty list (APPEND1), in a way similar to the classical PROLOC encoding.
Asshown for the type APPEND1, it is possible to have additional constraints that arenot represented in the feature structure proper: they are introduced by the ' : - '  sign.
7These conditions can be inherited and are conjoined using the logical and operation.For APPEND1, the condition states the recursive constraint on the concatenation f thelists, which is expressed as a feature structure of type APPEND.2.4 The TFS Abstract Rewrite MachineThe meaning (denotation) of a typed feature structure t in a universe U defined byan inheritance network is represented by the largest set of feature structures St ={h, .
.
.
,  tn} such that, for all ti1.
ti <_ t, and2.
for all substructures u = \[XIA\[...\] of ti, type A is a minimal type andu ~ def(A).The first condition says that all the elements of St satisfy the constraints expressedby t. The second condition says that all the elements of ,St satisfy the constraints defined6 Conversely, using the second typing rule, we can deduce that CONS is a possible type for\[first : Mary ,  rest : T\], since the combination of first and rest is defined as appropriate for CONS.7 This construction provides room for future evolution of the formalism by adding new kinds ofconstraints that cannot be directly expressed in the AVM format e.g., negation.
A definition"X : - Y, Z."
is read "X such that Y and Z.
"167Computational Linguistics Volume 18, Number 2LISTLIST \[!
: APPEND : LIST\]: LIST.\]APPEND : LIST\[: LIST\]NILCONSIfirst: TLrest: LIS'\[APPEND0/2: m L: ~Trst:NCO lSLrest: \[\]APPEND1 \[\] LIST: CONS\[ first:N\[rest:INFigure 3Rewrite rules for LIST and APPEND.by the network.
If St is empty, the feature structure t is inconsistent (modulo theconstraints of the inheritance network).
St can be finite, e.g.
in the case of a dictionary,but it can also be infinite in the case of recursive types: for example, the set of featurestructures ubsumed by LIST is the (infinite) set of all possible lists represented asfeature structures,In this section, we describe an abstract rewrite machine for computing the repre-sentation of the denotation of typed feature structures given an inheritance network.The rewrite mechanism is based on a variant of narrowing 9 adapted to feature struc-tures.An inheritance network of feature structures is compiled into a rewriting systemas follows: each direct link between a type A and a subtype B generates a rewrite ruleof the form A\[a\] --* B\[b\] where A\[a\] and B\[b\] are the definitions of A and B, respectively.Figure 3 shows the rewrite rules corresponding to the network of Figure 2.The interpreter is given a "query" (formulated as a typed feature structure) toevaluate.
The first step is to check that the feature structure respects the two typingrules (Section 2.3.1).
The idea is then to try to satisfy all the constraints defined bythe inheritance network by incrementally adding more constraints to the query usingthe rewrite rules (nondeterministically) to get closer to the solution step by step.
Therewriting process tops when conditions 1 and 2 described above hold.A rewrite step for a structure t is defined as follows: if u is a substructure of t atpath p and u is of type A, and there exists a rewrite rule A\[a\] --, B\[b\] such that?
A\[a\] A u  3_, and?
A\[a\] A u < A\[a\]then the right-hand side B\[b\] is unified with the substructure u at path p, giving a newstructure t' that is more specific than t (Figure 4).8 See A'ff-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-pointcharacterizations of the denotation of typed feature structures.9 Narrowing uses unification instead of pattern-matching for checking the applicability of the l.h.s, of arule.
Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution forimplementing PROLOG interpreters).
Pattern-matching is used in the functional programmingparadigm.168R6mi Zajac Inheritance and Constraint-Based Grammar FormalismsFigure 4A rewrite step.The first condition checks that the rule is applicable: the 1.h.s.
has to be consistentwith the substructure.
The implementation factorizes common 1.h.s., avoiding the DNFexpansion: if a 1.h.s.
l of a rule l --* r is not consistent with the substructure, thiscomputation branch is a failure branch, and all rules u ~ v where u < I are discardedin one step without further computation.The second condition implements a lazy rewriting strategy: if A\[aJ A u is equal toA\[a\], all rules A\[a\] --* B\[b\] could be applied with success, and failure could come onlyfrom the rewriting of some other substructures after the exploration of all choices foru.
To avoid the exploration of failure branches as much as possible, the evaluation ofthe substructure u is suspended until the evaluation of some other substructure havingsome part in common with u makes u more specific, narrowing the set of potentialchoices for the subtypes of A for u.
Thus, the search space is explored "intelligently,"postponing the evaluation of branches of computation that would correspond for ex-ample to uninstantiated PROLOC goals (see for example van Hentenryck and Dincbas\[1987\], van Hentenryck \[1989\] on evaluation techniques in constraint logic program-ming).Rewrite steps are applied nondeterministically everywhere in the structure untilno further rule is applicable.
1?The choice of which substructure to rewrite is only partly determined by theavailability of information (using the lazy rewriting rule).
When there are severalsubstructures that could be rewritten, the computation rule is to choose one of theoutermost ones, i.e., one closest o the root of the feature structure (innermost strategiesare usually nonterminating).
This outermost rewriting strategy is similar to hyper-resolution in logic programming.
In comparison, PROLOG uses a leftmost computationrule.For a given substructure, the choice of which rule to apply is done nondetermin-istically, and the search space is explored epth-first using a backtracking scheme.
Al-though this strategy is not complete (a complete breadth-first earch strategy could beused for debugging purposes), the use of the outermost rule has favorable termination10 Conditions do not change this general scheme (they are evaluated using the same rewritingmechanism) and are omitted from the presentation here for the sake of simplicity.
See for exampleDershowitz and Plaisted (1988) and Klop (1990) for a survey on rewriting.169Computational Linguistics Volume 18, Number 2properties when compared to PROLOG'S leftmost rule: there are problems where a TFScomputation will terminate when the corresponding logic program implemented inPROLOG will not; for example, for left-recursive rules in naive PROLOG implementa-tions of DCGs.3.
Inheritance and Constraint-Based Grammars3.1 Related ApproachesIn the constraint-based framework, a grammar is regarded as a set of constraints to besatisfied by a given linguistic object: parsing and generation differ only in the natureof the "input," and use the same constraint evaluation mechanism.
The properties ofa computational framework for implementing constraint-based grammars are:?
A unique general constraint solving mechanism is used: grammarsdefine constraints on the set of acceptable linguistic structures.?
As a consequence, there is no formal distinction between "input" and"output."
For example, the same kind of data structure could be used toencode both the string and the structural description, and, as for theHPSG sign (Pollard and Sag 1987), they could be embedded into a singledata structure that represents the relation between the string and theassociated linguistic structure.?
Specific mapping properties, based on constituency, linear precedence, orfunctional composition, are not part of the formalism itself, but can beencoded explicitly in the formalism.An approach that uses a unique deductive mechanism for parsing and generationis described in Dymetman and Isabelle (1988).
Within this approach, a lazy evaluationmechanism based on the specification of input/output arguments i implemented (inPROLOG), and the evaluation is completely data-driven: the same program parses orgenerates, depending only on the form of the input structure.A constraint-based grammar does not need a context-free mechanism to buildup constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault(1990) describe a class of reversible grammars ("Lexical Grammars") based on a fewcomposition rules that are very reminiscent of categorial grammars.
Other kinds ofapproaches have been proposed, e.g., using a dependency structure and linear prece-dence relations (Reape 1990; see also Pollard and Sag \[1987\]).
In Saint-Dizier (1991),linear precedence rules are defined as constraints in a language based on typed featurestructures and SLD-resolution, which is used to experiment with GB theory.In the following sections, we describe two examples of constraint-based grammars:an HPSG grammar for a fragment of English, and an LFG-style transfer grammar fora small machine translation problem between English and French.3.2 Head-Driven Phrase Structure GrammarIn general, a grammar describes the relation between strings of words and linguisticstructures.
To implement a constraint-based grammar in TFS, we have to encode bothkinds of structures using the same data structure provided by the TFS language: typedfeature structures.
A linguistic structure will be encoded using features and values.Conditions that constrain the set of valid linguistic structures have to be declared ex-plicitly.
A string of words will be encoded as a list of word forms, using the same kind170R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms\[sYH I Loc f SuBcAT ~ \]\[DTRS headed..~trueture\[ \] \]:~'\[DTRs\[HEAD-DTR I SYN \] LOC l SUBCAT append(ID,l~)\] J \[ \[COMP-DTRS ~\]PHRASE < SUBCAT-FP.t dtrs: \[comp-dtm: ~\[~ \[3: ~JFigure 5The HSPG subcategorization principle and its TFS encoding.of definitions as in Figure 2.11 HPSG is so far the only formal linguistic theory basedon the notion of typed feature structures (Pollard 1990), and is thus a good candidateto illustrate the possibilities of the TFS formalism.
The following presentation is basedon Emele (1988) and Emele and Zajac (1990b).The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguisticstructure, the "sign," with four levels of description: phonology, constituent structure,syntax, and semantics.
In HPSG, there is no distinction between "input" and "output:"the relation between a string and a linguistic structure is encoded as a single featurestructure representing the "sign.
"HPSG "principles" are encoded using inheritance: a feature structure of typePHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, andSEM-FP.
For example, the HPSG subcategorization principle is encoded in TFS us-ing inheritance to model implication, and the TFS APPEND relation to encode thefunctional constraint on concatenation (Figure 5).
In a similar way, HEAD-FP encodesthe HPSG Head Feature Principle and SEM-FP encodes the Semantics Principle.The type SIGN is divided into several subtypes corresponding to different map-pings between a string and a linguistic structure.
We first have the basic distinctionbetween phrases and words.
The definition of a phrase recursively relates ubphrasesand substrings, and defines the phrase as a composition of subphrases and the stringas the concatenation f substrings.
Since the formalism itself does not impose any con-straints on how the relations between phrases and strings are defined, the grammarwriter has to define them explicitly.
In HPSG (Pollard and Sag 1987), the orderingof phrases is defined using linear precedence r lations: the order in which the sub-strings associated with subphrases are concatenated to give the string associated witha phrase are guided by these linear precedence r lations (Reape 1990).In the example given below (Figure 6), we make simplifying assumptions: theLOCAL feature is not used and there are two possible orderings for complements.
Thetype IDP1 encodes Grammar Rule 1 (Pollard and Sag 1987, pp.
149-155), which saysthat a "saturated phrasal sign," i.e., a feature structure with \[syn : \[subcat : (>\]\], is thecombination of an unsaturated phrasal head with one phrasal complement on the left.For example, for structures like S ~ NP VP, S is the "saturated phrasal sign," NP is11 We will use a more condensed notation for lists with angle brackets provided by the TFS language: alistC0NS\[first: Mary, rest: CONS \[first : sings, rest: NIL\]\] is writtenas <Mary sings>.171Computational Linguistics Volume 18, Number 2SUBCAT-FP HEAD-FP SEM-:FP   Fphon: B \]/syn: \[subcat: (SIGN)\] \] ,  J\[phon: \[sl~Dubcat: 0 \] \] IDP2/.
\[hea(l-dtz: WOl~D\[phon: m \]LLcomp-dt~s: (SXGN\[phon: El\]) JJ:- APPEND :L3: ~J.
'~ APPEND :Figure 6Part of the HPSG PHRASE hierarchy: PHRASE inherits from "principles" (given here withouttheir definitions) and is subdivided into two subtypes corresponding todifferent complementorderings.the left phrasal complement and VP is the unsaturated phrasal head.
Furthermore, thestring (the value of the phon feature) of the IDP1 phrase is the concatenation f thestring of the complement with the string of the head.The type IDP2 encodes Grammar Rule 2 and states that an "unsaturated phrasalsign," i.e., a feature structure with \[syn:\[subcat:lSIGNI\] \], is the combination of alexical head with any number of complements on the r~ht (e.g., for VP --* V XP*): thestring associated with IDP2 is the concatenation f the string of the head with theconcatenation f the strings of the complements, where the relation ORDER-COMPdefines in which order the complements strings are concatenated.The difference between the parsing and the generation problem is then only in theform of the structure given to the interpreter for evaluation.
A query for the parsingproblem is an underspecified structure where only the string is given; conversely,a query for the generation problem is an underspecified structure where only thesemantic form is given (Figure 7).In both cases, the interpreter uses the same set of rewrite rules to fill in "missinginformation" according to the type definitions.
The result in both cases is exactly thesame: a fully specified structure containing the string, the full semantic form, and alsoall other syntactic information such as the constituent s ructure (Figure 8).3.3 Bi-directional Transfer in Machine TranslationWe have sketched above a very general framework for specifying mappings betweena linguistic structure, encoded as a feature structure, and a string, also encoded as afeature structure.
We apply a similar technique for specifying transfer rules for machinetranslation, which we prefer to call "contrastive rules" since there is no directionalityinvolved (Zajac 1989; 1990a).172R6mi Zajac Inheritance and Constraint-Based Grammar FormalismsQuery for parsingPHILASE\[phon: (Elm ate every cookie)\]Query for generationPaa.asz : |~gl:~,-d:\[r,~tr:\[n~e: KXM\]\]\] _\]/ g2:|spec: EVERY \[ Li.a: \[r--tr:\[ra": COOKm\]\]\] \]Figure 7Queries for parsing and generation.IDP1phon: (Kim ate every cookie)L subcat: 0 J__\[comp-dtrs: (...)dtrs: TR~:~\[head.dtr: IDP2..,'zeln: EAT\[m h?r 111 rein: NAMING m lr,.,r" ?
/ / /sem: 2ARG-REL \[ kname: KIM J J JFigure 8The common solution to the parsing and generation problems.The idea is rather simple: assume we are working with linguistic structures simi-lar to LFG's functional structures for English and French as proposed in Kaplan et al(1989).
We define a translation relation as a type TAU-LEX with two features, eng forthe English structure and fr for the French structure.
This "bilingual sign" is definedon the lexical structure: each subtype of TAU-LEX defines a lexical correspondencebetween a partial English structure and a partial French structure for a given lexicalequivalence.
Such a lexical contrastive definition for a verb also has to pair the argu-ments recursively.
This is expressed in the condition part of the definition (Figure 9)by a recursive condition TAU-LEX on the arguments.
The translation of syntactic andsemantic features, like tense or determination, is also specified in the condition part,and these contrastive definitions can and should be defined separately from the lexicaldefinitions as different subnetworks.Inheritance is used here to capture generalizations over relations: all the informa-tion in the example of Figure 9 can be unpacked and redefined as follows.
We have173Computational Linguistics Volume 18, Number 2TAU-LEX-FALL ~ TAU-LEX-ITV./eng:/subj:l \[tense:TAU-LEX-FALL \] ipred: TOMBEI 1 .I~: \[subj: ~L L tense:Figure 9A transfer ule.TAU / \TAU-LEX... TAU-TENSE...U L /Feng:rtense l 'tea*el U eng~reng subj ~ eng~reng pred FALL\]Figure 10Part of an inheritance network of transfer elations for verbs.three levels of generalization for the translation of verbs: TAU-LEX-V is subdivided intoseveral relations for translating, for example, transitive verbs (TAU-LEX-TV), intransi-tive verbs (TAU-LEX-ITV), etc.
The condition that specifies the translation of tenses isdefined for the whole class of verbs TAU-LEX-V.
The condition that specifies the trans-lation relation between subjects is defined for intransitive verbs (TAU-LEX-ITV), butcannot be specified for all verbs since it does not apply to, e.g., impersonal verbs.
Thisleaves only the translation of predicates to be defined at the leaves of the hierarchy(Figure 10).
Thus, at each level of generalization, we specify the minimal amount ofinformation eeded for translation.
The same kind of organization can be used fornouns, where syntactic features uch as determination are associated with the highertypes, and where the minimal types define the equivalences between lexical forms ofnouns themselves.174R6mi Zajac Inheritance and Constraint-Based Grammar FormalismsQueryFA '.
11/ rPre(l: STUDENT \] / /TAU eng: isubj: / B,ea: A 1 / / // Ln"": s GJJ \[ /Ltense: E-PRES J JAnswerTAU-LEX-FALLPrecI: FALL \]\] rPre(i: STUDENT 11o-s:/,.bj: !
\[p,~: A \ ] / !/ L*~:: L,,,,m: ~_s~JJ /Ltense: F-PRES Jrp,ed: TOMBER \]\[ rp,ed: ETUDIANT \] \[~: \[subj: \[ \[pred:UN "1// i L='>~:: L"u": F'S~GJ J /Ltense: F-PRES JFigure 11Query and answer for the translation of "A student falls.
"The transfer problem for each direction is then stated in the same way as forparsing or generation: the input structure is an underspecified "bilingual sign" whereonly the structure for the source language is given.
Using the contrastive grammar,the interpreter fills in missing information and builds a bilingual sign 12 (Figure 11).It is not necessary to specify in the contrastive definitions all monolingual con-straints that have to be satisfied by the English structure and by the French structure.We can assume that we have monolingual grammars that define the appropriate map-pings between the set of English sentences and the set of associated English structures,and similarily for French.
Using these monolingual constraints in addition to the con-trastive grammar, the TFS interpreter would build the fully specified monolingualstructures, implementing a constraint-based translation system.3.4 Termination ProblemsFor parsing and generation, since no constraints are imposed on the kind of mappingbetween the string and the semantic form, termination has to be proved for each classof grammar and for the particular mechanism used for either parsing or generationwith this grammar.
If we restrict ourselves to classes of grammars for which termi-nating evaluation algorithms are known, we can implement those directly in TFS.However, the TFS evaluation strategy allows more naive implementations of gram-mars, and the outermost rewriting of "sub-goals" terminates on a strictly larger classof programs than for corresponding logic programs implemented in PROLO~.
Further-more, the grammar writer does not need, and actually should not, be aware of thecontrol that follows the shape of the input rather than a fixed strategy, thanks to thelazy evaluation mechanism.12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock(1990).175Computational Linguistics Volume 18, Number 2For HPSG-style grammars, completeness and coherence as defined for LFG, andextended to the general case by Wedekind (1988), are implemented in HPSG using the"subcategorization feature principle" (Johnson 1987): for the TFS implementation ofHPSG, termination is guaranteed, at least for the simplified version containing onlyhead-complement structures, described in Section 3.2.
Termination conditions for pars-ing are well understood in the framework of context-free grammars.
For generationusing feature structures, one of the problems is that the input could be "extended"during processing, i.e., arbitrary feature structures could be introduced in the semanticpart of the input by unification with the semantic part of a rule.
However, if the seman-tic part of the input is fully specified according to a set of type definitions describingthe set of well-formed semantic structures (and this condition is easy to check), thiscannot arise in a type-based system since it is not possible to add arbitrary features toa typed feature structure.A more general approach is described in Dymetman, Isabelle, and Perrault (1990),who define sufficient ermination properties for parsing and generation for the classof "Lexical Grammars."
These termination properties are conditions on the existenceof "conservative guides" for parsing and generation and seem generalizable to otherclasses of grammars as well, and are also applicable to TFS implementations.
SinceLexical Grammars are implemented in PROLOG, lefbrecursion must be eliminated forparsing and for generation, but this does not apply to TFS implementations.
The ideaof conservative guides is relatively simple and says that for parsing, each rule mustconsume a nonempty part of the string, and for generation, each rule must consumea nonempty part of the semantic form.
These conditions seem to be equivalent asto require the existence of a well-founded relation on strings (for parsing) and ofa well-founded relation on semantic forms (for generation).
The existence of suchwell-founded relations is actually a necessary condition for proving the terminationof parsing and generation (see Deville \[1990\] for a more general discussion on well-founded relations in the context of logic programming).Termination for reversible transfer grammars is discussed in van Noord (1990).One of the problems mentioned there is the extension of the "input," as in generation,and the answer is similar (see above).
However, properties similar to the "conservativeguides" of Dymetman, Isabelle, and Perrault (1990) have to hold in order to ensuretermination.4.
ConclusionThe TFS system has been developed to provide a computational environment for thedesign and the implementation f formal models of natural anguage.
The TFS formal-ism is designed as a specification language that can be used to design and implementformal linguistic models.
It is not a programming language: it does not offer meansof defining control information that would make execution more efficient (but lessgeneral), as it would be needed if it would be envisaged to use the system in anapplication-oriented nvironment (e.g., as a parser in a natural anguage interface to adatabase system).
From formal linguistic models developed in TFS, it could be envis-aged to develop rograms, i.e., parsers or generators, that would efficiently implementthe declarative knowledge contained in the formal specifications.
1313 See for example in Biggerstaff and Perlis (1989) the papers on the development of programs fromspecifications, a very important issue in software ngineering.
See also Ait-Kaci and Meyer (1990) for aprogramming language based on typed feature structures.176R6mi Zajac Inheritance and Constraint-Based Grammar FormalismsThe TFS system is implemented using rewriting techniques in a constraint-basedarchitecture adapted to feature structures:?
The language is a logical language directly based on typed featurestructures, and supports an object-oriented style based on multipleinheritance.?
Grammars are expressed as inheritance networks of typed featurestructures.
They define constraints on the set of acceptable linguisticstructures.
As a consequence, there is no formal distinction between"input" and "output."?
A unique general constraint solving mechanism is used.
Specificmapping properties, based on constituency, linear precedence orfunctional composition, are not part of the formalism itself, but can beencoded explicitly using the formalism.Although the current implementation is very much at the level of an experimentalprototype, and is still evolving, it has validated the basic concepts of the languageand of the implementation, and has been used to test different linguistic models andformalisms uch as LFG, DCG, HPSG, and SFG on small examples.
From these var-ious experimentations, we have defined extensions and improvements, both on thelanguage and on the implementation, that are needed for scaling up the system.On the language side, more expressivity is needed.
For example, disjunctions overfeature structures, various kinds of negation (Ait-Kaci 1986), and sets of feature struc-tures (Pollard and Moshier 1990) are necessary to formalize, e.g., nontrivial semanticstructures.
Some types together with a specific syntax and associated operations couldbe conveniently added to the system as libraries of built-in types, e.g., characters,strings, and trees.On the implementation side, the use of implementation techniques adapted fromPROLOG implementations, constraint satisfaction languages, and object-oriented lan-guages can be beneficial to the implementation f typed feature structure-based sys-tems and have to be more thoroughly explored.14One of the major efficiency issues inthe current implementation is the lack of an efficient indexing scheme for typed fea-ture structures.
For example, since the dictionaries are accessed using unification only,each entry is tried one after the other, leading to an extremely inefficient behavior withlarge dictionaries.
Thus, the use of a general indexing scheme based on a combinationof methods used in PROLOG implementations and in object-oriented database systemsis necessary and will be implemented in a future version of the system.AcknowledgmentsThe design and the implementation f theTFS system have been carried out incooperation with Martin Emele.
I would liketo thank Stefan Momma and Ulrich Heid;their numerous comments and adviceshelped to make this article more readable.
Iwould also like to thank anonymousreferees for their many detailed and helpfulcomments.
All remaining errors are ofcourse the sole responsibility of the author.14 For example, the use in the current implementation of several techniques adapted from PROLOGimplementations such as structure sharing, chronological dereferencing (Emele 1991) and last calloptimization, have improved efficiency by several orders of magnitude over previous "naive"implementations.177Computational Linguistics Volume 18, Number 2ReferencesAit-Kaci, Hassan (1984).
"A lattice theoreticapproach to computation based on acalculus of partially ordered typestructures."
Doctoral dissertation,University of Pennsylvania, Philadelphia,PA.A'/t-Kaci, Hassan (1986).
"An algebraicsemantics approach to the effectiveresolution of type equations."
TheoreticalComputer Science, 45, 293-351.Ait-Kaci, Hassan, and Meyer, Richard(1990).
Wild_LIFE, a user manual.DEC-PRL Technical Note PRL-TN-1,Rueil-Malmaison, France.Ait-Kaci, Hassan, and Podelski, Andreas(1991).
Towards a meaning of LIFE.DEC-PRL Research Report PRL-RR-11,Rueil-Malmaison, France.Bateman, John A., and Momma, Stefan(1991).
The nondirectional representationof Systemic Functional Grammars andSemantics as Typed Feature Structures.IMS Technical Report, University ofStuttgart, Germany.Biggerstaff, Ted J., and Perlis, Alan J.
(eds).(1989).
Software Reusability.
ACMPress-Addison-Wesley.Birkoff, Garrett (1940).
Lattice Theory.American Mathematical Society, thirdedition, 1973.Borgida, Alexander; Brachman, Ronald J.;McGuinness, Deborah L.; and Resnick,Lori Alperin (1989).
"CLASSIC: astructural data model for objects."
InProceedings, 1989 ACM SIGMODInternational Conference on Management ofData, Portland, Oregon.Bouma, Gosse (1990).
"Non-monotonicinheritance and unification."
InProceedings, Workshop on Inheritance inNatural Language Processing, Institute forLanguage Technology and AI, TilburgUniversity, The Netherlands.Bourbeau, L.; Carcagno, D.; Goldberg, E.;Kittredge, R.; and Polgu6re, A.
(1990).
"Bilingual generation of weather forecastsin an operation environment."
InProceedings, 13th International Conference onComputational Linguistics (COLING'90).Helsinki.Brachman, Ronald J., and Schmolze, JamesG.
(1985).
"An overview of the KL-ONEknowledge representation language.
"Cognitive Science 9, 171-216.Carpenter, Bob (1990).
"Typed featurestructures: inheritance, (in)equality andextensionality."
In Proceedings, Workshop onInheritance in Natural Language Processing,Institute for Language Technology andAI, Tilburg University, The Netherlands.Daelemans, Walter (1990).
"Inheritance andobject-oriented natural anguageprocessing."
In Proceedings, Workshop onInheritance in Natural Language Processing,Institute for Language Technology andAI, Tilburg University, The Netherlands.Dershowitz, N., and Plaisted, D. A.
(1988).
"Equational programming."
In MachineIntelligence 11, edited by Hayes, Michie,and Richards.
Oxford: Clarendon Press.De Smedt, Koenraad, and de Graaf, Josje(1990).
"Structured inheritance inframe-based representation f linguisticcategories."
In Proceedings, Workshop onInheritance in Natural Language Processing.Institute for Language Technology andAI, Tilburg University, The Netherlands.Deville, Yves (1990).
Logic programming.Systematic Program Development.
Reading,MA: Addison-Wesley.Dymetman, Marc, and Isabelle, Pierre(1988).
"Reversible logic grammars formachine translation."
In Proceedings, 2ndInternational Conference on Theoretical ndMethodological Issues in Machine Translationof Natural Language.
Pittsburgh, PA.Dymetman, Marc; Isabelle, Pierre; andPerrault, Francois (1990).
"A symmetricalapproach to parsing and generation."
InProceedings, 13th International Conference onComputational Linguistics (COLING'90).Helsinki.Emele, Martin (1988).
"A typed featurestructure unification-based approach togeneration."
In Proceedings, WGNLC of theIECE.
Oiso University, Japan.Emele, Martin (1991).
"Unification with lazynon-redundant copying."
In Proceedings,29th Annual Meeting of the ACL.
Berkeley,CA.Emele, Martin, and Zajac, R6mi (1989).
"RETIF: A rewriting system for typedfeature structures."
ATR Technical reportTR-I-0071, ATR, Kyoto.Emele, Martin, and Zajac, R6mi (1989).
"Multiple inheritance in RETIE" ATRTechnical report TR-I-0114, ATR, Kyoto.Emele, Martin, and Zajac, R6mi (1990a).
"Afixed-point semantics for feature typesystems."
In Proceedings, 2nd Workshop onConditional and Typed Rewriting Systems(CTRS'90).
Montr6al, Qu6bec.Emele, Martin, and Zajac, R6mi (1990b).
"Typed unification grammars."
InProceedings, 13th International Conference onComputational Linguistics (COLING'90).Helsinki.Emele, Martin; Heid, Ulrich; Momma,Stefan; and Zajac, R6mi (1990).
"Organizing linguistic knowledge formultilingual generation."
In Proceedings,178R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms13th International Conference onComputational Linguistics (COLING'90).Helsinki.Etherington, David W.; Forbus, Kenneth D.;Ginsberg, Matthew L.; Israel, David; andLifschitz, Vladimir (1989).
"Critical issuesin nonmonotonic reasoning."
InProceedings, 1st International Conference onPrinciples of Knowledge Representation andReasoning.
Toronto, Ontario.Evans, Roger, and Gazdar, Gerald (1989).
"Inference in DATR."
In Proceedings, 4thEuropean ACL Conference.
Manchester, U.K.Franz, Alex (1990).
"A parser for HPSG.
"CMU report CMU-LCL-90-3, Laboratoryfor Computational Linguistics, CarnegieMellon University.Fraser, Norman M., and Hudson, RichardA.
(1990).
"Word grammar: Aninheritance-based theory of language."
InProceedings, Workshop on Inheritance inNatural Language Processing.
Institute forLanguage Technology and AI, TilburgUniversity, The Netherlands.Halliday, M. A. K. (1985).
Introduction toFunctional Grammar.
London: EdwardArnold.Huet, G6rard (1976).
Rdsolution d'dquationsdans les langages d'ordre 1, 2,..., w. Doctoraldissertation, Universit6 de Paris VII.Johnson, Mark (1987).
"Grammaticalrelations in attribute-value grammars."
InProceedings, West Coast Conference on FormalLinguistics, Vol.
6.
Stanford, CA.Kaplan, Ronald M.; Netter, Klaus;Wedekind, J~irgen; and Zaenen, Annie(1989).
"Translation by structuralcorrespondences."
In Proceedings, 4thEuropean ACL Conference.
Manchester, U.K.Kay, Martin (1984).
"Functional unificationgrammar: A formalism for machinetranslation."
In Proceedings, l OthInternational Conference on ComputationalLinguistics (COLING-84).
Stanford, CA.Klop, Jan Willem (1990).
"Term rewritingsystems."
To appear in Handbook of Logicin Computer Science, Volume 1, edited byS.
Abramsky, D. Gabbay and T. Maibaum.Oxford University Press.MacGregor, Robert M. (1988).
"A deductivepattern matcher."
In Proceedings, 7thNational Conference on Artificial Intelligence(AAAI'88).
St. Paul, MN, 403-408.MacGregor, Robert M. (1990).
"LOOM usermanual."
USC/ISI Technical Report,Marina del Rey, CA.Mann, William C., and Matthiessen,Christian I. M. I.
(1985).
"Demonstrationof the Nigel text generation computerprogram."
In Systemic Perspectives onDiscourse, Volume 1, edited by JamesD.
Benson and William S. Greaves.Norwood, NJ: Ablex.Nirenburg, Sergei, Carbonele, Jaime, Tomita,Masaru, and Goodman, Kenneth (1992).Machine Translation.
A Knowledge-BasedApproach.
San Mateo, CA: MorganKaufmann.Pollard, Carl J.
(In press).
"Sorts inunification-based grammar and what theymean."
In Unification in Natural LanguageAnalysis, edited by M. Pinkal andB.
Gregor.
Cambridge, MA: The MITPress.Pollard, Carl J., and Moshier, Drew (1989).
"Unifying partial descriptions of sets."
InInformation, Language and Cognition, editedby P. Hanson, Vancouver Studies inCognitive Science 1.
Vancouver:University of British Columbia Press.Pollard, Carl J., and Sag, Ivan A.
(1987).Information-Based Syntax and Semantics.Volume 1: Syntax.
CSLI Lecture Notes 13,Chicago University Press.Pollard, Carl J., and Sag, Ivan A.
(Unpublished research).
Information-BasedSyntax and Semantics.
Volume 2: Agreement,Binding and Control.Reape, Mike (1990).
"Parsing semi-freeword order and bounded iscontinuousconstituency and "shake 'n' bake"machine translation (or 'generation asparsing')."
International Workshop onConstraint Based Formalisms for NaturalLanguage Generation, Bad Teinach,Germany.Rounds, Williams C., and Kasper, Robert T.(1986).
"A Complete Logical Calculus forRecord Structures Representing LinguisticInformation."
IEEE Symposium on Logic inComputer Science.Saint-Dizier, Patrick (1991).
"Processinglanguage with logical types and activesconstraints."
In Proceedings, 5th Conferenceof the European Chapter of the ACL.
Berlin,Germany.Shieber, Stuart (1986).
An Introduction toUnification-based Grammar Formalisms.
CSLILectures Notes 4, Chicago UniversityPress.Smolka, Gert (1988).
"A feature logic withsubsorts."
LILOG Report 33, IBMDeutschland GmbH, Stuttgart.Smolka, Gert (1989).
"Feature constraintlogics for unification grammars."
IWBSReport 93, IBM Deutschland GmbH,Stuttgart.Smolka, Gert, and Ait-Kaci, Hassan (1988).
"Inheritance hierarchies: Semantics andunification."
J.
Symbolic Computation, 7,343-370.Stoy, Joseph E. (1977).
Denotational Semantics:179Computational Linguistics Volume 18, Number 2The Scott-Strachey Approach to ProgrammingLanguage Theory.
Cambridge, MA: TheMIT Press.van Hentenryck, Pascal (1989).
ConstraintSatisfaction i Logic Programming,Cambridge, MA: The MIT Press.van Hentenryck, Pascal, and Dincbas,Mehmet (1987).
"Forward checking inlogic programming."
In Proceedings, 4thInternational Conference on LogicProgramming.
Melbourne, Australia.van Noord, Gertjan (1990).
"Reversibleunification-based machine translation."
InProceedings, 13th International Conference onComputational Linguistics (COLING'90).Helsinki.Wedekind, Jiirgen (1988).
"Generation asstructure driven generation."
InProceedings, 12th International Conference onComputational Linguistics (COLING'88).Budapest.Whitelock, Pete (1990).
"Shake-and-baketranslation."
Ms. Sharp Laboratories ofEurope, Oxford.Yen, John; Neches, Robert; and MacGregor,Robert (1988).
"Classification-basedprogramming: A deep integration offrames and rules."
Technical ReportISI/RR-88-213, USC/Information ScienceInstitute.Zajac, R6mi (1989).
"A transfer model usinga typed feature structure rewriting systemwith inheritance."
In Proceedings, 27thAnnual Meeting of the ACL.
Vancouver, B.C.Zajac, R6mi (1990a).
"A relational approachto translation."
In Proceedings, 3rdInternational Conference on Theoretical ndMethodological Issues in Machine Translationof Natural Language.
Austin, Texas.Zajac, R6mi (1990).
"Semantics of typedfeature structures."
International Workshopon Constraint Based Formalisms for NaturalLanguage Generation.
Bad Teinach,Germany.180R6mi Zajac Inheritance and Constraint-Based Grammar FormalismsAppendix: Syntax of the TFS LanguageIn the TFS language, there are two kinds of comments.
In-line comments begin witha semicolon and end with the end of line.
These comments can appear anywherewhere a white space character is allowed in the syntax.
They are skipped duringreading.
Syntactic omments begin and end with the % character.
These comments canappear only where specified in the BNF syntax specification.
They are attached to thestructure produced by the reader and can be displayed if the appropriate printer optionis set.
Identifiers are case-sensitive.
Macros are expanded statically by the compiler.The operator ~ is interpreted as the meet (unification).The following extensions of the BNF notation are used:?
\[X\] denotes the optional element X (zero or one occurrence).?
X* denotes the free iteration of element X (zero, one or moreoccurrences).?
X + denotes the iteration of element X (one or more occurrences).?
Symbols in typewr i ter  font denote symbols of the TFS syntax.<entity> ::= <query> I <definition><query> ::-- .9 <expression> .<definition> ::= <po-definition> I <macro-definition> \] <type-definition><po-definition> ::= <type-symbol> < <type-symbol> .<macro-definition> ::=<identifier> "= <expression> .<type-definition> ::--<type-symbol> <expression> : - <conditions> .<conditions> ::= <expression> \[ , <conditions> \]<expression> ::= <feature-structure> \[ & <expression> \]<feature-structure> ::= <comments> <tagged-feature-structure><tagged-feature-structure> ::= <tag> \[ = <typed-feature-structure> \] I<typed-feature-structure><typed-feature-structure> ::-- <type-symbol> \[ <attribute-value-matrix> \] I<attribute-value-matrix > I<list><attribute-value-matrix> ::= \[ \[ <attribute-value-pairs> \] \]<attribute-value-pairs> ::= <attribute-value-pair> \[ , <attribute-value-pairs> \]<attribute-value-pair> ::= <attribute> : <expression><list> ::= < <expression>* \[ .
<expression> \] ><tag> ::= #<identif ier><type-symbol> ::= <identifier><attribute> ::= <identifier>Example 1: the textual definitmns ~r F~u~ 2.NIL < LIST.CONS < LIST.CONS\[first: T, res t :  LIST\].APPEND\[I: LIST, 2: LIST, 3: LIST\].181Computational Linguistics Volume 18, Number 2APPENDO< APPEND.APPENDO\[I: NIL, 2: #I=LIST, 3: #i\].APPEND1 < APPEND.APPEND1\[1: <#x #ii>, 2: #12, 3: <#x .
#13>\]?
-APPEND\[ I :  #ii, 2: #12, 3: #13\].Example 2: the textual definitions ~r F~ure 10.TAU-LEX < TAU.TAU-LEX-V < TAU-LEX.TAU-LEX-V\[eng: \[tense: #e-tense\],fr: \[tense: #f-tense\]\]:- TAU-TENSE\[eng: #e-tense, fr: #f-tense\].TAU-LEX-ITV < TAU-LEX-V.TAU-LEX-V\[eng: \[subj: #e-subj\],fr: \[subj: #f-subj\]\]?
- TAU-LEX\[eng: #e-subj, fr: #f-subj\].TAU-LEX-FALL < TAU-LEX-ITV.TAU-LEX-FALL\[eng: \[pred: FALL\],fr: \[pred: TOMBER\]\]182
