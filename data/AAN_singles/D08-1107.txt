Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021?1030,Honolulu, October 2008. c?2008 Association for Computational LinguisticsThe Linguistic Structure of English Web-Search QueriesCory Barr and Rosie JonesYahoo!
Inc.701 First AveSunnyvale, CA 94089barrc,jonesr@yahoo-inc.comMoira RegelsonPerfect Market, Inc.Pasadena, CA 91103mregelson@perfectmarket.comAbstractWeb-search queries are known to be short,but little else is known about their structure.In this paper we investigate the applicabilityof part-of-speech tagging to typical English-language web search-engine queries and thepotential value of these tags for improvingsearch results.
We begin by identifying aset of part-of-speech tags suitable for searchqueries and quantifying their occurrence.
Wefind that proper-nouns constitute 40% of queryterms, and proper nouns and nouns togetherconstitute over 70% of query terms.
We alsoshow that the majority of queries are noun-phrases, not unstructured collections of terms.We then use a set of queries manually la-beled with these tags to train a Brill tag-ger and evaluate its performance.
In addi-tion, we investigate classification of searchqueries into grammatical classes based on thesyntax of part-of-speech tag sequences.
Wealso conduct preliminary investigative experi-ments into the practical applicability of lever-aging query-trained part-of-speech taggers forinformation-retrieval tasks.
In particular, weshow that part-of-speech information can be asignificant feature in machine-learned search-result relevance.
These experiments also in-clude the potential use of the tagger in se-lecting words for omission or substitution inquery reformulation, actions which can im-prove recall.
We conclude that training a part-of-speech tagger on labeled corpora of queriessignificantly outperforms taggers based on tra-ditional corpora, and leveraging the uniquelinguistic structure of web-search queries canimprove search experience.1 IntroductionWeb-search queries are widely acknowledged to beshort (2.8 words (Spink et al, 2002)) and to be fre-quently reformulated, but little else is understoodabout their grammatical structure.
Since searchqueries are a fundamental part of the informationretrieval task, it is essential that we interpret themcorrectly.
However, the variable forms queries takecomplicate interpretation significantly.
We hypoth-esize that elucidating the grammatical structure ofsearch queries would be highly beneficial for the as-sociated information retrieval task.Previous work with queries (Allan and Raghavan,2002) considered that short queries may be ambigu-ous in their part of speech and that different docu-ments are relevant depending on how this ambigu-ity is resolved.
For example, the word ?boat?
in aquery may be intended as subject of a verb, objectof a verb, or as a verb, with each case reflectinga distinct intent.
To distinguish between the possi-bilities, Allan and Raghavan (Allan and Raghavan,2002) propose eliciting feedback from the user byshowing them possible contexts for the query terms.In addition to disambiguating query terms for re-trieval of suitable documents, part-of-speech tag-ging can help increase recall by facilitating queryreformulation.
Zukerman and Raskutti (Zukermanand Raskutti, 2002) part-of-speech tag well-formedquestions, and use the part-of-speech tags to substi-tute synonyms for the content words.Several authors have leveraged part-of-speechtagging towards improved index constructionfor information retrieval through part-of-speech-1021based weighting schemas and stopword detection(Crestani et al, 1998), (Chowdhury and McCabe,2000), (Dincer and Karaoglan, 2004).
Their exper-iments show degrees of success.
Recently, alongwith term weighting, Lioma has been using part-of-speech n-grams for noise and content detectionin indexes (Lioma, 2008).
Our study differs fromthese in that linguistic and part-of-speech focus isalmost exclusively placed on queries as opposedto the indexed documents, reflecting our opinionthat queries exhibit their own partially predictableand unique linguistic structure different from thatof the natural language of indexed documents.Similarly, (Strzalkowski et al, 1998) added a layerof natural language processing using part-of-speechtags and syntactical parsing to the common statis-tical information-retrieval framework, much likeexperiments detailed in sections 4 and 5.
Oursystem differs in that our syntactic parsing systemwas applied to web-search queries and uses rulesderived from the observed linguistic structure ofqueries as opposed to natural-language corpora.By focusing on the part-of-speech distribution andsyntactic structure of queries over tagged indexeddocuments, with a simple bijection mapping ourquery tags to other tag sets, our system offers acomplementary approach that can be used in tandemwith the techniques referenced above.Lima and Pederson (de Lima and Pederson, 1999)conducted related work in which part-of-speechtagging using morphological analysis was used asa preprocessing step for labeling tokens of web-search queries before being parse by a probabilis-tic context-free grammar tuned to query syntax.
Webelieve this technique and others relying on part-of-speech tagging of queries could benefit from using aquery-trained tagger prior to deeper linguistic anal-ysis.Pasca (Pasca, 2007) showed that queries can beused as a linguistic resource for discovering namedentities.
In this paper we show that the majorityof query terms are proper nouns, and the majorityof queries are noun-phrases, which may explain thesuccess of this data source for named-entity discov-ery.In this work, we use metrics that assume a uniquecorrect part-of-speech tagging for each query, im-plicitly addressing the disambiguation issue throughinter-annotator-agreement scores and tagger gener-alization error.
To identify these tags, we first ana-lyze the different general forms of queries.
In Sec-tion 2 we determine a suitable set of part-of-speechlabels for use with search queries.
We then use man-ually labeled query data to train a tagger and eval-uate its performance relative to one trained on theBrown corpus in Section 3.
We make observationsabout the syntactic structure of web-search queriesin Section 4, showing that the majority (70%) ofqueries are noun-phrases, in contrast with the com-monly held belief that queries consist of unstruc-tured collections of terms.
Finally, we examine thepotential use of tagging in the tasks of search rele-vance evaluation and query reformulation in Section5.2 DataWe sampled queries from the Yahoo!
search en-gine recorded in August 2006.
Queries were sys-tematically lower-cased and white-spaced normal-ized.
We removed any query containing a non-ASCII character.
Queries were then passed througha high-precision proprietary query spelling correc-tor, followed by the Penn Treebank tokenizer.
Noother normalization was carried out.
Despite Penn-tokenization, queries were typical in their averagelength (Jansen et al, 2000).
We sampled 3,283queries from our dataset to label, for a total of 2,508unique queries comprised of 8,423 individual to-kens.2.1 Inter-rater AgreementThe sparse textual information in search queriespresents difficulties beyond standard corpora, notonly for part-of-speech tagging software but also forhuman labelers.
To quantify the level of these diffi-culties we measured inter-rater agreement on a setof 100 queries labeled by each editor.
Since onelabeler annotated 84.4% of the queries, we used anon-standard metric to determine agreement.
Onehundred queries were selected at random from eachof our secondary labelers.
Our primary labeler thenre-labeled these queries.
Accuracy was then calcu-lated as a weighted average, specifically the mean ofthe agreement between our primary labeler and sec-ondary labelers, weighted by the number of queries1022contributed by each secondary labeler.
Measuringagreement with respect to the individual part-of-speech tag for each token, our corpus has an inter-rater agreement of 79.3%.
If we require agreementbetween all tokens in a query, agreement falls to65.4%.
Using Cohen?s kappa coefficient, we havethat token-level agreement is a somewhat low 0.714and query-level agreement is an even lower 0.641.We attempted to accurately quantify token-levelambiguity in queries by examining queries wherechosen labels differ.
An author-labeler examinedconflicting labels and made a decision whether thedifference was due to error or genuine ambigu-ity.
Error can be a result of accidentally select-ing the wrong label, linguistic misunderstanding(e.g., ?chatting?
labeled as a verb or gerund), orlack of consensus between editors (e.g., model num-bers could be nouns, proper nouns, or even num-bers).
Examples of genuinely ambiguous queries in-clude ?download?
and ?rent,?
both of which couldbe a noun or verb.
Another major source of gen-uine token-level ambiguity comes from strings ofproper nouns.
For example, some editors consid-ered ?stillwater chamber of commerce?
one entityand hence four proper-noun tokens while others con-sidered only the first token a proper noun.
Of the 99conflicting token labels in our queries used to mea-sure inter-annotator agreement, 69 were judged dueto genuine ambiguity.
This left us with a metric in-dicating query ambiguity accounts for 69.7% of la-beling error.2.2 Tags for Part-of-Speech Tagging QueriesIn preliminary labeling experiments we found manystandard part-of-speech tags to be extremely rare inweb-search queries.
Adding them to the set of possi-ble tags made labeling more difficult without addingany necessary resolution.
In Table 1 we give theset of tags we used for labeling.
In general, part-of-speech tags are defined according to the distribu-tional behavior of the corresponding parts of speech.Our tag set differs dramatically from the Brown orPenn tag sets.
Perhaps most noticeably, the sizes ofthe tag sets are radically different.
The Brown tag setcontains roughly 90 tags.
In addition, several tagscan be appended with additional symbols to indicatenegation, genitives, etc.
Our tag set contains just 19unique classes.Tag Example Count (%)proper-noun texas 3384 (40.2%)noun pictures 2601 (30.9%)adjective big 599 (7.1%)URI ebay.com 495 (5.9%)preposition in 310 (3.7%)unknown y 208 (2.5%)verb get 198 (2.4%)other conference06-07 174 (2.1%)comma , 72 (0.9%)gerund running 69 (0.8%)number 473 67 (0.8%)conjunction and 65 (0.8%)determiner the 56 (0.7%)pronoun she 53 (0.6%)adverb quickly 28 (0.3%)possessive ?s 19 (0.2%)symbol ( 18 (0.2%)sentence-ender ?
5 (0.1%)not n?t 2 (0.0%)Table 1: Tags used for labeling part-of-speech in web-search queries.Our contrasting tag sets reflect an extremely dif-ferent use of the English language and correspond-ing part-of-speech distribution.
For example, theBrown tag set contains unique tags for 35 types ofverbs.
We use a single label to indicate all cases ofverbs.
However, the corpora the Brown tag set wasdesigned for consists primarily of complete, natural-language sentences.
Essentially, every sentence con-tains at least one verb.
In contrast, a verb of any typeaccounts for only 2.35% of our tags.
Similarly, theBrown corpus contains labels for 15 types of deter-miners.
This class makes up just 0.66% of our data.Our most common tag is the proper noun, whichconstitutes 40% of all query terms, and proper nounsand nouns together constitute 71% of query terms.In the Brown corpus, by contrast, the most commontag, noun, constitutes about 13% of terms.
Thus thedistribution of tag types in queries is quite differentfrom typical edited and published texts, and in par-ticular, proper nouns are more common than regularnouns.2.3 Capitalization in Query DataAlthough we have chosen to work with lowercasedata, web search queries sometimes contain capi-1023Use of Capitals Count % ExampleProper-nouns capitalized 48 47% list of Filipino riddlesQuery-Initial-Caps 10 10% Nautical mapInit-Caps + Proper-Nouns 7 7% Condos in YonkersAcronym 4 4% location by IP addressTotal standard capitalization 69 67%All-caps 26 25% FAX NUMBER FORALLEN CANNING COEach word capitalized 6 6% Direct SellingMixed 2 2% SONGS OF MEDONAmusic feature:audioTotal non-standard capitalization 34 33%Table 2: Ways capitalization is used in web-searchqueries.talization information.
Since capitalization is fre-quently used in other corpora to identify propernouns, we reviewed its use in web-search queries.We found that the use of capitalization is inconsis-tent.
On a sample of 290,122 queries from Au-gust 2006 only 16.8% contained some capitaliza-tion, with 3.9% of these all-caps.
To review the useof capitalization, we hand-labeled 103 queries con-taining capital letters (Table 2).Neither all-lowercase (83.2%) nor all-caps (3.9%)queries can provide us with any part-of-speechclues.
But we would like to understand the use ofcapitalization in queries with varied case.
In par-ticular, how frequently does first-letter capitalizationindicate a proper noun?
We manually part-of-speechtagged 75 mixed-case queries, which contained 289tokens, 148 of which were proper nouns.
The base-line fraction of proper nouns in this sample is thus51% (higher than the overall background of 40.2%).A total of 176 tokens were capitalized, 125 of themproper nouns.
Proper nouns thus made up 73.3%of capitalized tokens, which is larger than the back-ground occurrence of proper nouns.
We can con-clude from this that capitalization in a mixed-casequery is a fair indicator that a word is a proper noun.However, the great majority of queries contain noinformative capitalization, so the great majority ofproper nouns in search queries must be uncapital-ized.
We cannot, therefore, rely on capitalization toidentify proper nouns.With this knowledge of the infrequent use of capi-tal letters in search queries in mind, we will examinethe effects of ignoring or using a query?s capitaliza-tion for part-of-speech tagging in Section 3.4.2.3 Tagger Accuracy on Search QueriesTo investigate automation of the tagging process,we trained taggers on our manually labeled queryset.
We used 10-fold cross-validation, with 90% ofthe data used for training and the remaining dataused for testing.
In the sections below, we used twodatasets.
The first consists of 1602 manually labeledqueries.
For the experiments in Section 3.5 we la-beled additional queries, for a total of 2503 manu-ally labeled queries.3.1 Part-of-Speech Tagging SoftwareWe experimented with two freely available part-of-speech taggers: The Brill Tagger (Brill, 1995)and The Stanford Tagger (Toutanova and Manning,2000; Toutanova et al, 2003).The Brill tagger works in two stages.
The initialtagger queries a lexicon and labels each token withits most common part-of-speech tag.
If the tokenis not in the lexicon, it labels the token with a de-fault tag, which was ?proper noun?
in our case.
Inthe second stage, the tagger applies a set of lexicalrules which examine prefixes, suffixes, and infixes.The tagger may then exchange the default tag basedon lexical characteristics common to particular partsof speech.
After application of lexical rules, a setof contextual rules analyze surrounding tokens andtheir parts of speech, altering tags accordingly.We chose to experiment primarily with the Brilltagger because of its popularity, the human-readablerules it generates, and its easily modifiable codebase.
In addition, the clearly defined stages and in-corporation of the lexicon provide an accessible wayto supply external lexicons or entity-detection rou-tines, which could compensate for the sparse con-textual information of search queries.We also experimented with the Stanford Log-Linear Part-of-Speech Tagger, which presentlyholds the best published performance in the field at96.86% on the Penn Treebank corpus.
It achievesthis accuracy by expanding information sources fortagging.
In particular, it provides ?
(i) more exten-sive treatment of capitalization for unknown words;(ii) features for the disambiguation of the tenseforms of verbs; (iii) features for disambiguating par-ticles from prepositions and adverbs.?
It uses amaximum-entropy approach to handle information1024diversity without assuming predictor independence(Toutanova and Manning, 2000).3.2 Baseline: Most Common TagWith proper nouns dominating the distribution, wefirst considered using the accuracy of labeling all to-kens ?proper noun?
as a baseline.
In this case, welabeled 1953 of 4759 (41.0%) tokens correctly.
Thisis a significant improvement over the accuracy oftagging all words as ?noun?
on the Brown corpus(approximately 13%), reflecting the frequent occur-rence of proper nouns in search queries.
However, toexamine the grammatical structure of search querieswe must demonstrate that they are not simply col-lections of words.
With this in mind, we chose in-stead to use the most common part-of-speech tagfor a word as a baseline.
We evaluated the baselineperformance on our manually labeled dataset, withURLs removed.
Each token in the set was assignedits most common part of speech, according to theBrill lexicon.
In this case, 4845 of 7406 tokens weretagged correctly (65.42%).3.3 Effect of Type of Training DataThe Brill tagger software is pre-trained on the stan-dard Wall Street Journal corpus, so the simplest pos-sible approach is to apply it directly to the query dataset.
We evaluated this ?out-of-the-box?
performanceon our 1602 manually labeled queries, after mappingtags to our reduced tag set.
(Our effective training-set size is 1440 queries, since 10% were held outto measure accuracy through cross validation.)
TheWSJ-trained tagger labeled 2293 of 4759 (48.2%)tags correctly, a number well below the baselineperformance, demonstrating that application of thecontextual rules that Brill learns from the syntax ofnatural-language corpora has a negative effect on ac-curacy in the context of queries.
When we re-trainedBrill?s tagger on a manually labeled set of queries,we saw accuracy increase to 69.7%.
The data usedto train the tagger therefore has a significant effecton its accuracy (Table 3).
The accuracy of the tag-ger trained on query data is above the baseline, in-dicating that search queries are somewhat more thancollections of words.3.4 Improving Tagger AccuracyWe conducted several experiments in improving tag-ger accuracy, summarized in Table 3 and describedin detail below.3.4.1 Adding External LexiconWith a training-set size of 1500 queries, compris-ing a lexicon of roughly 4500 words, it is natural toquestion if expanding the lexicon by incorporatingexternal sources boosts performance.
To this end,we lower-cased the lexicon of 93,696 words pro-vided by the Brill tagger, mapped the tags to ourown tag set, and merged our lexicon from queries.This experiment resulted in an accuracy of 71.1%, a1.4% increase.One explanation for the limited increase is thatthis lexicon is derived from the Brown corpus andthe Penn Treebank tagging of the Wall Street Jour-nal.
These corpora are based on works publishedin 1961 and 1989-1992 respectively.
As shown inTable 1, proper nouns dominate the distribution ofsearch-engine queries.
Many of these queries willinvolve recent products, celebrities, and other time-sensitive proper nouns.
We speculate that Web-based information resources could be leveraged toexpand the lexicon of timely proper nouns, therebyenhancing performance.3.4.2 Experiments with Perfect CapitalizationThe overall performance of the pre-trained Brilltagger on our query set may be due to its poor per-formance on proper nouns, our most frequent partof speech.
In the WSJ newspaper training data,proper-nouns always start with a capital letter.
Asdiscussed in Section 2.3, capitalization is rare inweb-search queries.
To examine the effect of themissing capitalization of proper nouns, we evaluateda pre-trained Brill tagger on our previously men-tioned manually labeled corpus of 1602 queries al-tered such that only the proper nouns were capital-ized.
In this case, the tagger reached an extraordi-nary 89.4% accuracy (Table 3).
Unfortunately, thevast majority of queries do not contain capitalizationinformation and those that do often contain mislead-ing information.
The pre-trained tagger achievedonly a 45.6% accuracy on non-lowercased queries,performing even worse than on the set with no capi-talization at all.1025Experiment AccuracyLabel-all-proper-noun 41.0%WSJ-trained 48.2%most-freq-tag-WSJ 64.4%re-trained 69.7%retrained + WSJ lexicon 71.1%user capitalization 45.6%oracle capitalization 89.4%automatic capitalization 70.9%Table 3: Tagging experiments on small labeled corpus.Experiments were conducted on lower-cased queries ex-cept where specifically indicated.3.4.3 Automatic CapitalizationWe saw in Section 2.3 that web searchers rarelyuse capitalization.
We have also seen that a pre-trained Brill tagger run on queries with perfectcapitalization (?oracle?
capitalization) can achieve89.4% accuracy.
We now look at how performancemight be affected if we used an imperfect algorithmfor capitalization.In order to attempt to capitalize the proper nounsin queries, we used a machine-learned system whichsearches for the query terms and examines how of-ten they are capitalized in the search results, weight-ing each capitalization occurrence by various fea-tures (Bartz et al, 2008).
Though the capitalizationsystem provides 79.3% accuracy, using this systemwe see an only a small increase of accuracy in part-of-speech tagging at 70.9%.
This system does notimprove significantly over the tagger trained on thelower-cased corpus.
One explanation is that cap-italization information of this type could only beobtained for 81.9% of our queries.
Multiplied byaccuracy, this implies that roughly 81.9% * 79.3%= 65.0% of our proper nouns are correctly cased.This suggests that any technique for proper-noun de-tection in search-engine queries must provide over65.0% accuracy to see any performance increase.Finally we looked at the capitalization as input bysearchers.
We trained on the oracle-capitalized cor-pus, and tested on raw queries without normaliza-tion.
We saw an accuracy of just 45.6%.
Thus usingthe capitalization input by web searchers is mislead-ing and actually hurts performance.Figure 1: Brill?s tagger trained on web-search queries.We see that the most significant gains in performance arewith the first few hundred labeled examples, but even af-ter 2500 examples are labeled, more labeled data contin-ues to improve performance.3.5 Learning CurveIt is important to understand whether tagger accu-racy is limited by the small size of our manually la-beled dataset.
To examine the effect of dataset size,we trained Brill?s tagger with increasing numbers oflabeled queries and evaluated accuracy with each setsize.
In the interim between conducting the experi-ments of sections 3.1 through 3.3 and those of sec-tion 3.5, we were able to obtain 1120 new labeledqueries, allowing us to extend the learning curve.With our complete corpus of 2722 labeled exam-ples (for a cross-validated training-set size of 2450labeled examples, URLs omitted), we see an accu-racy of 78.6% on a per-token basis.
We see the mostsignificant gains in performance with the first fewhundred labeled examples, but even after 2500 ex-amples are labeled, more labeled data continues toimprove performance.3.6 Comparing Taggers to SuggestMethods for Boosting PerformanceIn Table 4 we see a comparison of Brill?s tagger tothe Stanford tagger trained on 2450 labeled queries.The 0.3% performance increase is not statisticallysignificant.
As listed in Section 3, the featuresthe Stanford tagger adds to achieve high accuracyin traditional natural-language corpora are not in-1026Tagger AccuracyBrill 78.6%Stanford 78.9%Table 4: Comparison of Brill?s tagger to the Stanford tag-ger, on our corpus of manually annotated query logs.formative in the domain of search-engine queries.We believe greater performance on our data will beachieved primarily through examination of commonsources of inter-rater disagreement (such as consis-tent handling of ambiguity) and incorporation of ex-ternal sources to detect proper nouns not in the lexi-con.To validate our intuition that expanding the lexi-con will boost performance, we obtained a propri-etary list of 7385 known trademarked terms usedin the sponsored-search industry.
Treating thesephrases as proper nouns and adding them to the lex-icon from the Wall Street Journal supplied with theBrill tagger, we see our cross validated accuracy im-prove to 80.2% (with a standard deviation of 1.85%),the highest score achieved in our experiments.
Wefind it likely that incorporation of more external lex-ical sources will result in increased performance.Our experiments also support our hypothesis thataddressing inter-annotator agreement will boost per-formance.
We can see this by examining the resultsof the experiments in section 3.3 verses section 3.5.In section 3.3, we see the accuracy on the query-trained Brill tagger is 69.7%.
As mentioned, forthe experiment in section 3.5, we were able to ob-tain 1120 new queries.
Each of these newly labeledqueries came from the same labeler, who believestheir handling of the ambiguities inherent in searchqueries became more consistent over time.
Withthe same training-set size of 1440 used in section3.3, Figure 1 shows performance at 1440 queries isroughly 6% higher.
We believe this significant im-provement is a result of more consistent handling ofquery ambiguity obtained through labeling experi-ence.4 Query GrammarThe above-baseline performance of the Brill taggertrained on web-search queries suggests that web-search queries exhibit some degree of syntacticalstructure.
With a corpus of queries labeled with part-of-speech information, we are in a position to ana-lyze this structure and characterize the typical pat-terns of part-of-speech used by web searchers.
Tothis end, we randomly sampled and manually la-beled a set of 222 queries from the part-of-speechdataset used for tagger training mentioned above.Each query was labeled with a single meta-tag in-dicating query type.
Two author-judges simultane-ously labeled queries and created the set of meta-tags during much discussion, debate, and linguisticresearch.
A list of our meta-tags and the distribu-tion of each are provided in Table 5.
We can seethat queries consisting of a noun-phrase dominatethe distribution of query types, in contrast with thepopularly held belief that queries consist of unstruc-tured collections of terms.To determine how accurately a meta-tag can bedetermined based on part-of-speech labels, we cre-ated a grammar consisting of a set of rules to rewritepart-of-speech tags into higher-level grammaticalstructures.
These higher-level grammatical struc-tures are then rewritten into one of the seven classesof meta-tags seen in Table 5.
Our grammar was con-structed by testing the output of our rewrite rules onqueries labeled with par-of-speech tags that were notpart of the 222 queries sampled for meta-tag label-ing.
Grammar rules were revised until the failurerate on previously untested part-of-speech-labeledqueries stabilized.
Failure was evaluated by twomeans.
In the first case, the grammar rules failedto parse the sequence of part-of-speech tags.
In thesecond case, the grammar rules led to an inappro-priate classification for a query type.
As during thelabeling phase, the two author-labelers simultane-ously reached a consensus on whether a parse failedor succeeded, rendering an inter-annotator score in-applicable.
The resulting grammar was then testedon the 222 queries with query-type meta-tags.Our rules function much like production rules incontext-free grammars.
As an example, the two-tag sequence ?determiner noun?
will be rewrittenas ?noun phrase.?
This in turn could be re-writteninto a larger structure, which will then be rewritteninto a meta-tag of query type.
The primary differ-ence between a context-free grammar or probabilis-tic context-free grammar (such as that employed byLima and Pederson (de Lima and Pederson, 1999))1027Query Gramm.
Type Example Freq (%)noun-phrase free mp3s 155 (69.8%)URI http:answers.yahoo.com/ 24 (10.8%)word salad mp3s free 19 (8.1%)other-query florida elementary reading 15 (6.8%)conference2006-2007unknown nama-nama calon praja ipdn 6 (2.7%)verb-phrase download free mp3s 3 (1.4%)question where can I download free mp3s 1 (0.45%)Table 5: Typical grammatical forms of queries used byweb searchers, with distribution based on a sample of 222hand-labeled queries.and our grammar is that our rules are applied itera-tively as opposed to recursively.
As such, our gram-mar yields a single parse for each input.Some of our rules reflect the telegraphic nature ofweb queries.
For example, it is much more com-mon to see an abbreviated noun-phrase consisting ofadjective-noun, than one consisting of determiner-adjective-noun.Examining the Table 5, we see that just label-ing a query ?noun-phrase?
results in an accuracy of69.8%.
Our grammar boosted this high baseline by14% to yield an final accuracy result of 83.3% at la-beling queries with their correct meta-type.
Thesemeta-types could be useful in deciding how to han-dle a query.
Further enhancements to the grammarwould likely yield a performance increase.
How-ever, we feel accuracy is currently high enough tocontinue with experiments towards application ofleveraging grammar-deduced query types for infor-mation retrieval.We can think of some of these meta-types aselided sentences.
For example, the noun-phrasequeries could be interpreted as requests of the form?how can I obtain X?
or ?where can I get informa-tion on X?, while the verb-phrase queries are re-quests of the form ?I would like to DO-X?.5 Applications of Part-of-Speech TaggingSince search queries are part of an information re-trieval task, we would like to demonstrate that part-of-speech tagging can assist with that task.
We con-ducted two experiments with a large-scale machine-learned web-search ranking system.
In addition, weconsidered the applicability of part-of-speech tags tothe question of query reformulation.5.1 Web Search RankingWe worked with a proprietary experimental testbedin which features for predicting the relevance of aquery to a document can be tested in a machine-learning framework.
Features can take a wide va-riety of forms (boolean, real-valued, relational) andapply to a variety of scopes (the page, the query,or the combination).
These features are evaluatedagainst editorial judgements and ranked accordingto their significance in improving the relevance ofresults.
We evaluated two part-of-speech tag-basedfeatures in this testbed.The first experiment involved a simple query-levelfeature indicating whether the query contained anoun or a proper noun.
This feature was evaluatedon thousands of queries for the test.
At the conclu-sion of the test, this feature was found to be in thetop 13% of model features, ranked in order of signif-icance.
We believe this significance represents theimportance of recognizing the presence of a nounin a query and, of course, matching it.
Within thisexperimental testbed a statistically significant im-provement of information-retrieval effectiveness isnotoriously difficult to attain.
We did not see a sig-nificant improvement in this metric.
However, wefeel that our feature?s high ranking warrants report-ing and hints at a potentially genuine boost in re-trieval performance in a system less feature-rich.The second experiment was more involved and re-flected more of our intuition about the likely applica-tion of part-of-speech tagging to the improvement ofsearch results.
In this experiment, we part-of-speechtagged both queries and documents.
Documentswere tagged with a conventionally trained Brill tag-ger with the resulting Penn-style tags mapped to ourtag set.
Many thousands of query-document pairswere processed in this manner.
The feature wasbased on the percent of times the part-of-speech tagof a word in the query matched the part-of-speechtag of the same word in the document.
This featurewas ranked in the top 12% by significance, thoughwe again saw no statistically significant increase inoverall retrieval performance.5.2 Query ReformulationWe considered the application of part-of-speech tag-ging to the problem of query reformulation, in which1028Part-of-speech p(subst) subst / seenNumber 0.49 148 / 302Adjective 0.46 2877 / 6299Noun 0.42 15038 / 35515Proper noun 0.39 21478 / 55331Gerund 0.37 112 / 300Verb 0.31 1769 / 5718Pronoun 0.23 300 / 1319Conjunction 0.18 85 / 464Adverb 0.13 105 / 790Determiner 0.10 22 / 219Preposition 0.08 369 / 4574Possessive 0.08 25 / 330Not 0.03 1 / 32Symbol 0.02 16 / 879Other 0.02 78 / 3294Sentence-ender 0.01 3 / 234Comma 0.00 4 / 991Table 6: Probability of a word being reformulated fromone query to the next, by part-of-speech tag.
Whileproper-nouns are the most frequent tag in our corpus, ad-jectives are more frequently reformulated, reflecting thefact that the proper nouns carry the core meaning of thequery.a single word in the query is altered within thesame user session.
We used a set of automaticallytagged queries to calculate change probabilities ofeach word by part-of-speech tag and the results areshown in Table 6.The type of word most likely to be reformu-lated is ?number.?
Examples included changing ayear (?most popular baby names 2007?
?
?mostpopular baby names 2008?
), while others includedmodel, version and edition numbers (?harry potter6?
?
?harry potter 7?)
most likely indicating thatthe user is looking at variants on a theme, or cor-recting their search need.
Typically a number is amodifier of the core search meaning.
The next mostcommonly changed type was ?adjective,?
perhapsindicating that adjectives can be used to refine, butnot fundamentally alter, the search intent.
Nounsand proper nouns are the next most commonly mod-ified types, perhaps reflecting user modification oftheir search need, refining the types of documentsretrieved.
Other parts of speech are relatively sel-dom modified, perhaps indicating that they are notviewed as having a large impact on the documentsretrieved.We can see from the impact of the search engineranking features and from the table of query refor-mulation likelihood that making use of the grammat-ical structure of search queries can have an impacton result relevance.
It can also assist with tasks as-sociated with improving recall, such as query refor-mulation.6 ConclusionWe have quantified, through a lexicostatistical anal-ysis, fundamental differences between the naturallanguage used in standard English-language corporaand English search-engine queries.
These differ-ences include reduced granularity in part-of-speechclasses as well as the dominance of the noun classesin queries at the expense of classes such as verbsfrequently found in traditional corpora.
In addi-tion, we have demonstrated the poor performance oftaggers trained on traditional corpora when appliedto search-engine queries, and how this poor perfor-mance can be overcome through query-based cor-pora.
We have suggested that greater improvementcan be achieved by proper-noun detection throughincorporation of external lexicons or entity detec-tion.
Finally, in preliminary investigations into ap-plications of our findings, we have shown that querypart-of-speech tagging can be used to create signif-icant features for improving the relevance of websearch results and may assist with query reformu-lation.
Improvements in accuracy can only increasethe value of POS information for these applications.We believe that query grammar can be further ex-ploited to increase query understanding and that thisunderstanding can improve the overall search expe-rience.ReferencesJames Allan and Hema Raghavan.
2002.
Using part-of-speech patterns to reduce query ambiguity.
In Pro-ceedings of SIGIR, pages 307?314.Kevin Bartz, Cory Barr, and Adil Aijaz.
2008.
Natu-ral language generation in sponsored-search advertise-ments.
In Proceedings of the 9th ACM Conference onElectronic Commerce, pages 1?9, Chicago, Illinois.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case1029study in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Abdur Chowdhury and M. Catherine McCabe.
2000.Improving information retrieval systems using part ofspeech tagging.Fabio Crestani, Mark Sanderson, and Mounia Lalmas.1998.
Short queries, natural language and spoken doc-ument retrieval: Experiments at glasgow university.In Proceedings of the Sixth Text Retrieval Conference(TREC-6), pages 667?686.Erika F. de Lima and Jan O. Pederson.
1999.
Phraserecognition and expansion for short, precision-biasedqueries based on a query log.
In Annual ACM Con-ference on Research and Development in Informa-tion Retrieval Proceedings of the 22nd annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, pages 145?152,Berkeley, California.Bekir Taner Dincer and Bahar Karaoglan.
2004.
Theeffect of part-of-speech tagging on ir performance forturkish.
pages 771?778.Bernard J. Jansen, Amanda Spink, and Tefko Saracevic.2000.
Real life, real users, and real needs: a studyand analysis of user queries on the web.
InformationProcessing and Management, 36(2):207?227.Christina Amalia Lioma.
2008.
Part of speech N-gramsfor information retrieval.
Ph.D. thesis, University ofGlasgow, Glasgow, Scotland, UK.Marius Pasca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In CIKM,pages 683?690.Amanda Spink, B. J. Jansen, D. Wolfram, and T. Sarace-vic.
2002.
From e-sex to e-commerce: Web searchchanges.
IEEE Computer, 35(3):107?109.Tomek Strzalkowski, Jose Perez Carballo, and MihneaMarinescu.
1998.
Natural language information re-trieval: Trec-3 report.
In Proceedings of the Sixth TextRetrieval Conference (TREC-6), page 39.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora(EMNLP/VLC-2000).Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL, pages 252?259.Ingrid Zukerman and Bhavani Raskutti.
2002.
Lexicalquery paraphrasing for document retrieval.
In COL-ING, pages 1177?1183, Taipei, Taiwan.1030
