Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190?199,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSubjectivity Word Sense DisambiguationCem Akkaya and Janyce WiebeUniversity of Pittsburgh{cem,wiebe}@cs.pitt.eduRada MihalceaUniversity of North Texasrada@cs.unt.eduAbstractThis paper investigates a new task, subjec-tivity word sense disambiguation (SWSD),which is to automatically determine whichword instances in a corpus are being usedwith subjective senses, and which are be-ing used with objective senses.
We pro-vide empirical evidence that SWSD ismore feasible than full word sense dis-ambiguation, and that it can be exploitedto improve the performance of contextualsubjectivity and sentiment analysis sys-tems.1 IntroductionThe automatic extraction of opinions, emotions,and sentiments in text (subjectivity analysis) tosupport applications such as product review min-ing, summarization, question answering, and in-formation extraction is an active area of researchin NLP.Many approaches to opinion, sentiment, andsubjectivity analysis rely on lexicons of words thatmay be used to express subjectivity.
Examples ofsuch words are the following (in bold):(1) He is a disease to every team he has gone to.Converting to SMF is a headache.The concert left me cold.That guy is such a pain.Knowing the meaning (and thus subjectivity) ofthese words would help a system recognize thenegative sentiments in these sentences.Most subjectivity lexicons are compiled as listsof keywords, rather than word meanings (senses).However, many keywords have both subjectiveand objective senses.
False hits ?
subjectivityclues used with objective senses ?
are a signifi-cant source of error in subjectivity and sentimentanalysis.
For example, even though the follow-ing sentence contains all of the negative keywordsabove, it is nevertheless objective, as they are allfalse hits:(2) Early symptoms of the disease include severeheadaches, red eyes, fevers and cold chills, bodypain, and vomiting.To tackle this source of error, we define anew task, subjectivity word sense disambigua-tion (SWSD), which is to automatically determinewhich word instances in a corpus are being usedwith subjective senses, and which are being usedwith objective senses.
We hypothesize that SWSDis more feasible than full word sense disambigua-tion, because it is more coarse grained ?
often, theexact sense need not be pinpointed.
We also hy-pothesize that SWSD can be exploited to improvethe performance of contextual subjectivity analy-sis systems via sense-aware classification.The paper consists of two parts.
In the firstpart, we build and evaluate a targeted supervisedSWSD system that aims to disambiguate membersof a subjectivity lexicon.
It labels clue instances ashaving a subjective sense or an objective sense incontext.
The system relies on common machinelearning features for word sense disambiguation(WSD).
The performance is substantially aboveboth baseline and the performance of full WSDon the same data, suggesting that the task is feasi-ble, and that subjectivity provides a natural coarse-grained grouping of senses.The second part demonstrates the promise ofSWSD for contextual subjectivity analysis.
First,we show that subjectivity sense ambiguity ishighly prevalent in the MPQA opinion-annotatedcorpus (Wiebe et al, 2005; Wilson, 2008), thusestablishing the potential benefit of performingSWSD.
Then, we exploit SWSD to improve per-formance on several subjectivity analysis tasks,from subjective/objective sentence-level classi-fication to positive/negative/neutral expression-level classification.
To our knowledge, this is the190first attempt to explicitly use sense-level subjec-tivity tags in contextual subjectivity and sentimentanalysis.2 BackgroundWe adopt the definitions of subjective and objec-tive from (Wiebe et al, 2005; Wiebe and Mi-halcea, 2006; Wilson, 2008).
Subjective expres-sions are words and phrases being used to ex-press mental and emotional states, such as spec-ulations, evaluations, sentiments, and beliefs.
Ageneral covering term for such states is privatestate (Quirk et al, 1985), an internal state thatcannot be directly observed or verified by others.
(Wiebe and Mihalcea, 2006) give the followingexamples:(3) His alarm grew.He absorbed the information quickly.UCC/Disciples leaders roundly condemned theIranian President?s verbal assault on Israel.What?s the catch?Polarity (also called semantic orientation) isalso important to NLP applications.
In reviewmining, for example, we want to know whetheran opinion about a product is positive or negative.Nonetheless, as argued by (Wiebe and Mihalcea,2006; Su and Markert, 2008), there are also mo-tivations for a separate subjective/objective (S/O)classification.First, expressions may be subjective but nothave any particular polarity.
An example given by(Wilson et al, 2005a) is Jerome says the hospi-tal feels no different than a hospital in the states.An NLP application system may want to find awide range of private states attributed to a person,such as their motivations, thoughts, and specula-tions, in addition to their positive and negative sen-timents.
Second, benefits for sentiment analysiscan be realized by decomposing the problem intoS/O (or neutral versus polar) and polarity classifi-cation (Yu and Hatzivassiloglou, 2003; Pang andLee, 2004; Wilson et al, 2005a; Kim and Hovy,2006).
We will see further evidence of this in Sec-tion 4.2.3 in this paper.The contextual subjectivity analysis experi-ments in Section 4 include both S/O and polarityclassifications.
The data used in those experimentsis from the MPQA Corpus (Wiebe et al, 2005;Wilson, 2008),1 which consists of texts from theworld press annotated for subjective expressions.1Available at http://www.cs.pitt.edu/mpqaIn the MPQA Corpus, subjective expressions ofvarying lengths are marked, from single words tolong phrases.
In addition, other properties are an-notated, including polarity.For SWSD, we need the notions of subjectiveand objective senses of words in a dictionary.
Weadopt the definitions from (Wiebe and Mihalcea,2006), who describe the annotation scheme as fol-lows.
Classifying a sense as S means that, whenthe sense is used in a text or conversation, one ex-pects it to express subjectivity, and also that thephrase or sentence containing it expresses subjec-tivity.
As noted in (Wiebe and Mihalcea, 2006),sentences containing objective senses may not beobjective.
Thus, objective senses are defined asfollows: Classifying a sense as O means that,when the sense is used in a text or conversation,one does not expect it to express subjectivity and,if the phrase or sentence containing it is subjective,the subjectivity is due to something else.
Finally,classifying a sense as B means it covers both sub-jective and objective usages.The following subjective examples are given in(Wiebe and Mihalcea, 2006):His alarm grew.alarm, dismay, consternation ?
(fear resulting from the aware-ness of danger)=> fear, fearfulness, fright ?
(an emotion experienced inanticipation of some specific pain or danger (usually ac-companied by a desire to flee or fight))What?s the catch?catch ?
(a hidden drawback; ?it sounds good but what?s thecatch??
)=> drawback ?
(the quality of being a hindrance; ?hepointed out all the drawbacks to my plan?
)They give the following objective examples:The alarm went off.alarm, warning device, alarm system ?
(a device that signalsthe occurrence of some undesirable event)=> device ?
(an instrumentality invented for a particu-lar purpose; ?the device is small enough to wear on yourwrist?
; ?a device intended to conserve water?
)He sold his catch at the market.catch, haul ?
(the quantity that was caught; ?the catch wasonly 10 fish?
)=> indefinite quantity ?
(an estimated quantity)Wiebe and Mihalcea performed an agreementstudy and report that good agreement (?=0.74) canbe achieved between human annotators labelingthe subjectivity of senses.
For a similar task, (Suand Markert, 2008) also report good agreement(?=0.79).1913 Subjectivity Word SenseDisambiguation3.1 Task Definition and MethodWe now turn to SWSD, and our method for per-forming it.Note that SWSD is midway between pure dic-tionary classification and pure contextual interpre-tation.
For SWSD, the context of the word is con-sidered in order to perform the task, but the sub-jectivity is determined solely by the dictionary.
Incontrast, full contextual interpretation can deviatefrom a sense?s subjectivity label in the dictionary.As noted above, words used with objective sensesmay appear in subjective expressions.
For exam-ple, an SWSD system would label the followingexamples of alarm as S, O and O, respectively.
Onthe other hand, a sentence-level subjectivity clas-sifier would label the sentences as S, S, and O, re-spectively.
(4) His alarm grew.Will someone shut that darn alarm off?The alarm went off.We use a supervised approach to SWSD.
Wetrain a different classifier for each lexicon entryfor which we have training data.
Thus, our ap-proach is like targeted WSD (in contrast to all-words WSD), with two labels: S and O.We borrow machine learning features whichhave been successfully used in WSD.
Specifically,given an ambiguous target word, we use the fol-lowing features from (Mihalcea, 2002):CW : the target word itselfCP : POS of the target wordCF : surrounding context of 3 words and their POSHNP : the head of the noun phrase to which thetarget word belongsNB : the first noun before the target wordVB : the first verb before the target wordNA : the first noun after the target wordVA : the first verb after the target wordSK : at most 10 context words occurring at least 5times; determined for each sense3.2 Lexicon and DataOur target words are members of a subjectivitylexicon, because, since they are in such a lexicon,we know they have subjective usages.
Specifically,we use the lexicon of (Wilson et al, 2005b; Wil-son, 2008).2 The entries have been divided into2Available at http://www.cs.pitt.edu/mpqathose that are strongly subjective (strongsubj) andthose that are weakly subjective (weaksubj), re-flecting their reliability as subjectivity clues.
Thesources of the entries in the lexicon are identifiedin (Wilson, 2008).
In the second part of this pa-per, we evaluate systems against the MPQA cor-pus.
Wilson also uses this corpus for her eval-uations.
To enable this, entries were added tothe lexicon independently from the MPQA corpus(that is, none of the entries were derived using theMPQA corpus).The training and test data for SWSD consists ofword instances in a corpus labeled as S or O, indi-cating whether they are used with a subjective orobjective sense.
Because we do not have data la-beled with the S/O coarse-grained senses and wedid not want to undertake the annotation effort atthis stage, we created an annotated corpus by com-bining two types of sense annotations: (1) labelsof senses within a dictionary as S or O (i.e., sub-jectivity sense labels), and (2) sense tags of wordinstances in a corpus (i.e., sense-tagged data).
Thesubjectivity sense labels are used to collapse thesense labels in the sense-tagged data into the twonew senses, S and O.Our sense-tagged data are the lexical samplecorpora (training and test data) from SENSEVAL1(Kilgarriff and Palmer, 2000), SENSEVAL2 (Preissand Yarowsky, 2001), and SENSEVAL3 (Mihal-cea and Edmonds, 2004).
We selected all of theSENSEVAL words that are also in the subjectivitylexicon, and labeled their dictionary senses as S,O, or B according to the annotation scheme de-scribed above in Section 2.
We did this subjectiv-ity sense labeling according to the sense inventoryof the underlying corpus (Hector for SENSEVAL1;WordNet1.7 for SENSEVAL2; and WordNet1.7.1for SENSEVAL3).Among the words, we found that 11 are notambiguous - either they have only S or only Osenses (in the corresponding sense inventory), orthe senses of their instances in the SENSEVAL dataare all S or all O.
So as not to inflate our results, weremoved those 11 from the data, leaving 39 words.In addition, we excluded the senses labeled B (a to-tal of 10 senses).
This leaves a total of 372 senses:9 words (64 senses) from SENSEVAL1, 18 words(201 senses) from SENSEVAL2, and 12 words (107senses) from SENSEVAL3.192Base Acc SP SR SF OP OR OF IB EB(%)All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0Table 1: Overall SWSD results (micro averages).
Base is majority-class baseline; Acc is accuracy; SP,SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF.
IB is absoluteimprovement in Acc over Base; EB is percent error reduction in Acc.3.3 SWSD ExperimentsIn this section, we evaluate our SWSD system, andcompare its performance to an WSD system on thesame data.Note that, although generally in the SENSEVALdatasets, training and test data are provided sep-arately, a few target words from SENSEVAL1 donot have both training and testing data.
Thus, weopted to combine the training and test data into onedataset, and then perform 10-fold cross validationexperiments.For our classifier, we use the SVM classifierfrom the Weka package (Witten and Frank., 2005)with its default settings.We were interested in how well the systemwould perform on more and less ambiguouswords.
Thus, we split the words into three sub-sets according to their majority-class baselines,and report separate results: S1 (9 words), S2 (18words), and S3 (12 words) have majority-classbaselines in the intervals [50%,70%) , [70%,90%),and [90%,100%), respectively.Table 1 contains the results, giving the overallresults (micro averages), as well as results for thesubsets S1, S2, and S3.The improvement for SWSD over baseline isespecially high for the less skewed set, S1.
Thisis very encouraging because these words are themore ambiguous words, and thus are the ones thatmost need SWSD (assuming the SENSEVAL pri-ors are similar to the priors in the corpus).
Theaverage error reduction over baseline for S1 wordsis 54.2%.
Even for the more skewed sets S2 andS3, reductions are 32.8% and 28.0%, respectively,with an overall reduction of 41.8%.To compare SWSD with WSD, we re-ran the10-fold cross validation experiments, but this timeusing the original sense labels, rather than Sand O.
The (micro-averaged) accuracy is 67.9%,much lower than the overall accuracy for SWSD(88.3%).The positive results provide evidence thatSWSD is a feasible variant of WSD, and that theS/O sense groupings are natural ones, since thesystem is able to learn to distinguish between themwith high accuracy.
There is also potential for im-provement by using a richer feature set, includingsubjectivity features.4 Opinion Analysis with SubjectivityWord Sense DisambiguationIn this section, we explore the promise of SWSDfor contextual subjectivity analysis.
First, we pro-vide evidence that a subjectivity lexicon can havesubstantial coverage of the subjective expressionsin a corpus, yet still be responsible for significantsubjectivity sense ambiguity in that corpus.
Then,we exploit SWSD in several contextual opinionanalysis systems, comparing the performance ofsense-aware and non-sense-aware versions.
Theyare all variations of components of the Opinion-Finder opinion recognition system.34.1 Coverage and Ambiguity of LexiconEntries in the MPQA CorpusIn this section, we consider the distribution of lex-icon entries in the MPQA corpus.The lexicon covers a substantial subset of thesubjective expressions in the corpus: 67.1% of thesubjective expressions contain one or more lexi-con entries.On the other hand, fully 42.9% of the instancesof the lexicon entries in the MPQA corpus arenot in subjective expressions.
An instance thatis not in a subjective expression is, by definition,being used with an objective sense.
Thus, theseinstances are false hits of subjectivity clues.
Asmentioned above, the entries in the lexicon havebeen pre-classified as either more (strongsubj) orless (weaksubj) reliable.
We see this difference re-flected in their degree of ambiguity ?
53% of the3Available at http://www.cs.pitt.edu/opin193weaksubj instances are false hits, while only 22%of the strongsubj instances are.The high coverage of the lexicon demonstratesits potential usefulness for opinion analysis sys-tems, while its degree of ambiguity, in the form offalse hits in a subjectivity annotated corpus, showsthe potential benefit to opinion analysis of per-forming SWSD.As mentioned above, our experiments involveonly lexicon entries that are covered by the SEN-SEVAL data, as we did not perform manual sensetagging for this work.
We have hope to expandthe system?s coverage in the future, as more word-sense tagged data is produced (e.g., ONTONOTES(Hovy et al, 2006)).
We also have evidence that amoderate amount of manual annotation would beworth the effort.
For example, let us order the lexi-con entries from highest to lowest by frequency inthe MPQA corpus.
The top 20 are responsible for25% of all false hits in the corpus; the top 40 areresponsible for 34%; and the top 80 are responsi-ble for 44%.
If the SWSD system could be trainedfor these words, the potential impact on reducingfalse hits could be substantial, especially consid-ering the good performance of the SWSD systemon the more ambiguous words.
Note that we donot want to simply discard these clues.
The top 20cover 9.4% of all subjective expressions; the top40 cover 15.4%; and the top 80 cover 29.5%.
Notethat SWSD only needs the data annotated with thecoarse-grained binary labels, which should be lesstime consuming to produce than full word sensetags.4.2 Contextual ClassificationWe found in Section 3.3 that SWSD is a feasibletask and then in Section 4.1 that there is a greatdeal of subjectivity sense ambiguity in a standardsubjectivity-annotated corpus (MPQA).
We nowturn to exploiting the results of SWSD to automat-ically recognize subjectivity and sentiment in theMPQA corpus.A motivation for using the MPQA data is thatmany types of classifiers have been evaluated onit, and we can directly test the effect of SWSD onthese classifiers.Note that, for the SWSD experiments, the num-ber of words does not limit the amount of data,as SENSEVAL provides data for each word.
How-ever, the only parts of the MPQA corpus for whichSWSD could affect performance is the subset con-taining instances of the words in the SWSD sys-tem?s coverage.
Thus, for the classifiers in thissection, the data used is the SenMPQA dataset,which consists of the sentences in the MPQA Cor-pus that contain at least one instance of the 39 key-words.
There are 689 such sentences (containing,in total, 723 instances of the 39 keywords).Even though this dataset is smaller than the oneused above, it gives us enough data to draw con-clusions according to McNemar?s test for statisti-cal significance.4.2.1 Rule-based ClassifierWe first apply SWSD to the rule-based classifierfrom (Riloff and Wiebe, 2003).
The classifier,which is a sentence-level S/O classifier, has lowsubjective and objective recall but high subjectiveand objective precision.
It is useful for creatingtraining data for subsequent processing by apply-ing it to large amounts of unannotated data.The classifier is a good candidate for directlymeasuring the effects of SWSD on contextual sub-jectivity analysis, because it classifies sentencesonly by looking for the presence of subjectivitykeywords.
Performance will improve if false hitscan be ignored.The classifier labels a sentence as S if it containstwo or more strongsubj clues.
On the other hand,it considers three conditions to classify a sentenceas O: there are no strongsubj clues in the currentsentence, there are together at most one strongsubjclue in the previous and next sentence, and thereare together at most 2 weaksubj clues in the cur-rent, previous, and next sentence.
A sentence thatis not labeled S or O is labeled unknown.The rule-based classifier is made sense awareby making it blind to the target word instances la-beled O by the SWSD system, as these representfalse hits of subjectivity keywords.
We comparethis sense-aware method (SE), with the originalclassifier (ORB), in order to see if SWSD wouldimprove performance.
We also built another modi-fied rule-based classifier RE to demonstrate the ef-fect of randomly ignoring subjectivity keywords.RE ignores a keyword instance randomly with aprobability of 0.429, the expected value of falsehits in the MPQA corpus.
The results are listed inTable 2.The rule-based classifier looks for the presenceof the keywords to find subjective sentences andfor the absence of the keywords to find objectivesentences.
It is obvious that a variant working on194Acc OP OR OF SP SR SFORB27.0 50.0 4.1 7.6 92.7 36.0 51.8SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2Table 2: Effect of SWSD on the rule-based classi-fiers.fewer keyword instances than ORBwill alwayshave the same or higher objective recall and thesame or lower subjective recall than ORB.
That isthe case for both SE and RE.
The real benefit wesee is in objective precision, which is substantiallyhigher for SE than ORB.
For our experiments, OPgives a better idea of the impact of SWSD, be-cause most of the keyword instances SWSD dis-ambiguates are weaksubj clues, and weaksubj key-words figure more prominently in objective classi-fication.
On the other hand, RE has both lower OPand SP than ORB.
Note that accuracy for all threesystems is low, because all unknown predictionsare counted as incorrect.These findings suggest that SWSD performswell on disambiguating keyword instances in theMPQA corpus,4 and demonstrates a positive im-pact of SWSD on sentence-level subjectivity clas-sification.4.2.2 Subjective/Objective ClassifierWe now move to more fine-grained expression-level subjectivity classification.
Since sentencesoften contain multiple subjective expressions,expression-level classification is more informativethan sentence-level classification.The classifier in this section is an implementa-tion of the neutral/polar supervised classifier of(Wilson et al, 2005a) (using the same features),except that the classes are S/O rather than neu-tral/polar.
These classifiers label instances of lex-icon entries.
The gold standard is defined on theMPQA Corpus as follows: If an instance is in asubjective expression, it is contextually S. If theinstance is in an objective expression, it is contex-tually O.
We evaluate the system on the 723 clueinstances in the SenMPQA dataset.We incorporate SWSD information into thecontextual subjectivity classifier in a straight-forward fashion: outputs are modified accordingto simple, intuitive rules.4which we cannot evaluate directly, as the MPQA corpusis not sense tagged.Our strategy is defined by the relation betweensense subjectivity and contextual subjectivity andinvolves two rules, R1 and R2.We know that a keyword instance used with aS sense must be in a subjective expression.
R1 isto simply trust SWSD: If the contextual classifierlabels an instance as O, but SWSD determines thatit has an S sense, then R1 flips the contextual clas-sifier?s label to S.Things are not as simple in the case of O senses,since they may appear in both subjective and ob-jective expressions.
We will state R2, and then ex-plain it: If the contextual classifier labels an in-stance as S, but (1) SWSD determines that it hasan O sense, (2) the contextual classifier?s confi-dence is low, and (3) there is no other subjectivekeyword in the same expression, then R2 flips thecontextual classifier?s label to O.
First, considerconfidence: though a keyword with an O sensemay appear in either subjective or objective ex-pressions, it is more likely to appear in an objec-tive expression.
We assume that this is reflectedto some extent in the contextual classifier?s confi-dence.
Second, if a keyword with an O sense ap-pears in a subjective expression, then the subjec-tivity is not due to that keyword but rather due tosomething else.
Thus, the presence of another lex-icon entry ?explains away?
the presence of the Osense in the subjective expression, and we do notwant SWSD to overrule the contextual classifier.Only when the contextual classifier isn?t certainand only when there isn?t another keyword doesR2 flip the label to O.Our definition of low confidence is in termsof the label weights assigned by BoosTexter(Schapire and Singer, 2000), which is the under-lying machine learning algorithm of the classifier.We use the difference between the largest labelweight and the second largest label weight as ameasure of confidence, as suggested in the Boos-Texter documentation.
The threshold we use is0.0008.5We apply the contextual classifier and theSWSD system to the data, and compare the per-formance of the original system (OS/O) and threesense-aware variants: one using only R1, one us-5As will be noted below, we experimented with threethresholds for the classifier in Section 4.2.3, with no signif-icant difference in accuracy.
Here, we simply adopt 0.0008,without further experimentation.
In addition, we did not ex-periment with other conditions than those incorporated in thetwo rules in this section and the two rules in Section 4.2.3below.195Acc OP OR OF SP SR SFOS/O75.4 68.0 62.9 65.4 79.2 82.7 80.9R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8Table 3: Effect of SWSD on the subjec-tive/objective classifiering only R2, and one using both (R1R2).
The re-sults are in Table 3.
The R1 variant shows an im-provement of 2.3 points in accuracy (a 9.4% errorreduction).
The R2 variant shows an improvementof 3.6 points in accuracy (a 14.6% error reduc-tion).
Applying both rules (R1R2) gives an im-provement of 5.9 percentage points in accuracy (a24% error reduction).In our case, a paired t-test is not appropriateto measure statistical significance, as we are notdoing multiple runs.
Thus, we apply McNemar?stest, which is a non-parametric method for algo-rithms that can be executed only once, meaningtraining once and testing once (Dietterich, 1998).For R1, the improvement in accuracy is statisti-cally significant at the p < .05 level.
For R2 andR1R2, the improvement in accuracy is statisticallysignificant at the p < .01 level.
Moreover, in allcases, we see improvement in both objective andsubjective F-measure.4.2.3 Contextual Polarity ClassifierWe now apply SWSD to contextual polarity clas-sification (positive/negative/neutral), in the hopethat avoiding false hits of subjectivity keywordswill also lead to performance improvement in con-textual sentiment analysis.We use an implementation of the classifier of(Wilson et al, 2005a).
This classifier labels in-stances of lexicon entries.
The gold standard isdefined on the MPQA Corpus as follows: If aninstance is in a positive subjective expression, itis contextually positive (Ps); if in a negative sub-jective expression, it is contextually negative (Ng);and if it is in an objective expression or a neu-tral subjective expression, then it is contextuallyN(eutral).
As above, we evaluate the system onthe keyword instances in the SenMPQA dataset.Wilson et al use a two step approach.
The firststep classifies keyword instances as being in a po-lar (positive or negative) or a neutral context.
Thefirst step is performed by the neutral/polar classi-fier mentioned above in Section 4.2.2.
The sec-ond step decides the contextual polarity (positiveor negative) of the instances classified as polar inthe first step, and is performed by a separate clas-sifier.To make a sense-aware version of the system,we use rules to change some of the answers of theneutral/polar classifier.Unfortunately, we cannot simply trust SWSDwhen it labels a keyword as an S sense, because anS sense might be in a N(eutral) expression (sincethere are neutral subjective expressions).
But, anS sense is more likely to appear in a P(olar) ex-pression.
Thus, we consider confidence (rule R3):If the contextual classifier labels an instance as N,but SWSD determines it has an S sense and thecontextual classifier?s confidence is low,6 then R3flips the contextual classifier?s label to P.Rule R4 is analogous to R2 in the previous sec-tion: If the contextual classifier labels an instanceas P, but (1) SWSD determines that it has an Osense, (2) the contextual classifier?s confidence islow, and (3) there is no other subjective keyword inthe same expression, then R2 flips the contextualclassifier?s label to N.We compare the performance of the originalneutral/polar classifier (ON/P) and sense-awarevariants using R3 and R4.
The results are in Table4.
This time, the table does not include a combinedmethod, because only R4 improves performance.This is consistent with the finding in (Wilson etal., 2005a) that most errors are caused by subjec-tivity keywords with non-neutral prior polarity ap-pearing in phrases with neutral contextual polarity.R4 targets these cases.
It is promising to see thatSWSD provides enough information to fix some ofthem.
There is a 2.6 point improvement in accu-racy (a 12.4% error reduction).
The improvementin accuracy is statistically significant at the p <.01 level with McNemar?s test.
The improvementin accuracy is accompanied by improvements inboth neutral and polar F-measure.We wanted to see if the improvements in the6As in the previous section, low confidence is definedin terms of the difference between the largest label weightand the second largest label weight assigned by BoosTexter.We tried three thresholds, 0.0007, 0.0008, and 0.0009, re-sulting in only a slight difference in accuracy: 0.0007 and0.0009 both give 81.5 accuracy compared to 81.6 accuracyfor 0.0008.
We report results using 0.0008, though the ac-curacy using the other thresholds is statistically significantlybetter than the accuracy of the original classifier at the samelevel.196Acc NP NR NF NgP NgR NgF PsP PsR PsFOPs/Ng/N77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0Table 5: Effect of SWSD on the contextual polarity classifierAcc NP NR NF PP PR PFON/P79.0 81.5 92.5 86.7 65.8 40.7 50.3R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3Table 4: Effect of SWSD on the neutral/polar clas-sifierfirst step of Wilson et als system can be propa-gated to their second step, yielding an overall im-provement in positive /negative/neutral (Ps/Ng/N)classification.The sense-aware variant of the overall two-partsystem is the same as the original except that weapply R4 to the output of the first step (flippingsome of the neutral/polar classifier?s P labels toN).
Thus, since the second step in Wilson et al?sclassifier processes only those instances labeled Pin the first step, in the sense-aware system, fewerinstances are passed from the first to the secondstep.Table 5 reports results for the original sys-tem (OPs/Ng/N) and the sense-aware variant (R4).These results are for the entire SenMPQA dataset,not just those labeled P in the first step.The accuracy improves 3 percentage points (a13.4% error reduction).
The improvement in accu-racy is statistically significant at the p < .01 levelwith McNemar?s test.
We see the real benefit whenwe look at the precision of the positive and neg-ative classes.
Negative precision goes from 60.4to 82.1 and positive precision goes from 52.2 to68.6, with no loss in recall.
This is evidence thatthe SWSD system is doing a good job of removingsome false hits of subjectivity clues that harm theoriginal version of the system.5 Comparisons to Previous WorkSeveral researchers exploit lexical resources forcontextual subjectivity and sentiment analysis.These systems typically look for the presence ofsubjective or sentiment-bearing words in the text.They may rely only on this information (e.g.,(Turney, 2002; Whitelaw et al, 2005; Riloff andWiebe, 2003)), or they may combine it with addi-tional information as well (e.g., (Yu and Hatzivas-siloglou, 2003; Kim and Hovy, 2004; Bloom et al,2007; Wilson et al, 2005a)).
We apply SWSD tosome of those systems to show the effect of SWSDon contextual subjectivity and sentiment analysis.Another set of related work is on subjectivityand polarity labeling of word senses (e.g.
(Esuliand Sebastiani, 2006; Andreevskaia and Bergler,2006; Wiebe and Mihalcea, 2006; Su and Markert,2008)).
They label senses of words in a dictionary.In comparison, we label senses of word instancesin a corpus.Moreover, our work extends findings in (Wiebeand Mihalcea, 2006) and (Su and Markert, 2008).
(Wiebe and Mihalcea, 2006) demonstrates thatsubjectivity is a property that can be associatedwith word senses.
We show that it is a naturalgrouping of word senses and that it provides aprincipled way for clustering senses.
They alsodemonstrate that subjectivity helps with WSD.
Weshow that a coarse-grained WSD variant (SWSD)helps with subjectivity and sentiment analysis.Both (Wiebe and Mihalcea, 2006) and (Su andMarkert, 2008) show that even reliable subjectiv-ity clues have objective senses.
We demonstratethat this ambiguity is also prevalent in a corpus.Several researchers (e.g., (Palmer et al, 2004;Navigli, 2006; Snow et al, 2007; Hovy et al,2006)) work on reducing the granularity of senseinventories for WSD.
They aim for a more coarse-grained sense inventory to overcome performanceshortcomings related to fine-grained sense distinc-tions.
Our work is similar in the sense that wereduce all senses of a word to two senses (S/O).The difference is the criterion driving the group-ing.
Related work concentrates on syntactic andsemantic similarity between senses to group them.In contrast, our grouping is driven by subjectivitywith a specific application area in mind, namelysubjectivity and sentiment analysis.6 Conclusions and Future WorkWe introduced the task of subjectivity word sensedisambiguation (SWSD), and evaluated a super-vised method inspired by research in WSD.
The197system achieves high accuracy, especially onhighly ambiguous words, and substantially outper-forms WSD on the same data.
The positive resultsprovide evidence that SWSD is a feasible variantof WSD, and that the S/O sense groupings are nat-ural ones.We also explored the promise of SWSD for con-textual subjectivity analysis.
We showed that asubjectivity lexicon can have substantial coverageof the subjective expressions in the corpus, yetstill be responsible for significant sense ambiguity.This demonstrates the potential benefit to opin-ion analysis of performing SWSD.
We then ex-ploit SWSD in several contextual opinion analysissystems, including positive/negative/neutral senti-ment classification.
Improvements in performancewere realized for all of the systems.We plan several future directions which promiseto further increase the impact of SWSD on sub-jectivity and sentiment analysis.
We will manu-ally annotate a moderate number of strategicallychosen words, namely frequent ones which arehighly ambiguous.
In addition, we will add fea-tures to the SWSD system reflecting the subjec-tivity of the surrounding context.
Finally, thereare more sophisticated strategies to explore forimproving subjectivity and sentiment analysis viaSWSD than the simple, intuitive rules we beganwith in this paper.AcknowledgmentsThis material is based in part upon work supportedby National Science Foundation awards #0840632and #0840608.
Any opinions, findings, and con-clusions or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the views of the National ScienceFoundation.ReferencesA.
Andreevskaia and S. Bergler.
2006.
Mining word-net for a fuzzy sentiment: Sentiment tag extractionfrom wordnet glosses.
In (EACL-2006).K.
Bloom, N. Garg, and S. Argamon.
2007.
Extractingappraisal expressions.
In HLT-NAACL 2007, pages308?315, Rochester, NY.T.
G. Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning al-gorithms.
Neural Computation, 10:1895?1923.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion min-ing.
In (LREC-06), Genova, IT.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solu-tion.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Companion Vol-ume: Short Papers, New York City.A.
Kilgarriff and M. Palmer, editors.
2000.
Com-puter and the Humanities.
Special issue: SENSE-VAL.
Evaluating Word Sense Disambiguation pro-grams, volume 34, April.S.-M. Kim and E. Hovy.
2004.
Determining the senti-ment of opinions.
In (COLING 2004), pages 1267?1373, Geneva, Switzerland.S.-M. Kim and E. Hovy.
2006.
Identifying and analyz-ing judgment opinions.
In (HLT/NAACL-06), pages200?207, New York, New York.R.
Mihalcea and P. Edmonds, editors.
2004.
Pro-ceedings of SENSEVAL-3, Association for Compu-tational Linguistics Workshop, Barcelona, Spain.R.
Mihalcea.
2002.
Instance based learning withautomatic feature selection applied to Word SenseDisambiguation.
In Proceedings of the 19th Inter-national Conference on Computational Linguistics(COLING 2002), Taipei, Taiwan, August.R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-mance.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics, Sydney,Australia.M.
Palmer, O. Babko-Malaya, and H. T. Dang.
2004.Different sense granularities for different applica-tions.
In HLT-NAACL 2004 Workshop: 2nd Work-shop on Scalable Natural Language Understanding,Boston, Massachusetts.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In (ACL-04), pages 271?278, Barcelona, ES.
Association for ComputationalLinguistics.J.
Preiss and D. Yarowsky, editors.
2001.
Pro-ceedings of SENSEVAL-2, Association for Compu-tational Linguistics Workshop, Toulouse, France.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In (EMNLP-2003),pages 105?112, Sapporo, Japan.R.
E. Schapire and Y.
Singer.
2000.
BoosTexter: Aboosting-based system for text categorization.
Ma-chine Learning, 39(2/3):135?168.R.
Snow, S. Prakash, D. Jurafsky, and A. Ng.
2007.Learning to merge word senses.
In Proceedings ofthe Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL), Prague,Czech Republic.F.
Su and K. Markert.
2008.
From word to sense: acase study of subjectivity recognition.
In (COLING-2008), Manchester.198P.
Turney.
2002.
Thumbs up or thumbs down?
seman-tic orientation applied to unsupervised classificationof reviews.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), pages 417?424, Philadelphia.C.
Whitelaw, N. Garg, and S. Argamon.
2005.
Us-ing appraisal groups for sentiment analysis.
In Pro-ceedings of CIKM-05, the ACM SIGIR Conferenceon Information and Knowledge Management, Bre-men, DE.J.
Wiebe and R. Mihalcea.
2006.
Word sense and sub-jectivity.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, Syd-ney, Australia.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Anno-tating expressions of opinions and emotions in lan-guage.
Language Resources and Evaluation (for-merly Computers and the Humanities), 39(2/3):164?210.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005a.
Recog-nizing contextual polarity in phrase-level sentimentanalysis.
In (HLT/EMNLP-2005), pages 347?354,Vancouver, Canada.T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J.Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-han.
2005b.
OpinionFinder: A system for subjec-tivity analysis.
In Proc.
Human Language Technol-ogy Conference and Conference on Empirical Meth-ods in Natural Language Processing (HLT/EMNLP-2005) Companion Volume (software demonstration).T.
Wilson.
2008.
Fine-grained Subjectivity and Sen-timent Analysis: Recognizing the Intensity, Polarity,and Attitudes of private states.
Ph.D. thesis, Intelli-gent Systems Program, University of Pittsburgh.I.
Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques, SecondEdition.
Morgan Kaufmann, June.H.
Yu and V. Hatzivassiloglou.
2003.
Towards an-swering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-03), pages 129?136, Sapporo, Japan.199
