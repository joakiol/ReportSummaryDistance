Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe CUED HiFST System for the WMT10 Translation Shared TaskJuan Pino Gonzalo Iglesias?1 Adria` de GispertGraeme Blackwood Jamie Brunning William ByrneDepartment of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk?
Department of Signal Processing and Communications, University of Vigo, Vigo, SpainAbstractThis paper describes the Cambridge Uni-versity Engineering Department submis-sion to the Fifth Workshop on StatisticalMachine Translation.
We report results forthe French-English and Spanish-Englishshared translation tasks in both directions.The CUED system is based on HiFST, ahierarchical phrase-based decoder imple-mented using weighted finite-state trans-ducers.
In the French-English task, weinvestigate the use of context-dependentalignment models.
We also show thatlattice minimum Bayes-risk decoding isan effective framework for multi-sourcetranslation, leading to large gains in BLEUscore.1 IntroductionThis paper describes the Cambridge UniversityEngineering Department (CUED) system submis-sion to the ACL 2010 Fifth Workshop on Statis-tical Machine Translation (WMT10).
Our trans-lation system is HiFST (Iglesias et al, 2009a), ahierarchical phrase-based decoder that generatestranslation lattices directly.
Decoding is guidedby a CYK parser based on a synchronous context-free grammar induced from automatic word align-ments (Chiang, 2007).
The decoder is imple-mented with Weighted Finite State Transducers(WFSTs) using standard operations available inthe OpenFst libraries (Allauzen et al, 2007).
Theuse of WFSTs allows fast and efficient explorationof a vast translation search space, avoiding searcherrors in decoding.
It also allows better integrationwith other steps in our translation pipeline such as5-gram language model (LM) rescoring and latticeminimum Bayes-risk (LMBR) decoding.1Now a member of the Department of Engineering, Uni-versity of Cambridge, Cambridge, CB2 1PZ, U.K.# Sentences # Tokens # Types(A)Europarl+News-CommentaryFR 1.7 M 52.4M 139.7kEN 47.6M 121.6k(B)Europarl+News-Commentary+UNFR 8.7 M 277.9M 421.0kEN 241.4M 482.1k(C)Europarl+News-Commentary+UN+GigaFR 30.2 M 962.4M 2.4MEN 815.3M 2.7MTable 1: Parallel data sets used for French-to-English experiments.We participated in the French-English andSpanish-English translation shared tasks in eachtranslation direction.
This paper describes the de-velopment of these systems.
Additionally, we re-port multi-source translation experiments that leadto very large gains in BLEU score.The paper is organised as follows.
Section 2describes each step in the development of our sys-tem for submission, from pre-processing to post-processing.
Section 3 presents and discusses re-sults and Section 4 describes an additional experi-ment on multi-source translation.2 System DevelopmentWe built three French-English and two Spanish-English systems, trained on different portions ofthe parallel data sets available for this shared task.Statistics for the different parallel sets are sum-marised in Tables 1 and 2.
No additional paralleldata was used.
As will be shown, the largest paral-lel corpus gave the best results in French, but thiswas not the case in Spanish.2.1 Pre-processingThe data was minimally cleaned by replacingHTML-related metatags by their corresponding155# Sentences # Tokens # Types(A) Europarl + News-CommentarySP 1.7M 49.4M 167.2kEN 47.0M 122.7k(B) Europarl + News-Commentary + UNSP 6.5M 205.6M 420.8kEN 192.0M 402.8kTable 2: Parallel data sets used for Spanish-to-English experiments.UTF8 token (e.g., replacing ?&amp?
by ?&?)
asthis interacts with tokenization.
Data was then to-kenized and lowercased, so mixed case is added aspost-processing.2.2 AlignmentsParallel data was aligned using the MTTK toolkit(Deng and Byrne, 2005).
In the English-to-Frenchand English-to-Spanish directions, we traineda word-to-phrase HMM model with maximumphrase length of 2.
In the French to English andSpanish to English directions, we trained a word-to-phrase HMM Model with a bigram translationtable and maximum phrase length of 4.We also trained context-dependent alignmentmodels (Brunning et al, 2009) for the French-English medium-size (B) dataset.
The context ofa word is based on its part-of-speech and the part-of-speech tags of the surrounding words.
Thesetags were obtained by applying the TnT Tagger(Brants, 2000) for English and the TreeTagger(Schmid, 1994) for French.
Decision tree clus-tering based on optimisation of the EM auxiliaryfunction was used to group contexts that trans-late similarly.
Unfortunately, time constraints pre-vented us from training context-dependent modelsfor the larger (C) dataset.2.3 Language ModelFor each target language, we used the SRILMToolkit (Stolcke, 2002) to estimate separate 4-gram LMs with Kneser-Ney smoothing (Kneserand Ney, 1995), for each of the corpora listed inTables 3, 4 and 5.
The LM vocabulary was ad-justed to the parallel data set used.
The compo-nent models of each language pair were then in-terpolated to form a single LM for use in first-passtranslation decoding.
For French-to-English trans-lation, the interpolation weights were optimisedfor perplexity on a development set.Corpus # Sentences # TokensEU + NC + UN 9.0M 246.4MCNA 1.3M 34.8MLTW 12.9M 298.7MXIN 16.0M 352.5MAFP 30.4M 710.6MAPW 62.1M 1268.6MNYT 73.6M 1622.5MGiga 21.4M 573.8MNews 48.7M 1128.4MTotal 275.4M 6236.4MTable 3: English monolingual training corpora.Corpus # Sentences # TokensEU + NC + UN 9.0M 282.8AFP 25.2M 696.0MAPW 12.7M 300.6MNews 15.2M 373.5MGiga 21.4M 684.4MTotal 83.5 M 2337.3MTable 4: French monolingual training corpora.Corpus # Sentences # TokensNC + News 4.0M 110.8MEU + Gigaword (5g) 249.4M 1351.5MTotal 253.4 M 1462.3MTable 5: Spanish monolingual training corpora.The Spanish-English first pass LM was traineddirectly on the NC+News portion of monolingualdata, as we did not find improvements by usingEuroparl.
The second pass rescoring LM used allavailable data.2.4 Grammar Extraction and DecodingAfter unioning the Viterbi alignments, phrase-based rules of up to five source words in lengthwere extracted, hierarchical rules with up to twonon-contiguous non-terminals in the source sidewere then extracted applying the restrictions de-scribed in (Chiang, 2007).
For Spanish-Englishand French-English tasks, we used a shallow-1grammar where hierarchical rules are allowed tobe applied only once on top of phrase-based rules.This has been shown to perform as well as afully hierarchical grammar for a Europarl Spanish-English task (Iglesias et al, 2009b).For translation, we used the HiFST de-156coder (Iglesias et al, 2009a).
HiFST is a hierarchi-cal decoder that builds target word lattices guidedby a probabilistic synchronous context-free gram-mar.
Assuming N to be the set of non-terminalsand T the set of terminals or words, then we candefine the grammar as a set R = {Rr} of rulesRr : N ?
??r,?r?
/ pr, where N ?
N; and?, ?
?
{N ?
T}+.HiFST translates in three steps.
The first stepis a variant of the CYK algorithm (Chappelier andRajman, 1998), in which we apply hypothesis re-combination without pruning.
Only the sourcelanguage sentence is parsed using the correspond-ing source-side context-free grammar with rulesN ?
?.
Each cell in the CYK grid is specifiedby a non-terminal symbol and position: (N,x, y),spanning sx+y?1x on the source sentence s1...sJ .For the second step, we use a recursive algo-rithm to construct word lattices with all possi-ble translations produced by the hierarchical rules.Construction proceeds by traversing the CYK gridalong the back-pointers established in parsing.
Ineach cell (N,x, y) of the CYK grid, we build atarget language word lattice L(N,x, y) containingevery translation of sx+y?1x from every derivationheaded by N .
For efficiency, this lattice can usepointers to lattices on other cells of the grid.In the third step, we apply the word-based LMvia standard WFST composition with failure tran-sitions, and perform likelihood-based pruning (Al-lauzen et al, 2007) based on the combined trans-lation and LM scores.As explained before, we are using shallow-1 hi-erarchical grammars (de Gispert et al, 2010) inour experiments for WMT2010.
One very inter-esting aspect is that HiFST is able to build ex-act search spaces with this model, i.e.
there is nopruning in search that may lead to spurious under-generation errors.2.5 Parameter OptimisationMinimum error rate training (MERT) (Och, 2003)under the BLEU score (Papineni et al, 2001) opti-mises the weights of the following decoder fea-tures with respect to the newstest2008 develop-ment set: target LM, number of usages of theglue rule, word and rule insertion penalties, worddeletion scale factor, source-to-target and target-to-source translation models, source-to-target andtarget-to-source lexical models, and three binaryrule count features inspired by Bender et al (2007)indicating whether a rule occurs once, twice, ormore than twice in the parallel training data.2.6 Lattice RescoringOne of the advantages of HiFST is direct gener-ation of large translation lattices encoding manyalternative translation hypotheses.
These first-passlattices are rescored with second-pass higher-orderLMs prior to LMBR.2.6.1 5-gram LM Lattice RescoringWe build sentence-specific, zero-cutoff stupid-backoff (Brants et al, 2007) 5-gram LMs esti-mated over approximately 6.2 billion words forEnglish, 2.3 billion words for French, and 1.4 bil-lion words for Spanish.
For the English-Frenchtask, the second-pass LM training data is the samemonolingual data used for the first-pass LMs (assummarised in Tables 3, 4).
The Spanish second-pass 5-gram LM includes an additional 1.4 billionwords of monolingual data from the Spanish Giga-Word Second Edition (Mendonca et al, 2009) andEuroparl, which were not included in the first-passLM (see Table 5).2.6.2 LMBR DecodingMinimum Bayes-risk (MBR) decoding (Kumarand Byrne, 2004) over the full evidence spaceof the 5-gram rescored lattices was applied toselect the translation hypothesis that maximisesthe conditional expected gain under the linearisedsentence-level BLEU score (Tromble et al, 2008;Blackwood and Byrne, 2010).
The unigram preci-sion p and average recall ratio r were set as de-scribed in Tromble et al (2008) using the new-stest2008 development set.2.7 Hypothesis CombinationLinearised lattice minimum Bayes-risk decoding(Tromble et al, 2008) can also be used as an ef-fective framework for multiple lattice combination(de Gispert et al, 2010).
For the French-Englishlanguage pair, we used LMBR to combine transla-tion lattices produced by systems trained on alter-native data sets.2.8 Post-processingFor both Spanish-English and French-English sys-tems, the recasing procedure was performed withthe SRILM toolkit.
For the Spanish-English sys-tem, we created models from the GigaWord setcorresponding to each system output language.157Task Configuration newstest2008 newstest2009 newstest2010FR ?
ENHiFST (A) 23.4 26.4 ?HiFST (B) 24.0 27.3 ?HiFST (B)CD 24.2 27.6 28.0+5g+LMBR 24.6 28.4 28.9HiFST (C) 24.7 28.4 28.5+5g+LMBR 25.3 29.1 29.3LMBR (B)CD+(C) 25.6 29.3 29.6EN ?
FRHiFST (A) 22.5 24.2 ?HiFST (B) 23.4 24.8 ?HiFST (B)CD 23.3 24.8 26.7+5g+LMBR 23.7 25.3 27.1HiFST (C) 23.6 25.6 27.4+5g+LMBR 23.9 25.8 27.8LMBR (B)CD+(C) 24.2 26.1 28.2Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-referencelowercase IBM BLEU.
Bold results correspond to submitted systems.For the French-English system, the English modelwas trained using the monolingual News corpusand the target side of the News-Commentary cor-pus, whereas the French model was trained usingall available constrained French data.English, Spanish and French outputs were alsodetokenized before submission.
In French, wordsseparated by apostrophes were joined.3 Results and DiscussionFrench?English Language PairResults are reported in Table 6.
We can seethat using more parallel data consistently improvesperformance.
In the French-to-English direction,the system HiFST (B) improves over HiFST (A)by +0.9 BLEU and HiFST (C) improves overHiFST (B) by +1.1 BLEU on the newstest2009development set prior to any rescoring.
Thesame trend can be observed in the English-to-French direction (+0.6 BLEU and +0.8 BLEU im-provement).
The use of context dependent align-ment models gives a small improvement in theFrench-to-English direction: system (B)CD im-proves by +0.3 BLEU over system (B) on new-stest2009.
In the English-to-French direction,there is no improvement nor degradation in per-formance.
5-gram and LMBR rescoring also giveconsistent improvement throughout the datasets.Finally, combination between the medium-sizesystem (B)CD and the full-size system (C) givesfurther small gains in BLEU over LMBR on eachindividual system.Spanish?English Language PairResults are reported in Table 7.
We report experi-mental results on two systems.
The HiFST(A) sys-tem is built on the Europarl + News-Commentarytraining set.
Systems HiFST (B),(B2) and (B3)use UN data in different ways.
System (B) simplyuses all the data for the standard rule extractionprocedure.
System HiFST (B2) includes UN datato build alignment models and therefore reinforcealignments obtained from smaller dataset (A), butextracts rules only from dataset (A).
HiFST (B3)combines hierarchical phrases extracted for sys-tem (A) with phrases extracted from system (B).Unfortunately, these three larger data strategieslead to degradation over using only the smallerdataset (A).
For this reason, our best systems onlyuse the Euparl + News-Commentary parallel data.This is surprising given that additional data washelpful for the French-English task.
Solving thisissue is left for future work.4 Multi-Source Translation ExperimentsMulti-source translation (Och and Ney, 2001;Schroeder et al, 2009) is possible whenever mul-tiple translations of the source language input sen-tence are available.
The motivation for multi-source translation is that some of the ambiguitythat must be resolved in translating between onepair of languages may not be present in a differ-ent pair.
In the following experiments, multipleLMBR is applied for the first time to the task ofmulti-source translation.158Task Configuration newstest2008 newstest2009 newstest2010SP ?
ENHiFST (A) 24.6 26.0 29.1+5g+LMBR 25.4 27.0 30.5HiFST (B) 23.7 25.4 ?HiFST (B2) 24.3 25.7 ?HiFST (B3) 24.2 25.6 ?EN ?
SP HiFST (A) 23.9 24.5 28.0+5g+LMBR 24.7 25.5 29.1Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBMBLEU.
Bold results correspond to submitted systems.Configuration newstest2008 newstest2009 newstest2010FR?EN HiFST+5g 24.8 28.5 28.8+LMBR 25.3 29.0 29.2ES?EN HiFST+5g 25.2 26.8 30.1+LMBR 25.4 26.9 30.3FR?EN + ES?EN LMBR 27.2 30.4 32.0Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translationof French (FR) and Spanish (ES) into English (EN).Separate second-pass 5-gram rescored latticesEFR and EES are generated for each test set sen-tence using the French-to-English and Spanish-to-English HiFST translation systems.
The MBR hy-pothesis space is formed as the union of these lat-tices.
In a similar manner to MBR decoding overmultiple k-best lists in de Gispert et al (2009),the path posterior probability of each n-gram u re-quired for linearised LMBR is computed as a lin-ear interpolation of the posterior probabilities ac-cording to each individual lattice so that p(u|E) =?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is thesum of the posterior probabilities of all paths con-taining the n-gram u.
The interpolation weights?FR + ?ES = 1 are optimised for BLEU score onthe development set newstest2008.The results of single-system and multi-sourceLMBR decoding are shown in Table 8.
The opti-mised interpolation weights were ?FR = 0.55 and?ES = 0.45.
Single-system LMBR gives relativelysmall gains on these test sets.
Much larger gainsare obtained through multi-source MBR combina-tion.
Compared to the best of the single-system 5-gram rescored lattices, the BLEU score improvesby +2.0 for newstest2008, +1.9 for newstest2009,and +1.9 for newstest2010.
For scoring with re-spect to a single reference, these are very largegains indeed.5 SummaryWe have described the CUED submission toWMT10 using HiFST, a hierarchical phrase-basedtranslation system.
Results are very competitive interms of automatic metric for both English-Frenchand English-Spanish tasks in both directions.
Inthe French-English task, we have seen that the UNand Giga additional parallel data are helpful.
Itis surprising that UN data did not help for theSpanish-English language pair.Future work includes investigating this issue,developing detokenization tailored to each outputlanguage and applying context dependent align-ment models to larger parallel datasets.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7-ICT-2009-4) under grantagreement number 247762, and was supported inpart by the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-C-0022.
Gonzalo Iglesias was sup-ported by the Spanish Government research grantBES-2007-15956 (projects TEC2006-13694-C03-03 and TEC2009-14094-C04-04).159ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of CIAA, pages 11?23.Oliver Bender, Evgeny Matusov, Stefan Hahn, SasaHasan, Shahram Khadivi, and Hermann Ney.
2007.The RWTH Arabic-to-English spoken languagetranslation system.
In Proceedings of ASRU, pages396?401.Graeme Blackwood and William Byrne.
2010.
Ef-ficient Path Counting Transducers for MinimumBayes-Risk Decoding of Statistical Machine Trans-lation Lattices (to appear).
In Proceedings of theACL.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofEMNLP-ACL, pages 858?867.Thorsten Brants.
2000.
Tnt ?
a statistical part-of-speech tagger.
In Proceedings of ANLP, pages 224?231, April.Jamie Brunning, Adria` de Gispert, and William Byrne.2009.
Context-dependent alignment models forstatistical machine translation.
In Proceedings ofHLT/NAACL, pages 110?118.Jean-Ce?dric Chappelier and Martin Rajman.
1998.
Ageneralized CYK algorithm for parsing stochasticCFG.
In Proceedings of TAPD, pages 133?137.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Adria` de Gispert, Sami Virpioja, Mikko Kurimo, andWilliam Byrne.
2009.
Minimum Bayes Risk Com-bination of Translation Hypotheses from Alterna-tive Morphological Decompositions.
In Proceed-ings of HLT/NAACL, Companion Volume: Short Pa-pers, pages 73?76.Adria` de Gispert, Gonzalo Iglesias, Graeme Black-wood, Eduardo R. Banga, and William Byrne.
2010.Hierarchical phrase-based translation with weightedfinite state transducers and shallow-n grammars (toappear).
In Computational Linguistics.Yonggang Deng and William Byrne.
2005.
HMMWord and Phrase Alignment for Statistical MachineTranslation.
In Proceedings of HLT/EMNLP, pages169?176.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009a.
Hierarchical phrase-based translation with weighted finite state transduc-ers.
In Proceedings of NAACL, pages 433?441.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009b.
The HiFST System forthe EuroParl Spanish-to-English Task.
In Proceed-ings of SEPLN, pages 207?214.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of ICASSP, volume 1, pages 181?184.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 169?176.Angelo Mendonca, David Graff, and Denise DiPersio.2009.
Spanish Gigaword Second Edition, LinguisticData Consortium.Franz Josef Och and Hermann Ney.
2001.
Statisti-cal multi-source translation.
In Machine TranslationSummit 2001, pages 253?258.Franz J. Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, volume 12.
Manchester, UK.Josh Schroeder, Trevor Cohn, and Philipp Koehn.2009.
Word Lattices for Multi-Source Translation.In Proceedings of EACL, pages 719?727.Andreas Stolcke.
2002.
SRILM?An Extensible Lan-guage Modeling Toolkit.
In Proceedings of ICSLP,volume 3, pages 901?904.Roy W. Tromble, Shankar Kumar, Franz Och, andWolfgang Macherey.
2008.
Lattice MinimumBayes-Risk decoding for statistical machine trans-lation.
In Proceedings of EMNLP, pages 620?629.160
