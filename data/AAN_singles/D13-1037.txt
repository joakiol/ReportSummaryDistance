Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380?391,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLatent Anaphora Resolution for Cross-Lingual Pronoun PredictionChristian Hardmeier J?rg Tiedemann Joakim NivreUppsala UniversityDepartment of Linguistics and PhilologyBox 635, 751 26 Uppsala, Swedenfirstname.lastname@lingfil.uu.seAbstractThis paper addresses the task of predicting thecorrect French translations of third-person sub-ject pronouns in English discourse, a problemthat is relevant as a prerequisite for machinetranslation and that requires anaphora resolu-tion.
We present an approach based on neu-ral networks that models anaphoric links aslatent variables and show that its performanceis competitive with that of a system with sep-arate anaphora resolution while not requiringany coreference-annotated training data.
Thisdemonstrates that the information contained inparallel bitexts can successfully be used to ac-quire knowledge about pronominal anaphorain an unsupervised way.1 MotivationWhen texts are translated from one language intoanother, the translation reconstructs the meaning orfunction of the source text with the means of thetarget language.
Generally, this has the effect thatthe entities occurring in the translation and their mu-tual relations will display similar patterns as the enti-ties in the source text.
In particular, coreference pat-terns tend to be very similar in translations of a text,and this fact has been exploited with good results toproject coreference annotations from one languageinto another by using word alignments (Postolacheet al 2006; Rahman and Ng, 2012).On the other hand, what is true in general neednot be true for all types of linguistic elements.
Forinstance, a substantial percentage of the English third-person subject pronouns he, she, it and they doesnot get realised as pronouns in French translations(Hardmeier, 2012).
Moreover, it has been recognisedby various authors in the statistical machine transla-tion (SMT) community (Le Nagard and Koehn, 2010;Hardmeier and Federico, 2010; Guillou, 2012) thatpronoun translation is a difficult problem because,even when a pronoun does get translated as a pro-noun, it may require choosing the correct word formbased on agreement features that are not easily pre-dictable from the source text.The work presented in this paper investigatesthe problem of cross-lingual pronoun prediction forEnglish-French.
Given an English pronoun and itsdiscourse context as well as a French translation ofthe same discourse and word alignments betweenthe two languages, we attempt to predict the Frenchword aligned to the English pronoun.
As far as weknow, this task has not been addressed in the litera-ture before.
In our opinion, it is interesting for severalreasons.
By studying pronoun prediction as a task inits own right, we hope to contribute towards a betterunderstanding of pronoun translation with a long-term view to improving the performance of SMTsystems.
Moreover, we believe that this task can leadto interesting insights about anaphora resolution in amulti-lingual context.
In particular, we show in thispaper that the pronoun prediction task makes it possi-ble to model the resolution of pronominal anaphoraas a latent variable and opens up a way to solve atask relying on anaphora resolution without usingany data annotated for anaphora.
This is what weconsider the main contribution of our present work.We start by modelling cross-lingual pronoun pre-diction as an independent machine learning task afterdoing anaphora resolution in the source language(English) using the BART software (Broscheit etal., 2010).
We show that it is difficult to achievesatisfactory performance with standard maximum-380The latest version released in March is equipped with ...
It is sold at ...La derni?re version lanc?e en mars est dot?e de ... ?
est vendue ...Figure 1: Task setupentropy classifiers especially for low-frequency pro-nouns such as the French feminine plural pronounelles.
We propose a neural network classifier thatachieves better precision and recall and manages tomake reasonable predictions for all pronoun cate-gories in many cases.We then go on to extend our neural network archi-tecture to include anaphoric links as latent variables.We demonstrate that our classifier, now with its ownsource language anaphora resolver, can be trainedsuccessfully with backpropagation.
In this setup, weno longer use the machine learning component in-cluded in the external coreference resolution system(BART) to predict anaphoric links.
Anaphora reso-lution is done by our neural network classifier andrequires only some quantity of word-aligned paralleldata for training, completely obviating the need for acoreference-annotated training set.2 Task SetupThe overall setup of the classification task we addressin this paper is shown in Figure 1.
We are given anEnglish discourse containing a pronoun along withits French translation and word alignments betweenthe two languages, which in our case were computedautomatically using a standard SMT pipeline withGIZA++ (Och and Ney, 2003).
We focus on the fourEnglish third-person subject pronouns he, she, it andthey.
The output of the classifier is a multinomialdistribution over six classes: the four French subjectpronouns il, elle, ils and elles, corresponding to mas-culine and feminine singular and plural, respectively;the impersonal pronoun ce/c?, which occurs in somevery frequent constructions such as c?est (it is); anda sixth class OTHER, which indicates that none ofthese pronouns was used.
In general, a pronoun maybe aligned to multiple words; in this case, a trainingexample is counted as a positive example for a classif the target word occurs among the words alignedto the pronoun, irrespective of the presence of other0 0 0 1 0version0 1 0 0 0la0 0 1 0 0elle0 .5 0 .5 00 0 1 0 00 .05 .9 .05 0p1 = .9p2 =.1word candidate training ex.Figure 2: Antecedent feature aggregationaligned tokens.This task setup resembles the problem that anSMT system would have to solve to make informedchoices when translating pronouns, an aspect of trans-lation neglected by most existing SMT systems.
Animportant difference between the SMT setup and ourown classifiers is that we use context from human-made translations for prediction.
This potentiallymakes the task both easier and more difficult; easier,because the context can be relied on to be correctlytranslated, and more difficult, because human transla-tors frequently create less literal translations than anSMT system would.
Integrating pronoun predictioninto the translation process would require significantchanges to the standard SMT decoding setup in orderto take long-range dependencies in the target lan-guage into account, which is why we do not addressthis issue in our current work.In all the experiments presented in this paper, weused features from two different sources:?
Anaphora context features describe the sourcelanguage pronoun and its immediate contextconsisting of three words to its left and threewords to its right.
They are encoded as vec-tors whose dimensionality is equal to the sourcevocabulary size with a single non-zero compo-nent indicating the word referred to (one-hotvectors).?
Antecedent features describe an antecedent can-didate.
Antecedent candidates are representedby the target language words aligned to the syn-tactic head of the source language markable381TED Newsce 16.3 % 6.4 %elle 7.1 % 10.1 %elles 3.0 % 3.9 %il 17.1 % 26.5 %ils 15.6 % 15.1 %OTHER 40.9 % 38.0 %Table 1: Distribution of classes in the training datanoun phrase as identified by the Collins headfinder (Collins, 1999).The different handling of anaphora context featuresand antecedent features is due to the fact that we al-ways consider a constant number of context wordson the source side, whereas the number of wordvectors to be considered depends on the number ofantecedent candidates and on the number of targetwords aligned to each antecedent.The encoding of the antecedent features is illus-trated in Figure 2 for a training example with twoantecedent candidates translated to elle and la ver-sion, respectively.
The target words are representedas one-hot vectors with the dimensionality of the tar-get language vocabulary.
These vectors are then av-eraged to yield a single vector per antecedent candi-date.
Finally, the vectors of all candidates for a giventraining example are weighted by the probabilitiesassigned to them by the anaphora resolver (p1 andp2) and summed to yield a single vector per trainingexample.3 Data Sets and External ToolsWe run experiments with two different test sets.
TheTED data set consists of around 2.6 million tokens oflecture subtitles released in the WIT3 corpus (Cet-tolo et al 2012).
The WIT3 training data yields71,052 examples, which were randomly partitionedinto a training set of 63,228 examples and a test setof 7,824 examples.
The official WIT3 developmentand test sets were not used in our experiments.
Thenews-commentary data set is version 6 of the parallelnews-commentary corpus released as a part of theWMT 2011 training data1.
It contains around 2.8 mil-lion tokens of news text and yields 31,017 data points,1http://www.statmt.org/wmt11/translation-task.html (3 July 2013).which were randomly split into 27,900 training exam-ples and 3,117 test instances.
The distribution of theclasses in the two training sets is shown in Table 1.One thing to note is the dominance of the OTHERclass, which pools together such different phenom-ena as translations with other pronouns not in our list(e. g., celui-ci) and translations with full noun phrasesinstead of pronouns.
Splitting this group into moremeaningful subcategories is not straightforward andmust be left to future work.The feature setup of all our classifiers requiresthe detection of potential antecedents and the extrac-tion of features pairing anaphoric pronouns with an-tecedent candidates.
Some of our experiments alsorely on an external anaphora resolution component.We use the open-source anaphora resolver BART togenerate this information.
BART (Broscheit et al2010) is an anaphora resolution toolkit consisting ofa markable detection and feature extraction pipelinebased on a variety of standard natural language pro-cessing (NLP) tools and a machine learning com-ponent to predict coreference links including bothpronominal anaphora and noun-noun coreference.
Inour experiments, we always use BART?s markabledetection and feature extraction machinery.
Mark-able detection is based on the identification of nounphrases in constituency parses generated with theStanford parser (Klein and Manning, 2003).
The setof features extracted by BART is an extension of thewidely used mention-pair anaphora resolution featureset by Soon et al(2001) (see below, Section 6).In the experiments of the next two sections, wealso use BART to predict anaphoric links for pro-nouns.
The model used with BART is a maximumentropy ranker trained on the ACE02-npaper corpus(LDC2003T11).
In order to obtain a probability dis-tribution over antecedent candidates rather than one-best predictions or coreference sets, we modified theranking component with which BART resolves pro-nouns to normalise and output the scores assignedby the ranker to all candidates instead of picking thehighest-scoring candidate.4 Baseline ClassifiersIn order to create a simple, but reasonable baselinefor our task, we trained a maximum entropy (ME)382TED(Accuracy: 0.685)P R Fce 0.593 0.728 0.654elle 0.798 0.523 0.632elles 0.812 0.164 0.273il 0.764 0.550 0.639ils 0.632 0.949 0.759OTHER 0.724 0.692 0.708News commentary(Accuracy: 0.576)P R Fce 0.508 0.294 0.373elle 0.530 0.312 0.393elles 0.538 0.062 0.111il 0.600 0.666 0.631ils 0.593 0.769 0.670OTHER 0.564 0.609 0.586Table 2: Maximum entropy classifier resultsTED(Accuracy: 0.700)P R Fce 0.634 0.747 0.686elle 0.756 0.617 0.679elles 0.679 0.319 0.434il 0.719 0.591 0.649ils 0.663 0.940 0.778OTHER 0.743 0.678 0.709News commentary(Accuracy: 0.576)P R Fce 0.477 0.344 0.400elle 0.498 0.401 0.444elles 0.565 0.116 0.193il 0.655 0.626 0.640ils 0.570 0.834 0.677OTHER 0.567 0.573 0.570Table 3: Neural network classifier with anaphoras resolved by BARTclassifier with the MegaM software package2 usingthe features described in the previous section and theanaphora links found by BART.
Results are shownin Table 2.
The baseline results show an overallhigher accuracy for the TED data than for the news-commentary data.
While the precision is above 50 %in all categories and considerably higher in some,recall varies widely.The pronoun elles is particularly interesting.
Thisis the feminine plural of the personal pronoun, andit usually corresponds to the English pronoun they,which is not marked for gender.
In French, elles is amarked choice which is only used if the antecedentexclusively refers to females or feminine-genderedobjects.
The presence of a single item with mascu-line grammatical gender in the antecedent will triggerthe use of the masculine plural pronoun ils instead.This distinction cannot be predicted from the Englishsource pronoun or its context; making correct pre-dictions requires knowledge about the antecedent ofthe pronoun.
Moreover, elles is a low-frequency pro-noun.
There are only 1,909 occurrences of this pro-2http://www.umiacs.umd.edu/~hal/megam/ (20 June2013).noun in the TED training data, and 1,077 in the news-commentary training set.
Because of these specialproperties of the feminine plural class, we argue thatthe performance of a classifier on elles is a good indi-cator of how well it can represent relevant knowledgeabout pronominal anaphora as opposed to overfittingto source contexts or acting on prior assumptionsabout class frequencies.In accordance with the general linguistic prefer-ence for ils, the classifier tends to predict ils muchmore often than elles when encountering an Englishplural pronoun.
This is reflected in the fact that elleshas much lower recall than ils.
Clearly, the classifierachieves a good part of its accuracy by making ma-jority choices without exploiting deeper knowledgeabout the antecedents of pronouns.An additional experiment with a subset of 27,900training examples from the TED data confirms thatthe difference between TED and news commentariesis not just an effect of training data size, but that TEDdata is genuinely easier to predict than news com-mentaries.
In the reduced data TED condition, theclassifier achieves an accuracy of 0.673.
Precisionand recall of all classifiers are much closer to the383EPR1L1R2L2R3L3p3p2p1321HSAFigure 3: Neural network for pronoun predictionlarge-data TED condition than to the news commen-tary experiments, except for elles, where we obtainan F-score of 0.072 (P 0.818, R 0.038), indicatingthat small training data size is a serious problem forthis low-frequency class.5 Neural Network ClassifierIn the previous section, we saw that a simple multi-class maximum entropy classifier, while making cor-rect predictions for much of the data set, has a signifi-cant bias towards making majority class decisions, re-lying more on prior assumptions about the frequencydistribution of the classes than on antecedent featureswhen handling examples of less frequent classes.
Inorder to create a system that can be trained to relymore explicitly on antecedent information, we cre-ated a neural network classifier for our task.
The intro-duction of a hidden layer should enable the classifierto learn abstract concepts such as gender and numberthat are useful across multiple output categories, sothat the performance of sparsely represented classescan benefit from the training examples of the morefrequent classes.The overall structure of the network is shown inFigure 3.
As inputs, the network takes the same fea-tures that were available to the baseline ME classifier,based on the source pronoun (P) with three wordsof context to its left (L1 to L3) and three words toits right (R1 to R3) as well as the words aligned tothe syntactic head words of all possible antecedentcandidates as found by BART (A).
All words areencoded as one-hot vectors whose dimensionality isequal to the vocabulary size.
If multiple words arealigned to the syntactic head of an antecedent candi-date, their word vectors are averaged with uniformweights.
The resulting vectors for each antecedentare then averaged with weights defined by the pos-terior distribution of the anaphora resolver in BART(p1 to p3).The network has two hidden layers.
The first layer(E) maps the input word vectors to a low-dimensionalrepresentation.
In this layer, the embedding weightsfor all the source language vectors (the pronounand its 6 context words) are tied, so if two wordsare the same, they are mapped to the same lower-dimensional embedding irrespective of their positionrelative to the pronoun.
The embedding of the an-tecedent word vectors is independent, as these wordvectors represent target language words.
The entireembedding layer is then mapped to another hiddenlayer (H), which is in turn connected to a softmax out-put layer (S) with 6 outputs representing the classesce, elle, elles, il, ils and OTHER.
The non-linearity ofboth hidden layers is the logistic sigmoid function,f (x) = 1/(1+ e?x).In all experiments reported in this paper, the dimen-sionality of the source and target language word em-beddings is 20, resulting in a total embedding layersize of 160, and the size of the last hidden layer isequal to 50.
These sizes are fairly small.
In experi-ments with larger layer sizes, we were able to obtainsimilar, but no better results.384The neural network is trained with mini-batchstochastic gradient descent with backpropagated gra-dients using the RMSPROP algorithm with cross-entropy as the objective function.3 In contrast tostandard gradient descent, RMSPROP normalises themagnitude of the gradient components by dividingthem by a root-mean-square moving average.
Wefound this led to faster convergence.
Other featuresof our training algorithm include the use of momen-tum to even out gradient oscillations, adaptive learn-ing rates for each weight as well as adaptation ofthe global learning rate as a function of current train-ing progress.
The network is regularised with an `2weight penalty.
Good settings of the initial learningrate and the weight cost parameter (both around 0.001in most experiments) were found by manual experi-mentation.
Generally, we train our networks for 300epochs, compute the validation error on a held-outset of some 10 % of the training data after each epochand use the model that achieved the lowest validationerror for testing.Since the source context features are very infor-mative and it is comparatively more difficult to learnfrom the antecedents, the network sometimes had atendency to overfit to the source features and disre-gard antecedent information.
We found that this prob-lem can be solved effectively by presenting a part ofthe training without any source features, forcing thenetwork to learn from the information contained inthe antecedents.
In all experiments in this paper, wezero out all source features (input layers P, L1 to L3and R1 to R3) with a probability of 50 % in eachtraining example.
At test time, no information is ze-roed out.Classification results with this network are shownin Table 3.
We note that the accuracy has increasedslightly for the TED test set and remains exactly thesame for the news commentary corpus.
However, acloser look on the results for individual classes re-veals that the neural network makes better predictionsfor almost all classes.
In terms of F-score, the onlyclass that becomes slightly worse is the OTHER classfor the news commentary corpus because of lowerrecall, indicating that the neural network classifier isless biased towards using the uninformative OTHER3Our training procedure is greatly inspired by a series of on-line lectures held by Geoffrey Hinton in 2012 (https://www.coursera.org/course/neuralnets, 10 September 2013).category.
Recall for elle and elles increases consider-ably, but especially for elles it is still quite low.
Theincrease in recall comes with some loss in precision,but the net effect on F-score is clearly positive.6 Latent Anaphora ResolutionConsidering Figure 1 again, we note that the bilin-gual setting of our classification task adds some in-formation not available to the monolingual anaphoraresolver that can be helpful when determining thecorrect antecedent for a given pronoun.
Knowing thegender of the translation of a pronoun limits the setof possible antecedents to those whose translation ismorphologically compatible with the target languagepronoun.
We can exploit this fact to learn how toresolve anaphoric pronouns without requiring datawith manually annotated anaphoric links.To achieve this, we extend our neural network witha component to predict the probability of each an-tecedent candidate to be the correct antecedent (Fig-ure 4).
The extended network is identical to the previ-ous version except for the upper left part dealing withanaphoric link features.
The only difference betweenthe two networks is the fact that anaphora resolutionis now performed by a part of our neural networkitself instead of being done by an external moduleand provided to the classifier as an input.In this setup, we still use some parts of the BARTtoolkit to extract markables and compute features.However, we do not make use of the machine learn-ing component in BART that makes the actual pre-dictions.
Since this is the only component trained oncoreference-annotated data in a typical BART con-figuration, no coreference annotations are used any-where in our system even though we continue to relyon the external anaphora resolver for preprocessingto avoid implementing our own markable and featureextractors and to make comparison easier.For each candidate markable identified by BART?spreprocessing pipeline, the anaphora resolutionmodel receives as input a link feature vector (T) de-scribing relevant aspects of the antecedent candidate-anaphora pair.
This feature vector is generated by thefeature extraction machinery in BART and includesa standard feature set for coreference resolution par-tially based on work by Soon et al(2001).
We usethe following feature extractors in BART, each of385EHS123L3R3L2R2L1R1PTUV1 2 3AFigure 4: Neural network with latent anaphora resolutionwhich can generate multiple features:?
Anaphora mention type?
Gender match?
Number match?
String match?
Alias feature (Soon et al 2001)?
Appositive position feature (Soon et al 2001)?
Semantic class (Soon et al 2001)?
Semantic class match?
Binary distance feature?
Antecedent is first mention in sentenceOur baseline set of features was borrowed whole-sale from a working coreference system and includessome features that are not relevant to the task at hand,e.
g., features indicating that the anaphora is a pro-noun, is not a named entity, etc.
After removing allfeatures that assume constant values in the trainingset when resolving antecedents for the set of pro-nouns we consider, we are left with a basic set of 37anaphoric link features that are fed as inputs to ournetwork.
These features are exactly the same as thoseavailable to the anaphora resolution classifier in theBART system used in the previous section.Each training example for our network can havean arbitrary number of antecedent candidates, each ofwhich is described by an antecedent word vector (A)and by an anaphoric link vector (T).
The anaphoriclink features are first mapped to a regular hidden layerwith logistic sigmoid units (U).
The activations of thehidden units are then mapped to a single value, whichfunctions as an element in a softmax layer over all an-tecedent candidates (V).
This softmax layer assignsa probability to each antecedent candidate, which wethen use to compute a weighted average over the an-tecedent word vector, replacing the probabilities piin Figures 2 and 3.At training time, the network?s anaphora resolu-tion component is trained in exactly the same way asthe rest of the network.
The error signal from the em-bedding layer is backpropagated both to the weightmatrix defining the antecedent word embedding andto the anaphora resolution subnetwork.
Note that thenumber of weights in the network is the same forall training examples even though the number of an-tecedent candidates varies because all weights relatedto antecedent word features and anaphoric link fea-tures are shared between all antecedent candidates.One slightly uncommon feature of our neural net-work is that it contains an internal softmax layer togenerate normalised probabilities over all possibleantecedent candidates.
Moreover, weights are sharedbetween all antecedent candidates, so the inputs ofour internal softmax layer share dependencies onthe same weight variables.
When computing deriva-tives with backpropagation, these shared dependen-cies must be taken into account.
In particular, theoutputs yi of the antecedent resolution layer are the re-sult of a softmax applied to functions of some sharedvariables q:yi =exp fi(q)?k exp fk(q)(1)386The derivatives of any yi with respect to q, whichcan be any of the weights in the anaphora resolutionsubnetwork, have dependencies on the derivatives ofthe other softmax inputs with respect to q:?yi?q = yi(?
fi(q)?q ??kyk?
fk(q)?q)(2)This makes the implementation of backpropagationfor this part of the network somewhat more compli-cated, but in the case of our networks, it has no majorimpact on training time.Experimental results for this network are shownin Table 4.
Compared with Table 3, we note that theoverall accuracy is only very slightly lower for TED,and for the news commentaries it is actually better.When it comes to F-scores, the performance for ellesimproves by a small amount, while the effect on theother classes is a bit more mixed.
Even where it getsworse, the differences are not dramatic consideringthat we eliminated a very knowledge-rich resourcefrom the training process.
This demonstrates that itis possible, in our classification task, to obtain goodresults without using any data manually annotated foranaphora and to rely entirely on unsupervised latentanaphora resolution.7 Further ImprovementsThe results presented in the preceding section repre-sent a clear improvement over the ME classifiers inTable 2, even though the overall accuracy increasedonly slightly.
Not only does our neural network clas-sifier achieve better results on the classification taskat hand without requiring an anaphora resolution clas-sifier trained on manually annotated data, but it per-forms clearly better for the feminine categories thatreflect minority choices requiring knowledge aboutthe antecedents.
Nevertheless, the performance is stillnot entirely satisfactory.By subjecting the output of our classifier on a de-velopment set to a manual error analysis, we foundthat a fairly large number of errors belong to two errortypes: On the one hand, the preprocessing pipelineused to identify antecedent candidates does not al-ways include the correct antecedent in the set pre-sented to the neural network.
Whenever this occurs,it is obvious that the classifier cannot possibly findthe correct antecedent.
Out of 76 examples of the cat-egory elles that had been mistakenly predicted as ils,we found that 43 suffered from this problem.
In otherclasses, the problem seems to be somewhat less com-mon, but it still exists.
On the other hand, in manycases (23 out of 76 for the category mentioned be-fore) the anaphora resolution subnetwork does iden-tify an antecedent manually recognised to belong tothe right gender/number group, but still predicts an in-correct pronoun.
This may indicate that the networkhas difficulties learning a correct gender/number rep-resentation for all words in the vocabulary.7.1 Relaxing Markable ExtractionThe pipeline we use to extract potential antecedentcandidates is borrowed from the BART anaphoraresolution toolkit.
BART uses a syntactic parser toidentify noun phrases as markables.
When extract-ing antecedent candidates for coreference prediction,it starts by considering a window consisting of thesentence in which the anaphoric pronoun is locatedand the two immediately preceding sentences.
Mark-ables in this window are checked for morphologicalcompatibility in terms of gender and number with theanaphoric pronoun, and only compatible markablesare extracted as antecedent candidates.
If no compat-ible markables are found in the initial window, thewindow is successively enlarged one sentence at atime until at least one suitable markable is found.Our error analysis shows that this proceduremisses some relevant markables both because the ini-tial two-sentence extraction window is too small andbecause the morphological compatibility check incor-rectly filters away some markables that should havebeen considered as candidates.
By contrast, the ex-traction procedure does extract quite a number of firstand second person noun phrases (I, we, you and theiroblique forms) in the TED talks which are extremelyunlikely to be the antecedent of a later occurrence ofhe, she, it or they.
As a first step, we therefore adjustthe extraction criteria to our task by increasing theinitial extraction window to five sentences, exclud-ing first and second person markables and removingthe morphological compatibility requirement.
Thecompatibility check is still used to control expansionof the extraction window, but it is no longer appliedto filter the extracted markables.
This increases theaccuracy to 0.701 for TED and 0.602 for the news387TED(Accuracy: 0.696)P R Fce 0.618 0.722 0.666elle 0.754 0.548 0.635elles 0.737 0.340 0.465il 0.718 0.629 0.670ils 0.652 0.916 0.761OTHER 0.741 0.682 0.711News commentary(Accuracy: 0.597)P R Fce 0.419 0.368 0.392elle 0.547 0.460 0.500elles 0.539 0.135 0.215il 0.623 0.719 0.667ils 0.596 0.783 0.677OTHER 0.614 0.544 0.577Table 4: Neural network classifier with latent anaphora resolutionTED(Accuracy: 0.713)P R Fce 0.611 0.723 0.662elle 0.749 0.596 0.664elles 0.602 0.616 0.609il 0.733 0.638 0.682ils 0.710 0.884 0.788OTHER 0.760 0.704 0.731News commentary(Accuracy: 0.626)P R Fce 0.492 0.324 0.391elle 0.526 0.439 0.478elles 0.547 0.558 0.552il 0.599 0.757 0.669ils 0.671 0.878 0.761OTHER 0.681 0.526 0.594Table 5: Final classifier resultscommentaries, while the performance for elles im-proves to F-scores of 0.531 (TED; P 0.690, R 0.432)and 0.304 (News commentaries; P 0.444, R 0.231),respectively.
Note that these and all the following re-sults are not directly comparable to the ME baselineresults in Table 2, since they include modificationsand improvements to the training data extraction pro-cedure that might possibly lead to benefits in the MEsetting as well.7.2 Adding Lexicon KnowledgeIn order to make it easier for the classifier to iden-tify the gender and number properties of infrequentwords, we extend the word vectors with features indi-cating possible morphological features for each word.In early experiments with ME classifiers, we foundthat our attempts to do proper gender and numbertagging in French text did not improve classificationperformance noticeably, presumably because the an-notation was too noisy.
In more recent experiments,we just add features indicating all possible morpho-logical interpretations of each word, rather than try-ing to disambiguate them.
To do this, we look upthe morphological annotations of the French wordsin the Lefff dictionary (Sagot et al 2006) and intro-duce a set of new binary features to indicate whethera particular reading of a word occurs in that dictio-nary.
These features are then added to the one-hotrepresentation of the antecedent words.
Doing so im-proves the classifier accuracy to 0.711 (TED) and0.604 (News commentaries), while the F-scores forelles reach 0.589 (TED; P 0.649, R 0.539) and 0.500(News commentaries; P 0.545, R 0.462), respectively.7.3 More Anaphoric Link FeaturesEven though the modified antecedent candidate ex-traction with its larger context window and withoutthe morphological filter results in better performanceon both test sets, additional error analysis revealsthat the classifiers has greater problems identifyingthe correct markable in this setting.
One reason forthis may be that the baseline anaphoric link featureset described above (Section 6) only includes twovery rough binary distance features which indicatewhether or not the anaphora and the antecedent can-didate occur in the same or in immediately adjacentsentences.
With the larger context window, this maybe too unspecific.
In our final experiment, we there-fore enable some additional features which are avail-able in BART, but disabled in the baseline system:388?
Distance in number of markables?
Distance in number of sentences?
Sentence distance, log-transformed?
Distance in number of words?
Part of speech of head wordMost of these encode the distance between theanaphora and the antecedent candidate in more pre-cise ways.
Complete results for this final system arepresented in Table 5.Including these additional features leads to anotherslight increase in accuracy for both corpora, with sim-ilar or increased classifier F-scores for most classesexcept elle in the news commentary condition.
In par-ticular, we should like to point out the performanceof our benchmark classifier for elles, which sufferedfrom extremely low recall in the first classifiers andapproaches the performance of the other classes, withnearly balanced precision and recall, in this final sys-tem.
Since elles is a low-frequency class and cannotbe reliably predicted using source context alone, weinterpret this as evidence that our final neural networkclassifier has incorporated some relevant knowledgeabout pronominal anaphora that the baseline ME clas-sifier and earlier versions of our network have no ac-cess to.
This is particularly remarkable because nodata manually annotated for coreference was used fortraining.8 Related workEven though it was recognised years ago that theinformation contained in parallel corpora may pro-vide valuable information for the improvement ofanaphora resolution systems, there have not beenmany attempts to cash in on this insight.
Mitkov andBarbu (2003) exploit parallel data in English andFrench to improve pronominal anaphora resolutionby combining anaphora resolvers for the individuallanguages with handwritten rules to resolve conflictsbetween the output of the language-specific resolvers.Veselovsk?
et al(2012) apply a similar strategy toEnglish-Czech data to resolve different uses of thepronoun it.
Other work has used word alignments toproject coreference annotations from one languageto another with a view to training anaphora resolversin the target language (Postolache et al 2006; deSouza and Ora?san, 2011).
Rahman and Ng (2012)instead use machine translation to translate their testdata into a language for which they have an anaphoraresolver and then project the annotations back to theoriginal language.
Completely unsupervised mono-lingual anaphora resolution has been approached us-ing, e. g., Markov logic (Poon and Domingos, 2008)and the Expectation-Maximisation algorithm (Cherryand Bergsma, 2005; Charniak and Elsner, 2009).
Tothe best of our knowledge, the direct application ofmachine learning techniques to parallel data in a taskrelated to anaphora resolution is novel in our work.Neural networks and deep learning techniqueshave recently gained some popularity in natural lan-guage processing.
They have been applied to taskssuch as language modelling (Bengio et al 2003;Schwenk, 2007), translation modelling in statisticalmachine translation (Le et al 2012), but also part-of-speech tagging, chunking, named entity recognitionand semantic role labelling (Collobert et al 2011).In tasks related to anaphora resolution, standard feed-forward neural networks have been tested as a clas-sifier in an anaphora resolution system (Stuckardt,2007), but the network design presented in our workis novel.9 ConclusionIn this paper, we have introduced cross-lingual pro-noun prediction as an independent natural languageprocessing task.
Even though it is not an end-to-endtask, pronoun prediction is interesting for several rea-sons.
It is related to the problem of pronoun transla-tion in SMT, a currently unsolved problem that hasbeen addressed in a number of recent research publi-cations (Le Nagard and Koehn, 2010; Hardmeier andFederico, 2010; Guillou, 2012) without reaching amajor breakthrough.
In this work, we have shown thatpronoun prediction can be effectively modelled in aneural network architecture with relatively simplefeatures.
More importantly, we have demonstratedthat the task can be exploited to train a classifier witha latent representation of anaphoric links.
With paral-lel text as its only supervision this classifier achievesa level of performance that is similar to, if not bet-ter than, that of a classifier using a regular anaphoraresolution system trained with manually annotateddata.389ReferencesYoshua Bengio, R?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Samuel Broscheit, Massimo Poesio, Simone PaoloPonzetto, Kepa Joseba Rodriguez, Lorenza Romano,Olga Uryupina, Yannick Versley, and Roberto Zanoli.2010.
BART: A multilingual anaphora resolution sys-tem.
In Proceedings of the 5th International Work-shop on Semantic Evaluations (SemEval-2010), Upp-sala, Sweden, 15?16 July 2010.Mauro Cettolo, Christian Girardi, and Marcello Federico.2012.
WIT3: Web inventory of transcribed and trans-lated talks.
In Proceedings of the 16th Conferenceof the European Association for Machine Translation(EAMT), pages 261?268, Trento, Italy.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), pages 148?156, Athens, Greece.Colin Cherry and Shane Bergsma.
2005.
An Expecta-tion Maximization approach to pronoun resolution.
InProceedings of the Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005), pages 88?95, Ann Arbor, Michigan.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2461?2505.Jos?
de Souza and Constantin Ora?san.
2011.
Can pro-jected chains in parallel corpora help coreference reso-lution?
In Iris Hendrickx, Sobha Lalitha Devi, Ant?nioBranco, and Ruslan Mitkov, editors, Anaphora Process-ing and Applications, volume 7099 of Lecture Notes inComputer Science, pages 59?69.
Springer, Berlin.Liane Guillou.
2012.
Improving pronoun translation forstatistical machine translation.
In Proceedings of theStudent Research Workshop at the 13th Conference ofthe European Chapter of the Association for Computa-tional Linguistics, pages 1?10, Avignon, France.Christian Hardmeier and Marcello Federico.
2010.
Mod-elling pronominal anaphora in statistical machine trans-lation.
In Proceedings of the seventh InternationalWorkshop on Spoken Language Translation (IWSLT),pages 283?289, Paris, France.Christian Hardmeier.
2012.
Discourse in statistical ma-chine translation: A survey and a case study.
Discours,11.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan.Hai-Son Le, Alexandre Allauzen, and Fran?ois Yvon.2012.
Continuous space translation models with neuralnetworks.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies,pages 39?48, Montr?al, Canada.Ronan Le Nagard and Philipp Koehn.
2010.
Aiding pro-noun translation with co-reference resolution.
In Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, pages 252?261,Uppsala, Sweden.Ruslan Mitkov and Catalina Barbu.
2003.
Using bilingualcorpora to improve pronoun resolution.
Languages inContrast, 4(2):201?211.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment models.Computational linguistics, 29:19?51.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 650?659, Honolulu, Hawaii.Oana Postolache, Dan Cristea, and Constantin Ora?san.2006.
Transferring coreference chains through wordalignment.
In Proceedings of the 5th Conferenceon International Language Resources and Evaluation(LREC-2006), pages 889?892, Genoa.Altaf Rahman and Vincent Ng.
2012.
Translation-basedprojection for multilingual coreference resolution.
InProceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 720?730, Montr?al, Canada.Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de LaClergerie, and Pierre Boullier.
2006.
The Lefff 2syntactic lexicon for French: architecture, acquisition,use.
In Proceedings of the 5th Conference on Inter-national Language Resources and Evaluation (LREC-2006), pages 1348?1351, Genoa.Holger Schwenk.
2007.
Continuous space language mod-els.
Computer Speech and Language, 21(3):492?518.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational lin-guistics, 27(4):521?544.Roland Stuckardt.
2007.
Applying backpropagation net-works to anaphor resolution.
In Ant?nio Branco, editor,Anaphora: Analysis, Algorithms and Applications.
6th390Discourse Anaphora and Anaphor Resolution Collo-quium, DAARC 2007, number 4410 in Lecture Notesin Artificial Intelligence, pages 107?124, Berlin.Kater?ina Veselovsk?, Ngu.y Giang Linh, and MichalNov?k.
2012.
Using Czech-English parallel corpora inautomatic identification of it.
In Proceedings of the 5thWorkshop on Building and Using Comparable Corpora,pages 112?120, Istanbul, Turkey.391
