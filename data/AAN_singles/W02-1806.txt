PCFG Parsing for Restricted Classical Chinese TextsLiang HUANGDepartment of Computer Science,Shanghai Jiaotong UniversityNo.
1954 Huashan Road, ShanghaiP.R.
China 200030lhuang@sjtu.edu.cnYinan PENGDepartment of Computer Science,Shanghai Jiaotong UniversityNo.
1954 Huashan Road, ShanghaiP.R.
China 200030ynpeng@sjtu.edu.cnHuan WANGDepartment of Chinese Literature and Linguistics,East China Normal UniversityNo.
3663 North Zhongshan Road, Shanghai,P.R.
China 200062Zhenyu WUDepartment of Computer Science,Shanghai Jiaotong UniversityNo.
1954 Huashan Road, ShanghaiP.R.
China 200030neochinese@sjtu.edu.cnAbstractThe Probabilistic Context-Free Grammar(PCFG) model is widely used for parsingnatural languages, including ModernChinese.
But for Classical Chinese, thecomputer processing is just commencing.Our previous study on the part-of-speech(POS) tagging of Classical Chinese is apioneering work in this area.
Now in thispaper, we move on to the PCFG parsing ofClassical Chinese texts.
We continue touse the same tagset and corpus as ourprevious study, and apply thebigram-based forward-backward algorithmto obtain the context-dependentprobabilities.
Then for the PCFG model,we restrict the rewriting rules to bebinary/unary rules, which will simplify ourprogramming.
A small-sized rule-set wasdeveloped that could account for thegrammatical phenomena occurred in thecorpus.
The restriction of texts lies in thelimitation on the amount of proper nounsand difficult characters.
In ourpreliminary experiments, the parser givesa promising accuracy of 82.3%.IntroductionClassical Chinese is an essentially differentlanguage from Modern Chinese, especially insyntax and morphology.
While there has beena number of works on Modern ChineseProcessing over the past decade (Yao and Lua,1998a), Classical Chinese is largely neglected,mainly because of its obsolete and difficultgrammar patterns.
In our previous work (2002),however, we have stated that in terms ofcomputer processing, Classical Chinese iseven easier as there is no need of wordsegmentation, an inevitable obstacle in theprocessing of Modern Chinese texts.
Now inthis paper, we move on to the parsing ofClassical Chinese by PCFG model.
In thissection, we will first briefly review relatedworks, then provide the background ofClassical Chinese processing, and finally givethe outline of the rest of the paper.A number of parsing methods have beendeveloped in the past few decades.
They canbe roughly classified into two categories:rule-based approaches and statisticalapproaches.
Typical rule-based approaches asdescribed in James (1995) are driven bygrammar rules.
Statistical approaches such asYao and Lua (1998a), Klein and Manning(2001) and Johnson, M. (2001), on the otherhand, learn the parameters the distributionalregularities from a usually large-sized corpus.In recent years, the statistical approaches havebeen more successful both in part-of-speechtagging and parsing.
In this paper, we applythe PCFG parsing with context-dependentprobabilities.A special difficulty lies in the wordsegmentation for Modern Chinese processing.Unlike Indo-European languages, ModernChinese words are written without whitespaces indicating the gaps between twoadjacent words.
And different possiblesegmentations may cause consistentlydifferent meanings.
In this sense, ModernChinese is much more ambiguous than thoseIndo-European Languages and thus moredifficult to process automatically (Huang et al,2002).For Classical Chinese processing, suchsegmentation is largely unnecessary, sincemost Classical Chinese words aresingle-syllable and single-character formed.To this end, it is easier than Modern Chinesebut actually Classical Chinese is even moreambiguous because more than half of thewords have two or more possible lexicalcategories and dynamic shifts of lexicalcategories are the most common grammaticalphenomena in Classical Chinese.
Despite ofthese difficulties, our work (2002) onpart-of-speech tagging has shown anencouraging result.The rest of the paper is organized asfollows.
In Section 1, a tagset designedspecially for Classical Chinese is introducedand the forward-backward algorithm forobtaining the context-dependent probabilitiesbriefly discussed.
We will briefly present thetraditional two-level PCFG model, thesyntactic tagset and CFG rule-set for ClassicalChinese in Section 2.
Features of the ClassicalChinese grammar will also be covered in thissection.
In Section 3 we will present ourexperimental results.
A summary of the paperis given in the conclusion section.1 Tagset and Context-DependentProbabilitiesGenerally speaking, the design of tagset isvery crucial to the accuracy and efficiency oftagging and parsing, and this was commonlyneglected in the literature where manyresearchers use those famous corpora and theirtagset as the standard test-beds.
Still thereshould be a tradeoff between accuracy andefficiency.
In our previous work (2002), asmall-sized tagset for Classical Chinese ispresented that is shown to be accurate in theirPOS tagging experiments.
We will continue touse their tagset in this paper.
We will also usea forward-backward algorithm to obtain thecontext-dependent probabilities.1.1 TagsetThe tagset was designed with special interestnot only to the lexical categories, but also thecategories of components, namelysubcategories a word may belong.
Forexample, it discriminates adjectives into 4subcategories like Adjective as attributive, etc.
(See table 1).
And several grammaticalfeatures should be reflected in the tagset.These discriminations and features turn out tobe an important contributing factor of theaccuracy in our parsing experiments.Table 1.
The tagset for Classical Chinese1.2 Tagging AlgorithmsWe apply the Hidden Markov Model (HMM)(Viterbi, 1967) and the forward-backwardalgorithm (James, 1995) to obtain thecontext-dependent probabilities.Generally there are 2 types of HMM taggersfor parsers, the trigram model and the bigramforward-backward model.
Charniak (1996)suggested that the former is better for parsers.But the former only result in a deterministicsequence of most probable POS, in otherwords, it assigns only one POS tag for eachword.
Although the accuracy of trigram by ourprevious work (2002) is as high as 97.6%, fora sentence of 10 words long, the possibility ofall-correctness is as low as low as78.4%(97.6%)10 = , and the single-tag schemedoes not allow parsers to re-call the correcttags, as is often done if we apply theforward-backward model.
So in this paper westill apply the traditional bigramforward-backward algorithm.
We suggest thata combination of trigram andforward-backward model would be the bestchoice, although no such attempt exists in theliterature.2 PCFG Model and Classical ChineseGrammarIn this section we will cover the PCFG modeland context-sensitive rules designed forClassical Chinese.
Features of the rule-set willbe also discussed.2.1 PCFG Model and Rule RestrictionCFG: A context-free grammar (CFG) is aquadruple ),,,( RSVV TN  where TV  is a set ofterminals (POS tags), NV  is a set ofnon-terminals (syntactic tags), NVS ?
is thestart non-terminal, and R is the finite set ofrules, which are pairs from +?VVN , where Vdenotes TN VV  .
A rule >< ?,A  is written in theform ?
?A , A is called the left hand side(LHS) and ?
the right hand side (RHS).PCFG: A probabilistic context-free grammar(PCFG) is a quintuple ),,,,( PRSVV TN , where),,,( RSVV TN  is a CFG and ]1,0(: RP  is aprobability function suchthatNVN ??
:?
??
=?RN NP??
?
: 1)(Rule Restriction:We restrict the CFG rules tobe binary or unary rules, but NOT as strict asthe Chomsky Normal Form (CNF).
EachRRi ?
could be in the following two formsonly:1.
ABNR ji ?:2.
ANR ji ?
:where Nj VN ?
and VBA ?,The advantage of binary/unary rules lies in thesimplicity of parsing algorithm, and will bediscussed in Section 4.The major difference between our model andCNF is that for unary rules, we do not requirethe right-hand-side to be terminals.
And thisenables us easier representation of theClassical Chinese language.2.2 Rule-Set for Classical ChineseAn important advantage of PCFG is that itneeds fewer rules and parameters.
Accordingto our corpus, which is representative ofClassical Chinese classics, only 100-150 ruleswould be sufficient.
This is mainly becauseour rule set is linguistically sound.
A summaryof the set of rules is presented as follows.Table 2.
Our non-terminals (also called syntactic tagset,or constituent set)A subset of most frequently used rules isshown in the following table.Table 3.
A simple subset of PCFG Rules forClassical Chinese1.
S ->   NP VP ; simple S/V2.
S ->   VP ; S omitted3.
S ->   VP NP ; S/V inversion4.
S ->  ad S5.
VP -> vi6.
VP -> vt NP ; simple V/O7.
VP -> NP vt ; V/O inversion8.
VP -> ad VP9.
VP -> PP VP ; prepositioned PP10.
VP -> VP PP ; postpositioned PP11.
VP -> NP ; NP as VP12.
VP -> VP yq13.
NP -> n14.
NP -> npron15.
NP -> ADJP NP16.
NP -> POSTADJP17.
NP -> VP ; V/O as NP18.
NP -> fy NP19.
ADJP -> aa20.
ADJP -> apron21.
ADJP -> NP zd22.
PP -> prep NP ; P+NP23.
PP -> NP prep ; inversion24.
PP -> prepb ; object omitted25.
PP -> NP ; prep.
omitted26.
POSTADJP-> VP zjExamples of parse trees are shown in thefollowing figure.
(a)      (b)Fig.
1. the parse trees of 2 sentences2.3 Features of Classical ChineseGrammar RulesAs an aside, it is worthwhile to point out heresome peculiarities of the Classical Chinesegrammar used in our work.
Readers notinterested in grammar modeling may simplyskip this subsection.
As mentioned before, thegrammar of Classical Chinese is entirelydifferent from that of English, so a few specialfeatures must be studied.
Although thesefeatures bring many difficulties to the parser,we have developed successful programmingtechniques to solve them.From the rule-set, the reader might find thattwo special grammatical structures is verycommon in Classical Chinese:1.
Inversion: subject/verb inversion (rule 3),preposition/object inversion (rule 23).2.
Omission: Subject omitted (rule 2),preposition?s object omitted (rule 24),preposition omitted (rule 25).Maybe the strangest feature is the structure ofPP.
English PP is always P+NP.
But here inClassical Chinese, by inversion and omission,the PP may have up to 4 forms, as shown inrule 22-25.Table 4.
The 4 rules from PP.
The object of thepreposition is in brackets, and [] indicate an omission.Another feature that must be pointed out hereis the cycle.
In our rule-set, there are 2 rules(rule 11 and rule 17) forming a cycle:Fig.
2.
A cycle in the rule-set.
Rule 11: NP-> VP, Rule 17:VP-> NP.It will ease our parsing because ClassicalChinese is lexically and syntactically veryambiguous.
An NP can act as a VP (a mainverb), while a VP can act as a NP (subject orobject).
These two features are exemplified infigure 3.
There are actually more cycles in therule-set.
Helpful as they are, the cycles bringgreat difficulty to the memory-based top-downparser.
In practice, we develop aclosure?based method to solve this problem,as shown in the following pseudo-code:better_results_found=true;while (better_results_found){better_results_found=false;memory_based_top_down_parse();// if better results found, the variable will be set true}Another point is the use of preferences forambiguity resolution.
While the ambiguities inour rule-set greatly ease our modelingClassical Chinese grammar, it causes theparser to make a lot of ridiculous errors.
So wehere apply some predefined preferences suchas ?an fy must be at the first of an NP?
and ?ayq must be at the end of a VP?.
Thisconsideration results in a significant increasein the parsing accuracies.3 EvaluationsIn our preliminary experiments, weconstructed a treebank of 1000 manuallyparsed sentences (quite large for ClassicalChinese treebank), in which 100 sentences areselected as the test set using thecross-validation scheme, while the others asthe learning set.
The majority of thesesentences are extracted from classics ofpre-Tsin Classical Chinese such as Hanfeiziand Xunzi because in these texts there arefewer proper nouns and difficult words.
Thatis the restriction we put on the selection ofClassical Chinese texts.
It must be pointed outhere that compared from other languages,Classical Chinese sentences are so short thatthe average length is only about 4-6 wordslong.Fig.
3.Sentence Distributions and ParsingAccuraciesFigure 3 shows the distribution of sentencesand parsing accuracies for different sentencelengths.
For distribution, we can see that those4-word, 5-word, and 6-word sentencesconstitute for the majority of the corpus, whilethose 1-word and 2-word sentences are veryfew.
For accuracy, the parser is more effectivefor shorter sentences than for longer sentences.And for 1-word and 2-word sentences, there isno error report from the parse results.ConclusionComputer processing of ClassicalChinese has just been commencing.
WhileClassical Chinese is generally considered toodifficult to process, our previous work onpart-of-speech tagging has been largelysuccessful because there is almost no need tosegment Classical Chinese words.
And wecontinue to use the tagset and corpus into thiswork.
We first apply the forward-backwardalgorithm to obtain the context-dependentprobabilities.
The PCFG model is thenpresented where we restrict the rules intobinary/unary rules, which greatly simplifiesour parsing programming.
According to themodel, we developed a CFG rule-set ofClassical Chinese.
Some special features of theset are also studied.
Classical Chineseprocessing is generally considered too difficultand thus neglected, while our works haveshown that by good modelling and propertechniques, we can still get encouraging results.Although Classical Chinese is currently a deadlanguage, our work still has applications inthose areas as Classical-Modern ChineseTranslation.For future work of this paper, we expectto incorporate trigram model into theforward-backward algorithm, which willincrease the tagging accuracy.
And mostimportant of all, it is obvious that thestate-of-the-art PCFG model is stilltwo-leveled, we expect to devise a three-levelmodel, just like trigram versus bigram.AcknowledgementsOur special thanks go to Prof. Lu Ruzhan ofShanghai Jiaotong University for his sincereguidance.ReferencesAllen, J.
(1995) Natural Language Understanding,The Benjamin/Cummings Publishing Company,Inc.Viterbi, A.
(1967) Error bounds for convolutioncodes and an asymptotically optimal decodingalgorithm.
IEEE Trans.
on Information Theory13:260-269.Yao Y., Lua K. (1998a) A ProbabilisticContext-Free Grammar Parser for Chinese,Computer Processing of Oriental Languages, Vol.11, No.
4,  pp.
393-407Huang L., Peng Y., Wang H. (2002) StatisticalPart-of-Speech Tagging for Classical Chinese,Proceedings of the 5th International Conferenceon Text, Speech, and Dialog (TSD), Brno, inpressKlein, D., and Manning C. (2001) NaturalLanguage Grammar Induction using aConstituent-Context Model, Proceedings ofNeural Information Processing Systems,Vancouver.Yao Y., Lua K. (1998b) Mutual Information andTrigram Based Merging for Grammar RuleInduction and Sentence Parsing, ComputerProcessing of Oriental Languages, Vol.
11, No.
4,pp.
393-407Johnson, M. (2001) Joint and conditionalestimation of tagging and parsing models,Proceedings of International computationallinguistics conference, ToulouseCharniak, E. (1996) Taggers for Parsers, ArtificialIntelligence, Vol.
85, No.
1-2, pp.
45-47.
