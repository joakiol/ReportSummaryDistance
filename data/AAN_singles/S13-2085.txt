Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 513?519, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticsteragram:Rule-based detection of sentiment phrases using SAS Sentiment AnalysisHilke Reckman, Cheyanne Baird, Jean Crawford, Richard Crowell,Linnea Micciulla, Saratendu Sethi, and Fruzsina VeressSAS Institute10 Fawcett StreetCambridge, MA 02138, USAhilke.reckman@sas.comAbstractFor SemEval-2013 Task 2, A and B (Sen-timent Analysis in Twitter), we use a rule-based pattern matching system that is based onan existing ?Domain Independent?
sentimenttaxonomy for English, essentially a highlyphrasal sentiment lexicon.
We have madesome modifications to our set of rules, basedon what we found in the annotated trainingdata that was made available for the task.
Theresulting system scores competitively, espe-cially on task B.1 IntroductionSAS taxonomies for sentiment analysis are primar-ily topic-focused.
They are designed to track sen-timent around brands, entities, or other topics andsubtopics in a domain (Lange and Sethi, 2011;Lakkaraju and Sethi, 2012; Albright and Lakkaraju,2011).
Domain-independent taxonomies have asecond function.
In addition to performing topic-focused tasks, they can be set up to perform senti-ment analysis at the document level, classifying thewhole document as positive, negative, or neutral.
Inthis task all sentiment expressions are taken into ac-count, rather than only those which are related tothe tracked topic.
This second function is becom-ing increasingly important.
It allows for a broaderperspective that is complementary to topic-focusedopinion mining.We participated in both subtask A and B ofSemEval-2013 Task 2: Sentiment Analysis in Twit-ter (Wilson et al 2013) with an adaptation of ourexisting system.
For task B, identifying the overallsentiment of a tweet, our taxonomy mainly neededsome fine-tuning to specifically accommodate Twit-ter data.
(Normally tweets only make up a smallpart of the data we work with.)
We also made afew adaptations to focus entirely on document levelsentiment, whereas originally the main focus of oursystem was on tracking sentiment around products.For task A, identifying the sentiment of ambiguousphrases in a tweet, a few more modifications wereneeded.Our system is entirely rule-based, and the rulesare hand-written.
In some cases, statistical text min-ing approaches are used for the discovery of topicsand terms to facilitate rule writing.
Our sentimentanalysis software does offer a statistical component,but our experience is that purely rule-based modelswork better for our typical sentiment analysis tasks.Advantages of rules are that problems observedin the output can be targeted directly, and the modelcan become more and more refined over time.
Also,they allow for simple customization.
In our brand-centered work, we customize our taxonomies forone or more brands that we want to track.
Whenwe build a taxonomy for a new domain, we buildupon work we have done before in other domains.The assignment of sentiment to certain phrases canbe sensitive to context where it needs to be.
Thecanceled task C, identifying sentiment related to atopic, could have been approached successfully witha rule-based approach, as our rules are specificallydesigned to connect sentiment to targeted topics.Section 2 describes the basic architecture of oursystem, followed by a section on related work.
Thensections 4 and 5 describe the adaptations made for513each subtask and present the results.
This is fol-lowed by a more general discussion of our approachin the light of these results in section 6, and the con-clusion in section 7.2 The base systemThe datasets we normally use for the developmentof our taxonomies include blogs, forums, news, andTwitter.
When developing a domain-specific taxon-omy, we collect data for that particular domain, e.g.Banking, Retail, Hospitality.
We build the taxonomywith the terms we encounter in those documents,and test on a new set of documents.
The DomainIndependent taxonomy started out as the commonbase derived from several of these taxonomies, andwas then built out and tested using a wider range ofEnglish-language documents.
Since we used someother tweets in the development of the original sys-tem, our submission is considered unconstrained.Our rules are patterns that match words or se-quences of words, which makes our approach essen-tially lexicon-based.
Matching occurs left-to-rightand longer matches take precedence over shorterones.
The top level rules in our sentiment taxonomyare set up to recognize positive and negative word-sequences.
There is also a set of ?neutral?
rules atthat level that block the assignment of positive ornegative sentiment in certain cases.A positive or negative sequence can consist of asingle word from the positive or negative word-lists,or a spelled out phrase from the positive or nega-tive phrase-lists.
Alternatively, it can be built up outof multiple components, for example an emphaticmodifier and a sentiment term, or a negation and asentiment term.
We call these sequences Positiveand Negative ?Contexts?, since they are contexts forthe topic-terms that we normally track.Documents are preprocessed by an in-house POS-tagger.
Rules can require a word to have a particularpart of speech.The words in the word-list, or in any of the otherrules, can be marked with an ?
@?-sign to enablemorphological expansion, and in that case they willmatch any of the forms in their paradigm.
For ex-ample ?love@?
will match love, loves, loved, andloving.
This functionality is supported by a mor-phological dictionary that links these forms to theirstem.The rules are organized into lists that representuseful concepts, which can be referred to in otherrules as a means of abstraction.
For example therule:def{Negation} def{PositiveAdjectives}matches phrases that are composed of a negation (asdefined in the list named Negation) and a positiveadjective (as defined in the list named PositiveAd-jectives).
Negation includes rules like ?hasn?t been?,?doesnt?
[sic], ?not exactly the most?, etc., and Posi-tiveAdjectives contains a rule that matches words inPositiveWords if they are also tagged as adjectives.For efficiency reasons the dependencies cannot becircular, hence not allowing for recursion.Distance rules can be used to capture a longerspan, matching a specified pattern at the beginningand at the end, including arbitrary intervening wordsup to a specified number.
They can also be used tomake matching a term dependent on specified termsin the context.
For example,(SENT, (DIST 4, ?
a{ def{HigherIsBetter}}?,?
a{ def{Lowering}}?
))will capture phrases that say a company?s profit(HigherIsBetter) went down (Lowering).
TheSENT-operator prevents matching across sentenceboundaries.
(ORDDIST 7, ?
def{PositiveContext}?,?
a{ def{PositiveAmbig}}?
)will capture ambiguous positive expressions whenthey follow an unambiguously positive sequencewithin a distance of 7 words.This ensemble of lists and rules has grown rela-tively organically, and is motivated by the data weencounter.
We introduce new distinctions when wefeel it will make a difference in terms of results,or sometimes for ease of development and mainte-nance.Usually each sentiment expression has the sameweight, and one positive and one negative expres-sion cancel each other out.
However at the top levelwe can introduce weights, and we have done so inthis model.
We have created lists of weak positiveand negative expressions, and we gave those very514Positive:?
(ORDDIST 2, ?
a{exceed@}?, ?
a{expectation@}?)?
:Pro could not be happier?
blown away by?
def{Negation} want@ it to end?
above and beyond?
break@ down barriers?
can?t go wrong with?
dying to def{Consume}?
save@ me def{Money}?
(ALIGNED, ?
c{treat@}?, ?:N?)Negative:?
def{Negation} find def{NounPhrases}def{PositivePhrases}?
(SENT, (ORDDIST 7, ?
a{disappointed that}?,?
a{ def{PositivePhrases}}?))?
I would have loved?
def{Negation} accept@?
breach of def{PositiveWords}?
def{Money} magically disappears?
lack of training?
make@ no sense?
subject@ me to?
fun dealing withFigure 1: Examples of rules for positive and negativephrases and patterns.low weights, so that they would only matter if therewere no regular-strength expressions present.
Welimited some of those weak sentiment rules to sub-task A only, but they clearly helped with recall there.Negations in the default case turn positives intonegatives and negatives into neutrals.
In addition tonegations we also have sentiment reversers, whichturn negatives into positives.
Simple negations nor-mally scope over a right-adjacent word or phrase, forexample a noun phrase or a verb.
A special class ofclausal negations (I don?t think that) by approxima-tion take scope over a clause.This system contains roughly 2500 positive wordsand 2000 positive phrases, and roughly 7500 neg-ative words and 3000 negative phrases.
Some ex-amples are given in Figure 1.
The neutral list alsocontains about 2000 rules.
Other helper lists such asNegation, EmphaticModifiers, and Money typicallycontain about a hundred rules each.A system like this takes about six to eight weeksto build for a new language.
This requires a deve-loper who is already familiar with the methodology,and assumes existing support for the language, in-cluding a morphological dictionary and a part-of-speech tagger.3 Related workIn tasks that are not topic-related, purely rule-basedmodels are rare, although the winning system ofSemEval-2010 Task 18 (Wu and Jin, 2010), some-what similar to task A, was rule-based (Yang andLiu, 2010).
Liu (2010) suggests that more rule-based work may be called for.
However, there aremany other systems with a substantial rule-basedcomponent (Nasukawa and Yi, 2003; Choi andCardie, 2008; Prabowo and Thelwall, 2009; Wilsonet al 2005).
Systems commonly have some rulesin place that account for the effect of negation (Wie-gand et al 2010) and modifiers.
Sentiment lexiconsare widely used, but mainly contain single words(Baccianella et al 2010; Taboada et al 2011).
Fortopic-related tasks, rule-based systems are a bit morecommon (Ding et al 2008).4 Task ATask A was to assign sentiment to a target in context.The target in isolation would often be ambiguous.
Itwas a novel challenge to adapt our model for thissubtask.Since we normally track sentiment around spe-cific topics, we can usually afford to ignore highlyambiguous phrases.
Typical examples of this areambiguous emoticons and comments like no joke atthe end a sentence, or directly following it.
Whenthese are used and could be disambiguated, usuallythere is a less ambiguous term available that occurscloser to the topic-term that we are interested in.
(Insome cases we do use the topic as disambiguatingcontext.
)Also, we generally place slightly more empha-sis on precision than on recall, assuming that withenough data the important trends will emerge, evenif we ignore some of the unclear cases and outliers.This makes the output cleaner and more pleasant to515work with for follow-up analysis.4.1 Model adaptations and processingWe adapted our model to task A by introducing listsof ambiguous positive and negative terms that werethen disambiguated in context, e.g.
if there was an-other sentiment term of a specified polarity nearby.We also added some larger patterns that included anambiguous term, but as a whole had a much clearerpolarity.
Below are some examples of rules for theword like, which is highly ambiguous in English.1.
(ALIGNED, ?
c{like@}?, ?:V?)
(pos)2. likes (pos)3.
I like (pos)4. like magic (pos)5. give it a ?like?
(pos)6. kinda like it (weakpos)7. doesn?t seem like (hypothetical)8.
How can you like (neg)9. don?t like (neg)10. like to pretend (neg)11. treated like a number (neg)12.
Is it like (neutral)13. a bit like (neutral)14. the likes of (neutral)A seemingly obvious rule for like is (1), restrict-ing it to usage as a verb.
However, disambiguatinglike is a difficult task for the tagger too, and the re-sult is not always correct.
Therefore this rule is afall-back case, when none of the longer rules apply.Inflected forms such as (2) are pretty safe, with afew exceptions, which can be caught by neutralizingrules, such as (14).
The hypothetical case, (7), is notused in task A, but it is in task B.A potential issue for our results on this task is thatour system only returns the longest match.
So in asentence such as ?I didn?t like it?, if you ask peopleto annotate like, they may say it is positive, whereasthe longer phrase didn?t like is negative.
In the out-put of our system, like will only be part of a negativesequence.
The information that it was originally rec-ognized as a positive word cannot be retrieved at theoutput level.We found that the annotators for task A were ingeneral much more liberal in assigning sentimentthan we normally are.
We made major gains by re-moving some of our neutralizing rules, for examplethose that neutralize sentiment in hypothetical con-texts, and by classifying negations that were not partof a larger recognized phrase as weak negatives.The annotations in the development data weresometimes confusing (see also section 6).
We hadsome difficulty in figuring out when certain termssuch as hope or miss you should be consideredpositive and when negative.
The verb apologizeturned out to be annotated sometimes positive andsometimes negative in near identical tweets.The test items were processed as follows:1. run the sentiment model on the text (tweet/SMS)2. identify the target phrase as a character span3.
collect detected sentiment that overlaps with the tar-get phrase(a) if there is no overlapping sentiment expres-sion, the sentiment is neutral(b) if there is exactly one overlapping sentimentexpression, that expression determines thesentiment(c) if there is more than one sentiment expressionthat overlaps with the target, compute whichsentiment has more weight (and in case of adraw, assign neutral)4.2 ResultsWe get a higher precision for positive and negativesentiment on task A than any of the other teams,but we generally under-predict sentiment.
Precisionon neutral sentiment is very low.
Detecting neutralphrases did not seem to be a very important goal inthe final version of this task, though.
The results ofour predictions on the Twitter portion of the data areshown in Figure 2.These results are slightly different from what wesubmitted, as we did not realize at the time of sub-mission that the encoding of the text was differentin the test data than it had been in the previously re-leased data.
The submitted results are included inthe summarizing Table 1 at the end of the discussionsection.Some targets are easily missed.
We do not havea good coverage of hashtags yet, for example.
Weincorporate frequent misspellings that are commonin Twitter and SMS.
However, we have no generalstrategy in place to systematically recognize uncon-ventionally spelled words (Eisenstein, 2013).
For516gs \ pred positive negative neutralpositive 1821 77 888 2734negative 47 1091 403 1541neutral 11 6 143 1601879 990 1382 4435class precision recall f-scorepositive 0.9691 0.6661 0.7895negative 0.9293 0.7080 0.8037neutral 0.1035 0.8938 0.1855average(pos and neg) 0.7966Figure 2: Confusion table and scores on task A, tweetsa project that processes Twitter data it would alsomake sense to periodically scan for new hashtagsand add them to the rules if they carry sentiment.However, a sentiment lexicon is never quite com-plete.Therefore we experimented with a guessing com-ponent.
If we do not detect any sentiment in the tar-get sequence, we let our model make a guess, basedon the overall sentiment it assigns to the document,assuming that an ambiguous target overall is morelikely to be positive in a positive context and neg-ative in a negative context.
(Note that this is differ-ent from our disambiguation rules, which only applyto explicitly listed items.)
This gives us substantialgains on this subtask (Figure 3).
However, this maynot hold up in a similar task where there are moreneutral instances than there were here, as we see adecrease in precision on positive and negative.gs \ pred positive negative neutralpositive 2147 230 357 2734negative 137 1249 155 1541neutral 50 33 77 1602334 1512 589 4435class precision recall f-scorepositive 0.9199 0.7853 0.8473negative 0.8261 0.8105 0.8182neutral 0.1307 0.4813 0.2056average(pos and neg) 0.8327Figure 3: Confusion table and scores on task A, tweets,with guessing5 Task BTask B was to predict the overall sentiment of atweet.
This was much closer to the task our tax-onomy is designed for, and yet it turned out to bedifferent in subtle ways.5.1 Model adaptations and processingWe quickly found that running the model as we hadadapted it for subtask A over-predicted sentimenton subtask B.
We therefore put most of our neu-tralizing rules back in place for this subtask, andrestricted a subset of the weak sentiment terms tosubtask A only.
We disabled the mechanism thathelped us catch ambiguous terms in subtask A (seesection 4.1).For processing we used our standard method,comparing the added weights of the positive and ofthe negative sequences found.
The highest scorewins.
In case of a draw, the document is classified asneutral.
?Unclassified?
(no sentiment terms found)also maps to neutral for this task.
A confidence scoreis computed, but not used here.5.2 ResultsOur system compares positively to those of the otherteams.
Originally we were in 3rd place as a teamon the Twitter data.
After correcting for the encod-ing problem we rise to second (assuming the otherteams did not have the same problem).
Among un-constrained systems only, we are first on tweets andsecond on SMS.
The results, after the correction, areshown in Figure 4.
As for task A, the original resultsare included in the final summarizing Table 1.gs \ pred positive negative neutralpositive 1188 88 296 1572negative 66 373 162 601neutral 408 202 1030 16401662 663 1488 3813class precision recall f-scorepositive 0.7148 0.7557 0.7347negative 0.5626 0.6206 0.5902neutral 0.6922 0.6280 0.6586average(pos and neg) 0.6624Figure 4: Confusion table and scores on task B, tweets5176 DiscussionWe modified an existing rule-based system for Sem-Eval Task 2.
While the development of this exist-ing system was a considerable time investment, themodifications for the two SemEval subtasks tookno more than about 2 person-weeks in total.
Themodels used in task A and B have a large commonbase, and our rule-based approach measures up wellagainst other systems.
This shows that if the work isdone once, it can be re-used, modified, and refined.As mentioned in section 4.1, the annotations didnot always seem consistent.
The guidelines did notask the annotators to keep in mind a particular taskor purpose for their annotations.
However, the cor-rect annotation of a tweet or fragment can vary de-pending on the purpose of the annotation.
Non-arbitrary choices have to be made as to what countsas sentiment: Do you try to identify cases of im-plicit sentiment?
Do you count cases of quoted orreported ?3rd-party?-sentiment?
.
.
.
Ultimately itdepends on what you are interested in: Do you wantto: -track sentiment around certain topics?
-knowhow authors are feeling?
-assess the general mood?-track distressing versus optimistic messages in thenews?
.
.
.
While manual rule writing allows us tochoose a consistent strategy, it was not obvious whatthe optimal strategy was in this SemEval task.There were considerable differences in annotationstrategy between task A and task B, which shared thesame tweets.
The threshold for detecting sentimentappeared to be considerably lower in task A than intask B.
This suggests that different choices had beenmade.
These choices probably reflect how the anno-tators perceived the tasks.In our core business, we primarily track sentimentaround brands.
One of the choices we made wasto also include good and bad news about the brand(such as that the company?s stock went up or down)where no explicit sentiment is expressed, becausethe circulation of such messages reflects on the rep-utation of the brand.
(Liu (2010) points out that alot of sentiment is implicit.)
In task B, we noticedthat ?newsy?
tweets had a tendency to be annotatedas neutral.
We did not have the time to thoroughlyadapt our model for that interpretation.Both manually annotating training data for super-vised machine learning and using training data formanual rule writing require a lot of work.
Bothcan be crowd-sourced to a large extent if the pro-cess is made simple enough, and the instructionsare clear enough.
All methods that use lists of sen-timent terms benefit from automatically extractingsuch terms from a corpus (Qiu et al 2009; Wiebeand Riloff, 2005).
As those methods become moresophisticated, the work of rule writers becomes eas-ier.
Since the correct annotation depends on the taskat hand, and there are many different choices thatcan be made, annotated data can be hard to reuse fora slightly different task than the one for which it wascreated.
In rule-based models it is easier to leverageearlier work and to slightly modify the model for anew task.
Both the rules and the model?s decision-making process are human-interpretable.Table 1 (next page) summarizes our results on thevarious portions of the task, and under different con-ditions.
The results on SMS-data are consistentlylower than their counterparts on tweets, but they fol-low the same pattern.
We conclude that the modelgeneralizes to SMS, but not perfectly.
This is notsurprising, since we have never looked at SMS-databefore, and the genre does appear to have some id-iosyncrasies.7 ConclusionOur model is essentially a highly phrasal sentimentlexicon.
Ways of defining slightly more abstract pat-terns keep the amount of work and the number ofrules manageable.
The model is applied through pat-tern matching on text, and returns a sentiment pre-diction based on the number of positive and nega-tive expressions found, based on the sum of theirweights.
This is not mediated by any machine learn-ing.Slightly different versions of this system were em-ployed in subtasks A and B.
It turned out to be astrong competitor in Task 2 of SemEval-2013, espe-cially on subtask B, where it scored in the top three.ReferencesRussell Albright and Praveen Lakkaraju.
2011.
Com-bining knowledge and data mining to understand sen-timent: A practical assessment of approaches.
Techni-cal report, SAS White Paper, January.518Task A Twitter Task A SMS Task B Twitter Task B SMSF-score rank F-score rank F-score rank F-score rankSubmitted 0.7489 3of7 0.7283 4of7 0.6486 1of15 0.5910 2of1513of23 11of19 3of34 5of29After fixing encoding 0.7966 3of7 0.7454 3of7 0.6624 1of15 0.6014 1of1511of23 8of19 2of34 4of29With guessing 0.8327 (2of7) 0.7840 (2of7) NA NA(8of23) (7of19)Table 1: Summary of results.
The first rank indication is relative to the other systems in the unconstrained category.The second is relative to the total number of participating teams (by highest scoring system).Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th conference on InternationalLanguage Resources and Evaluation (LREC10), Val-letta, Malta, May.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 793?801.
Association for Compu-tational Linguistics.Xiaowen Ding, Bing Liu, and Philip S Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProceedings of the international conference on Websearch and web data mining, pages 231?240.
ACM.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In Proc.
of NAACL.Praveen Lakkaraju and Saratendu Sethi.
2012.
Corre-lating the analysis of opinionated texts using sas textanalytics with application of sabermetrics to cricketstatistics.
In Proceedings of SAS Global Forum 2012,number 136.Kathy Lange and Saratendu Sethi.
2011.
What are peo-ple saying about your company, your products, or yourbrand?
In Proceedings of SAS Global Forum 2011,number 158.Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of natural language processing, 2:568.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Senti-ment analysis: Capturing favorability using naturallanguage processing.
In Proceedings of the 2nd in-ternational conference on Knowledge capture, pages70?77.
ACM.Rudy Prabowo and Mike Thelwall.
2009.
Sentimentanalysis: A combined approach.
Journal of Informet-rics, 3(2):143?157.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.Expanding domain sentiment lexicon through doublepropagation.
In Proceedings of the 21st internationaljont conference on Artifical intelligence, pages 1199?1204.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational linguistics,37(2):267?307.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In Computational Linguistics and Intel-ligent Text Processing, pages 486?497.
Springer.Michael Wiegand, Alexandra Balahur, Benjamin Roth,Dietrich Klakow, and Andre?s Montoyo.
2010.
A sur-vey on the role of negation in sentiment analysis.
InProceedings of the workshop on negation and specu-lation in natural language processing, pages 60?68.Association for Computational Linguistics.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
2005.Opinionfinder: A system for subjectivity analysis.
InProceedings of HLT/EMNLP on Interactive Demon-strations, pages 34?35.
Association for ComputationalLinguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.Yunfang Wu and Peng Jin.
2010.
Semeval-2010 task18: Disambiguating sentiment ambiguous adjectives.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 81?85.
Association forComputational Linguistics.Shi-Cai Yang and Mei-Juan Liu.
2010.
Ysc-dsaa: Anapproach to disambiguate sentiment ambiguous adjec-tives based on saaol.
In Proceedings of the 5th In-ternational Workshop on Semantic Evaluation, pages440?443.
Association for Computational Linguistics.519
