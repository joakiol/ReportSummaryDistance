mIllBDii\[\]inHimiimIignniiiImRmm- imimiIntegration and Synchronization of Input Modesduring Multimodal Human-Computer Interaction*Sharon Oviatt, Antonella DeAngeli & Karen Kuhn**Center for Human-Computer CommunicationDepartment of Computer Science and EngineeringOregon Graduate Institute of  Science & TechnologyP.
O.
Box 91000, Portland, OR 97291 USA+1 503 690 1342oviatt@cse.ogi.edu; http://www.cse.ogi.edu/~oviatt/ABSTRACTOur ability to develop robust multimodal systems will depend on knowledge of the natural integration patterns that ypifypeople's combined use of different input modes.
To provide a foundation for theory and design, the present research analyzedmultimodal interaction while people spoke and wrote to a simulated ynamic map system.
Task analysis revealed thatmultimodal interaction occurred most frequently during spatial location commands, and with intermediate frequency duringselection commands.
In addition, microanalysis ofinput signals identified sequential, simultaneous, point-and-speak, ndcompound integration patterns, as well as data on the temporal precedence ofmodes and on inter-modal lags.
Insynchronizing input streams, the temporal precedence ofwriting over speech was a major theme, with pen input conveyinglocation information first in a sentence.
Linguistic analysis also revealed that he spoken and written modes consistentlysupplied complementary semantic nformation, rather than redundant.
One long-term goal of this research isthe developmentof predictive models of natural modality integration to guide the design of emerging multimodal rchitectures.Keywordsmultimodal interaction, integration and synchronization, speech and pen input, dynamic interactive maps, spatial locationinformation, predictive modelingINTRODUC~ONAs a new generation of multimodal/media systems begins to define itself, one theme that emerges frequently is theintegration and synchronization requirements for combining different modes into a strategic whole system.
From a linguisticperspective, the joint use of natural modes uch as speech and manual gesturing has been described uring human-humaninteraction, as has the role of gesture in beth discourse and in thought \[6,7\].
Gesture has been viewed as a cognitive aid in therealization of thinking, and also as a carrier of different semantic content than speech:"Speech and gestures are different material carriers.., they are not redundant but are related, and so the necessary tension canexist between them to propel thought forward.., to make the gesture is to bring the new thought into being on a concreteplane."
\[7, p.18\]The temporal synchrony between speech and gesture also has been analyzed for different languages \[7,8\].Currently, little parallel work is available on modality integration during human-computer interaction, although such work2 S. Oviatt, A. DeAngeli and K. Kuhnmmmmwill be crutial to guiding the design of planned multimodal systems.
Using simulated systems, empirical research as begunto reveal that contrastivefunctionality is an influential theme in users' multimodal integration of speech and writing.
That is,people use input modes in a contrastive manner to designate a shift in linguistic ontent or functionality-such as digit versustext, data versus command, or original versus corrected input \[14,15\].
Furthermore, during map-based tasks, interactingmultimodally with speech and writing has numerous performance advantages over unimodal interaction, primarily becausepeople have difficulty articulating spatial information \[10\].
In addition, users' frequency of composing multimodalcommands i higher in visual/spatial domains than in verbal or quantitative ones \[10\].
Among other things, these data suggestthat spatial domains may be ideal ones for developing early multimodal systems.The purpose of this research was to conduct acomprehensive exploratory analysis of multimodal integration andsynchronization patterns during pen/voice human-computer interaction.
To achieve this, a simulation experiment wasconducted in which people could combine spoken and pen-based input to interact multimodally while completing variedtasks using a dynamic map system.
One goal of the study was to identify when users are most likely to compose their inputmultimodally, rather than unimodally.
A task analysis of user commands was performed to distinguish the commonality ofthose expressed multimodally.mmmmmmmmA second goal of this research was to analyze the main linguistic features of multimodal constructions, aswell as differencesfrom standard unimodal ones.
Basic semantic onstituents were examined to determine their content, order, and the preferredmode used to convey them.
The type of pen input (e.g., graphics, symbols, pointing, words) also was analyzed for differenttypes of multimodal task command.A third goal of this research was to investigate how spoken and written modes are naturally integrated and synchronizedduring multimodal constructions.
The frequency of qualitatively different integration patterns was examined, such assequential, simultaneous, and point & speak.
Synchrony observed between the spoken and written signals was assessed fortemporal precedence ofone mode over the other, and for the typical ag between modes.METHODSubjects, Tasks, and ProcedureEighteen ative English speakers participated in this research, alf male and half female, and representing a broad spectrumof ages and professional careers.A "Service Transaction System" was simulated that could assist users with map-based tasks.
During a real estate selectiontask, participants were asked to select an appropriate home for a client.
They were provided with a thumbnail sketch of theclient's needs, such as acceptable price range.
Using a city map, they filtered available homes until locating one meeting theirconstraints.
For example, during such a task, a user could interact multimodally by circling a lakeside house icon with the penand asking "Is this house in a flood zone?
No flood zones, please."
In response, the system would answer textually whiledisplaying waterways and flood zones, and it would remove the house icon from the map if located in a hazard region.
In adistance calculation, as shown in Figure 1, the user could circle two entities and connect a line between them while asking,"How far from here to here?"
In response, the system would provide a numeric value in miles and a graphic onfirmation ofthe map endpoints.Integration and Synchronization ofInput Modes 3FIGURE 1.
A multimodal distance calculation request, inwhich the user circles two locations and connects hem with ink while speaking, "How far fromhere to here?
"During a map update task, people added, deleted, and modified information to represent changes in a high-growth municipalarea.
For example, a user could interact multimodally by drawing a square at a given location and saying "Make that achildren's hospital."
They also could draw a line along a road and say "Closed to traffic," or point to an arc across a highwayand say, "Move this overpass here \[drawing an arrow east\] so the main hospital connects with the children's hospital."
Duringall map tasks, users also controlled the map display by scrolling, automatically locating entities, zooming, and so forth, andthey used speech and pen input for these controls too.
In all cases, they interacted with an underlying map-based applicationas they added, removed, retrieved, or otherwise manipulated information to accomplish their task.During the study, subjects received instructions, a general orientation to the map system's coverage, and practice using thesystem until its capabilities were clear.
This orientation explained how to enter information on the LCD tablet when using thepen, speaking, or using a combination of both modes.
During practice, users completed entire tasks using only speech or onlypen, so they realized that the coverage of these alternative modes was equivalent.
When writing, they were free to use cursiveor printing, gestures, ymbols, drawn graphics, pointing, or other marks.
They were told to write information with theelectronic stylus directly onto the color map displayed on their LCD tablet.When speaking, subjects were instructed to tap and hold the stylus on the map as they spoke.
A click-to-speak interface wasused because off-line speech as been demonstrated to contain as many as 12,400% more unintelligible words than on-linespeech directed to the system \[13\].
That is, massive differences can exist between the intelligibility and processability ofspeech in a click-to-speak versus open-microphone implementation, with click-to-speak interfaces presently offering themore viable alternative.During the interactions reported in this study, people were free to use either or both input modes whenever they wished.
Theywere encouraged tospeak and write naturally, to work at their own pace, and to focus on completing their task.
Since the goalwas to uncover people's natural tendencies to interact multimodally and to integrate modes, an effort was made not toinfluence the manner in which they expressed themselves.
People were told that the map system was well developed andtested, so it would be able to handle most of their input.
If the system did not recognize their input, they always had theopportunity to re-enter their information.People also were instructed on completing tasks using two different presentation formats: (1) a structured reference map, withthe full network of roads, buildings, overlay information, and labels conventionally found on hard-copy reference maps, and(2) a less structured "minimalist" map, with one-third of the roads and overlay information as the more structured isplay,and only what was immediately needed to complete the task.
Both map formats provided the same rapid interactivity andmultimedia feedback (e.g., textual, graphic, synthetic speech) in response to user input.S.
Oviatt, A. DeAngeli and K. KuhnAfter the session, apost-experimental interview as conducted in which users were asked their preferences and evaluation ofthe system.
All users reported believing that the "system" was a functional one, after which they were debriefed aboutsimulation details.Semi-Automatic Simulation TechniquePeople's input actually was received by an informed assistant, who performed the role of interpreting and responding as thesystem.
The assistant tracked the subject's written and spoken input, and clicked on predefined fields at a Sun SPARCstationto send new map displays and confirmations back to the subject.
An emphasis was placed on automating the simulation tocreate rapid subject-paced interactions (i.e., averaging less than 1 sec delay) with clear feedback.
Details of the simulationmethod, capabilities, environment, and performance haracteristics have been provided elsewhere \[11\], although the methodwas adapted extensively for this study to handle the dynamic display of maps, overlays, and photographs.Research Design and Data CaptureEach of the 18 subjects completed one real estate and one map update task in each of the two formats-or four map tasksapiece.
Therefore, the present analyses focus on data collected uring 72 tasks, half using each of the two map formats, withorder counterbalanced.All interaction was videotaped and included areal-time record of all spoken and written input and system responses.Hardcopy multimodal transcripts also were created, with the subject's written input captured automatically in the current mapcontext, and verbatim spoken input transcribed onto the printouts.
Sequencing information was annotated for the two inputstreams, including temporal overlap at the word level.Transcript Preparation and CodingCoding was conducted on the multimodal corpus for the following dependent measures:User PreferenceThe percentage of subjects who chose to interact unimodally or mulfimodally during map tasks was summarized, as was thepercentage of subjects who reported uring interviews that they preferred to interact either unimodally or multimodally.TaskAc~onsIndividual user commands to the map system were classified into the following types of action command: (1) Add object orsubset of objects-which could involve specifying spatial location information about a point(s), line, or area-e.g., "Add historichomes here"; "Add open space park," (2) Move object o new location-e.g., "Move highway on-ramp to here," (3) Modifyspecific route or spatial area- e.g., "Close road west of May Lake as shown," (4) Calculate distance between twolocations-e.g., "How far from here to here?"
(5) Query for information about object-e.g.,"Is this house in Nevada City Schooldistrict?"
(6) Delete object-e.g., "Erase this line," (7) Label object-e.g., "This is an apple orchard," (8) Zoom on object-e.g.,"View house," (9) Control task procedures-e.g., "Next task please," (10) Scroll map-e.g., "Let's go up this way," (11) Printscreen display-e.g., "Print photo," (12) Automatically ocate out-of-view object-e.g., "Show me American Hill Park," (13)Call up overlay-e.g., "Show lakes," and (14) Specify constraints for filtering information-e.g., "Show me houses between$300,000 and $350,000."
Each individual user command found in the corpus was classified by type, and was scored ashaving been expressed unimodally or multimodally.Linguistic ContentAnalyses were conducted of multimodal constructions todetermine whether speech or writing was used to convey eachsemantic onstituent.
To assess whether the input order of multimodal constructions conformed with the canonical S-V-Oordering expected for English, the order of basic semantic constituents such as subject (S), verb (V), object (O), and locatives(LOC, e.g., "east of May Lake") was analyzed for individual multimodal constructions and unimodal spoken ones.For pen-based input, the basic type of semantic content was classified into the following: (1) drawn graphics (e.g., rectangleIntegration and Synchronization of Input Modes 5to indicate abuilding), (2) symbols and signs (e.g., > to indicate greater than), (3) simple pointing, (4) full and abbreviatedlexical words (e.g., BR for bedroom), and (5) digits.
The percentage of total pen input representing each of these categorieswas summarized for multimodal constructions a a function of task command, and for unimodal written ones.The type and frequency of spoken spatial deictics (e.g., "there") was summarized, aswas the percentage ofmultimodalconstructions containing a deictic term.Multimodal Integration PatternsIndividual constructions were classified into these integration patterns: (1) Simultaneous-spoken andwritten input overlappedtemporally, (2) Sequential-either spoken or written input preceded, with the other mode following after a time lag, (3) Point& speak-pointing at or on the border of an object while simultaneously peaking about it, but without creating drawn marksother than a singular dot, and (4) Compound-a two-part sequence involving a written input phase and a point & speak phase(e.g., creation of drawing, then point-and-speak bout it).For the sequential nd compound integration patterns, microanalyses were conducted from videotapes of which modepreceded the other, and of the average time lag between the end of the first input mode and the onset of the second one.Simultaneous constructions were classified into nine logically possible overlap atterns, displayed in Table 4.
Theseclassifications were designed to code the relative temporal order of signal onset and offset for spoken and written input, andto provide temporal distinctions about coordination between signals accurate to within 0.1 sec.
Simple point & speak inputwas not included in this analysis of simultaneity, since it was considered to involve completely overlapped signals bynecessity of the click-to-speak interface.In addition to analyses at the utterance l vel, the integration of spoken and written input was analyzed for multimodalconstructions with a spoken deictic.
In these cases, the temporal relation between the spoken deictic term and the specificpen-based mark that disambiguated the deictic's meaning were microanalyzed to determine whether they occurredsimultaneously or sequentially, and to assess typical precedence r lations and time lags.
These analyses were based ontemporal information about he onset and offset of the spoken deictic term, as well as spatial/temporal information about hebeginning and end of the formation of the relevant written mark, which were analyzed from videotapes.6 S. Oviatt, A. DeAngeli and K. Kuhn0@.9q20080706050403020100o 0 ,.~0?
_ n(,.I~ mFIGURE 2.
Percentage of all constructions that users expressed multimodally asa function of task command, with spatial location commands onthe right,selection commands in the middle, and general ction commands onthe left.ReliabilityAll reported measures had reliabilities of 0.80 or above, with inter-modal lags accurate to within 0.1 sec.Implemented System TestingAdditional testing was conducted to determine whether multimodal integration patterns would remain the same with a: (1)click-to-speak vs. open-microphone interface implementation, and (2) simulated vs. actual system testing.
The Quicksetsystem, which recognizes spoken and pen-based input to maps and has been modeled on the present simulation \[16\], wasadapted to accept open-microphone speech for testing purposes.
Data then were collected from 8 subjects as they providedspatial ocation commands (e.g., add or move objects) to the Quickset system.,RESULTSSince no differences were found in any measures due to the map's visual display, this section reports results collapsed overformat.User PreferenceUsers had a strong preference to interact multimodally during map tasks, rather than unimodally.
All of them, or 100%, usedboth spoken and pen input at different points during each task.
During interviews, 95% of users also reported apreference tointeract multimodally.Of 871 individual constructions, users expressed 167 multimodally by combining speech and writing within the samesentence, or 19%.
Unimodal writing accounted for 17.5% of all sentences, and the remaining 63.5% of sentences were utteredIntegration and Synchronization of Input Modes 7just using speech.Task Action AnalysisFigure 2 illustrates that type of task command greatly influenced the likelihood of expressing an utterance multimodally.
Inparticular, the four user commands most likely to be expressed multimodally were ones that required specifying a spatiallocation description.
These spatial location commands, which accounted for 86% of sentences that users constructedmultimodally, are displayed on the right side of Figure 2.
They tended to involve graphic input to add, move, modify, orcalculate the distance between objects.
Spatial ocation commands were the only ones more likely to be expressedmultimodally than unimodally-ranging between 51 and 77% of the time.Commands that involved selecting a specific object from others displayed on the map had an intermediate likelihood of beingexpressed multimodally.
These selection commands accounted for 11% of users' multimodal constructions, ranging between14 and 36% (Figure 2, middle).
Such commands identified an object of interest and its location, but no complex spatialinformation.
They included querying for information about an object, and deleting, labeling, or zooming on an object.Commands that involved selecting an in-view object were more likely to be expressed unimodally than multimodally,because the object sometimes was already in focus from: (1) previous dialogue context (e.g., user adds new map object, thendeletes it), or (2) visual context (e.g., user zooms on a house photo, then queries for information about it).
Sometimes theobject simply was one of a kind or easy to describe.
In none of these cases did the user have a compelling need to physicallygesture to an object o select it.The remaining six types of task command were rarely expressed multimodally, accounting for only 3% of all constructions(Figure 2, far left).
This third subgroup involved general action commands, which required neither a spatial description oridentification of an in-view object.
They included controlling task procedures, crolling the display, printing, automaticallylocating out-of-view objects, calling up map overlays, and specifying constraints for filtering information.A Wilcoxon Signed Ranks analysis confirmed that spatial ocation commands were significantly more likely to be expressedmultimodally than selection commands, T+ = 115 (df = 15), p < .0003, one-tailed.
In addition, selection commands weresignificantly more often multimodal than general action commands, T+ = 28 (df = 7), p < .008, one-tailed.Linguistic ContentMultimodal constructions were expressed in a telegraphic command language, which was briefer and less complexsyntactically than unimodal spoken sentences ( ee \[12\] for full linguistic analysis and processing implications).
Thefollowing illustrate typical user input from transcripts.Unimodal speech:"Add a boat dock on the west end of Reward Lake.
""I want to see the photo of the house on the southwest end of Reward Lake, please.
"Multimodal:\[draws line\] "Add dock here.
"\[circles house\] "Show photo.
"Order of Semantic ConstituentsAs expected, 98% of the unimodal spoken constructions conformed with the standard subject-verb-object order typical ofEnglish.
Almost all, or 97%, of multimodal constructions also conformed with this expected order, and when basicconstituents were elided the remaining ones still conformed (e.g., V-O, as in "Show photo").
The primary difference betweenspoken and multimodal sentences, as Table 1 clarifies, was in the typical position of locative constituents.
Spoken utterances8 S. Oviatt, A. DeAngeli and K. Kuhnrarely began with a locative descriptor or clause (i.e., 1% of constructions), and instead reserved LOCs for sentence-finalposition (i.e., 96%)-as in "Add apple orchard east of Sugarloaf Mountain Park" \[V-O-LOC\].
In contrast, multimodalconstructions invariably began with drawn graphics conveying LOC information (i.e., 95%) and were followed by spokenS-V-O constituents-as in \[draws oval\] "Add pool" \[LOC-V-O\], and LOCs never occurred in sentence-final position.TABLE 1.
Percentage of multimodal and speech-only constructions forwhich the locative constituent \[LOC\] occurred insentence initial vs. final position,rather than mid-sentence.Initial LOC Final LOCSpeech-only 1% 96%Multi-Modal 95% 0%Mode of Semantic ConstituentsIn multimodal constructions, pen input was used 100% of the time to convey location and spatial information about objects(i.e., size, shape, number).
In 2% of cases, speech provided uplicate but less precise information about location constituents.In comparison, speech was used for 100% of subject and verb constituents, although most subjects were elided as a result ofthe command language style.
The majority of object constituents, or 85%, also were spoken, although in 15% of cases writteninput identified the specific object.
At a semantic level, then, speech and writing clearly contributed different andcomplementary information.
It was rare for information to be duplicated in both modes.
However, there is a sense in whichspoken deictics provided uplicate information, since they simply flagged the fact that a location or object had been indicatedin writing.
Likewise, drawn graphics provided partially duplicated information about he type of object being added to themap (e.g., rectangles to indicate a shopping center), although subjects always followed drawings with more precise spokenobject descriptions.Type of Pen-based ContentOf the pen-based written input that people used during multimodal constructions, the majority, or 48%, involved rawngraphics (e.g., a square to represent a building; a line to represent roadway).
Another 28% involved symbols or signs (e.g., anX to delete; an arrow to indicate movement), and 17% involved simple pointing.
Only 7% of written input were actual words,and none were digits.Table 2 illustrates that different classes of written input predominated during different types of multimodal task command.Graphic input was the most prevalent dunng spatial location commands, occurring primarily when an object was added to themap.
In contrast, pointing predominated during selection commands, and written words during general action commands.Written symbols and signs were used in a relatively stable manner, irrespective of command type.
For comparison, the farright column of Table 2 illustrates that words were predominant during unimodal written sentences, with 52% of thesegeneral action commands.TABLE 2.
Categories of pen-based input during multimodal constructions (li ted by task command), and tmimodal written constructions.GraphicSymbolPointingWordsDigitsMultimodal Constructions by Command TypeGeneral \] SpatialAction Selection Location0% 9.5% 53%25% 33.5% 27.5%0% ~ 57% l 14%75% 0% 5.5%0% 0% 0%Unimodal PenConstructions9%32%0%48%11%Integration and Synchronization of Input Modes 9Deictic TermsMost multimodal utterances, or59%, did not contain a spoken deictic term.
When present, 96% of deictics involved the terms"here," there," this," or "that" (e.g., user circles house and says: "Is this brick?
").Mult imodal Integration PatternsTable 3 reveals that 86% of the 167 multimodal constructions involved adraw and speak pattern, with simultaneousintegration of drawing and speaking in 42% of constructions, sequential input in 32%, and a compound pattern in another12% (i.e., draw graphic, then point to it while speaking).
A point and speak pattern occurred far less frequently than drawingand speaking- in just 14% of constructions.TABLE 3.
Percentage of multimodal constructions represented by ifferent types of speech/writing tegration pattern.Type of Integration PatternPoint & SpeakDraw & Speak---Simultaneous Draw & Speak---Sequential Draw & Speak---Compound Draw & SpeakPercent of MultimodalConstructions14%86%(42%)(32%)(12%)Simultaneous IntegrationsSimultaneous draw-and-speak constructions were classified into nine possible synchronization patterns, displayed in Table 4with the percent of simultaneous constructions represented byeach.
Constructions in which the speech signal showedtemporal precedence (left column) accounted for 14% of the total, whereas those in which writing preceded speech (middlecolumn) accounted for most of the total, or 57%.
In the remaining cases, neither mode preceded (fight column).
A WilcoxonSigned Ranks analysis confirmed that written input was significantly more likely to precede speech than the reverse, T+ = 42(N = 9), p < .01, one-tailed.TABLE 4.
All logically-possible temporal overlap atterns between speech and written i put for simultaneous integrations, subclassified bytemporalprecedence of input mode.Speech Wd~ng NeJ~er ModePrecedes 14% Precedes 57% Precedes 29%vIll Ill Illv v'~(15~\] e__  (3~\] e 14~1 ev vTwo subjects produced utterances in which speech was abnormally elongated as a result of attempting toperfectlysynchronize the beginning and end of speech and drawing.
For example, while marking a closed section of road one subjectsaid, "No automobiles" (underlined syllables elongated).
However, only 2% of mulfimodal utterances were affected by suchdistortion.Sequential IntegrationsAnalysis of the sequential constructions again revealed the temporal precedence of written input.
In 99% of such10 S. Oviatt, A. DeAngeli and K. Kuhn\[\]\[\]\[\]\[\]constructions, a drawn graphic was completed before the onset of spoken input.
A Wilcoxon Signed Ranks analysisconfirmed that users were significantly more likely to complete their pen input before speaking than the reverse, T+ = 105 (N= 14), p < .001, one-tailed.Figure 3 illustrates the distribution of lags for sequential constructions.
The lag between the end of the pen signal and start ofspeech averaged 1.4 sees, with 70% of all lags ranging between 0.0 and 2.0 see, 88% between 0.0 and 3.0 sec, and 100%between 0.0 and 4.0 see.4035ao~ z56ao~!1 10~ 5oO- 1 1-2 2-3 3-4Lo4' ia ile?o~sFIGURE 3.
Distribution of lag times between end of pen signal and onset of speech in sequential multimodal constructions.
I!Integration of Deictic TermsIn addition to analyses at the utterance l vel, the temporal relation between spoken deictic terms and the specific written markthat disambiguated the deietic's meaning also was analyzed to determine whether these related signals co-occurred.
Out ofthe 41% of multimodal constructions that did contain a spoken deictic, most (43%) displayed asequential integration pattern.In 100% of these cases, the drawn mark that disambiguated the deictic's meaning was completed before the deicfic term wasspoken, with an average lag of 1.1 see and a maximum lag of 3 see 97% of the time.
In another 27% of multimodalconstructions, the spoken deicfic and relevant ink occurred simultaneously.
Further analysis of the temporal overlap revealedthat the onset of writing still preceded speech in 60% of these cases, and in only 5% did speech onset precede writing.
In theremaining 30% of multimodal constructions, spoken deictics were accompanied by simultaneous pointing or a compoundintegration pattern.
In summary, the temporal precedence of written input continued as a theme in deictic integration patterns.Integration in Open-Microphone System \[\]Testing with an open-mic system was conducted mainly to evaluate whether use of the pen to engage the click-to-speakmechanism ight influence the temporal precedence of written input over speech.
However, the results of analyses on 170additional mulfimodal constructions replicated simulation findings: (1) in sequential constructions, pen input preceded speech100% of the time, and (2) in simultaneous constructions, the onset of pen input preceded speech 61% of the time (speechpreceded 5%; neither mode 34%).
The general frequency of sequential nd simultaneous constructions also was comparableto that found in the simulation data, and pen input again conveyed locative semantic ontent in LOC-S-V-O order.DISCUSSIONAlthough users overwhelmingly preferred to interact mulfimodally rather than unimodally, they nonetheless did not issueevery command to the system mulfimodally.
Knowledge of the type of task command provides considerable predictiveinformation about its likelihood of being expressed multimodally.
During interaction with complex visual displays uch asmaps, spatial ocation commands (e.g., adding, moving) were by far more likely to be composed multimodaily thanunimodally-by a factor of 2-to-1.
In fact, spatial ocation commands accounted for the vast majority, or over 86%, of thesentences that users chose to express multimodally.
For these commands, users needed to specify information about helocation, number, size, and/or shape of a point, line, or area.
In contrast, selection commands in which users identified oneobject from a set (e.g., zooming on, deleting) only occasionally were expressed multimodally, since the presence of a uniqueIntegration and Synchronization of Input Modes 11descriptor, immediate dialogue context, or the map context often made physical selection unnecessary.
In users' view, whencontextual information was present, hey expected to use it and did not automatically issue a selection gesture too.
Likewise,general action commands that entailed neither spatial nor selection information (e.g., calling up overlay, printing) rarely wereexpressed multimodally-only 1% of the time.One implication of these findings is that knowledge of the task commands anticipated in an application could influence thefundamental design choice to build a multimodal versus unimodal interface.
In a multimodal system, knowledge of a givencommand (generally indicated by the spoken verb) also could be used to weight likelihoods that the incoming signal is: (1)unimodal, or (2) part of a multimodal construction i which speech input is expected to follow pen within a given lag.
In thelatter case, knowledge about he type of command could influence architectural decisions about when to begin processing,and the signal's interpretation.Among the powerful interface features of pen-based input are its ability to convey precise location information and detailedgraphic renderings.
Another is the multi-functional capability of pen systems, which can support qualitatively different typesof input such as drawings, symbols and signs, gestures, words, digits, and pointing.
The present data indicate that knowledgeof the command type also provides predictive information about he kind of pen input most likely to be elicited from users,which will need to be processed by the recognizer.
For example, spatial location commands (e.g., add) primarily eliciteddrawn graphics, whereas election commands elicited pointing and gestures (e.g., circling an object), and general actioncommands elicited words.
In designing future multimodal pplications, information about expected task commands thereforeought o be considered before specifying a planned system's basic recognition capabilities.The past literature on multimodal systems has focused largely on simple selection of objects or locations in a display, ratherthan considering the broader range of multimodal integration patterns.
In this corpus, speech combined with pointing forselection was not the dominant integration theme, accounting for only 17% of multimodal constructions.
Most pen input wasnot written words either (accounting for 7%), perhaps contrary to expectations of the handwriting recognition community.Instead, drawn graphics (e.g., square for building) and symbols/signs (e.g., arrow to indicate movement) accounted for most,or 76%, of all written input.
Given the more powerful and multifunctional capabilities of new pen devices, which cangenerate symbolic information as well as selecting things, it is clear that a broader set of multimodal integration issues needsto be addressed in future work.Previous pecialized processing approaches based on the interpretation f spoken deictics via synchronous pointing (i.e., by"calling out" for a matching x,y coordinate on a display to resolve an intended referent in a phrase like "that blue square"\[ 1,4\]) are unlikely to play a large role in handling the types of construction actually observed in the present corpus.
First,most multimodal constructions, or 59%, did not contain any spoken deictic, so one cannot count on their presence to flag andassist in interpreting the referent in a visual display.
Second, even fewer multimodal constructions, or 25%, contained aspoken deictic that overlapped in time with the pen input needed to disambiguate itsmeaning.
Third, as noted above, only17% of multimodal constructions involved asimple point-and-speak pattern.
Finally, as the present data attest, users actuallymay only compose individual sentences multimodally a limited percentage of the time.
To process what may be as many as80% of linguistic onstructions unimodally in a multimodal-capable interface, a system designed for a real application mustbe able to interpret standard unimodal referring expressions and resolve reference through both dialogue and visual context asin previous multimodal designs \[2,9\].
In this context, specialized algorithms for processing deictic-point relations havelimited practical utility.One important distinguishing characteristic of spoken and pen input is that both modes can convey symbolic ontent such aslanguage.
Analysis of the linguistic ontent of integrated speech/writing constructions in this study revealed severalinteresting things.
First, at a semantic level, the spoken and written modes consistently contributed different andcomplementary information.
Basic constituents describing the subject, verb, and object almost always were spoken, whereasconstituents describing locative information i variably were written.
Furthermore, consistent with McNeill's \[7\]observations, it was extremely rare for such information to be duplicated inboth modes.
These data confirm the importanceof contrastivefunctionality s a major theme that drives the overall patterning of people's integrated use of input modes\[14\]-with locative/nonlocative content the salient contrast in this visual/spatial domain.
Second, multimodal constructionswere briefer and syntactically simpler than unimodal spoken ones, and therefore potentially easier for a system to process(see \[12\] for further discussion).
Third, the order of incoming linguistic information i multimodal constructions clearlydeparted from the canonical S-V-O-LOC order typical of spoken English.
Instead, pen-based locative information waspresented first and followed by spoken constituents, resulting in a LOC-S-V-O sequence.With respect to synchronization f input streams, a major theme for both sequential nd simultaneous patterns was the strong12 S. Oviatt, A. DeAngeli and K. Kuhntemporal precedence ofwritten input, which prevailed independent of the click-to-speak oropen-microphoneimplementation.
Dunng sequentially integrated draw-and-speak constructions, a drawn graphic was completed before theonset of any spoken input 99% of the time.
Analysis of the lags revealed that speech followed writing within an average of1.4 seconds, and always began within 4 seconds of pen input.
When drawing and speech overlapped in simultaneousconstructions, the onset of pen input still preceded speech more often than the reverse (57% vs. 14% of cases).
Finally,analysis of spoken deictics and their disambiguating marks revealed that pen input preceded the deictic term 100% of thetime when these signals were sequential, and 60% of the time when simultaneous.
This observed precedence ofpen inputgeneralized over both system and simulation testing, involving click-to-speak and open-microphone i terfaces.
Futuresimulation research should explore typical integration patterns between other promising modality combinations, such asspeech and 3-D gestures or speech and gaze, for interacting with other types of visual display- as well as their relation to thespoken and pen-based integration patterns reported here.One interpretation f the temporal precedence ofwriting to convey locative content is that users were elaborating the visualcontext of the map with their ink marks and, after this expanded context was available, they then continued by speaking aboutit.
The act of drawing and permanence of the written marks may have had an important self-organizing influence on usersthinking and subsequent speech.
During interpersonal communication, both signed language and natural gestures also havebeen reported to precede or occur simultaneously with their spoken lexical analogues \[3,5,8\].
Some variation has been foundin integration patterns between languages, uch that topic-prominent languages like Chinese present gestures further inadvance of the speech stream (i.e., as a kind of "framing constraint" for the sentence) than do subject-prominent languageslike Spanish or English \[7\].
Although gestunng is ephemeral nd seemingly unlike the permanence of ink, people sometimesengage in a "poststroke hold" that can perpetuate he gesture as a visual context for speech in the same way that ink does.
Inthis sense, the dynamics of context-setting can function similarly in eliciting advance writing and manual gestunng.From a more pragmatic perspective, the order of input modes and average lag times reported in this paper could be used toweight probabilities associated with the likelihood that a sentence is mulfimodal versus unimodal, the likelihoods associatedwith different utterance segmentations (e.g., that an input stream containing \[speech, writing, speech\] should be segmentedinto \[S / W S\] rather than \[S W / S\]), and to correctly recognize content within the spoken and written input streams.
Currentsystems that ime-stamp and jointly process two or more input modes have not reported temporal thresholds for performingintegrations between modes.
Data on typical inter-modal lags collected uring realistic interactive tasks, such as thosereported here, could form the basis of highly accurate mode integrations in future multimodal systems.The present empirical research as inspired the design and architectural implementation f multimodal systems in ourlaboratory, which support map-based applications ranging from real-estate and health-care s lection to military simulation\[16\].
In these systems, the user communicates through ahand-held PC that processes speech and pen input in parallel, using ajoint interpretation strategy involving a statistically-ranked unification of semantic interpretations.
Compared with unimodalrecognition, such systems have the advantage of supporting mutual disambiguation f linguistic ontent and reduction oferror.
Given the complex and nonintuitive nature of users' mulfimodal interaction during real tasks, empirical work will beessential in guiding the design of future robust multimodal systems.FOOTNOTES* This research was supported inpart by NSF Grants No.
IRI-9213472 and IRI-9530666, DARPA Contract DABT63-95-C-0007, and grants, contracts, andequipment donations from Apple, AT&T/NCR, ETRI, Intel, NTT, Scnptel, Southwestern Bell, Sun Microsystems, and US West.
** Collaborators' affiliations: Psychology Dept., University of Trieste, and Linguistics Dept., Portland State University.1 Empirical analysis confirmed that intentional pointing to a particular referent was distinct from untargeted tapping on the tablet simply to engage theclick-to-speak interface (i.e., for which the pen could drop to the nearest tablet location), with the former averaging 1.7 sec, versus 1.4 sec for the latter.2 Spoken deictic terms uch as "here" and "this" point out locations in the spatial context shared by communication participants, and often are accompaniedby gesturing.3 Gesture formation can be classified into a preparatory phase, main stroke, poststroke hold, and retraction.ACKNOWLEDGMENTSIntegration and Synchronization of Input Modes 13Thanks to A. Cheyer, M. Reinfreid, and L. Waugh for assistance_ with programming the simulation and acting as simulationassistant, and to P. Schmidt and D. McGee for preparing the open-mic Quickset system.
Thanks also to P. Li, E. Olsen, M.Heitz and R. vanGent for transcription and data analysis, and to P. Cohen, M. Johnston and a CHI reviewer for valuablecomments.REFERENCES1.
Bolt, R. "Put-That-There": Voice and Gesture at the graphics interface, Computer Graphics, 1980, 14 (3): 262-270.2.
Cohen, P., Dalrymple, M., Moran, D. & Pereira, F. Synergistic use of direct manipulation and natural language, CHI '89Conf.
Proc., ACM: Addison Wesley, New York, 1989, 227-234.3.
Kendon, A. Gesticulation and speech: Two aspects of the process of utterance, The Relationship of Verbal and NonverbalCommunication (ed.
by M. Key), The Hague: Mouton, 1980, 207-227.4.
Koons, D., Sparrell, C. & Thorisson, K. Integrating simultaneous input from speech, gaze, and hand gestures, IntelligentMultimedia Interfaces, ed.
by M. Maybury, MIT Press: Cambridge, MA, 1993, 257-76.5.
Levelt, W., Richardson, G. & Heu, W. Pointing and voicing in deictic expressions, Jour.
of Memory and Language, 1985,24, 133-164.6.
McNeill, D. Hand and Mind: What gestures reveal about hought, Univ.
of Chicago Press: Chicago, II1., 1992.7.
McNeill, D. Language as gesture (Gesture as language), Proc.
of the Workshop on the Integration of Gesture in Language& Speech, ed.
by L. Messing, Univ.
of Delaware, Oct. 1996, 1-20.8.
Naughton, K. Spontaneous gesture and sign: A study of ASL signs co-occurring with speech, Proc.
of the Workshop on theIntegration of Gesture in Language & Speech, ed.
by L. Messing, Univ.
of Delaware, Oct. 1996, 125-34.9.
Neal, J.
& Shapiro, S. Intelligent multi-media nterface technology, in Intelligent User Interfaces (J. Sullivan & S.
Tyler,eds.
), ACM: Addison Wesley, New York, 1991, ch.
3, 45-68.10.
Oviatt, S.L.
Multimodal interfaces for dynamic interactive maps, CHI '96 Conf.
Proc., New York, ACM Press, 1996,95-102.11.
Oviatt, S., Cohen, P., Fong, M., & Frank, M. A rapid semi-automatic simulation technique for investigating interactivespeech and handwriting, Proc.
of the Intl.
Conf.
on Spoken Language Processing, 1992, 2, 1351-54.12.
Oviatt, S., Cohen, P. Johnston, M. & Kuhn, K. Mulfimodal anguage: Linguistic features and processing requirements,forthcoming.13.
Oviatt, S., Cohen, P~ & Wang, M. Toward interface design for human language technology: Modality and structure asdeterminants of linguistic complexity, Speech Communication, 1994,15 (3-4), 283-300.14.
Oviatt, S. & Olsen, E. Integration themes in multimodal human-computer interaction, Proc.
of the lntl Conf.
on SpokenLanguage Processing, 1994, 2, 551-554.15.
Oviatt, S. L. & vanGent, R. Error resolution during multimodal human-computer interaction, Proc.
of the Intl.
Conf.
onSpoken Language Processing, 1996.16.
Pittman, J., Cohen, P., Smith, I., Yang, T. & Oviatt, S. Quickset: A multimodal interface for distributed interactivesimulations, Proc.
of the 6th Conf.
on Computer-Generated Forces & Behavior Representation, Univ.
of Central Florida,Orlando, FL., 1996, 217-24.
