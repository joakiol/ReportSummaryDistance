Mining Discourse Markers for Chinese Textual SummarizationSamuel W. K. Chan I, Tom B. Y. Lai 2, W. J. Gao 3, Benjamin K. T'sou 4J 24Languag e Information Sciences Research CentreCity Un!versity of Hong KongTat CheeAvenue, Kowloon Tong,Hong Kong SAR, China3 North Eastern University, ChinaIswkchan@cs.cityu.edu.hk, {2cttomlai, 4rlbtsou} @cpccux0.cityu.edu.hk, 3wjgao@ramm.neu.edu.cnAbstractDiscourse markers foreshadow the messagethrust of texts and saliently guide theirrhetorical structure which are important forcontent filtering and text abstraction.
Thispaper reports on efforts to automaticallyidentify and classify discourse markers inChinese texts using heuristic-based andcorpus-based ata-mining methods, as anintegral part of automatic textsummarization via rhetorical structure andDiscourse Markers.
Encouraging results arereported.1 IntroductionDiscourse is understood to refer to any form oflanguage-based communication involving multiplesentences or utterances.
The most important formsof discourse of interest o computerized natural.language processing are text and dialogue.
Whilediscourse such as written text normally appears to?
be a linear sequence of clauses and sentences, ithas "long been recognized by linguists that theseclauses and sentences tend to cluster together intounits, called discourse segments, that are relatedpragmatically toform a hierarchical structure.Discourse analysis goes beyond the levels ofsyntactic and semantic analysis, which typicallytreats each sentence as an isolated, independentunit.
The function of discourse analysis is todivide a text into discourse segments, and torecognize and re-construct the discourse structureof the text as intended by its author.
Results ofdiscourse analysis can be used to solve manyimportant NLP problems such as anaphoricreference (Hirst 1981), tense and aspect analysis(Hwang and Schubert 1992), intention recognition(Grosz and Sidner 1986; Litman and Allen 1990),or'can be directly applied to computational NLPapplications uch as text abstraction (Ono et al1994; T'sou et al 1996) and text generation(McKeown 1985; Lin et al 1991).Automatic text abstraction has receivedconsiderable attention (see Paice (1990) for acomprehensive r view).
While some statisticalapproaches have had some success in extractingone or more sentences which can serve as asummary (Brandow et al 1995; Kupiec et al 1995;Salton et al 1997), summarization i  general hasremained an elusive task.
McKeown and Radev(1995) develop a system SUMMONS tosummarize full text input using templatesproduced by the message understanding systems,developed under ARPA human languagetechnology.
Unlike previous approaches, theirsystem summarizes a series of news articles on thesame event, producing a paragraph consisting ofone or more sentences.
Endres-Niggemeyer et al(1995) uses a blackboard system architecture withco-operating object-oriented agents and a dynamictext representation which borrows its conceptualrelations from Rhetorical Structure Theory (RST)(Mann and Thompson 1986).
Furthermore,connectionist models of discourse summarizationhave also attracted a lot of attention (Aretoulaki etal.
1998).
The main underlying principles are thedistributed encoding of concepts and thesimulation of human association with a largeamount of processing nodes.
What is crucial inthis approach is to provide a subconceptual l yerin the linguistic reasoning.As in Paice (1990), summarizationtechniques in text analysis are severely impairedby the absence of a generally accepted iscourse11model and the use of superstructural schemes ispromising for abstracting text.
Johnson et al (1993)describes a text processing system that canidentify anaphors o that they may be utilized toenhance sentence selection.
It is based on theassumption that sentences which contain non-anaphoric noun phrases and introduce keyconcepts into the text are worthy of inclusion in anabstract.
Ono et al (1994), T'sou et al (1992) andMarcu (1997) focus on discourse structure insummarization using the Rhetorical StructureTheory (RST).
The theory has been exploited in a.number of computational systems (e.g.
Hovy1993).
The main idea is to build a discourse treewhere each node of the tree represents a RSTrelation.
Summarization is achieved by trimmingunimportant sentences on the basis of the relativesaliency or rhetorical relations.
On the other hand,cohesion can also provide context o aid in theresolution of ambiguity as well as in textsummarization (Halliday and Hasan 1976; Morrisand Hirst 1991; Hearst 1997).
Mani et al (1998)describes a method based on text coherence whichmodels text in terms of macro-level relationsbetween clauses or sentences tohelp determine theoverall argumentative structure of the text.
Theyexamine the extent to which cohesion andcoherence can each be used to establish saliency oftextual units.The SIFAS (S,yntactic Marker based Eull-Text Abstration System) system has been designedand implemented to use discourse markers in theautomatic summarization of Chinese.
Section 2provides an introduction to discourse markers inChinese.
An overview of SIFAS is presented inSection 3.
In Section 4, we describe a codingscheme for tagging every discourse markerappearing in the SIFAS corpus.
In Section 5, weintroduce a heuristic-based algorithm forautomatic tagging of discourse markers.
In Section6, we describe the application of the C4.5algorithm to the same task.
In Section 7, wepresent he evaluation results of applying the twoalgorithms to corpus tagging, followed by aconclusion.2 Chinese Discourse MarkersAmong all kinds of information that may be foundin a piece of discourse, discourse markers (alsoknown as discourse connectives, clue words(Reichman 1978; Siegel et al 1994) or cue phrases(Grosz et al 1986; Litman 1996) are regarded asthe major linguistic deviceavailable for a writer tostructure a discourse.
Discourse markers areexpressions which signal a sequential relationshipbetween the current basic message and theprevious discourse.
Schiffrin (1987) is concernedwith elements which mark sequentially dependentunits of discourse.
She examines discoursemarkers in interview data, looking specifically attheir distribution and their particularinterpretation(s).
She proposes that these markerstypically serve three functions: (i) they indexadjacent utterances to the speaker, the hearer, orboth; (ii) they index adjacent utterances to priorand/or subsequent discourse; (iii) they work ascontextual coordinates for utterances by locatingthem on one or more planes of her discoursemodel.Discourse markers also figure prominently inChinese which has a tendency to delay topicintroduction (Kaplan 1996; Kirkpatrick 1993).Hinds (1982) and Kong (1998) also maintain thatthe Chinese tendency of delayed topic introductionis heavily influenced by the qi cheng zhuan hecanonical structure (a Chinese rhetorical pattern).In a study examining rhetorical structure inChinese, Kirkpatrick (1993) found that severalmajor patterns, favored and considered to be goodstyle by native Chinese writers, are hinted at byChinese discourse markers.
Although the effect ofdiscourse markers in other languages might not betoo prominent, here is a great necessity to studydiscourse markers in Chinese in order to capturethe major associated rhetorical patterns in Chinesetexts.
While the full semantic understanding inChinese texts is obviously much more difficult toaccomplish, the approach using text miningtechniques in identifying discourse markers andassociated rhetorical structures in a sizeableChinese corpus will be certainly beneficial to anylanguage processing, such as summarization andknowledge xtraction i  Chinese.In Chinese, two distinct classes of discoursemarkers are useful for identification andinterpretation of the discourse structure of aChinese text: pr imary discourse markers andsecondary discourse markers (T'sou et al 1999).Discourse markers can be either words or phrases.Table 1 provides a sample listing of various12IIIIiIIrhetorical relations and examples considered inthis research.\[Discourse TypeSufficiencyNecessityCausalityDeduction?\[dversativityConcessionConjunctionDisjunctionProgressionTableDiscoursePrimary Markerruguo 'if', name 'then'zhiyou 'only if', cai 'only \[hen'?inwei 'because', suoyi 'therefore'iiran 'given that', name 'then'suiran 'although', danshi 'but'"ishi 'even if', rengran 'still'chule 'except', j ianzhi 'also'huozhe 'or', huozhe 'or'~udan 'not only', erqie 'but also'/Examples of Discourse MarkersMarkersDiscourse TypeSummaryContrastfflustrationSpecificationGeneralizationDigressionrtemizationParaphrasingEquivalenceEnquiryludgmentSecondary Markerzong er yan zhi 'in one word'~hishi shang 'in fact'liru 'for example'tebie shi 'in particular'dati er yan 'in general'wulun ruhe 'anyway'shouxian 'first', qici "next"huan ju hua shuo 'in other words'zhengru 'just as'nandao ('does it mean... ')kexi 'unfortunately'and Associated Rhetorical Relations in ChineseIt may be noted that our analysis of Chinesehas yielded about 150 discourse markers, and thaton the average, argumentative t xt (e.g.
editorials)in Chinese shows more than one third of thediscourse segments to contain discourse markers.While primary discourse markers can be paireddiscontinuous constituents, with each markerattached to one of the two utterances orpropositions, the socondary discourse markerstend to be unitary constituents only.
In the case ofprimary discourse markers, it is quite common thatone member of the pair is deleted, unless foremphasis.
The deletion of both discourse markersts also possible.
The recovery process thereforefaces considerable challenge ven when concerned?
with the deletion of only one member of the paireddiscourse markers.
Since these discourse markers'have no unique lexical realization, there is also theneed for disambiguation i  a homocode problem.Moreover, primary discourse markers canalso be classified as simple adverbials, as is thecase in English:(I) Even though a child, John is so tall thathe has problem getting half-fare.
(2) Even though a child, (because) John istall, so he has problem getting half-fare.In (1), so is usually classified as an adverbwithin a sentence, but in (2) so is recognized asmarking a change in message thrust at thediscourse level.In the deeper linguistic analysis the two so'smay be related, for they refer to a situationinvolving excessive height with impliedconsequence which may or may not be stated.
Interms of the surface syntactic structure, so in (1)can occur in a simple (exclamatory) sentence (e.g.
"John is so tall!
"), but so in (2) must occur in thecontext of complex sentences.
Our concern in thisproject is to identify so in the discourse sense as in(2) in contrast to so used as an adverb in thesentential sense as in (1).
Similar difficulties arefound in Chinese, as discussed in Section 7.3 SIFAS System ArchitectureFrom the perspective of discourse analysis, thestudy of discourse markers basically involves fourdistinct but fundamental issues: 1) the occurrenceand the frequency of occurrence of discoursemarkers (Moser and Moore 1995), 2) determiningwhether a candidate linguistic item is a discoursemarker (identification / disambiguation)(Hirschberg and Litman 1993; Siegel andMcKeown 1994), 3) determination or selection ofthe discourse function of an identified discoursemarker (Moser and Moore 1995), and 4) thecoverage capabilities (in terms of levels ofembedding) among rhetorical relations, as well asamong individual discourse markers.
Discussionof these problems for Chinese compoundsentences can be found in Wang et al (1994).Previous attempts to address the aboveproblems in Chinese text have usually been basedon the investigators' intuition and knowledge, oron a small number of constructed examples.
In ourcurrent research, we adopt heuristics-based13corpus-basedlearning to discover the correlation betweenvarious linguistic features and different aspects ofapproaches, and use machine discourse marker usage.
Our research frameworkiStatistical Analysis iDiscourse AnalysisText Abstraction ii Natural Language 1\[ Understanding\[!IAnalysis&Applicationis shown in Figure I.Raw Corpus ....... (Editorials)...... ~ ' " !
....... ~,~uto Tagging &ProofreadingSegmentedCorpus "~'~-~ ~ J  Word~.-~.._~.
: .
.
.
.
SegmentationDiscourseMarker &RhetoricalRelation TaggedCorpusFeature.. ~ ~.
.
ExtractiovFeatureDatabase; i i 1f f  ~ iji Dicti?naries \] ,~ ,,Heuristics I / .......... ;-i Induced Rules !
\ ........... ~ i?
"- '~- ~ ML &Evaluationxq=_.OQK?-)- i?tlt-' ?D,<Figure 1 Framework for Corpus-based Study of Discourse Marker Usage in Chinese TextData in the segmented corpus are dividedinto two sets of texts, namely, the training set and:the test set, each of which includes 40 editorials in:our present research.
Texts in the training set are.
manually and semi-automatically tagged to reflect where,the  properties of every Candidate Discourse DMi:Marker (CDM).
Texts in the test set areautomatically tagged and proofread.
Differentalgorithms, depending on the features being RRi:investigated, are derived to automatically extractthe interesting features to form a feature database.
RPi:Machine learning algorithms are then applied tothe feature database to generate linguistic rules(decision trees) reflecting the characteristics ofvarious discourse markers and the relevant CT~:rhetorical relations.
For every induced rule (or acombination of them), its performance is evaluatedby tagging the discourse markers appearing in th-test set of the corpus.4 A Framework for Tagging MN~:Discourse MarkersThe following coding scheme is designed toencode all and only Real Discourse Markers RN~:(RDM) appearing in the SIFAS corpus.
Wedescribe the i th discourse marker with a 7-tupleRDMi,RDMi=< DMI, RR/,  RPI, CTi, MNi ,  RNI ,>the lexical item of the Discourse Marker,or the value 'NULL'.the Rhetorical Relation in which DMi isone of the constituting markers.the Relative Position of DM;.
The valueof RPi can be either 'Front' or 'Back'denoting the relative posit ion of themarker in the rhetorical relation RRi.the Connection Type of RRi.
The valueof CT~ can be either 'Inter" or ' Intra',which indicates that the DM~ functions asa discourse marker in an inter-sentencerelation or an Intra-sentence relation.the Discourse Marker Sequence Number.The value of MNi is assignedsequentially from the beginning of theprocessed text to the end.the Rhetorical Relation SequenceNumber.
The value of RNi is assigned14IIIIIlsequentially to the correspondingrhetorical relation RR; in the text.OTi: the Order Type of RR;.
The value of OTican be 1, -1 or 0, denoting respectivelythe normal order, reverse order orirrelevance of the premise-consequenceordering of RRI.For Apparent Discourse Markers (ADM) that donot function as real discourse markers in a text, adifferent 3-tuple coding scheme is used to encodethem:ADM~ = < LIi, *, SNi > where,LIi: the Lexical Item of the ADM.SNi: the Sequence Number of the ADM.To illustrate the above coding schemeconsider the following examples of encodedsentences where every CDM has been tagged to beeither a 7-tuple or a 3-tuple.Example 1<vouvu ('because').Causalitv.
Front.
lntra.
2.
2./> Zhu Pei ('Jospin') zhengfu ('government')taidu ('attitude') qiangying ('adamant'), chaoye('government-public') duikang ('confrontation')yue-yan-yue ('more-develop-more') -lie('strong'), <NULL.
Causality.
Back.
Intra, O.
2./> gongchao ('labour unrest') <vi ('with').
*:1> liaoyuan ('bum-plain') zhi ('gen') shi'tendency' xunshu 'quick' poji 'spread to' ge('every') hang ('profession') ge ('every') ye, ('trade').
'As a result of the adamant attitude of theJospin administration, confrontation betweenthe government and the public is becomingw.orse and worse.
Labour unrest has spreadquickly to all industrial sectors.
'From the above tagging, we can immediatelyobtain the discourse structure that the two clausesencapsulated by the two discourse markers youyu(with sequence number 2) and NULL (withsequence number 0).
They have formed a causalityrelation (with sequence number 2).
We denote thisas a binary relationCausality(FrontClause(2), BaekClause(2))where FrontClause(n) denotes the discoursesegment that is encapsulated by the Frontdiscourse marker of the corresponding rhetoricalrelation whose sequence number is n.15BackClause(n) can be defined similarly.
Note thatalthough yi is a CDM, it does not function as adiscourse indicator in this sentence.
Therefore, it is "encoded as an apparent discourse marker.Example 2<dan ('however').
Adversativitv.
Back.
Inter.17.
14.
1> <ruguo 'if'.
Su_~ciencv.
Front.
Inter,18.
15.
1> Zhu Pei ('Jospin') zhengfu('government') cici ('this time') zai ('at')gongchao ('labour unrest') mianqian ('in theface of') tuique ('back down'), <NULL.Su.~ciencv.
Back.
Inter.
O.
15.
1> houguo('result') <geng.('more').
*.
3> shi bukan ('isunbearable') shexian ('imagine').
'However, if the Jospin administration backsdown in the face of the labour unrest, the resultwill be terrible.
'From the above tagging, we can obtain thefollowing discourse structure with embeddingrelations:A dversativity ( &F (14 ),Sufficiency(F rontClause(15),BackClause(15)))where &F(n) denotes the Front discourse segmentof an inter-sentence rhetorical relation whosesequence number is n. We can define &B(n)similarly.5 Heuristic-based Tagging ofDiscourse MarkersIn the previous section, we have introduced acoding, scheme for CDMs, and have explainedhow to automatically derive the discoursestructure from sentences with tagged discoursemarkers.
Now, the problem we have to resolve is:Is there an algorithm that will tag the markersaccording to the above encoding scheme?To derive such an algorithm,-even animperfect one, it is necessary that we haveknowledge of the usage patterns and statistics ofdiscourse markers in unrestricted texts.
This isexactly what project SIFAS intends to achieve asexplained in Section 3.
Instead of completelyrelying on a human encoder to encode all thetraining texts in the SIFAS corpus, we haveexperimented with a simple algorithm using asmall number of heuristic rules to automaticallyencode the CDMs.
The algorithm is astraightforward matching algorithm for rhetoricalrelations based recognition of their constituentdiscourse markers as specified in the RhetoricalRelation Dictionary (T'sou et al 1999).
Thefollowing principles are adopted by the heuristic-based algorithm to resolve ambiguous ituationsencountered in the process of matching discoursemarkers:(1) Principle of Greediness: When matching apair of CDMs for a rhetorical relation,priority is given to the first matched relationfrom the left.
(2) Principle of Locality: When matching a pairof CDMs for a rhetorical relation, priority isgiven to the relation where the distancebetween its constituent CDMs is shortest.
(3) Principle of Explicitness: When matching apair of CDMs for a rhetorical relation,priority is given to the relation that has bothCDMs explicitly present.
(4) Principle of Superiority: When matching apair of CDMs for a rhetorical relation,priority is given to the inter-sentence relationwhose back discourse marker matches thefirst CDM of a sentence.
(5) Principle of Back-Marker Preference: thisprinciple is applicable only to rhetoricalrelations where either the front or the backmarker is absent.
In such cases, priority isgiven to the relation with the back markerpresent.'
Application of the above principles toprocess a text is in the order shown, with the?
exception that the principle of greediness isapplied whenever none of the other principles canbe, used to resolve an ambiguous ituation.
Thefollowing pseudo code realizes principles 1, 2 and3:I := l ;whi le I < NumberOfCDMsInTheSentence  dobeg infor J := l  to NumberOfCDMsInTheSentencen  -I doi f  ((not CDMs\ [ J \ ] .Tagged)  and (notCDMs\ [ J+ I \ ] .Tagged)}  thenMatch ing(CDMs\ [ J \ ] ,  CDMs\ [ J+ I \ ] )  ;I := I + 1 ;end ;The following code realizes principles 1,4 and 5:16for I :=l  to NumberOfCDMs lnTheSentence  dobeginif (not CDMs\ [ I \ ] .Tagged)  thenMatch ing(NULL ,  CDMs\[ I \ ] )  ;i f  (not CDMs\ [ I \ ] .Tagged)  thenMatch ing(CDMs\ [ I \ ] ,  NULL) ;end ;In the above pseudo codes, CDMs\[\] denotesthe array holding the candidate discourse markers,and the Boolean variable Tagged is used toindicate whether a CDM has been tagged.Furthermore, the procedure Matching0 is toexamine whether the first word or phraseappearing in a sentence is an inter-sentenceCDMs\[I\].6 Mining Discourse Marker UsingMachine LearningData mining techniques constitute a fielddedicated to the development of computationalmethods underlying learning processes and theyhave been applied in various disciplines in textprocessing, such as finding associations in acollection of texts (Feldman and Hirsh 1997) andmining online text (Knight 1999).
In this section,we focus on the problem of discourse markerdisambiguation using decision trees obtained bymachine learning techniques.
Our novel approachin mining Chinese discourse markers attempts toapply the C4.5 learning algorithm, as introducedby Quinlan (1993), in the context of non-tabular,unstructured ata.
A decision tree consists ofnodes and branches connecting the nodes.
Thenodes located at the bottom of the tree are calledleaves, and indicate classes.
The top node in thetree is called the root, and contains all the trainingexamples that are to be divided into classes.
Inorder to minimize the branches in the tree, the bestattribute is selected and used in the test at the rootnode of the tree.
A descendant of the root node isthen created for each possible value of thisattribute, and the training examples are sorted tothe appropriate descendant node.
The entireprocess is then repeated using the trainingexamples associated with each descendant ode toselect he best attribute for testing at that point inthe tree.
A statistical property, called informationgain, is used to measure how well a given attributedifferentiates the training examples according totheir target classificatory scheme and to select heIIII|III!IIIIIII.II-IIImost suitable candidate attribute at each step whileexpanding the tree.The attributes we use in this research includethe candidate discourse marker itself, two wordsimmediately to the left of the CDM, and twowords immediately to the right of the CDM.
Theattribute names are F2, F1, CDM, B1,  B2,respectively.
All these five attributes are discrete.The following are two examples:?
",", dan 'but', youyu 'since', Xianggang'Hong Kong', de 'of', T.?
zhe 'this', yi 'also', zhishi 'is only',Xianggang 'Hong Kong', de 'of', F.where "T" denotes the CDM youyu as a discoursemarker in the given context, and "F" denotes thatzhishi is not a discourse marker.In building up a decision-tree in ourapplication of C4.5 to the mining of discoursemarkers, entropy, first of all, is used to measurethe homogeneity of the examples.
For any possiblecandidate A chosen as an attribute in classifyingthe training data S, Gain(S, A) information gain,relative to a data set S is defined.
This informationgain measures the expected reduction in entropyand defines one branch for the possible subset Siof the training examples.
For each subset Si, a newtest is then chosen for any further split.
If Sisatisfies a stopping criterion, such as all theelement in S~ belong to one class, the decision treeis formed with all the leaf nodes associated withthe most frequent class in S. C4.5 uses argmax(Gain(S, A)) or arg max(Gain Ratio(S, A)) asdefined in the following to construct the minimaldecision tree.cEntropy(S) = -~_ -  p, log 2 p~ (Eqn.
I)i=1Gain(S,A) = Entropy(S)- ~ -~'Entropy(S~) isl(Eqn.
2)Gain Ratio - Gain(S,A) (Eqn.
3)Splitlnformation( S, A)?
?
j is, t.  s,i where Splitlnformation=-2./--,Jog 2 ~, Si is!S!
iS!subset of S for which A has value vtIn our text mining, according to the numberof times a CDM occurs in the 80 tagged editorials,we select 75 CDMs with more than 10 occurrences.To avoid decision trees being over-fitted or trivial,for F2, F1, B1 and B2, only values of attributeswith frequency more than 15 in the corpus areused in building the decision trees.
We denote allvalues of attributes with frequency less than 15 as'Other'.
If a CDM is the first, the second or thelast word of a sentence, values of F2, F1, or B2will be null, we denote a null-value as "*".
Thefollowing are two other examples:?
"*", "*", zheyang 'thus', ",", Other, T.?
"*", "*", zheyang 'thus', Other, de 'of', F.7 Evaluation7.1 Evaluation of Heuristic-basedAlgorithmIn order to evaluate the effectiveness of theheuristic-based algorithm, we randomly selected40 editorials from Ming Pao, a Chinese newspaperof Hong Kong, to form our test data.
Onlyeditorials are chosen because they are mainlyargumentative texts and their lengths are relativelyuniform.The steps of evaluation consist of: 1) taggingall of the test data using the heuristic-basedalgorithm, and 2) proofreading, correcting andrecording all the tagging errors by a humanencoder.
The resulting statistics include, for eacheditorial in the test data, the number of lexicalitems (#Lltms), the number of sentences (#Sens),the number of discourse markers (#Mrkrs), and thenumber of sentences containing at least onediscourse marker (#CSens).
Table 2 shows theminimum, maximum and average values of thesecharacteristics.
The ratio of the average number ofdiscourse markers to the average number of lexicalitems is 4.37%, and the ratio of the averagenumber of sentencesdiscourse marker tosentences i  62.66%.#LltmsMIN 466MAX 1082AVERAGE 676.25containing at least onethe average number of#Mrkrs #Sens #CSens14 11 652 45 2629.58 22.15 13.88Table 2 Characteristics of the Test DataOur evaluation is based on counting thenumber of discourse markers that are correctly17tagged.
For incorrectly tagged iscourse markers,we classify them according to the types of errorsthat we have introduced in T'sou et al (1999).
Wedefine two evaluation metrics as follows: GrossAccuracy (GA) is defined to be the percentage ofcorrectly tagged discourse markers to the totalnumber of discourse markers while Relation-Matching Accuracy (RMA) is defined to be thepercentage of correctly tagged discourse markersto the total number of discourse markers minusthose errors caused by non-markers andunrecorded markers.
The results for our testing.data have GA = 68.89% and RMA = 95.07%.Since the heuristic-based algorithm doesnot assume any knowledge of the statistics andbehavioral patterns of discourse markers, our GAdemonstrates the usefulness of the algorithm inalleviating the burden of human encoders indeveloping a sufficiently large-corpus for thepurpose of studying the usage of discoursemarkers.In our experiment, most errors come fromtagging non-discourse markers as discoursemarkers (T'sou et al 1999).
This is due to the factthat, similar to the question of cue phrasepolysemy (Hirschberg and Litman 1993), manyChinese discourse markers have both discoursesenses and alternate sentential senses in different;utterances.
For example:?
... Zhe ('this') buguo shi ('only is') yi ('one')ge ('classifier') wanxiao ('joke')...('This is only a joke'.)
(sentential sense)?
...Buguo ('however'), wo (T)  bu ('neg')zheyang ('thus') renwei ('consider')?
.
.
( 'But  I don't think so.')
(discourse sense)7.2 Evaluation of Decision TreeAlgorithm (with C4.5)In Section 6, we discuss how machine learningtechniques have been applied to the problem ofdiscourse marker disambiguation in Chinese.In our experiment, there are a total of 2627cases.
In our decision tree construction, we use 75percent of the total cases as a training set, and theremaining 25 percent of cases as a test set.
Manydecision trees can be generated by adjusting theparameters in the learning algorithm.
Manydecision trees generated in our experiment have anaccuracy around 80% for both the training set andthe test set.
Figure 2 shows one of the possibledecision trees in our experiment.
The last branchof the decision treeF1 = danshi 'but'I CDM in {ru 'if', reng 'still', geng 'even more', que'however' }:F (6/0)I CDM in {chule 'except', youyu 'since', ruo 'if"} : T(4/0)can be explained as:if (F1 = danshi 'but') thenif (CDM in {ru 'if', reng 'still', geng "even more', que'however' }) then dassify as Felseif (CDM in {chule 'except',youyu 'since', ru0 'if '  })then classify as TDecision Tree: (Size = 38, Items = 1971, Errors = 282)F1 in {di, ye, yi} : F (25/5)F i  in (,shi, ;} : T (712/131)F1 = Other:F I  = danshi :I CDM in {ru, reng, geng, que} : F (7/10)I CDM in {chule, youyu, ruo} : T (4/0)Evaluation on trainine data from Data.
Data (1971 cases~:Classified results:T F I <" Classified937 125 \] C lass :T157 752 C.lass : F Errors : 282 (14.3%)Evaluation on testin~ data from Data.
Test (656 cases):T F ~ f i e d293 62 \[Class : T68 233 IClass : F Errors : 130 (19.8%)1Figure 2 An Example of Decision TreesThe two numbers in the brackets denote thenumber of cases covered by the branch and thenumber of cases being misclassified respectively:The results of our experiment will be elaboratedon in future, when we shall also explore theapplication of machine learning techniques torecognizing rhetorical relations on the basis ofdiscourse markers, and extracting importantsentences from Chinese text.8.
ConclusionWe discuss in this paper the use of discoursemarkers in Chinese text summarization.
Discoursestructure trees with nodes representing RST(Rhetorical Structure Theory) relations are builtand summarization is achieved by trimming18unimportant sentences on the basis of the relativesaliency or rhetorical relations.
In order to studydiscourse markers for use in the automaticsummarization f Chinese, we have designed andimplemented the SIFAS system.
We investigatethe relationships between various linguisticfeatures and different aspects of discourse markerusage on naturally occurring text.
An encodingscheme that captures the essential features ofdiscourse marker usage is introduced.
A heuristic-based algorithm for automatic tagging of discoursemarkers is designed to alleviate the burden of ahuman encoder in developing a large corpus ofencoded texts and to discover potential problemsin automatic discourse marker tagging.
A study onapplying machine learning techniques todiscoursemarker disambiguation is also conducted.
C4.5 isused to generate decision tree classifiers.
Ourresults indicate that machine learning is apromising approach to improving the accuracy ofdiscourse marker tagging.9 Acknowledgement  ,Support for the research reported here isprovided through the Research Grants Councilof Hong Kong under Competitive EarmarkedResearch Grants (CERG) No.
9040067,9040233 and 9040326.10 References~Aretoulaki M., Scheler G. and Brauer W. (1998)"Connectionist Modeling of Human EventMemorization Processes with Application toAutomatic Text Summarization."
InProceedings of AAAI Spring Symposium onIntelligent Text Summarization, Stanford, pp.148-150.Brandow R., Mitze K. and Rau L. F. (1995)"Automatic Condensation of ElectronicPublications by Sentence Selection.
"Information Processing and Management,31(5): 675-685.Endres-Niggemeyer B., Maier E. and Sigel A.
(1995) "How to Implement a NaturalisticModel of Abstracting: Four Core WorkingSteps of an Expert Abstractor."
InformationProcessing and Management, 31(5): 631-674.19Feldman R. and Hirsh H. (1997).
"Findingassociations in collections of text."
In R.S.Michalski I. Bratko and Kubat M.
(Eds.
),"Machine Learning and Data Mining: Methodsand Applications, pp.
224-240.
Wiley.Grosz B.J.
and Sidner C. (1986) "Attention,Intention, and the Structure of Discourse,"Computational Linguistics 12(3): 175-204.Halliday M. A. K. and Hasan R. (1976) Cohesionin English, Longman.Hearst M. A.
(1997) "Texttiling: Segmenting Textinto Multi-paragraph Subtopic Passages.
"Computational Linguistics, 23(1):33-64.Hinds J.
(1982) "Inductive, deductive, quasi-.inductive: Expository writing in Japanese,Korean, Chinese, and Thai."
In U. Connor andA.M.
Johns (Eds.).
Coherence in Writing, pp.89-109.
TESOL publisher.Hirschberg J. and Litman D. (1993) "EmpiricalStudies on the Disambiguation f Cue Phrases.
"Computational Linguistics 19(3): 501-530.Hirst G. (1981) "Discourse Oriented AnaphoralResolution in Natural Language Understanding:A Review."
Computational Linguistics 7(2):85-98.Hovy E. (1993) "Automated Discourse Generationusing Discourse Structure Relations."
ArtificialIntelligence 63: 341-385.Hwang C. H. and Schubert L. K. (1992) "TenseTrees as the 'Fine Structure' of Discourse."
InProc.
30 th Annual Meeting, Assoc.
forComputational Linguistics, pp.
232-240.Johnson F. C., Paice C. D., Black W. J. and NealA.
P. (1993) "'The Application of LinguisticProcessing to Automatic Abstract Generation.
"Journal of Document and Text Management I:215-241.Kaplan R. B.
(1996) "Cultural though patterns inintercultural education."
Language Learning,l&2: 1-20.Kirkpatrick A.
(1993) "Information sequencing inmodem standard Chinese in a genre of extendedspoken discourse."
Text 13(3): 423-453.Kong K.C.C.
(1998) "Are simple business requestletters really simple?
A comparison of Chineseand English business request letters."
Text 18(1 ) :103-141.Knight K. (1999) "Mining online text.
"Communications of the A CM 42(11): 58-61.Kupiec J., Pedersen J., and Chen F. (1995) "ATrainable Document Summarizer."
InProceedings of the lff h Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, Seattle,pp.
68-73.Lin H. L., T'sou B. K., H. C. Ho, Lai T., Lun C.,C.
K. Choi and C.Y.
Kit.
(1991) "AutomaticChinese Text Generation Based on InferenceTrees.'"
In Proc.
of ROCL1NG ComputationalLinguistic Conference IV, Taipei, pp.
215-236.Litman D. J. and Allen J.
(1990) "DiscourseProcessing and Commonsense Plans."
In Cohenet al(ed.)
Intentions in Communications, pp.365-388.Litman D. J.
(1996) "Cue Phrase ClassificationUsing Machine Learning."
Journal of ArtificialIntelligence Research 5: 53-94.Mani I., Bloedorn E. and B.
Gates (1998) "UsingCohesion and Coherence Models for TextSummarization."
In Proceedings of AAAISpring Symposium on Intelligent TextSummarization, Stanford, pp.
69-76.Mann W. C. and Thompson S. A (1988)"Rhetorical Structure Theory: Towards aFunctional Theory of Text Organization."
Text8(3): 243-281.Marcu D. (1997) "From Discourse Structures toText Summaries."
In Proceedings of theA CL/EA CL '97 Workshop on Intelligent: Scalable Text Summarization, Spain, pp.
82-88.McKeown K. and Radev D. (1995) "Summaries ofMultiple News Articles."
In Proceedings of the18 'h Annual International A CM S1GIRConference on Research and Development inInformation Retrieval, Seattle, pp.
74-82.McKeown K. R. (1985) "Discourse Strategies forGenerating Natural-Language T xt."
ArtificialIntelligence 27(1): 1-41.Morris J. and Hirst G. (1991) "Lexical CohesionComputed by Thesaural Relations as anIndicator of the Structure of Text.
"Computational Linguistics 17(1): 21-48.Moser M. and Moore J. D. (1995) "InvestigatingCue Selection and Placement in TutorialDiscourse.'"
In Proceedings of ACL'95, pp.130-135.Ono K., Sumita K. and S. Miike.
(1994) "AbstractGeneration based on Rhetorical StructureExtraction."
In Proceedings of InternationalConference on Computational Linguistics,Japan, pp.
344-348.Paice C. D. (1990) "Constructing LiteratureAbstracts by Computer: Techniques andProspects."
Information Processing and"Management 26(1): 171-186.Quinlan J. Ross (1993)"C4.5 Programs forMachine Learning."
San Mateo, CA: MorganKaufmann.Reichman R. (1978) "Conversational Coherence.
"Cognitive Science 2(4): 283-328.Salton G., Singhal A., Mitra M. and Buckley C.(1997) "Automatic Text Structuring andSummarization."
Information Processing andManagement 33(2): 193-207.Schiffrin D. (1987) Discourse Markers.'
Cambridge: Cambridge University Press.Siegel E. V. and McKeown K. R. (1994)"Emergent Linguistic Rules from InducingDecision Trees: Disambiguating Discourse ClueWords."
In Proceedings of AAAI, pp.
820-826.T'sou B. K., Ho H. C., Lai B. Y., Lun C. and LinH.
L. (1992) "A Knowledge-based Machine-aided System for Chinese Text Abstraction."
InProceedings of International Conference onComputational Linguistics, France, pp, 1039-1042.T'sou B. K., Gao W. J., Lin H. L., Lai T. B. Y.and Ho H. C. (1999) "Tagging DiscourseMarkers: Towards a Corpus based Study ofDiscourse Marker Usage in Chinese Text" InProceedings of the 18th InternationalConference on Computer Processing ofOriental Languages, March 1999, Japan, pp.391-396.T'sou B. K., Lin H. L., Ho H. C., Lai T. and ChanT.
(1996) "Automated Chinese Full-textAbstraction Based on Rhetorical StructureAnalysis."
Computer Processing of OrientalLanguages 10(2): 225-238.Wang W. X., Zhang X. C., Lu M. Y. and Cheng H.Y.
(1994) "Xian Dai Han Yu Fu Ju Xian Jie (ANew Analysis of Complex Sentences in ModernStandard Chinese)", Hua Dong S.hi Fan Da XueChu Ban She, 1994.20!Iii!IIIIIiIIiiIIII
