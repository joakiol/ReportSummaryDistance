Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 1?4,Sydney, July 2006. c?2006 Association for Computational LinguisticsFAST ?
An Automatic Generation System for Grammar TestsChia-Yin ChenInst.
of Info.
Systems & ApplicationsNational Tsing Hua University101, Kuangfu Road,Hsinchu, 300, TaiwanG936727@oz.nthu.edu.twHsien-Chin LiouDep.
of Foreign Lang.
& Lit.National Tsing Hua University101, Kuangfu Road,Hsinchu, 300, Taiwanhcliu@mx.nthu.edu.twJason S. ChangDep.
of Computer ScienceNational Tsing Hua University101, Kuangfu Road,Hsinchu, 300, Taiwanjschang@cs.nthu.edu.twAbstractThis paper introduces a method for thesemi-automatic generation of grammartest items by applying Natural LanguageProcessing (NLP) techniques.
Based onmanually-designed patterns, sentencesgathered from the Web are transformedinto tests on grammaticality.
The methodinvolves representing test writingknowledge as test patterns, acquiringauthentic sentences on the Web, andapplying generation strategies totransform sentences into items.
Atruntime, sentences are converted into twotypes of TOEFL-style question: multiple-choice and error detection.
We alsodescribe a prototype system FAST (FreeAssessment of Structural Tests).Evaluation on a set of generatedquestions indicates that the proposedmethod performs satisfactory quality.Our methodology provides a promisingapproach and offers significant potentialfor computer assisted language learningand assessment.1 IntroductionLanguage testing, aimed to assess learners?language ability, is an essential part of languageteaching and learning.
Among all kinds of tests,grammar test is commonly used in everyeducational assessment and is included in well-established standardized tests like TOEFL (Testof English as Foreign Language).Larsen-Freeman (1997) defines grammar ismade of three dimensions: form, meaning, anduse (See Figure 1).
Hence, the goal of a grammartest is to test learners to use grammar accurately,meaningfully, and appropriately.
Consider thepossessive case of the personal noun in English.The possessive form comprises an apostropheand the letter ?s?.
For example, the possessiveform of the personal noun ?Mary?
is ?Mary?s?.The grammatical meaning of the possessive casecan be (1) showing the ownership: ?Mary?s bookis on the table.?
(= a book that belongs to Mary);(2) indicating the relationship: ?Mary?s sister isa student.?
(=the sister that Mary has).Therefore,a comprehensive grammar question needs toexamine learners?
grammatical knowledge fromall three aspects (morphosyntax, semantics andpragmatics).Figure 1: Three Dimensions of Grammar(Larsen-Freeman, 1997)The most common way of testing grammar isthe multiple-choice test (Kathleen and Kenji,1996).
Multiple-choice test format ongrammaticality consists of two kinds: one is thetraditional multiple-choice test and the other isthe error detection test.
Figure 2 shows a typicalexample of traditional multiple-choice item.
Asfor Figure 3, it shows a sample of error detectionquestion.Traditional multiple-choice is composed ofthree components, where we define the sentencewith a gap as the stem, the correct choice to thegap as the key and the other incorrect choices asthe distractors.
For instance, in Figure 2, theForm Meaning(appropriateness)(accuracy)ness)(meaningful-Use1In the Great Smoky Mountains, one can see _____ 150different kinds of tress.
(A) more than(B) as much as(C) up as(D) as many toAlthough maple trees are among the most colorful(A)varieties in the fall, they lose its leaves(B)                     (C)sooner than oak trees.
(D)partially blanked sentence acts as the stem andthe key ?more than?
is accompanied by threedistractors of ?as much as?, ?up as?, and ?asmany to?.
On the other hand, error detection itemconsists of a partially underlined sentence (stem)where one choice of the underlined partrepresents the error (key) and the otherunderlined parts act as distractors to distract testtakers.
In Figure 3, the stem is ?Although mapletrees are among the most colorful varieties in thefall, they lose its leaves sooner than oak trees.
?and ?its?
is the key with distractors ?among?, ?inthe fall?, and ?sooner than.
?Grammar tests are widely used to assesslearners?
grammatical competence, however, it iscostly to manually design these questions.
Inrecent years, some attempts (Coniam, 1997;Mitkov and Ha, 2003; Liu et al, 2005) have beenmade on the automatic generation of languagetesting.
Nevertheless, no attempt has been madeto generate English grammar tests.
Additionally,previous research merely focuses on generatingquestions of traditional multiple-choice task, noattempt has been made for the generation of errordetection test types.In this paper, we present a novel approach togenerate grammar tests of traditional multiple-choice and error detection types.
First, byanalyzing syntactic structure of Englishsentences, we constitute a number of patterns forthe development of structural tests.
For example,a verb-related pattern requiring an infinitive asthe complement (e.g., the verb ?tend?)
can beformed from the sentence ?The weather tends toimprove in May.?
For each pattern, distractorsare created for the completion of each grammarquestion.
As in the case of foregoing sentence,wrong alternatives are constructed by changingthe verb ?improve?
into different forms: ?toimproving?, ?improve?, and ?improving.?
Then,we collect authentic sentences from the Web asthe source of the tests.
Finally, by applyingdifferent generation strategies, grammar tests intwo test formats are produced.
A completegrammar question is generated as shown inFigure 4.
Intuitively, based on certain surfacepattern (See Figure 5), computer is able tocompose a grammar question presented in Figure4.
We have implemented a prototype systemFAST and the experiment results have shown thatabout 70 test patterns can be successfully writtento convert authentic Web-based texts intogrammar tests.
* X/INFINITIVE * CLAUSE.* _______* CLAUSE.
(A) X/INFINITIVE(B) X/to VBG(C) X/VBG(D) X/VB2 Related WorkSince the mid 1980s, item generation for testdevelopment has been an area of active research.In our work, we address an aspect of CAIG(computer-assisted item generation) centering onthe semi-automatic construction of grammar tests.Recently, NLP (Natural Language Processing)has been applied in CAIG to generate tests inmultiple-choice format.
Mitkov and Ha (2003)established a system which generates readingcomprehension tests in a semi-automatic way byusing an NLP-based approach to extract keyconcepts of sentences and obtain semanticallyalternative terms from WordNet.Coniam (1997) described a process tocompose vocabulary test items relying on corpusword frequency data.
Recently, Gao (2000)presented a system named AWETS that semi-automatically constructs vocabulary tests basedon word frequency and part-of-speechinformation.
Most recently, Hoshino andNakagawa (2005) established a real-time systemwhich automatically generates vocabularyquestions by utilizing machine learningtechniques.
Brown, Frishkoff, and Eskenazi(2005) also introduced a method on theautomatic generation of 6 types of vocabularyquestions by employing data from WordNet.I intend _______ you that we cannot approve yourapplication.
(A) to inform(B) to informing(C) informing(D) informFigure 4: An example of generated question.Figure 5: An example of surface pattern.
Figure 3: An example of error detection.Figure 2: An example of multiple-choice.2Liu, Wang, Gao, and Huang (2005) proposedways of the automatic composing of Englishcloze items by applying word sensedisambiguation method to choose target words ofcertain sense and collocation-based approach toselect distractors.Previous work emphasizes the automaticgeneration of reading comprehension,vocabulary, and cloze questions.
In contrast, wepresent a system that allows grammar test writersto represent common patterns of test items anddistractors.
With these patterns, the systemautomatically gathers authentic sentences andgenerates grammar test items.3 The FAST SystemThe question generation process of the FASTsystem includes manual design of test patterns(including construct pattern and distractorgeneration pattern), extracting sentences from theWeb, and semi-automatic generation of testitems by matching sentences against patterns.
Inthe rest of this section, we will thoroughlydescribe the generation procedure.3.1 Question Generation AlgorithmInput: P = common patterns for grammar testitems, URL = a Web site for gathering sentencesOutput: T, a set of grammar test items g1.
Crawl the site URL for webpages2.
Clean up HTML tags.
Get sentences Stherein that are self-contained.3.
Tag each word in S with part of speech (POS)and base phrase (or chunk).
(See Figure 6 forthe example of the tagging sentence ?Anuclear weapon is a weapon that derives itsand or fusion.?)4.
Match P against S to get a set of candidatesentences D.5.
Convert each sentence d in D into a grammartest item g.3.2 Writing Test PatternsGrammar tests usually include a set of patternscovering different grammatical categories.
Thesepatterns are easily to conceptualize and to writedown.
In the first step of the creation process, wedesign test patterns.A construct pattern can be observed throughanalyzing sentences of similar structural features.Sentences ?My friends enjoy traveling by plane.
?and ?I enjoy surfing on the Internet.?
areanalyzed as an illustration.
Two sentences shareidentical syntactic structure {* enjoy X/Gerund*}, indicating the grammatical rule for the verb?enjoy?
needing a gerund as the complement.Similar surface patterns can be found whenreplacing ?enjoy?
by verbs such as ?admit?
and?finish?
(e.g., {* admit X/Gerund *} and {*finish X/Gerund *} ).
These two generalize thesesurface patterns, we write a construct pattern {*VB VBG *} in terms of POS tags produced by aPOS tagger.
Thus, a construct patterncharacterizing that some verbs require a gerundin the complement is contrived.Distractor generation pattern is dependent oneach designed construct pattern and thereforeneeds to design separately.
Distractors areusually composed of words in the constructpattern with some modifications: changing partof speech, adding, deleting, replacing, orreordering of words.
By way of example, in thesentence ?Strauss finished writing two of hispublished compositions before his tenthbirthday.
?, ?writing?
is the pivot word accordingto the construct pattern {* VBD VBG *}.Distractors for this question are: ?write?,?written?, and ?wrote?.
Similar to the way for theconstruct pattern devise, we use POS tags torepresent distractor generation pattern: {VB},{VBN}, and {VBD}.
We define a notationscheme for the distractor designing.
The symbol$0 designates the changing of the pivot word inthe construct pattern while $9 and $1 are thewords proceeding and following the pivot word,respectively.
Hence, distractors for theabovementioned question are {$0 VB}, {$0VBN}, and {$0 VBD}3.3   Web Crawl for Candidate SentencesAs the second step, we extract authenticmaterials from the Web for the use of questionstems.
We collect a large number of sentencesfrom websites containing texts of learned genres(e.g., textbook, encyclopedia).Lemmatization:  a nuclear weapon be a weapon that derive itsenergy from the nuclear reaction of fissionand or fusion.POS:  a/at nuclear/jj weapon/nn be/bez a/at weapon/nn that/wpsderive/vbz its/pp$ energy/nn from/in the/at nuclear/jjreaction/nns of/in fission/nn  and/cc or/cc fusion/nn ./.Chunk:   a/B-NP nuclear/I-NP weapon/I-NP be/B-VP a/B-NPweapon/I-NP that/B-NP derive/B-VP its/B-NPenergy/I-NP from/B-PP the/B-NP nuclear/I-NPreaction/I-NP of/B-PP fission/B-NP and/O or/B-UCPfusion/B-NP ./OFigure 6: Lemmatization, POS tagging andchunking of a sentence.33.4 Test StrategyThe generation strategies of multiple-choice anderror detection questions are different.
Thegeneration strategy of traditional multiple-choicequestions involves three steps.
The first step is toempty words involved in the construct pattern.Then, according to the distractor generationpattern, three erroneous statements are produced.Finally, option identifiers (e.g., A, B, C, D) arerandomly assigned to each alternative.The test strategy for error detection questionsis involved with: (1) locating the target point, (2)replacing the construct by selecting wrongstatements produced based on distractorgeneration pattern, (3) grouping words of samechunk type to phrase chunk (e.g., ?the/B-NPnickname/I-NP?
becomes ?the nickname/NP?
)and randomly choosing three phrase chunks toact as distractors, and (4) assigning options basedon position order information.4 Experiment and Evaluation ResultsIn the experiment, we first constructed testpatterns by adapting a number of grammaticalrules organized and classified in ?How toPrepare for the TOEFL?, a book written bySharpe (2004).
We designed 69 test patternscovering nine grammatical categories.
Then, thesystem extracted articles from two websites,Wikipedia (an online encyclopedia) and VOA(Voice of American).
Concerning about thereadability issue (Dale-Chall, 1995) and the self-contained characteristic of grammar questionstems, we extracted the first sentence of eacharticle and selected sentences based on thereadability distribution of simulated TOEFL tests.Finally, the system matched the tagged sentencesagainst the test patterns.
With the assistance ofthe computer, 3,872 sentences are transformedinto 25,906 traditional multiple-choice questionswhile 2,780 sentences are converted into 24,221error detection questions.A large amount of verb-related grammarquestions were blindly evaluated by sevenprofessor/students from the TESOL program.From a total of 1,359 multiple-choice questions,77% were regarded as ?worthy?
(i.e., can bedirect use or only needed minor revision) while80% among 1,908 error detection tasks weredeemed to be ?worthy?.
The evaluation resultsindicate a satisfactory performance of theproposed method.5 ConclusionWe present a method for the semi-automaticgeneration of grammar tests in two test formatsby using authentic materials from the Web.
Atruntime, a given sentence sharing classifiedconstruct patterns is generated into tests ongrammaticality.
Experimental results assess thefacility and appropriateness of the introducedmethod and indicate that this novel approachdoes pave a new way of CAIG.ReferencesConiam, D. (1997).
A Preliminary Inquiry intoUsing Corpus Word Frequency Data in theAutomatic Generation of English Cloze Tests.CALICO Journal, No 2-4, pp.
15- 33.Gao, Z.M.
(2000).
AWETS: An Automatic Web-Based English Testing System.
In Proceedingsof the 8th Conference on Computers inEducation/International Conference onComputer-Assisted Instruction ICCE/ICCAI,2000, Vol.
1, pp.
28-634.Hoshino, A.
& Nakagawa, H. (2005).
A Real-Time Multiple-Choice Question Generation forLanguage Testing-A Preliminary Study-.
InProceedings of the Second Workshop onBuilding Educational Applications Using NLP,pp.
1-8, Ann Arbor, Michigan, 2005.Larsen-Freeman, D. (1997).
Grammar and itsteaching: Challenging the myths (ERIC Digest).Washington, DC: ERIC Clearinghouse onlanguages and Linguistics, Center for AppliedLinguistics.
Retrieved July 13, 2005, fromhttp://www.vtaide.com/png/ERIC/Grammar.htmLiu, C.L., Wang, C.H., Gao, Z.M., & Huang,S.M.
(2005).
Applications of LexicalInformation for Algorithmically ComposingMultiple-Choice Cloze Items, In Proceedings ofthe Second Workshop on Building EducationalApplications Using NLP, pp.
1-8, Ann Arbor,Michigan, 2005.Mitkov, R. & Ha, L.A. (2003).
Computer-AidedGeneration of Multiple-Choice Tests.
InProceedings of the HLT-NAACL 2003Workshop on Building EducationalApplications Using Natural LanguageProcessing, Edmonton, Canada, May, pp.
17 ?22.Sharpe, P.J.
(2004).
How to Prepare for theTOEFL.
Barrons?
Educational Series, Inc.Chall, J.S.
& Dale, E. (1995).
ReadabilityRevisited: The New Dale-Chall ReadabilityFormula.
Cambridge, MA:Brookline Books.4
