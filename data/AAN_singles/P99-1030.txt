Analysis System of Speech Acts and Discourse Structures UsingMaximum Entropy Model*Won Seug Choi, Jeong-Mi Cho and Jungyun SeoDept.
of Computer Science, Sogang UniversitySinsu-dong 1, Mapo-guSeoul, Korea, 121-742{dolhana, jmcho} @nlprep.sogang.ac.kr, seojy@ccs.sogang.ac.krAbstractWe propose a statistical dialogue analysismodel to determine discourse structures aswell as speech acts using maximum entropymodel.
The model can automatically acquireprobabilistic discourse knowledge from adiscourse tagged corpus to resolveambiguities.
We propose the idea of taggingdiscourse segment boundaries to representthe structural information of discourse.Using this representation we can effectivelycombine speech act analysis and discoursestructure analysis in one framework.IntroductionTo understand a natural language dialogue, acomputer system must be sensitive to thespeaker's intentions indicated through utterances.Since identifying the speech acts of utterances ivery important to identify speaker's intentions, itis an essential part of a dialogue analysis ystem.It is difficult, however, to infer the speech actfrom a surface utterance since an utterance mayrepresent more than one speech act according tothe context.
Most works done in the past on thedialogue analysis has analyzed speech acts basedon knowledge such as recipes for plan inferenceand domain specific knowledge (Litman (1987),Caberry (1989), Hinkelman (1990), Lambert(1991), Lambert (1993), Lee (1998)).
Sincethese knowledge-based models depend on costlyhand-crafted knowledge, these models aredifficult to be scaled up and expanded to otherdomains.Recently, machine learning models using adiscourse tagged corpus are utilized to analyzespeech acts in order to overcome such problems(Nagata (1994a), Nagata (1994b), Reithinger(1997), Lee (1997), Samuel (1998)).
Machinelearning offers promise as a means ofassociating features of utterances with particularspeech acts, since computers can automaticallyanalyze large quantities of data and considermany different feature interactions.
Thesemodels are based on the features such as cuephrases, change of speaker, short utterances,utterance length, speech acts tag n-grams, andword n-grams, etc.
Especially, in many cases,the speech act of an utterance influenced by thecontext of the utterance, i.e., previous utterances.So it is very important o reflect the informationabout he context o the model.Discourse structures of dialogues are usuallyrepresented as hierarchical structures, whichreflect embedding sub-dialogues (Grosz (1986))and provide very useful context for speech actanalysis.
For example, utterance 7 in Figure 1has several surface speech acts such asacknowledge, inform, and response.
Such anambiguity can be solved by analyzing thecontext.
If we consider the n utterances linearlyadjacent o utterance 7, i.e., utterances 6, 5, etc.,as context, we will get acknowledge or informwith high probabilities as the speech act ofutterance 7.
However, as shown in Figure 1,utterance 7 is a response utterance to utterance 2that is hierarchically recent to utterance 7according to the discourse structure of thedialogue.
If we know the discourse structure ofthe dialogue, we can determine the speech act ofutterance 7 as response.
* This work was supported by KOSEF under thecontract 97-0102-0301-3.230Some researchers have used the structuralinformation of discourse to the speech actanalysis (Lee (1997), Lee (1998)).
It is not,however, enough to cover various dialoguessince they used a restricted rule-based modelsuch as RDTN (Recursive Dialogue TransitionNetworks) for discourse structure analysis.
Mostof the previous related works, to our knowledge,tried to determine the speech act of an utterance,but did not mention about statistical models todetermine the discourse structure of a dialogue.I )User : I would like Io reserve a room.2) Agent : What kind of room do you want?3) User : What kind of room do you have'?4) Agent :We have single mid double rooms.5) User : How much are those rooms?6) Agent : Single costs 30,000 won and double ~SlS 40,000 WOll.7) User : A single room.
please.requestask-refask-refresponseask-telresponseacknowledgeinformr~mmseF igure  1 : An  example  o f  a d ia logue  w i th  speech  actsIn this paper, we propose a dialogue analysismodel to determine both the speech acts ofutterances and the discourse structure of adialogue using maximum entropy model.
In theproposed model, the speech act analysis and thediscourse structure analysis are combined in oneframework so that they can easily providefeedback to each other.
For the discoursestructure analysis, we suggest a statistical modelwith discourse segment boundaries (DSBs)similar to the idea of gaps suggested for astatistical parsing (Collins (1996)).
For training,we use a corpus tagged with various discourseknowledge.
To overcome the problem of datasparseness, which is common for corpus-basedworks, we use split partial context as well aswhole context.After explaining the tagged dialogue corpus weused in section 1, we discuss the statisticalmodels in detail in section 2.
In section 3, weexplain experimental results.
Finally, weconclude in section 4.1 Discourse taggingIn this paper, we use Korean dialogue corpustranscribed from recordings in real fields such ashotel reservation, airline reservation and tourreservation.
This corpus consists of 528dialogues, 10,285 utterances (19.48 utterancesper dialogue).
Each utterance in dialogues ismanually annotated with discourse knowledgesuch as speaker (SP), syntactic pattern (ST),speech acts (SA) and discourse structure (DS)information.
Figure 2 shows a part of theannotated dialogue corpus ~.
SP has a valueeither "User" or "Agent" depending on thespeaker./SPAJser/ENh'm a student and registered/br alanguage course at University of Georgia inU.S.ISTl\[decl,be,present,no,none,none\]/SA/introducing -oneself/DS/\[2I/SP/User~9_./EN/I have sa)me questions about lodgings.IST/Idecl,paa.presenl,no,none,nonel/SA/ask-ref~DS/121--> Continue/SP/Agent/EN/There is a dormitory in Universily ofGeorgia lot language course students.ISTIIdecl.pvg,present,no,none.none\]/SA/response/DS/\[21/SPAJser/ENfrhen, is meal included in tuilion lee?/ST/?yn quest.pvg ,present.no.none ,then I/SA/ask-if/DS/12.
I IF igure  2: A part  o f  the annotated  d ia logue  corpusThe syntactic pattern consists of the selectedsyntactic features of an utterance, whichapproximate the utterance.
In a real dialogue, aspeaker can express identical contents withdifferent surface utterances according to apersonal inguistic sense.
The syntactic patterngeneralizes these surface utterances usingsyntactic features.
The syntactic pattern used in(Lee (1997)) consists of four syntactic featuressuch as Sentence Type, Main-Verb, Aux-Verband Clue-Word because these features providestrong cues to infer speech acts.
We add twomore syntactic features, Tense and NegativeSentence, to the syntactic pattern and elaboratethe values of the syntactic features.
Table 1shows the syntactic features of a syntacticpattern with possible values.
The syntacticfeatures are automatically extracted from thecorpus using a conventional parser (Kim(1994)).Manual tagging of speech acts and discoursestructure information was done by graduatestudents majoring in dialogue analysis and post-processed for consistency.
The classification ofspeech acts is very subjective without an agreedcriterion.
In this paper, we classified the 17types of speech acts that appear in the dialogueKS represents the Korean sentence and ENrepresents he translated English sentence.231corpus.
Table 2 shows the distribution of speechacts in the tagged ialogue corpus.Discourse structures are determined by focusingon the subject of current dialogue and arehierarchically constructed according to thesubject.
Discourse structure information taggedin the corpus is an index that represents thehierarchical structure of discourse reflecting thedepth of the indentation of discourse segments.The proposed system transforms this indexinformation to discourse segment boundary(DSB) information to acquire various statisticalinformation.
In section 2.2.1, we will describethe DSBs in detail.Syntactic feature Valuesdecl, imperative,wh question, yn_questionNotesSentence T)~e The mood of all utterancepvg, pvd, paa, pad, be, The type of the main verb.
ForMain-Verb know, ask, etc.
special verbs, lexical items are(total 88 kinds) used.Tense past, present, future.
The tense of an utteranceNegative Sentence Yes or No Yes if an utterance is negative.serve, seem, want, will, The modality of an utterance.Aux-Verb etc.
(total 31 kinds)Yes, No, OK., etc.
The special word used in theutterance having particular Clue-Word (total 26 kinds speech acts.Table I : Syntactic features used in the syntactic patternSpeech Act Type Ratio(%)Accept 2.49Acknowledge 5.75Ask-confirm 3.16Ask-if 5.36Ask-tel 13.39Closing 3.39Correct 0.03Expressive 5,64biform 11.90Speech Act Type Ratio(%)h~troducing-oneself 6.75Offer 0.40Opening 6.58Promise 2,42Reject 1.07Request 4.96Response 24.73Suggest 1.98Total 100.00Table 2: The distribution of speech acts in corpus2 Statistical modelsWe construct wo statistical models: one forspeech act analysis and the other for discoursestructure analysis.
We integrate the two modelsusing maximum entropy model.
In the followingsubsections, we describe these models in detail.2.1 Speech act analysis modelLet UI,, denote a dialogue which consists of asequence of n utterances, U1,U2 ..... U,,  and letS i denote the speech act of U.
With thesenotations, P(Si lU1, i )  means the probabilitythat S~ becomes the speech act of utterance U~given a sequence of utterances U1,U2,.. .
,Ui.We can approximate the probabilityP(Si I Ul.i) by the product of the sententialprobability P(Ui IS  i) and the contextualprobability P( Si I UI, i - i, $1, ~ - 1).
Also we canapproximate P(SilUl, i-l, Si,i-i) byP(Si l SI, g - l )  (Charniak (1993)).P(S~IUI,~)= P(S i lS~,~- I )P(U~IS i )  (1)It has been widely believed that there is a strongrelation between the speaker's peech act andthe surface utterances expressing that speech act(Hinkelman (1989), Andernach (1996)).
That is,the speaker utters a sentence, which most wellexpresses his/her intention (speech act) so thatthe hearer can easily infer what the speaker'sspeech act is.
The sentential probabilityP(U i lSO represents the relationship betweenthe speech acts and the features of surfacesentences.
Therefore, we approximate thesentential probability using the syntactic patternPi"P(Ui I Si) = P (P i IS i )  (2)The contextual probability P(Si  I $1, ~ - 1) is theprobability that utterance with speech act S i isuttered given that utterances with speech act$1, $2 ..... S/- 1 were previously uttered.
Since itis impossible to consider all precedingutterances $1, $2 ..... Si - ~ as contextualinformation, we use the n-gram model.Generally, dialogues have a hierarchicaldiscourse structure.
So we approximate thecontext as speech acts of n utterances that arehierarchically recent to the utterance.
Anutterance A is hierarchically recent to anutterance B if A is adjacent o B in the treestructure of the discourse (Walker (1996)).Equation (3) represents the approximatedcontextual probability in the case of usingtrigram where Uj and U~ are hierarchicallyrecent to the utterance U, wherel< j<k<i -1 .232P(Si I S\],, - ,) = P(Si I Sj, Sk) (3)As a result, the statistical model for speech actanalysis is represented in equation (4).P(S, I U,, 0 = P(Si I S,,, - ,)P(Ui I S,)= P(Si IS j, Sk)P(Pi \[St)(4)2.2 Discourse structure analysis model2.2.1 Discourse segment boundary taggingWe define a set of discourse segment boundaries(DSBs) as the markers for discourse structuretagging.
A DSB represents the relationshipbetween two consecutive utterances in adialogue.
Table 3 shows DSBs and theirmeanings, and Figure 3 shows an example ofDSB tagged ialogue.DSB MeaningDE Start a new dialogueDC Continue a dialogueSS Start a sub-dialoguenE End n level sub-dialoguesnB nE and then SSTable 3: DSBs and their meaningsDS DSB1) User : I would like to reserve a room.
I NULL2) Agent : What kind of room do you want?
1.1 SS3) User : What kind of room do you have?
1.1.1 SS4) Agent : We have single and double rooms.
1.1.1 DC5) User : How much are those rooms?
1.!.2 I B6) Agent : Single costs 30,000 won and double costs 40,000 won.
1.1.2 DC7) User : A single room, please.
I.
1 1EF igure  3: An  example  o f  DSB tagg ingSince the DSB of an utterance represents arelationship between the utterance and theprevious utterance, the DSB of utterance 1 in theexample dialogue becomes NULL.
Bycomparing utterance 2 with utterance 1 in Figure3, we know that a new sub-dialogue starts atutterance 2.
Therefore the DSB of utterance 2becomes SS.
Similarly, the DSB of utterance 3is SS.
Since utterance 4 is a response forutterance 3, utterance 3 and 4 belong to the samediscourse segment.
So the DSB of utterance 4becomes DC.
Since a sub-dialogue of one level(i.e., the DS 1.1.2) consisting of utterances 3 and4 ends, and new sub-dialogue starts at utterance5.
Therefore, the DSB of utterance 5 becomeslB.
Finally, utterance 7 is a response forutterance 2, i.e., the sub-dialogue consisting ofutterances 5 and 6 ends and the segment 1.1 isresumed.
Therefore the DSB of utterance 7becomes 1E.2.2.2 Statistical model for  discourse structureanalysisWe construct a statistical model for discoursestructure analysis using DSBs.
In the trainingphase, the model transforms discourse structure(DS) information in the corpus into DSBs bycomparing the DS information of an utterancewith that of the previous utterance.
Aftertransformation, we estimate probabilities forDSBs.
In the analyzing process, the goal of thesystem is simply determining the DSB of acurrent utterance using the probabilities.
Nowwe describe the model in detail.Let G, denote the DSB of U,.
With this notation,P(Gi lU\ ] ,O means the probability that G/becomes the DSB of utterance U~ given asequence of utterances U~, U 2 ..... Ui.
As shownin the equation (5), we can approximateP(Gi lU~,O by the product of the sententialprobability P(Ui I Gi) and the contextualprobability P( Gi I U \], i - \].
GI, i - \]) :P(Gi lU1, i)= P(Gi I U\], i - \], Gi, i - OP(Ui I Gi)(5)In order to analyze discourse structure, weconsider the speech act of each correspondingutterance.
Thus we can approximate eachutterance by the corresponding speech act in thesentential probability P(Ui I Gi):P(Ui I G0 --- P(S i lGO (6)233Let F, be a pair of the speech act and DSB of U,to simplify notations:Fi ::- (Si, Gi) (7 )We can approximate the contextual probabilityP(Gi lU l .
i - i ,  Gl.
i - l )  as equation (8) in thecase of using trigram.P(Gi IUl, i - l ,Gl,  i-1)= P(Gi I FI, i - 1) = P(Gi I Fi - 2, Fi - l)(8)As a result, the statistical model for thediscourse structure analysis is represented asequation (9).P(Gi I UI.
i)= P(Gi IUl.
i - i ,  Gl .
i -OP(Ui IGi)= P(G, I F~ - 2, F, - OP(& I GO(9)2.3 Integrated ialogue analysis modelGiven a dialogue UI, .
,  P(Si, Gi IUl, i) meansthe probability that S~ and G i will be,respectively, the speech act and the DSB of anutterance U/ given a sequence of utterancesUt, U2 ..... U~.
By using a chain rule, we canrewrite the probability as in equation (10).P(Si ,  Gi I UI, i)= P (S i IU I ,  i )P (G i IS i ,  UI, i)(10)In the right hand side (RHS) of equation (10),the first term is equal to the speech act analysismodel shown in section 2.1.
The second termcan be approximated as the discourse structureanalysis model shown in section 2.2 because thediscourse structure analysis model is formulatedby considering utterances and speech actstogether.
Finally the integrated dialogue analysismodel can be formulated as the product of thespeech act analysis model and the discoursestructure analysis model:e(Si, Gi I Ul.i)= P(S, I ULi)P(Gi I Ul.i)= P(S, I Sj, &)P(P,  I SOx P(G~ I Fi - 2, F~ - OP(Si I GO(102.4 Maximum entropy modelAll terms in RHS of equation (11) arerepresented by conditional probabilities.
Weestimate the probability of each term using thefollowing representative equation:P(a lb )= P(a,b)y~ P(a', b)a(12)We can evaluate P(a,b) using maximumentropy model shown in equation (13) (Reynar1997).P(a,b) = lrI" I Ot\[ '(''b)i=1where 0 < c~ i < oo, i = { 1,2 ..... k }(13)In equation (13), a is either a speech act or aDSB depending on the term, b is the context (orhistory) of a, 7r is a normalization constant, andis the model parameter corresponding to eachfeature functionf.In this paper, we use two feature functions:unified feature function and separated featurefunction.
The former uses the whole context b asshown in equation (12), and the latter usespartial context split-up from the whole contextto cope with data sparseness problems.
Equation(14) and (15) show examples of these featurefunctions for estimating the sententialprobability of the speech act analysis model.iff a = response and (14)b = User : \[decl, pvd, future, no, will, then\]otherwise10 iff a = response andf(a,b) = SentenceType(b) = User : declotherwise(15)Equation (14) represents a unified featurefunction constructed with a syntactic pattern234having all syntactic features, and equation (15)represents a separated feature functionconstructed with only one feature, namedSentence Type, among all syntactic features inthe pattern.
The interpretation of the unifiedfeature function shown in equation (14) is that ifthe current utterance is uttered by "User", thesyntactic pattern of the utterance is\[decl,pvd,future,no,will,then\] and the speech actof the current utterance is response then f(a,b)= 1else f(a,b)=O.
We can construct five moreseparated feature functions using the othersyntactic features.
The feature functions for thecontextual probability can be constructed insimilar ways as the sentential probability.
Thoseare unified feature functions with featuretrigrams and separated feature functions withdistance-1 bigrams and distance-2 bigrams.Equation (16) shows an example of an unifiedfeature function, and equation (17) and (18)which are delivered by separating the conditionof b in equation (16) show examples ofseparated feature functions for the contextualprobability of the speech act analysis model.10 iff a = response andf(a, b) = b = User : request, Agent :ask - refotherwisewhere b is the information of Ujand Ukdefined in equation (3)(16)10 iff a = response andf(a,b) = b_ t = Agent : ask - refotherwisewhere b_~ is the information of Uk defined in equation (3)(17)f(a'b)={lo iffa=resp?nseandb-2otherwise=USer:requestwhere b_ 2 is the information of Ujdefined in equation (3)(18)Similarly, we can construct feature functions forthe discourse structure analysis model.
For thesentential probability of the discourse structureanalysis model, the unified feature function isidentical to the separated feature function sincethe whole context includes only a speech act.Using the separated feature functions, we cansolve the data sparseness problem when thereare not enough training examples to which theunified feature function is applicable.3 Experiments and resultsIn order to experiment the proposed model, weused the tagged corpus shown in section 1.
Thecorpus is divided into the training corpus with428 dialogues, 8,349 utterances (19.51utterances per dialogue), and the testing corpuswith 100 dialogues, 1,936 utterances (19.36utterances per dialogue).
Using the MaximumEntropy Modeling Toolkit (Ristad 1996), weestimated the model parameter ~ correspondingto each feature functionf in equation (13).We made experiments with two models for eachanalysis model.
Modem uses only the unifiedfeature function, and Model-II uses the unifiedfeature function and the separated featurefunction together.
Among the ways to combinethe unified feature function with the separatedfeature function, we choose the combination iwhich the separated feature function is used onlywhen there is no training example applicable forthe unified feature function.First, we tested the speech act analysis modeland the discourse analysis model.
Table 4 and 5show the results for each analysis model.
Theresults shown in table 4 are obtained by usingthe correct structural information of discourse,i.e., DSB, as marked in the tagged corpus.Similarly those in table 5 are obtained by usingthe correct speech act information from thetagged corpus.Accuracy (Closed test) Accuracy (Open test)Candidates Top-1 Top-3 Top-1 Top-3Lee (1997) 78.59% 97.88%Samuel (1998) 73.17%Reithinger (1997) 74.70%Model I 90.65% 99.66% 81.61% 93.18%Model II 90.65% 99.66% 83,37% 95.35%Table 4.
Results of speech act analysisAccuracy(Open test)Candidates Top-I Top-3Model I 81.51% 98.55%Model I\] 83.21% 99.02%Table 5, Results of discourse structure analysisIn the closed test in table 4, the results of Model-I and Model-II are the same since theprobabilities of the unified feature functionsalways exist in this case.
As shown in table 4,the proposed models show better results thanprevious work, Lee (1997).
As shown in table 4and 5, ModeMI shows better esults than Model-235I in all cases.
We believe that the separatedfeature functions are effective for the datasparseness problem.
In the open test in table 4, itis difficult to compare the proposed modeldirectly with the previous works like Samuel(1998) and Reithinger (1997) because test dataused in those works consists of Englishdialogues while we use Korean dialogues.Furthermore the speech acts used in theexperiments are different.
We will test ourmodel using the same data with the same speechacts as used in those works in the future work.We tested the integrated ialogue analysis modelin which speech act and discourse structureanalysis models are integrated.
The integratedmodel uses ModeMI for each analysis modelbecause it showed better performance.
In thismodel, after the system determing the speech actand DSB of an utterance, it uses the results toprocess the next utterance, recursively.
Theexperimental results are shown in table 6.As shown in table 6, the results of the integratedmodel are worse than the results of each analysismodel.
For top-1 candidate, the performance ofthe speech act analysis fell off about 2.89% andthat of the discourse structure analysis about7.07%.
Nevertheless, the integrated model stillshows better performance than previous work inthe speech act analysis.Accuracy(Open test)Candidates Top- 1 Top-3Result of speech act 80.48% 94.58% analysisResult of discourse 76.14% 95.45% structure analysisTable 6.
Results of the integrated anal, 'sis modelConclusionIn this paper, we propose a statistical dialogueanalysis model which can perform both speechact analysis and discourse structure analysisusing maximum entropy model.
The model canautomatically acquire discourse knowledge froma discourse tagged corpus to resolve ambiguities.We defined the DSBs to represent the structuralrelationship of discourse between twoconsecutive utterances in a dialogue and usedthem for statistically analyzing both the speechact of an utterance and the discourse structure ofa dialogue.
By using the separated featurefunctions together with the unified featurefunctions, we could alleviate the data sparsenessproblems to improve the system performance.The model can, we believe, analyze dialoguesmore effectively than other previous worksbecause it manages speech act analysis anddiscourse structure analysis at the same timeusing the same framework.AcknowledgementsAuthors are grateful to the anonymous reviewerfor their valuable comments on this paper.Without their comments, we may miss importantmistakes made in the original draft.ReferencesAndernach, T. 1996.
A Machine Learning Approachto the Classification of Dialogue Utterances.Proceedings of NeMLaP-2.Berger, Adam L., Stephen A. Della Pietra, andVincent J. Della Pietra.
1996.
A Maximum EntropyApproach to Natural Language Processing.Computational Linguistics, 22( 1):39-71.Caberry, Sandra.
1989.
A Pragmatics-BasedApproach to Ellipsis Resolution.
ComputationalLinguistics, 15(2):75-96.Carniak, Eugene.
1993.
Statistical LanguageLearning.
A Bradford Book, The MIT Press,Cambridge, Massachusetts, London, England.Collins, M. J.
1996.
A New Statistical Parser Basedon Bigram Lexical Dependencies.
Proceedings ofthe 34th Annual Meeting of the Association forComputational Linguistics, pages 184-191.Grosz, Barbara J. and Candace L. Sidner.
1986.Attention, Intentions, and the Structure ofDiscourse.
Computational Linguistics, 12(3): 175-204.Hinkelman, E. A.
1990.
Linguistic and PragmaticConstraints on Utterance Interpretation.
Ph.D.Dissertation, University of Rochester, Rochester,New York.Hinkelman, E. A. and J. F. Allen.
1989.
TwoConstraints on Speech Act Ambiguity.Proceedings of the 27th Annual Meeting of theAssociation of Computational Linguistics, pages212-219.Kim, Chang-Hyun, Jae-Hoon Kim, Jungyun Seo, andGil Chang Kim.
1994.
A Right-to-Left Chart236Parsing for Dependency Grammar using HeadablePath.
Proceeding of the 1994 InternationalConference on Computer Processing of OrientalLanguages (ICCPOL), pages 175-180.Lambert, Lynn.
1993.
Recognizing ComplexDiscourse Acts: A Tripartite Plan-Based Model ofDialogue.
Ph.D. Dissertation, The University ofDelaware, Newark, Delaware.Lambert, Lynn and Sandra Caberry.
1991.
ATripartite Plan-based Model of Dialogue.Proceedings of ACL, pages 47-54.Lee, Jae-won, Jungyun Seo, Gil Chang Kim.
1997.
ADialogue Analysis Model With Statistical SpeechAct Processing For Dialogue Machine Translation.Proceedings of Spoken Language Translation(Workshop in conjunction with (E)ACL'97), pages10-15.Lee, Hyunjung, Jae-Won Lee and Jungyun Seo.
1998.Speech Act Analysis Model of Korean Utterancesfor Automatic Dialog Translation.
Journal ofKorea Information Science Society (B): Softwareand Applications, 25(10): 1443-1552 (In Korean).Litman, Diane J. and James F. Allen.
1987.
A PlanRecognition Model for Subdialogues inConversations.
Cognitive Science, pages 163-200.Nagata, M. and T. Morimoto.
1994a.
First stepstoward statistical modeling of dialogue to predictthe speech act type of the next utterance.
SpeechCommunication, 15: 193-203.Nagata, M. and T. Morimoto.
1994b.
Aninformation-theoretic model of discourse for nextutterance type prediction.
Transactions ofInformation Processing Society of Japan,35(6):1050-1061.Reithinger, N. and M. Klesen.
1997.
Dialogue actclassification using language models.
Proceedingsof EuroSpeech-97, pages 2235-2238.Reynar, J. C. and A. Ratnaparkhi.
1997.
A MaximumEntropy Approach to Identifying SentenceBoundaries.
In Proceeding of the Fifth Conferenceon Applied Natural Language Processing, pages16-19.Ristad, E. 1996.
Maximum Entropy ModelingToolkit.
Technical Report, Department ofComputer Science, Princeton University.Samuel, Ken, Sandra Caberry, and K. Vijay-Shanker.1998.
Computing Dialogue Acts from Featureswith Transformation-Based Learning.
ApplyingMachine Learning to Discourse Processing:Papers from the 1998 AAAI Spring Symposium.Stanford, California.
Pages 90-97.Walker, Marilyn A.
1996.
Limited Attention andDiscourse Structure.
Computational Linguistics,22(2):255-264.237
