Proceedings of the Workshop on Linguistic Distances, pages 1?6,Sydney, July 2006. c?2006 Association for Computational LinguisticsLinguistic DistancesJohn NerbonneAlfa-informaticaUniversity of Groningenj.nerbonne@rug.nlErhard HinrichsSeminar fu?r SprachwissenschaftUniversita?t Tu?bingeneh@sfs.uni-tuebingen.deAbstractIn many theoretical and applied areas ofcomputational linguistics researchers op-erate with a notion of linguistic distanceor, conversely, linguistic similarity, whichis the focus of the present workshop.While many CL areas make frequent useof such notions, it has received little fo-cused attention, an honorable exceptionbeing Lebart & Rajman (2000).
Thisworkshop brings a number of these strandstogether, highlighting a number of com-mon issues.1 IntroductionIn many theoretical and applied areas of compu-tational linguistics researchers operate with a no-tion of linguistic distance or, conversely, linguisticsimilarity, which is the focus of the present work-shop.
While many CL areas make frequent use ofsuch notions, it has received little focused atten-tion, an honorable exception being Lebart & Raj-man (2000).In information retrieval (IR), also the focus ofLebart & Rajman?s work, similarity is at heartof most techniques seeking an optimal match be-tween query and document.
Techniques in vectorspace models operationalize this via (weighted)cosine measures, but older tf/idf models were alsoarguably aiming at a notion of similarity.Word sense disambiguation models often workwith a notion of similarity among the contextswithin which word (senses) appear, and MT iden-tifies candidate lexical translation equivalents viaa comparable measure of similarity.
Many learn-ing algorithms currently popular in CL, includingnot only supervised techniques such as memory-based learning (k-nn) and support-vector ma-chines, but also unsupervised techniques such asKohonen maps and clustering, rely essentially onmeasures of similarity for their processing.Notions of similarity are often invoked in lin-guistic areas such as dialectology, historical lin-guistics, stylometry, second-language learning (asa measure of learners?
proficiency), psycholin-guistics (accounting for lexical ?neighborhood?effects, where neighborhoods are defined by simi-larity) and even in theoretical linguistics (novel ac-counts of the phonological constraints on semiticroots).This volume reports on a workshop aimed atbringing together researchers employing variousmeasures of linguistic distance or similarity, in-cluding novel proposals, especially to demonstratethe importance of the abstract properties of suchmeasures (consistency, validity, stability over cor-pus size, computability, fidelity to the mathemati-cal distance axioms), but also to exchange infor-mation on how to analyze distance informationfurther.We assume that there is always a ?hidden vari-able?
in the similarity relation, so that we shouldalways speak of similarity with respect to someproperty, and we suspect that there is such aplethora of measures in part because researchersare often inexplicit on this point.
It is useful totease the different notions apart.
Finally, it is mostintriguing to try to make a start on understandinghow some of the different notions might construedas alternative realizations of a single abstract no-tion.2 PronunciationJohn Laver, the author of the most widely usedtextbook in phonetics, claimed that ?one of the1most basic concepts in phonetics, and one of theleast discussed, is that of phonetic similarity[boldface in original, JN & EH]?
(Laver, 1994,p.
391), justifying the attention the workshop paysto it.
Laver goes on to sketch the work that hasbeen done on phonetic similarity, or, more ex-actly, phonetic distance, in particular, the empir-ical derivation of confusion matrices, which indi-cate the likelihood with which people or speechrecognition systems confusion one sound for an-other.
Miller & Nicely (1955) founded this ap-proach with studies of how humans confused somesounds more readily than others.
Although ?con-fusability?
is a reasonable reflection of phoneticsimilarity, it is perhaps worth noting that confu-sion matrices are often asymmetric, suggestingthat something more complex is at play.
Clark& Yallop (1995, p. 319ff) discuss this line ofwork further, suggesting more sophisticated anal-yses which aggregate confusion matrices based onsegments.In addition to the phonetic interest (above), pho-nologists have likewise shown interest in the ques-tion of similarity, especially in recent work.
Al-bright and Hayes (2003) have proposed a modelof phonological learning which relies on ?mini-mal generalization?.
The idea is that children learne.g.
rules of allomorphy on the basis not merelyof rules and individual lexical exceptions (the ear-lier standard wisdom), but rather on the basis ofslight but reliable generalizations.
An example isthe formation of the past tense of verbs ending in[IN], ?ing?
(fling, sing, sting, spring, string) thatbuild past tenses as ?ung?
[2N].
We omit detailsbut note that the ?minimal generalization?
is min-imally DISTANT in pronunciation.Frisch, Pierrehumbert & Broe (2004) have alsokindled an interest in segmental similarity amongphonologists with their claim that syllables inSemitic languages are constrained to have unlikeconsonants in syllable onset and coda.
Their workhas not gone unchallenged (Bailey and Hahn,2005; Hahn and Bailey, 2005), but it has certainlycreated further theoretical interest in phonologicalsimilarity.There has been a great deal of attention inpsycholinguistics to the the problem of wordrecognition, and several models appeal explic-itly to the ?degree of phonetic similarity amongthe words?
(Luce and Pisoni, 1998, p. 1), butmost of these models employ relatively simple no-tions of sequence similarity and/or, e.g., the ideathat distance may be operationalized by the num-ber or replacements needed to derive one wordfrom another?ignoring the problem of similarityamong words of different lengths (Vitevitch andLuce, 1999).
Perhaps more sophisticated com-putational models of pronunciation distance couldplay a role in these models in the future.Kessler (1995) showed how to employ edit dis-tance to operationalize pronunciation difference inorder to investigate dialectology more precisely,an idea which, particular, Heeringa (2004) pursuedat great length.
Kondrak (2002) created a vari-ant of the dynamic programming algorithm usedto compute edit distance which he used to iden-tify cognates in historical linguistics.
McMahon& McMahon (2005) include investigations of pro-nunciation similarity in their recent book on phy-logenetic techniques in historical linguistics.
Sev-eral of the contributions to this volume build onthese earlier efforts or are relevant to them.Kondrak and Sherif (this volume) continue theinvestigation into techniques for identifying cog-nates, now comparing several techniques whichrely solely on parameters set by the researcher tomachine learning techniques which automaticallyoptimize those parameters.
They show the the ma-chine learning techniques to be superior, in partic-ular, techniques basic on hidden Markov modelsand dynamic Bayesian nets.Heeringa et al (this volume) investigate severalextensions of the fundamental edit distance algo-rithm for use in dialectology, including sensitivityto order and context as well syllabicity constraints,which they argue to be preferable, and length nor-malization and graded weighting schemes, whichthey argue against.Dinu & Dinu (this volume) investigate metricson string distances which attach more importanceto the initial parts of the string.
They embed thisinsight into a scheme in which n-grams are ranked(sorted) by frequency, and the difference in therankings is used to assay language differences.Their paper proves that difference in rankings isa proper mathematical metric.Singh (this volume) investigates the technicalquestion of identifying languages and characterencoding systems from limited amounts of text.He collects about 1, 000 or so of the most fre-quent n-grams of various sizes and then classifiesnext texts based on the similarity between the fre-2quency distributions of the known texts with thoseof texts to be classified.
His empirical results show?mutual cross entropy?
to identify similarity mostreliably, but there are several close competitors.3 SyntaxAlthough there is less interest in similarity at thesyntactic level among linguistic theorists, there isstill one important areas of theoretical research inwhich it could play an important role and severalinterdisciplinary studies in which similarity and/ordistant is absolutely crucial.
Syntactic TYPOLOGYis an area of linguistic theory which seeks to iden-tify syntactic features which tend to be associatedwith one another in all languages (Comrie, 1989;Croft, 2001).
The fundamental vision is that somesorts of languages may be more similar to oneanother?typologically?than would first appear.Further, there are two interdisciplinary linguis-tic studies in which similarity and/or distanceplays a great role, including similarity at the syn-tactic level (without, however, exclusively focus-ing on syntax).
LANGUAGE CONTACT studiesseek to identify the elements of one languagewhich have been adopted in a second in a situa-tion in which two or more languages are used inthe same community (Thomason and Kaufmann,1988; van Coetsem, 1988).
Naturally, these maybe non-syntactic, but syntactic CONTAMINATIONis a central concept which is recognized in con-taminated varieties which have become more sim-ilar to the languages which are the source of con-tamination.Essentially the same phenomena is studied inSECOND-LANGUAGE LEARNING, in which syn-tactic patterns from a dominant, usually first, lan-guage are imposed on a second.
Here the focus ison the psychology of the individual language useras opposed to the collective habits of the languagecommunity.Nerbonne and Wiersma (this volume) collectfrequency distributions of part-of-speech (POS)trigrams and explore simple measures of distancebetween these.
They approach issues of statisti-cal significance using permutation tests, which re-quires attention to tricky issues of normalizationbetween the frequency distributions.Homola & Kubon?
(this volume) join Nerbonneand Wiersma in advocating a surface-orientedmeasure of syntactic difference, but base theirmeasure on dependency trees rather than POStags, a more abstract level of analysis.
From therethey propose an analogue to edit distance to gaugethe degree of difference.
The difference betweentwo tree is the sum of the costs of the tree-editingoperations needed to obtain one tree from another(Noetzel and Selkow, 1999).Emms (this volume) concentrates on applica-tions of the notion ?tree similarity?
in particular inorder to identify text which is syntactically sim-ilar to questions and which may therefore be ex-pected to constitute an answer to the question.
Heis able to show that the tree-distance measure out-performs sequence distance measures, at least iflexical information is also emphasized.Ku?bler (this volume) uses the similarity mea-sure in memory-based learning to parse.
This isa surprising approach, since memory-based tech-niques are normally used in classification taskswhere the target is one of a small number of po-tential classifications.
In parsing, the targets maybe arbitrarily complex, so a key step is select aninitial structure in a memory-based way, and thento adapt it further.
In this paper Ku?bler first applieschunking to the sentence to be parsed and selectsan initial parse based on chunk similarity.4 SemanticsWhile similarity as such has not been a prominentterm in theoretical and computational research onnatural language semantics, the study of LEXICALSEMANTICS, which attempts to identify regulari-ties of and systematic relations among word mean-ings, is more often than not predicated on an im-plicit notion of ?semantic similarity?.
Researchon the lexical semantics of verbs tries to identifyverb classes whose members exhibit similar syn-tactic and semantic behavior.
In logic-based the-ories of word meaning (e.g., Vendler (1967) andDowty (1979)), verb classes are identified by sim-ilarity patterns of inference, while Levin?s (1993)study of English verb classes demonstrates thatsimilarities of word meanings for verbs can begleaned from their syntactic behavior, in particu-lar from their ability or inability to participate indiatheses, i.e.
patterns of argument alternations.With the increasing availability of large elec-tronic corpora, recent computational research onword meaning has focused on capturing the notionof ?context similarity?
of words.
Such studies fol-low the empiricist approach to word meaning sum-marized best in the famous dictum of the British3linguist J.R. Firth: ?You shall know a word by thecompany it keeps.?
(Firth, 1957, p. 11) Contextsimilarity has been used as a means of extract-ing collocations from corpora, e.g.
by Church &Hanks (1990) and by Dunning (1993), of identify-ing word senses, e.g.
by Yarowski (1995) and bySchu?tze (1998), of clustering verb classes, e.g.
bySchulte im Walde (2003), and of inducing selec-tional restrictions of verbs, e.g.
by Resnik (1993),by Abe & Li (1996), by Rooth et al (1999) and byWagner (2004).A third approach to lexical semantics, devel-oped by linguists and by cognitive psychologists,primarily relies on the intuition of lexicographersfor capturing word meanings, but is also informedby corpus evidence for determining word usageand word senses.
This type of approach has led totwo highly valued semantic resources: the Prince-ton WordNet (Fellbaum, 1998) and the BerkeleyFramenet (Baker et al, 1998).
While originallydeveloped for English, both approaches have beensuccessfully generalized to other languages.The three approaches to word meaning dis-cussed above try to capture different aspects ofthe notion of semantic similarity, all of which arehighly relevant for current and future research incomputational linguistics.
In fact, the five pa-pers that discuss issues of semantic similarity inthe present volume build on insights from thesethree frameworks or address open research ques-tions posed by these frameworks.
Zesch andGurevych (this volume) discuss how measuresof semantic similarity?and more generally: se-mantic relatedness?can be obtained by similarityjudgments of informants who are presented withword pairs and who, for each pair, are asked torate the degree of semantic relatedness on a pre-defined scale.
Such similarity judgments can pro-vide important empirical evidence for taxonomicmodels of word meanings such as wordnets, whichthus far rely mostly on expert knowledge of lexi-cographers.
To this end, Zesch and Gurevych pro-pose a corpus-based system that supports fast de-velopment of relevant data sets for large subjectdomains.St-Jacques and Barrie`re (this volume) reviewand contrast different philosophical and psycho-logical models for capturing the notion of seman-tic similarity and different mathematical modelsfor measuring semantic distance.
They draw at-tention to the fact that, depending on which un-derlying models are in use, different notions of se-mantic similarity emerge and conjecture that dif-ferent similarity metrics may be needed for differ-ent NLP tasks.
Dagan (this volume) also exploresthe idea that different notions of semantic similar-ity are needed when dealing with semantic disam-biguation and language modeling tasks on the onehand and with applications such as information ex-traction, summarization, and information retrievalon the other hand.Dridan and Bond (this volume) and Hachey(this volume) both consider semantic similarityfrom an application-oriented perspective.
Dri-dan and Bond employ the framework of robustminimal recursion semantics in order to obtaina more adequate measure of sentence similar-ity than can be obtained by word-overlap met-rics for bag-of-words representations of sentences.They show that such a more fine-grained mea-sure, which is based on compact representationsof predicate-logic, yields better performance forparaphrase detection as well as for sentence se-lection in question-answering tasks than simpleword-overlap metrics.
Hachey considers an au-tomatic content extraction (ACE) task, a particu-lar subtask of information extraction.
He demon-strates that representations based on term co-occurrence outperform representations based onterm-by-document matrices for the task of iden-tifying relationships between named objects intexts.AcknowledgmentsWe are indebted to our program committee andto the incidental reviewers named in the organi-zational section of the book, and to others whoremain anonymous.
We thank Peter Kleiweg formanaging the production of the book and ThereseLeinonen for discussions about phonetic similar-ity.
We are indebted to the Netherlands Organi-zation for Scientific Research (NWO), grant 200-02100, for cooperation between the Center forLanguage and Cognition, Groningen, and the Sem-inar fu?r Sprachwissenschaft, Tu?bingen, for sup-port of the work which is reported on here.
We arealso indebted to the Volkswagen Stiftung for theirsupport of a joint project ?Measuring LinguisticUnity and Diversity in Europe?
that is carried outin cooperation with the Bulgarian Academy ofScience, Sofia.
The work reported here is directlyrelated to the research objectives of this project.4ReferencesNaoki Abe and Hang Li.
1996.
Learning word associ-ation norms using tree cut pair models.
In Proceed-ings of 13th International Conference on MachineLearning.Adam Albright and Bruce Hayes.
2003.
Rulesvs.
analogy in English past tenses: A computa-tional/experimental study.
Cognition, 90:119?161.Todd M. Bailey and Ulrike Hahn.
2005.
PhonemeSimilarity and Confusability.
Journal of Memoryand Language, 52(3):339?362.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ChristianBoitet and Pete Whitelock, editors, Proceedings ofthe Thirty-Sixth Annual Meeting of the Associationfor Computational Linguistics and Seventeenth In-ternational Conference on Computational Linguis-tics, pages 86?90, San Francisco, California.
Mor-gan Kaufmann Publishers.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.John Clark and Colin Yallop.
1995.
An Introduction toPhonetics and Phonology.
Blackwell, Oxford.Bernard Comrie.
1989.
Language Universals and Lin-guistic Typology:Syntax and Morphology.
Oxford,Basil Blackwell.William Croft.
2001.
Radical Construction Grammar:Syntactic Theory in Typological Perspective.
Ox-ford University Press, Oxford.David Dowty.
1979.
Word Meaning and MontagueGrammar.
Reidel, Dordrecht.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.J.
R. Firth.
1957.
A synopsis of linguistic theory.
Ox-ford: Philological Society.
Reprinted in F.
Palmer(ed.)(1968).
Studies in Linguistic Analysis 1930-1955.
Selected Papers of J.R.
Firth., Harlow: Long-man.Stefan A. Frisch, Janet B. Pierrehumbert, andMichael B. Broe.
2004.
Similarity Avoidance andthe OCP.
Natural Language & Linguistic Theory,22(1):179?228.Ulrike Hahn and Todd M. Bailey.
2005.
What MakesWords Sound Similar?
Cognition, 97(3):227?267.Wilbert Heeringa.
2004.
Measuring Dialect Pronunci-ation Differences using Levenshtein Distance.
Ph.D.thesis, Rijksuniversiteit Groningen.Brett Kessler.
1995.
Computational dialectology inIrish Gaelic.
In Proc.
of the European ACL, pages60?67, Dublin.Grzegorz Kondrak.
2002.
Algorithms for LanguageReconstruction.
Ph.D. thesis, University of Toronto.John Laver.
1994.
Principles of Phonetics.
CambridgeUniveristy Press, Cambridge.Ludovic Lebart and Martin Rajman.
2000.
Comput-ing similarity.
In Robert Dale, Hermann Moisl, andHarold Somers, editors, Handbook of Natural Lan-guage Processing, pages 477?505.
Dekker, Basel.Beth Levin.
1993.
English Verb Classes and Alter-nations: a Preliminary Investigation.
University ofChicago Press, Chicago and London.Paul A. Luce and David B. Pisoni.
1998.
Recognizingspoken words: The neighborhood activation model.Ear and Hearing, 19(1):1?36.April McMahon and Robert McMahon.
2005.
Lan-guage Classification by the Numbers.
Oxford Uni-versity Press, Oxford.George A. Miller and Patricia E. Nicely.
1955.
AnAnalysis of Perceptual Confusions Among SomeEnglish Consonants.
The Journal of the AcousticalSociety of America, 27:338?352.Andrew S. Noetzel and Stanley M. Selkow.
1999.An analysis of the general tree-editing problem.
InDavid Sankoff and Joseph Kruskal, editors, TimeWarps, String Edits and Macromolecules: The The-ory and Practice of Sequence Comparison, pages237?252.
CSLI, Stanford.
11983.Philip Stuart Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing an semanti-cally annotated lexicon via em-based clustering.
InProceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics, Maryland.Sabine Schulte im Walde.
2003.
Experiments onthe Automatic Induction of German Semantic VerbClasses.
Ph.D. thesis, Institut fu?r MaschinelleSprachverarbeitung, Universita?t Stuttgart.
Pub-lished as AIMS Report 9(2).Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Sarah Thomason and Terrence Kaufmann.
1988.
Lan-guage Contact, Creolization, and Genetic Linguis-tics.
University of California Press, Berkeley.Frans van Coetsem.
1988.
Loan Phonology and theTwo Transfer Types in Language Contact.
Publica-tions in Language Sciences.
Foris Publications, Dor-drecht.5Zeno Vendler.
1967.
Linguistics in Philosophy.
Cor-nell University Press, Ithaca, NY.Michael S. Vitevitch and Paul A. Luce.
1999.
Prob-abilistic Phonotactics and Neighborhood Activationin Spoken Word Recognition.
Journal of Memoryand Language, 40(3):374?408.Andreas Wagner.
2004.
Learning Thematic Role Rela-tions for Lexical Semantic Nets.
Ph.D. thesis, Uni-versita?t Tu?bingen.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of 33rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 189?196,Cambridge, MA.6
