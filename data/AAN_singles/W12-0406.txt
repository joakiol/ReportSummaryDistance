Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 39?47,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsOn the Use of Homogenous Sets of Subjects in Deceptive LanguageAnalysisTommaso FornaciariCenter for Mind/Brain SciencesUniversity of Trentotommaso.fornaciari@unitn.itMassimo PoesioLanguage and Computation GroupUniversity of EssexCenter for Mind/Brain SciencesUniversity of Trentomassimo.poesio@unitn.itAbstractRecent studies on deceptive language sug-gest that machine learning algorithms canbe employed with good results for classi-fication of texts as truthful or untruthful.However, the models presented so far donot attempt to take advantage of the dif-ferences between subjects.
In this paper,models have been trained in order to clas-sify statements issued in Court as false ornot-false, not only taking into considera-tion the whole corpus, but also by identify-ing more homogenous subsets of producersof deceptive language.
The results suggestthat the models are effective in recogniz-ing false statements, and their performancecan be improved if subsets of homogeneousdata are provided.1 IntroductionDetecting deceptive communication is a challeng-ing task, but one that could have a number of use-ful applications.
A wide variety of approaches tothe discovery of deceptive statements have beenattempted, ranging from using physiological sen-sors such as lie detectors to using neurosciencemethods (Davatzikos et al, 2005; Ganis et al,2003).
More recently, a number of techniqueshave been developed for recognizing deceptionon the basis of the communicative behavior ofsubjects.
Given the difficulty of the task, manysuch methods rely on both verbal and non-verbalbehavior, to increase accuracy.
So for instanceDe Paulo et al (2003) considered more than 150cues, verbal and non-verbal, directly observedthrough experimental subjects.
But finding cluesindicating deception through manual inspection isnot easy.
De Paulo et al asserted that ?behaviorsthat are indicative of deception can be indicativeof other states and processes as well?.The same point is made in more recent liter-ature: thus Frank et al (2008) write ?We findthat there is no clue or clue pattern that is spe-cific to deception, although there are clues spe-cific to emotion and cognition?, and they wishfor ?real-world databases, identifying base ratesfor malfeasant behavior in security settings, opti-mizing training, and identifying preexisting excel-lence within security organizations?.
Jensen et al(2010) exploited cues coming from audio, videoand textual data.One solution is to let statistical and machinelearning methods discover the clues.
Work suchas Fornaciari and Poesio (2011a,b); Newman et al(2003); Strapparava and Mihalcea (2009) sug-gests that these techniques can perform reason-ably well at the task of discovering deceptioneven just from linguistic data, provided that cor-pora containing examples of deceptive and truth-ful texts are available.
The availability of suchcorpora is not a trivial problem, and indeed, thecreation of a realistic such corpus is one of theproblems in which we invested substantial effortin our own previous work, as discussed in Section3.In the work discussed in this paper, we tacklean issue which to our knowledge has not beenaddressed before, due to the limitations of thedatasets previously available: this is whether theindividual difference between experimental sub-jects affect deception detection.
In previous work,lexical (Fornaciari and Poesio, 2011a) and surface(Fornaciari and Poesio, 2011b) features were em-ployed to classify deceptive statements issued inItalian Courts.
In this study, we report the results39of experiments in which our methods were trainedeither over the whole corpus or over smaller sub-sets consisting of the utterances produced by morehomogenous subsets of subjects.
These subsetswere identified either automatically, by cluster-ing subjects according to their language profile,or by using meta-information about the subjectsincluded in the corpus, such as their gender.The structure of the paper is as follows.
In Sec-tion 2 some background knowledge is introduced.In Section 3 the data set is described.
In Section 4we discuss our machine learning and experimen-tal methods.
Finally, the results are presented inSection 5 and discussed in Section 6.2 Background2.1 Deceptive language analysisFrom a methodological point of view, to investi-gate deceptive language gives rise to some trickyissues: first of all, the strategy chosen to collectdata.
The literature can be divided in two mainfamilies of studies:?
Field studies;?
Laboratory studies.The first ones are usually interesting in forensicapplications but in such studies verifying the sin-cerity of the statements is often complicated (Vrij,2005).
Laboratory studies, instead, are character-ized by the artificiality of participants?
psycholog-ical conditions: therefore their findings may notbe generalized to deception encountered in reallife.Due to practical difficulties in collection andannotation of suitable data, in literature findingpapers in which real life linguistic data are em-ployed, where truthfulness is surely known, isless common and Zhou et al (2008) complainabout the lack of ?data set for evaluating decep-tion detection models?.
Just recently some studiestried to fill this gap, concerning both the English(Bachenko et al, 2008; Fitzpatrick and Bachenko,2009) and Italian language (Fornaciari and Poe-sio, 2011a,b).
Just the studies on Italian languagecome from data which have constituted the firstnucleus of the corpus analysed here.2.2 StylometryOur own work and that of other authors that re-cently employed machine learning techniques todetect deception in text employs techniques verysimilar to that of stylometry.
Stylometry is a dis-cipline which studies texts on the basis of theirstylistic features, usually in order to attribute themto an author - giving rise to the branch of authorattribution - or to get information about the authorhimself - this is the field of author profiling.Stylometric analyses, which relies mainly onmachine learning algorithms, turned out to be ef-fective in several forensic tasks: not only the clas-sical field of author profiling (Coulthard, 2004;Koppel et al, 2006; Peersman et al, 2011; Solanand Tiersma, 2004) and author attribution (Luy-ckx and Daelemans, 2008; Mosteller and Wallace,1964), but also emotion detection (Vaassen andDaelemans, 2011) and plagiarism analysis (Steinet al, 2007).
Therefore, from a methodologicalpoint of view, Deceptive Language Analysis is aparticular application of stylometry, exactly likeother branches of Forensic Linguistics.3 Data set3.1 False testimonies in CourtIn order to study deceptive language, we createdthe DECOUR - DEception in COURt - corpus,better described in Fornaciari and Poesio (2012).DECOUR is a corpus constituted by the tran-scripts of 35 hearings held in four Italian Courts:Bologna, Bolzano, Prato and Trento.
These tran-scripts report verbatim the statements issued by atotal of 31 different subjects - four of which havebeen heard twice.
All the hearings come fromcriminal proceedings for calumny and false tes-timony (artt.
368 and 372 of the Italian CriminalCode).In particular, the hearings of DECOUR comemainly from two situations:?
the defendant for any criminal proceedingtries to use calumny against someone;?
a witness in any criminal proceeding lies forsome reason.In both cases, a new criminal proceeding arises,in which the subjects can issue new statements ornot, and having as a body of evidence the tran-script of the hearing held in the previous proceed-ing.The crucial point is that DECOUR only in-cludes text from individuals who in the end havebeen found guilty.
Hence the proceeding ends40with a judgment of the Court which summarizethe facts, pointing out precisely the lies told bythe speaker in order to establish his punishment.Thanks to the transcripts of the hearing and to thefinal judgment of the Court, it is possible to anno-tate the statements of the speakers on the basis oftheir truthfulness or untruthfulness, as follows.3.2 Annotation and agreementThe hearings are dialogs, in which the judge, thepublic prosecutor and the lawyer pose questionsto the witness/defendant who in turn has to givethem answers.
These answers are the object ofinvestigation of this study.
Each answer is con-sidered a turn, delimited by the end of the pre-vious and the beginning of the following inter-vention of another individual.
Each turn is con-stituted by one or more utterances, delimited bypunctuation marks: period, triple-dots, questionand exclamation marks.
Utterances are the anal-ysis unit of DECOUR and have been annotated asfalse, true or uncertain.
In order to verify theagreement in the judgments about truthfulness oruntruthfulness of the utterances, three annotatorsseparately annotated about 600 utterances.
Theagreement study concerning the three classes ofutterances, described in detail in (Fornaciari andPoesio, 2012), showed that the agreement valuewas k=.57.
Instead, if the problem is reduced toa binary task - that is, if true and uncertain utter-ances are collapsed into a single category of not-false utterances, opposed to the category of falseones - the agreement value is k=.64.3.3 Corpus statisticsThe whole corpus has been tokenized and sensi-tive data have been made anonymous, accordingto the previous agreement with the Courts.
ThenDECOUR has been lemmatized and POS-taggedusing a version of TreeTagger1 (Schmid, 1994)trained for Italian.DECOUR is made up of 3015 utterances, whichcome from 2094 turns.
945 utterances have beenannotated as false, 1202 as true and 868 as un-certain.
The size of DECOUR is 41819 tokens,including punctuation blocks.1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html4 MethodsIn this Section we first summarize our classifica-tion methods from previous work, then discuss thethree experiments we carried out.4.1 Classification methodsEach utterance is described by a feature vector.As in our previous studies (Fornaciari and Poesio,2011a,b) three kinds of features were used.First of all, the feature vectors include very ba-sic linguistic information such as the length of ut-terances (with and without punctuation) and thenumber of words longer than six letters.The second type of information are lexical fea-tures.
These features have been collected mak-ing use of LIWC - Linguistic Inquiry and WordCount, a linguistic tool realized by Pennebakeret al (2001) and widely employed in deceptiondetection (Newman et al, 2003; Strapparava andMihalcea, 2009).
LIWC is based on a dictionaryin which each term is associated with an appro-priate set of syntactical, semantical and/or psy-chological categories.
When a text is analysedwith LIWC, the tokens of the text are comparedwith the LIWC dictionary.
Every time a wordpresent in the dictionary is found, the count ofthe corresponding categories grows.
The outputis a profile of the text which relies on the rate ofincidence of the different categories in the text it-self.
LIWC also includes different dictionaries forseveral languages, amongst which Italian (Agostiand Rellini, 2007).
Therefore it has been possi-ble to apply LIWC to Italian deceptive texts, andthe approximate 80 linguistic dimensions whichconstitute the Italian LIWC dictionary have beenincluded as features of the vectors.Lastly, frequencies of lemmas and part-of-speech n-grams were used.
Five kinds of n-grams of lemmas and part-of-speech were takeninto consideration: from unigrams to pentagrams.These frequency lists come from the part of DE-COUR employed as training set.
More precisely,they come from the utterances held as true or falseof the training set, while the uncertain utteranceshave not been considered.
In order to empha-size the collection of features effective in clas-sifying true and false statements, frequency listsof n-grams have been built considering true andfalse utterances separately.
This means that, inthe training set, homologous frequency lists of n-41Table 1: The most frequent n-grams collectedN-grams Lemmas POS TotalUnigrams 50 15Bigrams 40 12Trigrams 30 9Tetragrams 20 6Pentagrams 10 3Total 150 45 195grams - unigrams, bigrams and so on - have beencollected from the subset of true utterances andform the subset of false ones.
From these lists,the most frequent n-grams have been collected, ina decreasing amount according to the length of then-grams.
Table 1 shows in detail the number ofthe most frequent lemmas and part-of-speech col-lected for the different n-grams.
Then the couplesof frequency lists were merged into one.This procedure implies that the number of sur-face features is not determined a priori.
In factthe 195 features indicated in Table 1, which arecollected from true and false utterances, are uni-fied in a list where each feature has to appearonly once.
Therefore, theoretically in the case ofperfect identity of features in true and false ut-terances, a final list with the same 195 featureswould be obtained.
In the opposite case, if then-grams from true and false utterances would becompletely different, a list of 195 + 195, then 390n-grams would result.
The aim of this procedureis to get a list of n-grams which could be as muchas possible representative of the features of trueand false utterances.
Obviously, the smaller theoverlap of the features of the two subsets, thegreater the difference in the appearance of trueand false utterances, and greater the hope to reacha good performance in the classification task.We used the Support Vector Machine imple-mentation in R (Dimitriadou et al, 2011).
Asspecified above, the classes of the utterances arefalse vs. not-false, where the category of not-falseutterances results from the union of the true anduncertain ones.4.2 Corpus divisionWith the aim of training models able to classifythe utterances of DECOUR as false or not-false,the corpus has been divided as follows:Training set The 20 hearings coming from theCourts of Bologna and Bolzano have beenemployed as training set.
In terms of anal-ysis units, this means 2279 utterances, thatis 75.59% of DECOUR.
The features of thevectors come from this set of data.Test set The 9 hearings of the Court of Trentohave been employed as test set, in order toevaluate the effectiveness of the trained mod-els.
This test set was made up by 426 utter-ances, which are 14.13% of DECOUR.Development set The 6 hearings of the Court ofPrato have been employed as developmentset during the phase of choice and calibrationof vector features, therefore this set of utter-ances is not directly involved in the results ofthe following experiments.
The develpmentset was constituted by 310 utterances, that is10.28% of DECOUR.In the various experimental conditions, some sub-sets of DECOUR have been taken into consider-ation.
Hence, different hearings have been re-moved from the test and/or training set in orderto carry out different experiments.
Since the testsets vary in the different experiments, in relationto each of them different chance levels have beendetermined, in order to evaluate the effectivenessof the models?
performance.4.3 ExperimentsThree experiments were carried out.
In the firstexperiment, the entire corpus was used to trainand test our algorithms.
In the second and thirdexperiment, sub-corpora were identified.4.3.1 Experiment 1: whole test setIn the first experiment, the classification taskhas been carried out simply employing the train-ing set and the test set as described above, in orderto have a control as reference point in relation tothe following experiments.4.3.2 Experiment 2: no outliersIn the second experiment, a more homogeneoussubset of DECOUR was obtained by automati-cally identifying and removing outliers.
This wasdone in an unsupervised way by building vectordescriptions of the hearings and clustering them.The features of these vectors were the same n-grams described above, collected from the whole42Figure 1: Multi-Dimensional Scaling of DE-COURE?ach entity corresponds to a hearing; the lettersrepresent the sex of the speakers.corpus (not from the only test set); their valueswere the mean values of the frequencies of the ut-terances belonging to the hearing.This data set has been transformed into a ma-trix of between-hearing distances and a Multi-Dimensional Scaling - MDS function has beenapplied to this matrix (Baayen, 2008).
Figure 1shows the plot of MDS function.
Each entity cor-responds to a hearing, and is represented by a let-ter indicating the sex of the speaker.
Getting aglimpse at Figure 1, it is possible to notice that,in general, almost all the hearings are quite close- that is, similar - to each other.
Only three hear-ings seem to be clearly more peripheral than allthe others, particularly the three most to the left inFigure 1.
These hearings have been considered asoutliers and shut out from the experiment.
Theyare two hearings from Trento and one from Prato.In practice, it means that the training set, com-ing from the hearings of Bologna and Bolzano,remained the same as the previous experiment,while two hearings have been removed from thetest set, which was constituted only by the hear-ings of Trento.4.3.3 Experiment 3: only male speakersDifferent from the previous one, the third ex-periment does not rely on a subset of data au-tomatically identified.
Instead, the subset comesfrom personal information concerning the sub-jects involved in the hearings.
In fact, their sex,place of birth and age at the moment of the hear-ing are known.
In this paper, places of birthand age have not been taken into consideration,since grouping them together in reliable cate-gories raises issues that do not have a straightfor-ward solution, and the size of the subsets of cor-pus which would be obtained must be taken intoaccount.Therefore this experiment has been carried outtaking into consideration only the sex of the sub-jects, and in particular it concerned only the hear-ings involving men.
This meant reducing thetraining set consistently, where seven hearings ofwomen were present and thence removed.
Insteadfrom the test set just three hearings have beentaken off, one involving a woman and two involv-ing a transsexual.4.4 BaselinesThe chance levels for the various test sets havebeen calculated through Monte Carlo simula-tions, each one specific to every experiment.
Ineach simulation, 100000 times a number of ran-dom predictions has been produced, in the sameamount and with the same rate of false utterancesof the test set employed in the single experiment.Then this random output was compared to the realsequence of false and not-false utterances of thetest set, in order to count the amount of correctpredictions.
The rate of correct answers reachedby less than 0.01% of the random predictions hasbeen accepted as chance threshold for every ex-periment.As a baseline, a simple majority baseline wascomputed: to classify each utterance as belongingto the most numerous class in the test set (not-false).5 ResultsThe test set of the first experiemnt, carried outon the whole test set, was made up of 426 utter-ances, of which 190 were false, that is 44.60%.While the majority baseline is 55.40% of accu-racy, a Monte Carlo simulation applied to the testset showed that the chance level was 59.60% ofcorrect predictions.
The results are shown in Ta-ble 2.
The overall accuracy - almost 66% - isclearly above the chance level, being more thansix points greater than the baseline.43Table 2: Whole training and test setCorrectly Incorrectlyclassified entities classified entities Precision Recall F-measureFalse utterances 59 131 80.82% 31.05% 44.86%True utterances 222 14 62.89% 94.07% 75.38%Total 281 145Total percent 65.96% 34.04%Monte Carlo simulation 59.60%Majority baseline 55.40%Table 3: Test set without outliersCorrectly Incorrectlyclassified entities classified entities Precision Recall F-measureFalse utterances 51 90 80.95% 36.17% 50.00%True utterances 180 12 66.67% 93.75% 77.92%Total 231 102Total percent 69.37% 30.63%Monte Carlo simulation 61.26%Majority baseline 57.66%Table 4: Training and test set with only male speakersCorrectly Incorrectlyclassified entities classified entities Precision Recall F-measureFalse utterances 32 85 74.42% 27.35% 40.00%True utterances 179 11 67.80% 94.21% 78.85%Total 211 96Total percent 68.73% 31.27%Monte Carlo simulation 63.19%Majority baseline 61.89%In the second experiment, the test set withoutoutliers was made up of 333 utterances; 141 werefalse, which means 42.34% of the test set.
Themajority baseline was then at 57.66%, while thechance threshold determined with a Monte Carlosimulation had an accuracy rate of 61.26%.
Ta-ble 3 shows the results of the analyses.
Taking theoutliers out of the test set alows tthe best perfor-mance of the three experiments to be reached.
Infact the accuracy is more than 69%, which is morethan eight points above the highest chance level of61.26%.In the third experimental condition, where onlymale speakers were considered, the training setwas made up of 13 hearings and the test set of6 hearings.
The utterances in the test set were307, of which 117 were false, meaning 38.11%of the test set.
In this last case, the majority base-line is at 61.89% of accuracy, while according toa Monte Carlo simulation the chance level was63.19%.
The overall accuracy reached in this ex-periment, shown in Table 4, was more than 68%:higher than the first experiment, but in this casethe lower amount of false utterances in the testset led to higher chance thresholds.
Therefore thedifference between performance and the chance44level of 63.19% is now the smallest of all the ex-periments: just five points and half.From the point of view of detection of falseutterances, although with internal differences, allthe experiments are placed in the same referenceframe.
In particular, the weak point in perfor-mance is always the recall of false utterances,which remains more or less at 30%.
Instead thegood news comes from the precision in recogniz-ing them, which is close to 80%.
Regarding trueutterances, the recall is always good, being neverlower than 93%, while the precision is close to65%.6 DiscussionThe goal of this paper was to verify if restrictingthe analysis to more homogeneous subsets couldimprove the accuracy of our models.
The resultsare mixed.
On the one end, taking the outliers outof the corpus results in a remarkable improvementof accuracy in the classification task, in relationto the performance of the models tested on thewhole test set.
On the other end, in other cases- most clearly, considering only speakers of themale gender - we find no difference; our hypoth-esis is that any potential advantage derived fromthe increased homogeneity is offset by the reduc-tion in training material (seven hearings are re-moved in this case).
So the conclusion may bethat increasing homogeneity is effective providedthat the remaining set is still sufficiently large.Regarding the models?
capacity to detect falserather than true utterances, the difference betweenthe respective recalls is noteworthy.
In fact, whilethe recall of not-false utterances is very high, thatof false ones is poor.
In other words, the resultsindicate that an amount of false utterances is ef-fectively so similar to the not-false ones, that themodels are not able to detect them.
One challengefor future studies is surely to find a way to detectsome aspect currently neglected of deceptive lan-guage, which could be employed to widen the sizeof false utterances which can be recognized.On the other hand, in the two more reliable ex-periments the precision in detecting false utter-ances was about 80%.
This could suggest that anamount of false utterances exists, whose featuresare in some way peculiar and different from not-false ones.
The data seem to show that this subsetcould be more or less one third of all the false ut-terances.However, this study was not aimed to estimatethe possible performance of the models in an hy-pothetic practical application.
The experimentalconditions taken into consideration, in fact, areconsiderably different from those that would bepresent in a real life analysis.The main reason of this difference is that in areal case to classify every utterance of a hearingwould not be requested.
A lot of statements are ir-relevant or perfectly known as true.
Furthermoreit would not make sense to classify all the utter-ances which have not propositional value, such asquestions or meta-communicative acts.
In the per-spective of deception detection in a real life sce-nario, to classify this last kind of utterances is use-less.
Only a subset of the propositional statementsshould be classified.
In a previous study, carriedout on a selection of utterances with propositionalvalue of a part of DECOUR, machine learningmodels reached an accuracy of 75% in classifica-tion task (Fornaciari and Poesio, 2011b).
In thatstudy, precision and recall of false utterances arealso quite similar to those of this study, the firstbeing about 90% and the second about 50%.From a theoretical point of view, the presentstudy suggests that it is possible to be relativelyconfident in the effectiveness of the models in theanalysis of any kind of utterance.
This meansthat deceptive language is at least in part differ-ent from the truthful one and stylometric analysescan detect it.
If this is true, the rate of precisionwith which false statements are correctly classi-fied should clearly exceed the chance level.Also in this case, Monte Carlo simulation istaken as reference point.
Out of the 100000 ran-dom trials carried out to determine the baseline forthe first experiment, less than 0.01% had a preci-sion greater than 57.90% in classifying false ut-terances, in front of a precision of the models at80.82%.
Regarding the second experiment, thethreshold for precision related to false utteranceswas 58.15% against a precision of the models at80.95%.
In the third experiment, the baselinefor precision was 55.55% and the performance ofmodels was 74.42%.
In every experiment the gapis about twenty points per cent.
The same cannotbe said about the recall of false utterances: thebaselines of Monte Carlo simulations in the threeexperiments were about 51-54%, while the bestmodels?
performance (of the second experiment)did not exceed 36%.45The precision reached in recognizing falsestatements shows that the models were reliablein detection of deceptive language.
On the otherhand a remarkable amount of false utterances wasnot identified.
The challenge for the future is tounderstand to which extent it will be possible toimprove the recall in detecting false utterances,not losing and hopefully improving the relativeprecision.
At that point, although in specific con-texts, a computational linguistics?
approach couldbe really employed to detect deception in real lifescenarios.7 AcknowledgementsTo create DECOUR has been very complex, andit would not have been possible without the kindcollaboration of a lot of people.
Many thanks toDr.
Francesco Scutellari, President of the Courtof Bologna, to Dr. Heinrich Zanon, President ofthe Court of Bolzano, to Dr. Francesco AntonioGenovese, President of the Court of Prato and toDr.
Sabino Giarrusso, President of the Court ofTrento.ReferencesAgosti, A. and Rellini, A.
(2007).
The ItalianLIWC Dictionary.
Technical report, LIWC.net,Austin, TX.Baayen, R. (2008).
Analyzing linguistic data:a practical introduction to statistics using R.Cambridge University Press.Bachenko, J., Fitzpatrick, E., and Schonwetter,M.
(2008).
Verification and implementationof language-based deception indicators in civiland criminal narratives.
In Proceedings of the22nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08,pages 41?48, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Coulthard, M. (2004).
Author identification, idi-olect, and linguistic uniqueness.
Applied Lin-guistics, 25(4):431?447.Davatzikos, C., Ruparel, K., Fan, Y., Shen, D.,Acharyya, M., Loughead, J., Gur, R., and Lan-gleben, D. (2005).
Classifying spatial patternsof brain activity with machine learning meth-ods: Application to lie detection.
NeuroImage,28(3):663 ?
668.De Paulo, B. M., Lindsay, J. J., Malone, B. E.,Muhlenbruck, L., Charlton, K., and Cooper, H.(2003).
Cues to deception.
Psychological Bul-letin, 129(1):74?118.Dimitriadou, E., Hornik, K., Leisch, F., Meyer,D., and Weingessel, A.
(2011).
r-cran-e1071.
http://mloss.org/software/view/94/.Fitzpatrick, E. and Bachenko, J.
(2009).
Buildinga forensic corpus to test language-based indi-cators of deception.
Language and Computers,71(1):183?196.Fornaciari, T. and Poesio, M. (2011a).
Lexicalvs.
surface features in deceptive language anal-ysis.
In Proceedings of the ICAIL 2011 Work-shop Applying Human Language Technology tothe Law, AHLTL 2011, pages 2?8, Pittsburgh,USA.Fornaciari, T. and Poesio, M. (2011b).
Sin-cere and deceptive statements in italian crimi-nal proceedings.
In Proceedings of the Interna-tional Association of Forensic Linguists TenthBiennial Conference, IAFL 2011, Cardiff,Wales, UK.Fornaciari, T. and Poesio, M. (2012).
Decour: acorpus of deceptive statements in italian courts.In Proceedings of the eighth International Con-ference on Language Resources and Evalua-tion, LREC 2012.
In press.Frank, M. G., Menasco, M. A., and O?Sullivan,M.
(2008).
Human behavior and deception de-tection.
In Voeller, J. G., editor, Wiley Hand-book of Science and Technology for HomelandSecurity.
John Wiley & Sons, Inc.Ganis, G., Kosslyn, S., Stose, S., Thompson, W.,and Yurgelun-Todd, D. (2003).
Neural corre-lates of different types of deception: An fmriinvestigation.
Cerebral Cortex, 13(8):830?836.Jensen, M. L., Meservy, T. O., Burgoon, J. K., andNunamaker, J. F. (2010).
Automatic, Multi-modal Evaluation of Human Interaction.
GroupDecision and Negotiation, 19(4):367?389.Koppel, M., Schler, J., Argamon, S., and Pen-nebaker, J.
(2006).
Effects of age and gender onblogging.
In AAAI 2006 Spring Symposium onComputational Approaches to Analysing We-blogs.46Luyckx, K. and Daelemans, W. (2008).
Author-ship attribution and verification with many au-thors and limited data.
In Proceedings of the22nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08,pages 513?520, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Mosteller, F. and Wallace, D. (1964).
Infer-ence and Disputed Authorship: The Federalist.Addison-Wesley.Newman, M. L., Pennebaker, J. W., Berry, D. S.,and Richards, J. M. (2003).
Lying Words:Predicting Deception From Linguistic Styles.Personality and Social Psychology Bulletin,29(5):665?675.Peersman, C., Daelemans, W., and Van Vaeren-bergh, L. (2011).
Age and gender prediction onnetlog data.
Presented at the 21st Meeting ofComputational Linguistics in the Netherlands(CLIN21), Ghent, Belgium.Pennebaker, J. W., Francis, M. E., and Booth, R.
J.(2001).
Linguistic Inquiry and Word Count(LIWC): LIWC2001.
Lawrence Erlbaum As-sociates, Mahwah.Schmid, H. (1994).
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods inLanguage Processing.Solan, L. M. and Tiersma, P. M. (2004).
Authoridentification in american courts.
Applied Lin-guistics, 25(4):448?465.Stein, B., Koppel, M., and Stamatatos, E. (2007).Plagiarism analysis, authorship identification,and near-duplicate detection pan?07.
SIGIR Fo-rum, 41:68?71.Strapparava, C. and Mihalcea, R. (2009).
TheLie Detector: Explorations in the AutomaticRecognition of Deceptive Language.
In Pro-ceeding ACLShort ?09 - Proceedings of theACL-IJCNLP 2009 Conference Short Papers.Vaassen, F. and Daelemans, W. (2011).
Auto-matic emotion classification for interpersonalcommunication.
In 2nd Workshop on Compu-tational Approaches to Subjectivity and Senti-ment Analysis (WASSA 2.011).Vrij, A.
(2005).
Criteria-based content analysis- A Qualitative Review of the First 37 Studies.Psychology, Public Policy, and Law, 11(1):3?41.Zhou, L., Shi, Y., and Zhang, D. (2008).
AStatistical Language Modeling Approach toOnline Deception Detection.
IEEE Transac-tions on Knowledge and Data Engineering,20(8):1077?1081.47
