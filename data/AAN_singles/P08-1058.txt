Proceedings of ACL-08: HLT, pages 505?513,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRandomized Language Models via Perfect Hash FunctionsDavid Talbot?School of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh, UKd.r.talbot@sms.ed.ac.ukThorsten BrantsGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA 94303, USAbrants@google.comAbstractWe propose a succinct randomized languagemodel which employs a perfect hash func-tion to encode fingerprints of n-grams andtheir associated probabilities, backoff weights,or other parameters.
The scheme can repre-sent any standard n-gram model and is easilycombined with existing model reduction tech-niques such as entropy-pruning.
We demon-strate the space-savings of the scheme via ma-chine translation experiments within a dis-tributed language modeling framework.1 IntroductionLanguage models (LMs) are a core component instatistical machine translation, speech recognition,optical character recognition and many other areas.They distinguish plausible word sequences from aset of candidates.
LMs are usually implementedas n-gram models parameterized for each distinctsequence of up to n words observed in the train-ing corpus.
Using higher-order models and largeramounts of training data can significantly improveperformance in applications, however the size of theresulting LM can become prohibitive.With large monolingual corpora available in ma-jor languages, making use of all the available datais now a fundamental challenge in language mod-eling.
Efficiency is paramount in applications suchas machine translation which make huge numbersof LM requests per sentence.
To scale LMs to largercorpora with higher-order dependencies, researchers?Work completed while this author was at Google Inc.have considered alternative parameterizations suchas class-based models (Brown et al, 1992), modelreduction techniques such as entropy-based pruning(Stolcke, 1998), novel represention schemes such assuffix arrays (Emami et al, 2007), Golomb Coding(Church et al, 2007) and distributed language mod-els that scale more readily (Brants et al, 2007).In this paper we propose a novel randomized lan-guage model.
Recent work (Talbot and Osborne,2007b) has demonstrated that randomized encod-ings can be used to represent n-gram counts forLMs with signficant space-savings, circumventinginformation-theoretic constraints on lossless datastructures by allowing errors with some small prob-ability.
In contrast the representation scheme usedby our model encodes parameters directly.
It canbe combined with any n-gram parameter estimationmethod and existing model reduction techniquessuch as entropy-based pruning.
Parameters that arestored in the model are retrieved without error; how-ever, false positives may occur whereby n-grams notin the model are incorrectly ?found?
when requested.The false positive rate is determined by the space us-age of the model.Our randomized language model is based on theBloomier filter (Chazelle et al, 2004).
We encodefingerprints (random hashes) of n-grams togetherwith their associated probabilities using a perfecthash function generated at random (Majewski et al,1996).
Lookup is very efficient: the values of 3 cellsin a large array are combined with the fingerprint ofan n-gram.
This paper focuses on machine transla-tion.
However, many of our findings should transferto other applications of language modeling.5052 Scaling Language ModelsIn statistical machine translation (SMT), LMs areused to score candidate translations in the target lan-guage.
These are typically n-gram models that ap-proximate the probability of a word sequence by as-suming each token to be independent of all but n?1preceding tokens.
Parameters are estimated frommonolingual corpora with parameters for each dis-tinct word sequence of length l ?
[n] observed inthe corpus.
Since the number of parameters growssomewhat exponentially with n and linearly with thesize of the training corpus, the resulting models canbe unwieldy even for relatively small corpora.2.1 Scaling StrategiesVarious strategies have been proposed to scale LMsto larger corpora and higher-order dependencies.Model-based techniques seek to parameterize themodel more efficiently (e.g.
latent variable models,neural networks) or to reduce the model size directlyby pruning uninformative parameters, e.g.
(Stolcke,1998), (Goodman and Gao, 2000).
Representation-based techniques attempt to reduce space require-ments by representing the model more efficiently orin a form that scales more readily, e.g.
(Emami et al,2007), (Brants et al, 2007), (Church et al, 2007).2.2 Lossy Randomized EncodingsA fundamental result in information theory (Carteret al, 1978) states that a random set of objects can-not be stored using constant space per object as theuniverse from which the objects are drawn growsin size: the space required to uniquely identify anobject increases as the set of possible objects fromwhich it must be distinguished grows.
In languagemodeling the universe under consideration is theset of all possible n-grams of length n for givenvocabulary.
Although n-grams observed in natu-ral language corpora are not randomly distributedwithin this universe no lossless data structure that weare aware of can circumvent this space-dependencyon both the n-gram order and the vocabulary size.Hence as the training corpus and vocabulary grow, amodel will require more space per parameter.However, if we are willing to accept that occa-sionally our model will be unable to distinguish be-tween distinct n-grams, then it is possible to storeeach parameter in constant space independent ofboth n and the vocabulary size (Carter et al, 1978),(Talbot and Osborne, 2007a).
The space required insuch a lossy encoding depends only on the range ofvalues associated with the n-grams and the desirederror rate, i.e.
the probability with which two dis-tinct n-grams are assigned the same fingerprint.2.3 Previous Randomized LMsRecent work (Talbot and Osborne, 2007b) has usedlossy encodings based on Bloom filters (Bloom,1970) to represent logarithmically quantized cor-pus statistics for language modeling.
While the ap-proach results in significant space savings, workingwith corpus statistics, rather than n-gram probabil-ities directly, is computationally less efficient (par-ticularly in a distributed setting) and introduces adependency on the smoothing scheme used.
It alsomakes it difficult to leverage existing model reduc-tion strategies such as entropy-based pruning thatare applied to final parameter estimates.In the next section we describe our randomizedLM scheme based on perfect hash functions.
Thisscheme can be used to encode any standard n-grammodel which may first be processed using any con-ventional model reduction technique.3 Perfect Hash-based Language ModelsOur randomized LM is based on the Bloomier filter(Chazelle et al, 2004).
We assume the n-grams andtheir associated parameter values have been precom-puted and stored on disk.
We then encode the modelin an array such that each n-gram?s value can be re-trieved.
Storage for this array is the model?s onlysignificant space requirement once constructed.1The model uses randomization to map n-gramsto fingerprints and to generate a perfect hash func-tion that associates n-grams with their values.
Themodel can erroneously return a value for an n-gramthat was never actually stored, but will always returnthe correct value for an n-gram that is in the model.We will describe the randomized algorithm used toencode n-gram parameters in the model, analyze theprobability of a false positive, and explain how weconstruct and query the model in practice.1Note that we do not store the n-grams explicitly and there-fore that the model?s parameter set cannot easily be enumerated.5063.1 N -gram FingerprintsWe wish to encode a set of n-gram/value pairsS = {(x1, v(x1)), (x2, v(x2)), .
.
.
, (xN , v(xN ))}using an array A of size M and a perfect hash func-tion.
Each n-gram xi is drawn from some set of pos-sible n-grams U and its associated value v(xi) froma corresponding set of possible values V .We do not store the n-grams and their proba-bilities directly but rather encode a fingerprint ofeach n-gram f(xi) together with its associated valuev(xi) in such a way that the value can be retrievedwhen the model is queried with the n-gram xi.A fingerprint hash function f : U ?
[0, B ?
1]maps n-grams to integers between 0 and B ?
1.2The array A in which we encode n-gram/value pairshas addresses of size dlog2 Be hence B will deter-mine the amount of space used per n-gram.
Thereis a trade-off between space and error rate since thelarger B is, the lower the probability of a false pos-itive.
This is analyzed in detail below.
For now weassume only that B is at least as large as the rangeof values stored in the model, i.e.
B ?
|V|.3.2 Composite Perfect Hash FunctionsThe function used to associate n-grams with theirvalues (Eq.
(1)) combines a composite perfect hashfunction (Majewski et al, 1996) with the finger-print function.
An example is shown in Fig.
1.The composite hash function is made up of k in-dependent hash functions h1, h2, .
.
.
, hk where eachhi : U ?
[0,M ?
1] maps n-grams to locations inthe array A.
The lookup function is then defined asg : U ?
[0, B ?
1] by3g(xi) = f(xi)?
(k?i=1A[hi(xi)])(1)where f(xi) is the fingerprint of n-gram xi andA[hi(xi)] is the value stored in location hi(xi) of thearray A. Eq.
(1) is evaluated to retrieve an n-gram?sparameter during decoding.
To encode our modelcorrectly we must ensure that g(xi) = v(xi) for alln-grams in our set S. Generating A to encode this2The analysis assumes that all hash functions are random.3We use ?
to denote the exclusive bitwise OR operator.Figure 1: Encoding an n-gram?s value in the array.function for a given set of n-grams is a significantchallenge described in the following sections.3.3 Encoding n-grams in the modelAll addresses in A are initialized to zero.
The proce-dure we use to ensure g(xi) = v(xi) for all xi ?
Supdates a single, unique location in A for each n-gram xi.
This location is chosen from among the klocations given by hj(xi), j ?
[k].
Since the com-posite function g(xi) depends on the values stored atall k locations A[h1(xi)], A[h2(xi)], .
.
.
, A[hk(xi)]in A, we must also ensure that once an n-gram xihas been encoded in the model, these k locationsare not subsequently changed since this would inval-idate the encoding; however, n-grams encoded latermay reference earlier entries and therefore locationsin A can effectively be ?shared?
among parameters.In the following section we describe a randomizedalgorithm to find a suitable order in which to entern-grams in the model and, for each n-gram xi, de-termine which of the k hash functions, say hj , canbe used to update A without invalidating previousentries.
Given this ordering of the n-grams and thechoice of hash function hj for each xi ?
S, it is clearthat the following update rule will encode xi in thearray A so that g(xi) will return v(xi) (cf.
Eq.
(1))A[hj(xi)] = v(xi)?
f(xi)?k?i=1?i6=jA[hi(xi)].
(2)3.4 Finding an Ordered MatchingWe now describe an algorithm (Algorithm 1; (Ma-jewski et al, 1996)) that selects one of the k hash507functions hj , j ?
[k] for each n-gram xi ?
S andan order in which to apply the update rule Eq.
(2) sothat g(xi) maps xi to v(xi) for all n-grams in S.This problem is equivalent to finding an orderedmatching in a bipartite graph whose LHS nodes cor-respond to n-grams in S and RHS nodes correspondto locations in A.
The graph initially contains edgesfrom each n-gram to each of the k locations in Agiven by h1(xi), h2(xi), .
.
.
, hk(xi) (see Fig.
(2)).The algorithm uses the fact that any RHS node thathas degree one (i.e.
a single edge) can be safelymatched with its associated LHS node since no re-maining LHS nodes can be dependent on it.We first create the graph using the k hash func-tions hj , j ?
[k] and store a list (degree one)of those RHS nodes (locations) with degree one.The algorithm proceeds by removing nodes fromdegree one in turn, pairing each RHS node withthe unique LHS node to which it is connected.
Wethen remove both nodes from the graph and push thepair (xi, hj(xi)) onto a stack (matched).
We alsoremove any other edges from the matched LHS nodeand add any RHS nodes that now have degree oneto degree one.
The algorithm succeeds if, whilethere are still n-grams left to match, degree oneis never empty.
We then encode n-grams in the ordergiven by the stack (i.e., first-in-last-out).Since we remove each location in A (RHS node)from the graph as it is matched to an n-gram (LHSnode), each location will be associated with at mostone n-gram for updating.
Moreover, since we matchan n-gram to a location only once the location hasdegree one, we are guaranteed that any other n-grams that depend on this location are already onthe stack and will therefore only be encoded oncewe have updated this location.
Hence dependenciesin g are respected and g(xi) = v(xi) will remaintrue following the update in Eq.
(2) for each xi ?
S.3.5 Choosing Random Hash FunctionsThe algorithm described above is not guaranteed tosucceed.
Its success depends on the size of the arrayM , the number of n-grams stored |S| and the choiceof random hash functions hj , j ?
[k].
Clearly werequire M ?
|S|; in fact, an argument from Majew-ski et al (1996) implies that if M ?
1.23|S| andk = 3, the algorithm succeeds with high probabil-Figure 2: The ordered matching algorithm: matched =[(a, 1), (b, 2), (d, 4), (c, 5)]ity.
We use 2-universal hash functions (L. Carterand M. Wegman, 1979) defined for a range of sizeM via a prime P ?
M and two random numbers1 ?
aj ?
P and 0 ?
bj ?
P for j ?
[k] ashj(x) ?
ajx + bj mod Ptaken modulo M .
We generate a set of k hashfunctions by sampling k pairs of random numbers(aj , bj), j ?
[k].
If the algorithm does not finda matching with the current set of hash functions,we re-sample these parameters and re-start the algo-rithm.
Since the probability of failure on a singleattempt is low when M ?
1.23|S|, the probabilityof failing multiple times is very small.3.6 Querying the Model and False PositivesThe construction we have described above ensuresthat for any n-gram xi ?
S we have g(xi) = v(xi),i.e., we retrieve the correct value.
To retrieve a valuegiven an n-gram xi we simply compute the finger-print f(xi), the hash functions hj(xi), j ?
[k] andthen return g(xi) using Eq.
(1).
Note that unlike theconstructions in (Talbot and Osborne, 2007b) and(Church et al, 2007) no errors are possible for n-grams stored in the model.
Hence we will not makeerrors for common n-grams that are typically in S.508Algorithm 1 Ordered MatchingInput : Set of n-grams S; k hash functions hj , j ?
[k];number of available locations M .Output : Ordered matching matched or FAIL.matched ?
[ ]for all i ?
[0,M ?
1] dor2li ?
?end forfor all xi ?
S dol2ri ?
?for all j ?
[k] dol2ri ?
l2ri ?
hj(xi)r2lhj(xi) ?
r2lhj(xi) ?
xiend forend fordegree one ?
{i ?
[0,M ?
1] | |r2li| = 1}while |degree one| ?
1 dorhs ?
POP degree onelhs ?
POP r2lrhsPUSH (lhs, rhs) onto matchedfor all rhs?
?
l2rlhs doPOP r2lrhs?if |r2lrhs?
| = 1 thendegree one ?
degree one ?
rhs?end ifend forend whileif |matched| = |S| thenreturn matchedelsereturn FAILend ifOn the other hand, querying the model with an n-gram that was not stored, i.e.
with xi ?
U \ S wemay erroneously return a value v ?
V .Since the fingerprint f(xi) is assumed to be dis-tributed uniformly at random (u.a.r.)
in [0, B ?
1],g(xi) is also u.a.r.
in [0, B?1] for xi ?
U\S .
Hencewith |V| values stored in the model, the probabilitythat xi ?
U \ S is assigned a value in v ?
V isPr{g(xi) ?
V|xi ?
U \ S} = |V|/B.We refer to this event as a false positive.
If V is fixed,we can obtain a false positive rate  by setting B asB ?
|V|/.For example, if |V| is 128 then taking B = 1024gives an error rate of  = 128/1024 = 0.125 witheach entry inA using dlog2 1024e = 10 bits.
ClearlyB must be at least |V| in order to distinguish eachvalue.
We refer to the additional bits allocated toeach location (i.e.
dlog2 Be ?
log2 |V| or 3 in ourexample) as error bits in our experiments below.3.7 Constructing the Full ModelWhen encoding a large set of n-gram/value pairs S,Algorithm 1 will only be practical if the raw dataand graph can be held in memory as the perfect hashfunction is generated.
This makes it difficult to en-code an extremely large set S into a single array A.The solution we adopt is to split S into t smallersets S?i, i ?
[t] that are arranged in lexicographic or-der.4 We can then encode each subset in a separatearray A?i, i ?
[t] in turn in memory.
Querying eachof these arrays for each n-gram requested would beinefficient and inflate the error rate since a false posi-tive could occur on each individual array.
Instead westore an index of the final n-gram encoded in eacharray and given a request for an n-gram?s value, per-form a binary search for the appropriate array.3.8 Sanity ChecksOur models are consistent in the following sense(w1, w2, .
.
.
, wn) ?
S =?
(w2, .
.
.
, wn) ?
S.Hence we can infer that an n-gram can not bepresent in the model, if the n?
1-gram consisting ofthe final n ?
1 words has already tested false.
Fol-lowing (Talbot and Osborne, 2007a) we can avoidunnecessary false positives by not querying for thelonger n-gram in such cases.Backoff smoothing algorithms typically requestthe longest n-gram supported by the model first, re-questing shorter n-grams only if this is not found.
Inour case, however, if a query is issued for the 5-gram(w1, w2, w3, w4, w5) when only the unigram (w5) ispresent in the model, the probability of a false posi-tive using such a backoff procedure would not be  asstated above, but rather the probability that we fail toavoid an error on any of the four queries performedprior to requesting the unigram, i.e.
1?
(1?)4 ?
4.We therefore query the model first with the unigramworking up to the full n-gram requested by the de-coder only if the preceding queries test positive.
Theprobability of returning a false positive for any n-gram requested by the decoder (but not in the model)will then be at most .4In our system we use subsets of 5 million n-grams whichcan easily be encoded using less than 2GB of working space.5094 Experimental Set-up4.1 Distributed LM FrameworkWe deploy the randomized LM in a distributedframework which allows it to scale more easilyby distributing it across multiple language modelservers.
We encode the model stored on each lan-guagage model server using the randomized scheme.The proposed randomized LM can encode param-eters estimated using any smoothing scheme (e.g.Kneser-Ney, Katz etc.).
Here we choose to workwith stupid backoff smoothing (Brants et al, 2007)since this is significantly more efficient to train anddeploy in a distributed framework than a context-dependent smoothing scheme such as Kneser-Ney.Previous work (Brants et al, 2007) has shown it tobe appropriate to large-scale language modeling.4.2 LM Data SetsThe language model is trained on four data sets:target: The English side of Arabic-English paralleldata provided by LDC (132 million tokens).gigaword: The English Gigaword dataset providedby LDC (3.7 billion tokens).webnews: Data collected over several years, up toJanuary 2006 (34 billion tokens).web: The Web 1T 5-gram Version 1 corpus providedby LDC (1 trillion tokens).5An initial experiment will use the Web 1T 5-gramcorpus only; all other experiments will use a log-linear combination of models trained on each cor-pus.
The combined model is pre-compiled withweights trained on development data by our system.4.3 Machine TranslationThe SMT system used is based on the frameworkproposed in (Och and Ney, 2004) where translationis treated as the following optimization probleme?
= argmaxeM?i=1?i?i(e, f).
(3)Here f is the source sentence that we wish to trans-late, e is a translation in the target language, ?i, i ?
[M ] are feature functions and ?i, i ?
[M ] areweights.
(Some features may not depend on f .
)5N -grams with count < 40 are not included in this data set.Full Set Entropy-Pruned# 1-grams 13,588,391 13,588,391# 2-grams 314,843,401 184,541,402# 3-grams 977,069,902 439,430,328# 4-grams 1,313,818,354 407,613,274# 5-grams 1,176,470,663 238,348,867Total 3,795,790,711 1,283,522,262Table 1: Num.
of n-grams in the Web 1T 5-gram corpus.5 ExperimentsThis section describes three sets of experiments:first, we encode the Web 1T 5-gram corpus as a ran-domized language model and compare the result-ing size with other representations; then we mea-sure false positive rates when requesting n-gramsfor a held-out data set; finally we compare transla-tion quality when using conventional (lossless) lan-guages models and our randomized language model.Note that the standard practice of measuring per-plexity is not meaningful here since (1) for efficientcomputation, the language model is not normalized;and (2) even if this were not the case, quantizationand false positives would render it unnormalized.5.1 Encoding the Web 1T 5-gram corpusWe build a language model from the Web 1T 5-gramcorpus.
Parameters, corresponding to negative loga-rithms of relative frequencies, are quantized to 8-bitsusing a uniform quantizer.
More sophisticated quan-tizers (e.g.
(S. Lloyd, 1982)) may yield better resultsbut are beyond the scope of this paper.Table 1 provides some statistics about the corpus.We first encode the full set of n-grams, and then aversion that is reduced to approx.
1/3 of its originalsize using entropy pruning (Stolcke, 1998).Table 2 shows the total space and number of bytesrequired per n-gram to encode the model under dif-ferent schemes: ?LDC gzip?d?
is the size of the filesas delivered by LDC; ?Trie?
uses a compact trie rep-resentation (e.g., (Clarkson et al, 1997; Church etal., 2007)) with 3 byte word ids, 1 byte values, and 3byte indices; ?Block encoding?
is the encoding usedin (Brants et al, 2007); and ?randomized?
uses ournovel randomized scheme with 12 error bits.
Thelatter requires around 60% of the space of the nextbest representation and less than half of the com-510size (GB) bytes/n-gramFull SetLDC gzip?d 24.68 6.98Trie 21.46 6.07Block Encoding 18.00 5.14Randomized 10.87 3.08Entropy PrunedTrie 7.70 6.44Block Encoding 6.20 5.08Randomized 3.68 3.08Table 2: Web 1T 5-gram language model sizes with dif-ferent encodings.
?Randomized?
uses 12 error bits.monly used trie encoding.
Our method is the onlyone to use the same amount of space per parameterfor both full and entropy-pruned models.5.2 False Positive RatesAll n-grams explicitly inserted into our randomizedlanguage model are retrieved without error; how-ever, n-grams not stored may be incorrectly assigneda value resulting in a false positive.
Section (3) an-alyzed the theoretical error rate; here, we measureerror rates in practice when retrieving n-grams forapprox.
11 million tokens of previously unseen text(news articles published after the training data hadbeen collected).
We measure this separately for alln-grams of order 2 to 5 from the same text.The language model is trained on the four datasources listed above and contains 24 billion n-grams.
With 8-bit parameter values, the modelrequires 55.2/69.0/82.7 GB storage when using8/12/16 error bits respectively (this corresponds to2.46/3.08/3.69 bytes/n-gram).Using such a large language model results in alarge fraction of known n-grams in new text.
Table3 shows, e.g., that almost half of all 5-grams fromthe new text were seen in the training data.Column (1) in Table 4 shows the number of falsepositives that occurred for this test data.
Column(2) shows this as a fraction of the number of unseenn-grams in the data.
This number should be close to2?b where b is the number of error bits (i.e.
0.003906for 8 bits and 0.000244 for 12 bits).
The error ratesfor bigrams are close to their expected values.
Thenumbers are much lower for higher n-gram ordersdue to the use of sanity checks (see Section 3.8).total seen unseen2gms 11,093,093 98.98% 1.02%3gms 10,652,693 91.08% 8.92%4gms 10,212,293 68.39% 31.61%5gms 9,781,777 45.51% 54.49%Table 3: Number of n-grams in test set and percentagesof n-grams that were seen/unseen in the training data.
(1) (2) (3)false pos.
false posunseenfalse postotal8 error bits2gms 376 0.003339 0.0000343gms 2839 0.002988 0.0002674gms 6659 0.002063 0.0006525gms 6356 0.001192 0.000650total 16230 0.001687 0.00038812 error bits2gms 25 0.000222 0.0000023gms 182 0.000192 0.0000174gms 416 0.000129 0.0000415gms 407 0.000076 0.000042total 1030 0.000107 0.000025Table 4: False positive rates with 8 and 12 error bits.The overall fraction of n-grams requested forwhich an error occurs is of most interest in applica-tions.
This is shown in Column (3) and is around afactor of 4 smaller than the values in Column (2).
Onaverage, we expect to see 1 error in around 2,500 re-quests when using 8 error bits, and 1 error in 40,000requests with 12 error bits (see ?total?
row).5.3 Machine TranslationWe run an improved version of our 2006 NIST MTEvaluation entry for the Arabic-English ?Unlimited?data track.6 The language model is the same one asin the previous section.Table 5 shows baseline translation BLEU scoresfor a lossless (non-randomized) language modelwith parameter values quantized into 5 to 8 bits.
Weuse MT04 data for system development, with MT05data and MT06 (?NIST?
subset) data for blind test-ing.
As expected, results improve when using morebits.
There seems to be little benefit in going beyond6See http://www.nist.gov/speech/tests/mt/2006/doc/511dev test testbits MT04 MT05 MT065 0.5237 0.5608 0.46366 0.5280 0.5671 0.46497 0.5299 0.5691 0.46728 0.5304 0.5697 0.4663Table 5: Baseline BLEU scores with lossless n-grammodel and different quantization levels (bits).0.5540.5560.5580.560.5620.5640.5660.5680.578  9  10  11  12  13  14  15  16MT05BLEUNumber of Error Bits8 bit values7 bit values6 bit values5 bit valuesFigure 3: BLEU scores on the MT05 data set.8 bits.
Overall, our baseline results compare favor-ably to those reported on the NIST MT06 web site.We now replace the language model with a ran-domized version.
Fig.
3 shows BLEU scores for theMT05 evaluation set with parameter values quan-tized into 5 to 8 bits and 8 to 16 additional ?er-ror?
bits.
Figure 4 shows a similar graph for MT06data.
We again see improvements as quantizationuses more bits.
There is a large drop in performancewhen reducing the number of error bits from 10 to8, while increasing it beyond 12 bits offers almostno further gains with scores that are almost identi-cal to the lossless model.
Using 8-bit quantizationand 12 error bits results in an overall requirement of(8+12)?1.23 = 24.6 bits = 3.08 bytes per n-gram.All runs use the sanity checks described in Sec-tion 3.8.
Without sanity checks, scores drop, e.g.
by0.002 for 8-bit quantization and 12 error bits.Randomization and entropy pruning can be com-bined to achieve further space savings with minimalloss in quality as shown in Table (6).
The BLEUscore drops by between 0.0007 to 0.0018 while the0.4540.4560.4580.460.4620.4640.4660.4688  9  10  11  12  13  14  15  16MT06(NIST) BLEUNumber of Error Bits8 bit values7 bit values6 bit values5 bit valuesFigure 4: BLEU scores on MT06 data (?NIST?
subset).size dev test testLM GB MT04 MT05 MT06unpruned block 116 0.5304 0.5697 0.4663unpruned rand 69 0.5299 0.5692 0.4659pruned block 42 0.5294 0.5683 0.4665pruned rand 27 0.5289 0.5679 0.4656Table 6: Combining randomization and entropy pruning.All models use 8-bit values; ?rand?
uses 12 error bits.model is reduced to approx.
1/4 of its original size.6 ConclusionsWe have presented a novel randomized languagemodel based on perfect hashing.
It can associatearbitrary parameter types with n-grams.
Values ex-plicitly inserted into the model are retrieved withouterror; false positives may occur but are controlledby the number of bits used per n-gram.
The amountof storage needed is independent of the size of thevocabulary and the n-gram order.
Lookup is veryefficient: the values of 3 cells in a large array arecombined with the fingerprint of an n-gram.Experiments have shown that this randomizedlanguage model can be combined with entropy prun-ing to achieve further memory reductions; that errorrates occurring in practice are much lower than thosepredicted by theoretical analysis due to the use ofruntime sanity checks; and that the same translationquality as a lossless language model representationcan be achieved when using 12 ?error?
bits, resultingin approx.
3 bytes per n-gram (this includes one byteto store parameter values).512ReferencesB.
Bloom.
1970.
Space/time tradeoffs in hash codingwith allowable errors.
CACM, 13:422?426.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of EMNLP-CoNLL 2007, Prague.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Peter Brown, Stephen Della Pietra, Vincent Della Pietra,and Robert Mercer.
1993.
The mathematics of ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.Larry Carter, Robert W. Floyd, John Gill, GeorgeMarkowsky, and Mark N. Wegman.
1978.
Exact andapproximate membership testers.
In STOC, pages 59?65.L.
Carter and M. Wegman.
1979.
Universal classes ofhash functions.
Journal of Computer and System Sci-ence, 18:143?154.Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, andAyellet Tal.
2004.
The Bloomier Filter: an efficientdata structure for static support lookup tables.
In Proc.15th ACM-SIAM Symposium on Discrete Algoritms,pages 30?39.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with golombcoding.
In Proceedings of EMNLP-CoNLL 2007,Prague, Czech Republic, June.P.
Clarkson and R. Rosenfeld.
1997.
Statistical languagemodeling using the CMU-Cambridge toolkit.
In Pro-ceedings of EUROSPEECH, vol.
1, pages 2707?2710,Rhodes, Greece.Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.2007.
Large-scale distributed language modeling.
InProceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP)2007, Hawaii, USA.J.
Goodman and J. Gao.
2000.
Language model size re-duction by pruning and clustering.
In ICSLP?00, Bei-jing, China.S.
Lloyd.
1982.
Least squares quantization in PCM.IEEE Transactions on Information Theory, 28(2):129?137.B.S.
Majewski, N.C. Wormald, G. Havas, and Z.J.
Czech.1996.
A family of perfect hashing methods.
BritishComputer Journal, 39(6):547?554.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In Proc.
DARPA Broadcast NewsTranscription and Understanding Workshop, pages270?274.D.
Talbot andM.
Osborne.
2007a.
Randomised languagemodelling for statistical machine translation.
In 45thAnnual Meeting of the ACL 2007, Prague.D.
Talbot and M. Osborne.
2007b.
Smoothed Bloomfilter language models: Tera-scale LMs on the cheap.In EMNLP/CoNLL 2007, Prague.513
