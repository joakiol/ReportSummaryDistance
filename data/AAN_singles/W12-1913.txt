NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 96?99,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTurning the pipeline into a loop: Iterated unsupervised dependency parsingand PoS inductionChristos Christodoulopoulos?, Sharon Goldwater?, Mark Steedman?School of InformaticsUniversity of Edinburgh?christos.c@ed.ac.uk ?
{steedman,sgwater}@inf.ed.ac.uk1 MotivationMost unsupervised dependency systems rely ongold-standard Part-of-Speech (PoS) tags, either di-rectly, using the PoS tags instead of words, or indi-rectly in the back-off mechanism of fully lexicalizedmodels (Headden et al, 2009).It has been shown in supervised systems that us-ing a hierarchical syntactic structure model can pro-duce competitive sequence models; in other wordsthat a parser can be a good tagger (Li et al, 2011;Auli and Lopez, 2011; Cohen et al, 2011).
Thisis unsurprising, as the parser uses a rich set of hi-erarchical features that enable it to look at a lesslocalized environment than a PoS tagger which inmost cases relies solely on local contextual features.However this interaction has not been shown for theunsupervised setting.
To our knowledge, this workis the first to show that using dependencies for unsu-pervised PoS induction is indeed useful.2 Iterated learningAlthough most unsupervised systems depend ongold-standard PoS information, they can also beused in a fully unsupervised pipeline.
One reason fordoing so is to use dependency parsing as an extrinsicevaluation for unsupervised PoS induction (Headdenet al, 2008).
As discussed in that paper (and also byKlein and Manning (2004)) the quality of the de-pendencies drops with the use of induced tags.
Oneway of producing better PoS tags is to use the depen-dency parser?s output to influence the PoS inducer,thus turning the pipeline into a loop.The main difficulty of this approach is to finda way of incorporating dependency informationinto a PoS induction system.
In previous workBMMMDMVBMMMDMVBMMMGen.
0 Gen. 1 Gen. 2Figure 1: The iterated learning paradigm for induc-ing both PoS tags and dependencies.
(Christodoulopoulos et al, 2011) we have describedBMMM: a PoS induction system that makes it iseasy to incorporate multiple features either at thetype or token level.
For the dependency inductionsystem we chose the DMV model of Klein and Man-ning (2004) because of its simplicity and its popular-ity.
Both systems are described briefly in section 3.Using these two systems we performed an iter-ated learning experiment.
The term is borrowedfrom the language evolution literature meaning ?theprocess by which the output of one individual?slearning becomes the input to other individuals?learning?
(Smith et al, 2003).
Here we treat thetwo systems as the individuals1 that influence eachother in successive generations starting from a runof the original BMMM system without dependencyinformation (fig.
1).
We start with a run of the basicBMMM system using just context and morphologyfeatures (generation 0) and use the output to train theDMV.
To complete the first generation, we then usethe induced dependencies as features (as describedin section 4) for a new run of the BMMM system.As there is no single objective function, this setup1This is not directly analogous to the language evolution no-tion of iterated learning; here instead of a single type of indi-vidual we have two separate systems that learn/model differentrepresentations.96does not guarantee that either the quality of PoS tagsor the dependencies will improve after each genera-tion.
However, in practice this iterated learning ap-proach works well (as we discuss in section 4).3 Component models3.1 DMV modelThe basic DMV model (Klein and Manning, 2004)generates dependency trees based on three decisions(represented by three probability distributions) for agiven head node: whether to attach children in theleft or right direction; whether or not to stop attach-ing more children in the specific direction given theadjacency of the child in that direction; and finallywhether to attach a specific child node.
The proba-bility of an entire sentence is the sum of the probabil-ities of all the possible derivations headed by ROOT.The DMV model can be seen as (and is equiva-lent to) a Context Free Grammar (CFG) with only afew rules from head nodes to generated children andtherefore the model parameters can be estimated us-ing the Inside-Outside algorithm (Baker, 1979).3.2 BMMM modelThe Bayesian Multinomial Mixture Model(Christodoulopoulos et al, 2011), illustrated infigure 2, assumes that all tokens of a given wordtype belong to a single syntactic class, and eachtype is associated with a number of features (e.g.,morphological or contextual features), which formthe observed variables.
The generative process firstchooses a hidden class z for each word type and thenchooses values for each of the observed features ofthat word type, conditioned on the class.
Both thedistribution over classes ?
and the distributions overeach kind of feature ?
(t) are multinomials drawnfrom Dirichlet priors ?
and ?
(t) respectively.
Amain advantage of this model is its ability to easilyincorporate features either at the type or tokenlevel; as in Christodoulopoulos et al (2011) weassume a single type-level feature m (morphology,drawn from ?
(m)) and several token-level featuresf(1).
.
.
f(T ) (e.g., left and right context words and,in our extension, dependency features).Inference in the model is performed using a col-lapsed Gibbs sampler, integrating out the model pa-rameters and sampling the class label zj for each??zf(1)?(1)?(1).
.
.
.
.
.
.
.
.f(T )?
(T )?
(T )m?(m)?
(m)MnjnjZZZFigure 2: The BMMM with T kinds of token-levelfeatures (f (t) variables) and a single kind of type-level feature (morphology, m).
M is the total num-ber of word types, Z the number of classes, and njthe number of tokens of type j.word type j from the following posterior distribu-tion:P (zj | z?j , f , ?, ?)?
P (zj | z?j , ?, ?
)P (fj | f?j , z, ?, ?)
(1)where the first factor P (zj) is the prior distribu-tion over classes (the mixing weights) and the sec-ond (likelihood) factor P (fj) is the probability givenclass zj of all the features associated with word typej.
Since the different kinds of features are assumedto be independent, the likelihood can be rewritten as:P (fj | f?j , z, ?, ?)
= P (f(m)j | f(m)?j , z, ?, ?
)?T?t=1P (f (t)j | f(t)?j , z, ?)
(2)and, as explained in Christodoulopoulos et al(2011), the joint probability of all the token levelfeatures of kind t for word type j is computed as:P (f (t)j | f(t)?j , zj = z, z?j , ?
)=?K(t)k=1?njk?1i=0 (njk,z + i+ ?
)?nj?1i=0 (n?,z + i+ F?
)(3)9730354045505560650 1 2 3 4 5 6 7 8 9 10BMMM M-1 BMMM VM DMV Dir DMV Undir(a) Using only directed dependencies as features30354045505560650 1 2 3 4 5 6 7 8 9 10BMMM M-1 BMMM VM DMV Dir DMV Undir(b) Using directed and undirected dependencies as featuresFigure 3: Developmental results on WSJ10.
The performance of the PoS inducer is shown in terms ofmany-to-1 accuracy (BMMM M1) and V-Measure accuracy (BMMM VM) and the performance of thedependency inducer is shown using directed and undirected dependency accuracy (DMV Dir and DMVUndir respectively).where K(t) is the dimensionality of ?
(t) and njk isthe number of instances of feature k in word type j.4 Experimental designBecause the different kinds of features are assumedto be independent in the BMMM, it is easy to addmore features into the model; this simply increasesthe number of factors in equation 2.
To incorpo-rate dependency information, we added a feature forword-word dependencies.
In the model, this meansthat for a word type j with nj tokens, we observe njdependency features (each being the head of one to-ken of j).
Like all other features, these are assumedto be drawn from a class-specific multinomial ?
(d)zwith a Dirichlet prior ?
(d).Using lexicalized head dependencies introducessparsity issues in much the same way contextual in-formation does.
In the case of context words, theBMMM and most vector-based clustering systemsuse a fixed number of most frequent words as fea-tures; however in the case of dependencies we usethe induced PoS tags of the previous generation asgrouping labels: we aggregate the head dependencycounts of words that have the same PoS tag, so thedimension of ?
(d)z is just the number of PoS tags.The dependency features are used in tandem withthe features used in the original BMMM system,namely the 100 most frequent context words (?1context window), the suffixes extracted from theMorfessor system (Creutz and Lagus, 2005) andthe extended morphology features of Haghighi andKlein (2006).For designing the iterated learning experiments,we used the 10-word version of the WSJ corpus(WSJ10) as development data and ran the iterativelearning process for 10 generations.
To evaluate thequality of the induced PoS tags we used the many-to-1 (M1) and V-Measure (VM) metrics and for theinduced dependencies we used directed and undi-rected accuracy.Figure 3a presents the developmental result of theiterated learning experiments on WSJ10 where onlydirected dependencies where used as features.
Wecan see that although there was some improvementin the PoS induction score after the first generation,the rest of the metrics show no significant improve-ment throughout the experiment.When we used undirected dependencies as fea-tures (figure 3b) the improvement over iterationswas substantial: nearly 8.5% increase in M1 and1.3% in VM after only 5 iterations.
We can also seethat the results of the DMV parser are improving aswell: 7% increase in directed and 3.8% in undirectedaccuracy.
This is to be expected, since as Headdenet al (2008) show, there is a (weak) correlation be-tween the intrinsic scores of a PoS inducer and the98performance of an unsupervised dependency parsertrained on the inducer?s output.Using the same development set we selected theremaining system parameters; for the BMMM wefixed the number of induced classes to the numberof gold-standard PoS tags for each language andused 500 sampling iterations with annealing.
Forthe DMV model we used 20 EM iterations.
Finallywe used observed that after 5 generations the rate ofimprovement seems to level, so for the rest of thelanguages we use only 5 learning iterations.ReferencesMichael Auli and Adam Lopez.
2011.
A comparison ofloopy belief propagation and dual decomposition forintegrated CCG supertagging and parsing.
In Proceed-ings of ACL-HLT, pages 470?480.James K. Baker.
1979.
Trainable grammars for speechrecognition.
The Journal of the Acoustical Society ofAmerica, 65(S1):S132, June.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2011.
A Bayesian mixture modelfor pos induction using multiple features.
In Proceed-ings of EMNLP, pages 638?647, Edinburgh, Scotland,UK., July.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP,pages 50?61.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural language fromunannotated text.
In In Proceedings of AKRR, vol-ume 5, pages 106?113.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofNAACL, pages 320?327.William P. Headden, David McClosky, and Eugene Char-niak.
2008.
Evaluating unsupervised part-of-speechtagging for grammar induction.
In Proceedings ofCOLING, pages 329?336.William P. Headden, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of NAACL, pages 101?109.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In Proceedings of ACL.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-liang Chen, and Haizhou Li.
2011.
Joint models forchinese POS tagging and dependency parsing.
In Pro-ceedings of EMNLP, pages 1180?1191.Kenny Smith, Simon Kirby, and Henry Brighton.
2003.Iterated learning: a framework for the emergence oflanguage.
Artif.
Life, 9(4):371?386.99
