Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514?522,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSentiment Summarization: Evaluating and Learning User PreferencesKevin LermanColumbia UniversityNew York, NYklerman@cs.columbia.eduSasha Blair-GoldensohnGoogle, Inc.New York, NYsasha@google.comRyan McDonaldGoogle, Inc.New York, NYryanmcd@google.comAbstractWe present the results of a large-scale,end-to-end human evaluation of varioussentiment summarization models.
Theevaluation shows that users have a strongpreference for summarizers that modelsentiment over non-sentiment baselines,but have no broad overall preference be-tween any of the sentiment-based models.However, an analysis of the human judg-ments suggests that there are identifiablesituations where one summarizer is gener-ally preferred over the others.
We exploitthis fact to build a new summarizer bytraining a ranking SVM model over the setof human preference judgments that werecollected during the evaluation, which re-sults in a 30% relative reduction in errorover the previous best summarizer.1 IntroductionThe growth of the Internet as a commercemedium, and particularly the Web 2.0 phe-nomenon of user-generated content, have resultedin the proliferation of massive numbers of product,service and merchant reviews.
While this meansthat users have plenty of information on which tobase their purchasing decisions, in practice this isoften too much information for a user to absorb.To alleviate this information overload, research onsystems that automatically aggregate and summa-rize opinions have been gaining interest (Hu andLiu, 2004a; Hu and Liu, 2004b; Gamon et al,2005; Popescu and Etzioni, 2005; Carenini et al,2005; Carenini et al, 2006; Zhuang et al, 2006;Blair-Goldensohn et al, 2008).Evaluating these systems has been a challenge,however, due to the number of human judgmentsrequired to draw meaningful conclusions.
Of-ten systems are evaluated piecemeal, selectingpieces that can be evaluated easily and automati-cally (Blair-Goldensohn et al, 2008).
While thistechnique produces meaningful evaluations of theselected components, other components remainuntested, and the overall effectiveness of the entiresystem as a whole remains unknown.
When sys-tems are evaluated end-to-end by human judges,the studies are often small, consisting of only ahandful of judges and data points (Carenini etal., 2006).
Furthermore, automated summariza-tion metrics like ROUGE (Lin and Hovy, 2003)are non-trivial to adapt to this domain as they re-quire human curated outputs.We present the results of a large-scale, end-to-end human evaluation of three sentiment summa-rization models applied to user reviews of con-sumer products.
The evaluation shows that thereis no significant difference in rater preference be-tween any of the sentiment summarizers, but thatraters do prefer sentiment summarizers over non-sentiment baselines.
This indicates that even sim-ple sentiment summarizers provide users utility.An analysis of the rater judgments also indicatesthat there are identifiable situations where one sen-timent summarizer is generally preferred over theothers.
We attempt to learn these preferences bytraining a ranking SVM that exploits the set ofpreference judgments collected during the evalu-ation.
Experiments show that the ranking SVMsummarizer?s cross-validation error decreases byas much as 30% over the previous best model.Human evaluations of text summarization havebeen undertaken in the past.
McKeown et al(2005) presented a task-driven evaluation in thenews domain in order to understand the utility ofdifferent systems.
Also in the news domain, theDocument Understanding Conference1 has run anumber of multi-document and query-driven sum-marization shared-tasks that have used a wide1http://duc.nist.gov/514iPod Shuffle: 4/5 stars?In final analysis the iPod Shuffle is a decent player that offers a sleekcompact form factor an excessively simple user interface and a lowprice?
... ?It?s not good for carrying a lot of music but for a little bit ofmusic you can quickly grab and go with this nice little toy?
... ?Mine camein a nice bright orange color that makes it easy to locate.
?Figure 1: An example summary.range of automatic and human-based evaluationcriteria.
This year, the new Text Analysis Con-ference2 is running a shared-task that contains anopinion component.
The goal of that evaluation isto summarize answers to opinion questions aboutentities mentioned in blogs.Our work most closely resembles the evalua-tions in Carenini et al (2006, 2008).
Carenini etal.
(2006) had raters evaluate extractive and ab-stractive summarization systems.
Mirroring ourresults, they show that both extractive and abstrac-tive summarization outperform a baseline, but thatoverall, humans have no preference between thetwo.
Again mirroring our results, their analysis in-dicates that even though there is no overall differ-ence, there are situations where one system gener-ally outperforms the other.
In particular, Careniniand Cheung (2008) show that an entity?s contro-versiality, e.g., mid-range star rating, is correlatedwith which summary has highest value.The study presented here differs from Careniniet al in many respects: First, our evaluation isover different extractive summarization systems inan attempt to understand what model propertiesare correlated with human preference irrespectiveof presentation; Secondly, our evaluation is on alarger scale including hundreds of judgments byhundreds of raters; Finally, we take a major nextstep and show that it is possible to automaticallylearn significantly improved models by leveragingdata collected in a large-scale evaluation.2 Sentiment SummarizationA standard setting for sentiment summarizationassumes a set of documents D = {d1, .
.
.
, dm}that contain opinions about some entity of interest.The goal of the system is to generate a summary Sof that entity that is representative of the averageopinion and speaks to its important aspects.
Anexample summary is given in figure 1.
For sim-plicity we assume that all opinions in D are aboutthe entity being summarized.
When this assump-tion fails, one can parse opinions at a finer-level2http://www.nist.gov/tac/(Jindal and Liu, 2006; Stoyanov and Cardie, 2008)In this study, we look at an extractive summa-rization setting where S is built by extracting rep-resentative bits of text from the set D, subject topre-specified length constraints.
Specifically, as-sume each document di is segmented into can-didate text excerpts.
For ease of discussion wewill assume all excerpts are sentences, but in prac-tice they can be phrases or multi-sentence groups.Viewed this way, D is a set of candidate sentencesfor our summary, D = {s1, .
.
.
, sn}, and summa-rization becomes the following optimization:argmaxS?DL(S) s.t.
: LENGTH(S) ?
K (1)where L is some score over possible summaries,LENGTH(S) is the length of the summary and Kis the pre-specified length constraint.
The defini-tion of L will be the subject of much of this sec-tion and it is precisely different forms of L thatwill be compared in our evaluation.
The nature ofLENGTH is specific to the particular use case.Solving equation 1 is typically NP-hard, evenunder relatively strong independence assumptionsbetween the sentences selected for the summary(McDonald, 2007).
In cases where solving L isnon-trivial we use an approximate hill climbingtechnique.
First we randomly initialize the sum-mary S to length ?K.
Then we greedily in-sert/delete/swap sentences in and out of the sum-mary to maximize L(S) while maintaining thebound on length.
We run this procedure until nooperation leads to a higher scoring summary.
Inall our experiments convergence was quick, evenwhen employing random restarts.Alternate formulations of sentiment summa-rization are possible, including aspect-based sum-marization (Hu and Liu, 2004a), abstractive sum-marization (Carenini et al, 2006) or related taskssuch as opinion attribution (Choi et al, 2005).
Wechoose a purely extractive formulation as it makesit easier to develop baselines and allows raters tocompare summaries with a simple, consistent pre-sentation format.2.1 DefinitionsBefore delving into the details of the summariza-tion models we must first define some useful func-tions.
The first is the sentiment polarity func-tion that maps a lexical item t, e.g., word or shortphrase, to a real-valued score,LEX-SENT(t) ?
[?1, 1]515The LEX-SENT function maps items with positivepolarity to higher values and items with negativepolarity to lower values.
To build this function weconstructed large sentiment lexicons by seeding asemantic word graph induced from WordNet withpositive and negative examples and then propagat-ing this score out across the graph with a decayingconfidence.
This method is common among sen-timent analysis systems (Hu and Liu, 2004a; Kimand Hovy, 2004; Blair-Goldensohn et al, 2008).In particular, we use the lexicons that were createdand evaluated by Blair-Goldensohn et al (2008).Next we define sentiment intensity,INTENSITY(s) =?t?s|LEX-SENT(t)|which simply measures the magnitude of senti-ment in a sentence.
INTENSITY can be viewed as ameasure of subjectiveness irrespective of polarity.A central function in all our systems is a sen-tences normalized sentiment,SENT(s) =?t?s LEX-SENT(t)?+ INTENSITY(s)This function measures the (signed) ratio of lexicalsentiment to intensity in a sentence.
Sentences thatonly contain lexical items of the same polarity willhave high absolute normalized sentiment, whereassentences with mixed polarity items or no polar-ity items will have a normalized sentiment nearzero.
We include the constant ?
in the denomi-nator so that SENT gives higher absolute scores tosentences containing many strong sentiment itemsof the same polarity over sentences with a smallnumber of weak items of the same polarity.Most sentiment summarizers assume that as in-put, a system is given an overall rating of the en-tity it is attempting to summarize, R ?
[?1, 1],where a higher rating indicates a more favorableopinion.
This rating may be obtained directly fromuser provided information (e.g., star ratings) or au-tomatically derived by averaging the SENT func-tion over all sentences in D. Using R, we can de-fine a mismatch function between the sentiment ofa summary and the known sentiment of the entity,MISMATCH(S) = (R?1|S|?si?SSENT(si))2Summaries with a higher mismatch are thosewhose sentiment disagrees most with R.Another key input many sentiment summarizersassume is a list of salient entity aspects, which arespecific properties of an entity that people tend torate when expressing their opinion.
For example,aspects of a digital camera could include picturequality, battery life, size, color, value, etc.
Find-ing such aspects is a challenging research problemthat has been addressed in a number of ways (Huand Liu, 2004b; Gamon et al, 2005; Carenini etal., 2005; Zhuang et al, 2006; Branavan et al,2008; Blair-Goldensohn et al, 2008; Titov andMcDonald, 2008b; Titov and McDonald, 2008a).We denote the set of aspects for an entity as A andeach aspect as a ?
A.
Furthermore, we assumethat given A it is possible to determine whethersome sentence s ?
D mentions an aspect in A.For our experiments we use a hybrid supervised-unsupervised method for finding aspects as de-scribed and evaluated in Blair-Goldensohn et al(2008).Having defined what an aspect is, we next de-fine a summary diversity function over aspects,DIVERSITY(S) =?a?ACOVERAGE(a)where COVERAGE(a) ?
R is a function thatweights how well the aspect is covered in thesummary and is proportional to the importance ofthe aspect as some aspects are more important tocover than others, e.g., ?picture quality?
versus?strap?
for digital cameras.
The diversity func-tion rewards summaries that cover many importantaspects and plays the redundancy reducing rolethat is common in most extractive summarizationframeworks (Goldstein et al, 2000).2.2 SystemsFor our evaluation we developed three extractivesentiment summarization systems.
Each systemmodels increasingly complex objectives.2.2.1 Sentiment Match (SM)The first system that we look at attempts to ex-tract sentences so that the average sentiment of thesummary is as close as possible to the entity levelsentiment R, which was previously defined in sec-tion 2.1.
In this case L can be simply defined as,L(S) = ?MISMATCH(S)Thus, the model prefers summaries with averagesentiment as close as possible to the average sen-timent across all the reviews.516There is an obvious problem with this model.For entities that have a mediocre rating, i.e., R ?0, the model could prefer a summary that onlycontains sentences with no opinion whatsoever.There are two ways to alleviate this problem.
Thefirst is to include the INTENSITY function into L,L(S) = ?
?
INTENSITY(S)?
?
?
MISMATCH(S)Where the coefficients allow one to trade-off sen-timent intensity versus sentiment mismatch.The second method, and the one we chose basedon initial experiments, was to address the problemat inference time.
This is done by prohibiting thealgorithm from including a given positive or nega-tive sentence in the summary if another more pos-itive/negative sentence is not included.
Thus thesummary is forced to consist of only the most pos-itive and most negative sentences, the exact mixbeing dependent upon the overall star rating.2.2.2 Sentiment Match + Aspect Coverage(SMAC)The SM model extracts sentences for the summarywithout regard to the content of each sentence rel-ative to the others in the summary.
This is in con-trast to standard summarization models that lookto promote sentence diversity in order to cover asmany important topics as possible (Goldstein etal., 2000).
The sentiment match + aspect cov-erage system (SMAC) attempts to model diver-sity by building a summary that trades-off max-imally covering important aspects with matchingthe overall sentiment of the entity.
The model doesthis through the following linear score,L(S) = ?
?
INTENSITY(S)?
?
?
MISMATCH(S)+?
?
DIVERSITY(S)This score function rewards summaries for be-ing highly subjective (INTENSITY), reflecting theoverall product rating (MISMATCH), and coveringa variety of product aspects (DIVERSITY).
The co-efficients were set by inspection.This system has its roots in event-based summa-rization (Filatova and Hatzivassiloglou, 2004) forthe news domain.
In that work an optimizationproblem was developed that attempted to maxi-mize summary informativeness while covering asmany (weighted) sub-events as possible.2.2.3 Sentiment-Aspect Match (SAM)Because the SMAC model only utilizes an entity?soverall sentiment when calculating MISMATCH, itis susceptible to degenerate solutions.
Consider aproduct with aspects A and B, where reviewersoverwhelmingly like A and dislike B, resulting inan overall SENT close to zero.
If the SMAC modelfinds a very negative sentence describing A anda very positive sentence describing B, it will as-sign that summary a high score, as the summaryhas high intensity, has little overall mismatch, andcovers both aspects.
However, in actuality, thesummary is entirely misleading.To address this issue, we constructed thesentiment-aspect match model (SAM), which notonly attempts to cover important aspects, but coverthem with appropriate sentiment.
There are manyways one might design a model to do this, includ-ing linear combinations of functions similar to theSMAC model.
However, we decided to employ aprobabilistic approach as it provided performancebenefits based on development data experiments.Under the SAM model, each sentence is treated asa bag of aspects and their corresponding mentions?sentiments.
For a given sentence s, we define Asas the set of aspects mentioned within it.
For agiven aspect a ?
As, we denote SENT(as) as thesentiment associated with the textual mention of ain s. The probability of a sentence is defined as,p(s) = p(a1, .
.
.
, an, SENT(a1s), .
.
.
, SENT(ans ))which can be re-written as,?a?Asp(a, SENT(as)) =?a?Asp(a)p(SENT(as)|a)if we assume aspect mentions are generated inde-pendently of one another.
Thus we need to esti-mate both p(a) and p(SENT(as)|a).
The probabil-ity of seeing an aspect, p(a), is simply set to themaximum likelihood estimates over the data setD.
Furthermore, we assume that p(SENT(as)|a)is normal about the mean sentiment for the as-pect ?a with a constant standard deviation, ?a.The mean and standard deviation are estimatedstraight-forwardly using the data set D. Note thatthe number of parameters our system must es-timate is very small.
For every possible aspecta ?
A we need three values: p(a), ?a, and ?a.Since |A| is typically small ?
on the order of 5-10?
it is not difficult to estimate these models evenfrom small sets of data.Having constructed this model, one logical ap-proach to summarization would be to select sen-tences for the summary that have highest proba-bility under the model trained on D. We found,517however, that this produced very redundant sum-maries ?
if one aspect is particularly prevalent ina product?s reviews, this approach will select allsentences about that aspect, and discuss nothingelse.
To combat this we developed a technique thatscores the summary as a whole, rather than by in-dividual components.
First, denote SAM(D) as thepreviously described model learned over the set ofentity documents D. Next, denote SAM(S) as anidentical model, but learned over a candidate sum-mary S, i.e., given a summary S, compute p(a),ma, and ?a for all a ?
A using only the sentencesfrom S. We can then measure the difference be-tween these models using KL-divergence:L(S) = ?KL(SAM(D), SAM(S))In our case we have 1 + |A| distributions ?
p(a),and p(?|a) for all a ?
A ?
so we just sum the KL-divergence of each.
The key property of the SAMsystem is that it naturally builds summaries whereimportant aspects are discussed with appropriatesentiment, since it is precisely these aspects thatwill contribute the most to the KL-divergence.
Itis important to note that the short length of a can-didate summary S can make estimates in SAM(S)rather crude.
But we only care about finding the?best?
of a set of crude models, not about findingone that is ?good?
in absolute terms.
Between thefew parameters we must learn and the specific waywe use these models, we generally get models use-ful for our purposes.Alternatively we could have simply incorpo-rated the DIVERSITY measure into the objec-tive function or used an inference algorithm thatspecifically accounts for redundancy, e.g., maxi-mal marginal relevance (Goldstein et al, 2000).However, we found that this solution was wellgrounded and required no tuning of coefficients.Initial experiments indicated that the SAM sys-tem, as described above, frequently returned sen-tences with low intensity when important aspectshad luke-warm sentiment.
To combat this we re-moved low intensity sentences from consideration,which had the effect of encouraging importantluke-warm aspects to mentioned multiple times inorder to balance the overall sentiment.Though the particulars of this model are unique,fundamentally it is closest to the work of Hu andLiu (2004a) and Carenini et al (2006).3 ExperimentsWe evaluated summary performance for reviewsof consumer electronics.
In this setting an entityto be summarized is one particular product, D isa set of user reviews about that product, and R isthe normalized aggregate star ratings left by users.We gathered reviews for 165 electronics productsfrom several online review aggregators.
The prod-ucts covered a variety of electronics, such as MP3players, digital cameras, printers, wireless routers,and video game systems.
Each product had a min-imum of four reviews and up to a maximum ofnearly 3000.
The mean number of reviews perproduct was 148, and the median was 70.
Weran each of our algorithms over the review corpusand generated summaries for each product withK = 650.
All summaries were roughly equallength to avoid length-based rater bias3.
In totalwe ran four experiments for a combined number of1980 rater judgments (plus additional judgmentsduring the development phase of this study).Our initial set of experiments were over thethree opinion-based summarization systems: SM,SMAC, and SAM.
We ran three experiments com-paring SMAC to SM, SAM to SM, and SAM toSMAC.
In each experiment two summaries of thesame product were placed side-by-side in a ran-dom order.
Raters were also shown an overall rat-ing, R, for each product (these ratings are oftenprovided in a form such as ?3.5 of 5 stars?).
Thetwo summaries on either side were shown belowthis information with links to the full text of thereviews for the raters to explore.Raters were asked to express their preferencefor one summary over the other.
For two sum-maries SA and SB they could answer,1.
No preference2.
Strongly preferred SA (or SB)3.
Preferred SA (or SB)4.
Slightly preferred SA (or SB)Raters were free to choose any rating, but werespecifically instructed that their rating should ac-count for a summaries representativeness of theoverall set of reviews.
Raters were also askedto provide a brief comment justifying their rat-ing.
Over 100 raters participated in each study,and each comparison was evaluated by three raterswith no rater making more than five judgments.3In particular our systems each extracted four text ex-cerpts of roughly 160-165 characters.518Comparison (A v B) Agreement (%) No Preference (%) Preferred A (%) Preferred B (%) Mean NumericSM v SMAC 65.4 6.0 52.0 42.0 0.01SAM v SM 69.3 16.8 46.0 37.2 0.01SAM v SMAC?
73.9 11.5 51.6 36.9 0.08SMAC v LT?
64.1 4.1 70.4 25.5 0.24Table 1: Results of side-by-side experiments.
Agreement is the percentage of items for which all ratersagreed on a positive/negative/no-preference rating.
No Preference is the percentage of agreement itemsin which the raters had no preference.
Preferred A/B is the percentage of agreement items in which theraters preferred either A or B respectively.
Mean Numeric is the average of the numeric ratings (convertedfrom discreet preference decisions) indicating on average the raters preferred system A over B on a scaleof -1 to 1.
Positive scores indicate a preference for system A. ?
significant at a 95% confidence intervalfor the mean numeric score.We chose to have raters leave pairwise prefer-ences, rather than evaluate each candidate sum-mary in isolation, because raters can make a pref-erence decisions more quickly than a valuationjudgment, which allowed for collection of moredata points.
Furthermore, there is evidence thatrater agreement is much higher in preference deci-sions than in value judgments (Ariely et al, 2008).Results are shown in the first three rows of ta-ble 1.
The first column of the table indicates theexperiment that was run.
The second column indi-cates the percentage of judgments for which theraters were in agreement.
Agreement here is aweak agreement, where three raters are defined tobe in agreement if they all gave a no preference rat-ing, or if there was a preference rating, but no twopreferences conflicted.
The next three columns in-dicate the percentage of judgments for each pref-erence category, grouped here into three coarse as-signments.
The final column indicates a numericaverage for the experiment.
This was calculatedby converting users ratings to a scale of 1 (stronglypreferred SA) to -1 (strongly preferred SB) at 0.33intervals.
Table 1 shows only results for items inwhich the raters had agreement in order to drawreliable conclusions, though the results change lit-tle when all items are taken into account.Ultimately, the results indicate that none of thesentiment summarizers are strongly preferred overany other.
Only the SAM v SMAC model has adifference that can be considered statistically sig-nificant.
In terms of order we might conclude thatSAM is the most preferred, followed by SM, fol-lowed by SMAC.
However, the slight differencesmake any such conclusions tenuous at best.
Thisleads one to wonder whether raters even requireany complex modeling when summarizing opin-ions.
To test this we took the lowest scoring modeloverall, SMAC, and compared it to a leading textbaseline (LT) that simply selects the first sentencefrom a ranked list of reviews until the length con-straint is violated.
The results are given in the lastrow of 1.
Here there is a clear distinction as raterspreferred SMAC to LT, indicating that they didfind usefulness in systems that modeled aspectsand sentiment.
However, there are still 25.5%of agreement items where the raters did choose asimple leading text baseline.4 AnalysisLooking more closely at the results we observedthat, even though raters did not strongly preferany one sentiment-aware summarizer over anotheroverall, they mostly did express preferences be-tween systems on individual pairs of comparisons.For example, in the SAM vs SM experiment, only16.8% of the comparisons yielded a ?no prefer-ence?
judgment from all three raters ?
by far thehighest percentage of any experiment.
This left83.2% ?slight preference?
or higher judgments.With this in mind we began examining the com-ments left by raters throughout all our experi-ments, including a set of additional experimentsused during development of the systems.
We ob-served several trends: 1) Raters tended to pre-fer summaries with lists, e.g., pros-cons lists; 2)Raters often did not like text without sentiment,hence the dislike of the leading text system wherethere is no guarantee that the first sentence willhave any sentiment; 3) Raters disliked overly gen-eral comments, e.g., ?The product was good?.These statements carry no additional informationover a product?s overall star rating; 4) Raters didrecognize (and strongly disliked) when the overallsentiment of the summary was inconsistent withthe star rating; 5) Raters tended to prefer different519systems depending on what the star rating was.
Inparticular, the SMAC system was generally pre-ferred for products with neutral overall ratings,whereas the SAM system is preferred for productswith ratings at the extremes.
We hypothesize thatSAM?s low performance on neutral rated productsis because the system suffers from the dual imper-atives of selecting high intensity snippets and ofselecting snippets that individually reflect partic-ular sentiment polarities.
When the desired senti-ment polarity is neutral, it is difficult to find a snip-pet with lots of sentiment, whose overall polarityis still neutral, thus SAM may either ignore thataspect or include multiple mentions of that aspectat the expense of others; 6) Raters also preferredsummaries with grammatically fluent text, whichbenefitted the leading text baseline.These observations suggest that we could builda new system that takes into account all thesefactors (weighted accordingly) or we could builda rule-based meta-classifier that selects a singlesummary from the four systems described in thispaper based on the global characteristics of each.The problem with the former is that it will requirehand-tuning of coefficients for many different sig-nals that are all, for the most part, weakly corre-lated to summary quality.
The problem with thelatter is inefficiency, i.e., it will require the main-tenance and output of all four systems.
In the nextsection we explore an alternate method that lever-ages the data gathered in the evaluation to auto-matically learn a new model.
This approach isbeneficial as it will allow any coefficients to be au-tomatically tuned and will result in a single modelthat can be used to build new summaries.5 Summarization with Ranking SVMsBesides allowing us to assess the relative perfor-mance of our summarizers, our evaluation pro-duced several hundred points of empirical data in-dicating which among two summaries raters pre-fer.
In this section we explore how to build im-proved summarizers with this data by learningpreference ranking SVMs, which are designed tolearn relative to a set of preference judgments(Joachims, 2002).A ranking SVM typically assumes as input a setof queries and associated partial ordering on theitems returned by the query.
The training data isdefined as pairs of points, T = {(xki , xkj )t}|T |t=1,where each pair indicates that the ith item is pre-ferred over the jth item for the kth query.
Eachinput point xki ?
Rm is a feature vector repre-senting the properties of that particular item rel-ative to the query.
The goal is to learn a scoringfunction s(xki ) ?
R such that s(xki ) > s(xkj ) if(xki , xkj ) ?
T .
In other words, a ranking SVMlearns a scoring function whose induced rankingover data points respects all preferences in thetraining data.
The most straight-forward scoringfunction, and the one used here, is a linear classi-fier, s(xki ) = w ?
xki , making the goal of learningto find an appropriate weight vector w ?
Rm.In its simplest form, the ranking SVM opti-mization problem can be written as the followingquadratic programming problem,min12||w||2 s.t.
: ?
(xki , xkj ) ?
T ,s(xki )?
s(xkj ) ?
PREF(xki , xkj )where PREF(xki , xkj ) ?
R is a function indicatingto what degree item xki is preferred over xkj (andserves as the margin of the classifier).
This opti-mization is well studied and can be solved with awide variety of techniques.
In our experiments weused the SVM-light software package4.Our summarization evaluation provides us withprecisely a large collection of preference pointsover different summaries for different productqueries.
Thus, we naturally have a training set Twhere each query is analogous to a specific prod-uct of interest and training points are two possi-ble summarizations produced by two different sys-tems with corresponding rater preferences.
As-suming an appropriate choice of feature represen-tation it is straight-forward to then train the modelon our data using standard techniques for SVMs.To train and test the model we compiled 1906pairs of summary comparisons, each judged bythree different raters.
These pairs were extractedfrom the four experiments described in section 3as well as the additional experiments we ran dur-ing development.
For each pair of summaries(Ski , Skj ) (for some product query indexed by k),we recorded how many raters preferred each of theitems as vki and vkj respectively, i.e., vki is the num-ber of the three raters who preferred summary Siover Sj for product k. Note that vki + vkj does notnecessarily equal 3 since some raters expressed nopreference between them.
We set the loss functionPREF(Ski , Skj ) = vki ?
vkj , which in some cases4http://svmlight.joachims.org/520could be zero, but never negative since the pairsare ordered.
Note that this training set includes alldata points, even those in which raters disagreed.This is important as the model can still learn fromthese points as the margin function PREF encodesthe fact that these judgments are less certain.We used a variety of features for a candidatesummary: how much capitalization, punctuation,pros-cons, and (unique) aspects a summary had;the overall intensity, sentiment, min sentence sen-timent, and max sentence sentiment in the sum-mary; the overall ratingR of the product; and con-junctions of these.
Note that none of these fea-tures encode which system produced the summaryor which experiment it was drawn from.
This isimportant, as it allows the model to be used asstandalone scoring function, i.e., we can set L tothe learned linear classifier s(S).
Alternativelywe could have included features like what systemwas the summary produced from.
This would havehelped the model learn things like the SMAC sys-tem is typically preferred for products with mid-range overall ratings.
Such a model could only beused to rank the outputs of other summarizers andcannot be used standalone.We evaluated the trained model by measuringits accuracy on predicting a single preference pre-diction, i.e., given pairs of summaries (Ski , Skj ),how accurate is the model at predicting that Si ispreferred to Sj for product query k?
We measured10-fold cross-validation accuracy on the subset ofthe data for which the raters were in agreement.We measure accuracy for both weak agreementcases (at least one rater indicated a preference andthe other two raters were in agreement or had nopreference) and strong agreement cases (all threeraters indicated the same preference).
We ignoredpairs in which all three raters made a no preferencejudgment as both summaries can can be consid-ered equally valid.
Furthermore, we ignored pairsin which two raters indicated conflicting prefer-ences as there is no gold standard for such cases.Results are given in table 2.
We compare theranking SVM summarizer to a baseline systemthat always selects the overall-better-performingsummarization system from the experiment thatthe given datapoint was drawn from, e.g., for allthe data points drawn from the SAM versus SMACexperiment, the baseline always chooses the SAMsummary as its preference.
Note that in most ex-periments the two systems emerged in a statisticalPreference Prediction AccuracyWeak Agr.
Strong Agr.Baseline 54.3% 56.9%Ranking SVM 61.8% 69.9%Table 2: Accuracies for learned summarizers.tie, so this baseline performs only slightly betterthan chance.
Table 2 clearly shows that the rank-ing SVM can predict preference accuracy muchbetter than chance, and much better than that ob-tained by using only one summarizer (a reductionin error of 30% for strong agreement cases).We can thus conclude that the data gatheredin human preference evaluation experiments, suchas the one presented here, have a beneficial sec-ondary use as training data for constructing a newand more accurate summarizer.
This raises aninteresting line of future research: can we iter-ate this process to build even better summariz-ers?
That is, can we use this trained summarizer(and variants of it) to generate more examples forraters to judge, and then use that data to learn evenmore powerful summarizers, which in turn couldbe used to generate even more training judgments,etc.
This could be accomplished using Mechani-cal Turk5 or another framework for gathering largequantities of cheap annotations.6 ConclusionsWe have presented the results of a large-scale eval-uation of different sentiment summarization algo-rithms.
In doing so, we explored different waysof using sentiment and aspect information.
Ourresults indicated that humans prefer sentiment in-formed summaries over a simple baseline.
Thisshows the usefulness of modeling sentiment andaspects when summarizing opinions.
However,the evaluations also show no strong preference be-tween different sentiment summarizers.
A detailedanalysis of the results led us to take the next stepin this line of research ?
leveraging preferencedata gathered in human evaluations to automati-cally learn new summarization models.
These newlearned models show large improvements in pref-erence prediction accuracy over the previous sin-gle best model.Acknowledgements: The authors would like tothank Kerry Hannan, Raj Krishnan, Kristen Partonand Leo Velikovich for insightful discussions.5http://www.mturk.com521ReferencesD.
Ariely, G. Loewenstein, and D. Prelec.
2008.
Co-herent arbitrariness: Stable demand curves withoutstable preferences.
The Quarterly Journal of Eco-nomics, 118:73105.S.
Blair-Goldensohn, K. Hannan, R. McDonald,T.
Neylon, G.A.
Reis, and J. Reynar.
2008.
Buildinga sentiment summarizer for local service reviews.
InWWW Workshop on NLP in the Information Explo-sion Era.S.R.K.
Branavan, H. Chen, J. Eisenstein, and R. Barzi-lay.
2008.
Learning document-level semantic prop-erties from free-text annotations.
In Proceedings ofthe Annual Conference of the Association for Com-putational Linguistics (ACL).G.
Carenini and J. Cheung.
2008.
Extractive vs. nlg-based abstractive summarization of evaluative text:The effect of corpus controversiality.
In Interna-tional Conference on Natural Language Generation(INLG).G.
Carenini, R.T. Ng, and E. Zwart.
2005.
Extract-ing knowledge from evaluative text.
In Proceedingsof the International Conference on Knowledge Cap-ture.G.
Carenini, R. Ng, and A. Pauls.
2006.
Multi-document summarization of evaluative text.
In Pro-ceedings of the Conference of the European Chap-ter of the Association for Computational Linguistics(EACL).Y.
Choi, C. Cardie, E. Riloff, and S. Patwardhan.
2005.Identifying sources of opinions with conditional ran-dom fields and extraction patterns.
In Proceedingsthe Joint Conference on Human Language Technol-ogy and Empirical Methods in Natural LanguageProcessing (HLT-EMNLP).E.
Filatova and V. Hatzivassiloglou.
2004.
A formalmodel for information selection in multi-sentencetext extraction.
In Proceedings of the InternationalConference on Computational Linguistics (COL-ING).M.
Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.2005.
Pulse: Mining customer opinions from freetext.
In Proceedings of the 6th International Sympo-sium on Intelligent Data Analysis (IDA).J.
Goldstein, V. Mittal, J. Carbonell, andM.
Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Proceedingsof the ANLP/NAACL Workshop on AutomaticSummarization.M.
Hu and B. Liu.
2004a.
Mining and summariz-ing customer reviews.
In Proceedings of the Inter-national Conference on Knowledge Discovery andData Mining (KDD).M.
Hu and B. Liu.
2004b.
Mining opinion features incustomer reviews.
In Proceedings of National Con-ference on Artificial Intelligence (AAAI).N.
Jindal and B. Liu.
2006.
Mining comprative sen-tences and relations.
In Proceedings of 21st Na-tional Conference on Artificial Intelligence (AAAI).T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proceedings of the ACM Con-ference on Knowledge Discovery and Data Mining(KDD).S.M.
Kim and E. Hovy.
2004.
Determining the senti-ment of opinions.
In Proceedings of Conference onComputational Linguistics (COLING).C.Y.
Lin and E. Hovy.
2003.
Automatic evaluationof summaries using n-gram cooccurrence statistics.In Proceedings of the Conference on Human Lan-guage Technologies and the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL).R.
McDonald.
2007.
A Study of Global InferenceAlgorithms in Multi-document Summarization.
InProceedings of the European Conference on Infor-mation Retrieval (ECIR).K.
McKeown, R.J. Passonneau, D.K.
Elson,A.
Nenkova, and J. Hirschberg.
2005.
DoSummaries Help?
A Task-Based Evaluation ofMulti-Document Summarization.
In Proceedingsof the ACM SIGIR Conference on Research andDevelopment in Information Retrieval.A.M.
Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).V.
Stoyanov and C. Cardie.
2008.
Topic identificationfor fine-grained opinion analysis.
In Proceedings ofthe Conference on Computational Linguistics (COL-ING).I.
Titov and R. McDonald.
2008a.
A joint model oftext and aspect ratings.
In Proceedings of the An-nual Conference of the Association for Computa-tional Linguistics (ACL).I.
Titov and R. McDonald.
2008b.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceedings of the Annual World Wide Web Conference(WWW).L.
Zhuang, F. Jing, and X.Y.
Zhu.
2006.
Movie re-view mining and summarization.
In Proceedingsof the International Conference on Information andKnowledge Management (CIKM).522
