Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185?192,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsEnhanced Monitoring Tools and Online Dialogue Optimisation Mergedinto a New Spoken Dialogue System Design ExperienceGhislain PutoisOrange LabsLannion, FranceRomain LarocheOrange LabsIssy-les-Moulineaux, Francefirstname.surname@orange-ftgroup.comPhilippe BretierOrange LabsLannion, FranceAbstractBuilding an industrial spoken dialoguesystem (SDS) requires several iterationsof design, deployment, test, and evalua-tion phases.
Most industrial SDS develop-ers use a graphical tool to design dialoguestrategies.
They are critical to get goodsystem performances, but their evaluationis not part of the design phase.We propose integrating dialogue logs intothe design tool so that developers canjointly monitor call flows and their asso-ciated Key Performance Indicators (KPI).It drastically shortens the complete devel-opment cycle, and offers a new design ex-perience.Orange Dialogue Design Studio (ODDS),our design tool, allows developers to de-sign several alternatives and compare theirrelative performances.
It helps the SDSdevelopers to understand and analyse theuser behaviour, with the assistance of a re-inforcement learning algorithm.
The SDSdevelopers can thus confront the differentKPI and control the further SDS choicesby updating the call flow alternatives.Index Terms : Dialogue Design, Online Learning,Spoken Dialogue Systems, Monitoring Tools1 IntroductionRecent research in spoken dialogue systems(SDS) has called for a ?synergistic convergence?between research and industry (Pieraccini andHuerta, 2005).
This call for convergence concernsarchitectures, abstractions and methods from bothcommunities.
Under this motivation, several re-search orientations have been proposed.
This pa-per discusses three of them : dialogue design, di-alogue management, and dialogue evaluation.
Di-alogue design and dialogue management reflect inthis paper the respective paths that industry andresearch have followed for building their SDS.
Di-alogue evaluation is a concern for both communi-ties, but remains hard to put into operational per-spectives.The second Section presents the context andrelated research.
The third Section is devoted tothe presentation of the tools : the historical designtool, its adaptation to provide monitoring function-alities and the insertion of design alternatives.
It iseventually concluded with an attempt to reassess-ing the dialogue evaluation.
The fourth Section de-scribes the learning integration to the tool, the con-straints we impose to the learning technique andthe synergy between the tools and the embeddedlearning capabilities.
Finally, the last Section con-cludes the paper.2 ContextThe spoken dialogue industry is structuredaround the architecture of the well known in-dustrial standard VoiceXML 1.
The underlying di-alogue model of VoiceXML is a mapping ofthe simplistic turn-based linguistic model on thebrowser-server based Web architecture (McTear,2004).
The browser controls the speech engines(recognition and text-to-speech) integrated intothe voice platform according to the VoiceXMLdocument served by an application server.
AVoiceXML document contains a set of prompts toplay and the list of the possible interactions theuser is supposed to have at each point of the di-alogue.
The SDS developers 2, reusing Web stan-dards and technologies (e.g.
J2EE, JSP, XML.
.
.
),are used to designing directed dialogues modelledby finite state automata.
Such controlled and de-terministic development process allows the spoken1.
http ://www.w3c.org/TR/voicexml20/2.
In this paper, the term ?SDS developers?
denotes with-out any distinction VUI designers, application developers,and any industry engineers acting in SDS building.185dialogue industry to reach a balance between us-ability and cost (Paek, 2007).
This paper arguesthat tools are facilitators that improve both the us-ability vs. cost trade-off and the reliability of newtechnologies.Spoken dialogue research has developed vari-ous models and abstractions for dialogue manage-ment : rational agency (Sadek et al, 1997), Infor-mation State Update (Bos et al, 2003), functionalmodels (Pieraccini et al, 2001), planning problemsolving (Ferguson and Allen, 1998).
Only a verysmall number of these concepts have been trans-ferred to industry.
Since the late 90?s, the researchhas tackled the ambitious problem of automatingthe dialogue design (Lemon and Pietquin, 2007),aiming at both reducing the development costand optimising the dialogue efficiency and robust-ness.
Recently, criticisms (Paek and Pieraccini,2008) have been formulated and novel approaches(Williams, 2008) have been proposed, both aimingat bridging the gap between research ?focused onMarkov-Decision-Process (Bellman, 1957) baseddialogue management?
and industry ?focused ondialogue design process, model, and tools.
Thispaper contributes to extend this effort.
It addressesall these convergence questions together as a wayfor research and industry to reach a technologicalbreakthrough.Regarding the dialogue evaluation topic, Paek(Paek, 2007) has pointed out that while researchhas exerted attention about ?how best to evaluatea dialogue system ?
?, the industry has focused on?how best to design dialogue systems ??.
This pa-per unifies those two approaches by merging sys-tem and design evaluation in a single graphicaltool.
To our knowledge, ODDS is the only indus-trial tool which handles the complete system life-cycle, from design to evaluation.The tools and methods presented below havebeen tested and validated during the design andimplementation of a large real-world commercialsystem : the 1013+ service is the Spoken DialogueSystem for landline troubleshooting for France.It receives millions of calls a year and schedulesaround 8, 000 appointments a week.
When theuser calls the system, she is presented with an openquestion asking her for the reason of her call.
Ifher landline is out of service, the Spoken DialogueSystem then performs some automated tests onthe line, and if the problem is confirmed, try andschedule an appointment with the user for a man-ual intervention.
If the system and the user cannotagree on an appointment slot, the call is transferredto a human operator.3 The toolsIndustry follows the VUI-completeness princi-ple (Pieraccini and Huerta, 2005) : ?the behaviourof an application needs to be completely speci-fied with respect to every possible situation thatmay arise during the interaction.
No unpredictableuser input should ever lead to unforeseeable be-haviour?.
The SDS developers consider reliablethe technologies, tools, and methodologies thathelp them to reach the VUI-completeness and tocontrol it.3.1 The Dialogue Design ToolThe graphical abstraction proposed by our dia-logue design tool conforms to the general graphrepresentation of finite state automata, with thedifference that global and local variables enable tofactorise several dialogue states in a single node.Transitions relate to user inputs or to internal ap-plication events such as conditions based on in-ternal information from the current dialogue state,from the back-end, or from the dialogue history.
Inthat sense, dialogue design in the industry gener-ally covers more than strict dialogue management,since its specification may indicate the type of spo-ken utterance expected from the user at each stageof the dialogue, up to the precise speech recogni-tion model and parameter values to use, and thegeneration of the system utterance, from naturallanguage generation to speech synthesis or audiorecordings.Our dialogue design tool offers to the SDS de-velopers a graphical abstraction of the dialoguelogic, sometimes also named the call flow.
Thanksto a dynamic VoiceXML generation functional-ity, our dialogue design tool brings the SDS de-velopers the guarantee that VUI-completeness atthe design level automatically implies a similarcompleteness at the implementation level.
Duringmaintenance, If the SDS developers modify a spe-cific part of the dialogue design, the tool guar-antees that solely the corresponding code is im-pacted.
This guarantee impacts positively VUI-completeness, reliability, and development cost.Figure 1 presents the design of a typicalVoiceXML page.
This page is used when the sys-tem asks the user to accept an appointment time186FIGURE 1 ?
1013+ design excerpt : the system asks the user to confirm an appointment slotslot.
It first begins with a prompt box mixingstatic and dynamic prompts (the dynamic parts areunderlined and realised by service-specific javacode).
A log box is then used some contextual ses-sion variables.
Then, an interaction box is used tomodel the system reaction to the user behaviour :on the lower part of the Figure, we program thereaction to user inactivity or recognizer misunder-standing.
In the upper part, we use a recognitionbox followed by a Natural Language Understand-ing (NLU), and we program the different outputclasses : repeat, yes, no and not understood.
Eachoutput is linked to a transition box, which indi-cates which VoiceXML page the service shouldcall next.3.2 Monitoring Functionalities inside theDesign ToolWhile researchers are focused on measuring theprogress they incrementally reach, industry engi-neers have to deal with SDS tuning and upgrade.Their first dialogue evaluation KPI is task com-pletion also called the automation rate because aSDS is deployed to automate specifically selectedtasks.
Most of the time, task completion is esti-mated thanks to the KPI.
The KPI are difficult toexhaustively list and classify.
Some are related tosystem measures, others are obtained thanks to di-alogue annotations and the last ones are collectedfrom users through questionnaires.Some studies (Abella et al, 2004) investigatedgraphical monitoring tools.
The corpus to visualiseis a set of dialogue logs.
The tool aims at reveal-ing how the system transits between its possiblestates.
As a dialogue system is too complex to enu-merate all its possible states, the dialogue logs areregarded as a set of variables that evolve duringtime and the tool proposes to make a projection ona subset of these variables.
This way, the generatedgraphs can either display the call flow, how the dif-ferent steps are reached and where they lead, ordisplay how different variables, as the number oferrors evolve.
This is mainly a tool for understand-ing how the users behave, because it has no directconnection with the way how the system was built.As consequence to this, it does not help to diag-nose how to make it better.
In other words, it doesevaluate the system but does not meet one of ourgoal : the convergence between design and evalu-ation.On the opposite, our graphical design tool pro-vides an innovative functionality : local KPI pro-jection into the original dialogue design thanks toan extensive logging.
A large part of the KPI areautomatically computed and displayed.
As a con-sequence, it is possible to display percentage ofwhich responses the system recognised, the usersactually gave, and see how these numbers matchthe various KPI.
It is one example among the nu-merous analysis views this graphical tool can pro-vide.3.3 Insertion of AlternativesThe 1013+ service has been used to test threekinds of design alternatives.
The first kind is astrategy alternative : the service can choose be-tween offering an appointment time slot to theclient, or asking her for a time slot.
This deci-sion defines whether the next dialogue step willbe system-initiative or user-initiative.
The secondkind is a speaking style alternative : the servicecan either be personified by using the ?I?
pronoun,adopt a corporate style by using the ?We?
pro-noun, or speak in an impersonal style by using thepassive mode.
The third kind is a Text-To-Speechalternative : the service can use a different wordingor prosody for a given sentence.Figure 2 displays a monitoring view of an in-teraction implementation with alternatives.
Therecognition rate is the projected KPI on the graphat each branch.
Other performance indicators aredisplayed at the bottom of the window : here, it187FIGURE 2 ?
Some user experience feedbacks related to a selected prompt alternative.is the actual rate of correct semantic decoding, thesemantic substitution rate, and the semantic rejec-tion rate.
The selection of the highlighted box con-ditions the displayed logs.Our design tool also provides a multivariatetesting functionality.
This method consists in test-ing multiple alternatives and selecting the best oneon a fixed set of predetermined criteria.
Regardingthe VUI-completeness, presenting the completeautomaton to the SDS developers is acceptable, aslong as they can inspect and control every branchof the design.
In general, they even come up withseveral competing designs or points of choice,which can only be properly selected from in a sta-tistical manner.
The ability to compare all the di-alogue design alternatives in the same test-field isa major factor to boost up SDS enhancement bydrastically reducing the time needed.
When wewere developing the current 1013+ version, wehave been able to develop the 5 main alternativesin less than a month, where it had taken a monthand a half for a unique alternative in previous ver-sions.
It brings a statistical relevance in the causallink between the tested alternatives and the differ-ences in performance measures, because it ensuresa good random input space coverage.The KPI graphical projection into the dialoguedesign covers the dialogue alternatives : KPI com-putation just needs to be conditioned by the alter-natives.
Figure 2 illustrates the merge of severalsystem prompt alternatives inside a single design.It represents the prompt alternatives the systemcan choose when proposing an appointment timeslot.
An action block informs the Learning Man-ager about the current dialogue state and avail-able dialogue alternatives.
An ?If?
block then ac-tivates the prompt alternative corresponding to alocal variable ?choixPDC?
filled by the LearningManager.
The rest of the design is identical to thedesign presented in Figure 1.The displayed KPI are conditioned by the se-lected alternative (here, the second wording cir-cled in bold grey).
ODDS then indicates how thedialogue call flow is breakdown into the differentalternatives.
As we have here conditioned the dis-played information by the second alternative, thisalternative receives 100% of the calls displayed,when the other alternatives are not used.
We canthen see the different outcomes for the selectedalternative : the customer answer have lead to atimeout of the recognition in 11.78% of the cases,and amongst the recognised sentences, 80% werean agreement, 13.33% were a reject, and 6.67%were not understood.On the bottom-left part, one can display morespecific KPI, such as good interpretation rate, sub-stitution rate, and reject rate.
These KPI are com-puted after the collected logs have been manuallyannotated, which remains an indispensable pro-cess to monitor and improve the recognition andNLU quality, and thus the overall service quality.Conditioning on another alternative would haveimmediately led to different results, and someway,embedding the user experience feedback inside thedialogue design forms a new material to touch andfeel : the SDS developers can now sculpt a uniquereactive material which contains the design and theKPI measures distribution.
By looking at the influ-ence of each alternative on the KPI when graphi-cally selecting the alternatives, the SDS develop-ers are given a reliable means to understand howto improve the system.1883.4 Reassessing Dialogue EvaluationThe traditional approaches to dialogue evalu-ation attempt to measure how best the SDS isadapted to the users.
We remind that each inter-action between the user and the SDS appears tobe a unique performance.
First, each new dialogueis co-built in a unique way according to both theperson-specific abilities of the user and the possi-bilities of the SDS.
Second, the user adapts veryquickly to new situations and accordingly changesher practices.
The traditional approaches to dia-logue evaluation are eventually based on the frag-ile reference frame of the user, not reliable enoughfor a scientific and an industrial approach of thespoken dialogue field, mostly because of the in-ability to get statistical call volumes for all the di-alogue alternatives.This suggests for a shift in the reference frameused for dialogue evaluation : instead of trying tomeasure the adequacy between the SDS and theuser in the user?s reference frame, one can measurethe adequacy between the user and the SDS in thedesign reference frame composed by the dialoguelogic, the KPI and their expected values.
Takingthe design as the reference allows reassessing thedialogue evaluation.
The proposed basis for dia-logue evaluation is reliable for the SDS developersbecause it is both stable and entirely under con-trol.
Deviations from the predicted situations aredirectly translated into anomalous values of mea-surable KPI that raise alerts.
These automaticallycomputable alerts warn the SDS developers aboutthe presence of issues in their dialogue design.4 Dialogue design learningAs presented in previous Section, the alterna-tive insertion is an enabler for the dialogue systemanalysis tools.
It provides the SDS developers witha novel call flow visualisation experience.
The fur-ther step to this approach is to automate at leasta part of those analyses and improvements withlearning capabilities.4.1 ConstraintsThe objective is to automatically choose onlinethe best alternative among those proposed in thedesign tool, and to report this choice to the SDSdevelopers via the monitoring functionalities thatare integrated to the design tool.
This approachdiffers from the classical reinforcement learningmethods used in the dialogue literature, whichmake their decisions at the dialogue turn level.We use a technique from a previous work(Laroche et al, 2009).
It does not need to de-clare the reachable states : they are automaticallycreated when reached.
This is also a parameter-free algorithm, which is very important when weconsider that most dialogue application developersare not familiar with reinforcement learning the-ory.
We keep the developer focussed on its maintask.
The two additional tasks required for the re-inforcement learning are to define the variable seton which the alternative choice should depend,and to implement a reward function based on theexpected evaluation of the task completion, in or-der to get a fully automated optimisation with anonline evaluation.
The dialogue system automaticevaluation is a large problem that goes beyondthe scope of this paper.
However, sometimes, thedialogue application enables to have an explicitvalidation from the user.
For instance, in an ap-pointment scheduling application, the user is re-quired to explicitly confirm the schedule he wasproposed.
This user performative act completesthe task and provides a reliable automatic evalu-ation.4.2 Learning and Monitoring Synergy in theDesign OptimisationThe learning algorithm and the SDS developersare two actors on the same object : the dialoguesystem.
But, they work at a different time space.The learning algorithm updates its policy aftereach dialogue while the SDS developers moni-tor the system behaviour more occasionally.
Thesame kind of opposition can be made on the actionspace of those actors.
The learning algorithm canonly change its policy among a limited amount ofalternatives, while the SDS developers can makedeeper changes, such as implementing a new dia-logue branch, adding new alternatives, new alter-native points, removing alternatives, etc.
.
.Last but not least, their sight ranges vary a lottoo.
The learning algorithm is concentrated on thealternative sets and automatic evaluation and ig-nores the rest, while the SDS developers can ap-prehend the dialogue application as a whole, as asystem or as a service.
They can also have accessto additional evaluations through annotations, oruser subjective evaluations.These functionality differences make their re-spective roles complementary.
The SDS develop-1890 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30192021222324dayScore3-day meansdaily scoreFIGURE 3 ?
Evolution of the system scoreers have the responsibility for the whole appli-cation and the macro-strategic changes while thelearning manager holds the real-time optimisation.4.3 Control vs.
Automation : the TrustingThresholdAs argued by Pieraccini and Huerta (Pieracciniand Huerta, 2005), finite state machine applied todialogue management does not restrict the dia-logue model to strictly directed dialogues.
Finitestate machines are easily extensible to powerfuland flexible dialogue models.
Our dialogue designtool offers various extensions : dialogue modules,hierarchical design, arbitrary function invocationat any point of the design, conditional statementsto split the flow in different paths.
All those ex-tensions allow designing any topology of the fi-nite state machine required to handle complex dia-logue models like mixed-initiative interaction.
Di-alogue model is not the point where research andindustry fail to converge.The divergence point concerns the control as-pect of VUI-completeness versus the automationof the dialogue design.
As pointed out by recentworks (Paek and Pieraccini, 2008), MDP-baseddialogue management aiming at automating thewhole dialogue design is rejected by the SDS de-velopers.
Even more adaptive, it is seen as an un-controllable black box sensitive to the tuning pro-cess.
The SDS developers do not rely on systemsthat dynamically build their dialogue logic withouta sufficient degree of monitoring and control.Williams (Williams, 2008) has made a substan-tial effort to meet this industrial requirement.
Hissystem is a hybridisation of a conventional dia-logue system following an industrial process, witha POMDP decision module, which is a MDP-based approach to dialogue management enhancedwith dialogue state abstractions to model uncer-tainties.
The responsibilities of each part of thesystem are shared as follows : the conventionalsystem elects several candidate dialogue movesand the POMDP decision module selects the mostcompetitive one.
This is a great step towards in-dustry because the dialogue move chosen by thePOMDP module has been first controlled by theconventional system design.
Nevertheless, the so-built hybrid system is still not fully compliant withthe industrial constraints for the following reasons.First, contrary to our approach, the SDS devel-oper is called upon specific skills that cannot bedemanded to a developer (modeling and tuning a(PO)MDP).
This is a no-go for further integrationin an industrial process.Second, such a predictive module is not self-explanatory.
Although the SDS developers havethe control on the possible behaviour presented tothe POMDP decision module, they are given noclue to understand how the choices are made.
Infact, a learnt feature can never be exported to an-other context.
At the opposite, our approach al-lows us to learn at the design level and conse-quently to report in the automaton the optimisa-tion.
The learning results are therefore understand-190able, analysable and replicable on a larger scale, ina way similar to classical ergonomics guidelines(but statistically proved).4.4 Learning results on the 1013+ serviceIn the 1013+ service, our experiments have fo-cused on the appointment scheduling domain.
Wehave chosen to integrate the following rewards inthe service : each time a user successfully man-ages to get an appointment, the system is given a+30 reward.
If the system is unable to provide anappointment, but manages to transfer the user to ahuman operator, the system is given a +10 (a ?re-sit?).
Last, if the user hangs up, the system is notgiven any positive reward.
Every time the systemdoes not hear nor understand the user, it is given apenalty of 1.In the beginning of the experiment, when thesystem is still using a random policy, the comple-tion rate is as low as 51%, and the transfer rate isaround 36%.
When the system has learned its op-timal policy, the completion rate raises up to 70%,with a transfer rate around 20%.
In our experi-ment, the system has learned to favour an imper-sonal speaking style (passive mode) and it prefersproposing appointment time slots rather than ask-ing the user to make a proposition (the later caseleading to lot of ?in private?
user talks and hesita-tions, and worse recognition performance).Figure 3 shows the evolution of the mean di-alogue score during the first month.
Each serverhave its own Learning Manager database, and op-timises separately.
This is a welcome feature, aseach server can address a different part of theuser population, which is a frequent operationalrequirement.The dialogue score drawn on Figure 3 is com-puted by averaging the mean dialogue score perserver.
The crossed line represents the daily meandialogue score.
The normal line represents the 3-day smoothed dialogue mean score.
The grayedarea represents the 95% confidence interval.
Dur-ing this first month of commercial exploitation,one can notice two major trends : at first, the di-alogue score is gradually increasing until day 20,then the performances noticeably drops, beforerising up again.
It turns out that new servers wereintroduced on day 20, which had to learn the op-timal dialogue policy.
Ultimately (on the secondmonth), they converge to the same solution as thefirst servers.5 Conclusion5.1 A New Basis for Trusting AutomaticLearningThis paper presents an original dialogue designtool that mixes dialogue design and dialogue eval-uation in the same graphical interface.
The de-sign paradigm supported by the tool leads the SDSdevelopers to predict value ranges of local KPIwhile designing the dialogue logic.
It results a newevaluation paradigm using the system design asthe reference and trying to measure deviations be-tween the predicted and the measured values ofthe designed local KPI.
The SDS developers relyon the tool to fulfil the VUI-completeness princi-ple.
Classically applied to dialogue design, the toolenables its application to the dialogue evaluation,leading to the comparison of dialogue design al-ternatives.This places the SDS developers in a dialoguedesign improvement cycle close to the reinforce-ment learning decision process.
Moreover, the in-spector offered by the user experience feedbackfunctionality allows the SDS developers to un-derstand, analyse and generalize all the decisionsamong the dialogue design alternatives.
Combin-ing the learning framework and the design toolguarantees the SDS developers keep control of thesystem.
It preserves VUI-completeness and opensthe way to a reliable learning based dialogue man-agement.5.2 ImplementationThis approach to learning led us to deploy inOctober 2009 the first commercial spoken dia-logue system with online learning.
The system?stask is to schedule an appointment between thecustomer and a technician.
This service receivesapproximately 8, 000 calls every month.
At thetime those lines are written, we are already in a vir-tuous circle of removing low-rated alternatives andreplacing them with new ones, based on what thesystem learnt and what the designer understandsfrom the data.5.3 Future WorkOn a social studies side, we are interested incollaborations to test advanced dialogue strategiesand/or information presentation via generation.
In-deed, we consider our system as a good opportu-nity for large scope experiments.1916 AcknowledgementsThis research has received funding from theEuropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementnumber 216594 (CLASSIC project : www.classic-project.org).ReferencesA.
Abella, J.H.
Wright, and A.L.
Gorin.
2004.
Dia-log trajectory analysis.
In IEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), volume 1, pages 441?444, May.R.E.
Bellman.
1957.
A markovian decision process.Journal of Mathematics and Mechanics, 6 :679?684.J.
Bos, E. Klein, O.
Lemon, and T. Oka.
2003.
Dipper :Description and formalisation of an information-state update dialogue system architecture.George Ferguson and James F. Allen.
1998.
Trips : Anintegrated intelligent problem-solving assistant.
InIn Proc.
15th Nat.
Conf.
AI, pages 567?572.
AAAIPress.R.
Laroche, G. Putois, P. Bretier, and B. Bouchon-Meunier.
2009.
Hybridisation of expertise andreinforcement learning in dialogue systems.
InProceedings of Interspeech.
Special Session : Ma-chine Learning for Adaptivity in Spoken Dialogue,Brighton (United Knigdom), September.O.
Lemon and O. Pietquin.
2007.
Machine learn-ing for spoken dialogue systems.
In Proceedingsof the European Conference on Speech Commu-nication and Technologies (Interspeech?07), pages2685?2688, August.M.
F. McTear.
2004.
Spoken Dialogue Technol-ogy : Toward the Conversational User Interface.Springer, August.T.
Paek and R. Pieraccini.
2008.
Automating spokendialogue management design using machine learn-ing : An industry perspective.
Speech Communica-tion, 50 :716?729.T.
Paek.
2007.
Toward evaluation that leads tobest practices : Reconciling dialog evaluation in re-search and industry.
In Proceedings of the Work-shop on Bridging the Gap : Academic and Indus-trial Research in Dialog Technologies, pages 40?47, Rochester, NY, April.
Association for Compu-tational Linguistics.R.
Pieraccini and J. Huerta.
2005.
Where do we gofrom here ?
research and commercial spoken dialogsystems.
In Laila Dybkjaer and Wolfgang Minker,editors, Proceedings of the 6th SIGdial Workshop onDiscourse and Dialogue, pages 1?10.R.
Pieraccini, S. Caskey, K. Dayanidhi, B. Carpenter,and M. Phillips.
2001.
Etude, a recursive dialogmanager with embedded user interface patterns.
InAutomatic Speech Recognition and Understanding,2001 IEEE Workshop on, pages 244?247.M.
D. Sadek, P. Bretier, and F. Panaget.
1997.
Ar-timis : Natural dialogue meets rational agency.
Inin Proceedings of IJCAI-97, pages 1030?1035.
Mor-gan Kaufmann.J.
D. Williams.
2008.
The best of both worlds : Uni-fying conventional dialog systems and POMDPs.
InInternational Conference on Speech and LanguageProcessing.192
