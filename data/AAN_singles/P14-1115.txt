Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1220?1230,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAbstractive Summarization of Spoken and Written ConversationsBased on Phrasal QueriesYashar Mehdad Giuseppe Carenini Raymond T. NgDepartment of Computer Science, University of British ColumbiaVancouver, BC, V6T 1Z4, Canada{mehdad, carenini, rng}@cs.ubc.caAbstractWe propose a novel abstractive query-based summarization system for conversa-tions, where queries are defined as phrasesreflecting a user information needs.
Werank and extract the utterances in a con-versation based on the overall content andthe phrasal query information.
We clus-ter the selected sentences based on theirlexical similarity and aggregate the sen-tences in each cluster by means of a wordgraph model.
We propose a ranking strat-egy to select the best path in the con-structed graph as a query-based abstractsentence for each cluster.
A resulting sum-mary consists of abstractive sentences rep-resenting the phrasal query informationand the overall content of the conversa-tion.
Automatic and manual evaluationresults over meeting, chat and email con-versations show that our approach signifi-cantly outperforms baselines and previousextractive models.1 IntroductionOur lives are increasingly reliant on multimodalconversations with others.
We email for businessand personal purposes, attend meetings in per-son, chat online, and participate in blog or forumdiscussions.
While this growing amount of per-sonal and public conversations represent a valu-able source of information, going through suchoverwhelming amount of data, to satisfy a partic-ular information need, often leads to an informa-tion overload problem (Jones et al, 2004).
Au-tomatic summarization has been proposed in thepast as a way to address this problem (e.g., (Sakaiand Sparck-Jones, 2001)).
However, often a goodsummary cannot be generic and should be a briefand well-organized paragraph that answer a user?sinformation need.The Document Understanding Conference(DUC)1has launched query-focused multidocu-ment summarization as its main task since 2004,by focusing on complex queries with very specificanswers.
For example, ?How were the bombingsof the US embassies in Kenya and Tanzaniaconducted?
How and where were the attacksplanned??.
Such complex queries are appropriatefor a user who has specific information needs andcan formulate the questions precisely.
However,especially when dealing with conversational datathat tend to be less structured and less topicallyfocused, a user is often initially only exploring thesource documents, with less specific informationneeds.
Moreover, following the common practicein search engines, users are trained to formsimpler and shorter queries (Meng and Yu, 2010).For example, when a user is interested in certaincharacteristics of an entity in online reviews (e.g.,?location?
or ?screen?)
or a specific entity in ablog discussion (e.g., ?new model of iphone?
), shewould not initially compose a complex query.To address these issues, in this work, we tacklethe task of conversation summarization based onphrasal queries.
We define a phrasal query as aconcatenation of two or more keywords, which isa more realistic representation of a user?s informa-tion needs.
For conversational data, this definitionis more similar to the concept of search queries ininformation retrieval systems as well as to the con-cept of topic labels in the task of topic modeling.Example 1 shows two queries and their associatedhuman written summaries based on a single chatlog.
We can observe that the two summaries, al-though generated from the same chat log, are to-tally distinct.
This further demonstrates the impor-tance of phrasal query-based summarization sys-tems for long conversations.To date, most systems in the area of summa-1http://www-nlpir.nist.gov/projects/duc/index.html1220Query-1: Test/Sample database for GNUeAbstract-1: James Thompson asked Reinhard: I was going to workon the sample tonight.
You mentioned wanting a fishhook and all datatypes.
Any other things you want to see in there?
Reinhard said thatmaster/detail would be good, as there have been bugs only appearing in3-level case.
James said he already included that and I know I need toadd a boolean.
Did you want date as well as date-time?
Reinhard saidyes - we also have time values (time without date).
They are especiallyinteresting.
James had not ever had use for something like that so I?mnot sure where I would graft that in.Query-2: Passing parameters to FormsAbstract-2: James Thompson (jamest) asked how did parameter sup-port in forms change recently?
He reported the trigger namespace func-tion referencesGFForm.parameters - which no longer exists.
Reinhardsaid every GFForm should have a parameters.
James said he was usingparameters in on-startup.
Reinhard said that?s probably the only placewhere they don?t work.
James said that I?m thinking about moving thatto on-activation instead of on-startup anyway as it should still work fora main form - but i still wonder if the on-startup parameter issue shouldbe considered a bug - as it shouldn?t choke.
Reinhard was sure it shouldbe considered a bug but I have no idea how to fix it.
We haven?t found away to deal with parameters that works for every case.
I don?t know ifthere is any chance to pass the parameters to the form before it is acti-vated.
James asked how are parameters handled now?
Reinhard repliedthat they are passed to activateForm so they are available from activa-tion for the ?main?
form, the command line parameters are passed andfor dialogs, the parameters are passed that were given in runDialog.Example 1: Sample queries and associatedhuman-written query-based summaries for a chatlog.rization focus on news or other well-written docu-ments, while research on summarizing multipartywritten conversations (e.g., chats, emails) has beenlimited.
This is because traditional NLP ap-proaches developed for formal texts often are notsatisfactory when dealing with multiparty writtenconversations, which are typically in a casual styleand do not display a clear syntactic structure withproper grammar and spelling.
Even though someworks try to address the problem of summarizingmultiparty written conversions (e.g., (Mehdad etal., 2013b; Wang and Cardie, 2013; Murray etal., 2010; Zhou and Hovy, 2005; Gillick et al,2009)), they do so in a generic way (not query-based) and focus on only one conversational do-main (e.g., meetings).
Moreover, most of the pro-posed systems for conversation summarization areextractive.To address such limitations, we propose a fullyautomatic unsupervised abstract generation frame-work based on phrasal queries for multimodal con-versation summarization.
Our key contributions inthis work are as follows:1) To the best of our knowledge, our frameworkis the first abstractive system that generates sum-maries based on users phrasal queries, instead ofwell-formed questions.
As a by-product of ourapproach, we also propose an extractive summa-rization model based on phrasal queries to selectthe summary-worthy sentences in the conversationbased on query terms and signature terms (Lin andHovy, 2000).2) We propose a novel ranking strategy to selectthe best path in the constructed word graph by tak-ing the query content, overall information contentand grammaticality (i.e., fluency) of the sentenceinto consideration.3) Although most of the current summarizationapproaches use supervised algorithms as a partof their system (e.g., (Wang et al, 2013)), ourmethod can be totally unsupervised and does notdepend on human annotation.4) Although different conversational modali-ties (e.g., email vs. chat vs. meeting) underlinedomain-specific characteristics, in this work, wetake advantage of their underlying similarities togeneralize away from specific modalities and de-termine effective method for query-based summa-rization of multimodal conversations.We evaluate our system over GNUe Trafficarchive2Internet Relay Chat (IRC) logs, AMImeetings corpus (Carletta et al, 2005) and BC3emails dataset (Ulrich et al, 2008).
Automaticevaluation on the chat dataset and manual eval-uation over the meetings and emails show thatour system uniformly and statistically significantlyoutperforms baseline systems, as well as a state-of-the-art query-based extractive summarizationsystem.2 Phrasal Query AbstractionFrameworkOur phrasal query abstraction framework gener-ates a grammatical abstract from a conversationfollowing three steps, as shown in Figure 1.2.1 Utterance ExtractionAbstractive summary sentences can be created byaggregating and merging multiple sentences intoan abstract sentence.
In order to generate sucha sentence, we need to identify which sentencesfrom the original document should be extractedand combined to generate abstract sentences.
Inother words, we want to identify the summary-worthy sentences in the text that can be combinedinto an abstract sentence.
This task can be con-sidered as content selection.
Moreover, this step,stand alone, corresponds to an extractive summa-rization system.2http://kt.earth.li/GNUe/index.html1221OriginalconversationQueryExtractedutterancesFilteredutterancesExtractionRedundancyRemovalGenerationClusters Word graphsTop rankedsentences Query-basedabstractClustering Word Graph Ranking ConstructionFigure 1: Phrasal query abstraction framework.
The steps (arrows) influenced by the query are high-lighted.Signature terms: navigator, functionality, reports, UI, schema, gnuChat log:- but watching them build a UI in the flash demo?s is pretty damn im-pressive... and have started moving my sales app to all UI being builtvia ...- i?ll be expanding the technotes in navigator for a while ...- ... in terms of functionality of the underlying databases ...- you mean if I start GNU again I have to read bug reports too?- no, just in case you want to enter bug report- ...I expand the schema before populating with test data ...- i?m willing to scrap it if there is a better schema hidden in gnue some-where :)Example 2: Sample signature terms for a part of achat log.In order to select and extract the informativesummary-worthy utterances, based on the phrasalquery and the original text, we consider two cri-teria: i) utterances should carry the essence of theoriginal text; and ii) utterances should be relevantto the query.
To fulfill such requirements we definethe concepts of signature terms and query terms.2.1.1 Signature TermsSignature terms are generally indicative of thecontent of a document or collection of docu-ments.
To identify such terms, we can use fre-quency, word probability, standard statistic tests,information-theoretic measures or log-likelihoodratio.
In this work, we use log-likelihood ratio toextract the signature terms from chat logs, sincelog-likelihood ratio leads to better results (Guptaet al, 2007).
We use a method described in (Linand Hovy, 2000) in order to identify such termsand their associated weight.
Example 2 demon-strates a chat log and associated signature terms.2.1.2 Query TermsQuery terms are indicative of the content in aphrasal query.
In order to identify such terms,we first extract all content terms from the query.Then, following previous studies (e.g., (Gonzaloet al, 1998)), we use the synsets relations in Word-Net for query expansion.
We extract all conceptsthat are synonyms to the query terms and addthem to the original set of query terms.
Note thatwe limit our synsets to the nouns since verb syn-onyms do not prove to be effective in query ex-pansion (Hunemark, 2010).
While signature termsare weighted, we assume that all query terms areequally important and they all have wight equal to1.2.1.3 Utterance ScoringTo estimate the utterance score, we view boththe query terms and the signature terms as theterms that should appear in a human query-basedsummary.
To achieve this, the most relevant(summary-worthy) utterances that we select arethe ones that maximize the coverage of such terms.Given the query terms and signature terms, we canestimate the utterance score as follows:ScoreQ=1nn?i=1t(q)i(1)ScoreS=1nn?i=1t(s)i?
w(s)i(2)Score = ?
?
ScoreQ+ ?
?
ScoreS(3)where n is number of content words in the ut-terance, t(q)i= 1 if the term tiis a query termand 0 otherwise, and t(s)i= 1 if tiis a signatureterm and 0 otherwise, and w(s)iis the normalizedassociated weight for signature terms.
The param-eters ?
and ?
are tuned on a development set andsum up to 1.After all the utterances are scored, the topscored utterances are selected to be sent to the nextstep.
We estimate the percentage of the retrievedutterances based on the development set.12222.2 Redundancy RemovalUtterances selected in previous step often in-clude redundant information, which is semanti-cally equivalent but may vary in lexical choices.By identifying the semantic relations between thesentences, we can discover what information inone sentence is semantically equivalent, novel, ormore/less informative with respect to the contentof the other sentences.
Similar to earlier work(Berant et al, 2011; Adler et al, 2012), we setthis problem as a variant of the Textual Entail-ment (TE) recognition task (Dagan and Glickman,2004).
Using entailment in this phase is moti-vated by taking advantage of semantic relationsinstead of pure statistical methods (e.g., MaximalMarginal Relevance) and shown to be more effec-tive (Mehdad et al, 2013a).
We follow the samepractice as (Mehdad et al, 2013a) to build an en-tailment graph for all selected sentences to identifyrelevant sentences and eliminate the redundant (interms of meaning) and less informative ones.2.3 Abstract GenerationIn this phase, our goal is to generate understand-able informative abstract sentences that capturethe content of the source sentences and representsthe information needs defined by queries.
Thereare several ways of generating abstract sentences(e.g.
(Barzilay and McKeown, 2005; Liu and Liu,2009; Ganesan et al, 2010; Murray et al, 2010));however, most of them rely heavily on the sen-tence structure.
We believe that such approachesare suboptimal, especially in dealing with conver-sational data, because multiparty written conversa-tions are often poorly structured.
Instead, we ap-ply an approach that does not rely on syntax, noron a standard NLG architecture.
Moreover, sincedealing with user queries efficiency is an impor-tant aspect, we aim for an approach that is alsomotivated by the speed with which the abstractsare obtained.
We perform the task of abstract gen-eration in three steps, as follows:2.3.1 ClusteringIn order to generate an abstract summary, we needto identify which sentences from the previous step(i.e., redundancy removal) can be clustered andcombined in generated abstract sentences.
Thistask can be viewed as sentence clustering, whereeach sentence cluster can provide the content foran abstract sentence.We use the K-mean clustering algorithm by co-sine similarity as a distance function between sen-tence vectors composed of tf.idf scores.
Also no-tice that the lexical similarity between sentences inone cluster facilitates both the construction of theword graph and finding the best path in the wordgraph, as described next.2.3.2 Word GraphIn order to construct a word graph, we adoptthe method recently proposed by (Mehdad et al,2013a; Filippova, 2010) with some optimizations.Below, we show how the word graph is applied togenerate the abstract sentences.Let G = (W,L) be a directed graph with theset of nodes W representing words and a set ofdirected edges L representing the links betweenwords.
Given a cluster of related sentences S ={s1, s2, ..., sn}, a word graph is constructed by it-eratively adding sentences to it.
In the first step,the graph represents one sentence plus the startand end symbols.
A node is added to the graph foreach word in the sentence, and words adjacent arelinked with directed edges.
When adding a newsentence, a word from the sentence is merged inan existing node in the graph providing that theyhave the same POS tag and they satisfy one of thefollowing conditions:i) They have the same word form;ii) They are connected in WordNet by the syn-onymy relation.
In this case the lexical choice forthe node is selected based on the tf.idf score ofeach node;iii) They are from a hypernym/hyponym pair orshare a common direct hypernym.
In this case,both words are replaced by the hypernym;iv) They are in an entailment relation.
In thiscase, the entailing word is replaced by the entailedone.The motivation behind merging non-identicalwords is to enrich the common terms betweenthe phrases to increase the chance that they couldmerge into a single phrase.
This also helps tomove beyond the limitation of original lexicalchoices.
In case the merging is not possible anew node is created in the graph.
When a nodecan be merged with multiple nodes (i.e., mergingis ambiguous), either the preceding and followingwords in the sentence and the neighboring nodesin the graph or the frequency is used to select thecandidate node.We connect adjacent words with directed edges.1223For the new nodes or unconnected nodes, we drawan edge with a weight of 1.
In contrast, when twoalready connected nodes are added (merged), theweight of their connection is increased by 1.2.3.3 Path RankingA word graph, as described above, may containmany sequences connecting start and end.
How-ever, it is likely that most of the paths are not read-able.
We are aiming at generating an informativeabstractive sentence for each cluster based on auser query.
Moreover, the abstract sentence shouldbe grammatically correct.In order to satisfy both requirements, we havedevised the following ranking strategy.
First, weprune the paths in which a verb does not exist,to filter ungrammatical sentences.
Then we rankother paths as follows:Query focus: to identify the summary sentencewith the highest coverage of query content, wepropose a score that counts the number of queryterms that appear in the path.
In order to rewardthe ranking score to cover more salient terms inthe query content, we also consider the tf.idf scoreof query terms in the coverage formulation.Q(P ) =?qi?Ptfidf (qi)?qi?Gtfidf (qi)where the qiare the query terms.Fluency: in order to improve the grammaticalityof the generated sentence, we coach our rankingmodel to select more fluent (i.e., grammaticallycorrect) paths in the graph.
We estimate the gram-maticality of generated paths (Pr(P )) using a lan-guage model.Path weight: The purpose of this function is two-fold: i) to generate a grammatical sentence by fa-voring the links between nodes (words) which ap-pear often; and ii) to generate an informative sen-tence by increasing the weight of edges connectingsalient nodes.
For a path P with m nodes, we de-fine the edge weightw(ni, nj) and the path weightW (P ) as below:w(ni, nj) =freq(ni) + freq(nj)?P?
?Gni,nj?P?diff (P?, ni, nj)?1W (P ) =?m?1i=1w(ni, ni+1)m?
1where the function diff(P?, ni, nj) refers to thedistance between the offset positions pos(P?, ni)of nodes niand njin path P?
(any path in G con-taining niand nj) and is defined as |pos(P?, nj)?pos(P?, ni)|.Overal ranking score: In order to generate aquery-based abstract sentence that combines thescores above, we employ a ranking model.
Thepurpose of such a model is three-fold: i) to coverthe content of query information optimally; ii) togenerate a more readable and grammatical sen-tence; and iii) to favor strong connections betweenthe concepts.
Therefore, the final ranking score ofpath P is calculated over the normalized scores as:Score(P ) = ?
?Q(P ) + ?
?
Pr(P )?
?
?W (P )Where ?, ?
and ?
are the coefficient factors totune the ranking score and they sum up to 1.
In or-der to rank the graph paths, we select all the pathsthat contain at least one verb and rerank them us-ing our proposed ranking function to find the bestpath as the summary of the original sentences ineach cluster.3 Experimental SetupIn this section, we show the evaluation results ofour proposed framework and its comparison to thebaselines and a state-of-the-art query-focused ex-tractive summarization system.3.1 DatasetsOne of the challenges of this work is to find suit-able conversational datasets that can be used forevaluating our query-based summarization sys-tem.
Most available conversational corpora do notcontain any human written summaries, or the goldstandard human written summaries are generic(Carletta et al, 2005; Joty et al, 2013).
In thiswork, we use available corpora for emails andchats for written conversations, while for spokenconversation, we employ an available corpus inmultiparty meeting conversations.Chat: to the best of our knowledge, the only pub-licly available chat logs with human written sum-maries can be downloaded from the GNUe Trafficarchive (Zhou and Hovy, 2005; Uthus and Aha,2011; Uthus and Aha, 2013).
Each chat log hasa human created summary in the form of a digest.Each digest summarizes IRC logs for a period andconsists of few summaries over each chat log witha unique title for the associated human writtensummary.
In this way, the title of each summary1224can be counted as a phrasal query and the cor-responding summary is considered as the query-based abstract of the associated chat log includ-ing only the information most relevant to the title.Therefore, we can use the human-written query-based abstract as gold standards and evaluate oursystem automatically.
Our chat dataset consists of66 query-based (title-based) human written sum-maries with their associated queries (titles) andchat logs, created from 40 original chat logs.
Theaverage number of tokens are 1840, 325 and 6 forchat logs, query-based summaries and queries, re-spectively.Meeting: we use the AMI meeting corpus (Car-letta et al, 2005) that consists of 140 multipartymeetings with a wide range of annotations, includ-ing generic abstractive summaries for each meet-ing.
In order to create queries, we extract threekey-phrases from generic abstractive summariesusing TextRank algorithm (Mihalcea and Tarau,2004).
We use the extracted key-phrases as queriesto generate query-based abstracts.
Since there isno human-written query-based summary for AMIcorpus, we randomly select 10 meetings and eval-uate our system manually.Email: we use BC3 (Ulrich et al, 2008), whichcontains 40 threads from the W3C corpus.
BC3corpus is annotated with generic human-writtenabstractive summaries, and it has been used in sev-eral previous works (e.g., (Joty et al, 2011)).
Inorder to adapt this corpus to our framework, wefollowed the same query generation process as forthe meeting dataset.
Finally, we randomly select10 emails threads and evaluate the results manu-ally.3.2 BaselinesWe compare our approach with the followingbaselines:1) Cosine-1st: we rank the utterances in the chatlog based on the cosine similarity between the ut-terance and query.
Then, we select the first ut-trance as the summary;2) Cosine-all: we rank the utterances in the chatlog based on the cosine similarity between the ut-terance and query and then select the utteranceswith a cosine similarity greater than 0;3) TextRank: a widely used graph-based rank-ing model for single-document sentence extractionthat works by building a graph of all sentences in adocument and use similarity as edges to computethe salience of sentences in the graph (Mihalceaand Tarau, 2004);4) LexRank: another popular graph-based con-tent selection algorithm for multi-document sum-marization (Erkan and Radev, 2004);5) Biased LexRank: is a state-of-the-art query-focused summarization that uses LexRank algo-rithm in order to recursively retrieve additionalpassages that are similar to the query, as well asto the other nodes in the graph (Otterbacher et al,2009).Moreover, we compare our abstractive systemwith the first part of our framework (utterance ex-traction in Figure 1), which can be presented as anextractive query-based summarization system (ourextractive system).
We also show the results of theversion we use in our pipeline (our pipeline ex-tractive system).
The only difference between thetwo versions is the length of the generated sum-maries.
In our pipeline we aim at higher recall,since we later filter sentences and aggregate themto generate new abstract sentences.
In contrast,in the stand alone version (extractive system) welimit the number of retrieved sentences to the de-sired length of the summary.
We also compare theresults of our full system (i.e., with tuning) witha non-optimized version when the ranking coef-ficients are distributed equally (?
= ?
= ?
=0.33).
For parameters estimation, we tune all pa-rameters (utterance selection and path ranking) ex-haustively with 0.1 intervals using our develop-ment set.For manual evaluation of query-based abstracts(meeting and email datasets), we perform a sim-ple user study assessing the following aspects: i)Overall quality given a query (5-point scale)?
; andii) Responsiveness: how responsive is the gener-ated summary to the query (5-point scale)?
Eachquery-based abstract was rated by two annotators(native English speaker).
Evaluators are presentedwith the original conversation, query and gener-ated summary.
For the manual evaluation, weonly compare our full system with LexRank (LR)and Biased LexRank (Biased LR).
We also askthe evaluators to select the best summary for eachquery and conversation, given our system gener-ated summary and the two baselines.To evaluate the grammaticality of our generatedsummaries, following common practice (Barzilayand McKeown, 2005), we randomly selected 50sentences from original conversations and system1225Models ROUGE-1 (%) ROUGE-2 (%)Prc Rec F-1 Prc Rec F-1Cosine-1st 71 5 8 30 3 5Cosine-all 30 68 38 18 40 22TextRank 25 76 34 15 44 20LexRank 36 50 37 14 20 15Biased LexRank 36 51 38 15 21 16Utterance extraction (our extractive system) 34 66?40??20??40?24?
?Utterance extraction (our pipeline extractive system) 30 73?38 19??44?24?
?Our abstractive system (without tuning) 38?59?41?
?18?27?19?Our abstractive system (with tuning) 40??56?42??20??25?22?
?Table 1: Performance of different summarization algorithms on chat logs for query-based chat sum-marization.
Statistically significant improvements (p < 0.01) over the biased LexRank system aremarked with *.
?
indicates statistical significance (p < 0.01) over extractive approaches (TextRankand LexRank).
Systems in italics use the query in generating the summary.generated abstracts, for each dataset.
Then, weasked annotators to give one of three possible rat-ings for each sentence based on grammaticality:perfect (2 pts), only one mistake (1 pt) and not ac-ceptable (0 pts), ignoring capitalization or punc-tuation.
Each sentence was rated by two annota-tors.
Note that each sentence was evaluated indi-vidually, so the human judges were not affectedby intra-sentential problems posed by coreferenceand topic shifts.3.3 Experimental SettingsFor preprocessing our dataset we use OpenNLP3for tokenization, stemming and part-of-speechtagging.
We use six randomly selected query-logs from our chat dataset (about 10% of thedataset) for tuning the coefficient parameters.
Weset the k parameter in our clustering phase to10 based on the average number of sentencesin the human written summaries.
For our lan-guage model, we use a tri-gram smoothed lan-guage model trained using the newswire text pro-vided in the English Gigaword corpus (Graff andCieri, 2003).
For the automatic evaluation we usethe official ROUGE software with standard op-tions and report ROUGE-1 and ROUGE-2 preci-sion, recall and F-1 scores.3.4 Results3.4.1 Automatic Evaluation (Chat dataset)Abstractive vs. Extractive: our full query-based abstractive summariztion system show sta-tistically significant improvements over baselines3http://opennlp.apache.org/and other pure extractive summarization systemsfor ROUGE-14.
This means our systems can ef-fectively aggregate the extracted sentences andgenerate abstract sentences based on the querycontent.
We can also observe that our full systemproduces the highest ROUGE-1 precision scoreamong all models, which further confirms the suc-cess of this model in meeting the user informa-tion needs imposed by queries.
The absolute im-provement of 10% in precision for ROUGE-1 inour abstractive model over our extractive model(our pipeline) further confirms the effectiveness ofour ranking method in generating the abstract sen-tences considering the query related information.Our extractive query-based method beats allother extractive systems with a higher ROUGE-1 and ROUGE-2 which shows the effectiveness ofour utterance extraction model in comparison withother extractive models.
In other words, usingour extractive model described in section 2.1, asa stand alone system, is an effective query-basedextractive summarization model.
We also observethat our extractive model outperforms our abstrac-tive model for ROUGE-2 score.
This can be dueto word merging and word replacement choicesin the word graph construction, which sometimeschange or remove a word in a bigram and conse-quently may decrease the bigram overlap score.Query Relevance: another interesting observa-tion is that relying only on the cosine similarity(i.e., cosine-all) to measure the query relevancepresents a quite strong baseline.
This proves theimportance of query content in our dataset and fur-ther supports the main claim of our work that a4The statistical significance tests was calculated by ap-proximate randomization, as described in (Yeh, 2000).1226Dataset Overal Quality Responsiveness PreferenceOur Sys Biased LR LR Our Sys Biased LR LR Our Sys Biased LR LRMeeting 2.9 2.5 2.1 3.8 3.2 1.8 70% 30% 0%Email 2.7 1.8 1.7 3.7 3.0 1.5 60% 30% 10%Table 2: Manual evaluation scores for our phrasal query abstraction system in comparison with BiasedLexRank and LexRank (LR).Dataset Grammar G=2 G=1 G=0Orig Sys Orig Sys Orig Sys Orig SysChat 1.8 1.6 84% 73% 16% 24% 0% 3%Meeting 1.5 1.3 50% 40% 50% 55% 0% 5%Email 1.9 1.6 85% 60% 15% 35% 0% 5%Table 3: Average rating and distribution over grammaticality scores for phrasal query abstraction systemin comparison with original sentences.good summary should express a brief and well-organized abstract that answers the user?s query.Moreover, a precision of 71% for ROUGE-1 fromthe simple cosine-1st baseline confirms that someutterances contain more query relevant informa-tion in conversational discussions.Query-based vs. Generic: the high recalland low precision in TextRank baseline, both forthe ROUGE-1 and ROUGE-2 scores, shows thestrength of the model in extracting the generic in-formation from chat conversations while missingthe query-relevant content.
The LexRank baselineimproves the results of the TextRank system byincreasing the precision and balancing the preci-sion and recall scores for ROUGE-1 score.
Webelieve that this is due to the robustness of theLexRank method in dealing with noisy texts (chatconversations) (Erkan and Radev, 2004).
In addi-tion, the Biased LexRank model slightly improvesthe generic LexRank system.
Considering thismarginal improvement and relatively high resultsof pure extractive systems, we can infer that theBiased LexRank extracted summaries do not carrymuch query relevant content.
In contrast, the sig-nificant improvement of our model over the ex-tractive methods demonstrates the success of ourapproach in presenting the query related contentin generated abstracts.An example of a short chat log, its related queryand corresponding manual and automatic sum-maries are shown in Example 3.3.4.2 Manual EvaluationContent and User Preference: Table 2 demon-strates overall quality, responsiveness (query re-latedness) and user preference scores for the ab-stracts generated by our system and two base-lines.
Results indicate that our system signif-icantly outperforms baselines in overall qualityand responsiveness, for both meeting and emaildatasets.
This confirms the validity of the re-sults we obtained by conducting automatic evalu-ation over the chat dataset.
We also can observethat the absolute improvements in overall qual-ity and responsiveness for emails (0.9 and 0.7) isgreater than for meetings (0.4 and 0.6).
This isexpected since dealing with spoken conversationsis more challenging than written ones.
Note thatthe responsiveness scores are greater than over-all scores.
This further proves the effectiveness ofour approach in dealing with phrasal queries.
Wealso evaluate the users?
summary preferences.
Forboth datasets (meeting and email), in majority ofcases (70% and 60% respectively), the users preferthe query-based abstractive summary generated byour system.Grammaticality: Table 3 shows grammaticalityscores and distributions over the three possiblescores for all datasets.
The chat dataset resultsdemonstrate the highest scores: 73% of the sen-tences generated by our phrasal query abstrac-tion model are grammatically correct and 24% ofthe generated sentences are almost correct withonly one grammatical error, while only 3% ofthe abstract sentences are grammatically incor-rect.
However, the results varies moving to otherdatasets.
For meeting dataset, the percentage ofcompletely grammatical sentences drops dramati-cally.
This is due to the nature of spoken conver-sations which is more error prone and ungrammat-ical.
The grammaticality score of the original sen-tences also proves that the sentences from meet-1227Query: Trigger namespace and the self propertyChat log:A: good morningB: good morningC: good morning everyoneD: good morningD: good night allF: New GNUe Traffic onlineF: loadsa deep metaphyisical stuff this weekF: D & E discuss the meaning of ?self?
;-)E: yes, and he took the more metaphysical route, where I took the morescientific routeE: I say self?s meaning is derived from one?s ancestryE: self?s meaning is derived from how others use youE: okay, analogy extended too far, I guess :)F: is this a friends vs family debate?E: also noted that the cool part about that is if you have code that needsto happen both on a pre-insert and a pre-update - but only a few lines ofthe code is differentE: you could have one block of trigger code that used self.action to findout why it had been called and branch accordingly.E: there was a big jump from the previous paragraph to thatE: that took that out of contextE: iirc, I was saying an alternative was that ?self?
could refer to neitherthe trigger?s owner nor to the trigger?s callerE: but to the event itselfE: so self.action could be what action is being performedE: self.parent could be the trigger?s ownerE: and self.context could be the callerE: and self.name could be the name of the triggerF: umm, I?m afraid apparant non-sequiturs are always a hazard of doingsummaries ;-)E: iow, I was presenting a 3rd alternative where self referred to some-thing specific to the triggerF: I normally try to write around them, but not 100% successfulE: I?m just convulsing my thoughts to the irc logE: for an errata next week :)E: I think convulsing is a good descriptionF: hehTexRank: There was a big jump from the previous paragraph to that.that took that out of context iirc, I was saying an alternative was that?self?
could refer to neither the trigger?s owner nor to the trigger?s caller.but to the event itself.
so self.action could be what action is being per-formed, self.parent could be the trigger?s owner, and self.context couldbe the caller and self.name could be the name of the trigger.
umm, I?mafraid apparant non-sequiturs are always a hazard of doing summaries;-)LexRank: good morning everyone.
heh.
I?m just convulsing mythoughts to the irc log good morning.
Jason also noted that the coolpart about that is if you have code that needs to happen both on a pre-insert and a pre-update - but only a few lines of the code is different -you could have one block of trigger code that used self.action to find outwhy it had been called and branch accordingly.
for an errata next week:) self?s meaning is derived from how others use you.
I think convulsingis a good description reinhard & jcater discuss the meaning of ?self?
;-)Biased-LexRank: good morning everyone.
heh.
I?m just convulsingmy thoughts to the irc log.
Jason also noted that the cool part aboutthat is if you have code that needs to happen both on a pre-insert anda pre-update - but only a few lines of the code is different - you couldhave one block of trigger code that used self.action to find out why ithad been called and branch accordingly.
yes, and he took the moremetaphysical route, where I took the more scientific route there wasa big jump from the previous paragraph to that but to the event itself.iow, I was presenting a 3rd alternative where self referred to somethingspecific to the trigger.Our system: self could refer to neither the triggers owner nor caller.I was saying an alternative where self referred to something specific tothe trigger.
and self.name could be the name.so self.action could be what action is being performed, self.parent thetriggers owner and self.context caller.Gold: Further to, E clarified that he had suggested that ?self?
couldrefer to neither the trigger?s owner nor to the trigger?s caller - but tothe event itself.
So self.action could be what action is being performed,self.parent could be the trigger?s owner, and self.context could be thecaller.
In other words, I was presenting a 3rd alternative where selfreferred to something specific to the trigger.Example 3.
Summaries generated by our systemand other baselines in comparison with the human-written summary for a short chat log.
Speaker in-formation have been anonymized.ing transcripts, although generated by humans, arenot fully grammatical.
In comparison with theoriginal sentences, for all datasets, our model re-ports slightly lower results for the grammaticalityscore.
Considering the fact that the abstract sen-tences are automatically generated and the orig-inal sentences are human-written, the grammat-icality score and the percentage of fully gram-matical sentences generated by our system, withhigher ROUGE or quality scores in comparisonwith other methods, demonstrates that our systemis an effective phrasal query abstraction frame-work for both spoken and written conversations.4 ConclusionWe have presented an unsupervised framework forabstractive summarization of spoken and writtenconversations based on phrasal queries.
For con-tent selection, we propose a sentence extractionmodel that incorporates query relevance and con-tent importance into the extraction process.
Forthe generation phase, we propose a ranking strat-egy which selects the best path in the constructedword graph based on fluency, query relevanceand content.
Both automatic and manual evalua-tion of our model show substantial improvementover extraction-based methods, including BiasedLexRank, which is considered a state-of-the-artsystem.
Moreover, our system also yields goodgrammaticality score for human evaluation andachieves comparable scores with the original sen-tences.
Our future work is four-fold.
First, weare trying to improve our model by incorporatingconversational features (e.g., speech acts).
Sec-ond, we aim at implementing a strategy to or-der the clusters for generating more coherent ab-stracts.
Third, we try to improve our generatedsummary by resolving coreferences and incorpo-rating speaker information (e.g., names) in theclustering and sentence generation phases.
Fi-nally, we plan to take advantage of topic shifts tobetter segment the relevant parts of conversationsin relation to phrasal queries.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments and suggestionsto improve the paper, and the NSERC Business In-telligence Network for financial support.
We alsowould like to acknowledge the early discussionson the related topics with Frank Tompa.1228ReferencesMeni Adler, Jonathan Berant, and Ido Dagan.
2012.Entailment-based text exploration with applicationto the health-care domain.
In Proceedings ofthe ACL 2012 System Demonstrations, ACL ?12,pages 79?84, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Regina Barzilay and Kathleen R. McKeown.
2005.Sentence Fusion for Multidocument News Sum-marization.
Comput.
Linguist., 31(3):297?328,September.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global Learning of Typed Entailment Rules.In Proceedings of ACL, Portland, OR.Jean Carletta, Simone Ashby, Sebastien Bourban,Mike Flynn, Thomas Hain, Jaroslav Kadlec, VasilisKaraiskos, Wessel Kraaij, Melissa Kronenthal, Guil-laume Lathoud, Mike Lincoln, Agnes Lisowska, andMccowan Wilfried Post Dennis Reidsma.
2005.The AMI meeting corpus: A pre-announcement.
InProc.
MLMI, pages 28?39.I.
Dagan and O. Glickman.
2004.
Probabilistic Tex-tual Entailment: Generic applied modeling of lan-guage variability.
In PASCAL Workshop on Learn-ing Methods for Text Understanding and Mining.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res., 22(1):457?479,December.Katja Filippova.
2010.
Multi-sentence compression:finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 322?330, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages340?348, Stroudsburg, PA, USA.
Association forComputational Linguistics.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-tr.
2009.
A global optimizationframework for meeting summarization.
In Proc.IEEE ICASSP, pages 4769?4772.Julio Gonzalo, Felisa Verdejo, Irina Chugur, andJuan M. Cigarrn.
1998.
Indexing with wordnetsynsets can improve text retrieval.
CoRR.David Graff and Christopher Cieri.
2003.
English Gi-gaword Corpus.
Technical report, Linguistic DataConsortium, Philadelphia.Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.
2007.Measuring importance and query relevance in topic-focused multi-document summarization.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 193?196, Stroudsburg, PA, USA.Association for Computational Linguistics.Lisa Hunemark.
2010.
Query expansion using searchlogs and WordNet.
Technical report, Uppsala Uni-versity, mar.
Masters thesis in Computational Lin-guistics.Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli.
2004.Information overload and the message dynamicsof online interaction spaces: A theoretical modeland empirical exploration.
Info.
Sys.
Research,15(2):194?210, June.Shafiq Joty, Gabriel Murray, and Raymond T. Ng.2011.
Supervised topic segmentation of email con-versations.
In ICWSM11.
AAAI.Shafiq R. Joty, Giuseppe Carenini, and Raymond T.Ng.
2013.
Topic segmentation and labeling in asyn-chronous conversations.
J. Artif.
Intell.
Res.
(JAIR),47:521?573.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proc.
Of the COLING Conference, pages495?501.Fei Liu and Yang Liu.
2009.
From extractive to ab-stractive meeting summaries: can it be done by sen-tence compression?
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort?09, pages 261?264, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Yashar Mehdad, Giuseppe Carenini, and RaymondNG T. 2013a.
Towards Topic Labeling with PhraseEntailment and Aggregation.
In Proceedings ofNAACL 2013, pages 179?189, Atlanta, USA, June.Association for Computational Linguistics.Yashar Mehdad, Giuseppe Carenini, Frank Tompa, andRaymond T. NG.
2013b.
Abstractive meeting sum-marization with entailment and fusion.
In Proceed-ings of the 14th EuropeanWorkshop on Natural Lan-guage Generation, pages 136?146, Sofia, Bulgaria,August.
Association for Computational Linguistics.Weiyi Meng and Clement T. Yu.
2010.
AdvancedMetasearch Engine Technology.
Synthesis Lectureson Data Management.
Morgan and Claypool Pub-lishers.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringingorder into texts.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, July.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010.
Generating and validating abstracts of meet-ing conversations: a user study.
In Proceedings of1229the 6th International Natural Language GenerationConference, INLG ?10, pages 105?113, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jahna Otterbacher, Gnes Erkan, and Dragomir R.Radev.
2009.
Biased lexrank: Passage retrieval us-ing random walks with question-based priors.
Inf.Process.
Manage., 45(1):42?54.Tetsuya Sakai and Karen Sparck-Jones.
2001.
Genericsummaries for indexing in information retrieval.
InProceedings of the 24th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, SIGIR ?01, pages 190?198,New York, NY, USA.
ACM.J.
Ulrich, G. Murray, and G. Carenini.
2008.
Apublicly available annotated corpus for supervisedemail summarization.
In AAAI08 EMAIL Workshop,Chicago, USA.
AAAI.David C. Uthus and David W. Aha.
2011.
Plans towardautomated chat summarization.
In Proceedings ofthe Workshop on Automatic Summarization for Dif-ferent Genres, Media, and Languages, WASDGML?11, pages 1?7, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David C. Uthus and David W. Aha.
2013.
The ubuntuchat corpus for multiparticipant chat analysis.
InAAAI Spring Symposium: Analyzing Microtext.Lu Wang and Claire Cardie.
2013.
Domain-independent abstract generation for focused meet-ing summarization.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1395?1405, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-rian, and Claire Cardie.
2013.
A sentence com-pression based framework to query-focused multi-document summarization.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1384?1394, Sofia, Bulgaria, August.
Association forComputational Linguistics.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th Conference on ComputationalLinguistics - Volume 2, COLING ?00, pages 947?953.
Association for Computational Linguistics.Liang Zhou and Eduard Hovy.
2005.
Digesting vir-tual ?geek?
culture: The summarization of technicalinternet relay chats.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 298?305, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.1230
