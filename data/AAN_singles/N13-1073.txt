Proceedings of NAACL-HLT 2013, pages 644?648,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Simple, Fast, and Effective Reparameterization of IBM Model 2Chris Dyer Victor Chahuneau Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{cdyer,vchahune,nasmith}@cs.cmu.eduAbstractWe present a simple log-linear reparame-terization of IBM Model 2 that overcomesproblems arising from Model 1?s strongassumptions and Model 2?s overparame-terization.
Efficient inference, likelihoodevaluation, and parameter estimation algo-rithms are provided.
Training the model isconsistently ten times faster than Model 4.On three large-scale translation tasks, systemsbuilt using our alignment model outperformIBM Model 4.An open-source implementation of the align-ment model described in this paper is availablefrom http://github.com/clab/fast align .1 IntroductionWord alignment is a fundamental problem in statis-tical machine translation.
While the search for moresophisticated models that provide more nuanced ex-planations of parallel corpora is a key research activ-ity, simple and effective models that scale well arealso important.
These play a crucial role in manyscenarios such as parallel data mining and rapidlarge scale experimentation, and as subcomponentsof other models or training and inference algorithms.For these reasons, IBM Models 1 and 2, which sup-port exact inference in time ?
(|f| ?
|e|), continue tobe widely used.This paper argues that both of these models aresuboptimal, even in the space of models that per-mit such computationally cheap inference.
Model1 assumes all alignment structures are uniformlylikely (a problematic assumption, particularly forfrequent word types), and Model 2 is vastly overpa-rameterized, making it prone to degenerate behav-ior on account of overfitting.1 We present a simplelog-linear reparameterization of Model 2 that avoidsboth problems (?2).
While inference in log-linearmodels is generally computationally more expen-sive than in their multinomial counterparts, we showhow the quantities needed for alignment inference,likelihood evaluation, and parameter estimation us-ing EM and related methods can be computed usingtwo simple algebraic identities (?3), thereby defus-ing this objection.
We provide results showing ourmodel is an order of magnitude faster to train thanModel 4, that it requires no staged initialization, andthat it produces alignments that lead to significantlybetter translation quality on downstream translationtasks (?4).2 ModelOur model is a variation of the lexical translationmodels proposed by Brown et al(1993).
Lexicaltranslation works as follows.
Given a source sen-tence f with length n, first generate the length ofthe target sentence, m. Next, generate an alignment,a = ?a1, a2, .
.
.
, am?, that indicates which sourceword (or null token) each target word will be a trans-lation of.
Last, generate the m output words, whereeach ei depends only on fai .The model of alignment configurations we pro-pose is a log-linear reparameterization of Model 2.1Model 2 has independent parameters for every alignmentposition, conditioned on the source length, target length, andcurrent target index.644Given : f, n = |f|, m = |e|, p0, ?, ?h(i, j,m, n) = ????
?im ?jn?????
(ai = j | i,m, n) =????
?p0 j = 0(1?
p0)?
e?h(i,j,m,n)Z?
(i,m,n)0 < j ?
n0 otherwiseai | i,m, n ?
?(?
| i,m, n) 1 ?
i ?
mei | ai, fai ?
?(?
| fai) 1 ?
i ?
mnullj?
= 1j?
= 2j?
= 3j?
= 4j?
= 5i =3}n=5}m = 6i =1i =2i =4i =5i =6j?j?Figure 1: Our proposed generative process yielding a translation e and its alignment a to a source sentence f, given thesource sentence f, alignment parameters p0 and ?, and lexical translation probabilities ?
(left); an example visualizationof the distribution of alignment probability mass under this model (right).Our formulation, which we write as ?
(ai = j |i,m, n), is shown in Fig.
1.2 The distribution overalignments is parameterized by a null alignmentprobability p0 and a precision ?
?
0 which con-trols how strongly the model favors alignment pointsclose to the diagonal.
In the limiting case as ??
0,the distribution approaches that of Model 1, and, asit gets larger, the model is less and less likely to de-viate from a perfectly diagonal alignment.
The rightside of Fig.
1 shows a graphical illustration of thealignment distribution in which darker squares indi-cate higher probability.3 InferenceWe now discuss two inference problems and give ef-ficient techniques for solving them.
First, given asentence pair and parameters, compute the marginallikelihood and the marginal alignment probabilities.Second, given a corpus of training data, estimatelikelihood maximizing model parameters using EM.3.1 MarginalsUnder our model, the marginal likelihood of a sen-tence pair ?f, e?
can be computed exactly in time2Vogel et al(1996) hint at a similar reparameterization ofModel 2; however, its likelihood and its gradient are not effi-cient to evaluate, making it impractical to train and use.
Ochand Ney (2003) likewise remark on the overparameterizationissue, removing a single variable of the original conditioningcontext, which only slightly improves matters.?
(|f| ?
|e|).
This can be seen as follows.
Foreach position in the sentence being generated, i ?
[1, 2, .
.
.
,m], the alignment to the source and itstranslation is independent of all other translation andalignment decisions.
Thus, the probability that theith word of e is ei can be computed as:p(ei, ai | f,m, n) = ?
(ai | i,m, n)?
?
(ei | fai)p(ei | f,m, n) =n?j=0p(ei, ai = j | f,m, n).We can also compute the posterior probability overalignments using the above probabilities,p(ai | ei, f,m, n) =p(ei, ai | f,m, n)p(ei | f,m, n).
(1)Finally, since all words in e (and their alignments)are conditionally independent,3p(e | f) =m?i=1p(ei | f,m, n)=m?i=1n?j=0?
(ai | i,m, n)?
?
(ei | fai).3We note here that Brown et al(1993) derive their variantof this expression by starting with the joint probability of analignment and translation, marginalizing, and then reorganizingcommon terms.
While identical in implication, we find the di-rect probabilistic argument far more intuitive.6453.2 Efficient Partition Function EvaluationEvaluating and maximizing the data likelihood un-der log-linear models can be computationally ex-pensive since this requires evaluation of normalizingpartition functions.
In our case,Z?
(i,m, n) =n?j?=1exp?h(i, j?,m, n).While computing this sum is obviously possiblein ?
(|f|) operations, our formulation permits exactcomputation in ?
(1), meaning our model can be ap-plied even in applications where computational ef-ficiency is paramount (e.g., MCMC simulations).The key insight is that the partition function is the(partial) sum of two geometric series of unnormal-ized probabilities that extend up and down from theprobability-maximizing diagonal.
The closest pointon or above the diagonal j?, and the next point downj?
(see the right side of Fig.
1 for an illustration), iscomputed as follows:j?
=?
i?
nm?, j?
= j?
+ 1.Starting at j?
and moving up the alignment col-umn, as well as starting at j?
and moving down, theunnormalized probabilities decrease by a factor ofr = exp ?
?n per step.To compute the value of the partition, we onlyneed to evaluate the unnormalized probabilities atj?
and j?
and then use the following identity, whichgives the sum of the first ` terms of a geometric se-ries (Courant and Robbins, 1996):s`(g1, r) =?`k=1g1rk?1 = g11?
r`1?
r .Using this identity, Z?
(i,m, n) can be computed assj?
(e?h(i,j?,m,n), r) + sn?j?
(e?h(i,j?,m,n), r).3.3 Parameter OptimizationTo optimize the likelihood of a sample of paralleldata under our model, one can use EM.
In the E-step,the posterior probabilities over alignments are com-puted using Eq.
1.
In the M-step, the lexical trans-lation probabilities are updated by aggregating theseas counts and normalizing (Brown et al 1993).
Inthe experiments reported in this paper, we make thefurther assumption that ?f ?
Dirichlet(?)
where?i = 0.01 and approximate the posterior distribu-tion over the ?f ?s using a mean-field approximation(Riley and Gildea, 2012).4During the M-step, the ?
parameter must alsobe updated to make the E-step posterior distribu-tion over alignment points maximally probable un-der ?(?
| i,m, n).
This maximizing value cannotbe computed analytically, but a gradient-based op-timization can be used, where the first derivative(here, for a single target word) is:?
?L = Ep(ai|ei,f,m,n) [h(i, ai,m, n)]?
E?
(j?|i,m,n)[h(i, j?,m, n)](2)The first term in this expression (the expected valueof h under the E-step posterior) is fixed for the du-ration of each M-step, but the second term?s value(the derivative of the log-partition function) changesmany times as ?
is optimized.3.4 Efficient Gradient EvaluationFortunately, like the partition function, the deriva-tive of the log-partition function (i.e., the secondterm in Eq.
2) can be computed in constant time us-ing an algebraic identity.
To derive this, we observethat the values of h(i, j?,m, n) form an arithmeticsequence about the diagonal, with common differ-ence d = ?1/n.
Thus, the quantity we seek is thesum of a series whose elements are the products ofterms from an arithmetic sequence and those of thegeometric sequence above, divided by the partitionfunction value.
This construction is referred to asan arithmetico-geometric series, and its sum may becomputed as follows (Fernandez et al 2006):t`(g1,a1, r, d) =?`k=1[a1 + d(k ?
1)] g1rk?1= a`g`+1 ?
a1g11?
r +d (g`+1 ?
g1r)(1?
r)2 .In this expression r, the g1?s and the `?s have thesame values as above, d = ?1/n and the a1?s are4The ?i value was fixed at the beginning of experimentationby minimizing the AER on the 10k sentence French-English cor-pus discussed below.646equal to the value of h evaluated at the starting in-dices, j?
and j?
; thus, the derivative we seek at eachoptimization iteration inside the M-step is:?
?L =Ep(ai|ei,f,m,n) [h(i, ai,m, n)]?
1Z?(tj?
(e?h(i,j?,m,n), h(i, j?,m, n), r, d)+ tn?j?
(e?h(i,j?,m,n), h(i, j?,m, n), r, d)).4 ExperimentsIn this section we evaluate the performance ofour proposed model empirically.
Experiments areconducted on three datasets representing differentlanguage typologies and dataset sizes: the FBISChinese-English corpus (LDC2003E14); a French-English corpus consisting of version 7 of the Eu-roparl and news-commentary corpora;5 and a largeArabic-English corpus consisting of all parallel datamade available for the NIST 2012 Open MT evalua-tion.
Table 1 gives token counts.We begin with several preliminary results.
First,we quantify the benefit of using the geometric seriestrick (?3.2) for computing the partition function rel-ative to na?
?ve summation.
Our method requires only0.62 seconds to compute all partition function valuesfor 0 < i,m, n < 150, whereas the na?
?ve algorithmrequires 6.49 seconds for the same.6Second, using a 10k sample of the French-Englishdata set (only 0.5% of the corpus), we determined1) whether p0 should be optimized; 2) what the op-timal Dirichlet parameters ?i are; and 3) whetherthe commonly used ?staged initialization?
procedure(in which Model 1 parameters are used to initializeModel 2, etc.)
is necessary for our model.
First,like Och and Ney (2003) who explored this issue fortraining Model 3, we found that EM tended to findpoor values for p0, producing alignments that wereoverly sparse.
By fixing the value at p0 = 0.08,we obtained minimal AER.
Second, like Riley andGildea (2012), we found that small values of ?
im-proved the alignment error rate, although the im-pact was not particularly strong over large ranges of5http://www.statmt.org/wmt126While this computational effort is a small relative to thetotal cost in EM training, in algorithms where ?
changes morerapidly, for example in Bayesian posterior inference with MonteCarlo methods (Chahuneau et al 2013), this savings can havesubstantial value.Table 1: CPU time (hours) required to train alignmentmodels in one direction.Language Pair Tokens Model 4 Log-linearChinese-English 17.6M 2.7 0.2French-English 117M 17.2 1.7Arabic-English 368M 63.2 6.0Table 2: Alignment quality (AER) on the WMT 2012French-English and FBIS Chinese-English.
Rows withEM use expectation maximization to estimate the ?f , and?Dir use variational Bayes.Model Estimator FR-EN ZH-ENModel 1 EM 29.0 56.2Model 1 ?Dir 26.6 53.6Model 2 EM 21.4 53.3Log-linear EM 18.5 46.5Log-linear ?Dir 16.6 44.1Model 4 EM 10.4 45.8Table 3: Translation quality (BLEU) as a function ofalignment type.Language Pair Model 4 Log-linearChinese-English 34.1 34.7French-English 27.4 27.7Arabic-English 54.5 55.7?.
Finally, we (perhaps surprisingly) found that thestandard staged initialization procedure was less ef-fective in terms of AER than simply initializing ourmodel with uniform translation probabilities and asmall value of ?
and running EM.
Based on theseobservations, we fixed p0 = 0.08, ?i = 0.01, andset the initial value of ?
to 4 for the remaining ex-periments.7We next compare the alignments produced by ourmodel to the Giza++ implementation of the standardIBM models using the default training procedureand parameters reported in Och and Ney (2003).Our model is trained for 5 iterations using the pro-cedure described above (?3.3).
The algorithms are7As an anonymous reviewer pointed out, it is a near certaintythat tuning of these hyperparameters for each alignment taskwould improve results; however, optimizing hyperparameters ofalignment models is quite expensive.
Our intention is to showthat it is possible to obtain reasonable (if not optimal) resultswithout careful tuning.647compared in terms of (1) time required for training;(2) alignment error rate (AER, lower is better);8 and(3) translation quality (BLEU, higher is better) of hi-erarchical phrase-based translation system that usedthe alignments (Chiang, 2007).
Table 1 shows theCPU time in hours required for training (one direc-tion, English is generated).
Our model is at least10?
faster to train than Model 4.
Table 3 reportsthe differences in BLEU on a held-out test set.
Ourmodel?s alignments lead to consistently better scoresthan Model 4?s do.95 ConclusionWe have presented a fast and effective reparameteri-zation of IBM Model 2 that is a compelling replace-ment for for the standard Model 4.
Although thealignment quality results measured in terms of AERare mixed, the alignments were shown to work ex-ceptionally well in downstream translation systemson a variety of language pairs.AcknowledgmentsThis work was sponsored by the U. S. Army ResearchLaboratory and the U. S. Army Research Office undercontract/grant number W911NF-10-1-0533.ReferencesP.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263?311.V.
Chahuneau, N. A. Smith, and C. Dyer.
2013.Knowledge-rich morphological priors for Bayesianlanguage models.
In Proc.
NAACL.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.R.
Courant and H. Robbins.
1996.
The geometric pro-gression.
In What Is Mathematics?
: An ElementaryApproach to Ideas and Methods, pages 13?14.
OxfordUniversity Press.8Our Arabic training data was preprocessed using a seg-mentation scheme optimized for translation (Habash and Sadat,2006).
Unfortunately the existing Arabic manual alignmentsare preprocessed quite differently, so we did not evaluate AER.9The alignments produced by our model were generallysparser than the corresponding Model 4 alignments; however,the extracted grammar sizes were sometimes smaller and some-times larger, depending on the language pair.P.
A. Fernandez, T. Foregger, and J. Pahikkala.
2006.Arithmetic-geometric series.
PlanetMath.org.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Proc.
ofNAACL.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.D.
Riley and D. Gildea.
2012.
Improving the IBM align-ment models using Variational Bayes.
In Proc.
of ACL.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proc.
ofCOLING.648
