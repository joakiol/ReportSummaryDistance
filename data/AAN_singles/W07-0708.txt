Proceedings of the Second Workshop on Statistical Machine Translation, pages 56?63,Prague, June 2007. c?2007 Association for Computational LinguisticsSpeech-input multi-target machine translationAlicia Pe?rez, M. Ine?s TorresDep.
of Electricity and ElectronicsUniversity of the Basque Countrymanes@we.lc.ehu.esM.
Teresa Gonza?lez, Francisco CasacubertaDep.
of Information Systems and ComputationTechnical University of Valenciafcn@dsic.upv.esAbstractIn order to simultaneously translate speechinto multiple languages an extension ofstochastic finite-state transducers is pro-posed.
In this approach the speech trans-lation model consists of a single networkwhere acoustic models (in the input) and themultilingual model (in the output) are em-bedded.The multi-target model has been evaluatedin a practical situation, and the results havebeen compared with those obtained usingseveral mono-target models.
Experimentalresults show that the multi-target one re-quires less amount of memory.
In addition, asingle decoding is enough to get the speechtranslated into multiple languages.1 IntroductionIn this work we deal with finite-state models whichconstitute an important framework in syntactic pat-tern recognition for language and speech processingapplications (Mohri et al, 2002; Pereira and Riley,1997).
One of their outstanding characteristics is theavailability of efficient algorithms for both optimiza-tion and decoding purposes.Specifically, stochastic finite-state transducers(SFSTs) have proved to be useful for machine trans-lation tasks within restricted domains.
There areseveral approaches implemented over SFSTs whichrange from word-based systems (Knight and Al-Onaizan, 1998) to phrase-based systems (Pe?rez etal., 2007).
SFSTs usually offer high speed duringthe decoding step and they provide competitive re-sults in terms of error rates.
In addition, SFSTs haveproved to be versatile models, which can be easilyintegrated with other finite-state models, such as aspeech recognition system for speech-input transla-tion purposes (Vidal, 1997).
In fact, the integratedarchitecture has proved to work better than the de-coupled one.
Our main goal is, hence, to extendand assess these methodologies to accomplish spo-ken language multi-target translation.As far as multilingual translation is concerned,there are two main trends in machine translation de-voted to translate an input string simultaneously intom languages (Hutchins and Somers, 1992): inter-lingua and parallel transfer.
The former has his-torically been a knowledge-based technique that re-quires a deep-analysis effort, and the latter consistson m decoupled translators in a parallel architec-ture.
These translators can be either knowledge orexample-based.
On the other hand, in (Gonza?lezand Casacuberta, 2006) an example based techniqueconsisting of a single SFST that cope with multipletarget languages was presented.
In that approach,when translating an input sentence, only one searchthrough the multi-target SFST is required, instead ofthe m independent decoding processes required bythe mono-target translators.The classical layout for speech-input multi-targettranslation includes a speech recognition system ina serial architecture with m decoupled text-to-texttranslators.
Thus, this architecture entails a decod-ing stage of the speech signal into the source lan-guage text, and m further decoding stages to trans-late the source text into each of the m target lan-56guages.
If we supplant the m translators with themulti-target SFST, the problem would be reduced to2 searching stages.
Nevertheless, in this paper wepropose a natural way for acoustic models to be in-tegrated in the multilingual network itself, in sucha way that the input speech signal can be simulta-neously decoded and translated into m target lan-guages.
As a result, due to the fact that there is justa single searching stage, this novel approach entailsless computational cost.The remainder of the present paper is structuredas follows: section 2 describes both multi-target SF-STs and the inference algorithm from training ex-amples; in section 3 a novel integrated architecturefor speech-input multi-target translation is proposed;section 4 presents a practical application of thesemethods, including the experimental setup and theresults they produced; finally, section 5 summarizesthe main conclusions of this work.2 Multi-target stochastic finite-statetransducersA multi-target SFST is a generalization of standardSFSTs, in such a way that every input string in thesource language results in a tuple of output stringseach being associated to a different target language.2.1 DefinitionA multi-target stochastic finite-state transducer is atuple T = ?
?,?1 .
.
.
?m, Q, q0, R, F, P ?, where:?
is a finite set of input symbols (source vocabu-lary);?1 .
.
.
?m are m finite sets of output symbols (tar-get vocabularies);Q is a finite set of states;q0 ?
Q is the initial state;R ?
Q????
?1 .
.
.?
?m?Q is a set of transitionssuch as (q, w, p?1, .
.
.
, p?m, q?
), which is a tran-sition from the state q to the state q?, with thesource symbol w and producing the substrings(p?1, .
.
.
, p?m);P : R ?
[0, 1] is the transition probability distri-bution;F : Q ?
[0, 1] is the final state probability distri-bution;The probability distributions satisfy the stochasticconstraint:?q ?
Q (1)F (q)+?w,p?1,...,p?m,q?P (q, w, p?1, .
.
.
, p?m, q?)
= 12.2 Training the multilingual translation modelBoth topology and parameters of an SFST canbe learned fully automatically from bilingual ex-amples making use of underlying alignment mod-els (Casacuberta and Vidal, 2004).
Furthermore,a multi-target SFST can be inferred from a multi-lingual set of samples (Gonza?lez and Casacuberta,2006).
Even though in realistic situations multilin-gual corpora are too scarce, recent works (Popovic?et al, 2005) show that bilingual corpora covering thesame domain are sufficient to obtain generalized cor-pora based on which one can subsequently create therequired collections of aligned tuples.The inference algorithm, GIAMTI (grammaticalinference and alignments for multi-target transducerinference), requires a multilingual corpus, that is, afinite set of multilingual samples (s, t1, .
.
.
, tm) ??????1??
?
???
?m, where ti denotes the translationof the source sentence s into the i-th target language;?
denotes the source language vocabulary, and ?ithe i-th target language vocabulary; the algorithmcan be outlined as follows:1.
Each multilingual sample is transformed intoa single string from an extended vocabulary(?
?
?
?
?
?1 ?
?
?
?
?
?
?m) using a labelingfunction (Lm).
This transformation searches anadequate monotonic segmentation for each ofthe m source-target language pairs on the basisof bilingual alignments such as those given byGIZA++ (Och, 2000).
A monotonic segmen-tation copes with monotonic alignments, thatis, j < k ?
aj < ak following the notationof (Brown et al, 1993).
Each source token,which can be either a word or a phrase (Pe?rezet al, 2007), is then joined with a target phraseof each language as the corresponding segmen-tation suggests.
Each extended symbol consistsof a token from the source language plus zero57Alignment #00:tenperatura1:minimoa2:jeitsiko3:da0:temperaturas1:minimas2:en3:descenso(a) Spanish-BasqueAlignment #00:low1:temperatures2:falling0:temperaturas1:minimas2:en3:descenso(b) Spanish-English0 1temperaturas | temperatura | NIL 2maximas | maximoak | high temperaturesminimas | minimoak | low temperatures 3en | NIL | NIL 5descenso | jaitsiko da | fallingascenso | igoko da | rising(c) Multi-target SFST from Spanish into English and Basque.Figure 1: Example of a trilingual alignment over a trilingual sentence extracted from the task under consid-eration;the related multi-target SFST (with Spanish as input, and English and Basque as output).or more words from each target language intheir turn.2.
Once the set of multilingual samples has beenconverted into a set of single extended strings(z ?
??
), a stochastic regular grammar can beinferred.
Specifically, in this work we deal withk-testable in the string-sense grammars (Garc?
?aand Vidal, 1990), which are considered to bea syntactic approach of the n-gram models.
Inaddition, they allow the integration of severalorder models in a single smoothed automa-ton (Torres and Varona, 2001).3.
The extended symbols associated with thetransitions of the automaton are transformedinto one input token and m output phrases(w/p?1| .
.
.
|p?m) by the inverse labeling function(L?m), leading to the required transducer.Example An illustration of the inference of themulti-target SFST can be shown over a couple ofsimple trilingual sentences from the corpus (where?B?
stands for Basque, ?S?
for Spanish and ?E?
forEnglish):1-B tenperatura maximoa jaitsiko da1-S temperaturas ma?ximas en descenso1-E high temperatures falling2-B tenperatura minimoa igoko da2-S temperaturas m?
?nimas en ascenso2-E low temperatures risingFrom the alignments, depicted in Figures 1(a)and 1(b), an input-language-synchronizedmonotonous segmentation can be built (bear inmind that we are considering Spanish as the inputlanguage).
The corresponding extended strings withthe following constituents for the first and secondsamples respectively are the following ones:1 temperaturas|tenperatura|?m?
?nimas|minimoa|low temperaturesen|?|?descenso|jaitsiko da|falling582 temperaturas|tenperatura|?ma?ximas|maximoa|high temperaturesen|?|?ascenso|igoko da|risingFinally, from this representation of the data, themulti-target SFST can be built as shown in Fig-ure 1(c).2.3 DecodingGiven an input string s (a sentence in the source lan-guage), the decoding module has to search the opti-mal m output strings tm ?
?
?1 ?
?
?
?
??
?m (a sen-tence in each of the target language) according to theunderlying translation model (T ):t?m = arg maxtm???1??????
?mPT (s, tm) (2)Solving equation (2) is a hard computational prob-lem, however, it can be efficiently computed underthe so called maximum approach as follows:PT (s, tm) ?
max?
(s,tm)PT (?
(s, tm)) (3)where ?
(s, tm) is a translation form, that is, a se-quence of transitions in the multi-target SFST com-patible with both the input and the m output strings.?
(s, tm) : (q0, w1, p?m1 , q1) ?
?
?
(qJ?1, wJ , p?mJ , qJ)The input string (s) is a sequence of J input sym-bols, s = wJ1 , and each of the m output stringsconsists of J phrases in its corresponding languagetm = (t1, ?
?
?
, tm) = (p?1)J1 , ?
?
?
, (p?m)J1 .
Thus, theprobability supplied by the multi-target SFST to thetranslation form is given by:PT (?
(s, tm)) = F (qJ)J?j=1P (qj?1, wj , p?mj , qj)(4)In this context, the Viterbi algorithm can be usedto obtain the optimal sequence of states through themulti-target SFST for a given input string.
As aresult, the established m translations are built con-catenating the (J) output phrases for each languagethrough the optimal path.3 An embedded architecture forspeech-input multi-target translation3.1 Statistical frameworkGiven the acoustic representation (x) of a speechsignal, the goal of multi-target speech translationis to find the most likely m target strings (tm);that is, one string (ti) per target language involved(i ?
{1, .
.
.
,m}).
This approach is summarizedin eq.
(5), where the hidden variable s can be in-terpreted as the transcription of the speech signal:t?m = arg maxtmP (tm|x) = arg maxtm?sP (tm, s|x)(5)Making use of Bayes?
rule, the former expressionturns into:t?m = arg maxtm?sP (tm, s)P (x|tm, s) (6)Empirically, there is no loss of generality if we as-sume that the acoustic signal representation dependsonly on the source string, i.e.
P (x|tm, s) is inde-pendent of tm.
In this sense, eq.
(6) can be rewrittenas:t?m = arg maxtm?sP (tm, s)P (x|s) (7)Equation (7) combines a standard acoustic model,P (x|s), and a multi-target translation model,P (tm, s), both of whom can be integrated on the flyduring the searching routine as shown in Figure 2.That is, each acoustic sub-network is only expandedat decoding time when it is required.The outer sum is computationally very expensiveto search for the optimal tuple of target strings tmin an effective way.
Thus we make use of the socalled Viterbi approximation, which finds the bestpath over the whole transducer.3.2 Practical issuesThe underlying recognizer used in this work is ourown continuous-speech recognition system, whichimplements stochastic finite-state models at all lev-els: acoustic-phonetic, lexical and syntactic, andwhich allows to infer them based on samples.The signal analysis was carried out in a stan-dard way, based on the classical Mel-cepstrumparametrization.
Each phone-like unit was modeled591 /e/ | NIL | NIL 2/n/ | NIL | NILFigure 2: Integration on the fly of acoustic models in one edge of the SFST shown in Figure 1(c)by a typical left to right hidden Markov model.
Aphonetically-balanced Spanish database, called Al-bayzin (Moreno et al, 1993), was used to train thesemodels.The lexical model consisted of the extended to-kens of the multi-target SFST instead of runningwords.
The acoustic transcription for each extendedtoken was automatically obtained on the basis of theinput projection of each unit, that is, the Spanish vo-cabulary in this case.Instead of the usual language model, we make useof the multi-target SFST itself, which had the syn-tactic structure provided by a k-testable in the strictsense model, with k=3, and Witten-Bell smoothing.Note that the SFST implicitly involves both inputand output language models.4 Experimental results4.1 Task and corpusThe described general methodology has been putinto practice in a highly practical application thataims to translate on-line TV weather forecasts intoseveral languages, taking the speech of the presen-ter as the input and producing as output text-strings,or sub-titles, in several languages.
For this purpose,we used the corpus METEUS which consists of aset of trilingual sentences, in English, Spanish andBasque, as extracted from weather forecast reportsthat had been published on the Internet.
Let us no-tice that it is a real trilingual corpus, which they areusually quite scarce.Basque is a pre-Indoeuropean language of stillunknown origin.
It is a minority language, spo-ken in a small area of Europe and also within somesmall American communities (such as that in Reno,Nevada).
In the Basque Country (located in thenorth of Spain) it has an official status along withSpanish.
However, despite having coexisted for cen-turies in the same area, they differ greatly both insyntax and in semantics.
Hence, efforts are beingdevoted nowadays to machine translation tools in-volving these two languages (Alegria et al, 2004),although they are still scarce.
With regard to the or-der of the phrases within a sentence, the most com-mon one in Basque is Subject plus Objects plus Verb(even though some alternative structures are also ac-cepted), whereas in Spanish and English other con-structions such as Subject plus Verb plus Objects aremore frequent (see Figures 1(a) and 1(b)).
Anotherdifference between Basque and Spanish or Englishis that Basque is an extremely inflected language.In this experiment we intend to translate Span-ish speech simultaneously into both Basque and En-glish.
Just by having a look at the main features ofthe corpus in Table 1, we can realize that there aresubstantial differences among these three languages,in terms both of the size of the vocabulary and of theamount of running words.
These figures reveal theagglutinant nature of the Basque language in com-parison with English or Spanish.Spanish Basque EnglishTraining Total sentences 14,615Different sentences 7,225 7,523 6,634Words 191,156 187,462 195,627Vocabulary 702 1,147 498Average Length 13.0 12.8 13.3TestSentences 500Words 8,706 8,274 9,150Average Length 17.4 16.5 18.3Perplexity (3grams) 4.8 6.7 5.8Table 1: Main features of the METEUS corpus.With regard to the speech test, the input consistedof the speech signal recorded by 36 speakers, eachone reading out 50 sentences from the test-set in Ta-ble 1.
That is, each sentence was read out by at leastthree speakers.
The input speech resulted in approx-imately 3.50 hours of audio signal.
Needless to say,the application that we envisage has to be speaker-60independent if it is to be realistic.4.2 System evaluationThe performance obtained by the acoustic integra-tion has been experimentally tested for both multi-target and mono-target devices.
As a matter of com-parison, text-input translation results are also re-ported.The multi-target SFST was learned from the train-ing set described in Table 1 using the previously de-scribed GIAMTI algorithm.
The 500 test sentenceswere then translated by the multi-target SFST.
Thetranslation provided by the system in each languagewas compared to the corresponding reference sen-tence.
Additionally, two mono-target SFSTs wereinferred with their outputs for the aforementionedtest to be taken as baseline.
The evaluation includesboth computational cost and performance of the sys-tem.4.2.1 Computational costThe expected searching time and the amount ofmemory that needs to be allocated for a given modelare two key parameters to bear in mind in speech-input machine translation applications.
These val-ues can be objectively measured in terms of the sizeand on the average branching factor of the modeldisplayed in Table 2.multi-target mono-targetS2B S2ENodes 52,074 35,034 20,148Edges 163,146 115,526 69,690Branching factor 3.30 3.13 3.46Table 2: Features of multi-target model and the twodecoupled mono-target models (one for Spanish toBasque translation, referred to as S2B, and the sec-ond for Spanish to English, S2E).Adding the edges up for the two mono-target SF-STs that take part in the decoupled architecture (seeTable 2), we conclude that the decoupled modelneeds a total of 185, 216 edges to be allocated inmemory, which represents an increment of 13%in memory-space with respect to the multi-targetmodel.On the other hand, the multi-target approach of-fers a slightly smaller branching factor than eachmono-target approach.
As a result, fewer paths haveto be explored with the multi-target approach thanwith the decoupled one, which suggests that search-ing for a translation might be faster.
As a matter offact, experimental results in Table 3 show that themono-target architecture works 11% more slowlythan the multi-target one for speech-input machinetranslation and decoding, and 30% for text to texttranslation.Time (s)multi-target mono-targetS2B+S2EText-input 0.36 0.47Speech-input 16.9 18.9Table 3: Average time needed to translate each inputsentence into two languages.Summarizing, in terms of computational cost(space and time), a multi-target SFST performs bet-ter than the mono-target decoupled system.4.2.2 PerformanceSo far, the capability of the systems has been as-sessed in terms of time and spatial costs.
However,the quality of the translations they provide is, doubt-less, the most relevant evaluation criterion.
In or-der to determine the performance of the system ina quantitative manner, the following evaluation pa-rameters were computed for each scenario: bilingualevaluation under study (BLEU), position indepen-dent error rate (PER) and word error rate (WER).Both text and speech-input translation results pro-vided by the multi-target and the mono-target mod-els respectively are shown in Table 4.As can be derived from the translation results,for text-input translation the classical approach per-forms slightly better than the multi-target one, butfor speech-input translation from Spanish into En-glish is the other way around.
In any case, the dif-ferences in performance are marginal.Comparing the text-input with the speech-inputresults we realize that, as could be expected, the pro-cess of speech signal decoding is itself introducingsome errors.
In an attempt to measure these errors,the text transcription of the recognized input signalwas extracted and compared to the input referencein terms of WER as shown in the last row of the Ta-ble 4.
Note that even though the input sentences arethe same the three results differ due to the fact that61we are making use of different SFST models that de-code and translate at the same time.multi-target mono-targetS2B S2E S2B S2ETextBLEU 42.7 66.7 43.4 67.8PER 39.9 19.9 38.2 19.0WER 48.0 27.5 46.2 26.6SpeechBLEU 39.5 59.0 39.2 61.1PER 42.2 25.3 41.5 23.6WER 51.5 33.9 50.5 31.9recognition WER 10.7 9.3 9.1Table 4: Text-input and speech-input translation re-sults for Spanish into Basque (S2B) and Spanish intoEnglish (S2E) using a multi-target SFST (columnson the left) or two mono-target SFSTs (columns onthe right).
The last row shows Spanish speech de-coding results using each of the three devices.In these series of experiments the same task hasbeen compared with two extremely different lan-guage pairs under the same conditions.
There is anoticeable difference in terms of quality between theEnglish and the Basque translations.
The underlyingreason might be due to the fact that SFST modelsdo not capture properly the rich morphology of theBasque as they have to face long-distance reorderingissues.
These differences in the performance of thesystem when translating into English or into Basquehave been previously detected in other works (Or-tiz et al, 2003).
In our case, a manual review of themodels and the obtained translations encourage us tomake use of reordering models in future work, sincethey have proved to report good results in a similarframework (Kanthak et al, 2005).5 Concluding remarks and further workThe main contribution of this paper is the proposalof a fully embedded architecture for multiple speechtranslation.
Thus, acoustic models are integrated onthe fly into a multi-target translation model.
Themost significant feature of this approach is its abil-ity to carry out both the recognition and the transla-tion into multiple languages integrated in a uniquemodel.
Due to the finite-state nature of this model,the speech translation engine is based on a Viterbi-like algorithm.In contrast to the mono-target systems, multi-target SFSTs enable the translation from one sourcelanguage simultaneously into several target lan-guages with lower computational costs (in termsof space and time) and comparable qualitative re-sults.
Moreover, the integration of several languagesand acoustic models is straightforward on means offinite-state devices.Nevertheless, the integrated architecture needsmore parameters to be estimated.
In fact, as theamount of targets increase the data sparseness mightbecome a difficult problem to cope with.
In futurework we intend to make a deeper study on the per-formance of the multi-target system with regard tothe amount of parameters to be estimated.
In ad-dition, as the first step of the learning algorithm isdecisive, we are planning to make use of reorderingmodels in an attempt to face up to with long dis-tance reordering and in order to homogenize all thelanguages involved.AcknowledgmentsThis work has been partially supported by the Uni-versity of the Basque Country and by Spanish CI-CYT under grants 9/UPV 00224.310-15900/2004,TIC2003-08681-C02-02, and CICYT es TIN2005-08660-C04-03 respectively.ReferencesIn?aki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,Koldo Gojenola, and Ruben Urizar.
2004.
Repre-sentation and treatment of multiword expressions inbasque.
In Takaaki Tanaka, Aline Villavicencio, Fran-cis Bond, and Anna Korhonen, editors, Second ACLWorkshop on Multiword Expressions: Integrating Pro-cessing, pages 48?55, Barcelona, Spain, July.
Associ-ation for Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.Francisco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(2):205?225.P.
Garc?
?a and E. Vidal.
1990.
Inference of k-testablelanguages in the strict sense and application to syntac-tic pattern recognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 12(9):920?925.62M.T.
Gonza?lez and F. Casacuberta.
2006.
Multi-TargetMachine Translation using Finite-State Transducers.In Proceedings of TC-Star Speech to Speech Transla-tion Workshop, pages 105?110.John Hutchins and Harold L. Somers.
1992.
An In-troduction to Machine Translation.
Academic Press,Cambridge, MA.Stephan Kanthak, David Vilar, Evgeny Matusov, RichardZens, and Hermann Ney.
2005.
Novel reordering ap-proaches in phrase-based statistical machine transla-tion.
In Proceedings of the ACL Workshop on Buildingand Using Parallel Texts, pages 167?174, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.K.
Knight and Y. Al-Onaizan.
1998.
Translation withfinite-state devices.
In 4th AMTA (Association for Ma-chine Translation in the Americas).Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer, Speech and Language,16(1):69?88, January.A.
Moreno, D. Poch, A. Bonafonte, E. Lleida, J. Llisterri,J.
B. Mario, and C. Nadeu.
1993.
Albayzin speechdatabase: Design of the phonetic corpus.
In Proc.
ofthe European Conference on Speech Communicationsand Technology (EUROSPEECH), Berl?
?n, Germany.Franz J. Och.
2000.
GIZA++: Train-ing of statistical translation models.http://www.fjoch.com/GIZA++.html.Daniel Ortiz, Ismael Garc?
?a-Varea, Francisco Casacu-berta, Antonio Lagarda, and Jorge Gonza?lez.
2003.On the use of statistical machine translation techniqueswithin a memory-based translation system (AME-TRA).
In Proc.
of Machine Translation Summit IX,pages 115?120, New Orleans, USA, September.Fernando C.N.
Pereira and Michael D. Riley.
1997.Speech Recognition by Composition of Weighted Fi-nite Automata.
In Emmanuel Roche and Yves Sch-abes, editors, Finite-State Language Processing, Lan-guage, Speech and Communication series, pages 431?453.
The MIT Press, Cambridge, Massachusetts.Alicia Pe?rez, M. Ine?s Torres, and Francisco Casacuberta.2007.
Speech translation with phrase based stochas-tic finite-state transducers.
In Proceedings of the 32ndInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP 2007), Honolulu, HawaiiUSA, April 15-20.
IEEE.Maja Popovic?, David Vilar, Hermann Ney, SlobodanJovic?ic?, and Zoran S?aric?.
2005.
Augmenting a smallparallel text with morpho-syntactic language.
In Pro-ceedings of the ACL Workshop on Building and Us-ing Parallel Texts, pages 41?48, Ann Arbor, Michigan,June.
Association for Computational Linguistics.M.
Ine?s Torres and Amparo Varona.
2001. k-tss lan-guage models in speech recognition systems.
Com-puter Speech and Language, 15(2):127?149.Enrique Vidal.
1997.
Finite-state speech-to-speechtranslation.
In Proc.
IEEE International Conferenceon Acoustics, Speech, and Signal Processing, vol-ume 1, pages 111?114, Munich, Germany, April.63
