Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1232?1237,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsShort Text Understanding by Leveraging Knowledge into Topic ModelShansong Yang, Weiming Lu?, Dezhi Yang, Liang Yao, Baogang WeiZhejiang UniversityHangzhou, Zhejiang 310000, China{yangshansong,luwm,deathyyoung,yaoliang,wbg}@zju.edu.cnAbstractIn this paper, we investigate the challengingtask of understanding short text (STU task) byjointly considering topic modeling and knowl-edge incorporation.
Knowledge incorporationcan solve the content sparsity problem effec-tively for topic modeling.
Specifically, thephrase topic model is proposed to leveragethe auto-mined knowledge, i.e., the phrases,to guide the generative process of short tex-t.
Experimental results illustrate the effective-ness of the mechanism that utilizes knowledgeto improve topic modeling?s performance.1 IntroductionThe explosion of online text content, such as twittermessages, text advertisements, QA community mes-sages and product reviews has given rise to the ne-cessity of understanding these prevalent short texts.Conventional topic modeling, like PLSA(Hofmann, 1999) and LDA (Blei et al, 2003) arewidely used for uncovering the hidden topics fromtext corpus.
However, the sparsity of content inshort texts brings new challenges to topic modeling.In fact, short texts usually do not contain suf-ficient statistical signals to support many state-of-the-art approaches for text processing such as top-ic modeling (Hua et al, 2015).
Knowledge is indis-pensable to STU task, where knowledge-based topicmodel (Andrzejewski et al, 2009; Hu et al, 2011;Jagarlamudi et al, 2012; Mukherjee and Liu, 2012;Chen et al, 2013; Yan et al, 2013) has attractedmore attention recently.
?Corresponding authorWe consider, in the STU task, the availableknowledge can be divided into two classes: self-contained knowledge and external knowledge.Self-contained knowledge, which is focused inthis paper, is extracted from the short text itself,such as key-phrase.
External knowledge is con-structed without special purpose, such as WordNet(Miller, 1995), KnowItAll (Etzioni et al, 2005),Wikipedia (Gabrilovich and Markovitch, 2007),Yago (Suchanek et al, 2007), NELL(Carlson et al, 2010) and Probase (Wu et al, 2012).PLSA and LDA are the typical unsupervised topicmodels, that is non-knowledgeable model.
In con-trast, Biterm topic model (BTM) (Yan et al, 2013)leverages self-contained knowledge into semanticanalysis.
BTM learns topics over short texts by mod-eling the generation of biterms in the whole corpus.A biterm is an unordered word-pair co-occurring inshort contexts.
BTM posits that the two words in abiterm share the same topic drawn from a mixture oftopics over the whole corpus.
The major advantageof BTM is that BTM explicitly model the word co-occurrences in the local context, which well capturesthe short-range dependencies between words.External knowledge-based models incorporateexpert domain knowledge to help guide the mod-els.
DF-LDA (Andrzejewski et al, 2009) model in-corporates domain knowledge in the form of must-link and cannot-link.
Must-link states that two word-s should belong to the same topic, while cannot-link states that two words should not be in the sametopic.
GK-LDA (Chen et al, 2013) leverages lexi-cal semantic relations of words such as synonyms,antonyms and adjective attributes in topic models.
A1232],1[ Mm],1[ mNn],1[ ,nmNl!
m"nmz ,lnmw ,,sm,#sm,$],1[ mSs],1[ Kk%k&Figure 1: The phrase topic model proposed in this paper.vast amount of lexical knowledge about words andtheir relationships, denoted as LR-sets, available inonline dictionaries or other resources can be exploit-ed by this model to generate more coherent topics.However, for external knowledge-based model-s, the incorporated knowledge is too general to beconsistent with the short text in the semantic space.On the other hand, BTM, as a typical self-containedknowledge-based model, makes rough assumptionon the generated biterms.
The generated biterms areinundated with noise, for not any two terms in shorttext share same topic.
Based on the above anal-ysis, we first identify key-phrases from short text,which can be deemed as self-contained knowledge,then propose phrase topic model (PTM), which con-strains same topic for terms in key-phrase and sam-ple topics for non-phrase terms from mixture of key-phrase?s topic.2 Phrase Topic Model2.1 ModelA phrase is defined as a consecutive sequenceof terms, or unigrams.
In this paper, we fo-cus on self-contained knowledge in short text,i.e., the key-phrases.
Key-phrase extraction is afundamental component in our work.
We useCRF++1to identify key-phrases in a short text.The training data is built manually, and the fea-tures contain the word itself, the part of speechtagged by Stanford Log-linear Part-Of-Speech Tag-1http://crfpp.googlecode.com/svn/trunk/doc/index.htmlger (Toutanova et al, 2003).
Sample identified key-phrases are shown in Table 2.In this paper, our phrase topic model is proposedbased on three assumptions:?
Key-phrases are the key points of interest in theshort text, which should be the focus.?
Terms consisting of the same key-phrase willshare common topic.?
Non-phrase term?s topic assignment should de-pend on that of key-phrases in the same text.Our assumptions is indeed similar to other mod-els (Gruber et al, 2007), for example each sentenceis assumed to be assigned to one topic, however thisassumption is too general, in many cases, differen-t words should be assigned different topics even inshort text.
Our model is more refined to distinguishkey-phrase and non-phrase.
In addition, if two ormore key-phrases exist in the same short text, theyare probably assigned different topics.The graphical representation of PTM is illustrat-ed in Figure 1. ?
and ?
are hyper-parameters, whichare experienced tuned.
?
is corpus-level parameter,while ?
is document-level parameter.
The hiddenvariables consist of zm,nand ?m,s.
The generativeprocess of phrase topic model is presented as fol-lows.?
For each topic k ?
[1,K]?
draw a topic-specific word distribution?k?
Dir(?)?
For each document m ?
[1,M ]?
draw a topic distribution ?m?
Dir(?)?
For each key-phrase n ?
[1, Nm]?
draw topic assignment zm,n?Multi(?m)?
For each word l ?
[1, Nm,n]?
draw wm,n,l?
Multi(?zm,n)?
For each non-phrase word s ?
[1, Sm]?
draw a topic assignment ?m,s?Uniform(zm,1, .
.
.
, zm,Nm)?
draw word om,s?
Multi(?
?m,s)From this process, we can see the generation of key-phrases and non-phrases are distinguished and non-phrase?s generation is based on the topic assignmentof key-phrases in the same document.12332.2 Inference By Gibbs SamplingSimilarly with LDA, collapsed Gibbs sampling(Griffiths and Steyvers, 2004) can be utilized to per-form approximate inference.
In our model, the hid-den variables are key-phrase?s topic assignment zand non-phrase word?s topic assignment ?.
To per-form Gibbs sampling, we first randomly initializethe hidden variables.
Then we sample the top-ic assignment based on the conditional distribu-tion p(zm,n= k|z?
(m,n),w,o, ?)
and p(?m,s=k|z,w,o, ??
(m,s)).We can derive the conditional probability forzm,nfollowing Equation 1, where nkm,?
(m,n)de-notes the number of key-phrases whose topic assign-ment are k in document m without consideration ofkey-phrase {m,n}, which is similar to nk?m,?(m,n).nwm,n,lk,?
(m,n)denotes the number of times key-phraseterm wm,n,lassigned to topic k without consid-eration of key-phrase {m,n}, which is similar tonwk,?(m,n).
nom,sk,?mdenotes the number of times non-phrase term om,sassigned to topic k without consid-eration of document m, which is similar to nwk,?m.Similarly, we can derive the conditional probabil-ity for ?m,sfollowing Equation 2, where nom,sk,?
(m,s)denotes the number of times non-phrase term om,sassigned to topic k without consideration of non-phrase term {m, s}, which is similar to nwk,?
(m,s).Lmdenotes the number of topics assigned to key-phrases in document m.Finally, we can easily estimate the topic distribu-tion ?m,kand topic-word distribution ?k,wfollowingEquation 3 and 4.?m,k=nkm+ ?
?Kk?=1nk?m+ K?
(3)?k,w=nwk+ ?
?Vw?=1nw?k+ V ?
(4)3 Experiments and ResultsOnline reviews dataset (Chen et al, 2013), whichconsists of four domains, is utilized to evaluate ourmodel, where each domain collection contains 500reviews.
Each review?s average length is 20.42.
Thestatistics of each domain are presented in Table 1.It?s worth noting that the Phrase is auto-identifiedby the key-phrase extraction method.
And the Wordrepresents the whole distinct words for those identi-fied key-phrases.In our paper, we assumed each domain has a sin-gle topic model.
For different domain, we think thesemantic space is quite different.
So we performedthe proposed topic model with respect to differentdomain.
The number of topics is usually determinedby experience, in our experiment, each domain col-lection contains 500 reviews, we think the numberof topics ranging from 2 to 20 is appropriate, andthese reviews are sufficient to train a topic model.Table 1: Statistic information of the dataset.Dataset Phrase Word VocabularyComputer 1439 1423 5109Cellphone 1110 1109 4184Camera 2962 2620 8366Food 1235 1350 4488Recent research (Chang et al, 2009;Newman et al, 2010) shows that the modelswhich achieve better predictive perplexity oftenhave less interpretable latent spaces.
So the TopicCoherence Metric (Mimno et al, 2011) is utilizedto assess topic quality, which is consistent withhuman labeling.We compare our model with four baseline mod-els: non-knowledgeable model LDA, self-containedknowledgeable model BTM, external knowledge-based model GK-LDA (Chen et al, 2013) and DF-LDA (Andrzejewski et al, 2009).
Those identifiedkey-phrases are used as must-links in DF-LDA andLR-sets in GK-LDA.
This can ensure the incorporat-ed knowledge upon different models are equal.Table 2 illustrates the auto-identified phrases fromcellphone dataset.
From this result, we can seekey-phrase extraction method can efficiently identi-fy mostly phrases.
More than one phrase, for exam-ple warranty service and android phone, may appearin a single sentence, and their topic assignments areprobably different.
Our proposed phrase topic mod-el(PTM) can well handle this case, which is morewell-defined than the assumption of all words withina sentence share one topic.
Our phrase topic modelassumes non-phrase term?s topic assignment shoulddepend on that of key-phrases in the same text.
Thisassumption can be clearly confirmed by Table 2, forexample, Nokia N97 mini is semantic dependent US-1234p(zm,n= k|z?
(m,n),w,o, ?)
=nkm,?
(m,n)+ ??Kk?=1nk?m,?
(m,n)+ K???Nm,nl=1(nwm,n,lk,?
(m,n)+ ?)?Nm,nl=1(?Vw=1nwk,?
(m,n)+ V ?)?
?Sms=1(nom,sk,?m+ ?
)?Sms=1(?Vw=1nwk,?m+ V ?
)(1)p(?m,s= k|z,w,o, ??
(m,s)) =nom,sk,?
(m,s)+ ??Vw=1nwk,?
(m,s)+ V ?
?1Lm(2)B charge cable, the same as company and warrantyservice.For all models, posterior inference was drawn af-ter 1000 Gibbs iterations with an initial burn-in of800 iterations.
For all models, we set the hyperpa-rameters ?
= 2 and ?
= 0.5.The evaluation results over Topic Coherence Met-ric are presented in Figure 2 and Figure 3.
Thisfigure indicates our model and BTM can get high-er topic coherence score than GK-LDA and DF-LDA, which means the self-defined knowledge andthe mechanism of knowledge incorporation are ef-fective to topic model.
LDA?s performance is ac-ceptable but not stable.
Our model performs betterthan BTM, which is probably because the rough as-sumption of BTM on generated biterms.
From theabove analysis, we can see our proposed model canget the best performance.T-test results show that the performance improve-ment of our model over baselines is statistically sig-nificant on Topic Coherence Metric.
All p-values fort-test are less than 0.00001.Figure 4 presents the fluctuation of topic coher-ence when tuning the hyper-parameter ?
and ?.
Wecan see that the performance fluctuates within a lim-ited range as we vary ?
and ?.
The topic coherencefluctuates between ?550 and ?950 other than fooddataset, which gets less fluctuation range.Table 3 shows example topics for each domain,where inconsistent words are highlighted in red.From this results, we can see the number of errors inphrase topic model(PTM) is significantly less thanLDA, which indicates our proposed topic model ismore suitable than LDA for short text.4 Conclusions and Future WorkIn this paper, we present a topic model to achieveSTU task starting from key-phrases.
The terms inkey-phrases identified from the short texts are sup-posed to share a common topic respectively.
Andthose key-phrases are assumed to be the central fo-cus in the generative process of documents.
Inthe future work, the self-contained knowledge, suchas those identified key-phrases, and the externalknowledge-base should be integrated to guide top-ic modeling.AcknowledgementsThis work is supported by the National Natu-ral Science Foundation of China No.61103099,the Fundamental Research Funds for the Cen-tral Universities(2014QNA5008), Chinese Knowl-edge Center of Engineering Science and Tech-nology(CKCEST) and Specialized Research Fundfor the Doctoral Program of Higher Educa-tion(SRFDP)(No.20130101110136).References[Andrzejewski et al2009] David Andrzejewski, XiaojinZhu, and Mark Craven.
2009.
Incorporating domainknowledge into topic modeling via dirichlet forest pri-ors.
In Proceedings of the 26th Annual InternationalConference on Machine Learning, ICML 2009, pages25?32.
[Blei et al2003] David M Blei, Andrew Y Ng, andMichael I Jordan.
2003.
Latent dirichlet alocation.the Journal of machine Learning research, 3:993?1022.
[Carlson et al2010] Andrew Carlson, Justin Betteridge,Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr.,1235Table 2: Identified Key-phrase in Cellphone dataset[1] Both sites list compatible devices including the Samsung Galaxy Tab.
[2] My Dell Streak needed more power than any normal USB car adapter could give me.
[3] This actually comes with a micro USB charge cable which fits and works perfectlyfor my Nokia N97 mini.
[4] I contacted the company for warranty service.
On my android phone I paired .
.
.
[5] Everything from pulling it out of the box to syncing it with both my IPhone andIpod touch 4g were effortless.
!1400.000!1200.000!1000.000!800.000!600.000!400.000!200.0002 4 6 8 10 12 14 16 18 20TopicCoherenceNumber of TopicsTopic Coherence vs.
Number of TopicsOurModelLDABTMGK!LDADF!LDAFigure 2: Average Topic Coherence score of each model.
!2500!2000!1500!1000!5000Computer Cellphone Camera FoodOurModelLDABTMGK!LDADF!LDAFigure 3: Detailed Topic Coherence score of T = 15.115010509508500.5!1.5!2.5!
3.5!
4.5!,UUJ0.1!0.6!!9895050105050115010505950850750650500.5!1.5!2).5!
3.5!
4.5!GSKXG0.1!0.6!!6505507506508507509508506950850750500.5!
2.0!)03.5!
5.0!KRRVNUT.1!0.6!!K6506007506508507509508509508507506505500.5!1.5!
2.5!
3.5!
4.5!Compu0.1!0.6!
!ter650550750650850750950850Figure 4: Parameter influences with fixed topic number T = 15.Table 3: Example topics.
First row: domain, Second row: inferred topic tag.
Errors are highlighted in red.Cellphone Computer Food Cameramusic game dinner buyLDA PTM LDA PTM LDA PTM LDA PTMphone phone buy fps coffee soup camera cameramusic music make disruption product good bought boughtiphone car games dips found bread wanted picturescalls radio time playable love mix year videoplay device fast wars amazon popcorn time sonybluetooth sound play age bread taste happy backhear quality people pretty popcorn great purchase canonfree iphone thing update eating bag day batterycell volume card star ordered flavor month timelisten bluetooth full empires bought make love pricehands easy product laggy good coffee ago lenscharge good read unplayable taste eat week quality1236and Tom M. Mitchell.
2010.
Toward an architecturefor never-ending language learning.
In Proceedings ofthe Twenty-Fourth AAAI Conference on Artificial In-telligence, AAAI 2010.
[Chang et al2009] Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish, Chong Wang, and David M.Blei.
2009.
Reading tea leaves: How humans interprettopic models.
In 23rd Annual Conference on NeuralInformation Processing Systems 2009, pages 288?296.
[Chen et al2013] Zhiyuan Chen, Arjun Mukherjee, BingLiu, Meichun Hsu, Malu Castellanos, and RiddhimanGhosh.
2013.
Discovering coherent topics using gen-eral knowledge.
In Proceedings of the 22nd ACM in-ternational conference on Conference on information& knowledge management, pages 209?218.
ACM.
[Etzioni et al2005] Oren Etzioni, Michael J. Cafarel-la, Doug Downey, Ana-Maria Popescu, Tal Shaked,Stephen Soderland, Daniel S. Weld, and AlexanderYates.
2005.
Unsupervised named-entity extractionfrom the web: An experimental study.
Artif.
Intell.,165(1):91?134.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovichand Shaul Markovitch.
2007.
Computing seman-tic relatedness using wikipedia-based explicit seman-tic analysis.
In Proceedings of the 20th Internation-al Joint Conference on Artificial Intelligence, pages1606?1611.
[Griffiths and Steyvers2004] Thomas L Griffiths andMark Steyvers.
2004.
Finding scientific topics.
Pro-ceedings of the National academy of Sciences of theUnited States of America, 101(Suppl 1):5228?5235.
[Gruber et al2007] Amit Gruber, Yair Weiss, and MichalRosen-Zvi.
2007.
Hidden topic markov models.
InProceedings of the Eleventh International Conferenceon Artificial Intelligence and Statistics, AISTATS 2007,San Juan, Puerto Rico, March 21-24, 2007, pages163?170.
[Hofmann1999] Thomas Hofmann.
1999.
Probabilis-tic latent semantic indexing.
In Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 50?57.
[Hu et al2011] Yuening Hu, Jordan L. Boyd-Graber, andBrianna Satinoff.
2011.
Interactive topic modeling.
InThe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 248?257.
[Hua et al2015] Wen Hua, Zhongyuan Wang, HaixunWang, Kai Zheng, and Xiaofang Zhou.
2015.
Shorttext understanding through lexical-semantic analy-sis.
In International Conference on Data Engineering(ICDE).
[Jagarlamudi et al2012] Jagadeesh Jagarlamudi,Hal Daum III, and Raghavendra Udupa.
2012.Incorporating lexical priors into topic models.
In13th Conference of the European Chapter of theAssociation for Computational Linguistics, pages204?213.
[Miller1995] George A. Miller.
1995.
Wordnet: A lexicaldatabase for english.
Commun.
ACM, 38(11):39?41.
[Mimno et al2011] David M. Mimno, Hanna M. Wal-lach, Edmund M. Talley, Miriam Leenders, and An-drew McCallum.
2011.
Optimizing semantic coher-ence in topic models.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2011, pages 262?272.
[Mukherjee and Liu2012] Arjun Mukherjee and Bing Li-u.
2012.
Aspect extraction through semi-supervisedmodeling.
In The 50th Annual Meeting of the Associ-ation for Computational Linguistics, pages 339?348.
[Newman et al2010] David Newman, Youn Noh, Ed-mund M. Talley, Sarvnaz Karimi, and Timothy Bald-win.
2010.
Evaluating topic models for digital li-braries.
In Proceedings of the 2010 Joint InternationalConference on Digital Libraries, pages 215?224.
[Suchanek et al2007] Fabian M. Suchanek, Gjergji Kas-neci, and Gerhard Weikum.
2007.
Yago: a core ofsemantic knowledge.
In Proceedings of the 16th Inter-national Conference on World Wide Web, WWW 2007,Banff, Alberta, Canada, May 8-12, 2007, pages 697?706.
[Toutanova et al2003] Kristina Toutanova, Dan Klein,Christopher D. Manning, and Yoram Singer.
2003.Feature-rich part-of-speech tagging with a cyclic de-pendency network.
In HLT-NAACL 2003, HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.
[Wu et al2012] Wentao Wu, Hongsong Li, Haixun Wang,and Kenny Qili Zhu.
2012.
Probase: a probabilistictaxonomy for text understanding.
In Proceedings ofthe ACM SIGMOD International Conference on Man-agement of Data, SIGMOD 2012, Scottsdale, AZ, US-A, May 20-24, 2012, pages 481?492.
[Yan et al2013] Xiaohui Yan, Jiafeng Guo, Yanyan Lan,and Xueqi Cheng.
2013.
A biterm topic model forshort texts.
In 22nd International World Wide WebConference, pages 1445?1456.1237
