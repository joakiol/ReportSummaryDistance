Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897?906,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Structured Vector Space Model for Word Meaning in ContextKatrin ErkDepartment of LinguisticsUniversity of Texas at Austinkatrin.erk@mail.utexas.eduSebastian Pado?Department of LinguisticsStanford Universitypado@stanford.eduAbstractWe address the task of computing vector spacerepresentations for the meaning of word oc-currences, which can vary widely according tocontext.
This task is a crucial step towards arobust, vector-based compositional account ofsentence meaning.
We argue that existing mod-els for this task do not take syntactic structuresufficiently into account.We present a novel structured vector spacemodel that addresses these issues by incorpo-rating the selectional preferences for words?argument positions.
This makes it possible tointegrate syntax into the computation of wordmeaning in context.
In addition, the model per-forms at and above the state of the art for mod-eling the contextual adequacy of paraphrases.1 IntroductionSemantic spaces are a popular framework for the rep-resentation of word meaning, encoding the meaningof lemmas as high-dimensional vectors.
In the de-fault case, the components of these vectors measurethe co-occurrence of the lemma with context featuresover a large corpus.
These vectors are able to pro-vide a robust model of semantic similarity that hasbeen used in NLP (Salton et al, 1975; McCarthy andCarroll, 2003; Manning et al, 2008) and to modelexperimental results in cognitive science (Landauerand Dumais, 1997; McDonald and Ramscar, 2001).Semantic spaces are attractive because they provide amodel of word meaning that is independent of dictio-nary senses and their much-discussed problems (Kil-garriff, 1997; McCarthy and Navigli, 2007).In a default semantic space as described above,each vector represents one lemma, averaging overall its possible usages (Landauer and Dumais, 1997;Lund and Burgess, 1996).
Since the meaning ofwords can vary substantially between occurrences(e.g., for polysemous words), the next necessary stepis to characterize the meaning of individual words incontext.There have been several approaches in the liter-ature (Smolensky, 1990; Schu?tze, 1998; Kintsch,2001; McDonald and Brew, 2004; Mitchell and La-pata, 2008) that compute meaning in context fromlemma vectors.
Most of these studies phrase the prob-lem as one of vector composition: The meaning of atarget occurrence a in context b is a single new vectorc that is a function (for example, the centroid) of thevectors: c = a b.The context b can consist of as little as one word,as shown in Example (1).
In (1a), the meaning ofcatch combined with ball is similar to grab, while in(1b), combined with disease, it can be paraphrasedby contract.
Conversely, verbs can influence the in-terpretation of nouns: In (1a), ball is understood as aspherical object, and in (1c) as a dancing event.
(1) a. catch a ballb.
catch a diseasec.
attend a ballIn this paper, we argue that models of word mean-ing relying on this procedure of vector compositionare limited both in their scope and scalability.
Theunderlying shortcoming is a failure to consider syntaxin two important ways.The syntactic relation is ignored.
The first problemconcerns the manner of vector composition, whichignores the relation between the target a and its con-text b.
This relation can have a decisive influence ontheir interpretation, as Example (2) shows:897(2) a. a horse drawsb.
draw a horseIn (2a), the meaning of the verb draw can be para-phrased as pull, while in (2b) it is similar to sketch.This difference in meaning is due to the difference inrelation: in (2a), horse is the subject, while in (2b)it is the object.
On the modeling side, however, avector combination function that ignores the relationwill assign the same representation to (2a) and (2b).Thus, existing models are systematically unable tocapture this class of phenomena.Single vectors are too weak to represent phrases.The second problem arises in the context of the im-portant open question of how semantic spaces can?scale up?
to provide interesting meaning representa-tions for entire sentences.
We believe that the currentvector composition methods, which result in a singlevector c, are not informative enough for this purpose.One proposal for ?scaling up?
is to straightforwardlyinterpret c = a  b as the meaning of the phrasea + b (Kintsch, 2001; Mitchell and Lapata, 2008).The problem is that the vector c can only encode afixed amount of structural information if its dimen-sionality is fixed, but there is no upper limit on sen-tence length, and hence on the amount of structureto be encoded.
It is difficult to conceive how c couldencode deeper semantic properties, like predicate-argument structure (distinguishing ?dog bites man?and ?man bites dog?
), that are crucial for sentence-level semantic tasks such as the recognition of textualentailment (Dagan et al, 2006).
An alternative ap-proach to sentence meaning would be to use the vec-tor space representation only for representing wordmeaning, and to represent sentence structure sepa-rately.
Unfortunately, present models cannot providethis grounding either, since they compute a singlevector c that provides the same representations forboth the meanings of a and b in context.In this paper, we propose a new, structured vectorspace model for word meaning (SVS) that addressesthese problems.
A SVS representation of a lemmacomprises several vectors representing the word?slexical meaning as well as the selectional preferencesthat it has for its argument positions.
The meaningof word a in context b is computed by combining awith b?s selectional preference vector specific to therelation between a and b, addressing the first problemabove.
In an expression a + b, the meanings of aand b in this context are computed as two separatevectors a?
and b?.
These vectors can then be combinedwith a representation of the structure?s expression(e.g., a parse tree), to address the second problemdiscussed above.
We test the SVS model on the taskof recognizing contextually appropriate paraphrases,finding that SVS performs at and above the state-of-the-art.Plan of the paper.
Section 2 reviews related work.Section 3 presents the SVS model for word meaningin context.
Sections 4 to 6 relate experiments on theparaphrase appropriateness task.2 Related WorkIn this section we give a short overview over existingvector space based approaches to computing wordmeaning in context.General context effects.
The first category ofmodels aims at integrating the widest possible rangeof context information without recourse to linguisticstructure.
The best-known work in this category isSchu?tze (1998).
He first computes ?first-order?
vec-tor representations for word meaning by collectingco-occurrence counts from the entire corpus.
Then,he determines ?second-order?
vectors for individualword instances in their context, which is taken to be asimple surface window, by summing up all first-ordervectors of the words in this context.
The resultingvectors form sense clusters.McDonald and Brew (2004) present a similarmodel.
They compute the expectation for a wordwi in a sequence by summing the first-order vectorsfor the words w1 to wi?1 and showed that the dis-tance between expectation and first-order vector forwi correlates with human reading times.Predicate-argument combination.
The secondcategory of prior studies concentrates on contextsconsisting of a single word only, typically modelingthe combination of a predicate p and an argument a.Kintsch (2001) uses vector representations of p anda to identify the set of words that are similar to bothp and a.
After this set has been narrowed down in aself-inhibitory network, the meaning of the predicate-argument combination is obtained by computing the898centroid of its members?
vectors.
The procedure doesnot take the relation between p and a into account.Mitchell and Lapata (2008) propose a frameworkto represent the meaning of the combination p+ a asa function f operating on four components:c = f(p, a,R,K) (3)R is the relation holding between p and a, and Kadditional knowledge.
This framework allows sen-sitivity to the relation.
However, the concrete in-stantiations that Mitchell and Lapata consider disre-gards K and R, thus sharing the other models?
limi-tations.
They focus instead on methods for the directcombination of p and a: In a comparison betweencomponent-wise addition and multiplication of p anda, they find far superior results for the multiplicationapproach.Tensor product-basedmodels.
Smolensky (1990)uses tensor product to combine two word vectors aand b into a vector c representing the expression a+b.The vector c is located in a very high-dimensionalspace and is thus capable of encoding the structureof the expression; however, this makes the modelinfeasible in practice, as dimensionality rises withevery word added to the representation.
Jones andMewhort (2007) represent lemma meaning by usingcircular convolution to encode n-gram co-occurrenceinformation into vectors of fixed dimensionality.
Sim-ilar to Brew and McDonald (2004), they predict mostlikely next words in a sequence, without taking syn-tax into account.Kernel methods.
One of the main tests for thequality of models of word meaning in context is theability to predict the appropriateness of paraphrasesin given a context.
Typically, a paraphrase appliesonly to some senses of a word, not all, as can be seenin the paraphrases ?grab?
and ?contract?
of ?catch?.Vector space models generally predict paraphrase ap-propriateness based on the similarity between vectors.This task can also be addressed with kernel methods,which project items into an implicit feature spacefor efficient similarity computation.
Consequently,vector space methods and kernel methods have bothbeen used for NLP tasks based on similarity, no-tably Information Retrieval and Textual Entailment.Nevertheless, they place their emphasis on differenttypes of information.
Current kernels are mostly treekernels that compare syntactic structure, and use se-mantic information mostly for smoothing syntacticsimilarity (Moschitti and Quarteroni, 2008).
In con-trast, vector-space models focus on the interactionbetween the lexical meaning of words in composi-tion.3 A structured vector space model forword meaning in contextIn this section, we define the structured vector space(SVS) model of word meaning.The main intuition behind our model is to viewthe interpretation of a word in context as guided byexpectations about typical events.
For example, in(1a), we assume that upon hearing the phrase ?catch aball?, the hearer will interpret the meaning of ?catch?to match typical actions that can be performed with aball.
Similarly, the interpretation of ?ball?
will reflectthe hearer?s expectations about typical things that canbe caught.
This move to include typical argumentsand predicates into a model of word meaning can bemotivated both on cognitive and linguistic grounds.In cognitive science, the central role of expecta-tions about typical events for human language pro-cessing is well-established.
Expectations affect read-ing times (McRae et al, 1998), the interpretation ofparticiples (Ferretti et al, 2003), and sentence pro-cessing generally (Narayanan and Jurafsky, 2002;Pado?
et al, 2006).
Expectations exist both for verbsand nouns (McRae et al, 1998; McRae et al, 2005).In linguistics, expectations, in the form of selec-tional restrictions and selectional preferences, havelong been used in semantic theories (Katz and Fodor,1964; Wilks, 1975), and more recently inducedfrom corpora (Resnik, 1996; Brockmann and Lapata,2003).
Attention has mostly been limited to selec-tional preferences of verbs, which have been usedfor example for syntactic disambiguation (Hindleand Rooth, 1993), word sense disambiguation (Mc-Carthy and Carroll, 2003) and semantic role label-ing (Gildea and Jurafsky, 2002).
Recently, a vector-spaced model of selectional preferences has beenproposed that computes the typicality of an argumentsimply through similarity to previously seen argu-ments (Erk, 2007; Pado?
et al, 2007).We first present the SVS model of word meaning899catchhefielderdogcoldbaseballdriftobjsubjaccusesayclaimcomp-1ballwhirlflyprovidethrowcatchorganiseobj-1subj-1modredgolfelegantFigure 1: Structured meaning representations for nounball and verb catch : lexical information plus expectationsthat integrates lexical information with selectionalpreferences.
Then, we show how the SVS model pro-vides a new way of computing meaning in context.Representing lemma meaning.
We abandon thetraditional choice of representing word meaning asa single vector.
Instead, we encode each word asa combination of (a) one vector that models thelexical meaning of the word, and (b) a set of vec-tors, each of which represents the semantic expecta-tions/selectional preferences for one particular rela-tion that the word supports.1The idea is illustrated in Fig.
1.
In the representa-tion of the verb catch, the central square stands forthe lexical vector of catch itself.
The three arrowslink it to catch ?s preferences for its subjects (subj),its objects (obj), and for verbs for which it appearsas a complement (comp?1).
The figure shows the se-lectional preferences as word lists for readability; inpractice, each selectional preference is a single vector(cf.
Section 4).
Likewise, ball is represented by onevector for ball itself, one for ball ?s preferences for itsmodifiers (mod), one vector for the verbs of which itis a subject (subj?1), and one for the verbs of whichis an object (obj?1).This representation includes selectional prefer-ences (like subj, obj, mod) exactly parallel toinverse selectional preferences (subj?1, obj?1,comp?1).
To our knowledge, preferences of the lat-ter kind have not been studied in computational lin-guistics.
However, their existence is supported inpsycholinguistics by priming effects from nouns totypical verbs (McRae et al, 2005).Formally, let D be a vector space (the set of possi-1We do not commit to a particular set of relations; see thediscussion at the end of this section.catch...coldbaseballdriftobjsubj...comp-1ball...throwcatchorganiseobj-1subj-1mod...!
!Figure 2: Combining predicate and argument via relation-specific semantic expectationsble vectors), and let R be some set of relation labels.In the structured vector space (SVS) model, we rep-resent the meaning of a lemma w as a triplew = (v,R,R?1)where v ?
D is a lexical vector describing the wordw itself, R : R ?
D maps each relation label ontoa vector that describes w?s selectional preferences,and R?1 : R ?
D maps from role labels to vec-tors describing inverse selectional preferences of w.Both R and R?1 are partial functions.
For example,the direct object preference would be undefined forintransitive verbs.Computing meaning in context.
The SVS modelof lemma meaning permits us to compute the mean-ing of a word a in the context of another word bin a new way, via their selectional preferences.
Let(va, Ra, R?1a ) and (vb, Rb, R?1b ) be the representa-tions of the two words, and let r ?
R be the relationlinking a to b.
Then, we define the meaning of a andb in this context as a pair (a?, b?)
of vectors, wherea?
is the meaning of a in the context of b, and b?
themeaning of b in the context of a:a?
=(va R?1b (r), Ra ?
{r}, R?1a)b?
=(vb Ra(r), Rb, R?1b ?
{r}) (4)where v1 v2 is a direct vector combination functionas in traditional models, e.g.
addition or component-wise multiplication.
If either Ra(r) or R?1b (r) arenot defined, the combination fails.
Afterwards, the ar-gument position r is considered filled, and is deletedfrom Ra and R?1b .900Figure 2 illustrates this procedure on the represen-tations from Figure 1.
The dotted lines indicate thatthe lexical vector for catch is combined with the in-verse object preference of ball.
Likewise, the lexicalvector for ball is combined with the object preferencevector of catch.Note that our procedure for computing meaningin context can be expressed within the framework ofMitchell and Lapata (Eq.
(3)).
We can encode theexpectations of a and b as additional knowledge K.The combined representation c is the pair (a?, b?)
thatis computed according to our model (Eq.
(4)).The SVS scheme we have proposed incorporatessyntactic information in a more general manner thanprevious models, and thus addresses the issues wehave discussed in Section 1.
Since the representationretains individual selectional preferences for all rela-tions, combining the same words through differentrelations can (and will in general) result in differentadapted representations.
For instance, in the case ofExample (2), we would expect the inverse subjectpreference of horse (?things that a horse typicallydoes?)
to push the lexical vector of draw into the di-rection of pulling, while its inverse object preference(?things that are done to horses?)
suggest a differentinterpretation.Rather than yielding a single, joint vector for thewhole expression, our procedure for computing mean-ing in context results in one context-adapted meaningrepresentation per word, similar to the output of aWSD system.
As a consequence, our model canbe combined with any formalism representing thestructure of an expression.
(The formalism used thendetermines the set R of relations.)
For example, com-bining SVS with a dependency tree would yield a treein which each node is labeled by a SVS tuple thatrepresents the word?s meaning in context.4 Experimental setupThis section provides the background to the followingexperimental evaluation of SVS, including parametersused for computing the SVS representations that willbe used in the experiments.4.1 Experimental rationaleIn this paper, we evaluate the SVS model against thetask of predicting, given a predicate-argument pair,how appropriate a paraphrase (of either the predicateor the argument) is in that context.
We perform twoexperiments that both use the paraphrase task, butdiffer in their emphasis.
Experiment 1 replicates anexisting evaluation against human judgments.
Thisevaluation uses synthetic dataset, limited to one par-ticular construction, and constructed to provide max-imally distinct paraphrase candidates.
Experiment 2considers a broader class of constructions along withannotator-generated paraphrase candidates that arenot screened for distinctness.
In both experiments,we compare the SVS model against the state-of-the-art model by Mitchell and Lapata 2008 (henceforthM&L; cf.
Sec.
2 for model details).4.2 Parameter choicesVector space.
In our parameterization of the vectorspace, we largely follow M&L because their modelhas been rigorously evaluated and found to outper-form a range of other models.Our first space is a traditional ?bag-of-words?
vec-tor space (BOW, (Lund and Burgess, 1996)).
Foreach pair of a target word and context word, the BOWspace records a function of their co-occurrence fre-quency within a surface window of size 10.
Thespace is constructed from the British National Cor-pus (BNC), and uses the 2,000 most frequent contextwords as dimensions.We also consider a ?dependency-based?
vectorspace (SYN, (Pado?
and Lapata, 2007)).
In this space,target and context words have to be linked by a ?valid?dependency path in a dependency graph to count asco-occurring.2 This space was built from BNC de-pendency parses obtained from Minipar (Lin, 1993).For both spaces, we used pre-experiments to com-pare two methods for the computation of vector com-ponents, namely raw co-occurrence counts, the stan-dard model, and the pointwise mutual information(PMI) definition employed by M&L.Selectional preferences.
We use a simple,knowledge-lean representation for selectionalpreferences inspired by Erk (2007), who modelsselectional preference through similarity to seen fillervectors ~va: We compute the selectional preferencevector for word b and relation r as the weighted2More specifically, we used the minimal context specificationand plain weight function.
See Pado?
and Lapata (2007).901centroid of seen filler vectors ~va.
We collect seenfillers from the Minipar-parse of the BNC.Let f(a, r, b) denote the frequency of a occurringin relation r to b in the parsed BNC, thenRb(r)SELPREF =?a:f(a,r,b)>0f(a, r, b) ?
~va (5)We call this base model SELPREF.
We will alsostudy two variants of SELPREF, based on two dif-ferent hypotheses about what properties of the se-lectional preferences are particularly important formeaning adaption.
The first model aims specificallyat alleviating noise introduced by infrequent fillers, acommon problem in data-driven approaches.
It onlyuses fillers seen more often than a threshold ?.
Wecall this model SELPREF-CUT:Rb(r)SELPREF-CUT =?a:f(a,r,b)>?f(a, r, b) ?
~va (6)Our second variant again aims at alleviating noise,but noise introduced by low-valued dimensions ratherthan infrequent fillers.
It achieves this by taking eachcomponent of the selectional preference vector tothe nth power.
In this manner, dimensions with highcounts are further inflated, while dimensions with lowcounts are depressed.3 This model, SELPREF-POW, isdefined as follows: If Rb(r)SELPREF = ?v1, .
.
.
, vm?,Rb(r)SELPREF-POW = ?vn1 , .
.
.
, vnm?
(7)The inverse selectional preferences R?1b are de-fined analogously for all three model variants.
Weinstantiate the vector combination function  ascomponent-wise multiplication, following M&L.Baselines and significance testing.
All tasks thatwe consider below involve judgments for the mean-ing of a word a in the context of a word b.
A firstbaseline that every model must beat is simply usingthe original vector for a.
We call this baseline ?targetonly?.
Since we assume that the selectional prefer-ences of b model the expectations for a, we use b?sselectional preference vector for the given relation asa second baseline, ?selpref only?.3Since we focus on the size-invariant cosine similarity, theuse of this model does not require normalization.verb subject landmark sim judgmentslump shoulder slouch high 7slump shoulder decline low 2slump value slouch low 3slump value decline high 7Figure 3: Experiment 1: Human similarity judgements forsubject-verb pair with high- and low-similarity landmarksDifferences between the performance of mod-els were tested for significance using a stratifiedshuffling-based randomization test (Yeh, 2000).4.5 Exp.
1: Predicting similarity ratingsIn our first experiment, we attempt to predict humansimilarity judgments.
This experiment is a replicationof the evaluation of M&L on their dataset5.Dataset.
The M&L dataset comprises a total of3,600 human similarity judgements for 120 experi-mental items.
Each item, as shown in Figure 3, con-sists of an intransitive verb and a subject noun thatare combined with a ?landmark?, a synonym of theverb that is chosen to be either similar or dissimilarto the verb in the context of the given subject.The dataset was constructed by extracting pairsof subjects and intransitive verbs from a parsed ver-sion of the BNC.
Each item was paired with twolandmarks, chosen to be as dissimilar as possible ac-cording to a WordNet similarity measure.
All nounsand verbs were subjected to a pretest, where onlythose with highly significant variations in humanjudgments across landmarks were retained.For each item of the final dataset, judgements ona 7-point scale were elicited.
For example, judgesconsidered the compatible landmark ?slouch?
to bemuch more similar to ?shoulder slumps?
than theincompatible landmark ?decline?.
In Figure 3, thecolumn sim shows whether the experiment designersconsidered the respective landmark to have high orlow similarity to the verb, and the column judgmentshows a participant?s judgments.Experimental procedure.
We used cosine to com-pute similarity to the lexical vector of the landmark.4The software is available at http://www.nlpado.de/?sebastian/sigf.html.5We thank J. Mitchell and M. Lapata for providing their data.902Model high low ?BOW spaceTarget only 0.32 0.32 0.0Selpref only 0.46 0.4 0.06**M&L 0.25 0.15 0.20**SELPREF 0.32 0.26 0.12**SELPREF-CUT, ?=10 0.31 0.24 0.11**SELPREF-POW, n=20 0.11 0.03 0.27**Upper bound ?
?
0.4SYN spaceTarget only 0.2 0.2 0.08**Selpref only 0.27 0.21 0.16**M&L 0.13 0.06 0.24**SELPREF 0.22 0.16 0.13**SELPREF-CUT, ?=10 0.2 0.13 0.13**SELPREF-POW, n=30 0.08 0.04 0.22**Upper bound ?
?
0.4Table 1: Experiment 1: Mean cosine similarity for itemswith high- and low-similarity landmarks; correlation withhuman judgements (?).
(**: p < 0.01)?Target only?
compares the landmark against the lexi-cal vector of the verb, and ?selpref only?
comparesit to the noun?s subj?1 preference.
For the M&Lmodel, the comparison is to the combined lexicalvectors of verb and noun.
For our models SELPREF,SELPREF-CUT and SELPREF-POW, we combine theverb?s lexical vector with the subj?1 preference ofthe noun.
We used a held-out dataset of 10% of thedata to optimize the parameters of ?
of SELPREF-CUTand n of SELPREF-POW.
Vectors with PMI compo-nents could model the data, while raw frequencycomponents could not; we report only the former.We use the same two evaluation scores as M&L:The first score is the average similarity to compatiblelandmarks (high) and incompatible landmarks (low).The second is Spearman?s ?, a nonparametric corre-lation coefficient.
We compute ?
between individualhuman similarity scores and our predictions.
Basedon agreement between human judges, M&L estimatean upper bound ?
of 0.4 for the dataset.Results and discussion.
Table 1 shows the resultsof Exp.
1 on the test set.
In the upper half (BOW), wereplicate M&L?s main finding that simple component-wise multiplication of the predicate and argumentvectors results in a highly significant correlation ofModel lex.
vector obj?1 selprefSELPREF 0.23 (0.09) 0.88 (0.07)SELPREF-CUT (10) 0.20 (0.10) 0.72 (0.18)SELPREF-POW (30) 0.03 (0.08) 0.52 (0.48)Table 2: Experiment 1: Average similarity (and standarddeviation) between the inverse subject preferences of anoun and (left) its lexical vector and (right) inverse objectpreferences vector (cosine similarity in SYN space)?
= 0.2, significantly outperforming both baselines.It is interesting, though, that the subj?1 preferenceitself (?Selpref only?)
is already highly significantlycorrelated with the human judgments.A comparison of the upper half (BOW) with thelower half (SYN) shows that the dependency-basedspace generally shows better correlation with humanjudgements.
This corresponds to a beneficial effect ofsyntactic information found for other applications ofsemantic spaces (Lin, 1998; Pado?
and Lapata, 2007).All instances of the SELPREF model show highlysignificant correlations.
SELPREF and SELPREF-CUTshow very similar performance.
They do better thanboth baselines in the BOW space; however, in thecleaner SYN space, their performance is numericallylower than using selectional preferences only (?
=0.13 vs. 0.16).
SELPREF-POW is always significantlybetter than SELPREF and SELPREF-CUT, and showsthe best result of all tested models (?
= 0.27, BOWspace).
The performance is somewhat lower in theSYN space (?
= 0.22).
However, this difference, andthe difference to the best M&L model at ?
= 0.24,are not statistically significant.The SVS model computes meaning in context bycombining a word?s lexical representation with thepreference vector of its context.
In this, it differs fromprevious models, including that by M&L, which usedwhat we have been calling ?direct combination?.
Soit is important to ask to what extent this differencein method translate to a difference in predictions.We analyzed this by measuring the similarity by thenouns?
lexical vectors, used by direct combinationmethods, and their inverse subject preferences, whichSVS uses.
The result is shown in the first columnin Table 2, computed as mean cosine similaritiesand standard deviations between noun vectors andselectional preferences.
The table shows that thesevectors have generally low similarity, which is further903reduced by applying cutoff and potentiation.
Thus,the predictions of SVS will differ from those of directcombination models like M&L.A related question is whether syntax-aware vec-tor combination makes a difference: Does the modelencode different expectations for different syntacticrelations (cf.
Example 2)?
The second column of Ta-ble 2 explores this question by comparing inverse se-lectional preferences for the subject and object slots.We observe that the similarity is very high for rawpreferences, but becomes lower when noise is elim-inated.
Since the SELPREF-POW model performedbest in our evaluation, we read this as evidence thatpotentiation helps to suppress noise introduced bymis-identified subject and object fillers.In Experiment 1, all experimental items wereverbs, which means that all disambiguation was donethrough inverse selectional preferences.
As inverseselectional preferences are currently largely unex-plored, it is interesting to note that the evidence thatthey provide for the paraphrase task is as strong asthat of the context nouns themselves.6 Exp.
2: Ranking paraphrasesThis section reports on a second, more NLP-orientedexperiment whose task is to distinguish between ap-propriate and inappropriate paraphrases on a broaderrange of constructions.Dataset.
For this experiment, we use the SemEval-1 lexical substitution (lexsub) dataset (McCarthy andNavigli, 2007), which contains 10 instances each of200 target words in sentential contexts, drawn fromSharoff?s (2006) English Internet Corpus.
Contex-tually appropriate paraphrases for each instance ofeach target word were elicited from up to 6 partic-ipants.
Fig.
4 shows two instances for the verb towork.
The distribution over paraphrases can be seenas a characterization of the target word?s meaning ineach context.Experimental procedure.
In this paper, we pre-dict appropriate paraphrases solely on the basis of asingle context word that stands in a direct predicate-argument relation to the target word.
We extractedall instances from the lexsub test data with such arelation.
After parsing all sentences with verbal andnominal targets with Minipar, this resulted in threeSentence SubstitutesBy asking people who workthere, I have since determinedthat he didn?t.
(# 2002)be employed 4;labour 1Remember how hard your an-cestors worked.
(# 2005)toil 4; labour 3;task 1Figure 4: Lexical substitution example items for ?work?sets of sentences: (a), target intransitive verbs withnoun subjects (V-SUBJ, 48 sentences); (b), target tran-sitive verbs with noun objects (V-OBJ, 213 sent.
); and(c), target nouns occurring as objects of verbs (N-OBJ,102 sent.
).6 Note that since we use only part of thelexical substitution dataset in this experiment, a di-rect comparison with results from the SemEval taskis not possible.As in the original SemEval task, we phrase thetask as a ranking problem.
For each target word, theparaphrases given for all 10 instances are pooled.
Thetask is to rank the list for each item so that appropriateparaphrases (such as ?be employed?
for # 2002) rankhigher than paraphrases not given (e.g., ?toil?
).Our model ranks paraphrases by their similarityto the following combinations (Eq.
(4)): for V-SUBJ,verb plus the noun?s subj?1 preferences; for V-OBJ,verb plus the noun?s obj?1 preferences; and for N-OBJ, the noun plus the verb?s obj preferences.
Ourcomparison model, M&L, ranks all paraphrases bytheir similarity to the direct noun-verb combination.To avoid overfitting, we consider only the two mod-els that performed optimally in in the SYN space inExperiment 1 (SELPREF-POW with n=30 and M&L).However, since we found that vectors with raw fre-quency components could model the data, while PMIcomponents could not, we only report the former.For evaluation, we adopt the SemEval ?out often?
precision metric POOT.
It uses the model?s tentop-ranked paraphrases as its guesses for appropri-ate paraphrases.
Let Gi be the gold paraphrases foritem i, Mi the model?s top ten paraphrases for i, andf(s, i) the frequency of s as paraphrase for i:POOT = 1/|I|?i?s?Mi?Gif(s, i)?s?Gif(s, i)(8)McCarthy and Navigli propose this metric for the6The specification of this dataset will be made available.904Model V-SUBJ V-OBJ N-OBJTarget only 47.9 47.4 49.6Selpref only 54.8 51.4 55.0M&L 50.3 52.0 53.4SELPREF-POW, n=30 63.1 55.8 56.9Table 3: Experiment 2: Mean ?out of ten?
precision (POOT)dataset for robustness.
Due to the sparsity of para-phrases, a metric that considers fewer guesses leadsto artificially low results when a ?good?
paraphrasewas not mentioned by the annotators by chance butis ranked highly by a model.Results and discussion.
Table 6 shows the meanout-of-ten precision for all models.
The behavior isfairly uniform across all three datasets.
Unsurpris-ingly, ?target only?, which uses the same ranking forall instances of a target, yields the worst results.7M&L?s direct combination model outperforms ?tar-get only?
significantly (p < 0.05).
However, on boththe V-SUBJ and the N-OBJ the ?selpref only?
baselinedoes better than direct combination.
The best resultson all datasets are obtained by SELPREF-POW.
Thedifference between SELPREF-POW and the ?targetonly?
baseline is highly significant (p < 0.01).
Thedifference to M&L?s model is significant at p = 0.05.We interpret these results as encouraging evidencefor the usefulness of selectional preferences for judg-ing substitutability in context.
Knowledge about theselectional preferences of a single context word canalready lead to a significant improvement in precision.We find this overall effect even though the word isnot informative in all cases.
For instance, the subjectof item 2002 in Fig.
4, ?who?, presumably helps littlein determining the verb?s context-adapted meaning.It is interesting that the improvement of SELPREF-POW over ?selpref only?
is smallest for the N-OBJdataset (1.9% POOT).
N-OBJ uses selectional prefer-ences for nouns that may fill the direct object position,, while V-SUBJ and V-OBJ use inverse selectionalpreferences for verbs (cf.
the two graphs in Fig.
1).7?Target only?
still does very much better than a randombaseline, which performs at 22% POOT.7 ConclusionIn this paper, we have considered semantic spacemodels that can account for the meaning of wordoccurrences in context.
Arguing that existing modelsdo not sufficiently take syntax into account, we haveintroduced the new structured vector space (SVS)model of word meaning.
In addition to a vector rep-resenting a word?s lexical meaning, it contains vec-tors representing the word?s selectional preferences.These selectional preferences play a central role inthe computation of meaning in context.We have evaluated the SVS model on two datasetson the task of predicting the felicitousness of para-phrases in given contexts.
On the M&L dataset,SVS outperforms the state-of-the-art model of M&L,though the difference is not significant.
On the Lex-ical Substitution dataset, SVS significantly outper-forms the state-of-the-art.
This is especially interest-ing as the Lexical Substitution dataset, in contrast tothe M&L data, uses ?realistic?
paraphrase candidatesthat are not necessarily maximally distinct.The most important limitation of the evaluationthat we have given in this paper is that we have onlyconsidered single words as context.
Our next stepwill be to integrate information from multiple rela-tions (such as both the subject and object positionsof a verb) into the computation of context-specificmeaning.
Our eventual aim is a model that can givea compositional account of a word?s meaning in con-text, where all words in an expression disambiguateone another according to the relations between them.We will explore the usability of vector space mod-els of word meaning in NLP applications, formulatedas the question of how to perform inferences on themin the context of the Textual Entailment task (Daganet al, 2006).
Paraphrase-based inference rules playa large role in several recent approaches to TextualEntailment (e.g.
Szpektor et al(2008)); appropriate-ness judgments of paraphrases in context, the task ofExperiments 1 and 2 above, can be viewed as testingthe applicability of these inferences rules.Acknowledgments.
Many thanks for helpful dis-cussion to Jason Baldridge, David Beaver, DedreGentner, James Hampton, Dan Jurafsky, AlexanderKoller, Brad Love, and Ray Mooney.905ReferencesC.
Brockmann, M. Lapata.
2003.
Evaluating and combin-ing approaches to selectional preference acquisition.
InProceedings of EACL, 27?34.I.
Dagan, O. Glickman, B. Magnini.
2006.
The PASCALRecognising Textual Entailment Challenge.
In Ma-chine Learning Challenges, Lecture Notes in ComputerScience, 177?190.
Springer.K.
Erk.
2007.
A simple, similarity-based model for selec-tional preferences.
In Proceedings of ACL, 216?223.T.
Ferretti, C.
Gagne?, K. McRae.
2003.
Thematic role fo-cusing by participle inflections: evidence form concep-tual combination.
Journal of Experimental Psychology,29(1):118?127.D.
Gildea, D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.D.
Hindle, M. Rooth.
1993.
Structural ambiguity andlexical relations.
Computational Linguistics, 19(1):103?120.M.
Jones, D. Mewhort.
2007.
Representing word mean-ing and order information in a composite holographiclexicon.
Psychological review, 114:1?37.J.
J. Katz, J.
A. Fodor.
1964.
The structure of a semantictheory.
In The Structure of Language.
Prentice-Hall.A.
Kilgarriff.
1997.
I don?t believe in word senses.
Com-puters and the Humanities, 31(2):91?113.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.T.
Landauer, S. Dumais.
1997.
A solution to Platos prob-lem: the latent semantic analysis theory of acquisition,induction, and representation of knowledge.
Psycho-logical Review, 104(2):211?240.D.
Lin.
1993.
Principle-based parsing without overgener-ation.
In Proceedings of ACL, 112?120.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING-ACL, 768?774.K.
Lund, C. Burgess.
1996.
Producing high-dimensionalsemantic spaces from lexical co-occurrence.
Behav-ior Research Methods, Instruments, and Computers,28:203?208.C.
D. Manning, P. Raghavan, H. Schu?tze.
2008.
Introduc-tion to Information Retrieval.
CUP.D.
McCarthy, J. Carroll.
2003.
Disambiguating nouns,verbs, and adjectives using automatically acquiredselectional preferences.
Computational Linguistics,29(4):639?654.D.
McCarthy, R. Navigli.
2007.
SemEval-2007 Task 10:English Lexical Substitution Task.
In Proceedings ofSemEval, 48?53.S.
McDonald, C. Brew.
2004.
A distributional modelof semantic context effects in lexical processing.
InProceedings of ACL, 17?24.S.
McDonald, M. Ramscar.
2001.
Testing the distribu-tional hypothesis: The influence of context on judge-ments of semantic similarity.
In Proceedings of CogSci,611?616.K.
McRae, M. Spivey-Knowlton, M. Tanenhaus.
1998.Modeling the influence of thematic fit (and other con-straints) in on-line sentence comprehension.
Journal ofMemory and Language, 38:283?312.K.
McRae, M. Hare, J. Elman, T. Ferretti.
2005.
Abasis for generating expectancies for verbs from nouns.Memory and Cognition, 33(7):1174?1184.J.
Mitchell, M. Lapata.
2008.
Vector-based models ofsemantic composition.
In Proceedings of ACL, 236?244.A.
Moschitti, S. Quarteroni.
2008.
Kernels on linguisticstructures for answer extraction.
In Proceedings ofACL, 113?116, Columbus, OH.S.
Narayanan, D. Jurafsky.
2002.
A Bayesian modelpredicts human parse preference and reading time insentence processing.
In Proceedings of NIPS, 59?65.S.
Pado?, M. Lapata.
2007.
Dependency-based construc-tion of semantic space models.
Computational Linguis-tics, 33(2):161?199.U.
Pado?, F. Keller, M. W. Crocker.
2006.
Combining syn-tax and thematic fit in a probabilistic model of sentenceprocessing.
In Proceedings of CogSci, 657?662.S.
Pado?, U.
Pado?, K. Erk.
2007.
Flexible, corpus-basedmodelling of human plausibility judgements.
In Pro-ceedings of EMNLP/CoNLL, 400?409.P.
Resnik.
1996.
Selectional constraints: An information-theoretic model and its computational realization.
Cog-nition, 61:127?159.G.
Salton, A. Wang, C. Yang.
1975.
A vector-space modelfor information retrieval.
Journal of the American So-ciety for Information Science, 18:613?620.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.S.
Sharoff.
2006.
Open-source corpora: Using the net tofish for linguistic data.
International Journal of CorpusLinguistics, 11(4):435?462.P.
Smolensky.
1990.
Tensor product variable binding andthe representation of symbolic structures in connection-ist systems.
Artificial Intelligence, 46:159?216.I.
Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger.
2008.Contextual preferences.
In Proceedings of ACL, 683?691, Columbus, OH.Y.
Wilks.
1975.
Preference semantics.
In Formal Seman-tics of Natural Language.
CUP.A.
Yeh.
2000.
More accurate tests for the statisticalsignificance of result differences.
In Proceeedings ofCOLING, 947?953.906
