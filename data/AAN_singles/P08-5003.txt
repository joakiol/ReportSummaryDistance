Tutorial Abstracts of ACL-08: HLT, page 3,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSemi-supervised Learning for Natural Language ProcessingJohn BlitzerNatural Language Computing GroupMicrosoft Research AsiaBeijing, Chinablitzer@cis.upenn.eduXiaojin Jerry ZhuDepartment of Computer ScienceUniversity of Wisconsin, MadisonMadison, WI, USAjerryzhu@cs.wisc.edu1 IntroductionThe amount of unlabeled linguistic data availableto us is much larger and growing much faster thanthe amount of labeled data.
Semi-supervised learn-ing algorithms combine unlabeled data with a smalllabeled training set to train better models.
Thistutorial emphasizes practical applications of semi-supervised learning; we treat semi-supervised learn-ing methods as tools for building effective modelsfrom limited training data.
An attendee will leaveour tutorial with1.
A basic knowledge of the most common classesof semi-supervised learning algorithms and wherethey have been used in NLP before.2.
The ability to decide which class will be usefulin her research.3.
Suggestions against potential pitfalls in semi-supervised learning.2 Content OverviewSelf-training methods Self-training methods usethe labeled data to train an initial model and thenuse that model to label the unlabeled data and re-train a new model.
We will examine in detail the co-training method of Blum and Mitchell [2], includ-ing the assumptions it makes, and two applicationsof co-training to NLP data.
Another popular self-training method treats the labels of the unlabeleddata as hidden and estimates a single model fromlabeled and unlabeled data.
We explore new meth-ods in this framework that make use of declarativelinguistic side information to constrain the solutionsfound using unlabeled data [3].Graph regularization methods Graph regulariza-tion methods build models based on a graph on in-stances, where edges in the graph indicate similarity.The regularization constraint is one of smoothnessalong this graph.
We wish to find models that per-form well on the training data, but we also regularizeso that unlabeled nodes which are similar accordingto the graph have similar labels.
For this section, wefocus in detail on the Gaussian fields method of Zhuet al [4].Structural learning Structural learning [1] uses un-labeled data to find a new, reduced-complexity hy-pothesis space by exploiting regularities in featurespace via unlabeled data.
If this new hypothesisspace still contains good hypotheses for our super-vised learning problem, we may achieve high accu-racy with much less training data.
The regularitieswe use come in the form of lexical features that func-tion similarly for prediction.
This section will fo-cus on the assumptions behind structural learning, aswell as applications to tagging and sentiment analy-sis.References[1] Rie Ando and Tong Zhang.
A Framework for Learn-ing Predictive Structures from Multiple Tasks and Unla-beled Data.
JMLR 2005.
[2] Avrim Blum and Tom Mitchell.
Combining Labeledand Unlabeled Data with Co-training.
COLT 1998.
[3] Aria Haghighi and Dan Klein.
Prototype-drivenLearning for Sequence Models.
HLT/NAACL 2006.
[4] Xiaojin Zhu, Zoubin Ghahramani, and John Laf-ferty.
Semi-supervised Learning using Gaussian Fieldsand Harmonic Functions.
ICML 2003.3
