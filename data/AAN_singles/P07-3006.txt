Proceedings of the ACL 2007 Student Research Workshop, pages 31?36,Prague, June 2007. c?2007 Association for Computational LinguisticsExploiting Structure for Event Discovery Using the MDI AlgorithmMartina NaughtonSchool of Computer Science & InformaticsUniversity College DublinIrelandmartina.naughton@ucd.ieAbstractEffectively identifying events in unstruc-tured text is a very difficult task.
This islargely due to the fact that an individualevent can be expressed by several sentences.In this paper, we investigate the use of clus-tering methods for the task of grouping thetext spans in a news article that refer to thesame event.
The key idea is to cluster thesentences, using a novel distance metric thatexploits regularities in the sequential struc-ture of events within a document.
Whenthis approach is compared to a simple bagof words baseline, a statistically significantincrease in performance is observed.1 IntroductionAccurately identifying events in unstructured text isan important goal for many applications that requirenatural language understanding.
There has been anincreased focus on this problem in recent years.
TheAutomatic Content Extraction (ACE) program1 isdedicated to developing methods that automaticallyinfer meaning from language data.
Tasks includethe detection and characterisation of Entities, Rela-tions, and Events.
Extensive research has been ded-icated to entity recognition and binary relation de-tection with significant results (Bikel et al, 1999).However, event extraction is still considered as oneof the most challenging tasks because an individualevent can be expressed by several sentences (Xu etal., 2006).In this paper, we primarily focus on techniquesfor identifying events within a given news article.Specifically, we describe and evaluate clustering1http://www.nist.gov/speech/tests/ace/methods for the task of grouping sentences in a newsarticle that refer to the same event.
We generatesentence clusters using three variations of the well-documented Hierarchical Agglomerative Clustering(HAC) (Manning and Schu?tze, 1999) as a baselinefor this task.
We provide convincing evidence sug-gesting that inherent structures exist in the manner inwhich events appear in documents.
In Section 3.1,we present an algorithm which uses such structuresduring the clustering process and as a result a mod-est increase in accuracy is observed.Developing methods capable of identifying alltypes of events from free text is challenging for sev-eral reasons.
Firstly, different applications considerdifferent types of events and with different levels ofgranularity.
A change in state, a horse winning arace and the race meeting itself can be consideredas events.
Secondly, interpretation of events can besubjective.
How people understand an event can de-pend on their knowledge and perspectives.
There-fore in this current work, the type of event to extractis known in advance.
As a detailed case study, weinvestigate event discovery using a corpus of newsarticles relating to the recent Iraqi War where the tar-get event is the ?Death?
event type.
Figure 1 showsa sample article depicting such events.The remainder of this paper is organised as fol-lows: We begin with a brief discussion of relatedwork in Section 2.
We describe our approach toEvent Discovery in Section 3.
Our techniques areexperimentally evaluated in Section 4.
Finally, weconclude with a discussion of experimental observa-tions and opportunities for future work in Section 5.2 Related ResearchThe aim of Event Extraction is to identify any in-stance of a particular class of events in a natural31World NewsInsurgents Kill 17 in IraqIn Tikrit, gunmen killed 17 Iraqis as they were heading to work Sunday at a U.S. military facility.Capt.
Bill Coppernoll, said insurgents fired at several buses of Iraqis from two cars.. .
.
.
.
.
.
.
.
.
.
.
.
.
.Elsewhere, an explosion at a market in Baqubah, about 30 miles north of Baghdad late Thursday.The market was struck by mortar bombs according to U.S. military spokesman Sgt.
Danny Martin.. .
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 1: Sample news article that describes multiple events.language text, extract the relevant arguments of theevent, and represent the extracted information intoa structured form (Grishman, 1997).
The types ofevents to extract are known in advance.
For exam-ple, ?Attack?
and ?Death?
are possible event typesto be extracted.
Previous work in this area focusesmainly on linguistic and statistical methods to ex-tract the relevant arguments of a event type.
Lin-guistic methods attempt to capture linguists knowl-edge in determining constraints for syntax, mor-phology and the disambiguation of both.
Statisticalmethods generate models based in the internal struc-tures of sentences, usually identifying dependencystructures using an already annotated corpus of sen-tences.
However, since an event can be expressedby several sentences, our approach to event extrac-tion is as follows: First, identify all the sentences ina document that refer to the event in question.
Sec-ond, extract event arguments from these sentencesand finally represent the extracted information of theevent in a structured form.Particularly, in this paper we focus on clusteringmethods for grouping sentences in an article that dis-cuss the same event.
The task of clustering simi-lar sentences is a problem that has been investigatedparticularly in the area of text summarisation.
InSimFinder (Hatzivassiloglou et al, 2001), a flexibleclustering tool for summarisation, the task is definedas finding text units (sentences or paragraphs) thatcontain information about a specific subject.
How-ever, the text features used in their similarity metricare selected using a Machine Learning model.3 Identifying Events within ArticlesWe treat the task of grouping together sentences thatrefer to the same event(s) as a clustering problem.As a baseline, we generate sentence clusters us-ing average-link, single-link and complete-link Hi-erarchical Agglomerative Clustering.
HAC initiallyassigns each data point to a singleton cluster, andrepeatedly merges clusters until a specified termi-nation criteria is satisfied (Manning and Schu?tze,1999).
These methods require a similarity metricbetween two sentences.
We use the standard co-sine metric over a bag-of-words encoding of eachsentence.
We remove stopwords and stem each re-maining term using the Porter stemming algorithm(Porter, 1997).
Our algorithms begin by placingeach sentence in its own cluster.
At each itera-tion we merge the two closest clusters.
A fully-automated approach must use some termination cri-teria to decide when to stop clustering.
In exper-iments presented here, we adopt two manually su-pervised methods to set the desired number of clus-ters (k): ?correct?
k and ?best?
k. ?Correct?
sets kto be the actual number of events.
This value wasobtained during the annotation process (see Section4.1).
?Best?
tunes k so as to maximise the quality ofthe resulting clusters.3.1 Exploiting Article StructureOur baseline ignores an important constraint on theevent associated with each sentence: the positionof the sentence within the document.
Documentsconsist of sentences arranged in a linear order andnearby sentences in terms of this ordering typicallyrefer to the same topic (Zha, 2002).
Similarly we as-sume that adjacent sentences are more likely to referto the same event, later sentences are likely to intro-duce new events, etc.
In this Section, we describe analgorithm that exploits this document structure dur-ing the sentence clustering process.32The basic idea is to learn a model capable of cap-turing document structure, i.e.
the way events arereported.
Each document is treated as a sequence oflabels (1 label per sentence) where each label repre-sents the event(s) discussed in that sentence.
We de-fine four generalised event label types: N, representsa new event sentence; C, represents a continuingevent sentence (i.e.
it discusses the same event as thepreceding sentence); B, represents a back-referenceto an earlier event; X, represents a sentence that doesnot reference an event.
This model takes the form ofa Finite State Automaton (FSA) where:?
States correspond to event labels.?
Transitions correspond to adjacent sentencesthat mention the pair of events.More formally, E = (S, s0, F, L, T) is a modelwhere S is the set of states, s0 ?
S is the initial state,F ?
S is the set of final states, L is the set of edgelabels and T ?
(S?L)?S is the set of transitions.We note that it is the responsibility of the learningalgorithm to discover the correct number of states.We treat the task of discovering an event model asthat of learning a regular grammar from a set of pos-itive examples.
Following Golds research on learn-ing regular languages (Gold, 1967), the problem hasreceived significant attention.
In our current experi-ments, we use Thollard et als MDI algorithm (Thol-lard et al, 2000) for learning the automaton.
MDIhas been shown to be effective on a wide range oftasks, but it must be noted that any grammar infer-ence algorithm could be substituted.To estimate how much sequential structure existsin the sentence labels, the document collection wasrandomly split into training and test sets.
The au-tomaton produced by MDI was learned using thetraining data, and the probability that each test se-quence was generated by the automaton was calcu-lated.
These probabilities were compared with thoseof a set of random sequences (generated to have thesame distribution of length as the test data).
Theprobabilities of event sequences from our datasetand the randomly generated sequences are shownin Figure 2.
The test and random sequences aresorted by probability.
The vertical axis shows therank in each sequence and the horizontal axis showsthe negative log probability of the sequence at eachFigure 2: Distribution in the probability that actualand random event sequences are generated by theautomaton produced by MDI.rank.
The data suggests that the documents are in-deed structured, as real document sequences tend tobe much more likely under the trained FSA than ran-domly generated sequences.We modify our baseline clustering algorithm toutilise the structural information omitted by the au-tomaton as follows: Let L(c1, c2) be a sequenceof labels induced by merging two clusters c1 andc2.
If P (L(c1, c2)) is the probability that sequenceL(c1, c2) is accepted by the automaton, and letcos(c1, c2) be the cosine distance between c1 and c2.We can measure the similarity between c1 and c2 as:SIM(c1, c2) = cos(c1, c2)?
P (L(c1, c2)) (1)Let r be the number of clusters remaining.
Thenthere are r(r?1)2 pairs of clusters.
For each pair ofclusters c1,c2 we generate the resulting sequence oflabels that would result if c1 and c2 were merged.We then input each label sequence to our trainedFSA to obtain the probability that it is generated bythe automaton.
At each iteration, the algorithm pro-ceeds by merging the most similar pair according tothis metric.
Figure 3 illustrates this process in moredetail.
To terminate the clustering process, we adopteither the ?correct?
k or ?best?
k halting criteria de-scribed earlier.4 Experiments4.1 Experimental SetupIn our experiments, we used a corpus of news arti-cles which is a subset of the Iraq Body Count (IBC)33Figure 3: The sequence-based clustering process.dataset2.
This is an independent public database ofmedia-reported civilian deaths in Iraq resulting di-rectly from military attack by the U.S. forces.
Casu-alty figures for each event reported are derived solelyfrom a comprehensive manual survey of online me-dia reports from various news sources.
We obtaineda portion of their corpus which consists of 342 newarticles from 56 news sources.
The articles are ofvarying size (average sentence length per documentis 25.96).
Most of the articles contain references tomultiple events.
The average number of events perdocument is 5.09.
Excess HTML (image captionsetc.)
was removed, and sentence boundaries wereidentified using the Lingua::EN::Sentence perl mod-ule available from CPAN3.To evaluate our clustering methods, we use thedefinition of precision and recall proposed by (Hessand Kushmerick, 2003).
We assign each pair ofsentences into one of four categories: (i) clusteredtogether (and annotated as referring to the sameevent); (ii) not clustered together (but annotated asreferring to the same event); (iii) incorrectly clus-tered together; (iv) correctly not clustered together.Precision and recall are thus found to be computedas P = aa+c and R =aa+b , and F1 =2PRP+R .The corpus was annotated by a set of ten vol-unteers.
Within each article, events were uniquelyidentified by integers.
These values were thenmapped to one of the four label categories, namely?N?, ?C?, ?X?, and ?B?.
For instance, sentences de-scribing previously unseen events were assigned anew integer.
This value was mapped to the label cat-egory ?N?
signifying a new event.
Similarly, sen-2http://iraqbodycount.org/3http://cpan.org/tences referring to events in a preceding sentencewere assigned the same integer identifier as thatassigned to the preceding sentence and mapped tothe label category ?C?.
Sentences that referenced anevent mentioned earlier in the document but not inthe preceding sentence were assigned the same inte-ger identifier as that sentence but mapped to the labelcategory ?B?.
Furthermore, If a sentence did not re-fer to any event, it was assigned the label 0 and wasmapped to the label category ?X?.
Finally, each doc-ument was also annotated with the distinct numberof events reported in it.In order to approximate the level of inter-annotation agreement, two annotators were asked toannotate a disjoint set of 250 documents.
Inter-rateragreements were calculated using the kappa statis-tic that was first proposed by (Cohen, 1960).
Thismeasure calculates and removes from the agreementrate the amount of agreement expected by chance.Therefore, the results are more informative than asimple agreement average (Cohen, 1960; Carletta,1996).
Some extensions were developed including(Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow etal., 1991).
In this paper the methodology proposedby (Fleiss, 1981) was implemented.
Each sentencein the document set was rated by the two annotatorsand the assigned values were mapped into one of thefour label categories (?N?, ?C?, ?X?, and ?B?).
Forcomplete instructions on how kappa was calculated,we refer the reader to (Fleiss, 1981).
Using the an-notated data, a kappa score of 0.67 was obtained.This indicates that the annotations are somewhat in-consistent, but nonetheless are useful for producingtentative conclusions.To determine why the annotators were having dif-ficulty agreeing, we calculated the kappa score foreach category.
For the ?N?, ?C?
and ?X?
categories,reasonable scores of 0.69, 0.71 and 0.72 were ob-tained respectively.
For the ?B?
category a relativelypoor score of 0.52 was achieved indicating that theraters found it difficult to identify sentences that ref-erenced events mentioned earlier in the document.To illustrate the difficulty of the annotation task anexample where the raters disagreed is depicted inFigure 4.
The raters both agreed when assigninglabels to sentence 1 and 2 but disagreed when as-signing a label to Sentence 23 .
In order to correctlyannotate this sentence as referring to the event de-34Sentence 1: A suicide attacker set off a bomb that tore through a funeral tent jammed with Shiite mourners Thursday.Rater 1: label=1.
Rater 2: label=1Sentence 2: The explosion, in a working class neighbourhood of Mosul, destroyed the tent killing nearly 50 people.Rater 1: label=1.
Rater 2: label=1.. .
.
.
.
.
.
.
.Sentence 23: At the hospital of this northern city, doctor Saher Maher said that at least 47 people were killed.Rater 1: label=1.
Rater 2: label=2.Figure 4: Sample sentences where the raters disagreed.Algorithm a-link c-link s-linkBL(correct k) 40.5 % 39.2% 39.6%SEQ(correct k) 47.6%* 45.5%* 44.9%*BL(best k) 52.0% 48.2% 50.9%SEQ(best k) 61.0%* 56.9%* 58.6%*Table 1: % F1 achieved using average-link (a-link),complete-link (c-link) and single-link (s-link) varia-tions of the baseline and sequence-based algorithmswhen the correct and best k halting criteria are used.Scores marked with * are statistically significant toa confidence level of 99%.scribe in sentence 1 and 2, the rater have to resolvethat ?the northern city?
is referring to ?Mosul?
andthat ?nearly 50?
equates to ?at least 47?.
These andsimilar ambiguities in written text make such an an-notation task very difficult.4.2 ResultsWe evaluated our clustering algorithms using the F1metric.
Results presented in Table 1 were obtainedusing 50:50 randomly selected train/test splits aver-aged over 5 runs.
For each run, the automaton pro-duced by MDI was generated using the training setand the clustering algorithms were evaluated usingthe test set.
On average, the sequence-based clus-tering approach achieves an 8% increase in F1 whencompared to the baseline.
Specifically the average-link variation exhibits the highest F1 score, achiev-ing 62% when the ?best?
k termination method isused.It is important to note that the inference producedby the automaton depends on two values: the thresh-old ?
of the MDI algorithm and the amount of labelsequences used for learning.
The closer ?
is to 0,the more general the inferred automaton becomes.In an attempt to produce a more general automaton,we chose ?
= 0.1.
Intuitively, as more training datais used to train the automaton, more accurate infer-ences are expected.
To confirm this we calculatedthe %F1 achieved by the average-link variation ofthe method for varying levels of training data.
Over-all, an improvement of approx.
5% is observed asthe percentage training data used is increased from10% to 90%.5 DiscussionAccurately identifying events in unstructured text isa very difficult task.
This is partly because the de-scription of an individual event can spread acrossseveral sentences.
In this paper, we investigatedthe use of clustering for the task of grouping sen-tences in a document that refer to the same event.However, there are limitations to this approach thatneed to be considered.
Firstly, results presentedin Section 4.2 suggest that the performance of theclusterer depends somewhat on the chosen valueof k (i.e.
the number of events in the document).This information is not readily available.
However,preliminary analysis presented in (Naughton et al,2006) indicate that is possible to estimate this valuewith reasonable accuracy.
Furthermore, promisingresults are observed when this estimated value isused halt the clustering process.
Secondly, labelleddata is required to train the automation used by ournovel clustering method.
Evidence presented in Sec-tion 4.1 suggests that reasonable inter-annotationagreement for such an annotation task is difficult toachieve.
Nevertheless, clustering allows us to takeinto account that the manner in which events are de-scribed is not always linear.
To assess exactly howbeneficial this is, we are currently treating this prob-lem as a text segmentation task.
Although this is a35crude treatment of the complexity of written text, itwill help us to approximate the benefit (if any) ofapplying clustering-based techniques to this task.In the future, we hope to further evaluate ourmethods using a larger dataset containing moreevent types.
We also hope to examine the inter-esting possibility that inherent structures learnedfrom documents originating from one news source(e.g.
Aljazeera) differ from structures learned us-ing documents originating from another source (e.g.Reuters).
Finally, a single sentence often containsreferences to multiple events.
For example, considerthe sentence ?These two bombings have claimed thelives of 23 Iraqi soldiers?.
Our algorithms assumethat each sentence describes just one event.
Futurework will focus on developing methods to automati-cally recognise such sentences and techniques to in-corporate them into the clustering process.Acknowledgements.
This research was supportedby the Irish Research Council for Science, Engineer-ing & Technology (IRCSET) and IBM under grantRS/2004/IBM/1.
The author also wishes to thankDr.
Joe Carthy and Dr. Nicholas Kushmerick fortheir helpful discussions.ReferencesW.
Barlow, N. Lai, and S. Azen.
1991.
A comparison ofmethods for calculating a stratified kappa.
Statistics inMedicine, 10:1465?1472.Daniel Bikel, Richard Schwartz, and Ralph Weischedel.1999.
An algorithm that learns what?s in a name.
Ma-chine Learning, 34(1-3):211?231.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Linguis-tics, 22:249?254.Jacob Cohen.
1960.
A coeficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20(1):37?46.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement with provision for scaled disagreement orpartial credit.
Psychological Bulletin, 70.B.S.
Everitt.
1968.
Moments of the statistics kappa andthe weighted kappa.
The British Journal of Mathemat-ical and Statistical Psychology, 21:97?103.J.L.
Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin, 76.J.L.
Fleiss, 1981.
Statistical methods for rates and pro-portions, pages 212?36.
John Wiley & Sons.E.
Mark Gold.
1967.
Grammar identification in the limit.Information and Control, 10(5):447?474.Ralph Grishman.
1997.
Information extraction: Tech-niques and challenges.
In Proceedings of the sev-enth International Message Understanding Confer-ence, pages 10?27.Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-combe, Regina Barzilay, Min-Yen Kan, and KathleenMcKeown.
2001.
SIMFINDER: A flexible clusteringtool for summarisation.
In Proceedings of the NAACLWorkshop on Automatic Summarisation, Associationfor Computational Linguistics, pages 41?49.Andreas Hess and Nicholas Kushmerick.
2003.
Learn-ing to attach semantic metadata to web services.
InProceedings of the International Semantic Web Con-ference (ISWC 2003), pages 258?273.
Springer.Christopher Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press.Martina Naughton, Nicholas Kushmerick, and JosephCarthy.
2006.
Event extraction from heterogeneousnews sources.
In Proceedings of the AAAI WorkshopEvent Extraction and Synthesis, pages 1?6, Boston.Martin Porter.
1997.
An algorithm for suffix stripping.Readings in Information Retrieval, pages 313?316.Franck Thollard, Pierre Dupont, and Colin de la Higuera.2000.
Probabilistic DFA inference using Kullback-Leibler divergence and minimality.
In Proceedings ofthe 17th International Conference on Machine Learn-ing, pages 975?982.
Morgan Kaufmann, San Fran-cisco.Feiyu Xu, Hans Uszkoreit, and Hong Li.
2006.
Auto-matic event and relation detection with seeds of vary-ing complexity.
In Proceedings of the AAAI WorkshopEvent Extraction and Synthesis, pages 12?17, Boston.Hongyuan Zha.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcement prin-ciple and sentence clustering.
In Proceedings of the25th annual international ACM SIGIR conference onResearch and development in Information Retrieval,pages 113?120, New York, NY.
ACM Press.36
