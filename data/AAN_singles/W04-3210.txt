Automatic Paragraph Identification:A Study across Languages and DomainsCaroline SporlederSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKcsporled@inf.ed.ac.ukMirella LapataDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello StreetSheffield S1 4DP, UKmlap@dcs.shef.ac.ukAbstractIn this paper we investigate whether paragraphs canbe identified automatically in different languagesand domains.
We propose a machine learning ap-proach which exploits textual and discourse cuesand we assess how well humans perform on thistask.
Our best models achieve an accuracy that issignificantly higher than the best baseline and, formost data sets, comes to within 6% of human per-formance.1 IntroductionWritten texts are usually broken up into sentencesand paragraphs.
Sentence splitting is a necessarypre-processing step for a number of Natural Lan-guage Processing (NLP) tasks including part-of-speech tagging and parsing.
Since sentence-finalpunctuation can be ambiguous (e.g., a period canalso be used in an abbreviation as well as to markthe end of a sentence), the task is not trivial and hasconsequently attracted a lot of attention (e.g., Rey-nar and Ratnaparkhi (1997)).
In contrast, therehas been virtually no previous research on inferringparagraph boundaries automatically.
One reason forthis is that paragraph boundaries are usually markedunambiguously by a new line and extra white space.However, a number of applications could bene-fit from a paragraph detection mechanism.
Text-to-text generation applications such as single- andmultidocument summarisation as well as text sim-plification usually take naturally occurring texts asinput and transform them into new texts satisfyingspecific constraints (e.g., length, style, language).The output texts do not always preserve the struc-ture and editing conventions of the original text.In summarisation, for example, sentences are typ-ically extracted verbatim and concatenated to forma summary.
Insertion of paragraph breaks could im-prove the readability of the summaries by indicatingtopic shifts and providing visual targets to the reader(Stark, 1988).Machine translation is another application forwhich automatic paragraph detection is relevant.Current systems deal with paragraph boundary in-sertion in the target language simply by preserv-ing the boundaries from the source language.
How-ever, there is evidence for cross-linguistic variationin paragraph formation and placement, particularlyfor languages that are not closely related such as En-glish and Chinese (Zhu, 1999).
So, a paragraph in-sertion mechanism that is specific to the target lan-guage, instead of one that relies solely on the sourcelanguage, may yield more readable texts.Paragraph boundary detection is also relevant forspeech-to-text applications.
The output of auto-matic speech recognition systems is usually rawtext without any punctuation or paragraph breaks.This naturally makes the text very hard to read,which can cause processing difficulties, especiallyif speech recognition is used to provide deaf stu-dents with real-time transcripts of lectures.
Further-more, sometimes the output of a speech recogniserneeds to be processed automatically by applicationssuch as information extraction or summarisation.Most of these applications (e.g., Christensen et al,(2004)) port techniques developed for written textsto spoken texts and therefore require input that ispunctuated and broken into paragraphs.
While therehas been some research on finding sentence bound-aries in spoken text (Stevenson and Gaizauskas,2000), there has been little research on determiningparagraph boundaries.1If paragraph boundaries were mainly an aes-thetic device for visually breaking up long texts intosmaller chunks, as has previously been suggested(see Longacre (1979)), paragraph boundaries couldbe easily inserted by splitting a text into severalequal-size segments.
Psycho-linguistic research,however, indicates that paragraph boundaries arenot purely aesthetic.
For example, Stark (1988)1There has been research on using phonetic cues to segmentspeech into ?acoustic paragraphs?
(Hauptmann and Smith,1995).
However, these do not necessarily correspond to writtenparagraphs.
But even if they did, textual cues could comple-ment phonetic information to identify paragraphs.asked her subjects to reinstate paragraph bound-aries into fiction texts from which all boundarieshad been removed and found that humans are ableto do so with an accuracy that is higher than wouldbe expected by chance.
Crucially, she also foundthat (a) individual subjects did not make all theirparagraphs the same length and (b) paragraphs inthe original text whose length deviated significantlyfrom the average paragraph length were still iden-tified correctly by a large proportion of subjects.These results show that people are often able toidentify paragraphs correctly even if they are excep-tionally short or long without defaulting to a simpletemplate of average paragraph length.Human agreement on the task suggests that thetext itself provides cues for paragraph insertion,even though there is some disagreement over whichspecific cues are used by humans (see Stark (1988)).Possible cues include repeated content words, pro-noun coreference, paragraph length, and local se-mantic connectedness.In this paper, we investigate whether it is possi-ble to exploit some of these textual cues togetherwith syntactic and discourse related information todetermine paragraph boundaries automatically.
Wetreat paragraph boundary identification as a classi-fication task and examine whether the difficulty ofthe task and the utility of individual textual cuesvaries across languages and across domains.
Wealso assess human performance on the same taskand whether it differs across domains.2 Related WorkPrevious work has focused extensively on the taskof automatic text segmentation whose primary goalis to divide individual texts into sub-topics.
De-spite their differences, most methods are unsuper-vised and typically rely on the distribution of wordsin a given text to provide cues for topic segmenta-tion.2 Hearst?s (1997) TextTiling algorithm, for ex-ample, determines sub-topic boundaries on the basisof term overlap in adjacent text blocks.
In more re-cent work, Utiyama and Isahara (2001) combine astatistical segmentation model with a graph searchalgorithm to find the segmentation with the maxi-mum probability.
Beeferman et al (1999) use su-pervised learning methods to infer boundaries be-tween texts.
They employ language models to de-tect topic shifts and combine them with cue wordfeatures.2Due to lack of space we do not describe previous work intext segmentation here in detail; we refer the reader to Utiyamaand Isahara (2001) and Pevzener and Hearst (2002) for a com-prehensive overview.Our work differs from these previous approachesin that paragraphs do not always correspond to sub-topics.
While topic shifts often correspond to para-graph breaks, not all paragraph breaks indicate atopic change.
Breaks between paragraphs are ofteninserted for other (not very well understood) reasons(see Stark (1988)).
Therefore, the segment granular-ity is more fine-grained for paragraphs than for top-ics.
An important advantage for methods developedfor paragraph detection (as opposed to those de-veloped for text-segmentation) is that training datais readily available, since paragraph boundaries areusually unambiguously marked in texts.
Hence, su-pervised methods are ?cheap?
for this task.3 Our Approach3.1 CorporaOur study focused on three languages: English,German, and Greek.
These languages differ interms of word order (fixed in English, semi-free inGerman, fairly flexible in Greek).
Greek and Ger-man also have richer morphology than English.
Ad-ditionally, Greek has a non-Latin writing system.For each language we created corpora represen-tative of three domains: fiction, news, and parlia-mentary proceedings.
Previous work on the role ofparagraph markings (Stark, 1988) has focused ex-clusively on fiction texts, and has shown that hu-mans can identify paragraph boundaries in this do-main reliably.
It therefore seemed natural to test ourautomatic method on a domain for which the taskhas been shown to be feasible.
We selected newstexts since most summarisation methods today fo-cus on this domain and we can therefore explore therelevance of our approach for this application.
Fi-nally, parliamentary proceedings are transcripts ofspeech, and we can examine whether a method thatrelies solely on textual cues is also useful for spokentexts.For English, we used the whole Hansard sectionof the BNC, as our corpus of parliamentary proceed-ings.
We then created a fiction corpus of similar sizeby randomly selecting prose files from the fictionpart of the BNC.
In the same way a news corpuswas created from the Penn Treebank.For German, we used the prose part of ProjectGutenberg?s e-book collection3 as our fiction corpusand the complete Frankfurter Rundschau part of theECI corpus4 as our news corpus.
The corpus of par-liamentary proceedings was obtained by randomly3http://www.gutenberg.net/ For copyright reasons,this web site mainly contains books published before 1923.4http://www.elsnet.org/eci.htmlfiction news parliamentEnglish 1,140,000 1,156,000 1,156,000German 2,500,000 4,100,000 3,400,000Greek 563,000 1,500,000 1,500,000Table 1: Number of words per corpusselecting a subset of the German section from theEuroparl corpus (Koehn, 2002).For Greek, a fiction corpus was compiled fromthe ECI corpus by selecting all prose files that con-tained paragraph markings.
Our news corpus wasdownloaded from the WWW site of the ModernGreek newspaper Eleftherotypia and consists of fi-nancial news from the period of 2001?2002.
A cor-pus of parliamentary proceedings was again createdby randomly selecting a subset of the Greek sectionof the Europarl corpus (Koehn, 2002).Parts of the data were further pre-processed toinsert sentence boundaries.
We trained a publiclyavailable sentence splitter (Reynar and Ratnaparkhi,1997) on a small manually annotated sample (1,000sentences per domain per language) and applied itto our corpora.
Table 1 shows the corpus sizes.
Allcorpora were split into training (72%), development(24%) and test set (4%).3.2 Machine LearningWe used BoosTexter (Schapire and Singer, 2000) asour machine learning system.
BoosTexter was orig-inally developed for text categorisation and com-bines a boosting algorithm with simple decisionrules.
For all domains and languages our trainingexamples were sentences.
Class labels encoded foreach sentence whether it was starting a paragraph ornot.The features we used fall broadly into three dif-ferent areas: non-syntactic features, language mod-elling features and syntactic features.
The latterwere only applied to English as we did not have suit-able parsers for German and Greek.The values of our features are numeric, booleanor ?text?.
BoosTexter applies unigram models whenforming classification hypotheses for features with?text?
values.
These can be simply words or anno-tations such as part-of-speech tags.We deliberately did not include anaphora-basedfeatures.
While anaphors can help determine para-graph boundaries (paragraph initial sentences tendto contain few or no anaphors), anaphora structureis dependent on paragraph structure rather than theother way round.
Hence, in applications which ma-nipulate texts and thereby potentially ?mess-up?
theanaphora structure (e.g., multi-document summari-sation), anaphors are not a reliable cue for paragraphidentification.53.2.1 Non-syntactic FeaturesDistance (Ds, Dw): These features encode the dis-tance of the current sentence from the previous para-graph break.
We measured distance in terms of thenumber of intervening sentences (Ds) as well as interms of the number of intervening words (Dw).
Ifparagraph breaks were driven purely by aestheticsone would expect this feature to be among the mostsuccessful ones.6Sentence Length (Length): This feature encodesthe number of words in the current sentence.
Aver-age sentence length is known to vary with text posi-tion (Genzel and Charniak, 2003) and it is possiblethat it also varies with paragraph position.Relative Position (Pos): The relative position of asentence in the text is calculated by dividing the cur-rent sentence number by the number of sentencesin the text.
The motivation for this feature is thatparagraph length may vary with text position.
Forexample, it is possible that paragraphs at the begin-ning and end of a text are shorter than paragraphsin the middle and hence a paragraph break is morelikely at the two former text positions.Quotes (Quotep, Quotec, Quotei): These featuresencode whether the previous or current sentencecontain a quotation (Quotep and Quotec, respec-tively) and whether the current sentence contin-ues a quotation that started in a preceding sentence(Quotei).
The presence of quotations can providecues for speaker turns, which are often signalled byparagraph breaks.Final Punctuation (FinPun): This feature keepstrack of the final punctuation mark of the previoussentence.
Some punctuation marks may providehints as to whether a break should be introduced.For example, in the news domain, where there ishardly any dialogue, if the previous sentence endedin a question mark, it is likely that the current sen-tence supplies an answer to this question, thus mak-ing a paragraph break improbable.Words (W1, W2, W3, Wall ): These text-valued fea-tures encode the words in the sentence.
Wall takesthe complete sentence as its value.
W1, W2 and W3encode the first word, the first two words and thefirst three words, respectively.5This is also true for some of the other features we use(e.g., sentence length) but not quite to the same extent.6One could also use the history of class labels assigned toprevious sentences as a feature (as in part-of-speech tagging);however, we leave this to future research.3.2.2 Language Modelling FeaturesOur motivation for including language modellingfeatures stems from Genzel and Charniak?s (2003)work where they show that the word entropy rate islower for paragraph initial sentences than for non-paragraph initial ones.
We therefore decided to ex-amine whether word entropy rate is a useful featurefor the paragraph prediction task.
Using the train-ing set for each language and domain, we createdlanguage models with the CMU language modellingtoolkit (Clarkson and Rosenfeld, 1997).
We exper-imented with language models of variable length(i.e., 1?5) and estimated two features: the prob-ability of a given sentence according to the lan-guage model (LMp) and the per-word entropy rate(LMpwe).
The latter was estimated by dividing thesentence probability as assigned by the languagemodel by the number of sentence words (see Genzeland Charniak (2003)).We additionally experimented with characterlevel n-gram models.
Such models are defined overa relatively small vocabulary and can be easily con-structed for any language without pre-processing.Character level n-gram models have been appliedto the problem of authorship attribution and ob-tained state-of-the art results (Peng et al, 2003).If some characters are more often attested in para-graph starting sentences (e.g., ?A?
or ?T?
), then weexpect these sentences to have a higher probabilitycompared to non-paragraph starting ones.
Again,we used the CMU toolkit for building the characterlevel n-gram models.
We experimented with mod-els whose length varied from 2 to 8 and estimatedthe probability assigned to a sentence according tothe character level model (CMp).3.2.3 Syntactic FeaturesFor the English data we also used several featuresencoding syntactic complexity.
Genzel and Char-niak (2003) suggested that the syntactic complex-ity of sentences varies with their position in a para-graph.
Roughly speaking, paragraph initial sen-tences are less complex.
Hence, complexity mea-sures may be a good indicator of paragraph bound-aries.
To estimate complexity, we parsed the textswith Charniak?s (2001) parser and implemented thefollowing features:Parsed: This feature states whether the currentsentence could be parsed.
While this is not a realmeasure of syntactic complexity it is probably cor-related with it.Number of phrases (nums, numvp, numnp, numpp):These features measure syntactic complexity interms of the number of S, VP, NP, and PP con-stituents in the parse tree.Signature (Sign, Signp): These text-valued fea-tures encode the sequence of part-of-speech tags inthe current sentence.
Sign only encodes word tags,while Signp also includes punctuation tags.Children of Top-Level Nodes (Childrs1, Childrs):These text-valued features encode the top-levelcomplexity of a parse tree: Childrs1 takes as itsvalue the sequence of syntactic labels of the childrenof the S1-node (i.e., the root of the parse tree), whileChildrs encodes the syntactic labels of the childrenof the highest S-node(s).
For example, Childrs1 mayencode that the sentence consists of one clause andChildrs may encode that this clause consists of anNP, a VP, and a PP.Branching Factor (Branchs, Branchvp, Branchnp,Branchpp): These features express the averagenumber of children of a given non-terminal con-stituent (cf.
Genzel and Charniak (2003)).
We com-pute the branching factor for S, VP, NP, and PP con-stituents.Tree Depth: We define tree depth as the averagelength of a path (from root node to leaf node).Cue Words (Cues, Cuem, Cuee): These featuresare not strictly syntactic but rather discourse-based.They encode discourse cues (such as because) atthe start (Cues), in the middle (Cuem) and at theend (Cuee) of the sentence, where ?start?
is the firstword, ?end?
the last one, and everything else is?middle?.
We keep track of all cue word occur-rences, without attempting to distinguish betweentheir syntactic and discourse usages.For English, there are extensive lists of discoursecues (we used Knott (1996)), but such lists are notwidely available for German and Greek.
Hence, weonly used this feature on the English data.4 ExperimentsBoosTexter is parametrised with respect to the num-ber of training iterations.
In all our experiments,this parameter was optimised on the developmentset; BoosTexter was initially trained for 500 itera-tions, and then re-trained with the number of itera-tions that led to the lowest error rate on the devel-opment set.
Throughout this paper all results are re-ported on the unseen test set and were obtained us-ing models optimised on the development set.
Wereport the models?
accuracy at predicting the rightlabel (i.e., paragraph starting or not) for each sen-tence.English German Greekfeature fiction news parl.
fiction news parl.
fiction news parl.Bd 60.16 51.73 59.50 65.44 59.03 58.26 59.00 52.85 66.48Bm 71.04 51.44 69.38 75.75 68.24 66.17 67.57 53.99 76.25Dists 71.07 57.74 54.02 75.80 68.25 66.23 67.69 57.94 76.30Distw 71.02 63.08 65.64 75.80 67.70 67.20 68.31 59.76 76.30Length 72.08 56.11 68.45 75.75 72.55 67.10 67.52 60.84 76.55Position 71.04 49.18 38.71 75.68 68.05 66.35 67.57 56.52 76.35Quotep 80.84 56.25 30.62 72.97 68.24 66.23 72.80 58.00 76.30Quotec 80.64 54.95 31.00 72.35 68.24 66.17 71.03 53.99 76.25Quotei 71.04 51.44 30.62 75.75 68.24 66.17 67.57 53.99 76.25FinPun 72.08 54.18 71.75 73.15 76.36 69.53 73.33 59.86 76.55W1 72.96 57.74 82.05 75.43 73.87 75.25 67.05 67.41 76.81W2 73.47 58.51 80.62 75.80 74.77 76.74 66.37 68.22 78.48W3 73.68 59.90 80.73 75.60 74.50 76.79 67.63 67.88 78.43Wall 73.99 61.78 75.40 75.60 73.03 76.20 67.78 67.88 77.26BestLMp 72.83 55.96 69.66 75.93 71.39 67.40 67.57 61.64 76.50BestLMpwe 72.16 52.21 69.88 75.90 69.24 66.98 67.83 56.29 76.40BestCMp 72.70 57.36 69.49 75.88 73.37 67.53 67.68 61.68 76.51allns lcm 82.45   70.77   82.71   76.55  79.28   79.17   78.03   76.31   79.35  Table 2: Accuracy of non-syntactic and language modelling features on test set4.1 The Influence of Non-syntactic FeaturesIn the first set of experiments, we ran BoosTexteron all 9 corpora using non-syntactic and languagemodelling features.
To evaluate the contribution ofindividual features to the classification task, we builtone-feature classifiers in addition to a classifier thatcombined all features.
Table 2 shows the test setclassification accuracy of the individual features andtheir combination (allns lcms).
The length of the lan-guage and character models was optimised on thedevelopment set.
The test set accuracy of the opti-mised models is shown as BestLMp and BestLMpwe(language models) and BestCMp (character mod-els).7 The results for the three best performing one-feature classifiers and the combined classifier areshown in boldface.BoosTexter?s classification accuracy was furthercompared against two baselines.
A distance-basedbaseline (Bd) was obtained by hypothesising a para-graph break after every d sentences.
We estimated din the training data by counting the average numberof sentences between two paragraphs.
Our secondbaseline, Bm, defaults to the majority class, i.e., as-sumes that the text does not have paragraph breaks.For all languages and domains, the combinedmodels perform better than the best baseline.
In or-der to determine whether this difference is signifi-cant, we applied ?2 tests.
The diacritic   (   ) in Ta-7Which language and character models perform best variesslightly across corpora but no clear trends emerge.ble 2 indicates whether a given model is (not) sig-nificantly different from the best baseline.
Signifi-cant results are achieved across the board with theexception of German fiction.
We believe the rea-son for this lies in the corpus itself, as it is veryheterogeneous, containing texts whose publicationdate ranges from 1766 to 1999 and which exhibit awide variation in style and orthography.
This makesit difficult for any given model to reliably identifyparagraph boundaries in all texts.In general, the best performing features varyacross domains but not languages.
Word features(W1?W3, Wall ) yield the best classification accura-cies for news and parliamentary domains, whereasfor fiction, quotes and punctuation seem more use-ful.
The only exception is the German fiction cor-pus, which consists mainly of 19th century texts.These contain less direct speech than the two fic-tion corpora for English and Greek (which containcontemporary texts).
Furthermore, while examplesof direct speech in the English corpus often involveshort dialogues, where a paragraph boundary is in-troduced after each speaker turn, the German cor-pus contains virtually no dialogues and examples ofdirect speech are usually embedded in a longer nar-rative and not surrounded by paragraph breaks.Note that the distance in words from the previ-ous paragraph boundary (Distw) is a good indicatorfor a paragraph break in the English news domain.However, this feature is less useful for the other twolanguages.
An explanation might be that the En-glish news corpus is very homogeneous (i.e., it con-tains articles that not only have similar content butare also structurally alike).
The Greek news cor-pus is relatively homogeneous; it mainly containsfinancial news articles but also some interviews, sothere is greater variation in paragraph length, whichmeans that the distance feature is overtaken by theword-based features.
Finally, the German news cor-pus is highly heterogeneous, containing not onlynews stories but also weather forecasts, sports re-sults and cinema listings.
This leads to a large vari-ation in paragraph length, which in turn means thatthe distance feature performs worse than the bestbaseline.The heterogeneity of the German news corpusmay also explain another difference: while the fi-nal punctuation of the previous sentence (FinPun)is among the less useful features for English andGreek (albeit still outperforming the baseline), itis the best performing feature for German.
TheGerman news corpus contains many ?sentences?that end in atypical end-of-sentence markers suchas semi-colons (which are found often in cinemalistings).
Atypical markers will often not occurbefore paragraph breaks, whereas typical markerswill.
This fact renders final punctuation a betterpredictor of paragraph breaks in the German corpusthan in the other two corpora.The language models behave similarly across do-mains and languages.
With the exception of thenews domain, they do not seem to be able to out-perform the majority baseline by more than 1%.The word entropy rate yields the worst performance,whereas character-based models perform as well asword-based models.
In general, our results showthat language modelling features are not particularlyuseful for this task.4.2 The Influence of Syntactic FeaturesOur second set of experiments concentrated solelyon the English data and investigated the useful-ness of the syntactic features (see Table 3).
Again,we created one-feature classifiers and a classifierthat combined all features, i.e., language and char-acter models, non-syntactic, and syntactic features(allns lcm syn).
Table 3 also repeats the performanceof the two baselines (Bd and Bm) and the combinednon-syntactic models (allns lcm).
The accuracies ofthe three best performing one-feature models andthe combined model are again shown in boldface.As can be seen, syntactic features do not con-tribute very much to the overall performance.
Theyonly increase the accuracy by about 1%.
A ?2 testEnglishfeature fiction news parl.Bd 60.16 51.73 59.50Bm 71.04 51.44 69.38Cues 71.48 51.49 40.64Cuem 70.97 54.28 59.03Cuee 71.04 51.78 31.61Parse 71.04 51.88 30.62Nums 71.04 53.56 69.05Numvp 71.04 54.18 70.59Numnp 71.77 56.11 68.94Numpp 71.04 53.61 64.98Numad jp 71.04 51.11 42.62Numadvp 71.04 52.40 47.96Sign 75.39 57.02 67.95Signp 75.49 59.18 70.76Childrs1 71.69 55.87 79.35Childrs 75.34 55.53 79.52Branchs 71.35 55.82 69.11Branchvp 71.33 53.46 70.48Branchnp 71.77 56.11 33.09Branchpp 71.04 51.44 30.62TreeDepth 72.57 54.04 69.00allns lcm 82.45 70.77 82.71allns lcm syn 82.91   ?
71.83    ?
83.92    ?Table 3: Syntactic features on English test datarevealed that the difference between allns lcm andallns lcm syn is not statistically significant (indicatedby ?
in Table 3) for any of the three domains.The syntactic features seem to be less domain de-pendent than the non-syntactic ones.
In general, thepart-of-speech signature features (Sign, Signp) area good predictor, followed by the syntactic labelsof the children of the top nodes (Childrs, Childrs1).The number of NPs (Numnp) and their branchingfactor (Branchnp) are also good indicators for somedomains, particularly the news domain.
This isplausible since paragraph initial sentences in theWall Street Journal often contain named entities,such as company names, which are parsed as flatNPs, i.e., have a relatively high branching factor.4.3 The Effect of Training SizeFinally, we examined the effect of the size of thetraining data on the learner?s classification accuracy.We conducted our experiments solely on the Englishdata, however we expect the results to generalise toGerman and Greek.
From each English training setwe created ten progressively smaller data sets, thefirst being identical to the original set, the secondcontaining 9/10 of sentences in the original train-6.8 13.6 20.4 27.2 33.9 40.8 47.6 54.4 61.1 67.9708090100Accuracy(%)Fiction3.9 7.8 11.7 15.6 19.5 23.4 27.3 31.20 35.1 39.360708090100Accuracy(%)News4.5 9.1 13.6 18.1 22.6 27.2 31.7 36.2 40.7 45.4Thousand instances of training data708090100Accuracy(%)ParliamentFigure 1: Learning Curves for EnglishKappa % Agrfiction .72 88.58news .47 77.45parl.
.76 88.50Table 4: Human agreement on the paragraph identi-fication tasking set, the third containing 8/10, etc.
The traininginstances in each data set were selected randomly.BoosTexter was trained on each of these sets (usingall features), as described previously, and tested onthe test set.Figure 1 shows the learning curves obtained thisway.
The curves are more or less flat, i.e., increas-ing the amount of training data does not have a largeeffect on the performance of the model.
Further-more, even the smallest of our training sets is bigenough to outperform the best baseline.
Hence, it ispossible to do well on this task even with less train-ing data.
This is important, given that for spokentexts, paragraph boundaries may have to be obtainedby manual annotation.
The learning curves indicatethat relatively modest effort would be required toobtain training data were it not freely available.4.4 Human EvaluationWe established an upper bound against which ourautomatic methods could be compared by conduct-ing an experiment that assessed how well humansagree on identifying paragraph boundaries.
Fiveparticipants were given three English texts (onefrom each domain), selected randomly from the testcorpus.
Each text consisted of approximately a tenthof the original test set (i.e., 200?400 sentences).The participants were asked to insert paragraphbreaks wherever it seemed appropriate to them.
Noother instructions were given, as we wanted to seewhether they could independently perform the taskwithout any specific knowledge regarding the do-mains and their paragraphing conventions.We measured the agreement of the judges usingthe Kappa coefficient (Siegel and Castellan, 1988)but also report percentage agreement to facilitatecomparison with our models.
In all cases, we com-pute pairwise agreements and report the mean.
Ourresults are shown in Table 4.As can be seen, participants tend to agree witheach other on the task.
The least agreement is ob-served for the news domain.
This is somewhat ex-pected as the Wall Street Journal texts are rather dif-ficult to process for non-experts.
Also remember,that our subjects were given no instructions or train-ing.
In all cases our models yield an accuracy lowerthan the human agreement.
For the fiction domainthe best model is 5.67% lower than the upper bound,for the news domain it is 5.62% and for the parlia-ment domain it is 5.42% (see Tables 4 and 3).5 ConclusionIn this paper, we investigated whether it is possibleto predict paragraph boundaries automatically usinga supervised approach which exploits textual, syn-tactic and discourse cues.
We achieved accuraciesbetween 71.83% and 83.92%.
These were in all butone case significantly higher than the best baseline.We conducted our study in three different do-mains and languages and found that the best fea-tures for the news and parliamentary proceedingsdomains are based on word co-occurrence, whereasfeatures that exploit punctuation are better predic-tors for the fiction domain.
Models which incor-porate syntactic and discourse cue features do notlead to significant improvements over models thatdo not.
This means that paragraph boundaries canbe predicted by relying on low-level, language in-dependent features.
The task is therefore feasibleeven for languages for which parsers or cue wordlists are not readily available.We also experimented with training sets of differ-ent sizes and found that more training data does notnecessarily lead to significantly better results andthat it is possible to beat the best baseline comfort-ably even with a relatively small training set.Finally, we examined how well humans do onthis task.
Our results indicate that humans achievean average accuracy of about 77.45% to 88.58%,where some domains seem to be easier than others.Our models achieved accuracies of within 6% of hu-man performance.In the future, we plan to apply our model to newdomains (e.g., broadcast news or scientific papers),to non-Indo-European languages such as Arabic andChinese, and to machine generated texts.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.Machine Learning, 34(1/3):177?210.Eugene Charniak.
2001.
Immediate-head parsingfor language models.
In Proceedings of the 39thAnnual Meeting of the Association for Computa-tional Linguistics, pages 116?123, Toulouse.Heidi Christensen, Bala Kolluru, Yoshi Gotoh, andSteve Renals.
2004.
From text summarisation tostyle-specific summarisation for broadcast news.In Proceedings of the European Conference onInformation Retrieval, Sunderland.Philip Clarkson and Ronald Rosenfeld.
1997.Statistical language modeling.
In Proceedingsof ESCA EuroSpeech?97, pages 2707?2710,Rhodes.Dmitriy Genzel and Eugene Charniak.
2003.
Vari-ation of entropy and parse trees of sentences asa function of the sentence number.
In Proceed-ings of Empirical Methods in Natural LanguageProcessing, pages 65?72, Sapporo.Alexander G. Hauptmann and Michael A. Smith.1995.
Text, speech and vision for video segmen-tation: The informedia project.
In Proceedings ofthe AAAI Fall Symposium, Computational Mod-els for Integrating Language and Vision, Cam-bridge, MA.Marti A. Hearst.
1997.
TextTiling: Segmenting textinto multi-paragraph subtopic passages.
Compu-tational Linguistics, 23(1):33?64.Alistair Knott.
1996.
A Data-Driven Methodol-ogy for Motivating a Set of Coherence Rela-tions.
Ph.D. thesis, Department of Artificial In-telligence, University of Edinburgh.Philipp Koehn.
2002.
Europarl: A multilin-gual corpus for evaluation of machine trans-lation.
Unpublished Draft, http://www.isi.edu/?koehn/publications/europarl.ps.R.
E. Longacre.
1979.
The paragraph as a gram-matical unit.
Syntax and Semantics, 12:115?134.Fuchun Peng, Dale Schuurmans, Vlado Keselj, andShaojun Wang.
2003.
Language independentauthorship attribution using character level lan-guage models.
In Proceedings of the 11th Con-ference of the European Chapter of the Associ-ation for Computational Linguistics, pages 267?274, Budapest.Lev Pevzner and Marti Hearst.
2002.
A critiqueand improvement of an evaluation metric fortext segmentation.
Computational Linguistics,28(1o):19?36.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.A maximum entropy approach to identifying sen-tence boundaries.
In Proceedings of the FifthConference on Applied Natural Language Pro-cessing, Washington, DC.Robert E. Schapire and Yoram Singer.
2000.
Boos-texter: A boosting-based system for text catego-rization.
Machine Learning, 39(2/3):135?168.Sidney Siegel and N. John Castellan.
1988.
NonParametric Statistics for the Behavioral Sciences.McGraw-Hill, New York.Heather A. Stark.
1988.
What do paragraph mark-ings do?
Discourse Processes, 11:275?303.Mark Stevenson and Robert Gaizauskas.
2000.
Ex-periments on sentence boundary detection.
InProceedings of the 6th Applied Natural LangaugeProcessing Conference, pages 84?89, Seattle,WA.Masao Utiyama and Hitoshi Isahara.
2001.
A sta-tistical model for domain-independent text seg-mentation.
In Proceedings of the 39th AnnualMeeting of the Association for ComputationalLinguistics, pages 491?498, Toulouse.Chunshen Zhu.
1999.
Ut once more: The sentenceas the key functional unit of translation.
Meta,44(3):429?447.
