Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 109?116,Sydney, July 2006. c?2006 Association for Computational LinguisticsMultimodal Dialog Description Languagefor Rapid System DevelopmentMasahiro Araki               Kenji TachibanaKyoto Institute of TechnologyGraduate School of Science and Technology, Department of Information ScienceMatsugasaki Sakyo-ku Kyoto 606-8585 Japanaraki@dj.kit.ac.jpAbstractIn this paper, we explain a rapid devel-opment method of multimodal dialoguesys-tem using MIML (Multimodal Inter-action Markup Language), which definesdialogue patterns between human andvarious types of interactive agents.
Thefeature of this language is three-layereddescription of agent-based interactivesystems which separates task level de-scription, interaction description and de-vice dependent realization.
MIML hasadvantages in high-level interaction de-scription, modality extensibility andcompatibility with standardized tech-nologies.1 IntroductionIn recent years, various types of interactiveagents, such as personal robots, life-like agents(Kawamoto et al 2004), and animated agents aredeveloped for many purposes.
Such interactiveagents have an ability of speech communicationwith human by using automatic speech recog-nizer and speech synthesizer as a main modalityof communication.
The purpose of these interac-tive agents is to realize a user-friendly interfacefor information seeking, remote operation task,entertainment, etc.Each agent system is controlled by differentdescription language.
For example, Microsoftagent is controlled by JavaScript / VBScript em-bedded in HTML files, Galatea (Kawamoto et al.2004) is controlled by extended VoiceXML (inLinux version) and XISL (Katsurada et al 2003)(in Windows version).
In addition to this differ-ence, these languages do not have the ability ofhigher level task definition because the mainelements of these languages are the control ofmodality functions for each agent.
These makerapid development of multimodal system diffi-cult.In order to deal with these problems, we pro-pose a multimodal interaction description lan-guage, MIML (Multimodal Interaction MarkupLanguage), which defines dialogue patterns be-tween human and various types of interactiveagents by abstracting their functions.
The featureof this language is three-layered description ofagent-based interactive systems.The high-level description is a task definitionthat can easily construct typical agent-based in-teractive task control information.
The middle-level description is an interaction description thatdefines agent?s behavior and user?s input at thegranularity of dialogue segment.
The low-leveldescription is a platform dependent descriptionthat can override the pre-defined function in theinteraction description.The connection between task-level and inter-action-level is realized by generation of interac-tion description templates from the task leveldescription.
The connection between interaction-level and platform-level is realized by a bindingmechanism of XML.The rest of this paper consists as follows.
Sec-tion 2 describes the specification of the proposedlanguage.
Section 3 explains a process of rapidmultimodal dialogue system development.
Sec-tion 4 gives a comparison with existing multi-modal languages.
Section 5 states conclusionsand future works.1092 Specification of MIML2.1 Task level markup language2.1.1 Task classificationIn spoken dialogue system development, we pro-posed task classification based on the directionof information flow (Araki et al 1999).
We con-sider that the same analysis can be applied toagent based interactive systems (see Table 1).Table 1: Task classification of agent-based inter-active systemsClass Direction of Info.
flow Typical taskInformationassistantuser  agent Interactive presentationUser agentuser                                  agentcontrol of homenetwork equip-mentsQuestionand Answeruser                 agent daily life in-formation queryIn the information assistant class, the agenthas information to be presented to the user.Typically, the information contents are Webpages, an instruction of consumer product usage,an educational content, etc.
Sometimes the con-tents are too long to deliver all the information tothe user.
Therefore, it needs user model that canmanage user?s preference and past interactionrecords in order to select or filter out the contents.In the user agent class, the user has informa-tion to be delivered to the agent in order toachieve a user?s goal.
Typically, the informationis a command to control networked homeequipments, travel schedule to reserve a trainticket, etc.
The agent mediates between user andtarget application in order to make user?s inputappropriate and easy at the client side process(e.g.
checking a mandatory filed to be filled,automatic filling with personal data (name, ad-dress, e-mail, etc.
)).In the Question and Answer class, the user hasan intention to acquire some information fromthe agent that can access to the Web or a data-base.
First, the user makes a query in natural lan-guage, and then the agent makes a response ac-cording to the result of the information retrieval.If too much information is retrieved, the agentmakes a narrowing down subdialogue.
If there isno information that matches user?s query, theagent makes a request to reformulate an initialquery.
If the amount of retrieved information isappropriate to deliver to the user by using currentmodality, the agent reports the results to the user.The appropriate amount of information differs inthe main interaction modality of the target device,such as small display, normal graphic display orspeech.
Therefore, it needs the information ofmedia capability of the target device.2.1.2 Overview of task markup languageAs a result of above investigation, we specifythe task level interaction description languageshown in Figure 1.taskmlbodyheaduserModel deviceModelsection*xformsqasearchquery resultmodel inputFigure.
1  Structure of the Task Markup Lan-guage.The features of this language are (1) the abilityto model each participant of dialogue (i.e.
userand agent) and (2) to provide an executionframework of each class of task.The task markup language <taskml> consistsof two parts corresponding to above mentionedfeatures: <head> part and <body> part.
The<head> part specifies models of the user (by<userModel> element) and the agent (by <de-viceModel> element).
The content of each modelis described in section 2.1.3.
The <body> partspecifies a class of interaction task.
The contentof each task is declaratively specified under the<section>, <xforms> and <qa> elements, whichare explained in section 2.1.4.2.1.3 Head part of task markup languageIn the <head> element of the task markup lan-guage, the developer can specify user model in<userModel> element and agent model in <de-viceModel> element.In the <userModel> element, the developerdeclares variables which represent user?s infor-mation, such as expertise to domain, expertise todialogue system, interest level to the contents,etc.In the <deviceModel> element, the developercan specify the type of interactive agent andmain modality of interaction.
This information is(* means theelement canrepeat morethan 1 time)110used for generating template from this task de-scription to interaction descriptions.2.1.4 Body part of task markup languageAccording to the class of the task, the <body>element consists of a sequence of <section> ele-ments, a <xforms> element or a <qa> element.The <section> element represents a piece ofinformation in the task of the information assis-tant class.
The attributes of this element are id,start time and end time of the presentation mate-rial and declared user model variable which indi-cates whether this section meets the user?s needsor knowledge level.
The child elements of the<section> element specify multimodal presenta-tion.
These elements are the same set of the childelements of <output> element in the interactionlevel description explained in the next subsection.Also, there is a <interaction> element as a childelement of the <section> element which specifiesagent interaction pattern description as an exter-nal pointer.
It is used for additional commentgenerated by the agent to the presented contents.For the sake of this separation of contents andadditional comments, the developer can easilyadd agent?s behavior in accordance with the usermodel.
The interaction flow of this class isshown in Figure 2.startinteractionpresentationquestion andanswersubdialogyesendnoend ofsections?Multimediacontentsmatchesuser model?yesnext sectionno.Figure.
2  Interaction flow of Information AssistclassThe <xforms> element represents a group ofinformation in the task of the user agent class.
Itspecifies a data model, constraint of the valueand submission action following the notation ofXForms 1.0.In the task of user agent class, the role of in-teractive agent is to collect information from theuser in order to achieve a specific task, such ashotel reservation.
XForms is designed to separatethe data structure of information and the appear-ance at the user?s client, such as using text fieldinput, radio button, pull-down menu, etc.
becausesuch interface appearances are different in de-vices even in GUI-based systems.
If the devel-oper wants to use multimodal input for the user?sclient, such separation of the data structure andthe appearance, i.e.
how to show the necessaryinformation and how to get user?s input, is veryimportant.In MIML, such device dependent ?appearance?information is defined in interaction level.
There-fore, in this user agent class, the task descriptionis only to define data structure because interac-tion flows of this task can be limited to the typi-cal patterns.
For example, in hotel reservation, asa result of AP (application) access, if there is noavailable room at the requested date, the user?sreservation request is rejected.
If the system rec-ommends an alternative choice to the user, theinteraction branches to subdialogue of recom-mendation, after the first user?s request is proc-essed (see Figure 3).
The interaction pattern ofeach subdialogue is described in the interactionlevel markup language.startslot fillingAP accessall requiredslots are filled?confirmationdialoguerejectiondialogueyesnoendapplicationrecommendationdialogueaccept?yesnoFigure.
3  Interaction flow of User Agent classThe <qa> element consists of three children:<query>, <search> and <result>.The content of <query> element is the same asthe <xforms> element explained above.
However,generated interaction patterns are different inuser agent class and question and answer class.In user agent class, all the values (except for op-tional slots indicated explicitly) are expected tobe filled.
On the contrary, in question and answerclass, a subset of slots defined by form descrip-tion can make a query.
Therefore, the first ex-111change of the question and answer class task issystem?s prompt and user?s query input.The <search> element represents applicationcommand using the variable defined in the<query> element.
Such application commandcan be a database access command or SPARQL(Simple Protocol And RDF Query Language)1 incase of Semantic Web search.The <result> element specifies which informa-tion to be delivered to the user from the queryresult.
The behavior of back-end application ofthis class is not as simple as user agent class.
Iftoo many results are searched, the system transitsto narrowing down subdialogue.
If no result issearched, the system transits to subdialogue thatrelaxes initial user?s query.
If appropriate num-ber (it depends on presentation media) of resultsare searched, the presentation subdialogue begins.The flow of interaction is shown in Figure 4.Figure.
4  Interaction flow of Question and An-swer class2.2 Interaction level markup language2.2.1 Overview of interaction markup lan-guagePreviously, we proposed a multimodal interac-tion markup language (Araki et al 2004) as anextension of VoiceXML2.
In this paper, we mod-ify the previous proposal for specializing human-agent interaction and for realizing interactionpattern defined in the task level markup language.The main extension is a definition of modalityindependent elements for input and output.
InVoiceXML, system?s audio prompt is defined in<prompt> element as a child of <field> element1 http://www.w3.org/TR/rdf-sparql-query/2 http://www.w3.org/TR/voicexml20/that defines atomic interaction acquiring thevalue of the variable.
User?s speech input patternis defined by <grammar> element under <field>element.
In our MIML, <grammar> element isreplaced by the <input> element which specifiesactive input modalities and their input pattern tobe bund to the variable that is indicated as nameattribute of the <field> element.
Also, <prompt>element is replaced by the <output> elementwhich specifies active output modalities and asource media file or contents to be presented tothe user.
In <output> element, the developer canspecify agent?s behavior by using <agent> ele-ment.
The outline of this interaction levelmarkup language is shown in Figure 5.mmvxmlformlinkblock*field filledoutputinput filled***initialinputcatch*audiovideopageagentsmilspeechimagetouchFigure.
5  Structure of Interaction level MarkupLanguage2.2.2 Input and output control in agentThe <input> element and the <output> elementare designed for implementing various types ofinteractive agent systems.The <input> element specifies the input proc-essing of each modality.
For speech input,grammar attribute of <speech> element specifiesuser?s input pattern by SRGS (Speech Recogni-tion Grammar Specification)3 , or alternatively,type attribute specifies built-in grammar such asBoolean, date, digit, etc.
For image input, typeattribute of <image> element specifies built-inbehavior for camera input, such as nod, faceRec-ognition, etc.
For touch input, the value of thevariable is given by referring external definitionof the relation between displayed object and itsvalue.The <output> element specifies the outputcontrol of each modality.
Each child element of3 http://www.w3.org/TR/speech-grammar/startinitial queryinputsearchDB# ofresultsrelaxationdialoguereportdialoguenarrowingdownsubdialog0appropriatetoo manyendWeb112this element is performed in parallel.
If the de-veloper wants to make sequential output, itshould be written in <smil> element (Synchro-nized Multimedia Integration Language) 4 , Foraudio output, <audio> element works as thesame way as VoiceXML, that is, the content ofthe element is passed to TTS (Text-to-Speechmodule) and if the audio file is specified by thesrc attribute, it is a prior output.
In <video>,<page> (e.g.
HTML) and <smil> (for rich mul-timedia presentation) output, each element speci-fies the contents file by src attribute.
In <agent>element, the agent?s behavior definition, such asmove, emotion, status attribute specifies the pa-rameter for each action.2.3 Platform level descriptionThe differences of agent and other devices forinput/output are absorbed in this level.
In interac-tion level markup language, <agent> elementspecifies agent?s behavior.
However, some agentcan move in a real world (e.g.
personal robot),some agent can move on a computer screen (e.g.Microsoft Agent), and some cannot move butdisplay their face (e.g.
life-like agent).One solution for dealing with such variety ofbehavior is to define many attributes at <agent>element, for example, move, facial expression,gesture, point, etc.
However, the defects of thissolution are inflexibility of correspondence toprogress of agent technology (if an agent addsnew ability to its behavior, the specification oflanguage should be changed) and interference ofreusability of interaction description (descriptionfor one agent cannot apply to another agent).Our solution is to use the binding mechanismin XML language between interaction level andplatform dependent level.
We assume defaultbehavior for each value of the move, emotionand status attributes of the <agent> element.
Ifsuch default behavior is not enough for somepurpose, the developer can override the agent?sbehavior using binding mechanism and theagent?s native control language.
As a result, theplatform level description is embedded in bind-ing language described in next section.3 Rapid system development3.1 Usage of application frameworkEach task class has a typical execution steps asinvestigated in previous section.
Therefore a sys-tem developer has to specify a data model and4 http://www.w3.org/AudioVideo/specific information for each task execution.Web application framework can drive interactivetask using these declarative parameters.As an application framework, we use Struts5which is based on Model-View-Controller (MVC)model.
It clearly separates application logic(model part), transition of interaction (controllerpart) and user interface (view part).
AlthoughMVC model is popular in GUI-based Web appli-cation, it can be applied in speech-based applica-tion because any modality dependent informationcan be excluded from the view part.
Struts pro-vides (1) a controller mechanism and (2) integra-tion mechanism with the back-end applicationpart and the user interface part.
In driving Struts,a developer has to (1) define a data class whichstores the user?s input and responding results, (2)make action mapping rules which defines a tran-sition pattern of the target interactive system, and(3) make the view part which defines human-computer interaction patterns.
The process ofStruts begins by the request from the user client(typically in HTML, form data is submitted tothe Web server via HTTP post method).The controller catches the request and storesthe submitted data to the data class, and thencalls the action class specified by the request fol-lowing the definition of action mapping rules.The action class communicates with the back-end application, such as database managementsystem or outside Web servers by referring thedata class, and returns the status of the process-ing to the controller.
According to the status, thecontroller refers the action mapping rules andselects the view file which is passed to the user?sclient.
Basically, this view file is written in JavaServer Pages, which can be any XML file thatincludes Java code or useful tag libraries.
Usingthis embedded programming method, the resultsof the application processing is reflected to theresponse.
The flow of processing in the Struts isshown in Figure 6.Figure.
6  MVC model.5 http:// struts.apache.orguserinterface controllerapplicationlogicdataclassActionmappingrequestresultscallstatuslookupview modelcontroller113The first step of rapid development is to pre-pare backend application (Typically using Data-base Management System) and their applicationlogic code.
The action mapping file and dataclass file are created automatically from the tasklevel description described next subsection.3.2 Task definitionFigure 7 shows an example description of theinformation assistant task.
In this task setting,video contents which are divided into sectionsare presented to the user one by one.
At the endof a section, a robot agent put in a word in orderto help user?s understanding and to measure theuser?s preference (e.g.
by the recognition of ac-knowledging, nodding, etc.)
.
If low user?s pref-erence is observed, unimportant parts of thepresentation are skipped and comments of therobot are adjusted to beginner?s level.
The im-portance of the section is indicated by interes-tLevel attribute and knowledgeLevel attributethat are introduced in the <userModel> element.If one of the values of these attribute is below thecurrent value of the user model, the relevant sec-tion is skipped.
The skipping mechanism usinguser model variables is automatically insertedinto an interaction level description.Figure.
7  An Example of Task Markup Lan-guage.3.3 Describing InteractionThe connection between task-level and interac-tion-level is realized by generation of interactiondescription templates from the task level descrip-tion.
The interaction level description corre-sponds to the view part of the MVC model onwhich task level description is based.
From thispoint of view, task level language specificationgives higher level parameters over MVC frame-work which restricts behavior of the model fortypical interactive application patterns.
Therefore,from this pattern information, the skeletons ofthe view part of each typical pattern can be gen-erated based on the device model information intask markup language.For example, by the task level descriptionshown in Figure 7, data class is generated from<userModel> element by mapping the field ofthe class to user model variable, and action map-ping rule set is generated using the sequence in-formation of <section> elements.
The branch isrealized by calling application logic which com-pares the attribute variables of the <section> anduser model data class.
Following action mappingrule, the interaction level description is generatedfor each <section> element.
In information assis-tant class, a <section> element corresponds totwo interaction level descriptions: the one is pre-senting contents which transform <video> ele-ment to the <output> elements and the other isinteracting with user, such as shown in Figure 8.The latter file is merely a skeleton.
Therefore,the developer has to fill the system?s prompt,specify user?s input and add corresponding ac-tions.Figure 8 describes an interaction as follows: atthe end of some segment, the agent asks the userwhether the contents are interesting or not.
Theuser can reply by speech or by nodding gesture.If the user?s response is affirmative, the globalvariable of interest level in user model is incre-mented.<taskml type="infoAssist"><head><userModel><interestLevel/><knowledgeLevel/></userModel><deviceModelmainMode="speech" agentType="robot"/></head><body><section id="001"s_time="00:00:00" e_time="00:00:50"intersetLevel="1"  knowledgeLevel="1"><video src="vtr1.avi" /><interaction name="interest1.mmi"s_time="00:00:30"/></section>...</body></taskml>114Bool speak(String message){Module m=Call TTS-module;m.set(message);m.speak(message);release m;}Bool speak(String message){Module m=Call TTS-module;m.set(message);m.speak(message);release m;}<message><head><to>TTS-module</to><from>DM</fro ><head><body>Set Text ?hello?</body></message><audio>Hello</audio>??
??
?nChild Place+???
?nFigure.
8  An Example of Interaction levelMarkup Language.3.4 Adaptation to multiple interaction de-vicesThe connection between interaction-level andplatform-level is realized by binding mechanismof XML.
XBL (XML Binding Language)6 wasoriginally defined for smart user interface de-scription, extended for SVG afterwards, and fur-thermore, for general XML language.
The con-cept of binding in XBL is a tree extension byinheriting the value of attributes to the sub tree(see Figure 9).
As a result of this mechanism, thebase language, in this the case interactionmarkup language, can keep its simplicity butdoes not loose flexibility.Figure.
9  Concept of XML binding.By using this mechanism, we implementedvarious types of weather information system,6 http://www.w3.org/TR/xbl/such as Microsoft agent (Figure 10), Galatea(Figure 11) and a personal robot.
The platformchange is made only by modifying agentTypeattribute of <deviceModel> element of taskML.Figure.
10 Interaction with Microsoft agent.Figure.
11 Interaction with Galatea.4 Comparison with existing multimodallanguageThere are several multimodal interaction systems,mainly in research level (L?pez-C?zar and Araki2005).
XHTML+Voice 7  and SALT 8  are mostpopular multimodal interaction description lan-guages.
These two languages concentrate on howto add speech interaction on graphical Webpages by adding spoken dialogue description to(X)HTML codes.
These are not suitable for adescription of virtual agent interactions.
(Fernando D?Haro et al 2005) proposes newmultimodal languages for several layers.
Theirproposal is mainly on development environmentwhich supports development steps but for lan-guage itself.
In contrary to that, our proposal is a7 http://www-306.ibm.com/software/pervasive/multimodal/x%2Bv/11/spec.htm8 http://www.saltforum.org/<mmvxml><form><field name=?question?><input><speech type=?boolean?/><image type=?nod?/></input><output><audio> Is it interesting?
</audio></output><filled><if cond=?question==true?><assign name=?intersestLevel?expr=?
intersestLevel+1?/></if><submit src=?http://localhost:8080/step2/></filled></field></form></mmvxml>115simplified language and framework that auto-mate several steps for system development.5 Conclusion and future worksIn this paper, we explained a rapid developmentmethod of multimodal dialogue system usingMIML.
This language can be extended for morecomplex task settings, such as multi-scenariopresentation and multiple-task agents.
Althoughit is difficult to realize multi-scenario presenta-tion by the proposed filtering method, it can betreated by extending filtering concept to discretevariable and enriching the data type of <user-Model> variables.
For example, if the value of<knowledgeLevel> variable in Figure 7 can takeone of ?expert?, ?moderate?
and ?novice?, andeach scenario in multi-scenario presentation ismarked with these values, multi-scenario presen-tation can be realized by filtering with discretevariables.
In case of multiple-task agents, we canimplement such agents by adding one additionalinteraction description which guides to branchvarious tasks.AcknowledgmentsAuthors would like to thank the members ofISTC/MMI markup language working group fortheir useful discussions.ReferencesM.
Araki, K. Komatani, T. Hirata and S. Doshita.1999.
A Dialogue Library for Task-oriented Spo-ken Dialogue Systems, Proc.
IJCAI Workshop onKnowledge and Reasoning in Practical DialogueSystems, pp.1-7.M.
Araki, K. Ueda, M. Akita, T. Nishimoto and Y.Niimi.
2002.
Proposal of a Multimodal DialogueDescription Language, In Proc.
of PRICAI 02.L.
Fernando D?Haro et al 2005.
An advanced plat-form to speed up the design of multilingual dialogapplications for multiple modalities, Speech Com-munication, in Press.R.
L?pez-C?zar Delgado, M Araki.
2005.
Spoken,Multilingual and Multimodal Dialogue Systems:Development and Assessment, Wiley.K.
Katsurada, Y. Nakamura, H. Yamada, T. Nitta.2003.
XISL: A Language for Describing Multimo-dal Interaction Scenarios, Proc.
of ICMI'03,pp.281-284.S.
Kawamoto, H. Shimodaira, T. Nitta, T. Nishimoto,S.
Nakamura, K. Itou, S. Morishima, T. Yotsukura,A.
Kai, A. Lee, Y. Yamashita, T. Kobayashi, K.Tokuda, K. Hirose, N. Minematsu, A. Yamada, Y.Den, T. Utsuro and S. Sagayama.
2004.
Galatea:Open-Source Software for Developing Anthropo-morphic Spoken Dialog Agents, In Life-Like Char-acters.
Tools, Affective   Functions, and Applica-tions.
ed.
H. Prendinger and M. Ishizuka, pp.187-212, Springer.116
