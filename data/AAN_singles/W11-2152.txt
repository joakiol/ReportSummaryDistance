Proceedings of the 6th Workshop on Statistical Machine Translation, pages 426?432,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsTwo-step translation with grammatical post-processing?David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?
and Ondr?ej BojarCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, Prague{marecek,rosa,galuscakova,bojar}@ufal.mff.cuni.czAbstractThis paper describes an experiment in whichwe try to automatically correct mistakes ingrammatical agreement in English to CzechMT outputs.
We perform several rule-basedcorrections on sentences parsed to dependencytrees.
We prove that it is possible to improvethe MT quality of majority of the systems par-ticipating in WMT shared task.
We made bothautomatic (BLEU) and manual evaluations.1 IntroductionThis paper is a joint report on two English-to-Czechsubmissions to the WMT11 shared translation task.The main contribution is however the proposal andevaluation of a rule-based post-processing systemDEPFIX aimed at correcting errors in Czech gram-mar applicable to any MT system.
This is somewhatthe converse of other approaches (e.g.
Simard et al(2007)) where a statistical system was applied forthe post-processing of a rule-based one.2 Our phrase-based systemsThis section briefly describes our underlying phrase-based systems.
One of them (CU-BOJAR) was sub-mitted directly to the WMT11 manual evaluation,the other one (CU-TWOSTEP) was first corrected bythe proposed method (Section 3 below) and thensubmitted under the name CU-MARECEK.
?This research has been supported by the European UnionSeventh Framework Programme (FP7) under grant agreementn?
247762 (Faust), n?
231720 (EuroMatrix Plus), and by thegrants GAUK 116310 and GA201/09/H057.2.1 Data for statistical systemsOur training parallel data consists of CzEng 0.9(Bojar and Z?abokrtsky?, 2009), the News Commen-tary corpus v.6 as released by the WMT11 orga-nizers, the EMEA corpus, a corpus collected fromthe transcripts of TED talks (http://www.ted.com),the parallel news and separately some of the par-allel web pages of the European Commission(http://ec.europa.eu), and the Official Journal of theEuropean Union as released by the Apertium con-sortium (http://apertium.eu/data).A custom web crawler was used for the EuropeanCommission website.
English and Czech websiteswere matched according to their URLs.
Unfortu-nately, Czech websites very often contain untrans-lated parts of English texts.
Because of this, weaimed especially at the news articles, which are veryoften translated correctly and also more relevant forthe shared task.
Texts were segmented using train-able tokenizer (Klyueva and Bojar, 2008) and dedu-plicated.
Processed texts were automatically alignedby Hunalign (Varga and others, 2005).The data from the Official Journal were first con-verted from XML to plain text.
The documents werepaired according to their filenames.
To better han-dle the nature of these data, we decided to dividethe documents into two classes based on the aver-age number of words per sentence: ?lists?
are docu-ments with less than 2.8 words per sentence, otherdocuments are called ?texts?.
The corresponding?lists?
were aligned line by line.
The corresponding?texts?
were automatically segmented by trainabletokenizer and aligned automatically by Hunalign.We use the following two Czech language mod-426els, their weights are optimized in MERT:?
5-gram LM from the Czech side of CzEng (ex-cluding the Navajo section).
The LM was con-structed by interpolating LMs of the individual do-mains (news, EU legislation, technical documenta-tion, etc.)
to achieve the lowest perplexity on theWMT08 news test set.?
6-gram LM from the monolingual data supplied byWMT11 organizers (news of the individual yearsand News Commentary), the Czech National Cor-pus and a web collection of Czech texts.
Again, thefinal LM is constructed by interpolating the smallerLMs1 for the WMT08 news test set.2.2 Baseline Moses (CU-BOJAR)The system denoted CU-BOJAR for English-to-Czech is simple phrase-based translation, i.e.
Moseswithout factors.
We tokenized, lemmatized andtagged all texts using the tools wrapped in TectoMT(Popel and Z?abokrtsky?, 2010).
We further tokenizee.g.
dashed words (?23-year?)
after all the process-ing is finished.
Phrase-based MT is then able tohandle such expressions both at once, or decomposethem as needed to cover unseen variations.
We uselexicalized reordering (orientation-bidirectional-fe).The translation runs in ?supervised truecase?, whichmeans that we use the output of our lemmatizersto decide whether the word should be lowercasedor should preserve uppercasing.
After the transla-tion, the first letter in the output is simply upper-cased.
The model is optimized using Moses?
stan-dard MERT on the WMT09 test set.The organizers of WMT11 encouraged partici-pants to apply simple normalization to their data(both for training and testing).2 The main purposeof the normalization is to improve the consistency oftypographical rules.
Unfortunately, some of the au-tomatic changes may accidentally damage the mean-ing of the expression.3 We therefore opted to submit1The interpolated LM file (gzipped ARPA format) is 5.1 GBso we applied LM pruning as implemented in SRI toolkit withthe threshold 10?14 to reduce the file size to 2.3 GB.2http://www.statmt.org/wmt11/normalize-punctuation.perl3Fixing the ordering of the full stop and the quote is wrongbecause the order (at least in Czech typesetting) depends onwhether it is the full sentence or a final phrase that is capturedin the quotes.
Even riskier are rules handling decimal and thou-sand separators in numbers.
While there are language-specificconventions, they are not always followed and the normaliza-tion can in such cases confuse the order of magnitude by 3.the output based on non-normalized test sets as ourprimary English-to-Czech submission.We invested much less effort into the submissioncalled CU-BOJAR for Czech-to-English.
The onlyinteresting feature there is the use of alternative de-coding paths to translate either from the Czech formor from the Czech lemma equipped with meaning-bearing morphological properties, e.g.
the numberof nouns.
Bojar and Kos (2010) used the same setupwith simple lemmas in the fallback decoding path.The enriched lemmas perform marginally better.2.3 Two-step translationOur two-step translation is essentially the samesetup as detailed by Bojar and Kos (2010): (1)the English source is translated to simplified Czech,and (2) the simplified Czech is monotonically trans-lated to fully inflected Czech.
Both steps are sim-ple phrase-based models.
Instead of word forms, thesimplified Czech uses lemmas enriched by a sub-set of morphological features selected manually toencode only properties overt both in English andCzech such as the tense of verbs or number of nouns.Czech-specific morphological properties indicatingvarious agreements (e.g.
number and gender of ad-jectives, gender of verbs) are imposed in the secondstep solely on the basis of the language model.The first step uses the same parallel and mono-lingual corpora as CU-BOJAR, except the LMs beingtrained on the enriched lemmas, not on word forms.The second step uses exactly the same LM as CU-BOJAR but the phrase-table is extracted from all ourCzech monolingual data (phrase length limit of 1.
)3 Grammatical post-processingPhrase-based machine translation systems oftenhave problems with grammatical agreement, espe-cially on longer dependencies.
Sometimes, there isa mistake in agreement even between adjacent wordsbecause each one belongs to a different phrase.
Thegoal of our post-processing is to correct forms ofsome words so that they do not violate grammaticalrules (eg.
grammatical agreement).The problem is how to find the correct syntacticrelations in the output of an MT system.
Parserstrained on correct sentences can rely on grammat-ical agreement, according to which they determine427the dependencies between words.
Unfortunately, theagreement in MT outputs is often wrong and theparser fails to produce a correct parse tree.
There-fore, we would need a parser trained on a manuallyannotated treebank consisting of specific outputs ofmachine translation systems.
Such a treebank doesnot exist and we do not even want to create one, be-cause the MT systems are changing constantly andalso because manual annotation of texts that are of-ten not even understandable would be almost a su-perhuman task.The DEPFIX system was implemented in TectoMTframework (Popel and Z?abokrtsky?, 2010).
MT out-puts were tagged by Morc?e tagger (Spoustova?
et al,2007) and then parsed with MST parser (McDon-ald et al, 2005) that was trained on the Prague De-pendency Treebank (Hajic?
and others, 2006), i.e.on correct Czech sentences.
We used an improvedimplementation with some additional features es-pecially tuned for Czech (Nova?k and Z?abokrtsky?,2007).
The parser accuracy is much lower on the?noisy?
MT output sentences, but a lot of dependen-cies in which we are to correct grammatical agree-ment are determined correctly.
Adapting the parserfor outputs of MT systems will be addressed in thecoming months.A typical example of a correction is the agreementbetween the subject and the predicate: they shouldshare the morphological number and gender.
If theydo not, we simply change the number and genderof the predicate in agreement with the subject.4 Anexample of such a changed predicate is in Figure 1.Apart from the dependency tree of the target sen-tence, we can also use the dependency tree of thesource sentence.
Source sentences are grammat-ically correct and the accuracy of the tagger andthe parser is accordingly higher there.
Words inthe source and target sentences are aligned usingGIZA++5 (Och and Ney, 2003) but verbose outputsof the original MT systems would be possibly a bet-ter option.
The rules for fixing grammatical agree-ment between words can thus consider also the de-pendency relations and morphological caregories oftheir English counterparts in the input sentence.4In this case, we suppose that the number of the subject hasa much higher chance to be correct.5GIZA++ was run on lemmatized texts in both directionsand intersection symmetrization was used.SomepeoplecamelaterAtrSbPredAdvplpl.AuxKp?i?liPredplN?kte?
?lid?p?i?elpozd?jiAtrSbPredAdvsg, mpl.AuxKFigure 1: Example of fixing subject-predicate agreement.The Czech word pr?is?el [he came] has a wrong morpho-logical number and gender.3.1 Grammatical rulesWe have manually devised a set of the followingrules.
Their input is the dependency tree of a Czechsentence (MT output) and its English source sen-tence (MT input) with the nodes aligned where pos-sible.
Each of the rules fires if the specified con-ditions (?IF?)
are matched, executes the command(?DO?)
, usually changing one or more morphologi-cal categories of the word, and generates a new wordform for any word which was changed.The rules make use of several morphological cat-egories of the word (node:number, node:gender...),its syntactic relation to its parent in the dependencytree (node:afun) and the same information for itsEnglish counterpart (node:en) and other nodes inthe dependency trees.The order of the rules in this paper follows theorder in which they are applied; this is important, asoften a rule changes a morphological category of aword which is then used by a subsequent rule.3.1.1 Noun number (NounNum)In Czech, a word in singular sometimes has thesame form as in plural.
Because the tagger oftenfails to tag the word correctly, we try to correct thetag of a noun tagged as singular if its English coun-terpart is in plural, so that the subsequent rules canwork correctly.We trust the form of the word but changing thenumber may also require to change the morphologi-cal case (i.e.
the tagger was wrong with both numberand case).
In such cases we choose the first (linearly428from nominative to instrumentative) case matchingthe form.
The rule is:IF: node:pos = noun &node:number = singular &node:en:number = pluralDO: node:number := plural;node:case := find case(node:form, plural);3.1.2 Subject case (SubjCase)The subject of a Czech sentence must be in thenominative case.
Since the parser often fails inmarking the correct word as a subject, we use theEnglish source sentence and presuppose that theCzech counterpart of the English subject is also asubject in the Czech sentence.IF: node:en:afun = subjectDO: node:case := nominative;3.1.3 Subject-predicate agreement (SubjPred)Subject and predicate in Czech agree in their mor-phological number.
To identify a Czech Subject, wetrust the subject in the English sentence.
Then wecopy the number from the (Czech) Subject to theCzech Predicate.IF: node:en:afun = subject &parent:afun = predicateDO: parent:number := node:number;3.1.4 Subject-past participle agreement (SubjPP)Czech past participles agree with subject inmorphological gender.IF: node:pos = noun|pronoun &node:en:afun = subject &parent:pos = verb past participleDO: parent:number := node:number;parent:gender := node:gender;3.1.5 Preposition without children (PrepNoCh)In our dependency trees, the preposition is theparent of the words it belongs to (usually a noun).
Apreposition without children is incorrect so we findnodes aligned to its English counterpart?s childrenand rehang them under the preposition.IF: node:afun = preposition &!node:has children &node:en:has childrenDO: foreach node:en:child;node:en:child:cs:parent := node;3.1.6 Preposition-noun agreement (PrepNoun)Every prepositions gets a morphological case as-signed to it by the tagger, with which the dependentnoun should agree.IF: parent:pos = preposition &node:pos = nounDO: node:case := parent:case;3.1.7 Noun-adjective agreement (NounAdj)Czech adjectives and nouns agree in morpholog-ical gender, number and case.
We assume that thenoun is correct and change the adjective accordingly.IF: node:pos = adjective &parent:pos = nounDO: node:gender := parent:gender;node:number := parent:number;node:case := parent:case;3.1.8 Reflexive particle deletion (ReflTant)Czech reflexive verbs are accompanied by reflex-ive particles (?se?
and ?si?).
We delete particles notbeloning to any verb (or adjective derived from averb).IF: node:form = ?se?|?si?
&node:pos = pronoun &parent:pos != verb|verbal adjectiveDO: remove node;4 Experiments and resultsWe tested our CU-TWOSTEP system with DEPFIXpost-processing on both WMT10 and WMT11 test-ing data.
This combined system was submitted toshared translation task as CU-MARECEK.
We alsoran the DEPFIX post-processing on all other partici-pating systems.4.1 Automatic evaluationThe achieved BLEU scores are shown in Tables 1and 2.
They show the scores before and after theDEPFIX post-processing.
It is interesting that theimprovements are quite different between the years2010 and 2011 in terms of their BLEU score.
Whilethe average improvement on WMT10 test set was0.21 BLEU points, it was only 0.05 BLEU points onthe WMT11 test set.
Even the results of the sameTWOSTEP system differ in a similar way, so it musthave been caused by the different data.429system before after improvementcu-twostep 15.98 16.13 0.15 (0.05 - 0.26)cmu-heaf.
16.95 17.04 0.09 (-0.01 - 0.20)cu-bojar 15.85 16.09 0.24 (0.14 - 0.36)cu-zeman 12.33 12.55 0.22 (0.12 - 0.32)dcu 13.36 13.59 0.23 (0.13 - 0.37)dcu-combo 18.79 18.90 0.11 (0.02 - 0.23)eurotrans 10.10 10.11 0.01 (-0.04 - 0.07)koc 11.74 11.91 0.17 (0.08 - 0.26)koc-combo 16.60 16.86 0.26 (0.16 - 0.37)onlineA 11.81 12.08 0.27 (0.17 - 0.38)onlineB 16.57 16.79 0.22 (0.11 - 0.33)potsdam 12.34 12.57 0.23 (0.14 - 0.35)rwth-combo 17.54 17.79 0.25 (0.15 - 0.35)sfu 11.43 11.83 0.40 (0.29 - 0.52)uedin 15.91 16.19 0.28 (0.18 - 0.40)upv-combo 17.51 17.73 0.22 (0.10 - 0.34)Table 1: Depfix improvements on the WMT10 systemsin BLEU score.
Confidence intervals, which were com-puted on 1000 bootstrap samples, are in brackets.system before after improvementcu-twostep 16.57 16.60 0.03 (-0.07 - 0.13)cmu-heaf.
20.24 20.32 0.08 (-0.03 - 0.19)commerc2 09.32 09.32 0.00 (-0.04 - 0.04)cu-bojar 16.88 16.85 -0.03 (-0.12 - 0.07)cu-popel 14.12 14.11 -0.01 (-0.06 - 0.03)cu-tamch.
16.32 16.28 -0.04 (-0.14 - 0.06)cu-zeman 14.61 14.80 0.19 (0.09 - 0.29)jhu 17.36 17.42 0.06 (-0.03 - 0.16)online-B 20.26 20.31 0.05 (-0.06 - 0.16)udein 17.80 17.88 0.08 (-0.02 - 0.17)upv-prhlt.
20.68 20.69 0.01 (-0.08 - 0.11)Table 2: Depfix improvements on the WMT11 systemsin BLEU score.
Confidence intervals are in brackets.4.2 Manual evaluationTwo independent annotators evaluated DEPFIX man-ually on the outputs of CU-TWOSTEP and ONLINE-B.
We randomly selected 1000 sentences from thenewssyscombtest2011 data set and the appropri-ate translations made by these two systems.
Theannotators got the outputs before and after DEPFIXpost-processing and their task was to decide whichtranslation6 from these two is better and label it bythe letter ?a?.
If it was not possible to determine6They were also provided with the source English sentenceand the reference translation.
The options were shuffled andindentical candidate sentences were collapsed.A / B improved worsened indefinite totalimproved 273 20 15 308worsened 12 59 7 78indefinite 53 35 42 130total 338 114 64 516Table 5: Matrix of the inter-annotator agreementrule fired impr.
wors.
% impr.SubjCase 51 46 5 90.2SubjPP 193 165 28 85.5NounAdj 434 354 80 81.6NounNum 156 122 34 78.2PrepNoun 135 99 36 73.3SubjPred 68 48 20 70.6ReflTant 15 10 5 66.7PrepNoCh 45 29 16 64.4Table 6: Rules and their utility.which is better, they labeled both by ?n?.Table 3 below shows that about 60% of sentencesfixed by DEPFIX were improved and only about 20%were worsened.
DEPFIX worked a little better on theONLINE-B, making fewer changes but also fewerwrong changes.
It is probably connected with thefact that overall better translations by ONLINE-B areeasier to parse.The matrix of inter-annotator agreement is in Ta-ble 5.
Our two annotators agreed in 374 sentences(out of 516), that is 72.5%.
On the other hand, ifwe consider only cases where both annotators chosedifferent translation as better (no indefinite marks),we get only 8.8% disagreement (32 out of 364).Using the manual evaluation, we can also measureperformance of the individual rules.
Table 6 showsthe number of all, improved or worsened sentenceswhere a particular rule was applied.
Definitely, themost useful rule (used often and quite reliable) wasthe one correcting noun-adjective agreement, fol-lowed by the subject-pastparticiple agreement rule.In each changed sentence, two rules (not neces-sarily related ones) were applied on average.4.3 Manual evaluation across data setsThe fact that the improvements in BLEU scores onWMT10 test set are much higher has led us to onemore experiment: we compare manual annotationsof 330 sentences from each of the WMT10 and430system annotator changed improved worsened indefinitecount % count % count %cu-bojar-twostep A 269 152 56.5 39 14.5 78 29.0cu-bojar-twostep B 269 173 64.3 50 18.6 46 17.1online-B A 247 156 63.1 39 15.9 52 21.1online-B B 247 165 66.8 64 25.9 18 7.3Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.test set changed improved worsened indefinite BLEUcount % count % count % before after diffnewssyscombtest2010 104 52 50.0 20 19.2 32 30.8 16.99 17.38 0.39newssyscombtest2011 101 66 65.3 19 18.8 16 15.8 13.99 13.87 -0.12Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across differentdatasets.
330 sentences were randomly selected from each of the WMT10 and WMT11 test sets.
Both manual scoresand BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.WMT11 sets as translated by CU-TWOSTEP and cor-rected by DEPFIX.
Table 4 shows that WMT10 andWMT11 are comparable in manually estimated im-provement (50?65%).
BLEU does not indicate thatand even estimates a drop in quality on this subsetWMT11.
(The absolute BLEU scores differ fromBLEUs on the whole test sets but we are interestedonly in the change of the scores.)
BLEU is thus notvery suitable for the evaluation of DEPFIX.5 Conclusions and future workManual evaluation shows that our DEPFIX approachto improving MT output quality is sensible.
Al-though it is unable to correct many serious MT er-rors, such as wrong lexical choices, it can improvethe grammaticality of the output in a way that thelanguage model often cannot, which leads to out-put that is considered to be better by humans.
Wealso suggest that BLEU is not appropriate metricfor measuring changes in grammatical correctnessof sentences, especially with inflective languages.An advantage of our method is that it is possibleto apply it on output of any MT system (although itworks better for phrase-based MT systems).
WhileDEPFIX has been developed using the output of CU-BOJAR, the rules we devised are not specific to anyMT system.
They simply describe several grammat-ical rules of Czech language that can be machine-checked and if errors are found, the output can becorrected.
Moreover, our method only requires thesource sentence and the translation output for its op-eration ?
i.e.
it is not necessary to modify the MTsystem itself.We are now considering modifications of theparser so that it is able to parse the incorrect sen-tences produced by MT.
Theoretically it would bepossible to train the parser on annotated ungrammat-ical sentences, but we do not want to invest such an-notation labour.
Instead, when parsing the Czechsentence we will make the parser utilize the infor-mation contained in the parse tree of the Englishsentence, which is usually correct.
We will proba-bly also have to make the parser put less weight tothe often incorrect tagger output.
An alternative isto avoid parsing of the target and project the sourceparse to the target side using word alignments, ifprovided by the MT system.Because some of our rules are able to work usingonly the tagger output, we will also try to apply thembefore the parsing as they might help the parser bycorrecting some of the tags.We will also try several modifications of the tag-ger, but the English sentence does not help us somuch here, because it does not contain any infor-mation regarding the most common errors ?
in-correct assignment of morphological gender andcase.
However, it could help with part of speechand morphological number disambiguation.
More-over, it would be probably helpful for us if the tag-ger included several most probable hypotheses, asthe single-output-only disambiguation is often erro-neous on ungrammatical sentences.431ReferencesOndrej Bojar and Kamil Kos.
2010.
2010 Failures inEnglish-Czech Phrase-Based MT.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 60?66, Uppsala, Swe-den, July.
Association for Computational Linguistics.Ondr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng0.9:Large Parallel Treebank with Rich Annotation.Prague Bulletin of Mathematical Linguistics, 92.Jan Hajic?
et al 2006.
Prague Dependency Treebank 2.0.CD-ROM, Linguistic Data Consortium, LDC CatalogNo.
: LDC2006T0 1, Philadelphia.Natalia Klyueva and Ondr?ej Bojar.
2008.
UMC 0.1:Czech-Russian-English Multilingual Corpus.
In Pro-ceedings of International Conference Corpus Linguis-tics, pages 188?195, Saint-Petersburg.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In HLT ?05: Proceed-ings of the conference on Human Language Technol-ogy and Empirical Methods in Natural Language Pro-cessing, pages 523?530, Vancouver, British Columbia,Canada.Va?clav Nova?k and Zdene?k Z?abokrtsky?.
2007.
Featureengineering in maximum spanning tree dependencyparser.
In Va?clav Matous?ek and Pavel Mautner, edi-tors, Lecture Notes in Artificial Intelligence, Proceed-ings of the 10th I nternational Conference on Text,Speech and Dialogue, Lecture Notes in Computer Sci-ence, pages 92?98, Pilsen, Czech Republic.
SpringerScience+Business Media Deutschland GmbH.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
TectoMT:modular NLP framework.
In Proceedings of the 7thinternational conference on Advances in natural lan-guage processing, IceTAL?10, pages 293?304, Berlin,Heidelberg.
Springer-Verlag.Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007.Statistical phrase-based post-editing.
In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 508?515, Rochester, New York, April.Association for Computational Linguistics.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-bec, and Pavel Kve?ton?.
2007.
The best of two worlds:Cooperation of statistical and rule-based taggers forczech.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007,pages 67?74, Praha.Da?niel Varga et al 2005.
Parallel corpora for mediumdensity languages.
In Proceedings of the Recent Ad-vances in Natural Language Processing, pages 590?596, Borovets, Bulgaria.432
